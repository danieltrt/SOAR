file_path,api_count,code
emotions.py,0,"b""import cv2\nimport numpy as np\nfrom keras.models import load_model\nfrom statistics import mode\nfrom utils.datasets import get_labels\nfrom utils.inference import detect_faces\nfrom utils.inference import draw_text\nfrom utils.inference import draw_bounding_box\nfrom utils.inference import apply_offsets\nfrom utils.inference import load_detection_model\nfrom utils.preprocessor import preprocess_input\n\nUSE_WEBCAM = True # If false, loads video file source\n\n# parameters for loading data and images\nemotion_model_path = './models/emotion_model.hdf5'\nemotion_labels = get_labels('fer2013')\n\n# hyper-parameters for bounding boxes shape\nframe_window = 10\nemotion_offsets = (20, 40)\n\n# loading models\nface_cascade = cv2.CascadeClassifier('./models/haarcascade_frontalface_default.xml')\nemotion_classifier = load_model(emotion_model_path)\n\n# getting input model shapes for inference\nemotion_target_size = emotion_classifier.input_shape[1:3]\n\n# starting lists for calculating modes\nemotion_window = []\n\n# starting video streaming\n\ncv2.namedWindow('window_frame')\nvideo_capture = cv2.VideoCapture(0)\n\n# Select video or webcam feed\ncap = None\nif (USE_WEBCAM == True):\n    cap = cv2.VideoCapture(0) # Webcam source\nelse:\n    cap = cv2.VideoCapture('./demo/dinner.mp4') # Video file source\n\nwhile cap.isOpened(): # True:\n    ret, bgr_image = cap.read()\n\n    #bgr_image = video_capture.read()[1]\n\n    gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n\n    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5,\n\t\t\tminSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n\n    for face_coordinates in faces:\n\n        x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n        gray_face = gray_image[y1:y2, x1:x2]\n        try:\n            gray_face = cv2.resize(gray_face, (emotion_target_size))\n        except:\n            continue\n\n        gray_face = preprocess_input(gray_face, True)\n        gray_face = np.expand_dims(gray_face, 0)\n        gray_face = np.expand_dims(gray_face, -1)\n        emotion_prediction = emotion_classifier.predict(gray_face)\n        emotion_probability = np.max(emotion_prediction)\n        emotion_label_arg = np.argmax(emotion_prediction)\n        emotion_text = emotion_labels[emotion_label_arg]\n        emotion_window.append(emotion_text)\n\n        if len(emotion_window) > frame_window:\n            emotion_window.pop(0)\n        try:\n            emotion_mode = mode(emotion_window)\n        except:\n            continue\n\n        if emotion_text == 'angry':\n            color = emotion_probability * np.asarray((255, 0, 0))\n        elif emotion_text == 'sad':\n            color = emotion_probability * np.asarray((0, 0, 255))\n        elif emotion_text == 'happy':\n            color = emotion_probability * np.asarray((255, 255, 0))\n        elif emotion_text == 'surprise':\n            color = emotion_probability * np.asarray((0, 255, 255))\n        else:\n            color = emotion_probability * np.asarray((0, 255, 0))\n\n        color = color.astype(int)\n        color = color.tolist()\n\n        draw_bounding_box(face_coordinates, rgb_image, color)\n        draw_text(face_coordinates, rgb_image, emotion_mode,\n                  color, 0, -45, 1, 1)\n\n    bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n    cv2.imshow('window_frame', bgr_image)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n"""
utils/__init__.py,0,b''
utils/data_augmentation.py,0,"b'import numpy as np\nfrom random import shuffle\nfrom .preprocessor import preprocess_input\nfrom .preprocessor import _imread as imread\nfrom .preprocessor import _imresize as imresize\nfrom .preprocessor import to_categorical\nimport scipy.ndimage as ndi\nimport cv2\n\nclass ImageGenerator(object):\n    """""" Image generator with saturation, brightness, lighting, contrast,\n    horizontal flip and vertical flip transformations. It supports\n    bounding boxes coordinates.\n\n    TODO:\n        - Finish support for not using bounding_boxes\n            - Random crop\n            - Test other transformations\n    """"""\n    def __init__(self, ground_truth_data, batch_size, image_size,\n                train_keys, validation_keys,\n                ground_truth_transformer=None,\n                path_prefix=None,\n                saturation_var=0.5,\n                brightness_var=0.5,\n                contrast_var=0.5,\n                lighting_std=0.5,\n                horizontal_flip_probability=0.5,\n                vertical_flip_probability=0.5,\n                do_random_crop=False,\n                grayscale=False,\n                zoom_range=[0.75, 1.25],\n                translation_factor=.3):\n\n        self.ground_truth_data = ground_truth_data\n        self.ground_truth_transformer = ground_truth_transformer\n        self.batch_size = batch_size\n        self.path_prefix = path_prefix\n        self.train_keys = train_keys\n        self.validation_keys = validation_keys\n        self.image_size = image_size\n        self.grayscale = grayscale\n        self.color_jitter = []\n        if saturation_var:\n            self.saturation_var = saturation_var\n            self.color_jitter.append(self.saturation)\n        if brightness_var:\n            self.brightness_var = brightness_var\n            self.color_jitter.append(self.brightness)\n        if contrast_var:\n            self.contrast_var = contrast_var\n            self.color_jitter.append(self.contrast)\n        self.lighting_std = lighting_std\n        self.horizontal_flip_probability = horizontal_flip_probability\n        self.vertical_flip_probability = vertical_flip_probability\n        self.do_random_crop = do_random_crop\n        self.zoom_range = zoom_range\n        self.translation_factor = translation_factor\n\n    def _do_random_crop(self, image_array):\n        """"""IMPORTANT: random crop only works for classification since the\n        current implementation does no transform bounding boxes""""""\n        height = image_array.shape[0]\n        width = image_array.shape[1]\n        x_offset = np.random.uniform(0, self.translation_factor * width)\n        y_offset = np.random.uniform(0, self.translation_factor * height)\n        offset = np.array([x_offset, y_offset])\n        scale_factor = np.random.uniform(self.zoom_range[0],\n                                        self.zoom_range[1])\n        crop_matrix = np.array([[scale_factor, 0],\n                                [0, scale_factor]])\n\n        image_array = np.rollaxis(image_array, axis=-1, start=0)\n        image_channel = [ndi.interpolation.affine_transform(image_channel,\n                        crop_matrix, offset=offset, order=0, mode=\'nearest\',\n                        cval=0.0) for image_channel in image_array]\n\n        image_array = np.stack(image_channel, axis=0)\n        image_array = np.rollaxis(image_array, 0, 3)\n        return image_array\n\n    def do_random_rotation(self, image_array):\n        """"""IMPORTANT: random rotation only works for classification since the\n        current implementation does no transform bounding boxes""""""\n        height = image_array.shape[0]\n        width = image_array.shape[1]\n        x_offset = np.random.uniform(0, self.translation_factor * width)\n        y_offset = np.random.uniform(0, self.translation_factor * height)\n        offset = np.array([x_offset, y_offset])\n        scale_factor = np.random.uniform(self.zoom_range[0],\n                                        self.zoom_range[1])\n        crop_matrix = np.array([[scale_factor, 0],\n                                [0, scale_factor]])\n\n        image_array = np.rollaxis(image_array, axis=-1, start=0)\n        image_channel = [ndi.interpolation.affine_transform(image_channel,\n                        crop_matrix, offset=offset, order=0, mode=\'nearest\',\n                        cval=0.0) for image_channel in image_array]\n\n        image_array = np.stack(image_channel, axis=0)\n        image_array = np.rollaxis(image_array, 0, 3)\n        return image_array\n\n    def _gray_scale(self, image_array):\n        return image_array.dot([0.299, 0.587, 0.114])\n\n    def saturation(self, image_array):\n        gray_scale = self._gray_scale(image_array)\n        alpha = 2.0 * np.random.random() * self.brightness_var\n        alpha = alpha + 1 - self.saturation_var\n        image_array = alpha * image_array + (1 - alpha) * gray_scale[:, :, None]\n        return np.clip(image_array, 0, 255)\n\n    def brightness(self, image_array):\n        alpha = 2 * np.random.random() * self.brightness_var\n        alpha = alpha + 1 - self.saturation_var\n        image_array = alpha * image_array\n        return np.clip(image_array, 0, 255)\n\n    def contrast(self, image_array):\n        gray_scale = (self._gray_scale(image_array).mean() *\n                        np.ones_like(image_array))\n        alpha = 2 * np.random.random() * self.contrast_var\n        alpha = alpha + 1 - self.contrast_var\n        image_array = image_array * alpha + (1 - alpha) * gray_scale\n        return np.clip(image_array, 0, 255)\n\n    def lighting(self, image_array):\n        covariance_matrix = np.cov(image_array.reshape(-1,3) /\n                                    255.0, rowvar=False)\n        eigen_values, eigen_vectors = np.linalg.eigh(covariance_matrix)\n        noise = np.random.randn(3) * self.lighting_std\n        noise = eigen_vectors.dot(eigen_values * noise) * 255\n        image_array = image_array + noise\n        return np.clip(image_array, 0 ,255)\n\n    def horizontal_flip(self, image_array, box_corners=None):\n        if np.random.random() < self.horizontal_flip_probability:\n            image_array = image_array[:, ::-1]\n            if box_corners != None:\n                box_corners[:, [0, 2]] = 1 - box_corners[:, [2, 0]]\n        return image_array, box_corners\n\n    def vertical_flip(self, image_array, box_corners=None):\n        if (np.random.random() < self.vertical_flip_probability):\n            image_array = image_array[::-1]\n            if box_corners != None:\n                box_corners[:, [1, 3]] = 1 - box_corners[:, [3, 1]]\n        return image_array, box_corners\n\n    def transform(self, image_array, box_corners=None):\n        shuffle(self.color_jitter)\n        for jitter in self.color_jitter:\n            image_array = jitter(image_array)\n\n        if self.lighting_std:\n            image_array = self.lighting(image_array)\n\n        if self.horizontal_flip_probability > 0:\n            image_array, box_corners = self.horizontal_flip(image_array,\n                                                            box_corners)\n\n        if self.vertical_flip_probability > 0:\n            image_array, box_corners = self.vertical_flip(image_array,\n                                                            box_corners)\n        return image_array, box_corners\n\n    def preprocess_images(self, image_array):\n        return preprocess_input(image_array)\n\n    def flow(self, mode=\'train\'):\n            while True:\n                if mode ==\'train\':\n                    shuffle(self.train_keys)\n                    keys = self.train_keys\n                elif mode == \'val\' or  mode == \'demo\':\n                    shuffle(self.validation_keys)\n                    keys = self.validation_keys\n                else:\n                    raise Exception(\'invalid mode: %s\' % mode)\n\n                inputs = []\n                targets = []\n                for key in keys:\n                    image_path = self.path_prefix + key\n                    image_array = imread(image_path)\n                    image_array = imresize(image_array, self.image_size)\n\n                    num_image_channels = len(image_array.shape)\n                    if num_image_channels != 3:\n                        continue\n\n                    ground_truth = self.ground_truth_data[key]\n\n                    if self.do_random_crop:\n                        image_array = self._do_random_crop(image_array)\n\n                    image_array = image_array.astype(\'float32\')\n                    if mode == \'train\' or mode == \'demo\':\n                        if self.ground_truth_transformer != None:\n                            image_array, ground_truth = self.transform(\n                                                                image_array,\n                                                                ground_truth)\n                            ground_truth = (\n                                self.ground_truth_transformer.assign_boxes(\n                                                            ground_truth))\n                        else:\n                            image_array = self.transform(image_array)[0]\n\n                    if self.grayscale:\n                        image_array = cv2.cvtColor(image_array.astype(\'uint8\'),\n                                        cv2.COLOR_RGB2GRAY).astype(\'float32\')\n                        image_array = np.expand_dims(image_array, -1)\n\n                    inputs.append(image_array)\n                    targets.append(ground_truth)\n                    if len(targets) == self.batch_size:\n                        inputs = np.asarray(inputs)\n                        targets = np.asarray(targets)\n                        # this will not work for boxes\n                        targets = to_categorical(targets)\n                        if mode == \'train\' or mode == \'val\':\n                            inputs = self.preprocess_images(inputs)\n                            yield self._wrap_in_dictionary(inputs, targets)\n                        if mode == \'demo\':\n                            yield self._wrap_in_dictionary(inputs, targets)\n                        inputs = []\n                        targets = []\n\n    def _wrap_in_dictionary(self, image_array, targets):\n        return [{\'input_1\':image_array},\n                {\'predictions\':targets}]\n'"
utils/datasets.py,0,"b'from scipy.io import loadmat\nimport pandas as pd\nimport numpy as np\nfrom random import shuffle\nimport os\nimport cv2\n\nclass DataManager(object):\n    """"""Class for loading fer2013 emotion classification dataset or\n        imdb gender classification dataset.""""""\n    def __init__(self, dataset_name=\'imdb\', dataset_path=None, image_size=(48, 48)):\n\n        self.dataset_name = dataset_name\n        self.dataset_path = dataset_path\n        self.image_size = image_size\n        if self.dataset_path != None:\n            self.dataset_path = dataset_path\n        elif self.dataset_name == \'imdb\':\n            self.dataset_path = \'../datasets/imdb_crop/imdb.mat\'\n        elif self.dataset_name == \'fer2013\':\n            self.dataset_path = \'../datasets/fer2013/fer2013.csv\'\n        elif self.dataset_name == \'KDEF\':\n            self.dataset_path = \'../datasets/KDEF/\'\n        else:\n            raise Exception(\'Incorrect dataset name, please input imdb or fer2013\')\n\n    def get_data(self):\n        if self.dataset_name == \'imdb\':\n            ground_truth_data = self._load_imdb()\n        elif self.dataset_name == \'fer2013\':\n            ground_truth_data = self._load_fer2013()\n        elif self.dataset_name == \'KDEF\':\n            ground_truth_data = self._load_KDEF()\n        return ground_truth_data\n\n    def _load_imdb(self):\n        face_score_treshold = 3\n        dataset = loadmat(self.dataset_path)\n        image_names_array = dataset[\'imdb\'][\'full_path\'][0, 0][0]\n        gender_classes = dataset[\'imdb\'][\'gender\'][0, 0][0]\n        face_score = dataset[\'imdb\'][\'face_score\'][0, 0][0]\n        second_face_score = dataset[\'imdb\'][\'second_face_score\'][0, 0][0]\n        face_score_mask = face_score > face_score_treshold\n        second_face_score_mask = np.isnan(second_face_score)\n        unknown_gender_mask = np.logical_not(np.isnan(gender_classes))\n        mask = np.logical_and(face_score_mask, second_face_score_mask)\n        mask = np.logical_and(mask, unknown_gender_mask)\n        image_names_array = image_names_array[mask]\n        gender_classes = gender_classes[mask].tolist()\n        image_names = []\n        for image_name_arg in range(image_names_array.shape[0]):\n            image_name = image_names_array[image_name_arg][0]\n            image_names.append(image_name)\n        return dict(zip(image_names, gender_classes))\n\n    def _load_fer2013(self):\n        data = pd.read_csv(self.dataset_path)\n        pixels = data[\'pixels\'].tolist()\n        width, height = 48, 48\n        faces = []\n        for pixel_sequence in pixels:\n            face = [int(pixel) for pixel in pixel_sequence.split(\' \')]\n            face = np.asarray(face).reshape(width, height)\n            face = cv2.resize(face.astype(\'uint8\'), self.image_size)\n            faces.append(face.astype(\'float32\'))\n        faces = np.asarray(faces)\n        faces = np.expand_dims(faces, -1)\n        emotions = pd.get_dummies(data[\'emotion\']).as_matrix()\n        return faces, emotions\n\n    def _load_KDEF(self):\n        class_to_arg = get_class_to_arg(self.dataset_name)\n        num_classes = len(class_to_arg)\n\n        file_paths = []\n        for folder, subfolders, filenames in os.walk(self.dataset_path):\n            for filename in filenames:\n                if filename.lower().endswith((\'.jpg\')):\n                    file_paths.append(os.path.join(folder, filename))\n\n        num_faces = len(file_paths)\n        y_size, x_size = self.image_size\n        faces = np.zeros(shape=(num_faces, y_size, x_size))\n        emotions = np.zeros(shape=(num_faces, num_classes))\n        for file_arg, file_path in enumerate(file_paths):\n            image_array = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n            image_array = cv2.resize(image_array, (y_size, x_size))\n            faces[file_arg] = image_array\n            file_basename = os.path.basename(file_path)\n            file_emotion = file_basename[4:6]\n            # there are two file names in the dataset that don\'t match the given classes\n            try:\n                emotion_arg = class_to_arg[file_emotion]\n            except:\n                continue\n            emotions[file_arg, emotion_arg] = 1\n        faces = np.expand_dims(faces, -1)\n        return faces, emotions\n\ndef get_labels(dataset_name):\n    if dataset_name == \'fer2013\':\n        return {0:\'angry\',1:\'disgust\',2:\'fear\',3:\'happy\',\n                4:\'sad\',5:\'surprise\',6:\'neutral\'}\n    elif dataset_name == \'imdb\':\n        return {0:\'woman\', 1:\'man\'}\n    elif dataset_name == \'KDEF\':\n        return {0:\'AN\', 1:\'DI\', 2:\'AF\', 3:\'HA\', 4:\'SA\', 5:\'SU\', 6:\'NE\'}\n    else:\n        raise Exception(\'Invalid dataset name\')\n\ndef get_class_to_arg(dataset_name=\'fer2013\'):\n    if dataset_name == \'fer2013\':\n        return {\'angry\':0, \'disgust\':1, \'fear\':2, \'happy\':3, \'sad\':4,\n                \'surprise\':5, \'neutral\':6}\n    elif dataset_name == \'imdb\':\n        return {\'woman\':0, \'man\':1}\n    elif dataset_name == \'KDEF\':\n        return {\'AN\':0, \'DI\':1, \'AF\':2, \'HA\':3, \'SA\':4, \'SU\':5, \'NE\':6}\n    else:\n        raise Exception(\'Invalid dataset name\')\n\ndef split_imdb_data(ground_truth_data, validation_split=.2, do_shuffle=False):\n    ground_truth_keys = sorted(ground_truth_data.keys())\n    if do_shuffle == True:\n        shuffle(ground_truth_keys)\n    training_split = 1 - validation_split\n    num_train = int(training_split * len(ground_truth_keys))\n    train_keys = ground_truth_keys[:num_train]\n    validation_keys = ground_truth_keys[num_train:]\n    return train_keys, validation_keys\n\ndef split_data(x, y, validation_split=.2):\n    num_samples = len(x)\n    num_train_samples = int((1 - validation_split)*num_samples)\n    train_x = x[:num_train_samples]\n    train_y = y[:num_train_samples]\n    val_x = x[num_train_samples:]\n    val_y = y[num_train_samples:]\n    train_data = (train_x, train_y)\n    val_data = (val_x, val_y)\n    return train_data, val_data\n\n'"
utils/grad_cam.py,5,"b'import cv2\nimport h5py\nimport keras\nimport keras.backend as K\nfrom keras.layers.core import Lambda\nfrom keras.models import Sequential\nfrom keras.models import load_model\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\n\nfrom .preprocessor import preprocess_input\n\n\ndef reset_optimizer_weights(model_filename):\n    model = h5py.File(model_filename, \'r+\')\n    del model[\'optimizer_weights\']\n    model.close()\n\n\ndef target_category_loss(x, category_index, num_classes):\n    return tf.multiply(x, K.one_hot([category_index], num_classes))\n\n\ndef target_category_loss_output_shape(input_shape):\n    return input_shape\n\n\ndef normalize(x):\n    # utility function to normalize a tensor by its L2 norm\n    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n\n\ndef load_image(image_array):\n    image_array = np.expand_dims(image_array, axis=0)\n    image_array = preprocess_input(image_array)\n    return image_array\n\n\ndef register_gradient():\n    if ""GuidedBackProp"" not in ops._gradient_registry._registry:\n        @ops.RegisterGradient(""GuidedBackProp"")\n        def _GuidedBackProp(op, gradient):\n            dtype = op.inputs[0].dtype\n            guided_gradient = (gradient * tf.cast(gradient > 0., dtype) *\n                               tf.cast(op.inputs[0] > 0., dtype))\n            return guided_gradient\n\n\ndef compile_saliency_function(model, activation_layer=\'conv2d_7\'):\n    input_image = model.input\n    layer_output = model.get_layer(activation_layer).output\n    max_output = K.max(layer_output, axis=3)\n    saliency = K.gradients(K.sum(max_output), input_image)[0]\n    return K.function([input_image, K.learning_phase()], [saliency])\n\n\ndef modify_backprop(model, name, task):\n    graph = tf.get_default_graph()\n    with graph.gradient_override_map({\'Relu\': name}):\n\n        # get layers that have an activation\n        activation_layers = [layer for layer in model.layers\n                             if hasattr(layer, \'activation\')]\n\n        # replace relu activation\n        for layer in activation_layers:\n            if layer.activation == keras.activations.relu:\n                layer.activation = tf.nn.relu\n\n        # re-instanciate a new model\n        if task == \'gender\':\n            model_path = \'../trained_models/gender_models/gender_mini_XCEPTION.21-0.95.hdf5\'\n        elif task == \'emotion\':\n            model_path = \'../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5\'\n            # model_path = \'../trained_models/fer2013_mini_XCEPTION.119-0.65.hdf5\'\n            # model_path = \'../trained_models/fer2013_big_XCEPTION.54-0.66.hdf5\'\n        new_model = load_model(model_path, compile=False)\n    return new_model\n\n\ndef deprocess_image(x):\n    """""" Same normalization as in:\n    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n    """"""\n    if np.ndim(x) > 3:\n        x = np.squeeze(x)\n    # normalize tensor: center on 0., ensure std is 0.1\n    x = x - x.mean()\n    x = x / (x.std() + 1e-5)\n    x = x * 0.1\n\n    # clip to [0, 1]\n    x = x + 0.5\n    x = np.clip(x, 0, 1)\n\n    # convert to RGB array\n    x = x * 255\n    if K.image_dim_ordering() == \'th\':\n        x = x.transpose((1, 2, 0))\n    x = np.clip(x, 0, 255).astype(\'uint8\')\n    return x\n\ndef compile_gradient_function(input_model, category_index, layer_name):\n    model = Sequential()\n    model.add(input_model)\n\n    num_classes = model.output_shape[1]\n    target_layer = lambda x: target_category_loss(x, category_index, num_classes)\n    model.add(Lambda(target_layer,\n                     output_shape = target_category_loss_output_shape))\n\n    loss = K.sum(model.layers[-1].output)\n    conv_output = model.layers[0].get_layer(layer_name).output\n    gradients = normalize(K.gradients(loss, conv_output)[0])\n    gradient_function = K.function([model.layers[0].input, K.learning_phase()],\n                                                    [conv_output, gradients])\n    return gradient_function\n\ndef calculate_gradient_weighted_CAM(gradient_function, image):\n    output, evaluated_gradients = gradient_function([image, False])\n    output, evaluated_gradients = output[0, :], evaluated_gradients[0, :, :, :]\n    weights = np.mean(evaluated_gradients, axis = (0, 1))\n    CAM = np.ones(output.shape[0 : 2], dtype=np.float32)\n    for weight_arg, weight in enumerate(weights):\n        CAM = CAM + (weight * output[:, :, weight_arg])\n    CAM = cv2.resize(CAM, (64, 64))\n    CAM = np.maximum(CAM, 0)\n    heatmap = CAM / np.max(CAM)\n\n    #Return to BGR [0..255] from the preprocessed image\n    image = image[0, :]\n    image = image - np.min(image)\n    image = np.minimum(image, 255)\n\n    CAM = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n    CAM = np.float32(CAM) + np.float32(image)\n    CAM = 255 * CAM / np.max(CAM)\n    return np.uint8(CAM), heatmap\n\ndef calculate_guided_gradient_CAM(preprocessed_input, gradient_function, saliency_function):\n    CAM, heatmap = calculate_gradient_weighted_CAM(gradient_function, preprocessed_input)\n    saliency = saliency_function([preprocessed_input, 0])\n    gradCAM = saliency[0] * heatmap[..., np.newaxis]\n    #return deprocess_image(gradCAM)\n    return deprocess_image(saliency[0])\n    #return saliency[0]\n\ndef calculate_guided_gradient_CAM_v2(preprocessed_input, gradient_function,\n                                    saliency_function, target_size=(128, 128)):\n    CAM, heatmap = calculate_gradient_weighted_CAM(gradient_function, preprocessed_input)\n    heatmap = np.squeeze(heatmap)\n    heatmap = cv2.resize(heatmap.astype(\'uint8\'), target_size)\n    saliency = saliency_function([preprocessed_input, 0])\n    saliency = np.squeeze(saliency[0])\n    saliency = cv2.resize(saliency.astype(\'uint8\'), target_size)\n    gradCAM = saliency * heatmap\n    gradCAM =  deprocess_image(gradCAM)\n    return np.expand_dims(gradCAM, -1)\n\n\nif __name__ == \'__main__\':\n    import pickle\n    faces = pickle.load(open(\'faces.pkl\',\'rb\'))\n    face = faces[0]\n    model_filename = \'../../trained_models/emotion_models/mini_XCEPTION.523-0.65.hdf5\'\n    #reset_optimizer_weights(model_filename)\n    model = load_model(model_filename)\n\n    preprocessed_input = load_image(face)\n    predictions = model.predict(preprocessed_input)\n    predicted_class = np.argmax(predictions)\n    gradient_function = compile_gradient_function(model, predicted_class, \'conv2d_6\')\n    register_gradient()\n    guided_model = modify_backprop(model, \'GuidedBackProp\')\n    saliency_function = compile_saliency_function(guided_model)\n    guided_gradCAM = calculate_guided_gradient_CAM(preprocessed_input,\n                                gradient_function, saliency_function)\n\n    cv2.imwrite(\'guided_gradCAM.jpg\', guided_gradCAM)\n\n\n'"
utils/inference.py,0,"b'import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras.preprocessing import image\n\ndef load_image(image_path, grayscale=False, target_size=None):\n    pil_image = image.load_img(image_path, grayscale, target_size)\n    return image.img_to_array(pil_image)\n\ndef load_detection_model(model_path):\n    detection_model = cv2.CascadeClassifier(model_path)\n    return detection_model\n\ndef detect_faces(detection_model, gray_image_array):\n    return detection_model.detectMultiScale(gray_image_array, 1.3, 5)\n\ndef draw_bounding_box(face_coordinates, image_array, color):\n    x, y, w, h = face_coordinates\n    cv2.rectangle(image_array, (x, y), (x + w, y + h), color, 2)\n\ndef apply_offsets(face_coordinates, offsets):\n    x, y, width, height = face_coordinates\n    x_off, y_off = offsets\n    return (x - x_off, x + width + x_off, y - y_off, y + height + y_off)\n\ndef draw_text(coordinates, image_array, text, color, x_offset=0, y_offset=0,\n                                                font_scale=2, thickness=2):\n    x, y = coordinates[:2]\n    cv2.putText(image_array, text, (x + x_offset, y + y_offset),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                font_scale, color, thickness, cv2.LINE_AA)\n\ndef get_colors(num_classes):\n    colors = plt.cm.hsv(np.linspace(0, 1, num_classes)).tolist()\n    colors = np.asarray(colors) * 255\n    return colors\n\n'"
utils/preprocessor.py,0,"b""import numpy as np\nfrom scipy.misc import imread, imresize\n\n\ndef preprocess_input(x, v2=True):\n    x = x.astype('float32')\n    x = x / 255.0\n    if v2:\n        x = x - 0.5\n        x = x * 2.0\n    return x\n\ndef _imread(image_name):\n        return imread(image_name)\n\ndef _imresize(image_array, size):\n        return imresize(image_array, size)\n\ndef to_categorical(integer_classes, num_classes=2):\n    integer_classes = np.asarray(integer_classes, dtype='int')\n    num_samples = integer_classes.shape[0]\n    categorical = np.zeros((num_samples, num_classes))\n    categorical[np.arange(num_samples), integer_classes] = 1\n    return categorical\n\n"""
utils/visualizer.py,0,"b'import numpy as np\nimport matplotlib.cm as cm\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport matplotlib.pyplot as plt\nimport numpy.ma as ma\nimport cv2\nfrom .inference import draw_text\n\ndef make_mosaic(images, num_rows, num_cols, border=1, class_names=None):\n    num_images = len(images)\n    image_shape = images.shape[1:]\n    mosaic = ma.masked_all((num_rows * image_shape[0] + (num_rows - 1) * border,\n                            num_cols * image_shape[1] + (num_cols - 1) * border),\n                            dtype=np.float32)\n    paddedh = image_shape[0] + border\n    paddedw = image_shape[1] + border\n    for image_arg in range(num_images):\n        row = int(np.floor(image_arg / num_cols))\n        col = image_arg % num_cols\n        image = np.squeeze(images[image_arg])\n        image_shape = image.shape\n        mosaic[row * paddedh:row * paddedh + image_shape[0],\n               col * paddedw:col * paddedw + image_shape[1]] = image\n    return mosaic\n\ndef make_mosaic_v2(images, num_mosaic_rows=None,\n                num_mosaic_cols=None, border=1):\n    images = np.squeeze(images)\n    num_images, image_pixels_rows, image_pixels_cols = images.shape\n    if num_mosaic_rows is None and num_mosaic_cols is None:\n        box_size = int(np.ceil(np.sqrt(num_images)))\n        num_mosaic_rows = num_mosaic_cols = box_size\n    num_mosaic_pixel_rows = num_mosaic_rows * (image_pixels_rows + border)\n    num_mosaic_pixel_cols = num_mosaic_cols * (image_pixels_cols + border)\n    mosaic = np.empty(shape=(num_mosaic_pixel_rows, num_mosaic_pixel_cols))\n    mosaic_col_arg = 0\n    mosaic_row_arg = 0\n    for image_arg in range(num_images):\n        if image_arg % num_mosaic_cols == 0 and image_arg != 0:\n            mosaic_col_arg = mosaic_col_arg + 1\n            mosaic_row_arg = 0\n        x0 = image_pixels_cols * (mosaic_row_arg)\n        x1 = image_pixels_cols * (mosaic_row_arg + 1)\n        y0 = image_pixels_rows * (mosaic_col_arg)\n        y1 = image_pixels_rows * (mosaic_col_arg + 1)\n        image = images[image_arg]\n        mosaic[y0:y1, x0:x1] = image\n        mosaic_row_arg = mosaic_row_arg + 1\n    return mosaic\n\ndef pretty_imshow(axis, data, vmin=None, vmax=None, cmap=None):\n    if cmap is None:\n        cmap = cm.jet\n    if vmin is None:\n        vmin = data.min()\n    if vmax is None:\n        vmax = data.max()\n    cax = None\n    divider = make_axes_locatable(axis)\n    cax = divider.append_axes(\'right\', size=\'5%\', pad=0.05)\n    image = axis.imshow(data, vmin=vmin, vmax=vmax,\n                        interpolation=\'nearest\', cmap=cmap)\n    plt.colorbar(image, cax=cax)\n\ndef normal_imshow(axis, data, vmin=None, vmax=None,\n                        cmap=None, axis_off=True):\n    if cmap is None:\n        cmap = cm.jet\n    if vmin is None:\n        vmin = data.min()\n    if vmax is None:\n        vmax = data.max()\n    image = axis.imshow(data, vmin=vmin, vmax=vmax,\n                        interpolation=\'nearest\', cmap=cmap)\n    if axis_off:\n        plt.axis(\'off\')\n    return image\n\ndef display_image(face, class_vector=None,\n                    class_decoder=None, pretty=False):\n    if class_vector is not None and class_decoder is None:\n        raise Exception(\'Provide class decoder\')\n    face = np.squeeze(face)\n    color_map = None\n    if len(face.shape) < 3:\n        color_map = \'gray\'\n    plt.figure()\n    if class_vector is not None:\n        class_arg = np.argmax(class_vector)\n        class_name = class_decoder[class_arg]\n        plt.title(class_name)\n    if pretty:\n        pretty_imshow(plt.gca(), face, cmap=color_map)\n    else:\n        plt.imshow(face, color_map)\n\ndef draw_mosaic(data, num_rows, num_cols, class_vectors=None,\n                            class_decoder=None, cmap=\'gray\'):\n\n    if class_vectors is not None and class_decoder is None:\n        raise Exception(\'Provide class decoder\')\n\n    figure, axis_array = plt.subplots(num_rows, num_cols)\n    figure.set_size_inches(8, 8, forward=True)\n    titles = []\n    if class_vectors is not None:\n        for vector_arg in range(len(class_vectors)):\n            class_arg = np.argmax(class_vectors[vector_arg])\n            class_name = class_decoder[class_arg]\n            titles.append(class_name)\n\n    image_arg = 0\n    for row_arg in range(num_rows):\n        for col_arg in range(num_cols):\n            image = data[image_arg]\n            image = np.squeeze(image)\n            axis_array[row_arg, col_arg].axis(\'off\')\n            axis_array[row_arg, col_arg].imshow(image, cmap=cmap)\n            axis_array[row_arg, col_arg].set_title(titles[image_arg])\n            image_arg = image_arg + 1\n    plt.tight_layout()\n\nif __name__ == \'__main__\':\n    #from utils.data_manager import DataManager\n    from utils.utils import get_labels\n    from keras.models import load_model\n    import pickle\n\n    #dataset_name = \'fer2013\'\n    #model_path = \'../trained_models/emotion_models/simple_CNN.985-0.66.hdf5\'\n    dataset_name = \'fer2013\'\n    class_decoder = get_labels(dataset_name)\n    #data_manager = DataManager(dataset_name)\n    #faces, emotions = data_manager.get_data()\n    faces = pickle.load(open(\'faces.pkl\', \'rb\'))\n    emotions = pickle.load(open(\'emotions.pkl\', \'rb\'))\n    pretty_imshow(plt.gca(), make_mosaic(faces[:4], 2, 2), cmap=\'gray\')\n    plt.show()\n\n    """"""\n    image_arg = 0\n    face = faces[image_arg:image_arg + 1]\n    emotion = emotions[image_arg:image_arg + 1]\n    display_image(face, emotion, class_decoder)\n    plt.show()\n\n    normal_imshow(plt.gca(), make_mosaic(faces[:4], 3, 3), cmap=\'gray\')\n    plt.show()\n\n    draw_mosaic(faces, 2, 2, emotions, class_decoder)\n    plt.show()\n\n    """"""\n    model = load_model(\'../trained_models/emotion_models/simple_CNN.985-0.66.hdf5\')\n    conv1_weights = model.layers[2].get_weights()\n    kernel_conv1_weights = conv1_weights[0]\n    kernel_conv1_weights = np.squeeze(kernel_conv1_weights)\n    kernel_conv1_weights = np.rollaxis(kernel_conv1_weights, 2, 0)\n    kernel_conv1_weights = np.expand_dims(kernel_conv1_weights, -1)\n    num_kernels = kernel_conv1_weights.shape[0]\n    box_size = int(np.ceil(np.sqrt(num_kernels)))\n    print(\'Box size:\', box_size)\n\n    print(\'Kernel shape\', kernel_conv1_weights.shape)\n    plt.figure(figsize=(15, 15))\n    plt.title(\'conv1 weights\')\n    pretty_imshow(plt.gca(),\n            make_mosaic(kernel_conv1_weights, box_size, box_size),\n            cmap=cm.binary)\n    plt.show()\n'"
