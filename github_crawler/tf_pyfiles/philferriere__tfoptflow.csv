file_path,api_count,code
tfoptflow/augment.py,0,"b'""""""\naugment.py\n\nAugmentation utility functions and classes.\nUses numpy, to be run on CPU while GPU learns model params.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n\nTo look at later:\n    https://github.com/Johswald/Bayesian-FlowNet/blob/master/flownet.py (reproduces original FlowNet aug in np)\n    https://github.com/simonmeister/UnFlow/blob/master/src/e2eflow/core/augment.py\n    https://github.com/sampepose/flownet2-tf/blob/master/src/dataloader.py\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport numpy as np\nimport cv2\nimport random  # so we don\'t interfere with the use of np.random in the dataset loader\n\nfrom utils import scale\n\n_DBG_AUG_SET = -1\n\n_DEFAULT_AUG_OPTIONS = {\n    \'aug_type\': \'heavy\',  # in [\'basic\', \'heavy\']\n    \'aug_labels\': True,  # If True, augment both images and labels; otherwise, only augment images\n    \'fliplr\': 0.5,  # Horizontally flip 50% of images\n    \'flipud\': 0.5,  # Vertically flip 50% of images\n    # Translate 50% of images by a value between -5 and +5 percent of original size on x- and y-axis independently\n    \'translate\': (0.5, 0.05),\n    \'scale\': (0.5, 0.05),  # Scale 50% of images by a factor between 95 and 105 percent of original size\n    \'random_seed\': 1969,\n}\n\n\nclass Augmenter(object):\n    """"""Augmenter class.\n    """"""\n\n    def __init__(self, options=_DEFAULT_AUG_OPTIONS):\n        """"""Initialize the Augmenter object.\n        In \'basic\' mode, we only consider \'fliplr\' and \'flipud\'.\n        In \'heavy\' mode, we also consider \'translate\' and \'scale\'.\n        Args:\n            options: see _DEFAULT_AUG_OPTIONS comments\n        """"""\n        self.opts = options\n        assert (self.opts[\'aug_type\'] in [\'basic\', \'heavy\'])\n        random.seed(self.opts[\'random_seed\'])\n\n    ###\n    # Augmentation\n    ###\n    def augment(self, images, labels=None, as_tuple=False):\n        """"""Augment training samples.\n        Args:\n            images: Image pairs in format [N, 2, H, W, 3] or list(((H, W, 3),(H, W, 3)))\n            labels: Optical flows in format [N, H, W, 2] or list((H, W, 2))\n            as_tuple: If True, return image pair tuple; otherwise, return np array in [2, H, W, 3] format\n        Returns:\n            aug_images: list or array of augmented image pairs.\n            aug_labels: list or array of augmented optical flows.\n        """"""\n        # Augment image pairs\n        assert(isinstance(images, list) or isinstance(images, np.ndarray))\n        if labels is not None:\n            assert(isinstance(labels, list) or isinstance(labels, np.ndarray))\n        do_labels = True if self.opts[\'aug_labels\'] and labels is not None else False\n\n        aug_images, aug_labels = [], []\n        for idx in range(len(images)):\n            img_pair = images[idx]\n            assert(len(img_pair[0].shape) == 3 and (img_pair[0].shape[2] == 1 or img_pair[0].shape[2] == 3))\n            assert(len(img_pair[1].shape) == 3 and (img_pair[1].shape[2] == 1 or img_pair[1].shape[2] == 3))\n\n            aug_img_pair = [np.copy(img_pair[0]), np.copy(img_pair[1])]\n            if do_labels:\n                aug_flow = np.copy(labels[idx])\n\n            # Flip horizontally?\n            if self.opts[\'fliplr\'] > 0.:\n                rand = random.random()\n                if rand < self.opts[\'fliplr\']:\n                    aug_img_pair = [np.fliplr(aug_img_pair[0]), np.fliplr(aug_img_pair[1])]\n                    if do_labels:\n                        aug_flow = np.fliplr(aug_flow)\n                        aug_flow[:, :, 0] *= -1\n\n            # Flip vertically?\n            if self.opts[\'flipud\'] > 0.:\n                rand = random.random()\n                if rand < self.opts[\'flipud\']:\n                    aug_img_pair = [np.flipud(aug_img_pair[0]), np.flipud(aug_img_pair[1])]\n                    if do_labels:\n                        aug_flow = np.flipud(aug_flow)\n                        aug_flow[:, :, 1] *= -1\n\n            if self.opts[\'aug_type\'] == \'heavy\':\n                # Translate?\n                if self.opts[\'translate\'][0] > 0.:\n                    rand = random.random()\n                    if rand < self.opts[\'translate\'][0]:\n                        h, w, _ = aug_img_pair[0].shape\n                        tw = int(random.uniform(-self.opts[\'translate\'][1], self.opts[\'translate\'][1]) * w)\n                        th = int(random.uniform(-self.opts[\'translate\'][1], self.opts[\'translate\'][1]) * h)\n                        translation_matrix = np.float32([[1, 0, tw], [0, 1, th]])\n                        aug_img_pair[1] = cv2.warpAffine(aug_img_pair[1], translation_matrix, (w, h))\n                        aug_flow[:, :, 0] += tw\n                        aug_flow[:, :, 1] += th\n\n                # Scale? (clipped, so that the result has the same size as the input)\n                if self.opts[\'scale\'][0] > 0.:\n                    rand = random.random()\n                    if rand < self.opts[\'scale\'][0]:\n                        ratio = random.uniform(1.0 - self.opts[\'scale\'][1], 1.0 + self.opts[\'scale\'][1])\n                        aug_img_pair[0] = scale(aug_img_pair[0], ratio)\n                        aug_img_pair[1] = scale(aug_img_pair[1], ratio)\n                        if do_labels:\n                            aug_flow = scale(aug_flow, ratio)\n                            aug_flow *= ratio\n\n            aug_images.append((aug_img_pair[0], aug_img_pair[1]))\n            if do_labels:\n                aug_labels.append(aug_flow)\n\n        # Return using image in the same format as the input\n        if isinstance(images, np.ndarray):\n            aug_images = np.asarray(aug_images)\n        if do_labels:\n            if isinstance(labels, np.ndarray):\n                aug_labels = np.asarray(aug_labels)\n\n        if do_labels:\n            return aug_images, aug_labels\n        else:\n            return aug_images\n\n    ###\n    # Debug utils\n    ###\n    def print_config(self):\n        """"""Display configuration values.""""""\n        print(""\\nAugmenter Configuration:"")\n        for k, v in self.options.items():\n            print(""  {:20} {}"".format(k, v))\n'"
tfoptflow/ckpt_mgr.py,8,"b'""""""\nckpt_mgr.py\n\nMaintains a directory containing only the best n checkpoints.\n\nWritten by Domenick Poster, modifications by Phil Ferriere\n\nModifications licensed under the MIT License (see LICENSE for details)\n\nBased on:\n    - https://github.com/vonclites/checkmate/blob/master/checkmate.py\n        Written by Domenick Poster, Copyright (C) 2018 Domenick Poster\n        Licensed under MIT License\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport os\nimport glob\nimport json\nimport numpy as np\nimport tensorflow as tf\n\n\nclass BestCheckpointSaver(object):\n    """"""Maintains a directory containing only the best n checkpoints\n\n    Inside the directory is a best_checkpoints JSON file containing a dictionary\n    mapping of the best checkpoint filepaths to the values by which the checkpoints\n    are compared.  Only the best n checkpoints are contained in the directory and JSON file.\n\n    This is a light-weight wrapper class only intended to work in simple,\n    non-distributed settings.  It is not intended to work with the tf.Estimator\n    framework.\n    """"""\n\n    def __init__(self, save_dir, save_file, num_to_keep=5, maximize=True, saver=None):\n        """"""Creates a `BestCheckpointSaver`\n\n        `BestCheckpointSaver` acts as a wrapper class around a `tf.train.Saver`\n\n        Args:\n            save_dir: The directory in which the checkpoint files will be saved\n            save_file: The prefix of the checkpoint filenames\n            num_to_keep: The number of best checkpoint files to retain\n            maximize: Define \'best\' values to be the highest values.  For example,\n              set this to True if selecting for the checkpoints with the highest\n              given accuracy.  Or set to False to select for checkpoints with the\n              lowest given error rate.\n            saver: A `tf.train.Saver` to use for saving checkpoints.  A default\n              `tf.train.Saver` will be created if none is provided.\n        """"""\n        self._num_to_keep = num_to_keep\n        self._save_dir = save_dir\n        self._save_file = save_file\n        self._save_path = os.path.join(save_dir, f\'{save_file}.ckpt\')\n        self._maximize = maximize\n        self._saver = saver if saver else tf.train.Saver(\n            max_to_keep=None,\n            save_relative_paths=True\n        )\n\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        self.best_checkpoints_file = os.path.join(save_dir, \'best_checkpoints\')\n\n    def save(self, ranking_value, sess, global_step_tensor):\n        """"""Updates the set of best checkpoints based on the given result.\n\n        Args:\n            ranking_value: The ranking value by which to rank the checkpoint.\n            sess: A tf.Session to use to save the checkpoint\n            global_step_tensor: A `tf.Tensor` represent the global step\n        """"""\n        global_step = sess.run(global_step_tensor)\n        current_ckpt = f\'{self._save_file}.ckpt-{global_step}\'\n        ranking_value = float(ranking_value)\n        if not os.path.exists(self.best_checkpoints_file):\n            self._save_best_checkpoints_file({current_ckpt: ranking_value})\n            return self._saver.save(sess, self._save_path, global_step_tensor)\n\n        best_checkpoints = self._load_best_checkpoints_file()\n\n        if len(best_checkpoints) < self._num_to_keep:\n            best_checkpoints[current_ckpt] = ranking_value\n            self._save_best_checkpoints_file(best_checkpoints)\n            return self._saver.save(sess, self._save_path, global_step_tensor)\n\n        if self._maximize:\n            should_save = not all(current_best >= ranking_value\n                                  for current_best in best_checkpoints.values())\n        else:\n            should_save = not all(current_best <= ranking_value\n                                  for current_best in best_checkpoints.values())\n        if should_save:\n            best_checkpoint_list = self._sort(best_checkpoints)\n\n            worst_checkpoint = os.path.join(self._save_dir,\n                                            best_checkpoint_list.pop(-1)[0])\n            self._remove_outdated_checkpoint_files(worst_checkpoint)\n            self._update_internal_saver_state(best_checkpoint_list)\n\n            best_checkpoints = dict(best_checkpoint_list)\n            best_checkpoints[current_ckpt] = ranking_value\n            self._save_best_checkpoints_file(best_checkpoints)\n\n            return self._saver.save(sess, self._save_path, global_step_tensor)\n\n    def restore(self, sess, ckpt):\n        """"""Restore from a checkpoint\n        Args:\n            sess: A tf.Session to use to save the checkpoint\n            ckpt: Checkpoint file to restore from\n        """"""\n        self._saver.restore(sess, ckpt)\n\n    def best_checkpoint(self, best_checkpoint_dir, maximize=True):\n        """""" Returns filepath to the best checkpoint\n\n        Reads the best_checkpoints file in the best_checkpoint_dir directory.\n        Returns the filepath in the best_checkpoints file associated with\n        the highest value if select_maximum_value is True, or the filepath\n        associated with the lowest value if select_maximum_value is False.\n\n        Args:\n            best_checkpoint_dir: Directory containing best_checkpoints JSON file\n            maximize: If True, select the filepath associated\n              with the highest value.  Otherwise, select the filepath associated\n              with the lowest value.\n\n        Returns:\n            The full path to the best checkpoint file\n\n        """"""\n        best_checkpoints_file = os.path.join(best_checkpoint_dir, \'best_checkpoints\')\n        if not os.path.exists(best_checkpoints_file):\n            return None\n        with open(best_checkpoints_file, \'r\') as f:\n            best_checkpoints = json.load(f)\n        best_checkpoints = [\n            ckpt for ckpt in sorted(best_checkpoints,\n                                    key=best_checkpoints.get,\n                                    reverse=maximize)\n        ]\n        return os.path.join(best_checkpoint_dir, best_checkpoints[0])\n\n    def _save_best_checkpoints_file(self, updated_best_checkpoints):\n        with open(self.best_checkpoints_file, \'w\') as f:\n            json.dump(updated_best_checkpoints, f, indent=3)\n\n    def _remove_outdated_checkpoint_files(self, worst_checkpoint):\n        os.remove(os.path.join(self._save_dir, \'checkpoint\'))\n        for ckpt_file in glob.glob(worst_checkpoint + \'.*\'):\n            os.remove(ckpt_file)\n\n    def _update_internal_saver_state(self, best_checkpoint_list):\n        best_checkpoint_files = [\n            (ckpt[0], np.inf)  # TODO: Try to use actual file timestamp\n            for ckpt in best_checkpoint_list\n        ]\n        self._saver.set_last_checkpoints_with_time(best_checkpoint_files)\n\n    def _load_best_checkpoints_file(self):\n        with open(self.best_checkpoints_file, \'r\') as f:\n            best_checkpoints = json.load(f)\n        return best_checkpoints\n\n    def _sort(self, best_checkpoints):\n        best_checkpoints = [\n            (ckpt, best_checkpoints[ckpt])\n            for ckpt in sorted(best_checkpoints,\n                               key=best_checkpoints.get,\n                               reverse=self._maximize)\n        ]\n        return best_checkpoints\n'"
tfoptflow/core_costvol.py,6,"b'""""""\ncore_costvol.py\n\nComputes cross correlation between two feature maps.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n\nBased on:\n    - https://github.com/tensorpack/tensorpack/blob/master/examples/OpticalFlow/flownet_models.py\n        Written by Patrick Wieschollek, Copyright Yuxin Wu\n        Apache License 2.0\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\n\n\ndef cost_volume(c1, warp, search_range, name):\n    """"""Build cost volume for associating a pixel from Image1 with its corresponding pixels in Image2.\n    Args:\n        c1: Level of the feature pyramid of Image1\n        warp: Warped level of the feature pyramid of image22\n        search_range: Search range (maximum displacement)\n    """"""\n    padded_lvl = tf.pad(warp, [[0, 0], [search_range, search_range], [search_range, search_range], [0, 0]])\n    _, h, w, _ = tf.unstack(tf.shape(c1))\n    max_offset = search_range * 2 + 1\n\n    cost_vol = []\n    for y in range(0, max_offset):\n        for x in range(0, max_offset):\n            slice = tf.slice(padded_lvl, [0, y, x, 0], [-1, h, w, -1])\n            cost = tf.reduce_mean(c1 * slice, axis=3, keepdims=True)\n            cost_vol.append(cost)\n    cost_vol = tf.concat(cost_vol, axis=3)\n    cost_vol = tf.nn.leaky_relu(cost_vol, alpha=0.1, name=name)\n\n    return cost_vol\n'"
tfoptflow/core_warp.py,3,"b'""""""\ncore_warp.py\n\nWarp a level of image1\'s feature pyramid using the up-sampled flow at level+1 of image2\'s pyramid.\n\nThe official TF implementation requires that batch image height and width be known during graph building.\nOur implementation below doesn\'t have the same requirements. We care about this flexibility because we sometimes train\non small image patches (e.g., (384, 448)) but do online validation on larger sizes (e.g., (436, 1024)).\n\nWritten by The TensorFlow Authors, modifications by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n\nBased on:\n    - https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/contrib/image/python/ops/dense_image_warp.py\n    Written by The TensorFlow Authors, Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n    Licensed under the Apache License 2.0\n\nTF Ref:\n    - tf.contrib.image.dense_image_warp\n    https://www.tensorflow.org/api_docs/python/tf/contrib/image/dense_image_warp\n\nNotes:\n    - A list of CUDA-accelerated implementations:\n    https://github.com/sampepose/flownet2-tf/tree/master/src/ops/correlation\n    https://github.com/jgorgenucsd/corr_tf\n    https://github.com/simonmeister/UnFlow/blob/8e74f2b33138ab72d775bf1c3a9256105677834e/ops/correlation_op.cu.cc\n    - Will there be a cost-volume implementation in tf.contrib at some point?\n    See https://github.com/tensorflow/tensorflow/pull/21392\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\n\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\n\n\ndef _interpolate_bilinear(grid,\n                          query_points,\n                          name=\'interpolate_bilinear\',\n                          indexing=\'ij\'):\n    """"""Similar to Matlab\'s interp2 function.\n\n    Finds values for query points on a grid using bilinear interpolation.\n\n    Args:\n      grid: a 4-D float `Tensor` of shape `[batch, height, width, channels]`.\n      query_points: a 3-D float `Tensor` of N points with shape `[batch, N, 2]`.\n      name: a name for the operation (optional).\n      indexing: whether the query points are specified as row and column (ij),\n        or Cartesian coordinates (xy).\n\n    Returns:\n      values: a 3-D `Tensor` with shape `[batch, N, channels]`\n\n    Raises:\n      ValueError: if the indexing mode is invalid, or if the shape of the inputs\n        invalid.\n    """"""\n    if indexing != \'ij\' and indexing != \'xy\':\n        raise ValueError(\'Indexing mode must be \\\'ij\\\' or \\\'xy\\\'\')\n\n    with ops.name_scope(name):\n        grid = ops.convert_to_tensor(grid)\n        query_points = ops.convert_to_tensor(query_points)\n        shape = array_ops.unstack(array_ops.shape(grid))\n        if len(shape) != 4:\n            msg = \'Grid must be 4 dimensional. Received: \'\n            raise ValueError(msg + str(shape))\n\n        batch_size, height, width, channels = shape\n        query_type = query_points.dtype\n        query_shape = array_ops.unstack(array_ops.shape(query_points))\n        grid_type = grid.dtype\n\n        if len(query_shape) != 3:\n            msg = (\'Query points must be 3 dimensional. Received: \')\n            raise ValueError(msg + str(query_shape))\n\n        _, num_queries, _ = query_shape\n\n        alphas = []\n        floors = []\n        ceils = []\n\n        index_order = [0, 1] if indexing == \'ij\' else [1, 0]\n        unstacked_query_points = array_ops.unstack(query_points, axis=2)\n\n        for dim in index_order:\n            with ops.name_scope(\'dim-\' + str(dim)):\n                queries = unstacked_query_points[dim]\n\n                size_in_indexing_dimension = shape[dim + 1]\n\n                # max_floor is size_in_indexing_dimension - 2 so that max_floor + 1\n                # is still a valid index into the grid.\n                max_floor = math_ops.cast(size_in_indexing_dimension - 2, query_type)\n                min_floor = constant_op.constant(0.0, dtype=query_type)\n                floor = math_ops.minimum(\n                    math_ops.maximum(min_floor, math_ops.floor(queries)), max_floor)\n                int_floor = math_ops.cast(floor, dtypes.int32)\n                floors.append(int_floor)\n                ceil = int_floor + 1\n                ceils.append(ceil)\n\n                # alpha has the same type as the grid, as we will directly use alpha\n                # when taking linear combinations of pixel values from the image.\n                alpha = math_ops.cast(queries - floor, grid_type)\n                min_alpha = constant_op.constant(0.0, dtype=grid_type)\n                max_alpha = constant_op.constant(1.0, dtype=grid_type)\n                alpha = math_ops.minimum(math_ops.maximum(min_alpha, alpha), max_alpha)\n\n                # Expand alpha to [b, n, 1] so we can use broadcasting\n                # (since the alpha values don\'t depend on the channel).\n                alpha = array_ops.expand_dims(alpha, 2)\n                alphas.append(alpha)\n\n        flattened_grid = array_ops.reshape(grid,\n                                           [batch_size * height * width, channels])\n        batch_offsets = array_ops.reshape(\n            math_ops.range(batch_size) * height * width, [batch_size, 1])\n\n        # This wraps array_ops.gather. We reshape the image data such that the\n        # batch, y, and x coordinates are pulled into the first dimension.\n        # Then we gather. Finally, we reshape the output back. It\'s possible this\n        # code would be made simpler by using array_ops.gather_nd.\n        def gather(y_coords, x_coords, name):\n            with ops.name_scope(\'gather-\' + name):\n                linear_coordinates = batch_offsets + y_coords * width + x_coords\n                gathered_values = array_ops.gather(flattened_grid, linear_coordinates)\n                return array_ops.reshape(gathered_values,\n                                         [batch_size, num_queries, channels])\n\n        # grab the pixel values in the 4 corners around each query point\n        top_left = gather(floors[0], floors[1], \'top_left\')\n        top_right = gather(floors[0], ceils[1], \'top_right\')\n        bottom_left = gather(ceils[0], floors[1], \'bottom_left\')\n        bottom_right = gather(ceils[0], ceils[1], \'bottom_right\')\n\n        # now, do the actual interpolation\n        with ops.name_scope(\'interpolate\'):\n            interp_top = alphas[1] * (top_right - top_left) + top_left\n            interp_bottom = alphas[1] * (bottom_right - bottom_left) + bottom_left\n            interp = alphas[0] * (interp_bottom - interp_top) + interp_top\n\n        return interp\n\n\ndef dense_image_warp(image, flow, name=\'dense_image_warp\'):\n    """"""Image warping using per-pixel flow vectors.\n\n    Apply a non-linear warp to the image, where the warp is specified by a dense\n    flow field of offset vectors that define the correspondences of pixel values\n    in the output image back to locations in the  source image. Specifically, the\n    pixel value at output[b, j, i, c] is\n    images[b, j - flow[b, j, i, 0], i - flow[b, j, i, 1], c].\n\n    The locations specified by this formula do not necessarily map to an int\n    index. Therefore, the pixel value is obtained by bilinear\n    interpolation of the 4 nearest pixels around\n    (b, j - flow[b, j, i, 0], i - flow[b, j, i, 1]). For locations outside\n    of the image, we use the nearest pixel values at the image boundary.\n\n\n    Args:\n      image: 4-D float `Tensor` with shape `[batch, height, width, channels]`.\n      flow: A 4-D float `Tensor` with shape `[batch, height, width, 2]`.\n      name: A name for the operation (optional).\n\n      Note that image and flow can be of type tf.half, tf.float32, or tf.float64,\n      and do not necessarily have to be the same type.\n\n    Returns:\n      A 4-D float `Tensor` with shape`[batch, height, width, channels]`\n        and same type as input image.\n\n    Raises:\n      ValueError: if height < 2 or width < 2 or the inputs have the wrong number\n                  of dimensions.\n    """"""\n    with ops.name_scope(name):\n        batch_size, height, width, channels = array_ops.unstack(array_ops.shape(image))\n        # The flow is defined on the image grid. Turn the flow into a list of query\n        # points in the grid space.\n        grid_x, grid_y = array_ops.meshgrid(\n            math_ops.range(width), math_ops.range(height))\n        stacked_grid = math_ops.cast(\n            array_ops.stack([grid_y, grid_x], axis=2), flow.dtype)\n        batched_grid = array_ops.expand_dims(stacked_grid, axis=0)\n        query_points_on_grid = batched_grid - flow\n        query_points_flattened = array_ops.reshape(query_points_on_grid,\n                                                   [batch_size, height * width, 2])\n        # Compute values at the query points, then reshape the result back to the\n        # image grid.\n        interpolated = _interpolate_bilinear(image, query_points_flattened)\n        interpolated = array_ops.reshape(interpolated,\n                                         [batch_size, height, width, channels])\n        return interpolated\n'"
tfoptflow/dataset_base.py,19,"b'""""""\ndataset_base.py\n\nOptical flow dataset base class.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport sys\nimport tensorflow as tf\nimport numpy as np\nfrom tqdm import tqdm\nfrom skimage.io import imread\nimport cv2\n\nfrom augment import Augmenter\nfrom optflow import flow_read\n\nif sys.platform.startswith(""win""):\n    _DATASET_ROOT = \'E:/datasets/\'\nelse:\n    _DATASET_ROOT = \'/media/EDrive/datasets/\'\n\n_DBG_TRAIN_VAL_TEST_SETS = -1  # 128 # -1\n\n_DEFAULT_DS_TRAIN_OPTIONS = {\n    \'verbose\': False,\n    \'in_memory\': False,  # True loads all samples upfront, False loads them on-demand\n    \'crop_preproc\': (384, 448),  # None or (h, w), use (384, 768) for FlyingThings3D\n    \'scale_preproc\': None,  # None or (h, w),\n    # \'type\': \'final\',  # [\'clean\' | \'final\'] for MPISintel, [\'noc\' | \'occ\'] for KITTI, \'into_future\' for FlyingThings3D\n    \'tb_test_imgs\': False,  # If True, make test images available to model in training mode\n    # Sampling and split options\n    \'random_seed\': 1969,  # random seed used for sampling\n    \'val_split\': 0.03,  # portion of data reserved for the validation split\n    # Augmentation options\n    \'aug_type\': \'heavy\',  # in [None, \'basic\', \'heavy\'] to add augmented data to training set\n    \'aug_labels\': True,  # If True, augment both images and labels; otherwise, only augment images\n    \'fliplr\': 0.5,  # Horizontally flip 50% of images\n    \'flipud\': 0.5,  # Vertically flip 50% of images\n    # Translate 50% of images by a value between -5 and +5 percent of original size on x- and y-axis independently\n    \'translate\': (0.5, 0.05),\n    \'scale\': (0.5, 0.05),  # Scale 50% of images by a factor between 95 and 105 percent of original size\n}\n\n_DEFAULT_DS_TUNE_OPTIONS = {\n    \'verbose\': False,\n    \'in_memory\': False,  # True loads all samples upfront, False loads them on-demand\n    \'crop_preproc\': (384, 768),  # None or (h, w), use (384, 768) for FlyingThings3D\n    \'scale_preproc\': None,  # None or (h, w),\n    # [\'clean\' | \'final\'] for MPISintel, [\'noc\' | \'occ\'] for KITTI, \'into_future\' for FlyingThings3D\n    \'type\': \'into_future\',\n    \'tb_test_imgs\': False,  # If True, make test images available to model in training mode\n    # Sampling and split options\n    \'random_seed\': 1969,  # random seed used for sampling\n    \'val_split\': 0.03,  # portion of data reserved for the validation split\n    # Augmentation options\n    \'aug_type\': \'heavy\',  # in [None, \'basic\', \'heavy\'] to add augmented data to training set\n    \'aug_labels\': True,  # If True, augment both images and labels; otherwise, only augment images\n    \'fliplr\': 0.5,  # Horizontally flip 50% of images\n    \'flipud\': 0.5,  # Vertically flip 50% of images\n    # Translate 50% of images by a value between -5 and +5 percent of original size on x- and y-axis independently\n    \'translate\': (0.5, 0.05),\n    \'scale\': (0.5, 0.05),  # Scale 50% of images by a factor between 95 and 105 percent of original size\n}\n\n_DEFAULT_DS_VAL_OPTIONS = {\n    \'verbose\': False,\n    \'in_memory\': False,  # True loads all samples upfront, False loads them on-demand\n    \'crop_preproc\': None,  # None or (h, w),\n    \'scale_preproc\': None,  # None or (h, w),\n    \'type\': \'final\',  # [\'clean\' | \'final\'] for MPISintel, [\'noc\' | \'occ\'] for KITTI, \'into_future\' for FlyingThings3D\n    # Sampling and split options\n    \'random_seed\': 1969,  # random seed used for sampling\n    \'val_split\': 0.03,  # portion of data reserved for the validation split\n    # Augmentation options\n    \'aug_type\': None,  # in [None, \'basic\', \'heavy\'] to add augmented data to training set\n}\n\n_DEFAULT_DS_TEST_OPTIONS = {\n    \'verbose\': False,\n    \'in_memory\': False,  # True loads all samples upfront, False loads them on-demand\n    \'crop_preproc\': None,  # None or (h, w),\n    \'scale_preproc\': None,  # None or (h, w),\n    \'type\': \'final\',  # [\'clean\' | \'final\'] for MPISintel, [\'noc\' | \'occ\'] for KITTI, \'into_future\' for FlyingThings3D\n    # Sampling and split options\n    \'random_seed\': 1969,  # random seed used for sampling\n    \'val_split\': 0.03,  # portion of data reserved for the validation split\n    # Augmentation options\n    # \'aug_type\': None,  # in [None, \'basic\', \'heavy\'] to add augmented data to training set\n}\n\n\nclass OpticalFlowDataset(object):\n    """"""Optical flow dataset.\n    """"""\n\n    def __init__(self, mode=\'train_with_val\', ds_root=_DATASET_ROOT, options=_DEFAULT_DS_TRAIN_OPTIONS):\n        """"""Initialize the OFDataset object\n        Args:\n            mode: Possible values:\n                \'train_noval\', the entire dataset will be used for training (no data set aside for validation)\n                \'train_with_val\', the dataset will be split between a training and a holdout validation set\n                \'val\', the holdout validation set will be used for evaluation\n                \'val_notrain\', the entire dataset will be used to validate the performance of a model\n                \'test\', the dataset will be used to generate predictions on a test set that has no groundtruths\n            ds_root: Path to the root of the dataset\n            options: see _DEFAULT_DS_TRAIN_OPTIONS comments\n        """"""\n        # Only options supported in this initial implementation\n        assert (mode in [\'train_noval\', \'train_with_val\', \'val\', \'val_notrain\', \'test\'])\n        self.mode = mode\n        self.opts = options\n\n        # Setup train/val/test paths and file names\n        self._ds_root = ds_root\n\n        # Set the train, val, test, label and prediction label folders\n        self.set_folders()\n\n        # Set the names of the train/val/test files that will hold the list of sample/label IDs\n        self.set_IDs_filenames()\n\n        self._trn_IDs = self._val_IDs = self._tst_IDs = None\n        self._trn_IDs_simpl = self._val_IDs_simpl = self._tst_IDs_simpl = None\n        self._images_train = self._labels_train = self._images_val = self._labels_val = self._images_test = None\n        self._img_trn_path = self._lbl_trn_path = self._img_val_path = self._lbl_val_path = None\n        self._pred_lbl_val_path = self._pred_lbl_tst_path = self._img_tst_path = None\n\n        # Load ID files\n        if not self._load_ID_files():\n            self.prepare()\n\n        # Collect flow stats - the below data members MUST be set in any class that\n        # derives from this base class BEFORE calling this constructor!\n        if self.min_flow is None and self.avg_flow is None and self.max_flow is None:\n            self._get_flow_stats()\n\n        # Load all data in memory, if requested\n        if self.opts[\'in_memory\']:\n            self._preload_all_samples()\n\n        # Shuffle the data and set trackers\n        np.random.seed(self.opts[\'random_seed\'])\n        if self.mode in [\'train_noval\', \'train_with_val\']:\n            # Train over the original training set, in the first case\n            self._trn_ptr = 0\n            self.trn_size = len(self._trn_IDs)\n            self._trn_idx = np.arange(self.trn_size)\n            np.random.shuffle(self._trn_idx)\n            if self.mode == \'train_with_val\':\n                # Train over the training split, validate over the validation split, in the second case\n                self._val_ptr = 0\n                self.val_size = len(self._val_IDs)\n                self._val_idx = np.arange(self.val_size)\n                np.random.shuffle(self._val_idx)\n            if self.opts[\'tb_test_imgs\'] is True:\n                # Make test images available to model in training mode\n                self._tst_ptr = 0\n                self.tst_size = len(self._tst_IDs)\n                self._tst_idx = np.arange(self.tst_size)\n            # Instantiate augmenter, if requested\n            if self.opts[\'aug_type\'] is not None:\n                assert (self.opts[\'aug_type\'] in [\'basic\', \'heavy\'])\n                self._aug = Augmenter(self.opts)\n\n        elif self.mode in [\'val\', \'val_notrain\']:\n            # Validate over the validation split or turn the whole dataset into an evaluation dataset\n            self._val_ptr = 0\n            self.val_size = len(self._val_IDs)\n            self._val_idx = np.arange(self.val_size)\n            # np.random.shuffle(self._val_idx)\n\n        else:\n            # Test over the entire testing set\n            self._tst_ptr = 0\n            self.tst_size = len(self._tst_IDs)\n            self._tst_idx = np.arange(self.tst_size)\n\n    ###\n    # training/val/test ID files\n    ###\n    def set_folders(self):\n        """"""Set the train, val, test, label and prediction label folders.\n        Override this for each dataset, if necessary.\n        Called by the base class on init.\n        """"""\n        self._tst_dir = self._val_dir = self._trn_dir = self._ds_root\n        self._lbl_dir = f""{self._ds_root}/flow""\n        self._pred_lbl_dir = f""{self._ds_root}/flow_pred""\n\n    def set_IDs_filenames(self):\n        """"""Set the names of the train/val/test files that will hold the list of sample/label IDs\n        Override this for each dataset, if necessary.\n        Called by the base class on init.\n        """"""\n        self._trn_IDs_file = f""{self._ds_root}/train_{self.opts[\'val_split\']}split.txt""\n        self._val_IDs_file = f""{self._ds_root}/val_{self.opts[\'val_split\']}split.txt""\n        self._tst_IDs_file = f""{self._ds_root}/test.txt""\n\n    def prepare(self):\n        """"""Do all the preprocessing needed before training/val/test samples can be used.\n        """"""\n        if self.opts[\'verbose\']:\n            print(""Preparing dataset (one-time operation)..."")\n        # Create paths files and load them back in\n        self._build_ID_sets()\n        self._create_ID_files()\n        self._load_ID_files()\n        if self.opts[\'verbose\']:\n            print(""... done with preparing the dataset."")\n\n    def _build_ID_sets(self):\n        """"""Build the list of samples and their IDs, split them in the proper datasets.\n         Each ID is a tuple of the form (image1, image2, flow).\n         This method must be overriden.\n        """"""\n        raise NotImplementedError\n\n    def _get_flow_stats(self):\n        """"""Get the min, avg, max flow of the training data according to OpenCV.\n        This will allow us to normalize the rendering of flows to images across the entire dataset. Why?\n        Because low magnitude flows should appear lighter than high magnitude flows when rendered as images.\n        """"""\n        flow_mags = []\n        desc = ""Collecting training flow stats""\n        num_flows = len(self._lbl_trn_path)\n        with tqdm(total=num_flows, desc=desc, ascii=True, ncols=100) as pbar:\n            for flow_path in self._lbl_trn_path:\n                pbar.update(1)\n                flow = flow_read(flow_path)\n                flow_magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n                nans = np.isnan(flow_magnitude)\n                if np.any(nans):\n                    nans = np.where(nans)\n                    flow_magnitude[nans] = 0.\n                flow_mags.append(flow_magnitude)\n        self.min_flow, self.max_flow = np.min(flow_mags), np.max(flow_mags)\n        self.avg_flow = np.mean(flow_mags)\n        print(\n            f""train flow min={self.min_flow}, avg={self.avg_flow}, max={self.max_flow} ({num_flows} flows)"")\n\n    def _create_ID_files(self):\n        """"""Create the ID files for each split of the dataset\n        """"""\n        for file, IDs in [(self._trn_IDs_file, self._trn_IDs), (self._val_IDs_file,\n                                                                self._val_IDs), (self._tst_IDs_file, self._tst_IDs)]:\n            with open(file, \'w\') as f:\n                f.write(\'\\n\'.join(\'{}###{}###{}\'.format(ID[0], ID[1], ID[2]) for ID in IDs))\n\n    def _load_ID_files(self):\n        """"""Load the ID files and build the full file paths associated with those IDs\n        Returns:\n              True if ID files were loaded, False if ID files weren\'t found\n        """"""\n        if self.mode in [\'train_noval\', \'train_with_val\']:\n            if not os.path.exists(self._trn_IDs_file) or not os.path.exists(self._val_IDs_file):\n                return False\n\n            with open(self._trn_IDs_file, \'r\') as f:\n                self._trn_IDs = f.readlines()\n                self._trn_IDs = [tuple(ID.rstrip().split(""###"")) for ID in self._trn_IDs]\n\n            with open(self._val_IDs_file, \'r\') as f:\n                self._val_IDs = f.readlines()\n                self._val_IDs = [tuple(ID.rstrip().split(""###"")) for ID in self._val_IDs]\n\n            self._img_trn_path = [(self._trn_dir + \'/\' + ID[0], self._trn_dir + \'/\' + ID[1]) for ID in self._trn_IDs]\n            self._lbl_trn_path = [self._trn_lbl_dir + \'/\' + ID[2] for ID in self._trn_IDs]\n\n            if self.mode == \'train_noval\':\n                # Train over the original training set (no validation split)\n                self._trn_IDs += self._val_IDs\n                for ID in self._val_IDs:\n                    self._img_trn_path.append((self._val_dir + \'/\' + ID[0], self._val_dir + \'/\' + ID[1]))\n                    self._lbl_trn_path.append(self._val_lbl_dir + \'/\' + ID[2])\n            else:\n                # Train over the training split, validate over the validation split\n                self._img_val_path, self._lbl_val_path, self._pred_lbl_val_path = [], [], []\n                for ID in self._val_IDs:\n                    self._img_val_path.append((self._val_dir + \'/\' + ID[0], self._val_dir + \'/\' + ID[1]))\n                    self._lbl_val_path.append(self._val_lbl_dir + \'/\' + ID[2])\n                    lbl_id = ID[2].replace(\'.pfm\', \'.flo\').replace(\'.png\', \'.flo\')\n                    self._pred_lbl_val_path.append(self._val_pred_lbl_dir + \'/\' + lbl_id)\n\n            if self.opts[\'tb_test_imgs\'] is True:\n                # Make test images available to model in training mode\n                if not os.path.exists(self._tst_IDs_file):\n                    return False\n\n                with open(self._tst_IDs_file, \'r\') as f:\n                    self._tst_IDs = f.readlines()\n                    self._tst_IDs = [tuple(ID.rstrip().split(""###"")) for ID in self._tst_IDs]\n\n                self._img_tst_path, self._pred_lbl_tst_path = [], []\n                for ID in self._tst_IDs:\n                    self._img_tst_path.append((self._tst_dir + \'/\' + ID[0], self._tst_dir + \'/\' + ID[1]))\n                    self._pred_lbl_tst_path.append(self._tst_pred_lbl_dir + \'/\' + ID[2])\n\n        elif self.mode in [\'val\', \'val_notrain\']:\n            # Validate over the validation split\n            if not os.path.exists(self._val_IDs_file):\n                return False\n\n            with open(self._val_IDs_file, \'r\') as f:\n                self._val_IDs = f.readlines()\n                self._val_IDs = [tuple(ID.rstrip().split(""###"")) for ID in self._val_IDs]\n\n            if self.mode == \'val_notrain\':\n                with open(self._trn_IDs_file, \'r\') as f:\n                    self._trn_IDs = f.readlines()\n                    self._trn_IDs = [tuple(ID.rstrip().split(""###"")) for ID in self._trn_IDs]\n                self._val_IDs += self._trn_IDs\n\n            self._img_val_path, self._lbl_val_path, self._pred_lbl_val_path = [], [], []\n            for ID in self._val_IDs:\n                self._img_val_path.append((self._val_dir + \'/\' + ID[0], self._val_dir + \'/\' + ID[1]))\n                self._lbl_val_path.append(self._val_lbl_dir + \'/\' + ID[2])\n                lbl_id = ID[2].replace(\'.pfm\', \'.flo\').replace(\'.png\', \'.flo\')\n                self._pred_lbl_val_path.append(self._val_pred_lbl_dir + \'/\' + lbl_id)\n\n        else:\n            # Test over the entire testing set\n            if not os.path.exists(self._tst_IDs_file):\n                return False\n\n            with open(self._tst_IDs_file, \'r\') as f:\n                self._tst_IDs = f.readlines()\n                self._tst_IDs = [tuple(ID.rstrip().split(""###"")) for ID in self._tst_IDs]\n\n            self._img_tst_path, self._pred_lbl_tst_path = [], []\n            for ID in self._tst_IDs:\n                self._img_tst_path.append((self._tst_dir + \'/\' + ID[0], self._tst_dir + \'/\' + ID[1]))\n                self._pred_lbl_tst_path.append(self._tst_pred_lbl_dir + \'/\' + ID[2])\n\n        # Build a list of simplified IDs for Tensorboard logging\n        if self._trn_IDs is not None:\n            self._trn_IDs_simpl = self.simplify_IDs(self._trn_IDs)\n        if self._val_IDs is not None:\n            self._val_IDs_simpl = self.simplify_IDs(self._val_IDs)\n        if self._tst_IDs is not None:\n            self._tst_IDs_simpl = self.simplify_IDs(self._tst_IDs)\n\n        if _DBG_TRAIN_VAL_TEST_SETS != -1:  # Debug mode only\n            if self._trn_IDs is not None:\n                self._trn_IDs = self._trn_IDs[0:_DBG_TRAIN_VAL_TEST_SETS]\n            if self._img_trn_path is not None:\n                self._img_trn_path = self._img_trn_path[0:_DBG_TRAIN_VAL_TEST_SETS]\n            if self._lbl_trn_path is not None:\n                self._lbl_trn_path = self._lbl_trn_path[0:_DBG_TRAIN_VAL_TEST_SETS]\n            if self._val_IDs is not None:\n                self._val_IDs = self._val_IDs[0:_DBG_TRAIN_VAL_TEST_SETS]\n            if self._img_val_path is not None:\n                self._img_val_path = self._img_val_path[0:_DBG_TRAIN_VAL_TEST_SETS]\n            if self._lbl_val_path is not None:\n                self._lbl_val_path = self._lbl_val_path[0:_DBG_TRAIN_VAL_TEST_SETS]\n            if self._pred_lbl_val_path is not None:\n                self._pred_lbl_val_path = self._pred_lbl_val_path[0:_DBG_TRAIN_VAL_TEST_SETS]\n            if self._tst_IDs is not None:\n                self._tst_IDs = self._tst_IDs[0:_DBG_TRAIN_VAL_TEST_SETS]\n            if self._img_tst_path is not None:\n                self._img_tst_path = self._img_tst_path[0:_DBG_TRAIN_VAL_TEST_SETS]\n            if self._pred_lbl_tst_path is not None:\n                self._pred_lbl_tst_path = self._pred_lbl_tst_path[0:_DBG_TRAIN_VAL_TEST_SETS]\n\n        return True\n\n    ###\n    # Batch Management\n    ###\n    def _preload_all_samples(self):\n        """"""Preload all samples (input image pairs + associated flows) in memory.\n        """"""\n        if self.mode in [\'train_noval\', \'train_with_val\']:\n\n            self._images_train, self._labels_train = [], []\n            desc = ""Loading train image pairs & flows""\n            with tqdm(total=len(self._img_trn_path), desc=desc, ascii=True, ncols=100) as pbar:\n                for n, image_path in enumerate(self._img_trn_path):\n                    pbar.update(1)\n                    label_path = self._lbl_trn_path[n]\n                    image, label = self._load_sample(image_path, label_path)\n                    self._labels_train.append(label)\n                    self._images_train.append(image)\n\n            if self.mode == \'train_with_val\':\n                self._images_val, self._labels_val = [], []\n                desc = ""Loading val image pairs & flows""\n                with tqdm(total=len(self._img_val_path), desc=desc, ascii=True, ncols=100) as pbar:\n                    for n, image_path in enumerate(self._img_val_path):\n                        pbar.update(1)\n                        label_path = self._lbl_val_path[n]\n                        image, label = self._load_sample(image_path, label_path, preprocess=False)\n                        self._labels_val.append(label)\n                        self._images_val.append(image)\n\n            if self.opts[\'tb_test_imgs\'] is True:\n                self._images_test = []\n                desc = ""Loading test samples""\n                with tqdm(total=len(self._img_tst_path), desc=desc, ascii=True, ncols=100) as pbar:\n                    for image_path in self._img_tst_path:\n                        pbar.update(1)\n                        self._images_test.append(self._load_sample(image_path, preprocess=False))\n\n        elif self.mode in [\'val\', \'val_notrain\']:\n\n            self._images_val, self._labels_val = [], []\n            desc = ""Loading val image pairs & flows""\n            with tqdm(total=len(self._img_val_path), desc=desc, ascii=True, ncols=100) as pbar:\n                for n, image_path in enumerate(self._img_val_path):\n                    pbar.update(1)\n                    label_path = self._lbl_val_path[n]\n                    image, label = self._load_sample(image_path, label_path, preprocess=False)\n                    self._labels_val.append(label)\n                    self._images_val.append(image)\n\n        elif self.mode == \'test\':\n            self._images_test = []\n            desc = ""Loading test samples""\n            with tqdm(total=len(self._img_tst_path), desc=desc, ascii=True, ncols=100) as pbar:\n                for image_path in self._img_tst_path:\n                    pbar.update(1)\n                    self._images_test.append(self._load_sample(image_path, preprocess=False))\n\n    def next_batch(self, batch_size, split=\'train\'):\n        """"""Get next batch of samples and labels (input image pairs + associated flows)\n        In \'*_with_pred_paths\' mode, also return a destination folder where to save predicted flows.\n        Args:\n            batch_size: Size of the batch\n            split: \'train\', \'val\', \'val_with_preds\', \'val_with_pred_paths\', \'test\', or \'test_with_pred_paths\'\n        In training and validation mode, returns:\n            images: Batch of image pairs in format [N, 2, H, W, 3]\n            labels: Batch of optical flows in format [N, H, W, 2], or file paths to predicted label\n        In testing mode, returns:\n            images: Batch of RGB images in format [N, 2, H, W, 3]\n            pred_folder: List of output folders where to save the predicted instance masks\n        """"""\n        assert(split in [\'train\', \'val\', \'val_with_preds\', \'val_with_pred_paths\', \'test\', \'test_with_pred_paths\'])\n\n        # Come up with list of indices to load\n        if split == \'train\':\n            assert(self.mode in [\'train_noval\', \'train_with_val\'])\n            if self._trn_ptr + batch_size < self.trn_size:\n                idx = np.array(self._trn_idx[self._trn_ptr:self._trn_ptr + batch_size])\n                new_ptr = self._trn_ptr + batch_size\n            else:\n                old_idx = np.array(self._trn_idx[self._trn_ptr:])\n                np.random.shuffle(self._trn_idx)\n                new_ptr = (self._trn_ptr + batch_size) % self.trn_size\n                idx = np.concatenate((old_idx, np.array(self._trn_idx[:new_ptr])))\n\n        elif split in [\'val\', \'val_with_preds\', \'val_with_pred_paths\']:\n            assert(self.mode in [\'val\', \'val_notrain\', \'train_with_val\'])\n            if self._val_ptr + batch_size < self.val_size:\n                idx = np.array(self._val_idx[self._val_ptr:self._val_ptr + batch_size])\n                new_ptr = self._val_ptr + batch_size\n            else:\n                old_idx = np.array(self._val_idx[self._val_ptr:])\n                # np.random.shuffle(self._val_idx)\n                new_ptr = (self._val_ptr + batch_size) % self.val_size\n                idx = np.concatenate((old_idx, np.array(self._val_idx[:new_ptr])))\n\n        elif split in [\'test\', \'test_with_pred_paths\']:\n            assert (self.mode == \'test\')\n            if self._tst_ptr + batch_size < self.tst_size:\n                new_ptr = self._tst_ptr + batch_size\n                idx = list(range(self._tst_ptr, self._tst_ptr + batch_size))\n            else:\n                new_ptr = (self._tst_ptr + batch_size) % self.tst_size\n                idx = list(range(self._tst_ptr, self.tst_size)) + list(range(0, new_ptr))\n\n        # Move pointers forward\n        if split == \'train\':\n            self._trn_ptr = new_ptr\n        elif split in [\'val\', \'val_with_preds\', \'val_with_pred_paths\']:\n            self._val_ptr = new_ptr\n        elif split in [\'test\', \'test_with_pred_paths\']:\n            self._tst_ptr = new_ptr\n\n        # Return samples and labels\n        return self.get_samples(idx=idx, split=split, as_list=False, simple_IDs=True)\n\n    ###\n    # Sample loaders and getters\n    ###\n\n    def _load_sample(self, image_path=None, label_path=None, preprocess=True, as_tuple=False, is_training=False):\n        """"""Load a properly formatted sample (image pair + optical flow)\n        Args:\n            image_path: Pair of image paths, if any\n            label_path: Path to optical flow, if any\n            preprocess: If True, apply preprocessing steps (cropping + scaling); otherwise, don\'t\n            as_tuple: If true, return image pair as a tuple; otherwise, return a np array in [2, H, W, 3] format\n            is_training: If true, sample is a training sample subject to data augmentation\n        Returns:\n            image: Image pair in format [2, H, W, 3] or ([H, W, 3],[H, W, 3]), if any\n            label: Optical flow in format [W, H, 2], if any\n        """"""\n        # Read in RGB image, if any\n        if image_path:\n            image1, image2 = imread(image_path[0]), imread(image_path[1])\n            assert(len(image1.shape) == 3 and image1.shape[2] == 3 and len(image2.shape) == 3 and image2.shape[2] == 3)\n\n        # Read in label, if any\n        if label_path:\n            label = flow_read(label_path)\n            assert (len(label.shape) == 3 and label.shape[2] == 2)\n        else:\n            label = None\n\n        # Return image and/or label\n        if label_path:\n            if image_path:\n                if as_tuple:\n                    return (image1, image2), label\n                else:\n                    return np.array([image1, image2]), label\n            else:\n                return label\n        else:\n            if image_path:\n                if as_tuple:\n                    return (image1, image2)\n                else:\n                    return np.array([image1, image2])\n\n    def _augment_sample(self, image=None, label=None, as_tuple=False):\n        """"""Augment sample (input image pair + associated flow, if any)\n        Args:\n            image: Image pair\n            label: optical flow, if any\n            as_tuple: If True, return image pair tuple; otherwise, return np array in [2, H, W, 3] format\n        Returns:\n            image: Augmented image pair in format ([H, W, 3],[H, W, 3]) or [2, H, W, 3]?\n            label: Augmented label in format [H, W, 2], if any label\n        """"""\n        # Use augmentation, if requested\n        if label is None:\n            assert(self._aug.opts[\'aug_labels\'] is False)\n            # TODO Test this code path with basic horizontal flipping\n            aug_image = self._aug.augment([image], None, as_tuple)\n            image = aug_image[0]\n        else:\n            aug_image, aug_label = self._aug.augment([image], [label], as_tuple)\n            image, label = aug_image[0], aug_label[0]\n\n        # Return image and label\n        if as_tuple:\n            return (image[0], image[1]), label\n        else:\n            return np.array([image[0], image[1]]), label\n\n    def _get_train_samples(self, idx, as_tuple=False, simple_IDs=False):\n        """"""Get training images with associated labels\n        Args:\n            idx: List of sample indices to return\n            as_tuple: If True, return image pairs as tuples; otherwise, return them as np arrays in [2, H, W, 3] format\n            simple_IDs: If True, return concatenated IDs; otherwise, return individual image and label IDs\n        Returns:\n            images: List of RGB images in format list(([H, W, 3],[H, W, 3])) or list([2, H, W, 3])\n            labels: List of labels in format list([H, W, 2])\n            IDs: List of IDs in format list(str) or list([str,str,str])\n        """"""\n        images, labels, IDs = [], [], []\n        for l in idx:\n            if self.opts[\'in_memory\']:\n                image = self._images_train[l]\n                label = self._labels_train[l]\n            else:\n                image, label = self._load_sample(self._img_trn_path[l], self._lbl_trn_path[l],\n                                                 is_training=True)\n\n            # Crop images and/or labels to a fixed size, if requested\n            if self.opts[\'crop_preproc\'] is not None:\n                h, w = image[0].shape[:2]\n                h_max, w_max = self.opts[\'crop_preproc\']\n                assert (h >= h_max and w >= w_max)\n                max_y_offset, max_x_offset = h - h_max, w - w_max\n                if max_y_offset > 0 or max_x_offset > 0:\n                    y_offset = np.random.randint(max_y_offset + 1)\n                    x_offset = np.random.randint(max_x_offset + 1)\n                    # The following assumes the image pair is in [2,H,W,3] format\n                    image = image[:, y_offset:y_offset + h_max, x_offset:x_offset + w_max, :]\n                    label = label[y_offset:y_offset + h_max, x_offset:x_offset + w_max, :]\n\n            # Scale images and/or labels to a fixed size, if requested\n            if self.opts[\'scale_preproc\'] is not None:\n                scale_shape = (int(self.opts[\'scale_preproc\'][0]), int(self.opts[\'scale_preproc\'][1]))\n                image[0] = cv2.resize(image[0], scale_shape)\n                image[1] = cv2.resize(image[1], scale_shape)\n                label = cv2.resize(label, scale_shape) * scale_shape[0] / image[0].shape[0]\n\n            # Augment the samples, if requested\n            if self.opts[\'aug_type\'] is not None:\n                image, label = self._augment_sample((image[0], image[1]), label, as_tuple)\n\n            # Don\'t move augmentation to _load_sample() otherwise, if the samples are in-memory they will have been\n            # augmented once and that\'s it, the first time they were loaded. The augmentation code needs to be\n            # here so that, no matter the memory mode, every sample is augmented differently with every batch\n\n            images.append(image)\n            labels.append(label)\n            if simple_IDs is True:\n                IDs.append(self._trn_IDs_simpl[l])\n            else:\n                IDs.append(self._trn_IDs[l])\n\n        return images, labels, IDs\n\n    def _get_val_samples(self, idx, as_tuple=False, simple_IDs=False):\n        """"""Get validation images with associated labels\n        Args:\n            idx: List of sample indices to return\n            as_tuple: If True, return image pairs as tuples; otherwise, return them as np arrays in [2, H, W, 3] format\n            simple_IDs: If True, return concatenated IDs; otherwise, return individual image and label IDs\n        Returns:\n            images: List of RGB images in format list(([H, W, 3],[H, W, 3])) or list([2, H, W, 3])\n            labels: List of labels in format list([H, W, 2])\n            IDs: List of IDs in format list(str) or list([str,str,str])\n        """"""\n        images, labels, IDs = [], [], []\n        for l in idx:\n            if self.opts[\'in_memory\']:\n                image = self._images_val[l]\n                label = self._labels_val[l]\n            else:\n                image, label = self._load_sample(\n                    self._img_val_path[l], self._lbl_val_path[l], preprocess=False, as_tuple=as_tuple)\n\n            # We also must crop validation images and labels to a fixed size during online evaluation.\n            # Why do this with validation data? Because we are currently stuck with having the same\n            # batch size between the training and validation data sets.  Since our images all have\n            # different sizes, if we want to batch validation samples, they must all have the same size...\n            # Crop images and/or labels to a fixed size, if requested\n            if self.opts[\'crop_preproc\'] is not None:\n                h, w = image[0].shape[:2]\n                h_max, w_max = self.opts[\'crop_preproc\']\n                assert (h >= h_max and w >= w_max)\n                max_y_offset, max_x_offset = h - h_max, w - w_max\n                if max_y_offset > 0 or max_x_offset > 0:\n                    y_offset = np.random.randint(max_y_offset + 1)\n                    x_offset = np.random.randint(max_x_offset + 1)\n                    # The following assumes the image pair is in [2,H,W,3] format\n                    image = image[:, y_offset:y_offset + h_max, x_offset:x_offset + w_max, :]\n                    label = label[y_offset:y_offset + h_max, x_offset:x_offset + w_max, :]\n\n            # Scale images and/or labels to a fixed size, if requested\n            if self.opts[\'scale_preproc\'] is not None:\n                scale_shape = (int(self.opts[\'scale_preproc\'][0]), int(self.opts[\'scale_preproc\'][1]))\n                image[0] = cv2.resize(image[0], scale_shape)\n                image[1] = cv2.resize(image[1], scale_shape)\n                label = cv2.resize(label, scale_shape) * scale_shape[0] / image[0].shape[0]\n\n            images.append(image)\n            labels.append(label)\n            if simple_IDs is True:\n                IDs.append(self._val_IDs_simpl[l])\n            else:\n                IDs.append(self._val_IDs[l])\n\n        return images, labels, IDs\n\n    def _get_val_samples_with_preds(self, idx, as_tuple=False, simple_IDs=False):\n        """"""Get validation images with associated labels and predictions\n        Args:\n            idx: List of sample indices to return\n            as_tuple: If True, return image pairs as tuples; otherwise, return them as np arrays in [2, H, W, 3] format\n            simple_IDs: If True, return concatenated IDs; otherwise, return individual image and label IDs\n        Returns:\n            images: List of RGB images in format list(([H, W, 3],[H, W, 3])) or list([2, H, W, 3])\n            labels: List of labels in format list([H, W, 2])\n            pred_labels: List of predicted labels in format list([H, W, 2])\n            IDs: List of IDs in format list(str) or list([str,str,str])\n        """"""\n        images, labels, IDs = self._get_val_samples(idx, as_tuple=as_tuple, simple_IDs=simple_IDs)\n\n        pred_labels = []\n        for l in idx:\n            if os.path.exists(self._pred_lbl_val_path[l]):\n                pred_label = self._load_sample(None, self._pred_lbl_val_path[l], preprocess=False, as_tuple=as_tuple)\n                pred_labels.append(pred_label)\n\n        return images, labels, pred_labels, IDs\n\n    def _get_val_samples_with_pred_paths(self, idx, as_tuple=False, simple_IDs=False):\n        """"""Get validation images with associated labels and prediction paths\n        Args:\n            as_tuple: If True, return image pairs as tuples; otherwise, return them as np arrays in [2, H, W, 3] format\n            idx: List of sample indices to return\n            simple_IDs: If True, return concatenated IDs; otherwise, return individual image and label IDs\n        Returns:\n            images: List of RGB images in format list(([H, W, 3],[H, W, 3])) or list([2, H, W, 3])\n            labels: List of labels in format list([H, W, 2])\n            pred_label_paths: List of paths to the predicted labels\n            IDs: List of IDs in format list(str) or list([str,str,str])\n        """"""\n        images, labels, IDs = self._get_val_samples(idx, as_tuple=as_tuple, simple_IDs=simple_IDs)\n\n        pred_label_paths = []\n        for l in idx:\n            pred_label_paths.append(self._pred_lbl_val_path[l])\n\n        return images, labels, pred_label_paths, IDs\n\n    def _get_test_samples_with_preds(self, idx, as_tuple=False, simple_IDs=False):\n        """"""Get test images with predicted labels\n        Args:\n            idx: List of sample indices to return\n            as_tuple: If True, return image pairs as tuples; otherwise, return them as np arrays in [2, H, W, 3] format\n            simple_IDs: If True, return concatenated IDs; otherwise, return individual image and label IDs\n        Returns:\n            images: List of RGB images in format list(([H, W, 3],[H, W, 3])) or list([2, H, W, 3])\n            pred_labels: List of predicted labels in format list([H, W, 2])\n            IDs: List of IDs in format list(str) or list([str,str,str])\n        """"""\n        images, pred_labels, IDs = [], [], []\n        for l in idx:\n            if self.opts[\'in_memory\']:\n                image = self._images_test[l]\n            else:\n                image = self._load_sample(self._img_tst_path[l], preprocess=False, as_tuple=as_tuple)\n            images.append(image)\n            if os.path.exists(self._pred_lbl_tst_path[l]):\n                pred_label = self._load_sample(\n                    None,\n                    self._pred_lbl_tst_path[l],\n                    preprocess=False,\n                    as_tuple=as_tuple)\n                pred_labels.append(pred_label)\n            if simple_IDs is True:\n                IDs.append(self._tst_IDs_simpl[l])\n            else:\n                IDs.append(self._tst_IDs[l])\n\n        return images, pred_labels, IDs\n\n    def _get_test_samples_with_pred_paths(self, idx, as_tuple=False, simple_IDs=False):\n        """"""Get test images with predicted labels\n        Args:\n            idx: List of sample indices to return\n            as_tuple: If True, return image pairs as tuples; otherwise, return them as np arrays in [2, H, W, 3] format\n            simple_IDs: If True, return concatenated IDs; otherwise, return individual image and label IDs\n        Returns:\n            images: List of RGB images in format list(([H, W, 3],[H, W, 3])) or list([2, H, W, 3])\n            pred_label_paths: List of paths to the predicted labels\n            IDs: List of IDs in format list(str) or list([str,str,str])\n        """"""\n        images, pred_label_paths, IDs = [], [], []\n        for l in idx:\n            if self.opts[\'in_memory\']:\n                image = self._images_test[l]\n            else:\n                image = self._load_sample(self._img_tst_path[l], preprocess=False, as_tuple=as_tuple)\n            images.append(image)\n            pred_label_paths.append(self._pred_lbl_tst_path[l])\n            if simple_IDs is True:\n                IDs.append(self._tst_IDs_simpl[l])\n            else:\n                IDs.append(self._tst_IDs[l])\n\n        return images, pred_label_paths, IDs\n\n    def get_samples(\n            self,\n            num_samples=0,\n            idx=None,\n            split=\'val\',\n            as_list=True,\n            deterministic=False,\n            as_tuple=False,\n            simple_IDs=False):\n        """"""Get a few (or all) random (or ordered) samples from the dataset.\n        Used for debugging purposes (testing how the model is improving over time, for instance).\n        If sampling from the training/validation set, there is a label; otherwise, there isn\'t.\n        Note that this doesn\'t return a valid np array if the images don\'t have the same size.\n        Args:\n            num_samples: Number of samples to return (used only if idx is set to None)\n            idx: Specific list of indices to pull from the dataset split (no need to set num_samples in this case)\n            split: \'train\',\'val\',\'val_with_preds\',\'val_with_pred_paths\',\'test\',\'test_with_preds\',\'test_with_pred_paths\'\n            as_list: Return as list or np array?\n            return_IDs: If True, also return ID of the sample\n            deterministic: If True, return first num_samples samples, otherwise, sample randomly\n            as_tuple: If True, return image pairs as tuples; otherwise, return them as np arrays in [2, H, W, 3] format\n            simple_IDs: If True, return concatenated IDs; otherwise, return individual image and label IDs\n        In training and validation mode, returns:\n            images: Batch of image pairs in format [num_samples, 2, H, W, 3] or list([2, H, W, 3])\n            labels: Batch of optical flows in format [num_samples, H, W, 2] or list([H, W, 2])\n            IDs: List of ID strings in list or np.ndarray format\n        In testing mode, returns:\n            images: Batch of image pairs in format [num_samples, 2, H, W, 3]\n            output_files: List of output file names that match the input file names\n            IDs: List of ID strings in list or np.ndarray format\n        """"""\n        assert(idx is not None or num_samples > 0)\n\n        if split == \'train\':\n            assert(self.mode in [\'train_noval\', \'train_with_val\'])\n            if idx is None:\n                if deterministic:\n                    idx = self._trn_idx[0:num_samples]\n                else:\n                    idx = np.random.choice(self._trn_idx, size=num_samples, replace=False)\n\n            images, labels, IDs = self._get_train_samples(idx, as_tuple=as_tuple, simple_IDs=simple_IDs)\n\n            if as_list:\n                return images, labels, IDs\n            else:\n                return map(np.asarray, (images, labels, IDs))\n\n        elif split == \'val\':\n            assert(self.mode in [\'val\', \'val_notrain\', \'train_with_val\'])\n            if idx is None:\n                if deterministic:\n                    idx = self._val_idx[0:num_samples]\n                else:\n                    idx = np.random.choice(self._val_idx, size=num_samples, replace=False)\n\n            images, gt_labels, IDs = self._get_val_samples(idx, as_tuple=as_tuple, simple_IDs=simple_IDs)\n\n            if as_list:\n                return images, gt_labels, IDs\n            else:\n                return map(np.asarray, (images, gt_labels, IDs))\n\n        elif split == \'val_with_preds\':\n            assert(self.mode in [\'val\', \'val_notrain\', \'train_with_val\'])\n            if idx is None:\n                if deterministic:\n                    idx = self._val_idx[0:num_samples]\n                else:\n                    idx = np.random.choice(self._val_idx, size=num_samples, replace=False)\n\n            images, gt_labels, pred_labels, IDs = self._get_val_samples_with_preds(\n                idx, as_tuple=as_tuple, simple_IDs=simple_IDs)\n\n            if as_list:\n                return images, gt_labels, pred_labels, IDs\n            else:\n                return map(np.asarray, (images, gt_labels, pred_labels, IDs))\n\n        elif split == \'val_with_pred_paths\':\n            assert(self.mode in [\'val\', \'val_notrain\', \'train_with_val\'])\n            if idx is None:\n                if deterministic:\n                    idx = self._val_idx[0:num_samples]\n                else:\n                    idx = np.random.choice(self._val_idx, size=num_samples, replace=False)\n\n            images, gt_labels, pred_label_paths, IDs = self._get_val_samples_with_pred_paths(\n                idx, as_tuple=as_tuple, simple_IDs=simple_IDs)\n\n            if as_list:\n                return images, gt_labels, pred_label_paths, IDs\n            else:\n                return map(np.asarray, (images, gt_labels, pred_label_paths, IDs))\n\n        elif split == \'test\':\n            if idx is None:\n                if deterministic:\n                    idx = self._tst_idx[0:num_samples]\n                else:\n                    idx = np.random.choice(self._tst_idx, size=num_samples, replace=False)\n\n            images, IDs = [], []\n            for l in idx:\n                if self.opts[\'in_memory\']:\n                    image = self._images_test[l]\n                else:\n                    image = self._load_sample(self._img_tst_path[l], preprocess=False, as_tuple=as_tuple)\n                images.append(image)\n                if simple_IDs is True:\n                    IDs.append(self._tst_IDs_simpl[l])\n                else:\n                    IDs.append(self._tst_IDs[l])\n\n            if as_list:\n                return images, IDs\n            else:\n                return map(np.asarray, (images, IDs))\n\n        elif split == \'test_with_preds\':\n            if idx is None:\n                if deterministic:\n                    idx = self._tst_idx[0:num_samples]\n                else:\n                    idx = np.random.choice(self._tst_idx, size=num_samples, replace=False)\n\n            images, pred_labels, IDs = self._get_test_samples_with_preds(idx, as_tuple=as_tuple, simple_IDs=simple_IDs)\n\n            if as_list:\n                return images, pred_labels, IDs\n            else:\n                return map(np.asarray, (images, pred_labels, IDs))\n\n        elif split == \'test_with_pred_paths\':\n            if idx is None:\n                if deterministic:\n                    idx = self._tst_idx[0:num_samples]\n                else:\n                    idx = np.random.choice(self._tst_idx, size=num_samples, replace=False)\n\n            images, pred_label_paths, IDs = self._get_test_samples_with_pred_paths(\n                idx, as_tuple=as_tuple, simple_IDs=simple_IDs)\n\n            if as_list:\n                return images, pred_label_paths, IDs\n            else:\n                return map(np.asarray, (images, pred_label_paths, IDs))\n\n        else:\n            return None, None\n\n    def get_samples_by_flow_ID(self, flow_IDs, split=\'val\', as_list=True, as_tuple=False):\n        """"""Get specific samples from the dataset, looking them up by flow ID.\n        Used for error analysis purposes (seeing which predicted flows have the lowest/highest EPE, for instance).\n        If sampling from the training/validation set, there is a label; otherwise, there isn\'t.\n        Doesn\'t return a valid np array if all the image pairs don\'t have the same size (use as_list=True instead).\n        Args:\n            idx: Specific list of flow IDs to pull from the dataset split\n            split: \'train\',\'val\',\'val_with_preds\',\'val_with_pred_paths\',\'test\',\'test_with_preds\',\'test_with_pred_paths\'\n            as_list: Return as list or np array?\n            return_IDs: If True, also return ID of the sample\n            as_tuple: If True, return image pairs as tuples; otherwise, return them as np arrays in [2, H, W, 3] format\n        In training and validation mode, returns:\n            images: Batch of image pairs in format [num_samples, 2, H, W, 3] or list([2, H, W, 3])\n            labels: Batch of optical flows in format [num_samples, H, W, 2]\n        In testing mode, returns:\n            images: Batch of image pairs in format [num_samples, 2, H, W, 3]\n            output_files: List of output file names that match the input file names\n        """"""\n        # Which split of IDs should we look into?\n        if split == \'train\':\n            IDs_to_search = self._trn_IDs_simpl\n        elif split in [\'val\', \'val_with_preds\', \'val_with_pred_paths\']:\n            IDs_to_search = self._val_IDs_simpl\n        elif split in [\'test\', \'test_with_preds\', \'test_with_pred_paths\']:\n            IDs_to_search = self._tst_IDs_simpl\n        else:\n            raise ValueError\n\n        # Build the list of indices to look up\n        indices = []\n        for flow_ID in flow_IDs:\n            for idx, test_ID in enumerate(IDs_to_search):\n                if flow_ID in test_ID:\n                    indices.append(idx)\n                    break\n\n        # Return the samples based on their indices\n        if len(indices) > 0:\n            return self.get_samples(idx=indices, split=split, as_list=as_list, as_tuple=as_tuple)\n        else:\n            raise ValueError\n\n    ###\n    # Various utility functions\n    ###\n    def simplify_IDs(self, IDs):\n        """"""Simplify list of ID ID string tuples.\n        This is dataset specific and needs override.\n        Go from (\'video_path/frame_0019.png\', \'video_path/frame_0020.png\', \'video_path/frame_0019.flo/\')\n        to \'video_path/frames_0019_0020\n        Args:\n            IDs: List of ID string tuples to simplify\n        Returns:\n            IDs: Simplified IDs\n        """"""\n        raise NotImplementedError\n\n    ###\n    # Debug utils\n    ###\n    def print_config(self):\n        """"""Display configuration values.""""""\n        print(""\\nDataset Configuration:"")\n        for k, v in self.opts.items():\n            print(f""  {k:20} {v}"")\n        print(f""  {\'mode\':20} {self.mode}"")\n        if self.mode in [\'train_noval\', \'train_with_val\']:\n            print(f""  {\'train size\':20} {self.trn_size}"")\n        if self.mode in [\'train_with_val\', \'val\']:\n            print(f""  {\'val size\':20} {self.val_size}"")\n        if self.mode == \'test\':\n            print(f""  {\'test size\':20} {self.tst_size}"")\n\n    ###\n    # tf.data helpers\n    ###\n    def _train_stub(self, idx):\n        """"""tf.py_func stub for _get_train_samples()\n        Args:\n            idx: Index of unique sample to return\n        Returns:\n            x: Image pair in [2, H, W, 3] format\n            y: Groundtruth flow in [H, W, 2] format\n            ID: string, sample ID\n        """"""\n        x, y, ID = self.get_samples(idx=[idx], split=\'train\', as_list=False, simple_IDs=True)\n        return np.squeeze(x), np.squeeze(y), ID[0]\n\n    def _val_stub(self, idx):\n        """"""tf.py_func stub for _get_val_samples()\n        Args:\n            idx: Index of unique sample to return\n        Returns:\n            x: Image pair in [2, H, W, 3] format\n            y: Groundtruth flow in [H, W, 2] format\n            path: string, destination path where to save the predicted flow\n            ID: string, sample ID\n        """"""\n        x, y, path, ID = self.get_samples(idx=[idx], split=\'val_with_pred_paths\', as_list=False, simple_IDs=True)\n        return np.squeeze(x), np.squeeze(y), path[0], ID[0]\n\n    def _test_stub(self, idx):\n        """"""tf.py_func stub for _get_test_samples()\n        Args:\n            idx: Index of unique sample to return\n        Returns:\n            x: Image pair in [2, H, W, 3] format\n            path: string, destination path where to save the predicted flow\n            ID: string, sample ID\n        """"""\n        x, path, ID = self.get_samples(idx=[idx], split=\'test_with_pred_paths\', as_list=False, simple_IDs=True)\n        return np.squeeze(x), path[0], ID[0]\n\n    def get_tf_ds(self, batch_size=1, num_gpus=1, split=\'train\', sess=None):\n        """"""Get a tf.data.Dataset ""view"" or the dataset\n        Args:\n            batch_size: Size of the batch\n            split: \'train\', \'val\', \'val_with_preds\', \'val_with_pred_paths\', \'test\', or \'test_with_pred_paths\'\n        In training and validation mode, returns:\n            images: Batch of image pairs in format [N, 2, H, W, 3]\n            labels: Batch of optical flows in format [N, H, W, 2], or file paths to predicted label\n        In testing mode, returns:\n            images: Batch of RGB images in format [N, 2, H, W, 3]\n            pred_folder: List of output folders where to save the predicted instance masks\n        Refs:\n            - Modules: tf.data and tf.contrib.data\n            https://www.tensorflow.org/api_docs/python/tf/data\n            https://www.tensorflow.org/api_docs/python/tf/contrib/data\n            - Also:\n            ttps://cs230-stanford.github.io/tensorflow-input-data.html\n            http://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/\n            https://github.com/kratzert/finetune_alexnet_with_tensorflow/blob/master/datagenerator.py\n            https://jhui.github.io/2017/11/21/TensorFlow-Importing-data/\n            https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/5_DataManagement/tensorflow_dataset_api.py\n            https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428\n        """"""\n        assert(split in [\'train\', \'val\', \'test\'])\n        threads = min(os.cpu_count(), 12)  # os.cpu_count() returns 20 on SERVERP\n\n        # Use the train/val/test indices as the elements of the tf.data.Dataset\n        if split == \'train\':\n            assert(self.mode in [\'train_noval\', \'train_with_val\'])\n            tf_ds = tf.data.Dataset.from_tensor_slices(self._trn_idx)\n            tf_ds = tf_ds.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=len(self._trn_idx), count=-1))\n            tf_ds = tf_ds.apply(tf.contrib.data.map_and_batch(\n                map_func=lambda idx: tf.py_func(self._train_stub, [idx], [tf.uint8, tf.float32, tf.string]),\n                batch_size=batch_size * num_gpus, num_parallel_batches=threads))\n\n        elif split == \'val\':\n            assert(self.mode in [\'val\', \'val_notrain\', \'train_noval\', \'train_with_val\'])\n            tf_ds = tf.data.Dataset.from_tensor_slices(self._val_idx)\n            tf_ds = tf_ds.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=len(self._val_idx), count=-1))\n            tf_ds = tf_ds.apply(tf.contrib.data.map_and_batch(\n                map_func=lambda idx: tf.py_func(self._val_stub, [idx], [tf.uint8, tf.float32, tf.string, tf.string]),\n                batch_size=batch_size * num_gpus, num_parallel_batches=threads))\n\n        else:  # if split == \'test\':\n            tf_ds = tf.data.Dataset.from_tensor_slices(self._tst_idx)\n            tf_ds = tf_ds.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=len(self._tst_idx), count=-1))\n            tf_ds = tf_ds.apply(tf.contrib.data.map_and_batch(\n                map_func=lambda idx: tf.py_func(self._test_stub, [idx], [tf.uint8, tf.string, tf.string]),\n                batch_size=batch_size * num_gpus, num_parallel_batches=threads))\n\n        # Return tf dataset\n        return tf_ds\n\n    ###\n    # To look at later (TFRecords support):\n    ###\n    # https://github.com/sampepose/flownet2-tf/blob/master/src/dataloader.py for both TFRecords support and aug\n    # https://github.com/fperazzi/davis-2017/blob/master/python/lib/davis/dataset/base.py\n    # https://github.com/fperazzi/davis-2017/blob/master/python/lib/davis/dataset/loader.py\n    # https://github.com/kwotsin/create_tfrecords\n    # https://kwotsin.github.io/tech/2017/01/29/tfrecords.html\n    # http://yeephycho.github.io/2016/08/15/image-data-in-tensorflow/\n    # E:\\repos\\models-master\\research\\inception\\inception\\data\\build_imagenet_data.py\n    # E:\\repos\\models-master\\research\\object_detection\\dataset_tools\\create_kitti_tf_record.py\n    # https://github.com/ferreirafabio/video2tfrecords/blob/master/video2tfrecords.py\n    # http://www.machinelearninguru.com/deep_learning/tensorflow/basics/tfrecord/tfrecord.html\n    # https://github.com/linchuming/ImageSR-Tensorflow/blob/master/data_loader.py\n    ###\n    def _load_from_tfrecords(self):\n        pass\n\n    def _write_to_tfrecords(self):\n        pass\n'"
tfoptflow/dataset_flyingchairs.py,0,"b'""""""\ndataset_flyingchairs.py\n\nFlyingChairs (384x512) optical flow dataset class.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport os\nimport random\nfrom sklearn.model_selection import train_test_split\n\nfrom dataset_base import OpticalFlowDataset, _DATASET_ROOT, _DEFAULT_DS_TRAIN_OPTIONS\n\n_FLYINGCHAIRS_ROOT = _DATASET_ROOT + \'FlyingChairs_release\'\n\n\nclass FlyingChairsDataset(OpticalFlowDataset):\n    """"""FlyingChairs optical flow dataset.\n    """"""\n\n    def __init__(self, mode=\'train_with_val\', ds_root=_FLYINGCHAIRS_ROOT, options=_DEFAULT_DS_TRAIN_OPTIONS):\n        """"""Initialize the FlyingChairsDataset object\n        Args:\n            mode: Possible options: \'train_noval\', \'val\', \'train_with_val\' or \'test\'\n            ds_root: Path to the root of the dataset\n            options: see base class documentation\n        """"""\n        self.min_flow = 0.\n        self.avg_flow = 11.113031387329102\n        self.max_flow = 300.007568359375\n        super().__init__(mode, ds_root, options)\n\n    def set_folders(self):\n        """"""Set the train, val, test, label and prediction label folders.\n        Overriden by each dataset. Called by the base class on init.\n        Sample results:\n            self._trn_dir          = \'E:/datasets/FlyingChairs_release/data\'\n            self._trn_lbl_dir      = \'E:/datasets/FlyingChairs_release/data\'\n            self._val_dir          = \'E:/datasets/FlyingChairs_release/data\'\n            self._val_lbl_dir      = \'E:/datasets/FlyingChairs_release/data\'\n            self._val_pred_lbl_dir = \'E:/datasets/FlyingChairs_release/flow_pred\'\n            self._tst_dir          = \'E:/datasets/FlyingChairs_release/data\'\n            self._tst_pred_lbl_dir = \'E:/datasets/FlyingChairs_release/flow_pred\'\n        """"""\n        self._trn_dir = f""{self._ds_root}/data""\n        self._val_dir = self._trn_dir\n        self._tst_dir = self._trn_dir\n\n        self._trn_lbl_dir = self._trn_dir\n        self._val_lbl_dir = self._trn_lbl_dir\n        self._val_pred_lbl_dir = f""{self._ds_root}/flow_pred""\n        self._tst_pred_lbl_dir = self._val_pred_lbl_dir\n\n    def set_IDs_filenames(self):\n        """"""Set the names of the train/val/test files that will hold the list of sample/label IDs\n        Called by the base class on init.\n        Typical ID filenames:\n            \'E:/datasets/MPI-Sintel/final_train.txt\'\n            \'E:/datasets/MPI-Sintel/final_val.txt\'\n            \'E:/datasets/MPI-Sintel/final_test.txt\'\n        """"""\n        if os.path.exists(self._ds_root + \'/FlyingChairs_train_val.txt\'):\n            self._trn_IDs_file = f""{self._ds_root}/train.txt""\n            self._val_IDs_file = f""{self._ds_root}/val.txt""\n            self._tst_IDs_file = f""{self._ds_root}/test.txt""\n        else:\n            super().set_IDs_filenames()\n\n    def _build_ID_sets(self):\n        """"""Build the list of samples and their IDs, split them in the proper datasets.\n         Each ID is a tuple.\n         For the training/val/test datasets, they look like (\'12518_img1.ppm\', \'12518_img2.ppm\', \'12518_flow.flo\')\n         The original dataset has 22872 image pairs. Using FlyingChairs_train_val.txt, the samples are split between\n         22232 training samples (97.2%) and 640 validation samples (2.8%).\n        """"""\n        # Search the train folder for the samples, create string IDs for them\n        frames = sorted(os.listdir(self._trn_dir))\n        self._IDs, idx = [], 0\n        while idx < len(frames) - 1:\n            self._IDs.append((frames[idx + 1], frames[idx + 2], frames[idx]))\n            idx += 3\n\n        # Build the train/val datasets\n        if os.path.exists(self._ds_root + \'/FlyingChairs_train_val.txt\'):\n            with open(self._ds_root + \'/FlyingChairs_train_val.txt\', \'r\') as f:\n                train_val_IDs = f.readlines()\n            train_val_IDs = list(map(int, train_val_IDs))\n            train_indices = [idx for idx, value in enumerate(train_val_IDs) if value == 1]\n            self._trn_IDs = [self._IDs[idx] for idx in train_indices]\n            val_indices = [idx for idx, value in enumerate(train_val_IDs) if value == 2]\n            self._val_IDs = [self._IDs[idx] for idx in val_indices]\n            random.seed(self.opts[\'random_seed\'])\n            random.shuffle(self._trn_IDs)\n            random.shuffle(self._val_IDs)\n        elif self.opts[\'val_split\'] > 0.:\n            self._trn_IDs, self._val_IDs = train_test_split(self._IDs, test_size=self.opts[\'val_split\'],\n                                                            random_state=self.opts[\'random_seed\'])\n        else:\n            self._trn_IDs, self._val_IDs = self._IDs, None\n\n        # Build the test dataset.\n        # Note that we\'re only using the FlyingChairs dataset to pre-train our network. Since, we don\'t really need a\n        # final unbiased estimate after hyper-param tuning, so we set the test set to the val set.\n        self._tst_IDs = self._val_IDs.copy()\n\n        # Build a list of simplified IDs for Tensorboard logging\n        self._trn_IDs_simpl = self.simplify_IDs(self._trn_IDs)\n        self._val_IDs_simpl = self.simplify_IDs(self._val_IDs)\n        self._tst_IDs_simpl = self.simplify_IDs(self._tst_IDs)\n\n    def simplify_IDs(self, IDs):\n        """"""Simplify list of ID string tuples.\n        Go from (\'video_path/frame_0019.png\', \'video_path/frame_0020.png\', \'video_path/frame_0019.flo/\')\n        to \'video_path/frames_0019_0020\n        Args:\n            IDs: List of ID string tuples to simplify\n        Returns:\n            IDs: Simplified IDs\n        """"""\n        simple_IDs = [f""pair_{ID[0][0:5]}"" for ID in IDs]\n        return simple_IDs\n'"
tfoptflow/dataset_flyingthings3d.py,0,"b'""""""\ndataset_flyingthings3d.py\n\nFlyingThnigs3D (540x960) and FlyingThinbs3DHalfRes (270x480) optical flow dataset class.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport os\nimport warnings\nfrom tqdm import tqdm\nfrom skimage.io import imread, imsave\nimport cv2\nfrom sklearn.model_selection import train_test_split\n\nfrom dataset_base import OpticalFlowDataset, _DATASET_ROOT, _DEFAULT_DS_TRAIN_OPTIONS\nfrom optflow import flow_read, flow_write\n\n_FLYINGTHINGS3D_ROOT = \'//naspro/devt/datasets/FlyingThings3D\'\n_FLYINGTHINGS3DHALFRES_ROOT = _DATASET_ROOT + \'FlyingThings3D_HalfRes\'\n_FLYINGTHINGS3DHALFRES_ROOTS = (_FLYINGTHINGS3D_ROOT, _FLYINGTHINGS3DHALFRES_ROOT)\n\n\nclass FlyingThings3DDataset(OpticalFlowDataset):\n    """"""FlyingThings3D optical flow dataset.\n    """"""\n\n    def __init__(self, mode=\'train_with_val\', ds_root=_FLYINGTHINGS3D_ROOT, options=_DEFAULT_DS_TRAIN_OPTIONS):\n        """"""Initialize the FlyingThings3DDataset object\n        Args:\n            mode: Possible options: \'train_noval\', \'val\', \'train_with_val\' or \'test\'\n            ds_root: Path to the root of the dataset\n            options: see base class documentation\n        """"""\n        self.min_flow = 0.\n        self.avg_flow = 37.99934768676758\n        self.max_flow = 1714.8948974609375\n        super().__init__(mode, ds_root, options)\n        if \'type\' not in self.opts:\n            self.opts[\'type\'] = \'into_future\'\n\n    def set_folders(self):\n        """"""Set the train, val, test, label and prediction label folders.\n        Overriden by each dataset. Called by the base class on init.\n        Sample results:\n            self._trn_dir          = \'E:/datasets/FlyingThings3D/frames_cleanpass/TRAIN\'\n            self._trn_lbl_dir      = \'E:/datasets/FlyingThings3D/optical_flow/TRAIN\'\n            self._val_dir          = \'E:/datasets/FlyingThings3D/frames_cleanpass/TRAIN\'\n            self._val_lbl_dir      = \'E:/datasets/FlyingThings3D/optical_flow/TRAIN\'\n            self._val_pred_lbl_dir = \'E:/datasets/FlyingThings3D/flow_pred\'\n            self._tst_dir          = \'E:/datasets/FlyingThings3D/frames_cleanpass/TEST\'\n            self._tst_pred_lbl_dir = \'E:/datasets/FlyingThings3D/flow_pred\'\n        """"""\n        self._trn_dir = f""{self._ds_root}/frames_cleanpass/TRAIN""\n        self._val_dir = self._trn_dir\n        self._tst_dir = f""{self._ds_root}/frames_cleanpass/TEST""\n\n        self._trn_lbl_dir = f""{self._ds_root}/optical_flow/TRAIN""\n        self._val_lbl_dir = self._trn_lbl_dir\n        self._val_pred_lbl_dir = f""{self._ds_root}/flow_pred""\n        self._tst_pred_lbl_dir = self._val_pred_lbl_dir\n\n    def _build_ID_sets(self):\n        """"""Build the list of samples and their IDs, split them in the proper datasets.\n         Each ID is a tuple.\n         For the training/val/test datasets, they look like (\'12518_img1.ppm\', \'12518_img2.ppm\', \'12518_flow.flo\')\n         The original dataset has 22872 image pairs. Using FlyingChairs_train_val.txt, the samples are split between\n         22232 training samples (97.2%) and 640 validation samples (2.8%).\n        """"""\n        # Load the exclusion file, if it is present\n        exclusion_file = self._ds_root + \'/frames_cleanpass/all_unused_files.txt\'\n        exclusion_list = None\n        if os.path.exists(exclusion_file):\n            with open(exclusion_file, \'r\') as f:\n                exclusion_list = f.readlines()\n                exclusion_list = [line.rstrip() for line in exclusion_list]\n\n        # Search the train folder for the samples, create string IDs for them\n        self._IDs = []\n        for subset in sorted(os.listdir(self._trn_dir)):  # subset: \'A\'\n            scenes = sorted(os.listdir(f""{self._trn_dir}/{subset}""))\n            for scene in scenes:  # scene: \'0000\'\n                frames = sorted(os.listdir(f""{self._trn_dir}/{subset}/{scene}/left""))\n                for idx in range(len(frames) - 1):\n                    frame1_ID = f\'{subset}/{scene}/left/{frames[idx]}\'\n                    frame2_ID = f\'{subset}/{scene}/left/{frames[idx+1]}\'\n                    frame = frames[idx].split(\'.\')[0]\n                    flow_ID = f""{subset}/{scene}/{self.opts[\'type\']}/left/OpticalFlowIntoFuture_{frame}_L.pfm""\n                    if exclusion_list is None or f""TRAIN/{frame1_ID}"" not in exclusion_list:\n                        self._IDs.append((frame1_ID, frame2_ID, flow_ID))\n\n        # Build the train/val datasets\n        if self.opts[\'val_split\'] > 0.:\n            self._trn_IDs, self._val_IDs = train_test_split(self._IDs, test_size=self.opts[\'val_split\'],\n                                                            random_state=self.opts[\'random_seed\'])\n        else:\n            self._trn_IDs, self._val_IDs = self._IDs, None\n\n        # Build the test dataset\n        self._tst_IDs = []\n        for subset in sorted(os.listdir(self._tst_dir)):  # subset: \'A\'\n            scenes = sorted(os.listdir(f""{self._tst_dir}/{subset}""))\n            for scene in scenes:  # scene: \'0000\'\n                frames = sorted(os.listdir(f""{self._tst_dir}/{subset}/{scene}/left""))\n                for idx in range(len(frames) - 1):\n                    frame1_ID = f\'{subset}/{scene}/left/{frames[idx]}\'\n                    frame2_ID = f\'{subset}/{scene}/left/{frames[idx+1]}\'\n                    frame = frames[idx].split(\'.\')[0]\n                    flow_ID = f""{subset}/{scene}/{self.opts[\'type\']}/left/OpticalFlowIntoFuture_{frame}_L.flo""\n                    if exclusion_list is None or f""TEST/{frame1_ID}"" not in exclusion_list:\n                        self._tst_IDs.append((frame1_ID, frame2_ID, flow_ID))\n\n        # Build a list of simplified IDs for Tensorboard logging\n        self._trn_IDs_simpl = self.simplify_IDs(self._trn_IDs)\n        self._val_IDs_simpl = self.simplify_IDs(self._val_IDs)\n        self._tst_IDs_simpl = self.simplify_IDs(self._tst_IDs)\n\n    def simplify_IDs(self, IDs):\n        """"""Simplify list of ID ID string tuples.\n        Go from (\'A/0005/left/0006.png\', \'A/0005/left/0007.png\', \'A/0005/left/0006.flo\')\n        to \'A_0005_left_0006_0007\'\n        Args:\n            IDs: List of ID string tuples to simplify\n        Returns:\n            IDs: Simplified IDs\n        """"""\n        simple_IDs = [ID[0].replace(\'/\', \'_\').split(\'.\')[0] + \'_\' + ID[1][-8:-4] for ID in IDs]\n        return simple_IDs\n\n\nclass FlyingThings3DHalfResDataset(FlyingThings3DDataset):\n    """"""FlyingThings3D half-res optical flow dataset.\n    """"""\n\n    def __init__(self, mode=\'train_with_val\', ds_root=_FLYINGTHINGS3DHALFRES_ROOTS, options=_DEFAULT_DS_TRAIN_OPTIONS):\n        """"""Initialize the FlyingThings3DDataset object\n        Args:\n            mode: Possible options: \'train_noval\', \'val\', \'train_with_val\' or \'test\'\n            ds_root: Path to the root of the dataset\n            options: see base class documentation\n        """"""\n        # Generate the half-res version of the dataset, if it doesn\'t already exit\n        if isinstance(ds_root, list) or isinstance(ds_root, tuple):\n            self.fullres_root, self.halfres_root = ds_root\n            if os.path.exists(self.halfres_root):\n                self.generate_files = False\n                ds_root = self.halfres_root\n            else:\n                self.generate_files = True\n                ds_root = self.halfres_root\n                # ds_root = self.fullres_root\n        super().__init__(mode, ds_root, options)\n        self.min_flow = 0.\n        self.avg_flow = 18.954660415649414\n        self.max_flow = 856.9017944335938\n        if \'type\' not in self.opts:\n            self.opts[\'type\'] = \'into_future\'\n\n    def set_folders(self):\n        """"""Set the train, val, test, label and prediction label folders.\n        Overriden by each dataset. Called by the base class on init.\n        Sample results:\n            self._trn_dir          = \'E:/datasets/FlyingThings3D_HalfRes/frames_cleanpass/TRAIN\'\n            self._trn_lbl_dir      = \'E:/datasets/FlyingThings3D_HalfRes/optical_flow/TRAIN\'\n            self._val_dir          = \'E:/datasets/FlyingThings3D_HalfRes/frames_cleanpass/TRAIN\'\n            self._val_lbl_dir      = \'E:/datasets/FlyingThings3D_HalfRes/optical_flow/TRAIN\'\n            self._val_pred_lbl_dir = \'E:/datasets/FlyingThings3D_HalfRes/flow_pred\'\n            self._tst_dir          = \'E:/datasets/FlyingThings3D_HalfRes/frames_cleanpass/TEST\'\n            self._tst_pred_lbl_dir = \'E:/datasets/FlyingThings3D_HalfRes/flow_pred\'\n        """"""\n        self._trn_dir = f""{self._ds_root}/frames_cleanpass/TRAIN""\n        self._val_dir = self._trn_dir\n        self._tst_dir = f""{self._ds_root}/frames_cleanpass/TEST""\n\n        self._trn_lbl_dir = f""{self._ds_root}/optical_flow/TRAIN""\n        self._val_lbl_dir = self._trn_lbl_dir\n        self._val_pred_lbl_dir = f""{self._ds_root}/flow_pred""\n        self._tst_pred_lbl_dir = self._val_pred_lbl_dir\n\n    def _build_ID_sets(self):\n        """"""Build the list of samples and their IDs, split them in the proper datasets.\n         Each ID is a tuple that looks like (\'12518_img1.ppm\', \'12518_img2.ppm\', \'12518_flow.flo\')\n        """"""\n        # Load the exclusion file, if it is present\n        if self.generate_files is True:\n            ds_root = self.fullres_root\n            trn_dir = self._trn_dir.replace(self.halfres_root, self.fullres_root)\n            trn_lbl_dir = self._trn_lbl_dir.replace(self.halfres_root, self.fullres_root)\n            tst_dir = self._tst_dir.replace(self.halfres_root, self.fullres_root)\n        else:\n            ds_root = self.halfres_root\n            trn_dir = self._trn_dir\n            trn_lbl_dir = self._trn_lbl_dir\n            tst_dir = self._tst_dir\n\n        # Load the exclusion file, if it is present\n        exclusion_file = ds_root + \'/frames_cleanpass/all_unused_files.txt\'\n        exclusion_list = None\n        if os.path.exists(exclusion_file):\n            with open(exclusion_file, \'r\') as f:\n                exclusion_list = f.readlines()\n                exclusion_list = [line.rstrip() for line in exclusion_list]\n\n        # Search the train folder for the samples, create string IDs for them\n        self._IDs = []\n        for subset in sorted(os.listdir(trn_dir)):  # subset: \'A\'\n            scenes = sorted(os.listdir(f""{trn_dir}/{subset}""))\n            for scene in scenes:  # scene: \'0000\'\n                frames = sorted(os.listdir(f""{trn_dir}/{subset}/{scene}/left""))\n                for idx in range(len(frames) - 1):\n                    frame1_ID = f\'{subset}/{scene}/left/{frames[idx]}\'\n                    frame2_ID = f\'{subset}/{scene}/left/{frames[idx+1]}\'\n                    frame = frames[idx].split(\'.\')[0]\n                    flow_ID = f""{subset}/{scene}/{self.opts[\'type\']}/left/OpticalFlowIntoFuture_{frame}_L.pfm""\n                    if exclusion_list is None or f""TRAIN/{frame1_ID}"" not in exclusion_list:\n                        self._IDs.append((frame1_ID, frame2_ID, flow_ID))\n\n        # Create half-res version of the train/val dataset, if it doesn\'t exist yet\n        if self.generate_files is True:\n            if not os.path.exists(self.halfres_root):\n                os.makedirs(self.halfres_root)\n            with tqdm(total=len(self._IDs), desc=""Downsampling train images & flows"", ascii=True, ncols=100) as pbar:\n                for n, ID in enumerate(self._IDs):\n                    pbar.update(1)\n                    fullres_path = f\'{trn_lbl_dir}/{ID[2]}\'\n                    halfres_path = fullres_path.replace(self.fullres_root, self.halfres_root).replace(\'.pfm\', \'.flo\')\n                    if not os.path.exists(halfres_path):\n                        flow = flow_read(fullres_path)\n                        flow = cv2.resize(flow, (int(flow.shape[1] * 0.5), int(flow.shape[0] * 0.5))) * 0.5\n                        flow_write(flow, halfres_path)\n                    for frame_ID in [ID[0], ID[1]]:\n                        frame_path_fullres = f\'{trn_dir}/{frame_ID}\'\n                        frame_path_halfres = frame_path_fullres.replace(self.fullres_root, self.halfres_root)\n                        if not os.path.exists(frame_path_halfres):\n                            frame = imread(frame_path_fullres)\n                            frame = cv2.resize(frame, (int(frame.shape[1] * 0.5), int(frame.shape[0] * 0.5)))\n                            with warnings.catch_warnings():\n                                warnings.simplefilter(""ignore"")\n                                folder = os.path.dirname(frame_path_halfres)\n                                if not os.path.exists(folder):\n                                    os.makedirs(folder)\n                                imsave(frame_path_halfres, frame)\n                    self._IDs[n] = (ID[0], ID[1], ID[2].replace(\'.pfm\', \'.flo\'))\n\n        # Build the train/val datasets\n        if self.opts[\'val_split\'] > 0.:\n            self._trn_IDs, self._val_IDs = train_test_split(self._IDs, test_size=self.opts[\'val_split\'],\n                                                            random_state=self.opts[\'random_seed\'])\n        else:\n            self._trn_IDs, self._val_IDs = self._IDs, None\n\n        # Build the test dataset\n        self._tst_IDs = []\n        for subset in sorted(os.listdir(tst_dir)):  # subset: \'A\'\n            scenes = sorted(os.listdir(f""{tst_dir}/{subset}""))\n            for scene in scenes:  # scene: \'0000\'\n                frames = sorted(os.listdir(f""{tst_dir}/{subset}/{scene}/left""))\n                for idx in range(len(frames) - 1):\n                    frame1_ID = f\'{subset}/{scene}/left/{frames[idx]}\'\n                    frame2_ID = f\'{subset}/{scene}/left/{frames[idx+1]}\'\n                    frame = frames[idx].split(\'.\')[0]\n                    flow_ID = f""{subset}/{scene}/{self.opts[\'type\']}/left/OpticalFlowIntoFuture_{frame}_L.flo""\n                    if exclusion_list is None or f""TEST/{frame1_ID}"" not in exclusion_list:\n                        self._tst_IDs.append((frame1_ID, frame2_ID, flow_ID))\n\n        # Create half-res version of the test dataset, if it doesn\'t exist yet\n        if self.generate_files is True:\n            with tqdm(total=len(self._tst_IDs), desc=""Downsampling test images"", ascii=True, ncols=100) as pbar:\n                for ID in self._tst_IDs:\n                    pbar.update(1)\n                    for frame_ID in [ID[0], ID[1]]:\n                        frame_path_fullres = f\'{tst_dir}/{frame_ID}\'\n                        frame_path_halfres = frame_path_fullres.replace(self.fullres_root, self.halfres_root)\n                        if not os.path.exists(frame_path_halfres):\n                            frame = imread(frame_path_fullres)\n                            frame = cv2.resize(frame, (int(frame.shape[1] * 0.5), int(frame.shape[0] * 0.5)))\n                            with warnings.catch_warnings():\n                                warnings.simplefilter(""ignore"")\n                                folder = os.path.dirname(frame_path_halfres)\n                                if not os.path.exists(folder):\n                                    os.makedirs(folder)\n                                imsave(frame_path_halfres, frame)\n\n        # Build a list of simplified IDs for Tensorboard logging\n        self._trn_IDs_simpl = self.simplify_IDs(self._trn_IDs)\n        self._val_IDs_simpl = self.simplify_IDs(self._val_IDs)\n        self._tst_IDs_simpl = self.simplify_IDs(self._tst_IDs)\n'"
tfoptflow/dataset_kitti.py,0,"b'""""""\ndataset_kitti.py\n\nKITTI optical flow dataset class.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport os\nimport numpy as np\nfrom tqdm import tqdm\nimport cv2\nfrom sklearn.model_selection import train_test_split\n\nfrom dataset_base import OpticalFlowDataset, _DATASET_ROOT, _DEFAULT_DS_TRAIN_OPTIONS\nfrom optflow import flow_read\n\n_KITTI2012_ROOT = _DATASET_ROOT + \'KITTI12\'\n_KITTI2015_ROOT = _DATASET_ROOT + \'KITTI15\'\n_KITTI_ROOT = _KITTI2015_ROOT\n\n\nclass KITTIDataset(OpticalFlowDataset):\n    """"""KITTI optical flow dataset.\n    List of adapted KITTI12 sizes: [(1, 375, 1242, 2), (1, 370, 1226, 2), (1, 376, 1241, 2), (1, 374, 1238, 2)]\n    List of adapted KITTI15 sizes: [(1, 375, 1242, 2), (1, 370, 1224, 2), (1, 376, 1241, 2), (1, 374, 1238, 2)]\n    """"""\n\n    def __init__(self, mode=\'train_with_val\', ds_root=_KITTI_ROOT, options=_DEFAULT_DS_TRAIN_OPTIONS):\n        """"""Initialize the KITTIDataset object\n        Args:\n            mode: Possible options: \'train_noval\', \'val\', \'train_with_val\' or \'test\'\n            ds_root: Path to the root of the dataset\n            options: see base class documentation\n        Flow stats:\n            KITTI2012: training flow mag min=0.0, avg=6.736172669242419, max=232.20108032226562 (194 flows)\n            KITTI2015: raining flow mag min=0.0, avg=4.7107220490319, max=256.4881896972656 (200 flows)\n        """"""\n        self.min_flow = 0.\n        self.avg_flow = 4.7107220490319\n        self.max_flow = 256.4881896972656\n        super().__init__(mode, ds_root, options)\n        assert(self.opts[\'type\'] in [\'noc\', \'occ\'])\n\n    def set_folders(self):\n        """"""Set the train, val, test, label and prediction label folders.\n        Overriden by each dataset. Called by the base class on init.\n        Sample results:\n            self._trn_dir          = \'E:/datasets/KITTI12/training/colored_0\'\n            self._trn_lbl_dir      = \'E:/datasets/KITTI12/training/flow_occ\'\n            self._val_dir          = \'E:/datasets/KITTI12/training/colored_0\'\n            self._val_lbl_dir      = \'E:/datasets/KITTI12/training/flow_occ\'\n            self._val_pred_lbl_dir = \'E:/datasets/KITTI12/training/flow_occ_pred\'\n            self._tst_dir          = \'E:/datasets/KITTI12/testing/colored_0\'\n            self._tst_pred_lbl_dir = \'E:/datasets/KITTI12/testing/flow_occ_pred\'\n        """"""\n        if os.path.exists(self._ds_root + \'/training/colored_0\'):\n            self._trn_dir = self._ds_root + \'/training/colored_0\'\n            self._tst_dir = self._ds_root + \'/testing/colored_0\'\n        elif os.path.exists(self._ds_root + \'/training/image_2\'):\n            self._trn_dir = self._ds_root + \'/training/image_2\'\n            self._tst_dir = self._ds_root + \'/testing/image_2\'\n        else:\n            raise IOError\n        self._val_dir = self._trn_dir\n\n        self._trn_lbl_dir = self._ds_root + \'/training/flow_\' + self.opts[\'type\']\n        self._val_lbl_dir = self._trn_lbl_dir\n        self._val_pred_lbl_dir = self._ds_root + \'/training/flow_\' + self.opts[\'type\'] + \'_pred\'\n        self._tst_pred_lbl_dir = self._ds_root + \'/testing/flow_\' + self.opts[\'type\'] + \'_pred\'\n\n    def _build_ID_sets(self):\n        """"""Build the list of samples and their IDs, split them in the proper datasets.\n        Called by the base class on init.\n        Each ID is a tuple.\n        For the training/val datasets, they look like (\'000065_10.png\', \'000065_11.png\', \'000065_10.png\')\n         -> gt flows are stored as 48-bit PNGs\n        For the test dataset, they look like (\'000000_10.png\', \'00000_11.png\', \'000000_10.flo\')\n        """"""\n        # Search the train folder for the samples, create string IDs for them\n        frames = sorted(os.listdir(self._trn_dir))\n        self._IDs, idx = [], 0\n        while idx < len(frames) - 1:\n            self._IDs.append((frames[idx], frames[idx + 1], frames[idx]))\n            idx += 2\n\n        # Build the train/val datasets\n        if self.opts[\'val_split\'] > 0.:\n            self._trn_IDs, self._val_IDs = train_test_split(self._IDs, test_size=self.opts[\'val_split\'],\n                                                            random_state=self.opts[\'random_seed\'])\n        else:\n            self._trn_IDs, self._val_IDs = self._IDs, None\n\n        # Build the test dataset\n        self._tst_IDs, idx = [], 0\n        frames = sorted(os.listdir(self._tst_dir))\n        while idx < len(frames) - 1:\n            flow_ID = frames[idx].replace(\'.png\', \'.flo\')\n            self._tst_IDs.append((frames[idx], frames[idx + 1], flow_ID))\n            idx += 2\n\n        self._trn_IDs_simpl = self.simplify_IDs(self._trn_IDs)\n        self._val_IDs_simpl = self.simplify_IDs(self._val_IDs)\n        self._tst_IDs_simpl = self.simplify_IDs(self._tst_IDs)\n\n    def simplify_IDs(self, IDs):\n        """"""Simplify list of ID string tuples.\n        Go from (\'000065_10.png\', \'000065_11.png\', \'000065_10.png\') to \'frames_000065_10_11\n        Args:\n            IDs: List of ID string tuples to simplify\n        Returns:\n            IDs: Simplified IDs\n        """"""\n        simple_IDs = []\n        for ID in IDs:\n            simple_IDs.append(f""frames_{ID[0][:-4]}_{ID[1][-6:-4]}"")\n        return simple_IDs\n\n    def _get_flow_stats(self):\n        """"""Get the min, avg, max flow of the training data according to OpenCV.\n        This will allow us to normalize the rendering of flows to images across the entire dataset. Why?\n        Because low magnitude flows should appear lighter than high magnitude flows when rendered as images.\n        We need to override the base class implementation that assumes images in the dataset all have the same size.\n        """"""\n        flow_mags = []\n        desc = ""Collecting training flow stats""\n        num_flows = len(self._lbl_trn_path)\n        with tqdm(total=num_flows, desc=desc, ascii=True, ncols=100) as pbar:\n            for flow_path in self._lbl_trn_path:\n                pbar.update(1)\n                flow = flow_read(flow_path)\n                flow_magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n                nans = np.isnan(flow_magnitude)\n                if np.any(nans):\n                    nans = np.where(nans)\n                    flow_magnitude[nans] = 0.\n                flow_mags.append(flow_magnitude)\n\n        # Get the max width and height and combine the flow magnitudes into one masked array\n        h_max = np.max([flow_mag.shape[0] for flow_mag in flow_mags])\n        w_max = np.max([flow_mag.shape[1] for flow_mag in flow_mags])\n        masked = np.ma.empty((h_max, w_max, len(flow_mags)), dtype=np.float32)\n        masked.mask = True\n        for idx, flow_mag in enumerate(flow_mags):\n            masked[:flow_mag.shape[0], :flow_mag.shape[1], idx] = flow_mag\n        self.min_flow, self.avg_flow, self.max_flow = np.min(masked), np.mean(masked), np.max(masked)\n        print(\n            f""training flow min={self.min_flow}, avg={self.avg_flow}, max={self.max_flow} ({num_flows} flows)"")\n'"
tfoptflow/dataset_mixer.py,0,"b'""""""\ndataset_mixer.py\n\nMixer optical flow dataset class.\nWill generate mixed samples from a list of dataset objects (e.g., FlyingChairs and FlyingThings3DHalfRes).\nDataset options useb by the individual datasets and the mixed dataset must be compatible.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport numpy as np\n\nfrom augment import Augmenter\nfrom dataset_base import OpticalFlowDataset, _DEFAULT_DS_TRAIN_OPTIONS\n\n\nclass MixedDataset(OpticalFlowDataset):\n    """"""Mixed optical flow dataset.\n    """"""\n\n    def __init__(self, mode=\'train_with_val\', datasets=None, options=_DEFAULT_DS_TRAIN_OPTIONS):\n        """"""Initialize the MixedDataset object\n        Args:\n            mode: Possible options: \'train_noval\', \'val\', \'train_with_val\' or \'test\'\n            datasets: List of dataset objects to mix samples from\n            options: see _DEFAULT_DS_TRAIN_OPTIONS comments\n        """"""\n        # Only options supported in this initial implementation\n        assert (mode in [\'train_noval\', \'val\', \'train_with_val\', \'test\'])\n        self.mode = mode\n        self.opts = options\n\n        # Combine dataset fields\n        self._trn_IDs, self._val_IDs, self._tst_IDs = [], [], []\n        self._trn_IDs_simpl, self._val_IDs_simpl, self._tst_IDs_simpl = [], [], []\n        self._img_trn_path, self._img_val_path, self._img_tst_path = [], [], []\n        self._lbl_trn_path, self._lbl_val_path, self._pred_lbl_val_path, self._pred_lbl_tst_path = [], [], [], []\n        self.min_flow = np.finfo(np.float32).max\n        self.avg_flow = []\n        self.max_flow = 0.\n        for ds in datasets:\n            if ds._trn_IDs is not None:\n                self._trn_IDs.extend(ds._trn_IDs)\n            if ds._val_IDs is not None:\n                self._val_IDs.extend(ds._val_IDs)\n            if ds._tst_IDs is not None:\n                self._tst_IDs.extend(ds._tst_IDs)\n            if ds._trn_IDs_simpl is not None:\n                self._trn_IDs_simpl.extend(ds._trn_IDs_simpl)\n            if ds._val_IDs_simpl is not None:\n                self._val_IDs_simpl.extend(ds._val_IDs_simpl)\n            if ds._tst_IDs_simpl is not None:\n                self._tst_IDs_simpl.extend(ds._tst_IDs_simpl)\n            if ds._img_trn_path is not None:\n                self._img_trn_path.extend(ds._img_trn_path)\n            if ds._img_val_path is not None:\n                self._img_val_path.extend(ds._img_val_path)\n            if ds._img_tst_path is not None:\n                self._img_tst_path.extend(ds._img_tst_path)\n            if ds._lbl_trn_path is not None:\n                self._lbl_trn_path.extend(ds._lbl_trn_path)\n            if ds._lbl_val_path is not None:\n                self._lbl_val_path.extend(ds._lbl_val_path)\n            if ds._pred_lbl_val_path is not None:\n                self._pred_lbl_val_path.extend(ds._pred_lbl_val_path)\n            if ds._pred_lbl_tst_path is not None:\n                self._pred_lbl_tst_path.extend(ds._pred_lbl_tst_path)\n            self.min_flow = min(self.min_flow, ds.min_flow)\n            self.avg_flow.append(ds.avg_flow)\n            self.max_flow = max(self.max_flow, ds.max_flow)\n\n        self.avg_flow = np.mean(self.avg_flow)  # yes, this is only an approximation of the average...\n\n        # Load all data in memory, if requested\n        if self.opts[\'in_memory\']:\n            self._preload_all_samples()\n\n        # Shuffle the data and set trackers\n        np.random.seed(self.opts[\'random_seed\'])\n        if self.mode in [\'train_noval\', \'train_with_val\']:\n            # Train over the original training set, in the first case\n            self._trn_ptr = 0\n            self.trn_size = len(self._trn_IDs)\n            self._trn_idx = np.arange(self.trn_size)\n            np.random.shuffle(self._trn_idx)\n            if self.mode == \'train_with_val\':\n                # Train over the training split, validate over the validation split, in the second case\n                self._val_ptr = 0\n                self.val_size = len(self._val_IDs)\n                self._val_idx = np.arange(self.val_size)\n                np.random.shuffle(self._val_idx)\n            if self.opts[\'tb_test_imgs\'] is True:\n                # Make test images available to model in training mode\n                self._tst_ptr = 0\n                self.tst_size = len(self._tst_IDs)\n                self._tst_idx = np.arange(self.tst_size)\n                np.random.shuffle(self._tst_idx)\n            # Instantiate augmenter, if requested\n            if self.opts[\'aug_type\'] is not None:\n                assert (self.opts[\'aug_type\'] in [\'basic\', \'heavy\'])\n                self._aug = Augmenter(self.opts)\n\n        elif self.mode == \'val\':\n            # Validate over the validation split\n            self._val_ptr = 0\n            self.val_size = len(self._val_IDs)\n            self._val_idx = np.arange(self.val_size)\n            np.random.shuffle(self._val_idx)\n\n        else:\n            # Test over the entire testing set\n            self._tst_ptr = 0\n            self.tst_size = len(self._tst_IDs)\n            self._tst_idx = np.arange(self.tst_size)\n            np.random.shuffle(self._tst_idx)\n'"
tfoptflow/dataset_mpisintel.py,0,"b'""""""\ndataset_mpisintel.py\n\nMPI-Sintel (436x1024) optical flow dataset class.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport os\nfrom sklearn.model_selection import train_test_split\n\nfrom dataset_base import OpticalFlowDataset, _DATASET_ROOT, _DEFAULT_DS_TRAIN_OPTIONS\n\n_MPISINTEL_ROOT = _DATASET_ROOT + \'MPI-Sintel\'\n\n\nclass MPISintelDataset(OpticalFlowDataset):\n    """"""MPI-Sintel optical flow dataset.\n    """"""\n\n    def __init__(self, mode=\'train_with_val\', ds_root=_MPISINTEL_ROOT, options=_DEFAULT_DS_TRAIN_OPTIONS):\n        """"""Initialize the MPISintelDataset object\n        Args:\n            mode: Possible options: \'train_noval\', \'val\', \'train_with_val\' or \'test\'\n            ds_root: Path to the root of the dataset\n            options: see base class documentation\n        """"""\n        self.min_flow = 0.\n        self.avg_flow = 13.495569229125977\n        self.max_flow = 455.44061279296875\n        super().__init__(mode, ds_root, options)\n        assert(self.opts[\'type\'] in [\'clean\', \'final\'])\n\n    def set_folders(self):\n        """"""Set the train, val, test, label and prediction label folders.\n        Overriden by each dataset. Called by the base class on init.\n        Sample results:\n            self._trn_dir          = \'E:/datasets/MPI-Sintel/training/final\'\n            self._trn_lbl_dir      = \'E:/datasets/MPI-Sintel/training/flow\'\n            self._val_dir          = \'E:/datasets/MPI-Sintel/training/final\'\n            self._val_lbl_dir      = \'E:/datasets/MPI-Sintel/training/flow\'\n            self._val_pred_lbl_dir = \'E:/datasets/MPI-Sintel/training/final_flow_pred\'\n            self._tst_dir          = \'E:/datasets/MPI-Sintel/test/final\'\n            self._tst_pred_lbl_dir = \'E:/datasets/MPI-Sintel/test/final_flow_pred\'\n        """"""\n        self._trn_dir = f""{self._ds_root}/training/{self.opts[\'type\']}""\n        self._val_dir = self._trn_dir\n        self._tst_dir = f""{self._ds_root}/test/{self.opts[\'type\']}""\n\n        self._trn_lbl_dir = f""{self._ds_root}/training/flow""\n        self._val_lbl_dir = self._trn_lbl_dir\n        self._val_pred_lbl_dir = f""{self._ds_root}/training/{self.opts[\'type\']}_flow_pred""\n        self._tst_pred_lbl_dir = f""{self._ds_root}/test/{self.opts[\'type\']}_flow_pred""\n\n    def set_IDs_filenames(self):\n        """"""Set the names of the train/val/test files that will hold the list of sample/label IDs\n        Called by the base class on init.\n        Typical ID filenames:\n            \'E:/datasets/MPI-Sintel/final_train.txt\'\n            \'E:/datasets/MPI-Sintel/final_val.txt\'\n            \'E:/datasets/MPI-Sintel/final_test.txt\'\n        """"""\n        self._trn_IDs_file = f""{self._ds_root}/{self.opts[\'type\']}_train_{self.opts[\'val_split\']}split.txt""\n        self._val_IDs_file = f""{self._ds_root}/{self.opts[\'type\']}_val_{self.opts[\'val_split\']}split.txt""\n        self._tst_IDs_file = f""{self._ds_root}/{self.opts[\'type\']}_test.txt""\n\n    def _build_ID_sets(self):\n        """"""Build the list of samples and their IDs, split them in the proper datasets.\n        Called by the base class on init.\n        Each ID is a tuple.\n        For the training/val/test datasets, they look like:\n            (\'alley_1/frame_0001.png\', \'alley_1/frame_0002.png\', \'alley_1/frame_0001.flo\')\n        """"""\n        # Search the train folder for the samples, create string IDs for them\n        self._IDs = []\n        for video in os.listdir(self._trn_dir):  # video: \'alley_1\'\n            frames = sorted(os.listdir(self._trn_dir + \'/\' + video))\n            for idx in range(len(frames) - 1):\n                frame1_ID = f\'{video}/{frames[idx]}\'\n                frame2_ID = f\'{video}/{frames[idx+1]}\'\n                flow_ID = f\'{video}/{frames[idx].replace("".png"", "".flo"")}\'\n                self._IDs.append((frame1_ID, frame2_ID, flow_ID))\n\n        # Build the train/val datasets\n        if self.opts[\'val_split\'] > 0.:\n            self._trn_IDs, self._val_IDs = train_test_split(self._IDs, test_size=self.opts[\'val_split\'],\n                                                            random_state=self.opts[\'random_seed\'])\n        else:\n            self._trn_IDs, self._val_IDs = self._IDs, None\n\n        # Build the test dataset\n        self._tst_IDs = []\n        for video in os.listdir(self._tst_dir):  # video: \'ambush_1\'\n            frames = sorted(os.listdir(self._tst_dir + \'/\' + video))\n            for idx in range(len(frames) - 1):\n                frame1_ID = f\'{video}/{frames[idx]}\'\n                frame2_ID = f\'{video}/{frames[idx+1]}\'\n                flow_ID = f\'{video}/{frames[idx].replace("".png"", "".flo"")}\'\n                self._tst_IDs.append((frame1_ID, frame2_ID, flow_ID))\n\n        # Build a list of simplified IDs for Tensorboard logging\n        self._trn_IDs_simpl = self.simplify_IDs(self._trn_IDs)\n        self._val_IDs_simpl = self.simplify_IDs(self._val_IDs)\n        self._tst_IDs_simpl = self.simplify_IDs(self._tst_IDs)\n\n    def simplify_IDs(self, IDs):\n        """"""Simplify list of ID ID string tuples.\n        Go from (\'video_path/frame_0019.png\', \'video_path/frame_0020.png\', \'video_path/frame_0019.flo/\')\n        to \'video_path/frames_0019_0020\n        Args:\n            IDs: List of ID string tuples to simplify\n        Returns:\n            IDs: Simplified IDs\n        """"""\n        simple_IDs = []\n        for ID in IDs:\n            pos = ID[0].find(\'frame_\')\n            simple_IDs.append(f""{ID[0][:pos]}frames_{ID[0][pos + 6:pos + 10]}_{ID[1][pos + 6:pos + 10]}"")\n        return simple_IDs\n'"
tfoptflow/logger.py,11,"b'""""""\nlogger.py\n\nTensor ops-free logger to Tensorboard.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n\nBased on:\n  - https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n    Written by Michael Gygli\n    License: Copyleft\n\nTo look at later:\n    - Display Image1 and Image2 as an animated GIF rather than side-by-side? Demonstrated below:\n    https://github.com/djl11/PWC_Net_TensorFlow/blob/e321b0ee825416a84258559fc90e13cf722e4608/custom_ops/native.py#L216-L242\n\n    - Add error between predicted flow and gt? Demonstrated below:\n    https://github.com/djl11/PWC_Net_TensorFlow/blob/e321b0ee825416a84258559fc90e13cf722e4608/custom_ops/native.py#L198-L214\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom visualize import plot_img_pairs_w_flows\n\n\nclass TBLogger(object):\n    """"""Logging in tensorboard without tensorflow ops.""""""\n\n    def __init__(self, log_dir, tag=None, graph=None):\n        """"""Creates a summary writer logging to log_dir.\n        Args:\n            log_dir: Tensorboard logging directory\n            tag: Suggested logger types: (\'train\', \'val\', \'test\')\n            graph: Optional, TF graph\n        """"""\n        if graph is None:\n            graph = tf.get_default_graph()\n        if tag is not None:\n            log_dir = log_dir + tag\n        self.writer = tf.summary.FileWriter(log_dir, graph=graph)\n        self._tag = tag\n\n    @property\n    def tag(self):\n        if self._tag is None:\n            return self._tag\n        else:\n            return """"\n\n    def log_scalar(self, tag, value, step):\n        """"""Log a scalar variable.\n        Args:\n            tag: name of the scalar\n            value: scalar value to log\n            step: training iteration\n        """"""\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n        self.writer.add_summary(summary, step)\n\n    def log_images(self, tag, images, step, IDs=None):\n        """"""Logs a list of images.\n        Args:\n            tag: format for the name of the summary (will format ID accordingly)\n            images: list of images\n            step: training iteration\n            IDs: list of IDs\n        """"""\n        if images is None:\n            return\n\n        im_summaries = []\n        for n in range(len(images)):\n            # Write the image to a string\n            faux_file = BytesIO()  # StringIO()\n            if len(images[n].shape) == 3 and images[n].shape[2] == 1:\n                image = np.squeeze(images[n], axis=-1)  # (H, W, 1) -> (H, W)\n                cmap = \'gray\'\n            else:\n                image = images[n]\n                cmap = None\n            plt.imsave(faux_file, image, cmap=cmap, format=\'png\')  # (?, H, W, ?)\n            # Create an Image object\n            img_sum = tf.Summary.Image(encoded_image_string=faux_file.getvalue(), height=image.shape[0],\n                                       width=image.shape[1])\n            # Create a Summary value\n            img_tag = tag.format(IDs[n]) if IDs is not None else tag.format(n)\n            im_summaries.append(tf.Summary.Value(tag=img_tag, image=img_sum))\n\n        # Create and write Summary\n        summary = tf.Summary(value=im_summaries)\n        self.writer.add_summary(summary, step)\n\n    def log_histogram(self, tag, values, step, bins=1000):\n        """"""Logs the histogram of a list/vector of values.""""""\n        # Convert to a numpy array\n        values = np.array(values)\n\n        # Create histogram using numpy\n        counts, bin_edges = np.histogram(values, bins=bins)\n\n        # Fill fields of histogram proto\n        hist = tf.HistogramProto()\n        hist.min = float(np.min(values))\n        hist.max = float(np.max(values))\n        hist.num = int(np.prod(values.shape))\n        hist.sum = float(np.sum(values))\n        hist.sum_squares = float(np.sum(values ** 2))\n\n        # Requires equal number as bins, where the first goes from -DBL_MAX to bin_edges[1]\n        # See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/summary.proto#L30\n        # Thus, we drop the start of the first bin\n        bin_edges = bin_edges[1:]\n\n        # Add bin edges and counts\n        for edge in bin_edges:\n            hist.bucket_limit.append(edge)\n        for c in counts:\n            hist.bucket.append(c)\n\n        # Create and write Summary\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n        self.writer.add_summary(summary, step)\n        self.writer.flush()\n\n\nclass OptFlowTBLogger(TBLogger):\n    """"""Logging of optical flows and pyramids in tensorboard without tensorflow ops.""""""\n\n    def __init__(self, log_dir, tag=None, graph=None):\n        """"""Creates a summary writer logging to log_dir. See base class inplementation for details.\n        """"""\n        super().__init__(log_dir, tag, graph)\n\n    def log_imgs_w_flows(self, tag, img_pairs, flow_pyrs, num_lvls, flow_preds, flow_gts, step, IDs=None, info=None):\n        """"""Logs a list of optical flows.\n        Args:\n            tag: format for the name of the summary (will format ID accordingly)\n            img_pairs: list of image pairs in [batch_size, 2, H, W, 3]\n            flow_pyrs: predicted optical flow pyramids [batch_size, H, W, 2] or list([H, W, 2]) format.\n            num_lvls: number of levels per pyramid (flow_pyrs must be set)\n            flow_preds: list of predicted flows in [batch_size, H, W, 2]\n            flow_gts: optional, list of groundtruth flows in [batch_size, H, W, 2]\n            step: training iteration\n            IDs: optional, list of IDs\n        """"""\n        assert(len(img_pairs) == len(flow_preds))\n        im_summaries = []\n        for n in range(len(img_pairs)):\n            # Combine image pair, predicted flow, flow pyramid, and groundtruth flow in a single plot\n            img_pair = np.expand_dims(img_pairs[n], axis=0)\n            flow_pyr = np.expand_dims(flow_pyrs[n], axis=0) if flow_pyrs is not None else None\n            flow_pred = np.expand_dims(flow_preds[n], axis=0) if flow_preds is not None else None\n            gt_flow = np.expand_dims(flow_gts[n], axis=0) if flow_gts is not None else None\n\n            plt = plot_img_pairs_w_flows(img_pair, flow_pyr, num_lvls, flow_pred, gt_flow, None, info)\n\n            # Write the image to a string\n            faux_file = BytesIO()  # StringIO()\n            plt.savefig(faux_file, bbox_inches=\'tight\', pad_inches=0.1)\n            plt.close()\n\n            # Create an Image object\n            img_sum = tf.Summary.Image(encoded_image_string=faux_file.getvalue())\n\n            # Create a Summary value\n            img_tag = tag.format(IDs[n]) if IDs is not None else tag.format(n)\n            im_summaries.append(tf.Summary.Value(tag=img_tag, image=img_sum))\n\n        # Create and write Summary\n        summary = tf.Summary(value=im_summaries)\n        self.writer.add_summary(summary, step)\n'"
tfoptflow/losses.py,9,"b'""""""\nlosses.py\n\nLoss functions.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n\nBased on:\n    - https://github.com/NVlabs/PWC-Net/blob/master/Caffe/model/train.prototxt\n        Copyright (C) 2018 NVIDIA Corporation. All rights reserved. Licensed under the CC BY-NC-SA 4.0 license\n    - https://github.com/daigo0927/PWC-Net_tf/blob/master/losses.py\n        Written by Daigo Hirooka, Copyright (c) 2018 Daigo Hirooka\n        MIT License\n\nRef:\n    Per page 4 of paper, section ""Training loss,"" the loss function used in regular training mode is the same as the\n    one used in Dosovitskiy et al\'s ""FlowNet: Learning optical flow with convolutional networks"" paper (multiscale\n    training loss). For fine-tuning, the loss function used is described at the top of page 5 (robust training loss).\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\n\n\ndef pwcnet_loss(y, y_hat_pyr, opts):\n    """"""Adds the L2-norm or L1-norm losses at all levels of the pyramid.\n    In regular training mode, the L2-norm is used to compute the multiscale loss.\n    In fine-tuning mode, the L1-norm is used to compute the robust loss.\n    Note that the total loss returned is not regularized.\n    Args:\n        y: Optical flow groundtruths in [batch_size, H, W, 2] format\n        y_hat_pyr: Pyramid of optical flow predictions in list([batch_size, H, W, 2]) format\n        opts: options (see below)\n        Options:\n            pyr_lvls: Number of levels in the pyramid\n            alphas: Level weights (scales contribution of loss at each level toward total loss)\n            epsilon: A small constant used in the computation of the robust loss, 0 for the multiscale loss\n            q: A q<1 gives less penalty to outliers in robust loss, 1 for the multiscale loss\n            mode: Training mode, one of [\'multiscale\', \'robust\']\n    Returns:\n        Loss tensor opp\n    Ref:\n        - https://github.com/NVlabs/PWC-Net/blob/master/Caffe/model/train.prototxt\n    """"""\n    # Use a different norm based on the training mode we\'re in (training vs fine-tuning)\n    norm_order = 2 if opts[\'loss_fn\'] == \'loss_multiscale\' else 1\n\n    with tf.name_scope(opts[\'loss_fn\']):\n        total_loss = 0.\n        _, gt_height, _, _ = tf.unstack(tf.shape(y))\n\n        # Add individual pyramid level losses to the total loss\n        for lvl in range(opts[\'pyr_lvls\'] - opts[\'flow_pred_lvl\'] + 1):\n            _, lvl_height, lvl_width, _ = tf.unstack(tf.shape(y_hat_pyr[lvl]))\n\n            # Scale the full-size groundtruth to the correct lower res level\n            scaled_flow_gt = tf.image.resize_bilinear(y, (lvl_height, lvl_width))\n            scaled_flow_gt /= tf.cast(gt_height / lvl_height, dtype=tf.float32)\n\n            # Compute the norm of the difference between scaled groundtruth and prediction\n            if opts[\'use_mixed_precision\'] is False:\n                y_hat_pyr_lvl = y_hat_pyr[lvl]\n            else:\n                y_hat_pyr_lvl = tf.cast(y_hat_pyr[lvl], dtype=tf.float32)\n            norm = tf.norm(scaled_flow_gt - y_hat_pyr_lvl, ord=norm_order, axis=3)\n            level_loss = tf.reduce_mean(tf.reduce_sum(norm, axis=(1, 2)))\n\n            # Scale total loss contribution of the loss at each individual level\n            total_loss += opts[\'alphas\'][lvl] * tf.pow(level_loss + opts[\'epsilon\'], opts[\'q\'])\n\n        return total_loss\n'"
tfoptflow/lr.py,18,"b'""""""\nlr.py\n\nAdaptive learning rate utility functions.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n\nBased on:\n    - The learning rate scheme of the FlowNet2 paper\n    https://github.com/NVlabs/PWC-Net/blob/master/Caffe/model/solver.prototxt\n    Copyright (C) 2018 NVIDIA Corporation. All rights reserved. Licensed under the CC BY-NC-SA 4.0 license\n\n    - Add support for Cyclic Learning Rate #20785\n    https://github.com/tensorflow/tensorflow/pull/20785/commits/e1b30b2c50776fc1e660503d07451a6f169a7ff9\n    Written by Mahmoud Aslan, Copyright (c) 2018 Mahmoud Aslan\n    MIT License\n""""""\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\n\n\ndef lr_multisteps_long(g_step_op, boundaries=None, values=None):\n    """"""Setup the S<sub>long</sub> learning rate schedule introduced in E. Ilg et al.\'s ""FlowNet 2.0:\n        Evolution of optical flow estimation with deep networks""\n        Note that we tailor this schedule to the batch size and number of GPUs.\n        If the number of GPUs is one and batch_size is 8, then we use S<sub>long</sub>.\n        For every additional GPU, we divide the length of the schedule by that number.\n        For every additional 8 samples in the batch size, we divide the length of the schedule by 2.\n    Args:\n        g_step_op: Global step op\n        boundaries: Learning rate boundary changes\n        values: Learning rate values after boundary changes\n    Based on:\n        - https://github.com/NVlabs/PWC-Net/blob/master/Caffe/model/solver.prototxt\n        Copyright (C) 2018 NVIDIA Corporation. All rights reserved. Licensed under the CC BY-NC-SA 4.0 license\n\n        # use the learning rate scheme as the FlowNet2 paper\n        net: ""../model/train.prototxt""\n        snapshot_prefix: ""flow""\n        base_lr: 0.0001\n        lr_policy: ""multistep""\n        gamma: 0.5\n        stepvalue: 400000\n        stepvalue: 600000\n        stepvalue: 800000\n        stepvalue: 1000000\n        stepvalue: 1200000\n        momentum: 0.9\n        weight_decay: 0.0004\n        display: 100\n        max_iter: 1200000\n        snapshot: 20000\n        solver_mode: GPU\n        solver_type: ADAM\n        momentum2: 0.999\n    Ref:\n        Per page 5 of paper, section ""Implementation details,"" we first train the models using the FlyingChairs\n        dataset using the S<sub>long</sub> learning rate schedule, starting from 0.0001 and reducing the learning\n        rate by half at 0.4M, 0.6M, 0.8M, and 1M iterations.\n    """"""\n    if boundaries is None and values is None:\n        boundaries = [400000, 600000, 800000, 1000000, 1200000]\n        values = [0.0001 / (2 ** boundary) for boundary in range(len(boundaries) + 1)]\n    return tf.train.piecewise_constant(g_step_op, boundaries, values, \'lr_multisteps\')\n\n\ndef lr_multisteps_fine(g_step_op, boundaries=None, values=None):\n    """"""Setup the S<sub>fine</sub> learning rate schedule introduced in E. Ilg et al.\'s ""FlowNet 2.0:\n    Evolution of optical flow estimation with deep networks""\n    Args:\n        g_step_op: Global step op\n        boundaries: Learning rate boundary changes\n        values: Learning rate values after boundary changes\n    """"""\n    if boundaries is None and values is None:\n        boundaries = [1400000, 1500000, 1600000, 1700000]\n        values = [0.00001 / (2 ** boundary) for boundary in range(len(boundaries) + 1)]\n    return tf.train.piecewise_constant(g_step_op, boundaries, values, \'lr_multisteps\')\n\n\ndef lr_cyclic_long(g_step_op, base_lr=None, max_lr=None, step_size=None):\n    """"""Setup a cyclic learning rate for long pre-training\n    Args:\n        g_step_op: Global step op\n        base_lr: Initial learning rate and minimum bound of the cycle.\n        max_lr:  Maximum learning rate bound.\n        step_size: Number of iterations in half a cycle.\n    """"""\n    if base_lr is None and max_lr is None and step_size is None:\n        base_lr = 0.00001\n        max_lr = 0.0001\n        step_size = 10000\n    return _lr_cyclic(g_step_op, base_lr, max_lr, step_size, op_name=\'lr_cyclic\')\n\n\ndef lr_cyclic_fine(g_step_op, base_lr=None, max_lr=None, step_size=None):\n    """"""Setup a cyclic learning rate for fine-tuning\n    Args:\n        g_step_op: Global step op\n        base_lr: Initial learning rate and minimum bound of the cycle.\n        max_lr:  Maximum learning rate bound.\n        step_size: Number of iterations in half a cycle.\n    """"""\n    if base_lr is None and max_lr is None and step_size is None:\n        base_lr = 0.000001\n        max_lr = 0.00001\n        step_size = 10000\n    return _lr_cyclic(g_step_op, base_lr, max_lr, step_size, op_name=\'lr_cyclic\')\n\n\ndef _lr_cyclic(g_step_op, base_lr=None, max_lr=None, step_size=None, gamma=0.99994, mode=\'triangular2\', op_name=None):\n    """"""Computes a cyclic learning rate, based on L.N. Smith\'s ""Cyclical learning rates for training neural networks.""\n    [https://arxiv.org/pdf/1506.01186.pdf]\n\n    This method lets the learning rate cyclically vary between the minimum (base_lr) and the maximum (max_lr)\n    achieving improved classification accuracy and often in fewer iterations.\n\n    This code returns the cyclic learning rate computed as:\n\n    ```python\n    cycle = floor( 1 + global_step / ( 2 * step_size ) )\n    x = abs( global_step / step_size \xe2\x80\x93 2 * cycle + 1 )\n    clr = learning_rate + ( max_lr \xe2\x80\x93 learning_rate ) * max( 0 , 1 - x )\n    ```\n\n    Policies:\n        \'triangular\': Default, linearly increasing then linearly decreasing the learning rate at each cycle.\n\n        \'triangular2\': The same as the triangular policy except the learning rate difference is cut in half at the end\n        of each cycle. This means the learning rate difference drops after each cycle.\n\n        \'exp_range\': The learning rate varies between the minimum and maximum boundaries and each boundary value\n        declines by an exponential factor of: gamma^global_step.\n\n    Args:\n        global_step: Session global step.\n        base_lr: Initial learning rate and minimum bound of the cycle.\n        max_lr:  Maximum learning rate bound.\n        step_size: Number of iterations in half a cycle. The paper suggests 2-8 x training iterations in epoch.\n        gamma: Constant in \'exp_range\' mode gamma**(global_step)\n        mode: One of {\'triangular\', \'triangular2\', \'exp_range\'}. Default \'triangular\'.\n        name: String.  Optional name of the operation.  Defaults to \'CyclicLearningRate\'.\n    Returns:\n        The cyclic learning rate.\n    """"""\n    assert (mode in [\'triangular\', \'triangular2\', \'exp_range\'])\n    lr = tf.convert_to_tensor(base_lr, name=""learning_rate"")\n    global_step = tf.cast(g_step_op, lr.dtype)\n    step_size = tf.cast(step_size, lr.dtype)\n\n    # computing: cycle = floor( 1 + global_step / ( 2 * step_size ) )\n    double_step = tf.multiply(2., step_size)\n    global_div_double_step = tf.divide(global_step, double_step)\n    cycle = tf.floor(tf.add(1., global_div_double_step))\n\n    # computing: x = abs( global_step / step_size \xe2\x80\x93 2 * cycle + 1 )\n    double_cycle = tf.multiply(2., cycle)\n    global_div_step = tf.divide(global_step, step_size)\n    tmp = tf.subtract(global_div_step, double_cycle)\n    x = tf.abs(tf.add(1., tmp))\n\n    # computing: clr = learning_rate + ( max_lr \xe2\x80\x93 learning_rate ) * max( 0, 1 - x )\n    a1 = tf.maximum(0., tf.subtract(1., x))\n    a2 = tf.subtract(max_lr, lr)\n    clr = tf.multiply(a1, a2)\n\n    if mode == \'triangular2\':\n        clr = tf.divide(clr, tf.cast(tf.pow(2, tf.cast(cycle - 1, tf.int32)), tf.float32))\n    if mode == \'exp_range\':\n        clr = tf.multiply(tf.pow(gamma, global_step), clr)\n\n    return tf.add(clr, lr, name=op_name)\n'"
tfoptflow/mixed_precision.py,3,"b'""""""\nmixed_precision.py\n\nHelpers to train a model using mixed-precision training.\n\nModified by Phil Ferriere\n\nModifications licensed under the MIT License (see LICENSE for details)\n\nBased on:\n    - https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/Classification/imagenet/nvcnn_hvd.py\n    Written by The TensorFlow Authors, Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n    Licensed under the Apache License 2.0\n\n    - 5.6.2. TensorFlow Example\n    https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html#example_tensorflow\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\n\n\ndef float32_variable_storage_getter(getter, name, shape=None, dtype=None,\n                                    initializer=None, regularizer=None,\n                                    trainable=True,\n                                    *args, **kwargs):\n    """"""Custom variable getter that forces trainable variables to be stored in\n    float32 precision and then casts them to the training precision.\n    """"""\n    storage_dtype = tf.float32 if trainable else dtype\n    variable = getter(name, shape, dtype=storage_dtype,\n                      initializer=initializer, regularizer=regularizer,\n                      trainable=trainable,\n                      *args, **kwargs)\n    if trainable and dtype != tf.float32:\n        variable = tf.cast(variable, dtype)\n    return variable\n'"
tfoptflow/model_base.py,23,"b'""""""\nmodel_base.py\n\nModel base class.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom ckpt_mgr import BestCheckpointSaver\nfrom logger import OptFlowTBLogger\nfrom dataset_base import _DBG_TRAIN_VAL_TEST_SETS\nfrom lr import lr_multisteps_long, lr_multisteps_fine, lr_cyclic_long, lr_cyclic_fine\nfrom mixed_precision import float32_variable_storage_getter\n\n_DEBUG_USE_REF_IMPL = False\n\n\nclass ModelBase:\n    def __init__(self, name=\'base\', mode=\'train_with_val\', session=None, options=None):\n        """"""Initialize the ModelBase object\n        Args:\n            mode: Must be in [\'train_noval\', \'val\', \'train_with_val\', \'test\']\n            session: optional TF session\n            options: see _DEFAULT_PWCNET_TRAIN_OPTIONS comments\n        Mote:\n            As explained [here](https://stackoverflow.com/a/36282423), you don\'t need to use with blocks if you only\n            have one default graph and one default session. However, we sometimes create notebooks where we pit the\n            performance of models against each other. Because of that, we need the with block.\n            # tf.reset_default_graph()\n            # self.graph = tf.Graph()\n            # with self.graph.as_default():\n        """"""\n        assert(mode in [\'train_noval\', \'train_with_val\', \'val\', \'val_notrain\', \'test\'])\n        self.mode, self.sess, self.opts = mode, session, options\n        self.y_hat_train_tnsr = self.y_hat_val_tnsr = self.y_hat_test_tnsr = None\n        self.name = name\n        self.num_gpus = len(self.opts[\'gpu_devices\'])\n        self.dbg = False  # Set this to True for a detailed log of operation\n\n        if _DBG_TRAIN_VAL_TEST_SETS != -1:  # Debug mode only\n            if self.mode in [\'train_noval\', \'train_with_val\']:\n                self.opts[\'display_step\'] = 10  # show progress every 10 training batches\n                self.opts[\'snapshot_step\'] = 100  # save trained model every 100 training batches\n                self.opts[\'val_step\'] = 100  # Test trained model on validation split every 1000 training batches\n                if self.opts[\'lr_boundaries\'] == \'multisteps\':\n                    self.opts[\'lr_boundaries\'] = [int(boundary / 1000) for boundary in self.opts[\'lr_boundaries\']]\n                    self.opts[\'max_steps\'] = self.opts[\'lr_boundaries\'][-1]\n                else:\n                    self.opts[\'cyclic_lr_stepsize\'] = 50\n                    self.opts[\'max_steps\'] = 500  # max number of training iterations (i.e., batches to run)\n\n        tf.reset_default_graph()\n        self.graph = tf.Graph()\n        with self.graph.as_default():\n            # Configure a TF session, if one doesn\'t already exist\n            self.config_session(session)\n\n            # Build the TF graph\n            self.build_graph()\n\n    ###\n    # Session mgmt\n    ###\n    def config_session(self, sess):\n        """"""Configure a TF session, if one doesn\'t already exist.\n        Args:\n            sess: optional TF session\n        """"""\n        if sess is None:\n            config = tf.ConfigProto()\n            config.gpu_options.allow_growth = True\n            if self.dbg:\n                config.log_device_placement = True\n            config.allow_soft_placement = True\n            self.sess = tf.Session(config=config)\n        else:\n            self.sess = sess\n\n        tf.logging.set_verbosity(tf.logging.INFO)\n\n    ###\n    # Training-specific helpers\n    ###\n    def config_train_ops(self):\n        """"""Configure training ops. Override this to train your model.\n        Called by the base class when building the TF graph to setup all the training ops, including:\n            - setting up loss computations,\n            - setting up metrics computations,\n            - selecting an optimizer,\n            - creating a training schedule.\n        """"""\n        raise NotImplementedError\n\n    def config_loggers(self):\n        """"""Configure train logger and, optionally, val logger.\n        """"""\n        if self.mode == \'train_with_val\':\n            self.tb_train = OptFlowTBLogger(self.opts[\'ckpt_dir\'], \'train\')\n            self.tb_val = OptFlowTBLogger(self.opts[\'ckpt_dir\'], \'val\')\n        elif self.mode == \'train_noval\':\n            self.tb_train = OptFlowTBLogger(self.opts[\'ckpt_dir\'], \'train\')\n\n    ###\n    # Checkpoint mgmt\n    ###\n    def init_saver(self):\n        """"""Creates a default saver to load/save model checkpoints. Override, if necessary.\n        """"""\n        if self.mode in [\'train_noval\', \'train_with_val\']:\n            self.saver = BestCheckpointSaver(self.opts[\'ckpt_dir\'], self.name, self.opts[\'max_to_keep\'], maximize=False)\n        else:\n            self.saver = tf.train.Saver()\n\n    def save_ckpt(self, ranking_value=0):\n        """"""Save a model checkpoint\n        Args:\n            ranking_value: The ranking value by which to rank the checkpoint.\n        """"""\n        assert(self.mode in [\'train_noval\', \'train_with_val\'])\n        if self.opts[\'verbose\']:\n            print(""Saving model..."")\n\n        # save_path = self.saver.save(self.sess, self.opts[\'ckpt_dir\'] + self.name, self.g_step_op)\n        save_path = self.saver.save(ranking_value, self.sess, self.g_step_op)\n\n        if self.opts[\'verbose\']:\n            if save_path is None:\n                msg = f""... model wasn\'t saved -- its score ({ranking_value:.2f}) doesn\'t outperform other checkpoints""\n            else:\n                msg = f""... model saved in {save_path}""\n            print(msg)\n\n    def load_ckpt(self):\n        """"""Load a model checkpoint\n        In train mode, load the latest checkpoint from the checkpoint folder if it exists; otherwise, run initializer.\n        In other modes, load from the specified checkpoint file.\n        """"""\n        if self.mode in [\'train_noval\', \'train_with_val\']:\n            self.last_ckpt = None\n            if self.opts[\'train_mode\'] == \'fine-tune\':\n                # In fine-tuning mode, we just want to load the trained params from the file and that\'s it...\n                assert(tf.train.checkpoint_exists(self.opts[\'ckpt_path\']))\n                if self.opts[\'verbose\']:\n                    print(f""Initializing from pre-trained model at {self.opts[\'ckpt_path\']} for finetuning...\\n"")\n                # ...however, the AdamOptimizer also stores variables in the graph, so reinitialize them as well\n                self.sess.run(tf.variables_initializer(self.optim.variables()))\n                # Now initialize the trained params with actual values from the checkpoint\n                _saver = tf.train.Saver(var_list=tf.trainable_variables())\n                _saver.restore(self.sess, self.opts[\'ckpt_path\'])\n                if self.opts[\'verbose\']:\n                    print(""... model initialized"")\n                self.last_ckpt = self.opts[\'ckpt_path\']\n            else:\n                # In training mode, we either want to start a new training session or resume from a previous checkpoint\n                self.last_ckpt = self.saver.best_checkpoint(self.opts[\'ckpt_dir\'], maximize=False)\n                if self.last_ckpt is None:\n                    self.last_ckpt = tf.train.latest_checkpoint(self.opts[\'ckpt_dir\'])\n\n                if self.last_ckpt:\n                    # We\'re resuming a session -> initialize the graph with the content of the checkpoint\n                    if self.opts[\'verbose\']:\n                        print(f""Initializing model from previous checkpoint {self.last_ckpt} to resume training...\\n"")\n                    self.saver.restore(self.sess, self.last_ckpt)\n                    if self.opts[\'verbose\']:\n                        print(""... model initialized"")\n                else:\n                    # Initialize all the variables of the graph\n                    if self.opts[\'verbose\']:\n                        print(f""Initializing model with random values for initial training...\\n"")\n                    assert (self.mode in [\'train_noval\', \'train_with_val\'])\n                    self.sess.run(tf.global_variables_initializer())\n                    if self.opts[\'verbose\']:\n                        print(""... model initialized"")\n        else:\n            # Initialize the graph with the content of the checkpoint\n            self.last_ckpt = self.opts[\'ckpt_path\']\n            assert(self.last_ckpt is not None)\n            if self.opts[\'verbose\']:\n                print(f""Loading model checkpoint {self.last_ckpt} for eval or testing...\\n"")\n            self.saver.restore(self.sess, self.last_ckpt)\n            if self.opts[\'verbose\']:\n                print(""... model loaded"")\n\n    ###\n    # Model mgmt\n    ###\n    def build_model(self):\n        """"""Build model. Override this.\n        """"""\n        raise NotImplementedError\n\n    def set_output_tnsrs(self):\n        """"""Initialize output tensors. Override this.\n        """"""\n        raise NotImplementedError\n\n    ###\n    # Graph mgmt\n    ###\n    def config_placeholders(self):\n        """"""Configure input and output tensors\n        Args:\n            x_dtype, x_shape:  type and shape of elements in the input tensor\n            y_dtype, y_shape:  shape of elements in the input tensor\n        """"""\n        # Increase the batch size with the number of GPUs dedicated to computing TF ops\n        batch_size = self.num_gpus * self.opts[\'batch_size\']\n        self.x_tnsr = tf.placeholder(self.opts[\'x_dtype\'], [batch_size] + self.opts[\'x_shape\'], \'x_tnsr\')\n        self.y_tnsr = tf.placeholder(self.opts[\'y_dtype\'], [batch_size] + self.opts[\'y_shape\'], \'y_tnsr\')\n\n    def build_graph(self):\n        """""" Build the complete graph in TensorFlow\n        """"""\n        # with tf.device(self.main_device):\n        # Configure input and output tensors\n        self.config_placeholders()\n\n        # Build the backbone network, then:\n        # In training mode, configure training ops (loss, metrics, optimizer, and lr schedule)\n        # Also, config train logger and, optionally, val logger\n        # In validation mode, configure validation ops (loss, metrics)\n        if self.mode in [\'train_noval\', \'train_with_val\']:\n            if self.opts[\'use_mixed_precision\'] is True:\n                with tf.variable_scope(\'fp32_vars\', custom_getter=float32_variable_storage_getter):\n                    if self.num_gpus == 1:\n                        self.build_model()\n                        self.config_train_ops()\n                    else:\n                        self.build_model_towers()\n            else:\n                if self.num_gpus == 1:\n                    self.build_model()\n                    self.config_train_ops()\n                else:\n                    self.build_model_towers()\n\n            self.config_loggers()\n\n        elif self.mode in [\'val\', \'val_notrain\']:\n            if self.opts[\'use_mixed_precision\'] is True:\n                with tf.variable_scope(\'fp32_vars\', custom_getter=float32_variable_storage_getter):\n                    self.build_model()\n                    self.setup_metrics_ops()\n            else:\n                self.build_model()\n                self.setup_metrics_ops()\n\n        else:  # inference mode\n            if self.opts[\'use_mixed_precision\'] is True:\n                with tf.variable_scope(\'fp32_vars\', custom_getter=float32_variable_storage_getter):\n                    self.build_model()\n            else:\n                self.build_model()\n\n        # Set output tensors\n        self.set_output_tnsrs()\n\n        # Init saver (override if you wish) and load checkpoint if it exists\n        self.init_saver()\n        self.load_ckpt()\n\n    ###\n    # Sample mgmt (preprocessing and postprocessing)\n    ###\n    def adapt_x(self, x):\n        """"""Preprocess the input samples to adapt them to the network\'s requirements\n        Here, x, is the actual data, not the x TF tensor. Override as necessary.\n        Args:\n            x: input samples\n        Returns:\n            Samples ready to be given to the network (w. same shape as x) and companion adaptation info\n        """"""\n        return x, None\n\n    def adapt_y(self, y):\n        """"""Preprocess the labels to adapt them to the loss computation requirements of the network\n        Here, y, is the actual data, not the y TF tensor. Override as necessary.\n        Args:\n            y: training labels\n        Returns:\n            Labels ready to be used by the network\'s loss function (w. same shape as y) and companion adaptation inf\n        """"""\n        return y, None\n\n    def postproc_y_hat(self, y_hat):\n        """"""Postprocess the predictions coming from the network. Override as necessary.\n        Here, y_hat, is the actual data, not the y_hat TF tensor.\n        Args:\n            y_hat: predictions\n        Returns:\n            Postprocessed labels\n        """"""\n        return y_hat\n\n    ###\n    # Learning rate helpers\n    ###\n    def setup_lr_sched(self):\n        """"""Setup a learning rate training schedule and setup the global step. Override as necessary.\n        """"""\n        assert (self.opts[\'lr_policy\'] in [None, \'multisteps\', \'cyclic\'])\n        self.g_step_op = tf.train.get_or_create_global_step()\n\n        # Use a set learning rate, if requested\n        if self.opts[\'lr_policy\'] is None:\n            self.lr = tf.constant(self.opts[\'init_lr\'])\n            return\n\n        # Use a learning rate schedule, if requested\n        assert (self.opts[\'train_mode\'] in [\'train\', \'fine-tune\'])\n        if self.opts[\'lr_policy\'] == \'multisteps\':\n            boundaries = self.opts[\'lr_boundaries\']\n            values = self.opts[\'lr_values\']\n            if self.opts[\'train_mode\'] == \'train\':\n                self.lr = lr_multisteps_long(self.g_step_op, boundaries, values)\n            else:\n                self.lr = lr_multisteps_fine(self.g_step_op, boundaries, values)\n        else:\n            lr_base = self.opts[\'cyclic_lr_base\']\n            lr_max = self.opts[\'cyclic_lr_max\']\n            lr_stepsize = self.opts[\'cyclic_lr_stepsize\']\n            if self.opts[\'train_mode\'] == \'train\':\n                self.lr = lr_cyclic_long(self.g_step_op, lr_base, lr_max, lr_stepsize)\n            else:\n                self.lr = lr_cyclic_fine(self.g_step_op, lr_base, lr_max, lr_stepsize)\n\n    ###\n    # Debug utils\n    ###\n    def summary(self):\n        model_vars = tf.trainable_variables()\n        slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n\n    def print_config(self):\n        """"""Display configuration values.\n        Ref:\n            - How to count total number of trainable parameters in a tensorflow model?\n            https://stackoverflow.com/questions/38160940/how-to-count-total-number-of-trainable-parameters-in-a-tensorflow-model\n        """"""\n        with self.graph.as_default():\n            print(""\\nModel Configuration:"")\n            for k, v in self.opts.items():\n                if self.mode in [\'train_noval\', \'train_with_val\']:\n                    if self.opts[\'lr_policy\'] == \'multisteps\':\n                        if k in [\'init_lr\', \'cyclic_lr_max\', \'cyclic_lr_base\', \'cyclic_lr_stepsize\']:\n                            continue\n                    if self.opts[\'lr_policy\'] == \'cyclic\':\n                        if k in [\'init_lr\', \'lr_boundaries\', \'lr_values\']:\n                            continue\n                print(f""  {k:22} {v}"")\n            print(f""  {\'mode\':22} {self.mode}"")\n            # if self.mode in [\'train_noval\', \'train_with_val\']:\n            if self.dbg:\n                self.summary()\n            print(f""  {\'trainable params\':22} {np.sum([np.prod(v.shape) for v in tf.trainable_variables()])}"")\n'"
tfoptflow/model_pwcnet.py,97,"b'""""""\nmodel_pwcnet.py\n\nPWC-Net model class.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport time\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tqdm import trange\nfrom tensorflow.contrib.mixed_precision import LossScaleOptimizer, FixedLossScaleManager\n\nfrom model_base import ModelBase\nfrom optflow import flow_write, flow_write_as_png, flow_mag_stats\nfrom losses import pwcnet_loss\nfrom logger import OptFlowTBLogger\nfrom multi_gpus import assign_to_device, average_gradients\nfrom core_warp import dense_image_warp\nfrom core_costvol import cost_volume\nfrom utils import tf_where\n\n_DEBUG_USE_REF_IMPL = False\n\n# Default options\n_DEFAULT_PWCNET_TRAIN_OPTIONS = {\n    \'verbose\': False,\n    \'ckpt_dir\': \'./ckpts_trained/\',  # where training checkpoints are stored\n    \'max_to_keep\': 10,\n    \'x_dtype\': tf.float32,  # image pairs input type\n    \'x_shape\': [2, 384, 448, 3],  # image pairs input shape [2, H, W, 3]\n    \'y_dtype\': tf.float32,  # u,v flows output type\n    \'y_shape\': [384, 448, 2],  # u,v flows output shape [H, W, 2]\n    \'train_mode\': \'train\',  # in [\'train\', \'fine-tune\']\n    \'adapt_info\': None,  # if predicted flows are padded by the model, crop them back by to this size\n    \'sparse_gt_flow\': False,  # if gt flows are sparse (KITTI), only compute average EPE where gt flows aren\'t (0., 0.)\n    # Logging/Snapshot params\n    \'display_step\': 100,  # show progress every 100 training batches\n    \'snapshot_step\': 1000,  # save trained model every 1000 training batches\n    \'val_step\': 1000,  # Test trained model on validation split every 1000 training batches\n    \'val_batch_size\': -1,  # Use -1 to use entire validation split, or set number of val samples (0 disables it)\n    # None or in [\'top_flow\', \'pyramid\'|; runs trained model on batch_size random val images, log results\n    \'tb_val_imgs\': \'pyramid\',\n    # None or in [\'top_flow\', \'pyramid\'|; runs trained model on batch_size random test images, log results\n    \'tb_test_imgs\': None,\n    # Multi-GPU config\n    # list devices on which to run the model\'s train ops (can be more than one GPU)\n    \'gpu_devices\': [\'/device:GPU:0\', \'/device:GPU:1\'],\n    # controller device to put the model\'s variables on (usually, /cpu:0 or /gpu:0 -> try both!)\n    \'controller\': \'/device:CPU:0\',\n    # Training config and hyper-params\n    \'use_tf_data\': True,  # Set to True to get data from tf.data.Dataset; otherwise, use feed_dict with numpy\n    \'use_mixed_precision\': False,  # Set to True to use mixed precision training (fp16 inputs)\n    \'loss_scaler\': 128.,  # Loss scaler (only used in mixed precision training)\n    \'batch_size\': 8,\n    \'lr_policy\': \'multisteps\',  # choose between None, \'multisteps\', and \'cyclic\'; adjust the max_steps below too\n    # Multistep lr schedule\n    \'init_lr\': 1e-04,  # initial learning rate\n    \'max_steps\': 1200000,  # max number of training iterations (i.e., batches to run)\n    \'lr_boundaries\': [400000, 600000, 800000, 1000000, 1200000],  # step schedule boundaries\n    \'lr_values\': [0.0001, 5e-05, 2.5e-05, 1.25e-05, 6.25e-06, 3.125e-06],  # step schedule values\n    # Cyclic lr schedule\n    \'cyclic_lr_max\': 5e-04,  # max bound, anything higher will generate NaNs on `FlyingChairs+FlyingThings3DHalfRes` mix\n    \'cyclic_lr_base\': 1e-05,  # min bound\n    \'cyclic_lr_stepsize\': 20000,  # step schedule values\n    # \'max_steps\': 200000, # max number of training iterations\n    # Loss functions hyper-params\n    \'loss_fn\': \'loss_multiscale\',  # See \'Implementation details"" on page 5 of ref PDF\n    \'alphas\': [0.32, 0.08, 0.02, 0.01, 0.005, 0.0025],  # See \'Implementation details"" on page 5 of ref PDF\n    \'gamma\': 0.0004,  # See \'Implementation details"" on page 5 of ref PDF\n    \'q\': 1.,  # See \'Implementation details"" on page 5 of ref PDF\n    \'epsilon\': 0.,  # See \'Implementation details"" on page 5 of ref PDF\n    # Model hyper-params\n    \'pyr_lvls\': 6,  # number of feature levels in the flow pyramid\n    \'flow_pred_lvl\': 2,  # which level to upsample to generate the final optical flow prediction\n    \'search_range\': 4,  # cost volume search range\n    # if True, use model with dense connections (4705064 params w/o, 9374274 params with (no residual conn.))\n    \'use_dense_cx\': False,\n    # if True, use model with residual connections (4705064 params w/o, 6774064 params with (+2069000) (no dense conn.))\n    \'use_res_cx\': False,\n}\n\n_DEFAULT_PWCNET_FINETUNE_OPTIONS = {\n    \'verbose\': False,\n    \'ckpt_path\': \'./ckpts_trained/pwcnet.ckpt\',  # original checkpoint to finetune\n    \'ckpt_dir\': \'./ckpts_finetuned/\',  # where finetuning checkpoints are stored\n    \'max_to_keep\': 10,\n    \'x_dtype\': tf.float32,  # image pairs input type\n    \'x_shape\': [2, 384, 768, 3],  # image pairs input shape [2, H, W, 3]\n    \'y_dtype\': tf.float32,  # u,v flows output type\n    \'y_shape\': [384, 768, 2],  # u,v flows output shape [H, W, 2]\n    \'train_mode\': \'fine-tune\',  # in [\'train\', \'fine-tune\']\n    \'adapt_info\': None,  # if predicted flows are padded by the model, crop them back by to this size\n    \'sparse_gt_flow\': False,  # if gt flows are sparse (KITTI), only compute average EPE where gt flows aren\'t (0., 0.)\n    # Logging/Snapshot params\n    \'display_step\': 100,  # show progress every 100 training batches\n    \'snapshot_step\': 1000,  # save trained model every 1000 training batches\n    \'val_step\': 1000,  # Test trained model on validation split every 1000 training batches\n    \'val_batch_size\': -1,  # Use -1 to use entire validation split, or set number of val samples (0 disables it)\n    \'tb_val_imgs\': \'top_flow\',  # None, \'top_flow\', or \'pyramid\'; runs model on batch_size val images, log results\n    \'tb_test_imgs\': None,  # None, \'top_flow\', or \'pyramid\'; runs trained model on batch_size test images, log results\n    # Multi-GPU config\n    # list devices on which to run the model\'s train ops (can be more than one GPU)\n    \'gpu_devices\': [\'/device:GPU:0\', \'/device:GPU:1\'],\n    # controller device to put the model\'s variables on (usually, /cpu:0 or /gpu:0 -> try both!)\n    \'controller\': \'/device:CPU:0\',\n    # Training config and hyper-params\n    \'use_tf_data\': True,  # Set to True to get data from tf.data.Dataset; otherwise, use feed_dict with numpy\n    \'use_mixed_precision\': False,  # Set to True to use mixed precision training (fp16 inputs)\n    \'loss_scaler\': 128.,  # Loss scaler (only used in mixed precision training)\n    \'batch_size\': 4,\n    \'lr_policy\': \'multisteps\',  # choose between None, \'multisteps\', and \'cyclic\'; adjust the max_steps below too\n    # Multistep lr schedule\n    \'init_lr\': 1e-05,  # initial learning rate\n    \'max_steps\': 500000,  # max number of training iterations (i.e., batches to run)\n    \'lr_boundaries\': [200000, 300000, 400000, 500000],  # step schedule boundaries\n    \'lr_values\': [1e-05, 5e-06, 2.5e-06, 1.25e-06, 6.25e-07],  # step schedule values\n    # Cyclic lr schedule\n    \'cyclic_lr_max\': 2e-05,  # maximum bound\n    \'cyclic_lr_base\': 1e-06,  # min bound\n    \'cyclic_lr_stepsize\': 20000,  # step schedule values\n    # \'max_steps\': 200000, # max number of training iterations\n    # Loss functions hyper-params\n    \'loss_fn\': \'loss_robust\',  # \'loss_robust\' doesn\'t really work; the loss goes down but the EPE doesn\'t\n    \'alphas\': [0.32, 0.08, 0.02, 0.01, 0.005],  # See \'Implementation details"" on page 5 of ref PDF\n    \'gamma\': 0.0004,  # See \'Implementation details"" on page 5 of ref PDF\n    \'q\': 0.4,  # See \'Implementation details"" on page 5 of ref PDF\n    \'epsilon\': 0.01,  # See \'Implementation details"" on page 5 of ref PDF\n    # Model hyper-params\n    \'pyr_lvls\': 6,  # number of feature levels in the flow pyramid\n    \'flow_pred_lvl\': 2,  # which level to upsample to generate the final optical flow prediction\n    \'search_range\': 4,  # cost volume search range\n    # if True, use model with dense connections (4705064 params w/o, 9374274 params with (no residual conn.))\n    \'use_dense_cx\': False,\n    # if True, use model with residual connections (4705064 params w/o, 6774064 params with (+2069000) (no dense conn.))\n    \'use_res_cx\': False,\n}\n\n_DEFAULT_PWCNET_VAL_OPTIONS = {\n    \'verbose\': False,\n    \'ckpt_path\': \'./ckpts_trained/pwcnet.ckpt\',\n    \'x_dtype\': tf.float32,  # image pairs input type\n    \'x_shape\': [2, None, None, 3],  # image pairs input shape [2, H, W, 3]\n    \'y_dtype\': tf.float32,  # u,v flows output type\n    \'y_shape\': [None, None, 2],  # u,v flows output shape [H, W, 2]\n    \'adapt_info\': None,  # if predicted flows are padded by the model, crop them back by to this size\n    \'sparse_gt_flow\': False,  # if gt flows are sparse (KITTI), only compute average EPE where gt flows aren\'t (0., 0.)\n    # Multi-GPU config\n    # list devices on which to run the model\'s train ops (can be more than one GPU)\n    \'gpu_devices\': [\'/device:GPU:0\', \'/device:GPU:1\'],\n    # controller device to put the model\'s variables on (usually, /cpu:0 or /gpu:0 -> try both!)\n    \'controller\': \'/device:CPU:0\',\n    # Eval config and hyper-params\n    \'batch_size\': 1,\n    \'use_tf_data\': True,  # Set to True to get data from tf.data.Dataset; otherwise, use feed_dict with numpy\n    \'use_mixed_precision\': False,  # Set to True to use fp16 inputs\n    # Model hyper-params\n    \'pyr_lvls\': 6,  # number of feature levels in the flow pyramid\n    \'flow_pred_lvl\': 2,  # which level to upsample to generate the final optical flow prediction\n    \'search_range\': 4,  # cost volume search range\n    # if True, use model with dense connections (4705064 params w/o, 9374274 params with (no residual conn.))\n    \'use_dense_cx\': False,\n    # if True, use model with residual connections (4705064 params w/o, 6774064 params with (+2069000) (no dense conn.))\n    \'use_res_cx\': False,\n}\n\n_DEFAULT_PWCNET_TEST_OPTIONS = {\n    \'verbose\': False,\n    \'ckpt_path\': \'./ckpts_trained/pwcnet.ckpt\',\n    \'x_dtype\': tf.float32,  # image pairs input type\n    \'x_shape\': [2, None, None, 3],  # image pairs input shape\n    \'y_dtype\': tf.float32,  # u,v flows output type\n    \'y_shape\': [None, None, 2],  # u,v flows output shape\n    # Multi-GPU config\n    # list devices on which to run the model\'s train ops (can be more than one GPU)\n    \'gpu_devices\': [\'/device:GPU:0\', \'/device:GPU:1\'],\n    # controller device to put the model\'s variables on (usually, /cpu:0 or /gpu:0 -> try both!)\n    \'controller\': \'/device:CPU:0\',\n    # Eval config and hyper-params\n    \'batch_size\': 1,\n    \'use_tf_data\': True,  # Set to True to get data from tf.data.Dataset; otherwise, use feed_dict with numpy\n    \'use_mixed_precision\': False,  # Set to True to use fp16 inputs\n    # Model hyper-params\n    \'pyr_lvls\': 6,  # number of feature levels in the flow pyramid\n    \'flow_pred_lvl\': 2,  # which level to upsample to generate the final optical flow prediction\n    \'search_range\': 4,  # cost volume search range\n    # if True, use model with dense connections (4705064 params w/o, 9374274 params with (no residual conn.))\n    \'use_dense_cx\': False,\n    # if True, use model with residual connections (4705064 params w/o, 6774064 params with (+2069000) (no dense conn.))\n    \'use_res_cx\': False,\n}\n\n# from ref_model import PWCNet\n\n\nclass ModelPWCNet(ModelBase):\n    def __init__(self, name=\'pwcnet\', mode=\'train\', session=None, options=_DEFAULT_PWCNET_TEST_OPTIONS, dataset=None):\n        """"""Initialize the ModelPWCNet object\n        Args:\n            name: Model name\n            mode: Possible values: \'train\', \'val\', \'test\'\n            session: optional TF session\n            options: see _DEFAULT_PWCNET_TRAIN_OPTIONS comments\n            dataset: Dataset loader\n        Training Ref:\n            Per page 4 of paper, section ""Training loss,"" the loss function used in regular training mode is the same as\n            the one used in Dosovitskiy et al\'s ""FlowNet: Learning optical flow with convolutional networks"" paper\n            (multiscale training loss). For fine-tuning, the loss function used is described at the top of page 5\n            (robust training loss).\n\n            Per page 5 of paper, section ""Implementation details,"" the trade-off weight gamma in the regularization term\n            is usually set to 0.0004.\n\n            Per page 5 of paper, section ""Implementation details,"" we first train the models using the FlyingChairs\n            dataset using the S<sub>long</sub> learning rate schedule introduced in E. Ilg et al.\'s ""FlowNet 2.0:\n            Evolution of optical flow estimation with deep networks"", starting from 0.0001 and reducing the learning\n            rate by half at 0.4M, 0.6M, 0.8M, and 1M iterations. The data augmentation scheme is the same as in that\n            paper. We crop 448 \xc3\x97 384 patches during data augmentation and use a batch size of 8. We then fine-tune the\n            models on the FlyingThings3D dataset using the S<sub>fine</sub> schedule while excluding image pairs with\n            extreme motion (magnitude larger than 1000 pixels). The cropped image size is 768 \xc3\x97 384 and the batch size\n            is 4. Finally, we finetune the models using the Sintel and KITTI training set as detailed in section ""4.1.\n            Main results"".\n        """"""\n        super().__init__(name, mode, session, options)\n        self.ds = dataset\n        # self.adapt_infos = []\n        # self.unique_y_shapes = []\n\n    ###\n    # Model mgmt\n    ###\n    def build_model(self):\n        """"""Build model\n        Called by the base class when building the TF graph to setup the list of output tensors\n        """"""\n        if self.opts[\'verbose\']:\n            print(""Building model..."")\n        assert(self.num_gpus <= 1)\n\n        # Build the backbone neural nets and collect the output tensors\n        with tf.device(self.opts[\'controller\']):\n            self.flow_pred_tnsr, self.flow_pyr_tnsr = self.nn(self.x_tnsr)\n\n        if self.opts[\'verbose\']:\n            print(""... model built."")\n\n    def build_model_towers(self):\n        """"""Build model towers. A tower is the name used to describe a copy of the model on a device.\n        Called by the base class when building the TF graph to setup the list of output tensors\n        """"""\n        if self.opts[\'verbose\']:\n            print(""Building model towers..."")\n\n        # Setup a learning rate training schedule\n        self.setup_lr_sched()\n\n        # Instantiate an optimizer\n        # see https://stackoverflow.com/questions/42064941/tensorflow-float16-support-is-broken\n        # for float32 epsilon=1e-08, for float16 use epsilon=1e-4\n        epsilon = 1e-08 if self.opts[\'use_mixed_precision\'] is False else 1e-4\n        assert (self.opts[\'train_mode\'] in [\'train\', \'fine-tune\'])\n        if self.opts[\'loss_fn\'] == \'loss_multiscale\':\n            self.optim = tf.train.AdamOptimizer(self.lr, epsilon=epsilon)\n        else:\n            self.optim = tf.train.ProximalGradientDescentOptimizer(self.lr)\n\n        # Keep track of the gradients and losses per tower\n        tower_grads, losses, metrics = [], [], []\n\n        # Get the current variable scope so we can reuse all variables we need once we get\n        # to the next iteration of the for loop below\n        with tf.variable_scope(tf.get_variable_scope()) as outer_scope:\n            for n, ops_device in enumerate(self.opts[\'gpu_devices\']):\n                print(f""  Building tower_{n}..."")\n                # Use the assign_to_device function to ensure that variables are created on the controller.\n                with tf.device(assign_to_device(ops_device, self.opts[\'controller\'])), tf.name_scope(f\'tower_{n}\'):\n                    # Get a slice of the input batch and groundtruth label\n                    x_tnsr = self.x_tnsr[n * self.opts[\'batch_size\']:(n + 1) * self.opts[\'batch_size\'], :]\n                    y_tnsr = self.y_tnsr[n * self.opts[\'batch_size\']:(n + 1) * self.opts[\'batch_size\'], :]\n\n                    # Build the model for that slice\n                    flow_pred_tnsr, flow_pyr_tnsr = self.nn(x_tnsr)\n\n                    # The first tower is also the model we will use to perform online evaluation\n                    if n == 0:\n                        self.flow_pred_tnsr, self.flow_pyr_tnsr = flow_pred_tnsr, flow_pyr_tnsr\n\n                    # Compute the loss for this tower, with regularization term if requested\n                    loss_unreg = pwcnet_loss(y_tnsr, flow_pyr_tnsr, self.opts)\n                    if self.opts[\'gamma\'] == 0.:\n                        loss = loss_unreg\n                    else:\n                        loss_reg = self.opts[\'gamma\'] * \\\n                            tf.reduce_sum([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n                        loss = loss_unreg + loss_reg\n\n                    # Evaluate model performance on this tower\n                    metrics.append(tf.reduce_mean(tf.norm(y_tnsr - flow_pred_tnsr, ord=2, axis=3)))\n\n                    # Compute the gradients for this tower, but don\'t apply them yet\n                    with tf.name_scope(""compute_gradients""):\n                        # The function compute_gradients() returns a list of (gradient, variable) pairs\n                        if self.opts[\'use_mixed_precision\'] is True:\n                            grads, vars = zip(*self.optim.compute_gradients(loss * self.opts[\'loss_scaler\']))\n                            # Return the gradients (now float32) to the correct exponent and keep them in check\n                            grads = [grad / self.opts[\'loss_scaler\'] for grad in grads]\n                            grads, _ = tf.clip_by_global_norm(grads, 5.0)\n                            tower_grads.append(zip(grads, vars))\n                        else:\n                            grad_and_vars = self.optim.compute_gradients(loss)\n                            tower_grads.append(grad_and_vars)\n\n                    losses.append(loss)\n\n                # After the first iteration, we want to reuse the variables.\n                outer_scope.reuse_variables()\n                print(f""  ...tower_{n} built."")\n\n        # Apply the gradients on the controlling device\n        with tf.name_scope(""apply_gradients""), tf.device(self.opts[\'controller\']):\n            # Note that what we are doing here mathematically is equivalent to returning the average loss over the\n            # towers and compute the gradients relative to that. Unfortunately, this would place all gradient\n            # computations on one device, which is why we had to compute the gradients above per tower and need to\n            # average them here. The function average_gradients() takes the list of (gradient, variable) lists\n            # and turns it into a single (gradient, variables) list.\n            avg_grads_op = average_gradients(tower_grads)\n            self.optim_op = self.optim.apply_gradients(avg_grads_op, self.g_step_op)\n            self.loss_op = tf.reduce_mean(losses)\n            self.metric_op = tf.reduce_mean(metrics)\n\n        if self.opts[\'verbose\']:\n            print(""... model towers built."")\n\n    def set_output_tnsrs(self):\n        """"""Initialize output tensors\n        """"""\n        if self.mode in [\'train_noval\', \'train_with_val\']:\n            # self.y_hat_train_tnsr = [self.loss_op, self.metric_op, self.optim_op, self.g_step_inc_op]\n            self.y_hat_train_tnsr = [self.loss_op, self.metric_op, self.optim_op]\n\n        if self.mode == \'train_with_val\':\n            # In online evaluation mode, we only care about the average loss and metric for the batch:\n            self.y_hat_val_tnsr = [self.loss_op, self.metric_op]\n\n        if self.mode in [\'val\', \'val_notrain\']:\n            # In offline evaluation mode, we only care about the individual predictions and metrics:\n            self.y_hat_val_tnsr = [self.flow_pred_tnsr, self.metric_op]\n\n            # if self.opts[\'sparse_gt_flow\'] is True:\n            #     # Find the location of the zerod-out flows in the gt\n            #     zeros_loc = tf.logical_and(tf.equal(self.y_tnsr[:, :, :, 0], 0.0), tf.equal(self.y_tnsr[:, :, :, 1], 0.0))\n            #     zeros_loc = tf.expand_dims(zeros_loc, -1)\n            #\n            #     # Zero out flow predictions at the same location so we only compute the EPE at the sparse flow points\n            #     sparse_flow_pred_tnsr = tf_where(zeros_loc, tf.zeros_like(self.flow_pred_tnsr), self.flow_pred_tnsr)\n            #\n            #     self.y_hat_val_tnsr = [sparse_flow_pred_tnsr, self.metric_op]\n\n        self.y_hat_test_tnsr = [self.flow_pred_tnsr, self.flow_pyr_tnsr]\n\n    ###\n    # Sample mgmt\n    ###\n    def adapt_x(self, x):\n        """"""Preprocess the input samples to adapt them to the network\'s requirements\n        Here, x, is the actual data, not the x TF tensor.\n        Args:\n            x: input samples in list[(2,H,W,3)] or (N,2,H,W,3) np array form\n        Returns:\n            Samples ready to be given to the network (w. same shape as x)\n            Also, return adaptation info in (N,2,H,W,3) format\n        """"""\n        # Ensure we\'re dealing with RGB image pairs\n        assert (isinstance(x, np.ndarray) or isinstance(x, list))\n        if isinstance(x, np.ndarray):\n            assert (len(x.shape) == 5)\n            assert (x.shape[1] == 2 and x.shape[4] == 3)\n        else:\n            assert (len(x[0].shape) == 4)\n            assert (x[0].shape[0] == 2 or x[0].shape[3] == 3)\n\n        # Bring image range from 0..255 to 0..1 and use floats (also, list[(2,H,W,3)] -> (batch_size,2,H,W,3))\n        if self.opts[\'use_mixed_precision\'] is True:\n            x_adapt = np.array(x, dtype=np.float16) if isinstance(x, list) else x.astype(np.float16)\n        else:\n            x_adapt = np.array(x, dtype=np.float32) if isinstance(x, list) else x.astype(np.float32)\n        x_adapt /= 255.\n\n        # Make sure the image dimensions are multiples of 2**pyramid_levels, pad them if they\'re not\n        _, pad_h = divmod(x_adapt.shape[2], 2**self.opts[\'pyr_lvls\'])\n        if pad_h != 0:\n            pad_h = 2 ** self.opts[\'pyr_lvls\'] - pad_h\n        _, pad_w = divmod(x_adapt.shape[3], 2**self.opts[\'pyr_lvls\'])\n        if pad_w != 0:\n            pad_w = 2 ** self.opts[\'pyr_lvls\'] - pad_w\n        x_adapt_info = None\n        if pad_h != 0 or pad_w != 0:\n            padding = [(0, 0), (0, 0), (0, pad_h), (0, pad_w), (0, 0)]\n            x_adapt_info = x_adapt.shape  # Save original shape\n            x_adapt = np.pad(x_adapt, padding, mode=\'constant\', constant_values=0.)\n\n        return x_adapt, x_adapt_info\n\n    def adapt_y(self, y):\n        """"""Preprocess the labels to adapt them to the loss computation requirements of the network\n        Here, y, is the actual data, not the y TF tensor.\n        Args:\n            y: labels in list[(H,W,2)] or (N,H,W,2) np array form\n        Returns:\n            Labels ready to be used by the network\'s loss function (w. same shape as y)\n            Also, return adaptation info in (N,H,W,2) format\n        """"""\n        # Ensure we\'re dealing with u,v flows\n        assert (isinstance(y, np.ndarray) or isinstance(y, list))\n        if isinstance(y, np.ndarray):\n            assert (len(y.shape) == 4)\n            assert (y.shape[3] == 2)\n        else:\n            assert (len(y[0].shape) == 3)\n            assert (y[0].shape[2] == 2)\n\n        y_adapt = np.array(y, dtype=np.float32) if isinstance(y, list) else y  # list[(H,W,2)] -> (batch_size,H,W,2)\n\n        # Make sure the flow dimensions are multiples of 2**pyramid_levels, pad them if they\'re not\n        _, pad_h = divmod(y.shape[1], 2**self.opts[\'pyr_lvls\'])\n        if pad_h != 0:\n            pad_h = 2 ** self.opts[\'pyr_lvls\'] - pad_h\n        _, pad_w = divmod(y.shape[2], 2**self.opts[\'pyr_lvls\'])\n        if pad_w != 0:\n            pad_w = 2 ** self.opts[\'pyr_lvls\'] - pad_w\n        y_adapt_info = None\n        if pad_h != 0 or pad_w != 0:\n            padding = [(0, 0), (0, pad_h), (0, pad_w), (0, 0)]\n            y_adapt_info = y_adapt.shape  # Save original shape\n            y_adapt = np.pad(y_adapt, padding, mode=\'constant\', constant_values=0.)\n\n        # if y_adapt_info is not None and not y_adapt_info in self.adapt_infos: self.adapt_infos.append(y_adapt_info)\n        # if not y.shape in self.unique_y_shapes: self.unique_y_shapes.append(y.shape)\n\n        return y_adapt, y_adapt_info\n\n    def postproc_y_hat_test(self, y_hat, adapt_info=None):\n        """"""Postprocess the results coming from the network during the test mode.\n        Here, y_hat, is the actual data, not the y_hat TF tensor. Override as necessary.\n        Args:\n            y_hat: predictions, see set_output_tnsrs() for details\n            adapt_info: adaptation information in (N,H,W,2) format\n        Returns:\n            Postprocessed labels\n        """"""\n        assert (isinstance(y_hat, list) and len(y_hat) == 2)\n\n        # Have the samples been padded to fit the network\'s requirements? If so, crop flows back to original size.\n        pred_flows = y_hat[0]\n        if adapt_info is not None:\n            pred_flows = pred_flows[:, 0:adapt_info[1], 0:adapt_info[2], :]\n\n        # Individuate flows of the flow pyramid (at this point, they are still batched)\n        pyramids = y_hat[1]\n        pred_flows_pyramid = []\n        for idx in range(len(pred_flows)):\n            pyramid = []\n            for lvl in range(self.opts[\'pyr_lvls\'] - self.opts[\'flow_pred_lvl\'] + 1):\n                pyramid.append(pyramids[lvl][idx])\n            pred_flows_pyramid.append(pyramid)\n\n        return pred_flows, pred_flows_pyramid\n\n    def postproc_y_hat_train(self, y_hat, adapt_info=None):\n        """"""Postprocess the results coming from the network during training.\n        Here, y_hat, is the actual data, not the y_hat TF tensor. Override as necessary.\n        Args:\n            y_hat: losses and metrics, see set_output_tnsrs() for details\n            adapt_info: adaptation information in (N,H,W,2) format\n        Returns:\n            Batch loss and metric\n        """"""\n        assert (isinstance(y_hat, list) and len(y_hat) == 3)\n\n        return y_hat[0], y_hat[1]\n\n    def postproc_y_hat_val(self, y_hat, adapt_info=None):\n        """"""Postprocess the results coming from the network during validation.\n        Here, y_hat, is the actual data, not the y_hat TF tensor. Override as necessary.\n        Args:\n            y_hat: batch loss and metric, or predicted flows and metrics, see set_output_tnsrs() for details\n            adapt_info: adaptation information in (N,H,W,2) format\n        Returns:\n            Either, batch loss and metric\n            Or, predicted flows and metrics\n        """"""\n        if self.mode in [\'train_noval\', \'train_with_val\']:\n            # In online evaluation mode, we only care about the average loss and metric for the batch:\n            assert (isinstance(y_hat, list) and len(y_hat) == 2)\n            return y_hat[0], y_hat[1]\n\n        if self.mode in [\'val\', \'val_notrain\']:\n            # Have the samples been padded to fit the network\'s requirements? If so, crop flows back to original size.\n            pred_flows = y_hat[0]\n            if adapt_info is not None:\n                pred_flows = pred_flows[:, 0:adapt_info[1], 0:adapt_info[2], :]\n            return pred_flows, y_hat[1]\n\n    ###\n    # Training  helpers\n    ###\n    def setup_loss_ops(self):\n        """"""Setup loss computations. See pwcnet_loss() function for unregularized loss implementation details.\n        """"""\n        # Setup unregularized loss\n        loss_unreg = pwcnet_loss(self.y_tnsr, self.flow_pyr_tnsr, self.opts)\n\n        # Add regularization term\n        if self.opts[\'gamma\'] == 0.:\n            self.loss_op = loss_unreg\n        else:\n            loss_reg = self.opts[\'gamma\'] * tf.reduce_sum([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n            self.loss_op = loss_unreg + loss_reg\n\n    def setup_optim_op(self):\n        """"""Select the Adam optimizer, define the optimization process.\n        """"""\n        # Instantiate optimizer\n        # see https://stackoverflow.com/questions/42064941/tensorflow-float16-support-is-broken\n        # for float32 epsilon=1e-08, for float16 use epsilon=1e-4\n        epsilon = 1e-08 if self.opts[\'use_mixed_precision\'] is False else 1e-4\n        if self.opts[\'loss_fn\'] == \'loss_multiscale\':\n            self.optim = tf.train.AdamOptimizer(self.lr, epsilon=epsilon)\n        else:\n            self.optim = tf.train.ProximalGradientDescentOptimizer(self.lr)\n\n        if self.opts[\'use_mixed_precision\'] is True:\n            # Choose a loss scale manager which decides how to pick the right loss scale throughout the training process.\n            loss_scale_mgr = FixedLossScaleManager(self.opts[\'loss_scaler\'])\n\n            # Wrap the original optimizer in a LossScaleOptimizer\n            self.optim = LossScaleOptimizer(self.optim, loss_scale_mgr)\n\n            # Let minimize() take care of both computing the gradients and applying them to the model variables\n            self.optim_op = self.optim.minimize(self.loss_op, self.g_step_op, tf.trainable_variables())\n        else:\n            # Let minimize() take care of both computing the gradients and applying them to the model variables\n            self.optim_op = self.optim.minimize(self.loss_op, self.g_step_op, tf.trainable_variables())\n\n    def config_train_ops(self):\n        """"""Configure training ops.\n        Called by the base class when building the TF graph to setup all the training ops, including:\n            - setting up loss computations,\n            - setting up metrics computations,\n            - creating a learning rate training schedule,\n            - selecting an optimizer,\n            - creating lists of output tensors.\n        """"""\n        assert (self.opts[\'train_mode\'] in [\'train\', \'fine-tune\'])\n        if self.opts[\'verbose\']:\n            print(""Configuring training ops..."")\n\n        # Setup loss computations\n        self.setup_loss_ops()\n\n        # Setup metrics computations\n        self.setup_metrics_ops()\n\n        # Setup a learning rate training schedule\n        self.setup_lr_sched()\n\n        # Setup optimizer computations\n        self.setup_optim_op()\n\n        if self.opts[\'verbose\']:\n            print(""... training ops configured."")\n\n    def config_loggers(self):\n        """"""Configure train logger and, optionally, val logger. Here add a logger for test images, if requested.\n        """"""\n        super().config_loggers()\n        if self.opts[\'tb_test_imgs\'] is True:\n            self.tb_test = OptFlowTBLogger(self.opts[\'ckpt_dir\'], \'test\')\n\n    def train(self):\n        """"""Training loop\n        """"""\n        with self.graph.as_default():\n            # Reset step counter\n            if self.opts[\'train_mode\'] == \'fine-tune\':\n                step = 1\n                self.sess.run(self.g_step_op.assign(0))\n                if self.opts[\'verbose\']:\n                    print(""Start finetuning..."")\n            else:\n                if self.last_ckpt is not None:\n                    step = self.g_step_op.eval(session=self.sess) + 1\n                    if self.opts[\'verbose\']:\n                        print(f""Resume training from step {step}..."")\n                else:\n                    step = 1\n                    if self.opts[\'verbose\']:\n                        print(""Start training from scratch..."")\n\n            # Get batch sizes\n            batch_size = self.opts[\'batch_size\']\n            val_batch_size = self.opts[\'val_batch_size\']\n            if self.mode == \'train_noval\':\n                warnings.warn(""Setting val_batch_size=0 because dataset is in \'train_noval\' mode"")\n                val_batch_size = 0\n            if val_batch_size == -1:\n                val_batch_size = self.ds.val_size\n\n            # Init batch progress trackers\n            train_loss, train_epe, duration = [], [], []\n            ranking_value = 0\n\n            # Only load Tensorboard validation/test images once\n            if self.opts[\'tb_val_imgs\'] is not None:\n                tb_val_loaded = False\n            if self.opts[\'tb_test_imgs\'] is not None:\n                tb_test_loaded = False\n\n            # Use feed_dict from np or with tf.data.Dataset?\n            if self.opts[\'use_tf_data\'] is True:\n                # Create tf.data.Dataset managers\n                train_tf_ds = self.ds.get_tf_ds(batch_size, self.num_gpus, split=\'train\', sess=self.sess)\n                val_tf_ds = self.ds.get_tf_ds(batch_size, self.num_gpus, split=\'val\', sess=self.sess)\n\n                # Ops for initializing the two different iterators\n                train_next_batch = train_tf_ds.make_one_shot_iterator().get_next()\n                val_next_batch = val_tf_ds.make_one_shot_iterator().get_next()\n\n            while step < self.opts[\'max_steps\'] + 1:\n\n                # Get a batch of samples and make them conform to the network\'s requirements\n                # x: [batch_size*num_gpus,2,H,W,3] uint8 y: [batch_size*num_gpus,H,W,2] float32\n                # x_adapt: [batch_size,2,H,W,3] float32 y_adapt: [batch_size,H,W,2] float32\n                if self.opts[\'use_tf_data\'] is True:\n                    x, y, _ = self.sess.run(train_next_batch)\n                else:\n                    x, y, _ = self.ds.next_batch(batch_size * self.num_gpus, split=\'train\')\n                x_adapt, _ = self.adapt_x(x)\n                y_adapt, _ = self.adapt_y(y)\n\n                # Run the samples through the network (loss, error rate, and optim ops (backprop))\n                feed_dict = {self.x_tnsr: x_adapt, self.y_tnsr: y_adapt}\n                start_time = time.time()\n                y_hat = self.sess.run(self.y_hat_train_tnsr, feed_dict=feed_dict)\n                duration.append(time.time() - start_time)\n                loss, epe = self.postproc_y_hat_train(y_hat)  # y_hat: [107.0802, 5.8556495, None]\n                # if self.num_gpus == 1: # Single-GPU case\n                # else: # Multi-CPU case\n\n                train_loss.append(loss), train_epe.append(epe)\n\n                # Show training progress\n                if step % self.opts[\'display_step\'] == 0:\n                    # Send results to tensorboard\n                    loss, epe = np.mean(train_loss), np.mean(train_epe)\n                    ranking_value = epe\n                    self.tb_train.log_scalar(""losses/loss"", loss, step)\n                    self.tb_train.log_scalar(""metrics/epe"", epe, step)\n                    lr = self.lr.eval(session=self.sess)\n                    self.tb_train.log_scalar(""optim/lr"", lr, step)\n\n                    # Print results, if requested\n                    if self.opts[\'verbose\']:\n                        sec_per_step = np.mean(duration)\n                        samples_per_step = batch_size * self.num_gpus\n                        samples_per_sec = samples_per_step / sec_per_step\n                        eta = round((self.opts[\'max_steps\'] - step) * sec_per_step)\n                        ts = time.strftime(""%Y-%m-%d %H:%M:%S"")\n                        status = f""{ts} Iter {self.g_step_op.eval(session=self.sess)}"" \\\n                                 f"" [Train]: loss={loss:.2f}, epe={epe:.2f}, lr={lr:.6f},"" \\\n                                 f"" samples/sec={samples_per_sec:.1f}, sec/step={sec_per_step:.3f},"" \\\n                                 f"" eta={datetime.timedelta(seconds=eta)}""\n                        print(status)\n\n                    # Reset batch progress trackers\n                    train_loss, train_epe, duration = [], [], []\n\n                # Show progress on validation ds, if requested\n                if val_batch_size > 0 and step % self.opts[\'val_step\'] == 0:\n\n                    val_loss, val_epe = [], []\n                    rounds, _ = divmod(val_batch_size, batch_size * self.num_gpus)\n                    for _round in range(rounds):\n                        if self.opts[\'use_tf_data\'] is True:\n                            x, y, _, _ = self.sess.run(val_next_batch)\n                        else:\n                            # Get a batch of val samples and make them conform to the network\'s requirements\n                            x, y, _ = self.ds.next_batch(batch_size * self.num_gpus, split=\'val\')\n                            # x: [batch_size * self.num_gpus,2,H,W,3] uint8 y: [batch_size,H,W,2] float32\n                        x_adapt, _ = self.adapt_x(x)\n                        y_adapt, _ = self.adapt_y(y)\n                        # x_adapt: [batch_size * self.num_gpus,2,H,W,3] float32 y_adapt: [batch_size,H,W,2] float32\n\n                        # Run the val samples through the network (loss and error rate ops)\n                        feed_dict = {self.x_tnsr: x_adapt, self.y_tnsr: y_adapt}\n                        y_hat = self.sess.run(self.y_hat_val_tnsr, feed_dict=feed_dict)\n                        loss, epe = self.postproc_y_hat_val(y_hat)\n                        val_loss.append(loss), val_epe.append(epe)\n\n                    # Send the results to tensorboard\n                    loss, epe = np.mean(val_loss), np.mean(val_epe)\n                    ranking_value = epe\n                    self.tb_val.log_scalar(""losses/loss"", loss, step)\n                    self.tb_val.log_scalar(""metrics/epe"", epe, step)\n\n                    # Print results, if requested\n                    if self.opts[\'verbose\']:\n                        ts = time.strftime(""%Y-%m-%d %H:%M:%S"")\n                        status = f""{ts} Iter {self.g_step_op.eval(session=self.sess)} [Val]: loss={loss:.2f}, epe={epe:.2f}""\n                        print(status)\n\n                # Save a checkpoint every snapshot_step\n                if step % self.opts[\'snapshot_step\'] == 0 or step == self.opts[\'max_steps\']:\n\n                    # Log evolution of test images to Tensorboard, if requested\n                    if self.opts[\'tb_test_imgs\'] is not None:\n                        # Get a batch of test samples and make them conform to the network\'s requirements\n                        if tb_test_loaded is False:\n                            x_tb_test, IDs_tb_test = self.ds.get_samples(\n                                batch_size * self.num_gpus, split=\'test\', simple_IDs=True)\n                            x_tb_test_adapt, _ = self.adapt_x(x_tb_test)\n                            # IDs_tb_test = self.ds.simplify_IDs(x_IDs)\n                            tb_test_loaded = True\n\n                        # Run the test samples through the network\n                        feed_dict = {self.x_tnsr: x_tb_test_adapt}\n                        y_hat = self.sess.run(self.y_hat_test_tnsr, feed_dict=feed_dict)\n                        pred_flows, pred_flows_pyr = self.postproc_y_hat_test(y_hat)\n\n                        # Only show batch_size results, no matter what the GPU count is\n                        pred_flows, pred_flows_pyr = pred_flows[0:batch_size], pred_flows_pyr[0:batch_size]\n\n                        # Send the results to tensorboard\n                        if self.opts[\'tb_test_imgs\'] == \'top_flow\':\n                            self.tb_test.log_imgs_w_flows(\'test/{}_flows\', x_tb_test, None, 0, pred_flows,\n                                                          None, step, IDs_tb_test)\n                        else:\n                            self.tb_test.log_imgs_w_flows(\'test/{}_flows_pyr\', x_tb_test, pred_flows_pyr,\n                                                          self.opts[\'pyr_lvls\'] - self.opts[\'flow_pred_lvl\'], pred_flows,\n                                                          None, step, IDs_tb_test)\n\n                    # Log evolution of val images, if requested\n                    if self.opts[\'tb_val_imgs\'] is not None:\n                        # Get a batch of val samples and make them conform to the network\'s requirements\n                        if tb_val_loaded is False:\n                            x_tb_val, y_tb_val, IDs_tb_val = self.ds.get_samples(\n                                batch_size * self.num_gpus, split=\'val\', simple_IDs=True)\n                            x_tb_val_adapt, _ = self.adapt_x(x_tb_val)\n                            # IDs_tb_val = self.ds.simplify_IDs(x_IDs)\n                            tb_val_loaded = True\n\n                        # Run the val samples through the network (top flow and pyramid)\n                        feed_dict = {self.x_tnsr: x_tb_val_adapt}\n                        y_hat = self.sess.run(self.y_hat_test_tnsr, feed_dict=feed_dict)\n                        pred_flows, pred_flows_pyr = self.postproc_y_hat_test(y_hat)\n\n                        # Only show batch_size results, no matter what the GPU count is\n                        x_tb_val, y_tb_val = x_tb_val[0:batch_size], y_tb_val[0:batch_size]\n                        IDs_tb_val = IDs_tb_val[0:batch_size]\n                        pred_flows, pred_flows_pyr = pred_flows[0:batch_size], pred_flows_pyr[0:batch_size]\n\n                        # Send the results to tensorboard\n                        if self.opts[\'tb_val_imgs\'] == \'top_flow\':\n                            self.tb_val.log_imgs_w_flows(\'val/{}_flows\', x_tb_val, None, 0, pred_flows,\n                                                         y_tb_val, step, IDs_tb_val)\n                        else:\n                            self.tb_val.log_imgs_w_flows(\'val/{}_flows_pyr\', x_tb_val[0:batch_size], pred_flows_pyr,\n                                                         self.opts[\'pyr_lvls\'] - self.opts[\'flow_pred_lvl\'], pred_flows,\n                                                         y_tb_val, step, IDs_tb_val)\n\n                    # Save model\n                    self.save_ckpt(ranking_value)\n\n                step += 1\n\n            if self.opts[\'verbose\']:\n                print(""... done training."")\n\n    ###\n    # Evaluation helpers\n    ###\n    def setup_metrics_ops(self):\n        """"""Setup metrics computations. Use the endpoint error metric to track progress.\n        Note that, if the label flows come back from the network padded, it isn\'t a fair assessment of the performance\n        of the model if we also measure the EPE in the padded area. This area is to be cropped out before returning\n        the predicted flows to the caller, so exclude that area when computing the performance metric.\n        """"""\n        # Have the samples been padded to the nn\'s requirements? If so, crop flows back to original size.\n        y_tnsr, flow_pred_tnsr = self.y_tnsr, self.flow_pred_tnsr\n        if self.opts[\'adapt_info\'] is not None:\n            y_tnsr = y_tnsr[:, 0:self.opts[\'adapt_info\'][1], 0:self.opts[\'adapt_info\'][2], :]\n            flow_pred_tnsr = flow_pred_tnsr[:, 0:self.opts[\'adapt_info\'][1], 0:self.opts[\'adapt_info\'][2], :]\n\n        if self.opts[\'sparse_gt_flow\'] is True:\n            # Find the location of the zerod-out flows in the gt\n            zeros_loc = tf.logical_and(tf.equal(y_tnsr[:, :, :, 0], 0.0), tf.equal(y_tnsr[:, :, :, 1], 0.0))\n            zeros_loc = tf.expand_dims(zeros_loc, -1)\n\n            # Zero out flow predictions at the same location so we only compute the EPE at the sparse flow points\n            flow_pred_tnsr = tf_where(zeros_loc, tf.zeros_like(flow_pred_tnsr), flow_pred_tnsr)\n\n        if self.mode in [\'train_noval\', \'train_with_val\']:\n            # In online evaluation mode, we only care about the average loss and metric for the batch:\n            self.metric_op = tf.reduce_mean(tf.norm(y_tnsr - flow_pred_tnsr, ord=2, axis=3))\n\n        if self.mode in [\'val\', \'val_notrain\']:\n            # In offline evaluation mode, we actually care about each individual prediction and metric -> axis=(1, 2)\n            self.metric_op = tf.reduce_mean(tf.norm(y_tnsr - flow_pred_tnsr, ord=2, axis=3), axis=(1, 2))\n\n    def eval(self, metric_name=None, save_preds=False):\n        """"""Evaluation loop. Test the trained model on the validation split of the dataset.\n        Args:\n            save_preds: if True, the predictions are saved to disk\n        Returns:\n            Aaverage score for the entire dataset, a panda df with individual scores for further error analysis\n        """"""\n        with self.graph.as_default():\n            # Use feed_dict from np or with tf.data.Dataset?\n            batch_size = self.opts[\'batch_size\']\n            if self.opts[\'use_tf_data\'] is True:\n                # Create tf.data.Dataset manager\n                tf_ds = self.ds.get_tf_ds(batch_size=batch_size, split=\'val\', sess=self.sess)\n\n                # Ops for initializing the iterator\n                next_batch = tf_ds.make_one_shot_iterator().get_next()\n\n            # Store results in a dataframe\n            if metric_name is None:\n                metric_name = \'Score\'\n            df = pd.DataFrame(columns=[\'ID\', metric_name, \'Duration\', \'Avg_Flow_Mag\', \'Max_Flow_Mag\'])\n\n            # Chunk dataset\n            rounds, rounds_left = divmod(self.ds.val_size, batch_size)\n            if rounds_left:\n                rounds += 1\n\n            # Loop through samples and track their model performance\n            desc = f\'Measuring {metric_name} and saving preds\' if save_preds else f\'Measuring {metric_name}\'\n            idx = 0\n            for _round in trange(rounds, ascii=True, ncols=100, desc=desc):\n\n                # Fetch and adapt sample\n                if self.opts[\'use_tf_data\'] is True:\n                    x, y, y_hat_paths, IDs = self.sess.run(next_batch)\n                    y_hat_paths = [y_hat_path.decode() for y_hat_path in y_hat_paths]\n                    IDs = [ID.decode() for ID in IDs]\n                else:\n                    # Get a batch of samples and make them conform to the network\'s requirements\n                    x, y, y_hat_paths, IDs = self.ds.next_batch(batch_size, split=\'val_with_pred_paths\')\n                    # x: [batch_size * self.num_gpus,2,H,W,3] uint8 y: [batch_size,H,W,2] float32\n\n                x_adapt, _ = self.adapt_x(x)\n                y_adapt, y_adapt_info = self.adapt_y(y)\n                # x_adapt: [batch_size * self.num_gpus,2,H,W,3] float32 y_adapt: [batch_size,H,W,2] float32\n\n                # Run the sample through the network (metric op)\n                feed_dict = {self.x_tnsr: x_adapt, self.y_tnsr: y_adapt}\n                start_time = time.time()\n                y_hat = self.sess.run(self.y_hat_val_tnsr, feed_dict=feed_dict)\n                duration = time.time() - start_time\n                y_hats, metrics = self.postproc_y_hat_val(y_hat, y_adapt_info)\n\n                # Save the individual results in df\n                duration /= batch_size\n                for y_hat, metric, y_hat_path, ID in zip(y_hats, metrics, y_hat_paths, IDs):\n                    _, flow_mag_avg, flow_mag_max = flow_mag_stats(y_hat)\n                    df.loc[idx] = (ID, metric, duration, flow_mag_avg, flow_mag_max)\n                    if save_preds:\n                        flow_write(y_hat, y_hat_path)\n                        info=f""{metric_name}={metric:.2f}""\n                        flow_write_as_png(y_hat, y_hat_path.replace(\'.flo\', \'.png\'), info=info)\n                    idx += 1\n\n            # Compute stats\n            avg_metric, avg_duration = df.loc[:, metric_name].mean(), df.loc[:, \'Duration\'].mean()\n\n        # print(self.unique_y_shapes)\n        return avg_metric, avg_duration, df\n\n    ###\n    # Inference helpers\n    ###\n    def predict(self, return_preds=False, save_preds=True):\n        """"""Inference loop. Run the trained model on the test split of the dataset.\n        The data samples are provided by the OpticalFlowDataset object associated with this ModelPWCNet instance.\n        To predict flows for image pairs not provided by such object, use predict_from_img_pairs() instead.\n        Args:\n            return_preds: if True, the predictions are returned to the caller in list([2, H, W, 3]) format.\n            save_preds: if True, the predictions are saved to disk in .flo and .png format\n        Returns:\n            if return_preds is True, the predictions and their IDs are returned (might require a lot of RAM...)\n            if return_preds is False, return None\n        """"""\n        with self.graph.as_default():\n            # Use feed_dict from np or with tf.data.Dataset?\n            batch_size = self.opts[\'batch_size\']\n            if self.opts[\'use_tf_data\'] is True:\n                # Create tf.data.Dataset manager\n                tf_ds = self.ds.get_tf_ds(batch_size=batch_size, split=\'test\', sess=self.sess)\n\n                # Ops for initializing the iterator\n                next_batch = tf_ds.make_one_shot_iterator().get_next()\n\n            # Chunk dataset\n            rounds, rounds_left = divmod(self.ds.tst_size, batch_size)\n            if rounds_left:\n                rounds += 1\n\n            # Loop through input samples and run inference on them\n            if return_preds is True:\n                preds, ids = [], []\n            desc = f\'Predicting flows and saving preds\' if save_preds else f\'Predicting flows\'\n            for _round in trange(rounds, ascii=True, ncols=100, desc=desc):\n\n                # Fetch and adapt sample\n                if self.opts[\'use_tf_data\'] is True:\n                    x, y_hat_paths, IDs = self.sess.run(next_batch)\n                    y_hat_paths = [y_hat_path.decode() for y_hat_path in y_hat_paths]\n                    IDs = [ID.decode() for ID in IDs]\n                else:\n                    # Get a batch of samples and make them conform to the network\'s requirements\n                    x, y_hat_paths, IDs = self.ds.next_batch(batch_size, split=\'test_with_pred_paths\')\n                    # x: [batch_size,2,H,W,3] uint8; x_adapt: [batch_size,2,H,W,3] float32\n\n                x_adapt, x_adapt_info = self.adapt_x(x)\n                if x_adapt_info is not None:\n                    y_adapt_info = (x_adapt_info[0], x_adapt_info[2], x_adapt_info[3], 2)\n                else:\n                    y_adapt_info = None\n\n                # Run the sample through the network\n                feed_dict = {self.x_tnsr: x_adapt}\n                y_hat = self.sess.run(self.y_hat_test_tnsr, feed_dict=feed_dict)\n                y_hats, _ = self.postproc_y_hat_test(y_hat, y_adapt_info)\n\n                # Save the predicted flows to disk, if requested\n                for y_hat, y_hat_path, ID in zip(y_hats, y_hat_paths, IDs):\n                    if return_preds is True:\n                        preds.append(y_hat)\n                        ids.append(ID)\n                    if save_preds is True:\n                        flow_write(y_hat, y_hat_path)\n                        flow_write_as_png(y_hat, y_hat_path.replace(\'.flo\', \'.png\'))\n\n        if return_preds is True:\n            return preds[0:self.ds.tst_size], ids[0:self.ds.tst_size]\n        else:\n            return None\n\n    def predict_from_img_pairs(self, img_pairs, batch_size=1, verbose=False):\n        """"""Inference loop. Run inference on a list of image pairs.\n        Args:\n            img_pairs: list of image pairs/tuples in list((img_1, img_2),...,(img_n, img_nplusone)) format.\n            batch_size: size of the batch to process (all images must have the same dimension, if batch_size>1)\n            verbose: if True, show progress bar\n        Returns:\n            Predicted flows in list format\n        """"""\n        with self.graph.as_default():\n            # Chunk image pair list\n            batch_size = self.opts[\'batch_size\']\n            test_size = len(img_pairs)\n            rounds, rounds_left = divmod(test_size, batch_size)\n            if rounds_left:\n                rounds += 1\n\n            # Loop through input samples and run inference on them\n            preds, test_ptr = [], 0\n            rng = trange(rounds, ascii=True, ncols=100, desc=\'Predicting flows\') if verbose else range(rounds)\n            for _round in rng:\n                # In batch mode, make sure to wrap around if there aren\'t enough input samples to process\n                if test_ptr + batch_size < test_size:\n                    new_ptr = test_ptr + batch_size\n                    indices = list(range(test_ptr, test_ptr + batch_size))\n                else:\n                    new_ptr = (test_ptr + batch_size) % test_size\n                    indices = list(range(test_ptr, test_size)) + list(range(0, new_ptr))\n                test_ptr = new_ptr\n\n                # Repackage input image pairs as np.ndarray\n                x = np.array([img_pairs[idx] for idx in indices])\n\n                # Make input samples conform to the network\'s requirements\n                # x: [batch_size,2,H,W,3] uint8; x_adapt: [batch_size,2,H,W,3] float32\n                x_adapt, x_adapt_info = self.adapt_x(x)\n                if x_adapt_info is not None:\n                    y_adapt_info = (x_adapt_info[0], x_adapt_info[2], x_adapt_info[3], 2)\n                else:\n                    y_adapt_info = None\n\n                # Run the adapted samples through the network\n                feed_dict = {self.x_tnsr: x_adapt}\n                y_hat = self.sess.run(self.y_hat_test_tnsr, feed_dict=feed_dict)\n                y_hats, _ = self.postproc_y_hat_test(y_hat, y_adapt_info)\n\n                # Return flat list of predicted labels\n                for y_hat in y_hats:\n                    preds.append(y_hat)\n\n        return preds[0:test_size]\n\n    ###\n    # PWC-Net pyramid helpers\n    ###\n    def extract_features(self, x_tnsr, name=\'featpyr\'):\n        """"""Extract pyramid of features\n        Args:\n            x_tnsr: Input tensor (input pair of images in [batch_size, 2, H, W, 3] format)\n            name: Variable scope name\n        Returns:\n            c1, c2: Feature pyramids\n        Ref:\n            Per page 3 of paper, section ""Feature pyramid extractor,"" given two input images I1 and I2, we generate\n            L-level pyramids of feature representations, with the bottom (zeroth) level being the input images,\n            i.e., Ct<sup>0</sup> = It. To generate feature representation at the l-th layer, Ct<sup>l</sup>, we use\n            layers of convolutional filters to downsample the features at the (l\xe2\x88\x921)th pyramid level, Ct<sup>l-1</sup>,\n            by a factor of 2. From the first to the sixth levels, the number of feature channels are respectively\n            16, 32, 64, 96, 128, and 196. Also see page 15 of paper for a rendering of the network architecture.\n            Per page 15, individual images of the image pair are encoded using the same Siamese network. Each\n            convolution is followed by a leaky ReLU unit. The convolutional layer and the x2 downsampling layer at\n            each level is implemented using a single convolutional layer with a stride of 2.\n\n            Note that Figure 4 on page 15 differs from the PyTorch implementation in two ways:\n            - It\'s missing a convolution layer at the end of each conv block\n            - It shows a number of filters of 192 (instead of 196) at the end of the last conv block\n\n        Ref PyTorch code:\n            def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):\n                return nn.Sequential(\n                    nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                    padding=padding, dilation=dilation, bias=True), nn.LeakyReLU(0.1))\n            [...]\n            self.conv1a  = conv(3,   16, kernel_size=3, stride=2)\n            self.conv1aa = conv(16,  16, kernel_size=3, stride=1)\n            self.conv1b  = conv(16,  16, kernel_size=3, stride=1)\n\n            self.conv2a  = conv(16,  32, kernel_size=3, stride=2)\n            self.conv2aa = conv(32,  32, kernel_size=3, stride=1)\n            self.conv2b  = conv(32,  32, kernel_size=3, stride=1)\n\n            self.conv3a  = conv(32,  64, kernel_size=3, stride=2)\n            self.conv3aa = conv(64,  64, kernel_size=3, stride=1)\n            self.conv3b  = conv(64,  64, kernel_size=3, stride=1)\n\n            self.conv4a  = conv(64,  96, kernel_size=3, stride=2)\n            self.conv4aa = conv(96,  96, kernel_size=3, stride=1)\n            self.conv4b  = conv(96,  96, kernel_size=3, stride=1)\n\n            self.conv5a  = conv(96, 128, kernel_size=3, stride=2)\n            self.conv5aa = conv(128,128, kernel_size=3, stride=1)\n            self.conv5b  = conv(128,128, kernel_size=3, stride=1)\n\n            self.conv6aa = conv(128,196, kernel_size=3, stride=2)\n            self.conv6a  = conv(196,196, kernel_size=3, stride=1)\n            self.conv6b  = conv(196,196, kernel_size=3, stride=1)\n            [...]\n            c11 = self.conv1b(self.conv1aa(self.conv1a(im1))) # Higher-res\n            c21 = self.conv1b(self.conv1aa(self.conv1a(im2)))\n            c12 = self.conv2b(self.conv2aa(self.conv2a(c11)))\n            c22 = self.conv2b(self.conv2aa(self.conv2a(c21)))\n            c13 = self.conv3b(self.conv3aa(self.conv3a(c12)))\n            c23 = self.conv3b(self.conv3aa(self.conv3a(c22)))\n            c14 = self.conv4b(self.conv4aa(self.conv4a(c13)))\n            c24 = self.conv4b(self.conv4aa(self.conv4a(c23)))\n            c15 = self.conv5b(self.conv5aa(self.conv5a(c14)))\n            c25 = self.conv5b(self.conv5aa(self.conv5a(c24)))\n            c16 = self.conv6b(self.conv6a(self.conv6aa(c15)))\n            c26 = self.conv6b(self.conv6a(self.conv6aa(c25))) # Lower-res\n\n        Ref Caffee code:\n            https://github.com/NVlabs/PWC-Net/blob/438ca897ae77e08f419ddce5f0d7fa63b0a27a77/Caffe/model/train.prototxt#L314-L1141\n        """"""\n        assert(1 <= self.opts[\'pyr_lvls\'] <= 6)\n        if self.dbg:\n            print(f""Building feature pyramids (c11,c21) ... (c1{self.opts[\'pyr_lvls\']},c2{self.opts[\'pyr_lvls\']})"")\n        # Make the feature pyramids 1-based for better readability down the line\n        num_chann = [None, 16, 32, 64, 96, 128, 196]\n        c1, c2 = [None], [None]\n        init = tf.keras.initializers.he_normal()\n        with tf.variable_scope(name):\n            for pyr, x, reuse, name in zip([c1, c2], [x_tnsr[:, 0], x_tnsr[:, 1]], [None, True], [\'c1\', \'c2\']):\n                for lvl in range(1, self.opts[\'pyr_lvls\'] + 1):\n                    # tf.layers.conv2d(inputs, filters, kernel_size, strides=(1, 1), padding=\'valid\', ... , name, reuse)\n                    # reuse is set to True because we want to learn a single set of weights for the pyramid\n                    # kernel_initializer = \'he_normal\' or tf.keras.initializers.he_normal(seed=None)\n                    f = num_chann[lvl]\n                    x = tf.layers.conv2d(x, f, 3, 2, \'same\', kernel_initializer=init, name=f\'conv{lvl}a\', reuse=reuse)\n                    x = tf.nn.leaky_relu(x, alpha=0.1)  # , name=f\'relu{lvl+1}a\') # default alpha is 0.2 for TF\n                    x = tf.layers.conv2d(x, f, 3, 1, \'same\', kernel_initializer=init, name=f\'conv{lvl}aa\', reuse=reuse)\n                    x = tf.nn.leaky_relu(x, alpha=0.1)  # , name=f\'relu{lvl+1}aa\')\n                    x = tf.layers.conv2d(x, f, 3, 1, \'same\', kernel_initializer=init, name=f\'conv{lvl}b\', reuse=reuse)\n                    x = tf.nn.leaky_relu(x, alpha=0.1, name=f\'{name}{lvl}\')\n                    pyr.append(x)\n        return c1, c2\n\n    ###\n    # PWC-Net warping helpers\n    ###\n    def warp(self, c2, sc_up_flow, lvl, name=\'warp\'):\n        """"""Warp a level of Image1\'s feature pyramid using the upsampled flow at level+1 of Image2\'s pyramid.\n        Args:\n            c2: The level of the feature pyramid of Image2 to warp\n            sc_up_flow: Scaled and upsampled estimated optical flow (from Image1 to Image2) used for warping\n            lvl: Index of that level\n            name: Op scope name\n        Ref:\n            Per page 4 of paper, section ""Warping layer,"" at the l-th level, we warp features of the second image toward\n            the first image using the x2 upsampled flow from the l+1th level:\n                C1w<sup>l</sup>(x) = C2<sup>l</sup>(x + Up2(w<sup>l+1</sup>)(x))\n            where x is the pixel index and the upsampled flow Up2(w<sup>l+1</sup>) is set to be zero at the top level.\n            We use bilinear interpolation to implement the warping operation and compute the gradients to the input\n            CNN features and flow for backpropagation according to E. Ilg\'s FlowNet 2.0 paper.\n            For non-translational motion, warping can compensate for some geometric distortions and put image patches\n            at the right scale.\n\n            Per page 3 of paper, section ""3. Approach,"" the warping and cost volume layers have no learnable parameters\n            and, hence, reduce the model size.\n\n        Ref PyTorch code:\n            # warp an image/tensor (im2) back to im1, according to the optical flow\n            # x: [B, C, H, W] (im2)\n            # flo: [B, 2, H, W] flow\n            def warp(self, x, flo):\n\n                B, C, H, W = x.size()\n                # mesh grid\n                xx = torch.arange(0, W).view(1,-1).repeat(H,1)\n                yy = torch.arange(0, H).view(-1,1).repeat(1,W)\n                xx = xx.view(1,1,H,W).repeat(B,1,1,1)\n                yy = yy.view(1,1,H,W).repeat(B,1,1,1)\n                grid = torch.cat((xx,yy),1).float()\n\n                if x.is_cuda:\n                    grid = grid.cuda()\n                vgrid = Variable(grid) + flo\n\n                # scale grid to [-1,1]\n                vgrid[:,0,:,:] = 2.0*vgrid[:,0,:,:]/max(W-1,1)-1.0\n                vgrid[:,1,:,:] = 2.0*vgrid[:,1,:,:]/max(H-1,1)-1.0\n\n                vgrid = vgrid.permute(0,2,3,1)\n                output = nn.functional.grid_sample(x, vgrid)\n                mask = torch.autograd.Variable(torch.ones(x.size())).cuda()\n                mask = nn.functional.grid_sample(mask, vgrid)\n\n                mask[mask<0.9999] = 0\n                mask[mask>0] = 1\n\n                return output*mask\n            [...]\n            warp5 = self.warp(c25, up_flow6*0.625)\n            warp4 = self.warp(c24, up_flow5*1.25)\n            warp3 = self.warp(c23, up_flow4*2.5)\n            warp2 = self.warp(c22, up_flow3*5.0)\n\n        Ref TF documentation:\n            tf.contrib.image.dense_image_warp(image, flow, name=\'dense_image_warp\')\n            https://www.tensorflow.org/api_docs/python/tf/contrib/image/dense_image_warp\n            https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/image/python/kernel_tests/dense_image_warp_test.py\n\n        Other implementations:\n            https://github.com/bryanyzhu/deepOF/blob/master/flyingChairsWrapFlow.py\n            https://github.com/bryanyzhu/deepOF/blob/master/ucf101wrapFlow.py\n            https://github.com/rajat95/Optical-Flow-Warping-Tensorflow/blob/master/warp.py\n        """"""\n        op_name = f\'{name}{lvl}\'\n        if self.dbg:\n            msg = f\'Adding {op_name} with inputs {c2.op.name} and {sc_up_flow.op.name}\'\n            print(msg)\n        with tf.name_scope(name):\n            return dense_image_warp(c2, sc_up_flow, name=op_name)\n\n    def deconv(self, x, lvl, name=\'up_flow\'):\n        """"""Upsample, not using a bilinear filter, but rather learn the weights of a conv2d_transpose op filters.\n        Args:\n            x: Level features or flow to upsample\n            lvl: Index of that level\n            name: Op scope name\n        Ref PyTorch code:\n            def deconv(in_planes, out_planes, kernel_size=4, stride=2, padding=1):\n                return nn.ConvTranspose2d(in_planes, out_planes, kernel_size, stride, padding, bias=True)\n            [...]\n            self.deconv6 = deconv(2, 2, kernel_size=4, stride=2, padding=1)\n            self.upfeat6 = deconv(od+dd[4], 2, kernel_size=4, stride=2, padding=1)\n            ...\n            self.deconv5 = deconv(2, 2, kernel_size=4, stride=2, padding=1)\n            self.upfeat5 = deconv(od+dd[4], 2, kernel_size=4, stride=2, padding=1)\n            ...\n            self.deconv4 = deconv(2, 2, kernel_size=4, stride=2, padding=1)\n            self.upfeat4 = deconv(od+dd[4], 2, kernel_size=4, stride=2, padding=1)\n            ...\n            self.deconv3 = deconv(2, 2, kernel_size=4, stride=2, padding=1)\n            self.upfeat3 = deconv(od+dd[4], 2, kernel_size=4, stride=2, padding=1)\n            ...\n            self.deconv2 = deconv(2, 2, kernel_size=4, stride=2, padding=1)\n            [...]\n            up_flow6 = self.deconv6(flow6)\n            up_feat6 = self.upfeat6(x)\n            ...\n            up_flow5 = self.deconv5(flow5)\n            up_feat5 = self.upfeat5(x)\n            ...\n            up_flow4 = self.deconv4(flow4)\n            up_feat4 = self.upfeat4(x)\n            ...\n            up_flow3 = self.deconv3(flow3)\n            up_feat3 = self.upfeat3(x)\n        """"""\n        op_name = f\'{name}{lvl}\'\n        if self.dbg:\n            print(f\'Adding {op_name} with input {x.op.name}\')\n        with tf.variable_scope(\'upsample\'):\n            # tf.layers.conv2d_transpose(inputs, filters, kernel_size, strides=(1, 1), padding=\'valid\', ... , name)\n            return tf.layers.conv2d_transpose(x, 2, 4, 2, \'same\', name=op_name)\n\n    ###\n    # Cost Volume helpers\n    ###\n    def corr(self, c1, warp, lvl, name=\'corr\'):\n        """"""Build cost volume for associating a pixel from Image1 with its corresponding pixels in Image2.\n        Args:\n            c1: The level of the feature pyramid of Image1\n            warp: The warped level of the feature pyramid of image22\n            lvl: Index of that level\n            name: Op scope name\n        Ref:\n            Per page 3 of paper, section ""Cost Volume,"" a cost volume stores the data matching costs for associating\n            a pixel from Image1 with its corresponding pixels in Image2. Most traditional optical flow techniques build\n            the full cost volume at a single scale, which is both computationally expensive and memory intensive. By\n            contrast, PWC-Net constructs a partial cost volume at multiple pyramid levels.\n\n            The matching cost is implemented as the correlation between features of the first image and warped features\n            of the second image:\n                CV<sup>l</sup>(x1,x2) = (C1<sup>l</sup>(x1))<sup>T</sup> . Cw<sup>l</sup>(x2) / N\n            where where T is the transpose operator and N is the length of the column vector C1<sup>l</sup>(x1).\n            For an L-level pyramid, we only need to compute a partial cost volume with a limited search range of d\n            pixels. A one-pixel motion at the top level corresponds to 2**(L\xe2\x88\x921) pixels at the full resolution images.\n            Thus we can set d to be small, e.g. d=4. The dimension of the 3D cost volume is d**2 \xc3\x97 Hl \xc3\x97 Wl, where Hl\n            and Wl denote the height and width of the L-th pyramid level, respectively.\n\n            Per page 3 of paper, section ""3. Approach,"" the warping and cost volume layers have no learnable parameters\n            and, hence, reduce the model size.\n\n            Per page 5 of paper, section ""Implementation details,"" we use a search range of 4 pixels to compute the\n            cost volume at each level.\n\n        Ref PyTorch code:\n        from correlation_package.modules.corr import Correlation\n        self.corr = Correlation(pad_size=md, kernel_size=1, max_displacement=4, stride1=1, stride2=1, corr_multiply=1)\n        [...]\n        corr6 = self.corr(c16, c26)\n        corr6 = self.leakyRELU(corr6)\n        ...\n        corr5 = self.corr(c15, warp5)\n        corr5 = self.leakyRELU(corr5)\n        ...\n        corr4 = self.corr(c14, warp4)\n        corr4 = self.leakyRELU(corr4)\n        ...\n        corr3 = self.corr(c13, warp3)\n        corr3 = self.leakyRELU(corr3)\n        ...\n        corr2 = self.corr(c12, warp2)\n        corr2 = self.leakyRELU(corr2)\n        """"""\n        op_name = f\'corr{lvl}\'\n        if self.dbg:\n            print(f\'Adding {op_name} with inputs {c1.op.name} and {warp.op.name}\')\n        with tf.name_scope(name):\n            return cost_volume(c1, warp, self.opts[\'search_range\'], op_name)\n\n    ###\n    # Optical flow estimator helpers\n    ###\n    def predict_flow(self, corr, c1, up_flow, up_feat, lvl, name=\'predict_flow\'):\n        """"""Estimate optical flow.\n        Args:\n            corr: The cost volume at level lvl\n            c1: The level of the feature pyramid of Image1\n            up_flow: An upsampled version of the predicted flow from the previous level\n            up_feat: An upsampled version of the features that were used to generate the flow prediction\n            lvl: Index of the level\n            name: Op scope name\n        Args:\n            upfeat: The features used to generate the predicted flow\n            flow: The predicted flow\n        Ref:\n            Per page 4 of paper, section ""Optical flow estimator,"" the optical flow estimator is a multi-layer CNN. Its\n            input are the cost volume, features of the first image, and upsampled optical flow and its output is the\n            flow w<sup>l</sup> at the l-th level. The numbers of feature channels at each convolutional layers are\n            respectively 128, 128, 96, 64, and 32, which are kept fixed at all pyramid levels. The estimators at\n            different levels have their own parameters instead of sharing the same parameters. This estimation process\n            is repeated until the desired level, l0.\n\n            Per page 5 of paper, section ""Implementation details,"" we use a 7-level pyramid and set l0 to be 2, i.e.,\n            our model outputs a quarter resolution optical flow and uses bilinear interpolation to obtain the\n            full-resolution optical flow.\n\n            The estimator architecture can be enhanced with DenseNet connections. The inputs to every convolutional\n            layer are the output of and the input to its previous layer. DenseNet has more direct connections than\n            traditional layers and leads to significant improvement in image classification.\n\n            Note that we do not use DenseNet connections in this implementation because a) they increase the size of the\n            model, and, b) per page 7 of paper, section ""Optical flow estimator,"" removing the DenseNet connections\n            results in higher training error but lower validation errors when the model is trained on FlyingChairs\n            (that being said, after the model is fine-tuned on FlyingThings3D, DenseNet leads to lower errors).\n\n        Ref PyTorch code:\n            def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):\n                return nn.Sequential(\n                    nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                    padding=padding, dilation=dilation, bias=True),\n                nn.LeakyReLU(0.1))\n            def predict_flow(in_planes):\n                return nn.Conv2d(in_planes,2,kernel_size=3,stride=1,padding=1,bias=True)\n            [...]\n            nd = (2*md+1)**2\n            dd = np.cumsum([128,128,96,64,32])\n            od = nd\n            self.conv6_0 = conv(od,      128, kernel_size=3, stride=1)\n            self.conv6_1 = conv(od+dd[0],128, kernel_size=3, stride=1)\n            self.conv6_2 = conv(od+dd[1],96,  kernel_size=3, stride=1)\n            self.conv6_3 = conv(od+dd[2],64,  kernel_size=3, stride=1)\n            self.conv6_4 = conv(od+dd[3],32,  kernel_size=3, stride=1)\n            self.predict_flow6 = predict_flow(od+dd[4])\n            [...]\n            od = nd+128+4\n            self.conv5_0 = conv(od,      128, kernel_size=3, stride=1)\n            self.conv5_1 = conv(od+dd[0],128, kernel_size=3, stride=1)\n            self.conv5_2 = conv(od+dd[1],96,  kernel_size=3, stride=1)\n            self.conv5_3 = conv(od+dd[2],64,  kernel_size=3, stride=1)\n            self.conv5_4 = conv(od+dd[3],32,  kernel_size=3, stride=1)\n            self.predict_flow5 = predict_flow(od+dd[4])\n            [...]\n            od = nd+96+4\n            self.conv4_0 = conv(od,      128, kernel_size=3, stride=1)\n            self.conv4_1 = conv(od+dd[0],128, kernel_size=3, stride=1)\n            self.conv4_2 = conv(od+dd[1],96,  kernel_size=3, stride=1)\n            self.conv4_3 = conv(od+dd[2],64,  kernel_size=3, stride=1)\n            self.conv4_4 = conv(od+dd[3],32,  kernel_size=3, stride=1)\n            self.predict_flow4 = predict_flow(od+dd[4])\n            [...]\n            od = nd+64+4\n            self.conv3_0 = conv(od,      128, kernel_size=3, stride=1)\n            self.conv3_1 = conv(od+dd[0],128, kernel_size=3, stride=1)\n            self.conv3_2 = conv(od+dd[1],96,  kernel_size=3, stride=1)\n            self.conv3_3 = conv(od+dd[2],64,  kernel_size=3, stride=1)\n            self.conv3_4 = conv(od+dd[3],32,  kernel_size=3, stride=1)\n            self.predict_flow3 = predict_flow(od+dd[4])\n            [...]\n            od = nd+32+4\n            self.conv2_0 = conv(od,      128, kernel_size=3, stride=1)\n            self.conv2_1 = conv(od+dd[0],128, kernel_size=3, stride=1)\n            self.conv2_2 = conv(od+dd[1],96,  kernel_size=3, stride=1)\n            self.conv2_3 = conv(od+dd[2],64,  kernel_size=3, stride=1)\n            self.conv2_4 = conv(od+dd[3],32,  kernel_size=3, stride=1)\n            self.predict_flow2 = predict_flow(od+dd[4])\n            [...]\n            self.dc_conv1 = conv(od+dd[4], 128, kernel_size=3, stride=1, padding=1,  dilation=1)\n            self.dc_conv2 = conv(128,      128, kernel_size=3, stride=1, padding=2,  dilation=2)\n            self.dc_conv3 = conv(128,      128, kernel_size=3, stride=1, padding=4,  dilation=4)\n            self.dc_conv4 = conv(128,      96,  kernel_size=3, stride=1, padding=8,  dilation=8)\n            self.dc_conv5 = conv(96,       64,  kernel_size=3, stride=1, padding=16, dilation=16)\n            self.dc_conv6 = conv(64,       32,  kernel_size=3, stride=1, padding=1,  dilation=1)\n            self.dc_conv7 = predict_flow(32)\n            [...]\n            x = torch.cat((self.conv6_0(corr6), corr6),1)\n            x = torch.cat((self.conv6_1(x), x),1)\n            x = torch.cat((self.conv6_2(x), x),1)\n            x = torch.cat((self.conv6_3(x), x),1)\n            x = torch.cat((self.conv6_4(x), x),1)\n            flow6 = self.predict_flow6(x)\n            ...\n            x = torch.cat((corr5, c15, up_flow6, up_feat6), 1)\n            x = torch.cat((self.conv5_0(x), x),1)\n            x = torch.cat((self.conv5_1(x), x),1)\n            x = torch.cat((self.conv5_2(x), x),1)\n            x = torch.cat((self.conv5_3(x), x),1)\n            x = torch.cat((self.conv5_4(x), x),1)\n            flow5 = self.predict_flow5(x)\n            ...\n            x = torch.cat((corr4, c14, up_flow5, up_feat5), 1)\n            x = torch.cat((self.conv4_0(x), x),1)\n            x = torch.cat((self.conv4_1(x), x),1)\n            x = torch.cat((self.conv4_2(x), x),1)\n            x = torch.cat((self.conv4_3(x), x),1)\n            x = torch.cat((self.conv4_4(x), x),1)\n            flow4 = self.predict_flow4(x)\n            ...\n            x = torch.cat((corr3, c13, up_flow4, up_feat4), 1)\n            x = torch.cat((self.conv3_0(x), x),1)\n            x = torch.cat((self.conv3_1(x), x),1)\n            x = torch.cat((self.conv3_2(x), x),1)\n            x = torch.cat((self.conv3_3(x), x),1)\n            x = torch.cat((self.conv3_4(x), x),1)\n            flow3 = self.predict_flow3(x)\n            ...\n            x = torch.cat((corr2, c12, up_flow3, up_feat3), 1)\n            x = torch.cat((self.conv2_0(x), x),1)\n            x = torch.cat((self.conv2_1(x), x),1)\n            x = torch.cat((self.conv2_2(x), x),1)\n            x = torch.cat((self.conv2_3(x), x),1)\n            x = torch.cat((self.conv2_4(x), x),1)\n            flow2 = self.predict_flow2(x)\n        """"""\n        op_name = f\'flow{lvl}\'\n        init = tf.keras.initializers.he_normal()\n        with tf.variable_scope(name):\n            if c1 is None and up_flow is None and up_feat is None:\n                if self.dbg:\n                    print(f\'Adding {op_name} with input {corr.op.name}\')\n                x = corr\n            else:\n                if self.dbg:\n                    msg = f\'Adding {op_name} with inputs {corr.op.name}, {c1.op.name}, {up_flow.op.name}, {up_feat.op.name}\'\n                    print(msg)\n                x = tf.concat([corr, c1, up_flow, up_feat], axis=3)\n\n            conv = tf.layers.conv2d(x, 128, 3, 1, \'same\', kernel_initializer=init, name=f\'conv{lvl}_0\')\n            act = tf.nn.leaky_relu(conv, alpha=0.1)  # default alpha is 0.2 for TF\n            x = tf.concat([act, x], axis=3) if self.opts[\'use_dense_cx\'] else act\n\n            conv = tf.layers.conv2d(x, 128, 3, 1, \'same\', kernel_initializer=init, name=f\'conv{lvl}_1\')\n            act = tf.nn.leaky_relu(conv, alpha=0.1)\n            x = tf.concat([act, x], axis=3) if self.opts[\'use_dense_cx\'] else act\n\n            conv = tf.layers.conv2d(x, 96, 3, 1, \'same\', kernel_initializer=init, name=f\'conv{lvl}_2\')\n            act = tf.nn.leaky_relu(conv, alpha=0.1)\n            x = tf.concat([act, x], axis=3) if self.opts[\'use_dense_cx\'] else act\n\n            conv = tf.layers.conv2d(x, 64, 3, 1, \'same\', kernel_initializer=init, name=f\'conv{lvl}_3\')\n            act = tf.nn.leaky_relu(conv, alpha=0.1)\n            x = tf.concat([act, x], axis=3) if self.opts[\'use_dense_cx\'] else act\n\n            conv = tf.layers.conv2d(x, 32, 3, 1, \'same\', kernel_initializer=init, name=f\'conv{lvl}_4\')\n            act = tf.nn.leaky_relu(conv, alpha=0.1)  # will also be used as an input by the context network\n            upfeat = tf.concat([act, x], axis=3, name=f\'upfeat{lvl}\') if self.opts[\'use_dense_cx\'] else act\n\n            flow = tf.layers.conv2d(upfeat, 2, 3, 1, \'same\', name=op_name)\n\n            return upfeat, flow\n\n    ###\n    # PWC-Net context network helpers\n    ###\n    def refine_flow(self, feat, flow, lvl, name=\'ctxt\'):\n        """"""Post-ptrocess the estimated optical flow using a ""context"" nn.\n        Args:\n            feat: Features of the second-to-last layer from the optical flow estimator\n            flow: Estimated flow to refine\n            lvl: Index of the level\n            name: Op scope name\n        Ref:\n            Per page 4 of paper, section ""Context network,"" traditional flow methods often use contextual information\n            to post-process the flow. Thus we employ a sub-network, called the context network, to effectively enlarge\n            the receptive field size of each output unit at the desired pyramid level. It takes the estimated flow and\n            features of the second last layer from the optical flow estimator and outputs a refined flow.\n\n            The context network is a feed-forward CNN and its design is based on dilated convolutions. It consists of\n            7 convolutional layers. The spatial kernel for each convolutional layer is 3\xc3\x973. These layers have different\n            dilation constants. A convolutional layer with a dilation constant k means that an input unit to a filter\n            in the layer are k-unit apart from the other input units to the filter in the layer, both in vertical and\n            horizontal directions. Convolutional layers with large dilation constants enlarge the receptive field of\n            each output unit without incurring a large computational burden. From bottom to top, the dilation constants\n            are 1, 2, 4, 8, 16, 1, and 1.\n\n        Ref PyTorch code:\n            def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):\n                return nn.Sequential(\n                    nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                    padding=padding, dilation=dilation, bias=True),\n                nn.LeakyReLU(0.1))\n            def predict_flow(in_planes):\n                return nn.Conv2d(in_planes,2,kernel_size=3,stride=1,padding=1,bias=True)\n            [...]\n            self.dc_conv1 = conv(od+dd[4], 128, kernel_size=3, stride=1, padding=1,  dilation=1)\n            self.dc_conv2 = conv(128,      128, kernel_size=3, stride=1, padding=2,  dilation=2)\n            self.dc_conv3 = conv(128,      128, kernel_size=3, stride=1, padding=4,  dilation=4)\n            self.dc_conv4 = conv(128,      96,  kernel_size=3, stride=1, padding=8,  dilation=8)\n            self.dc_conv5 = conv(96,       64,  kernel_size=3, stride=1, padding=16, dilation=16)\n            self.dc_conv6 = conv(64,       32,  kernel_size=3, stride=1, padding=1,  dilation=1)\n            self.dc_conv7 = predict_flow(32)\n            [...]\n            x = torch.cat((corr2, c12, up_flow3, up_feat3), 1)\n            x = torch.cat((self.conv2_0(x), x),1)\n            x = torch.cat((self.conv2_1(x), x),1)\n            x = torch.cat((self.conv2_2(x), x),1)\n            x = torch.cat((self.conv2_3(x), x),1)\n            x = torch.cat((self.conv2_4(x), x),1)\n            flow2 = self.predict_flow2(x)\n            x = self.dc_conv4(self.dc_conv3(self.dc_conv2(self.dc_conv1(x))))\n            flow2 += self.dc_conv7(self.dc_conv6(self.dc_conv5(x)))\n        """"""\n        op_name = f\'refined_flow{lvl}\'\n        if self.dbg:\n            print(f\'Adding {op_name} sum of dc_convs_chain({feat.op.name}) with {flow.op.name}\')\n        init = tf.keras.initializers.he_normal()\n        with tf.variable_scope(name):\n            x = tf.layers.conv2d(feat, 128, 3, 1, \'same\', dilation_rate=1, kernel_initializer=init, name=f\'dc_conv{lvl}1\')\n            x = tf.nn.leaky_relu(x, alpha=0.1)  # default alpha is 0.2 for TF\n            x = tf.layers.conv2d(x, 128, 3, 1, \'same\', dilation_rate=2, kernel_initializer=init, name=f\'dc_conv{lvl}2\')\n            x = tf.nn.leaky_relu(x, alpha=0.1)\n            x = tf.layers.conv2d(x, 128, 3, 1, \'same\', dilation_rate=4, kernel_initializer=init, name=f\'dc_conv{lvl}3\')\n            x = tf.nn.leaky_relu(x, alpha=0.1)\n            x = tf.layers.conv2d(x, 96, 3, 1, \'same\', dilation_rate=8, kernel_initializer=init, name=f\'dc_conv{lvl}4\')\n            x = tf.nn.leaky_relu(x, alpha=0.1)\n            x = tf.layers.conv2d(x, 64, 3, 1, \'same\', dilation_rate=16, kernel_initializer=init, name=f\'dc_conv{lvl}5\')\n            x = tf.nn.leaky_relu(x, alpha=0.1)\n            x = tf.layers.conv2d(x, 32, 3, 1, \'same\', dilation_rate=1, kernel_initializer=init, name=f\'dc_conv{lvl}6\')\n            x = tf.nn.leaky_relu(x, alpha=0.1)\n            x = tf.layers.conv2d(x, 2, 3, 1, \'same\', dilation_rate=1, kernel_initializer=init, name=f\'dc_conv{lvl}7\')\n\n            return tf.add(flow, x, name=op_name)\n\n    ###\n    # PWC-Net nn builder\n    ###\n    def nn(self, x_tnsr, name=\'pwcnet\'):\n        """"""Defines and connects the backbone neural nets\n        Args:\n            inputs: TF placeholder that contains the input frame pairs in [batch_size, 2, H, W, 3] format\n            name: Name of the nn\n        Returns:\n            net: Output tensors of the backbone network\n        Ref:\n            RE: the scaling of the upsampled estimated optical flow, per page 5, section ""Implementation details,"" we\n            do not further scale the supervision signal at each level, the same as the FlowNet paper. As a result, we\n            need to scale the upsampled flow at each pyramid level for the warping layer. For example, at the second\n            level, we scale the upsampled flow from the third level by a factor of 5 (=20/4) before warping features\n            of the second image.\n        Based on:\n            - https://github.com/daigo0927/PWC-Net_tf/blob/master/model.py\n            Written by Daigo Hirooka, Copyright (c) 2018 Daigo Hirooka\n            MIT License\n        """"""\n        with tf.variable_scope(name):\n\n            # Extract pyramids of CNN features from both input images (1-based lists))\n            c1, c2 = self.extract_features(x_tnsr)\n\n            flow_pyr = []\n\n            for lvl in range(self.opts[\'pyr_lvls\'], self.opts[\'flow_pred_lvl\'] - 1, -1):\n\n                if lvl == self.opts[\'pyr_lvls\']:\n                    # Compute the cost volume\n                    corr = self.corr(c1[lvl], c2[lvl], lvl)\n\n                    # Estimate the optical flow\n                    upfeat, flow = self.predict_flow(corr, None, None, None, lvl)\n                else:\n                    # Warp level of Image1\'s using the upsampled flow\n                    scaler = 20. / 2**lvl  # scaler values are 0.625, 1.25, 2.5, 5.0\n                    warp = self.warp(c2[lvl], up_flow * scaler, lvl)\n\n                    # Compute the cost volume\n                    corr = self.corr(c1[lvl], warp, lvl)\n\n                    # Estimate the optical flow\n                    upfeat, flow = self.predict_flow(corr, c1[lvl], up_flow, up_feat, lvl)\n\n                _, lvl_height, lvl_width, _ = tf.unstack(tf.shape(c1[lvl]))\n\n                if lvl != self.opts[\'flow_pred_lvl\']:\n                    if self.opts[\'use_res_cx\']:\n                        flow = self.refine_flow(upfeat, flow, lvl)\n\n                    # Upsample predicted flow and the features used to compute predicted flow\n                    flow_pyr.append(flow)\n\n                    up_flow = self.deconv(flow, lvl, \'up_flow\')\n                    up_feat = self.deconv(upfeat, lvl, \'up_feat\')\n                else:\n                    # Refine the final predicted flow\n                    flow = self.refine_flow(upfeat, flow, lvl)\n                    flow_pyr.append(flow)\n\n                    # Upsample the predicted flow (final output) to match the size of the images\n                    scaler = 2**self.opts[\'flow_pred_lvl\']\n                    if self.dbg:\n                        print(f\'Upsampling {flow.op.name} by {scaler} in each dimension.\')\n                    size = (lvl_height * scaler, lvl_width * scaler)\n                    flow_pred = tf.image.resize_bilinear(flow, size, name=""flow_pred"") * scaler\n                    break\n\n            return flow_pred, flow_pyr\n'"
tfoptflow/multi_gpus.py,3,"b'""""""\nmulti_gpus.py\n\nHelpers to train a model using multi-GPU in-graph replication with synchronous updates.\nWe create one copy of the model (aka, a tower) per device and instruct it to compute forward and backward passes.\nThe gradients are then averaged and applied on the controller device where all the model\xe2\x80\x99s variables reside.\nThe controller device is the CPU, meaning that all variables live on the CPU and are copied to the GPUs in each step.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n\nBased on:\n    - https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py\n    Written by The TensorFlow Authors, Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n    Licensed under the Apache License 2.0\n\n    - TensorFlow - Multi GPU Computation\n    http://blog.s-schoener.com/2017-12-15-parallel-tensorflow-intro/\n    Written by Sebastian Sch\xc3\xb6ner, License unknown\n\n    - Tensorflow Multi-GPU VAE-GAN implementation\n    https://timsainb.github.io/multi-gpu-vae-gan-in-tensorflow.html\n    Written by Sebastian Sch\xc3\xb6ner, License unknown\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nfrom tensorflow.python.client import device_lib\nimport tensorflow as tf\n\nPS_OPS = [\'Variable\', \'VariableV2\', \'AutoReloadVariable\',\n          \'MutableHashTable\', \'MutableHashTableOfTensors\', \'MutableDenseHashTable\']\n\n\ndef assign_to_device(ops_device, var_device):\n    """"""Returns a function to place variables on the var_device.\n    If var_device is not set then the variables will be placed on the default device.\n    The best device for shared variables depends on the platform as well as the model.\n    Start with CPU:0 and then test GPU:0 to see if there is an improvement.\n    Args:\n        ops_device: Device for everything but variables. Sample values are /device:GPU:0 and /device:GPU:1.\n        var_device: Device to put the variables on. Sample values are /device:CPU:0 or /device:GPU:0.\n    Ref:\n        - Placing Variables on the cpu using `tf.contrib.layers` functions\n        https://github.com/tensorflow/tensorflow/issues/9517\n    """"""\n\n    def _assign(op):\n        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n        if node_def.op in PS_OPS:\n            return var_device\n        else:\n            return ops_device\n\n    return _assign\n\n\ndef get_available_gpus():\n    """"""Returns a list of the identifiers of all visible GPUs.\n    Ref:\n        - How to get current available GPUs in tensorflow?\n        https://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow\n    """"""\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos if x.device_type == \'GPU\']\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers. A tower is the name used to describe\n    a copy of the model on a device. Note that average_gradients() provides a synchronization point across all towers.\n    Args:\n        tower_grads: List of lists of (gradient, variable) tuples. The outer list is over individual gradients. The\n        inner list is over the gradient calculation for each tower.\n    Returns:\n        List of pairs of (gradient, variable) where the gradient has been averaged across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Each grad_and_vars looks like the following: ((grad0_gpu0, var0_gpu0),..., (grad0_gpuN, var0_gpuN))\n        grads = [grad for grad, _ in grad_and_vars]\n        grad = tf.reduce_mean(grads, 0)\n        # Keep in mind that the Variables are redundant because they are shared across towers. So, we only need to\n        # return the first tower\'s pointer to the Variable.\n        var = grad_and_vars[0][1]\n        average_grads.append((grad, var))\n    return average_grads\n'"
tfoptflow/optflow.py,0,"b'""""""\noptflow.py\n\nOptical flow I/O and visualization functions.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n\nRefs:\n    - Per MPI-Sintel/flow_code/C/flowIO.h and flowIO.cpp:\n\n    // the ""official"" threshold - if the absolute value of either\n    // flow component is greater, it\'s considered unknown\n    #define UNKNOWN_FLOW_THRESH 1e9\n\n    // value to use to represent unknown flow\n    #define UNKNOWN_FLOW 1e10\n\n    // first four bytes, should be the same in little endian\n    #define TAG_FLOAT 202021.25  // check for this when READING the file\n    #define TAG_STRING ""PIEH""    // use this when WRITING the file\n\n    // "".flo"" file format used for optical flow evaluation\n    //\n    // Stores 2-band float image for horizontal (u) and vertical (v) flow components.\n    // Floats are stored in little-endian order.\n    // A flow value is considered ""unknown"" if either |u| or |v| is greater than 1e9.\n    //\n    //  bytes  contents\n    //\n    //  0-3     tag: ""PIEH"" in ASCII, which in little endian happens to be the float 202021.25\n    //          (just a sanity check that floats are represented correctly)\n    //  4-7     width as an integer\n    //  8-11    height as an integer\n    //  12-end  data (width*height*2*4 bytes total)\n    //          the float values for u and v, interleaved, in row order, i.e.,\n    //          u[row0,col0], v[row0,col0], u[row0,col1], v[row0,col1], ...\n\n    - Numpy docs:\n    ndarray.tofile(fid, sep="""", format=""%s"")\n    https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.ndarray.tofile.html#numpy.ndarray.tofile\n\n    numpy.fromfile(file, dtype=float, count=-1, sep=\'\')\n    https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.fromfile.html\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport os\nimport warnings\nimport numpy as np\nimport cv2\nfrom skimage.io import imsave\n\nfrom utils import clean_dst_file\n\n\n##\n# I/O utils\n##\n\nTAG_FLOAT = 202021.25\n\n\ndef flow_read(src_file):\n    """"""Read optical flow stored in a .flo, .pfm, or .png file\n    Args:\n        src_file: Path to flow file\n    Returns:\n        flow: optical flow in [h, w, 2] format\n    Refs:\n        - Interpret bytes as packed binary data\n        Per https://docs.python.org/3/library/struct.html#format-characters:\n        format: f -> C Type: float, Python type: float, Standard size: 4\n        format: d -> C Type: double, Python type: float, Standard size: 8\n    Based on:\n        - To read optical flow data from 16-bit PNG file:\n        https://github.com/ClementPinard/FlowNetPytorch/blob/master/datasets/KITTI.py\n        Written by Cl\xc3\xa9ment Pinard, Copyright (c) 2017 Cl\xc3\xa9ment Pinard\n        MIT License\n        - To read optical flow data from PFM file:\n        https://github.com/liruoteng/OpticalFlowToolkit/blob/master/lib/pfm.py\n        Written by Ruoteng Li, Copyright (c) 2017 Ruoteng Li\n        License Unknown\n        - To read optical flow data from FLO file:\n        https://github.com/daigo0927/PWC-Net_tf/blob/master/flow_utils.py\n        Written by Daigo Hirooka, Copyright (c) 2018 Daigo Hirooka\n        MIT License\n    """"""\n    # Read in the entire file, if it exists\n    assert(os.path.exists(src_file))\n\n    if src_file.lower().endswith(\'.flo\'):\n\n        with open(src_file, \'rb\') as f:\n\n            # Parse .flo file header\n            tag = float(np.fromfile(f, np.float32, count=1)[0])\n            assert(tag == TAG_FLOAT)\n            w = np.fromfile(f, np.int32, count=1)[0]\n            h = np.fromfile(f, np.int32, count=1)[0]\n\n            # Read in flow data and reshape it\n            flow = np.fromfile(f, np.float32, count=h * w * 2)\n            flow.resize((h, w, 2))\n\n    elif src_file.lower().endswith(\'.png\'):\n\n        # Read in .png file\n        flow_raw = cv2.imread(src_file, -1)\n\n        # Convert from [H,W,1] 16bit to [H,W,2] float formet\n        flow = flow_raw[:, :, 2:0:-1].astype(np.float32)\n        flow = flow - 32768\n        flow = flow / 64\n\n        # Clip flow values\n        flow[np.abs(flow) < 1e-10] = 1e-10\n\n        # Remove invalid flow values\n        invalid = (flow_raw[:, :, 0] == 0)\n        flow[invalid, :] = 0\n\n    elif src_file.lower().endswith(\'.pfm\'):\n\n        with open(src_file, \'rb\') as f:\n\n            # Parse .pfm file header\n            tag = f.readline().rstrip().decode(""utf-8"")\n            assert(tag == \'PF\')\n            dims = f.readline().rstrip().decode(""utf-8"")\n            w, h = map(int, dims.split(\' \'))\n            scale = float(f.readline().rstrip().decode(""utf-8""))\n\n            # Read in flow data and reshape it\n            flow = np.fromfile(f, \'<f\') if scale < 0 else np.fromfile(f, \'>f\')\n            flow = np.reshape(flow, (h, w, 3))[:, :, 0:2]\n            flow = np.flipud(flow)\n    else:\n        raise IOError\n\n    return flow\n\n\ndef flow_write(flow, dst_file):\n    """"""Write optical flow to a .flo file\n    Args:\n        flow: optical flow\n        dst_file: Path where to write optical flow\n    """"""\n    # Create the output folder, if necessary\n    # Empty the output folder of previous predictions, if any\n    clean_dst_file(dst_file)\n\n    # Save optical flow to disk\n    with open(dst_file, \'wb\') as f:\n        np.array(TAG_FLOAT, dtype=np.float32).tofile(f)\n        height, width = flow.shape[:2]\n        np.array(width, dtype=np.uint32).tofile(f)\n        np.array(height, dtype=np.uint32).tofile(f)\n        flow.astype(np.float32).tofile(f)\n\n##\n# Visualization utils\n##\n\n\ndef flow_mag_stats(flow):\n    """"""Get the average flow magnitude from a flow field.\n    Args:\n        flow: optical flow\n    Returns:\n        Average flow magnitude\n    Ref:\n        - OpenCV 3.0.0-dev documentation \xc2\xbb OpenCV-Python Tutorials \xc2\xbb Video Analysis \xc2\xbb\n        https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_video/py_lucas_kanade/py_lucas_kanade.html\n    """"""\n    # Convert the u,v flow field to angle,magnitude vector representation\n    flow_magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n\n    # A couple times, we\'ve gotten NaNs out of the above...\n    nans = np.isnan(flow_magnitude)\n    if np.any(nans):\n        nans = np.where(nans)\n        flow_magnitude[nans] = 0.\n\n    return np.min(flow_magnitude), np.mean(flow_magnitude), np.max(flow_magnitude)\n\n\ndef flow_to_img(flow, normalize=True, info=None, flow_mag_max=None):\n    """"""Convert flow to viewable image, using color hue to encode flow vector orientation, and color saturation to\n    encode vector length. This is similar to the OpenCV tutorial on dense optical flow, except that they map vector\n    length to the value plane of the HSV color model, instead of the saturation plane, as we do here.\n    Args:\n        flow: optical flow\n        normalize: Normalize flow to 0..255\n        info: Text to superimpose on image (typically, the epe for the predicted flow)\n        flow_mag_max: Max flow to map to 255\n    Returns:\n        img: viewable representation of the dense optical flow in RGB format\n        flow_avg: optionally, also return average flow magnitude\n    Ref:\n        - OpenCV 3.0.0-dev documentation \xc2\xbb OpenCV-Python Tutorials \xc2\xbb Video Analysis \xc2\xbb\n        https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_video/py_lucas_kanade/py_lucas_kanade.html\n    """"""\n    hsv = np.zeros((flow.shape[0], flow.shape[1], 3), dtype=np.uint8)\n    flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0].astype(np.float32), flow[..., 1].astype(np.float32))\n\n    # A couple times, we\'ve gotten NaNs out of the above...\n    nans = np.isnan(flow_magnitude)\n    if np.any(nans):\n        nans = np.where(nans)\n        flow_magnitude[nans] = 0.\n\n    # Normalize\n    hsv[..., 0] = flow_angle * 180 / np.pi / 2\n    if normalize is True:\n        if flow_mag_max is None:\n            hsv[..., 1] = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)\n        else:\n            hsv[..., 1] = flow_magnitude * 255 / flow_mag_max\n    else:\n        hsv[..., 1] = flow_magnitude\n    hsv[..., 2] = 255\n    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n\n    # Add text to the image, if requested\n    if info is not None:\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        cv2.putText(img, info, (20, 20), font, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n\n    return img\n\n\ndef flow_write_as_png(flow, dst_file, info=None, flow_mag_max=None):\n    """"""Write optical flow to a .PNG file\n    Args:\n        flow: optical flow\n        dst_file: Path where to write optical flow as a .PNG file\n        info: Text to superimpose on image (typically, the epe for the predicted flow)\n        flow_mag_max: Max flow to map to 255\n    """"""\n    # Convert the optical flow field to RGB\n    img = flow_to_img(flow, flow_mag_max=flow_mag_max)\n\n    # Create the output folder, if necessary\n    # Empty the output folder of previous predictions, if any\n    clean_dst_file(dst_file)\n\n    # Add text to the image, if requested\n    if info is not None:\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        cv2.putText(img, info, (20, 20), font, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n\n    # Save RGB version of optical flow to disk\n    with warnings.catch_warnings():\n        warnings.simplefilter(""ignore"")\n        imsave(dst_file, img)\n'"
tfoptflow/pwcnet_predict.py,0,"b'""""""\npwcnet_predict.py\n\nRun inference on test split of dataset.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport sys\nfrom copy import deepcopy\nfrom dataset_base import _DEFAULT_DS_TEST_OPTIONS\nfrom dataset_mpisintel import MPISintelDataset\nfrom model_pwcnet import ModelPWCNet, _DEFAULT_PWCNET_TEST_OPTIONS\nfrom visualize import display_img_pairs_w_flows\n\n# TODO: You MUST set _DATASET_ROOT to the correct path on your machine!\nif sys.platform.startswith(""win""):\n    _DATASET_ROOT = \'E:/datasets/\'\nelse:\n    _DATASET_ROOT = \'/media/EDrive/datasets/\'\n_MPISINTEL_ROOT = _DATASET_ROOT + \'MPI-Sintel\'\n\n# TODO: Set device to use for inference\n# Here, we\'re using a GPU (use \'/device:CPU:0\' to run inference on the CPU)\ngpu_devices = [\'/device:GPU:0\']  \ncontroller = \'/device:GPU:0\'\n\n# More options...\n# Set the number of samples to visually inspect after inference\n# Set the path to the trained model (make sure you\'ve downloaded it first from http://bit.ly/tfoptflow)\n# Set the batch size\nnum_samples = 10\nckpt_path = \'./models/pwcnet-lg-6-2-multisteps-chairsthingsmix/pwcnet.ckpt-595000\'\nbatch_size = 8\n\n# Load the dataset in inference mode, starting with the default options\nds_opts = deepcopy(_DEFAULT_DS_TEST_OPTIONS)\nds_opts[\'type\'] = \'clean\'\nds = MPISintelDataset(mode=\'test\', ds_root=_MPISINTEL_ROOT, options=ds_opts)\n\n# Display dataset configuration\nds.print_config()\n\n# Configure the model for inference, starting with the default options\nnn_opts = deepcopy(_DEFAULT_PWCNET_TEST_OPTIONS)\nnn_opts[\'verbose\'] = True\nnn_opts[\'ckpt_path\'] = ckpt_path\nnn_opts[\'batch_size\'] = batch_size\nnn_opts[\'use_tf_data\'] = True\nnn_opts[\'gpu_devices\'] = gpu_devices\nnn_opts[\'controller\'] = controller\n\n# We\'re running the PWC-Net-large model in quarter-resolution mode\n# That is, with a 6 level pyramid, and upsampling of level 2 by 4 in each dimension as the final flow prediction\nnn_opts[\'use_dense_cx\'] = True\nnn_opts[\'use_res_cx\'] = True\nnn_opts[\'pyr_lvls\'] = 6\nnn_opts[\'flow_pred_lvl\'] = 2\n\n# The size of the images in this dataset are not multiples of 64, while the model generates flows padded to multiples\n# of 64. Hence, we need to crop the predicted flows to their original size\nnn_opts[\'adapt_info\'] = (1, 436, 1024, 2)\n\n# Instantiate the model in inference mode and display the model configuration\nnn = ModelPWCNet(mode=\'test\', options=nn_opts, dataset=ds)\nnn.print_config()\n\n# Generate the predictions and save them directly to disk; no need to return them here\nnn.predict(return_preds=False, save_preds=True)\n\n# Display a few random samples and their predictions\nimg_pairs, pred_labels, ids = ds.get_samples(num_samples, split=\'test_with_preds\', deterministic=False, as_tuple=True)\ndisplay_img_pairs_w_flows(img_pairs, pred_labels, titles=ids)\n'"
tfoptflow/pwcnet_predict_from_img_pairs.py,0,"b'""""""\npwcnet_predict_from_img_pairs.py\n\nRun inference on a list of images pairs.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nfrom copy import deepcopy\nfrom skimage.io import imread\nfrom model_pwcnet import ModelPWCNet, _DEFAULT_PWCNET_TEST_OPTIONS\nfrom visualize import display_img_pairs_w_flows\n\n# TODO: Set device to use for inference\n# Here, we\'re using a GPU (use \'/device:CPU:0\' to run inference on the CPU)\ngpu_devices = [\'/device:GPU:0\']  \ncontroller = \'/device:GPU:0\'\n\n# TODO: Set the path to the trained model (make sure you\'ve downloaded it first from http://bit.ly/tfoptflow)\nckpt_path = \'./models/pwcnet-lg-6-2-multisteps-chairsthingsmix/pwcnet.ckpt-595000\'\n\n# Build a list of image pairs to process\nimg_pairs = []\nfor pair in range(1, 4):\n    image_path1 = f\'./samples/mpisintel_test_clean_ambush_1_frame_00{pair:02d}.png\'\n    image_path2 = f\'./samples/mpisintel_test_clean_ambush_1_frame_00{pair+1:02d}.png\'\n    image1, image2 = imread(image_path1), imread(image_path2)\n    img_pairs.append((image1, image2))\n\n# Configure the model for inference, starting with the default options\nnn_opts = deepcopy(_DEFAULT_PWCNET_TEST_OPTIONS)\nnn_opts[\'verbose\'] = True\nnn_opts[\'ckpt_path\'] = ckpt_path\nnn_opts[\'batch_size\'] = 1\nnn_opts[\'gpu_devices\'] = gpu_devices\nnn_opts[\'controller\'] = controller\n\n# We\'re running the PWC-Net-large model in quarter-resolution mode\n# That is, with a 6 level pyramid, and upsampling of level 2 by 4 in each dimension as the final flow prediction\nnn_opts[\'use_dense_cx\'] = True\nnn_opts[\'use_res_cx\'] = True\nnn_opts[\'pyr_lvls\'] = 6\nnn_opts[\'flow_pred_lvl\'] = 2\n\n# The size of the images in this dataset are not multiples of 64, while the model generates flows padded to multiples\n# of 64. Hence, we need to crop the predicted flows to their original size\nnn_opts[\'adapt_info\'] = (1, 436, 1024, 2)\n\n# Instantiate the model in inference mode and display the model configuration\nnn = ModelPWCNet(mode=\'test\', options=nn_opts)\nnn.print_config()\n\n# Generate the predictions and display them\npred_labels = nn.predict_from_img_pairs(img_pairs, batch_size=1, verbose=False)\ndisplay_img_pairs_w_flows(img_pairs, pred_labels)\n'"
tfoptflow/utils.py,7,"b'""""""\nutils.py\n\nUtility functions.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport cv2\n\n\ndef clean_dst_file(dst_file):\n    """"""Create the output folder, if necessary; empty the output folder of previous predictions, if any\n    Args:\n        dst_file: Destination path\n    """"""\n    # Create the output folder, if necessary\n    dst_file_dir = os.path.dirname(dst_file)\n    if not os.path.exists(dst_file_dir):\n        os.makedirs(dst_file_dir)\n\n    # Empty the output folder of previous predictions, if any\n    if os.path.exists(dst_file):\n        os.remove(dst_file)\n\n\ndef tf_where(condition, x=None, y=None, *args, **kwargs):\n    """"""tf.where does not support broadcasting like its numpy equivaleny. This function implements it.\n    Args:\n        Same as tf.where\n    Refs:\n        - Broadcasting support in `tf.where\n        https://github.com/tensorflow/tensorflow/issues/9284#issuecomment-294778959\n        Written by Till Hoffmann\n    """"""\n    if x is None and y is None:\n        return tf.where(condition, x, y, *args, **kwargs)\n    else:\n        _shape = tf.broadcast_dynamic_shape(tf.shape(condition), tf.shape(x))\n        _broadcaster = tf.ones(_shape)\n        return tf.where(condition & (_broadcaster > 0.0), x * _broadcaster, y * _broadcaster, *args, **kwargs)\n\n\ndef scale(img, zoom_factor):\n    """"""\n    Center zoom in/out of the given image and returning an enlarged/shrinked view of\n    the image without changing dimensions\n    Args:\n        img : Image array\n        zoom_factor : amount of zoom as a ratio (0 to Inf)\n    Based on:\n        - Scipy rotate and zoom an image without changing its dimensions\n        https://stackoverflow.com/a/48097478\n        Written by Mohamed Ezz\n        License: MIT License\n    """"""\n    height, width = img.shape[:2]  # It\'s also the final desired shape\n    new_height, new_width = int(height * zoom_factor), int(width * zoom_factor)\n\n    # Crop only the part that will remain in the result (more efficient)\n    # Centered bbox of the final desired size in resized (larger/smaller) image coordinates\n    y1, x1 = max(0, new_height - height) // 2, max(0, new_width - width) // 2\n    y2, x2 = y1 + height, x1 + width\n    bbox = np.array([y1, x1, y2, x2])\n\n    # Map back to original image coordinates\n    bbox = (bbox / zoom_factor).astype(np.int)\n    y1, x1, y2, x2 = bbox\n    cropped_img = img[y1:y2, x1:x2]\n\n    # Handle padding when downscaling\n    resize_height, resize_width = min(new_height, height), min(new_width, width)\n    pad_height1, pad_width1 = (height - resize_height) // 2, (width - resize_width) // 2\n    pad_height2, pad_width2 = (height - resize_height) - pad_height1, (width - resize_width) - pad_width1\n    pad_spec = [(pad_height1, pad_height2), (pad_width1, pad_width2)] + [(0, 0)] * (img.ndim - 2)\n\n    result = cv2.resize(cropped_img, (resize_width, resize_height))\n    result = np.pad(result, pad_spec, mode=\'constant\')\n\n    assert(result.shape[0] == height and result.shape[1] == width)\n    return result\n'"
tfoptflow/visualize.py,0,"b'""""""\nvisualize.py\n\nVisualization helpers.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nimport matplotlib.pyplot as plt\n\nfrom utils import clean_dst_file\nfrom optflow import flow_to_img\n\n\ndef plot_img_pairs_w_flows(\n        img_pairs,\n        flow_pyrs=None,\n        num_lvls=0,\n        flow_preds=None,\n        flow_gts=None,\n        titles=None,\n        info=None,\n        flow_mag_max=None):\n    """"""Plot the given set of image pairs, optionally with flows and titles.\n    Args:\n        img_pairs: image pairs in [batch_size, 2, H, W, 3] or list([2, H, W, 3]) format.\n        flow_pyrs: optional, predicted optical flow pyramids [batch_size, H, W, 2] or list([H, W, 2]) format.\n        num_lvls: number of levels to show per pyramid (flow_pyrs must be set)\n        flow_preds: optional, predicted flows in [batch_size, H, W, 2] or list([H, W, 2]) format.\n        flow_gts: optional, groundtruth flows in [batch_size, H, W, 2] or list([H, W, 2]) format.\n        titles: optional, list of image and flow IDs to display with each image.\n        info: optional, stats to display above predicted flow\n        flow_mag_max: Max flow to map to 255\n    Returns:\n        plt: plot\n    """"""\n    # Setup drawing canvas\n    fig_height, fig_width = 5, 5\n    row_count = len(img_pairs)\n    col_count = 2\n    if flow_preds is not None:\n        col_count += 1\n    if flow_gts is not None:\n        col_count += 1\n    if flow_pyrs is not None:\n        row_count += len(img_pairs)\n        jump = num_lvls - col_count\n        col_count = max(num_lvls, col_count)\n    plt.figure(figsize=(fig_width * col_count, fig_height * row_count))\n\n    # Plot img_pairs inside the canvas\n    plot = 1\n    for row in range(len(img_pairs)):\n        # Plot image pair\n        plt.subplot(row_count, col_count, plot)\n        if titles is not None:\n            plt.title(titles[row][0], fontsize=fig_width * 2)\n        plt.axis(\'off\')\n        plt.imshow(img_pairs[row][0])\n        plt.subplot(row_count, col_count, plot + 1)\n        if titles is not None:\n            plt.title(titles[row][1], fontsize=fig_width * 2)\n        plt.axis(\'off\')\n        plt.imshow(img_pairs[row][1])\n        plot += 2\n\n        # Plot predicted flow, if any\n        if flow_preds is not None:\n            plt.subplot(row_count, col_count, plot)\n            title = ""predicted flow "" + info[row] if info is not None else ""predicted flow""\n            plt.title(title, fontsize=fig_width * 2)\n            plt.axis(\'off\')\n            plt.imshow(flow_to_img(flow_preds[row], flow_mag_max=flow_mag_max))\n            plot += 1\n\n        # Plot groundtruth flow, if any\n        if flow_gts is not None:\n            plt.subplot(row_count, col_count, plot)\n            plt.title(""groundtruth flow"", fontsize=fig_width * 2)\n            plt.axis(\'off\')\n            plt.imshow(flow_to_img(flow_gts[row], flow_mag_max=flow_mag_max))\n            plot += 1\n\n        # Plot the flow pyramid on the next row\n        if flow_pyrs is not None:\n            if jump > 0:\n                plot += jump\n            for lvl in range(num_lvls):\n                plt.subplot(row_count, col_count, plot)\n                plt.title(f""level {len(flow_pyrs[row]) - lvl + 1}"", fontsize=fig_width * 2)\n                plt.axis(\'off\')\n                plt.imshow(flow_to_img(flow_pyrs[row][lvl], flow_mag_max=flow_mag_max))\n                plot += 1\n            if jump < 0:\n                plot -= jump\n\n    plt.tight_layout()\n    return plt\n\n\ndef display_img_pairs_w_flows(\n        img_pairs,\n        flow_preds=None,\n        flow_gts=None,\n        titles=None,\n        info=None,\n        flow_mag_max=None):\n    """"""Display the given set of image pairs, optionally with flows and titles.\n    Args:\n        img_pairs: image pairs in [batch_size, 2, H, W, 3] or list([2, H, W, 3]) format.\n        flow_preds: optional, predicted flows in [batch_size, H, W, 2] or list([H, W, 2]) format.\n        flow_gts: optional, groundtruth flows in [batch_size, H, W, 2] or list([H, W, 2]) format.\n        titles: optional, list of image and flow IDs to display with each image.\n        info: optional, stats to display above predicted flow\n        flow_mag_max: Max flow to map to 255\n    """"""\n    plt = plot_img_pairs_w_flows(img_pairs, None, 0, flow_preds, flow_gts, titles, info, flow_mag_max)\n    plt.show()\n\n\ndef archive_img_pairs_w_flows(\n        img_pairs,\n        dst_file,\n        flow_preds=None,\n        flow_gts=None,\n        titles=None,\n        info=None,\n        flow_mag_max=None):\n    """"""Plot and save to disk te given set of image pairs, optionally with flows and titles.\n    Args:\n        img_pairs: image pairs in [batch_size, 2, H, W, 3] or list([2, H, W, 3]) format.\n        dst_file: Path where to save resulting image\n        flow_preds: optional, predicted flows in [batch_size, H, W, 2] or list([H, W, 2]) format.\n        flow_gts: optional, groundtruth flows in [batch_size, H, W, 2] or list([H, W, 2]) format.\n        titles: optional, list of image and flow IDs to display with each image.\n        info: optional, stats to display above predicted flow\n        flow_mag_max: Max flow to map to 255\n    """"""\n    # Create the output folder, if necessary\n    # Empty the output folder of previous predictions, if any\n    clean_dst_file(dst_file)\n\n    # Build plot and save it to disk\n    plt = plot_img_pairs_w_flows(img_pairs, None, 0, flow_preds, flow_gts, titles, info, flow_mag_max)\n    plt.savefig(dst_file, bbox_inches=\'tight\', pad_inches=0.1)\n    plt.close()\n\n\ndef display_img_pairs_w_flow_pyrs(\n        img_pairs,\n        flow_pyrs=None,\n        num_lvls=0,\n        flow_preds=None,\n        flow_gts=None,\n        titles=None,\n        info=None,\n        flow_mag_max=None):\n    """"""Display the given set of image pairs, optionally with flows and titles.\n    Args:\n        img_pairs: image pairs in [batch_size, 2, H, W, 3] or list([2, H, W, 3]) format.\n        flow_pyrs: optional, predicted optical flow pyramids [batch_size, H, W, 2] or list([H, W, 2]) format.\n        num_lvls: number of levels per pyramid (flow_pyrs must be set)\n        flow_preds: optional, predicted flows in [batch_size, H, W, 2] or list([H, W, 2]) format.\n        flow_gts: optional, groundtruth flows in [batch_size, H, W, 2] or list([H, W, 2]) format.\n        titles: optional, list of image and flow IDs to display with each image.\n        info: optional, stats to display above predicted flow\n        flow_mag_max: Max flow to map to 255\n    """"""\n    plt = plot_img_pairs_w_flows(img_pairs, flow_pyrs, num_lvls, flow_preds, flow_gts, titles, info, flow_mag_max)\n    plt.show()\n\n\ndef archive_img_pairs_w_flow_pyrs(\n        img_pairs,\n        dst_file,\n        flow_pyrs=None,\n        num_lvls=0,\n        flow_preds=None,\n        flow_gts=None,\n        titles=None,\n        info=None,\n        flow_mag_max=None):\n    """"""Plot and save to disk te given set of image pairs, optionally with flows and titles.\n    Args:\n        img_pairs: image pairs in [batch_size, 2, H, W, 3] or list([2, H, W, 3]) format.\n        dst_file: Path where to save resulting image\n        flow_pyrs: optional, predicted optical flow pyramids [batch_size, H, W, 2] or list([H, W, 2]) format.\n        num_lvls: number of levels per pyramid (flow_pyrs must be set)\n        flow_preds: optional, predicted flows in [batch_size, H, W, 2] or list([H, W, 2]) format.\n        flow_gts: optional, groundtruth flows in [batch_size, H, W, 2] or list([H, W, 2]) format.\n        titles: optional, list of image and flow IDs to display with each image.\n        info: optional, stats to display above predicted flow\n        flow_mag_max: Max flow to map to 255\n    """"""\n    # Create the output folder, if necessary\n    # Empty the output folder of previous predictions, if any\n    clean_dst_file(dst_file)\n\n    # Build plot and save it to disk\n    plt = plot_img_pairs_w_flows(img_pairs, flow_pyrs, num_lvls, flow_preds, flow_gts, titles, info, flow_mag_max)\n    plt.savefig(dst_file, bbox_inches=\'tight\', pad_inches=0.1)\n    plt.close()\n'"
