file_path,api_count,code
tfwss/bboxes.py,0,"b'""""""\nbboxes.py\n\nBounding box utility functions.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n\nBased on:\n    - https://github.com/matterport/Mask_RCNN/blob/master/utils.py\n        Copyright (c) 2017 Matterport, Inc. / Written by Waleed Abdulla\n        Licensed under the MIT License\n\nReferences for future work:\n    - https://github.com/tensorflow/models/blob/master/research/object_detection/utils/np_box_ops.py\n      https://github.com/tensorflow/models/blob/master/research/object_detection/utils/ops.py\n        Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n        Licensed under the Apache License, Version 2.0\n    - https://github.com/tangyuhao/DAVIS-2016-Chanllege-Solution/blob/master/Step1-SSD/tf_extended/bboxes.py\n        https://github.com/tangyuhao/DAVIS-2016-Chanllege-Solution/blob/master/Step1-SSD/bounding_box.py\n        Copyright (c) 2017 Paul Balanca / Written by Paul Balanca\n        Licensed under the Apache License, Version 2.0, January 2004\n""""""\n\nimport numpy as np\n\ndef extract_bbox(mask, order=\'y1x1y2x2\'):\n    """"""Compute bounding box from a mask.\n    Param:\n        mask: [height, width]. Mask pixels are either >0 or 0.\n        order: [\'y1x1y2x2\' | ]\n    Returns:\n        bbox numpy array [y1, x1, y2, x2] or tuple x1, y1, x2, y2.\n    Based on:\n        https://stackoverflow.com/questions/31400769/bounding-box-of-numpy-array\n    """"""\n    horizontal_indicies = np.where(np.any(mask, axis=0))[0]\n    vertical_indicies = np.where(np.any(mask, axis=1))[0]\n    if horizontal_indicies.shape[0]:\n        x1, x2 = horizontal_indicies[[0, -1]]\n        y1, y2 = vertical_indicies[[0, -1]]\n        # x2 and y2 should not be part of the box. Increment by 1.\n        x2 += 1\n        y2 += 1\n    else:\n        # No mask for this instance. Might happen due to\n        # resizing or cropping. Set bbox to zeros\n        x1, x2, y1, y2 = 0, 0, 0, 0\n    if order == \'x1y1x2y2\':\n        return x1, y1, x2, y2\n    else:\n        return np.array([y1, x1, y2, x2])\n\ndef extract_bboxes(mask):\n    """"""Compute bounding boxes from an array of masks.\n    Params\n        mask: [height, width, num_instances]. Mask pixels are either >0 or 0.\n    Returns:\n        bbox numpy arrays [num_instances, (y1, x1, y2, x2)].\n    """"""\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        boxes[i] = extract_bbox(mask[:, :, i])\n    return boxes.astype(np.int32)\n\n'"
tfwss/dataset.py,0,"b'""""""\ndataset.py\n\nDataset utility functions and classes.\n\nFollowing the instructions provided in Section ""6. Instance Segmentation Results"" of the ""Simple Does It"" paper, we use\nthe Berkeley-augmented Pascal VOC segmentation dataset that provides per-instance segmentation masks for VOC2012 data.\nThe Berkley augmented dataset can be downloaded from here:\nhttp://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n\nBased on:\n  - https://github.com/warmspringwinds/tf-image-segmentation/blob/master/tf_image_segmentation/utils/tf_records.py\n    https://github.com/warmspringwinds/tf-image-segmentation/blob/master/tf_image_segmentation/utils/pascal_voc.py\n    https://github.com/warmspringwinds/tf-image-segmentation/blob/master/tf_image_segmentation/recipes/pascal_voc/convert_pascal_voc_to_tfrecords.ipynb\n    Copyright (c) 2017 Daniil Pakhomov / Written by Daniil Pakhomov\n    Licensed under the MIT License\n\nMore to look at later to add support for TFRecords:\n  https://github.com/fperazzi/davis-2017/blob/master/python/lib/davis/dataset/base.py\n  https://github.com/fperazzi/davis-2017/blob/master/python/lib/davis/dataset/loader.py\n  https://github.com/kwotsin/create_tfrecords\n  https://kwotsin.github.io/tech/2017/01/29/tfrecords.html\n  http://yeephycho.github.io/2016/08/15/image-data-in-tensorflow/\n  - http://www.machinelearninguru.com/deep_learning/tensorflow/basics/tfrecord/tfrecord.html\n    How to write into and read from a tfrecords file in TensorFlow\n    Writeen by Hadi Kazemi\n  - https://github.com/ferreirafabio/video2tfrecords/blob/master/video2tfrecords.py\n    Copyright (c) 2017 F\xc3\xa1bio Ferreira / Written F\xc3\xa1bio Ferreira\n    Licensed under the MIT License\n""""""\n\n# TODO Add support for TFRecords\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os, sys\nimport glob\nimport warnings\nimport tensorflow as tf\nimport numpy as np\nfrom scipy.io import loadmat\nfrom tqdm import tqdm\nfrom skimage import img_as_ubyte\nfrom skimage.io import imread, imsave\nfrom bboxes import extract_bbox\nfrom segment import rect_mask, grabcut\nfrom visualize import draw_masks\n\nif sys.platform.startswith(""win""):\n    _BK_VOC_DATASET = ""E:/datasets/bk-voc/benchmark_RELEASE/dataset""\nelse:\n    _BK_VOC_DATASET = \'/media/EDrive/datasets/bk-voc/benchmark_RELEASE/dataset\'\n\n_DBG_TRAIN_SET = -1\n\n_DEFAULT_BKVOC_OPTIONS = {\n    \'in_memory\': False,\n    \'data_aug\': False,\n    \'use_cache\': False,\n    \'use_grabcut_labels\': True}\n\nclass BKVOCDataset(object):\n    """"""Berkeley-augmented Pascal VOC 2012 segmentation dataset.\n    """"""\n\n    def __init__(self, phase=\'train\', dataset_root=_BK_VOC_DATASET, options=_DEFAULT_BKVOC_OPTIONS):\n        """"""Initialize the Dataset object\n        Args:\n            phase: Possible options: \'train\' or \'test\'\n            dataset_root: Path to the root of the dataset\n            options: see below\n        Options:\n            in_memory: True loads all the training images upfront, False loads images in small batches\n            data_aug: True adds augmented data to training set\n            use_cache: True stores training files and augmented versions in npy file\n            use_grabcut_labels: True computes magnitudes of forward and backward flows\n        """"""\n        # Only options supported in this initial implementation\n        assert (options == _DEFAULT_BKVOC_OPTIONS)\n\n        # Save file and folder name\n        self._dataset_root = dataset_root\n        self._phase = phase\n        self._options = options\n\n        # Set paths and file names\n        self._img_folder = self._dataset_root + \'/img\'\n        self._mats_folder = self._dataset_root + \'/inst\'\n        self._masks_folder = self._dataset_root + \'/inst_masks\'\n        self._grabcuts_folder = self._dataset_root + \'/inst_grabcuts\'\n        self.pred_masks_path = self._dataset_root + \'/predicted_inst_masks\'\n        self.img_pred_masks_path = self._dataset_root + \'/img_with_predicted_inst_masks\'\n        self._train_IDs_file = self._dataset_root + \'/train.txt\'\n        self._test_IDs_file = self._dataset_root + \'/val.txt\'\n        self._img_mask_pairs_file = self._dataset_root + \'/img_mask_pairs.txt\'\n        self._train_img_mask_pairs_file = self._dataset_root + \'/train_img_mask_pairs.txt\'\n        self._test_img_mask_pairs_file = self._dataset_root + \'/val_img_mask_pairs.txt\'\n\n        # Load ID files\n        if not self._load_img_mask_pairs_file(self._img_mask_pairs_file):\n            self.prepare()\n\n        # Init batch parameters\n        if self._phase == \'train\':\n            self._load_img_mask_pairs_file(self._train_img_mask_pairs_file)\n            self._grabcut_files = [self._grabcuts_folder + \'/\' + os.path.basename(img_mask_pair[1]) for img_mask_pair in self._img_mask_pairs]\n            self._train_ptr = 0\n            self.train_size = len(self._img_mask_pairs) if _DBG_TRAIN_SET == -1 else _DBG_TRAIN_SET\n            self._train_idx = np.arange(self.train_size)\n            np.random.seed(1)\n            np.random.shuffle(self._train_idx)\n        else:\n            self._options[\'use_grabcut_labels\'] = False\n            self._load_img_mask_pairs_file(self._test_img_mask_pairs_file)\n            self._test_ptr = 0\n            self.test_size = len(self._img_mask_pairs)\n\n    ###\n    ### Input Samples and Labels Prep\n    ###\n    def prepare(self):\n        """"""Do all the preprocessing needed before training/val/test samples can be generated.\n        """"""\n        # Convert instance masks stored in .mat files to .png files and compute their bboxes\n        self._mat_masks_to_png()\n    \n        # Generate grabcuts, if they don\'t exist yet\n        self._bboxes_to_grabcuts()\n    \n        # Generate train and test image/mask pair files, if they don\'t exist yet\n        self._split_img_mask_pairs()\n\n    def _save_img_mask_pairs_file(self):\n        """"""Create the file that matches masks with their image file (and has bbox info)\n        """"""\n        assert (len(self._mask_bboxes) == len(self._img_mask_pairs))\n        with open(self._img_mask_pairs_file, \'w\') as img_mask_pairs_file:\n            for img_mask_pair, mask_bbox in zip(self._img_mask_pairs, self._mask_bboxes):\n                img_path = os.path.basename(img_mask_pair[0])\n                mask_path = os.path.basename(img_mask_pair[1])\n                line = \'{}###{}###{}###{}###{}###{}\\n\'.format(img_path, mask_path, mask_bbox[0],\n                                                              mask_bbox[1], mask_bbox[2], mask_bbox[3])\n                img_mask_pairs_file.write(line)\n    \n    \n    def _split_img_mask_pairs(self):\n        """"""Create the training and test portions of the image mask pairs\n        """"""\n        if os.path.exists(self._train_img_mask_pairs_file) and os.path.exists(self._test_img_mask_pairs_file):\n            return False\n\n        with open(self._train_IDs_file, \'r\') as f:\n            train_IDs = f.readlines()\n\n        with open(self._test_IDs_file, \'r\') as f:\n            test_IDs = f.readlines()\n\n        # Load complete list of entries and separate training and test entries\n        assert(os.path.exists(self._img_mask_pairs_file))\n        with open(self._img_mask_pairs_file, \'r\') as img_mask_pairs_file:\n            lines = img_mask_pairs_file.readlines()\n            train_lines = []\n            test_lines = []\n            for line in lines:\n                splits = line.split(\'###\')\n                file_ID = \'{}\\n\'.format(str(splits[0])[-15:-4])\n                if file_ID in train_IDs:\n                    train_lines.append(line)\n                elif file_ID in test_IDs:\n                    test_lines.append(line)\n                else:\n                    raise ValueError(\'Error in processing train/val text files.\')\n\n        # Save result\n        with open(self._train_img_mask_pairs_file, \'w\') as f:\n            for line in train_lines:\n                f.write(line)\n        with open(self._test_img_mask_pairs_file, \'w\') as f:\n            for line in test_lines:\n                f.write(line)\n        return True\n\n\n    def _load_img_mask_pairs_file(self, img_mask_pairs_path):\n        """"""Load the file that matches masks with their image file (and has bbox info)\n        Args:\n            img_mask_pairs_path: path to file\n        Returns:\n          True if file was correctly loaded, False otherwise\n        """"""\n        if os.path.exists(img_mask_pairs_path):\n            with open(img_mask_pairs_path, \'r\') as img_mask_pairs_file:\n                lines = img_mask_pairs_file.readlines()\n                self._img_mask_pairs = []\n                self._mask_bboxes = []\n                for line in lines:\n                    splits = line.split(\'###\')\n                    img_path = self._img_folder + \'/\' + str(splits[0])\n                    mask_path = self._masks_folder + \'/\' + str(splits[1])\n                    self._img_mask_pairs.append((img_path, mask_path))\n                    self._mask_bboxes.append((int(splits[2]), int(splits[3]), int(splits[4]), int(splits[5])))\n                return True\n        return False\n    \n    \n    def _mat_masks_to_png(self):\n        """"""Converts instance masks stored in .mat files to .png files.\n        PNG files are created in the same folder as where the .mat files are.\n        If the name of this folder ends with ""cls"", class masks are created.\n        If the name of this folder ends with ""inst"", instance masks are created.\n    \n        Returns:\n          True if files were created, False if the masks folder already contains PNG files\n        """"""\n        mat_files = glob.glob(self._mats_folder + \'/*.mat\')\n    \n        # Build the list of image files for which we have mat masks\n        key = os.path.basename(os.path.normpath(self._mats_folder))\n        if key == \'cls\':\n            key = \'GTcls\'\n        elif key == \'inst\':\n            key = \'GTinst\'\n        else:\n            raise ValueError(\'ERR: Expected mask folder path to end with ""/inst"" or ""/cls""\')\n    \n        # Create output folder, if necessary\n        img_files = [self._img_folder + \'/\' + os.path.basename(file).replace(\'.mat\', \'.jpg\') for file in mat_files]\n        if not os.path.exists(self._masks_folder):\n            os.makedirs(self._masks_folder)\n    \n        # Generate image mask pairs and compute their bboxes\n        self._img_mask_pairs = []\n        self._mask_bboxes = []\n        with warnings.catch_warnings():\n            warnings.simplefilter(""ignore"")\n            for mat_file, img_file in tqdm(zip(mat_files, img_files), total=len(mat_files), ascii=True, ncols=80,\n                                           desc=\'MAT to PNG masks\'):\n                mat = loadmat(mat_file, mat_dtype=True, squeeze_me=True, struct_as_record=False)\n                masks = mat[key].Segmentation\n                mask_file_basename = os.path.basename(mat_file)\n                for instance in np.unique(masks)[1:]:\n                    mask_file = self._masks_folder + \'/\' + mask_file_basename[:-4] + \'_\' + str(int(instance)) + \'.png\'\n                    self._img_mask_pairs.append((img_file, mask_file))\n                    # Build mask for object instance\n                    mask = img_as_ubyte(masks == instance)\n                    # Compute the mask\'s bbox\n                    self._mask_bboxes.append(extract_bbox(mask))\n                    # Save the mask in PNG format to mask folder\n                    imsave(mask_file, mask)\n    \n        # Save the results to disk\n        self._save_img_mask_pairs_file()\n    \n        return True\n    \n    \n    def _bboxes_to_grabcuts(self):\n        """"""Generate segmentation masks from images and bounding boxes using Grabcut.\n        """"""\n        mask_files = glob.glob(self._masks_folder + \'/*.png\')\n        if os.path.exists(self._grabcuts_folder):\n            self._grabcut_files = glob.glob(self._grabcuts_folder + \'/*.png\')\n            if _DBG_TRAIN_SET == -1:\n                if self._grabcut_files and len(self._grabcut_files) == len(mask_files):\n                    return False\n            else:\n                if self._grabcut_files and len(self._grabcut_files) >= _DBG_TRAIN_SET:\n                    return False\n\n        # Create output folder, if necessary\n        grabcut_files = [self._grabcuts_folder + \'/\' + os.path.basename(img_mask_pair[1]) for img_mask_pair in\n                         self._img_mask_pairs]\n        if not os.path.exists(self._grabcuts_folder):\n            os.makedirs(self._grabcuts_folder)\n    \n        # Run Grabcut on input data\n        self._grabcut_files = []\n        with warnings.catch_warnings():\n            warnings.simplefilter(""ignore"")\n            for img_mask_pair, mask_bbox, grabcut_file in tqdm(zip(self._img_mask_pairs, self._mask_bboxes, grabcut_files),\n                                                               total=len(self._img_mask_pairs), ascii=True, ncols=80,\n                                                               desc=\'Grabcuts\'):\n                # Continue generating grabcuts from where you last stopped after OpenCV crash\n                if not os.path.exists(grabcut_file):\n                    # Use Grabcut to create a segmentation within the bbox\n                    mask = grabcut(img_mask_pair[0], mask_bbox)\n                    # Save the mask in PNG format to the grabcuts folder\n                    imsave(grabcut_file, mask)\n                self._grabcut_files.append(grabcut_file)\n    \n        return True\n\n    ###\n    ### Batch Management\n    ###\n    def _load_sample(self, input_rgb_path, input_bbox, label_path=None):\n        """"""Load a propertly formatted sample (input sample + associated label)\n        In training mode, there is a label; in testimg mode, there isn\'t.\n        Args:\n            input_rgb_path: Path to RGB image\n            input_bbox: Bounding box to convert to a binary mask\n            label_path: Path to grabcut label, if any label\n        Returns in training:\n            input sample: RGB+bbox binary mask concatenated in format [W, H, 4]\n            label: Grabcut segmentation in format [W, H, 1], if any label\n        """"""\n        input_rgb = imread(input_rgb_path)\n        input_shape = input_rgb.shape\n        input_bin_mask = rect_mask((input_shape[0], input_shape[1], 1), input_bbox)\n        assert (len(input_bin_mask.shape) == 3 and input_bin_mask.shape[2] == 1)\n        input = np.concatenate((input_rgb, input_bin_mask), axis=-1)\n        assert (len(input.shape) == 3 and input.shape[2] == 4)\n        if label_path:\n            label = imread(label_path)\n            label = np.expand_dims(label, axis=-1)\n            assert (len(label.shape) == 3 and label.shape[2] == 1)\n        else:\n            label = None\n        return input, label\n\n    def next_batch(self, batch_size, phase=\'train\', segnet_stream=\'weak\'):\n        """"""Get next batch of image (path) and masks\n        Args:\n            batch_size: Size of the batch\n            phase: Possible options:\'train\' or \'test\'\n            segnet_stream: Binary segmentation net stream [\'weak\'|\'full\']\n        Returns in training:\n            inputs: Batch of 4-channel inputs (RGB+bbox binary mask) in format [batch_size, W, H, 4]\n            labels: Batch of grabcut segmentations in format [batch_size, W, H, 1]\n        Returns in testing:\n            inputs: Batch of 4-channel inputs (RGB+bbox binary mask) in format [batch_size, W, H, 4]\n            output_file: List of output file names that match the bbox file names\n        """"""\n        assert (self._options[\'in_memory\'] is False)  # Only option supported at this point\n        assert (segnet_stream == \'weak\')  # Only option supported at this point\n        if phase == \'train\':\n            inputs, labels = [], []\n            if self._train_ptr + batch_size < self.train_size:\n                idx = np.array(self._train_idx[self._train_ptr:self._train_ptr + batch_size])\n                for l in idx:\n                    input, label = self._load_sample(self._img_mask_pairs[l][0], self._mask_bboxes[l],\n                                   self._grabcut_files[l])\n                    inputs.append(input)\n                    labels.append(label)\n                self._train_ptr += batch_size\n            else:\n                old_idx = np.array(self._train_idx[self._train_ptr:])\n                np.random.shuffle(self._train_idx)\n                new_ptr = (self._train_ptr + batch_size) % self.train_size\n                idx = np.array(self._train_idx[:new_ptr])\n                inputs_1, labels_1, inputs_2, labels_2 = [], [], [], []\n                for l in old_idx:\n                    input, label = self._load_sample(self._img_mask_pairs[l][0], self._mask_bboxes[l],\n                                   self._grabcut_files[l])\n                    inputs_1.append(input)\n                    labels_1.append(label)\n                for l in idx:\n                    input, label = self._load_sample(self._img_mask_pairs[l][0], self._mask_bboxes[l],\n                                   self._grabcut_files[l])\n                    inputs_2.append(input)\n                    labels_2.append(label)\n                inputs = inputs_1 + inputs_2\n                labels = labels_1 + labels_2\n                self._train_ptr = new_ptr\n            return np.asarray(inputs), np.asarray(labels)\n        elif phase == \'test\':\n            inputs, output_files = [], []\n            if self._test_ptr + batch_size < self.test_size:\n                for l in range(self._test_ptr, self._test_ptr + batch_size):\n                    input, _ = self._load_sample(self._img_mask_pairs[l][0], self._mask_bboxes[l])\n                    output_file = os.path.basename(self._img_mask_pairs[l][1])\n                    inputs.append(input)\n                    output_files.append(output_file)\n                self._test_ptr += batch_size\n            else:\n                new_ptr = (self._test_ptr + batch_size) % self.test_size\n                inputs_1, output_files_1, inputs_2, output_files_2 = [], [], [], []\n                for l in range(self._test_ptr, self.test_size):\n                    input, _ = self._load_sample(self._img_mask_pairs[l][0], self._mask_bboxes[l])\n                    output_file = os.path.basename(self._img_mask_pairs[l][1])\n                    inputs_1.append(input)\n                    output_files_1.append(output_file)\n                for l in range(0, new_ptr):\n                    input, _ = self._load_sample(self._img_mask_pairs[l][0], self._mask_bboxes[l])\n                    output_file = os.path.basename(self._img_mask_pairs[l][1])\n                    inputs_2.append(input)\n                    output_files_2.append(output_file)\n                inputs = inputs_1 + inputs_2\n                output_files = output_files_1 + output_files_2\n                self._test_ptr = new_ptr\n            return np.asarray(inputs), output_files\n        else:\n            return None, None\n\n    ###\n    ### Debug utils\n    ###\n    def print_config(self):\n        """"""Display configuration values.""""""\n        print(""\\nConfiguration:"")\n        for k, v in self._options.items():\n            print(""  {:20} {}"".format(k, v))\n        print(""  {:20} {}"".format(\'phase\', self._phase))\n        print(""  {:20} {}"".format(\'samples\', len(self._img_mask_pairs)))\n\n    ###\n    ### TODO TFRecords helpers\n    ### See:\n    ### https://github.com/fperazzi/davis-2017/blob/master/python/lib/davis/dataset/base.py\n    ### https://github.com/fperazzi/davis-2017/blob/master/python/lib/davis/dataset/loader.py\n    ### https://github.com/kwotsin/create_tfrecords\n    ### https://kwotsin.github.io/tech/2017/01/29/tfrecords.html\n    ### http://yeephycho.github.io/2016/08/15/image-data-in-tensorflow/\n    ### E:\\repos\\models-master\\research\\inception\\inception\\data\\build_imagenet_data.py\n    ### E:\\repos\\models-master\\research\\object_detection\\dataset_tools\\create_kitti_tf_record.py\n    ###\n    def _load_from_tfrecords(self):\n        # TODO _load_from_tfrecords\n        pass\n\n    def _write_to_tfrecords(self):\n        # TODO _write_to_tfrecords\n        pass\n\n    def combine_images_with_predicted_masks(self):\n        """"""Build list of individual test immages with predicted masks overlayed.""""""\n        # Overlay masks on top of images\n        prev_image, bboxes, masks = None, [], []\n        with tqdm(total=len(self._mask_bboxes), desc=""Combining JPGs with predictions"", ascii=True, ncols=80) as pbar:\n            for img_mask_pair, bbox in zip(self._img_mask_pairs, self._mask_bboxes):\n                pbar.update(1)\n                if img_mask_pair[0] == prev_image:\n                    # Accumulate predicted masks and bbox instances belonging to the same image\n                    bboxes.append(bbox)\n                    masks.append(self.pred_masks_path + \'/\' + os.path.basename(img_mask_pair[1]))\n                else:\n                    if prev_image:\n                        # Combine image, masks and bboxes in a single image and save the result to disk\n                        image = imread(prev_image)\n                        masks = np.asarray([imread(mask) for mask in masks])\n                        masks = np.expand_dims(masks, axis=-1)\n                        draw_masks(image, np.asarray(bboxes), np.asarray(masks))\n                        imsave(self.img_pred_masks_path + \'/\' + os.path.basename(prev_image), image)\n                    prev_image = img_mask_pair[0]\n                    bboxes = [bbox]\n                    masks = [self.pred_masks_path + \'/\' + os.path.basename(img_mask_pair[1])]\n\n# def test():\n#     dataset = BKVOCDataset()\n#     dataset.print_config()\n#     # WARNING: THE NEXT LINE WILL FORCE REGENERATION OF INTERMEDIARY FILES\n#     # dataset.prepare()\n#\n# if __name__ == \'__main__\':\n#     test()\n'"
tfwss/model.py,78,"b'""""""\nmodel.py\n\nSegmentation backbone networks.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n\nBased on:\n  - https://github.com/scaelles/OSVOS-TensorFlow/blob/master/osvos_parent_demo.py\n    Written by Sergi Caelles (scaelles@vision.ee.ethz.ch)\n    This file is part of the OSVOS paper presented in:\n      Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixe, Daniel Cremers, Luc Van Gool\n      One-Shot Video Object Segmentation\n      CVPR 2017\n    Unknown code license\n\nReferences for future work:\n    https://github.com/scaelles/OSVOS-TensorFlow\n    http://localhost:8889/notebooks/models-master/research/slim/slim_walkthrough.ipynb\n    https://github.com/bryanyzhu/two-stream-pytorch\n    https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim\n    https://github.com/kwotsin/TensorFlow-ENet/blob/master/predict_segmentation.py\n    https://github.com/fperazzi/davis-2017/tree/master/python/lib/davis/measures\n    https://github.com/suyogduttjain/fusionseg\n    https://gist.github.com/omoindrot/dedc857cdc0e680dfb1be99762990c9c/\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os, sys, warnings\nimport numpy as np\nfrom datetime import datetime\nfrom skimage.io import imsave\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import utils\nslim = tf.contrib.slim\n\nfrom tqdm import trange\n\ndef backbone_arg_scope(weight_decay=0.0002):\n    """"""Defines the network\'s arg scope.\n    Args:\n        weight_decay: The l2 regularization coefficient.\n    Returns:\n        An arg_scope.\n    """"""\n    with slim.arg_scope([slim.conv2d, slim.convolution2d_transpose],\n                        activation_fn=tf.nn.relu,\n                        weights_initializer=tf.random_normal_initializer(stddev=0.001),\n                        weights_regularizer=slim.l2_regularizer(weight_decay),\n                        biases_initializer=tf.zeros_initializer(),\n                        biases_regularizer=None,\n                        padding=\'SAME\') as arg_sc:\n        return arg_sc\n\n\ndef crop_features(feature, out_size):\n    """"""Crop the center of a feature map\n    This is necessary when large upsampling results in a (width x height) size larger than the original input.\n    Args:\n        feature: Feature map to crop\n        out_size: Size of the output feature map\n    Returns:\n        Tensor that performs the cropping\n    """"""\n    up_size = tf.shape(feature)\n    ini_w = tf.div(tf.subtract(up_size[1], out_size[1]), 2)\n    ini_h = tf.div(tf.subtract(up_size[2], out_size[2]), 2)\n    slice_input = tf.slice(feature, (0, ini_w, ini_h, 0), (-1, out_size[1], out_size[2], -1))\n    # slice_input = tf.slice(feature, (0, ini_w, ini_w, 0), (-1, out_size[1], out_size[2], -1))  # Caffe cropping way\n    return tf.reshape(slice_input, [int(feature.get_shape()[0]), out_size[1], out_size[2], int(feature.get_shape()[3])])\n\n\ndef backbone(inputs, segnet_stream=\'weak\'):\n    """"""Defines the backbone network (same as the OSVOS network, with variation in input size)\n    Args:\n        inputs: Tensorflow placeholder that contains the input image (either 3 or 4 channels)\n        segnet_stream: Is this the 3-channel or the 4-channel input version?\n    Returns:\n        net: Output Tensor of the network\n        end_points: Dictionary with all Tensors of the network\n    Reminder:\n        This is how a VGG16 network looks like:\n\n        Layer (type)                     Output Shape          Param #     Connected to\n        ====================================================================================================\n        input_1 (InputLayer)             (None, 480, 854, 3)   0\n        ____________________________________________________________________________________________________\n        block1_conv1 (Convolution2D)     (None, 480, 854, 64)  1792        input_1[0][0]\n        ____________________________________________________________________________________________________\n        block1_conv2 (Convolution2D)     (None, 480, 854, 64)  36928       block1_conv1[0][0]\n        ____________________________________________________________________________________________________\n        block1_pool (MaxPooling2D)       (None, 240, 427, 64)  0           block1_conv2[0][0]\n        ____________________________________________________________________________________________________\n        block2_conv1 (Convolution2D)     (None, 240, 427, 128) 73856       block1_pool[0][0]\n        ____________________________________________________________________________________________________\n        block2_conv2 (Convolution2D)     (None, 240, 427, 128) 147584      block2_conv1[0][0]\n        ____________________________________________________________________________________________________\n        block2_pool (MaxPooling2D)       (None, 120, 214, 128) 0           block2_conv2[0][0]\n        ____________________________________________________________________________________________________\n        block3_conv1 (Convolution2D)     (None, 120, 214, 256) 295168      block2_pool[0][0]\n        ____________________________________________________________________________________________________\n        block3_conv2 (Convolution2D)     (None, 120, 214, 256) 590080      block3_conv1[0][0]\n        ____________________________________________________________________________________________________\n        block3_conv3 (Convolution2D)     (None, 120, 214, 256) 590080      block3_conv2[0][0]\n        ____________________________________________________________________________________________________\n        block3_conv4 (Convolution2D)     (None, 120, 214, 256) 590080      block3_conv3[0][0]\n        ____________________________________________________________________________________________________\n        block3_pool (MaxPooling2D)       (None, 60, 107, 256)  0           block3_conv4[0][0]\n        ____________________________________________________________________________________________________\n        block4_conv1 (Convolution2D)     (None, 60, 107, 512)  1180160     block3_pool[0][0]\n        ____________________________________________________________________________________________________\n        block4_conv2 (Convolution2D)     (None, 60, 107, 512)  2359808     block4_conv1[0][0]\n        ____________________________________________________________________________________________________\n        block4_conv3 (Convolution2D)     (None, 60, 107, 512)  2359808     block4_conv2[0][0]\n        ____________________________________________________________________________________________________\n        block4_conv4 (Convolution2D)     (None, 60, 107, 512)  2359808     block4_conv3[0][0]\n        ____________________________________________________________________________________________________\n        block4_pool (MaxPooling2D)       (None, 30, 54, 512)   0           block4_conv4[0][0]\n        ____________________________________________________________________________________________________\n        block5_conv1 (Convolution2D)     (None, 30, 54, 512)   2359808     block4_pool[0][0]\n        ____________________________________________________________________________________________________\n        block5_conv2 (Convolution2D)     (None, 30, 54, 512)   2359808     block5_conv1[0][0]\n        ____________________________________________________________________________________________________\n        block5_conv3 (Convolution2D)     (None, 30, 54, 512)   2359808     block5_conv2[0][0]\n        ____________________________________________________________________________________________________\n        block5_conv4 (Convolution2D)     (None, 30, 54, 512)   2359808     block5_conv3[0][0]\n        ____________________________________________________________________________________________________\n        block5_pool (MaxPooling2D)       (None, 15, 27, 512)   0           block5_conv4[0][0]\n        ____________________________________________________________________________________________________\n        flatten (Flatten)                (None, 207360)        0           block5_pool[0][0]\n        ____________________________________________________________________________________________________\n        fc1 (Dense)                      (None, 4096)          xxx         flatten[0][0]\n        ____________________________________________________________________________________________________\n        fc2 (Dense)                      (None, 4096)          yyy         fc1[0][0]\n        ____________________________________________________________________________________________________\n        predictions (Dense)              (None, 1000)          zzz         fc2[0][0]\n        ====================================================================================================\n    Original Code:\n        ETH Zurich\n    """"""\n    im_size = tf.shape(inputs)\n\n    with tf.variable_scope(segnet_stream, segnet_stream, [inputs]) as sc:\n        end_points_collection = sc.name + \'_end_points\'\n        # Collect outputs of all intermediate layers.\n        # Make sure convolution and max-pooling layers use SAME padding by default\n        # Also, group all end points in the same container/collection\n        with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                            padding=\'SAME\',\n                            outputs_collections=end_points_collection):\n\n            # VGG16 stage 1 has 2 convolution blocks followed by max-pooling\n            net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n            net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n\n            # VGG16 stage 2 has 2 convolution blocks followed by max-pooling\n            net_2 = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n            net = slim.max_pool2d(net_2, [2, 2], scope=\'pool2\')\n\n            # VGG16 stage 3 has 3 convolution blocks followed by max-pooling\n            net_3 = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n            net = slim.max_pool2d(net_3, [2, 2], scope=\'pool3\')\n\n            # VGG16 stage 4 has 3 convolution blocks followed by max-pooling\n            net_4 = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n            net = slim.max_pool2d(net_4, [2, 2], scope=\'pool4\')\n\n            # VGG16 stage 5 has 3 convolution blocks...\n            net_5 = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n            # ...but here, it is not followed by max-pooling, as in the original VGG16 architecture.\n\n            # This is where the specialization of the VGG network takes place, as described in DRIU and\n            # OSVOS-S. The idea is to extract *side feature maps* and design *specialized layers* to perform\n            # *deep supervision* targeted at a different task (here, segmentation) than the one used to\n            # train the base network originally (i.e., large-scale natural image classification).\n\n            # As explained in DRIU, each specialized side output produces feature maps in 16 different channels,\n            # which are resized to the original image size and concatenated, creating a volume of fine-to-coarse\n            # feature maps. one last convolutional layer linearly combines the feature maps from the volume\n            # created by the specialized side outputs into a regressed result.  The convolutional layers employ\n            # 3 x 3 convolutional filters for efficiency, except the ones used for linearly combining the outputs\n            # (1 x 1 filters).\n\n            with slim.arg_scope([slim.conv2d], activation_fn=None):\n\n                # Convolve last layer of stage 2 (before max-pooling) -> side_2 (None, 240, 427, 16)\n                side_2 = slim.conv2d(net_2, 16, [3, 3], scope=\'conv2_2_16\')\n\n                # Convolve last layer of stage 3 (before max-pooling) -> side_3 (None, 120, 214, 16)\n                side_3 = slim.conv2d(net_3, 16, [3, 3], scope=\'conv3_3_16\')\n\n                # Convolve last layer of stage 4 (before max-pooling) -> side_3 (None, 60, 117, 16)\n                side_4 = slim.conv2d(net_4, 16, [3, 3], scope=\'conv4_3_16\')\n\n                # Convolve last layer of stage 3 (before max-pooling) -> side_3 (None, 30, 54, 16)\n                side_5 = slim.conv2d(net_5, 16, [3, 3], scope=\'conv5_3_16\')\n\n                # The _S layears are the side output that will be used for deep supervision\n\n                # Dim reduction - linearly combine side_2 feature maps -> side_2_s (None, 240, 427, 1)\n                side_2_s = slim.conv2d(side_2, 1, [1, 1], scope=\'score-dsn_2\')\n\n                # Dim reduction - linearly combine side_3 feature maps -> side_3_s (None, 120, 214, 1)\n                side_3_s = slim.conv2d(side_3, 1, [1, 1], scope=\'score-dsn_3\')\n\n                # Dim reduction - linearly combine side_4 feature maps -> side_4_s (None, 60, 117, 1)\n                side_4_s = slim.conv2d(side_4, 1, [1, 1], scope=\'score-dsn_4\')\n\n                # Dim reduction - linearly combine side_5 feature maps -> side_5_s (None, 30, 54, 1)\n                side_5_s = slim.conv2d(side_5, 1, [1, 1], scope=\'score-dsn_5\')\n\n                # As repeated in OSVOS-S, upscaling operations take place wherever necessary, and feature\n                # maps from the separate paths are concatenated to construct a volume with information from\n                # different levels of detail. We linearly fuse the feature maps to a single output which has\n                # the same dimensions as the input image.\n                with slim.arg_scope([slim.convolution2d_transpose],\n                                    activation_fn=None, biases_initializer=None, padding=\'VALID\',\n                                    outputs_collections=end_points_collection, trainable=False):\n\n                    # Upsample the side outputs for deep supervision and center-cop them to the same size as\n                    # the input. Note that this is straight upsampling (we\'re not trying to learn upsampling\n                    # filters), hence the trainable=False param.\n\n                    # Upsample side_2_s (None, 240, 427, 1) -> (None, 480, 854, 1)\n                    # Center-crop (None, 480, 854, 1) to original image size (None, 480, 854, 1)\n                    side_2_s = slim.convolution2d_transpose(side_2_s, 1, 4, 2, scope=\'score-dsn_2-up\')\n                    side_2_s = crop_features(side_2_s, im_size)\n                    utils.collect_named_outputs(end_points_collection, segnet_stream + \'/score-dsn_2-cr\', side_2_s)\n\n                    # Upsample side_3_s (None, 120, 214, 1) -> (None, 484, 860, 1)\n                    # Center-crop (None, 484, 860, 1) to original image size (None, 480, 854, 1)\n                    side_3_s = slim.convolution2d_transpose(side_3_s, 1, 8, 4, scope=\'score-dsn_3-up\')\n                    side_3_s = crop_features(side_3_s, im_size)\n                    utils.collect_named_outputs(end_points_collection, segnet_stream + \'/score-dsn_3-cr\', side_3_s)\n\n                    # Upsample side_4_s (None, 60, 117, 1) -> (None, 488, 864, 1)\n                    # Center-crop (None, 488, 864, 1) to original image size (None, 480, 854, 1)\n                    side_4_s = slim.convolution2d_transpose(side_4_s, 1, 16, 8, scope=\'score-dsn_4-up\')\n                    side_4_s = crop_features(side_4_s, im_size)\n                    utils.collect_named_outputs(end_points_collection, segnet_stream + \'/score-dsn_4-cr\', side_4_s)\n\n                    # Upsample side_5_s (None, 30, 54, 1) -> (None, 496, 880, 1)\n                    # Center-crop (None, 496, 880, 1) to original image size (None, 480, 854, 1)\n                    side_5_s = slim.convolution2d_transpose(side_5_s, 1, 32, 16, scope=\'score-dsn_5-up\')\n                    side_5_s = crop_features(side_5_s, im_size)\n                    utils.collect_named_outputs(end_points_collection, segnet_stream + \'/score-dsn_5-cr\', side_5_s)\n\n                    # Upsample the main outputs and center-cop them to the same size as the input\n                    # Note that this is straight upsampling (we\'re not trying to learn upsampling filters),\n                    # hence the trainable=False param. Then, concatenate thm in a big volume of fine-to-coarse\n                    # feature maps of the same size.\n\n                    # Upsample side_2 (None, 240, 427, 16) -> side_2_f (None, 480, 854, 16)\n                    # Center-crop (None, 480, 854, 16) to original image size (None, 480, 854, 16)\n                    side_2_f = slim.convolution2d_transpose(side_2, 16, 4, 2, scope=\'score-multi2-up\')\n                    side_2_f = crop_features(side_2_f, im_size)\n                    utils.collect_named_outputs(end_points_collection, segnet_stream + \'/side-multi2-cr\', side_2_f)\n\n                    # Upsample side_2 (None, 120, 214, 16) -> side_2_f (None, 488, 864, 16)\n                    # Center-crop (None, 488, 864, 16) to original image size (None, 480, 854, 16)\n                    side_3_f = slim.convolution2d_transpose(side_3, 16, 8, 4, scope=\'score-multi3-up\')\n                    side_3_f = crop_features(side_3_f, im_size)\n                    utils.collect_named_outputs(end_points_collection, segnet_stream + \'/side-multi3-cr\', side_3_f)\n\n                    # Upsample side_2 (None, 60, 117, 16) -> side_2_f (None, 488, 864, 16)\n                    # Center-crop (None, 488, 864, 16) to original image size (None, 480, 854, 16)\n                    side_4_f = slim.convolution2d_transpose(side_4, 16, 16, 8, scope=\'score-multi4-up\')\n                    side_4_f = crop_features(side_4_f, im_size)\n                    utils.collect_named_outputs(end_points_collection, segnet_stream + \'/side-multi4-cr\', side_4_f)\n\n                    # Upsample side_2 (None, 30, 54, 16) -> side_2_f (None, 496, 880, 16)\n                    # Center-crop (None, 496, 880, 16) to original image size (None, 480, 854, 16)\n                    side_5_f = slim.convolution2d_transpose(side_5, 16, 32, 16, scope=\'score-multi5-up\')\n                    side_5_f = crop_features(side_5_f, im_size)\n                    utils.collect_named_outputs(end_points_collection, segnet_stream + \'/side-multi5-cr\', side_5_f)\n\n                # Build the main volume concat_side (None, 496, 880, 16x4)\n                concat_side = tf.concat([side_2_f, side_3_f, side_4_f, side_5_f], axis=3)\n\n                # Dim reduction - linearly combine concat_side feature maps -> (None, 496, 880, 1)\n                net = slim.conv2d(concat_side, 1, [1, 1], scope=\'upscore-fuse\')\n\n                # Note that the FC layers of the original VGG16 network are not part of the DRIU architecture\n\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        return net, end_points\n\n\ndef upsample_filt(size):\n    factor = (size + 1) // 2\n    if size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = np.ogrid[:size, :size]\n    return (1 - abs(og[0] - center) / factor) * \\\n           (1 - abs(og[1] - center) / factor)\n\n\n# Set deconvolutional layers to compute bilinear interpolation\ndef interp_surgery(variables):\n    interp_tensors = []\n    for v in variables:\n        if \'-up\' in v.name:\n            h, w, k, m = v.get_shape()\n            tmp = np.zeros((m, k, h, w))\n            if m != k:\n                raise ValueError(\'input + output channels need to be the same\')\n            if h != w:\n                raise ValueError(\'filters need to be square\')\n            up_filter = upsample_filt(int(h))\n            tmp[range(m), range(k), :, :] = up_filter\n            interp_tensors.append(tf.assign(v, tmp.transpose((2, 3, 1, 0)), validate_shape=True, use_locking=True))\n    return interp_tensors\n\n# TODO: Move preprocessing to Tensorflow API?\ndef preprocess_inputs(inputs, segnet_stream=\'weak\'):\n    """"""Preprocess the inputs to adapt them to the network requirements\n    Args:\n        Image we want to input to the network in (batch_size,W,H,3) or (batch_size,W,H,4) np array\n    Returns:\n        Image ready to input to the network with means substracted\n    """"""\n    assert(len(inputs.shape) == 4)\n\n    if segnet_stream == \'weak\':\n        new_inputs = np.subtract(inputs.astype(np.float32), np.array((104.00699, 116.66877, 122.67892, 128.), dtype=np.float32))\n    else:\n        new_inputs = np.subtract(inputs.astype(np.float32), np.array((104.00699, 116.66877, 122.67892), dtype=np.float32))\n    # input = tf.subtract(tf.cast(input, tf.float32), np.array((104.00699, 116.66877, 122.67892), dtype=np.float32))\n    # input = np.expand_dims(input, axis=0)\n    return new_inputs\n\n\n# TODO: Move preprocessing to Tensorflow API?\ndef preprocess_labels(labels):\n    """"""Preprocess the labels to adapt them to the loss computation requirements\n    Args:\n        Labels (batch_size,W,H) or (batch_size,W,H,1) in numpy array\n    Returns:\n        Label ready to compute the loss (batch_size,W,H,1)\n    """"""\n    assert(len(labels.shape) == 4)\n\n    max_mask = np.max(labels) * 0.5\n    labels = np.greater(labels, max_mask).astype(np.float32)\n    if len(labels.shape) == 3:\n        labels = np.expand_dims(labels, axis=-1)\n    # label = tf.cast(np.array(label), tf.float32)\n    # max_mask = tf.multiply(tf.reduce_max(label), 0.5)\n    # label = tf.cast(tf.greater(label, max_mask), tf.float32)\n    # label = tf.expand_dims(tf.expand_dims(label, 0), 3)\n    return labels\n\n\ndef load_vgg_imagenet(ckpt_path, segnet_stream=\'weak\'):\n    """"""Initialize the network parameters from the VGG-16 pre-trained model provided by TF-SLIM\n    Args:\n        Path to the checkpoint, either the 3-channel or 4-channel input version\n        segnet_stream: Is this the 3-channel or the 4-channel input version?\n    Returns:\n        Function that takes a session and initializes the network\n    """"""\n    assert(segnet_stream in [\'weak\',\'full\'])\n    reader = tf.train.NewCheckpointReader(ckpt_path)\n    var_to_shape_map = reader.get_variable_to_shape_map()\n    vars_corresp = dict()\n    for v in var_to_shape_map:\n        if ""conv"" in v:\n            vars_corresp[v] = slim.get_model_variables(v.replace(""vgg_16"", segnet_stream))[0]\n    init_fn = slim.assign_from_checkpoint_fn(ckpt_path, vars_corresp)\n    return init_fn\n\ndef class_balanced_cross_entropy_loss(output, label):\n    """"""Define the class balanced cross entropy loss to train the network\n    Args:\n    output: Output of the network\n    label: Ground truth label\n    Returns:\n    Tensor that evaluates the loss\n    """"""\n\n    labels = tf.cast(tf.greater(label, 0.5), tf.float32)\n\n    num_labels_pos = tf.reduce_sum(labels)\n    num_labels_neg = tf.reduce_sum(1.0 - labels)\n    num_total = num_labels_pos + num_labels_neg\n\n    output_gt_zero = tf.cast(tf.greater_equal(output, 0), tf.float32)\n    loss_val = tf.multiply(output, (labels - output_gt_zero)) - tf.log(\n        1 + tf.exp(output - 2 * tf.multiply(output, output_gt_zero)))\n\n    loss_pos = tf.reduce_sum(-tf.multiply(labels, loss_val))\n    loss_neg = tf.reduce_sum(-tf.multiply(1.0 - labels, loss_val))\n\n    final_loss = num_labels_neg / num_total * loss_pos + num_labels_pos / num_total * loss_neg\n\n    return final_loss\n\n\ndef class_balanced_cross_entropy_loss_theoretical(output, label):\n    """"""Theoretical version of the class balanced cross entropy loss to train the network (Produces unstable results)\n    Args:\n    output: Output of the network\n    label: Ground truth label\n    Returns:\n    Tensor that evaluates the loss\n    """"""\n    output = tf.nn.sigmoid(output)\n\n    labels_pos = tf.cast(tf.greater(label, 0), tf.float32)\n    labels_neg = tf.cast(tf.less(label, 1), tf.float32)\n\n    num_labels_pos = tf.reduce_sum(labels_pos)\n    num_labels_neg = tf.reduce_sum(labels_neg)\n    num_total = num_labels_pos + num_labels_neg\n\n    loss_pos = tf.reduce_sum(tf.multiply(labels_pos, tf.log(output + 0.00001)))\n    loss_neg = tf.reduce_sum(tf.multiply(labels_neg, tf.log(1 - output + 0.00001)))\n\n    final_loss = -num_labels_neg / num_total * loss_pos - num_labels_pos / num_total * loss_neg\n\n    return final_loss\n\n\ndef load_caffe_weights(weights_path):\n    """"""Initialize the network parameters from a .npy caffe weights file\n    Args:\n    Path to the .npy file containing the value of the network parameters\n    Returns:\n    Function that takes a session and initializes the network\n    """"""\n    osvos_weights = np.load(weights_path).item()\n    vars_corresp = dict()\n    vars_corresp[\'osvos/conv1/conv1_1/weights\'] = osvos_weights[\'conv1_1_w\']\n    vars_corresp[\'osvos/conv1/conv1_1/biases\'] = osvos_weights[\'conv1_1_b\']\n    vars_corresp[\'osvos/conv1/conv1_2/weights\'] = osvos_weights[\'conv1_2_w\']\n    vars_corresp[\'osvos/conv1/conv1_2/biases\'] = osvos_weights[\'conv1_2_b\']\n\n    vars_corresp[\'osvos/conv2/conv2_1/weights\'] = osvos_weights[\'conv2_1_w\']\n    vars_corresp[\'osvos/conv2/conv2_1/biases\'] = osvos_weights[\'conv2_1_b\']\n    vars_corresp[\'osvos/conv2/conv2_2/weights\'] = osvos_weights[\'conv2_2_w\']\n    vars_corresp[\'osvos/conv2/conv2_2/biases\'] = osvos_weights[\'conv2_2_b\']\n\n    vars_corresp[\'osvos/conv3/conv3_1/weights\'] = osvos_weights[\'conv3_1_w\']\n    vars_corresp[\'osvos/conv3/conv3_1/biases\'] = osvos_weights[\'conv3_1_b\']\n    vars_corresp[\'osvos/conv3/conv3_2/weights\'] = osvos_weights[\'conv3_2_w\']\n    vars_corresp[\'osvos/conv3/conv3_2/biases\'] = osvos_weights[\'conv3_2_b\']\n    vars_corresp[\'osvos/conv3/conv3_3/weights\'] = osvos_weights[\'conv3_3_w\']\n    vars_corresp[\'osvos/conv3/conv3_3/biases\'] = osvos_weights[\'conv3_3_b\']\n\n    vars_corresp[\'osvos/conv4/conv4_1/weights\'] = osvos_weights[\'conv4_1_w\']\n    vars_corresp[\'osvos/conv4/conv4_1/biases\'] = osvos_weights[\'conv4_1_b\']\n    vars_corresp[\'osvos/conv4/conv4_2/weights\'] = osvos_weights[\'conv4_2_w\']\n    vars_corresp[\'osvos/conv4/conv4_2/biases\'] = osvos_weights[\'conv4_2_b\']\n    vars_corresp[\'osvos/conv4/conv4_3/weights\'] = osvos_weights[\'conv4_3_w\']\n    vars_corresp[\'osvos/conv4/conv4_3/biases\'] = osvos_weights[\'conv4_3_b\']\n\n    vars_corresp[\'osvos/conv5/conv5_1/weights\'] = osvos_weights[\'conv5_1_w\']\n    vars_corresp[\'osvos/conv5/conv5_1/biases\'] = osvos_weights[\'conv5_1_b\']\n    vars_corresp[\'osvos/conv5/conv5_2/weights\'] = osvos_weights[\'conv5_2_w\']\n    vars_corresp[\'osvos/conv5/conv5_2/biases\'] = osvos_weights[\'conv5_2_b\']\n    vars_corresp[\'osvos/conv5/conv5_3/weights\'] = osvos_weights[\'conv5_3_w\']\n    vars_corresp[\'osvos/conv5/conv5_3/biases\'] = osvos_weights[\'conv5_3_b\']\n\n    vars_corresp[\'osvos/conv2_2_16/weights\'] = osvos_weights[\'conv2_2_16_w\']\n    vars_corresp[\'osvos/conv2_2_16/biases\'] = osvos_weights[\'conv2_2_16_b\']\n    vars_corresp[\'osvos/conv3_3_16/weights\'] = osvos_weights[\'conv3_3_16_w\']\n    vars_corresp[\'osvos/conv3_3_16/biases\'] = osvos_weights[\'conv3_3_16_b\']\n    vars_corresp[\'osvos/conv4_3_16/weights\'] = osvos_weights[\'conv4_3_16_w\']\n    vars_corresp[\'osvos/conv4_3_16/biases\'] = osvos_weights[\'conv4_3_16_b\']\n    vars_corresp[\'osvos/conv5_3_16/weights\'] = osvos_weights[\'conv5_3_16_w\']\n    vars_corresp[\'osvos/conv5_3_16/biases\'] = osvos_weights[\'conv5_3_16_b\']\n\n    vars_corresp[\'osvos/score-dsn_2/weights\'] = osvos_weights[\'score-dsn_2_w\']\n    vars_corresp[\'osvos/score-dsn_2/biases\'] = osvos_weights[\'score-dsn_2_b\']\n    vars_corresp[\'osvos/score-dsn_3/weights\'] = osvos_weights[\'score-dsn_3_w\']\n    vars_corresp[\'osvos/score-dsn_3/biases\'] = osvos_weights[\'score-dsn_3_b\']\n    vars_corresp[\'osvos/score-dsn_4/weights\'] = osvos_weights[\'score-dsn_4_w\']\n    vars_corresp[\'osvos/score-dsn_4/biases\'] = osvos_weights[\'score-dsn_4_b\']\n    vars_corresp[\'osvos/score-dsn_5/weights\'] = osvos_weights[\'score-dsn_5_w\']\n    vars_corresp[\'osvos/score-dsn_5/biases\'] = osvos_weights[\'score-dsn_5_b\']\n\n    vars_corresp[\'osvos/upscore-fuse/weights\'] = osvos_weights[\'new-score-weighting_w\']\n    vars_corresp[\'osvos/upscore-fuse/biases\'] = osvos_weights[\'new-score-weighting_b\']\n    return slim.assign_from_values_fn(vars_corresp)\n\n\ndef parameter_lr(segnet_stream=\'weak\'):\n    """"""Specify the relative learning rate for every parameter. The final learning rate\n    in every parameter will be the one defined here multiplied by the global one\n    Args:\n        segnet_stream: Is this the 3-channel or the 4-channel input version?\n    Returns:\n        Dictionary with the relative learning rate for every parameter\n    """"""\n    assert(segnet_stream in [\'weak\',\'full\'])\n    vars_corresp = dict()\n    vars_corresp[segnet_stream + \'/conv1/conv1_1/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv1/conv1_1/biases\'] = 2\n    vars_corresp[segnet_stream + \'/conv1/conv1_2/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv1/conv1_2/biases\'] = 2\n\n    vars_corresp[segnet_stream + \'/conv2/conv2_1/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv2/conv2_1/biases\'] = 2\n    vars_corresp[segnet_stream + \'/conv2/conv2_2/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv2/conv2_2/biases\'] = 2\n\n    vars_corresp[segnet_stream + \'/conv3/conv3_1/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv3/conv3_1/biases\'] = 2\n    vars_corresp[segnet_stream + \'/conv3/conv3_2/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv3/conv3_2/biases\'] = 2\n    vars_corresp[segnet_stream + \'/conv3/conv3_3/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv3/conv3_3/biases\'] = 2\n\n    vars_corresp[segnet_stream + \'/conv4/conv4_1/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv4/conv4_1/biases\'] = 2\n    vars_corresp[segnet_stream + \'/conv4/conv4_2/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv4/conv4_2/biases\'] = 2\n    vars_corresp[segnet_stream + \'/conv4/conv4_3/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv4/conv4_3/biases\'] = 2\n\n    vars_corresp[segnet_stream + \'/conv5/conv5_1/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv5/conv5_1/biases\'] = 2\n    vars_corresp[segnet_stream + \'/conv5/conv5_2/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv5/conv5_2/biases\'] = 2\n    vars_corresp[segnet_stream + \'/conv5/conv5_3/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv5/conv5_3/biases\'] = 2\n\n    vars_corresp[segnet_stream + \'/conv2_2_16/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv2_2_16/biases\'] = 2\n    vars_corresp[segnet_stream + \'/conv3_3_16/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv3_3_16/biases\'] = 2\n    vars_corresp[segnet_stream + \'/conv4_3_16/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv4_3_16/biases\'] = 2\n    vars_corresp[segnet_stream + \'/conv5_3_16/weights\'] = 1\n    vars_corresp[segnet_stream + \'/conv5_3_16/biases\'] = 2\n\n    vars_corresp[segnet_stream + \'/score-dsn_2/weights\'] = 0.1\n    vars_corresp[segnet_stream + \'/score-dsn_2/biases\'] = 0.2\n    vars_corresp[segnet_stream + \'/score-dsn_3/weights\'] = 0.1\n    vars_corresp[segnet_stream + \'/score-dsn_3/biases\'] = 0.2\n    vars_corresp[segnet_stream + \'/score-dsn_4/weights\'] = 0.1\n    vars_corresp[segnet_stream + \'/score-dsn_4/biases\'] = 0.2\n    vars_corresp[segnet_stream + \'/score-dsn_5/weights\'] = 0.1\n    vars_corresp[segnet_stream + \'/score-dsn_5/biases\'] = 0.2\n\n    vars_corresp[segnet_stream + \'/upscore-fuse/weights\'] = 0.01\n    vars_corresp[segnet_stream + \'/upscore-fuse/biases\'] = 0.02\n    return vars_corresp\n\n\ndef _train(dataset, initial_ckpt, supervison, learning_rate, logs_path, max_training_iters, save_step, display_step,\n           global_step, segnet_stream=\'weak\', iter_mean_grad=1, batch_size=1, momentum=0.9, resume_training=False, config=None, finetune=1,\n           test_image_path=None, ckpt_name=\'weak\'):\n    """"""Train OSVOS\n    Args:\n    dataset: Reference to a Dataset object instance\n    initial_ckpt: Path to the checkpoint to initialize the network (May be parent network or pre-trained Imagenet)\n    supervison: Level of the side outputs supervision: 1-Strong 2-Weak 3-No supervision\n    learning_rate: Value for the learning rate. It can be a number or an instance to a learning rate object.\n    logs_path: Path to store the checkpoints\n    max_training_iters: Number of training iterations\n    save_step: A checkpoint will be created every save_steps\n    display_step: Information of the training will be displayed every display_steps\n    global_step: Reference to a Variable that keeps track of the training steps\n    segnet_stream: Binary segmentation network stream; either ""appearance stream"" or ""flow stream"" [\'weak\'|\'full\']\n    iter_mean_grad: Number of gradient computations that are average before updating the weights\n    batch_size: Size of the training batch\n    momentum: Value of the momentum parameter for the Momentum optimizer\n    resume_training: Boolean to try to restore from a previous checkpoint (True) or not (False)\n    config: Reference to a Configuration object used in the creation of a Session\n    finetune: Use to select the type of training, 0 for the parent network and 1 for finetunning\n    test_image_path: If image path provided, every save_step the result of the network with this image is stored\n    ckpt_name: Checkpoint name\n    Returns:\n    """"""\n    model_name = os.path.join(logs_path, ckpt_name+"".ckpt"")\n    if config is None:\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        # config.log_device_placement = True\n        config.allow_soft_placement = True\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    # Prepare the input data:\n    # Section ""3.3 Binary Segmentation"" of the MaskRNN paper and ""Figure 2"" are inconsistent when it comes to describing\n    # the inputs of the two-stream network. In this implementation, we chose the input of the appearance stream\n    # \'weak\' to be the concatenation of the current frame I<sub>t</sub> and the warped prediction of the previous\n    # frame\'s segmentation mask b<sub>t-1</sub>, denoted as Phi<sub>t-1,t</sub>(b<sub>t-1</sub>). The warping function\n    # Phi<sub>t-1,t</sub>(.) transforms the input based on the optical flow fields from frame I<sub>t-1</sub> to\n    # frame I<sub>t</sub>.\n    # We chose the input of the flow stream \'full\' to be the concatenation of the magnitude of the flow field from\n    # I<sub>t-1</sub> to I<sub>t</sub> and I<sub>t</sub> to frame I<sub>t+1</sub> and, again, the warped prediction\n    # of the previous frame\'s segmentation mask b<sub>t-1</sub>.\n    # The architecture of both streams is identical.\n    assert(segnet_stream in [\'weak\',\'full\'])\n    if segnet_stream == \'weak\':\n        input_image = tf.placeholder(tf.float32, [batch_size, None, None, 4])\n    else:\n        input_image = tf.placeholder(tf.float32, [batch_size, None, None, 3])\n    input_label = tf.placeholder(tf.float32, [batch_size, None, None, 1])\n\n    # Create the convnet\n    with slim.arg_scope(backbone_arg_scope()):\n        net, end_points = backbone(input_image, segnet_stream)\n\n    # Print name and shape of each tensor.\n    print(""Network Layers:"")\n    for k, v in end_points.items():\n        print(\'   name = {}, shape = {}\'.format(v.name, v.get_shape()))\n\n    # Print name and shape of parameter nodes (values not yet initialized)\n    print(""Network Parameters:"")\n    for v in slim.get_model_variables():\n        print(\'   name = {}, shape = {}\'.format(v.name, v.get_shape()))\n\n    # Initialize weights from pre-trained model\n    if finetune == 0:\n        init_weights = load_vgg_imagenet(initial_ckpt, segnet_stream)\n\n    # Define loss\n    with tf.name_scope(\'losses\'):\n        if supervison == 1 or supervison == 2:\n            dsn_2_loss = class_balanced_cross_entropy_loss(end_points[segnet_stream + \'/score-dsn_2-cr\'], input_label)\n            tf.summary.scalar(\'dsn_2_loss\', dsn_2_loss)\n            dsn_3_loss = class_balanced_cross_entropy_loss(end_points[segnet_stream + \'/score-dsn_3-cr\'], input_label)\n            tf.summary.scalar(\'dsn_3_loss\', dsn_3_loss)\n            dsn_4_loss = class_balanced_cross_entropy_loss(end_points[segnet_stream + \'/score-dsn_4-cr\'], input_label)\n            tf.summary.scalar(\'dsn_4_loss\', dsn_4_loss)\n            dsn_5_loss = class_balanced_cross_entropy_loss(end_points[segnet_stream + \'/score-dsn_5-cr\'], input_label)\n            tf.summary.scalar(\'dsn_5_loss\', dsn_5_loss)\n\n        main_loss = class_balanced_cross_entropy_loss(net, input_label)\n        tf.summary.scalar(\'main_loss\', main_loss)\n\n        if supervison == 1:\n            output_loss = dsn_2_loss + dsn_3_loss + dsn_4_loss + dsn_5_loss + main_loss\n        elif supervison == 2:\n            output_loss = 0.5 * dsn_2_loss + 0.5 * dsn_3_loss + 0.5 * dsn_4_loss + 0.5 * dsn_5_loss + main_loss\n        elif supervison == 3:\n            output_loss = main_loss\n        else:\n            sys.exit(\'Incorrect supervision id, select 1 for supervision of the side outputs, 2 for weak supervision \'\n                     \'of the side outputs and 3 for no supervision of the side outputs\')\n        total_loss = output_loss + tf.add_n(tf.losses.get_regularization_losses())\n        tf.summary.scalar(\'total_loss\', total_loss)\n\n    # Define optimization method\n    with tf.name_scope(\'optimization\'):\n        tf.summary.scalar(\'learning_rate\', learning_rate)\n        optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n        grads_and_vars = optimizer.compute_gradients(total_loss)\n        with tf.name_scope(\'grad_accumulator\'):\n            grad_accumulator = {}\n            for ind in range(0, len(grads_and_vars)):\n                if grads_and_vars[ind][0] is not None:\n                    grad_accumulator[ind] = tf.ConditionalAccumulator(grads_and_vars[ind][0].dtype)\n        with tf.name_scope(\'apply_gradient\'):\n            layer_lr = parameter_lr(segnet_stream)\n            grad_accumulator_ops = []\n            for var_ind, grad_acc in grad_accumulator.items(): # Phil: was: for var_ind, grad_acc in grad_accumulator.iteritems():\n                var_name = str(grads_and_vars[var_ind][1].name).split(\':\')[0]\n                var_grad = grads_and_vars[var_ind][0]\n                grad_accumulator_ops.append(grad_acc.apply_grad(var_grad * layer_lr[var_name], local_step=global_step))\n        with tf.name_scope(\'take_gradients\'):\n            mean_grads_and_vars = []\n            for var_ind, grad_acc in grad_accumulator.items(): # Phil: was: for var_ind, grad_acc in grad_accumulator.iteritems():\n                mean_grads_and_vars.append(\n                    (grad_acc.take_grad(iter_mean_grad), grads_and_vars[var_ind][1]))\n            apply_gradient_op = optimizer.apply_gradients(mean_grads_and_vars, global_step=global_step)\n    # Log training info\n    merged_summary_op = tf.summary.merge_all()\n\n    # Log evolution of test image\n    if test_image_path is not None:\n        probabilities = tf.nn.sigmoid(net)\n        img_summary = tf.summary.image(""Output probabilities"", probabilities, max_outputs=1)\n    # Initialize variables\n    init = tf.global_variables_initializer()\n\n    # Create objects to record timing and memory of the graph execution\n    # run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) # Option in the session options=run_options\n    # run_metadata = tf.RunMetadata() # Option in the session run_metadata=run_metadata\n    # summary_writer.add_run_metadata(run_metadata, \'step%d\' % i)\n    with tf.Session(config=config) as sess:\n        print(\'Init variable\')\n        sess.run(init)\n\n        # op to write logs to Tensorboard\n        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n\n        # Create saver to manage checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        last_ckpt_path = tf.train.latest_checkpoint(logs_path)\n        if last_ckpt_path is not None and resume_training:\n            # Load last checkpoint\n            print(\'Initializing from previous checkpoint...\')\n            saver.restore(sess, last_ckpt_path)\n            step = global_step.eval() + 1\n        else:\n            # Load pre-trained model\n            if finetune == 0:\n                print(\'Initializing from pre-trained imagenet model...\')\n                init_weights(sess)\n            else:\n                print(\'Initializing from specified pre-trained model...\')\n                # init_weights(sess)\n                var_list = []\n                for var in tf.global_variables():\n                    var_type = var.name.split(\'/\')[-1]\n                    if \'weights\' in var_type or \'bias\' in var_type:\n                        var_list.append(var)\n                saver_res = tf.train.Saver(var_list=var_list)\n                saver_res.restore(sess, initial_ckpt)\n            step = 1\n        sess.run(interp_surgery(tf.global_variables()))\n        print(\'Weights initialized\')\n\n        print(\'Start training\')\n        while step < max_training_iters + 1:\n            # Average the gradient\n            for _ in range(0, iter_mean_grad):\n                batch_inputs, batch_labels = dataset.next_batch(batch_size, \'train\', segnet_stream)\n                inputs = preprocess_inputs(batch_inputs, segnet_stream)\n                labels = preprocess_labels(batch_labels)\n                run_res = sess.run([total_loss, merged_summary_op] + grad_accumulator_ops,\n                                   feed_dict={input_image: inputs, input_label: labels})\n                batch_loss = run_res[0]\n                summary = run_res[1]\n\n            # Apply the gradients\n            sess.run(apply_gradient_op)  # Momentum updates here its statistics\n\n            # Save summary reports\n            summary_writer.add_summary(summary, step)\n\n            # Display training status\n            if step % display_step == 0:\n                print(""{} Iter {}: Training Loss = {:.4f}"".format(datetime.now(), step, batch_loss))\n\n            # Save a checkpoint\n            if step % save_step == 0:\n                if test_image_path is not None:\n                    curr_output = sess.run(img_summary, feed_dict={input_image: preprocess_inputs(test_image_path, segnet_stream)})\n                    summary_writer.add_summary(curr_output, step)\n                save_path = saver.save(sess, model_name, global_step=global_step)\n                print(""Model saved in file: %s"" % save_path)\n\n            step += 1\n\n        if (step - 1) % save_step != 0:\n            save_path = saver.save(sess, model_name, global_step=global_step)\n            print(""Model saved in file: %s"" % save_path)\n\n        print(\'Finished training.\')\n\n\ndef train_parent(dataset, initial_ckpt, supervison, learning_rate, logs_path, max_training_iters, save_step,\n                 display_step, global_step, segnet_stream=\'full\', iter_mean_grad=1, batch_size=1, momentum=0.9, resume_training=False,\n                 config=None, test_image_path=None, ckpt_name=\'full\'):\n    """"""Train OSVOS parent network\n    Args:\n    See _train()\n    Returns:\n    """"""\n    finetune = 0\n    _train(dataset, initial_ckpt, supervison, learning_rate, logs_path, max_training_iters, save_step, display_step,\n           global_step, segnet_stream, iter_mean_grad, batch_size, momentum, resume_training, config, finetune, test_image_path,\n           ckpt_name)\n\n\ndef train_finetune(dataset, initial_ckpt, supervison, learning_rate, logs_path, max_training_iters, save_step,\n                   display_step, global_step, segnet_stream=\'full\', iter_mean_grad=1, batch_size=1, momentum=0.9, resume_training=False,\n                   config=None, test_image_path=None, ckpt_name=\'full\'):\n    """"""Finetune OSVOS\n    Args:\n    See _train()\n    Returns:\n    """"""\n    finetune = 1\n    _train(dataset, initial_ckpt, supervison, learning_rate, logs_path, max_training_iters, save_step, display_step,\n           global_step, segnet_stream, iter_mean_grad, batch_size, momentum, resume_training, config, finetune, test_image_path,\n           ckpt_name)\n\n\ndef test(dataset, checkpoint_file, pred_masks_path, img_pred_masks_path, segnet_stream=\'full\', config=None):\n    """"""Test one sequence\n    Args:\n        dataset: Reference to a Dataset object instance\n        checkpoint_path: Path of the checkpoint to use for the evaluation\n        segnet_stream: Binary segmentation network stream; either ""appearance stream"" or ""flow stream"" [\'weak\'|\'full\']\n        pred_masks_path: Path to save the individual predicted masks\n        img_pred_masks_path: Path to save the composite of the input image overlayed with the predicted masks\n        config: Reference to a Configuration object used in the creation of a Session\n    Returns:\n    """"""\n    if config is None:\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        # config.log_device_placement = True\n        config.allow_soft_placement = True\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    # Input data\n    assert(segnet_stream in [\'weak\',\'full\'])\n    batch_size = 1\n    if segnet_stream == \'weak\':\n        input_image = tf.placeholder(tf.float32, [batch_size, None, None, 4])\n    else:\n        input_image = tf.placeholder(tf.float32, [batch_size, None, None, 3])\n\n    # Create the convnet\n    with slim.arg_scope(backbone_arg_scope()):\n        net, end_points = backbone(input_image, segnet_stream)\n    probabilities = tf.nn.sigmoid(net)\n    global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\n    # Create a saver to load the network\n    saver = tf.train.Saver([v for v in tf.global_variables() if \'-up\' not in v.name and \'-cr\' not in v.name])\n\n    if not os.path.exists(pred_masks_path):\n        os.makedirs(pred_masks_path)\n    if not os.path.exists(img_pred_masks_path):\n        os.makedirs(img_pred_masks_path)\n\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(interp_surgery(tf.global_variables()))\n        saver.restore(sess, checkpoint_file)\n        rounds, rounds_left = divmod(dataset.test_size, batch_size)\n        if rounds_left:\n            rounds += 1\n        with warnings.catch_warnings():\n            warnings.simplefilter(""ignore"")\n            for _round in trange(rounds, ascii=True, ncols=80, desc=\'Saving predictions as PNGs\'):\n                samples, output_files = dataset.next_batch(batch_size, \'test\', segnet_stream)\n                inputs = preprocess_inputs(samples, segnet_stream)\n                masks = sess.run(probabilities, feed_dict={input_image: inputs})\n                masks = np.where(masks.astype(np.float32) < 162.0/255.0, 0, 255).astype(\'uint8\')\n                for mask, output_file in zip(masks, output_files):\n                    imsave(os.path.join(pred_masks_path, output_file), mask[:, :, 0])\n'"
tfwss/model_test.py,3,"b'""""""\nmodel_test.py\n\nThe SDI Grabcut testing is done using a model trained in the [""Simple Does It"" Grabcut Training for Instance Segmentation](model_train.ipynb) notebook, so make sure you\'ve run that notebook first! We test the model on the **validation** split of the Berkeley-augmented dataset.\n\nThe Berkley augmented dataset can be downloaded from [here](\nhttp://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz).\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n\nBased on:\n  - https://github.com/scaelles/OSVOS-TensorFlow/blob/master/osvos_demo.py\n    Written by Sergi Caelles (scaelles@vision.ee.ethz.ch)\n    This file is part of the OSVOS paper presented in:\n      Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixe, Daniel Cremers, Luc Van Gool\n      One-Shot Video Object Segmentation\n      CVPR 2017\n    Unknown code license\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport sys\nimport tensorflow as tf\nslim = tf.contrib.slim\n\n# Import model files\nimport model\nfrom dataset import BKVOCDataset\n\n# Parameters\ngpu_id = 0\n# Modify the value below to match the value of max_training_iters_3 in the training notebook!\nmax_training_iters = 50000\n\n# Model paths\nsegnet_stream = \'weak\'\nckpt_name = \'vgg_16_4chan_\' + segnet_stream\nckpt_path = \'models/\' + ckpt_name + \'/\' + ckpt_name + \'.ckpt-\' + str(max_training_iters)\n\n# Load the Berkeley-augmented Pascal VOC 2012 segmentation dataset\nif sys.platform.startswith(""win""):\n    dataset_root = ""E:/datasets/bk-voc/benchmark_RELEASE/dataset""\nelse:\n    dataset_root = \'/media/EDrive/datasets/bk-voc/benchmark_RELEASE/dataset\'\ndataset = BKVOCDataset(phase=\'test\', dataset_root=dataset_root)\n\n# Display dataset configuration\ndataset.print_config()\n\n# Test the model\nwith tf.Graph().as_default():\n    with tf.device(\'/gpu:\' + str(gpu_id)):\n        model.test(dataset, ckpt_path, dataset.pred_masks_path, dataset.img_pred_masks_path, segnet_stream)\n\n# Combine original images with predicted instance masks\ndataset.combine_images_with_predicted_masks()'"
tfwss/model_train.py,13,"b'""""""\nmodel_train.py\n\nThis file performs training of the SDI Grabcut weakly supervised model for **instance segmentation**.\nFollowing the instructions provided in Section ""6. Instance Segmentation Results"" of the ""Simple Does It"" paper, we use\nthe Berkeley-augmented Pascal VOC segmentation dataset that provides per-instance segmentation masks for VOC2012 data.\n\nThe Berkley augmented dataset can be downloaded from [here](\nhttp://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz)\n\nThe SDI Grabcut training is done using a **4-channel input** VGG16 network pre-trained on ImageNet, so make sure to run\nthe [`VGG16 Surgery`](vgg16_surgery.ipynb) notebook first!\n\nTo monitor training, run:\n```\n# On Windows\ntensorboard --logdir E:\\repos\\tf-wss\\tfwss\\models\\vgg_16_4chan_weak\n# On Ubuntu\ntensorboard --logdir /media/EDrive/repos/tf-wss/tfwss/models/vgg_16_4chan_weak\nhttp://<hostname>:6006\n```\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n\nBased on:\n  - https://github.com/scaelles/OSVOS-TensorFlow/blob/master/osvos_parent_demo.py\n    Written by Sergi Caelles (scaelles@vision.ee.ethz.ch)\n    This file is part of the OSVOS paper presented in:\n      Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixe, Daniel Cremers, Luc Van Gool\n      One-Shot Video Object Segmentation\n      CVPR 2017\n    Unknown code license\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport os\nimport sys\nimport tensorflow as tf\nslim = tf.contrib.slim\n\n# Import model files\nimport model\nfrom dataset import BKVOCDataset\n\n# Model paths\n# Pre-trained VGG_16 downloaded from http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\nimagenet_ckpt = \'models/vgg_16_4chan/vgg_16_4chan.ckpt\'\nsegnet_stream = \'weak\'\nckpt_name = \'vgg_16_4chan_\' + segnet_stream\nlogs_path = \'models/\' + ckpt_name\n\n# Training parameters\ngpu_id = 0\niter_mean_grad = 10\nmax_training_iters_1 = 15000\nmax_training_iters_2 = 30000\nmax_training_iters_3 = 50000\nsave_step = 5000\ntest_image = None\ndisplay_step = 100\nini_lr = 1e-8\nboundaries = [10000, 15000, 25000, 30000, 40000]\nvalues = [ini_lr, ini_lr * 0.1, ini_lr, ini_lr * 0.1, ini_lr, ini_lr * 0.1]\n\n# Load the Berkeley-augmented Pascal VOC 2012 segmentation dataset\nif sys.platform.startswith(""win""):\n    dataset_root = ""E:/datasets/bk-voc/benchmark_RELEASE/dataset""\nelse:\n    dataset_root = \'/media/EDrive/datasets/bk-voc/benchmark_RELEASE/dataset\'\ndataset = BKVOCDataset(phase=\'train\', dataset_root=dataset_root)\n\n# Display dataset configuration\ndataset.print_config()\n\n# Train the network with strong side outputs supervision\nwith tf.Graph().as_default():\n    with tf.device(\'/gpu:\' + str(gpu_id)):\n        global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n        learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\n        model.train_parent(dataset, imagenet_ckpt, 1, learning_rate, logs_path, max_training_iters_1, save_step,\n                           display_step, global_step, segnet_stream, iter_mean_grad=iter_mean_grad, test_image_path=test_image,\n                           ckpt_name=ckpt_name)\n# Train the network with weak side outputs supervision\nwith tf.Graph().as_default():\n    with tf.device(\'/gpu:\' + str(gpu_id)):\n        global_step = tf.Variable(max_training_iters_1, name=\'global_step\', trainable=False)\n        learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\n        model.train_parent(dataset, imagenet_ckpt, 2, learning_rate, logs_path, max_training_iters_2, save_step,\n                           display_step, global_step, segnet_stream, iter_mean_grad=iter_mean_grad, resume_training=True,\n                           test_image_path=test_image, ckpt_name=ckpt_name)\n# Train the network without side outputs supervision\nwith tf.Graph().as_default():\n    with tf.device(\'/gpu:\' + str(gpu_id)):\n        global_step = tf.Variable(max_training_iters_2, name=\'global_step\', trainable=False)\n        learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\n        model.train_parent(dataset, imagenet_ckpt, 3, learning_rate, logs_path, max_training_iters_3, save_step,\n                           display_step, global_step, segnet_stream, iter_mean_grad=iter_mean_grad, resume_training=True,\n                           test_image_path=test_image, ckpt_name=ckpt_name)\n\n\n'"
tfwss/segment.py,0,"b'""""""\nsegment.py\n\nSegmentation utility functions.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n""""""\n\n# TODO The SDI paper uses Grabcut+ (Grabcut on HED boundaries). How difficult is it to implement?\n# TODO Try this version of Grabcut: https://github.com/meng-tang/KernelCut_ICCV15 ?\n# TODO Try this version of Grabcut: https://github.com/meng-tang/OneCut ?\n\nimport numpy as np\nimport cv2 as cv\n\n_MIN_AREA = 9\n_ITER_COUNT = 5\n_RECT_SHRINK = 3\n\ndef rect_mask(shape, bbox):\n    """"""Given a bbox and a shape, creates a mask (white rectangle foreground, black background)\n    Param:\n        shape: shape (H,W) or (H,W,1)\n        bbox: bbox numpy array [y1, x1, y2, x2]\n    Returns:\n        mask\n    """"""\n    mask = np.zeros(shape[:2], np.uint8)\n    mask[bbox[0]:bbox[2], bbox[1]:bbox[3]] = 255\n    if len(shape) == 3 and shape[2] == 1:\n        mask = np.expand_dims(mask, axis=-1)\n    return mask\n\ndef grabcut(img_path, bbox):\n    """"""Use Grabcut to create a binary segmentation of an image within a bounding box\n    Param:\n        img: path to input image astype(\'uint8\')\n        bbox: bbox numpy array [y1, x1, y2, x2]\n    Returns:\n        mask with binary segmentation astype(\'uint8\')\n    Based on:\n        https://docs.opencv.org/trunk/d8/d83/tutorial_py_grabcut.html\n    """"""\n    img = cv.imread(img_path)\n    width, height = bbox[3] - bbox[1], bbox[2] - bbox[0]\n    if width * height < _MIN_AREA:\n        # OpenCV\'s Grabcut breaks if the rectangle is too small!\n        # This happens with instance mask 2008_002212_4.png\n        # Fix: Draw a filled rectangle at that location, making the assumption everything in the rectangle is foreground\n        assert(width*height > 0)\n        mask = rect_mask(img.shape[:2], bbox)\n    elif width * height == img.shape[0] * img.shape[1]:\n        # If the rectangle covers the entire image, grabCut can\'t distinguish between background and foreground\n        # because it assumes what\'s outside the rect is background (no ""outside"" if the rect is as large as the input)\n        # This happens with instance mask 2008_002638_3.png\n        # Crappy Fix: Shrink the rectangle corners by _RECT_SHRINK on all sides\n        # Use Grabcut to create a segmentation within the bbox\n        rect = (_RECT_SHRINK, _RECT_SHRINK, width - _RECT_SHRINK * 2, height - _RECT_SHRINK * 2)\n        gct = np.zeros(img.shape[:2], np.uint8)\n        bgdModel, fgdModel = np.zeros((1, 65), np.float64), np.zeros((1, 65), np.float64)\n        cv.grabCut(img, gct, rect, bgdModel,fgdModel, _ITER_COUNT, cv.GC_INIT_WITH_RECT)\n        mask = np.where((gct == 2) | (gct == 0), 0, 255).astype(\'uint8\')\n    else:\n        # Use Grabcut to create a segmentation within the bbox\n        rect = (bbox[1], bbox[0], width, height)\n        gct = np.zeros(img.shape[:2], np.uint8)\n        bgdModel, fgdModel = np.zeros((1, 65), np.float64), np.zeros((1, 65), np.float64)\n        cv.grabCut(img, gct, rect, bgdModel,fgdModel, _ITER_COUNT, cv.GC_INIT_WITH_RECT)\n        mask = np.where((gct == 2) | (gct == 0), 0, 255).astype(\'uint8\')\n    return mask\n'"
tfwss/visualize.py,0,"b'""""""\nvisualize.py\n\nDavis 2016 visualization helpers.\n\nWritten by Phil Ferriere\n\nLicensed under the MIT License (see LICENSE for details)\n\nBased on:\n    - https://github.com/matterport/Mask_RCNN/blob/master/visualize.py\n        Copyright (c) 2017 Matterport, Inc. / Written by Waleed Abdulla\n        Licensed under the MIT License\n\nReferences for future work:\n    E:/repos/models-master/research/object_detection/utils/visualization_utils.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom IPython.display import HTML\nimport io, base64\nimport imageio\nimport numpy as np\nimport random\nimport colorsys\nimport matplotlib.pyplot as plt\n\ndef random_colors(N, bright=True, RGB_max=255):\n    """"""\n    Generate random colors. To get visually distinct colors, generate them in HSV space then convert to RGB.\n        Args:\n            N: number of colors to generate.\n            bright: set to True for bright colors.\n            RGB_max: set to 1.0 or 255, based on image type you\'re working with\n    """"""\n    brightness = 1.0 if bright else 0.7\n    hsv = [(i / N, 1, brightness) for i in range(N)]\n    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n    colors = [(color[0] * RGB_max, color[1] * RGB_max, color[2] * RGB_max) for color in colors]\n    random.shuffle(colors)\n    return colors\n\ndef display_images(images, titles=None, cols=4, cmap=None, norm=None, interpolation=None):\n    """"""Display the given set of images, optionally with titles.\n        Args:\n            images: list or array of image tensors in HWC format.\n            titles: optional. A list of titles to display with each image.\n            cols: number of images per row\n            cmap: Optional. Color map to use. For example, ""Blues"".\n            norm: Optional. A Normalize instance to map values to colors.\n            interpolation: Optional. Image interporlation to use for display.\n    """"""\n    titles = titles if titles is not None else [""""] * len(images)\n    rows = len(images) // cols + 1\n    width = 20\n    plt.figure(figsize=(width, width * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        # plt.title(title, fontsize=9)\n        plt.axis(\'off\')\n        plt.imshow(image.astype(np.uint8), cmap=cmap, norm=norm, interpolation=interpolation)\n        i += 1\n    plt.tight_layout()\n    plt.show()\n\ndef draw_box(image, bbox, color, in_place=True):\n    """"""Draw (in-place, or not) 3-pixel-width bounding bboxes on an image.\n        Args:\n            image: video frame (H,W,3)\n            bbox: y1, x1, y2, x2 bounding box\n            color: color list of 3 int values for RGB\n            in_place: in place / copy flag\n        Returns:\n            image with bounding box\n    """"""\n    y1, x1, y2, x2 = bbox\n    result = image if in_place == True else np.copy(image)\n    result[y1:y1 + 2, x1:x2] = color\n    result[y2:y2 + 2, x1:x2] = color\n    result[y1:y2, x1:x1 + 2] = color\n    result[y1:y2, x2:x2 + 2] = color\n    return result\n\ndef draw_mask(image, mask, color, alpha=0.5, in_place=False):\n    """"""Draw (in-place, or not) a mask on an image.\n        Args:\n            image: input image (H,W,3)\n            mask: mask (H,W,1)\n            color: color list of 3 int values for RGB\n            alpha: alpha blending level\n            in_place: in place / copy flag\n        Returns:\n            image with mask\n    """"""\n    assert(len(image.shape) == len(mask.shape) == len(color) == 3)\n    assert(image.shape[0] == mask.shape[0] and image.shape[1] == mask.shape[1])\n    threshold = (np.max(mask) - np.min(mask)) / 2\n    multiplier = 1 if np.amax(color) > 1 else 255\n    masked_image = image if in_place == True else np.copy(image)\n    for c in range(3):\n        masked_image[:, :, c] = np.where(mask[:,:,0] > threshold,\n                                         masked_image[:, :, c] *\n                                         (1 - alpha) + alpha * color[c] * multiplier,\n                                         masked_image[:, :, c])\n    return masked_image\n\ndef draw_masks(image, bboxes, masks, alpha=0.5, in_place=True):\n    """"""Apply the given instance masks to the image and draw their bboxes.\n        Args:\n            image: input image (H, W, 3)\n            bboxes: (num_instances, (y1, x1, y2, x2)) bounding boxes as numpy array\n            masks: masks (num_instances, H, W, 1) as numpy array\n            alpha: alpha blending level\n            in_place: in place / copy flag\n        Returns:\n            image with masks overlaid\n    """"""\n    # Number of instances\n    num_instances = bboxes.shape[0]\n    assert(num_instances == masks.shape[0])\n\n    # Make a copy of the input image, if requested\n    masked_image = image if in_place == True else np.copy(image)\n\n    # Draw bboxes and masks on the image, if the bbox is not empty, using a random color\n    colors = random_colors(num_instances)\n    for instance in range(num_instances):\n        if not np.any(bboxes[instance]):\n            continue\n        color = colors[instance]\n        draw_mask(masked_image, masks[instance], color, alpha=alpha, in_place=True)\n        # draw_mask(masked_image, masks[instance, :, :, 0], color, alpha=alpha, in_place=True)\n        draw_box(masked_image, bboxes[instance], color, in_place=True)\n\n    return masked_image\n'"
tfwss/tools/inspect_checkpoint.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A simple script for inspect checkpoint files.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nimport numpy as np\n\nfrom tensorflow.python import pywrap_tensorflow\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import flags\n\nFLAGS = None\n\n\ndef print_tensors_in_checkpoint_file(file_name, tensor_name, all_tensors,\n                                     all_tensor_names):\n  """"""Prints tensors in a checkpoint file.\n\n  If no `tensor_name` is provided, prints the tensor names and shapes\n  in the checkpoint file.\n\n  If `tensor_name` is provided, prints the content of the tensor.\n\n  Args:\n    file_name: Name of the checkpoint file.\n    tensor_name: Name of the tensor in the checkpoint file to print.\n    all_tensors: Boolean indicating whether to print all tensors.\n    all_tensor_names: Boolean indicating whether to print all tensor names.\n  """"""\n  try:\n    reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n    debug_string = reader.debug_string()\n    if all_tensors or all_tensor_names:\n      var_to_shape_map = reader.get_variable_to_shape_map()\n      for key in sorted(var_to_shape_map):\n        print(""tensor_name: "", key)\n        if all_tensors:\n          print(reader.get_tensor(key))\n    elif not tensor_name:\n      print(reader.debug_string().decode(""utf-8""))\n    else:\n      print(""tensor_name: "", tensor_name)\n      if tensor_name == ""vgg_16/conv1/conv1_1/weights"":\n          tensor = reader.get_tensor(tensor_name)\n          feature_maps = tensor.shape[-1]\n          for feature_map in range(feature_maps):\n              print(""{},{},{},{}"".format(tensor.shape[0], tensor.shape[1], tensor.shape[2], feature_map))\n              print(tensor[:,:,:,feature_map])\n          print(tensor.shape)\n          print(tensor)\n      else:\n        print(reader.get_tensor(tensor_name))\n  except Exception as e:  # pylint: disable=broad-except\n    print(str(e))\n    if ""corrupted compressed block contents"" in str(e):\n      print(""It\'s likely that your checkpoint file has been compressed ""\n            ""with SNAPPY."")\n    if (""Data loss"" in str(e) and\n        (any([e in file_name for e in ["".index"", "".meta"", "".data""]]))):\n      proposed_file = ""."".join(file_name.split(""."")[0:-1])\n      v2_file_error_template = """"""\nIt\'s likely that this is a V2 checkpoint and you need to provide the filename\n*prefix*.  Try removing the \'.\' and extension.  Try:\ninspect checkpoint --file_name = {}""""""\n      print(v2_file_error_template.format(proposed_file))\n\n\ndef parse_numpy_printoption(kv_str):\n  """"""Sets a single numpy printoption from a string of the form \'x=y\'.\n\n  See documentation on numpy.set_printoptions() for details about what values\n  x and y can take. x can be any option listed there other than \'formatter\'.\n\n  Args:\n    kv_str: A string of the form \'x=y\', such as \'threshold=100000\'\n\n  Raises:\n    argparse.ArgumentTypeError: If the string couldn\'t be used to set any\n        nump printoption.\n  """"""\n  k_v_str = kv_str.split(""="", 1)\n  if len(k_v_str) != 2 or not k_v_str[0]:\n    raise argparse.ArgumentTypeError(""\'%s\' is not in the form k=v."" % kv_str)\n  k, v_str = k_v_str\n  printoptions = np.get_printoptions()\n  if k not in printoptions:\n    raise argparse.ArgumentTypeError(""\'%s\' is not a valid printoption."" % k)\n  v_type = type(printoptions[k])\n  if v_type is type(None):\n    raise argparse.ArgumentTypeError(\n        ""Setting \'%s\' from the command line is not supported."" % k)\n  try:\n    v = (\n        v_type(v_str)\n        if v_type is not bool else flags.BooleanParser().parse(v_str))\n  except ValueError as e:\n    raise argparse.ArgumentTypeError(e.message)\n  np.set_printoptions(**{k: v})\n\n\ndef main(unused_argv):\n  if not FLAGS.file_name:\n    print(""Usage: inspect_checkpoint --file_name=checkpoint_file_name ""\n          ""[--tensor_name=tensor_to_print] ""\n          ""[--all_tensors] ""\n          ""[--all_tensor_names] ""\n          ""[--printoptions]"")\n    sys.exit(1)\n  else:\n    print_tensors_in_checkpoint_file(FLAGS.file_name, FLAGS.tensor_name,\n                                     FLAGS.all_tensors, FLAGS.all_tensor_names)\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")\n  parser.add_argument(\n      ""--file_name"",\n      type=str,\n      default="""",\n      help=""Checkpoint filename. ""\n      ""Note, if using Checkpoint V2 format, file_name is the ""\n      ""shared prefix between all files in the checkpoint."")\n  parser.add_argument(\n      ""--tensor_name"",\n      type=str,\n      default="""",\n      help=""Name of the tensor to inspect"")\n  parser.add_argument(\n      ""--all_tensors"",\n      nargs=""?"",\n      const=True,\n      type=""bool"",\n      default=False,\n      help=""If True, print the values of all the tensors."")\n  parser.add_argument(\n      ""--all_tensor_names"",\n      nargs=""?"",\n      const=True,\n      type=""bool"",\n      default=False,\n      help=""If True, print the names of all the tensors."")\n  parser.add_argument(\n      ""--printoptions"",\n      nargs=""*"",\n      type=parse_numpy_printoption,\n      help=""Argument for numpy.set_printoptions(), in the form \'k=v\'."")\n  FLAGS, unparsed = parser.parse_known_args()\n  app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
tfwss/tools/vgg_16-conv1-conv1_1-weights.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A simple script for inspect checkpoint files.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nimport numpy as np\n\nfrom tensorflow.python import pywrap_tensorflow\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import flags\n\nFLAGS = None\n\n\ndef print_tensors_in_checkpoint_file(file_name, tensor_name, all_tensors,\n                                     all_tensor_names):\n  """"""Prints tensors in a checkpoint file.\n\n  If no `tensor_name` is provided, prints the tensor names and shapes\n  in the checkpoint file.\n\n  If `tensor_name` is provided, prints the content of the tensor.\n\n  Args:\n    file_name: Name of the checkpoint file.\n    tensor_name: Name of the tensor in the checkpoint file to print.\n    all_tensors: Boolean indicating whether to print all tensors.\n    all_tensor_names: Boolean indicating whether to print all tensor names.\n  """"""\n  try:\n    reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n    debug_string = reader.debug_string()\n    print(debug_string.decode(""utf-8""))\n    if all_tensors or all_tensor_names:\n      var_to_shape_map = reader.get_variable_to_shape_map()\n      for key in sorted(var_to_shape_map):\n        print(""tensor_name: "", key)\n        if all_tensors:\n          print(reader.get_tensor(key))\n    elif not tensor_name:\n      print(reader.debug_string().decode(""utf-8""))\n    else:\n      print(""tensor_name: "", tensor_name)\n      if tensor_name == ""vgg_16/conv1/conv1_1/weights"":\n          tensor = reader.get_tensor(tensor_name)\n          feature_maps = tensor.shape[-1]\n          for feature_map in range(feature_maps):\n              print(""{},{},{},{}"".format(tensor.shape[0], tensor.shape[1], tensor.shape[2], feature_map))\n              print(tensor[:,:,:,feature_map])\n          print(tensor.shape)\n          print(tensor)\n      else:\n        print(reader.get_tensor(tensor_name))\n  except Exception as e:  # pylint: disable=broad-except\n    print(str(e))\n    if ""corrupted compressed block contents"" in str(e):\n      print(""It\'s likely that your checkpoint file has been compressed ""\n            ""with SNAPPY."")\n    if (""Data loss"" in str(e) and\n        (any([e in file_name for e in ["".index"", "".meta"", "".data""]]))):\n      proposed_file = ""."".join(file_name.split(""."")[0:-1])\n      v2_file_error_template = """"""\nIt\'s likely that this is a V2 checkpoint and you need to provide the filename\n*prefix*.  Try removing the \'.\' and extension.  Try:\ninspect checkpoint --file_name = {}""""""\n      print(v2_file_error_template.format(proposed_file))\n\n\ndef parse_numpy_printoption(kv_str):\n  """"""Sets a single numpy printoption from a string of the form \'x=y\'.\n\n  See documentation on numpy.set_printoptions() for details about what values\n  x and y can take. x can be any option listed there other than \'formatter\'.\n\n  Args:\n    kv_str: A string of the form \'x=y\', such as \'threshold=100000\'\n\n  Raises:\n    argparse.ArgumentTypeError: If the string couldn\'t be used to set any\n        nump printoption.\n  """"""\n  k_v_str = kv_str.split(""="", 1)\n  if len(k_v_str) != 2 or not k_v_str[0]:\n    raise argparse.ArgumentTypeError(""\'%s\' is not in the form k=v."" % kv_str)\n  k, v_str = k_v_str\n  printoptions = np.get_printoptions()\n  if k not in printoptions:\n    raise argparse.ArgumentTypeError(""\'%s\' is not a valid printoption."" % k)\n  v_type = type(printoptions[k])\n  if v_type is type(None):\n    raise argparse.ArgumentTypeError(\n        ""Setting \'%s\' from the command line is not supported."" % k)\n  try:\n    v = (\n        v_type(v_str)\n        if v_type is not bool else flags.BooleanParser().parse(v_str))\n  except ValueError as e:\n    raise argparse.ArgumentTypeError(e.message)\n  np.set_printoptions(**{k: v})\n\n\ndef main(unused_argv):\n  if not FLAGS.file_name:\n    print(""Usage: inspect_checkpoint --file_name=checkpoint_file_name ""\n          ""[--tensor_name=tensor_to_print] ""\n          ""[--all_tensors] ""\n          ""[--all_tensor_names] ""\n          ""[--printoptions]"")\n    sys.exit(1)\n  else:\n    print_tensors_in_checkpoint_file(FLAGS.file_name, FLAGS.tensor_name,\n                                     FLAGS.all_tensors, FLAGS.all_tensor_names)\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")\n  parser.add_argument(\n      ""--file_name"",\n      type=str,\n      default="""",\n      help=""Checkpoint filename. ""\n      ""Note, if using Checkpoint V2 format, file_name is the ""\n      ""shared prefix between all files in the checkpoint."")\n  parser.add_argument(\n      ""--tensor_name"",\n      type=str,\n      default="""",\n      help=""Name of the tensor to inspect"")\n  parser.add_argument(\n      ""--all_tensors"",\n      nargs=""?"",\n      const=True,\n      type=""bool"",\n      default=False,\n      help=""If True, print the values of all the tensors."")\n  parser.add_argument(\n      ""--all_tensor_names"",\n      nargs=""?"",\n      const=True,\n      type=""bool"",\n      default=False,\n      help=""If True, print the names of all the tensors."")\n  parser.add_argument(\n      ""--printoptions"",\n      nargs=""*"",\n      type=parse_numpy_printoption,\n      help=""Argument for numpy.set_printoptions(), in the form \'k=v\'."")\n  FLAGS, unparsed = parser.parse_known_args()\n  app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
