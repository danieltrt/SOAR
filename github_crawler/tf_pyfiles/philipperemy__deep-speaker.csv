file_path,api_count,code
audio.py,0,"b'import logging\nimport os\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport librosa\nimport numpy as np\nfrom python_speech_features import fbank\nfrom tqdm import tqdm\n\nfrom constants import SAMPLE_RATE, NUM_FBANKS\nfrom utils import find_files, ensures_dir\n\nlogger = logging.getLogger(__name__)\n\n\ndef read_mfcc(input_filename, sample_rate):\n    audio = Audio.read(input_filename, sample_rate)\n    energy = np.abs(audio)\n    silence_threshold = np.percentile(energy, 95)\n    offsets = np.where(energy > silence_threshold)[0]\n    # left_blank_duration_ms = (1000.0 * offsets[0]) // self.sample_rate  # frame_id to duration (ms)\n    # right_blank_duration_ms = (1000.0 * (len(audio) - offsets[-1])) // self.sample_rate\n    # TODO: could use trim_silence() here or a better VAD.\n    audio_voice_only = audio[offsets[0]:offsets[-1]]\n    mfcc = mfcc_fbank(audio_voice_only, sample_rate)\n    return mfcc\n\n\ndef extract_speaker_and_utterance_ids(filename: str):  # LIBRI.\n    # \'audio/dev-other/116/288045/116-288045-0000.flac\'\n    speaker, _, basename = Path(filename).parts[-3:]\n    filename.split(\'-\')\n    utterance = os.path.splitext(basename.split(\'-\', 1)[-1])[0]\n    assert basename.split(\'-\')[0] == speaker\n    return speaker, utterance\n\n\nclass Audio:\n\n    def __init__(self, cache_dir: str, audio_dir: str = None, sample_rate: int = SAMPLE_RATE, ext=\'flac\'):\n        self.ext = ext\n        self.cache_dir = os.path.join(cache_dir, \'audio-fbanks\')\n        ensures_dir(self.cache_dir)\n        if audio_dir is not None:\n            self.build_cache(os.path.expanduser(audio_dir), sample_rate)\n        self.speakers_to_utterances = defaultdict(dict)\n        for cache_file in find_files(self.cache_dir, ext=\'npy\'):\n            # /path/to/speaker_utterance.npy\n            speaker_id, utterance_id = Path(cache_file).stem.split(\'_\')\n            self.speakers_to_utterances[speaker_id][utterance_id] = cache_file\n\n    @property\n    def speaker_ids(self):\n        return sorted(self.speakers_to_utterances)\n\n    @staticmethod\n    def trim_silence(audio, threshold):\n        """"""Removes silence at the beginning and end of a sample.""""""\n        energy = librosa.feature.rms(audio)\n        frames = np.nonzero(np.array(energy > threshold))\n        indices = librosa.core.frames_to_samples(frames)[1]\n\n        # Note: indices can be an empty array, if the whole audio was silence.\n        audio_trim = audio[0:0]\n        left_blank = audio[0:0]\n        right_blank = audio[0:0]\n        if indices.size:\n            audio_trim = audio[indices[0]:indices[-1]]\n            left_blank = audio[:indices[0]]  # slice before.\n            right_blank = audio[indices[-1]:]  # slice after.\n        return audio_trim, left_blank, right_blank\n\n    @staticmethod\n    def read(filename, sample_rate=SAMPLE_RATE):\n        audio, sr = librosa.load(filename, sr=sample_rate, mono=True, dtype=np.float32)\n        assert sr == sample_rate\n        return audio\n\n    def build_cache(self, audio_dir, sample_rate):\n        logger.info(f\'audio_dir: {audio_dir}.\')\n        logger.info(f\'sample_rate: {sample_rate:,} hz.\')\n        audio_files = find_files(audio_dir, ext=self.ext)\n        audio_files_count = len(audio_files)\n        assert audio_files_count != 0, f\'Could not find any {self.ext} files in {audio_dir}.\'\n        logger.info(f\'Found {audio_files_count:,} files in {audio_dir}.\')\n        with tqdm(audio_files) as bar:\n            for audio_filename in bar:\n                bar.set_description(audio_filename)\n                self.cache_audio_file(audio_filename, sample_rate)\n\n    def cache_audio_file(self, input_filename, sample_rate):\n        sp, utt = extract_speaker_and_utterance_ids(input_filename)\n        cache_filename = os.path.join(self.cache_dir, f\'{sp}_{utt}.npy\')\n        if not os.path.isfile(cache_filename):\n            try:\n                mfcc = read_mfcc(input_filename, sample_rate)\n                np.save(cache_filename, mfcc)\n            except librosa.util.exceptions.ParameterError as e:\n                logger.error(e)\n\n\ndef pad_mfcc(mfcc, max_length):  # num_frames, nfilt=64.\n    if len(mfcc) < max_length:\n        mfcc = np.vstack((mfcc, np.tile(np.zeros(mfcc.shape[1]), (max_length - len(mfcc), 1))))\n    return mfcc\n\n\ndef mfcc_fbank(signal: np.array, sample_rate: int):  # 1D signal array.\n    # Returns MFCC with shape (num_frames, n_filters, 3).\n    filter_banks, energies = fbank(signal, samplerate=sample_rate, nfilt=NUM_FBANKS)\n    frames_features = normalize_frames(filter_banks)\n    # delta_1 = delta(filter_banks, N=1)\n    # delta_2 = delta(delta_1, N=1)\n    # frames_features = np.transpose(np.stack([filter_banks, delta_1, delta_2]), (1, 2, 0))\n    return np.array(frames_features, dtype=np.float32)  # Float32 precision is enough here.\n\n\ndef normalize_frames(m, epsilon=1e-12):\n    return [(v - np.mean(v)) / max(np.std(v), epsilon) for v in m]\n'"
batcher.py,0,"b'import json\nimport logging\nimport os\nfrom collections import deque, Counter\nfrom random import choice\nfrom time import time\n\nimport dill\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom audio import pad_mfcc, Audio\nfrom constants import NUM_FRAMES, NUM_FBANKS\nfrom conv_models import DeepSpeakerModel\nfrom utils import ensures_dir, load_pickle, load_npy, train_test_sp_to_utt\n\nlogger = logging.getLogger(__name__)\n\n\ndef extract_speaker(utt_file):\n    return utt_file.split(\'/\')[-1].split(\'_\')[0]\n\n\ndef sample_from_mfcc(mfcc, max_length):\n    if mfcc.shape[0] >= max_length:\n        r = choice(range(0, len(mfcc) - max_length + 1))\n        s = mfcc[r:r + max_length]\n    else:\n        s = pad_mfcc(mfcc, max_length)\n    return np.expand_dims(s, axis=-1)\n\n\ndef sample_from_mfcc_file(utterance_file, max_length):\n    mfcc = np.load(utterance_file)\n    return sample_from_mfcc(mfcc, max_length)\n\n\nclass KerasFormatConverter:\n\n    def __init__(self, working_dir, load_test_only=False):\n        self.working_dir = working_dir\n        self.output_dir = os.path.join(self.working_dir, \'keras-inputs\')\n        ensures_dir(self.output_dir)\n        self.categorical_speakers = load_pickle(os.path.join(self.output_dir, \'categorical_speakers.pkl\'))\n        if not load_test_only:\n            self.kx_train = load_npy(os.path.join(self.output_dir, \'kx_train.npy\'))\n            self.ky_train = load_npy(os.path.join(self.output_dir, \'ky_train.npy\'))\n        self.kx_test = load_npy(os.path.join(self.output_dir, \'kx_test.npy\'))\n        self.ky_test = load_npy(os.path.join(self.output_dir, \'ky_test.npy\'))\n        self.audio = Audio(cache_dir=self.working_dir, audio_dir=None)\n        if self.categorical_speakers is None:\n            self.categorical_speakers = SparseCategoricalSpeakers(self.audio.speaker_ids)\n\n    def persist_to_disk(self):\n        with open(os.path.join(self.output_dir, \'categorical_speakers.pkl\'), \'wb\') as w:\n            dill.dump(self.categorical_speakers, w)\n        np.save(os.path.join(self.output_dir, \'kx_train.npy\'), self.kx_train)\n        np.save(os.path.join(self.output_dir, \'kx_test.npy\'), self.kx_test)\n        np.save(os.path.join(self.output_dir, \'ky_train.npy\'), self.ky_train)\n        np.save(os.path.join(self.output_dir, \'ky_test.npy\'), self.ky_test)\n\n    def generate_per_phase(self, max_length=NUM_FRAMES, num_per_speaker=3000, is_test=False):\n        # train OR test.\n        num_speakers = len(self.audio.speaker_ids)\n        sp_to_utt = train_test_sp_to_utt(self.audio, is_test)\n\n        # 64 fbanks 1 channel(s).\n        # float32\n        kx = np.zeros((num_speakers * num_per_speaker, max_length, NUM_FBANKS, 1), dtype=np.float32)\n        ky = np.zeros((num_speakers * num_per_speaker, 1), dtype=np.float32)\n\n        desc = f\'Converting to Keras format [{""test"" if is_test else ""train""}]\'\n        for i, speaker_id in enumerate(tqdm(self.audio.speaker_ids, desc=desc)):\n            utterances_files = sp_to_utt[speaker_id]\n            for j, utterance_file in enumerate(np.random.choice(utterances_files, size=num_per_speaker, replace=True)):\n                self.load_into_mat(utterance_file, self.categorical_speakers, speaker_id, max_length, kx, ky,\n                                   i * num_per_speaker + j)\n        return kx, ky\n\n    def generate(self, max_length=NUM_FRAMES, counts_per_speaker=(3000, 500)):\n        kx_train, ky_train = self.generate_per_phase(max_length, counts_per_speaker[0], is_test=False)\n        kx_test, ky_test = self.generate_per_phase(max_length, counts_per_speaker[1], is_test=True)\n        logger.info(f\'kx_train.shape = {kx_train.shape}\')\n        logger.info(f\'ky_train.shape = {ky_train.shape}\')\n        logger.info(f\'kx_test.shape = {kx_test.shape}\')\n        logger.info(f\'ky_test.shape = {ky_test.shape}\')\n        self.kx_train, self.ky_train, self.kx_test, self.ky_test = kx_train, ky_train, kx_test, ky_test\n\n    @staticmethod\n    def load_into_mat(utterance_file, categorical_speakers, speaker_id, max_length, kx, ky, i):\n        kx[i] = sample_from_mfcc_file(utterance_file, max_length)\n        ky[i] = categorical_speakers.get_index(speaker_id)\n\n\nclass SparseCategoricalSpeakers:\n\n    def __init__(self, speakers_list):\n        self.speaker_ids = sorted(speakers_list)\n        assert len(set(self.speaker_ids)) == len(self.speaker_ids)  # all unique.\n        self.map = dict(zip(self.speaker_ids, range(len(self.speaker_ids))))\n\n    def get_index(self, speaker_id):\n        return self.map[speaker_id]\n\n\nclass OneHotSpeakers:\n\n    def __init__(self, speakers_list):\n        from tensorflow.keras.utils import to_categorical\n        self.speaker_ids = sorted(speakers_list)\n        self.int_speaker_ids = list(range(len(self.speaker_ids)))\n        self.map_speakers_to_index = dict([(k, v) for (k, v) in zip(self.speaker_ids, self.int_speaker_ids)])\n        self.map_index_to_speakers = dict([(v, k) for (k, v) in zip(self.speaker_ids, self.int_speaker_ids)])\n        self.speaker_categories = to_categorical(self.int_speaker_ids, num_classes=len(self.speaker_ids))\n\n    def get_speaker_from_index(self, index):\n        return self.map_index_to_speakers[index]\n\n    def get_one_hot(self, speaker_id):\n        index = self.map_speakers_to_index[speaker_id]\n        return self.speaker_categories[index]\n\n\nclass LazyTripletBatcher:\n    def __init__(self, working_dir: str, max_length: int, model: DeepSpeakerModel):\n        self.working_dir = working_dir\n        self.audio = Audio(cache_dir=working_dir)\n        logger.info(f\'Picking audio from {working_dir}.\')\n        self.sp_to_utt_train = train_test_sp_to_utt(self.audio, is_test=False)\n        self.sp_to_utt_test = train_test_sp_to_utt(self.audio, is_test=True)\n        self.max_length = max_length\n        self.model = model\n        self.nb_per_speaker = 2\n        self.nb_speakers = 640\n        self.history_length = 4\n        self.history_every = 100  # batches.\n        self.total_history_length = self.nb_speakers * self.nb_per_speaker * self.history_length  # 25,600\n        self.metadata_train_speakers = Counter()\n        self.metadata_output_file = os.path.join(self.working_dir, \'debug_batcher.json\')\n\n        self.history_embeddings_train = deque(maxlen=self.total_history_length)\n        self.history_utterances_train = deque(maxlen=self.total_history_length)\n        self.history_model_inputs_train = deque(maxlen=self.total_history_length)\n\n        self.history_embeddings = None\n        self.history_utterances = None\n        self.history_model_inputs = None\n\n        self.batch_count = 0\n        for _ in tqdm(range(self.history_length), desc=\'Initializing the batcher\'):  # init history.\n            self.update_triplets_history()\n\n    def update_triplets_history(self):\n        model_inputs = []\n        speakers = list(self.audio.speakers_to_utterances.keys())\n        np.random.shuffle(speakers)\n        selected_speakers = speakers[: self.nb_speakers]\n        embeddings_utterances = []\n        for speaker_id in selected_speakers:\n            train_utterances = self.sp_to_utt_train[speaker_id]\n            for selected_utterance in np.random.choice(a=train_utterances, size=self.nb_per_speaker, replace=False):\n                mfcc = sample_from_mfcc_file(selected_utterance, self.max_length)\n                embeddings_utterances.append(selected_utterance)\n                model_inputs.append(mfcc)\n        embeddings = self.model.m.predict(np.array(model_inputs))\n        assert embeddings.shape[-1] == 512\n        embeddings = np.reshape(embeddings, (len(selected_speakers), self.nb_per_speaker, 512))\n        self.history_embeddings_train.extend(list(embeddings.reshape((-1, 512))))\n        self.history_utterances_train.extend(embeddings_utterances)\n        self.history_model_inputs_train.extend(model_inputs)\n\n        # reason: can\'t index a deque with a np.array.\n        self.history_embeddings = np.array(self.history_embeddings_train)\n        self.history_utterances = np.array(self.history_utterances_train)\n        self.history_model_inputs = np.array(self.history_model_inputs_train)\n\n        with open(self.metadata_output_file, \'w\') as w:\n            json.dump(obj=dict(self.metadata_train_speakers), fp=w, indent=2)\n\n    def get_batch(self, batch_size, is_test=False):\n        return self.get_batch_test(batch_size) if is_test else self.get_random_batch(batch_size, is_test=False)\n\n    def get_batch_test(self, batch_size):\n        return self.get_random_batch(batch_size, is_test=True)\n\n    def get_random_batch(self, batch_size, is_test=False):\n        sp_to_utt = self.sp_to_utt_test if is_test else self.sp_to_utt_train\n        speakers = list(self.audio.speakers_to_utterances.keys())\n        anchor_speakers = np.random.choice(speakers, size=batch_size // 3, replace=False)\n\n        anchor_utterances = []\n        positive_utterances = []\n        negative_utterances = []\n        for anchor_speaker in anchor_speakers:\n            negative_speaker = np.random.choice(list(set(speakers) - {anchor_speaker}), size=1)[0]\n            assert negative_speaker != anchor_speaker\n            pos_utterances = np.random.choice(sp_to_utt[anchor_speaker], 2, replace=False)\n            neg_utterance = np.random.choice(sp_to_utt[negative_speaker], 1, replace=True)[0]\n            anchor_utterances.append(pos_utterances[0])\n            positive_utterances.append(pos_utterances[1])\n            negative_utterances.append(neg_utterance)\n\n        # anchor and positive should have difference utterances (but same speaker!).\n        anc_pos = np.array([positive_utterances, anchor_utterances])\n        assert np.all(anc_pos[0, :] != anc_pos[1, :])\n        assert np.all(np.array([extract_speaker(s) for s in anc_pos[0, :]]) == np.array(\n            [extract_speaker(s) for s in anc_pos[1, :]]))\n\n        pos_neg = np.array([positive_utterances, negative_utterances])\n        assert np.all(pos_neg[0, :] != pos_neg[1, :])\n        assert np.all(np.array([extract_speaker(s) for s in pos_neg[0, :]]) != np.array(\n            [extract_speaker(s) for s in pos_neg[1, :]]))\n\n        batch_x = np.vstack([\n            [sample_from_mfcc_file(u, self.max_length) for u in anchor_utterances],\n            [sample_from_mfcc_file(u, self.max_length) for u in positive_utterances],\n            [sample_from_mfcc_file(u, self.max_length) for u in negative_utterances]\n        ])\n\n        batch_y = np.zeros(shape=(len(batch_x), 1))  # dummy. sparse softmax needs something.\n        return batch_x, batch_y\n\n    def get_batch_train(self, batch_size):\n        from test import batch_cosine_similarity\n        s1 = time()\n        self.batch_count += 1\n        if self.batch_count % self.history_every == 0:\n            self.update_triplets_history()\n\n        all_indexes = range(len(self.history_embeddings_train))\n        anchor_indexes = np.random.choice(a=all_indexes, size=batch_size // 3, replace=False)\n\n        s2 = time()\n        similar_negative_indexes = []\n        dissimilar_positive_indexes = []\n        # could be made parallel.\n        for anchor_index in anchor_indexes:\n            s21 = time()\n            anchor_embedding = self.history_embeddings[anchor_index]\n            anchor_speaker = extract_speaker(self.history_utterances[anchor_index])\n\n            # why self.nb_speakers // 2? just random. because it is fast. otherwise it\'s too much.\n            negative_indexes = [j for (j, a) in enumerate(self.history_utterances)\n                                if extract_speaker(a) != anchor_speaker]\n            negative_indexes = np.random.choice(negative_indexes, size=self.nb_speakers // 2)\n\n            s22 = time()\n\n            anchor_embedding_tile = [anchor_embedding] * len(negative_indexes)\n            anchor_cos = batch_cosine_similarity(anchor_embedding_tile, self.history_embeddings[negative_indexes])\n\n            s23 = time()\n            similar_negative_index = negative_indexes[np.argsort(anchor_cos)[-1]]  # [-1:]\n            similar_negative_indexes.append(similar_negative_index)\n\n            s24 = time()\n            positive_indexes = [j for (j, a) in enumerate(self.history_utterances) if\n                                extract_speaker(a) == anchor_speaker and j != anchor_index]\n            s25 = time()\n            anchor_embedding_tile = [anchor_embedding] * len(positive_indexes)\n            s26 = time()\n            anchor_cos = batch_cosine_similarity(anchor_embedding_tile, self.history_embeddings[positive_indexes])\n            dissimilar_positive_index = positive_indexes[np.argsort(anchor_cos)[0]]  # [:1]\n            dissimilar_positive_indexes.append(dissimilar_positive_index)\n            s27 = time()\n\n        s3 = time()\n        batch_x = np.vstack([\n            self.history_model_inputs[anchor_indexes],\n            self.history_model_inputs[dissimilar_positive_indexes],\n            self.history_model_inputs[similar_negative_indexes]\n        ])\n\n        s4 = time()\n\n        # for anchor, positive, negative in zip(history_utterances[anchor_indexes],\n        #                                       history_utterances[dissimilar_positive_indexes],\n        #                                       history_utterances[similar_negative_indexes]):\n        # print(\'anchor\', os.path.basename(anchor),\n        #       \'positive\', os.path.basename(positive),\n        #       \'negative\', os.path.basename(negative))\n        # print(\'_\' * 80)\n\n        # assert utterances as well positive != anchor.\n        anchor_speakers = [extract_speaker(a) for a in self.history_utterances[anchor_indexes]]\n        positive_speakers = [extract_speaker(a) for a in self.history_utterances[dissimilar_positive_indexes]]\n        negative_speakers = [extract_speaker(a) for a in self.history_utterances[similar_negative_indexes]]\n\n        assert len(anchor_indexes) == len(dissimilar_positive_indexes)\n        assert len(similar_negative_indexes) == len(dissimilar_positive_indexes)\n        assert list(self.history_utterances[dissimilar_positive_indexes]) != list(\n            self.history_utterances[anchor_indexes])\n        assert anchor_speakers == positive_speakers\n        assert negative_speakers != anchor_speakers\n\n        batch_y = np.zeros(shape=(len(batch_x), 1))  # dummy. sparse softmax needs something.\n\n        for a in anchor_speakers:\n            self.metadata_train_speakers[a] += 1\n        for a in positive_speakers:\n            self.metadata_train_speakers[a] += 1\n        for a in negative_speakers:\n            self.metadata_train_speakers[a] += 1\n\n        s5 = time()\n        # print(\'1-2\', s2 - s1)\n        # print(\'2-3\', s3 - s2)\n        # print(\'3-4\', s4 - s3)\n        # print(\'4-5\', s5 - s4)\n        # print(\'21-22\', (s22 - s21) * (batch_size // 3))\n        # print(\'22-23\', (s23 - s22) * (batch_size // 3))\n        # print(\'23-24\', (s24 - s23) * (batch_size // 3))\n        # print(\'24-25\', (s25 - s24) * (batch_size // 3))\n        # print(\'25-26\', (s26 - s25) * (batch_size // 3))\n        # print(\'26-27\', (s27 - s26) * (batch_size // 3))\n\n        return batch_x, batch_y\n\n    def get_speaker_verification_data(self, anchor_speaker, num_different_speakers):\n        speakers = list(self.audio.speakers_to_utterances.keys())\n        anchor_utterances = []\n        positive_utterances = []\n        negative_utterances = []\n        negative_speakers = np.random.choice(list(set(speakers) - {anchor_speaker}), size=num_different_speakers)\n        assert [negative_speaker != anchor_speaker for negative_speaker in negative_speakers]\n        pos_utterances = np.random.choice(self.sp_to_utt_test[anchor_speaker], 2, replace=False)\n        neg_utterances = [np.random.choice(self.sp_to_utt_test[neg], 1, replace=True)[0] for neg in negative_speakers]\n        anchor_utterances.append(pos_utterances[0])\n        positive_utterances.append(pos_utterances[1])\n        negative_utterances.extend(neg_utterances)\n\n        # anchor and positive should have difference utterances (but same speaker!).\n        anc_pos = np.array([positive_utterances, anchor_utterances])\n        assert np.all(anc_pos[0, :] != anc_pos[1, :])\n        assert np.all(np.array([extract_speaker(s) for s in anc_pos[0, :]]) == np.array(\n            [extract_speaker(s) for s in anc_pos[1, :]]))\n\n        batch_x = np.vstack([\n            [sample_from_mfcc_file(u, self.max_length) for u in anchor_utterances],\n            [sample_from_mfcc_file(u, self.max_length) for u in positive_utterances],\n            [sample_from_mfcc_file(u, self.max_length) for u in negative_utterances]\n        ])\n\n        batch_y = np.zeros(shape=(len(batch_x), 1))  # dummy. sparse softmax needs something.\n        return batch_x, batch_y\n\n\nclass TripletBatcher:\n\n    def __init__(self, kx_train, ky_train, kx_test, ky_test):\n        self.kx_train = kx_train\n        self.ky_train = ky_train\n        self.kx_test = kx_test\n        self.ky_test = ky_test\n        speakers_list = sorted(set(ky_train.argmax(axis=1)))\n        num_different_speakers = len(speakers_list)\n        assert speakers_list == sorted(set(ky_test.argmax(axis=1)))  # train speakers = test speakers.\n        assert speakers_list == list(range(num_different_speakers))\n        self.train_indices_per_speaker = {}\n        self.test_indices_per_speaker = {}\n\n        for speaker_id in speakers_list:\n            self.train_indices_per_speaker[speaker_id] = list(np.where(ky_train.argmax(axis=1) == speaker_id)[0])\n            self.test_indices_per_speaker[speaker_id] = list(np.where(ky_test.argmax(axis=1) == speaker_id)[0])\n\n        # check.\n        # print(sorted(sum([v for v in self.train_indices_per_speaker.values()], [])))\n        # print(range(len(ky_train)))\n        assert sorted(sum([v for v in self.train_indices_per_speaker.values()], [])) == sorted(range(len(ky_train)))\n        assert sorted(sum([v for v in self.test_indices_per_speaker.values()], [])) == sorted(range(len(ky_test)))\n        self.speakers_list = speakers_list\n\n    def select_speaker_data(self, speaker, n, is_test):\n        x = self.kx_test if is_test else self.kx_train\n        indices_per_speaker = self.test_indices_per_speaker if is_test else self.train_indices_per_speaker\n        indices = np.random.choice(indices_per_speaker[speaker], size=n)\n        return x[indices]\n\n    def get_batch(self, batch_size, is_test=False):\n        # y = self.ky_test if is_test else self.ky_train\n\n        two_different_speakers = np.random.choice(self.speakers_list, size=2, replace=False)\n        anchor_positive_speaker = two_different_speakers[0]\n        negative_speaker = two_different_speakers[1]\n        assert negative_speaker != anchor_positive_speaker\n\n        batch_x = np.vstack([\n            self.select_speaker_data(anchor_positive_speaker, batch_size // 3, is_test),\n            self.select_speaker_data(anchor_positive_speaker, batch_size // 3, is_test),\n            self.select_speaker_data(negative_speaker, batch_size // 3, is_test)\n        ])\n\n        batch_y = np.zeros(shape=(len(batch_x), len(self.speakers_list)))\n        return batch_x, batch_y\n\n\nclass TripletBatcherMiner(TripletBatcher):\n\n    def __init__(self, kx_train, ky_train, kx_test, ky_test, model: DeepSpeakerModel):\n        super().__init__(kx_train, ky_train, kx_test, ky_test)\n        self.model = model\n        self.num_evaluations_to_find_best_batch = 10\n\n    def get_batch(self, batch_size, is_test=False):\n        if is_test:\n            return super().get_batch(batch_size, is_test)\n        max_loss = 0\n        max_batch = None, None\n        for i in range(self.num_evaluations_to_find_best_batch):\n            bx, by = super().get_batch(batch_size, is_test=False)  # only train here.\n            loss = self.model.m.evaluate(bx, by, batch_size=batch_size, verbose=0)\n            if loss > max_loss:\n                max_loss = loss\n                max_batch = bx, by\n        return max_batch\n\n\nclass TripletBatcherSelectHardNegatives(TripletBatcher):\n\n    def __init__(self, kx_train, ky_train, kx_test, ky_test, model: DeepSpeakerModel):\n        super().__init__(kx_train, ky_train, kx_test, ky_test)\n        self.model = model\n\n    def get_batch(self, batch_size, is_test=False, predict=None):\n        if predict is None:\n            predict = self.model.m.predict\n        from test import batch_cosine_similarity\n        num_triplets = batch_size // 3\n        inputs = []\n        k = 2  # do not change this.\n        for speaker in self.speakers_list:\n            inputs.append(self.select_speaker_data(speaker, n=k, is_test=is_test))\n        inputs = np.array(inputs)  # num_speakers * [k, num_frames, num_fbanks, 1].\n        embeddings = predict(np.vstack(inputs))\n        assert embeddings.shape[-1] == 512\n        # (speaker, utterance, 512)\n        embeddings = np.reshape(embeddings, (len(self.speakers_list), k, 512))\n        cs = batch_cosine_similarity(embeddings[:, 0], embeddings[:, 1])\n        arg_sort = np.argsort(cs)\n        assert len(arg_sort) > num_triplets\n        anchor_speakers = arg_sort[0:num_triplets]\n\n        anchor_embeddings = embeddings[anchor_speakers, 0]\n        negative_speakers = sorted(set(self.speakers_list) - set(anchor_speakers))\n        negative_embeddings = embeddings[negative_speakers, 0]\n\n        selected_negative_speakers = []\n        for anchor_embedding in anchor_embeddings:\n            cs_negative = [batch_cosine_similarity([anchor_embedding], neg) for neg in negative_embeddings]\n            selected_negative_speakers.append(negative_speakers[int(np.argmax(cs_negative))])\n\n        # anchor with frame 0.\n        # positive with frame 1.\n        # negative with frame 0.\n        assert len(set(selected_negative_speakers).intersection(anchor_speakers)) == 0\n        negative = inputs[selected_negative_speakers, 0]\n        positive = inputs[anchor_speakers, 1]\n        anchor = inputs[anchor_speakers, 0]\n        batch_x = np.vstack([anchor, positive, negative])\n        batch_y = np.zeros(shape=(len(batch_x), len(self.speakers_list)))\n        return batch_x, batch_y\n\n\nclass TripletEvaluator:\n\n    def __init__(self, kx_test, ky_test):\n        self.kx_test = kx_test\n        self.ky_test = ky_test\n        speakers_list = sorted(set(ky_test.argmax(axis=1)))\n        num_different_speakers = len(speakers_list)\n        assert speakers_list == list(range(num_different_speakers))\n        self.test_indices_per_speaker = {}\n        for speaker_id in speakers_list:\n            self.test_indices_per_speaker[speaker_id] = list(np.where(ky_test.argmax(axis=1) == speaker_id)[0])\n        assert sorted(sum([v for v in self.test_indices_per_speaker.values()], [])) == sorted(range(len(ky_test)))\n        self.speakers_list = speakers_list\n\n    def _select_speaker_data(self, speaker):\n        indices = np.random.choice(self.test_indices_per_speaker[speaker], size=1)\n        return self.kx_test[indices]\n\n    def get_speaker_verification_data(self, positive_speaker, num_different_speakers):\n        all_negative_speakers = list(set(self.speakers_list) - {positive_speaker})\n        assert len(self.speakers_list) - 1 == len(all_negative_speakers)\n        negative_speakers = np.random.choice(all_negative_speakers, size=num_different_speakers, replace=False)\n        assert positive_speaker not in negative_speakers\n        anchor = self._select_speaker_data(positive_speaker)\n        positive = self._select_speaker_data(positive_speaker)\n        data = [anchor, positive]\n        data.extend([self._select_speaker_data(n) for n in negative_speakers])\n        return np.vstack(data)\n\n\nif __name__ == \'__main__\':\n    np.random.seed(123)\n    ltb = LazyTripletBatcher(working_dir=\'/Users/premy/deep-speaker/\',\n                             max_length=NUM_FRAMES,\n                             model=DeepSpeakerModel())\n    for i in range(1000):\n        print(i)\n        start = time()\n        ltb.get_batch_train(batch_size=9)\n        print(time() - start)\n        # ltb.get_batch(batch_size=96)\n'"
cli.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport logging\nimport os\n\nimport click\n\nfrom audio import Audio\nfrom batcher import KerasFormatConverter\nfrom constants import SAMPLE_RATE, NUM_FRAMES\nfrom test import test\nfrom train import start_training\nfrom utils import ClickType as Ct, ensures_dir\nfrom utils import init_pandas\n\nlogger = logging.getLogger(__name__)\n\nVERSION = '3.0a'\n\n\n@click.group()\ndef cli():\n    logging.basicConfig(format='%(asctime)12s - %(levelname)s - %(message)s', level=logging.INFO)\n    init_pandas()\n\n\n@cli.command('version', short_help='Prints the version.')\ndef version():\n    print(f'Version is {VERSION}.')\n\n\n@cli.command('build-mfcc-cache', short_help='Build audio cache.')\n@click.option('--working_dir', required=True, type=Ct.output_dir())\n@click.option('--audio_dir', default=None)\n@click.option('--sample_rate', default=SAMPLE_RATE, show_default=True, type=int)\ndef build_audio_cache(working_dir, audio_dir, sample_rate):\n    ensures_dir(working_dir)\n    if audio_dir is None:\n        audio_dir = os.path.join(working_dir, 'LibriSpeech')\n    Audio(cache_dir=working_dir, audio_dir=audio_dir, sample_rate=sample_rate)\n\n\n@cli.command('build-keras-inputs', short_help='Build inputs to Keras.')\n@click.option('--working_dir', required=True, type=Ct.input_dir())\n@click.option('--counts_per_speaker', default='600,100', show_default=True, type=str)  # train,test\ndef build_keras_inputs(working_dir, counts_per_speaker):\n    # counts_per_speaker: If you specify --counts_per_speaker 600,100, that means for each speaker, \n    # you're going to generate 600 samples for training and 100 for testing. One sample is 160 frames \n    # by default (~roughly 1.6 seconds).\n    counts_per_speaker = [int(b) for b in counts_per_speaker.split(',')]\n    kc = KerasFormatConverter(working_dir)\n    kc.generate(max_length=NUM_FRAMES, counts_per_speaker=counts_per_speaker)\n    kc.persist_to_disk()\n\n\n@cli.command('test-model', short_help='Test a Keras model.')\n@click.option('--working_dir', required=True, type=Ct.input_dir())\n@click.option('--checkpoint_file', required=True, type=Ct.input_file())\ndef test_model(working_dir, checkpoint_file=None):\n    # export CUDA_VISIBLE_DEVICES=0; python cli.py test-model\n    # --working_dir /home/philippe/ds-test/triplet-training/\n    # --checkpoint_file ../ds-test/checkpoints-softmax/ResCNN_checkpoint_102.h5\n    # f-measure = 0.789, true positive rate = 0.733, accuracy = 0.996, equal error rate = 0.043\n\n    # export CUDA_VISIBLE_DEVICES=0; python cli.py test-model\n    # --working_dir /home/philippe/ds-test/triplet-training/\n    # --checkpoint_file ../ds-test/checkpoints-triplets/ResCNN_checkpoint_175.h5\n    # f-measure = 0.849, true positive rate = 0.798, accuracy = 0.997, equal error rate = 0.025\n    test(working_dir, checkpoint_file)\n\n\n@cli.command('train-model', short_help='Train a Keras model.')\n@click.option('--working_dir', required=True, type=Ct.input_dir())\n@click.option('--pre_training_phase/--no_pre_training_phase', default=False, show_default=True)\ndef train_model(working_dir, pre_training_phase):\n    # PRE TRAINING\n\n    # commit a5030dd7a1b53cd11d5ab7832fa2d43f2093a464\n    # Merge: a11d13e b30e64e\n    # Author: Philippe Remy <premy.enseirb@gmail.com>\n    # Date:   Fri Apr 10 10:37:59 2020 +0900\n    # LibriSpeech train-clean-data360 (600, 100). 0.985 on test set (enough for pre-training).\n\n    # TRIPLET TRAINING\n    # [...]\n    # Epoch 175/1000\n    # 2000/2000 [==============================] - 919s 459ms/step - loss: 0.0077 - val_loss: 0.0058\n    # Epoch 176/1000\n    # 2000/2000 [==============================] - 917s 458ms/step - loss: 0.0075 - val_loss: 0.0059\n    # Epoch 177/1000\n    # 2000/2000 [==============================] - 927s 464ms/step - loss: 0.0075 - val_loss: 0.0059\n    # Epoch 178/1000\n    # 2000/2000 [==============================] - 948s 474ms/step - loss: 0.0073 - val_loss: 0.0058\n    start_training(working_dir, pre_training_phase)\n\n\nif __name__ == '__main__':\n    cli()\n"""
constants.py,0,"b""# Constants.\n\nSAMPLE_RATE = 16000  # not higher than that otherwise we may have errors when computing the fbanks.\n\n# Train/Test sets share the same speakers. They contain different utterances.\n# 0.8 means 20% of the utterances of each speaker will be held out and placed in the test set.\nTRAIN_TEST_RATIO = 0.8\n\nCHECKPOINTS_SOFTMAX_DIR = 'checkpoints-softmax'\n\nCHECKPOINTS_TRIPLET_DIR = 'checkpoints-triplets'\n\nBATCH_SIZE = 32 * 3  # have to be a multiple of 3.\n\n# Input to the model will be a 4D image: (batch_size, num_frames, num_fbanks, 3)\n# Where the 3 channels are: FBANK, DIFF(FBANK), DIFF(DIFF(FBANK)).\nNUM_FRAMES = 160  # 1 second ~ 100 frames with default params winlen=0.025,winstep=0.01\nNUM_FBANKS = 64\n"""
conv_models.py,0,"b""import logging\nimport os\n\nimport numpy as np\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Lambda, Dense\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\nfrom constants import NUM_FBANKS, NUM_FRAMES\nfrom triplet_loss import deep_speaker_loss\n\nlogger = logging.getLogger(__name__)\n\n\nclass DeepSpeakerModel:\n\n    # I thought it was 3 but maybe energy is added at a 4th dimension.\n    # would be better to have 4 dimensions:\n    # MFCC, DIFF(MFCC), DIFF(DIFF(MFCC)), ENERGIES (probably tiled across the frequency domain).\n    # this seems to help match the parameter counts.\n    def __init__(self, batch_input_shape=(None, NUM_FRAMES, NUM_FBANKS, 1), include_softmax=False,\n                 num_speakers_softmax=None):\n        self.include_softmax = include_softmax\n        if self.include_softmax:\n            assert num_speakers_softmax > 0\n        self.clipped_relu_count = 0\n\n        # http://cs231n.github.io/convolutional-networks/\n        # conv weights\n        # #params = ks * ks * nb_filters * num_channels_input\n\n        # Conv128-s\n        # 5*5*128*128/2+128\n        # ks*ks*nb_filters*channels/strides+bias(=nb_filters)\n\n        # take 100 ms -> 4 frames.\n        # if signal is 3 seconds, then take 100ms per 100ms and average out this network.\n        # 8*8 = 64 features.\n\n        # used to share all the layers across the inputs\n\n        # num_frames = K.shape() - do it dynamically after.\n        inputs = Input(batch_shape=batch_input_shape, name='input')\n        x = self.cnn_component(inputs)\n\n        x = Reshape((-1, 2048))(x)\n        # Temporal average layer. axis=1 is time.\n        x = Lambda(lambda y: K.mean(y, axis=1), name='average')(x)\n        if include_softmax:\n            logger.info('Including a Dropout layer to reduce overfitting.')\n            # used for softmax because the dataset we pre-train on might be too small. easy to overfit.\n            x = Dropout(0.5)(x)\n        x = Dense(512, name='affine')(x)\n        if include_softmax:\n            # Those weights are just when we train on softmax.\n            x = Dense(num_speakers_softmax, activation='softmax')(x)\n        else:\n            # Does not contain any weights.\n            x = Lambda(lambda y: K.l2_normalize(y, axis=1), name='ln')(x)\n        self.m = Model(inputs, x, name='ResCNN')\n\n    def keras_model(self):\n        return self.m\n\n    def get_weights(self):\n        w = self.m.get_weights()\n        if self.include_softmax:\n            w.pop()  # last 2 are the W_softmax and b_softmax.\n            w.pop()\n        return w\n\n    def clipped_relu(self, inputs):\n        relu = Lambda(lambda y: K.minimum(K.maximum(y, 0), 20), name=f'clipped_relu_{self.clipped_relu_count}')(inputs)\n        self.clipped_relu_count += 1\n        return relu\n\n    def identity_block(self, input_tensor, kernel_size, filters, stage, block):\n        conv_name_base = f'res{stage}_{block}_branch'\n\n        x = Conv2D(filters,\n                   kernel_size=kernel_size,\n                   strides=1,\n                   activation=None,\n                   padding='same',\n                   kernel_initializer='glorot_uniform',\n                   kernel_regularizer=regularizers.l2(l=0.0001),\n                   name=conv_name_base + '_2a')(input_tensor)\n        x = BatchNormalization(name=conv_name_base + '_2a_bn')(x)\n        x = self.clipped_relu(x)\n\n        x = Conv2D(filters,\n                   kernel_size=kernel_size,\n                   strides=1,\n                   activation=None,\n                   padding='same',\n                   kernel_initializer='glorot_uniform',\n                   kernel_regularizer=regularizers.l2(l=0.0001),\n                   name=conv_name_base + '_2b')(x)\n        x = BatchNormalization(name=conv_name_base + '_2b_bn')(x)\n\n        x = self.clipped_relu(x)\n\n        x = layers.add([x, input_tensor])\n        x = self.clipped_relu(x)\n        return x\n\n    def conv_and_res_block(self, inp, filters, stage):\n        conv_name = 'conv{}-s'.format(filters)\n        # TODO: why kernel_regularizer?\n        o = Conv2D(filters,\n                   kernel_size=5,\n                   strides=2,\n                   activation=None,\n                   padding='same',\n                   kernel_initializer='glorot_uniform',\n                   kernel_regularizer=regularizers.l2(l=0.0001), name=conv_name)(inp)\n        o = BatchNormalization(name=conv_name + '_bn')(o)\n        o = self.clipped_relu(o)\n        for i in range(3):\n            o = self.identity_block(o, kernel_size=3, filters=filters, stage=stage, block=i)\n        return o\n\n    def cnn_component(self, inp):\n        x = self.conv_and_res_block(inp, 64, stage=1)\n        x = self.conv_and_res_block(x, 128, stage=2)\n        x = self.conv_and_res_block(x, 256, stage=3)\n        x = self.conv_and_res_block(x, 512, stage=4)\n        return x\n\n    def set_weights(self, w):\n        for layer, layer_w in zip(self.m.layers, w):\n            layer.set_weights(layer_w)\n            logger.info(f'Setting weights for [{layer.name}]...')\n\n\ndef main():\n    # Looks correct to me.\n    # I have 37K but paper reports 41K. which is not too far.\n    dsm = DeepSpeakerModel()\n    dsm.m.summary()\n\n    # I suspect num frames to be 32.\n    # Then fbank=64, then total would be 32*64 = 2048.\n    # plot_model(dsm.m, to_file='model.png', dpi=300, show_shapes=True, expand_nested=True)\n\n\ndef _train():\n    # x = np.random.uniform(size=(6, 32, 64, 4))  # 6 is multiple of 3.\n    # y_softmax = np.random.uniform(size=(6, 100))\n    # dsm = DeepSpeakerModel(batch_input_shape=(None, 32, 64, 4), include_softmax=True, num_speakers_softmax=100)\n    # dsm.m.compile(optimizer=Adam(lr=0.01), loss='categorical_crossentropy')\n    # print(dsm.m.predict(x).shape)\n    # print(dsm.m.evaluate(x, y_softmax))\n    # w = dsm.get_weights()\n    dsm = DeepSpeakerModel(batch_input_shape=(None, 32, 64, 4), include_softmax=False)\n    # dsm.m.set_weights(w)\n    dsm.m.compile(optimizer=Adam(lr=0.01), loss=deep_speaker_loss)\n\n    # it works!!!!!!!!!!!!!!!!!!!!\n    # unit_batch_size = 20\n    # anchor = np.ones(shape=(unit_batch_size, 32, 64, 4))\n    # positive = np.array(anchor)\n    # negative = np.ones(shape=(unit_batch_size, 32, 64, 4)) * (-1)\n    # batch = np.vstack((anchor, positive, negative))\n    # x = batch\n    # y = np.zeros(shape=(len(batch), 512))  # not important.\n    # print('Starting to fit...')\n    # while True:\n    #     print(dsm.m.train_on_batch(x, y))\n\n    # should not work... and it does not work!\n    unit_batch_size = 20\n    negative = np.ones(shape=(unit_batch_size, 32, 64, 4)) * (-1)\n    batch = np.vstack((negative, negative, negative))\n    x = batch\n    y = np.zeros(shape=(len(batch), 512))  # not important.\n    print('Starting to fit...')\n    while True:\n        print(dsm.m.train_on_batch(x, y))\n\n\ndef _test_checkpoint_compatibility():\n    dsm = DeepSpeakerModel(batch_input_shape=(None, 32, 64, 4), include_softmax=True, num_speakers_softmax=10)\n    dsm.m.save_weights('test.h5')\n    dsm = DeepSpeakerModel(batch_input_shape=(None, 32, 64, 4), include_softmax=False)\n    dsm.m.load_weights('test.h5', by_name=True)\n    os.remove('test.h5')\n\n\nif __name__ == '__main__':\n    _test_checkpoint_compatibility()\n"""
eval_metrics.py,0,"b'import numpy as np\n\n\ndef evaluate(sims, labels):\n    # Calculate evaluation metrics\n    thresholds = np.arange(0, 1.0, 0.001)\n    fm, tpr, acc = calculate_roc(thresholds, sims, labels)\n    eer = calculate_eer(thresholds, sims, labels)\n    return fm, tpr, acc, eer\n\n\ndef calculate_roc(thresholds, sims, labels):\n    nrof_pairs = min(len(labels), len(sims))\n    nrof_thresholds = len(thresholds)\n\n    tprs = np.zeros((nrof_thresholds))\n    fprs = np.zeros((nrof_thresholds))\n    acc_train = np.zeros((nrof_thresholds))\n    precisions = np.zeros((nrof_thresholds))\n    fms = np.zeros((nrof_thresholds))\n    accuracy = 0.0\n\n    indices = np.arange(nrof_pairs)\n\n    # Find the best threshold for the fold\n\n    for threshold_idx, threshold in enumerate(thresholds):\n        tprs[threshold_idx], fprs[threshold_idx], precisions[threshold_idx], \\\n        fms[threshold_idx], acc_train[threshold_idx] = calculate_accuracy(threshold, sims, labels)\n\n    bestindex = np.argmax(fms)\n    bestfm = fms[bestindex]\n    besttpr = tprs[bestindex]\n    bestacc = acc_train[bestindex]\n\n    return bestfm, besttpr, bestacc\n\n\ndef calculate_accuracy(threshold, sims, actual_issame):\n    predict_issame = np.greater(sims, threshold)\n    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n\n    tpr = 0 if (tp + fn == 0) else float(tp) / float(tp + fn)  # recall\n    fpr = 0 if (fp + tn == 0) else float(fp) / float(fp + tn)\n    precision = 0 if (tp + fp == 0) else float(tp) / float(tp + fp)\n    fm = 2 * precision * tpr / (precision + tpr + 1e-12)\n    acc = float(tp + tn) / (sims.size + 1e-12)\n    return tpr, fpr, precision, fm, acc\n\n\ndef calculate_eer(thresholds, sims, labels):\n    nrof_pairs = min(len(labels), len(sims))\n    nrof_thresholds = len(thresholds)\n\n    indices = np.arange(nrof_pairs)\n\n    # Find the threshold that gives FAR = far_target\n    far_train = np.zeros(nrof_thresholds)\n    frr_train = np.zeros(nrof_thresholds)\n    eer_index = 0\n    eer_diff = 100000000\n    for threshold_idx, threshold in enumerate(thresholds):\n        frr_train[threshold_idx], far_train[threshold_idx] = calculate_val_far(threshold, sims, labels)\n        if abs(frr_train[threshold_idx] - far_train[threshold_idx]) < eer_diff:\n            eer_diff = abs(frr_train[threshold_idx] - far_train[threshold_idx])\n            eer_index = threshold_idx\n\n    frr, far = frr_train[eer_index], far_train[eer_index]\n\n    eer = (frr + far) / 2\n\n    return eer\n\n\ndef calculate_val_far(threshold, sims, actual_issame):\n    predict_issame = np.greater(sims, threshold)\n    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    n_same = np.sum(actual_issame)\n    n_diff = np.sum(np.logical_not(actual_issame))\n    if n_diff == 0:\n        n_diff = 1\n    if n_same == 0:\n        return 0, 0\n    val = float(true_accept) / float(n_same)\n    frr = 1 - val\n    far = float(false_accept) / float(n_diff)\n    return frr, far\n'"
example.py,0,"b""import numpy as np\nimport random\nfrom audio import read_mfcc\nfrom batcher import sample_from_mfcc\nfrom constants import SAMPLE_RATE, NUM_FRAMES\nfrom conv_models import DeepSpeakerModel\nfrom test import batch_cosine_similarity\n\nnp.random.seed(123)\nrandom.seed(123)\n\nmodel = DeepSpeakerModel()\nmodel.m.load_weights('/Users/premy/deep-speaker/checkpoints/ResCNN_triplet_training_checkpoint_175.h5', by_name=True)\n\nmfcc_001 = sample_from_mfcc(read_mfcc('samples/PhilippeRemy/PhilippeRemy_001.wav', SAMPLE_RATE), NUM_FRAMES)\nmfcc_002 = sample_from_mfcc(read_mfcc('samples/PhilippeRemy/PhilippeRemy_002.wav', SAMPLE_RATE), NUM_FRAMES)\n\npredict_001 = model.m.predict(np.expand_dims(mfcc_001, axis=0))\npredict_002 = model.m.predict(np.expand_dims(mfcc_002, axis=0))\n\nmfcc_003 = sample_from_mfcc(read_mfcc('samples/1255-90413-0001.flac', SAMPLE_RATE), NUM_FRAMES)\npredict_003 = model.m.predict(np.expand_dims(mfcc_003, axis=0))\n\nprint('SAME SPEAKER', batch_cosine_similarity(predict_001, predict_002))\nprint('DIFF SPEAKER', batch_cosine_similarity(predict_001, predict_003))\n"""
test.py,0,"b""import logging\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom audio import Audio\nfrom batcher import LazyTripletBatcher\nfrom constants import NUM_FBANKS, NUM_FRAMES, CHECKPOINTS_TRIPLET_DIR, BATCH_SIZE\nfrom conv_models import DeepSpeakerModel\nfrom eval_metrics import evaluate\nfrom utils import load_best_checkpoint, enable_deterministic\n\nlogger = logging.getLogger(__name__)\n\n\ndef batch_cosine_similarity(x1, x2):\n    # https://en.wikipedia.org/wiki/Cosine_similarity\n    # 1 = equal direction ; -1 = opposite direction\n    mul = np.multiply(x1, x2)\n    s = np.sum(mul, axis=1)\n\n    # l1 = np.sum(np.multiply(x1, x1),axis=1)\n    # l2 = np.sum(np.multiply(x2, x2), axis=1)\n    # as values have have length 1, we don't need to divide by norm (as it is 1)\n    return s\n\n\ndef eval_model(working_dir: str, model: DeepSpeakerModel):\n    enable_deterministic()\n    audio = Audio(working_dir)\n    batcher = LazyTripletBatcher(working_dir, NUM_FRAMES, model)\n    speakers_list = list(audio.speakers_to_utterances.keys())\n    num_negative_speakers = 99\n    num_speakers = len(speakers_list)\n    y_pred = np.zeros(shape=(num_speakers, num_negative_speakers + 1))  # negatives + positive\n    for i, positive_speaker in tqdm(enumerate(speakers_list), desc='test', total=num_speakers):\n        # convention id[0] is anchor speaker, id[1] is positive, id[2:] are negative.\n        input_data = batcher.get_speaker_verification_data(positive_speaker, num_negative_speakers)\n        # batch size is not relevant. just making sure we don't push too much on the GPU.\n        predictions = model.m.predict(input_data, batch_size=BATCH_SIZE)\n        anchor_embedding = predictions[0]\n        for j, other_than_anchor_embedding in enumerate(predictions[1:]):  # positive + negatives\n            y_pred[i][j] = batch_cosine_similarity([anchor_embedding], [other_than_anchor_embedding])[0]\n        # y_pred[i] = softmax(y_pred[i])\n    # could apply softmax here.\n    y_true = np.zeros_like(y_pred)  # positive is at index 0.\n    y_true[:, 0] = 1.0\n    print(np.matrix(y_true))\n    print(np.matrix(y_pred))\n    print(np.min(y_pred), np.max(y_pred))\n    fm, tpr, acc, eer = evaluate(y_pred, y_true)\n    return fm, tpr, acc, eer\n\n\ndef test(working_dir, checkpoint_file=None):\n    batch_input_shape = [None, NUM_FRAMES, NUM_FBANKS, 1]\n    dsm = DeepSpeakerModel(batch_input_shape)\n    if checkpoint_file is None:\n        checkpoint_file = load_best_checkpoint(CHECKPOINTS_TRIPLET_DIR)\n    if checkpoint_file is not None:\n        logger.info(f'Found checkpoint [{checkpoint_file}]. Loading weights...')\n        dsm.m.load_weights(checkpoint_file, by_name=True)\n    else:\n        logger.info(f'Could not find any checkpoint in {checkpoint_file}.')\n        exit(1)\n\n    fm, tpr, acc, eer = eval_model(working_dir, model=dsm)\n    logger.info(f'f-measure = {fm:.3f}, true positive rate = {tpr:.3f}, '\n                f'accuracy = {acc:.3f}, equal error rate = {eer:.3f}')\n"""
train.py,0,"b""import logging\nimport os\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.optimizers import SGD\nfrom tqdm import tqdm\n\nfrom batcher import KerasFormatConverter, LazyTripletBatcher\nfrom constants import BATCH_SIZE, CHECKPOINTS_SOFTMAX_DIR, CHECKPOINTS_TRIPLET_DIR, NUM_FRAMES, NUM_FBANKS\nfrom conv_models import DeepSpeakerModel\nfrom triplet_loss import deep_speaker_loss\nfrom utils import load_best_checkpoint, ensures_dir\n\nlogger = logging.getLogger(__name__)\n\n# Otherwise it's just too much logging from Tensorflow...\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\ndef fit_model(dsm: DeepSpeakerModel, working_dir: str, max_length: int = NUM_FRAMES, batch_size=BATCH_SIZE):\n    batcher = LazyTripletBatcher(working_dir, max_length, dsm)\n\n    # build small test set.\n    test_batches = []\n    for _ in tqdm(range(200), desc='Build test set'):\n        test_batches.append(batcher.get_batch_test(batch_size))\n\n    def test_generator():\n        while True:\n            for bb in test_batches:\n                yield bb\n\n    def train_generator():\n        while True:\n            yield batcher.get_random_batch(batch_size, is_test=False)\n\n    checkpoint_name = dsm.m.name + '_checkpoint'\n    checkpoint_filename = os.path.join(CHECKPOINTS_TRIPLET_DIR, checkpoint_name + '_{epoch}.h5')\n    checkpoint = ModelCheckpoint(monitor='val_loss', filepath=checkpoint_filename, save_best_only=True)\n    dsm.m.fit(x=train_generator(), y=None, steps_per_epoch=2000, shuffle=False,\n              epochs=1000, validation_data=test_generator(), validation_steps=len(test_batches),\n              callbacks=[checkpoint])\n\n\ndef fit_model_softmax(dsm: DeepSpeakerModel, kx_train, ky_train, kx_test, ky_test,\n                      batch_size=BATCH_SIZE, max_epochs=1000, initial_epoch=0):\n    checkpoint_name = dsm.m.name + '_checkpoint'\n    checkpoint_filename = os.path.join(CHECKPOINTS_SOFTMAX_DIR, checkpoint_name + '_{epoch}.h5')\n    checkpoint = ModelCheckpoint(monitor='val_accuracy', filepath=checkpoint_filename, save_best_only=True)\n\n    # if the accuracy does not increase by 0.1% over 20 epochs, we stop the training.\n    early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=20, verbose=1, mode='max')\n\n    # if the accuracy does not increase over 10 epochs, we reduce the learning rate by half.\n    reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=10, min_lr=0.0001, verbose=1)\n\n    max_len_train = len(kx_train) - len(kx_train) % batch_size\n    kx_train = kx_train[0:max_len_train]\n    ky_train = ky_train[0:max_len_train]\n    max_len_test = len(kx_test) - len(kx_test) % batch_size\n    kx_test = kx_test[0:max_len_test]\n    ky_test = ky_test[0:max_len_test]\n\n    dsm.m.fit(x=kx_train,\n              y=ky_train,\n              batch_size=batch_size,\n              epochs=initial_epoch + max_epochs,\n              initial_epoch=initial_epoch,\n              verbose=1,\n              shuffle=True,\n              validation_data=(kx_test, ky_test),\n              callbacks=[early_stopping, reduce_lr, checkpoint])\n\n\ndef start_training(working_dir, pre_training_phase=True):\n    ensures_dir(CHECKPOINTS_SOFTMAX_DIR)\n    ensures_dir(CHECKPOINTS_TRIPLET_DIR)\n    batch_input_shape = [None, NUM_FRAMES, NUM_FBANKS, 1]\n    if pre_training_phase:\n        logger.info('Softmax pre-training.')\n        kc = KerasFormatConverter(working_dir)\n        num_speakers_softmax = len(kc.categorical_speakers.speaker_ids)\n        dsm = DeepSpeakerModel(batch_input_shape, include_softmax=True, num_speakers_softmax=num_speakers_softmax)\n        dsm.m.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        pre_training_checkpoint = load_best_checkpoint(CHECKPOINTS_SOFTMAX_DIR)\n        if pre_training_checkpoint is not None:\n            initial_epoch = int(pre_training_checkpoint.split('/')[-1].split('.')[0].split('_')[-1])\n            logger.info(f'Initial epoch is {initial_epoch}.')\n            logger.info(f'Loading softmax checkpoint: {pre_training_checkpoint}.')\n            dsm.m.load_weights(pre_training_checkpoint)  # latest one.\n        else:\n            initial_epoch = 0\n        fit_model_softmax(dsm, kc.kx_train, kc.ky_train, kc.kx_test, kc.ky_test, initial_epoch=initial_epoch)\n    else:\n        logger.info('Training with the triplet loss.')\n        dsm = DeepSpeakerModel(batch_input_shape, include_softmax=False)\n        triplet_checkpoint = load_best_checkpoint(CHECKPOINTS_TRIPLET_DIR)\n        pre_training_checkpoint = load_best_checkpoint(CHECKPOINTS_SOFTMAX_DIR)\n        if triplet_checkpoint is not None:\n            logger.info(f'Loading triplet checkpoint: {triplet_checkpoint}.')\n            dsm.m.load_weights(triplet_checkpoint)\n        elif pre_training_checkpoint is not None:\n            logger.info(f'Loading pre-training checkpoint: {pre_training_checkpoint}.')\n            # If `by_name` is True, weights are loaded into layers only if they share the\n            # same name. This is useful for fine-tuning or transfer-learning models where\n            # some of the layers have changed.\n            dsm.m.load_weights(pre_training_checkpoint, by_name=True)\n        dsm.m.compile(optimizer=SGD(), loss=deep_speaker_loss)\n        fit_model(dsm, working_dir, NUM_FRAMES)\n"""
triplet_loss.py,0,"b""import keras.backend as K\n\n# ALPHA = 0.2  # used in FaceNet https://arxiv.org/pdf/1503.03832.pdf\nALPHA = 0.1  # used in Deep Speaker.\n\n\ndef batch_cosine_similarity(x1, x2):\n    # https://en.wikipedia.org/wiki/Cosine_similarity\n    # 1 = equal direction ; -1 = opposite direction\n    dot = K.squeeze(K.batch_dot(x1, x2, axes=1), axis=1)\n    # as values have have length 1, we don't need to divide by norm (as it is 1)\n    return dot\n\n\ndef deep_speaker_loss(y_true, y_pred, alpha=ALPHA):\n    # y_true is not used. we respect this convention:\n    # y_true.shape = (batch_size, embedding_size) [not used]\n    # y_pred.shape = (batch_size, embedding_size)\n    # EXAMPLE:\n    # _____________________________________________________\n    # ANCHOR 1 (512,)\n    # ANCHOR 2 (512,)\n    # POS EX 1 (512,)\n    # POS EX 2 (512,)\n    # NEG EX 1 (512,)\n    # NEG EX 2 (512,)\n    # _____________________________________________________\n    split = K.shape(y_pred)[0] // 3\n\n    anchor = y_pred[0:split]\n    positive_ex = y_pred[split:2 * split]\n    negative_ex = y_pred[2 * split:]\n\n    # If the loss does not decrease below ALPHA then the model does not learn anything.\n    # If all anchor = positive = negative (model outputs the same vector always).\n    # Then sap = san = 1. and loss = max(alpha,0) = alpha.\n    # On the contrary if anchor = positive = [1] and negative = [-1].\n    # Then sap = 1 and san = -1. loss = max(-1-1+0.1,0) = max(-1.9, 0) = 0.\n    sap = batch_cosine_similarity(anchor, positive_ex)\n    san = batch_cosine_similarity(anchor, negative_ex)\n    loss = K.maximum(san - sap + alpha, 0.0)\n    total_loss = K.mean(loss)\n    return total_loss\n\n\nif __name__ == '__main__':\n    import numpy as np\n\n    print(deep_speaker_loss(alpha=0.1, y_true=0, y_pred=np.array([[0.9], [1.0], [-1.0]])))\n    print(deep_speaker_loss(alpha=1, y_true=0, y_pred=np.array([[0.9], [1.0], [-1.0]])))\n    print(deep_speaker_loss(alpha=2, y_true=0, y_pred=np.array([[0.9], [1.0], [-1.0]])))\n    print('--------------')\n    print(deep_speaker_loss(alpha=2, y_true=0, y_pred=np.array([[0.6], [1.0], [0.0]])))\n    print(deep_speaker_loss(alpha=1, y_true=0, y_pred=np.array([[0.6], [1.0], [0.0]])))\n    print(deep_speaker_loss(alpha=0.1, y_true=0, y_pred=np.array([[0.6], [1.0], [0.0]])))\n    print(deep_speaker_loss(alpha=0.2, y_true=0, y_pred=np.array([[0.6], [1.0], [0.0]])))\n\n    print('--------------')\n    print(deep_speaker_loss(alpha=2, y_true=0, y_pred=np.array([[0.9], [1.0], [-1.0]])))\n    print(deep_speaker_loss(alpha=1, y_true=0, y_pred=np.array([[0.9], [1.0], [-1.0]])))\n    print(deep_speaker_loss(alpha=0.1, y_true=0, y_pred=np.array([[0.9], [1.0], [-1.0]])))\n    print(deep_speaker_loss(alpha=0.2, y_true=0, y_pred=np.array([[0.9], [1.0], [-1.0]])))\n"""
utils.py,0,"b""import logging\nimport os\nimport random\nimport shutil\nfrom glob import glob\n\nimport click\nimport dill\nimport numpy as np\nimport pandas as pd\nfrom natsort import natsorted\n\nfrom constants import TRAIN_TEST_RATIO\n\nlogger = logging.getLogger(__name__)\n\n\ndef find_files(directory, ext='wav'):\n    return sorted(glob(directory + f'/**/*.{ext}', recursive=True))\n\n\ndef init_pandas():\n    pd.set_option('display.float_format', lambda x: '%.3f' % x)\n    pd.set_option('display.max_rows', None)\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.width', 1000)\n\n\ndef create_new_empty_dir(directory: str):\n    if os.path.exists(directory):\n        shutil.rmtree(directory)\n    os.makedirs(directory)\n\n\ndef ensure_dir_for_filename(filename: str):\n    ensures_dir(os.path.dirname(filename))\n\n\ndef ensures_dir(directory: str):\n    if len(directory) > 0 and not os.path.exists(directory):\n        os.makedirs(directory)\n\n\nclass ClickType:\n\n    @staticmethod\n    def input_file(writable=False):\n        return click.Path(exists=True, file_okay=True, dir_okay=False,\n                          writable=writable, readable=True, resolve_path=True)\n\n    @staticmethod\n    def input_dir(writable=False):\n        return click.Path(exists=True, file_okay=False, dir_okay=True,\n                          writable=writable, readable=True, resolve_path=True)\n\n    @staticmethod\n    def output_file():\n        return click.Path(exists=False, file_okay=True, dir_okay=False,\n                          writable=True, readable=True, resolve_path=True)\n\n    @staticmethod\n    def output_dir():\n        return click.Path(exists=False, file_okay=False, dir_okay=True,\n                          writable=True, readable=True, resolve_path=True)\n\n\ndef parallel_function(f, sequence, num_threads=None):\n    from multiprocessing import Pool\n    pool = Pool(processes=num_threads)\n    result = pool.map(f, sequence)\n    cleaned = [x for x in result if x is not None]\n    pool.close()\n    pool.join()\n    return cleaned\n\n\ndef load_best_checkpoint(checkpoint_dir):\n    checkpoints = natsorted(glob(os.path.join(checkpoint_dir, '*.h5')))\n    if len(checkpoints) != 0:\n        return checkpoints[-1]\n    return None\n\n\ndef delete_older_checkpoints(checkpoint_dir, max_to_keep=5):\n    assert max_to_keep > 0\n    checkpoints = natsorted(glob(os.path.join(checkpoint_dir, '*.h5')))\n    checkpoints_to_keep = checkpoints[-max_to_keep:]\n    for checkpoint in checkpoints:\n        if checkpoint not in checkpoints_to_keep:\n            os.remove(checkpoint)\n\n\ndef enable_deterministic():\n    print('Deterministic mode enabled.')\n    np.random.seed(123)\n    random.seed(123)\n\n\ndef load_pickle(file):\n    if not os.path.exists(file):\n        return None\n    logger.info(f'Loading PKL file: {file}.')\n    with open(file, 'rb') as r:\n        return dill.load(r)\n\n\ndef load_npy(file):\n    if not os.path.exists(file):\n        return None\n    logger.info(f'Loading NPY file: {file}.')\n    return np.load(file)\n\n\ndef train_test_sp_to_utt(audio, is_test):\n    sp_to_utt = {}\n    for speaker_id, utterances in audio.speakers_to_utterances.items():\n        utterances_files = sorted(utterances.values())\n        train_test_sep = int(len(utterances_files) * TRAIN_TEST_RATIO)\n        sp_to_utt[speaker_id] = utterances_files[train_test_sep:] if is_test else utterances_files[:train_test_sep]\n    return sp_to_utt\n"""
tests/__init__.py,0,b''
tests/batcher_test.py,0,"b""import sys\n\nimport numpy as np\n\nimport triplet_loss\nfrom batcher import KerasFormatConverter, TripletBatcherSelectHardNegatives, TripletBatcher\nfrom constants import NUM_FBANKS, NUM_FRAMES, CHECKPOINTS_TRIPLET_DIR, CHECKPOINTS_SOFTMAX_DIR, BATCH_SIZE\nfrom conv_models import DeepSpeakerModel\nfrom triplet_loss import deep_speaker_loss\nfrom utils import load_best_checkpoint\n\n\ndef predict(x):\n    y = np.tile(np.expand_dims(np.mean(x, axis=(1, 2, 3)), axis=1), (1, 512))\n    return y\n    # norm_y = np.linalg.norm(y, axis=1, ord=2, keepdims=True)\n    # return y / (norm_y + 1e-12)\n\n\ndef main2():\n    num_utterances_per_speaker = 50\n    num_speakers = 100\n    num_samples = num_speakers * num_utterances_per_speaker\n    kx_train = np.zeros(shape=(num_samples, 32, 64, 1))\n    ky_train = np.zeros(shape=(num_samples, num_speakers))\n    for i in range(num_samples):\n        speaker_id = i % num_speakers\n        ky_train[i, speaker_id] = 1\n        kx_train[i] = speaker_id\n    kx_test = np.array(kx_train)\n    ky_test = np.array(ky_train)\n\n    tpshn = TripletBatcherSelectHardNegatives(kx_train, ky_train, kx_test, ky_test, None)\n    tp = TripletBatcher(kx_train, ky_train, kx_test, ky_test)\n    avg = []\n    avg2 = []\n    while True:\n        bx, by = tp.get_batch(BATCH_SIZE, is_test=False)\n        avg.append(float(triplet_loss.deep_speaker_loss(predict(bx), predict(bx))))\n\n        bx, by = tpshn.get_batch(BATCH_SIZE, is_test=False, predict=predict)\n        avg2.append(float(triplet_loss.deep_speaker_loss(predict(bx), predict(bx))))\n\n        print(np.mean(avg), np.mean(avg2))\n\n\ndef main():\n    select = True\n    try:\n        sys.argv[1]\n    except:\n        select = False\n    print('select', select)\n\n    working_dir = '/media/philippe/8TB/deep-speaker'\n    # by construction this  losses should be much higher than the normal losses.\n    # we select batches this way.\n    batch_input_shape = [None, NUM_FRAMES, NUM_FBANKS, 1]\n    print('Testing with the triplet losses.')\n    dsm = DeepSpeakerModel(batch_input_shape, include_softmax=False)\n    triplet_checkpoint = load_best_checkpoint(CHECKPOINTS_TRIPLET_DIR)\n    pre_training_checkpoint = load_best_checkpoint(CHECKPOINTS_SOFTMAX_DIR)\n    if triplet_checkpoint is not None:\n        print(f'Loading triplet checkpoint: {triplet_checkpoint}.')\n        dsm.m.load_weights(triplet_checkpoint)\n    elif pre_training_checkpoint is not None:\n        print(f'Loading pre-training checkpoint: {pre_training_checkpoint}.')\n        # If `by_name` is True, weights are loaded into layers only if they share the\n        # same name. This is useful for fine-tuning or transfer-learning models where\n        # some of the layers have changed.\n        dsm.m.load_weights(pre_training_checkpoint, by_name=True)\n    dsm.m.compile(optimizer='adam', loss=deep_speaker_loss)\n    kc = KerasFormatConverter(working_dir)\n    if select:\n        print('TripletBatcherSelectHardNegatives()')\n        batcher = TripletBatcherSelectHardNegatives(kc.kx_train, kc.ky_train, kc.kx_test, kc.ky_test, dsm)\n    else:\n        print('TripletBatcher()')\n        batcher = TripletBatcher(kc.kx_train, kc.ky_train, kc.kx_test, kc.ky_test)\n    batch_size = BATCH_SIZE\n    losses = []\n    while True:\n        _bx, _by = batcher.get_batch(batch_size, is_test=False)\n        losses.append(dsm.m.evaluate(_bx, _by, verbose=0, batch_size=BATCH_SIZE))\n        print(np.mean(losses))\n\n\nif __name__ == '__main__':\n    main2()\n"""
tests/batcher_test2.py,0,"b""import numpy as np\n\nfrom batcher import LazyTripletBatcher\nfrom constants import NUM_FBANKS, NUM_FRAMES\nfrom conv_models import DeepSpeakerModel\nfrom triplet_loss import deep_speaker_loss\n\n\ndef main2():\n    batch_input_shape = [None, NUM_FRAMES, NUM_FBANKS, 1]\n    dsm = DeepSpeakerModel(batch_input_shape, include_softmax=False)\n    dsm.m.compile(optimizer='adam', loss=deep_speaker_loss)\n    dsm.m.load_weights('/Users/premy/deep-speaker/ResCNN_checkpoint_102.h5', by_name=True)\n    dsm.m.summary()\n    batcher = LazyTripletBatcher(working_dir='/Users/premy/deep-speaker', max_length=NUM_FRAMES, model=dsm)\n    bs = 18\n\n    print(np.mean(\n        [dsm.m.evaluate(*batcher.get_batch_train(batch_size=bs), batch_size=bs, verbose=0) for _ in range(100)]))\n    print(\n        np.mean([dsm.m.evaluate(*batcher.get_batch_test(batch_size=bs), batch_size=bs, verbose=0) for _ in range(100)]))\n    print(np.mean(\n        [dsm.m.evaluate(*batcher.get_random_batch(batch_size=bs, is_test=False), batch_size=bs, verbose=0) for _ in\n         range(100)]))\n    print(np.mean(\n        [dsm.m.evaluate(*batcher.get_random_batch(batch_size=bs, is_test=True), batch_size=bs, verbose=0) for _ in\n         range(100)]))\n\n\nif __name__ == '__main__':\n    main2()\n"""
tests/triplet_loss_test.py,5,"b'import unittest\n\nimport keras.backend as K\nimport numpy as np\nimport tensorflow as tf\nfrom last.triplet_loss import deep_speaker_loss\n\nBATCH_SIZE = 3\n\n\ndef opposite_positive_equal_negative_batch():\n    # should be the highest\n    b = np.random.uniform(low=-1, high=1, size=(BATCH_SIZE * 3, 512))\n    b[0] = -b[6]\n    b[1] = -b[7]\n    b[2] = -b[8]\n    b[3] = -b[9]\n    b[4] = -b[10]\n    b[5] = -b[11]\n    b[12] = b[0]\n    b[13] = b[1]\n    b[14] = b[2]\n    b[15] = b[3]\n    b[16] = b[4]\n    b[17] = b[5]\n    return b\n\n\ndef random_positive_random_negative_batch():\n    # should be high\n    b = np.random.uniform(low=-1, high=1, size=(BATCH_SIZE * 3, 512))\n    return b\n\n\ndef equal_positive_random_negative_batch():\n    # should be low\n    b = np.random.uniform(low=-1, high=1, size=(12, 512))\n    b[0] = b[6]\n    b[1] = b[7]\n    b[2] = b[8]\n    b[3] = b[9]\n    b[4] = b[10]\n    b[5] = b[11]\n    return b\n\n\ndef equal_positive_opposite_negative_batch():\n    # should be the lowest\n    b = np.random.uniform(low=-1, high=1, size=(BATCH_SIZE * 3, 512))\n    b[0] = b[6]\n    b[1] = b[7]\n    b[2] = b[8]\n    b[3] = b[9]\n    b[4] = b[10]\n    b[5] = b[11]\n    b[12] = -b[0]\n    b[13] = -b[1]\n    b[14] = -b[2]\n    b[15] = -b[3]\n    b[16] = -b[4]\n    b[17] = -b[5]\n    return b\n\n\nclass TripleLossTest(unittest.TestCase):\n\n    def test_1(self):\n        # ANCHOR 1 (512,), index = 0\n        # ANCHOR 2 (512,), index = 1\n        # ANCHOR 3 (512,), index = 2\n        # ANCHOR 4 (512,), index = 3\n        # ANCHOR 5 (512,), index = 4\n        # ANCHOR 6 (512,), index = 5\n        # POS EX 1 (512,), index = 6\n        # POS EX 2 (512,), index = 7\n        # POS EX 3 (512,), index = 8\n        # POS EX 4 (512,), index = 9\n        # POS EX 5 (512,), index = 10\n        # POS EX 6 (512,), index = 11\n        # NEG EX 1 (512,), index = 12\n        # NEG EX 2 (512,), index = 13\n        # NEG EX 3 (512,), index = 14\n        # NEG EX 4 (512,), index = 15\n        # NEG EX 5 (512,), index = 16\n        # NEG EX 6 (512,), index = 17\n\n        x2 = 1\n        sess = tf.InteractiveSession()\n        K.set_session(sess)\n\n        highest_loss = deep_speaker_loss(tf.constant(opposite_positive_equal_negative_batch()), x2).eval()\n        high_loss = deep_speaker_loss(tf.constant(random_positive_random_negative_batch()), x2).eval()\n        low_loss = deep_speaker_loss(tf.constant(equal_positive_random_negative_batch()), x2).eval()\n        lowest_loss = deep_speaker_loss(tf.constant(equal_positive_opposite_negative_batch()), x2).eval()\n\n        self.assertTrue(highest_loss >= high_loss >= low_loss >= lowest_loss)\n\n    def test_2(self):\n        b = equal_positive_random_negative_batch()\n        a = 2\n'"
viz/triplet_visualization.py,0,"b""import logging\n\nimport matplotlib\n\nmatplotlib.use('Agg')\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef remove_values_along_axes():\n    from matplotlib import pylab\n    frame = pylab.gca()\n    frame.axes.get_xaxis().set_ticks([])\n    frame.axes.get_yaxis().set_ticks([])\n\n\ndef get_coordinates_from_cosine_similarity(cos_sim):\n    cosine_similarities = np.linspace(1, -1, 1000)\n    i = np.argmin(np.square(cosine_similarities - cos_sim))\n    x, y = find_all_x_y_along_circle()\n    return x[i], y[i]\n\n\ndef find_all_x_y_along_circle():\n    theta = np.linspace(0, 2 * np.pi, 1000)\n\n    # the radius of the circle\n    r = np.sqrt(1)\n\n    # compute x1 and x2\n    x1 = r * np.cos(theta)\n    x2 = r * np.sin(theta)\n    return x1, x2\n\n\ndef newline(p1, p2, color):\n    import matplotlib.pyplot as plt\n    import matplotlib.lines as mlines\n    ax = plt.gca()\n    x_min = p1[0]\n    x_max = p1[1]\n    y_min = p2[0]\n    y_max = p2[1]\n    logging.info('{} {}'.format([x_min, x_max], [y_min, y_max]))\n    l = mlines.Line2D([x_min, x_max], [y_min, y_max], color=color, linestyle='dashdot', linewidth=3)\n    ax.add_line(l)\n    return l\n\n\n# plt.ion()\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nxs, ys = find_all_x_y_along_circle()\nfig, ax = plt.subplots(1)\nax.plot(xs, ys)\nax.set_aspect(1)\n\nx_anchor, y_anchor = get_coordinates_from_cosine_similarity(0)  # anchor\nnewline([0, x_anchor], [0, y_anchor], color='blue')\n\nx_pos, y_pos = get_coordinates_from_cosine_similarity(0.1)  # anchor\nnewline([0, x_pos], [0, y_pos], color='green')\n\nx_neg, y_neg = get_coordinates_from_cosine_similarity(-0.5)  # anchor\nnewline([0, x_neg], [0, y_neg], color='red')\n\nplt.legend(('', 'AnchorEx', 'PositiveEx', 'NegativeEx'), loc='lower right')\n\n#\n# for cos_sim in np.linspace(1, -1, 100):\n#     x_i, y_i = get_coordinates_from_cosine_similarity(cos_sim)\n#     newline([0, x_i], [0, y_i], color='green')\n#     plt.draw()\n#     plt.pause(0.0001)\n\nremove_values_along_axes()\nprint('Save to anchor.png')\nplt.savefig('anchor.png')\nplt.close(fig)\n"""
