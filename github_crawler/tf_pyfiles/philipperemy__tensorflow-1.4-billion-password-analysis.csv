file_path,api_count,code
data_gen.py,0,"b'import pickle\nfrom collections import Counter\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom train_constants import ENCODING_MAX_PASSWORD_LENGTH, ENCODING_MAX_SIZE_VOCAB\n\n\ndef get_indices_token():\n    return pickle.load(open(\'/tmp/indices_token.pkl\', \'rb\'))\n\n\ndef get_token_indices():\n    return pickle.load(open(\'/tmp/token_indices.pkl\', \'rb\'))\n\n\ndef get_vocab_size():\n    return len(get_token_indices())\n\n\ndef discard_password(password):\n    return len(password) > ENCODING_MAX_PASSWORD_LENGTH or \' \' in password\n\n\nclass CharacterTable(object):\n    """"""Given a set of characters:\n    + Encode them to a one hot integer representation\n    + Decode the one hot integer representation to their character output\n    + Decode a vector of probabilities to their character output\n    """"""\n\n    def __init__(self, chars):\n        """"""Initialize character table.\n        # Arguments\n            chars: Characters that can appear in the input.\n        """"""\n        self.chars = sorted(set(chars))\n        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n\n    def encode(self, C, num_rows):\n        """"""One hot encode given string C.\n        # Arguments\n            num_rows: Number of rows in the returned one hot encoding. This is\n                used to keep the # of rows for each data the same.\n        """"""\n        x = np.zeros((num_rows, len(self.chars)))\n        for i in range(num_rows):\n            try:\n                c = C[i]\n                if c not in self.char_indices:\n                    x[i, self.char_indices[\'\xef\xbc\x9f\']] = 1\n                else:\n                    x[i, self.char_indices[c]] = 1\n            except IndexError:\n                x[i, self.char_indices[\' \']] = 1\n        return x\n\n    def decode(self, x, calc_argmax=True):\n        if calc_argmax:\n            x = x.argmax(axis=-1)\n        return \'\'.join(self.indices_char[x] for x in x)\n\n\nclass colors:\n    ok = \'\\033[92m\'\n    fail = \'\\033[91m\'\n    close = \'\\033[0m\'\n\n\ndef get_chars_and_ctable():\n    chars = \'\'.join(list(get_token_indices().values()))\n    ctable = CharacterTable(chars)\n    return chars, ctable\n\n\ndef build_vocabulary(training_filename):\n    vocabulary = {}\n    print(\'Reading file {}.\'.format(training_filename))\n    with open(training_filename, \'rb\') as r:\n        for l in tqdm(r.readlines(), desc=\'Build Vocabulary\'):\n            line_id, x, y = l.decode(\'utf8\').strip().split(\' ||| \')\n            if discard_password(y) or discard_password(x):\n                continue\n            for element in list(y + x):\n                if element not in vocabulary:\n                    vocabulary[element] = 0\n                vocabulary[element] += 1\n    vocabulary_sorted_list = sorted(dict(Counter(vocabulary).most_common(ENCODING_MAX_SIZE_VOCAB)).keys())\n    oov_char = \'\xef\xbc\x9f\'\n    pad_char = \' \'\n    print(\'Out of vocabulary (OOV) char is {}\'.format(oov_char))\n    print(\'Pad char is ""{}""\'.format(pad_char))\n    vocabulary_sorted_list.append(oov_char)  # out of vocabulary.\n    vocabulary_sorted_list.append(pad_char)  # pad char.\n    print(\'Vocabulary = \' + \' \'.join(vocabulary_sorted_list))\n    token_indices = dict((c, i) for (c, i) in enumerate(vocabulary_sorted_list))\n    indices_token = dict((i, c) for (c, i) in enumerate(vocabulary_sorted_list))\n    assert len(token_indices) == len(indices_token)\n\n    with open(\'/tmp/token_indices.pkl\', \'wb\') as w:\n        pickle.dump(obj=token_indices, file=w)\n\n    with open(\'/tmp/indices_token.pkl\', \'wb\') as w:\n        pickle.dump(obj=indices_token, file=w)\n\n    print(\'Done... File is /tmp/token_indices.pkl\')\n    print(\'Done... File is /tmp/indices_token.pkl\')\n\n\ndef stream_from_file(training_filename):\n    with open(training_filename, \'rb\') as r:\n        for l in r.readlines():\n            _, x, y = l.decode(\'utf8\').strip().split(\' ||| \')\n            if discard_password(y) or discard_password(x):\n                continue\n            yield x.strip(), y.strip()\n\n\nclass LazyDataLoader:\n    def __init__(self, training_filename):\n        self.training_filename = training_filename\n        self.stream = stream_from_file(self.training_filename)\n\n    def next(self):\n        try:\n            return next(self.stream)\n        except:\n            self.stream = stream_from_file(self.training_filename)\n            return self.next()\n\n    def statistics(self):\n        max_len_value_x = 0\n        max_len_value_y = 0\n        num_lines = 0\n        self.stream = stream_from_file(self.training_filename)\n        for x, y in self.stream:\n            max_len_value_x = max(max_len_value_x, len(x))\n            max_len_value_y = max(max_len_value_y, len(y))\n            num_lines += 1\n\n        print(\'max_len_value_x =\', max_len_value_x)\n        print(\'max_len_value_y =\', max_len_value_y)\n        print(\'num_lines =\', num_lines)\n        return max_len_value_x, max_len_value_y, num_lines\n\n\nif __name__ == \'__main__\':\n    # how to use it.\n    ldl = LazyDataLoader(\'/home/premy/BreachCompilationAnalysis/edit-distances/1.csv\')\n    print(ldl.statistics())\n    while True:\n        print(ldl.next())\n'"
processing_callbacks.py,0,"b""import json\n\nimport editdistance\nimport numpy as np\nimport os\n\nfrom shp import find_shortest_hamiltonian_path_in_complete_graph\n\n\nclass Callback:\n    def __init__(self):\n        pass\n\n    def call(self, emails_passwords):\n        # emails_passwords = list of tuples\n        pass\n\n\nclass ReducePasswordsOnSimilarEmailsCallback(Callback):\n    NAME = 'reduce-passwords-on-similar-emails'\n\n    def __init__(self, persisted_filename, output_folder):\n        super().__init__()\n        self.cache = {}\n        self.cache_key_edit_distance_keep_user_struct = {}\n        self.cache_key_edit_distance_list = {}\n        self.filename = persisted_filename\n        self.output_folder = output_folder\n\n    def _finalize_cache(self):\n        keys = list(self.cache.keys())\n        for key in keys:\n            orig_password_list = list(self.cache[key])\n            del self.cache[key]\n            if len(orig_password_list) > 1:\n                shp = list(find_shortest_hamiltonian_path_in_complete_graph(orig_password_list, False))\n                if len(shp) == 0:\n                    continue  # shortest_hamiltonian_path did not return well.\n\n                edit_distances = []\n                for a, b in zip(shp, shp[1:]):\n                    ed = editdistance.eval(a, b)\n                    edit_distances.append(ed)\n                    if ed not in self.cache_key_edit_distance_list:\n                        self.cache_key_edit_distance_list[ed] = []\n                    self.cache_key_edit_distance_list[ed].append((a, b))\n\n                self.cache[key] = {}\n                self.cache[key]['password'] = shp\n                self.cache[key]['edit_distance'] = [0] + edit_distances\n                mean_edit_distance_key = float('{0:.2f}'.format(np.mean(edit_distances)))\n                if mean_edit_distance_key not in self.cache_key_edit_distance_keep_user_struct:\n                    self.cache_key_edit_distance_keep_user_struct[mean_edit_distance_key] = []\n                new_elt = {'password': self.cache[key]['password'],\n                           'edit_distance': self.cache[key]['edit_distance'],\n                           'email': key}\n                self.cache_key_edit_distance_keep_user_struct[mean_edit_distance_key].append(new_elt)\n\n    def call(self, emails_passwords):\n        for (email, password) in emails_passwords:\n            if email not in self.cache:\n                self.cache[email] = set()\n            self.cache[email].add(password)\n\n    def persist(self):\n        self._finalize_cache()\n        with open(self.filename + '_per_user.json', 'w') as w:\n            json.dump(fp=w, obj=self.cache_key_edit_distance_keep_user_struct, indent=4, sort_keys=True)\n\n        sep = ' ||| '\n        for edit_distance in sorted(self.cache_key_edit_distance_list):\n            def csv_line_format(x):\n                return str(edit_distance) + sep + x[0] + sep + x[1] + '\\n'\n\n            output_dir = os.path.join(os.path.expanduser(self.output_folder), 'edit-distances')\n            if not os.path.exists(output_dir):\n                os.makedirs(output_dir)\n            csv_file = os.path.join(output_dir, str(edit_distance) + '.csv')\n            # print('Updating: ' + csv_file)\n            with open(csv_file, encoding='utf8', mode='a') as w:\n                password_pairs = self.cache_key_edit_distance_list[edit_distance]\n                lines = list(map(csv_line_format, password_pairs))\n                w.writelines(lines)\n"""
run_data_processing.py,0,"b'import argparse\n\nfrom processing_callbacks import ReducePasswordsOnSimilarEmailsCallback\nfrom utils import process\n\nparser = argparse.ArgumentParser(\'Data Processing Tool.\')\nparser.add_argument(\'--breach_compilation_folder\', type=str,\n                    help=\'BreachCompilation/ folder containing the 1.4 billion passwords dataset.\', required=True)\nparser.add_argument(\'--output_folder\', type=str,\n                    default=\'~/BreachCompilationAnalysis\',\n                    help=\'Output folder containing the generated datasets.\')\nparser.add_argument(\'--max_num_files\', type=int,\n                    help=\'Maximum number of files to read. The entire dataset contains around 2000 files.\'\n                         \'Can be useful to create mini datasets for the models.\')\n\n\n# ------------------------------------------------------------------------------\n# EXPLANATION\n# ------------------------------------------------------------------------------\n# INPUT: BreachCompilation/\n# BreachCompilation is organized as:\n#\n# a/          - folder of emails starting with a\n# a/a         - file of emails starting with aa\n# a/b\n# a/d\n# ...\n# z/\n# ...\n# z/y\n# z/z\n# ------------------------------------------------------------------------------\n# OUTPUT: - BreachCompilationAnalysis/edit-distance/1.csv\n#         - BreachCompilationAnalysis/edit-distance/2.csv\n#         - BreachCompilationAnalysis/edit-distance/3.csv\n#         [...]\n#         > cat 1.csv\n#             1 ||| samsung94 ||| samsung94@\n#             1 ||| 040384alexej ||| 040384alexey\n#             1 ||| HoiHalloDoeii14 ||| hoiHalloDoeii14\n#             1 ||| hoiHalloDoeii14 ||| hoiHalloDoeii13\n#             1 ||| hoiHalloDoeii13 ||| HoiHalloDoeii13\n#             1 ||| 8znachnuu ||| 7znachnuu\n#         EXPLANATION: edit-distance/ contains the passwords pairs sorted by edit distances.\n#         1.csv contains all pairs with edit distance = 1 (exactly one addition, substitution or deletion).\n#         2.csv => edit distance = 2, and so on.\n#\n#         - BreachCompilationAnalysis/ReducePasswordsOnSimilarEmailsCallback/99_per_user.json\n#         - BreachCompilationAnalysis/ReducePasswordsOnSimilarEmailsCallback/9j_per_user.json\n#         - BreachCompilationAnalysis/ReducePasswordsOnSimilarEmailsCallback/9a_per_user.json\n#         [...]\n#         > cat 96_per_user.json\n#         {\n#             ""1.0"": [\n#             {\n#                 ""edit_distance"": [\n#                     0,\n#                     1\n#                 ],\n#                 ""email"": ""96-000@mail.ru"",\n#                 ""password"": [\n#                     ""090698d"",\n#                     ""090698D""\n#                 ]\n#             },\n#         {\n#                 ""edit_distance"": [\n#                     0,\n#                     1\n#                 ],\n#                 ""email"": ""96-96.1996@mail.ru"",\n#                 ""password"": [\n#                     ""5555555555q"",\n#                     ""5555555555Q""\n#                 ]\n#          }\n#         EXPLANATION: ReducePasswordsOnSimilarEmailsCallback/ contains files sorted by the first 2 letters of\n#         the email address. For example 96-000@mail.ru will be located in 96_per_user.json\n#         Each file lists all the passwords grouped by user and by edit distance.\n#         For example, 96-000@mail.ru had 2 passwords: 090698d and 090698D. The edit distance between them is 1.\n#         The edit_distance and the password arrays are of the same length, hence, a first 0 in the edit distance array.\n#         Those files are useful to model how users change passwords over time.\n#         We can\'t recover which one was the first password, but a shortest hamiltonian path algorithm is run\n#         to detect the most probably password ordering for a user. For example:\n#         hello => hello1 => hell@1 => hell@11 is the shortest path.\n#         We assume that users are lazy by nature and that they prefer to change their password by the lowest number\n#         of characters.\n\n\ndef run():\n    # example: --breach_compilation_folder /media/philippe/DATA/BreachCompilation/\n    # --max_num_files 100 --output_folder ~/BreachCompilationAnalysis2\n    arg_p = parser.parse_args()\n    process(breach_compilation_folder=arg_p.breach_compilation_folder,\n            num_files=arg_p.max_num_files,\n            output_folder=arg_p.output_folder,\n            on_file_read_call_back_class=ReducePasswordsOnSimilarEmailsCallback)\n\n\nif __name__ == \'__main__\':\n    run()\n'"
run_encoding.py,0,"b""import argparse\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom data_gen import LazyDataLoader, build_vocabulary, get_chars_and_ctable\n\nparser = argparse.ArgumentParser('Data Encoding Tool.')\nparser.add_argument('--training_filename', type=str,\n                    help='Result of run_data_processing.py. '\n                         'Something like: /home/premy/BreachCompilationAnalysis/edit-distances/1.csv',\n                    required=True)\n# parser.add_argument('--encoding_output_folder', type=str, help='Will be used for training')\n\narg_p = parser.parse_args()\n\nprint('Building vocabulary...')\n\nbuild_vocabulary(arg_p.training_filename)\n\nprint('Vectorization...')\n\nDATA_LOADER = LazyDataLoader(arg_p.training_filename)\n\n_, _, training_records_count = DATA_LOADER.statistics()\n\n# TOKEN_INDICES = get_token_indices()\n\nchars, c_table = get_chars_and_ctable()\n\ninputs = []\ntargets = []\nprint('Generating data...')\nfor i in tqdm(range(training_records_count), desc='Generating inputs and targets'):\n    x_, y_ = DATA_LOADER.next()\n    # Pad the data with spaces such that it is always MAXLEN.\n    inputs.append(x_)\n    targets.append(y_)\n\nnp.savez_compressed('/tmp/x_y.npz', inputs=inputs, targets=targets)\n\nprint('Done... File is /tmp/x_y.npz')\n"""
shp.py,0,"b""import itertools\nimport json\nfrom itertools import combinations\n\nimport editdistance\nimport numpy as np\n\nPASSWORDS = ['hello1', 'hello22', 'h@llo22', 'h@llo223']\n\n\ndef find_shortest_hamiltonian_path_in_complete_graph(passwords, debug=True):\n    # complexity is paramount! This script runs in factorial(n)\n\n    if len(passwords) > 6:  # 6! = 720 combinations.\n        return []\n\n    map_edit_distance = {}\n\n    # shortest hamiltonian path in complete graph. NP-complete.\n    for combo in combinations(passwords, 2):  # 2 for pairs, 3 for triplets, etc\n        ed = editdistance.eval(combo[0], combo[1])\n        if debug:\n            print(combo[0], combo[1], ed)\n        map_edit_distance[(combo[0], combo[1])] = ed\n        map_edit_distance[(combo[1], combo[0])] = ed\n\n    # factorial(n)\n    # permutations = list(itertools.permutations(passwords))\n    permutations = list(filter(lambda x: len(x[0]) == min([len(a) for a in x]),\n                               list(itertools.permutations(passwords))))\n\n    all_solutions = {}\n    for permutation in permutations:\n        full_ed = 0\n        for a, b in zip(permutation, permutation[1:]):\n            full_ed += map_edit_distance[(a, b)]\n\n        if debug:\n            print(full_ed, permutation)\n\n        if full_ed not in all_solutions:\n            all_solutions[full_ed] = []\n        all_solutions[full_ed].append(permutation)\n\n    if debug:\n        print(json.dumps(all_solutions, indent=2))\n\n    lowest_ed = sorted(all_solutions.keys())[0]\n\n    if debug:\n        print(lowest_ed)\n    # we consider that the first password is the easiest one (at least the shortest one).\n    best_solutions = all_solutions[lowest_ed]\n\n    if debug:\n        print(best_solutions)\n\n    final_solution = best_solutions[np.argmin([len(bs[0]) for bs in best_solutions])]\n\n    if debug:\n        print(final_solution)\n\n    return final_solution\n\n\nif __name__ == '__main__':\n    print(find_shortest_hamiltonian_path_in_complete_graph(PASSWORDS, False))\n"""
train_constants.py,0,"b'#\n# train_constants.py: Contains the constants necessary for the encoding and the training phases.\n#\n\n# Maximum password length. Passwords greater than this length will be discarded during the encoding phase.\nENCODING_MAX_PASSWORD_LENGTH = 12\n\n# Maximum number of characters for encoding. By default, we use the 80 most frequent characters and\n# we bin the other ones in a OOV (out of vocabulary) group.\nENCODING_MAX_SIZE_VOCAB = 80\n'"
train_model.py,0,"b""# -*- coding: utf-8 -*-\nimport argparse\nimport multiprocessing\nfrom collections import Counter\n\nimport numpy as np\nimport os\nfrom keras import layers\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\n\nfrom data_gen import get_chars_and_ctable, colors\nfrom train_constants import ENCODING_MAX_PASSWORD_LENGTH, ENCODING_MAX_SIZE_VOCAB\n\nINPUT_MAX_LEN = ENCODING_MAX_PASSWORD_LENGTH\nOUTPUT_MAX_LEN = ENCODING_MAX_PASSWORD_LENGTH\n\ntry:\n    chars, c_table = get_chars_and_ctable()\nexcept FileNotFoundError:\n    print('Run first run_encoding.py to generate the required files.')\n    exit(1)\n\n\ndef get_arguments(parser):\n    args = None\n    try:\n        args = parser.parse_args()\n    except:\n        parser.print_help()\n        exit(1)\n    return args\n\n\ndef get_script_arguments():\n    parser = argparse.ArgumentParser(description='Training a password model.')\n    # Something like: /home/premy/BreachCompilationAnalysis/edit-distances/1.csv\n    # Result of run_data_processing.py.\n    # parser.add_argument('--training_filename', required=True, type=str)\n    parser.add_argument('--hidden_size', default=256, type=int)\n    parser.add_argument('--batch_size', default=256, type=int)\n    args = get_arguments(parser)\n    print(args)\n    return args\n\n\ndef gen_large_chunk_single_thread(inputs_, targets_, chunk_size):\n    random_indices = np.random.choice(a=range(len(inputs_)), size=chunk_size, replace=True)\n    sub_inputs = inputs_[random_indices]\n    sub_targets = targets_[random_indices]\n\n    x = np.zeros((chunk_size, ENCODING_MAX_PASSWORD_LENGTH, len(chars)), dtype=np.bool)\n    y = np.zeros((chunk_size, ENCODING_MAX_PASSWORD_LENGTH, len(chars)), dtype=np.bool)\n\n    for i_, element in enumerate(sub_inputs):\n        x[i_] = c_table.encode(element, ENCODING_MAX_PASSWORD_LENGTH)\n    for i_, element in enumerate(sub_targets):\n        y[i_] = c_table.encode(element, ENCODING_MAX_PASSWORD_LENGTH)\n\n    split_at = len(x) - len(x) // 10\n    (x_train, x_val) = x[:split_at], x[split_at:]\n    (y_train, y_val) = y[:split_at], y[split_at:]\n\n    return x_train, y_train, x_val, y_val\n\n\ndef predict_top_most_likely_passwords_monte_carlo(model_, rowx_, n_, mc_samples=10000):\n    samples = predict_top_most_likely_passwords(model_, rowx_, mc_samples)\n    return dict(Counter(samples).most_common(n_)).keys()\n\n\ndef predict_top_most_likely_passwords(model_, rowx_, n_):\n    p_ = model_.predict(rowx_, batch_size=32, verbose=0)[0]\n    most_likely_passwords = []\n    for ii in range(n_):\n        # of course should take the edit distance constraint.\n        pa = np.array([np.random.choice(a=range(ENCODING_MAX_SIZE_VOCAB + 2), size=1, p=p_[jj, :])\n                       for jj in range(ENCODING_MAX_PASSWORD_LENGTH)]).flatten()\n        most_likely_passwords.append(c_table.decode(pa, calc_argmax=False))\n    return most_likely_passwords\n    # Could sample 1000 and take the most_common()\n\n\ndef gen_large_chunk_multi_thread(inputs_, targets_, chunk_size):\n    ''' This function is actually slower than gen_large_chunk_single_thread()'''\n\n    def parallel_function(f, sequence, num_threads=None):\n        from multiprocessing.pool import ThreadPool\n        pool = ThreadPool(processes=num_threads)\n        result = pool.map(f, sequence)\n        cleaned = np.array([x for x in result if x is not None])\n        pool.close()\n        pool.join()\n        return cleaned\n\n    random_indices = np.random.choice(a=range(len(inputs_)), size=chunk_size, replace=True)\n    sub_inputs = inputs_[random_indices]\n    sub_targets = targets_[random_indices]\n\n    def encode(elt):\n        return c_table.encode(elt, ENCODING_MAX_PASSWORD_LENGTH)\n\n    num_threads = multiprocessing.cpu_count() // 2\n    x = parallel_function(encode, sub_inputs, num_threads=num_threads)\n    y = parallel_function(encode, sub_targets, num_threads=num_threads)\n\n    split_at = len(x) - len(x) // 10\n    (x_train, x_val) = x[:split_at], x[split_at:]\n    (y_train, y_val) = y[:split_at], y[split_at:]\n\n    return x_train, y_train, x_val, y_val\n\n\nif not os.path.exists('/tmp/x_y.npz'):\n    raise Exception('Please run the vectorization script before.')\n\nprint('Loading data from prefetch...')\ndata = np.load('/tmp/x_y.npz')\ninputs = data['inputs']\ntargets = data['targets']\n\nprint('Data:')\nprint(inputs.shape)\nprint(targets.shape)\n\nARGS = get_script_arguments()\n\n# Try replacing GRU.\nRNN = layers.LSTM\nHIDDEN_SIZE = ARGS.hidden_size\nBATCH_SIZE = ARGS.batch_size\n\nprint('Build model...')\n\n\ndef model_1():\n    num_layers = 1\n    m = Sequential()\n    m.add(RNN(HIDDEN_SIZE, input_shape=(INPUT_MAX_LEN, len(chars))))\n    m.add(layers.RepeatVector(OUTPUT_MAX_LEN))\n    for _ in range(num_layers):\n        m.add(RNN(HIDDEN_SIZE, return_sequences=True))\n    m.add(layers.TimeDistributed(layers.Dense(len(chars))))\n    m.add(layers.Activation('softmax'))\n    return m\n\n\ndef model_2():\n    # too big in Memory!\n    m = Sequential()\n    from keras.layers.core import Flatten, Dense, Reshape\n    from keras.layers.wrappers import TimeDistributed\n    m.add(Flatten(input_shape=(INPUT_MAX_LEN, len(chars))))\n    m.add(Dense(OUTPUT_MAX_LEN * len(chars)))\n    m.add(Reshape((OUTPUT_MAX_LEN, len(chars))))\n    m.add(TimeDistributed(Dense(len(chars), activation='softmax')))\n    return m\n\n\ndef model_3():\n    m = Sequential()\n    from keras.layers.core import Dense, Reshape\n    from keras.layers.wrappers import TimeDistributed\n    m.add(RNN(HIDDEN_SIZE, input_shape=(INPUT_MAX_LEN, len(chars))))\n    m.add(Dense(OUTPUT_MAX_LEN * len(chars), activation='relu'))\n    m.add(Dropout(0.5))\n    m.add(Dense(OUTPUT_MAX_LEN * len(chars), activation='relu'))\n    m.add(Dropout(0.5))\n    m.add(Reshape((OUTPUT_MAX_LEN, len(chars))))\n    m.add(TimeDistributed(Dense(len(chars), activation='softmax')))\n    return m\n\n\nmodel = model_3()\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.summary()\n\n# Train the model each generation and show predictions against the validation data set.\nfor iteration in range(1, int(1e9)):\n    x_train, y_train, x_val, y_val = gen_large_chunk_single_thread(inputs, targets, chunk_size=BATCH_SIZE * 500)\n    print()\n    print('-' * 50)\n    print('Iteration', iteration)\n    # TODO: we need to update the loss to take into account that x!=y.\n    # TODO: We could actually if it's an ADD, DEL or MOD.\n    # TODO: Big improvement. We always have hello => hello1 right but never hello => 1hello\n    # It's mainly because we pad after and never before. So the model has to shift all the characters.\n    # And the risk for doing so is really since its a character based cross entropy loss.\n    # Even though accuracy is very high it does not really prove things since Identity would have a high\n    # Accuracy too.\n    # One way to do that is to predict the ADD/DEL/MOD op along with the character of interest and the index\n    # The index can just be a softmax over the indices of the password array, augmented (with a convention)\n    model.fit(x_train, y_train,\n              batch_size=BATCH_SIZE,\n              epochs=5,\n              validation_data=(x_val, y_val))\n    # Select 10 samples from the validation set at random so we can visualize\n    # errors.\n    for i in range(10):\n        ind = np.random.randint(0, len(x_val))\n        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]  # replace by x_val, y_val\n        preds = model.predict_classes(rowx, verbose=0)\n        q = c_table.decode(rowx[0])\n        correct = c_table.decode(rowy[0])\n        guess = c_table.decode(preds[0], calc_argmax=False)\n        top_passwords = predict_top_most_likely_passwords_monte_carlo(model, rowx, 100)\n        # p = model.predict(rowx, batch_size=32, verbose=0)[0]\n        # p.shape (12, 82)\n        # [np.random.choice(a=range(82), size=1, p=p[i, :]) for i in range(12)]\n        # s = [np.random.choice(a=range(82), size=1, p=p[i, :])[0] for i in range(12)]\n        # c_table.decode(s, calc_argmax=False)\n        # Could sample 1000 and take the most_common()\n        print('new    :', correct)\n        print('former :', q)\n        print('guess  :', guess, end=' ')\n\n        # if correct == guess:\n        if correct.strip() in [vv.strip() for vv in top_passwords]:\n            print(colors.ok + '\xe2\x98\x91' + colors.close)\n        else:\n            print(colors.fail + '\xe2\x98\x92' + colors.close)\n        print('top    :', ', '.join(top_passwords))\n        print('---')\n"""
utils.py,0,"b""from glob import glob\n\nimport os\nimport shutil\nfrom slugify import slugify\nfrom tqdm import tqdm\n\nfrom processing_callbacks import ReducePasswordsOnSimilarEmailsCallback\n\n\ndef extract_emails_and_passwords(txt_lines):\n    emails_passwords = []\n    for txt_line in txt_lines:\n        try:\n            if '@' in txt_line:  # does it contain an email address?\n                if all([char in txt_line for char in [':', ';']]):  # which separator is it? : or ;?\n                    separator = ':'\n                elif ':' in txt_line:  # '_---madc0w---_@live.com:iskandar89\n                    separator = ':'\n                elif ';' in txt_line:  # '_---lelya---_@mail.ru;ol1391ga\n                    separator = ';'\n                else:\n                    continue\n\n                strip_txt_line = txt_line.strip()\n                email, password = strip_txt_line.split(separator)\n                emails_passwords.append((email, password))\n        except:\n            pass\n    return emails_passwords\n\n\ndef process(breach_compilation_folder,\n            output_folder='~/BreachCompilationAnalysis',\n            num_files=None,\n            on_file_read_call_back_class=ReducePasswordsOnSimilarEmailsCallback):\n    breach_compilation_folder = os.path.join(os.path.expanduser(breach_compilation_folder), 'data')\n    all_filenames = glob(breach_compilation_folder + '/**/*', recursive=True)\n    all_filenames = sorted(list(filter(os.path.isfile, all_filenames)))\n    callback_class_name = on_file_read_call_back_class.NAME\n    analysis_folder = os.path.expanduser(output_folder)\n    callback_output_dir = os.path.join(analysis_folder, callback_class_name)\n    try:\n        print('OUTPUT FOLDER: {0}.'.format(analysis_folder))\n        shutil.rmtree(analysis_folder)\n    except:\n        pass\n    os.makedirs(callback_output_dir)\n\n    print('FOUND: {0} unique files in {1}.'.format(len(all_filenames), breach_compilation_folder))\n    if num_files is not None:\n        print('TRUNCATE DATASET TO: {0} files.'.format(num_files))\n        all_filenames = all_filenames[0:num_files]\n\n    bar = tqdm(all_filenames)\n    for current_filename in bar:\n        if os.path.isfile(current_filename):\n            suffix = slugify(current_filename.split('data')[-1])\n            output_filename = os.path.join(callback_output_dir, suffix)\n            callback = on_file_read_call_back_class(output_filename, analysis_folder)\n            with open(current_filename, 'r', encoding='utf8', errors='ignore') as r:\n                lines = r.readlines()\n                emails_passwords = extract_emails_and_passwords(lines)\n                callback.call(emails_passwords)\n            bar.set_description('Processing {0} passwords for {1}'.format(len(callback.cache), current_filename))\n            callback.persist()\n    bar.close()\n    print('DONE. SUCCESS.')\n    print('OUTPUT: Dataset was generated at: {0}.'.format(analysis_folder))\n"""
