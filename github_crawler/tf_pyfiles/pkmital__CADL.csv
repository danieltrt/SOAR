file_path,api_count,code
jupyter_notebook_config.py,0,"b""import os\nfrom IPython.lib import passwd\n\nc.NotebookApp.ip = '*'\nc.NotebookApp.port = 8888\nc.NotebookApp.open_browser = False\nc.MultiKernelManager.default_kernel_name = 'python3'\n\n# sets a password if PASSWORD is set in the environment\nif 'PASSWORD' in os.environ:\n    c.NotebookApp.password = passwd(os.environ['PASSWORD'])\n    del os.environ['PASSWORD']\n"""
session-1/libs/__init__.py,0,b''
session-1/libs/utils.py,19,"b'""""""Utilities used in the Kadenze Academy Course on Deep Learning w/ Tensorflow.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nParag K. Mital\n\nCopyright Parag K. Mital, June 2016.\n""""""\nfrom __future__ import print_function\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport urllib\nimport numpy as np\nimport zipfile\nimport os\nfrom scipy.misc import imsave\n\n\ndef imcrop_tosquare(img):\n    """"""Make any image a square image.\n    Parameters\n    ----------\n    img : np.ndarray\n        Input image to crop, assumed at least 2d.\n    Returns\n    -------\n    crop : np.ndarray\n        Cropped image.\n    """"""\n    size = np.min(img.shape[:2])\n    extra = img.shape[:2] - size\n    crop = img\n    for i in np.flatnonzero(extra):\n        crop = np.take(crop, extra[i] // 2 + np.r_[:size], axis=i)\n    return crop\n\n\ndef montage(images, saveto=\'montage.png\'):\n    """"""Draw all images as a montage separated by 1 pixel borders.\n\n    Also saves the file to the destination specified by `saveto`.\n\n    Parameters\n    ----------\n    images : numpy.ndarray\n        Input array to create montage of.  Array should be:\n        batch x height x width x channels.\n    saveto : str\n        Location to save the resulting montage image.\n\n    Returns\n    -------\n    m : numpy.ndarray\n        Montage image.\n    """"""\n    if isinstance(images, list):\n        images = np.array(images)\n    img_h = images.shape[1]\n    img_w = images.shape[2]\n    n_plots = int(np.ceil(np.sqrt(images.shape[0])))\n    if len(images.shape) == 4 and images.shape[3] == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 3)) * 0.5\n    elif len(images.shape) == 4 and images.shape[3] == 1:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 1)) * 0.5\n    elif len(images.shape) == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1)) * 0.5\n    else:\n        raise ValueError(\'Could not parse image shape of {}\'.format(\n            images.shape))\n    for i in range(n_plots):\n        for j in range(n_plots):\n            this_filter = i * n_plots + j\n            if this_filter < images.shape[0]:\n                this_img = images[this_filter]\n                m[1 + i + i * img_h:1 + i + (i + 1) * img_h,\n                  1 + j + j * img_w:1 + j + (j + 1) * img_w] = this_img\n    imsave(arr=np.squeeze(m), name=saveto)\n    return m\n\n\ndef get_celeb_files():\n    """"""Downloads the first 100 images of the celeb dataset.\n\n    Files will be placed in a directory \'img_align_celeba\' if one\n    doesn\'t exist.\n\n    Returns\n    -------\n    files : list of strings\n        Locations to the first 100 images of the celeb net dataset.\n    """"""\n    # Create a directory\n    if not os.path.exists(\'img_align_celeba\'):\n        os.mkdir(\'img_align_celeba\')\n\n    # Now perform the following 100 times:\n    for img_i in range(1, 101):\n\n        # create a string using the current loop counter\n        f = \'000%03d.jpg\' % img_i\n\n        if os.path.exists(\'img_align_celeba/\'+f):\n            continue\n\n        # and get the url with that string appended the end\n        url = \'https://s3.amazonaws.com/cadl/celeb-align/\' + f\n\n        # We\'ll print this out to the console so we can see how far we\'ve gone\n        print(url, end=\'\\r\')\n\n        # And now download the url to a location inside our new directory\n        urllib.request.urlretrieve(url, os.path.join(\'img_align_celeba\', f))\n\n    files = [os.path.join(\'img_align_celeba\', file_i)\n             for file_i in os.listdir(\'img_align_celeba\')\n             if \'.jpg\' in file_i]\n    return files\n\n\ndef get_celeb_imgs():\n    """"""Loads the first 100 images of the celeb dataset.\n\n    Returns\n    -------\n    imgs : list of np.ndarray\n        List of the first 100 images from the celeb dataset\n    """"""\n    return [plt.imread(f_i) for f_i in get_celeb_files()]\n\n\ndef gauss(mean, stddev, ksize):\n    """"""Uses Tensorflow to compute a Gaussian Kernel.\n\n    Parameters\n    ----------\n    mean : float\n        Mean of the Gaussian (e.g. 0.0).\n    stddev : float\n        Standard Deviation of the Gaussian (e.g. 1.0).\n    ksize : int\n        Size of kernel (e.g. 16).\n\n    Returns\n    -------\n    kernel : np.ndarray\n        Computed Gaussian Kernel using Tensorflow.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        x = tf.linspace(-3.0, 3.0, ksize)\n        z = (tf.exp(tf.negative(tf.pow(x - mean, 2.0) /\n                           (2.0 * tf.pow(stddev, 2.0)))) *\n             (1.0 / (stddev * tf.sqrt(2.0 * 3.1415))))\n        return z.eval()\n\n\ndef gauss2d(mean, stddev, ksize):\n    """"""Uses Tensorflow to compute a 2D Gaussian Kernel.\n\n    Parameters\n    ----------\n    mean : float\n        Mean of the Gaussian (e.g. 0.0).\n    stddev : float\n        Standard Deviation of the Gaussian (e.g. 1.0).\n    ksize : int\n        Size of kernel (e.g. 16).\n\n    Returns\n    -------\n    kernel : np.ndarray\n        Computed 2D Gaussian Kernel using Tensorflow.\n    """"""\n    z = gauss(mean, stddev, ksize)\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        z_2d = tf.matmul(tf.reshape(z, [ksize, 1]), tf.reshape(z, [1, ksize]))\n        return z_2d.eval()\n\n\ndef convolve(img, kernel):\n    """"""Uses Tensorflow to convolve a 4D image with a 4D kernel.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        4-dimensional image shaped N x H x W x C\n    kernel : np.ndarray\n        4-dimensional image shape K_H, K_W, C_I, C_O corresponding to the\n        kernel\'s height and width, the number of input channels, and the\n        number of output channels.  Note that C_I should = C.\n\n    Returns\n    -------\n    result : np.ndarray\n        Convolved result.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        convolved = tf.nn.conv2d(img, kernel, strides=[1, 1, 1, 1], padding=\'SAME\')\n        res = convolved.eval()\n    return res\n\n\ndef gabor(ksize=32):\n    """"""Uses Tensorflow to compute a 2D Gabor Kernel.\n\n    Parameters\n    ----------\n    ksize : int, optional\n        Size of kernel.\n\n    Returns\n    -------\n    gabor : np.ndarray\n        Gabor kernel with ksize x ksize dimensions.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        z_2d = gauss2d(0.0, 1.0, ksize)\n        ones = tf.ones((1, ksize))\n        ys = tf.sin(tf.linspace(-3.0, 3.0, ksize))\n        ys = tf.reshape(ys, [ksize, 1])\n        wave = tf.matmul(ys, ones)\n        gabor = tf.multiply(wave, z_2d)\n        return gabor.eval()\n\n\ndef build_submission(filename, file_list):\n    """"""Helper utility to check homework assignment submissions and package them.\n\n    Parameters\n    ----------\n    filename : str\n        Output zip file name\n    file_list : tuple\n        Tuple of files to include\n    """"""\n    # check each file exists\n    for part_i, file_i in enumerate(file_list):\n        assert os.path.exists(file_i), \\\n            \'\\nYou are missing the file {}.  \'.format(file_i) + \\\n            \'It does not look like you have completed Part {}.\'.format(\n                part_i + 1)\n\n    # great, each file exists\n    print(\'It looks like you have completed each part!\')\n\n    def zipdir(path, zf):\n        for root, dirs, files in os.walk(path):\n            for file in files:\n                # make sure the files are part of the necessary file list\n                if file.endswith(file_list):\n                    zf.write(os.path.join(root, file))\n\n    # create a zip file with the necessary files\n    zipf = zipfile.ZipFile(filename, \'w\', zipfile.ZIP_DEFLATED)\n    zipdir(\'.\', zipf)\n    zipf.close()\n    print(\'Great job!!!\')\n    print(\'Now submit the file:\\n{}\\nto Kadenze for grading!\'.format(\n        os.path.abspath(filename)))\n'"
session-1/tests/test_1.py,0,"b""import matplotlib\nmatplotlib.use('Agg')\nfrom libs import utils\nimport numpy as np\n\n\ndef test_celeb():\n    files = utils.get_celeb_files()\n    assert(len(files) == 100)\n\ndef test_crop():\n    assert(utils.imcrop_tosquare(np.zeros((64, 23))).shape == (23, 23))\n    assert(utils.imcrop_tosquare(np.zeros((23, 53))).shape == (23, 23))\n    assert(utils.imcrop_tosquare(np.zeros((23, 23))).shape == (23, 23))\n    assert(utils.imcrop_tosquare(np.zeros((24, 53))).shape == (24, 24))\n\ndef test_montage():\n    assert(utils.montage(np.zeros((100, 32, 32, 3))).shape == (331, 331, 3))\n\n"""
session-2/libs/__init__.py,0,b''
session-2/libs/gif.py,0,"b'""""""Utility for creating a GIF.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n\ndef build_gif(imgs, interval=0.1, dpi=72,\n              save_gif=True, saveto=\'animation.gif\',\n              show_gif=False, cmap=None):\n    """"""Take an array or list of images and create a GIF.\n\n    Parameters\n    ----------\n    imgs : np.ndarray or list\n        List of images to create a GIF of\n    interval : float, optional\n        Spacing in seconds between successive images.\n    dpi : int, optional\n        Dots per inch.\n    save_gif : bool, optional\n        Whether or not to save the GIF.\n    saveto : str, optional\n        Filename of GIF to save.\n    show_gif : bool, optional\n        Whether or not to render the GIF using plt.\n    cmap : None, optional\n        Optional colormap to apply to the images.\n\n    Returns\n    -------\n    ani : matplotlib.animation.ArtistAnimation\n        The artist animation from matplotlib.  Likely not useful.\n    """"""\n    imgs = np.asarray(imgs)\n    h, w, *c = imgs[0].shape\n    fig, ax = plt.subplots(figsize=(np.round(w / dpi), np.round(h / dpi)))\n    fig.subplots_adjust(bottom=0)\n    fig.subplots_adjust(top=1)\n    fig.subplots_adjust(right=1)\n    fig.subplots_adjust(left=0)\n    ax.set_axis_off()\n\n    if cmap is not None:\n        axs = list(map(lambda x: [\n            ax.imshow(x, cmap=cmap)], imgs))\n    else:\n        axs = list(map(lambda x: [\n            ax.imshow(x)], imgs))\n\n    ani = animation.ArtistAnimation(\n        fig, axs, interval=interval*1000, repeat_delay=0, blit=False)\n\n    if save_gif:\n        try:\n            ani.save(saveto, writer=\'imagemagick\', dpi=dpi)\n        except:\n            print(\'You do not have imagemagick installed.\\n\\nOn OSX \' +\n                  \'you can install this by first installing homebrew: \' +\n                  \'http://brew.sh\\nThen run: ""brew install imagemagick"".\\n\' +\n                  \'Windows users can obtain a binary installation here: \' +\n                  \'https://www.imagemagick.org/script/binary-releases.php\\n\' +\n                  \'And Linux users should be able to install imagemagick using \' +\n                  \'their package manager, e.g.: sudo apt-get install imagemagick.\')\n\n    if show_gif:\n        plt.show()\n\n    return ani\n'"
session-2/libs/utils.py,34,"b'""""""Utilities used in the Kadenze Academy Course on Deep Learning w/ Tensorflow.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nParag K. Mital\n\nCopyright Parag K. Mital, June 2016.\n""""""\nfrom __future__ import print_function\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport urllib\nimport numpy as np\nimport zipfile\nimport os\nfrom scipy.misc import imsave\n\n\ndef imcrop_tosquare(img):\n    """"""Make any image a square image.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        Input image to crop, assumed at least 2d.\n\n    Returns\n    -------\n    crop : np.ndarray\n        Cropped image.\n    """"""\n    size = np.min(img.shape[:2])\n    extra = img.shape[:2] - size\n    crop = img\n    for i in np.flatnonzero(extra):\n        crop = np.take(crop, extra[i] // 2 + np.r_[:size], axis=i)\n    return crop\n\n\ndef slice_montage(montage, img_h, img_w, n_imgs):\n    """"""Slice a montage image into n_img h x w images.\n\n    Performs the opposite of the montage function.  Takes a montage image and\n    slices it back into a N x H x W x C image.\n\n    Parameters\n    ----------\n    montage : np.ndarray\n        Montage image to slice.\n    img_h : int\n        Height of sliced image\n    img_w : int\n        Width of sliced image\n    n_imgs : int\n        Number of images to slice\n\n    Returns\n    -------\n    sliced : np.ndarray\n        Sliced images as 4d array.\n    """"""\n    sliced_ds = []\n    for i in range(int(np.sqrt(n_imgs))):\n        for j in range(int(np.sqrt(n_imgs))):\n            sliced_ds.append(montage[\n                1 + i + i * img_h:1 + i + (i + 1) * img_h,\n                1 + j + j * img_w:1 + j + (j + 1) * img_w])\n    return np.array(sliced_ds)\n\n\ndef montage(images, saveto=\'montage.png\'):\n    """"""Draw all images as a montage separated by 1 pixel borders.\n\n    Also saves the file to the destination specified by `saveto`.\n\n    Parameters\n    ----------\n    images : numpy.ndarray\n        Input array to create montage of.  Array should be:\n        batch x height x width x channels.\n    saveto : str\n        Location to save the resulting montage image.\n\n    Returns\n    -------\n    m : numpy.ndarray\n        Montage image.\n    """"""\n    if isinstance(images, list):\n        images = np.array(images)\n    img_h = images.shape[1]\n    img_w = images.shape[2]\n    n_plots = int(np.ceil(np.sqrt(images.shape[0])))\n    if len(images.shape) == 4 and images.shape[3] == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 3)) * 0.5\n    elif len(images.shape) == 4 and images.shape[3] == 1:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 1)) * 0.5\n    elif len(images.shape) == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1)) * 0.5\n    else:\n        raise ValueError(\'Could not parse image shape of {}\'.format(\n            images.shape))\n    for i in range(n_plots):\n        for j in range(n_plots):\n            this_filter = i * n_plots + j\n            if this_filter < images.shape[0]:\n                this_img = images[this_filter]\n                m[1 + i + i * img_h:1 + i + (i + 1) * img_h,\n                  1 + j + j * img_w:1 + j + (j + 1) * img_w] = this_img\n    imsave(arr=np.squeeze(m), name=saveto)\n    return m\n\n\ndef get_celeb_files():\n    """"""Download the first 100 images of the celeb dataset.\n\n    Files will be placed in a directory \'img_align_celeba\' if one\n    doesn\'t exist.\n\n    Returns\n    -------\n    files : list of strings\n        Locations to the first 100 images of the celeb net dataset.\n    """"""\n    # Create a directory\n    if not os.path.exists(\'img_align_celeba\'):\n        os.mkdir(\'img_align_celeba\')\n\n    # Now perform the following 100 times:\n    for img_i in range(1, 101):\n\n        # create a string using the current loop counter\n        f = \'000%03d.jpg\' % img_i\n\n        # and get the url with that string appended the end\n        url = \'https://s3.amazonaws.com/cadl/celeb-align/\' + f\n\n        # We\'ll print this out to the console so we can see how far we\'ve gone\n        print(url, end=\'\\r\')\n\n        # And now download the url to a location inside our new directory\n        urllib.request.urlretrieve(url, os.path.join(\'img_align_celeba\', f))\n\n    files = [os.path.join(\'img_align_celeba\', file_i)\n             for file_i in os.listdir(\'img_align_celeba\')\n             if \'.jpg\' in file_i]\n    return files\n\n\ndef get_celeb_imgs():\n    """"""Load the first 100 images of the celeb dataset.\n\n    Returns\n    -------\n    imgs : list of np.ndarray\n        List of the first 100 images from the celeb dataset\n    """"""\n    return [plt.imread(f_i) for f_i in get_celeb_files()]\n\n\ndef gauss(mean, stddev, ksize):\n    """"""Use Tensorflow to compute a Gaussian Kernel.\n\n    Parameters\n    ----------\n    mean : float\n        Mean of the Gaussian (e.g. 0.0).\n    stddev : float\n        Standard Deviation of the Gaussian (e.g. 1.0).\n    ksize : int\n        Size of kernel (e.g. 16).\n\n    Returns\n    -------\n    kernel : np.ndarray\n        Computed Gaussian Kernel using Tensorflow.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        x = tf.linspace(-3.0, 3.0, ksize)\n        z = (tf.exp(tf.negative(tf.pow(x - mean, 2.0) /\n                           (2.0 * tf.pow(stddev, 2.0)))) *\n             (1.0 / (stddev * tf.sqrt(2.0 * 3.1415))))\n        return z.eval()\n\n\ndef gauss2d(mean, stddev, ksize):\n    """"""Use Tensorflow to compute a 2D Gaussian Kernel.\n\n    Parameters\n    ----------\n    mean : float\n        Mean of the Gaussian (e.g. 0.0).\n    stddev : float\n        Standard Deviation of the Gaussian (e.g. 1.0).\n    ksize : int\n        Size of kernel (e.g. 16).\n\n    Returns\n    -------\n    kernel : np.ndarray\n        Computed 2D Gaussian Kernel using Tensorflow.\n    """"""\n    z = gauss(mean, stddev, ksize)\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        z_2d = tf.matmul(tf.reshape(z, [ksize, 1]), tf.reshape(z, [1, ksize]))\n        return z_2d.eval()\n\n\ndef convolve(img, kernel):\n    """"""Use Tensorflow to convolve a 4D image with a 4D kernel.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        4-dimensional image shaped N x H x W x C\n    kernel : np.ndarray\n        4-dimensional image shape K_H, K_W, C_I, C_O corresponding to the\n        kernel\'s height and width, the number of input channels, and the\n        number of output channels.  Note that C_I should = C.\n\n    Returns\n    -------\n    result : np.ndarray\n        Convolved result.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        convolved = tf.nn.conv2d(img, kernel, strides=[1, 1, 1, 1], padding=\'SAME\')\n        res = convolved.eval()\n    return res\n\n\ndef gabor(ksize=32):\n    """"""Use Tensorflow to compute a 2D Gabor Kernel.\n\n    Parameters\n    ----------\n    ksize : int, optional\n        Size of kernel.\n\n    Returns\n    -------\n    gabor : np.ndarray\n        Gabor kernel with ksize x ksize dimensions.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        z_2d = gauss2d(0.0, 1.0, ksize)\n        ones = tf.ones((1, ksize))\n        ys = tf.sin(tf.linspace(-3.0, 3.0, ksize))\n        ys = tf.reshape(ys, [ksize, 1])\n        wave = tf.matmul(ys, ones)\n        gabor = tf.multiply(wave, z_2d)\n        return gabor.eval()\n\n\ndef build_submission(filename, file_list, optional_file_list=[]):\n    """"""Helper utility to check homework assignment submissions and package them.\n\n    Parameters\n    ----------\n    filename : str\n        Output zip file name\n    file_list : tuple\n        Tuple of files to include\n    """"""\n    # check each file exists\n    for part_i, file_i in enumerate(file_list):\n        if not os.path.exists(file_i):\n            print(\'\\nYou are missing the file {}.  \'.format(file_i) +\n                  \'It does not look like you have completed Part {}.\'.format(\n                part_i + 1))\n\n    def zipdir(path, zf):\n        for root, dirs, files in os.walk(path):\n            for file in files:\n                # make sure the files are part of the necessary file list\n                if file.endswith(file_list) or file.endswith(optional_file_list):\n                    zf.write(os.path.join(root, file))\n\n    # create a zip file with the necessary files\n    zipf = zipfile.ZipFile(filename, \'w\', zipfile.ZIP_DEFLATED)\n    zipdir(\'.\', zipf)\n    zipf.close()\n    print(\'Your assignment zip file has been created!\')\n    print(\'Now submit the file:\\n{}\\nto Kadenze for grading!\'.format(\n        os.path.abspath(filename)))\n\n\ndef linear(x, n_output, name=None, activation=None, reuse=None):\n    """"""Fully connected layer.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to connect\n    n_output : int\n        Number of output neurons\n    name : None, optional\n        Scope to apply\n\n    Returns\n    -------\n    op : tf.Tensor\n        Output of fully connected layer.\n    """"""\n    if len(x.get_shape()) != 2:\n        x = flatten(x, reuse=reuse)\n\n    n_input = x.get_shape().as_list()[1]\n\n    with tf.variable_scope(name or ""fc"", reuse=reuse):\n        W = tf.get_variable(\n            name=\'W\',\n            shape=[n_input, n_output],\n            dtype=tf.float32,\n            initializer=tf.contrib.layers.xavier_initializer())\n\n        b = tf.get_variable(\n            name=\'b\',\n            shape=[n_output],\n            dtype=tf.float32,\n            initializer=tf.constant_initializer(0.0))\n\n        h = tf.nn.bias_add(\n            name=\'h\',\n            value=tf.matmul(x, W),\n            bias=b)\n\n        if activation:\n            h = activation(h)\n\n        return h, W\n\n\ndef flatten(x, name=None, reuse=None):\n    """"""Flatten Tensor to 2-dimensions.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to flatten.\n    name : None, optional\n        Variable scope for flatten operations\n\n    Returns\n    -------\n    flattened : tf.Tensor\n        Flattened tensor.\n    """"""\n    with tf.variable_scope(\'flatten\'):\n        dims = x.get_shape().as_list()\n        if len(dims) == 4:\n            flattened = tf.reshape(\n                x,\n                shape=[-1, dims[1] * dims[2] * dims[3]])\n        elif len(dims) == 2 or len(dims) == 1:\n            flattened = x\n        else:\n            raise ValueError(\'Expected n dimensions of 1, 2 or 4.  Found:\',\n                             len(dims))\n\n        return flattened\n'"
session-2/tests/test_2.py,2,"b""import matplotlib\nmatplotlib.use('Agg')\nimport tensorflow as tf\nimport numpy as np\nfrom libs import utils\n\n\ndef test_flatten():\n    assert(utils.flatten(\n        tf.constant(np.zeros((3, 100, 100, 3)))).get_shape().as_list() == [3, 30000])\n\n\ndef test_linear():\n    h, W = utils.linear(tf.constant(np.zeros((3, 100, 100, 3), dtype=np.float32)), 10)\n    assert(h.get_shape().as_list() == [3, 10])\n\n\ndef test_montage():\n    assert(utils.slice_montage(\n        utils.montage(np.zeros((100, 3, 3, 3))), 3, 3, 100).shape == (100, 3, 3, 3))\n"""
session-3/libs/__init__.py,0,b''
session-3/libs/batch_norm.py,14,"b'""""""Batch Normalization for TensorFlow.\nParag K. Mital, Jan 2016.\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef batch_norm(x, phase_train, name=\'bn\', decay=0.99, reuse=None, affine=True):\n    """"""\n    Batch normalization on convolutional maps.\n    from: https://stackoverflow.com/questions/33949786/how-could-i-\n    use-batch-normalization-in-tensorflow\n    Only modified to infer shape from input tensor x.\n    Parameters\n    ----------\n    x\n        Tensor, 4D BHWD input maps\n    phase_train\n        boolean tf.Variable, true indicates training phase\n    name\n        string, variable name\n    affine\n        whether to affine-transform outputs\n    Return\n    ------\n    normed\n        batch-normalized maps\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        og_shape = x.get_shape().as_list()\n        if len(og_shape) == 2:\n            x = tf.reshape(x, [-1, 1, 1, og_shape[1]])\n        shape = x.get_shape().as_list()\n        beta = tf.get_variable(name=\'beta\', shape=[shape[-1]],\n                               initializer=tf.constant_initializer(0.0),\n                               trainable=True)\n        gamma = tf.get_variable(name=\'gamma\', shape=[shape[-1]],\n                                initializer=tf.constant_initializer(1.0),\n                                trainable=affine)\n\n        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name=\'moments\')\n        ema = tf.train.ExponentialMovingAverage(decay=decay)\n        ema_apply_op = ema.apply([batch_mean, batch_var])\n        ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n\n        def mean_var_with_update():\n            """"""Summary\n            Returns\n            -------\n            name : TYPE\n                Description\n            """"""\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n        mean, var = control_flow_ops.cond(phase_train,\n                                          mean_var_with_update,\n                                          lambda: (ema_mean, ema_var))\n\n        # tf.nn.batch_normalization\n        normed = tf.nn.batch_norm_with_global_normalization(\n            x, mean, var, beta, gamma, 1e-5, affine)\n        if len(og_shape) == 2:\n            normed = tf.reshape(normed, [-1, og_shape[-1]])\n    return normed\n'"
session-3/libs/dataset_utils.py,8,"b'""""""Utils for dataset creation.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\n\nimport os\nimport pickle\nimport numpy as np\nimport tensorflow as tf\nfrom . import dft\nfrom .utils import download_and_extract_tar\n\n\ndef create_input_pipeline(files, batch_size, n_epochs, shape, crop_shape=None,\n                          crop_factor=1.0, n_threads=4):\n    """"""Creates a pipefile from a list of image files.\n    Includes batch generator/central crop/resizing options.\n    The resulting generator will dequeue the images batch_size at a time until\n    it throws tf.errors.OutOfRangeError when there are no more images left in\n    the queue.\n\n    Parameters\n    ----------\n    files : list\n        List of paths to image files.\n    batch_size : int\n        Number of image files to load at a time.\n    n_epochs : int\n        Number of epochs to run before raising tf.errors.OutOfRangeError\n    shape : list\n        [height, width, channels]\n    crop_shape : list\n        [height, width] to crop image to.\n    crop_factor : float\n        Percentage of image to take starting from center.\n    n_threads : int, optional\n        Number of threads to use for batch shuffling\n    """"""\n\n    # We first create a ""producer"" queue.  It creates a production line which\n    # will queue up the file names and allow another queue to deque the file\n    # names all using a tf queue runner.\n    # Put simply, this is the entry point of the computational graph.\n    # It will generate the list of file names.\n    # We also specify it\'s capacity beforehand.\n    producer = tf.train.string_input_producer(\n        files, capacity=len(files))\n\n    # We need something which can open the files and read its contents.\n    reader = tf.WholeFileReader()\n\n    # We pass the filenames to this object which can read the file\'s contents.\n    # This will create another queue running which dequeues the previous queue.\n    keys, vals = reader.read(producer)\n\n    # And then have to decode its contents as we know it is a jpeg image\n    imgs = tf.image.decode_jpeg(\n        vals,\n        channels=3 if len(shape) > 2 and shape[2] == 3 else 0)\n\n    # We have to explicitly define the shape of the tensor.\n    # This is because the decode_jpeg operation is still a node in the graph\n    # and doesn\'t yet know the shape of the image.  Future operations however\n    # need explicit knowledge of the image\'s shape in order to be created.\n    imgs.set_shape(shape)\n\n    # Next we\'ll centrally crop the image to the size of 100x100.\n    # This operation required explicit knowledge of the image\'s shape.\n    if shape[0] > shape[1]:\n        rsz_shape = [int(shape[0] / shape[1] * crop_shape[0] / crop_factor),\n                     int(crop_shape[1] / crop_factor)]\n    else:\n        rsz_shape = [int(crop_shape[0] / crop_factor),\n                     int(shape[1] / shape[0] * crop_shape[1] / crop_factor)]\n    rszs = tf.image.resize_images(imgs, rsz_shape)\n    crops = (tf.image.resize_image_with_crop_or_pad(\n        rszs, crop_shape[0], crop_shape[1])\n        if crop_shape is not None\n        else imgs)\n\n    # Now we\'ll create a batch generator that will also shuffle our examples.\n    # We tell it how many it should have in its buffer when it randomly\n    # permutes the order.\n    min_after_dequeue = len(files) // 10\n\n    # The capacity should be larger than min_after_dequeue, and determines how\n    # many examples are prefetched.  TF docs recommend setting this value to:\n    # min_after_dequeue + (num_threads + a small safety margin) * batch_size\n    capacity = min_after_dequeue + (n_threads + 1) * batch_size\n\n    # Randomize the order and output batches of batch_size.\n    batch = tf.train.shuffle_batch([crops],\n                                   enqueue_many=False,\n                                   batch_size=batch_size,\n                                   capacity=capacity,\n                                   min_after_dequeue=min_after_dequeue,\n                                   num_threads=n_threads)\n\n    # alternatively, we could use shuffle_batch_join to use multiple reader\n    # instances, or set shuffle_batch\'s n_threads to higher than 1.\n\n    return batch\n\n\ndef gtzan_music_speech_download(dst=\'gtzan_music_speech\'):\n    """"""Download the GTZAN music and speech dataset.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Location to put the GTZAN music and speech datset.\n    """"""\n    path = \'http://opihi.cs.uvic.ca/sound/music_speech.tar.gz\'\n    download_and_extract_tar(path, dst)\n\n\ndef gtzan_music_speech_load(dst=\'gtzan_music_speech\'):\n    """"""Load the GTZAN Music and Speech dataset.\n\n    Downloads the dataset if it does not exist into the dst directory.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Location of GTZAN Music and Speech dataset.\n\n    Returns\n    -------\n    Xs, ys : np.ndarray, np.ndarray\n        Array of data, Array of labels\n    """"""\n    from scipy.io import wavfile\n\n    if not os.path.exists(dst):\n        gtzan_music_speech_download(dst)\n    music_dir = os.path.join(os.path.join(dst, \'music_speech\'), \'music_wav\')\n    music = [os.path.join(music_dir, file_i)\n             for file_i in os.listdir(music_dir)\n             if file_i.endswith(\'.wav\')]\n    speech_dir = os.path.join(os.path.join(dst, \'music_speech\'), \'speech_wav\')\n    speech = [os.path.join(speech_dir, file_i)\n              for file_i in os.listdir(speech_dir)\n              if file_i.endswith(\'.wav\')]\n    Xs = []\n    ys = []\n    for i in music:\n        sr, s = wavfile.read(i)\n        s = s / 16384.0 - 1.0\n        re, im = dft.dft_np(s)\n        mag, phs = dft.ztoc(re, im)\n        Xs.append((mag, phs))\n        ys.append(0)\n    for i in speech:\n        sr, s = wavfile.read(i)\n        s = s / 16384.0 - 1.0\n        re, im = dft.dft_np(s)\n        mag, phs = dft.ztoc(re, im)\n        Xs.append((mag, phs))\n        ys.append(1)\n    Xs = np.array(Xs)\n    Xs = np.transpose(Xs, [0, 2, 3, 1])\n    ys = np.array(ys)\n    return Xs, ys\n\n\ndef cifar10_download(dst=\'cifar10\'):\n    """"""Download the CIFAR10 dataset.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Directory to download into.\n    """"""\n    path = \'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'\n    download_and_extract_tar(path, dst)\n\n\ndef cifar10_load(dst=\'cifar10\'):\n    """"""Load the CIFAR10 dataset.\n\n    Downloads the dataset if it does not exist into the dst directory.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Location of CIFAR10 dataset.\n\n    Returns\n    -------\n    Xs, ys : np.ndarray, np.ndarray\n        Array of data, Array of labels\n    """"""\n    if not os.path.exists(dst):\n        cifar10_download(dst)\n    Xs = None\n    ys = None\n    for f in range(1, 6):\n        cf = pickle.load(open(\n            \'%s/cifar-10-batches-py/data_batch_%d\' % (dst, f), \'rb\'),\n            encoding=\'LATIN\')\n        if Xs is not None:\n            Xs = np.r_[Xs, cf[\'data\']]\n            ys = np.r_[ys, np.array(cf[\'labels\'])]\n        else:\n            Xs = cf[\'data\']\n            ys = cf[\'labels\']\n    Xs = np.swapaxes(np.swapaxes(Xs.reshape(-1, 3, 32, 32), 1, 3), 1, 2)\n    return Xs, ys\n\n\ndef dense_to_one_hot(labels, n_classes=2):\n    """"""Convert class labels from scalars to one-hot vectors.\n\n    Parameters\n    ----------\n    labels : array\n        Input labels to convert to one-hot representation.\n    n_classes : int, optional\n        Number of possible one-hot.\n\n    Returns\n    -------\n    one_hot : array\n        One hot representation of input.\n    """"""\n    return np.eye(n_classes).astype(np.float32)[labels]\n\n\nclass DatasetSplit(object):\n    """"""Utility class for batching data and handling multiple splits.\n\n    Attributes\n    ----------\n    current_batch_idx : int\n        Description\n    images : np.ndarray\n        Xs of the dataset.  Not necessarily images.\n    labels : np.ndarray\n        ys of the dataset.\n    n_labels : int\n        Number of possible labels\n    num_examples : int\n        Number of total observations\n    """"""\n\n    def __init__(self, images, labels):\n        """"""Initialize a DatasetSplit object.\n\n        Parameters\n        ----------\n        images : np.ndarray\n            Xs/inputs\n        labels : np.ndarray\n            ys/outputs\n        """"""\n        self.images = np.array(images).astype(np.float32)\n        if labels is not None:\n            self.labels = np.array(labels).astype(np.int32)\n            self.n_labels = len(np.unique(labels))\n        else:\n            self.labels = None\n        self.num_examples = len(self.images)\n\n    def next_batch(self, batch_size=100):\n        """"""Batch generator with randomization.\n\n        Parameters\n        ----------\n        batch_size : int, optional\n            Size of each minibatch.\n\n        Returns\n        -------\n        Xs, ys : np.ndarray, np.ndarray\n            Next batch of inputs and labels (if no labels, then None).\n        """"""\n        # Shuffle each epoch\n        current_permutation = np.random.permutation(range(len(self.images)))\n        epoch_images = self.images[current_permutation, ...]\n        if self.labels is not None:\n            epoch_labels = self.labels[current_permutation, ...]\n\n        # Then iterate over the epoch\n        self.current_batch_idx = 0\n        while self.current_batch_idx < len(self.images):\n            end_idx = min(\n                self.current_batch_idx + batch_size, len(self.images))\n            this_batch = {\n                \'images\': epoch_images[self.current_batch_idx:end_idx],\n                \'labels\': epoch_labels[self.current_batch_idx:end_idx]\n                if self.labels is not None else None\n            }\n            self.current_batch_idx += batch_size\n            yield this_batch[\'images\'], this_batch[\'labels\']\n\n\nclass Dataset(object):\n    """"""Create a dataset from data and their labels.\n\n    Allows easy use of train/valid/test splits; Batch generator.\n\n    Attributes\n    ----------\n    all_idxs : list\n        All indexes across all splits.\n    all_inputs : list\n        All inputs across all splits.\n    all_labels : list\n        All labels across all splits.\n    n_labels : int\n        Number of labels.\n    split : list\n        Percentage split of train, valid, test sets.\n    test_idxs : list\n        Indexes of the test split.\n    train_idxs : list\n        Indexes of the train split.\n    valid_idxs : list\n        Indexes of the valid split.\n    """"""\n\n    def __init__(self, Xs, ys=None, split=[1.0, 0.0, 0.0], one_hot=False):\n        """"""Initialize a Dataset object.\n\n        Parameters\n        ----------\n        Xs : np.ndarray\n            Images/inputs to a network\n        ys : np.ndarray\n            Labels/outputs to a network\n        split : list, optional\n            Percentage of train, valid, and test sets.\n        one_hot : bool, optional\n            Whether or not to use one-hot encoding of labels (ys).\n        """"""\n        self.all_idxs = []\n        self.all_labels = []\n        self.all_inputs = []\n        self.train_idxs = []\n        self.valid_idxs = []\n        self.test_idxs = []\n        self.n_labels = 0\n        self.split = split\n\n        # Now mix all the labels that are currently stored as blocks\n        self.all_inputs = Xs\n        n_idxs = len(self.all_inputs)\n        idxs = range(n_idxs)\n        rand_idxs = np.random.permutation(idxs)\n        self.all_inputs = self.all_inputs[rand_idxs, ...]\n        if ys is not None:\n            self.all_labels = ys if not one_hot else dense_to_one_hot(ys)\n            self.all_labels = self.all_labels[rand_idxs, ...]\n        else:\n            self.all_labels = None\n\n        # Get splits\n        self.train_idxs = idxs[:round(split[0] * n_idxs)]\n        self.valid_idxs = idxs[len(self.train_idxs):\n                               len(self.train_idxs) + round(split[1] * n_idxs)]\n        self.test_idxs = idxs[\n            (len(self.valid_idxs) + len(self.train_idxs)):\n            (len(self.valid_idxs) + len(self.train_idxs)) +\n             round(split[2] * n_idxs)]\n\n    @property\n    def X(self):\n        """"""Inputs/Xs/Images.\n\n        Returns\n        -------\n        all_inputs : np.ndarray\n            Original Inputs/Xs.\n        """"""\n        return self.all_inputs\n\n    @property\n    def Y(self):\n        """"""Outputs/ys/Labels.\n\n        Returns\n        -------\n        all_labels : np.ndarray\n            Original Outputs/ys.\n        """"""\n        return self.all_labels\n\n    @property\n    def train(self):\n        """"""Train split.\n\n        Returns\n        -------\n        split : DatasetSplit\n            Split of the train dataset.\n        """"""\n        if len(self.train_idxs):\n            inputs = self.all_inputs[self.train_idxs, ...]\n            if self.all_labels is not None:\n                labels = self.all_labels[self.train_idxs, ...]\n            else:\n                labels = None\n        else:\n            inputs, labels = [], []\n        return DatasetSplit(inputs, labels)\n\n    @property\n    def valid(self):\n        """"""Validation split.\n\n        Returns\n        -------\n        split : DatasetSplit\n            Split of the validation dataset.\n        """"""\n        if len(self.valid_idxs):\n            inputs = self.all_inputs[self.valid_idxs, ...]\n            if self.all_labels is not None:\n                labels = self.all_labels[self.valid_idxs, ...]\n            else:\n                labels = None\n        else:\n            inputs, labels = [], []\n        return DatasetSplit(inputs, labels)\n\n    @property\n    def test(self):\n        """"""Test split.\n\n        Returns\n        -------\n        split : DatasetSplit\n            Split of the test dataset.\n        """"""\n        if len(self.test_idxs):\n            inputs = self.all_inputs[self.test_idxs, ...]\n            if self.all_labels is not None:\n                labels = self.all_labels[self.test_idxs, ...]\n            else:\n                labels = None\n        else:\n            inputs, labels = [], []\n        return DatasetSplit(inputs, labels)\n\n    def mean(self):\n        """"""Mean of the inputs/Xs.\n\n        Returns\n        -------\n        mean : np.ndarray\n            Calculates mean across 0th (batch) dimension.\n        """"""\n        return np.mean(self.all_inputs, axis=0)\n\n    def std(self):\n        """"""Standard deviation of the inputs/Xs.\n\n        Returns\n        -------\n        std : np.ndarray\n            Calculates std across 0th (batch) dimension.\n        """"""\n        return np.std(self.all_inputs, axis=0)\n'"
session-3/libs/datasets.py,0,"b'""""""Creative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport tensorflow.examples.tutorials.mnist.input_data as input_data\nfrom .dataset_utils import *\n\n\ndef MNIST(one_hot=True, split=[1.0, 0.0, 0.0]):\n    """"""Returns the MNIST dataset.\n\n    Returns\n    -------\n    mnist : DataSet\n        DataSet object w/ convenienve props for accessing\n        train/validation/test sets and batches.\n    """"""\n    ds = input_data.read_data_sets(\'MNIST_data/\', one_hot=one_hot)\n    return Dataset(np.r_[ds.train.images,\n                         ds.validation.images,\n                         ds.test.images],\n                   np.r_[ds.train.labels,\n                         ds.validation.labels,\n                         ds.test.labels],\n                   split=split)\n\n\ndef CIFAR10(flatten=True, split=[1.0, 0.0, 0.0]):\n    """"""Returns the CIFAR10 dataset.\n\n    Parameters\n    ----------\n    flatten : bool, optional\n        Convert the 3 x 32 x 32 pixels to a single vector\n\n    Returns\n    -------\n    cifar : Dataset\n        Description\n    """"""\n    # plt.imshow(np.transpose(np.reshape(\n    #   cifar.train.images[10], (3, 32, 32)), [1, 2, 0]))\n    Xs, ys = cifar10_load()\n    if flatten:\n        Xs = Xs.reshape((Xs.shape[0], -1))\n    return Dataset(Xs, ys, split=split)\n\n\ndef CELEB(path=\'./img_align_celeba/\'):\n    """"""Attempt to load the files of the CELEB dataset.\n\n    Requires the files already be downloaded and placed in the `dst` directory.\n\n    http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n\n    Parameters\n    ----------\n    path : str, optional\n        Directory where the aligned/cropped celeb dataset can be found.\n\n    Returns\n    -------\n    files : list\n        List of file paths to the dataset.\n    """"""\n    if not os.path.exists(path):\n        print(\'Could not find celeb dataset under {}.\'.format(path))\n        print(\'Try downloading the dataset from the ""Aligned and Cropped"" \' +\n              \'link located here (imgs/img_align_celeba.zip [1.34 GB]): \' +\n              \'http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\')\n        return None\n    else:\n        fs = [os.path.join(path, f)\n              for f in os.listdir(path) if f.endswith(\'.jpg\')]\n        if len(fs) < 202598:\n            print(\'It does not look like you have downloaded the entire \' +\n                  \'Celeb Dataset.\\n\' +\n                  \'Try downloading the dataset from the ""Aligned and Cropped"" \' +\n                  \'link located here (imgs/img_align_celeba.zip [1.34 GB]): \' +\n                  \'http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\')\n        return fs\n'"
session-3/libs/dft.py,0,"b'""""""Summary.\n\n#CADL\nCopyright Parag K. Mital 2016\n""""""\nimport numpy as np\nfrom scipy.signal import hann\n\n\ndef ztoc(re, im):\n    return np.sqrt(re**2 + im**2), np.angle(re + im * 1j)\n\n\ndef ctoz(mag, phs):\n    return mag * np.cos(phs), mag * np.sin(phs)\n\n\ndef dft_np(signal, hop_size=256, fft_size=512):\n    n_hops = len(signal) // hop_size\n    s = []\n    hann_win = hann(fft_size)\n    for hop_i in range(n_hops):\n        frame = signal[(hop_i * hop_size):(hop_i * hop_size + fft_size)]\n        frame = np.pad(frame, (0, fft_size - len(frame)), \'constant\')\n        frame *= hann_win\n        s.append(frame)\n    s = np.array(s)\n    N = s.shape[-1]\n    k = np.reshape(np.linspace(0.0, 2 * np.pi / N * (N // 2), N // 2), [1, N // 2])\n    x = np.reshape(np.linspace(0.0, N - 1, N), [N, 1])\n    freqs = np.dot(x, k)\n    reals = np.dot(s, np.cos(freqs)) * (2.0 / N)\n    imags = np.dot(s, np.sin(freqs)) * (2.0 / N)\n    return reals, imags\n\n\ndef idft_np(re, im, hop_size=256, fft_size=512):\n    N = re.shape[1] * 2\n    k = np.reshape(np.linspace(0.0, 2 * np.pi / N * (N // 2), N // 2), [N // 2, 1])\n    x = np.reshape(np.linspace(0.0, N - 1, N), [1, N])\n    freqs = np.dot(k, x)\n    signal = np.zeros((re.shape[0] * hop_size + fft_size,))\n    recon = np.dot(re, np.cos(freqs)) + np.dot(im, np.sin(freqs))\n    for hop_i, frame in enumerate(recon):\n        signal[(hop_i * hop_size): (hop_i * hop_size + fft_size)] += frame\n    return signal\n'"
session-3/libs/gif.py,0,"b'""""""Utility for creating a GIF.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n\ndef build_gif(imgs, interval=0.1, dpi=72,\n              save_gif=True, saveto=\'animation.gif\',\n              show_gif=False, cmap=None):\n    """"""Take an array or list of images and create a GIF.\n\n    Parameters\n    ----------\n    imgs : np.ndarray or list\n        List of images to create a GIF of\n    interval : float, optional\n        Spacing in seconds between successive images.\n    dpi : int, optional\n        Dots per inch.\n    save_gif : bool, optional\n        Whether or not to save the GIF.\n    saveto : str, optional\n        Filename of GIF to save.\n    show_gif : bool, optional\n        Whether or not to render the GIF using plt.\n    cmap : None, optional\n        Optional colormap to apply to the images.\n\n    Returns\n    -------\n    ani : matplotlib.animation.ArtistAnimation\n        The artist animation from matplotlib.  Likely not useful.\n    """"""\n    imgs = np.asarray(imgs)\n    h, w, *c = imgs[0].shape\n    fig, ax = plt.subplots(figsize=(np.round(w / dpi), np.round(h / dpi)))\n    fig.subplots_adjust(bottom=0)\n    fig.subplots_adjust(top=1)\n    fig.subplots_adjust(right=1)\n    fig.subplots_adjust(left=0)\n    ax.set_axis_off()\n\n    if cmap is not None:\n        axs = list(map(lambda x: [\n            ax.imshow(x, cmap=cmap)], imgs))\n    else:\n        axs = list(map(lambda x: [\n            ax.imshow(x)], imgs))\n\n    ani = animation.ArtistAnimation(\n        fig, axs, interval=interval*1000, repeat_delay=0, blit=False)\n\n    if save_gif:\n        ani.save(saveto, writer=\'imagemagick\', dpi=dpi)\n\n    if show_gif:\n        plt.show()\n\n    return ani\n'"
session-3/libs/utils.py,73,"b'""""""Utilities used in the Kadenze Academy Course on Deep Learning w/ Tensorflow.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nParag K. Mital\n\nCopyright Parag K. Mital, June 2016.\n""""""\nfrom __future__ import print_function\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport urllib\nimport numpy as np\nimport zipfile\nimport os\nfrom scipy.io import wavfile\nfrom scipy.misc import imsave\n\ndef download(path):\n    """"""Use urllib to download a file.\n\n    Parameters\n    ----------\n    path : str\n        Url to download\n\n    Returns\n    -------\n    path : str\n        Location of downloaded file.\n    """"""\n    import os\n    from six.moves import urllib\n\n    fname = path.split(\'/\')[-1]\n    if os.path.exists(fname):\n        return fname\n\n    print(\'Downloading \' + path)\n\n    def progress(count, block_size, total_size):\n        if count % 20 == 0:\n            print(\'Downloaded %02.02f/%02.02f MB\' % (\n                count * block_size / 1024.0 / 1024.0,\n                total_size / 1024.0 / 1024.0), end=\'\\r\')\n\n    filepath, _ = urllib.request.urlretrieve(\n        path, filename=fname, reporthook=progress)\n    return filepath\n\n\ndef download_and_extract_tar(path, dst):\n    """"""Download and extract a tar file.\n\n    Parameters\n    ----------\n    path : str\n        Url to tar file to download.\n    dst : str\n        Location to save tar file contents.\n    """"""\n    import tarfile\n    filepath = download(path)\n    if not os.path.exists(dst):\n        os.makedirs(dst)\n        tarfile.open(filepath, \'r:gz\').extractall(dst)\n\n\ndef download_and_extract_zip(path, dst):\n    """"""Download and extract a zip file.\n\n    Parameters\n    ----------\n    path : str\n        Url to zip file to download.\n    dst : str\n        Location to save zip file contents.\n    """"""\n    import zipfile\n    filepath = download(path)\n    if not os.path.exists(dst):\n        os.makedirs(dst)\n        zf = zipfile.ZipFile(file=filepath)\n        zf.extractall(dst)\n\n\ndef load_audio(filename, b_normalize=True):\n    """"""Load the audiofile at the provided filename using scipy.io.wavfile.\n\n    Optionally normalizes the audio to the maximum value.\n\n    Parameters\n    ----------\n    filename : str\n        File to load.\n    b_normalize : bool, optional\n        Normalize to the maximum value.\n    """"""\n    sr, s = wavfile.read(filename)\n    if b_normalize:\n        s = s.astype(np.float32)\n        s = (s / np.max(np.abs(s)))\n        s -= np.mean(s)\n    return s\n\n\ndef corrupt(x):\n    """"""Take an input tensor and add uniform masking.\n\n    Parameters\n    ----------\n    x : Tensor/Placeholder\n        Input to corrupt.\n    Returns\n    -------\n    x_corrupted : Tensor\n        50 pct of values corrupted.\n    """"""\n    return tf.multiply(x, tf.cast(tf.random_uniform(shape=tf.shape(x),\n                                               minval=0,\n                                               maxval=2,\n                                               dtype=tf.int32), tf.float32))\n\n\ndef interp(l, r, n_samples):\n    """"""Intepolate between the arrays l and r, n_samples times.\n\n    Parameters\n    ----------\n    l : np.ndarray\n        Left edge\n    r : np.ndarray\n        Right edge\n    n_samples : int\n        Number of samples\n\n    Returns\n    -------\n    arr : np.ndarray\n        Inteporalted array\n    """"""\n    return np.array([\n        l + step_i / (n_samples - 1) * (r - l)\n        for step_i in range(n_samples)])\n\n\ndef make_latent_manifold(corners, n_samples):\n    """"""Create a 2d manifold out of the provided corners: n_samples * n_samples.\n\n    Parameters\n    ----------\n    corners : list of np.ndarray\n        The four corners to intepolate.\n    n_samples : int\n        Number of samples to use in interpolation.\n\n    Returns\n    -------\n    arr : np.ndarray\n        Stacked array of all 2D interpolated samples\n    """"""\n    left = interp(corners[0], corners[1], n_samples)\n    right = interp(corners[2], corners[3], n_samples)\n\n    embedding = []\n    for row_i in range(n_samples):\n        embedding.append(interp(left[row_i], right[row_i], n_samples))\n    return np.vstack(embedding)\n\n\ndef imcrop_tosquare(img):\n    """"""Make any image a square image.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        Input image to crop, assumed at least 2d.\n\n    Returns\n    -------\n    crop : np.ndarray\n        Cropped image.\n    """"""\n    size = np.min(img.shape[:2])\n    extra = img.shape[:2] - size\n    crop = img\n    for i in np.flatnonzero(extra):\n        crop = np.take(crop, extra[i] // 2 + np.r_[:size], axis=i)\n    return crop\n\n\ndef slice_montage(montage, img_h, img_w, n_imgs):\n    """"""Slice a montage image into n_img h x w images.\n\n    Performs the opposite of the montage function.  Takes a montage image and\n    slices it back into a N x H x W x C image.\n\n    Parameters\n    ----------\n    montage : np.ndarray\n        Montage image to slice.\n    img_h : int\n        Height of sliced image\n    img_w : int\n        Width of sliced image\n    n_imgs : int\n        Number of images to slice\n\n    Returns\n    -------\n    sliced : np.ndarray\n        Sliced images as 4d array.\n    """"""\n    sliced_ds = []\n    for i in range(int(np.sqrt(n_imgs))):\n        for j in range(int(np.sqrt(n_imgs))):\n            sliced_ds.append(montage[\n                1 + i + i * img_h:1 + i + (i + 1) * img_h,\n                1 + j + j * img_w:1 + j + (j + 1) * img_w])\n    return np.array(sliced_ds)\n\n\ndef montage(images, saveto=\'montage.png\'):\n    """"""Draw all images as a montage separated by 1 pixel borders.\n\n    Also saves the file to the destination specified by `saveto`.\n\n    Parameters\n    ----------\n    images : numpy.ndarray\n        Input array to create montage of.  Array should be:\n        batch x height x width x channels.\n    saveto : str\n        Location to save the resulting montage image.\n\n    Returns\n    -------\n    m : numpy.ndarray\n        Montage image.\n    """"""\n    if isinstance(images, list):\n        images = np.array(images)\n    img_h = images.shape[1]\n    img_w = images.shape[2]\n    n_plots = int(np.ceil(np.sqrt(images.shape[0])))\n    if len(images.shape) == 4 and images.shape[3] == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 3)) * 0.5\n    elif len(images.shape) == 4 and images.shape[3] == 1:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 1)) * 0.5\n    elif len(images.shape) == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1)) * 0.5\n    else:\n        raise ValueError(\'Could not parse image shape of {}\'.format(\n            images.shape))\n    for i in range(n_plots):\n        for j in range(n_plots):\n            this_filter = i * n_plots + j\n            if this_filter < images.shape[0]:\n                this_img = images[this_filter]\n                m[1 + i + i * img_h:1 + i + (i + 1) * img_h,\n                  1 + j + j * img_w:1 + j + (j + 1) * img_w] = this_img\n    imsave(arr=np.squeeze(m), name=saveto)\n    return m\n\n\ndef montage_filters(W):\n    """"""Draws all filters (n_input * n_output filters) as a\n    montage image separated by 1 pixel borders.\n\n    Parameters\n    ----------\n    W : Tensor\n        Input tensor to create montage of.\n\n    Returns\n    -------\n    m : numpy.ndarray\n        Montage image.\n    """"""\n    W = np.reshape(W, [W.shape[0], W.shape[1], 1, W.shape[2] * W.shape[3]])\n    n_plots = int(np.ceil(np.sqrt(W.shape[-1])))\n    m = np.ones(\n        (W.shape[0] * n_plots + n_plots + 1,\n         W.shape[1] * n_plots + n_plots + 1)) * 0.5\n    for i in range(n_plots):\n        for j in range(n_plots):\n            this_filter = i * n_plots + j\n            if this_filter < W.shape[-1]:\n                m[1 + i + i * W.shape[0]:1 + i + (i + 1) * W.shape[0],\n                  1 + j + j * W.shape[1]:1 + j + (j + 1) * W.shape[1]] = (\n                    np.squeeze(W[:, :, :, this_filter]))\n    return m\n\n\ndef get_celeb_files(dst=\'img_align_celeba\', max_images=100):\n    """"""Download the first 100 images of the celeb dataset.\n\n    Files will be placed in a directory \'img_align_celeba\' if one\n    doesn\'t exist.\n\n    Returns\n    -------\n    files : list of strings\n        Locations to the first 100 images of the celeb net dataset.\n    """"""\n    # Create a directory\n    if not os.path.exists(dst):\n        os.mkdir(dst)\n\n    # Now perform the following 100 times:\n    for img_i in range(1, max_images + 1):\n\n        # create a string using the current loop counter\n        f = \'000%03d.jpg\' % img_i\n\n        if not os.path.exists(os.path.join(dst, f)):\n\n            # and get the url with that string appended the end\n            url = \'https://s3.amazonaws.com/cadl/celeb-align/\' + f\n\n            # We\'ll print this out to the console so we can see how far we\'ve gone\n            print(url, end=\'\\r\')\n\n            # And now download the url to a location inside our new directory\n            urllib.request.urlretrieve(url, os.path.join(dst, f))\n\n    files = [os.path.join(dst, file_i)\n             for file_i in os.listdir(dst)\n             if \'.jpg\' in file_i][:max_images]\n    return files\n\n\ndef get_celeb_imgs(max_images=100):\n    """"""Load the first `max_images` images of the celeb dataset.\n\n    Returns\n    -------\n    imgs : list of np.ndarray\n        List of the first 100 images from the celeb dataset\n    """"""\n    return [plt.imread(f_i) for f_i in get_celeb_files(max_images=max_images)]\n\n\ndef gauss(mean, stddev, ksize):\n    """"""Use Tensorflow to compute a Gaussian Kernel.\n\n    Parameters\n    ----------\n    mean : float\n        Mean of the Gaussian (e.g. 0.0).\n    stddev : float\n        Standard Deviation of the Gaussian (e.g. 1.0).\n    ksize : int\n        Size of kernel (e.g. 16).\n\n    Returns\n    -------\n    kernel : np.ndarray\n        Computed Gaussian Kernel using Tensorflow.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        x = tf.linspace(-3.0, 3.0, ksize)\n        z = (tf.exp(tf.negative(tf.pow(x - mean, 2.0) /\n                           (2.0 * tf.pow(stddev, 2.0)))) *\n             (1.0 / (stddev * tf.sqrt(2.0 * 3.1415))))\n        return z.eval()\n\n\ndef gauss2d(mean, stddev, ksize):\n    """"""Use Tensorflow to compute a 2D Gaussian Kernel.\n\n    Parameters\n    ----------\n    mean : float\n        Mean of the Gaussian (e.g. 0.0).\n    stddev : float\n        Standard Deviation of the Gaussian (e.g. 1.0).\n    ksize : int\n        Size of kernel (e.g. 16).\n\n    Returns\n    -------\n    kernel : np.ndarray\n        Computed 2D Gaussian Kernel using Tensorflow.\n    """"""\n    z = gauss(mean, stddev, ksize)\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        z_2d = tf.matmul(tf.reshape(z, [ksize, 1]), tf.reshape(z, [1, ksize]))\n        return z_2d.eval()\n\n\ndef convolve(img, kernel):\n    """"""Use Tensorflow to convolve a 4D image with a 4D kernel.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        4-dimensional image shaped N x H x W x C\n    kernel : np.ndarray\n        4-dimensional image shape K_H, K_W, C_I, C_O corresponding to the\n        kernel\'s height and width, the number of input channels, and the\n        number of output channels.  Note that C_I should = C.\n\n    Returns\n    -------\n    result : np.ndarray\n        Convolved result.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        convolved = tf.nn.conv2d(img, kernel, strides=[1, 1, 1, 1], padding=\'SAME\')\n        res = convolved.eval()\n    return res\n\n\ndef gabor(ksize=32):\n    """"""Use Tensorflow to compute a 2D Gabor Kernel.\n\n    Parameters\n    ----------\n    ksize : int, optional\n        Size of kernel.\n\n    Returns\n    -------\n    gabor : np.ndarray\n        Gabor kernel with ksize x ksize dimensions.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        z_2d = gauss2d(0.0, 1.0, ksize)\n        ones = tf.ones((1, ksize))\n        ys = tf.sin(tf.linspace(-3.0, 3.0, ksize))\n        ys = tf.reshape(ys, [ksize, 1])\n        wave = tf.matmul(ys, ones)\n        gabor = tf.multiply(wave, z_2d)\n        return gabor.eval()\n\n\ndef build_submission(filename, file_list, optional_file_list=()):\n    """"""Helper utility to check homework assignment submissions and package them.\n\n    Parameters\n    ----------\n    filename : str\n        Output zip file name\n    file_list : tuple\n        Tuple of files to include\n    """"""\n    # check each file exists\n    for part_i, file_i in enumerate(file_list):\n        if not os.path.exists(file_i):\n            print(\'\\nYou are missing the file {}.  \'.format(file_i) +\n                  \'It does not look like you have completed Part {}.\'.format(\n                part_i + 1))\n\n    def zipdir(path, zf):\n        for root, dirs, files in os.walk(path):\n            for file in files:\n                # make sure the files are part of the necessary file list\n                if file.endswith(file_list) or file.endswith(optional_file_list):\n                    zf.write(os.path.join(root, file))\n\n    # create a zip file with the necessary files\n    zipf = zipfile.ZipFile(filename, \'w\', zipfile.ZIP_DEFLATED)\n    zipdir(\'.\', zipf)\n    zipf.close()\n    print(\'Your assignment zip file has been created!\')\n    print(\'Now submit the file:\\n{}\\nto Kadenze for grading!\'.format(\n        os.path.abspath(filename)))\n\n\ndef normalize(a, s=0.1):\n    \'\'\'Normalize the image range for visualization\'\'\'\n    return np.uint8(np.clip(\n        (a - a.mean()) / max(a.std(), 1e-4) * s + 0.5,\n        0, 1) * 255)\n\n\n# %%\ndef weight_variable(shape, **kwargs):\n    \'\'\'Helper function to create a weight variable initialized with\n    a normal distribution\n    Parameters\n    ----------\n    shape : list\n        Size of weight variable\n    \'\'\'\n    if isinstance(shape, list):\n        initial = tf.random_normal(tf.stack(shape), mean=0.0, stddev=0.01)\n        initial.set_shape(shape)\n    else:\n        initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n    return tf.Variable(initial, **kwargs)\n\n\n# %%\ndef bias_variable(shape, **kwargs):\n    \'\'\'Helper function to create a bias variable initialized with\n    a constant value.\n    Parameters\n    ----------\n    shape : list\n        Size of weight variable\n    \'\'\'\n    if isinstance(shape, list):\n        initial = tf.random_normal(tf.stack(shape), mean=0.0, stddev=0.01)\n        initial.set_shape(shape)\n    else:\n        initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n    return tf.Variable(initial, **kwargs)\n\n\ndef binary_cross_entropy(z, x):\n    """"""Binary Cross Entropy measures cross entropy of a binary variable.\n\n    loss(x, z) = - sum_i (x[i] * log(z[i]) + (1 - x[i]) * log(1 - z[i]))\n\n    Parameters\n    ----------\n    z : tf.Tensor\n        A `Tensor` of the same type and shape as `x`.\n    x : tf.Tensor\n        A `Tensor` of type `float32` or `float64`.\n    """"""\n    eps = 1e-12\n    return (-(x * tf.log(z + eps) +\n              (1. - x) * tf.log(1. - z + eps)))\n\n\ndef conv2d(x, n_output,\n           k_h=5, k_w=5, d_h=2, d_w=2,\n           padding=\'SAME\', name=\'conv2d\', reuse=None):\n    """"""Helper for creating a 2d convolution operation.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to convolve.\n    n_output : int\n        Number of filters.\n    k_h : int, optional\n        Kernel height\n    k_w : int, optional\n        Kernel width\n    d_h : int, optional\n        Height stride\n    d_w : int, optional\n        Width stride\n    padding : str, optional\n        Padding type: ""SAME"" or ""VALID""\n    name : str, optional\n        Variable scope\n\n    Returns\n    -------\n    op : tf.Tensor\n        Output of convolution\n    """"""\n    with tf.variable_scope(name or \'conv2d\', reuse=reuse):\n        W = tf.get_variable(\n            name=\'W\',\n            shape=[k_h, k_w, x.get_shape()[-1], n_output],\n            initializer=tf.contrib.layers.xavier_initializer_conv2d())\n\n        conv = tf.nn.conv2d(\n            name=\'conv\',\n            input=x,\n            filter=W,\n            strides=[1, d_h, d_w, 1],\n            padding=padding)\n\n        b = tf.get_variable(\n            name=\'b\',\n            shape=[n_output],\n            initializer=tf.constant_initializer(0.0))\n\n        h = tf.nn.bias_add(\n            name=\'h\',\n            value=conv,\n            bias=b)\n\n    return h, W\n\n\ndef deconv2d(x, n_output_h, n_output_w, n_output_ch, n_input_ch=None,\n             k_h=5, k_w=5, d_h=2, d_w=2,\n             padding=\'SAME\', name=\'deconv2d\', reuse=None):\n    """"""Deconvolution helper.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to convolve.\n    n_output_h : int\n        Height of output\n    n_output_w : int\n        Width of output\n    n_output_ch : int\n        Number of filters.\n    k_h : int, optional\n        Kernel height\n    k_w : int, optional\n        Kernel width\n    d_h : int, optional\n        Height stride\n    d_w : int, optional\n        Width stride\n    padding : str, optional\n        Padding type: ""SAME"" or ""VALID""\n    name : str, optional\n        Variable scope\n\n    Returns\n    -------\n    op : tf.Tensor\n        Output of deconvolution\n    """"""\n    with tf.variable_scope(name or \'deconv2d\', reuse=reuse):\n        W = tf.get_variable(\n            name=\'W\',\n            shape=[k_h, k_w, n_output_ch, n_input_ch or x.get_shape()[-1]],\n            initializer=tf.contrib.layers.xavier_initializer_conv2d())\n\n        conv = tf.nn.conv2d_transpose(\n            name=\'conv_t\',\n            value=x,\n            filter=W,\n            output_shape=tf.stack(\n                [tf.shape(x)[0], n_output_h, n_output_w, n_output_ch]),\n            strides=[1, d_h, d_w, 1],\n            padding=padding)\n\n        conv.set_shape([None, n_output_h, n_output_w, n_output_ch])\n\n        b = tf.get_variable(\n            name=\'b\',\n            shape=[n_output_ch],\n            initializer=tf.constant_initializer(0.0))\n\n        h = tf.nn.bias_add(name=\'h\', value=conv, bias=b)\n\n    return h, W\n\n\ndef lrelu(features, leak=0.2):\n    """"""Leaky rectifier.\n\n    Parameters\n    ----------\n    features : tf.Tensor\n        Input to apply leaky rectifier to.\n    leak : float, optional\n        Percentage of leak.\n\n    Returns\n    -------\n    op : tf.Tensor\n        Resulting output of applying leaky rectifier activation.\n    """"""\n    f1 = 0.5 * (1 + leak)\n    f2 = 0.5 * (1 - leak)\n    return f1 * features + f2 * abs(features)\n\n\ndef linear(x, n_output, name=None, activation=None, reuse=None):\n    """"""Fully connected layer.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to connect\n    n_output : int\n        Number of output neurons\n    name : None, optional\n        Scope to apply\n\n    Returns\n    -------\n    h, W : tf.Tensor, tf.Tensor\n        Output of fully connected layer and the weight matrix\n    """"""\n    if len(x.get_shape()) != 2:\n        x = flatten(x, reuse=reuse)\n\n    n_input = x.get_shape().as_list()[1]\n\n    with tf.variable_scope(name or ""fc"", reuse=reuse):\n        W = tf.get_variable(\n            name=\'W\',\n            shape=[n_input, n_output],\n            dtype=tf.float32,\n            initializer=tf.contrib.layers.xavier_initializer())\n\n        b = tf.get_variable(\n            name=\'b\',\n            shape=[n_output],\n            dtype=tf.float32,\n            initializer=tf.constant_initializer(0.0))\n\n        h = tf.nn.bias_add(\n            name=\'h\',\n            value=tf.matmul(x, W),\n            bias=b)\n\n        if activation:\n            h = activation(h)\n\n        return h, W\n\n\ndef flatten(x, name=None, reuse=None):\n    """"""Flatten Tensor to 2-dimensions.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to flatten.\n    name : None, optional\n        Variable scope for flatten operations\n\n    Returns\n    -------\n    flattened : tf.Tensor\n        Flattened tensor.\n    """"""\n    with tf.variable_scope(\'flatten\'):\n        dims = x.get_shape().as_list()\n        if len(dims) == 4:\n            flattened = tf.reshape(\n                x,\n                shape=[-1, dims[1] * dims[2] * dims[3]])\n        elif len(dims) == 2 or len(dims) == 1:\n            flattened = x\n        else:\n            raise ValueError(\'Expected n dimensions of 1, 2 or 4.  Found:\',\n                             len(dims))\n\n        return flattened\n\n\ndef to_tensor(x):\n    """"""Convert 2 dim Tensor to a 4 dim Tensor ready for convolution.\n\n    Performs the opposite of flatten(x).  If the tensor is already 4-D, this\n    returns the same as the input, leaving it unchanged.\n\n    Parameters\n    ----------\n    x : tf.Tesnor\n        Input 2-D tensor.  If 4-D already, left unchanged.\n\n    Returns\n    -------\n    x : tf.Tensor\n        4-D representation of the input.\n\n    Raises\n    ------\n    ValueError\n        If the tensor is not 2D or already 4D.\n    """"""\n    if len(x.get_shape()) == 2:\n        n_input = x.get_shape().as_list()[1]\n        x_dim = np.sqrt(n_input)\n        if x_dim == int(x_dim):\n            x_dim = int(x_dim)\n            x_tensor = tf.reshape(\n                x, [-1, x_dim, x_dim, 1], name=\'reshape\')\n        elif np.sqrt(n_input / 3) == int(np.sqrt(n_input / 3)):\n            x_dim = int(np.sqrt(n_input / 3))\n            x_tensor = tf.reshape(\n                x, [-1, x_dim, x_dim, 3], name=\'reshape\')\n        else:\n            x_tensor = tf.reshape(\n                x, [-1, 1, 1, n_input], name=\'reshape\')\n    elif len(x.get_shape()) == 4:\n        x_tensor = x\n    else:\n        raise ValueError(\'Unsupported input dimensions\')\n    return x_tensor\n'"
session-3/libs/vae.py,40,"b'""""""Convolutional/Variational autoencoder, including demonstration of\ntraining such a network on MNIST, CelebNet and the film, ""Sita Sings The Blues""\nusing an image pipeline.\n\nCopyright Parag K. Mital, January 2016\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport os\nfrom libs.dataset_utils import create_input_pipeline\nfrom libs.datasets import CELEB, MNIST\nfrom libs.batch_norm import batch_norm\nfrom libs import utils\n\n\ndef VAE(input_shape=[None, 784],\n        n_filters=[64, 64, 64],\n        filter_sizes=[4, 4, 4],\n        n_hidden=32,\n        n_code=2,\n        activation=tf.nn.tanh,\n        dropout=False,\n        denoising=False,\n        convolutional=False,\n        variational=False):\n    """"""(Variational) (Convolutional) (Denoising) Autoencoder.\n\n    Uses tied weights.\n\n    Parameters\n    ----------\n    input_shape : list, optional\n        Shape of the input to the network. e.g. for MNIST: [None, 784].\n    n_filters : list, optional\n        Number of filters for each layer.\n        If convolutional=True, this refers to the total number of output\n        filters to create for each layer, with each layer\'s number of output\n        filters as a list.\n        If convolutional=False, then this refers to the total number of neurons\n        for each layer in a fully connected network.\n    filter_sizes : list, optional\n        Only applied when convolutional=True.  This refers to the ksize (height\n        and width) of each convolutional layer.\n    n_hidden : int, optional\n        Only applied when variational=True.  This refers to the first fully\n        connected layer prior to the variational embedding, directly after\n        the encoding.  After the variational embedding, another fully connected\n        layer is created with the same size prior to decoding.  Set to 0 to\n        not use an additional hidden layer.\n    n_code : int, optional\n        Only applied when variational=True.  This refers to the number of\n        latent Gaussians to sample for creating the inner most encoding.\n    activation : function, optional\n        Activation function to apply to each layer, e.g. tf.nn.relu\n    dropout : bool, optional\n        Whether or not to apply dropout.  If using dropout, you must feed a\n        value for \'keep_prob\', as returned in the dictionary.  1.0 means no\n        dropout is used.  0.0 means every connection is dropped.  Sensible\n        values are between 0.5-0.8.\n    denoising : bool, optional\n        Whether or not to apply denoising.  If using denoising, you must feed a\n        value for \'corrupt_prob\', as returned in the dictionary.  1.0 means no\n        corruption is used.  0.0 means every feature is corrupted.  Sensible\n        values are between 0.5-0.8.\n    convolutional : bool, optional\n        Whether or not to use a convolutional network or else a fully connected\n        network will be created.  This effects the n_filters parameter\'s\n        meaning.\n    variational : bool, optional\n        Whether or not to create a variational embedding layer.  This will\n        create a fully connected layer after the encoding, if `n_hidden` is\n        greater than 0, then will create a multivariate gaussian sampling\n        layer, then another fully connected layer.  The size of the fully\n        connected layers are determined by `n_hidden`, and the size of the\n        sampling layer is determined by `n_code`.\n\n    Returns\n    -------\n    model : dict\n        {\n            \'cost\': Tensor to optimize.\n            \'Ws\': All weights of the encoder.\n            \'x\': Input Placeholder\n            \'z\': Inner most encoding Tensor (latent features)\n            \'y\': Reconstruction of the Decoder\n            \'keep_prob\': Amount to keep when using Dropout\n            \'corrupt_prob\': Amount to corrupt when using Denoising\n            \'train\': Set to True when training/Applies to Batch Normalization.\n        }\n    """"""\n    # network input / placeholders for train (bn) and dropout\n    x = tf.placeholder(tf.float32, input_shape, \'x\')\n    phase_train = tf.placeholder(tf.bool, name=\'phase_train\')\n    keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n    corrupt_prob = tf.placeholder(tf.float32, [1])\n\n    # apply noise if denoising\n    x_ = (utils.corrupt(x) * corrupt_prob + x * (1 - corrupt_prob)) if denoising else x\n\n    # 2d -> 4d if convolution\n    x_tensor = utils.to_tensor(x_) if convolutional else x_\n    current_input = x_tensor\n\n    Ws = []\n    shapes = []\n\n    # Build the encoder\n    for layer_i, n_output in enumerate(n_filters):\n        with tf.variable_scope(\'encoder/{}\'.format(layer_i)):\n            shapes.append(current_input.get_shape().as_list())\n            if convolutional:\n                h, W = utils.conv2d(x=current_input,\n                                    n_output=n_output,\n                                    k_h=filter_sizes[layer_i],\n                                    k_w=filter_sizes[layer_i])\n            else:\n                h, W = utils.linear(x=current_input,\n                                    n_output=n_output)\n            h = activation(batch_norm(h, phase_train, \'bn\' + str(layer_i)))\n            if dropout:\n                h = tf.nn.dropout(h, keep_prob)\n            Ws.append(W)\n            current_input = h\n\n    shapes.append(current_input.get_shape().as_list())\n\n    with tf.variable_scope(\'variational\'):\n        if variational:\n            dims = current_input.get_shape().as_list()\n            flattened = utils.flatten(current_input)\n\n            if n_hidden:\n                h = utils.linear(flattened, n_hidden, name=\'W_fc\')[0]\n                h = activation(batch_norm(h, phase_train, \'fc/bn\'))\n                if dropout:\n                    h = tf.nn.dropout(h, keep_prob)\n            else:\n                h = flattened\n\n            z_mu = utils.linear(h, n_code, name=\'mu\')[0]\n            z_log_sigma = 0.5 * utils.linear(h, n_code, name=\'log_sigma\')[0]\n\n            # Sample from noise distribution p(eps) ~ N(0, 1)\n            epsilon = tf.random_normal(\n                tf.stack([tf.shape(x)[0], n_code]))\n\n            # Sample from posterior\n            z = z_mu + tf.multiply(epsilon, tf.exp(z_log_sigma))\n\n            if n_hidden:\n                h = utils.linear(z, n_hidden, name=\'fc_t\')[0]\n                h = activation(batch_norm(h, phase_train, \'fc_t/bn\'))\n                if dropout:\n                    h = tf.nn.dropout(h, keep_prob)\n            else:\n                h = z\n\n            size = dims[1] * dims[2] * dims[3] if convolutional else dims[1]\n            h = utils.linear(h, size, name=\'fc_t2\')[0]\n            current_input = activation(batch_norm(h, phase_train, \'fc_t2/bn\'))\n            if dropout:\n                current_input = tf.nn.dropout(current_input, keep_prob)\n\n            if convolutional:\n                current_input = tf.reshape(\n                    current_input, tf.stack([\n                        tf.shape(current_input)[0],\n                        dims[1],\n                        dims[2],\n                        dims[3]]))\n        else:\n            z = current_input\n\n    shapes.reverse()\n    n_filters.reverse()\n    Ws.reverse()\n\n    n_filters += [input_shape[-1]]\n\n    # %%\n    # Decoding layers\n    for layer_i, n_output in enumerate(n_filters[1:]):\n        with tf.variable_scope(\'decoder/{}\'.format(layer_i)):\n            shape = shapes[layer_i + 1]\n            if convolutional:\n                h, W = utils.deconv2d(x=current_input,\n                                      n_output_h=shape[1],\n                                      n_output_w=shape[2],\n                                      n_output_ch=shape[3],\n                                      n_input_ch=shapes[layer_i][3],\n                                      k_h=filter_sizes[layer_i],\n                                      k_w=filter_sizes[layer_i])\n            else:\n                h, W = utils.linear(x=current_input,\n                                    n_output=n_output)\n            h = activation(batch_norm(h, phase_train, \'dec/bn\' + str(layer_i)))\n            if dropout:\n                h = tf.nn.dropout(h, keep_prob)\n            current_input = h\n\n    y = current_input\n    x_flat = utils.flatten(x)\n    y_flat = utils.flatten(y)\n\n    # l2 loss\n    loss_x = tf.reduce_sum(tf.squared_difference(x_flat, y_flat), 1)\n\n    if variational:\n        # variational lower bound, kl-divergence\n        loss_z = -0.5 * tf.reduce_sum(\n            1.0 + 2.0 * z_log_sigma -\n            tf.square(z_mu) - tf.exp(2.0 * z_log_sigma), 1)\n\n        # add l2 loss\n        cost = tf.reduce_mean(loss_x + loss_z)\n    else:\n        # just optimize l2 loss\n        cost = tf.reduce_mean(loss_x)\n\n    return {\'cost\': cost, \'Ws\': Ws,\n            \'x\': x, \'z\': z, \'y\': y,\n            \'keep_prob\': keep_prob,\n            \'corrupt_prob\': corrupt_prob,\n            \'train\': phase_train}\n\n\ndef train_vae(files,\n              input_shape,\n              learning_rate=0.0001,\n              batch_size=100,\n              n_epochs=50,\n              n_examples=10,\n              crop_shape=[64, 64, 3],\n              crop_factor=0.8,\n              n_filters=[100, 100, 100, 100],\n              n_hidden=256,\n              n_code=50,\n              convolutional=True,\n              variational=True,\n              filter_sizes=[3, 3, 3, 3],\n              dropout=True,\n              keep_prob=0.8,\n              activation=tf.nn.relu,\n              img_step=100,\n              save_step=100,\n              ckpt_name=""vae.ckpt""):\n    """"""General purpose training of a (Variational) (Convolutional) Autoencoder.\n\n    Supply a list of file paths to images, and this will do everything else.\n\n    Parameters\n    ----------\n    files : list of strings\n        List of paths to images.\n    input_shape : list\n        Must define what the input image\'s shape is.\n    learning_rate : float, optional\n        Learning rate.\n    batch_size : int, optional\n        Batch size.\n    n_epochs : int, optional\n        Number of epochs.\n    n_examples : int, optional\n        Number of example to use while demonstrating the current training\n        iteration\'s reconstruction.  Creates a square montage, so make\n        sure int(sqrt(n_examples))**2 = n_examples, e.g. 16, 25, 36, ... 100.\n    crop_shape : list, optional\n        Size to centrally crop the image to.\n    crop_factor : float, optional\n        Resize factor to apply before cropping.\n    n_filters : list, optional\n        Same as VAE\'s n_filters.\n    n_hidden : int, optional\n        Same as VAE\'s n_hidden.\n    n_code : int, optional\n        Same as VAE\'s n_code.\n    convolutional : bool, optional\n        Use convolution or not.\n    variational : bool, optional\n        Use variational layer or not.\n    filter_sizes : list, optional\n        Same as VAE\'s filter_sizes.\n    dropout : bool, optional\n        Use dropout or not\n    keep_prob : float, optional\n        Percent of keep for dropout.\n    activation : function, optional\n        Which activation function to use.\n    img_step : int, optional\n        How often to save training images showing the manifold and\n        reconstruction.\n    save_step : int, optional\n        How often to save checkpoints.\n    ckpt_name : str, optional\n        Checkpoints will be named as this, e.g. \'model.ckpt\'\n    """"""\n    batch = create_input_pipeline(\n        files=files,\n        batch_size=batch_size,\n        n_epochs=n_epochs,\n        crop_shape=crop_shape,\n        crop_factor=crop_factor,\n        shape=input_shape)\n\n    ae = VAE(input_shape=[None] + crop_shape,\n             convolutional=convolutional,\n             variational=variational,\n             n_filters=n_filters,\n             n_hidden=n_hidden,\n             n_code=n_code,\n             dropout=dropout,\n             filter_sizes=filter_sizes,\n             activation=activation)\n\n    # Create a manifold of our inner most layer to show\n    # example reconstructions.  This is one way to see\n    # what the ""embedding"" or ""latent space"" of the encoder\n    # is capable of encoding, though note that this is just\n    # a random hyperplane within the latent space, and does not\n    # encompass all possible embeddings.\n    zs = np.random.uniform(\n        -1.0, 1.0, [4, n_code]).astype(np.float32)\n    zs = utils.make_latent_manifold(zs, n_examples)\n\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate=learning_rate).minimize(ae[\'cost\'])\n\n    # We create a session to use the graph\n    sess = tf.Session()\n    saver = tf.train.Saver()\n    sess.run(tf.global_variables_initializer())\n\n    # This will handle our threaded image pipeline\n    coord = tf.train.Coordinator()\n\n    # Ensure no more changes to graph\n    tf.get_default_graph().finalize()\n\n    # Start up the queues for handling the image pipeline\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    if os.path.exists(ckpt_name + \'.index\') or os.path.exists(ckpt_name):\n        saver.restore(sess, ckpt_name)\n\n    # Fit all training data\n    t_i = 0\n    batch_i = 0\n    epoch_i = 0\n    cost = 0\n    n_files = len(files)\n    test_xs = sess.run(batch) / 255.0\n    utils.montage(test_xs, \'test_xs.png\')\n    try:\n        while not coord.should_stop() and epoch_i < n_epochs:\n            batch_i += 1\n            batch_xs = sess.run(batch) / 255.0\n            train_cost = sess.run([ae[\'cost\'], optimizer], feed_dict={\n                ae[\'x\']: batch_xs, ae[\'train\']: True,\n                ae[\'keep_prob\']: keep_prob})[0]\n            print(batch_i, train_cost)\n            cost += train_cost\n            if batch_i % n_files == 0:\n                print(\'epoch:\', epoch_i)\n                print(\'average cost:\', cost / batch_i)\n                cost = 0\n                batch_i = 0\n                epoch_i += 1\n\n            if batch_i % img_step == 0:\n                # Plot example reconstructions from latent layer\n                recon = sess.run(\n                    ae[\'y\'], feed_dict={\n                        ae[\'z\']: zs,\n                        ae[\'train\']: False,\n                        ae[\'keep_prob\']: 1.0})\n                utils.montage(recon.reshape([-1] + crop_shape),\n                              \'manifold_%08d.png\' % t_i)\n\n                # Plot example reconstructions\n                recon = sess.run(\n                    ae[\'y\'], feed_dict={ae[\'x\']: test_xs,\n                                        ae[\'train\']: False,\n                                        ae[\'keep_prob\']: 1.0})\n                print(\'reconstruction (min, max, mean):\',\n                    recon.min(), recon.max(), recon.mean())\n                utils.montage(recon.reshape([-1] + crop_shape),\n                              \'reconstruction_%08d.png\' % t_i)\n                t_i += 1\n\n            if batch_i % save_step == 0:\n                # Save the variables to disk.\n                saver.save(sess, ""./"" + ckpt_name,\n                           global_step=batch_i,\n                           write_meta_graph=False)\n    except tf.errors.OutOfRangeError:\n        print(\'Done.\')\n    finally:\n        # One of the threads has issued an exception.  So let\'s tell all the\n        # threads to shutdown.\n        coord.request_stop()\n\n    # Wait until all threads have finished.\n    coord.join(threads)\n\n    # Clean up the session.\n    sess.close()\n\n\n# %%\ndef test_mnist(n_epochs=10):\n    """"""Train an autoencoder on MNIST.\n\n    This function will train an autoencoder on MNIST and also\n    save many image files during the training process, demonstrating\n    the latent space of the inner most dimension of the encoder,\n    as well as reconstructions of the decoder.\n    """"""\n\n    # load MNIST\n    n_code = 2\n    mnist = MNIST(split=[0.8, 0.1, 0.1])\n    ae = VAE(input_shape=[None, 784], n_filters=[512, 256],\n             n_hidden=64, n_code=n_code, activation=tf.nn.sigmoid,\n             convolutional=False, variational=True)\n\n    n_examples = 100\n    zs = np.random.uniform(\n        -1.0, 1.0, [4, n_code]).astype(np.float32)\n    zs = utils.make_latent_manifold(zs, n_examples)\n\n    learning_rate = 0.02\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate=learning_rate).minimize(ae[\'cost\'])\n\n    # We create a session to use the graph\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n\n    # Fit all training data\n    t_i = 0\n    batch_i = 0\n    batch_size = 200\n    test_xs = mnist.test.images[:n_examples]\n    utils.montage(test_xs.reshape((-1, 28, 28)), \'test_xs.png\')\n    for epoch_i in range(n_epochs):\n        train_i = 0\n        train_cost = 0\n        for batch_xs, _ in mnist.train.next_batch(batch_size):\n            train_cost += sess.run([ae[\'cost\'], optimizer], feed_dict={\n                ae[\'x\']: batch_xs, ae[\'train\']: True, ae[\'keep_prob\']: 1.0})[0]\n            train_i += 1\n            if batch_i % 10 == 0:\n                # Plot example reconstructions from latent layer\n                recon = sess.run(\n                    ae[\'y\'], feed_dict={\n                        ae[\'z\']: zs,\n                        ae[\'train\']: False,\n                        ae[\'keep_prob\']: 1.0})\n                utils.montage(recon.reshape((-1, 28, 28)),\n                    \'manifold_%08d.png\' % t_i)\n                # Plot example reconstructions\n                recon = sess.run(\n                    ae[\'y\'], feed_dict={ae[\'x\']: test_xs,\n                                        ae[\'train\']: False,\n                                        ae[\'keep_prob\']: 1.0})\n                utils.montage(recon.reshape(\n                    (-1, 28, 28)), \'reconstruction_%08d.png\' % t_i)\n                t_i += 1\n            batch_i += 1\n\n        valid_i = 0\n        valid_cost = 0\n        for batch_xs, _ in mnist.valid.next_batch(batch_size):\n            valid_cost += sess.run([ae[\'cost\']], feed_dict={\n                ae[\'x\']: batch_xs, ae[\'train\']: False, ae[\'keep_prob\']: 1.0})[0]\n            valid_i += 1\n        print(\'train:\', train_cost / train_i, \'valid:\', valid_cost / valid_i)\n\n\ndef test_celeb(n_epochs=50):\n    """"""Train an autoencoder on Celeb Net.\n    """"""\n    files = CELEB()\n    train_vae(\n        files=files,\n        input_shape=[218, 178, 3],\n        batch_size=100,\n        n_epochs=n_epochs,\n        crop_shape=[64, 64, 3],\n        crop_factor=0.8,\n        convolutional=True,\n        variational=True,\n        n_filters=[100, 100, 100],\n        n_hidden=250,\n        n_code=100,\n        dropout=True,\n        filter_sizes=[3, 3, 3],\n        activation=tf.nn.sigmoid,\n        ckpt_name=\'./celeb.ckpt\')\n\n\ndef test_sita():\n    """"""Train an autoencoder on Sita Sings The Blues.\n    """"""\n    if not os.path.exists(\'sita\'):\n        os.system(\'wget http://ossguy.com/sita/Sita_Sings_the_Blues_640x360_XviD.avi\')\n        os.mkdir(\'sita\')\n        os.system(\'ffmpeg -i Sita_Sings_the_Blues_640x360_XviD.avi -r 60 -f\' +\n                  \' image2 -s 160x90 sita/sita-%08d.jpg\')\n    files = [os.path.join(\'sita\', f) for f in os.listdir(\'sita\')]\n\n    train_vae(\n        files=files,\n        input_shape=[90, 160, 3],\n        batch_size=100,\n        n_epochs=50,\n        crop_shape=[90, 160, 3],\n        crop_factor=1.0,\n        convolutional=True,\n        variational=True,\n        n_filters=[100, 100, 100],\n        n_hidden=250,\n        n_code=100,\n        dropout=True,\n        filter_sizes=[3, 3, 3],\n        activation=tf.nn.sigmoid,\n        ckpt_name=\'./sita.ckpt\')\n\n\nif __name__ == \'__main__\':\n    test_celeb()\n'"
session-3/tests/test_3.py,0,"b""import matplotlib\nmatplotlib.use('Agg')\nimport tensorflow as tf\nimport numpy as np\nfrom libs import utils\nfrom libs import dataset_utils\nfrom libs import vae\n\n\ndef test_mnist_vae():\n    vae.test_mnist(n_epochs=1)\n\n\ndef test_cifar():\n    dataset_utils.cifar10_download('cifar10')\n\n"""
session-4/libs/__init__.py,0,b''
session-4/libs/batch_norm.py,14,"b'""""""Batch Normalization for TensorFlow.\nParag K. Mital, Jan 2016.\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef batch_norm(x, phase_train, name=\'bn\', decay=0.99, reuse=None, affine=True):\n    """"""\n    Batch normalization on convolutional maps.\n    from: https://stackoverflow.com/questions/33949786/how-could-i-\n    use-batch-normalization-in-tensorflow\n    Only modified to infer shape from input tensor x.\n    Parameters\n    ----------\n    x\n        Tensor, 4D BHWD input maps\n    phase_train\n        boolean tf.Variable, true indicates training phase\n    name\n        string, variable name\n    affine\n        whether to affine-transform outputs\n    Return\n    ------\n    normed\n        batch-normalized maps\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        og_shape = x.get_shape().as_list()\n        if len(og_shape) == 2:\n            x = tf.reshape(x, [-1, 1, 1, og_shape[1]])\n        shape = x.get_shape().as_list()\n        beta = tf.get_variable(name=\'beta\', shape=[shape[-1]],\n                               initializer=tf.constant_initializer(0.0),\n                               trainable=True)\n        gamma = tf.get_variable(name=\'gamma\', shape=[shape[-1]],\n                                initializer=tf.constant_initializer(1.0),\n                                trainable=affine)\n\n        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name=\'moments\')\n        ema = tf.train.ExponentialMovingAverage(decay=decay)\n        ema_apply_op = ema.apply([batch_mean, batch_var])\n        ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n\n        def mean_var_with_update():\n            """"""Summary\n            Returns\n            -------\n            name : TYPE\n                Description\n            """"""\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n        mean, var = control_flow_ops.cond(phase_train,\n                                          mean_var_with_update,\n                                          lambda: (ema_mean, ema_var))\n\n        # tf.nn.batch_normalization\n        normed = tf.nn.batch_norm_with_global_normalization(\n            x, mean, var, beta, gamma, 1e-5, affine)\n        if len(og_shape) == 2:\n            normed = tf.reshape(normed, [-1, og_shape[-1]])\n    return normed\n'"
session-4/libs/dataset_utils.py,8,"b'""""""Utils for dataset creation.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\n\nimport os\nimport pickle\nimport numpy as np\nimport tensorflow as tf\nfrom . import dft\nfrom .utils import download_and_extract_tar\n\n\ndef create_input_pipeline(files, batch_size, n_epochs, shape, crop_shape=None,\n                          crop_factor=1.0, n_threads=4):\n    """"""Creates a pipefile from a list of image files.\n    Includes batch generator/central crop/resizing options.\n    The resulting generator will dequeue the images batch_size at a time until\n    it throws tf.errors.OutOfRangeError when there are no more images left in\n    the queue.\n\n    Parameters\n    ----------\n    files : list\n        List of paths to image files.\n    batch_size : int\n        Number of image files to load at a time.\n    n_epochs : int\n        Number of epochs to run before raising tf.errors.OutOfRangeError\n    shape : list\n        [height, width, channels]\n    crop_shape : list\n        [height, width] to crop image to.\n    crop_factor : float\n        Percentage of image to take starting from center.\n    n_threads : int, optional\n        Number of threads to use for batch shuffling\n    """"""\n\n    # We first create a ""producer"" queue.  It creates a production line which\n    # will queue up the file names and allow another queue to deque the file\n    # names all using a tf queue runner.\n    # Put simply, this is the entry point of the computational graph.\n    # It will generate the list of file names.\n    # We also specify it\'s capacity beforehand.\n    producer = tf.train.string_input_producer(\n        files, capacity=len(files))\n\n    # We need something which can open the files and read its contents.\n    reader = tf.WholeFileReader()\n\n    # We pass the filenames to this object which can read the file\'s contents.\n    # This will create another queue running which dequeues the previous queue.\n    keys, vals = reader.read(producer)\n\n    # And then have to decode its contents as we know it is a jpeg image\n    imgs = tf.image.decode_jpeg(\n        vals,\n        channels=3 if len(shape) > 2 and shape[2] == 3 else 0)\n\n    # We have to explicitly define the shape of the tensor.\n    # This is because the decode_jpeg operation is still a node in the graph\n    # and doesn\'t yet know the shape of the image.  Future operations however\n    # need explicit knowledge of the image\'s shape in order to be created.\n    imgs.set_shape(shape)\n\n    # Next we\'ll centrally crop the image to the size of 100x100.\n    # This operation required explicit knowledge of the image\'s shape.\n    if shape[0] > shape[1]:\n        rsz_shape = [int(shape[0] / shape[1] * crop_shape[0] / crop_factor),\n                     int(crop_shape[1] / crop_factor)]\n    else:\n        rsz_shape = [int(crop_shape[0] / crop_factor),\n                     int(shape[1] / shape[0] * crop_shape[1] / crop_factor)]\n    rszs = tf.image.resize_images(imgs, rsz_shape)\n    crops = (tf.image.resize_image_with_crop_or_pad(\n        rszs, crop_shape[0], crop_shape[1])\n        if crop_shape is not None\n        else imgs)\n\n    # Now we\'ll create a batch generator that will also shuffle our examples.\n    # We tell it how many it should have in its buffer when it randomly\n    # permutes the order.\n    min_after_dequeue = len(files) // 10\n\n    # The capacity should be larger than min_after_dequeue, and determines how\n    # many examples are prefetched.  TF docs recommend setting this value to:\n    # min_after_dequeue + (num_threads + a small safety margin) * batch_size\n    capacity = min_after_dequeue + (n_threads + 1) * batch_size\n\n    # Randomize the order and output batches of batch_size.\n    batch = tf.train.shuffle_batch([crops],\n                                   enqueue_many=False,\n                                   batch_size=batch_size,\n                                   capacity=capacity,\n                                   min_after_dequeue=min_after_dequeue,\n                                   num_threads=n_threads)\n\n    # alternatively, we could use shuffle_batch_join to use multiple reader\n    # instances, or set shuffle_batch\'s n_threads to higher than 1.\n\n    return batch\n\n\ndef gtzan_music_speech_download(dst=\'gtzan_music_speech\'):\n    """"""Download the GTZAN music and speech dataset.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Location to put the GTZAN music and speech datset.\n    """"""\n    path = \'http://opihi.cs.uvic.ca/sound/music_speech.tar.gz\'\n    download_and_extract_tar(path, dst)\n\n\ndef gtzan_music_speech_load(dst=\'gtzan_music_speech\'):\n    """"""Load the GTZAN Music and Speech dataset.\n\n    Downloads the dataset if it does not exist into the dst directory.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Location of GTZAN Music and Speech dataset.\n\n    Returns\n    -------\n    Xs, ys : np.ndarray, np.ndarray\n        Array of data, Array of labels\n    """"""\n    from scipy.io import wavfile\n\n    if not os.path.exists(dst):\n        gtzan_music_speech_download(dst)\n    music_dir = os.path.join(os.path.join(dst, \'music_speech\'), \'music_wav\')\n    music = [os.path.join(music_dir, file_i)\n             for file_i in os.listdir(music_dir)\n             if file_i.endswith(\'.wav\')]\n    speech_dir = os.path.join(os.path.join(dst, \'music_speech\'), \'speech_wav\')\n    speech = [os.path.join(speech_dir, file_i)\n              for file_i in os.listdir(speech_dir)\n              if file_i.endswith(\'.wav\')]\n    Xs = []\n    ys = []\n    for i in music:\n        sr, s = wavfile.read(i)\n        s = s / 16384.0 - 1.0\n        re, im = dft.dft_np(s)\n        mag, phs = dft.ztoc(re, im)\n        Xs.append((mag, phs))\n        ys.append(0)\n    for i in speech:\n        sr, s = wavfile.read(i)\n        s = s / 16384.0 - 1.0\n        re, im = dft.dft_np(s)\n        mag, phs = dft.ztoc(re, im)\n        Xs.append((mag, phs))\n        ys.append(1)\n    Xs = np.array(Xs)\n    Xs = np.transpose(Xs, [0, 2, 3, 1])\n    ys = np.array(ys)\n    return Xs, ys\n\n\ndef cifar10_download(dst=\'cifar10\'):\n    """"""Download the CIFAR10 dataset.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Directory to download into.\n    """"""\n    path = \'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'\n    download_and_extract_tar(path, dst)\n\n\ndef cifar10_load(dst=\'cifar10\'):\n    """"""Load the CIFAR10 dataset.\n\n    Downloads the dataset if it does not exist into the dst directory.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Location of CIFAR10 dataset.\n\n    Returns\n    -------\n    Xs, ys : np.ndarray, np.ndarray\n        Array of data, Array of labels\n    """"""\n    if not os.path.exists(dst):\n        cifar10_download(dst)\n    Xs = None\n    ys = None\n    for f in range(1, 6):\n        cf = pickle.load(open(\n            \'%s/cifar-10-batches-py/data_batch_%d\' % (dst, f), \'rb\'),\n            encoding=\'LATIN\')\n        if Xs is not None:\n            Xs = np.r_[Xs, cf[\'data\']]\n            ys = np.r_[ys, np.array(cf[\'labels\'])]\n        else:\n            Xs = cf[\'data\']\n            ys = cf[\'labels\']\n    Xs = np.swapaxes(np.swapaxes(Xs.reshape(-1, 3, 32, 32), 1, 3), 1, 2)\n    return Xs, ys\n\n\ndef dense_to_one_hot(labels, n_classes=2):\n    """"""Convert class labels from scalars to one-hot vectors.\n\n    Parameters\n    ----------\n    labels : array\n        Input labels to convert to one-hot representation.\n    n_classes : int, optional\n        Number of possible one-hot.\n\n    Returns\n    -------\n    one_hot : array\n        One hot representation of input.\n    """"""\n    return np.eye(n_classes).astype(np.float32)[labels]\n\n\nclass DatasetSplit(object):\n    """"""Utility class for batching data and handling multiple splits.\n\n    Attributes\n    ----------\n    current_batch_idx : int\n        Description\n    images : np.ndarray\n        Xs of the dataset.  Not necessarily images.\n    labels : np.ndarray\n        ys of the dataset.\n    n_labels : int\n        Number of possible labels\n    num_examples : int\n        Number of total observations\n    """"""\n\n    def __init__(self, images, labels):\n        """"""Initialize a DatasetSplit object.\n\n        Parameters\n        ----------\n        images : np.ndarray\n            Xs/inputs\n        labels : np.ndarray\n            ys/outputs\n        """"""\n        self.images = np.array(images).astype(np.float32)\n        if labels is not None:\n            self.labels = np.array(labels).astype(np.int32)\n            self.n_labels = len(np.unique(labels))\n        else:\n            self.labels = None\n        self.num_examples = len(self.images)\n\n    def next_batch(self, batch_size=100):\n        """"""Batch generator with randomization.\n\n        Parameters\n        ----------\n        batch_size : int, optional\n            Size of each minibatch.\n\n        Returns\n        -------\n        Xs, ys : np.ndarray, np.ndarray\n            Next batch of inputs and labels (if no labels, then None).\n        """"""\n        # Shuffle each epoch\n        current_permutation = np.random.permutation(range(len(self.images)))\n        epoch_images = self.images[current_permutation, ...]\n        if self.labels is not None:\n            epoch_labels = self.labels[current_permutation, ...]\n\n        # Then iterate over the epoch\n        self.current_batch_idx = 0\n        while self.current_batch_idx < len(self.images):\n            end_idx = min(\n                self.current_batch_idx + batch_size, len(self.images))\n            this_batch = {\n                \'images\': epoch_images[self.current_batch_idx:end_idx],\n                \'labels\': epoch_labels[self.current_batch_idx:end_idx]\n                if self.labels is not None else None\n            }\n            self.current_batch_idx += batch_size\n            yield this_batch[\'images\'], this_batch[\'labels\']\n\n\nclass Dataset(object):\n    """"""Create a dataset from data and their labels.\n\n    Allows easy use of train/valid/test splits; Batch generator.\n\n    Attributes\n    ----------\n    all_idxs : list\n        All indexes across all splits.\n    all_inputs : list\n        All inputs across all splits.\n    all_labels : list\n        All labels across all splits.\n    n_labels : int\n        Number of labels.\n    split : list\n        Percentage split of train, valid, test sets.\n    test_idxs : list\n        Indexes of the test split.\n    train_idxs : list\n        Indexes of the train split.\n    valid_idxs : list\n        Indexes of the valid split.\n    """"""\n\n    def __init__(self, Xs, ys=None, split=[1.0, 0.0, 0.0], one_hot=False):\n        """"""Initialize a Dataset object.\n\n        Parameters\n        ----------\n        Xs : np.ndarray\n            Images/inputs to a network\n        ys : np.ndarray\n            Labels/outputs to a network\n        split : list, optional\n            Percentage of train, valid, and test sets.\n        one_hot : bool, optional\n            Whether or not to use one-hot encoding of labels (ys).\n        """"""\n        self.all_idxs = []\n        self.all_labels = []\n        self.all_inputs = []\n        self.train_idxs = []\n        self.valid_idxs = []\n        self.test_idxs = []\n        self.n_labels = 0\n        self.split = split\n\n        # Now mix all the labels that are currently stored as blocks\n        self.all_inputs = Xs\n        n_idxs = len(self.all_inputs)\n        idxs = range(n_idxs)\n        rand_idxs = np.random.permutation(idxs)\n        self.all_inputs = self.all_inputs[rand_idxs, ...]\n        if ys is not None:\n            self.all_labels = ys if not one_hot else dense_to_one_hot(ys)\n            self.all_labels = self.all_labels[rand_idxs, ...]\n        else:\n            self.all_labels = None\n\n        # Get splits\n        self.train_idxs = idxs[:round(split[0] * n_idxs)]\n        self.valid_idxs = idxs[len(self.train_idxs):\n                               len(self.train_idxs) + round(split[1] * n_idxs)]\n        self.test_idxs = idxs[\n            (len(self.valid_idxs) + len(self.train_idxs)):\n            (len(self.valid_idxs) + len(self.train_idxs)) +\n             round(split[2] * n_idxs)]\n\n    @property\n    def X(self):\n        """"""Inputs/Xs/Images.\n\n        Returns\n        -------\n        all_inputs : np.ndarray\n            Original Inputs/Xs.\n        """"""\n        return self.all_inputs\n\n    @property\n    def Y(self):\n        """"""Outputs/ys/Labels.\n\n        Returns\n        -------\n        all_labels : np.ndarray\n            Original Outputs/ys.\n        """"""\n        return self.all_labels\n\n    @property\n    def train(self):\n        """"""Train split.\n\n        Returns\n        -------\n        split : DatasetSplit\n            Split of the train dataset.\n        """"""\n        if len(self.train_idxs):\n            inputs = self.all_inputs[self.train_idxs, ...]\n            if self.all_labels is not None:\n                labels = self.all_labels[self.train_idxs, ...]\n            else:\n                labels = None\n        else:\n            inputs, labels = [], []\n        return DatasetSplit(inputs, labels)\n\n    @property\n    def valid(self):\n        """"""Validation split.\n\n        Returns\n        -------\n        split : DatasetSplit\n            Split of the validation dataset.\n        """"""\n        if len(self.valid_idxs):\n            inputs = self.all_inputs[self.valid_idxs, ...]\n            if self.all_labels is not None:\n                labels = self.all_labels[self.valid_idxs, ...]\n            else:\n                labels = None\n        else:\n            inputs, labels = [], []\n        return DatasetSplit(inputs, labels)\n\n    @property\n    def test(self):\n        """"""Test split.\n\n        Returns\n        -------\n        split : DatasetSplit\n            Split of the test dataset.\n        """"""\n        if len(self.test_idxs):\n            inputs = self.all_inputs[self.test_idxs, ...]\n            if self.all_labels is not None:\n                labels = self.all_labels[self.test_idxs, ...]\n            else:\n                labels = None\n        else:\n            inputs, labels = [], []\n        return DatasetSplit(inputs, labels)\n\n    def mean(self):\n        """"""Mean of the inputs/Xs.\n\n        Returns\n        -------\n        mean : np.ndarray\n            Calculates mean across 0th (batch) dimension.\n        """"""\n        return np.mean(self.all_inputs, axis=0)\n\n    def std(self):\n        """"""Standard deviation of the inputs/Xs.\n\n        Returns\n        -------\n        std : np.ndarray\n            Calculates std across 0th (batch) dimension.\n        """"""\n        return np.std(self.all_inputs, axis=0)\n'"
session-4/libs/datasets.py,0,"b'""""""Creative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport tensorflow.examples.tutorials.mnist.input_data as input_data\nfrom .dataset_utils import *\n\n\ndef MNIST(one_hot=True, split=[1.0, 0.0, 0.0]):\n    """"""Returns the MNIST dataset.\n\n    Returns\n    -------\n    mnist : DataSet\n        DataSet object w/ convenienve props for accessing\n        train/validation/test sets and batches.\n    """"""\n    ds = input_data.read_data_sets(\'MNIST_data/\', one_hot=one_hot)\n    return Dataset(np.r_[ds.train.images,\n                         ds.validation.images,\n                         ds.test.images],\n                   np.r_[ds.train.labels,\n                         ds.validation.labels,\n                         ds.test.labels],\n                   split=split)\n\n\ndef CIFAR10(flatten=True, split=[1.0, 0.0, 0.0]):\n    """"""Returns the CIFAR10 dataset.\n\n    Parameters\n    ----------\n    flatten : bool, optional\n        Convert the 3 x 32 x 32 pixels to a single vector\n\n    Returns\n    -------\n    cifar : Dataset\n        Description\n    """"""\n    # plt.imshow(np.transpose(np.reshape(\n    #   cifar.train.images[10], (3, 32, 32)), [1, 2, 0]))\n    Xs, ys = cifar10_load()\n    if flatten:\n        Xs = Xs.reshape((Xs.shape[0], -1))\n    return Dataset(Xs, ys, split=split)\n\n\ndef CELEB(path=\'./img_align_celeba/\'):\n    """"""Attempt to load the files of the CELEB dataset.\n\n    Requires the files already be downloaded and placed in the `dst` directory.\n\n    http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n\n    Parameters\n    ----------\n    path : str, optional\n        Directory where the aligned/cropped celeb dataset can be found.\n\n    Returns\n    -------\n    files : list\n        List of file paths to the dataset.\n    """"""\n    if not os.path.exists(path):\n        print(\'Could not find celeb dataset under {}.\'.format(path))\n        print(\'Try downloading the dataset from the ""Aligned and Cropped"" \' +\n              \'link located here (imgs/img_align_celeba.zip [1.34 GB]): \' +\n              \'http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\')\n        return None\n    else:\n        fs = [os.path.join(path, f)\n              for f in os.listdir(path) if f.endswith(\'.jpg\')]\n        if len(fs) < 202598:\n            print(\'It does not look like you have downloaded the entire \' +\n                  \'Celeb Dataset.\\n\' +\n                  \'Try downloading the dataset from the ""Aligned and Cropped"" \' +\n                  \'link located here (imgs/img_align_celeba.zip [1.34 GB]): \' +\n                  \'http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\')\n        return fs\n'"
session-4/libs/deepdream.py,25,"b'""""""Deep Dream using the Inception v5 network.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom scipy.ndimage.filters import gaussian_filter\nfrom skimage.transform import resize\nfrom scipy.misc import imsave\nfrom . import inception, vgg16, i2v\nfrom . import gif\n\n\ndef get_labels(model=\'inception\'):\n    """"""Return labels corresponding to the `neuron_i` parameter of deep dream.\n\n    Parameters\n    ----------\n    model : str, optional\n        Which model to load. Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n\n    Raises\n    ------\n    ValueError\n        Unknown model.  Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n    """"""\n    if model == \'inception\':\n        net = inception.get_inception_model()\n        return net[\'labels\']\n    elif model == \'i2v_tag\':\n        net = i2v.get_i2v_tag_model()\n        return net[\'labels\']\n    elif model == \'vgg16\':\n        net = vgg16.get_vgg_model()\n        return net[\'labels\']\n    elif model == \'vgg_face\':\n        net = vgg16.get_vgg_face_model()\n        return net[\'labels\']\n    else:\n        raise ValueError(""Unknown model or this model does not have labels!"")\n\n\ndef get_layer_names(model=\'inception\'):\n    """"""Retun every layer\'s index and name in the given model.\n\n    Parameters\n    ----------\n    model : str, optional\n        Which model to load. Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n\n    Returns\n    -------\n    names : list of tuples\n        The index and layer\'s name for every layer in the given model.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        if model == \'inception\':\n            net = inception.get_inception_model()\n        elif model == \'vgg_face\':\n            net = vgg16.get_vgg_face_model()\n        elif model == \'vgg16\':\n            net = vgg16.get_vgg_model()\n        elif model == \'i2v\':\n            net = i2v.get_i2v_model()\n        elif model == \'i2v-tag\':\n            net = i2v.get_i2v_tag_model()\n\n        tf.import_graph_def(net[\'graph_def\'], name=\'net\')\n        names = [(i, op.name) for i, op in enumerate(g.get_operations())]\n        return names\n\n\ndef _setup(input_img, model, downsize):\n    """"""Internal use only. Load the given model\'s graph and preprocess an image.\n\n    Parameters\n    ----------\n    input_img : np.ndarray\n        Image to process with the model\'s normalizaiton process.\n    model : str\n        Which model to load. Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n    downsize : bool\n        Optionally crop/resize the input image to the standard shape.  Only\n        applies to inception network which is all convolutional.\n\n    Returns\n    -------\n    net, img, preprocess, deprocess : dict, np.ndarray, function, function\n        net : The networks graph_def and labels\n        img : The preprocessed input image\n        preprocess: Function for preprocessing an image\n        deprocess: Function for deprocessing an image\n\n    Raises\n    ------\n    ValueError\n        If model is unknown.\n    """"""\n    if model == \'inception\':\n        net = inception.get_inception_model()\n        img = inception.preprocess(input_img, resize=downsize, crop=downsize)[np.newaxis]\n        deprocess, preprocess = inception.deprocess, inception.preprocess\n    elif model == \'vgg_face\':\n        net = vgg16.get_vgg_face_model()\n        img = vgg16.preprocess(input_img)[np.newaxis]\n        deprocess, preprocess = vgg16.deprocess, vgg16.preprocess\n    elif model == \'vgg16\':\n        net = vgg16.get_vgg_model()\n        img = vgg16.preprocess(input_img)[np.newaxis]\n        deprocess, preprocess = vgg16.deprocess, vgg16.preprocess\n    elif model == \'i2v\':\n        net = i2v.get_i2v_model()\n        img = i2v.preprocess(input_img)[np.newaxis]\n        deprocess, preprocess = i2v.deprocess, i2v.preprocess\n    elif model == \'i2v_tag\':\n        net = i2v.get_i2v_tag_model()\n        img = i2v.preprocess(input_img)[np.newaxis]\n        deprocess, preprocess = i2v.deprocess, i2v.preprocess\n    else:\n        raise ValueError(\n            ""Unknown model name!  Supported: "" +\n            ""[\'inception\', \'vgg_face\', \'vgg16\', \'i2v\', \'i2v_tag\']"")\n\n    return net, img, preprocess, deprocess\n\n\ndef _apply(img,\n           gradient,\n           it_i,\n           decay=0.998,\n           sigma=1.5,\n           blur_step=10,\n           step=1.0,\n           crop=0,\n           crop_step=1,\n           pth=0):\n    """"""Interal use only. Apply the gradient to an image with the given params.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        Tensor to apply gradient ascent to.\n    gradient : np.ndarray\n        Gradient to ascend to.\n    it_i : int\n        Current iteration (used for step modulos)\n    decay : float, optional\n        Amount to decay.\n    sigma : float, optional\n        Sigma for Gaussian Kernel.\n    blur_step : int, optional\n        How often to blur.\n    step : float, optional\n        Step for gradient ascent.\n    crop : int, optional\n        Amount to crop from each border.\n    crop_step : int, optional\n        How often to crop.\n    pth : int, optional\n        Percentile to mask out.\n\n    Returns\n    -------\n    img : np.ndarray\n        Ascended image.\n    """"""\n    gradient /= (np.std(gradient) + 1e-10)\n    img += gradient * step\n    img *= decay\n\n    if pth:\n        mask = (np.abs(img) < np.percentile(np.abs(img), pth))\n        img = img - img * mask\n\n    if blur_step and it_i % blur_step == 0:\n        for ch_i in range(3):\n            img[..., ch_i] = gaussian_filter(img[..., ch_i], sigma)\n\n    if crop and it_i % crop_step == 0:\n        height, width, *ch = img[0].shape\n\n        # Crop a 1 pixel border from height and width\n        img = img[:, crop:-crop, crop:-crop, :]\n\n        # Resize\n        img = resize(img[0], (height, width), order=3,\n                     clip=False, preserve_range=True\n                     )[np.newaxis].astype(np.float32)\n\n\ndef deep_dream(input_img,\n               downsize=False,\n               model=\'inception\',\n               layer_i=-1,\n               neuron_i=-1,\n               n_iterations=100,\n               save_gif=None,\n               save_images=\'imgs\',\n               device=\'/cpu:0\',\n               **kwargs):\n    """"""Deep Dream with the given parameters.\n\n    Parameters\n    ----------\n    input_img : np.ndarray\n        Image to apply deep dream to.  Should be 3-dimenionsal H x W x C\n        RGB uint8 or float32.\n    downsize : bool, optional\n        Whether or not to downsize the image.  Only applies to\n        model==\'inception\'.\n    model : str, optional\n        Which model to load.  Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n    layer_i : int, optional\n        Which layer to use for finding the gradient.  E.g. the softmax layer\n        for inception is -1, for vgg networks it is -2.  Use the function\n        ""get_layer_names"" to find the layer number that you need.\n    neuron_i : int, optional\n        Which neuron to use.  -1 for the entire layer.\n    n_iterations : int, optional\n        Number of iterations to dream.\n    save_gif : bool, optional\n        Save a GIF.\n    save_images : str, optional\n        Folder to save images to.\n    device : str, optional\n        Which device to use, e.g. [\'/cpu:0\'] or \'/gpu:0\'.\n    **kwargs : dict\n        See ""_apply"" for additional parameters.\n\n    Returns\n    -------\n    imgs : list of np.array\n        Images of every iteration\n    """"""\n    net, img, preprocess, deprocess = _setup(input_img, model, downsize)\n    batch, height, width, *ch = img.shape\n\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess, g.device(device):\n\n        tf.import_graph_def(net[\'graph_def\'], name=\'net\')\n        names = [op.name for op in g.get_operations()]\n        input_name = names[0] + \':0\'\n        x = g.get_tensor_by_name(input_name)\n\n        layer = g.get_tensor_by_name(names[layer_i] + \':0\')\n        layer_shape = sess.run(tf.shape(layer), feed_dict={x: img})\n        layer_vec = np.ones(layer_shape) / layer_shape[-1]\n        layer_vec[..., neuron_i] = 1.0 - (1.0 / layer_shape[-1])\n\n        ascent = tf.gradients(layer, x)\n\n        imgs = []\n        for it_i in range(n_iterations):\n            print(it_i, np.min(img), np.max(img))\n            if neuron_i == -1:\n                this_res = sess.run(\n                    ascent, feed_dict={x: img})[0]\n            else:\n                this_res = sess.run(\n                    ascent, feed_dict={x: img, layer: layer_vec})[0]\n\n            _apply(img, this_res, it_i, **kwargs)\n            imgs.append(deprocess(img[0]))\n\n            if save_images is not None:\n                imsave(os.path.join(save_images,\n                                    \'frame{}.png\'.format(it_i)), imgs[-1])\n\n        if save_gif is not None:\n            gif.build_gif(imgs, saveto=save_gif)\n\n    return imgs\n\n\ndef guided_dream(input_img,\n                 guide_img=None,\n                 downsize=False,\n                 layers=[162, 183, 184, 247],\n                 label_i=962,\n                 layer_i=-1,\n                 feature_loss_weight=1.0,\n                 tv_loss_weight=1.0,\n                 l2_loss_weight=1.0,\n                 softmax_loss_weight=1.0,\n                 model=\'inception\',\n                 neuron_i=920,\n                 n_iterations=100,\n                 save_gif=None,\n                 save_images=\'imgs\',\n                 device=\'/cpu:0\',\n                 **kwargs):\n    """"""Deep Dream v2.  Use an optional guide image and other techniques.\n\n    Parameters\n    ----------\n    input_img : np.ndarray\n        Image to apply deep dream to.  Should be 3-dimenionsal H x W x C\n        RGB uint8 or float32.\n    guide_img : np.ndarray, optional\n        Optional image to find features at different layers for.  Must pass in\n        a list of layers that you want to find features for.  Then the guided\n        dream will try to match this images features at those layers.\n    downsize : bool, optional\n        Whether or not to downsize the image.  Only applies to\n        model==\'inception\'.\n    layers : list, optional\n        A list of layers to find features for in the ""guide_img"".\n    label_i : int, optional\n        Which label to use for the softmax layer.  Use the ""get_labels"" function\n        to find the index corresponding the object of interest.  If None, not\n        used.\n    layer_i : int, optional\n        Which layer to use for finding the gradient.  E.g. the softmax layer\n        for inception is -1, for vgg networks it is -2.  Use the function\n        ""get_layer_names"" to find the layer number that you need.\n    feature_loss_weight : float, optional\n        Weighting for the feature loss from the guide_img.\n    tv_loss_weight : float, optional\n        Total variational loss weighting.  Enforces smoothness.\n    l2_loss_weight : float, optional\n        L2 loss weighting.  Enforces smaller values and reduces saturation.\n    softmax_loss_weight : float, optional\n        Softmax loss weighting.  Must set label_i.\n    model : str, optional\n        Which model to load.  Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n    neuron_i : int, optional\n        Which neuron to use.  -1 for the entire layer.\n    n_iterations : int, optional\n        Number of iterations to dream.\n    save_gif : bool, optional\n        Save a GIF.\n    save_images : str, optional\n        Folder to save images to.\n    device : str, optional\n        Which device to use, e.g. [\'/cpu:0\'] or \'/gpu:0\'.\n    **kwargs : dict\n        See ""_apply"" for additional parameters.\n\n    Returns\n    -------\n    imgs : list of np.ndarray\n        Images of the dream.\n    """"""\n    net, img, preprocess, deprocess = _setup(input_img, model, downsize)\n    print(img.shape, input_img.shape)\n    print(img.min(), img.max())\n\n    if guide_img is not None:\n        guide_img = preprocess(guide_img.copy(), model)[np.newaxis]\n        assert(guide_img.shape == img.shape)\n    batch, height, width, *ch = img.shape\n\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess, g.device(device):\n        tf.import_graph_def(net[\'graph_def\'], name=\'net\')\n        names = [op.name for op in g.get_operations()]\n        input_name = names[0] + \':0\'\n        x = g.get_tensor_by_name(input_name)\n\n        features = [names[layer_i] + \':0\' for layer_i in layers]\n        feature_loss = tf.Variable(0.0)\n        for feature_i in features:\n            layer = g.get_tensor_by_name(feature_i)\n            if guide_img is None:\n                feature_loss += tf.reduce_mean(layer)\n            else:\n                # Reshape it to 2D vector\n                layer = tf.reshape(layer, [-1, 1])\n                # Do the same for our guide image\n                guide_layer = sess.run(layer, feed_dict={x: guide_img})\n                guide_layer = guide_layer.reshape(-1, 1)\n                # Now calculate their dot product\n                correlation = tf.matmul(guide_layer.T, layer)\n                feature_loss += feature_loss_weight * tf.reduce_mean(correlation)\n        softmax_loss = tf.Variable(0.0)\n        if label_i is not None:\n            layer = g.get_tensor_by_name(names[layer_i] + \':0\')\n            layer_shape = sess.run(tf.shape(layer), feed_dict={x: img})\n            layer_vec = np.ones(layer_shape) / layer_shape[-1]\n            layer_vec[..., neuron_i] = 1.0 - 1.0 / layer_shape[1]\n            softmax_loss += softmax_loss_weight * tf.reduce_mean(tf.nn.l2_loss(layer - layer_vec))\n\n        dx = tf.square(x[:, :height - 1, :width - 1, :] - x[:, :height - 1, 1:, :])\n        dy = tf.square(x[:, :height - 1, :width - 1, :] - x[:, 1:, :width - 1, :])\n        tv_loss = tv_loss_weight * tf.reduce_mean(tf.pow(dx + dy, 1.2))\n        l2_loss = l2_loss_weight * tf.reduce_mean(tf.nn.l2_loss(x))\n\n        ascent = tf.gradients(feature_loss + softmax_loss + tv_loss + l2_loss, x)[0]\n        sess.run(tf.global_variables_initializer())\n        imgs = []\n        for it_i in range(n_iterations):\n            this_res, this_feature_loss, this_softmax_loss, this_tv_loss, this_l2_loss = sess.run(\n                [ascent, feature_loss, softmax_loss, tv_loss, l2_loss], feed_dict={x: img})\n            print(\'feature:\', this_feature_loss,\n                  \'softmax:\', this_softmax_loss,\n                  \'tv\', this_tv_loss,\n                  \'l2\', this_l2_loss)\n\n            _apply(img, -this_res, it_i, **kwargs)\n            imgs.append(deprocess(img[0]))\n\n            if save_images is not None:\n                imsave(os.path.join(save_images,\n                                    \'frame{}.png\'.format(it_i)), imgs[-1])\n\n        if save_gif is not None:\n            gif.build_gif(imgs, saveto=save_gif)\n\n    return imgs\n'"
session-4/libs/dft.py,0,"b'""""""Summary.\n\n#CADL\nCopyright Parag K. Mital 2016\n""""""\nimport numpy as np\nfrom scipy.signal import hann\n\n\ndef ztoc(re, im):\n    return np.sqrt(re**2 + im**2), np.angle(re + im * 1j)\n\n\ndef ctoz(mag, phs):\n    return mag * np.cos(phs), mag * np.sin(phs)\n\n\ndef dft_np(signal, hop_size=256, fft_size=512):\n    n_hops = len(signal) // hop_size\n    s = []\n    hann_win = hann(fft_size)\n    for hop_i in range(n_hops):\n        frame = signal[(hop_i * hop_size):(hop_i * hop_size + fft_size)]\n        frame = np.pad(frame, (0, fft_size - len(frame)), \'constant\')\n        frame *= hann_win\n        s.append(frame)\n    s = np.array(s)\n    N = s.shape[-1]\n    k = np.reshape(np.linspace(0.0, 2 * np.pi / N * (N // 2), N // 2), [1, N // 2])\n    x = np.reshape(np.linspace(0.0, N - 1, N), [N, 1])\n    freqs = np.dot(x, k)\n    reals = np.dot(s, np.cos(freqs)) * (2.0 / N)\n    imags = np.dot(s, np.sin(freqs)) * (2.0 / N)\n    return reals, imags\n\n\ndef idft_np(re, im, hop_size=256, fft_size=512):\n    N = re.shape[1] * 2\n    k = np.reshape(np.linspace(0.0, 2 * np.pi / N * (N // 2), N // 2), [N // 2, 1])\n    x = np.reshape(np.linspace(0.0, N - 1, N), [1, N])\n    freqs = np.dot(k, x)\n    signal = np.zeros((re.shape[0] * hop_size + fft_size,))\n    recon = np.dot(re, np.cos(freqs)) + np.dot(im, np.sin(freqs))\n    for hop_i, frame in enumerate(recon):\n        signal[(hop_i * hop_size): (hop_i * hop_size + fft_size)] += frame\n    return signal\n'"
session-4/libs/gif.py,0,"b'""""""Utility for creating a GIF.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n\ndef build_gif(imgs, interval=0.1, dpi=72,\n              save_gif=True, saveto=\'animation.gif\',\n              show_gif=False, cmap=None):\n    """"""Take an array or list of images and create a GIF.\n\n    Parameters\n    ----------\n    imgs : np.ndarray or list\n        List of images to create a GIF of\n    interval : float, optional\n        Spacing in seconds between successive images.\n    dpi : int, optional\n        Dots per inch.\n    save_gif : bool, optional\n        Whether or not to save the GIF.\n    saveto : str, optional\n        Filename of GIF to save.\n    show_gif : bool, optional\n        Whether or not to render the GIF using plt.\n    cmap : None, optional\n        Optional colormap to apply to the images.\n\n    Returns\n    -------\n    ani : matplotlib.animation.ArtistAnimation\n        The artist animation from matplotlib.  Likely not useful.\n    """"""\n    imgs = np.asarray(imgs)\n    h, w, *c = imgs[0].shape\n    fig, ax = plt.subplots(figsize=(np.round(w / dpi), np.round(h / dpi)))\n    fig.subplots_adjust(bottom=0)\n    fig.subplots_adjust(top=1)\n    fig.subplots_adjust(right=1)\n    fig.subplots_adjust(left=0)\n    ax.set_axis_off()\n\n    if cmap is not None:\n        axs = list(map(lambda x: [\n            ax.imshow(x, cmap=cmap)], imgs))\n    else:\n        axs = list(map(lambda x: [\n            ax.imshow(x)], imgs))\n\n    ani = animation.ArtistAnimation(\n        fig, axs, interval=interval*1000, repeat_delay=0, blit=True)\n\n    if save_gif:\n        ani.save(saveto, writer=\'imagemagick\', dpi=dpi)\n\n    if show_gif:\n        plt.show()\n\n    return ani\n'"
session-4/libs/i2v.py,10,"b'""""""\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport json\nimport numpy as np\nfrom tensorflow.python.platform import gfile\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize as imresize\nfrom .utils import download\n\n\ndef i2v_download():\n    """"""Download a pretrained i2v network.""""""\n    model = download(\'https://s3.amazonaws.com/cadl/models/illust2vec.tfmodel\')\n    return model\n\n\ndef i2v_tag_download():\n    """"""Download a pretrained i2v network.""""""\n    model = download(\'https://s3.amazonaws.com/cadl/models/illust2vec_tag.tfmodel\')\n    tags = download(\'https://s3.amazonaws.com/cadl/models/tag_list.json\')\n    return model, tags\n\n\ndef get_i2v_model():\n    """"""Get a pretrained i2v network.\n\n    Returns\n    -------\n    net : dict\n        {\'graph_def\': graph_def, \'labels\': synsets}\n        where the graph_def is a tf.GraphDef and the synsets\n        map an integer label from 0-1000 to a list of names\n    """"""\n    # Download the trained net\n    model = i2v_download()\n\n    # Load the saved graph\n    with gfile.GFile(model, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n\n    return {\'graph_def\': graph_def}\n\n\ndef get_i2v_tag_model():\n    """"""Get a pretrained i2v tag network.\n\n    Returns\n    -------\n    net : dict\n        {\'graph_def\': graph_def, \'labels\': synsets}\n        where the graph_def is a tf.GraphDef and the synsets\n        map an integer label from 0-1000 to a list of names\n    """"""\n    # Download the trained net\n    model, tags = i2v_tag_download()\n    tags = json.load(open(tags, \'r\'))\n\n    # Load the saved graph\n    with gfile.GFile(model, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n\n    return {\n        \'graph_def\': graph_def,\n        \'labels\': tags,\n        \'preprocess\': preprocess,\n        \'deprocess\': deprocess\n    }\n\n\ndef preprocess(img, crop=True, resize=True, dsize=(224, 224)):\n    mean_img = np.array([164.76139251, 167.47864617, 181.13838569])\n    if img.dtype == np.uint8:\n        img = (img[..., ::-1] - mean_img).astype(np.float32)\n    else:\n        img = img[..., ::-1] * 255.0 - mean_img\n\n    if crop:\n        short_edge = min(img.shape[:2])\n        yy = int((img.shape[0] - short_edge) / 2)\n        xx = int((img.shape[1] - short_edge) / 2)\n        crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n    else:\n        crop_img = img\n    if resize:\n        norm_img = imresize(crop_img, dsize, preserve_range=True)\n    else:\n        norm_img = crop_img\n    return (norm_img).astype(np.float32)\n\n\ndef deprocess(img):\n    mean_img = np.array([164.76139251, 167.47864617, 181.13838569])\n    processed = (img + mean_img)[..., ::-1]\n    return np.clip(processed, 0, 255).astype(np.uint8)\n    # return ((img / np.max(np.abs(img))) * 127.5 +\n    #         127.5).astype(np.uint8)\n\n\ndef test_i2v():\n    """"""Loads the i2v network and applies it to a test image.\n    """"""\n    with tf.Session() as sess:\n        net = get_i2v_model()\n        tf.import_graph_def(net[\'graph_def\'], name=\'i2v\')\n        g = tf.get_default_graph()\n        names = [op.name for op in g.get_operations()]\n        x = g.get_tensor_by_name(names[0] + \':0\')\n        softmax = g.get_tensor_by_name(names[-3] + \':0\')\n\n        from skimage import data\n        img = preprocess(data.coffee())[np.newaxis]\n        res = np.squeeze(softmax.eval(feed_dict={x: img}))\n        print([(res[idx], net[\'labels\'][idx])\n               for idx in res.argsort()[-5:][::-1]])\n\n        """"""Let\'s visualize the network\'s gradient activation\n        when backpropagated to the original input image.  This\n        is effectively telling us which pixels contribute to the\n        predicted class or given neuron""""""\n        pools = [name for name in names if \'pool\' in name.split(\'/\')[-1]]\n        fig, axs = plt.subplots(1, len(pools))\n        for pool_i, poolname in enumerate(pools):\n            pool = g.get_tensor_by_name(poolname + \':0\')\n            pool.get_shape()\n            neuron = tf.reduce_max(pool, 1)\n            saliency = tf.gradients(neuron, x)\n            neuron_idx = tf.arg_max(pool, 1)\n            this_res = sess.run([saliency[0], neuron_idx],\n                                feed_dict={x: img})\n\n            grad = this_res[0][0] / np.max(np.abs(this_res[0]))\n            axs[pool_i].imshow((grad * 128 + 128).astype(np.uint8))\n            axs[pool_i].set_title(poolname)\n'"
session-4/libs/inception.py,8,"b'""""""\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport os\nimport numpy as np\nfrom tensorflow.python.platform import gfile\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize as imresize\nfrom .utils import download_and_extract_tar, download_and_extract_zip\n\n\ndef inception_download(data_dir=\'inception\', version=\'v5\'):\n    """"""Download a pretrained inception network.\n\n    Parameters\n    ----------\n    data_dir : str, optional\n        Location of the pretrained inception network download.\n    version : str, optional\n        Version of the model: [\'v3\'] or \'v5\'.\n    """"""\n    if version == \'v3\':\n        download_and_extract_tar(\n            \'https://s3.amazonaws.com/cadl/models/inception-2015-12-05.tgz\',\n            data_dir)\n        return (os.path.join(data_dir, \'classify_image_graph_def.pb\'),\n                os.path.join(data_dir, \'imagenet_synset_to_human_label_map.txt\'))\n    else:\n        download_and_extract_zip(\n            \'https://s3.amazonaws.com/cadl/models/inception5h.zip\', data_dir)\n        return (os.path.join(data_dir, \'tensorflow_inception_graph.pb\'),\n                os.path.join(data_dir, \'imagenet_comp_graph_label_strings.txt\'))\n\n\ndef get_inception_model(data_dir=\'inception\', version=\'v5\'):\n    """"""Get a pretrained inception network.\n\n    Parameters\n    ----------\n    data_dir : str, optional\n        Location of the pretrained inception network download.\n    version : str, optional\n        Version of the model: [\'v3\'] or \'v5\'.\n\n    Returns\n    -------\n    net : dict\n        {\'graph_def\': graph_def, \'labels\': synsets}\n        where the graph_def is a tf.GraphDef and the synsets\n        map an integer label from 0-1000 to a list of names\n    """"""\n    # Download the trained net\n    model, labels = inception_download(data_dir, version)\n\n    # Parse the ids and synsets\n    txt = open(labels).readlines()\n    synsets = [(key, val.strip()) for key, val in enumerate(txt)]\n\n    # Load the saved graph\n    with gfile.GFile(model, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n    return {\n        \'graph_def\': graph_def,\n        \'labels\': synsets,\n        \'preprocess\': preprocess,\n        \'deprocess\': deprocess\n    }\n\n\ndef preprocess(img, crop=True, resize=True, dsize=(299, 299)):\n    if img.dtype != np.uint8:\n        img *= 255.0\n\n    if crop:\n        crop = np.min(img.shape[:2])\n        r = (img.shape[0] - crop) // 2\n        c = (img.shape[1] - crop) // 2\n        cropped = img[r: r + crop, c: c + crop]\n    else:\n        cropped = img\n\n    if resize:\n        rsz = imresize(cropped, dsize, preserve_range=True)\n    else:\n        rsz = cropped\n\n    if rsz.ndim == 2:\n        rsz = rsz[..., np.newaxis]\n\n    rsz = rsz.astype(np.float32)\n    # subtract imagenet mean\n    return (rsz - 117)\n\n\ndef deprocess(img):\n    return np.clip(img + 117, 0, 255).astype(np.uint8)\n\n\ndef test_inception():\n    """"""Loads the inception network and applies it to a test image.\n    """"""\n    with tf.Session() as sess:\n        net = get_inception_model()\n        tf.import_graph_def(net[\'graph_def\'], name=\'inception\')\n        g = tf.get_default_graph()\n        names = [op.name for op in g.get_operations()]\n        x = g.get_tensor_by_name(names[0] + \':0\')\n        softmax = g.get_tensor_by_name(names[-3] + \':0\')\n\n        from skimage import data\n        img = preprocess(data.coffee())[np.newaxis]\n        res = np.squeeze(softmax.eval(feed_dict={x: img}))\n        print([(res[idx], net[\'labels\'][idx])\n               for idx in res.argsort()[-5:][::-1]])\n\n        """"""Let\'s visualize the network\'s gradient activation\n        when backpropagated to the original input image.  This\n        is effectively telling us which pixels contribute to the\n        predicted class or given neuron""""""\n        pools = [name for name in names if \'pool\' in name.split(\'/\')[-1]]\n        fig, axs = plt.subplots(1, len(pools))\n        for pool_i, poolname in enumerate(pools):\n            pool = g.get_tensor_by_name(poolname + \':0\')\n            pool.get_shape()\n            neuron = tf.reduce_max(pool, 1)\n            saliency = tf.gradients(neuron, x)\n            neuron_idx = tf.arg_max(pool, 1)\n            this_res = sess.run([saliency[0], neuron_idx],\n                                feed_dict={x: img})\n\n            grad = this_res[0][0] / np.max(np.abs(this_res[0]))\n            axs[pool_i].imshow((grad * 128 + 128).astype(np.uint8))\n            axs[pool_i].set_title(poolname)\n'"
session-4/libs/nb_utils.py,2,"b'""""""Utility for displaying Tensorflow graphs from:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb\n""""""\nimport tensorflow as tf\nimport numpy as np\nfrom IPython.display import display, HTML\n\n\ndef show_graph(graph_def):\n    # Helper functions for TF Graph visualization\n    def _strip_consts(graph_def, max_const_size=32):\n        """"""Strip large constant values from graph_def.""""""\n        strip_def = tf.GraphDef()\n        for n0 in graph_def.node:\n            n = strip_def.node.add()\n            n.MergeFrom(n0)\n            if n.op == \'Const\':\n                tensor = n.attr[\'value\'].tensor\n                size = len(tensor.tensor_content)\n                if size > max_const_size:\n                    tensor.tensor_content = ""<stripped {} bytes>"".format(size).encode()\n        return strip_def\n\n    def _rename_nodes(graph_def, rename_func):\n        res_def = tf.GraphDef()\n        for n0 in graph_def.node:\n            n = res_def.node.add()\n            n.MergeFrom(n0)\n            n.name = rename_func(n.name)\n            for i, s in enumerate(n.input):\n                n.input[i] = rename_func(s) if s[0] != \'^\' else \'^\' + rename_func(s[1:])\n        return res_def\n\n    def _show_entire_graph(graph_def, max_const_size=32):\n        """"""Visualize TensorFlow graph.""""""\n        if hasattr(graph_def, \'as_graph_def\'):\n            graph_def = graph_def.as_graph_def()\n        strip_def = _strip_consts(graph_def, max_const_size=max_const_size)\n        code = """"""\n            <script>\n              function load() {{\n                document.getElementById(""{id}"").pbtxt = {data};\n              }}\n            </script>\n            <link rel=""import"" href=""https://tensorboard.appspot.com/tf-graph-basic.build.html"" onload=load()>\n            <div style=""height:600px"">\n              <tf-graph-basic id=""{id}""></tf-graph-basic>\n            </div>\n        """""".format(data=repr(str(strip_def)), id=\'graph\' + str(np.random.rand()))\n\n        iframe = """"""\n            <iframe seamless style=""width:800px;height:620px;border:0"" srcdoc=""{}""></iframe>\n        """""".format(code.replace(\'""\', \'&quot;\'))\n        display(HTML(iframe))\n    # Visualizing the network graph. Be sure expand the ""mixed"" nodes to see their\n    # internal structure. We are going to visualize ""Conv2D"" nodes.\n    tmp_def = _rename_nodes(graph_def, lambda s: ""/"".join(s.split(\'_\', 1)))\n    _show_entire_graph(tmp_def)\n'"
session-4/libs/stylenet.py,15,"b'""""""Style Net w/ tests for Video Style Net.\n\nVideo Style Net requires OpenCV 3.0.0+ w/ Contrib for Python to be installed.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nfrom __future__ import print_function\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom . import vgg16\nfrom . import gif\n\n\ndef make_4d(img):\n    """"""Create a 4-dimensional N x H x W x C image.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        Given image as H x W x C or H x W.\n\n    Returns\n    -------\n    img : np.ndarray\n        N x H x W x C image.\n\n    Raises\n    ------\n    ValueError\n        Unexpected number of dimensions.\n    """"""\n    if img.ndim == 2:\n        img = np.expand_dims(img[np.newaxis], 3)\n    elif img.ndim == 3:\n        img = img[np.newaxis]\n    elif img.ndim == 4:\n        return img\n    else:\n        raise ValueError(\'Incorrect dimensions for image!\')\n    return img\n\n\ndef stylize(content_img, style_img, base_img=None, saveto=None, gif_step=5,\n            n_iterations=100, style_weight=1.0, content_weight=1.0):\n    """"""Stylization w/ the given content and style images.\n\n    Follows the approach in Leon Gatys et al.\n\n    Parameters\n    ----------\n    content_img : np.ndarray\n        Image to use for finding the content features.\n    style_img : TYPE\n        Image to use for finding the style features.\n    base_img : None, optional\n        Image to use for the base content.  Can be noise or an existing image.\n        If None, the content image will be used.\n    saveto : str, optional\n        Name of GIF image to write to, e.g. ""stylization.gif""\n    gif_step : int, optional\n        Modulo of iterations to save the current stylization.\n    n_iterations : int, optional\n        Number of iterations to run for.\n    style_weight : float, optional\n        Weighting on the style features.\n    content_weight : float, optional\n        Weighting on the content features.\n\n    Returns\n    -------\n    stylization : np.ndarray\n        Final iteration of the stylization.\n    """"""\n    # Preprocess both content and style images\n    content_img = vgg16.preprocess(content_img, dsize=(224, 224))[np.newaxis]\n    style_img = vgg16.preprocess(style_img, dsize=(224, 224))[np.newaxis]\n    if base_img is None:\n        base_img = content_img\n    else:\n        base_img = make_4d(vgg16.preprocess(base_img, dsize=(224, 224)))\n\n    # Get Content and Style features\n    net = vgg16.get_vgg_model()\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        tf.import_graph_def(net[\'graph_def\'], name=\'vgg\')\n        names = [op.name for op in g.get_operations()]\n        x = g.get_tensor_by_name(names[0] + \':0\')\n        content_layer = \'vgg/conv3_2/conv3_2:0\'\n        content_features = g.get_tensor_by_name(\n            content_layer).eval(feed_dict={\n                x: content_img,\n                \'vgg/dropout_1/random_uniform:0\': [[1.0] * 4096],\n                \'vgg/dropout/random_uniform:0\': [[1.0] * 4096]})\n        style_layers = [\'vgg/conv1_1/conv1_1:0\',\n                        \'vgg/conv2_1/conv2_1:0\',\n                        \'vgg/conv3_1/conv3_1:0\',\n                        \'vgg/conv4_1/conv4_1:0\',\n                        \'vgg/conv5_1/conv5_1:0\']\n        style_activations = []\n        for style_i in style_layers:\n            style_activation_i = g.get_tensor_by_name(style_i).eval(\n                feed_dict={\n                    x: style_img,\n                    \'vgg/dropout_1/random_uniform:0\': [[1.0] * 4096],\n                    \'vgg/dropout/random_uniform:0\': [[1.0] * 4096]})\n            style_activations.append(style_activation_i)\n        style_features = []\n        for style_activation_i in style_activations:\n            s_i = np.reshape(style_activation_i,\n                             [-1, style_activation_i.shape[-1]])\n            gram_matrix = np.matmul(s_i.T, s_i) / s_i.size\n            style_features.append(gram_matrix.astype(np.float32))\n\n    # Optimize both\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        net_input = tf.Variable(base_img)\n        tf.import_graph_def(\n            net[\'graph_def\'],\n            name=\'vgg\',\n            input_map={\'images:0\': net_input})\n\n        content_loss = tf.nn.l2_loss((g.get_tensor_by_name(content_layer) -\n                                      content_features) /\n                                     content_features.size)\n        style_loss = np.float32(0.0)\n        for style_layer_i, style_gram_i in zip(style_layers, style_features):\n            layer_i = g.get_tensor_by_name(style_layer_i)\n            layer_shape = layer_i.get_shape().as_list()\n            layer_size = layer_shape[1] * layer_shape[2] * layer_shape[3]\n            layer_flat = tf.reshape(layer_i, [-1, layer_shape[3]])\n            gram_matrix = tf.matmul(\n                tf.transpose(layer_flat), layer_flat) / layer_size\n            style_loss = tf.add(\n                style_loss, tf.nn.l2_loss(\n                    (gram_matrix - style_gram_i) /\n                    np.float32(style_gram_i.size)))\n        loss = content_weight * content_loss + style_weight * style_loss\n        optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n\n        sess.run(tf.global_variables_initializer())\n        imgs = []\n        for it_i in range(n_iterations):\n            _, this_loss, synth = sess.run(\n                [optimizer, loss, net_input],\n                feed_dict={\n                    \'vgg/dropout_1/random_uniform:0\': np.ones(\n                        g.get_tensor_by_name(\n                            \'vgg/dropout_1/random_uniform:0\'\n                        ).get_shape().as_list()),\n                    \'vgg/dropout/random_uniform:0\': np.ones(\n                        g.get_tensor_by_name(\n                            \'vgg/dropout/random_uniform:0\'\n                        ).get_shape().as_list())\n                })\n            print(""iteration %d, loss: %f, range: (%f - %f)"" %\n                  (it_i, this_loss, np.min(synth), np.max(synth)), end=\'\\r\')\n            if it_i % gif_step == 0:\n                imgs.append(np.clip(synth[0], 0, 1))\n        if saveto is not None:\n            gif.build_gif(imgs, saveto=saveto)\n    return np.clip(synth[0], 0, 1)\n\n\ndef warp_img(img, dx, dy):\n    """"""Apply the motion vectors to the given image.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        Input image to apply motion to.\n    dx : np.ndarray\n        H x W matrix defining the magnitude of the X vector\n    dy : np.ndarray\n        H x W matrix defining the magnitude of the Y vector\n\n    Returns\n    -------\n    img : np.ndarray\n        Image with pixels warped according to dx, dy.\n    """"""\n    warped = img.copy()\n    for row_i in range(img.shape[0]):\n        for col_i in range(img.shape[1]):\n            dx_i = int(np.round(dx[row_i, col_i]))\n            dy_i = int(np.round(dy[row_i, col_i]))\n            sample_dx = np.clip(dx_i + col_i, 0, img.shape[0] - 1)\n            sample_dy = np.clip(dy_i + row_i, 0, img.shape[1] - 1)\n            warped[sample_dy, sample_dx, :] = img[row_i, col_i, :]\n    return warped\n\n\ndef test_video(style_img=\'arles.png\', videodir=\'kurosawa\',\n               max_files=3, rsz=224):\n    r""""""Test for artistic stylization using video.\n\n    This requires the python installation of OpenCV for the Deep Flow algorithm.\n    If cv2 is not found, then there will be reduced ""temporal coherence"".\n\n    Unfortunately, installing opencv for python3 is not the easiest thing to do.\n    OSX users can install this using:\n\n    $ brew install opencv --with-python3 --with-contrib\n\n    then will have to symlink the libraries.  I think you can do this w/:\n\n    $ brew link --force opencv3\n\n    But the problems start to arise depending on which python you have\n    installed, and it is always a mess w/ homebrew.  Sorry!\n\n    Your best bet is installing from source.  Something along\n    these lines should get you there:\n\n    $ cd ~\n    $ git clone https://github.com/Itseez/opencv.git\n    $ cd opencv\n    $ git checkout 3.1.0\n    $ cd ~\n    $ git clone https://github.com/Itseez/opencv_contrib.git\n    $ cd opencv_contrib\n    $ git checkout 3.1.0\n    $ cd ~/opencv\n    $ mkdir build\n    $ cd build\n    $ cmake -D CMAKE_BUILD_TYPE=RELEASE \\\n        -D CMAKE_INSTALL_PREFIX=/usr/local \\\n        -D INSTALL_C_EXAMPLES=OFF \\\n        -D INSTALL_PYTHON_EXAMPLES=OFF \\\n        -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib/modules \\\n        -D BUILD_EXAMPLES=OFF ..\n\n    Parameters\n    ----------\n    style_img : str, optional\n        Location to style image\n    videodir : str, optional\n        Location to directory containing images of each frame to stylize.\n\n    Returns\n    -------\n    imgs : list of np.ndarray\n        Stylized images for each frame.\n    """"""\n    has_cv2 = True\n    try:\n        import cv2\n        has_cv2 = True\n        optflow = cv2.optflow.createOptFlow_DeepFlow()\n    except ImportError:\n        has_cv2 = False\n\n    style_img = vgg16.preprocess(plt.imread(style_img)[..., :3])\n    content_files = [os.path.join(videodir, f)\n                     for f in os.listdir(videodir) if f.endswith(\'.png\')]\n    content_img = plt.imread(content_files[0])[..., :3]\n    from scipy.misc import imresize\n    style_img = imresize(style_img, (rsz, rsz)).astype(np.float32) / 255.0\n    content_img = imresize(content_img, (rsz, rsz)).astype(np.float32) / 255.0\n    if has_cv2:\n        prev_lum = cv2.cvtColor(content_img, cv2.COLOR_RGB2HSV)[:, :, 2]\n    else:\n        prev_lum = (content_img[..., 0] * 0.3 +\n                    content_img[..., 1] * 0.59 +\n                    content_img[..., 2] * 0.11)\n    imgs = []\n    stylized = stylize(content_img, style_img, content_weight=5.0,\n                       style_weight=0.5, n_iterations=50)\n    plt.imsave(fname=content_files[0] + \'stylized.png\', arr=stylized)\n    imgs.append(stylized)\n    for f in content_files[1:max_files]:\n        content_img = plt.imread(f)[..., :3]\n        content_img = imresize(content_img, (rsz, rsz)).astype(np.float32) / 255.0\n        if has_cv2:\n            lum = cv2.cvtColor(content_img, cv2.COLOR_RGB2HSV)[:, :, 2]\n            flow = optflow.calc(prev_lum, lum, None)\n            warped = warp_img(stylized, flow[..., 0], flow[..., 1])\n            stylized = stylize(content_img, style_img, content_weight=5.0,\n                               style_weight=0.5, base_img=warped, n_iterations=50)\n        else:\n            lum = (content_img[..., 0] * 0.3 +\n                   content_img[..., 1] * 0.59 +\n                   content_img[..., 2] * 0.11)\n            stylized = stylize(content_img, style_img, content_weight=5.0,\n                               style_weight=0.5, base_img=None, n_iterations=50)\n        imgs.append(stylized)\n        plt.imsave(fname=f + \'stylized.png\', arr=stylized)\n        prev_lum = lum\n    return imgs\n\n\ndef test():\n    """"""Test for artistic stylization.""""""\n    from six.moves import urllib\n    f = (\'https://upload.wikimedia.org/wikipedia/commons/thumb/5/54/\' +\n         \'Claude_Monet%2C_Impression%2C_soleil_levant.jpg/617px-Claude_Monet\' +\n         \'%2C_Impression%2C_soleil_levant.jpg?download\')\n    filepath, _ = urllib.request.urlretrieve(f, f.split(\'/\')[-1], None)\n    style = plt.imread(filepath).astype(np.float32) / 255.0\n\n    f = (\'https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/\' +\n         \'El_jard%C3%ADn_de_las_Delicias%2C_de_El_Bosco.jpg/640px-El_jard\' +\n         \'%C3%ADn_de_las_Delicias%2C_de_El_Bosco.jpg\')\n    filepath, _ = urllib.request.urlretrieve(f, f.split(\'/\')[-1], None)\n    content = plt.imread(filepath).astype(np.float32) / 255.0\n\n    stylize(content, style, n_iterations=20)\n\n\nif __name__ == \'__main__\':\n    test_video()\n'"
session-4/libs/utils.py,74,"b'""""""Utilities used in the Kadenze Academy Course on Deep Learning w/ Tensorflow.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nParag K. Mital\n\nCopyright Parag K. Mital, June 2016.\n""""""\nfrom __future__ import print_function\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport urllib\nimport numpy as np\nimport zipfile\nimport os\nfrom scipy.io import wavfile\nfrom scipy.misc import imsave\n\ndef download(path):\n    """"""Use urllib to download a file.\n\n    Parameters\n    ----------\n    path : str\n        Url to download\n\n    Returns\n    -------\n    path : str\n        Location of downloaded file.\n    """"""\n    import os\n    from six.moves import urllib\n\n    fname = path.split(\'/\')[-1]\n    if os.path.exists(fname):\n        return fname\n\n    print(\'Downloading \' + path)\n\n    def progress(count, block_size, total_size):\n        if count % 20 == 0:\n            print(\'Downloaded %02.02f/%02.02f MB\' % (\n                count * block_size / 1024.0 / 1024.0,\n                total_size / 1024.0 / 1024.0), end=\'\\r\')\n\n    filepath, _ = urllib.request.urlretrieve(\n        path, filename=fname, reporthook=progress)\n    return filepath\n\n\ndef download_and_extract_tar(path, dst):\n    """"""Download and extract a tar file.\n\n    Parameters\n    ----------\n    path : str\n        Url to tar file to download.\n    dst : str\n        Location to save tar file contents.\n    """"""\n    import tarfile\n    filepath = download(path)\n    if not os.path.exists(dst):\n        os.makedirs(dst)\n        tarfile.open(filepath, \'r:gz\').extractall(dst)\n\n\ndef download_and_extract_zip(path, dst):\n    """"""Download and extract a zip file.\n\n    Parameters\n    ----------\n    path : str\n        Url to zip file to download.\n    dst : str\n        Location to save zip file contents.\n    """"""\n    import zipfile\n    filepath = download(path)\n    if not os.path.exists(dst):\n        os.makedirs(dst)\n        zf = zipfile.ZipFile(file=filepath)\n        zf.extractall(dst)\n\n\ndef load_audio(filename, b_normalize=True):\n    """"""Load the audiofile at the provided filename using scipy.io.wavfile.\n\n    Optionally normalizes the audio to the maximum value.\n\n    Parameters\n    ----------\n    filename : str\n        File to load.\n    b_normalize : bool, optional\n        Normalize to the maximum value.\n    """"""\n    sr, s = wavfile.read(filename)\n    if b_normalize:\n        s = s.astype(np.float32)\n        s = (s / np.max(np.abs(s)))\n        s -= np.mean(s)\n    return s\n\n\ndef corrupt(x):\n    """"""Take an input tensor and add uniform masking.\n\n    Parameters\n    ----------\n    x : Tensor/Placeholder\n        Input to corrupt.\n    Returns\n    -------\n    x_corrupted : Tensor\n        50 pct of values corrupted.\n    """"""\n    return tf.multiply(x, tf.cast(tf.random_uniform(shape=tf.shape(x),\n                                               minval=0,\n                                               maxval=2,\n                                               dtype=tf.int32), tf.float32))\n\n\ndef interp(l, r, n_samples):\n    """"""Intepolate between the arrays l and r, n_samples times.\n\n    Parameters\n    ----------\n    l : np.ndarray\n        Left edge\n    r : np.ndarray\n        Right edge\n    n_samples : int\n        Number of samples\n\n    Returns\n    -------\n    arr : np.ndarray\n        Inteporalted array\n    """"""\n    return np.array([\n        l + step_i / (n_samples - 1) * (r - l)\n        for step_i in range(n_samples)])\n\n\ndef make_latent_manifold(corners, n_samples):\n    """"""Create a 2d manifold out of the provided corners: n_samples * n_samples.\n\n    Parameters\n    ----------\n    corners : list of np.ndarray\n        The four corners to intepolate.\n    n_samples : int\n        Number of samples to use in interpolation.\n\n    Returns\n    -------\n    arr : np.ndarray\n        Stacked array of all 2D interpolated samples\n    """"""\n    left = interp(corners[0], corners[1], n_samples)\n    right = interp(corners[2], corners[3], n_samples)\n\n    embedding = []\n    for row_i in range(n_samples):\n        embedding.append(interp(left[row_i], right[row_i], n_samples))\n    return np.vstack(embedding)\n\n\ndef imcrop_tosquare(img):\n    """"""Make any image a square image.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        Input image to crop, assumed at least 2d.\n\n    Returns\n    -------\n    crop : np.ndarray\n        Cropped image.\n    """"""\n    size = np.min(img.shape[:2])\n    extra = img.shape[:2] - size\n    crop = img\n    for i in np.flatnonzero(extra):\n        crop = np.take(crop, extra[i] // 2 + np.r_[:size], axis=i)\n    return crop\n\n\ndef slice_montage(montage, img_h, img_w, n_imgs):\n    """"""Slice a montage image into n_img h x w images.\n\n    Performs the opposite of the montage function.  Takes a montage image and\n    slices it back into a N x H x W x C image.\n\n    Parameters\n    ----------\n    montage : np.ndarray\n        Montage image to slice.\n    img_h : int\n        Height of sliced image\n    img_w : int\n        Width of sliced image\n    n_imgs : int\n        Number of images to slice\n\n    Returns\n    -------\n    sliced : np.ndarray\n        Sliced images as 4d array.\n    """"""\n    sliced_ds = []\n    for i in range(int(np.sqrt(n_imgs))):\n        for j in range(int(np.sqrt(n_imgs))):\n            sliced_ds.append(montage[\n                1 + i + i * img_h:1 + i + (i + 1) * img_h,\n                1 + j + j * img_w:1 + j + (j + 1) * img_w])\n    return np.array(sliced_ds)\n\n\ndef montage(images, saveto=\'montage.png\'):\n    """"""Draw all images as a montage separated by 1 pixel borders.\n\n    Also saves the file to the destination specified by `saveto`.\n\n    Parameters\n    ----------\n    images : numpy.ndarray\n        Input array to create montage of.  Array should be:\n        batch x height x width x channels.\n    saveto : str\n        Location to save the resulting montage image.\n\n    Returns\n    -------\n    m : numpy.ndarray\n        Montage image.\n    """"""\n    if isinstance(images, list):\n        images = np.array(images)\n    img_h = images.shape[1]\n    img_w = images.shape[2]\n    n_plots = int(np.ceil(np.sqrt(images.shape[0])))\n    if len(images.shape) == 4 and images.shape[3] == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 3)) * 0.5\n    elif len(images.shape) == 4 and images.shape[3] == 1:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 1)) * 0.5\n    elif len(images.shape) == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1)) * 0.5\n    else:\n        raise ValueError(\'Could not parse image shape of {}\'.format(\n            images.shape))\n    for i in range(n_plots):\n        for j in range(n_plots):\n            this_filter = i * n_plots + j\n            if this_filter < images.shape[0]:\n                this_img = images[this_filter]\n                m[1 + i + i * img_h:1 + i + (i + 1) * img_h,\n                  1 + j + j * img_w:1 + j + (j + 1) * img_w] = this_img\n    imsave(arr=np.squeeze(m), name=saveto)\n    return m\n\n\ndef montage_filters(W):\n    """"""Draws all filters (n_input * n_output filters) as a\n    montage image separated by 1 pixel borders.\n\n    Parameters\n    ----------\n    W : Tensor\n        Input tensor to create montage of.\n\n    Returns\n    -------\n    m : numpy.ndarray\n        Montage image.\n    """"""\n    W = np.reshape(W, [W.shape[0], W.shape[1], 1, W.shape[2] * W.shape[3]])\n    n_plots = int(np.ceil(np.sqrt(W.shape[-1])))\n    m = np.ones(\n        (W.shape[0] * n_plots + n_plots + 1,\n         W.shape[1] * n_plots + n_plots + 1)) * 0.5\n    for i in range(n_plots):\n        for j in range(n_plots):\n            this_filter = i * n_plots + j\n            if this_filter < W.shape[-1]:\n                m[1 + i + i * W.shape[0]:1 + i + (i + 1) * W.shape[0],\n                  1 + j + j * W.shape[1]:1 + j + (j + 1) * W.shape[1]] = (\n                    np.squeeze(W[:, :, :, this_filter]))\n    return m\n\n\ndef get_celeb_files(dst=\'img_align_celeba\', max_images=100):\n    """"""Download the first 100 images of the celeb dataset.\n\n    Files will be placed in a directory \'img_align_celeba\' if one\n    doesn\'t exist.\n\n    Returns\n    -------\n    files : list of strings\n        Locations to the first 100 images of the celeb net dataset.\n    """"""\n    # Create a directory\n    if not os.path.exists(dst):\n        os.mkdir(dst)\n\n    # Now perform the following 100 times:\n    for img_i in range(1, max_images + 1):\n\n        # create a string using the current loop counter\n        f = \'000%03d.jpg\' % img_i\n\n        if not os.path.exists(os.path.join(dst, f)):\n\n            # and get the url with that string appended the end\n            url = \'https://s3.amazonaws.com/cadl/celeb-align/\' + f\n\n            # We\'ll print this out to the console so we can see how far we\'ve gone\n            print(url, end=\'\\r\')\n\n            # And now download the url to a location inside our new directory\n            urllib.request.urlretrieve(url, os.path.join(dst, f))\n\n    files = [os.path.join(dst, file_i)\n             for file_i in os.listdir(dst)\n             if \'.jpg\' in file_i][:max_images]\n    return files\n\n\ndef get_celeb_imgs(max_images=100):\n    """"""Load the first `max_images` images of the celeb dataset.\n\n    Returns\n    -------\n    imgs : list of np.ndarray\n        List of the first 100 images from the celeb dataset\n    """"""\n    return [plt.imread(f_i) for f_i in get_celeb_files(max_images=max_images)]\n\n\ndef gauss(mean, stddev, ksize):\n    """"""Use Tensorflow to compute a Gaussian Kernel.\n\n    Parameters\n    ----------\n    mean : float\n        Mean of the Gaussian (e.g. 0.0).\n    stddev : float\n        Standard Deviation of the Gaussian (e.g. 1.0).\n    ksize : int\n        Size of kernel (e.g. 16).\n\n    Returns\n    -------\n    kernel : np.ndarray\n        Computed Gaussian Kernel using Tensorflow.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        x = tf.linspace(-3.0, 3.0, ksize)\n        z = (tf.exp(tf.negative(tf.pow(x - mean, 2.0) /\n                           (2.0 * tf.pow(stddev, 2.0)))) *\n             (1.0 / (stddev * tf.sqrt(2.0 * 3.1415))))\n        return z.eval()\n\n\ndef gauss2d(mean, stddev, ksize):\n    """"""Use Tensorflow to compute a 2D Gaussian Kernel.\n\n    Parameters\n    ----------\n    mean : float\n        Mean of the Gaussian (e.g. 0.0).\n    stddev : float\n        Standard Deviation of the Gaussian (e.g. 1.0).\n    ksize : int\n        Size of kernel (e.g. 16).\n\n    Returns\n    -------\n    kernel : np.ndarray\n        Computed 2D Gaussian Kernel using Tensorflow.\n    """"""\n    z = gauss(mean, stddev, ksize)\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        z_2d = tf.matmul(tf.reshape(z, [ksize, 1]), tf.reshape(z, [1, ksize]))\n        return z_2d.eval()\n\n\ndef convolve(img, kernel):\n    """"""Use Tensorflow to convolve a 4D image with a 4D kernel.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        4-dimensional image shaped N x H x W x C\n    kernel : np.ndarray\n        4-dimensional image shape K_H, K_W, C_I, C_O corresponding to the\n        kernel\'s height and width, the number of input channels, and the\n        number of output channels.  Note that C_I should = C.\n\n    Returns\n    -------\n    result : np.ndarray\n        Convolved result.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        convolved = tf.nn.conv2d(img, kernel, strides=[1, 1, 1, 1], padding=\'SAME\')\n        res = convolved.eval()\n    return res\n\n\ndef gabor(ksize=32):\n    """"""Use Tensorflow to compute a 2D Gabor Kernel.\n\n    Parameters\n    ----------\n    ksize : int, optional\n        Size of kernel.\n\n    Returns\n    -------\n    gabor : np.ndarray\n        Gabor kernel with ksize x ksize dimensions.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        z_2d = gauss2d(0.0, 1.0, ksize)\n        ones = tf.ones((1, ksize))\n        ys = tf.sin(tf.linspace(-3.0, 3.0, ksize))\n        ys = tf.reshape(ys, [ksize, 1])\n        wave = tf.matmul(ys, ones)\n        gabor = tf.multiply(wave, z_2d)\n        return gabor.eval()\n\n\ndef build_submission(filename, file_list, optional_file_list=()):\n    """"""Helper utility to check homework assignment submissions and package them.\n\n    Parameters\n    ----------\n    filename : str\n        Output zip file name\n    file_list : tuple\n        Tuple of files to include\n    """"""\n    # check each file exists\n    for part_i, file_i in enumerate(file_list):\n        if not os.path.exists(file_i):\n            print(\'\\nYou are missing the file {}.  \'.format(file_i) +\n                  \'It does not look like you have completed Part {}.\'.format(\n                part_i + 1))\n\n    def zipdir(path, zf):\n        for root, dirs, files in os.walk(path):\n            for file in files:\n                # make sure the files are part of the necessary file list\n                if file.endswith(file_list) or file.endswith(optional_file_list):\n                    zf.write(os.path.join(root, file))\n\n    # create a zip file with the necessary files\n    zipf = zipfile.ZipFile(filename, \'w\', zipfile.ZIP_DEFLATED)\n    zipdir(\'.\', zipf)\n    zipf.close()\n    print(\'Your assignment zip file has been created!\')\n    print(\'Now submit the file:\\n{}\\nto Kadenze for grading!\'.format(\n        os.path.abspath(filename)))\n\n\ndef normalize(a, s=0.1):\n    \'\'\'Normalize the image range for visualization\'\'\'\n    return np.uint8(np.clip(\n        (a - a.mean()) / max(a.std(), 1e-4) * s + 0.5,\n        0, 1) * 255)\n\n\n# %%\ndef weight_variable(shape, **kwargs):\n    \'\'\'Helper function to create a weight variable initialized with\n    a normal distribution\n    Parameters\n    ----------\n    shape : list\n        Size of weight variable\n    \'\'\'\n    if isinstance(shape, list):\n        initial = tf.random_normal(tf.stack(shape), mean=0.0, stddev=0.01)\n        initial.set_shape(shape)\n    else:\n        initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n    return tf.Variable(initial, **kwargs)\n\n\n# %%\ndef bias_variable(shape, **kwargs):\n    \'\'\'Helper function to create a bias variable initialized with\n    a constant value.\n    Parameters\n    ----------\n    shape : list\n        Size of weight variable\n    \'\'\'\n    if isinstance(shape, list):\n        initial = tf.random_normal(tf.stack(shape), mean=0.0, stddev=0.01)\n        initial.set_shape(shape)\n    else:\n        initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n    return tf.Variable(initial, **kwargs)\n\n\ndef binary_cross_entropy(z, x, name=None):\n    """"""Binary Cross Entropy measures cross entropy of a binary variable.\n\n    loss(x, z) = - sum_i (x[i] * log(z[i]) + (1 - x[i]) * log(1 - z[i]))\n\n    Parameters\n    ----------\n    z : tf.Tensor\n        A `Tensor` of the same type and shape as `x`.\n    x : tf.Tensor\n        A `Tensor` of type `float32` or `float64`.\n    """"""\n    with tf.variable_scope(name or \'bce\'):\n        eps = 1e-12\n        return (-(x * tf.log(z + eps) +\n                  (1. - x) * tf.log(1. - z + eps)))\n\n\ndef conv2d(x, n_output,\n           k_h=5, k_w=5, d_h=2, d_w=2,\n           padding=\'SAME\', name=\'conv2d\', reuse=None):\n    """"""Helper for creating a 2d convolution operation.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to convolve.\n    n_output : int\n        Number of filters.\n    k_h : int, optional\n        Kernel height\n    k_w : int, optional\n        Kernel width\n    d_h : int, optional\n        Height stride\n    d_w : int, optional\n        Width stride\n    padding : str, optional\n        Padding type: ""SAME"" or ""VALID""\n    name : str, optional\n        Variable scope\n\n    Returns\n    -------\n    op : tf.Tensor\n        Output of convolution\n    """"""\n    with tf.variable_scope(name or \'conv2d\', reuse=reuse):\n        W = tf.get_variable(\n            name=\'W\',\n            shape=[k_h, k_w, x.get_shape()[-1], n_output],\n            initializer=tf.contrib.layers.xavier_initializer_conv2d())\n\n        conv = tf.nn.conv2d(\n            name=\'conv\',\n            input=x,\n            filter=W,\n            strides=[1, d_h, d_w, 1],\n            padding=padding)\n\n        b = tf.get_variable(\n            name=\'b\',\n            shape=[n_output],\n            initializer=tf.constant_initializer(0.0))\n\n        h = tf.nn.bias_add(\n            name=\'h\',\n            value=conv,\n            bias=b)\n\n    return h, W\n\n\ndef deconv2d(x, n_output_h, n_output_w, n_output_ch, n_input_ch=None,\n             k_h=5, k_w=5, d_h=2, d_w=2,\n             padding=\'SAME\', name=\'deconv2d\', reuse=None):\n    """"""Deconvolution helper.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to convolve.\n    n_output_h : int\n        Height of output\n    n_output_w : int\n        Width of output\n    n_output_ch : int\n        Number of filters.\n    k_h : int, optional\n        Kernel height\n    k_w : int, optional\n        Kernel width\n    d_h : int, optional\n        Height stride\n    d_w : int, optional\n        Width stride\n    padding : str, optional\n        Padding type: ""SAME"" or ""VALID""\n    name : str, optional\n        Variable scope\n\n    Returns\n    -------\n    op : tf.Tensor\n        Output of deconvolution\n    """"""\n    with tf.variable_scope(name or \'deconv2d\', reuse=reuse):\n        W = tf.get_variable(\n            name=\'W\',\n            shape=[k_h, k_w, n_output_ch, n_input_ch or x.get_shape()[-1]],\n            initializer=tf.contrib.layers.xavier_initializer_conv2d())\n\n        conv = tf.nn.conv2d_transpose(\n            name=\'conv_t\',\n            value=x,\n            filter=W,\n            output_shape=tf.stack(\n                [tf.shape(x)[0], n_output_h, n_output_w, n_output_ch]),\n            strides=[1, d_h, d_w, 1],\n            padding=padding)\n\n        conv.set_shape([None, n_output_h, n_output_w, n_output_ch])\n\n        b = tf.get_variable(\n            name=\'b\',\n            shape=[n_output_ch],\n            initializer=tf.constant_initializer(0.0))\n\n        h = tf.nn.bias_add(name=\'h\', value=conv, bias=b)\n\n    return h, W\n\n\ndef lrelu(features, leak=0.2):\n    """"""Leaky rectifier.\n\n    Parameters\n    ----------\n    features : tf.Tensor\n        Input to apply leaky rectifier to.\n    leak : float, optional\n        Percentage of leak.\n\n    Returns\n    -------\n    op : tf.Tensor\n        Resulting output of applying leaky rectifier activation.\n    """"""\n    f1 = 0.5 * (1 + leak)\n    f2 = 0.5 * (1 - leak)\n    return f1 * features + f2 * abs(features)\n\n\ndef linear(x, n_output, name=None, activation=None, reuse=None):\n    """"""Fully connected layer.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to connect\n    n_output : int\n        Number of output neurons\n    name : None, optional\n        Scope to apply\n\n    Returns\n    -------\n    h, W : tf.Tensor, tf.Tensor\n        Output of fully connected layer and the weight matrix\n    """"""\n    if len(x.get_shape()) != 2:\n        x = flatten(x, reuse=reuse)\n\n    n_input = x.get_shape().as_list()[1]\n\n    with tf.variable_scope(name or ""fc"", reuse=reuse):\n        W = tf.get_variable(\n            name=\'W\',\n            shape=[n_input, n_output],\n            dtype=tf.float32,\n            initializer=tf.contrib.layers.xavier_initializer())\n\n        b = tf.get_variable(\n            name=\'b\',\n            shape=[n_output],\n            dtype=tf.float32,\n            initializer=tf.constant_initializer(0.0))\n\n        h = tf.nn.bias_add(\n            name=\'h\',\n            value=tf.matmul(x, W),\n            bias=b)\n\n        if activation:\n            h = activation(h)\n\n        return h, W\n\n\ndef flatten(x, name=None, reuse=None):\n    """"""Flatten Tensor to 2-dimensions.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to flatten.\n    name : None, optional\n        Variable scope for flatten operations\n\n    Returns\n    -------\n    flattened : tf.Tensor\n        Flattened tensor.\n    """"""\n    with tf.variable_scope(\'flatten\'):\n        dims = x.get_shape().as_list()\n        if len(dims) == 4:\n            flattened = tf.reshape(\n                x,\n                shape=[-1, dims[1] * dims[2] * dims[3]])\n        elif len(dims) == 2 or len(dims) == 1:\n            flattened = x\n        else:\n            raise ValueError(\'Expected n dimensions of 1, 2 or 4.  Found:\',\n                             len(dims))\n\n        return flattened\n\n\ndef to_tensor(x):\n    """"""Convert 2 dim Tensor to a 4 dim Tensor ready for convolution.\n\n    Performs the opposite of flatten(x).  If the tensor is already 4-D, this\n    returns the same as the input, leaving it unchanged.\n\n    Parameters\n    ----------\n    x : tf.Tesnor\n        Input 2-D tensor.  If 4-D already, left unchanged.\n\n    Returns\n    -------\n    x : tf.Tensor\n        4-D representation of the input.\n\n    Raises\n    ------\n    ValueError\n        If the tensor is not 2D or already 4D.\n    """"""\n    if len(x.get_shape()) == 2:\n        n_input = x.get_shape().as_list()[1]\n        x_dim = np.sqrt(n_input)\n        if x_dim == int(x_dim):\n            x_dim = int(x_dim)\n            x_tensor = tf.reshape(\n                x, [-1, x_dim, x_dim, 1], name=\'reshape\')\n        elif np.sqrt(n_input / 3) == int(np.sqrt(n_input / 3)):\n            x_dim = int(np.sqrt(n_input / 3))\n            x_tensor = tf.reshape(\n                x, [-1, x_dim, x_dim, 3], name=\'reshape\')\n        else:\n            x_tensor = tf.reshape(\n                x, [-1, 1, 1, n_input], name=\'reshape\')\n    elif len(x.get_shape()) == 4:\n        x_tensor = x\n    else:\n        raise ValueError(\'Unsupported input dimensions\')\n    return x_tensor\n'"
session-4/libs/vae.py,40,"b'""""""Convolutional/Variational autoencoder, including demonstration of\ntraining such a network on MNIST, CelebNet and the film, ""Sita Sings The Blues""\nusing an image pipeline.\n\nCopyright Parag K. Mital, January 2016\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport os\nfrom libs.dataset_utils import create_input_pipeline\nfrom libs.datasets import CELEB, MNIST\nfrom libs.batch_norm import batch_norm\nfrom libs import utils\n\n\ndef VAE(input_shape=[None, 784],\n        n_filters=[64, 64, 64],\n        filter_sizes=[4, 4, 4],\n        n_hidden=32,\n        n_code=2,\n        activation=tf.nn.tanh,\n        dropout=False,\n        denoising=False,\n        convolutional=False,\n        variational=False):\n    """"""(Variational) (Convolutional) (Denoising) Autoencoder.\n\n    Uses tied weights.\n\n    Parameters\n    ----------\n    input_shape : list, optional\n        Shape of the input to the network. e.g. for MNIST: [None, 784].\n    n_filters : list, optional\n        Number of filters for each layer.\n        If convolutional=True, this refers to the total number of output\n        filters to create for each layer, with each layer\'s number of output\n        filters as a list.\n        If convolutional=False, then this refers to the total number of neurons\n        for each layer in a fully connected network.\n    filter_sizes : list, optional\n        Only applied when convolutional=True.  This refers to the ksize (height\n        and width) of each convolutional layer.\n    n_hidden : int, optional\n        Only applied when variational=True.  This refers to the first fully\n        connected layer prior to the variational embedding, directly after\n        the encoding.  After the variational embedding, another fully connected\n        layer is created with the same size prior to decoding.  Set to 0 to\n        not use an additional hidden layer.\n    n_code : int, optional\n        Only applied when variational=True.  This refers to the number of\n        latent Gaussians to sample for creating the inner most encoding.\n    activation : function, optional\n        Activation function to apply to each layer, e.g. tf.nn.relu\n    dropout : bool, optional\n        Whether or not to apply dropout.  If using dropout, you must feed a\n        value for \'keep_prob\', as returned in the dictionary.  1.0 means no\n        dropout is used.  0.0 means every connection is dropped.  Sensible\n        values are between 0.5-0.8.\n    denoising : bool, optional\n        Whether or not to apply denoising.  If using denoising, you must feed a\n        value for \'corrupt_prob\', as returned in the dictionary.  1.0 means no\n        corruption is used.  0.0 means every feature is corrupted.  Sensible\n        values are between 0.5-0.8.\n    convolutional : bool, optional\n        Whether or not to use a convolutional network or else a fully connected\n        network will be created.  This effects the n_filters parameter\'s\n        meaning.\n    variational : bool, optional\n        Whether or not to create a variational embedding layer.  This will\n        create a fully connected layer after the encoding, if `n_hidden` is\n        greater than 0, then will create a multivariate gaussian sampling\n        layer, then another fully connected layer.  The size of the fully\n        connected layers are determined by `n_hidden`, and the size of the\n        sampling layer is determined by `n_code`.\n\n    Returns\n    -------\n    model : dict\n        {\n            \'cost\': Tensor to optimize.\n            \'Ws\': All weights of the encoder.\n            \'x\': Input Placeholder\n            \'z\': Inner most encoding Tensor (latent features)\n            \'y\': Reconstruction of the Decoder\n            \'keep_prob\': Amount to keep when using Dropout\n            \'corrupt_prob\': Amount to corrupt when using Denoising\n            \'train\': Set to True when training/Applies to Batch Normalization.\n        }\n    """"""\n    # network input / placeholders for train (bn) and dropout\n    x = tf.placeholder(tf.float32, input_shape, \'x\')\n    phase_train = tf.placeholder(tf.bool, name=\'phase_train\')\n    keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n    corrupt_prob = tf.placeholder(tf.float32, [1])\n\n    # apply noise if denoising\n    x_ = (utils.corrupt(x) * corrupt_prob + x * (1 - corrupt_prob)) if denoising else x\n\n    # 2d -> 4d if convolution\n    x_tensor = utils.to_tensor(x_) if convolutional else x_\n    current_input = x_tensor\n\n    Ws = []\n    shapes = []\n\n    # Build the encoder\n    for layer_i, n_output in enumerate(n_filters):\n        with tf.variable_scope(\'encoder/{}\'.format(layer_i)):\n            shapes.append(current_input.get_shape().as_list())\n            if convolutional:\n                h, W = utils.conv2d(x=current_input,\n                                    n_output=n_output,\n                                    k_h=filter_sizes[layer_i],\n                                    k_w=filter_sizes[layer_i])\n            else:\n                h, W = utils.linear(x=current_input,\n                                    n_output=n_output)\n            h = activation(batch_norm(h, phase_train, \'bn\' + str(layer_i)))\n            if dropout:\n                h = tf.nn.dropout(h, keep_prob)\n            Ws.append(W)\n            current_input = h\n\n    shapes.append(current_input.get_shape().as_list())\n\n    with tf.variable_scope(\'variational\'):\n        if variational:\n            dims = current_input.get_shape().as_list()\n            flattened = utils.flatten(current_input)\n\n            if n_hidden:\n                h = utils.linear(flattened, n_hidden, name=\'W_fc\')[0]\n                h = activation(batch_norm(h, phase_train, \'fc/bn\'))\n                if dropout:\n                    h = tf.nn.dropout(h, keep_prob)\n            else:\n                h = flattened\n\n            z_mu = utils.linear(h, n_code, name=\'mu\')[0]\n            z_log_sigma = 0.5 * utils.linear(h, n_code, name=\'log_sigma\')[0]\n\n            # Sample from noise distribution p(eps) ~ N(0, 1)\n            epsilon = tf.random_normal(\n                tf.stack([tf.shape(x)[0], n_code]))\n\n            # Sample from posterior\n            z = z_mu + tf.multiply(epsilon, tf.exp(z_log_sigma))\n\n            if n_hidden:\n                h = utils.linear(z, n_hidden, name=\'fc_t\')[0]\n                h = activation(batch_norm(h, phase_train, \'fc_t/bn\'))\n                if dropout:\n                    h = tf.nn.dropout(h, keep_prob)\n            else:\n                h = z\n\n            size = dims[1] * dims[2] * dims[3] if convolutional else dims[1]\n            h = utils.linear(h, size, name=\'fc_t2\')[0]\n            current_input = activation(batch_norm(h, phase_train, \'fc_t2/bn\'))\n            if dropout:\n                current_input = tf.nn.dropout(current_input, keep_prob)\n\n            if convolutional:\n                current_input = tf.reshape(\n                    current_input, tf.stack([\n                        tf.shape(current_input)[0],\n                        dims[1],\n                        dims[2],\n                        dims[3]]))\n        else:\n            z = current_input\n\n    shapes.reverse()\n    n_filters.reverse()\n    Ws.reverse()\n\n    n_filters += [input_shape[-1]]\n\n    # %%\n    # Decoding layers\n    for layer_i, n_output in enumerate(n_filters[1:]):\n        with tf.variable_scope(\'decoder/{}\'.format(layer_i)):\n            shape = shapes[layer_i + 1]\n            if convolutional:\n                h, W = utils.deconv2d(x=current_input,\n                                      n_output_h=shape[1],\n                                      n_output_w=shape[2],\n                                      n_output_ch=shape[3],\n                                      n_input_ch=shapes[layer_i][3],\n                                      k_h=filter_sizes[layer_i],\n                                      k_w=filter_sizes[layer_i])\n            else:\n                h, W = utils.linear(x=current_input,\n                                    n_output=n_output)\n            h = activation(batch_norm(h, phase_train, \'dec/bn\' + str(layer_i)))\n            if dropout:\n                h = tf.nn.dropout(h, keep_prob)\n            current_input = h\n\n    y = current_input\n    x_flat = utils.flatten(x)\n    y_flat = utils.flatten(y)\n\n    # l2 loss\n    loss_x = tf.reduce_sum(tf.squared_difference(x_flat, y_flat), 1)\n\n    if variational:\n        # variational lower bound, kl-divergence\n        loss_z = -0.5 * tf.reduce_sum(\n            1.0 + 2.0 * z_log_sigma -\n            tf.square(z_mu) - tf.exp(2.0 * z_log_sigma), 1)\n\n        # add l2 loss\n        cost = tf.reduce_mean(loss_x + loss_z)\n    else:\n        # just optimize l2 loss\n        cost = tf.reduce_mean(loss_x)\n\n    return {\'cost\': cost, \'Ws\': Ws,\n            \'x\': x, \'z\': z, \'y\': y,\n            \'keep_prob\': keep_prob,\n            \'corrupt_prob\': corrupt_prob,\n            \'train\': phase_train}\n\n\ndef train_vae(files,\n              input_shape,\n              learning_rate=0.0001,\n              batch_size=100,\n              n_epochs=50,\n              n_examples=10,\n              crop_shape=[64, 64, 3],\n              crop_factor=0.8,\n              n_filters=[100, 100, 100, 100],\n              n_hidden=256,\n              n_code=50,\n              convolutional=True,\n              variational=True,\n              filter_sizes=[3, 3, 3, 3],\n              dropout=True,\n              keep_prob=0.8,\n              activation=tf.nn.relu,\n              img_step=100,\n              save_step=100,\n              ckpt_name=""vae.ckpt""):\n    """"""General purpose training of a (Variational) (Convolutional) Autoencoder.\n\n    Supply a list of file paths to images, and this will do everything else.\n\n    Parameters\n    ----------\n    files : list of strings\n        List of paths to images.\n    input_shape : list\n        Must define what the input image\'s shape is.\n    learning_rate : float, optional\n        Learning rate.\n    batch_size : int, optional\n        Batch size.\n    n_epochs : int, optional\n        Number of epochs.\n    n_examples : int, optional\n        Number of example to use while demonstrating the current training\n        iteration\'s reconstruction.  Creates a square montage, so make\n        sure int(sqrt(n_examples))**2 = n_examples, e.g. 16, 25, 36, ... 100.\n    crop_shape : list, optional\n        Size to centrally crop the image to.\n    crop_factor : float, optional\n        Resize factor to apply before cropping.\n    n_filters : list, optional\n        Same as VAE\'s n_filters.\n    n_hidden : int, optional\n        Same as VAE\'s n_hidden.\n    n_code : int, optional\n        Same as VAE\'s n_code.\n    convolutional : bool, optional\n        Use convolution or not.\n    variational : bool, optional\n        Use variational layer or not.\n    filter_sizes : list, optional\n        Same as VAE\'s filter_sizes.\n    dropout : bool, optional\n        Use dropout or not\n    keep_prob : float, optional\n        Percent of keep for dropout.\n    activation : function, optional\n        Which activation function to use.\n    img_step : int, optional\n        How often to save training images showing the manifold and\n        reconstruction.\n    save_step : int, optional\n        How often to save checkpoints.\n    ckpt_name : str, optional\n        Checkpoints will be named as this, e.g. \'model.ckpt\'\n    """"""\n    batch = create_input_pipeline(\n        files=files,\n        batch_size=batch_size,\n        n_epochs=n_epochs,\n        crop_shape=crop_shape,\n        crop_factor=crop_factor,\n        shape=input_shape)\n\n    ae = VAE(input_shape=[None] + crop_shape,\n             convolutional=convolutional,\n             variational=variational,\n             n_filters=n_filters,\n             n_hidden=n_hidden,\n             n_code=n_code,\n             dropout=dropout,\n             filter_sizes=filter_sizes,\n             activation=activation)\n\n    # Create a manifold of our inner most layer to show\n    # example reconstructions.  This is one way to see\n    # what the ""embedding"" or ""latent space"" of the encoder\n    # is capable of encoding, though note that this is just\n    # a random hyperplane within the latent space, and does not\n    # encompass all possible embeddings.\n    zs = np.random.uniform(\n        -1.0, 1.0, [4, n_code]).astype(np.float32)\n    zs = utils.make_latent_manifold(zs, n_examples)\n\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate=learning_rate).minimize(ae[\'cost\'])\n\n    # We create a session to use the graph\n    sess = tf.Session()\n    saver = tf.train.Saver()\n    sess.run(tf.global_variables_initializer())\n\n    # This will handle our threaded image pipeline\n    coord = tf.train.Coordinator()\n\n    # Ensure no more changes to graph\n    tf.get_default_graph().finalize()\n\n    # Start up the queues for handling the image pipeline\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    if os.path.exists(ckpt_name + \'.index\') or os.path.exists(ckpt_name):\n        saver.restore(sess, ckpt_name)\n\n    # Fit all training data\n    t_i = 0\n    batch_i = 0\n    epoch_i = 0\n    cost = 0\n    n_files = len(files)\n    test_xs = sess.run(batch) / 255.0\n    utils.montage(test_xs, \'test_xs.png\')\n    try:\n        while not coord.should_stop() and epoch_i < n_epochs:\n            batch_i += 1\n            batch_xs = sess.run(batch) / 255.0\n            train_cost = sess.run([ae[\'cost\'], optimizer], feed_dict={\n                ae[\'x\']: batch_xs, ae[\'train\']: True,\n                ae[\'keep_prob\']: keep_prob})[0]\n            print(batch_i, train_cost)\n            cost += train_cost\n            if batch_i % n_files == 0:\n                print(\'epoch:\', epoch_i)\n                print(\'average cost:\', cost / batch_i)\n                cost = 0\n                batch_i = 0\n                epoch_i += 1\n\n            if batch_i % img_step == 0:\n                # Plot example reconstructions from latent layer\n                recon = sess.run(\n                    ae[\'y\'], feed_dict={\n                        ae[\'z\']: zs,\n                        ae[\'train\']: False,\n                        ae[\'keep_prob\']: 1.0})\n                utils.montage(recon.reshape([-1] + crop_shape),\n                              \'manifold_%08d.png\' % t_i)\n\n                # Plot example reconstructions\n                recon = sess.run(\n                    ae[\'y\'], feed_dict={ae[\'x\']: test_xs,\n                                        ae[\'train\']: False,\n                                        ae[\'keep_prob\']: 1.0})\n                print(\'reconstruction (min, max, mean):\',\n                    recon.min(), recon.max(), recon.mean())\n                utils.montage(recon.reshape([-1] + crop_shape),\n                              \'reconstruction_%08d.png\' % t_i)\n                t_i += 1\n\n            if batch_i % save_step == 0:\n                # Save the variables to disk.\n                saver.save(sess, ""./"" + ckpt_name,\n                           global_step=batch_i,\n                           write_meta_graph=False)\n    except tf.errors.OutOfRangeError:\n        print(\'Done.\')\n    finally:\n        # One of the threads has issued an exception.  So let\'s tell all the\n        # threads to shutdown.\n        coord.request_stop()\n\n    # Wait until all threads have finished.\n    coord.join(threads)\n\n    # Clean up the session.\n    sess.close()\n\n\n# %%\ndef test_mnist():\n    """"""Train an autoencoder on MNIST.\n\n    This function will train an autoencoder on MNIST and also\n    save many image files during the training process, demonstrating\n    the latent space of the inner most dimension of the encoder,\n    as well as reconstructions of the decoder.\n    """"""\n\n    # load MNIST\n    n_code = 2\n    mnist = MNIST(split=[0.8, 0.1, 0.1])\n    ae = VAE(input_shape=[None, 784], n_filters=[512, 256],\n             n_hidden=64, n_code=n_code, activation=tf.nn.sigmoid,\n             convolutional=False, variational=True)\n\n    n_examples = 100\n    zs = np.random.uniform(\n        -1.0, 1.0, [4, n_code]).astype(np.float32)\n    zs = utils.make_latent_manifold(zs, n_examples)\n\n    learning_rate = 0.02\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate=learning_rate).minimize(ae[\'cost\'])\n\n    # We create a session to use the graph\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n\n    # Fit all training data\n    t_i = 0\n    batch_i = 0\n    batch_size = 200\n    n_epochs = 10\n    test_xs = mnist.test.images[:n_examples]\n    utils.montage(test_xs.reshape((-1, 28, 28)), \'test_xs.png\')\n    for epoch_i in range(n_epochs):\n        train_i = 0\n        train_cost = 0\n        for batch_xs, _ in mnist.train.next_batch(batch_size):\n            train_cost += sess.run([ae[\'cost\'], optimizer], feed_dict={\n                ae[\'x\']: batch_xs, ae[\'train\']: True, ae[\'keep_prob\']: 1.0})[0]\n            train_i += 1\n            if batch_i % 10 == 0:\n                # Plot example reconstructions from latent layer\n                recon = sess.run(\n                    ae[\'y\'], feed_dict={\n                        ae[\'z\']: zs,\n                        ae[\'train\']: False,\n                        ae[\'keep_prob\']: 1.0})\n                utils.montage(recon.reshape((-1, 28, 28)),\n                    \'manifold_%08d.png\' % t_i)\n                # Plot example reconstructions\n                recon = sess.run(\n                    ae[\'y\'], feed_dict={ae[\'x\']: test_xs,\n                                        ae[\'train\']: False,\n                                        ae[\'keep_prob\']: 1.0})\n                utils.montage(recon.reshape(\n                    (-1, 28, 28)), \'reconstruction_%08d.png\' % t_i)\n                t_i += 1\n            batch_i += 1\n\n        valid_i = 0\n        valid_cost = 0\n        for batch_xs, _ in mnist.valid.next_batch(batch_size):\n            valid_cost += sess.run([ae[\'cost\']], feed_dict={\n                ae[\'x\']: batch_xs, ae[\'train\']: False, ae[\'keep_prob\']: 1.0})[0]\n            valid_i += 1\n        print(\'train:\', train_cost / train_i, \'valid:\', valid_cost / valid_i)\n\n\ndef test_celeb():\n    """"""Train an autoencoder on Celeb Net.\n    """"""\n    files = CELEB()\n    train_vae(\n        files=files,\n        input_shape=[218, 178, 3],\n        batch_size=100,\n        n_epochs=50,\n        crop_shape=[64, 64, 3],\n        crop_factor=0.8,\n        convolutional=True,\n        variational=True,\n        n_filters=[100, 100, 100],\n        n_hidden=250,\n        n_code=100,\n        dropout=True,\n        filter_sizes=[3, 3, 3],\n        activation=tf.nn.sigmoid,\n        ckpt_name=\'./celeb.ckpt\')\n\n\ndef test_sita():\n    """"""Train an autoencoder on Sita Sings The Blues.\n    """"""\n    if not os.path.exists(\'sita\'):\n        os.system(\'wget http://ossguy.com/sita/Sita_Sings_the_Blues_640x360_XviD.avi\')\n        os.mkdir(\'sita\')\n        os.system(\'ffmpeg -i Sita_Sings_the_Blues_640x360_XviD.avi -r 60 -f\' +\n                  \' image2 -s 160x90 sita/sita-%08d.jpg\')\n    files = [os.path.join(\'sita\', f) for f in os.listdir(\'sita\')]\n\n    train_vae(\n        files=files,\n        input_shape=[90, 160, 3],\n        batch_size=100,\n        n_epochs=50,\n        crop_shape=[90, 160, 3],\n        crop_factor=1.0,\n        convolutional=True,\n        variational=True,\n        n_filters=[100, 100, 100],\n        n_hidden=250,\n        n_code=100,\n        dropout=True,\n        filter_sizes=[3, 3, 3],\n        activation=tf.nn.sigmoid,\n        ckpt_name=\'./sita.ckpt\')\n\n\nif __name__ == \'__main__\':\n    test_celeb()\n'"
session-4/libs/vgg16.py,19,"b'""""""\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport tensorflow as tf\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize as imresize\nfrom .utils import download\n\n\ndef get_vgg_face_model():\n    download(\'https://s3.amazonaws.com/cadl/models/vgg_face.tfmodel\')\n    with open(""vgg_face.tfmodel"", mode=\'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python \' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n\n    download(\'https://s3.amazonaws.com/cadl/models/vgg_face.json\')\n    labels = json.load(open(\'vgg_face.json\'))\n\n    return {\n        \'graph_def\': graph_def,\n        \'labels\': labels,\n        \'preprocess\': preprocess,\n        \'deprocess\': deprocess\n    }\n\n\ndef get_vgg_model():\n    download(\'https://s3.amazonaws.com/cadl/models/vgg16.tfmodel\')\n    with open(""vgg16.tfmodel"", mode=\'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python \' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n\n    download(\'https://s3.amazonaws.com/cadl/models/synset.txt\')\n    with open(\'synset.txt\') as f:\n        labels = [(idx, l.strip()) for idx, l in enumerate(f.readlines())]\n\n    return {\n        \'graph_def\': graph_def,\n        \'labels\': labels,\n        \'preprocess\': preprocess,\n        \'deprocess\': deprocess\n    }\n\n\ndef preprocess(img, crop=True, resize=True, dsize=(224, 224)):\n    if img.dtype == np.uint8:\n        img = img / 255.0\n\n    if crop:\n        short_edge = min(img.shape[:2])\n        yy = int((img.shape[0] - short_edge) / 2)\n        xx = int((img.shape[1] - short_edge) / 2)\n        crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n    else:\n        crop_img = img\n\n    if resize:\n        norm_img = imresize(crop_img, dsize, preserve_range=True)\n    else:\n        norm_img = crop_img\n\n    return (norm_img).astype(np.float32)\n\n\ndef deprocess(img):\n    return np.clip(img * 255, 0, 255).astype(np.uint8)\n    # return ((img / np.max(np.abs(img))) * 127.5 +\n    #         127.5).astype(np.uint8)\n\n\ndef test_vgg():\n    """"""Loads the VGG network and applies it to a test image.\n    """"""\n    with tf.Session() as sess:\n        net = get_vgg_model()\n        tf.import_graph_def(net[\'graph_def\'], name=\'vgg\')\n        g = tf.get_default_graph()\n        names = [op.name for op in g.get_operations()]\n        input_name = names[0] + \':0\'\n        x = g.get_tensor_by_name(input_name)\n        softmax = g.get_tensor_by_name(names[-2] + \':0\')\n\n        og = plt.imread(\'bosch.png\')\n        img = preprocess(og)[np.newaxis, ...]\n        res = np.squeeze(softmax.eval(feed_dict={\n            x: img,\n            \'vgg/dropout_1/random_uniform:0\': [[1.0]],\n            \'vgg/dropout/random_uniform:0\': [[1.0]]}))\n        print([(res[idx], net[\'labels\'][idx])\n               for idx in res.argsort()[-5:][::-1]])\n\n        """"""Let\'s visualize the network\'s gradient activation\n        when backpropagated to the original input image.  This\n        is effectively telling us which pixels contribute to the\n        predicted class or given neuron""""""\n        features = [name for name in names if \'BiasAdd\' in name.split()[-1]]\n        from math import sqrt, ceil\n        n_plots = ceil(sqrt(len(features) + 1))\n        fig, axs = plt.subplots(n_plots, n_plots)\n        plot_i = 0\n        axs[0][0].imshow(img[0])\n        for feature_i, featurename in enumerate(features):\n            plot_i += 1\n            feature = g.get_tensor_by_name(featurename + \':0\')\n            neuron = tf.reduce_max(feature, 1)\n            saliency = tf.gradients(tf.reduce_sum(neuron), x)\n            neuron_idx = tf.arg_max(feature, 1)\n            this_res = sess.run([saliency[0], neuron_idx], feed_dict={\n                x: img,\n                \'vgg/dropout_1/random_uniform:0\': [[1.0]],\n                \'vgg/dropout/random_uniform:0\': [[1.0]]})\n\n            grad = this_res[0][0] / np.max(np.abs(this_res[0]))\n            ax = axs[plot_i // n_plots][plot_i % n_plots]\n            ax.imshow((grad * 127.5 + 127.5).astype(np.uint8))\n            ax.set_title(featurename)\n\n        """"""Deep Dreaming takes the backpropagated gradient activations\n        and simply adds it to the image, running the same process again\n        and again in a loop.  There are many tricks one can add to this\n        idea, such as infinitely zooming into the image by cropping and\n        scaling, adding jitter by randomly moving the image around, or\n        adding constraints on the total activations.""""""\n        og = plt.imread(\'street.png\')\n        crop = 2\n        img = preprocess(og)[np.newaxis, ...]\n        layer = g.get_tensor_by_name(features[3] + \':0\')\n        n_els = layer.get_shape().as_list()[1]\n        neuron_i = np.random.randint(1000)\n        layer_vec = np.zeros((1, n_els))\n        layer_vec[0, neuron_i] = 1\n        neuron = tf.reduce_max(layer, 1)\n        saliency = tf.gradients(tf.reduce_sum(neuron), x)\n        for it_i in range(3):\n            print(it_i)\n            this_res = sess.run(saliency[0], feed_dict={\n                x: img,\n                layer: layer_vec,\n                \'vgg/dropout_1/random_uniform:0\': [[1.0]],\n                \'vgg/dropout/random_uniform:0\': [[1.0]]})\n            grad = this_res[0] / np.mean(np.abs(grad))\n            img = img[:, crop:-crop - 1, crop:-crop - 1, :]\n            img = imresize(img[0], (224, 224))[np.newaxis]\n            img += grad\n        plt.imshow(deprocess(img[0]))\n\n\ndef test_vgg_face():\n    """"""Loads the VGG network and applies it to a test image.\n    """"""\n    with tf.Session() as sess:\n        net = get_vgg_face_model()\n        x = tf.placeholder(tf.float32, [1, 224, 224, 3], name=\'x\')\n        tf.import_graph_def(net[\'graph_def\'], name=\'vgg\',\n                            input_map={\'Placeholder:0\': x})\n        g = tf.get_default_graph()\n        names = [op.name for op in g.get_operations()]\n\n        og = plt.imread(\'bricks.png\')[..., :3]\n        img = preprocess(og)[np.newaxis, ...]\n        plt.imshow(img[0])\n        plt.show()\n\n        """"""Let\'s visualize the network\'s gradient activation\n        when backpropagated to the original input image.  This\n        is effectively telling us which pixels contribute to the\n        predicted class or given neuron""""""\n        features = [name for name in names if \'BiasAdd\' in name.split()[-1]]\n        from math import sqrt, ceil\n        n_plots = ceil(sqrt(len(features) + 1))\n        fig, axs = plt.subplots(n_plots, n_plots)\n        plot_i = 0\n        axs[0][0].imshow(img[0])\n        for feature_i, featurename in enumerate(features):\n            plot_i += 1\n            feature = g.get_tensor_by_name(featurename + \':0\')\n            neuron = tf.reduce_max(feature, 1)\n            saliency = tf.gradients(tf.reduce_sum(neuron), x)\n            neuron_idx = tf.arg_max(feature, 1)\n            this_res = sess.run([saliency[0], neuron_idx], feed_dict={x: img})\n\n            grad = this_res[0][0] / np.max(np.abs(this_res[0]))\n            ax = axs[plot_i // n_plots][plot_i % n_plots]\n            ax.imshow((grad * 127.5 + 127.5).astype(np.uint8))\n            ax.set_title(featurename)\n            plt.waitforbuttonpress()\n\n        """"""Deep Dreaming takes the backpropagated gradient activations\n        and simply adds it to the image, running the same process again\n        and again in a loop.  There are many tricks one can add to this\n        idea, such as infinitely zooming into the image by cropping and\n        scaling, adding jitter by randomly moving the image around, or\n        adding constraints on the total activations.""""""\n        og = plt.imread(\'street.png\')\n        crop = 2\n        img = preprocess(og)[np.newaxis, ...]\n        layer = g.get_tensor_by_name(features[3] + \':0\')\n        n_els = layer.get_shape().as_list()[1]\n        neuron_i = np.random.randint(1000)\n        layer_vec = np.zeros((1, n_els))\n        layer_vec[0, neuron_i] = 1\n        neuron = tf.reduce_max(layer, 1)\n        saliency = tf.gradients(tf.reduce_sum(neuron), x)\n        for it_i in range(3):\n            print(it_i)\n            this_res = sess.run(saliency[0], feed_dict={\n                x: img,\n                layer: layer_vec,\n                \'vgg/dropout_1/random_uniform:0\': [[1.0]],\n                \'vgg/dropout/random_uniform:0\': [[1.0]]})\n            grad = this_res[0] / np.mean(np.abs(grad))\n            img = img[:, crop:-crop - 1, crop:-crop - 1, :]\n            img = imresize(img[0], (224, 224))[np.newaxis]\n            img += grad\n        plt.imshow(deprocess(img[0]))\n\nif __name__ == \'__main__\':\n    test_vgg_face()\n'"
session-4/tests/test_4.py,6,"b""import matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nfrom libs import utils\nfrom libs import dataset_utils\nfrom libs import vgg16, inception, i2v\nfrom libs import stylenet\n\n\ndef test_libraries():\n    import os\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from skimage.transform import resize\n    from skimage import data\n    from scipy.misc import imresize\n    from scipy.ndimage.filters import gaussian_filter\n    import IPython.display as ipyd\n    import tensorflow as tf\n    from libs import utils, gif, datasets, dataset_utils, vae, dft, vgg16, nb_utils\n\n\ndef test_vgg():\n    net = vgg16.get_vgg_model()\n    guide_og = plt.imread('clinton.png')[..., :3]\n    dream_og = plt.imread('arles.png')[..., :3]\n    guide_img = net['preprocess'](guide_og)[np.newaxis]\n    dream_img = net['preprocess'](dream_og)[np.newaxis]\n    assert(guide_img.shape == (1, 224, 224, 3))\n    assert(dream_img.shape == (1, 224, 224, 3))\n    assert(guide_img.dtype == np.dtype('float32'))\n    assert(guide_img.dtype == np.dtype('float32'))\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        tf.import_graph_def(net['graph_def'], name='net')\n        names = [op.name for op in g.get_operations()]\n        x = g.get_tensor_by_name(names[0] + ':0')\n        softmax = g.get_tensor_by_name(names[-2] + ':0')\n        res = softmax.eval(feed_dict={x: guide_img,\n                                      'net/dropout_1/random_uniform:0': [[1.0] * 4096],\n                                      'net/dropout/random_uniform:0': [[1.0] * 4096]})[0]\n        assert(np.argmax(res) == 681)\n        res = softmax.eval(feed_dict={x: dream_img,\n                                      'net/dropout_1/random_uniform:0': [[1.0] * 4096],\n                                      'net/dropout/random_uniform:0': [[1.0] * 4096]})[0]\n        assert(np.argmax(res) == 540)\n    return\n\n\ndef test_inception():\n    net = inception.get_inception_model(version='v5')\n    guide_og = plt.imread('clinton.png')[..., :3]\n    dream_og = plt.imread('arles.png')[..., :3]\n    guide_img = net['preprocess'](guide_og)[np.newaxis]\n    dream_img = net['preprocess'](dream_og)[np.newaxis]\n    assert(guide_img.shape == (1, 299, 299, 3))\n    assert(dream_img.shape == (1, 299, 299, 3))\n    assert(guide_img.dtype == np.dtype('float32'))\n    assert(guide_img.dtype == np.dtype('float32'))\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        tf.import_graph_def(net['graph_def'], name='net')\n        names = [op.name for op in g.get_operations()]\n        x = g.get_tensor_by_name(names[0] + ':0')\n        softmax = g.get_tensor_by_name(names[-1] + ':0')\n        res = softmax.eval(feed_dict={x: guide_img})\n        assert(np.argmax(res[0][:1000]) == 899)\n        res = softmax.eval(feed_dict={x: dream_img})\n        assert(np.argmax(res[0][:1000]) == 834)\n    return\n\n\ndef test_stylenet():\n    stylenet.test()\n\n\n#def test_stylenet_video():\n#    stylenet.test_video()\n"""
session-5/libs/__init__.py,0,b''
session-5/libs/batch_norm.py,13,"b'""""""Batch Normalization for TensorFlow.\nParag K. Mital, Jan 2016.\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef batch_norm(x, phase_train, name=\'bn\', decay=0.9, reuse=None,\n               affine=True):\n    """"""\n    Batch normalization on convolutional maps.\n    from: https://stackoverflow.com/questions/33949786/how-could-i-\n    use-batch-normalization-in-tensorflow\n    Only modified to infer shape from input tensor x.\n    Parameters\n    ----------\n    x\n        Tensor, 4D BHWD input maps\n    phase_train\n        boolean tf.Variable, true indicates training phase\n    name\n        string, variable name\n    affine\n        whether to affine-transform outputs\n    Return\n    ------\n    normed\n        batch-normalized maps\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        shape = x.get_shape().as_list()\n        beta = tf.get_variable(name=\'beta\', shape=[shape[-1]],\n                               initializer=tf.constant_initializer(0.0),\n                               trainable=True)\n        gamma = tf.get_variable(name=\'gamma\', shape=[shape[-1]],\n                                initializer=tf.constant_initializer(1.0),\n                                trainable=affine)\n        if len(shape) == 4:\n            batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name=\'moments\')\n        else:\n            batch_mean, batch_var = tf.nn.moments(x, [0], name=\'moments\')\n        ema = tf.train.ExponentialMovingAverage(decay=decay)\n        ema_apply_op = ema.apply([batch_mean, batch_var])\n        ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n\n        def mean_var_with_update():\n            """"""Summary\n            Returns\n            -------\n            name : TYPE\n                Description\n            """"""\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n        mean, var = control_flow_ops.cond(phase_train,\n                                          mean_var_with_update,\n                                          lambda: (ema_mean, ema_var))\n\n        # tf.nn.batch_normalization\n        normed = tf.nn.batch_norm_with_global_normalization(\n            x, mean, var, beta, gamma, 1e-6, affine)\n    return normed\n'"
session-5/libs/celeb_vaegan.py,2,"b'""""""\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.platform import gfile\nfrom .utils import download\nfrom skimage.transform import resize as imresize\n\n\ndef celeb_vaegan_download():\n    """"""Download a pretrained celeb vae/gan network.""""""\n\n    # Load the model and labels\n    model = download(\'https://s3.amazonaws.com/cadl/models/celeb.vaegan.tfmodel\')\n    labels = download(\'https://s3.amazonaws.com/cadl/celeb-align/list_attr_celeba.txt\')\n    return model, labels\n\n\ndef get_celeb_vaegan_model():\n    """"""Get a pretrained model.\n\n    Returns\n    -------\n    net : dict\n        {\n            \'graph_def\': tf.GraphDef\n                The graph definition\n            \'labels\': list\n                List of different possible attributes from celeb\n            \'attributes\': np.ndarray\n                One hot encoding of the attributes per image\n                [n_els x n_labels]\n            \'preprocess\': function\n                Preprocess function\n        }\n    """"""\n    # Download the trained net\n    model, labels = celeb_vaegan_download()\n\n    # Parse the ids and synsets\n    txt = open(labels).readlines()\n    n_els = int(txt[0].strip())\n    labels = txt[1].strip().split()\n    n_labels = len(labels)\n    attributes = np.zeros((n_els, n_labels), dtype=bool)\n    for i, txt_i in enumerate(txt[2:]):\n        attributes[i] = (np.array(txt_i.strip().split()[1:]).astype(int) > 0)\n\n    # Load the saved graph\n    with gfile.GFile(model, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n    net = {\n        \'graph_def\': graph_def,\n        \'labels\': labels,\n        \'attributes\': attributes,\n        \'preprocess\': preprocess,\n    }\n    return net\n\n\ndef preprocess(img, crop_factor=0.8):\n    """"""Replicate the preprocessing we did on the VAE/GAN.\n\n    This model used a crop_factor of 0.8 and crop size of [100, 100, 3].\n    """"""\n    crop = np.min(img.shape[:2])\n    r = (img.shape[0] - crop) // 2\n    c = (img.shape[1] - crop) // 2\n    cropped = img[r: r + crop, c: c + crop]\n    r, c, *d = cropped.shape\n    if crop_factor < 1.0:\n        amt = (1 - crop_factor) / 2\n        h, w = int(c * amt), int(r * amt)\n        cropped = cropped[h:-h, w:-w]\n    rsz = imresize(cropped, (100, 100), preserve_range=False)\n    return rsz\n'"
session-5/libs/charrnn.py,43,"b'""""""Character-level Recurrent Neural Network.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport sys\nimport collections\nimport gzip\nfrom libs import utils\n\n\ndef build_model(txt,\n                batch_size=1,\n                sequence_length=1,\n                n_layers=2,\n                n_cells=100,\n                gradient_clip=10.0,\n                learning_rate=0.001):\n    """"""Summary\n\n    Parameters\n    ----------\n    txt : TYPE\n        Description\n    batch_size : int, optional\n        Description\n    sequence_length : int, optional\n        Description\n    n_layers : int, optional\n        Description\n    n_cells : int, optional\n        Description\n    gradient_clip : float, optional\n        Description\n    learning_rate : float, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    vocab = list(set(txt))\n    vocab.sort()\n    n_chars = len(vocab)\n    encoder = collections.OrderedDict(zip(vocab, range(n_chars)))\n    decoder = collections.OrderedDict(zip(range(n_chars), vocab))\n\n    X = tf.placeholder(tf.int32, [None, sequence_length], name=\'X\')\n    Y = tf.placeholder(tf.int32, [None, sequence_length], name=\'Y\')\n    keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n\n    with tf.variable_scope(\'embedding\'):\n        embedding = tf.get_variable(""embedding"", [n_chars, n_cells])\n        # Each sequence element will be connected to n_cells\n        Xs = tf.nn.embedding_lookup(embedding, X)\n        # Then slice each sequence element, giving us sequence number of\n        # batch x 1 x n_chars Tensors\n        Xs = tf.split(axis=1, num_or_size_splits=sequence_length, value=Xs)\n        # Get rid of singleton sequence element dimension\n        Xs = [tf.squeeze(X_i, [1]) for X_i in Xs]\n\n    with tf.variable_scope(\'rnn\'):\n        cells = tf.contrib.rnn.MultiRNNCell([\n            tf.contrib.rnn.DropoutWrapper(\n                tf.contrib.rnn.BasicLSTMCell(\n                    num_units=n_cells, forget_bias=0.0, state_is_tuple=True),\n                output_keep_prob=keep_prob) for _ in range(n_layers)\n        ])\n        initial_state = cells.zero_state(tf.shape(X)[0], tf.float32)\n        # returns a length sequence length list of outputs, one for each input\n        outputs, final_state = tf.contrib.rnn.static_rnn(\n            cells, Xs, initial_state=initial_state)\n        # now concat the sequence length number of batch x n_cells Tensors to\n        # give [sequence_length x batch, n_cells]\n        outputs_flat = tf.reshape(\n            tf.concat(axis=1, values=outputs), [-1, n_cells])\n\n    with tf.variable_scope(\'prediction\'):\n        W = tf.get_variable(\n            ""W"",\n            shape=[n_cells, n_chars],\n            initializer=tf.contrib.layers.xavier_initializer())\n        b = tf.get_variable(\n            ""b"", shape=[n_chars], initializer=tf.constant_initializer())\n        logits = tf.matmul(outputs_flat, W) + b\n        probs = tf.nn.softmax(logits)\n        Y_pred = tf.argmax(probs, 1)\n\n    with tf.variable_scope(\'loss\'):\n        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [\n            tf.reshape(tf.concat(axis=1, values=Y), [-1])\n        ], [tf.ones([batch_size * sequence_length])])\n        cost = tf.reduce_sum(loss) / batch_size\n\n    with tf.name_scope(\'optimizer\'):\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n        gradients = []\n        clip = tf.constant(gradient_clip, name=""clip"")\n        for grad, var in optimizer.compute_gradients(cost):\n            gradients.append((tf.clip_by_value(grad, -clip, clip), var))\n        updates = optimizer.apply_gradients(gradients)\n\n    model = {\n        \'X\': X,\n        \'Y\': Y,\n        \'logits\': logits,\n        \'probs\': probs,\n        \'Y_pred\': Y_pred,\n        \'keep_prob\': keep_prob,\n        \'cost\': cost,\n        \'updates\': updates,\n        \'initial_state\': initial_state,\n        \'final_state\': final_state,\n        \'decoder\': decoder,\n        \'encoder\': encoder,\n        \'vocab_size\': n_chars\n    }\n    return model\n\n\ndef train(txt,\n          batch_size=100,\n          sequence_length=150,\n          n_cells=200,\n          n_layers=3,\n          learning_rate=0.00001,\n          max_iter=50000,\n          gradient_clip=5.0,\n          ckpt_name=""model.ckpt"",\n          keep_prob=1.0):\n    """"""train\n\n    Parameters\n    ----------\n    txt : TYPE\n        Description\n    batch_size : int, optional\n        Description\n    sequence_length : int, optional\n        Description\n    n_cells : int, optional\n        Description\n    n_layers : int, optional\n        Description\n    learning_rate : float, optional\n        Description\n    max_iter : int, optional\n        Description\n    gradient_clip : float, optional\n        Description\n    ckpt_name : str, optional\n        Description\n    keep_prob : float, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        model = build_model(\n            txt=txt,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            n_layers=n_layers,\n            n_cells=n_cells,\n            gradient_clip=gradient_clip,\n            learning_rate=learning_rate)\n\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        saver = tf.train.Saver()\n        sess.run(init_op)\n        if os.path.exists(ckpt_name + \'.index\') or os.path.exists(ckpt_name):\n            saver.restore(sess, ckpt_name)\n            print(""Model restored."")\n\n        cursor = 0\n        it_i = 0\n        print_step = 1000\n        avg_cost = 0\n        while it_i < max_iter:\n            Xs, Ys = [], []\n            for batch_i in range(batch_size):\n                Xs.append([\n                    model[\'encoder\'][ch]\n                    for ch in txt[cursor:cursor + sequence_length]\n                ])\n                Ys.append([\n                    model[\'encoder\'][ch]\n                    for ch in txt[cursor + 1:cursor + sequence_length + 1]\n                ])\n                cursor += sequence_length\n                if (cursor + 1) >= len(txt) - sequence_length - 1:\n                    cursor = np.random.randint(0, high=sequence_length)\n\n            feed_dict = {\n                model[\'X\']: Xs,\n                model[\'Y\']: Ys,\n                model[\'keep_prob\']: keep_prob\n            }\n            out = sess.run(\n                [model[\'cost\'], model[\'updates\']], feed_dict=feed_dict)\n            avg_cost += out[0]\n\n            if (it_i + 1) % print_step == 0:\n                p = sess.run(\n                    model[\'probs\'],\n                    feed_dict={\n                        model[\'X\']: np.array(Xs[-1])[np.newaxis],\n                        model[\'keep_prob\']: 1.0\n                    })\n                print(p.shape, \'min:\',\n                      np.min(p), \'max:\',\n                      np.max(p), \'mean:\', np.mean(p), \'std:\', np.std(p))\n                if isinstance(txt[0], str):\n                    # Print original string\n                    print(\'original:\',\n                          """".join([model[\'decoder\'][ch] for ch in Xs[-1]]))\n\n                    # Print max guess\n                    amax = []\n                    for p_i in p:\n                        amax.append(model[\'decoder\'][np.argmax(p_i)])\n                    print(\'synth(amax):\', """".join(amax))\n\n                    # Print w/ sampling\n                    samp = []\n                    for p_i in p:\n                        p_i = p_i.astype(np.float64)\n                        p_i = p_i / p_i.sum()\n                        idx = np.argmax(np.random.multinomial(1, p_i.ravel()))\n                        samp.append(model[\'decoder\'][idx])\n                    print(\'synth(samp):\', """".join(samp))\n\n                print(it_i, avg_cost / print_step)\n                avg_cost = 0\n\n                save_path = saver.save(sess, ckpt_name, global_step=it_i)\n                print(""Model saved in file: %s"" % save_path)\n\n            print(it_i, out[0], end=\'\\r\')\n            it_i += 1\n\n        return model\n\n\ndef infer(txt,\n          ckpt_name,\n          n_iterations,\n          n_cells=200,\n          n_layers=3,\n          learning_rate=0.001,\n          max_iter=5000,\n          gradient_clip=10.0,\n          init_value=[0],\n          keep_prob=1.0,\n          sampling=\'prob\',\n          temperature=1.0):\n    """"""infer\n\n    Parameters\n    ----------\n    txt : TYPE\n        Description\n    ckpt_name : TYPE\n        Description\n    n_iterations : TYPE\n        Description\n    n_cells : int, optional\n        Description\n    n_layers : int, optional\n        Description\n    learning_rate : float, optional\n        Description\n    max_iter : int, optional\n        Description\n    gradient_clip : float, optional\n        Description\n    init_value : list, optional\n        Description\n    keep_prob : float, optional\n        Description\n    sampling : str, optional\n        Description\n    temperature : float, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        sequence_length = len(init_value)\n        model = build_model(\n            txt=txt,\n            batch_size=1,\n            sequence_length=sequence_length,\n            n_layers=n_layers,\n            n_cells=n_cells,\n            gradient_clip=gradient_clip,\n            learning_rate=learning_rate)\n\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        saver = tf.train.Saver()\n        sess.run(init_op)\n        saver.restore(sess, ckpt_name)\n        print(""Model restored."")\n\n        state = []\n        synth = [init_value]\n        for s_i in model[\'final_state\']:\n            state += sess.run(\n                [s_i.c, s_i.h],\n                feed_dict={\n                    model[\'X\']: [synth[-1]],\n                    model[\'keep_prob\']: keep_prob\n                })\n\n        for i in range(n_iterations):\n            # print(\'iteration: {}/{}\'.format(i, n_iterations), end=\'\\r\')\n            feed_dict = {model[\'X\']: [synth[-1]], model[\'keep_prob\']: keep_prob}\n            state_updates = []\n            for state_i in range(n_layers):\n                feed_dict[model[\'initial_state\'][state_i].c] = \\\n                    state[state_i * 2]\n                feed_dict[model[\'initial_state\'][state_i].h] = state[state_i * 2\n                                                                     + 1]\n                state_updates.append(model[\'final_state\'][state_i].c)\n                state_updates.append(model[\'final_state\'][state_i].h)\n            p = sess.run(model[\'probs\'], feed_dict=feed_dict)[0]\n            if sampling == \'max\':\n                p = np.argmax(p)\n            else:\n                p = p.astype(np.float64)\n                p = np.log(p) / temperature\n                p = np.exp(p) / np.sum(np.exp(p))\n                p = np.random.multinomial(1, p.ravel())\n                p = np.argmax(p)\n            # Get the current state\n            state = [\n                sess.run(s_i, feed_dict=feed_dict) for s_i in state_updates\n            ]\n            synth.append([p])\n            print(model[\'decoder\'][p], end=\'\')\n            sys.stdout.flush()\n            if model[\'decoder\'][p] in [\'.\', \'?\', \'!\']:\n                print(\'\\n\')\n        print(np.concatenate(synth).shape)\n    print("""".join([model[\'decoder\'][ch] for ch in np.concatenate(synth)]))\n    return [model[\'decoder\'][ch] for ch in np.concatenate(synth)]\n\n\ndef test_alice(max_iter=5):\n    """"""Summary\n\n    Parameters\n    ----------\n    max_iter : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    utils.download(\'https://s3.amazonaws.com/cadl/models/alice.txt.gz\')\n    with gzip.open(\'alice.txt.gz\', \'rb\') as fp:\n        txt = fp.read().decode(\'utf-8\')\n    return train(txt, n_layers=2, n_cells=20, max_iter=max_iter)\n\n\ndef test_trump(max_iter=100):\n    """"""Summary\n\n    Parameters\n    ----------\n    max_iter : int, optional\n        Description\n    """"""\n    utils.download(\n        \'https://s3.amazonaws.com/cadl/models/trump.ckpt.data-00000-of-00001\')\n    utils.download(\'https://s3.amazonaws.com/cadl/models/trump.ckpt.meta\')\n    utils.download(\'https://s3.amazonaws.com/cadl/models/trump.ckpt.index\')\n    utils.download(\'https://s3.amazonaws.com/cadl/models/trump.txt\')\n    with open(\'trump.txt\', \'r\') as fp:\n        txt = fp.read()\n    #train(txt, ckpt_name=\'trump\', max_iter=max_iter)\n    print(infer(txt, ckpt_name=\'./trump.ckpt\', n_iterations=max_iter))\n\n\ndef test_wtc():\n    """"""Summary\n    """"""\n    from scipy.io.wavfile import write, read\n    rate, aud = read(\'wtc.wav\')\n    txt = np.int8(np.round(aud / 16384.0 * 128.0))\n    txt = np.squeeze(txt).tolist()\n    # try with more than 100 iterations, e.g. 50k - 200k\n    train(txt, sequence_length=250, n_layers=3, n_cells=512, max_iter=100)\n    synthesis = infer(\n        txt,\n        \'./model.ckpt\',\n        8000 * 30,\n        n_layers=3,\n        n_cells=150,\n        keep_prob=1.0,\n        sampling=\'prob\')\n    snd = np.int16(np.array(synthesis) / 128.0 * 16384.0)\n    write(\'wtc-synth.wav\', 8000, snd)\n\n\nif __name__ == \'__main__\':\n    test_alice()\n'"
session-5/libs/dataset_utils.py,8,"b'""""""Utils for dataset creation.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\n\nimport os\nimport pickle\nimport numpy as np\nimport tensorflow as tf\nfrom . import dft\nfrom .utils import download_and_extract_tar\n\n\ndef create_input_pipeline(files, batch_size, n_epochs, shape, crop_shape=None,\n                          crop_factor=1.0, n_threads=2):\n    """"""Creates a pipefile from a list of image files.\n    Includes batch generator/central crop/resizing options.\n    The resulting generator will dequeue the images batch_size at a time until\n    it throws tf.errors.OutOfRangeError when there are no more images left in\n    the queue.\n\n    Parameters\n    ----------\n    files : list\n        List of paths to image files.\n    batch_size : int\n        Number of image files to load at a time.\n    n_epochs : int\n        Number of epochs to run before raising tf.errors.OutOfRangeError\n    shape : list\n        [height, width, channels]\n    crop_shape : list\n        [height, width] to crop image to.\n    crop_factor : float\n        Percentage of image to take starting from center.\n    n_threads : int, optional\n        Number of threads to use for batch shuffling\n    """"""\n\n    # We first create a ""producer"" queue.  It creates a production line which\n    # will queue up the file names and allow another queue to deque the file\n    # names all using a tf queue runner.\n    # Put simply, this is the entry point of the computational graph.\n    # It will generate the list of file names.\n    # We also specify it\'s capacity beforehand.\n    producer = tf.train.string_input_producer(\n        files, capacity=len(files))\n\n    # We need something which can open the files and read its contents.\n    reader = tf.WholeFileReader()\n\n    # We pass the filenames to this object which can read the file\'s contents.\n    # This will create another queue running which dequeues the previous queue.\n    keys, vals = reader.read(producer)\n\n    # And then have to decode its contents as we know it is a jpeg image\n    imgs = tf.image.decode_jpeg(\n        vals,\n        channels=3 if len(shape) > 2 and shape[2] == 3 else 0)\n\n    # We have to explicitly define the shape of the tensor.\n    # This is because the decode_jpeg operation is still a node in the graph\n    # and doesn\'t yet know the shape of the image.  Future operations however\n    # need explicit knowledge of the image\'s shape in order to be created.\n    imgs.set_shape(shape)\n\n    # Next we\'ll centrally crop the image to the size of 100x100.\n    # This operation required explicit knowledge of the image\'s shape.\n    if shape[0] > shape[1]:\n        rsz_shape = [int(shape[0] / shape[1] * crop_shape[0] / crop_factor),\n                     int(crop_shape[1] / crop_factor)]\n    else:\n        rsz_shape = [int(crop_shape[0] / crop_factor),\n                     int(shape[1] / shape[0] * crop_shape[1] / crop_factor)]\n    rszs = tf.image.resize_images(imgs, rsz_shape)\n    crops = (tf.image.resize_image_with_crop_or_pad(\n        rszs, crop_shape[0], crop_shape[1])\n        if crop_shape is not None\n        else imgs)\n\n    # Now we\'ll create a batch generator that will also shuffle our examples.\n    # We tell it how many it should have in its buffer when it randomly\n    # permutes the order.\n    min_after_dequeue = len(files) // 100\n\n    # The capacity should be larger than min_after_dequeue, and determines how\n    # many examples are prefetched.  TF docs recommend setting this value to:\n    # min_after_dequeue + (num_threads + a small safety margin) * batch_size\n    capacity = min_after_dequeue + (n_threads + 1) * batch_size\n\n    # Randomize the order and output batches of batch_size.\n    batch = tf.train.shuffle_batch([crops],\n                                   enqueue_many=False,\n                                   batch_size=batch_size,\n                                   capacity=capacity,\n                                   min_after_dequeue=min_after_dequeue,\n                                   num_threads=n_threads)\n\n    # alternatively, we could use shuffle_batch_join to use multiple reader\n    # instances, or set shuffle_batch\'s n_threads to higher than 1.\n\n    return batch\n\n\ndef gtzan_music_speech_download(dst=\'gtzan_music_speech\'):\n    """"""Download the GTZAN music and speech dataset.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Location to put the GTZAN music and speech datset.\n    """"""\n    path = \'http://opihi.cs.uvic.ca/sound/music_speech.tar.gz\'\n    download_and_extract_tar(path, dst)\n\n\ndef gtzan_music_speech_load(dst=\'gtzan_music_speech\'):\n    """"""Load the GTZAN Music and Speech dataset.\n\n    Downloads the dataset if it does not exist into the dst directory.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Location of GTZAN Music and Speech dataset.\n\n    Returns\n    -------\n    Xs, ys : np.ndarray, np.ndarray\n        Array of data, Array of labels\n    """"""\n    from scipy.io import wavfile\n\n    if not os.path.exists(dst):\n        gtzan_music_speech_download(dst)\n    music_dir = os.path.join(os.path.join(dst, \'music_speech\'), \'music_wav\')\n    music = [os.path.join(music_dir, file_i)\n             for file_i in os.listdir(music_dir)\n             if file_i.endswith(\'.wav\')]\n    speech_dir = os.path.join(os.path.join(dst, \'music_speech\'), \'speech_wav\')\n    speech = [os.path.join(speech_dir, file_i)\n              for file_i in os.listdir(speech_dir)\n              if file_i.endswith(\'.wav\')]\n    Xs = []\n    ys = []\n    for i in music:\n        sr, s = wavfile.read(i)\n        s = s / 16384.0 - 1.0\n        re, im = dft.dft_np(s)\n        mag, phs = dft.ztoc(re, im)\n        Xs.append((mag, phs))\n        ys.append(0)\n    for i in speech:\n        sr, s = wavfile.read(i)\n        s = s / 16384.0 - 1.0\n        re, im = dft.dft_np(s)\n        mag, phs = dft.ztoc(re, im)\n        Xs.append((mag, phs))\n        ys.append(1)\n    Xs = np.array(Xs)\n    Xs = np.transpose(Xs, [0, 2, 3, 1])\n    ys = np.array(ys)\n    return Xs, ys\n\n\ndef cifar10_download(dst=\'cifar10\'):\n    """"""Download the CIFAR10 dataset.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Directory to download into.\n    """"""\n    path = \'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'\n    download_and_extract_tar(path, dst)\n\n\ndef cifar10_load(dst=\'cifar10\'):\n    """"""Load the CIFAR10 dataset.\n\n    Downloads the dataset if it does not exist into the dst directory.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Location of CIFAR10 dataset.\n\n    Returns\n    -------\n    Xs, ys : np.ndarray, np.ndarray\n        Array of data, Array of labels\n    """"""\n    if not os.path.exists(dst):\n        cifar10_download(dst)\n    Xs = None\n    ys = None\n    for f in range(1, 6):\n        cf = pickle.load(open(\n            \'%s/cifar-10-batches-py/data_batch_%d\' % (dst, f), \'rb\'),\n            encoding=\'LATIN\')\n        if Xs is not None:\n            Xs = np.r_[Xs, cf[\'data\']]\n            ys = np.r_[ys, np.array(cf[\'labels\'])]\n        else:\n            Xs = cf[\'data\']\n            ys = cf[\'labels\']\n    Xs = np.swapaxes(np.swapaxes(Xs.reshape(-1, 3, 32, 32), 1, 3), 1, 2)\n    return Xs, ys\n\n\ndef dense_to_one_hot(labels, n_classes=2):\n    """"""Convert class labels from scalars to one-hot vectors.\n\n    Parameters\n    ----------\n    labels : array\n        Input labels to convert to one-hot representation.\n    n_classes : int, optional\n        Number of possible one-hot.\n\n    Returns\n    -------\n    one_hot : array\n        One hot representation of input.\n    """"""\n    return np.eye(n_classes).astype(np.float32)[labels]\n\n\nclass DatasetSplit(object):\n    """"""Utility class for batching data and handling multiple splits.\n\n    Attributes\n    ----------\n    current_batch_idx : int\n        Description\n    images : np.ndarray\n        Xs of the dataset.  Not necessarily images.\n    labels : np.ndarray\n        ys of the dataset.\n    n_labels : int\n        Number of possible labels\n    num_examples : int\n        Number of total observations\n    """"""\n\n    def __init__(self, images, labels):\n        """"""Initialize a DatasetSplit object.\n\n        Parameters\n        ----------\n        images : np.ndarray\n            Xs/inputs\n        labels : np.ndarray\n            ys/outputs\n        """"""\n        self.images = np.array(images).astype(np.float32)\n        if labels is not None:\n            self.labels = np.array(labels).astype(np.int32)\n            self.n_labels = len(np.unique(labels))\n        else:\n            self.labels = None\n        self.num_examples = len(self.images)\n\n    def next_batch(self, batch_size=100):\n        """"""Batch generator with randomization.\n\n        Parameters\n        ----------\n        batch_size : int, optional\n            Size of each minibatch.\n\n        Returns\n        -------\n        Xs, ys : np.ndarray, np.ndarray\n            Next batch of inputs and labels (if no labels, then None).\n        """"""\n        # Shuffle each epoch\n        current_permutation = np.random.permutation(range(len(self.images)))\n        epoch_images = self.images[current_permutation, ...]\n        if self.labels is not None:\n            epoch_labels = self.labels[current_permutation, ...]\n\n        # Then iterate over the epoch\n        self.current_batch_idx = 0\n        while self.current_batch_idx < len(self.images):\n            end_idx = min(\n                self.current_batch_idx + batch_size, len(self.images))\n            this_batch = {\n                \'images\': epoch_images[self.current_batch_idx:end_idx],\n                \'labels\': epoch_labels[self.current_batch_idx:end_idx]\n                if self.labels is not None else None\n            }\n            self.current_batch_idx += batch_size\n            yield this_batch[\'images\'], this_batch[\'labels\']\n\n\nclass Dataset(object):\n    """"""Create a dataset from data and their labels.\n\n    Allows easy use of train/valid/test splits; Batch generator.\n\n    Attributes\n    ----------\n    all_idxs : list\n        All indexes across all splits.\n    all_inputs : list\n        All inputs across all splits.\n    all_labels : list\n        All labels across all splits.\n    n_labels : int\n        Number of labels.\n    split : list\n        Percentage split of train, valid, test sets.\n    test_idxs : list\n        Indexes of the test split.\n    train_idxs : list\n        Indexes of the train split.\n    valid_idxs : list\n        Indexes of the valid split.\n    """"""\n\n    def __init__(self, Xs, ys=None, split=[1.0, 0.0, 0.0], one_hot=False):\n        """"""Initialize a Dataset object.\n\n        Parameters\n        ----------\n        Xs : np.ndarray\n            Images/inputs to a network\n        ys : np.ndarray\n            Labels/outputs to a network\n        split : list, optional\n            Percentage of train, valid, and test sets.\n        one_hot : bool, optional\n            Whether or not to use one-hot encoding of labels (ys).\n        """"""\n        self.all_idxs = []\n        self.all_labels = []\n        self.all_inputs = []\n        self.train_idxs = []\n        self.valid_idxs = []\n        self.test_idxs = []\n        self.n_labels = 0\n        self.split = split\n\n        # Now mix all the labels that are currently stored as blocks\n        self.all_inputs = Xs\n        n_idxs = len(self.all_inputs)\n        idxs = range(n_idxs)\n        rand_idxs = np.random.permutation(idxs)\n        self.all_inputs = self.all_inputs[rand_idxs, ...]\n        if ys is not None:\n            self.all_labels = ys if not one_hot else dense_to_one_hot(ys)\n            self.all_labels = self.all_labels[rand_idxs, ...]\n        else:\n            self.all_labels = None\n\n        # Get splits\n        self.train_idxs = idxs[:round(split[0] * n_idxs)]\n        self.valid_idxs = idxs[len(self.train_idxs):\n                               len(self.train_idxs) + round(split[1] * n_idxs)]\n        self.test_idxs = idxs[\n            (len(self.valid_idxs) + len(self.train_idxs)):\n            (len(self.valid_idxs) + len(self.train_idxs)) +\n             round(split[2] * n_idxs)]\n\n    @property\n    def X(self):\n        """"""Inputs/Xs/Images.\n\n        Returns\n        -------\n        all_inputs : np.ndarray\n            Original Inputs/Xs.\n        """"""\n        return self.all_inputs\n\n    @property\n    def Y(self):\n        """"""Outputs/ys/Labels.\n\n        Returns\n        -------\n        all_labels : np.ndarray\n            Original Outputs/ys.\n        """"""\n        return self.all_labels\n\n    @property\n    def train(self):\n        """"""Train split.\n\n        Returns\n        -------\n        split : DatasetSplit\n            Split of the train dataset.\n        """"""\n        if len(self.train_idxs):\n            inputs = self.all_inputs[self.train_idxs, ...]\n            if self.all_labels is not None:\n                labels = self.all_labels[self.train_idxs, ...]\n            else:\n                labels = None\n        else:\n            inputs, labels = [], []\n        return DatasetSplit(inputs, labels)\n\n    @property\n    def valid(self):\n        """"""Validation split.\n\n        Returns\n        -------\n        split : DatasetSplit\n            Split of the validation dataset.\n        """"""\n        if len(self.valid_idxs):\n            inputs = self.all_inputs[self.valid_idxs, ...]\n            if self.all_labels is not None:\n                labels = self.all_labels[self.valid_idxs, ...]\n            else:\n                labels = None\n        else:\n            inputs, labels = [], []\n        return DatasetSplit(inputs, labels)\n\n    @property\n    def test(self):\n        """"""Test split.\n\n        Returns\n        -------\n        split : DatasetSplit\n            Split of the test dataset.\n        """"""\n        if len(self.test_idxs):\n            inputs = self.all_inputs[self.test_idxs, ...]\n            if self.all_labels is not None:\n                labels = self.all_labels[self.test_idxs, ...]\n            else:\n                labels = None\n        else:\n            inputs, labels = [], []\n        return DatasetSplit(inputs, labels)\n\n    def mean(self):\n        """"""Mean of the inputs/Xs.\n\n        Returns\n        -------\n        mean : np.ndarray\n            Calculates mean across 0th (batch) dimension.\n        """"""\n        return np.mean(self.all_inputs, axis=0)\n\n    def std(self):\n        """"""Standard deviation of the inputs/Xs.\n\n        Returns\n        -------\n        std : np.ndarray\n            Calculates std across 0th (batch) dimension.\n        """"""\n        return np.std(self.all_inputs, axis=0)\n'"
session-5/libs/datasets.py,0,"b'""""""Creative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport tensorflow.examples.tutorials.mnist.input_data as input_data\nfrom .dataset_utils import *\n\n\ndef MNIST(one_hot=True, split=[1.0, 0.0, 0.0]):\n    """"""Returns the MNIST dataset.\n\n    Returns\n    -------\n    mnist : DataSet\n        DataSet object w/ convenienve props for accessing\n        train/validation/test sets and batches.\n    """"""\n    ds = input_data.read_data_sets(\'MNIST_data/\', one_hot=one_hot)\n    return Dataset(np.r_[ds.train.images,\n                         ds.validation.images,\n                         ds.test.images],\n                   np.r_[ds.train.labels,\n                         ds.validation.labels,\n                         ds.test.labels],\n                   split=split)\n\n\ndef CIFAR10(flatten=True, split=[1.0, 0.0, 0.0]):\n    """"""Returns the CIFAR10 dataset.\n\n    Parameters\n    ----------\n    flatten : bool, optional\n        Convert the 3 x 32 x 32 pixels to a single vector\n\n    Returns\n    -------\n    cifar : Dataset\n        Description\n    """"""\n    # plt.imshow(np.transpose(np.reshape(\n    #   cifar.train.images[10], (3, 32, 32)), [1, 2, 0]))\n    Xs, ys = cifar10_load()\n    if flatten:\n        Xs = Xs.reshape((Xs.shape[0], -1))\n    return Dataset(Xs, ys, split=split)\n\n\ndef CELEB(path=\'./img_align_celeba/\'):\n    """"""Attempt to load the files of the CELEB dataset.\n\n    Requires the files already be downloaded and placed in the `dst` directory.\n\n    http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n\n    Parameters\n    ----------\n    path : str, optional\n        Directory where the aligned/cropped celeb dataset can be found.\n\n    Returns\n    -------\n    files : list\n        List of file paths to the dataset.\n    """"""\n    if not os.path.exists(path):\n        print(\'Could not find celeb dataset under {}.\'.format(path))\n        print(\'Try downloading the dataset from the ""Aligned and Cropped"" \' +\n              \'link located here (imgs/img_align_celeba.zip [1.34 GB]): \' +\n              \'http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\')\n        return None\n    else:\n        fs = sorted([os.path.join(path, f)\n                     for f in os.listdir(path)\n                     if f.endswith(\'.jpg\')])\n        if len(fs) < 202598:\n            print(\'It does not look like you have downloaded the entire \' +\n                  \'Celeb Dataset.\\n\' +\n                  \'Try downloading the dataset from the ""Aligned and Cropped"" \' +\n                  \'link located here (imgs/img_align_celeba.zip [1.34 GB]): \' +\n                  \'http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\')\n        return fs\n'"
session-5/libs/deepdream.py,25,"b'""""""Deep Dream using the Inception v5 network.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom scipy.ndimage.filters import gaussian_filter\nfrom skimage.transform import resize\nfrom scipy.misc import imsave\nfrom . import inception, vgg16, i2v\nfrom . import gif\n\n\ndef get_labels(model=\'inception\'):\n    """"""Return labels corresponding to the `neuron_i` parameter of deep dream.\n\n    Parameters\n    ----------\n    model : str, optional\n        Which model to load. Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n\n    Raises\n    ------\n    ValueError\n        Unknown model.  Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n    """"""\n    if model == \'inception\':\n        net = inception.get_inception_model()\n        return net[\'labels\']\n    elif model == \'i2v_tag\':\n        net = i2v.get_i2v_tag_model()\n        return net[\'labels\']\n    elif model == \'vgg16\':\n        net = vgg16.get_vgg_model()\n        return net[\'labels\']\n    elif model == \'vgg_face\':\n        net = vgg16.get_vgg_face_model()\n        return net[\'labels\']\n    else:\n        raise ValueError(""Unknown model or this model does not have labels!"")\n\n\ndef get_layer_names(model=\'inception\'):\n    """"""Retun every layer\'s index and name in the given model.\n\n    Parameters\n    ----------\n    model : str, optional\n        Which model to load. Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n\n    Returns\n    -------\n    names : list of tuples\n        The index and layer\'s name for every layer in the given model.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        if model == \'inception\':\n            net = inception.get_inception_model()\n        elif model == \'vgg_face\':\n            net = vgg16.get_vgg_face_model()\n        elif model == \'vgg16\':\n            net = vgg16.get_vgg_model()\n        elif model == \'i2v\':\n            net = i2v.get_i2v_model()\n        elif model == \'i2v-tag\':\n            net = i2v.get_i2v_tag_model()\n\n        tf.import_graph_def(net[\'graph_def\'], name=\'net\')\n        names = [(i, op.name) for i, op in enumerate(g.get_operations())]\n        return names\n\n\ndef _setup(input_img, model, downsize):\n    """"""Internal use only. Load the given model\'s graph and preprocess an image.\n\n    Parameters\n    ----------\n    input_img : np.ndarray\n        Image to process with the model\'s normalizaiton process.\n    model : str\n        Which model to load. Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n    downsize : bool\n        Optionally crop/resize the input image to the standard shape.  Only\n        applies to inception network which is all convolutional.\n\n    Returns\n    -------\n    net, img, preprocess, deprocess : dict, np.ndarray, function, function\n        net : The networks graph_def and labels\n        img : The preprocessed input image\n        preprocess: Function for preprocessing an image\n        deprocess: Function for deprocessing an image\n\n    Raises\n    ------\n    ValueError\n        If model is unknown.\n    """"""\n    if model == \'inception\':\n        net = inception.get_inception_model()\n        img = inception.preprocess(input_img, resize=downsize, crop=downsize)[np.newaxis]\n        deprocess, preprocess = inception.deprocess, inception.preprocess\n    elif model == \'vgg_face\':\n        net = vgg16.get_vgg_face_model()\n        img = vgg16.preprocess(input_img)[np.newaxis]\n        deprocess, preprocess = vgg16.deprocess, vgg16.preprocess\n    elif model == \'vgg16\':\n        net = vgg16.get_vgg_model()\n        img = vgg16.preprocess(input_img)[np.newaxis]\n        deprocess, preprocess = vgg16.deprocess, vgg16.preprocess\n    elif model == \'i2v\':\n        net = i2v.get_i2v_model()\n        img = i2v.preprocess(input_img)[np.newaxis]\n        deprocess, preprocess = i2v.deprocess, i2v.preprocess\n    elif model == \'i2v_tag\':\n        net = i2v.get_i2v_tag_model()\n        img = i2v.preprocess(input_img)[np.newaxis]\n        deprocess, preprocess = i2v.deprocess, i2v.preprocess\n    else:\n        raise ValueError(\n            ""Unknown model name!  Supported: "" +\n            ""[\'inception\', \'vgg_face\', \'vgg16\', \'i2v\', \'i2v_tag\']"")\n\n    return net, img, preprocess, deprocess\n\n\ndef _apply(img,\n           gradient,\n           it_i,\n           decay=0.998,\n           sigma=1.5,\n           blur_step=10,\n           step=1.0,\n           crop=0,\n           crop_step=1,\n           pth=0):\n    """"""Interal use only. Apply the gradient to an image with the given params.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        Tensor to apply gradient ascent to.\n    gradient : np.ndarray\n        Gradient to ascend to.\n    it_i : int\n        Current iteration (used for step modulos)\n    decay : float, optional\n        Amount to decay.\n    sigma : float, optional\n        Sigma for Gaussian Kernel.\n    blur_step : int, optional\n        How often to blur.\n    step : float, optional\n        Step for gradient ascent.\n    crop : int, optional\n        Amount to crop from each border.\n    crop_step : int, optional\n        How often to crop.\n    pth : int, optional\n        Percentile to mask out.\n\n    Returns\n    -------\n    img : np.ndarray\n        Ascended image.\n    """"""\n    gradient /= (np.std(gradient) + 1e-10)\n    img += gradient * step\n    img *= decay\n\n    if pth:\n        mask = (np.abs(img) < np.percentile(np.abs(img), pth))\n        img = img - img * mask\n\n    if blur_step and it_i % blur_step == 0:\n        for ch_i in range(3):\n            img[..., ch_i] = gaussian_filter(img[..., ch_i], sigma)\n\n    if crop and it_i % crop_step == 0:\n        height, width, *ch = img[0].shape\n\n        # Crop a 1 pixel border from height and width\n        img = img[:, crop:-crop, crop:-crop, :]\n\n        # Resize\n        img = resize(img[0], (height, width), order=3,\n                     clip=False, preserve_range=True\n                     )[np.newaxis].astype(np.float32)\n\n\ndef deep_dream(input_img,\n               downsize=False,\n               model=\'inception\',\n               layer_i=-1,\n               neuron_i=-1,\n               n_iterations=100,\n               save_gif=None,\n               save_images=\'imgs\',\n               device=\'/cpu:0\',\n               **kwargs):\n    """"""Deep Dream with the given parameters.\n\n    Parameters\n    ----------\n    input_img : np.ndarray\n        Image to apply deep dream to.  Should be 3-dimenionsal H x W x C\n        RGB uint8 or float32.\n    downsize : bool, optional\n        Whether or not to downsize the image.  Only applies to\n        model==\'inception\'.\n    model : str, optional\n        Which model to load.  Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n    layer_i : int, optional\n        Which layer to use for finding the gradient.  E.g. the softmax layer\n        for inception is -1, for vgg networks it is -2.  Use the function\n        ""get_layer_names"" to find the layer number that you need.\n    neuron_i : int, optional\n        Which neuron to use.  -1 for the entire layer.\n    n_iterations : int, optional\n        Number of iterations to dream.\n    save_gif : bool, optional\n        Save a GIF.\n    save_images : str, optional\n        Folder to save images to.\n    device : str, optional\n        Which device to use, e.g. [\'/cpu:0\'] or \'/gpu:0\'.\n    **kwargs : dict\n        See ""_apply"" for additional parameters.\n\n    Returns\n    -------\n    imgs : list of np.array\n        Images of every iteration\n    """"""\n    net, img, preprocess, deprocess = _setup(input_img, model, downsize)\n    batch, height, width, *ch = img.shape\n\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess, g.device(device):\n\n        tf.import_graph_def(net[\'graph_def\'], name=\'net\')\n        names = [op.name for op in g.get_operations()]\n        input_name = names[0] + \':0\'\n        x = g.get_tensor_by_name(input_name)\n\n        layer = g.get_tensor_by_name(names[layer_i] + \':0\')\n        layer_shape = sess.run(tf.shape(layer), feed_dict={x: img})\n        layer_vec = np.ones(layer_shape) / layer_shape[-1]\n        layer_vec[..., neuron_i] = 1.0 - (1.0 / layer_shape[-1])\n\n        ascent = tf.gradients(layer, x)\n\n        imgs = []\n        for it_i in range(n_iterations):\n            print(it_i, np.min(img), np.max(img))\n            if neuron_i == -1:\n                this_res = sess.run(\n                    ascent, feed_dict={x: img})[0]\n            else:\n                this_res = sess.run(\n                    ascent, feed_dict={x: img, layer: layer_vec})[0]\n\n            _apply(img, this_res, it_i, **kwargs)\n            imgs.append(deprocess(img[0]))\n\n            if save_images is not None:\n                imsave(os.path.join(save_images,\n                                    \'frame{}.png\'.format(it_i)), imgs[-1])\n\n        if save_gif is not None:\n            gif.build_gif(imgs, saveto=save_gif)\n\n    return imgs\n\n\ndef guided_dream(input_img,\n                 guide_img=None,\n                 downsize=False,\n                 layers=[162, 183, 184, 247],\n                 label_i=962,\n                 layer_i=-1,\n                 feature_loss_weight=1.0,\n                 tv_loss_weight=1.0,\n                 l2_loss_weight=1.0,\n                 softmax_loss_weight=1.0,\n                 model=\'inception\',\n                 neuron_i=920,\n                 n_iterations=100,\n                 save_gif=None,\n                 save_images=\'imgs\',\n                 device=\'/cpu:0\',\n                 **kwargs):\n    """"""Deep Dream v2.  Use an optional guide image and other techniques.\n\n    Parameters\n    ----------\n    input_img : np.ndarray\n        Image to apply deep dream to.  Should be 3-dimenionsal H x W x C\n        RGB uint8 or float32.\n    guide_img : np.ndarray, optional\n        Optional image to find features at different layers for.  Must pass in\n        a list of layers that you want to find features for.  Then the guided\n        dream will try to match this images features at those layers.\n    downsize : bool, optional\n        Whether or not to downsize the image.  Only applies to\n        model==\'inception\'.\n    layers : list, optional\n        A list of layers to find features for in the ""guide_img"".\n    label_i : int, optional\n        Which label to use for the softmax layer.  Use the ""get_labels"" function\n        to find the index corresponding the object of interest.  If None, not\n        used.\n    layer_i : int, optional\n        Which layer to use for finding the gradient.  E.g. the softmax layer\n        for inception is -1, for vgg networks it is -2.  Use the function\n        ""get_layer_names"" to find the layer number that you need.\n    feature_loss_weight : float, optional\n        Weighting for the feature loss from the guide_img.\n    tv_loss_weight : float, optional\n        Total variational loss weighting.  Enforces smoothness.\n    l2_loss_weight : float, optional\n        L2 loss weighting.  Enforces smaller values and reduces saturation.\n    softmax_loss_weight : float, optional\n        Softmax loss weighting.  Must set label_i.\n    model : str, optional\n        Which model to load.  Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n    neuron_i : int, optional\n        Which neuron to use.  -1 for the entire layer.\n    n_iterations : int, optional\n        Number of iterations to dream.\n    save_gif : bool, optional\n        Save a GIF.\n    save_images : str, optional\n        Folder to save images to.\n    device : str, optional\n        Which device to use, e.g. [\'/cpu:0\'] or \'/gpu:0\'.\n    **kwargs : dict\n        See ""_apply"" for additional parameters.\n\n    Returns\n    -------\n    imgs : list of np.ndarray\n        Images of the dream.\n    """"""\n    net, img, preprocess, deprocess = _setup(input_img, model, downsize)\n    print(img.shape, input_img.shape)\n    print(img.min(), img.max())\n\n    if guide_img is not None:\n        guide_img = preprocess(guide_img.copy(), model)[np.newaxis]\n        assert(guide_img.shape == img.shape)\n    batch, height, width, *ch = img.shape\n\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess, g.device(device):\n        tf.import_graph_def(net[\'graph_def\'], name=\'net\')\n        names = [op.name for op in g.get_operations()]\n        input_name = names[0] + \':0\'\n        x = g.get_tensor_by_name(input_name)\n\n        features = [names[layer_i] + \':0\' for layer_i in layers]\n        feature_loss = tf.Variable(0.0)\n        for feature_i in features:\n            layer = g.get_tensor_by_name(feature_i)\n            if guide_img is None:\n                feature_loss += tf.reduce_mean(layer)\n            else:\n                # Reshape it to 2D vector\n                layer = tf.reshape(layer, [-1, 1])\n                # Do the same for our guide image\n                guide_layer = sess.run(layer, feed_dict={x: guide_img})\n                guide_layer = guide_layer.reshape(-1, 1)\n                # Now calculate their dot product\n                correlation = tf.matmul(guide_layer.T, layer)\n                feature_loss += feature_loss_weight * tf.reduce_mean(correlation)\n        softmax_loss = tf.Variable(0.0)\n        if label_i is not None:\n            layer = g.get_tensor_by_name(names[layer_i] + \':0\')\n            layer_shape = sess.run(tf.shape(layer), feed_dict={x: img})\n            layer_vec = np.ones(layer_shape) / layer_shape[-1]\n            layer_vec[..., neuron_i] = 1.0 - 1.0 / layer_shape[1]\n            softmax_loss += softmax_loss_weight * tf.reduce_mean(tf.nn.l2_loss(layer - layer_vec))\n\n        dx = tf.square(x[:, :height - 1, :width - 1, :] - x[:, :height - 1, 1:, :])\n        dy = tf.square(x[:, :height - 1, :width - 1, :] - x[:, 1:, :width - 1, :])\n        tv_loss = tv_loss_weight * tf.reduce_mean(tf.pow(dx + dy, 1.2))\n        l2_loss = l2_loss_weight * tf.reduce_mean(tf.nn.l2_loss(x))\n\n        ascent = tf.gradients(feature_loss + softmax_loss + tv_loss + l2_loss, x)[0]\n        sess.run(tf.global_variables_initializer())\n        imgs = []\n        for it_i in range(n_iterations):\n            this_res, this_feature_loss, this_softmax_loss, this_tv_loss, this_l2_loss = sess.run(\n                [ascent, feature_loss, softmax_loss, tv_loss, l2_loss], feed_dict={x: img})\n            print(\'feature:\', this_feature_loss,\n                  \'softmax:\', this_softmax_loss,\n                  \'tv\', this_tv_loss,\n                  \'l2\', this_l2_loss)\n\n            _apply(img, -this_res, it_i, **kwargs)\n            imgs.append(deprocess(img[0]))\n\n            if save_images is not None:\n                imsave(os.path.join(save_images,\n                                    \'frame{}.png\'.format(it_i)), imgs[-1])\n\n        if save_gif is not None:\n            gif.build_gif(imgs, saveto=save_gif)\n\n    return imgs\n'"
session-5/libs/dft.py,0,"b'""""""Summary.\n\n#CADL\nCopyright Parag K. Mital 2016\n""""""\nimport numpy as np\nfrom scipy.signal import hann\n\n\ndef ztoc(re, im):\n    return np.sqrt(re**2 + im**2), np.angle(re + im * 1j)\n\n\ndef ctoz(mag, phs):\n    return mag * np.cos(phs), mag * np.sin(phs)\n\n\ndef dft_np(signal, hop_size=256, fft_size=512):\n    n_hops = len(signal) // hop_size\n    s = []\n    hann_win = hann(fft_size)\n    for hop_i in range(n_hops):\n        frame = signal[(hop_i * hop_size):(hop_i * hop_size + fft_size)]\n        frame = np.pad(frame, (0, fft_size - len(frame)), \'constant\')\n        frame *= hann_win\n        s.append(frame)\n    s = np.array(s)\n    N = s.shape[-1]\n    k = np.reshape(np.linspace(0.0, 2 * np.pi / N * (N // 2), N // 2), [1, N // 2])\n    x = np.reshape(np.linspace(0.0, N - 1, N), [N, 1])\n    freqs = np.dot(x, k)\n    reals = np.dot(s, np.cos(freqs)) * (2.0 / N)\n    imags = np.dot(s, np.sin(freqs)) * (2.0 / N)\n    return reals, imags\n\n\ndef idft_np(re, im, hop_size=256, fft_size=512):\n    N = re.shape[1] * 2\n    k = np.reshape(np.linspace(0.0, 2 * np.pi / N * (N // 2), N // 2), [N // 2, 1])\n    x = np.reshape(np.linspace(0.0, N - 1, N), [1, N])\n    freqs = np.dot(k, x)\n    signal = np.zeros((re.shape[0] * hop_size + fft_size,))\n    recon = np.dot(re, np.cos(freqs)) + np.dot(im, np.sin(freqs))\n    for hop_i, frame in enumerate(recon):\n        signal[(hop_i * hop_size): (hop_i * hop_size + fft_size)] += frame\n    return signal\n'"
session-5/libs/gan.py,73,"b'""""""Generative Adversarial Network.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport libs.batch_norm as bn\nfrom libs.utils import *\n\n\ndef encoder(x, phase_train, dimensions=[], filter_sizes=[],\n            convolutional=False, activation=tf.nn.relu,\n            output_activation=tf.nn.sigmoid, reuse=False):\n    """"""Encoder network codes input `x` to layers defined by dimensions.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input to the encoder network, e.g. tf.Placeholder or tf.Variable\n    phase_train : tf.Placeholder\n        Placeholder defining whether the network is in train mode or not.\n        Used for changing the behavior of batch normalization which updates\n        its statistics during train mode.\n    dimensions : list, optional\n        List of the number of neurons in each layer (convolutional=False) -or-\n        List of the number of filters in each layer (convolutional=True), e.g.\n        [100, 100, 100, 100] for a 4-layer deep network with 100 in each layer.\n    filter_sizes : list, optional\n        List of the size of the kernel in each layer, e.g.:\n        [3, 3, 3, 3] is a 4-layer deep network w/ 3 x 3 kernels in every layer.\n    convolutional : bool, optional\n        Whether or not to use convolutional layers.\n    activation : fn, optional\n        Function for applying an activation, e.g. tf.nn.relu\n    output_activation : fn, optional\n        Function for applying an activation on the last layer, e.g. tf.nn.relu\n    reuse : bool, optional\n        For each layer\'s variable scope, whether to reuse existing variables.\n\n    Returns\n    -------\n    h : tf.Tensor\n        Output tensor of the encoder\n    """"""\n    # %%\n    # ensure 2-d is converted to square tensor.\n    if convolutional:\n        x_tensor = to_tensor(x)\n    else:\n        x_tensor = tf.reshape(\n            tensor=x,\n            shape=[-1, dimensions[0]])\n        dimensions = dimensions[1:]\n    current_input = x_tensor\n\n    for layer_i, n_output in enumerate(dimensions):\n        with tf.variable_scope(str(layer_i), reuse=reuse):\n            if convolutional:\n                h, W = conv2d(\n                    x=current_input,\n                    n_output=n_output,\n                    k_h=filter_sizes[layer_i],\n                    k_w=filter_sizes[layer_i],\n                    padding=\'SAME\',\n                    reuse=reuse)\n            else:\n                h, W = linear(\n                    x=current_input,\n                    n_output=n_output,\n                    reuse=reuse)\n            norm = bn.batch_norm(\n                x=h,\n                phase_train=phase_train,\n                name=\'bn\',\n                reuse=reuse)\n            output = activation(norm)\n\n        current_input = output\n\n    flattened = flatten(current_input, name=\'flatten\', reuse=reuse)\n\n    if output_activation is None:\n        return flattened\n    else:\n        return output_activation(flattened)\n\n\ndef decoder(z,\n            phase_train,\n            dimensions=[],\n            channels=[],\n            filter_sizes=[],\n            convolutional=False,\n            activation=tf.nn.relu,\n            output_activation=tf.nn.tanh,\n            reuse=None):\n    """"""Decoder network codes input `x` to layers defined by dimensions.\n\n    In contrast with `encoder`, this requires information on the number of\n    output channels in each layer for convolution.  Otherwise, it is mostly\n    the same.\n\n    Parameters\n    ----------\n    z : tf.Tensor\n        Input to the decoder network, e.g. tf.Placeholder or tf.Variable\n    phase_train : tf.Placeholder\n        Placeholder defining whether the network is in train mode or not.\n        Used for changing the behavior of batch normalization which updates\n        its statistics during train mode.\n    dimensions : list, optional\n        List of the number of neurons in each layer (convolutional=False) -or-\n        List of the number of filters in each layer (convolutional=True), e.g.\n        [100, 100, 100, 100] for a 4-layer deep network with 100 in each layer.\n    channels : list, optional\n        For decoding when convolutional=True, require the number of output\n        channels in each layer.\n    filter_sizes : list, optional\n        List of the size of the kernel in each layer, e.g.:\n        [3, 3, 3, 3] is a 4-layer deep network w/ 3 x 3 kernels in every layer.\n    convolutional : bool, optional\n        Whether or not to use convolutional layers.\n    activation : fn, optional\n        Function for applying an activation, e.g. tf.nn.relu\n    output_activation : fn, optional\n        Function for applying an activation on the last layer, e.g. tf.nn.relu\n    reuse : bool, optional\n        For each layer\'s variable scope, whether to reuse existing variables.\n\n    Returns\n    -------\n    h : tf.Tensor\n        Output tensor of the decoder\n    """"""\n\n    if convolutional:\n        with tf.variable_scope(\'fc\', reuse=reuse):\n            z1, W = linear(\n                x=z,\n                n_output=channels[0] * dimensions[0][0] * dimensions[0][1],\n                reuse=reuse)\n            rsz = tf.reshape(\n                z1, [-1, dimensions[0][0], dimensions[0][1], channels[0]])\n            current_input = activation(\n                features=bn.batch_norm(\n                    name=\'bn\',\n                    x=rsz,\n                    phase_train=phase_train,\n                    reuse=reuse))\n\n        dimensions = dimensions[1:]\n        channels = channels[1:]\n        filter_sizes = filter_sizes[1:]\n    else:\n        current_input = z\n\n    for layer_i, n_output in enumerate(dimensions):\n        with tf.variable_scope(str(layer_i), reuse=reuse):\n\n            if convolutional:\n                h, W = deconv2d(\n                    x=current_input,\n                    n_output_h=n_output[0],\n                    n_output_w=n_output[1],\n                    n_output_ch=channels[layer_i],\n                    k_h=filter_sizes[layer_i],\n                    k_w=filter_sizes[layer_i],\n                    padding=\'SAME\',\n                    reuse=reuse)\n            else:\n                h, W = linear(\n                    x=current_input,\n                    n_output=n_output,\n                    reuse=reuse)\n\n            if layer_i < len(dimensions) - 1:\n                norm = bn.batch_norm(\n                    x=h,\n                    phase_train=phase_train,\n                    name=\'bn\', reuse=reuse)\n                output = activation(norm)\n            else:\n                output = h\n        current_input = output\n\n    if output_activation is None:\n        return current_input\n    else:\n        return output_activation(current_input)\n\n\ndef generator(z, phase_train, output_h, output_w, convolutional=True,\n              n_features=32, rgb=False, reuse=None):\n    """"""Simple interface to build a decoder network given the input parameters.\n    \n    Parameters\n    ----------\n    z : tf.Tensor\n        Input to the generator, i.e. tf.Placeholder of tf.Variable\n    phase_train : tf.Placeholder of type bool\n        Whether or not the network should be trained (used for Batch Norm).\n    output_h : int\n        Final generated height\n    output_w : int\n        Final generated width\n    convolutional : bool, optional\n        Whether or not to build a convolutional generative network.\n    n_features : int, optional\n        Number of channels to use in the last hidden layer.\n    rgb : bool, optional\n        Whether or not the final generated image is RGB or not.\n    reuse : None, optional\n        Whether or not to reuse the variables if they are already created.\n    \n    Returns\n    -------\n    x_tilde : tf.Tensor\n        Output of the generator network.\n    """"""\n    n_channels = 3 if rgb else 1\n    with tf.variable_scope(\'generator\', reuse=reuse):\n        return decoder(z=z,\n                       phase_train=phase_train,\n                       convolutional=convolutional,\n                       filter_sizes=[5, 5, 5, 5, 5],\n                       channels=[n_features * 8, n_features * 4,\n                                 n_features * 2, n_features, n_channels],\n                       dimensions=[\n                           [output_h // 16, output_w // 16],\n                           [output_h // 8, output_w // 8],\n                           [output_h // 4, output_w // 4],\n                           [output_h // 2, output_w // 2],\n                           [output_h, output_w]]\n                       if convolutional else [384, 512, n_features],\n                       activation=tf.nn.relu6,\n                       output_activation=tf.nn.tanh,\n                       reuse=reuse)\n\n\ndef discriminator(x, phase_train, convolutional=True,\n                  n_features=32, rgb=False, reuse=False):\n    """"""Summary\n    \n    Parameters\n    ----------\n    x : TYPE\n        Description\n    phase_train : TYPE\n        Description\n    convolutional : bool, optional\n        Description\n    n_features : int, optional\n        Description\n    rgb : bool, optional\n        Description\n    reuse : bool, optional\n        Description\n    \n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    n_channels = 3 if rgb else 1\n    with tf.variable_scope(\'discriminator\', reuse=reuse):\n        return encoder(x=x,\n                       phase_train=phase_train,\n                       convolutional=convolutional,\n                       filter_sizes=[5, 5, 5, 5],\n                       dimensions=[n_features, n_features * 2,\n                                   n_features * 4, n_features * 8]\n                       if convolutional\n                       else [n_features, 128, 256],\n                       activation=tf.nn.relu6,\n                       output_activation=None,\n                       reuse=reuse)\n\n\ndef GAN(input_shape, n_latent, n_features, rgb, debug=True):\n    """"""Summary\n    \n    Parameters\n    ----------\n    input_shape : TYPE\n        Description\n    n_latent : TYPE\n        Description\n    n_features : TYPE\n        Description\n    rgb : TYPE\n        Description\n    debug : bool, optional\n        Description\n    \n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    # Real input samples\n    # n_features is either the image dimension or flattened number of features\n    x = tf.placeholder(tf.float32, input_shape, \'x\')\n    x = (x / 127.5) - 1.0\n    sum_x = tf.summary.image(""x"", x)\n    phase_train = tf.placeholder(tf.bool, name=\'phase_train\')\n\n    # Discriminator for real input samples\n    D_real_logits = discriminator(\n        x, phase_train, n_features=n_features, rgb=rgb)\n    D_real = tf.nn.sigmoid(D_real_logits)\n    sum_D_real = tf.summary.histogram(""D_real"", D_real)\n\n    # Generator tries to recreate input samples using latent feature vector\n    z = tf.placeholder(tf.float32, [None, n_latent], \'z\')\n    sum_z = tf.summary.histogram(""z"", z)\n    G = generator(\n        z, phase_train,\n        output_h=input_shape[1], output_w=input_shape[2],\n        n_features=n_features, rgb=rgb)\n    sum_G = tf.summary.image(""G"", G)\n\n    # Discriminator for generated samples\n    D_fake_logits = discriminator(\n        G, phase_train, n_features=n_features, rgb=rgb, reuse=True)\n    D_fake = tf.nn.sigmoid(D_fake_logits)\n    sum_D_fake = tf.summary.histogram(""D_fake"", D_fake)\n\n    with tf.variable_scope(\'loss\'):\n        # Loss functions\n        loss_D_real = binary_cross_entropy(\n            D_real, tf.ones_like(D_real), name=\'loss_D_real\')\n        loss_D_fake = binary_cross_entropy(\n            D_fake, tf.zeros_like(D_fake), name=\'loss_D_fake\')\n        loss_D = tf.reduce_mean((loss_D_real + loss_D_fake) / 2)\n        loss_G = tf.reduce_mean(binary_cross_entropy(\n            D_fake, tf.ones_like(D_fake), name=\'loss_G\'))\n\n        # Summaries\n        sum_loss_D_real = tf.summary.histogram(""loss_D_real"", loss_D_real)\n        sum_loss_D_fake = tf.summary.histogram(""loss_D_fake"", loss_D_fake)\n        sum_loss_D = tf.summary.scalar(""loss_D"", loss_D)\n        sum_loss_G = tf.summary.scalar(""loss_G"", loss_G)\n        sum_D_real = tf.summary.histogram(""D_real"", D_real)\n        sum_D_fake = tf.summary.histogram(""D_fake"", D_fake)\n\n    return {\n        \'loss_D\': loss_D,\n        \'loss_G\': loss_G,\n        \'x\': x,\n        \'G\': G,\n        \'z\': z,\n        \'train\': phase_train,\n        \'sums\': {\n            \'G\': sum_G,\n            \'D_real\': sum_D_real,\n            \'D_fake\': sum_D_fake,\n            \'loss_G\': sum_loss_G,\n            \'loss_D\': sum_loss_D,\n            \'loss_D_real\': sum_loss_D_real,\n            \'loss_D_fake\': sum_loss_D_fake,\n            \'z\': sum_z,\n            \'x\': sum_x\n        }\n    }\n\n\ndef train_ds():\n    """"""Summary\n    \n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    init_lr_g = 1e-4\n    init_lr_d = 1e-4\n    n_latent = 100\n    n_epochs = 1000000\n    batch_size = 200\n    n_samples = 15\n    input_shape = [218, 178, 3]\n    crop_shape = [64, 64, 3]\n    crop_factor = 0.8\n\n    from libs.dataset_utils import create_input_pipeline\n    from libs.datasets import CELEB\n\n    files = CELEB()\n    batch = create_input_pipeline(\n        files=files,\n        batch_size=batch_size,\n        n_epochs=n_epochs,\n        crop_shape=crop_shape,\n        crop_factor=crop_factor,\n        shape=input_shape)\n\n    gan = GAN(input_shape=[None] + crop_shape, n_features=10,\n              n_latent=n_latent, rgb=True, debug=False)\n\n    vars_d = [v for v in tf.trainable_variables()\n              if v.name.startswith(\'discriminator\')]\n    print(\'Training discriminator variables:\')\n    [print(v.name) for v in tf.trainable_variables()\n     if v.name.startswith(\'discriminator\')]\n\n    vars_g = [v for v in tf.trainable_variables()\n              if v.name.startswith(\'generator\')]\n    print(\'Training generator variables:\')\n    [print(v.name) for v in tf.trainable_variables()\n     if v.name.startswith(\'generator\')]\n    zs = np.random.uniform(\n        -1.0, 1.0, [4, n_latent]).astype(np.float32)\n    zs = make_latent_manifold(zs, n_samples)\n\n    lr_g = tf.placeholder(tf.float32, shape=[], name=\'learning_rate_g\')\n    lr_d = tf.placeholder(tf.float32, shape=[], name=\'learning_rate_d\')\n\n    try:\n        from tf.contrib.layers import apply_regularization\n        d_reg = apply_regularization(\n            tf.contrib.layers.l2_regularizer(1e-6), vars_d)\n        g_reg = apply_regularization(\n            tf.contrib.layers.l2_regularizer(1e-6), vars_g)\n    except:\n        d_reg, g_reg = 0, 0\n\n    opt_g = tf.train.AdamOptimizer(lr_g, name=\'Adam_g\').minimize(\n        gan[\'loss_G\'] + g_reg, var_list=vars_g)\n    opt_d = tf.train.AdamOptimizer(lr_d, name=\'Adam_d\').minimize(\n        gan[\'loss_D\'] + d_reg, var_list=vars_d)\n\n    # %%\n    # We create a session to use the graph\n    sess = tf.Session()\n    init_op = tf.global_variables_initializer()\n\n    saver = tf.train.Saver()\n    sums = gan[\'sums\']\n    G_sum_op = tf.summary.merge([\n        sums[\'G\'], sums[\'loss_G\'], sums[\'z\'],\n        sums[\'loss_D_fake\'], sums[\'D_fake\']])\n    D_sum_op = tf.summary.merge([\n        sums[\'loss_D\'], sums[\'loss_D_real\'], sums[\'loss_D_fake\'],\n        sums[\'z\'], sums[\'x\'], sums[\'D_real\'], sums[\'D_fake\']])\n    writer = tf.summary.FileWriter(""./logs"", sess.graph_def)\n\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    sess.run(init_op)\n    # g = tf.get_default_graph()\n    # [print(op.name) for op in g.get_operations()]\n\n    if os.path.exists(""gan.ckpt""):\n        saver.restore(sess, ""gan.ckpt"")\n        print(""GAN model restored."")\n\n    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n    step_i, t_i = 0, 0\n    loss_d = 1\n    loss_g = 1\n    n_loss_d, total_loss_d = 1, 1\n    n_loss_g, total_loss_g = 1, 1\n    try:\n        while not coord.should_stop():\n            batch_xs = sess.run(batch)\n            step_i += 1\n            batch_zs = np.random.uniform(\n                -1.0, 1.0, [batch_size, n_latent]).astype(np.float32)\n\n            this_lr_g = min(1e-2, max(1e-6, init_lr_g * (loss_g / loss_d)**2))\n            this_lr_d = min(1e-2, max(1e-6, init_lr_d * (loss_d / loss_g)**2))\n            # this_lr_d *= ((1.0 - (step_i / 100000)) ** 2)\n            # this_lr_g *= ((1.0 - (step_i / 100000)) ** 2)\n\n            # if np.random.random() > (loss_g / (loss_d + loss_g)):\n            if step_i % 3 == 1:\n                loss_d, _, sum_d = sess.run([gan[\'loss_D\'], opt_d, D_sum_op],\n                                            feed_dict={gan[\'x\']: batch_xs,\n                                                       gan[\'z\']: batch_zs,\n                                                       gan[\'train\']: True,\n                                                       lr_d: this_lr_d})\n                total_loss_d += loss_d\n                n_loss_d += 1\n                writer.add_summary(sum_d, step_i)\n                print(\'%04d d* = lr: %0.08f, loss: %08.06f, \\t\' %\n                      (step_i, this_lr_d, loss_d) +\n                      \'g  = lr: %0.08f, loss: %08.06f\' % (this_lr_g, loss_g))\n            else:\n                loss_g, _, sum_g = sess.run([gan[\'loss_G\'], opt_g, G_sum_op],\n                                            feed_dict={gan[\'z\']: batch_zs,\n                                                       gan[\'train\']: True,\n                                                       lr_g: this_lr_g})\n                total_loss_g += loss_g\n                n_loss_g += 1\n                writer.add_summary(sum_g, step_i)\n                print(\'%04d d  = lr: %0.08f, loss: %08.06f, \\t\' %\n                      (step_i, this_lr_d, loss_d) +\n                      \'g* = lr: %0.08f, loss: %08.06f\' % (this_lr_g, loss_g))\n\n            if step_i % 100 == 0:\n                samples = sess.run(gan[\'G\'], feed_dict={\n                    gan[\'z\']: zs,\n                    gan[\'train\']: False})\n                montage(np.clip((samples + 1) * 127.5, 0, 255).astype(np.uint8),\n                        \'imgs/gan_%08d.png\' % t_i)\n                t_i += 1\n\n                print(\'generator loss:\', total_loss_g / n_loss_g)\n                print(\'discriminator loss:\', total_loss_d / n_loss_d)\n\n                # Save the variables to disk.\n                save_path = saver.save(sess, ""./gan.ckpt"",\n                                       global_step=step_i,\n                                       write_meta_graph=False)\n                print(""Model saved in file: %s"" % save_path)\n    except tf.errors.OutOfRangeError:\n        print(\'Done training -- epoch limit reached\')\n    finally:\n        # One of the threads has issued an exception.  So let\'s tell all the\n        # threads to shutdown.\n        coord.request_stop()\n\n    # Wait until all threads have finished.\n    coord.join(threads)\n\n    # Clean up the session.\n    sess.close()\n\n\nif __name__ == \'__main__\':\n    train_ds()\n'"
session-5/libs/gif.py,0,"b'""""""Utility for creating a GIF.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n\ndef build_gif(imgs, interval=0.1, dpi=72,\n              save_gif=True, saveto=\'animation.gif\',\n              show_gif=False, cmap=None):\n    """"""Take an array or list of images and create a GIF.\n\n    Parameters\n    ----------\n    imgs : np.ndarray or list\n        List of images to create a GIF of\n    interval : float, optional\n        Spacing in seconds between successive images.\n    dpi : int, optional\n        Dots per inch.\n    save_gif : bool, optional\n        Whether or not to save the GIF.\n    saveto : str, optional\n        Filename of GIF to save.\n    show_gif : bool, optional\n        Whether or not to render the GIF using plt.\n    cmap : None, optional\n        Optional colormap to apply to the images.\n\n    Returns\n    -------\n    ani : matplotlib.animation.ArtistAnimation\n        The artist animation from matplotlib.  Likely not useful.\n    """"""\n    imgs = np.asarray(imgs)\n    h, w, *c = imgs[0].shape\n    fig, ax = plt.subplots(figsize=(np.round(w / dpi), np.round(h / dpi)))\n    fig.subplots_adjust(bottom=0)\n    fig.subplots_adjust(top=1)\n    fig.subplots_adjust(right=1)\n    fig.subplots_adjust(left=0)\n    ax.set_axis_off()\n\n    if cmap is not None:\n        axs = list(map(lambda x: [\n            ax.imshow(x, cmap=cmap)], imgs))\n    else:\n        axs = list(map(lambda x: [\n            ax.imshow(x)], imgs))\n\n    ani = animation.ArtistAnimation(\n        fig, axs, interval=interval*1000, repeat_delay=0, blit=True)\n\n    if save_gif:\n        ani.save(saveto, writer=\'imagemagick\', dpi=dpi)\n\n    if show_gif:\n        plt.show()\n    else:\n        plt.close(fig)\n\n    return ani\n'"
session-5/libs/i2v.py,10,"b'""""""\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport json\nimport numpy as np\nfrom tensorflow.python.platform import gfile\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize as imresize\nfrom .utils import download\n\n\ndef i2v_download():\n    """"""Download a pretrained i2v network.""""""\n    model = download(\'https://s3.amazonaws.com/cadl/models/illust2vec.tfmodel\')\n    return model\n\n\ndef i2v_tag_download():\n    """"""Download a pretrained i2v network.""""""\n    model = download(\'https://s3.amazonaws.com/cadl/models/illust2vec_tag.tfmodel\')\n    tags = download(\'https://s3.amazonaws.com/cadl/models/tag_list.json\')\n    return model, tags\n\n\ndef get_i2v_model():\n    """"""Get a pretrained i2v network.\n\n    Returns\n    -------\n    net : dict\n        {\'graph_def\': graph_def, \'labels\': synsets}\n        where the graph_def is a tf.GraphDef and the synsets\n        map an integer label from 0-1000 to a list of names\n    """"""\n    # Download the trained net\n    model = i2v_download()\n\n    # Load the saved graph\n    with gfile.GFile(model, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n\n    return {\'graph_def\': graph_def}\n\n\ndef get_i2v_tag_model():\n    """"""Get a pretrained i2v tag network.\n\n    Returns\n    -------\n    net : dict\n        {\'graph_def\': graph_def, \'labels\': synsets}\n        where the graph_def is a tf.GraphDef and the synsets\n        map an integer label from 0-1000 to a list of names\n    """"""\n    # Download the trained net\n    model, tags = i2v_tag_download()\n    tags = json.load(open(tags, \'r\'))\n\n    # Load the saved graph\n    with gfile.GFile(model, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n\n    return {\n        \'graph_def\': graph_def,\n        \'labels\': tags,\n        \'preprocess\': preprocess,\n        \'deprocess\': deprocess\n    }\n\n\ndef preprocess(img, crop=True, resize=True, dsize=(224, 224)):\n    mean_img = np.array([164.76139251, 167.47864617, 181.13838569])\n    if img.dtype == np.uint8:\n        img = (img[..., ::-1] - mean_img).astype(np.float32)\n    else:\n        img = img[..., ::-1] * 255.0 - mean_img\n\n    if crop:\n        short_edge = min(img.shape[:2])\n        yy = int((img.shape[0] - short_edge) / 2)\n        xx = int((img.shape[1] - short_edge) / 2)\n        crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n    else:\n        crop_img = img\n    if resize:\n        norm_img = imresize(crop_img, dsize, preserve_range=True)\n    else:\n        norm_img = crop_img\n    return (norm_img).astype(np.float32)\n\n\ndef deprocess(img):\n    mean_img = np.array([164.76139251, 167.47864617, 181.13838569])\n    processed = (img + mean_img)[..., ::-1]\n    return np.clip(processed, 0, 255).astype(np.uint8)\n    # return ((img / np.max(np.abs(img))) * 127.5 +\n    #         127.5).astype(np.uint8)\n\n\ndef test_i2v():\n    """"""Loads the i2v network and applies it to a test image.\n    """"""\n    with tf.Session() as sess:\n        net = get_i2v_model()\n        tf.import_graph_def(net[\'graph_def\'], name=\'i2v\')\n        g = tf.get_default_graph()\n        names = [op.name for op in g.get_operations()]\n        x = g.get_tensor_by_name(names[0] + \':0\')\n        softmax = g.get_tensor_by_name(names[-3] + \':0\')\n\n        from skimage import data\n        img = preprocess(data.coffee())[np.newaxis]\n        res = np.squeeze(softmax.eval(feed_dict={x: img}))\n        print([(res[idx], net[\'labels\'][idx])\n               for idx in res.argsort()[-5:][::-1]])\n\n        """"""Let\'s visualize the network\'s gradient activation\n        when backpropagated to the original input image.  This\n        is effectively telling us which pixels contribute to the\n        predicted class or given neuron""""""\n        pools = [name for name in names if \'pool\' in name.split(\'/\')[-1]]\n        fig, axs = plt.subplots(1, len(pools))\n        for pool_i, poolname in enumerate(pools):\n            pool = g.get_tensor_by_name(poolname + \':0\')\n            pool.get_shape()\n            neuron = tf.reduce_max(pool, 1)\n            saliency = tf.gradients(neuron, x)\n            neuron_idx = tf.arg_max(pool, 1)\n            this_res = sess.run([saliency[0], neuron_idx],\n                                feed_dict={x: img})\n\n            grad = this_res[0][0] / np.max(np.abs(this_res[0]))\n            axs[pool_i].imshow((grad * 128 + 128).astype(np.uint8))\n            axs[pool_i].set_title(poolname)\n'"
session-5/libs/inception.py,8,"b'""""""\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport os\nimport numpy as np\nfrom tensorflow.python.platform import gfile\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize as imresize\nfrom .utils import download_and_extract_tar, download_and_extract_zip\n\n\ndef inception_download(data_dir=\'inception\', version=\'v5\'):\n    """"""Download a pretrained inception network.\n\n    Parameters\n    ----------\n    data_dir : str, optional\n        Location of the pretrained inception network download.\n    version : str, optional\n        Version of the model: [\'v3\'] or \'v5\'.\n    """"""\n    if version == \'v3\':\n        download_and_extract_tar(\n            \'https://s3.amazonaws.com/cadl/models/inception-2015-12-05.tgz\',\n            data_dir)\n        return (os.path.join(data_dir, \'classify_image_graph_def.pb\'),\n                os.path.join(data_dir, \'imagenet_synset_to_human_label_map.txt\'))\n    else:\n        download_and_extract_zip(\n            \'https://s3.amazonaws.com/cadl/models/inception5h.zip\', data_dir)\n        return (os.path.join(data_dir, \'tensorflow_inception_graph.pb\'),\n                os.path.join(data_dir, \'imagenet_comp_graph_label_strings.txt\'))\n\n\ndef get_inception_model(data_dir=\'inception\', version=\'v5\'):\n    """"""Get a pretrained inception network.\n\n    Parameters\n    ----------\n    data_dir : str, optional\n        Location of the pretrained inception network download.\n    version : str, optional\n        Version of the model: [\'v3\'] or \'v5\'.\n\n    Returns\n    -------\n    net : dict\n        {\'graph_def\': graph_def, \'labels\': synsets}\n        where the graph_def is a tf.GraphDef and the synsets\n        map an integer label from 0-1000 to a list of names\n    """"""\n    # Download the trained net\n    model, labels = inception_download(data_dir, version)\n\n    # Parse the ids and synsets\n    txt = open(labels).readlines()\n    synsets = [(key, val.strip()) for key, val in enumerate(txt)]\n\n    # Load the saved graph\n    with gfile.GFile(model, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n    return {\n        \'graph_def\': graph_def,\n        \'labels\': synsets,\n        \'preprocess\': preprocess,\n        \'deprocess\': deprocess\n    }\n\n\ndef preprocess(img, crop=True, resize=True, dsize=(299, 299)):\n    if img.dtype != np.uint8:\n        img *= 255.0\n\n    if crop:\n        crop = np.min(img.shape[:2])\n        r = (img.shape[0] - crop) // 2\n        c = (img.shape[1] - crop) // 2\n        cropped = img[r: r + crop, c: c + crop]\n    else:\n        cropped = img\n\n    if resize:\n        rsz = imresize(cropped, dsize, preserve_range=True)\n    else:\n        rsz = cropped\n\n    if rsz.ndim == 2:\n        rsz = rsz[..., np.newaxis]\n\n    rsz = rsz.astype(np.float32)\n    # subtract imagenet mean\n    return (rsz - 117)\n\n\ndef deprocess(img):\n    return np.clip(img + 117, 0, 255).astype(np.uint8)\n\n\ndef test_inception():\n    """"""Loads the inception network and applies it to a test image.\n    """"""\n    with tf.Session() as sess:\n        net = get_inception_model()\n        tf.import_graph_def(net[\'graph_def\'], name=\'inception\')\n        g = tf.get_default_graph()\n        names = [op.name for op in g.get_operations()]\n        x = g.get_tensor_by_name(names[0] + \':0\')\n        softmax = g.get_tensor_by_name(names[-3] + \':0\')\n\n        from skimage import data\n        img = preprocess(data.coffee())[np.newaxis]\n        res = np.squeeze(softmax.eval(feed_dict={x: img}))\n        print([(res[idx], net[\'labels\'][idx])\n               for idx in res.argsort()[-5:][::-1]])\n\n        """"""Let\'s visualize the network\'s gradient activation\n        when backpropagated to the original input image.  This\n        is effectively telling us which pixels contribute to the\n        predicted class or given neuron""""""\n        pools = [name for name in names if \'pool\' in name.split(\'/\')[-1]]\n        fig, axs = plt.subplots(1, len(pools))\n        for pool_i, poolname in enumerate(pools):\n            pool = g.get_tensor_by_name(poolname + \':0\')\n            pool.get_shape()\n            neuron = tf.reduce_max(pool, 1)\n            saliency = tf.gradients(neuron, x)\n            neuron_idx = tf.arg_max(pool, 1)\n            this_res = sess.run([saliency[0], neuron_idx],\n                                feed_dict={x: img})\n\n            grad = this_res[0][0] / np.max(np.abs(this_res[0]))\n            axs[pool_i].imshow((grad * 128 + 128).astype(np.uint8))\n            axs[pool_i].set_title(poolname)\n'"
session-5/libs/nb_utils.py,2,"b'""""""Utility for displaying Tensorflow graphs from:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb\n""""""\nimport tensorflow as tf\nimport numpy as np\nfrom IPython.display import display, HTML\n\n\ndef show_graph(graph_def):\n    # Helper functions for TF Graph visualization\n    def _strip_consts(graph_def, max_const_size=32):\n        """"""Strip large constant values from graph_def.""""""\n        strip_def = tf.GraphDef()\n        for n0 in graph_def.node:\n            n = strip_def.node.add()\n            n.MergeFrom(n0)\n            if n.op == \'Const\':\n                tensor = n.attr[\'value\'].tensor\n                size = len(tensor.tensor_content)\n                if size > max_const_size:\n                    tensor.tensor_content = ""<stripped {} bytes>"".format(size).encode()\n        return strip_def\n\n    def _rename_nodes(graph_def, rename_func):\n        res_def = tf.GraphDef()\n        for n0 in graph_def.node:\n            n = res_def.node.add()\n            n.MergeFrom(n0)\n            n.name = rename_func(n.name)\n            for i, s in enumerate(n.input):\n                n.input[i] = rename_func(s) if s[0] != \'^\' else \'^\' + rename_func(s[1:])\n        return res_def\n\n    def _show_entire_graph(graph_def, max_const_size=32):\n        """"""Visualize TensorFlow graph.""""""\n        if hasattr(graph_def, \'as_graph_def\'):\n            graph_def = graph_def.as_graph_def()\n        strip_def = _strip_consts(graph_def, max_const_size=max_const_size)\n        code = """"""\n            <script>\n              function load() {{\n                document.getElementById(""{id}"").pbtxt = {data};\n              }}\n            </script>\n            <link rel=""import"" href=""https://tensorboard.appspot.com/tf-graph-basic.build.html"" onload=load()>\n            <div style=""height:600px"">\n              <tf-graph-basic id=""{id}""></tf-graph-basic>\n            </div>\n        """""".format(data=repr(str(strip_def)), id=\'graph\' + str(np.random.rand()))\n\n        iframe = """"""\n            <iframe seamless style=""width:800px;height:620px;border:0"" srcdoc=""{}""></iframe>\n        """""".format(code.replace(\'""\', \'&quot;\'))\n        display(HTML(iframe))\n    # Visualizing the network graph. Be sure expand the ""mixed"" nodes to see their\n    # internal structure. We are going to visualize ""Conv2D"" nodes.\n    tmp_def = _rename_nodes(graph_def, lambda s: ""/"".join(s.split(\'_\', 1)))\n    _show_entire_graph(tmp_def)\n'"
session-5/libs/stylenet.py,15,"b'""""""Style Net w/ tests for Video Style Net.\n\nVideo Style Net requires OpenCV 3.0.0+ w/ Contrib for Python to be installed.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nfrom __future__ import print_function\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom . import vgg16\nfrom . import gif\n\n\ndef make_4d(img):\n    """"""Create a 4-dimensional N x H x W x C image.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        Given image as H x W x C or H x W.\n\n    Returns\n    -------\n    img : np.ndarray\n        N x H x W x C image.\n\n    Raises\n    ------\n    ValueError\n        Unexpected number of dimensions.\n    """"""\n    if img.ndim == 2:\n        img = np.expand_dims(img[np.newaxis], 3)\n    elif img.ndim == 3:\n        img = img[np.newaxis]\n    elif img.ndim == 4:\n        return img\n    else:\n        raise ValueError(\'Incorrect dimensions for image!\')\n    return img\n\n\ndef stylize(content_img, style_img, base_img=None, saveto=None, gif_step=5,\n            n_iterations=100, style_weight=1.0, content_weight=1.0):\n    """"""Stylization w/ the given content and style images.\n\n    Follows the approach in Leon Gatys et al.\n\n    Parameters\n    ----------\n    content_img : np.ndarray\n        Image to use for finding the content features.\n    style_img : TYPE\n        Image to use for finding the style features.\n    base_img : None, optional\n        Image to use for the base content.  Can be noise or an existing image.\n        If None, the content image will be used.\n    saveto : str, optional\n        Name of GIF image to write to, e.g. ""stylization.gif""\n    gif_step : int, optional\n        Modulo of iterations to save the current stylization.\n    n_iterations : int, optional\n        Number of iterations to run for.\n    style_weight : float, optional\n        Weighting on the style features.\n    content_weight : float, optional\n        Weighting on the content features.\n\n    Returns\n    -------\n    stylization : np.ndarray\n        Final iteration of the stylization.\n    """"""\n    # Preprocess both content and style images\n    content_img = vgg16.preprocess(content_img, dsize=(224, 224))[np.newaxis]\n    style_img = vgg16.preprocess(style_img, dsize=(224, 224))[np.newaxis]\n    if base_img is None:\n        base_img = content_img\n    else:\n        base_img = make_4d(vgg16.preprocess(base_img, dsize=(224, 224)))\n\n    # Get Content and Style features\n    net = vgg16.get_vgg_model()\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        tf.import_graph_def(net[\'graph_def\'], name=\'vgg\')\n        names = [op.name for op in g.get_operations()]\n        x = g.get_tensor_by_name(names[0] + \':0\')\n        content_layer = \'vgg/conv3_2/conv3_2:0\'\n        content_features = g.get_tensor_by_name(\n            content_layer).eval(feed_dict={\n                x: content_img,\n                \'vgg/dropout_1/random_uniform:0\': [[1.0] * 4096],\n                \'vgg/dropout/random_uniform:0\': [[1.0] * 4096]})\n        style_layers = [\'vgg/conv1_1/conv1_1:0\',\n                        \'vgg/conv2_1/conv2_1:0\',\n                        \'vgg/conv3_1/conv3_1:0\',\n                        \'vgg/conv4_1/conv4_1:0\',\n                        \'vgg/conv5_1/conv5_1:0\']\n        style_activations = []\n        for style_i in style_layers:\n            style_activation_i = g.get_tensor_by_name(style_i).eval(\n                feed_dict={\n                    x: style_img,\n                    \'vgg/dropout_1/random_uniform:0\': [[1.0] * 4096],\n                    \'vgg/dropout/random_uniform:0\': [[1.0] * 4096]})\n            style_activations.append(style_activation_i)\n        style_features = []\n        for style_activation_i in style_activations:\n            s_i = np.reshape(style_activation_i,\n                             [-1, style_activation_i.shape[-1]])\n            gram_matrix = np.matmul(s_i.T, s_i) / s_i.size\n            style_features.append(gram_matrix.astype(np.float32))\n\n    # Optimize both\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        net_input = tf.Variable(base_img)\n        tf.import_graph_def(\n            net[\'graph_def\'],\n            name=\'vgg\',\n            input_map={\'images:0\': net_input})\n\n        content_loss = tf.nn.l2_loss((g.get_tensor_by_name(content_layer) -\n                                      content_features) /\n                                     content_features.size)\n        style_loss = np.float32(0.0)\n        for style_layer_i, style_gram_i in zip(style_layers, style_features):\n            layer_i = g.get_tensor_by_name(style_layer_i)\n            layer_shape = layer_i.get_shape().as_list()\n            layer_size = layer_shape[1] * layer_shape[2] * layer_shape[3]\n            layer_flat = tf.reshape(layer_i, [-1, layer_shape[3]])\n            gram_matrix = tf.matmul(\n                tf.transpose(layer_flat), layer_flat) / layer_size\n            style_loss = tf.add(\n                style_loss, tf.nn.l2_loss(\n                    (gram_matrix - style_gram_i) /\n                    np.float32(style_gram_i.size)))\n        loss = content_weight * content_loss + style_weight * style_loss\n        optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n\n        sess.run(tf.global_variables_initializer())\n        imgs = []\n        for it_i in range(n_iterations):\n            _, this_loss, synth = sess.run(\n                [optimizer, loss, net_input],\n                feed_dict={\n                    \'vgg/dropout_1/random_uniform:0\': np.ones(\n                        g.get_tensor_by_name(\n                            \'vgg/dropout_1/random_uniform:0\'\n                        ).get_shape().as_list()),\n                    \'vgg/dropout/random_uniform:0\': np.ones(\n                        g.get_tensor_by_name(\n                            \'vgg/dropout/random_uniform:0\'\n                        ).get_shape().as_list())\n                })\n            print(""iteration %d, loss: %f, range: (%f - %f)"" %\n                  (it_i, this_loss, np.min(synth), np.max(synth)), end=\'\\r\')\n            if it_i % gif_step == 0:\n                imgs.append(np.clip(synth[0], 0, 1))\n        if saveto is not None:\n            gif.build_gif(imgs, saveto=saveto)\n    return np.clip(synth[0], 0, 1)\n\n\ndef warp_img(img, dx, dy):\n    """"""Apply the motion vectors to the given image.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        Input image to apply motion to.\n    dx : np.ndarray\n        H x W matrix defining the magnitude of the X vector\n    dy : np.ndarray\n        H x W matrix defining the magnitude of the Y vector\n\n    Returns\n    -------\n    img : np.ndarray\n        Image with pixels warped according to dx, dy.\n    """"""\n    warped = img.copy()\n    for row_i in range(img.shape[0]):\n        for col_i in range(img.shape[1]):\n            dx_i = int(np.round(dx[row_i, col_i]))\n            dy_i = int(np.round(dy[row_i, col_i]))\n            sample_dx = np.clip(dx_i + col_i, 0, img.shape[0] - 1)\n            sample_dy = np.clip(dy_i + row_i, 0, img.shape[1] - 1)\n            warped[sample_dy, sample_dx, :] = img[row_i, col_i, :]\n    return warped\n\n\ndef test_video(style_img=\'arles.jpg\', videodir=\'kurosawa\'):\n    r""""""Test for artistic stylization using video.\n\n    This requires the python installation of OpenCV for the Deep Flow algorithm.\n    If cv2 is not found, then there will be reduced ""temporal coherence"".\n\n    Unfortunately, installing opencv for python3 is not the easiest thing to do.\n    OSX users can install this using:\n\n    $ brew install opencv --with-python3 --with-contrib\n\n    then will have to symlink the libraries.  I think you can do this w/:\n\n    $ brew link --force opencv3\n\n    But the problems start to arise depending on which python you have\n    installed, and it is always a mess w/ homebrew.  Sorry!\n\n    Your best bet is installing from source.  Something along\n    these lines should get you there:\n\n    $ cd ~\n    $ git clone https://github.com/Itseez/opencv.git\n    $ cd opencv\n    $ git checkout 3.1.0\n    $ cd ~\n    $ git clone https://github.com/Itseez/opencv_contrib.git\n    $ cd opencv_contrib\n    $ git checkout 3.1.0\n    $ cd ~/opencv\n    $ mkdir build\n    $ cd build\n    $ cmake -D CMAKE_BUILD_TYPE=RELEASE \\\n        -D CMAKE_INSTALL_PREFIX=/usr/local \\\n        -D INSTALL_C_EXAMPLES=OFF \\\n        -D INSTALL_PYTHON_EXAMPLES=OFF \\\n        -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib/modules \\\n        -D BUILD_EXAMPLES=OFF ..\n\n    Parameters\n    ----------\n    style_img : str, optional\n        Location to style image\n    videodir : str, optional\n        Location to directory containing images of each frame to stylize.\n\n    Returns\n    -------\n    imgs : list of np.ndarray\n        Stylized images for each frame.\n    """"""\n    has_cv2 = True\n    try:\n        import cv2\n        has_cv2 = True\n        optflow = cv2.optflow.createOptFlow_DeepFlow()\n    except ImportError:\n        has_cv2 = False\n\n    style_img = plt.imread(style_img)\n    content_files = [os.path.join(videodir, f)\n                     for f in os.listdir(videodir) if f.endswith(\'.png\')]\n    content_img = plt.imread(content_files[0])\n    from scipy.misc import imresize\n    style_img = imresize(style_img, (448, 448)).astype(np.float32) / 255.0\n    content_img = imresize(content_img, (448, 448)).astype(np.float32) / 255.0\n    if has_cv2:\n        prev_lum = cv2.cvtColor(content_img, cv2.COLOR_RGB2HSV)[:, :, 2]\n    else:\n        prev_lum = (content_img[..., 0] * 0.3 +\n                    content_img[..., 1] * 0.59 +\n                    content_img[..., 2] * 0.11)\n    imgs = []\n    stylized = stylize(content_img, style_img, content_weight=5.0,\n                       style_weight=0.5, n_iterations=50)\n    plt.imsave(fname=content_files[0] + \'stylized.png\', arr=stylized)\n    imgs.append(stylized)\n    for f in content_files[1:]:\n        content_img = plt.imread(f)\n        content_img = imresize(content_img, (448, 448)).astype(np.float32) / 255.0\n        if has_cv2:\n            lum = cv2.cvtColor(content_img, cv2.COLOR_RGB2HSV)[:, :, 2]\n            flow = optflow.calc(prev_lum, lum, None)\n            warped = warp_img(stylized, flow[..., 0], flow[..., 1])\n            stylized = stylize(content_img, style_img, content_weight=5.0,\n                               style_weight=0.5, base_img=warped, n_iterations=50)\n        else:\n            lum = (content_img[..., 0] * 0.3 +\n                   content_img[..., 1] * 0.59 +\n                   content_img[..., 2] * 0.11)\n            stylized = stylize(content_img, style_img, content_weight=5.0,\n                               style_weight=0.5, base_img=None, n_iterations=50)\n        imgs.append(stylized)\n        plt.imsave(fname=f + \'stylized.png\', arr=stylized)\n        prev_lum = lum\n    return imgs\n\n\ndef test():\n    """"""Test for artistic stylization.""""""\n    from six.moves import urllib\n    f = (\'https://upload.wikimedia.org/wikipedia/commons/thumb/5/54/\' +\n         \'Claude_Monet%2C_Impression%2C_soleil_levant.jpg/617px-Claude_Monet\' +\n         \'%2C_Impression%2C_soleil_levant.jpg?download\')\n    filepath, _ = urllib.request.urlretrieve(f, f.split(\'/\')[-1], None)\n    style = plt.imread(filepath).astype(np.float32) / 255.0\n\n    f = (\'https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/\' +\n         \'El_jard%C3%ADn_de_las_Delicias%2C_de_El_Bosco.jpg/640px-El_jard\' +\n         \'%C3%ADn_de_las_Delicias%2C_de_El_Bosco.jpg\')\n    filepath, _ = urllib.request.urlretrieve(f, f.split(\'/\')[-1], None)\n    content = plt.imread(filepath).astype(np.float32) / 255.0\n\n    stylize(content, style, n_iterations=20)\n\n\nif __name__ == \'__main__\':\n    test_video()\n'"
session-5/libs/utils.py,74,"b'""""""Utilities used in the Kadenze Academy Course on Deep Learning w/ Tensorflow.\n\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nParag K. Mital\n\nCopyright Parag K. Mital, June 2016.\n""""""\nfrom __future__ import print_function\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport urllib\nimport numpy as np\nimport zipfile\nimport os\nfrom scipy.io import wavfile\nfrom scipy.misc import imsave\n\ndef download(path):\n    """"""Use urllib to download a file.\n\n    Parameters\n    ----------\n    path : str\n        Url to download\n\n    Returns\n    -------\n    path : str\n        Location of downloaded file.\n    """"""\n    import os\n    from six.moves import urllib\n\n    fname = path.split(\'/\')[-1]\n    if os.path.exists(fname):\n        return fname\n\n    print(\'Downloading \' + path)\n\n    def progress(count, block_size, total_size):\n        if count % 20 == 0:\n            print(\'Downloaded %02.02f/%02.02f MB\' % (\n                count * block_size / 1024.0 / 1024.0,\n                total_size / 1024.0 / 1024.0), end=\'\\r\')\n\n    filepath, _ = urllib.request.urlretrieve(\n        path, filename=fname, reporthook=progress)\n    return filepath\n\n\ndef download_and_extract_tar(path, dst):\n    """"""Download and extract a tar file.\n\n    Parameters\n    ----------\n    path : str\n        Url to tar file to download.\n    dst : str\n        Location to save tar file contents.\n    """"""\n    import tarfile\n    filepath = download(path)\n    if not os.path.exists(dst):\n        os.makedirs(dst)\n        tarfile.open(filepath, \'r:gz\').extractall(dst)\n\n\ndef download_and_extract_zip(path, dst):\n    """"""Download and extract a zip file.\n\n    Parameters\n    ----------\n    path : str\n        Url to zip file to download.\n    dst : str\n        Location to save zip file contents.\n    """"""\n    import zipfile\n    filepath = download(path)\n    if not os.path.exists(dst):\n        os.makedirs(dst)\n        zf = zipfile.ZipFile(file=filepath)\n        zf.extractall(dst)\n\n\ndef load_audio(filename, b_normalize=True):\n    """"""Load the audiofile at the provided filename using scipy.io.wavfile.\n\n    Optionally normalizes the audio to the maximum value.\n\n    Parameters\n    ----------\n    filename : str\n        File to load.\n    b_normalize : bool, optional\n        Normalize to the maximum value.\n    """"""\n    sr, s = wavfile.read(filename)\n    if b_normalize:\n        s = s.astype(np.float32)\n        s = (s / np.max(np.abs(s)))\n        s -= np.mean(s)\n    return s\n\n\ndef corrupt(x):\n    """"""Take an input tensor and add uniform masking.\n\n    Parameters\n    ----------\n    x : Tensor/Placeholder\n        Input to corrupt.\n    Returns\n    -------\n    x_corrupted : Tensor\n        50 pct of values corrupted.\n    """"""\n    return tf.multiply(x, tf.cast(tf.random_uniform(shape=tf.shape(x),\n                                               minval=0,\n                                               maxval=2,\n                                               dtype=tf.int32), tf.float32))\n\n\ndef interp(l, r, n_samples):\n    """"""Intepolate between the arrays l and r, n_samples times.\n\n    Parameters\n    ----------\n    l : np.ndarray\n        Left edge\n    r : np.ndarray\n        Right edge\n    n_samples : int\n        Number of samples\n\n    Returns\n    -------\n    arr : np.ndarray\n        Inteporalted array\n    """"""\n    return np.array([\n        l + step_i / (n_samples - 1) * (r - l)\n        for step_i in range(n_samples)])\n\n\ndef make_latent_manifold(corners, n_samples):\n    """"""Create a 2d manifold out of the provided corners: n_samples * n_samples.\n\n    Parameters\n    ----------\n    corners : list of np.ndarray\n        The four corners to intepolate.\n    n_samples : int\n        Number of samples to use in interpolation.\n\n    Returns\n    -------\n    arr : np.ndarray\n        Stacked array of all 2D interpolated samples\n    """"""\n    left = interp(corners[0], corners[1], n_samples)\n    right = interp(corners[2], corners[3], n_samples)\n\n    embedding = []\n    for row_i in range(n_samples):\n        embedding.append(interp(left[row_i], right[row_i], n_samples))\n    return np.vstack(embedding)\n\n\ndef imcrop_tosquare(img):\n    """"""Make any image a square image.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        Input image to crop, assumed at least 2d.\n\n    Returns\n    -------\n    crop : np.ndarray\n        Cropped image.\n    """"""\n    size = np.min(img.shape[:2])\n    extra = img.shape[:2] - size\n    crop = img\n    for i in np.flatnonzero(extra):\n        crop = np.take(crop, extra[i] // 2 + np.r_[:size], axis=i)\n    return crop\n\n\ndef slice_montage(montage, img_h, img_w, n_imgs):\n    """"""Slice a montage image into n_img h x w images.\n\n    Performs the opposite of the montage function.  Takes a montage image and\n    slices it back into a N x H x W x C image.\n\n    Parameters\n    ----------\n    montage : np.ndarray\n        Montage image to slice.\n    img_h : int\n        Height of sliced image\n    img_w : int\n        Width of sliced image\n    n_imgs : int\n        Number of images to slice\n\n    Returns\n    -------\n    sliced : np.ndarray\n        Sliced images as 4d array.\n    """"""\n    sliced_ds = []\n    for i in range(int(np.sqrt(n_imgs))):\n        for j in range(int(np.sqrt(n_imgs))):\n            sliced_ds.append(montage[\n                1 + i + i * img_h:1 + i + (i + 1) * img_h,\n                1 + j + j * img_w:1 + j + (j + 1) * img_w])\n    return np.array(sliced_ds)\n\n\ndef montage(images, saveto=\'montage.png\'):\n    """"""Draw all images as a montage separated by 1 pixel borders.\n\n    Also saves the file to the destination specified by `saveto`.\n\n    Parameters\n    ----------\n    images : numpy.ndarray\n        Input array to create montage of.  Array should be:\n        batch x height x width x channels.\n    saveto : str\n        Location to save the resulting montage image.\n\n    Returns\n    -------\n    m : numpy.ndarray\n        Montage image.\n    """"""\n    if isinstance(images, list):\n        images = np.array(images)\n    img_h = images.shape[1]\n    img_w = images.shape[2]\n    n_plots = int(np.ceil(np.sqrt(images.shape[0])))\n    if len(images.shape) == 4 and images.shape[3] == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 3)) * 0.5\n    elif len(images.shape) == 4 and images.shape[3] == 1:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 1)) * 0.5\n    elif len(images.shape) == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1)) * 0.5\n    else:\n        raise ValueError(\'Could not parse image shape of {}\'.format(\n            images.shape))\n    for i in range(n_plots):\n        for j in range(n_plots):\n            this_filter = i * n_plots + j\n            if this_filter < images.shape[0]:\n                this_img = images[this_filter]\n                m[1 + i + i * img_h:1 + i + (i + 1) * img_h,\n                  1 + j + j * img_w:1 + j + (j + 1) * img_w] = this_img\n    imsave(arr=np.squeeze(m), name=saveto)\n    return m\n\n\ndef montage_filters(W):\n    """"""Draws all filters (n_input * n_output filters) as a\n    montage image separated by 1 pixel borders.\n\n    Parameters\n    ----------\n    W : Tensor\n        Input tensor to create montage of.\n\n    Returns\n    -------\n    m : numpy.ndarray\n        Montage image.\n    """"""\n    W = np.reshape(W, [W.shape[0], W.shape[1], 1, W.shape[2] * W.shape[3]])\n    n_plots = int(np.ceil(np.sqrt(W.shape[-1])))\n    m = np.ones(\n        (W.shape[0] * n_plots + n_plots + 1,\n         W.shape[1] * n_plots + n_plots + 1)) * 0.5\n    for i in range(n_plots):\n        for j in range(n_plots):\n            this_filter = i * n_plots + j\n            if this_filter < W.shape[-1]:\n                m[1 + i + i * W.shape[0]:1 + i + (i + 1) * W.shape[0],\n                  1 + j + j * W.shape[1]:1 + j + (j + 1) * W.shape[1]] = (\n                    np.squeeze(W[:, :, :, this_filter]))\n    return m\n\n\ndef get_celeb_files(dst=\'img_align_celeba\', max_images=100):\n    """"""Download the first 100 images of the celeb dataset.\n\n    Files will be placed in a directory \'img_align_celeba\' if one\n    doesn\'t exist.\n\n    Returns\n    -------\n    files : list of strings\n        Locations to the first 100 images of the celeb net dataset.\n    """"""\n    # Create a directory\n    if not os.path.exists(dst):\n        os.mkdir(dst)\n\n    # Now perform the following 100 times:\n    for img_i in range(1, max_images + 1):\n\n        # create a string using the current loop counter\n        f = \'000%03d.jpg\' % img_i\n\n        if not os.path.exists(os.path.join(dst, f)):\n\n            # and get the url with that string appended the end\n            url = \'https://s3.amazonaws.com/cadl/celeb-align/\' + f\n\n            # We\'ll print this out to the console so we can see how far we\'ve gone\n            print(url, end=\'\\r\')\n\n            # And now download the url to a location inside our new directory\n            urllib.request.urlretrieve(url, os.path.join(dst, f))\n\n    files = [os.path.join(dst, file_i)\n             for file_i in os.listdir(dst)\n             if \'.jpg\' in file_i][:max_images]\n    return files\n\n\ndef get_celeb_imgs(max_images=100):\n    """"""Load the first `max_images` images of the celeb dataset.\n\n    Returns\n    -------\n    imgs : list of np.ndarray\n        List of the first 100 images from the celeb dataset\n    """"""\n    return [plt.imread(f_i) for f_i in get_celeb_files(max_images=max_images)]\n\n\ndef gauss(mean, stddev, ksize):\n    """"""Use Tensorflow to compute a Gaussian Kernel.\n\n    Parameters\n    ----------\n    mean : float\n        Mean of the Gaussian (e.g. 0.0).\n    stddev : float\n        Standard Deviation of the Gaussian (e.g. 1.0).\n    ksize : int\n        Size of kernel (e.g. 16).\n\n    Returns\n    -------\n    kernel : np.ndarray\n        Computed Gaussian Kernel using Tensorflow.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        x = tf.linspace(-3.0, 3.0, ksize)\n        z = (tf.exp(tf.negative(tf.pow(x - mean, 2.0) /\n                           (2.0 * tf.pow(stddev, 2.0)))) *\n             (1.0 / (stddev * tf.sqrt(2.0 * 3.1415))))\n        return z.eval()\n\n\ndef gauss2d(mean, stddev, ksize):\n    """"""Use Tensorflow to compute a 2D Gaussian Kernel.\n\n    Parameters\n    ----------\n    mean : float\n        Mean of the Gaussian (e.g. 0.0).\n    stddev : float\n        Standard Deviation of the Gaussian (e.g. 1.0).\n    ksize : int\n        Size of kernel (e.g. 16).\n\n    Returns\n    -------\n    kernel : np.ndarray\n        Computed 2D Gaussian Kernel using Tensorflow.\n    """"""\n    z = gauss(mean, stddev, ksize)\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        z_2d = tf.matmul(tf.reshape(z, [ksize, 1]), tf.reshape(z, [1, ksize]))\n        return z_2d.eval()\n\n\ndef convolve(img, kernel):\n    """"""Use Tensorflow to convolve a 4D image with a 4D kernel.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        4-dimensional image shaped N x H x W x C\n    kernel : np.ndarray\n        4-dimensional image shape K_H, K_W, C_I, C_O corresponding to the\n        kernel\'s height and width, the number of input channels, and the\n        number of output channels.  Note that C_I should = C.\n\n    Returns\n    -------\n    result : np.ndarray\n        Convolved result.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        convolved = tf.nn.conv2d(img, kernel, strides=[1, 1, 1, 1], padding=\'SAME\')\n        res = convolved.eval()\n    return res\n\n\ndef gabor(ksize=32):\n    """"""Use Tensorflow to compute a 2D Gabor Kernel.\n\n    Parameters\n    ----------\n    ksize : int, optional\n        Size of kernel.\n\n    Returns\n    -------\n    gabor : np.ndarray\n        Gabor kernel with ksize x ksize dimensions.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        z_2d = gauss2d(0.0, 1.0, ksize)\n        ones = tf.ones((1, ksize))\n        ys = tf.sin(tf.linspace(-3.0, 3.0, ksize))\n        ys = tf.reshape(ys, [ksize, 1])\n        wave = tf.matmul(ys, ones)\n        gabor = tf.multiply(wave, z_2d)\n        return gabor.eval()\n\n\ndef build_submission(filename, file_list, optional_file_list=()):\n    """"""Helper utility to check homework assignment submissions and package them.\n\n    Parameters\n    ----------\n    filename : str\n        Output zip file name\n    file_list : tuple\n        Tuple of files to include\n    """"""\n    # check each file exists\n    for part_i, file_i in enumerate(file_list):\n        if not os.path.exists(file_i):\n            print(\'\\nYou are missing the file {}.  \'.format(file_i) +\n                  \'It does not look like you have completed Part {}.\'.format(\n                part_i + 1))\n\n    def zipdir(path, zf):\n        for root, dirs, files in os.walk(path):\n            for file in files:\n                # make sure the files are part of the necessary file list\n                if file.endswith(file_list) or file.endswith(optional_file_list):\n                    zf.write(os.path.join(root, file))\n\n    # create a zip file with the necessary files\n    zipf = zipfile.ZipFile(filename, \'w\', zipfile.ZIP_DEFLATED)\n    zipdir(\'.\', zipf)\n    zipf.close()\n    print(\'Your assignment zip file has been created!\')\n    print(\'Now submit the file:\\n{}\\nto Kadenze for grading!\'.format(\n        os.path.abspath(filename)))\n\n\ndef normalize(a, s=0.1):\n    \'\'\'Normalize the image range for visualization\'\'\'\n    return np.uint8(np.clip(\n        (a - a.mean()) / max(a.std(), 1e-4) * s + 0.5,\n        0, 1) * 255)\n\n\n# %%\ndef weight_variable(shape, **kwargs):\n    \'\'\'Helper function to create a weight variable initialized with\n    a normal distribution\n    Parameters\n    ----------\n    shape : list\n        Size of weight variable\n    \'\'\'\n    if isinstance(shape, list):\n        initial = tf.random_normal(tf.stack(shape), mean=0.0, stddev=0.01)\n        initial.set_shape(shape)\n    else:\n        initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n    return tf.Variable(initial, **kwargs)\n\n\n# %%\ndef bias_variable(shape, **kwargs):\n    \'\'\'Helper function to create a bias variable initialized with\n    a constant value.\n    Parameters\n    ----------\n    shape : list\n        Size of weight variable\n    \'\'\'\n    if isinstance(shape, list):\n        initial = tf.random_normal(tf.stack(shape), mean=0.0, stddev=0.01)\n        initial.set_shape(shape)\n    else:\n        initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n    return tf.Variable(initial, **kwargs)\n\n\ndef binary_cross_entropy(z, x, name=None):\n    """"""Binary Cross Entropy measures cross entropy of a binary variable.\n\n    loss(x, z) = - sum_i (x[i] * log(z[i]) + (1 - x[i]) * log(1 - z[i]))\n\n    Parameters\n    ----------\n    z : tf.Tensor\n        A `Tensor` of the same type and shape as `x`.\n    x : tf.Tensor\n        A `Tensor` of type `float32` or `float64`.\n    """"""\n    with tf.variable_scope(name or \'bce\'):\n        eps = 1e-12\n        return (-(x * tf.log(z + eps) +\n                  (1. - x) * tf.log(1. - z + eps)))\n\n\ndef conv2d(x, n_output,\n           k_h=5, k_w=5, d_h=2, d_w=2,\n           padding=\'SAME\', name=\'conv2d\', reuse=None):\n    """"""Helper for creating a 2d convolution operation.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to convolve.\n    n_output : int\n        Number of filters.\n    k_h : int, optional\n        Kernel height\n    k_w : int, optional\n        Kernel width\n    d_h : int, optional\n        Height stride\n    d_w : int, optional\n        Width stride\n    padding : str, optional\n        Padding type: ""SAME"" or ""VALID""\n    name : str, optional\n        Variable scope\n\n    Returns\n    -------\n    op : tf.Tensor\n        Output of convolution\n    """"""\n    with tf.variable_scope(name or \'conv2d\', reuse=reuse):\n        W = tf.get_variable(\n            name=\'W\',\n            shape=[k_h, k_w, x.get_shape()[-1], n_output],\n            initializer=tf.contrib.layers.xavier_initializer_conv2d())\n\n        conv = tf.nn.conv2d(\n            name=\'conv\',\n            input=x,\n            filter=W,\n            strides=[1, d_h, d_w, 1],\n            padding=padding)\n\n        b = tf.get_variable(\n            name=\'b\',\n            shape=[n_output],\n            initializer=tf.constant_initializer(0.0))\n\n        h = tf.nn.bias_add(\n            name=\'h\',\n            value=conv,\n            bias=b)\n\n    return h, W\n\n\ndef deconv2d(x, n_output_h, n_output_w, n_output_ch, n_input_ch=None,\n             k_h=5, k_w=5, d_h=2, d_w=2,\n             padding=\'SAME\', name=\'deconv2d\', reuse=None):\n    """"""Deconvolution helper.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to convolve.\n    n_output_h : int\n        Height of output\n    n_output_w : int\n        Width of output\n    n_output_ch : int\n        Number of filters.\n    k_h : int, optional\n        Kernel height\n    k_w : int, optional\n        Kernel width\n    d_h : int, optional\n        Height stride\n    d_w : int, optional\n        Width stride\n    padding : str, optional\n        Padding type: ""SAME"" or ""VALID""\n    name : str, optional\n        Variable scope\n\n    Returns\n    -------\n    op : tf.Tensor\n        Output of deconvolution\n    """"""\n    with tf.variable_scope(name or \'deconv2d\', reuse=reuse):\n        W = tf.get_variable(\n            name=\'W\',\n            shape=[k_h, k_w, n_output_ch, n_input_ch or x.get_shape()[-1]],\n            initializer=tf.contrib.layers.xavier_initializer_conv2d())\n\n        conv = tf.nn.conv2d_transpose(\n            name=\'conv_t\',\n            value=x,\n            filter=W,\n            output_shape=tf.stack(\n                [tf.shape(x)[0], n_output_h, n_output_w, n_output_ch]),\n            strides=[1, d_h, d_w, 1],\n            padding=padding)\n\n        conv.set_shape([None, n_output_h, n_output_w, n_output_ch])\n\n        b = tf.get_variable(\n            name=\'b\',\n            shape=[n_output_ch],\n            initializer=tf.constant_initializer(0.0))\n\n        h = tf.nn.bias_add(name=\'h\', value=conv, bias=b)\n\n    return h, W\n\n\ndef lrelu(features, leak=0.2):\n    """"""Leaky rectifier.\n\n    Parameters\n    ----------\n    features : tf.Tensor\n        Input to apply leaky rectifier to.\n    leak : float, optional\n        Percentage of leak.\n\n    Returns\n    -------\n    op : tf.Tensor\n        Resulting output of applying leaky rectifier activation.\n    """"""\n    f1 = 0.5 * (1 + leak)\n    f2 = 0.5 * (1 - leak)\n    return f1 * features + f2 * abs(features)\n\n\ndef linear(x, n_output, name=None, activation=None, reuse=None):\n    """"""Fully connected layer.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to connect\n    n_output : int\n        Number of output neurons\n    name : None, optional\n        Scope to apply\n\n    Returns\n    -------\n    h, W : tf.Tensor, tf.Tensor\n        Output of fully connected layer and the weight matrix\n    """"""\n    if len(x.get_shape()) != 2:\n        x = flatten(x, reuse=reuse)\n\n    n_input = x.get_shape().as_list()[1]\n\n    with tf.variable_scope(name or ""fc"", reuse=reuse):\n        W = tf.get_variable(\n            name=\'W\',\n            shape=[n_input, n_output],\n            dtype=tf.float32,\n            initializer=tf.contrib.layers.xavier_initializer())\n\n        b = tf.get_variable(\n            name=\'b\',\n            shape=[n_output],\n            dtype=tf.float32,\n            initializer=tf.constant_initializer(0.0))\n\n        h = tf.nn.bias_add(\n            name=\'h\',\n            value=tf.matmul(x, W),\n            bias=b)\n\n        if activation:\n            h = activation(h)\n\n        return h, W\n\n\ndef flatten(x, name=None, reuse=None):\n    """"""Flatten Tensor to 2-dimensions.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to flatten.\n    name : None, optional\n        Variable scope for flatten operations\n\n    Returns\n    -------\n    flattened : tf.Tensor\n        Flattened tensor.\n    """"""\n    with tf.variable_scope(\'flatten\'):\n        dims = x.get_shape().as_list()\n        if len(dims) == 4:\n            flattened = tf.reshape(\n                x,\n                shape=[-1, dims[1] * dims[2] * dims[3]])\n        elif len(dims) == 2 or len(dims) == 1:\n            flattened = x\n        else:\n            raise ValueError(\'Expected n dimensions of 1, 2 or 4.  Found:\',\n                             len(dims))\n\n        return flattened\n\n\ndef to_tensor(x):\n    """"""Convert 2 dim Tensor to a 4 dim Tensor ready for convolution.\n\n    Performs the opposite of flatten(x).  If the tensor is already 4-D, this\n    returns the same as the input, leaving it unchanged.\n\n    Parameters\n    ----------\n    x : tf.Tesnor\n        Input 2-D tensor.  If 4-D already, left unchanged.\n\n    Returns\n    -------\n    x : tf.Tensor\n        4-D representation of the input.\n\n    Raises\n    ------\n    ValueError\n        If the tensor is not 2D or already 4D.\n    """"""\n    if len(x.get_shape()) == 2:\n        n_input = x.get_shape().as_list()[1]\n        x_dim = np.sqrt(n_input)\n        if x_dim == int(x_dim):\n            x_dim = int(x_dim)\n            x_tensor = tf.reshape(\n                x, [-1, x_dim, x_dim, 1], name=\'reshape\')\n        elif np.sqrt(n_input / 3) == int(np.sqrt(n_input / 3)):\n            x_dim = int(np.sqrt(n_input / 3))\n            x_tensor = tf.reshape(\n                x, [-1, x_dim, x_dim, 3], name=\'reshape\')\n        else:\n            x_tensor = tf.reshape(\n                x, [-1, 1, 1, n_input], name=\'reshape\')\n    elif len(x.get_shape()) == 4:\n        x_tensor = x\n    else:\n        raise ValueError(\'Unsupported input dimensions\')\n    return x_tensor\n'"
session-5/libs/vae.py,40,"b'""""""Convolutional/Variational autoencoder, including demonstration of\ntraining such a network on MNIST, CelebNet and the film, ""Sita Sings The Blues""\nusing an image pipeline.\n\nCopyright Parag K. Mital, January 2016\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport os\nfrom libs.dataset_utils import create_input_pipeline\nfrom libs.datasets import CELEB, MNIST\nfrom libs.batch_norm import batch_norm\nfrom libs import utils\n\n\ndef VAE(input_shape=[None, 784],\n        n_filters=[64, 64, 64],\n        filter_sizes=[4, 4, 4],\n        n_hidden=32,\n        n_code=2,\n        activation=tf.nn.tanh,\n        dropout=False,\n        denoising=False,\n        convolutional=False,\n        variational=False):\n    """"""(Variational) (Convolutional) (Denoising) Autoencoder.\n\n    Uses tied weights.\n\n    Parameters\n    ----------\n    input_shape : list, optional\n        Shape of the input to the network. e.g. for MNIST: [None, 784].\n    n_filters : list, optional\n        Number of filters for each layer.\n        If convolutional=True, this refers to the total number of output\n        filters to create for each layer, with each layer\'s number of output\n        filters as a list.\n        If convolutional=False, then this refers to the total number of neurons\n        for each layer in a fully connected network.\n    filter_sizes : list, optional\n        Only applied when convolutional=True.  This refers to the ksize (height\n        and width) of each convolutional layer.\n    n_hidden : int, optional\n        Only applied when variational=True.  This refers to the first fully\n        connected layer prior to the variational embedding, directly after\n        the encoding.  After the variational embedding, another fully connected\n        layer is created with the same size prior to decoding.  Set to 0 to\n        not use an additional hidden layer.\n    n_code : int, optional\n        Only applied when variational=True.  This refers to the number of\n        latent Gaussians to sample for creating the inner most encoding.\n    activation : function, optional\n        Activation function to apply to each layer, e.g. tf.nn.relu\n    dropout : bool, optional\n        Whether or not to apply dropout.  If using dropout, you must feed a\n        value for \'keep_prob\', as returned in the dictionary.  1.0 means no\n        dropout is used.  0.0 means every connection is dropped.  Sensible\n        values are between 0.5-0.8.\n    denoising : bool, optional\n        Whether or not to apply denoising.  If using denoising, you must feed a\n        value for \'corrupt_prob\', as returned in the dictionary.  1.0 means no\n        corruption is used.  0.0 means every feature is corrupted.  Sensible\n        values are between 0.5-0.8.\n    convolutional : bool, optional\n        Whether or not to use a convolutional network or else a fully connected\n        network will be created.  This effects the n_filters parameter\'s\n        meaning.\n    variational : bool, optional\n        Whether or not to create a variational embedding layer.  This will\n        create a fully connected layer after the encoding, if `n_hidden` is\n        greater than 0, then will create a multivariate gaussian sampling\n        layer, then another fully connected layer.  The size of the fully\n        connected layers are determined by `n_hidden`, and the size of the\n        sampling layer is determined by `n_code`.\n\n    Returns\n    -------\n    model : dict\n        {\n            \'cost\': Tensor to optimize.\n            \'Ws\': All weights of the encoder.\n            \'x\': Input Placeholder\n            \'z\': Inner most encoding Tensor (latent features)\n            \'y\': Reconstruction of the Decoder\n            \'keep_prob\': Amount to keep when using Dropout\n            \'corrupt_prob\': Amount to corrupt when using Denoising\n            \'train\': Set to True when training/Applies to Batch Normalization.\n        }\n    """"""\n    # network input / placeholders for train (bn) and dropout\n    x = tf.placeholder(tf.float32, input_shape, \'x\')\n    phase_train = tf.placeholder(tf.bool, name=\'phase_train\')\n    keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n    corrupt_prob = tf.placeholder(tf.float32, [1])\n\n    # apply noise if denoising\n    x_ = (utils.corrupt(x) * corrupt_prob + x * (1 - corrupt_prob)) if denoising else x\n\n    # 2d -> 4d if convolution\n    x_tensor = utils.to_tensor(x_) if convolutional else x_\n    current_input = x_tensor\n\n    Ws = []\n    shapes = []\n\n    # Build the encoder\n    for layer_i, n_output in enumerate(n_filters):\n        with tf.variable_scope(\'encoder/{}\'.format(layer_i)):\n            shapes.append(current_input.get_shape().as_list())\n            if convolutional:\n                h, W = utils.conv2d(x=current_input,\n                                    n_output=n_output,\n                                    k_h=filter_sizes[layer_i],\n                                    k_w=filter_sizes[layer_i])\n            else:\n                h, W = utils.linear(x=current_input,\n                                    n_output=n_output)\n            h = activation(batch_norm(h, phase_train, \'bn\' + str(layer_i)))\n            if dropout:\n                h = tf.nn.dropout(h, keep_prob)\n            Ws.append(W)\n            current_input = h\n\n    shapes.append(current_input.get_shape().as_list())\n\n    with tf.variable_scope(\'variational\'):\n        if variational:\n            dims = current_input.get_shape().as_list()\n            flattened = utils.flatten(current_input)\n\n            if n_hidden:\n                h = utils.linear(flattened, n_hidden, name=\'W_fc\')[0]\n                h = activation(batch_norm(h, phase_train, \'fc/bn\'))\n                if dropout:\n                    h = tf.nn.dropout(h, keep_prob)\n            else:\n                h = flattened\n\n            z_mu = utils.linear(h, n_code, name=\'mu\')[0]\n            z_log_sigma = 0.5 * utils.linear(h, n_code, name=\'log_sigma\')[0]\n\n            # Sample from noise distribution p(eps) ~ N(0, 1)\n            epsilon = tf.random_normal(\n                tf.stack([tf.shape(x)[0], n_code]))\n\n            # Sample from posterior\n            z = z_mu + tf.multiply(epsilon, tf.exp(z_log_sigma))\n\n            if n_hidden:\n                h = utils.linear(z, n_hidden, name=\'fc_t\')[0]\n                h = activation(batch_norm(h, phase_train, \'fc_t/bn\'))\n                if dropout:\n                    h = tf.nn.dropout(h, keep_prob)\n            else:\n                h = z\n\n            size = dims[1] * dims[2] * dims[3] if convolutional else dims[1]\n            h = utils.linear(h, size, name=\'fc_t2\')[0]\n            current_input = activation(batch_norm(h, phase_train, \'fc_t2/bn\'))\n            if dropout:\n                current_input = tf.nn.dropout(current_input, keep_prob)\n\n            if convolutional:\n                current_input = tf.reshape(\n                    current_input, tf.stack([\n                        tf.shape(current_input)[0],\n                        dims[1],\n                        dims[2],\n                        dims[3]]))\n        else:\n            z = current_input\n\n    shapes.reverse()\n    n_filters.reverse()\n    Ws.reverse()\n\n    n_filters += [input_shape[-1]]\n\n    # %%\n    # Decoding layers\n    for layer_i, n_output in enumerate(n_filters[1:]):\n        with tf.variable_scope(\'decoder/{}\'.format(layer_i)):\n            shape = shapes[layer_i + 1]\n            if convolutional:\n                h, W = utils.deconv2d(x=current_input,\n                                      n_output_h=shape[1],\n                                      n_output_w=shape[2],\n                                      n_output_ch=shape[3],\n                                      n_input_ch=shapes[layer_i][3],\n                                      k_h=filter_sizes[layer_i],\n                                      k_w=filter_sizes[layer_i])\n            else:\n                h, W = utils.linear(x=current_input,\n                                    n_output=n_output)\n            h = activation(batch_norm(h, phase_train, \'dec/bn\' + str(layer_i)))\n            if dropout:\n                h = tf.nn.dropout(h, keep_prob)\n            current_input = h\n\n    y = current_input\n    x_flat = utils.flatten(x)\n    y_flat = utils.flatten(y)\n\n    # l2 loss\n    loss_x = tf.reduce_sum(tf.squared_difference(x_flat, y_flat), 1)\n\n    if variational:\n        # variational lower bound, kl-divergence\n        loss_z = -0.5 * tf.reduce_sum(\n            1.0 + 2.0 * z_log_sigma -\n            tf.square(z_mu) - tf.exp(2.0 * z_log_sigma), 1)\n\n        # add l2 loss\n        cost = tf.reduce_mean(loss_x + loss_z)\n    else:\n        # just optimize l2 loss\n        cost = tf.reduce_mean(loss_x)\n\n    return {\'cost\': cost, \'Ws\': Ws,\n            \'x\': x, \'z\': z, \'y\': y,\n            \'keep_prob\': keep_prob,\n            \'corrupt_prob\': corrupt_prob,\n            \'train\': phase_train}\n\n\ndef train_vae(files,\n              input_shape,\n              learning_rate=0.0001,\n              batch_size=100,\n              n_epochs=50,\n              n_examples=10,\n              crop_shape=[64, 64, 3],\n              crop_factor=0.8,\n              n_filters=[100, 100, 100, 100],\n              n_hidden=256,\n              n_code=50,\n              convolutional=True,\n              variational=True,\n              filter_sizes=[3, 3, 3, 3],\n              dropout=True,\n              keep_prob=0.8,\n              activation=tf.nn.relu,\n              img_step=100,\n              save_step=100,\n              ckpt_name=""vae.ckpt""):\n    """"""General purpose training of a (Variational) (Convolutional) Autoencoder.\n\n    Supply a list of file paths to images, and this will do everything else.\n\n    Parameters\n    ----------\n    files : list of strings\n        List of paths to images.\n    input_shape : list\n        Must define what the input image\'s shape is.\n    learning_rate : float, optional\n        Learning rate.\n    batch_size : int, optional\n        Batch size.\n    n_epochs : int, optional\n        Number of epochs.\n    n_examples : int, optional\n        Number of example to use while demonstrating the current training\n        iteration\'s reconstruction.  Creates a square montage, so make\n        sure int(sqrt(n_examples))**2 = n_examples, e.g. 16, 25, 36, ... 100.\n    crop_shape : list, optional\n        Size to centrally crop the image to.\n    crop_factor : float, optional\n        Resize factor to apply before cropping.\n    n_filters : list, optional\n        Same as VAE\'s n_filters.\n    n_hidden : int, optional\n        Same as VAE\'s n_hidden.\n    n_code : int, optional\n        Same as VAE\'s n_code.\n    convolutional : bool, optional\n        Use convolution or not.\n    variational : bool, optional\n        Use variational layer or not.\n    filter_sizes : list, optional\n        Same as VAE\'s filter_sizes.\n    dropout : bool, optional\n        Use dropout or not\n    keep_prob : float, optional\n        Percent of keep for dropout.\n    activation : function, optional\n        Which activation function to use.\n    img_step : int, optional\n        How often to save training images showing the manifold and\n        reconstruction.\n    save_step : int, optional\n        How often to save checkpoints.\n    ckpt_name : str, optional\n        Checkpoints will be named as this, e.g. \'model.ckpt\'\n    """"""\n    batch = create_input_pipeline(\n        files=files,\n        batch_size=batch_size,\n        n_epochs=n_epochs,\n        crop_shape=crop_shape,\n        crop_factor=crop_factor,\n        shape=input_shape)\n\n    ae = VAE(input_shape=[None] + crop_shape,\n             convolutional=convolutional,\n             variational=variational,\n             n_filters=n_filters,\n             n_hidden=n_hidden,\n             n_code=n_code,\n             dropout=dropout,\n             filter_sizes=filter_sizes,\n             activation=activation)\n\n    # Create a manifold of our inner most layer to show\n    # example reconstructions.  This is one way to see\n    # what the ""embedding"" or ""latent space"" of the encoder\n    # is capable of encoding, though note that this is just\n    # a random hyperplane within the latent space, and does not\n    # encompass all possible embeddings.\n    zs = np.random.uniform(\n        -1.0, 1.0, [4, n_code]).astype(np.float32)\n    zs = utils.make_latent_manifold(zs, n_examples)\n\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate=learning_rate).minimize(ae[\'cost\'])\n\n    # We create a session to use the graph\n    sess = tf.Session()\n    saver = tf.train.Saver()\n    sess.run(tf.global_variables_initializer())\n\n    # This will handle our threaded image pipeline\n    coord = tf.train.Coordinator()\n\n    # Ensure no more changes to graph\n    tf.get_default_graph().finalize()\n\n    # Start up the queues for handling the image pipeline\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    if os.path.exists(ckpt_name + \'.index\') or os.path.exists(ckpt_name):\n        saver.restore(sess, ckpt_name)\n\n    # Fit all training data\n    t_i = 0\n    batch_i = 0\n    epoch_i = 0\n    cost = 0\n    n_files = len(files)\n    test_xs = sess.run(batch) / 255.0\n    utils.montage(test_xs, \'test_xs.png\')\n    try:\n        while not coord.should_stop() and epoch_i < n_epochs:\n            batch_i += 1\n            batch_xs = sess.run(batch) / 255.0\n            train_cost = sess.run([ae[\'cost\'], optimizer], feed_dict={\n                ae[\'x\']: batch_xs, ae[\'train\']: True,\n                ae[\'keep_prob\']: keep_prob})[0]\n            print(batch_i, train_cost)\n            cost += train_cost\n            if batch_i % n_files == 0:\n                print(\'epoch:\', epoch_i)\n                print(\'average cost:\', cost / batch_i)\n                cost = 0\n                batch_i = 0\n                epoch_i += 1\n\n            if batch_i % img_step == 0:\n                # Plot example reconstructions from latent layer\n                recon = sess.run(\n                    ae[\'y\'], feed_dict={\n                        ae[\'z\']: zs,\n                        ae[\'train\']: False,\n                        ae[\'keep_prob\']: 1.0})\n                utils.montage(recon.reshape([-1] + crop_shape),\n                              \'manifold_%08d.png\' % t_i)\n\n                # Plot example reconstructions\n                recon = sess.run(\n                    ae[\'y\'], feed_dict={ae[\'x\']: test_xs,\n                                        ae[\'train\']: False,\n                                        ae[\'keep_prob\']: 1.0})\n                print(\'reconstruction (min, max, mean):\',\n                    recon.min(), recon.max(), recon.mean())\n                utils.montage(recon.reshape([-1] + crop_shape),\n                              \'reconstruction_%08d.png\' % t_i)\n                t_i += 1\n\n            if batch_i % save_step == 0:\n                # Save the variables to disk.\n                saver.save(sess, ckpt_name,\n                           global_step=batch_i,\n                           write_meta_graph=False)\n    except tf.errors.OutOfRangeError:\n        print(\'Done.\')\n    finally:\n        # One of the threads has issued an exception.  So let\'s tell all the\n        # threads to shutdown.\n        coord.request_stop()\n\n    # Wait until all threads have finished.\n    coord.join(threads)\n\n    # Clean up the session.\n    sess.close()\n\n\n# %%\ndef test_mnist():\n    """"""Train an autoencoder on MNIST.\n\n    This function will train an autoencoder on MNIST and also\n    save many image files during the training process, demonstrating\n    the latent space of the inner most dimension of the encoder,\n    as well as reconstructions of the decoder.\n    """"""\n\n    # load MNIST\n    n_code = 2\n    mnist = MNIST(split=[0.8, 0.1, 0.1])\n    ae = VAE(input_shape=[None, 784], n_filters=[512, 256],\n             n_hidden=64, n_code=n_code, activation=tf.nn.sigmoid,\n             convolutional=False, variational=True)\n\n    n_examples = 100\n    zs = np.random.uniform(\n        -1.0, 1.0, [4, n_code]).astype(np.float32)\n    zs = utils.make_latent_manifold(zs, n_examples)\n\n    learning_rate = 0.02\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate=learning_rate).minimize(ae[\'cost\'])\n\n    # We create a session to use the graph\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n\n    # Fit all training data\n    t_i = 0\n    batch_i = 0\n    batch_size = 200\n    n_epochs = 10\n    test_xs = mnist.test.images[:n_examples]\n    utils.montage(test_xs.reshape((-1, 28, 28)), \'test_xs.png\')\n    for epoch_i in range(n_epochs):\n        train_i = 0\n        train_cost = 0\n        for batch_xs, _ in mnist.train.next_batch(batch_size):\n            train_cost += sess.run([ae[\'cost\'], optimizer], feed_dict={\n                ae[\'x\']: batch_xs, ae[\'train\']: True, ae[\'keep_prob\']: 1.0})[0]\n            train_i += 1\n            if batch_i % 10 == 0:\n                # Plot example reconstructions from latent layer\n                recon = sess.run(\n                    ae[\'y\'], feed_dict={\n                        ae[\'z\']: zs,\n                        ae[\'train\']: False,\n                        ae[\'keep_prob\']: 1.0})\n                utils.montage(recon.reshape((-1, 28, 28)),\n                    \'manifold_%08d.png\' % t_i)\n                # Plot example reconstructions\n                recon = sess.run(\n                    ae[\'y\'], feed_dict={ae[\'x\']: test_xs,\n                                        ae[\'train\']: False,\n                                        ae[\'keep_prob\']: 1.0})\n                utils.montage(recon.reshape(\n                    (-1, 28, 28)), \'reconstruction_%08d.png\' % t_i)\n                t_i += 1\n            batch_i += 1\n\n        valid_i = 0\n        valid_cost = 0\n        for batch_xs, _ in mnist.valid.next_batch(batch_size):\n            valid_cost += sess.run([ae[\'cost\']], feed_dict={\n                ae[\'x\']: batch_xs, ae[\'train\']: False, ae[\'keep_prob\']: 1.0})[0]\n            valid_i += 1\n        print(\'train:\', train_cost / train_i, \'valid:\', valid_cost / valid_i)\n\n\ndef test_celeb():\n    """"""Train an autoencoder on Celeb Net.\n    """"""\n    files = CELEB()\n    train_vae(\n        files=files,\n        input_shape=[218, 178, 3],\n        batch_size=100,\n        n_epochs=50,\n        crop_shape=[64, 64, 3],\n        crop_factor=0.8,\n        convolutional=True,\n        variational=True,\n        n_filters=[100, 100, 100],\n        n_hidden=250,\n        n_code=100,\n        dropout=True,\n        filter_sizes=[3, 3, 3],\n        activation=tf.nn.sigmoid,\n        ckpt_name=\'./celeb.ckpt\')\n\n\ndef test_sita():\n    """"""Train an autoencoder on Sita Sings The Blues.\n    """"""\n    if not os.path.exists(\'sita\'):\n        os.system(\'wget http://ossguy.com/sita/Sita_Sings_the_Blues_640x360_XviD.avi\')\n        os.mkdir(\'sita\')\n        os.system(\'ffmpeg -i Sita_Sings_the_Blues_640x360_XviD.avi -r 60 -f\' +\n                  \' image2 -s 160x90 sita/sita-%08d.jpg\')\n    files = [os.path.join(\'sita\', f) for f in os.listdir(\'sita\')]\n\n    train_vae(\n        files=files,\n        input_shape=[90, 160, 3],\n        batch_size=100,\n        n_epochs=50,\n        crop_shape=[90, 160, 3],\n        crop_factor=1.0,\n        convolutional=True,\n        variational=True,\n        n_filters=[100, 100, 100],\n        n_hidden=250,\n        n_code=100,\n        dropout=True,\n        filter_sizes=[3, 3, 3],\n        activation=tf.nn.sigmoid,\n        ckpt_name=\'./sita.ckpt\')\n\n\nif __name__ == \'__main__\':\n    test_celeb()\n'"
session-5/libs/vaegan.py,63,"b'\n""""""Convolutional/Variational autoencoder, including demonstration of\ntraining such a network on MNIST, CelebNet and the film, ""Sita Sings The Blues""\nusing an image pipeline.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport os\nfrom libs.dataset_utils import create_input_pipeline\nfrom libs.datasets import CELEB\nfrom libs import utils\n\n\ndef encoder(x,\n            n_hidden=None,\n            dimensions=[],\n            filter_sizes=[],\n            convolutional=False,\n            activation=tf.nn.relu,\n            output_activation=tf.nn.sigmoid):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    n_hidden : None, optional\n        Description\n    dimensions : list, optional\n        Description\n    filter_sizes : list, optional\n        Description\n    convolutional : bool, optional\n        Description\n    activation : TYPE, optional\n        Description\n    output_activation : TYPE, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    if convolutional:\n        x_tensor = utils.to_tensor(x)\n    else:\n        x_tensor = tf.reshape(tensor=x, shape=[-1, dimensions[0]])\n        dimensions = dimensions[1:]\n    current_input = x_tensor\n\n    Ws = []\n    hs = []\n    shapes = []\n    for layer_i, n_output in enumerate(dimensions):\n        with tf.variable_scope(str(layer_i)):\n            shapes.append(current_input.get_shape().as_list())\n            if convolutional:\n                h, W = utils.conv2d(\n                    x=current_input,\n                    n_output=n_output,\n                    k_h=filter_sizes[layer_i],\n                    k_w=filter_sizes[layer_i],\n                    padding=\'SAME\')\n            else:\n                h, W = utils.linear(x=current_input, n_output=n_output)\n            h = activation(h)\n            Ws.append(W)\n            hs.append(h)\n\n        current_input = h\n\n    shapes.append(h.get_shape().as_list())\n\n    with tf.variable_scope(\'flatten\'):\n        flattened = utils.flatten(current_input)\n\n    with tf.variable_scope(\'hidden\'):\n        if n_hidden:\n            h, W = utils.linear(flattened, n_hidden, name=\'linear\')\n            h = activation(h)\n        else:\n            h = flattened\n\n    return {\'z\': h, \'Ws\': Ws, \'hs\': hs, \'shapes\': shapes}\n\n\ndef decoder(z,\n            shapes,\n            n_hidden=None,\n            dimensions=[],\n            filter_sizes=[],\n            convolutional=False,\n            activation=tf.nn.relu,\n            output_activation=tf.nn.relu):\n    """"""Summary\n\n    Parameters\n    ----------\n    z : TYPE\n        Description\n    shapes : TYPE\n        Description\n    n_hidden : None, optional\n        Description\n    dimensions : list, optional\n        Description\n    filter_sizes : list, optional\n        Description\n    convolutional : bool, optional\n        Description\n    activation : TYPE, optional\n        Description\n    output_activation : TYPE, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    with tf.variable_scope(\'hidden/1\'):\n        if n_hidden:\n            h = utils.linear(z, n_hidden, name=\'linear\')[0]\n            h = activation(h)\n        else:\n            h = z\n\n    with tf.variable_scope(\'hidden/2\'):\n        dims = shapes[0]\n        size = dims[1] * dims[2] * dims[3] if convolutional else dims[1]\n        h = utils.linear(h, size, name=\'linear\')[0]\n        current_input = activation(h)\n        if convolutional:\n            current_input = tf.reshape(\n                current_input,\n                tf.stack(\n                    [tf.shape(current_input)[0], dims[1], dims[2], dims[3]]))\n\n    Ws = []\n    hs = []\n    for layer_i, n_output in enumerate(dimensions[1:]):\n        with tf.variable_scope(\'decoder/{}\'.format(layer_i)):\n            if convolutional:\n                shape = shapes[layer_i + 1]\n                h, W = utils.deconv2d(\n                    x=current_input,\n                    n_output_h=shape[1],\n                    n_output_w=shape[2],\n                    n_output_ch=shape[3],\n                    n_input_ch=shapes[layer_i][3],\n                    k_h=filter_sizes[layer_i],\n                    k_w=filter_sizes[layer_i])\n            else:\n                h, W = utils.linear(x=current_input, n_output=n_output)\n            if (layer_i + 1) < len(dimensions):\n                h = activation(h)\n            else:\n                h = output_activation(h)\n            Ws.append(W)\n            hs.append(h)\n            current_input = h\n\n    z = tf.identity(current_input, name=""x_tilde"")\n    return {\'x_tilde\': current_input, \'Ws\': Ws, \'hs\': hs}\n\n\ndef variational_bayes(h, n_code):\n    """"""Summary\n\n    Parameters\n    ----------\n    h : TYPE\n        Description\n    n_code : TYPE\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    z_mu = utils.linear(h, n_code, name=\'mu\')[0]\n    z_log_sigma = 0.5 * utils.linear(h, n_code, name=\'log_sigma\')[0]\n\n    # Sample from noise distribution p(eps) ~ N(0, 1)\n    epsilon = tf.random_normal(tf.stack([tf.shape(h)[0], n_code]))\n\n    # Sample from posterior\n    z = tf.add(z_mu, tf.multiply(epsilon, tf.exp(z_log_sigma)), name=\'z\')\n    # -log(p(z)/q(z|x)), bits by coding.\n    # variational bound coding costs kl(p(z|x)||q(z|x))\n    # d_kl(q(z|x)||p(z))\n    loss_z = -0.5 * tf.reduce_sum(1.0 + 2.0 * z_log_sigma - tf.square(z_mu) -\n                                  tf.exp(2.0 * z_log_sigma), 1)\n    return z, z_mu, z_log_sigma, loss_z\n\n\ndef discriminator(x,\n                  convolutional=True,\n                  filter_sizes=[5, 5, 5, 5],\n                  activation=tf.nn.relu,\n                  n_filters=[100, 100, 100, 100]):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    convolutional : bool, optional\n        Description\n    filter_sizes : list, optional\n        Description\n    activation : TYPE, optional\n        Description\n    n_filters : list, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    encoding = encoder(\n        x=x,\n        convolutional=convolutional,\n        dimensions=n_filters,\n        filter_sizes=filter_sizes,\n        activation=activation)\n\n    # flatten, then linear to 1 value\n    res = utils.flatten(encoding[\'z\'], name=\'flatten\')\n    if res.get_shape().as_list()[-1] > 1:\n        res = utils.linear(res, 1)[0]\n\n    return {\n        \'logits\': res,\n        \'probs\': tf.nn.sigmoid(res),\n        \'Ws\': encoding[\'Ws\'],\n        \'hs\': encoding[\'hs\']\n    }\n\n\ndef VAE(input_shape=[None, 784],\n        n_filters=[64, 64, 64],\n        filter_sizes=[4, 4, 4],\n        n_hidden=32,\n        n_code=2,\n        activation=tf.nn.tanh,\n        convolutional=False,\n        variational=False):\n    """"""Summary\n\n    Parameters\n    ----------\n    input_shape : list, optional\n        Description\n    n_filters : list, optional\n        Description\n    filter_sizes : list, optional\n        Description\n    n_hidden : int, optional\n        Description\n    n_code : int, optional\n        Description\n    activation : TYPE, optional\n        Description\n    convolutional : bool, optional\n        Description\n    variational : bool, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    # network input / placeholders for train (bn)\n    x = tf.placeholder(tf.float32, input_shape, \'x\')\n\n    with tf.variable_scope(\'encoder\'):\n        encoding = encoder(\n            x=x,\n            n_hidden=n_hidden,\n            convolutional=convolutional,\n            dimensions=n_filters,\n            filter_sizes=filter_sizes,\n            activation=activation)\n\n    if variational:\n        with tf.variable_scope(\'variational\'):\n            z, z_mu, z_log_sigma, loss_z = variational_bayes(\n                h=encoding[\'z\'], n_code=n_code)\n    else:\n        z = encoding[\'z\']\n        loss_z = None\n\n    shapes = encoding[\'shapes\'].copy()\n    shapes.reverse()\n    n_filters = n_filters.copy()\n    n_filters.reverse()\n    n_filters += [input_shape[-1]]\n\n    with tf.variable_scope(\'generator\'):\n        decoding = decoder(\n            z=z,\n            shapes=shapes,\n            n_hidden=n_hidden,\n            dimensions=n_filters,\n            filter_sizes=filter_sizes,\n            convolutional=convolutional,\n            activation=activation)\n\n    x_tilde = decoding[\'x_tilde\']\n    x_flat = utils.flatten(x)\n    x_tilde_flat = utils.flatten(x_tilde)\n\n    # -log(p(x|z))\n    loss_x = tf.reduce_sum(tf.squared_difference(x_flat, x_tilde_flat), 1)\n    return {\n        \'loss_x\': loss_x,\n        \'loss_z\': loss_z,\n        \'x\': x,\n        \'z\': z,\n        \'Ws\': encoding[\'Ws\'],\n        \'hs\': decoding[\'hs\'],\n        \'x_tilde\': x_tilde\n    }\n\n\ndef VAEGAN(input_shape=[None, 784],\n           n_filters=[64, 64, 64],\n           filter_sizes=[4, 4, 4],\n           n_hidden=32,\n           n_code=2,\n           activation=tf.nn.tanh,\n           convolutional=False,\n           variational=False):\n    """"""Summary\n\n    Parameters\n    ----------\n    input_shape : list, optional\n        Description\n    n_filters : list, optional\n        Description\n    filter_sizes : list, optional\n        Description\n    n_hidden : int, optional\n        Description\n    n_code : int, optional\n        Description\n    activation : TYPE, optional\n        Description\n    convolutional : bool, optional\n        Description\n    variational : bool, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    # network input / placeholders for train (bn)\n    x = tf.placeholder(tf.float32, input_shape, \'x\')\n    z_samp = tf.placeholder(tf.float32, [None, n_code], \'z_samp\')\n\n    with tf.variable_scope(\'encoder\'):\n        encoding = encoder(\n            x=x,\n            n_hidden=n_hidden,\n            convolutional=convolutional,\n            dimensions=n_filters,\n            filter_sizes=filter_sizes,\n            activation=activation)\n\n        with tf.variable_scope(\'variational\'):\n            z, z_mu, z_log_sigma, loss_z = variational_bayes(\n                h=encoding[\'z\'], n_code=n_code)\n\n    shapes = encoding[\'shapes\'].copy()\n    shapes.reverse()\n    n_filters_decoder = n_filters.copy()\n    n_filters_decoder.reverse()\n    n_filters_decoder += [input_shape[-1]]\n\n    with tf.variable_scope(\'generator\'):\n        decoding_actual = decoder(\n            z=z,\n            shapes=shapes,\n            n_hidden=n_hidden,\n            convolutional=convolutional,\n            dimensions=n_filters_decoder,\n            filter_sizes=filter_sizes,\n            activation=activation)\n\n    with tf.variable_scope(\'generator\', reuse=True):\n        decoding_sampled = decoder(\n            z=z_samp,\n            shapes=shapes,\n            n_hidden=n_hidden,\n            convolutional=convolutional,\n            dimensions=n_filters_decoder,\n            filter_sizes=filter_sizes,\n            activation=activation)\n\n    with tf.variable_scope(\'discriminator\'):\n        D_real = discriminator(\n            x,\n            filter_sizes=filter_sizes,\n            n_filters=n_filters,\n            activation=activation)\n\n    with tf.variable_scope(\'discriminator\', reuse=True):\n        D_fake = discriminator(\n            decoding_actual[\'x_tilde\'],\n            filter_sizes=filter_sizes,\n            n_filters=n_filters,\n            activation=activation)\n\n    with tf.variable_scope(\'discriminator\', reuse=True):\n        D_samp = discriminator(\n            decoding_sampled[\'x_tilde\'],\n            filter_sizes=filter_sizes,\n            n_filters=n_filters,\n            activation=activation)\n\n    with tf.variable_scope(\'loss\'):\n        # Weights influence of content/style of decoder\n        gamma = tf.placeholder(tf.float32, name=\'gamma\')\n\n        # Discriminator_l Log Likelihood Loss\n        loss_D_llike = 0\n        for h_fake, h_real in zip(D_fake[\'hs\'][3:], D_real[\'hs\'][3:]):\n            loss_D_llike += tf.reduce_sum(0.5 * tf.squared_difference(\n                utils.flatten(h_fake), utils.flatten(h_real)), 1)\n\n        # GAN Loss\n        eps = 1e-12\n        loss_real = tf.reduce_sum(tf.log(D_real[\'probs\'] + eps), 1)\n        loss_fake = tf.reduce_sum(tf.log(1 - D_fake[\'probs\'] + eps), 1)\n        loss_samp = tf.reduce_sum(tf.log(1 - D_samp[\'probs\'] + eps), 1)\n\n        loss_GAN = (loss_real + loss_fake + loss_samp) / 3.0\n\n        loss_enc = tf.reduce_mean(loss_z + loss_D_llike)\n        loss_gen = tf.reduce_mean(gamma * loss_D_llike - loss_GAN)\n        loss_dis = -tf.reduce_mean(loss_GAN)\n\n    return {\n        \'x\': x,\n        \'z\': z,\n        \'x_tilde\': decoding_actual[\'x_tilde\'],\n        \'z_samp\': z_samp,\n        \'x_tilde_samp\': decoding_sampled[\'x_tilde\'],\n        \'loss_real\': loss_real,\n        \'loss_fake\': loss_fake,\n        \'loss_samp\': loss_samp,\n        \'loss_GAN\': loss_GAN,\n        \'loss_D_llike\': loss_D_llike,\n        \'loss_enc\': loss_enc,\n        \'loss_gen\': loss_gen,\n        \'loss_dis\': loss_dis,\n        \'gamma\': gamma\n    }\n\n\ndef train_vaegan(files,\n                 learning_rate=0.00001,\n                 batch_size=64,\n                 n_epochs=250,\n                 n_examples=10,\n                 input_shape=[218, 178, 3],\n                 crop_shape=[64, 64, 3],\n                 crop_factor=0.8,\n                 n_filters=[100, 100, 100, 100],\n                 n_hidden=None,\n                 n_code=128,\n                 convolutional=True,\n                 variational=True,\n                 filter_sizes=[3, 3, 3, 3],\n                 activation=tf.nn.elu,\n                 ckpt_name=""vaegan.ckpt""):\n    """"""Summary\n\n    Parameters\n    ----------\n    files : TYPE\n        Description\n    learning_rate : float, optional\n        Description\n    batch_size : int, optional\n        Description\n    n_epochs : int, optional\n        Description\n    n_examples : int, optional\n        Description\n    input_shape : list, optional\n        Description\n    crop_shape : list, optional\n        Description\n    crop_factor : float, optional\n        Description\n    n_filters : list, optional\n        Description\n    n_hidden : int, optional\n        Description\n    n_code : int, optional\n        Description\n    convolutional : bool, optional\n        Description\n    variational : bool, optional\n        Description\n    filter_sizes : list, optional\n        Description\n    activation : TYPE, optional\n        Description\n    ckpt_name : str, optional\n        Description\n\n    No Longer Returned\n    ------------------\n    name : TYPE\n        Description\n    """"""\n\n    ae = VAEGAN(\n        input_shape=[None] + crop_shape,\n        convolutional=convolutional,\n        variational=variational,\n        n_filters=n_filters,\n        n_hidden=n_hidden,\n        n_code=n_code,\n        filter_sizes=filter_sizes,\n        activation=activation)\n\n    batch = create_input_pipeline(\n        files=files,\n        batch_size=batch_size,\n        n_epochs=n_epochs,\n        crop_shape=crop_shape,\n        crop_factor=crop_factor,\n        shape=input_shape)\n\n    zs = np.random.randn(4, n_code).astype(np.float32)\n    zs = utils.make_latent_manifold(zs, n_examples)\n\n    opt_enc = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n        ae[\'loss_enc\'],\n        var_list=[\n            var_i for var_i in tf.trainable_variables()\n            if var_i.name.startswith(\'encoder\')\n        ])\n\n    opt_gen = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n        ae[\'loss_gen\'],\n        var_list=[\n            var_i for var_i in tf.trainable_variables()\n            if var_i.name.startswith(\'generator\')\n        ])\n\n    opt_dis = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n        ae[\'loss_dis\'],\n        var_list=[\n            var_i for var_i in tf.trainable_variables()\n            if var_i.name.startswith(\'discriminator\')\n        ])\n\n    sess = tf.Session()\n    saver = tf.train.Saver()\n    init_op = tf.group(tf.global_variables_initializer(),\n                       tf.local_variables_initializer())\n    sess.run(init_op)\n    coord = tf.train.Coordinator()\n    tf.get_default_graph().finalize()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    if os.path.exists(ckpt_name + \'.index\') or os.path.exists(ckpt_name):\n        saver.restore(sess, ckpt_name)\n        print(""VAE model restored."")\n\n    t_i = 0\n    batch_i = 0\n    epoch_i = 0\n\n    equilibrium = 0.693\n    margin = 0.4\n\n    n_files = len(files)\n    test_xs = sess.run(batch) / 255.0\n    utils.montage(test_xs, \'test_xs.png\')\n    try:\n        while not coord.should_stop() and epoch_i < n_epochs:\n            if batch_i % (n_files // batch_size) == 0:\n                batch_i = 0\n                epoch_i += 1\n                print(\'---------- EPOCH:\', epoch_i)\n\n            batch_i += 1\n            batch_xs = sess.run(batch) / 255.0\n            batch_zs = np.random.randn(batch_size, n_code).astype(np.float32)\n            real_cost, fake_cost, _ = sess.run(\n                [ae[\'loss_real\'], ae[\'loss_fake\'], opt_enc],\n                feed_dict={ae[\'x\']: batch_xs,\n                           ae[\'gamma\']: 0.5})\n            real_cost = -np.mean(real_cost)\n            fake_cost = -np.mean(fake_cost)\n            print(\'real:\', real_cost, \'/ fake:\', fake_cost)\n\n            gen_update = True\n            dis_update = True\n\n            if real_cost > (equilibrium + margin) or \\\n               fake_cost > (equilibrium + margin):\n                gen_update = False\n\n            if real_cost < (equilibrium - margin) or \\\n               fake_cost < (equilibrium - margin):\n                dis_update = False\n\n            if not (gen_update or dis_update):\n                gen_update = True\n                dis_update = True\n\n            if gen_update:\n                sess.run(\n                    opt_gen,\n                    feed_dict={\n                        ae[\'x\']: batch_xs,\n                        ae[\'z_samp\']: batch_zs,\n                        ae[\'gamma\']: 0.5\n                    })\n            if dis_update:\n                sess.run(\n                    opt_dis,\n                    feed_dict={\n                        ae[\'x\']: batch_xs,\n                        ae[\'z_samp\']: batch_zs,\n                        ae[\'gamma\']: 0.5\n                    })\n\n            if batch_i % 50 == 0:\n\n                # Plot example reconstructions from latent layer\n                recon = sess.run(ae[\'x_tilde\'], feed_dict={ae[\'z\']: zs})\n                print(\'recon:\', recon.min(), recon.max())\n                recon = np.clip(recon / recon.max(), 0, 1)\n                utils.montage(\n                    recon.reshape([-1] + crop_shape),\n                    \'imgs/manifold_%08d.png\' % t_i)\n\n                # Plot example reconstructions\n                recon = sess.run(ae[\'x_tilde\'], feed_dict={ae[\'x\']: test_xs})\n                print(\'recon:\', recon.min(), recon.max())\n                recon = np.clip(recon / recon.max(), 0, 1)\n                utils.montage(\n                    recon.reshape([-1] + crop_shape),\n                    \'imgs/reconstruction_%08d.png\' % t_i)\n                t_i += 1\n\n            if batch_i % 100 == 0:\n                # Save the variables to disk.\n                save_path = saver.save(\n                    sess,\n                    ckpt_name,\n                    global_step=batch_i,\n                    write_meta_graph=False)\n                print(""Model saved in file: %s"" % save_path)\n    except tf.errors.OutOfRangeError:\n        print(\'Done training -- epoch limit reached\')\n    finally:\n        # One of the threads has issued an exception.  So let\'s tell all the\n        # threads to shutdown.\n        coord.request_stop()\n\n    # Wait until all threads have finished.\n    coord.join(threads)\n\n    # Clean up the session.\n    sess.close()\n\n\ndef test_celeb(n_epochs=100,\n               filter_sizes=[3, 3, 3, 3],\n               n_filters=[100, 100, 100, 100],\n               crop_shape=[100, 100, 3]):\n    """"""Summary\n\n    Parameters\n    ----------\n    n_epochs : int, optional\n        Description\n\n    No Longer Returned\n    ------------------\n    name : TYPE\n        Description\n    """"""\n    files = CELEB()\n    train_vaegan(\n        files=files,\n        batch_size=64,\n        n_epochs=n_epochs,\n        crop_shape=crop_shape,\n        crop_factor=0.8,\n        input_shape=[218, 178, 3],\n        convolutional=True,\n        variational=True,\n        n_filters=n_filters,\n        n_hidden=None,\n        n_code=64,\n        filter_sizes=filter_sizes,\n        activation=tf.nn.elu,\n        ckpt_name=\'./celeb.ckpt\')\n\n\ndef test_sita(n_epochs=100):\n    """"""Summary\n\n    Parameters\n    ----------\n    n_epochs : int, optional\n        Description\n\n    No Longer Returned\n    ------------------\n    name : TYPE\n        Description\n    """"""\n    if not os.path.exists(\'sita\'):\n        os.system(\n            \'wget http://ossguy.com/sita/Sita_Sings_the_Blues_640x360_XviD.avi\')\n        os.mkdir(\'sita\')\n        os.system(\'ffmpeg -i Sita_Sings_the_Blues_640x360_XviD.avi -r 60 -f\' +\n                  \' image2 -s 160x90 sita/sita-%08d.jpg\')\n    files = [os.path.join(\'sita\', f) for f in os.listdir(\'sita\')]\n\n    train_vaegan(\n        files=files,\n        batch_size=64,\n        n_epochs=n_epochs,\n        crop_shape=[90, 160, 3],\n        crop_factor=1.0,\n        input_shape=[218, 178, 3],\n        convolutional=True,\n        variational=True,\n        n_filters=[100, 100, 100, 100, 100],\n        n_hidden=250,\n        n_code=100,\n        filter_sizes=[3, 3, 3, 3, 2],\n        activation=tf.nn.elu,\n        ckpt_name=\'./sita.ckpt\')\n\n\nif __name__ == \'__main__\':\n    test_celeb()\n'"
session-5/libs/vgg16.py,19,"b'""""""\nCreative Applications of Deep Learning w/ Tensorflow.\nKadenze, Inc.\nCopyright Parag K. Mital, June 2016.\n""""""\nimport tensorflow as tf\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize as imresize\nfrom .utils import download\n\n\ndef get_vgg_face_model():\n    download(\'https://s3.amazonaws.com/cadl/models/vgg_face.tfmodel\')\n    with open(""vgg_face.tfmodel"", mode=\'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python \' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n\n    download(\'https://s3.amazonaws.com/cadl/models/vgg_face.json\')\n    labels = json.load(open(\'vgg_face.json\'))\n\n    return {\n        \'graph_def\': graph_def,\n        \'labels\': labels,\n        \'preprocess\': preprocess,\n        \'deprocess\': deprocess\n    }\n\n\ndef get_vgg_model():\n    download(\'https://s3.amazonaws.com/cadl/models/vgg16.tfmodel\')\n    with open(""vgg16.tfmodel"", mode=\'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python \' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n\n    download(\'https://s3.amazonaws.com/cadl/models/synset.txt\')\n    with open(\'synset.txt\') as f:\n        labels = [(idx, l.strip()) for idx, l in enumerate(f.readlines())]\n\n    return {\n        \'graph_def\': graph_def,\n        \'labels\': labels,\n        \'preprocess\': preprocess,\n        \'deprocess\': deprocess\n    }\n\n\ndef preprocess(img, crop=True, resize=True, dsize=(224, 224)):\n    if img.dtype == np.uint8:\n        img = img / 255.0\n\n    if crop:\n        short_edge = min(img.shape[:2])\n        yy = int((img.shape[0] - short_edge) / 2)\n        xx = int((img.shape[1] - short_edge) / 2)\n        crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n    else:\n        crop_img = img\n\n    if resize:\n        norm_img = imresize(crop_img, dsize, preserve_range=True)\n    else:\n        norm_img = crop_img\n\n    return (norm_img).astype(np.float32)\n\n\ndef deprocess(img):\n    return np.clip(img * 255, 0, 255).astype(np.uint8)\n    # return ((img / np.max(np.abs(img))) * 127.5 +\n    #         127.5).astype(np.uint8)\n\n\ndef test_vgg():\n    """"""Loads the VGG network and applies it to a test image.\n    """"""\n    with tf.Session() as sess:\n        net = get_vgg_model()\n        tf.import_graph_def(net[\'graph_def\'], name=\'vgg\')\n        g = tf.get_default_graph()\n        names = [op.name for op in g.get_operations()]\n        input_name = names[0] + \':0\'\n        x = g.get_tensor_by_name(input_name)\n        softmax = g.get_tensor_by_name(names[-2] + \':0\')\n\n        og = plt.imread(\'bosch.png\')\n        img = preprocess(og)[np.newaxis, ...]\n        res = np.squeeze(softmax.eval(feed_dict={\n            x: img,\n            \'vgg/dropout_1/random_uniform:0\': [[1.0]],\n            \'vgg/dropout/random_uniform:0\': [[1.0]]}))\n        print([(res[idx], net[\'labels\'][idx])\n               for idx in res.argsort()[-5:][::-1]])\n\n        """"""Let\'s visualize the network\'s gradient activation\n        when backpropagated to the original input image.  This\n        is effectively telling us which pixels contribute to the\n        predicted class or given neuron""""""\n        features = [name for name in names if \'BiasAdd\' in name.split()[-1]]\n        from math import sqrt, ceil\n        n_plots = ceil(sqrt(len(features) + 1))\n        fig, axs = plt.subplots(n_plots, n_plots)\n        plot_i = 0\n        axs[0][0].imshow(img[0])\n        for feature_i, featurename in enumerate(features):\n            plot_i += 1\n            feature = g.get_tensor_by_name(featurename + \':0\')\n            neuron = tf.reduce_max(feature, 1)\n            saliency = tf.gradients(tf.reduce_sum(neuron), x)\n            neuron_idx = tf.arg_max(feature, 1)\n            this_res = sess.run([saliency[0], neuron_idx], feed_dict={\n                x: img,\n                \'vgg/dropout_1/random_uniform:0\': [[1.0]],\n                \'vgg/dropout/random_uniform:0\': [[1.0]]})\n\n            grad = this_res[0][0] / np.max(np.abs(this_res[0]))\n            ax = axs[plot_i // n_plots][plot_i % n_plots]\n            ax.imshow((grad * 127.5 + 127.5).astype(np.uint8))\n            ax.set_title(featurename)\n\n        """"""Deep Dreaming takes the backpropagated gradient activations\n        and simply adds it to the image, running the same process again\n        and again in a loop.  There are many tricks one can add to this\n        idea, such as infinitely zooming into the image by cropping and\n        scaling, adding jitter by randomly moving the image around, or\n        adding constraints on the total activations.""""""\n        og = plt.imread(\'street.png\')\n        crop = 2\n        img = preprocess(og)[np.newaxis, ...]\n        layer = g.get_tensor_by_name(features[3] + \':0\')\n        n_els = layer.get_shape().as_list()[1]\n        neuron_i = np.random.randint(1000)\n        layer_vec = np.zeros((1, n_els))\n        layer_vec[0, neuron_i] = 1\n        neuron = tf.reduce_max(layer, 1)\n        saliency = tf.gradients(tf.reduce_sum(neuron), x)\n        for it_i in range(3):\n            print(it_i)\n            this_res = sess.run(saliency[0], feed_dict={\n                x: img,\n                layer: layer_vec,\n                \'vgg/dropout_1/random_uniform:0\': [[1.0]],\n                \'vgg/dropout/random_uniform:0\': [[1.0]]})\n            grad = this_res[0] / np.mean(np.abs(grad))\n            img = img[:, crop:-crop - 1, crop:-crop - 1, :]\n            img = imresize(img[0], (224, 224))[np.newaxis]\n            img += grad\n        plt.imshow(deprocess(img[0]))\n\n\ndef test_vgg_face():\n    """"""Loads the VGG network and applies it to a test image.\n    """"""\n    with tf.Session() as sess:\n        net = get_vgg_face_model()\n        x = tf.placeholder(tf.float32, [1, 224, 224, 3], name=\'x\')\n        tf.import_graph_def(net[\'graph_def\'], name=\'vgg\',\n                            input_map={\'Placeholder:0\': x})\n        g = tf.get_default_graph()\n        names = [op.name for op in g.get_operations()]\n\n        og = plt.imread(\'bricks.png\')[..., :3]\n        img = preprocess(og)[np.newaxis, ...]\n        plt.imshow(img[0])\n        plt.show()\n\n        """"""Let\'s visualize the network\'s gradient activation\n        when backpropagated to the original input image.  This\n        is effectively telling us which pixels contribute to the\n        predicted class or given neuron""""""\n        features = [name for name in names if \'BiasAdd\' in name.split()[-1]]\n        from math import sqrt, ceil\n        n_plots = ceil(sqrt(len(features) + 1))\n        fig, axs = plt.subplots(n_plots, n_plots)\n        plot_i = 0\n        axs[0][0].imshow(img[0])\n        for feature_i, featurename in enumerate(features):\n            plot_i += 1\n            feature = g.get_tensor_by_name(featurename + \':0\')\n            neuron = tf.reduce_max(feature, 1)\n            saliency = tf.gradients(tf.reduce_sum(neuron), x)\n            neuron_idx = tf.arg_max(feature, 1)\n            this_res = sess.run([saliency[0], neuron_idx], feed_dict={x: img})\n\n            grad = this_res[0][0] / np.max(np.abs(this_res[0]))\n            ax = axs[plot_i // n_plots][plot_i % n_plots]\n            ax.imshow((grad * 127.5 + 127.5).astype(np.uint8))\n            ax.set_title(featurename)\n            plt.waitforbuttonpress()\n\n        """"""Deep Dreaming takes the backpropagated gradient activations\n        and simply adds it to the image, running the same process again\n        and again in a loop.  There are many tricks one can add to this\n        idea, such as infinitely zooming into the image by cropping and\n        scaling, adding jitter by randomly moving the image around, or\n        adding constraints on the total activations.""""""\n        og = plt.imread(\'street.png\')\n        crop = 2\n        img = preprocess(og)[np.newaxis, ...]\n        layer = g.get_tensor_by_name(features[3] + \':0\')\n        n_els = layer.get_shape().as_list()[1]\n        neuron_i = np.random.randint(1000)\n        layer_vec = np.zeros((1, n_els))\n        layer_vec[0, neuron_i] = 1\n        neuron = tf.reduce_max(layer, 1)\n        saliency = tf.gradients(tf.reduce_sum(neuron), x)\n        for it_i in range(3):\n            print(it_i)\n            this_res = sess.run(saliency[0], feed_dict={\n                x: img,\n                layer: layer_vec,\n                \'vgg/dropout_1/random_uniform:0\': [[1.0]],\n                \'vgg/dropout/random_uniform:0\': [[1.0]]})\n            grad = this_res[0] / np.mean(np.abs(grad))\n            img = img[:, crop:-crop - 1, crop:-crop - 1, :]\n            img = imresize(img[0], (224, 224))[np.newaxis]\n            img += grad\n        plt.imshow(deprocess(img[0]))\n\nif __name__ == \'__main__\':\n    test_vgg_face()\n'"
session-5/tests/test_5.py,3,"b""import matplotlib\nmatplotlib.use('Agg')\nimport tensorflow as tf\nimport numpy as np\nfrom libs import utils\nfrom libs import dataset_utils\nfrom libs import charrnn\nfrom libs import vaegan\nfrom libs import celeb_vaegan\n\n\ndef test_alice():\n    charrnn.test_alice()\n\n\ndef test_trump():\n    charrnn.test_trump()\n\n\ndef test_vaegan_training():\n    utils.get_celeb_files()\n    vaegan.test_celeb(n_epochs=1,\n                      crop_shape=[32, 32, 3],\n                      n_filters=[10],\n                      filter_sizes=[3])\n\ndef test_celeb_vaegan():\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        net = celeb_vaegan.get_celeb_vaegan_model()\n        tf.import_graph_def(\n            net['graph_def'],\n            name='net',\n            input_map={\n                'encoder/variational/random_normal:0':\n                np.zeros(512, dtype=np.float32)\n            }\n        )\n        names = [op.name for op in g.get_operations()]\n        print(names)\n"""
