file_path,api_count,code
conftest.py,0,"b'import pytest\nfrom tempfile import TemporaryDirectory\nfrom os import chdir\n\n@pytest.fixture\ndef cleandir():\n    """"""cd to a clean tempdir for tests""""""\n    tempdir = TemporaryDirectory()\n    chdir(tempdir.name)\n    return tempdir\n'"
setup.py,0,"b'import codecs\nimport os\nimport re\n\nfrom setuptools import setup\nfrom setuptools import find_packages\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\n\ndef read(*parts):\n    # intentionally *not* adding an encoding option to open\n    # see here: https://github.com/pypa/virtualenv/issues/201#issuecomment-3145690\n    return codecs.open(os.path.join(here, *parts), \'r\').read()\n\n\ndef find_version(*file_paths):\n    version_file = read(*file_paths)\n    version_match = re.search(r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]"",\n                              version_file, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(""Unable to find version string."")\n\nwith open(\'README.md\') as readme_file:\n    readme = readme_file.read()\n\nwith open(\'HISTORY.md\') as history_file:\n    history = history_file.read()\n\nrequirements = [\n    \'numpy\',\n    \'requests\',\n    \'tensorflow\',\n    \'scipy\',\n    \'matplotlib\',\n    \'scikit-image\',\n    \'magenta\',\n    \'nltk\',\n    \'librosa\',\n    \'bs4\',\n    \'scikit-learn\'\n]\n\n\nsetup(\n    name=\'cadl\',\n    version=find_version(""cadl"", ""__init__.py""),\n    description=""Creative Applications of Deep Learning with TensorFlow"",\n    long_description=readme + \'\\n\\n\' + history,\n    author=""Parag Mital"",\n    author_email=\'parag@pkmital.com\',\n    url=\'https://github.com/pkmital/pycadl\',\n    download_url=\'https://github.com/pkmital/pycadl/archive/{}.tar.gz\'.format(find_version(""cadl"", ""__init__.py"")),\n    packages=find_packages(),\n    include_package_data=True,\n    install_requires=requirements,\n    classifiers=[\n        #\'Development Status :: 3 - Pre-Alpha\',\n        \'Intended Audience :: Developers\',\n        \'Natural Language :: English\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\'\n    ],\n    tests_require=[\'tox\']\n)\n\n\n'"
cadl/__init__.py,0,"b'""""""\nCopyright 2017 Parag K. Mital\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n__version__ = \'v1.1.0\'\n__all__ = [\'batch_norm\', \'celeb_vaegan\', \'charrnn\', \'cornell\', \'cycle_gan\',\n        \'datasets\', \'dataset_utils\', \'deepdream\', \'dft\', \'draw\', \'fastwavenet\',\n        \'gan\', \'gif\', \'glove\', \'i2v\', \'inception\', \'librispeech\', \'magenta_utils\',\n        \'mdn\', \'nb_utils\', \'nsynth\', \'pixelcnn\', \'pixelrnn\', \'seq2seq\', \'squeezenet\',\n        \'stats\', \'stylenet\', \'tedlium\', \'utils\', \'vaegan\', \'vae\', \'vctk\',\n        \'vgg16\', \'wavenet\', \'wavenet_utils\', \'word2vec\']\n'"
cadl/batch_norm.py,13,"b'""""""Batch Normalization for TensorFlow.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef batch_norm(x, phase_train, name=\'bn\', decay=0.9, reuse=None, affine=True):\n    """"""\n    Batch normalization on convolutional maps.\n    from: https://stackoverflow.com/questions/33949786/how-could-i-\n    use-batch-normalization-in-tensorflow\n    Only modified to infer shape from input tensor x.\n\n    [DEPRECATED] Use tflearn or slim batch normalization instead.\n\n    Parameters\n    ----------\n    x\n        Tensor, 4D BHWD input maps\n    phase_train\n        boolean tf.Variable, true indicates training phase\n    name\n        string, variable name\n    decay : float, optional\n        Description\n    reuse : None, optional\n        Description\n    affine\n        whether to affine-transform outputs\n\n    Return\n    ------\n    normed\n        batch-normalized maps\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        shape = x.get_shape().as_list()\n        beta = tf.get_variable(\n            name=\'beta\',\n            shape=[shape[-1]],\n            initializer=tf.constant_initializer(0.0),\n            trainable=True)\n        gamma = tf.get_variable(\n            name=\'gamma\',\n            shape=[shape[-1]],\n            initializer=tf.constant_initializer(1.0),\n            trainable=affine)\n        if len(shape) == 4:\n            batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name=\'moments\')\n        else:\n            batch_mean, batch_var = tf.nn.moments(x, [0], name=\'moments\')\n        ema = tf.train.ExponentialMovingAverage(decay=decay)\n        ema_apply_op = ema.apply([batch_mean, batch_var])\n        ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n\n        def mean_var_with_update():\n           with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n\n        mean, var = control_flow_ops.cond(phase_train, mean_var_with_update,\n                                          lambda: (ema_mean, ema_var))\n\n        # tf.nn.batch_normalization\n        normed = tf.nn.batch_norm_with_global_normalization(\n            x, mean, var, beta, gamma, 1e-6, affine)\n    return normed\n'"
cadl/celeb_vaegan.py,2,"b'""""""Tools for downloading the celeb dataset and model, including preprocessing.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.platform import gfile\nfrom cadl.utils import download\nfrom skimage.transform import resize as imresize\n\n\ndef celeb_vaegan_download():\n    """"""Download a pretrained celeb vae/gan network.\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n\n    # Load the model and labels\n    model = download(\n        \'https://s3.amazonaws.com/cadl/models/celeb.vaegan.tfmodel\')\n    labels = download(\n        \'https://s3.amazonaws.com/cadl/celeb-align/list_attr_celeba.txt\')\n    return model, labels\n\n\ndef get_celeb_vaegan_model():\n    """"""Get a pretrained model.\n\n    Returns\n    -------\n    net : dict\n        {\n            \'graph_def\': tf.GraphDef\n                The graph definition\n            \'labels\': list\n                List of different possible attributes from celeb\n            \'attributes\': np.ndarray\n                One hot encoding of the attributes per image\n                [n_els x n_labels]\n            \'preprocess\': function\n                Preprocess function\n        }\n    """"""\n    # Download the trained net\n    model, labels = celeb_vaegan_download()\n\n    # Parse the ids and synsets\n    txt = open(labels).readlines()\n    n_els = int(txt[0].strip())\n    labels = txt[1].strip().split()\n    n_labels = len(labels)\n    attributes = np.zeros((n_els, n_labels), dtype=bool)\n    for i, txt_i in enumerate(txt[2:]):\n        attributes[i] = (np.array(txt_i.strip().split()[1:]).astype(int) > 0)\n\n    # Load the saved graph\n    with gfile.GFile(model, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n    net = {\n        \'graph_def\': graph_def,\n        \'labels\': labels,\n        \'attributes\': attributes,\n        \'preprocess\': preprocess,\n    }\n    return net\n\n\ndef preprocess(img, crop_factor=0.8):\n    """"""Replicate the preprocessing we did on the VAE/GAN.\n\n    This model used a crop_factor of 0.8 and crop size of [100, 100, 3].\n\n    Parameters\n    ----------\n    img : TYPE\n        Description\n    crop_factor : float, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    crop = np.min(img.shape[:2])\n    r = (img.shape[0] - crop) // 2\n    c = (img.shape[1] - crop) // 2\n    cropped = img[r:r + crop, c:c + crop]\n    r, c, *d = cropped.shape\n    if crop_factor < 1.0:\n        amt = (1 - crop_factor) / 2\n        h, w = int(c * amt), int(r * amt)\n        cropped = cropped[h:-h, w:-w]\n    rsz = imresize(cropped, (100, 100), preserve_range=False)\n    return rsz\n'"
cadl/charrnn.py,43,"b'""""""Character-level Recurrent Neural Network.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport sys\nimport collections\nimport gzip\nfrom cadl import utils\n\n\ndef build_model(txt,\n                batch_size=1,\n                sequence_length=1,\n                n_layers=2,\n                n_cells=100,\n                gradient_clip=10.0,\n                learning_rate=0.001):\n    """"""Summary\n\n    Parameters\n    ----------\n    txt : TYPE\n        Description\n    batch_size : int, optional\n        Description\n    sequence_length : int, optional\n        Description\n    n_layers : int, optional\n        Description\n    n_cells : int, optional\n        Description\n    gradient_clip : float, optional\n        Description\n    learning_rate : float, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    vocab = list(set(txt))\n    vocab.sort()\n    n_chars = len(vocab)\n    encoder = collections.OrderedDict(zip(vocab, range(n_chars)))\n    decoder = collections.OrderedDict(zip(range(n_chars), vocab))\n\n    X = tf.placeholder(tf.int32, [None, sequence_length], name=\'X\')\n    Y = tf.placeholder(tf.int32, [None, sequence_length], name=\'Y\')\n    keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n\n    with tf.variable_scope(\'embedding\'):\n        embedding = tf.get_variable(""embedding"", [n_chars, n_cells])\n        # Each sequence element will be connected to n_cells\n        Xs = tf.nn.embedding_lookup(embedding, X)\n        # Then slice each sequence element, giving us sequence number of\n        # batch x 1 x n_chars Tensors\n        Xs = tf.split(axis=1, num_or_size_splits=sequence_length, value=Xs)\n        # Get rid of singleton sequence element dimension\n        Xs = [tf.squeeze(X_i, [1]) for X_i in Xs]\n\n    with tf.variable_scope(\'rnn\'):\n        cells = tf.contrib.rnn.MultiRNNCell([\n            tf.contrib.rnn.DropoutWrapper(\n                tf.contrib.rnn.BasicLSTMCell(\n                    num_units=n_cells, forget_bias=0.0, state_is_tuple=True),\n                output_keep_prob=keep_prob) for _ in range(n_layers)\n        ])\n        initial_state = cells.zero_state(tf.shape(X)[0], tf.float32)\n        # returns a length sequence length list of outputs, one for each input\n        outputs, final_state = tf.contrib.rnn.static_rnn(\n            cells, Xs, initial_state=initial_state)\n        # now concat the sequence length number of batch x n_cells Tensors to\n        # give [sequence_length x batch, n_cells]\n        outputs_flat = tf.reshape(\n            tf.concat(axis=1, values=outputs), [-1, n_cells])\n\n    with tf.variable_scope(\'prediction\'):\n        W = tf.get_variable(\n            ""W"",\n            shape=[n_cells, n_chars],\n            initializer=tf.contrib.layers.xavier_initializer())\n        b = tf.get_variable(\n            ""b"", shape=[n_chars], initializer=tf.constant_initializer())\n        logits = tf.matmul(outputs_flat, W) + b\n        probs = tf.nn.softmax(logits)\n        Y_pred = tf.argmax(probs, 1)\n\n    with tf.variable_scope(\'loss\'):\n        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [\n            tf.reshape(tf.concat(axis=1, values=Y), [-1])\n        ], [tf.ones([batch_size * sequence_length])])\n        cost = tf.reduce_sum(loss) / batch_size\n\n    with tf.name_scope(\'optimizer\'):\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n        gradients = []\n        clip = tf.constant(gradient_clip, name=""clip"")\n        for grad, var in optimizer.compute_gradients(cost):\n            gradients.append((tf.clip_by_value(grad, -clip, clip), var))\n        updates = optimizer.apply_gradients(gradients)\n\n    model = {\n        \'X\': X,\n        \'Y\': Y,\n        \'logits\': logits,\n        \'probs\': probs,\n        \'Y_pred\': Y_pred,\n        \'keep_prob\': keep_prob,\n        \'cost\': cost,\n        \'updates\': updates,\n        \'initial_state\': initial_state,\n        \'final_state\': final_state,\n        \'decoder\': decoder,\n        \'encoder\': encoder,\n        \'vocab_size\': n_chars\n    }\n    return model\n\n\ndef train(txt,\n          batch_size=100,\n          sequence_length=150,\n          n_cells=200,\n          n_layers=3,\n          learning_rate=0.00001,\n          max_iter=50000,\n          gradient_clip=5.0,\n          ckpt_name=""model.ckpt"",\n          keep_prob=1.0):\n    """"""train\n\n    Parameters\n    ----------\n    txt : TYPE\n        Description\n    batch_size : int, optional\n        Description\n    sequence_length : int, optional\n        Description\n    n_cells : int, optional\n        Description\n    n_layers : int, optional\n        Description\n    learning_rate : float, optional\n        Description\n    max_iter : int, optional\n        Description\n    gradient_clip : float, optional\n        Description\n    ckpt_name : str, optional\n        Description\n    keep_prob : float, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        model = build_model(\n            txt=txt,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            n_layers=n_layers,\n            n_cells=n_cells,\n            gradient_clip=gradient_clip,\n            learning_rate=learning_rate)\n\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        saver = tf.train.Saver()\n        sess.run(init_op)\n        if os.path.exists(ckpt_name + \'.index\') or os.path.exists(ckpt_name):\n            saver.restore(sess, ckpt_name)\n            print(""Model restored."")\n\n        cursor = 0\n        it_i = 0\n        print_step = 1000\n        avg_cost = 0\n        while it_i < max_iter:\n            Xs, Ys = [], []\n            for batch_i in range(batch_size):\n                Xs.append([\n                    model[\'encoder\'][ch]\n                    for ch in txt[cursor:cursor + sequence_length]\n                ])\n                Ys.append([\n                    model[\'encoder\'][ch]\n                    for ch in txt[cursor + 1:cursor + sequence_length + 1]\n                ])\n                cursor += sequence_length\n                if (cursor + 1) >= len(txt) - sequence_length - 1:\n                    cursor = np.random.randint(0, high=sequence_length)\n\n            feed_dict = {\n                model[\'X\']: Xs,\n                model[\'Y\']: Ys,\n                model[\'keep_prob\']: keep_prob\n            }\n            out = sess.run(\n                [model[\'cost\'], model[\'updates\']], feed_dict=feed_dict)\n            avg_cost += out[0]\n\n            if (it_i + 1) % print_step == 0:\n                p = sess.run(\n                    model[\'probs\'],\n                    feed_dict={\n                        model[\'X\']: np.array(Xs[-1])[np.newaxis],\n                        model[\'keep_prob\']: 1.0\n                    })\n                print(p.shape, \'min:\',\n                      np.min(p), \'max:\',\n                      np.max(p), \'mean:\', np.mean(p), \'std:\', np.std(p))\n                if isinstance(txt[0], str):\n                    # Print original string\n                    print(\'original:\',\n                          """".join([model[\'decoder\'][ch] for ch in Xs[-1]]))\n\n                    # Print max guess\n                    amax = []\n                    for p_i in p:\n                        amax.append(model[\'decoder\'][np.argmax(p_i)])\n                    print(\'synth(amax):\', """".join(amax))\n\n                    # Print w/ sampling\n                    samp = []\n                    for p_i in p:\n                        p_i = p_i.astype(np.float64)\n                        p_i = p_i / p_i.sum()\n                        idx = np.argmax(np.random.multinomial(1, p_i.ravel()))\n                        samp.append(model[\'decoder\'][idx])\n                    print(\'synth(samp):\', """".join(samp))\n\n                print(it_i, avg_cost / print_step)\n                avg_cost = 0\n\n                save_path = saver.save(sess, ckpt_name, global_step=it_i)\n                print(""Model saved in file: %s"" % save_path)\n\n            print(it_i, out[0], end=\'\\r\')\n            it_i += 1\n\n        return model\n\n\ndef infer(txt,\n          ckpt_name,\n          n_iterations,\n          n_cells=200,\n          n_layers=3,\n          learning_rate=0.001,\n          max_iter=5000,\n          gradient_clip=10.0,\n          init_value=[0],\n          keep_prob=1.0,\n          sampling=\'prob\',\n          temperature=1.0):\n    """"""infer\n\n    Parameters\n    ----------\n    txt : TYPE\n        Description\n    ckpt_name : TYPE\n        Description\n    n_iterations : TYPE\n        Description\n    n_cells : int, optional\n        Description\n    n_layers : int, optional\n        Description\n    learning_rate : float, optional\n        Description\n    max_iter : int, optional\n        Description\n    gradient_clip : float, optional\n        Description\n    init_value : list, optional\n        Description\n    keep_prob : float, optional\n        Description\n    sampling : str, optional\n        Description\n    temperature : float, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        sequence_length = len(init_value)\n        model = build_model(\n            txt=txt,\n            batch_size=1,\n            sequence_length=sequence_length,\n            n_layers=n_layers,\n            n_cells=n_cells,\n            gradient_clip=gradient_clip,\n            learning_rate=learning_rate)\n\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        saver = tf.train.Saver()\n        sess.run(init_op)\n        saver.restore(sess, ckpt_name)\n        print(""Model restored."")\n\n        state = []\n        synth = [init_value]\n        for s_i in model[\'final_state\']:\n            state += sess.run(\n                [s_i.c, s_i.h],\n                feed_dict={\n                    model[\'X\']: [synth[-1]],\n                    model[\'keep_prob\']: keep_prob\n                })\n\n        for i in range(n_iterations):\n            # print(\'iteration: {}/{}\'.format(i, n_iterations), end=\'\\r\')\n            feed_dict = {model[\'X\']: [synth[-1]], model[\'keep_prob\']: keep_prob}\n            state_updates = []\n            for state_i in range(n_layers):\n                feed_dict[model[\'initial_state\'][state_i].c] = \\\n                    state[state_i * 2]\n                feed_dict[model[\'initial_state\'][state_i].h] = state[state_i * 2\n                                                                     + 1]\n                state_updates.append(model[\'final_state\'][state_i].c)\n                state_updates.append(model[\'final_state\'][state_i].h)\n            p = sess.run(model[\'probs\'], feed_dict=feed_dict)[0]\n            if sampling == \'max\':\n                p = np.argmax(p)\n            else:\n                p = p.astype(np.float64)\n                p = np.log(p) / temperature\n                p = np.exp(p) / np.sum(np.exp(p))\n                p = np.random.multinomial(1, p.ravel())\n                p = np.argmax(p)\n            # Get the current state\n            state = [\n                sess.run(s_i, feed_dict=feed_dict) for s_i in state_updates\n            ]\n            synth.append([p])\n            print(model[\'decoder\'][p], end=\'\')\n            sys.stdout.flush()\n            if model[\'decoder\'][p] in [\'.\', \'?\', \'!\']:\n                print(\'\\n\')\n        print(np.concatenate(synth).shape)\n    print("""".join([model[\'decoder\'][ch] for ch in np.concatenate(synth)]))\n    return [model[\'decoder\'][ch] for ch in np.concatenate(synth)]\n\n\ndef test_alice(max_iter=5):\n    """"""Summary\n\n    Parameters\n    ----------\n    max_iter : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    utils.download(\'https://s3.amazonaws.com/cadl/models/alice.txt.gz\')\n    with gzip.open(\'alice.txt.gz\', \'rb\') as fp:\n        txt = fp.read().decode(\'utf-8\')\n    return train(txt, n_layers=2, n_cells=20, max_iter=max_iter)\n\n\ndef test_trump(max_iter=100):\n    """"""Summary\n\n    Parameters\n    ----------\n    max_iter : int, optional\n        Description\n    """"""\n    utils.download(\n        \'https://s3.amazonaws.com/cadl/models/trump.ckpt.data-00000-of-00001\')\n    utils.download(\'https://s3.amazonaws.com/cadl/models/trump.ckpt.meta\')\n    utils.download(\'https://s3.amazonaws.com/cadl/models/trump.ckpt.index\')\n    utils.download(\'https://s3.amazonaws.com/cadl/models/trump.txt\')\n    with open(\'trump.txt\', \'r\') as fp:\n        txt = fp.read()\n    #train(txt, ckpt_name=\'trump\', max_iter=max_iter)\n    print(infer(txt, ckpt_name=\'./trump.ckpt\', n_iterations=max_iter))\n\n\ndef test_wtc():\n    """"""Summary\n    """"""\n    from scipy.io.wavfile import write, read\n    rate, aud = read(\'wtc.wav\')\n    txt = np.int8(np.round(aud / 16384.0 * 128.0))\n    txt = np.squeeze(txt).tolist()\n    # try with more than 100 iterations, e.g. 50k - 200k\n    train(txt, sequence_length=250, n_layers=3, n_cells=512, max_iter=100)\n    synthesis = infer(\n        txt,\n        \'./model.ckpt\',\n        8000 * 30,\n        n_layers=3,\n        n_cells=150,\n        keep_prob=1.0,\n        sampling=\'prob\')\n    snd = np.int16(np.array(synthesis) / 128.0 * 16384.0)\n    write(\'wtc-synth.wav\', 8000, snd)\n\n\nif __name__ == \'__main__\':\n    test_alice()\n'"
cadl/cornell.py,5,"b'""""""Tools for downloading and preprocessing the Cornell Movie DB.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nAttributes\n----------\nconversations : str\n    Description\nlines : str\n    Description\ntitles : str\n    Description\n""""""\nimport os\nimport ast\nimport nltk\nimport numpy as np\nimport pickle\nimport string\nimport bs4\nimport tensorflow as tf\nfrom cadl import utils\n\ntitles = \'movie_titles_metadata.txt\'\nconversations = \'movie_conversations.txt\'\nlines = \'movie_lines.txt\'\n\n\ndef download_cornell(dst=\'cornell movie-dialogs corpus\'):\n    """"""Summary\n\n    Parameters\n    ----------\n    dst : str, optional\n        Description\n    """"""\n    utils.download_and_extract_zip(\n        \'https://s3.amazonaws.com/cadl/models/cornell_movie_dialogs_corpus.zip\',\n        dst)\n\n\ndef get_characters(path=\'cornell movie-dialogs corpus\'):\n    \'\'\'\n    - movie_characters_metadata.txt\n        - contains information about each movie character\n        - fields:\n            - characterID\n            - character name\n            - movieID\n            - movie title\n            - gender (""?"" for unlabeled cases)\n            - position in credits (""?"" for unlabeled cases)\n\n    Parameters\n    ----------\n    path : TYPE, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    \'\'\'\n    characters = {}\n    download_cornell(path)\n    with open(\n            os.path.join(path, \'movie_characters_metadata.txt\'),\n            \'r\',\n            encoding=\'latin-1\') as f:\n        for line_i in f:\n            els = [el.strip() for el in line_i.split(\'+++$+++\')]\n            characters[els[0]] = {\n                \'character_id\': els[0],\n                \'name\': els[1],\n                \'movie_id\': els[2],\n                \'movie_name\': els[3]\n            }\n    return characters\n\n\ndef get_titles(path=\'cornell movie-dialogs corpus\'):\n    \'\'\'\n    - movie_titles_metadata.txt\n        - contains information about each movie title\n        - fields:\n            - movieID,\n            - movie title,\n            - movie year,\n            - IMDB rating,\n            - no. IMDB votes,\n            - genres in the format [\'genre1\',\'genre2\',\xc3\x89,\'genreN\']\n\n    Parameters\n    ----------\n    path : TYPE, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    \'\'\'\n    titles = {}\n    download_cornell(path)\n    with open(\n            os.path.join(path, \'movie_titles_metadata.txt\'),\n            \'r\',\n            encoding=\'latin-1\') as f:\n        for line_i in f:\n            els = [el.strip() for el in line_i.split(\'+++$+++\')]\n            titles[els[0]] = {\n                \'movie_id\': els[0],\n                \'name\': els[1],\n                \'year\': els[2],\n                \'imdb_rating\': els[3],\n                \'imdb_num_votes\': els[4],\n                \'genres\': els[5]\n            }\n    return titles\n\n\ndef get_conversations(path=\'cornell movie-dialogs corpus\'):\n    \'\'\'\n    - movie_conversations.txt\n        - the structure of the conversations\n        - fields\n            - characterID of the first character involved in the conversation\n            - characterID of the second character involved in the conversation\n            - movieID of the movie in which the conversation occurred\n            - list of the utterances that make the conversation, in\n                chronological order: [\'lineID1\',\'lineID2\',\xc3\x89,\'lineIDN\']\n                has to be matched with movie_lines.txt to reconstruct the\n                actual content\n\n    Parameters\n    ----------\n    path : TYPE, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    \'\'\'\n    conversations = []\n    download_cornell(path)\n    with open(\n            os.path.join(path, \'movie_conversations.txt\'), \'r\',\n            encoding=\'latin-1\') as f:\n        for line_i in f:\n            els = [el.strip() for el in line_i.split(\'+++$+++\')]\n            conversations.append({\n                \'character_id_1\': els[0],\n                \'character_id_2\': els[1],\n                \'movie_id\': els[2],\n                \'lines\': els[3]\n            })\n    return conversations\n\n\ndef get_lines(path=\'cornell movie-dialogs corpus\'):\n    \'\'\'\n    - movie_lines.txt\n        - contains the actual text of each utterance\n        - fields:\n            - lineID\n            - characterID (who uttered this phrase)\n            - movieID\n            - character name\n            - text of the utterance\n\n    Parameters\n    ----------\n    path : TYPE, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    \'\'\'\n    lines = {}\n    download_cornell(path)\n    with open(\n            os.path.join(path, \'movie_lines.txt\'), \'r\',\n            encoding=\'latin-1\') as f:\n        for line_i in f:\n            els = [el.strip() for el in line_i.split(\'+++$+++\')]\n            lines[els[0]] = {\n                \'line_id\': els[0],\n                \'character_id\': els[1],\n                \'movie_id\': els[2],\n                \'character_name\': els[3],\n                \'text\': els[4]\n            }\n    return lines\n\n\ndef get_scripts(path=\'cornell movie-dialogs corpus\'):\n    """"""Summary\n\n    Parameters\n    ----------\n    path : TYPE, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    lines = get_lines(path)\n    conversations = get_conversations(path)\n    script = []\n    for conv_i in conversations:\n        if len(conv_i[\'lines\']) >= 2:\n            for line_i in ast.literal_eval(conv_i[\'lines\']):\n                this_line = bs4.BeautifulSoup(lines[line_i][\'text\'],\n                                              \'lxml\').text\n                script.append(this_line)\n    return script\n\n\ndef preprocess(text, min_count=10, max_length=40):\n    """"""Summary\n\n    Parameters\n    ----------\n    text : TYPE\n        Description\n    min_count : int, optional\n        Description\n    max_length : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    sentences = [el for s in text for el in nltk.sent_tokenize(s)]\n\n    # We\'ll first tokenize each sentence into words to get a sense of\n    # how long each sentence is:\n    words = [[\n        word.lower() for word in nltk.word_tokenize(s)\n        if word not in string.punctuation\n    ] for s in sentences]\n\n    # Then see how long each sentence is:\n    lengths = np.array([len(s) for s in words])\n\n    good_idxs = np.where(lengths <= max_length)[0]\n    dataset = [words[idx] for idx in good_idxs]\n    fdist = nltk.FreqDist([word for sentence in dataset for word in sentence])\n\n    vocab_counts = [el for el in fdist.most_common() if el[1] > min_count]\n\n    # First sort the vocabulary\n    vocab = [v[0] for v in vocab_counts]\n    vocab.sort()\n    # Now add the special symbols:\n    vocab = [\'_PAD\', \'_GO\', \'_EOS\', \'_UNK\'] + vocab\n    # Then create the word to id mapping\n    vocab = {k: v for v, k in enumerate(vocab)}\n\n    with open(\'vocab.pkl\', \'wb\') as fp:\n        pickle.dump(vocab, fp)\n\n    unked = word2id(dataset, vocab)\n    return unked, vocab\n\n\ndef word2id(words, vocab, UNK_ID=3):\n    """"""Summary\n\n    Parameters\n    ----------\n    words : TYPE\n        Description\n    vocab : TYPE\n        Description\n    UNK_ID : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    unked = []\n    for s in words:\n        this_sentence = [vocab.get(w, UNK_ID) for w in s]\n        unked.append(this_sentence)\n    return unked\n\n\ndef id2word(ids, vocab):\n    """"""Summary\n\n    Parameters\n    ----------\n    ids : TYPE\n        Description\n    vocab : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    words = []\n    id2words = {v: k for k, v in vocab.items()}\n    for s in ids:\n        this_sentence = [id2words.get(w) for w in s]\n        words.append(this_sentence)\n    return words\n\n\ndef test_train():\n    """"""Test training of cornell dataset with deprecated bucketed seq2seq model.\n    """"""\n    from cadl.deprecated import seq2seq_model as seq\n    text = get_scripts()\n    unked, vocab = preprocess(text)\n\n    # Create bucketed pairs\n    buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n    pairs = {i: [] for i in range(len(buckets))}\n    for pair_input, pair_output in zip(unked[:-1], unked[1:]):\n        n_in, n_out = len(pair_input), len(pair_output)\n        for bucket_i, bucket in enumerate(buckets):\n            if n_in <= bucket[0] and n_out <= bucket[1]:\n                pairs[bucket_i].append((pair_input[::-1], pair_output))\n                break\n\n    vocab_size = len(vocab)\n\n    with tf.Session() as sess:\n        net = seq.Seq2SeqModel(\n            source_vocab_size=vocab_size,\n            target_vocab_size=vocab_size,\n            buckets=buckets,\n            size=300,\n            num_layers=3,\n            max_gradient_norm=10.0,\n            batch_size=64,\n            learning_rate=0.0001,\n            learning_rate_decay_factor=0.8,\n            use_lstm=False)\n\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        sess.run(init_op)\n\n        n_iterations = 10000000\n        lengths = [len(p) for p in pairs.values()]\n        lengths = np.cumsum(lengths) / sum(lengths)\n        for it_i in range(n_iterations):\n            r = np.random.rand()\n            bucket_id = 0\n            while r > lengths[bucket_id]:\n                bucket_id += 1\n            encoder_inputs, decoder_inputs, target_weights = \\\n                net.get_batch(pairs, bucket_id)\n            gradient_norm, perplexity, outputs = net.step(\n                sess,\n                encoder_inputs,\n                decoder_inputs,\n                target_weights,\n                bucket_id,\n                forward_only=False)\n            print(\'{}: {}\'.format(it_i, perplexity), end=\'\\r\')\n\n            if it_i % 10000 == 0:\n                net.saver.save(\n                    sess, \'./seq2seq.ckpt\', global_step=net.global_step)\n\n\ndef test_decode(sentence):\n    """"""Test decoding of cornell dataset with deprecated seq2seq model.\n\n    Parameters\n    ----------\n    sentence : TYPE\n        Description\n    """"""\n    from cadl.deprecated import seq2seq_model as seq\n    text = get_scripts()\n    pairs, vocab, buckets = preprocess(text)\n    vocab_size = len(vocab)\n\n    with tf.Session() as sess:\n        net = seq.Seq2SeqModel(\n            source_vocab_size=vocab_size,\n            target_vocab_size=vocab_size,\n            buckets=buckets,\n            size=300,\n            num_layers=3,\n            batch_size=1,\n            max_gradient_norm=10.0,\n            learning_rate=0.0001,\n            learning_rate_decay_factor=0.8,\n            forward_only=True,\n            use_lstm=False)\n\n        ckpt_path = tf.train.get_checkpoint_state(\'./\').model_checkpoint_path\n        net.saver.restore(sess, ckpt_path)\n\n        def decode(sentence):\n            """"""Summary\n\n            Parameters\n            ----------\n            sentence : TYPE\n                Description\n\n            Returns\n            -------\n            TYPE\n                Description\n            """"""\n            bucket_id = len(buckets) - 1\n            preprocessed = [\n                word\n                for s in nltk.sent_tokenize(sentence.lower())\n                for word in nltk.word_tokenize(s)\n                if word not in string.punctuation\n            ][::-1]\n            if len(preprocessed) <= 0:\n                return\n            for b_i, b in enumerate(buckets):\n                if b[0] >= len(preprocessed):\n                    bucket_id = b_i\n                    break\n            tokens = word2id(preprocessed, vocab)\n            encoder_inputs, decoder_inputs, target_weights = \\\n                net.get_batch({bucket_id: [(tokens[0], [])]}, bucket_id)\n            _, _, output_logits = net.step(sess, encoder_inputs, decoder_inputs,\n                                           target_weights, bucket_id, True)\n            outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n            print(sentence, \'\\n\', "" "".join(id2word([outputs], vocab)[0]))\n\n\nif __name__ == \'__main__\':\n    test_train()\n'"
cadl/cycle_gan.py,76,"b'""""""Cycle Generative Adversarial Network for Unpaired Image to Image translation.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport numpy as np\nimport os\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.contrib.layers as tfl\nfrom cadl.utils import imcrop_tosquare\nfrom scipy.misc import imresize\n\n\ndef l1loss(x, y):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    y : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    return tf.reduce_mean(tf.abs(x - y))\n\n\ndef l2loss(x, y):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    y : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    return tf.reduce_mean(tf.squared_difference(x, y))\n\n\ndef lrelu(x, leak=0.2, name=""lrelu""):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    leak : float, optional\n        Description\n    name : str, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    with tf.variable_scope(name):\n        return tf.maximum(x, leak * x)\n\n\ndef instance_norm(x, epsilon=1e-5):\n    """"""Instance Normalization.\n\n    See Ulyanov, D., Vedaldi, A., & Lempitsky, V. (2016).\n    Instance Normalization: The Missing Ingredient for Fast Stylization,\n    Retrieved from http://arxiv.org/abs/1607.08022\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    epsilon : float, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    with tf.variable_scope(\'instance_norm\'):\n        mean, var = tf.nn.moments(x, [1, 2], keep_dims=True)\n        scale = tf.get_variable(\n            name=\'scale\',\n            shape=[x.get_shape()[-1]],\n            initializer=tf.truncated_normal_initializer(mean=1.0, stddev=0.02))\n        offset = tf.get_variable(\n            name=\'offset\',\n            shape=[x.get_shape()[-1]],\n            initializer=tf.constant_initializer(0.0))\n        out = scale * tf.div(x - mean, tf.sqrt(var + epsilon)) + offset\n        return out\n\n\ndef conv2d(inputs,\n           activation_fn=lrelu,\n           normalizer_fn=instance_norm,\n           scope=\'conv2d\',\n           **kwargs):\n    """"""Summary\n\n    Parameters\n    ----------\n    inputs : TYPE\n        Description\n    activation_fn : TYPE, optional\n        Description\n    normalizer_fn : TYPE, optional\n        Description\n    scope : str, optional\n        Description\n    **kwargs\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    with tf.variable_scope(scope or \'conv2d\'):\n        h = tfl.conv2d(\n            inputs=inputs,\n            activation_fn=None,\n            normalizer_fn=None,\n            weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n            biases_initializer=None,\n            **kwargs)\n        if normalizer_fn:\n            h = normalizer_fn(h)\n        if activation_fn:\n            h = activation_fn(h)\n        return h\n\n\ndef conv2d_transpose(inputs,\n                     activation_fn=lrelu,\n                     normalizer_fn=instance_norm,\n                     scope=\'conv2d_transpose\',\n                     **kwargs):\n    """"""Summary\n\n    Parameters\n    ----------\n    inputs : TYPE\n        Description\n    activation_fn : TYPE, optional\n        Description\n    normalizer_fn : TYPE, optional\n        Description\n    scope : str, optional\n        Description\n    **kwargs\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    with tf.variable_scope(scope or \'conv2d_transpose\'):\n        h = tfl.conv2d_transpose(\n            inputs=inputs,\n            activation_fn=None,\n            normalizer_fn=None,\n            weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n            biases_initializer=None,\n            **kwargs)\n        if normalizer_fn:\n            h = normalizer_fn(h)\n        if activation_fn:\n            h = activation_fn(h)\n        return h\n\n\ndef residual_block(x, n_channels=128, kernel_size=3, scope=None):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    n_channels : int, optional\n        Description\n    kernel_size : int, optional\n        Description\n    scope : None, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    with tf.variable_scope(scope or \'residual\'):\n        h = tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]], ""REFLECT"")\n        h = conv2d(\n            inputs=h,\n            num_outputs=n_channels,\n            kernel_size=kernel_size,\n            padding=\'VALID\',\n            scope=\'1\')\n        h = tf.pad(h, [[0, 0], [1, 1], [1, 1], [0, 0]], ""REFLECT"")\n        h = conv2d(\n            inputs=h,\n            num_outputs=n_channels,\n            kernel_size=kernel_size,\n            activation_fn=None,\n            padding=\'VALID\',\n            scope=\'2\')\n        h = tf.add(x, h)\n    return h\n\n\ndef encoder(x, n_filters=32, k_size=3, scope=None):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    n_filters : int, optional\n        Description\n    k_size : int, optional\n        Description\n    scope : None, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    with tf.variable_scope(scope or \'encoder\'):\n        h = tf.pad(x, [[0, 0], [k_size, k_size], [k_size, k_size], [0, 0]],\n                   ""REFLECT"")\n        h = conv2d(\n            inputs=h,\n            num_outputs=n_filters,\n            kernel_size=7,\n            activation_fn=tf.nn.relu,\n            stride=1,\n            padding=\'VALID\',\n            scope=\'1\')\n        h = conv2d(\n            inputs=h,\n            num_outputs=n_filters * 2,\n            kernel_size=k_size,\n            activation_fn=tf.nn.relu,\n            stride=2,\n            scope=\'2\')\n        h = conv2d(\n            inputs=h,\n            num_outputs=n_filters * 4,\n            kernel_size=k_size,\n            activation_fn=tf.nn.relu,\n            stride=2,\n            scope=\'3\')\n    return h\n\n\ndef transform(x, img_size=256):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    img_size : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    h = x\n    if img_size >= 256:\n        n_blocks = 9\n    else:\n        n_blocks = 6\n    for block_i in range(n_blocks):\n        with tf.variable_scope(\'block_{}\'.format(block_i)):\n            h = residual_block(h)\n    return h\n\n\ndef decoder(x, n_filters=32, k_size=3, scope=None):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    n_filters : int, optional\n        Description\n    k_size : int, optional\n        Description\n    scope : None, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    with tf.variable_scope(scope or \'decoder\'):\n        h = conv2d_transpose(\n            inputs=x,\n            num_outputs=n_filters * 2,\n            kernel_size=k_size,\n            activation_fn=tf.nn.relu,\n            stride=2,\n            scope=\'1\')\n        h = conv2d_transpose(\n            inputs=h,\n            num_outputs=n_filters,\n            kernel_size=k_size,\n            activation_fn=tf.nn.relu,\n            stride=2,\n            scope=\'2\')\n        h = tf.pad(h, [[0, 0], [k_size, k_size], [k_size, k_size], [0, 0]],\n                   ""REFLECT"")\n        h = conv2d(\n            inputs=h,\n            num_outputs=3,\n            kernel_size=7,\n            stride=1,\n            padding=\'VALID\',\n            activation_fn=tf.nn.tanh,\n            scope=\'3\')\n    return h\n\n\ndef generator(x, scope=None, reuse=None):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    scope : None, optional\n        Description\n    reuse : None, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    img_size = x.get_shape().as_list()[1]\n    with tf.variable_scope(scope or \'generator\', reuse=reuse):\n        h = encoder(x)\n        h = transform(h, img_size)\n        h = decoder(h)\n    return h\n\n\ndef discriminator(x, n_filters=64, k_size=4, scope=None, reuse=None):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    n_filters : int, optional\n        Description\n    k_size : int, optional\n        Description\n    scope : None, optional\n        Description\n    reuse : None, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    with tf.variable_scope(scope or \'discriminator\', reuse=reuse):\n        h = conv2d(\n            inputs=x,\n            num_outputs=n_filters,\n            kernel_size=k_size,\n            stride=2,\n            normalizer_fn=None,\n            scope=\'1\')\n        h = conv2d(\n            inputs=h,\n            num_outputs=n_filters * 2,\n            kernel_size=k_size,\n            stride=2,\n            scope=\'2\')\n        h = conv2d(\n            inputs=h,\n            num_outputs=n_filters * 4,\n            kernel_size=k_size,\n            stride=2,\n            scope=\'3\')\n        h = conv2d(\n            inputs=h,\n            num_outputs=n_filters * 8,\n            kernel_size=k_size,\n            stride=1,\n            scope=\'4\')\n        h = conv2d(\n            inputs=h,\n            num_outputs=1,\n            kernel_size=k_size,\n            stride=1,\n            activation_fn=tf.nn.sigmoid,\n            scope=\'5\')\n        return h\n\n\ndef cycle_gan(img_size=256):\n    """"""Summary\n\n    Parameters\n    ----------\n    img_size : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    X_real = tf.placeholder(\n        name=\'X\', shape=[1, img_size, img_size, 3], dtype=tf.float32)\n    Y_real = tf.placeholder(\n        name=\'Y\', shape=[1, img_size, img_size, 3], dtype=tf.float32)\n    X_fake_sample = tf.placeholder(\n        name=\'X_fake_sample\',\n        shape=[None, img_size, img_size, 3],\n        dtype=tf.float32)\n    Y_fake_sample = tf.placeholder(\n        name=\'Y_fake_sample\',\n        shape=[None, img_size, img_size, 3],\n        dtype=tf.float32)\n\n    X_fake = generator(Y_real, scope=\'G_yx\')\n    Y_fake = generator(X_real, scope=\'G_xy\')\n    X_cycle = generator(Y_fake, scope=\'G_yx\', reuse=True)\n    Y_cycle = generator(X_fake, scope=\'G_xy\', reuse=True)\n\n    D_X_real = discriminator(X_real, scope=\'D_X\')\n    D_Y_real = discriminator(Y_real, scope=\'D_Y\')\n    D_X_fake = discriminator(X_fake, scope=\'D_X\', reuse=True)\n    D_Y_fake = discriminator(Y_fake, scope=\'D_Y\', reuse=True)\n    D_X_fake_sample = discriminator(X_fake_sample, scope=\'D_X\', reuse=True)\n    D_Y_fake_sample = discriminator(Y_fake_sample, scope=\'D_Y\', reuse=True)\n\n    # Create losses for generators\n    l1 = 10.0\n    loss_cycle_X = l1 * l1loss(X_real, X_cycle)\n    loss_cycle_Y = l1 * l1loss(Y_real, Y_cycle)\n    loss_G_xy = l2loss(D_Y_fake, 1.0)\n    loss_G_yx = l2loss(D_X_fake, 1.0)\n    loss_G = loss_G_xy + loss_G_yx + loss_cycle_X + loss_cycle_Y\n\n    # Create losses for discriminators\n    loss_D_Y = l2loss(D_Y_real, 1.0) + l2loss(D_Y_fake_sample, 0.0)\n    loss_D_X = l2loss(D_X_real, 1.0) + l2loss(D_X_fake_sample, 0.0)\n\n    # Summaries for monitoring training\n    tf.summary.histogram(""D_X_real"", D_X_real)\n    tf.summary.histogram(""D_Y_real"", D_Y_real)\n    tf.summary.histogram(""D_X_fake"", D_X_fake)\n    tf.summary.histogram(""D_Y_fake"", D_Y_fake)\n    tf.summary.histogram(""D_X_fake_sample"", D_X_fake_sample)\n    tf.summary.histogram(""D_Y_fake_sample"", D_Y_fake_sample)\n    tf.summary.image(""X_real"", X_real, max_outputs=1)\n    tf.summary.image(""Y_real"", Y_real, max_outputs=1)\n    tf.summary.image(""X_fake"", X_fake, max_outputs=1)\n    tf.summary.image(""Y_fake"", Y_fake, max_outputs=1)\n    tf.summary.image(""X_cycle"", X_cycle, max_outputs=1)\n    tf.summary.image(""Y_cycle"", Y_cycle, max_outputs=1)\n    tf.summary.histogram(""X_real"", X_real)\n    tf.summary.histogram(""Y_real"", Y_real)\n    tf.summary.histogram(""X_fake"", X_fake)\n    tf.summary.histogram(""Y_fake"", Y_fake)\n    tf.summary.histogram(""X_cycle"", X_cycle)\n    tf.summary.histogram(""Y_cycle"", Y_cycle)\n    tf.summary.scalar(""loss_D_X"", loss_D_X)\n    tf.summary.scalar(""loss_D_Y"", loss_D_Y)\n    tf.summary.scalar(""loss_cycle_X"", loss_cycle_X)\n    tf.summary.scalar(""loss_cycle_Y"", loss_cycle_Y)\n    tf.summary.scalar(""loss_G"", loss_G)\n    tf.summary.scalar(""loss_G_xy"", loss_G_xy)\n    tf.summary.scalar(""loss_G_yx"", loss_G_yx)\n    summaries = tf.summary.merge_all()\n\n    training_vars = tf.trainable_variables()\n    D_X_vars = [v for v in training_vars if v.name.startswith(\'D_X\')]\n    D_Y_vars = [v for v in training_vars if v.name.startswith(\'D_Y\')]\n    G_xy_vars = [v for v in training_vars if v.name.startswith(\'G_xy\')]\n    G_yx_vars = [v for v in training_vars if v.name.startswith(\'G_yx\')]\n    G_vars = G_xy_vars + G_yx_vars\n\n    return locals()\n\n\ndef get_images(path1, path2, img_size=256):\n    """"""Summary\n\n    Parameters\n    ----------\n    path1 : TYPE\n        Description\n    path2 : TYPE\n        Description\n    img_size : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    files1 = [os.path.join(path1, f) for f in os.listdir(path1)]\n    files2 = [os.path.join(path2, f) for f in os.listdir(path2)]\n    imgs1 = []\n    for f in files1:\n        try:\n            img = imresize(imcrop_tosquare(plt.imread(f)), (img_size, img_size))\n        except:\n            continue\n        if img.ndim == 3:\n            imgs1.append(img[..., :3])\n        else:\n            img = img[..., np.newaxis]\n            imgs1.append(np.concatenate([img] * 3, 2))\n    imgs1 = np.array(imgs1) / 127.5 - 1.0\n    imgs2 = []\n    for f in files2:\n        try:\n            img = imresize(imcrop_tosquare(plt.imread(f)), (img_size, img_size))\n        except:\n            continue\n        if img.ndim == 3:\n            imgs2.append(img[..., :3])\n        else:\n            img = img[..., np.newaxis]\n            imgs2.append(np.concatenate([img] * 3, 2))\n    imgs2 = np.array(imgs2) / 127.5 - 1.0\n\n    return imgs1, imgs2\n\n\ndef batch_generator_dataset(imgs1, imgs2):\n    """"""Summary\n\n    Parameters\n    ----------\n    imgs1 : TYPE\n        Description\n    imgs2 : TYPE\n        Description\n\n    Yields\n    ------\n    TYPE\n        Description\n    """"""\n    n_imgs = min(len(imgs1), len(imgs2))\n    rand_idxs1 = np.random.permutation(np.arange(len(imgs1)))[:n_imgs]\n    rand_idxs2 = np.random.permutation(np.arange(len(imgs2)))[:n_imgs]\n    for idx1, idx2 in zip(rand_idxs1, rand_idxs2):\n        yield imgs1[[idx1]], imgs2[[idx2]]\n\n\ndef batch_generator_random_crop(X, Y, min_size=256, max_size=512, n_images=100):\n    """"""Summary\n\n    Parameters\n    ----------\n    X : TYPE\n        Description\n    Y : TYPE\n        Description\n    min_size : int, optional\n        Description\n    max_size : int, optional\n        Description\n    n_images : int, optional\n        Description\n\n    Yields\n    ------\n    TYPE\n        Description\n    """"""\n    r, c, d = X.shape\n    Xs, Ys = [], []\n    for img_i in range(n_images):\n        size = np.random.randint(min_size, max_size)\n        max_r = r - size\n        max_c = c - size\n        this_r = np.random.randint(0, max_r)\n        this_c = np.random.randint(0, max_c)\n        img = imresize(X[this_r:this_r + size, this_c:this_c + size, :],\n                       (min_size, min_size))\n        Xs.append(img)\n        img = imresize(Y[this_r:this_r + size, this_c:this_c + size, :],\n                       (min_size, min_size))\n        Ys.append(img)\n    imgs1, imgs2 = np.array(Xs) / 127.5 - 1.0, np.array(Ys) / 127.5 - 1.0\n    n_imgs = min(len(imgs1), len(imgs2))\n    rand_idxs1 = np.random.permutation(np.arange(len(imgs1)))[:n_imgs]\n    rand_idxs2 = np.random.permutation(np.arange(len(imgs2)))[:n_imgs]\n    for idx1, idx2 in zip(rand_idxs1, rand_idxs2):\n        yield imgs1[[idx1]], imgs2[[idx2]]\n\n\ndef train(ds_X,\n          ds_Y,\n          ckpt_path=\'cycle_gan\',\n          learning_rate=0.0002,\n          n_epochs=100,\n          img_size=256):\n    """"""Summary\n\n    Parameters\n    ----------\n    ds_X : TYPE\n        Description\n    ds_Y : TYPE\n        Description\n    ckpt_path : str, optional\n        Description\n    learning_rate : float, optional\n        Description\n    n_epochs : int, optional\n        Description\n    img_size : int, optional\n        Description\n    """"""\n    if ds_X.ndim == 3:\n        batch_generator = batch_generator_random_crop\n    else:\n        batch_generator = batch_generator_dataset\n\n    # How many fake generations to keep around\n    capacity = 50\n\n    # Storage for fake generations\n    fake_Xs = capacity * [\n        np.zeros((1, img_size, img_size, 3), dtype=np.float32)\n    ]\n    fake_Ys = capacity * [\n        np.zeros((1, img_size, img_size, 3), dtype=np.float32)\n    ]\n    idx = 0\n    it_i = 0\n\n    # Train\n    with tf.Graph().as_default(), tf.Session() as sess:\n        # Load the network\n        net = cycle_gan(img_size=img_size)\n\n        # Build optimizers\n        D_X = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n            net[\'loss_D_X\'], var_list=net[\'D_X_vars\'])\n        D_Y = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n            net[\'loss_D_Y\'], var_list=net[\'D_Y_vars\'])\n        G = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n            net[\'loss_G\'], var_list=net[\'G_vars\'])\n\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        sess.run(init_op)\n        saver = tf.train.Saver()\n        writer = tf.summary.FileWriter(ckpt_path)\n        for epoch_i in range(n_epochs):\n            for X, Y in batch_generator(ds_X, ds_Y):\n\n                # First generate in both directions\n                X_fake, Y_fake = sess.run(\n                    [net[\'X_fake\'], net[\'Y_fake\']],\n                    feed_dict={net[\'X_real\']: X,\n                               net[\'Y_real\']: Y})\n\n                # Now sample from history\n                if it_i < capacity:\n                    # Not enough samples yet, fill up history buffer\n                    fake_Xs[idx] = X_fake\n                    fake_Ys[idx] = Y_fake\n                    idx = (idx + 1) % capacity\n                elif np.random.random() > 0.5:\n                    # Swap out a random idx from history\n                    rand_idx = np.random.randint(0, capacity)\n                    fake_Xs[rand_idx], X_fake = X_fake, fake_Xs[rand_idx]\n                    fake_Ys[rand_idx], Y_fake = Y_fake, fake_Ys[rand_idx]\n                else:\n                    # Use current generation\n                    pass\n\n                # Optimize G Networks\n                loss_G = sess.run(\n                    [net[\'loss_G\'], G],\n                    feed_dict={\n                        net[\'X_real\']: X,\n                        net[\'Y_real\']: Y,\n                        net[\'Y_fake_sample\']: Y_fake,\n                        net[\'X_fake_sample\']: X_fake\n                    })[0]\n\n                # Optimize D_Y\n                loss_D_Y = sess.run(\n                    [net[\'loss_D_Y\'], D_Y],\n                    feed_dict={\n                        net[\'X_real\']: X,\n                        net[\'Y_real\']: Y,\n                        net[\'Y_fake_sample\']: Y_fake\n                    })[0]\n\n                # Optimize D_X\n                loss_D_X = sess.run(\n                    [net[\'loss_D_X\'], D_X],\n                    feed_dict={\n                        net[\'X_real\']: X,\n                        net[\'Y_real\']: Y,\n                        net[\'X_fake_sample\']: X_fake\n                    })[0]\n\n                print(it_i, \'G:\', loss_G, \'D_X:\', loss_D_X, \'D_Y:\', loss_D_Y)\n\n                # Update summaries\n                if it_i % 100 == 0:\n                    summary = sess.run(\n                        net[\'summaries\'],\n                        feed_dict={\n                            net[\'X_real\']: X,\n                            net[\'Y_real\']: Y,\n                            net[\'X_fake_sample\']: X_fake,\n                            net[\'Y_fake_sample\']: Y_fake\n                        })\n                    writer.add_summary(summary, it_i)\n                it_i += 1\n\n            # Save\n            if epoch_i % 50 == 0:\n                saver.save(\n                    sess,\n                    os.path.join(ckpt_path, \'model.ckpt\'),\n                    global_step=epoch_i)\n'"
cadl/dataset_utils.py,8,"b'""""""Utils for creating datasets.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nimport os\nimport pickle\nimport numpy as np\nimport tensorflow as tf\nfrom . import dft\nfrom .utils import download_and_extract_zip, download_and_extract_tar\n\n\ndef create_input_pipeline(files,\n                          batch_size,\n                          n_epochs,\n                          shape,\n                          crop_shape=None,\n                          crop_factor=1.0,\n                          n_threads=2):\n    """"""Creates a pipefile from a list of image files.\n    Includes batch generator/central crop/resizing options.\n    The resulting generator will dequeue the images batch_size at a time until\n    it throws tf.errors.OutOfRangeError when there are no more images left in\n    the queue.\n\n    Parameters\n    ----------\n    files : list\n        List of paths to image files.\n    batch_size : int\n        Number of image files to load at a time.\n    n_epochs : int\n        Number of epochs to run before raising tf.errors.OutOfRangeError\n    shape : list\n        [height, width, channels]\n    crop_shape : list\n        [height, width] to crop image to.\n    crop_factor : float\n        Percentage of image to take starting from center.\n    n_threads : int, optional\n        Number of threads to use for batch shuffling\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n\n    # We first create a ""producer"" queue.  It creates a production line which\n    # will queue up the file names and allow another queue to deque the file\n    # names all using a tf queue runner.\n    # Put simply, this is the entry point of the computational graph.\n    # It will generate the list of file names.\n    # We also specify it\'s capacity beforehand.\n    producer = tf.train.string_input_producer(\n        files, capacity=len(files), num_epochs=n_epochs)\n\n    # We need something which can open the files and read its contents.\n    reader = tf.WholeFileReader()\n\n    # We pass the filenames to this object which can read the file\'s contents.\n    # This will create another queue running which dequeues the previous queue.\n    keys, vals = reader.read(producer)\n\n    # And then have to decode its contents as we know it is a jpeg image\n    imgs = tf.image.decode_jpeg(\n        vals, channels=3 if len(shape) > 2 and shape[2] == 3 else 0)\n\n    # We have to explicitly define the shape of the tensor.\n    # This is because the decode_jpeg operation is still a node in the graph\n    # and doesn\'t yet know the shape of the image.  Future operations however\n    # need explicit knowledge of the image\'s shape in order to be created.\n    imgs.set_shape(shape)\n\n    # Next we\'ll centrally crop the image to the size of 100x100.\n    # This operation required explicit knowledge of the image\'s shape.\n    if shape[0] > shape[1]:\n        rsz_shape = [\n            int(shape[0] / shape[1] * crop_shape[0] / crop_factor), int(\n                crop_shape[1] / crop_factor)\n        ]\n    else:\n        rsz_shape = [\n            int(crop_shape[0] / crop_factor),\n            int(shape[1] / shape[0] * crop_shape[1] / crop_factor)\n        ]\n    rszs = tf.image.resize_images(imgs, rsz_shape)\n    crops = (tf.image.resize_image_with_crop_or_pad(rszs, crop_shape[0],\n                                                    crop_shape[1])\n             if crop_shape is not None else imgs)\n\n    # Now we\'ll create a batch generator that will also shuffle our examples.\n    # We tell it how many it should have in its buffer when it randomly\n    # permutes the order.\n    min_after_dequeue = len(files) // 100\n\n    # The capacity should be larger than min_after_dequeue, and determines how\n    # many examples are prefetched.  TF docs recommend setting this value to:\n    # min_after_dequeue + (num_threads + a small safety margin) * batch_size\n    capacity = min_after_dequeue + (n_threads + 1) * batch_size\n\n    # Randomize the order and output batches of batch_size.\n    batch = tf.train.shuffle_batch(\n        [crops],\n        enqueue_many=False,\n        batch_size=batch_size,\n        capacity=capacity,\n        min_after_dequeue=min_after_dequeue,\n        num_threads=n_threads)\n\n    # alternatively, we could use shuffle_batch_join to use multiple reader\n    # instances, or set shuffle_batch\'s n_threads to higher than 1.\n\n    return batch\n\n\ndef gtzan_music_speech_download(dst=\'gtzan_music_speech\'):\n    """"""Download the GTZAN music and speech dataset.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Location to put the GTZAN music and speech datset.\n    """"""\n    path = \'http://opihi.cs.uvic.ca/sound/music_speech.tar.gz\'\n    download_and_extract_tar(path, dst)\n\n\ndef gtzan_music_speech_load(dst=\'gtzan_music_speech\'):\n    """"""Load the GTZAN Music and Speech dataset.\n\n    Downloads the dataset if it does not exist into the dst directory.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Location of GTZAN Music and Speech dataset.\n\n    Returns\n    -------\n    Xs, ys : np.ndarray, np.ndarray\n        Array of data, Array of labels\n    """"""\n    from scipy.io import wavfile\n\n    if not os.path.exists(dst):\n        gtzan_music_speech_download(dst)\n    music_dir = os.path.join(os.path.join(dst, \'music_speech\'), \'music_wav\')\n    music = [\n        os.path.join(music_dir, file_i) for file_i in os.listdir(music_dir)\n        if file_i.endswith(\'.wav\')\n    ]\n    speech_dir = os.path.join(os.path.join(dst, \'music_speech\'), \'speech_wav\')\n    speech = [\n        os.path.join(speech_dir, file_i) for file_i in os.listdir(speech_dir)\n        if file_i.endswith(\'.wav\')\n    ]\n    Xs = []\n    ys = []\n    for i in music:\n        sr, s = wavfile.read(i)\n        s = s / 16384.0 - 1.0\n        re, im = dft.dft_np(s)\n        mag, phs = dft.ztoc(re, im)\n        Xs.append((mag, phs))\n        ys.append(0)\n    for i in speech:\n        sr, s = wavfile.read(i)\n        s = s / 16384.0 - 1.0\n        re, im = dft.dft_np(s)\n        mag, phs = dft.ztoc(re, im)\n        Xs.append((mag, phs))\n        ys.append(1)\n    Xs = np.array(Xs)\n    Xs = np.transpose(Xs, [0, 2, 3, 1])\n    ys = np.array(ys)\n    return Xs, ys\n\n\ndef cifar10_download(dst=\'cifar10\'):\n    """"""Download the CIFAR10 dataset.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Directory to download into.\n    """"""\n    path = \'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'\n    download_and_extract_tar(path, dst)\n\n\ndef tiny_imagenet_load(dst=\'tiny_imagenet\'):\n    """"""Loads the paths to every file in the Tiny Imagenet Dataset.\n\n    Downloads the dataset if it does not exist into the dst directory.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Location of Tiny ImageNet dataset.\n\n    Returns\n    -------\n    all_files : list\n        List of paths to every file in the Tiny ImageNet Dataset\n    """"""\n    if not os.path.exists(dst):\n        tiny_imagenet_download(dst)\n\n    all_files = []\n    all_labels = []\n\n    words = {}\n    with open(\n            os.path.join(os.path.join(dst, \'tiny-imagenet-200\'), \'words.txt\'),\n            \'r\') as fp:\n        for line in fp:\n            s = line.split(\'\\t\', maxsplit=1)\n            words.update({s[0]: s[1].strip()})\n\n    for ds_type in [\'train\', \'val\', \'test\']:\n        path = os.path.join(dst, \'tiny-imagenet-200\')\n        path = os.path.join(path, ds_type)\n        for root, dirs, files in os.walk(path):\n            for f in files:\n                if f.endswith(\'JPEG\'):\n                    if ds_type == \'train\':\n                        try:\n                            label = words[root.split(\'/\')[-2]]\n                        except:\n                            print(root, f)\n                            raise\n                    else:\n                        label = \'\'\n                    all_files.append(os.path.join(root, f))\n                    all_labels.append(label)\n\n    return all_files, all_labels\n\n\ndef tiny_imagenet_download(dst=\'tiny_imagenet\'):\n    """"""Download the Tiny ImageNet dataset.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Directory to download into.\n    """"""\n    path = \'http://cs231n.stanford.edu/tiny-imagenet-200.zip\'\n    download_and_extract_zip(path, dst)\n\n\ndef cifar10_load(dst=\'cifar10\'):\n    """"""Load the CIFAR10 dataset.\n\n    Downloads the dataset if it does not exist into the dst directory.\n\n    Parameters\n    ----------\n    dst : str, optional\n        Location of CIFAR10 dataset.\n\n    Returns\n    -------\n    Xs, ys : np.ndarray, np.ndarray\n        Array of data, Array of labels\n    """"""\n    if not os.path.exists(dst):\n        cifar10_download(dst)\n    Xs = None\n    ys = None\n    for f in range(1, 6):\n        cf = pickle.load(\n            open(\'%s/cifar-10-batches-py/data_batch_%d\' % (dst, f), \'rb\'),\n            encoding=\'LATIN\')\n        if Xs is not None:\n            Xs = np.r_[Xs, cf[\'data\']]\n            ys = np.r_[ys, np.array(cf[\'labels\'])]\n        else:\n            Xs = cf[\'data\']\n            ys = cf[\'labels\']\n    Xs = np.swapaxes(np.swapaxes(Xs.reshape(-1, 3, 32, 32), 1, 3), 1, 2)\n    return Xs, ys\n\n\ndef dense_to_one_hot(labels, n_classes=2):\n    """"""Convert class labels from scalars to one-hot vectors.\n\n    Parameters\n    ----------\n    labels : array\n        Input labels to convert to one-hot representation.\n    n_classes : int, optional\n        Number of possible one-hot.\n\n    Returns\n    -------\n    one_hot : array\n        One hot representation of input.\n    """"""\n    return np.eye(n_classes).astype(np.float32)[labels]\n\n\nclass DatasetSplit(object):\n    """"""Utility class for batching data and handling multiple splits.\n\n    Attributes\n    ----------\n    current_batch_idx : int\n        Description\n    images : np.ndarray\n        Xs of the dataset.  Not necessarily images.\n    labels : np.ndarray\n        ys of the dataset.\n    n_classes : int\n        Number of possible labels\n    num_examples : int\n        Number of total observations\n    """"""\n\n    def __init__(self, images, labels):\n        """"""Initialize a DatasetSplit object.\n\n        Parameters\n        ----------\n        images : np.ndarray\n            Xs/inputs\n        labels : np.ndarray\n            ys/outputs\n        """"""\n        self.images = np.array(images).astype(np.float32)\n        if labels is not None:\n            self.labels = np.array(labels).astype(np.int32)\n            self.n_classes = len(np.unique(labels))\n        else:\n            self.labels = None\n        self.num_examples = len(self.images)\n\n    def next_batch(self, batch_size=100):\n        """"""Batch generator with randomization.\n\n        Parameters\n        ----------\n        batch_size : int, optional\n            Size of each minibatch.\n\n        Yields\n        ------\n        Xs, ys : np.ndarray, np.ndarray\n            Next batch of inputs and labels (if no labels, then None).\n        """"""\n        # Shuffle each epoch\n        current_permutation = np.random.permutation(range(len(self.images)))\n        epoch_images = self.images[current_permutation, ...]\n        if self.labels is not None:\n            epoch_labels = self.labels[current_permutation, ...]\n\n        # Then iterate over the epoch\n        self.current_batch_idx = 0\n        while self.current_batch_idx < len(self.images):\n            end_idx = min(self.current_batch_idx + batch_size, len(self.images))\n            this_batch = {\n                \'images\':\n                epoch_images[self.current_batch_idx:end_idx],\n                \'labels\':\n                epoch_labels[self.current_batch_idx:end_idx]\n                if self.labels is not None else None\n            }\n            self.current_batch_idx += batch_size\n            yield this_batch[\'images\'], this_batch[\'labels\']\n\n\nclass Dataset(object):\n    """"""Create a dataset from data and their labels.\n\n    Allows easy use of train/valid/test splits; Batch generator.\n\n    Attributes\n    ----------\n    all_idxs : list\n        All indexes across all splits.\n    all_inputs : list\n        All inputs across all splits.\n    all_labels : list\n        All labels across all splits.\n    n_classes : int\n        Number of labels.\n    split : list\n        Percentage split of train, valid, test sets.\n    test_idxs : list\n        Indexes of the test split.\n    train_idxs : list\n        Indexes of the train split.\n    valid_idxs : list\n        Indexes of the valid split.\n    """"""\n\n    def __init__(self, Xs, ys=None, split=[1.0, 0.0, 0.0], one_hot=False, n_classes=1):\n        """"""Initialize a Dataset object.\n\n        Parameters\n        ----------\n        Xs : np.ndarray\n            Images/inputs to a network\n        ys : np.ndarray\n            Labels/outputs to a network\n        split : list, optional\n            Percentage of train, valid, and test sets.\n        one_hot : bool, optional\n            Whether or not to use one-hot encoding of labels (ys).\n        n_classes : int, optional\n            Number of classes represented in ys (used for one hot embedding).\n        """"""\n        self.all_idxs = []\n        self.all_labels = []\n        self.all_inputs = []\n        self.train_idxs = []\n        self.valid_idxs = []\n        self.test_idxs = []\n        self.n_classes = n_classes\n        self.split = split\n\n        # Now mix all the labels that are currently stored as blocks\n        self.all_inputs = Xs\n        n_idxs = len(self.all_inputs)\n        idxs = range(n_idxs)\n        rand_idxs = np.random.permutation(idxs)\n        self.all_inputs = self.all_inputs[rand_idxs, ...]\n        if ys is not None:\n            self.all_labels = ys if not one_hot else dense_to_one_hot(ys, n_classes=n_classes)\n            self.all_labels = self.all_labels[rand_idxs, ...]\n        else:\n            self.all_labels = None\n\n        # Get splits\n        self.train_idxs = idxs[:round(split[0] * n_idxs)]\n        self.valid_idxs = idxs[len(self.train_idxs):\n                               len(self.train_idxs) + round(split[1] * n_idxs)]\n        self.test_idxs = idxs[(len(self.valid_idxs) + len(self.train_idxs)):\n                              (len(self.valid_idxs) + len(self.train_idxs)\n                               ) + round(split[2] * n_idxs)]\n\n    @property\n    def X(self):\n        """"""Inputs/Xs/Images.\n\n        Returns\n        -------\n        all_inputs : np.ndarray\n            Original Inputs/Xs.\n        """"""\n        return self.all_inputs\n\n    @property\n    def Y(self):\n        """"""Outputs/ys/Labels.\n\n        Returns\n        -------\n        all_labels : np.ndarray\n            Original Outputs/ys.\n        """"""\n        return self.all_labels\n\n    @property\n    def train(self):\n        """"""Train split.\n\n        Returns\n        -------\n        split : DatasetSplit\n            Split of the train dataset.\n        """"""\n        if len(self.train_idxs):\n            inputs = self.all_inputs[self.train_idxs, ...]\n            if self.all_labels is not None:\n                labels = self.all_labels[self.train_idxs, ...]\n            else:\n                labels = None\n        else:\n            inputs, labels = [], []\n        return DatasetSplit(inputs, labels)\n\n    @property\n    def valid(self):\n        """"""Validation split.\n\n        Returns\n        -------\n        split : DatasetSplit\n            Split of the validation dataset.\n        """"""\n        if len(self.valid_idxs):\n            inputs = self.all_inputs[self.valid_idxs, ...]\n            if self.all_labels is not None:\n                labels = self.all_labels[self.valid_idxs, ...]\n            else:\n                labels = None\n        else:\n            inputs, labels = [], []\n        return DatasetSplit(inputs, labels)\n\n    @property\n    def test(self):\n        """"""Test split.\n\n        Returns\n        -------\n        split : DatasetSplit\n            Split of the test dataset.\n        """"""\n        if len(self.test_idxs):\n            inputs = self.all_inputs[self.test_idxs, ...]\n            if self.all_labels is not None:\n                labels = self.all_labels[self.test_idxs, ...]\n            else:\n                labels = None\n        else:\n            inputs, labels = [], []\n        return DatasetSplit(inputs, labels)\n\n    def mean(self):\n        """"""Mean of the inputs/Xs.\n\n        Returns\n        -------\n        mean : np.ndarray\n            Calculates mean across 0th (batch) dimension.\n        """"""\n        return np.mean(self.all_inputs, axis=0)\n\n    def std(self):\n        """"""Standard deviation of the inputs/Xs.\n\n        Returns\n        -------\n        std : np.ndarray\n            Calculates std across 0th (batch) dimension.\n        """"""\n        return np.std(self.all_inputs, axis=0)\n'"
cadl/datasets.py,0,"b'""""""Utils for loading common datasets.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport tensorflow.examples.tutorials.mnist.input_data as input_data\nimport os\nimport numpy as np\nfrom cadl.dataset_utils import \\\n    Dataset, cifar10_load, tiny_imagenet_load, gtzan_music_speech_load\n\n\ndef MNIST(one_hot=True, split=[1.0, 0.0, 0.0]):\n    """"""Returns the MNIST dataset.\n\n    Returns\n    -------\n    mnist : DataSet\n        DataSet object w/ convenienve props for accessing\n        train/validation/test sets and batches.\n\n    Parameters\n    ----------\n    one_hot : bool, optional\n        Description\n    split : list, optional\n        Description\n    """"""\n    ds = input_data.read_data_sets(\'MNIST_data/\', one_hot=one_hot)\n    return Dataset(\n        np.r_[ds.train.images, ds.validation.images, ds.test.images],\n        np.r_[ds.train.labels, ds.validation.labels, ds.test.labels],\n        split=split)\n\n\ndef CIFAR10(flatten=True, split=[1.0, 0.0, 0.0]):\n    """"""Returns the CIFAR10 dataset.\n\n    Parameters\n    ----------\n    flatten : bool, optional\n        Convert the 3 x 32 x 32 pixels to a single vector\n    split : list, optional\n        Description\n\n    Returns\n    -------\n    cifar : Dataset\n        Description\n    """"""\n    # plt.imshow(np.transpose(np.reshape(\n    #   cifar.train.images[10], (3, 32, 32)), [1, 2, 0]))\n    Xs, ys = cifar10_load()\n    if flatten:\n        Xs = Xs.reshape((Xs.shape[0], -1))\n    return Dataset(Xs, ys, split=split)\n\n\ndef CELEB(path=\'./img_align_celeba/\'):\n    """"""Attempt to load the files of the CELEB dataset.\n\n    Requires the files already be downloaded and placed in the `dst` directory.\n    The first 100 files can be downloaded from the cadl.utils function get_celeb_files\n\n    http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n\n    Parameters\n    ----------\n    path : str, optional\n        Directory where the aligned/cropped celeb dataset can be found.\n\n    Returns\n    -------\n    files : list\n        List of file paths to the dataset.\n    """"""\n    if not os.path.exists(path):\n        print(\'Could not find celeb dataset under {}.\'.format(path))\n        print(\'Try downloading the dataset from the ""Aligned and Cropped"" \'\n              \'link located here (imgs/img_align_celeba.zip [1.34 GB]): \'\n              \'http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html \'\n              \'Or you can download the first 100 files using the \'\n              \'utils.get_celeb_files function.\')\n        return None\n    else:\n        fs = [\n            os.path.join(path, f) for f in os.listdir(path)\n            if f.endswith(\'.jpg\')\n        ]\n        if len(fs) < 202598:\n            print(\n                \'WARNING: Loaded only a small subset of the CELEB dataset. \'\n                \'If you want to use the entire 1.3 GB dataset, try downloading \'\n                \'the dataset from the ""Aligned and Cropped"" \'\n                \'link located here (imgs/img_align_celeba.zip [1.34 GB]): \'\n                \'http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\')\n        return fs\n\n\ndef TINYIMAGENET(path=\'./tiny_imagenet/\'):\n    """"""Attempt to load the files of the Tiny ImageNet dataset.\n\n    http://cs231n.stanford.edu/tiny-imagenet-200.zip\n    https://tiny-imagenet.herokuapp.com/\n\n    Parameters\n    ----------\n    path : str, optional\n        Directory where the dataset can be found or else will be placed.\n\n    Returns\n    -------\n    files : list\n        List of file paths to the dataset.\n    labels : list\n        List of labels for each file (only training files have labels)\n    """"""\n    files, labels = tiny_imagenet_load(path)\n    return files, labels\n\n\ndef GTZAN(path=\'./gtzan_music_speech\'):\n    """"""Load the GTZAN Music and Speech dataset.\n\n    Downloads the dataset if it does not exist into the dst directory.\n\n    Parameters\n    ----------\n    path : str, optional\n        Description\n\n    Returns\n    -------\n    ds : Dataset\n        Dataset object with array of data in X and array of labels in Y\n\n    Deleted Parameters\n    ------------------\n    dst : str, optional\n        Location of GTZAN Music and Speech dataset.\n    """"""\n    return Dataset(*gtzan_music_speech_load())\n'"
cadl/deepdream.py,27,"b'""""""Deep Dream using the Inception v5 network.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom scipy.ndimage.filters import gaussian_filter\nfrom skimage.transform import resize\nfrom scipy.misc import imsave\nfrom cadl import inception, vgg16, i2v\nfrom cadl import gif\n\n\ndef get_labels(model=\'inception\'):\n    """"""Return labels corresponding to the `neuron_i` parameter of deep dream.\n\n    Parameters\n    ----------\n    model : str, optional\n        Which model to load. Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n\n    Raises\n    ------\n    ValueError\n        Unknown model.  Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    if model == \'inception\':\n        net = inception.get_inception_model()\n        return net[\'labels\']\n    elif model == \'i2v_tag\':\n        net = i2v.get_i2v_tag_model()\n        return net[\'labels\']\n    elif model == \'vgg16\':\n        net = vgg16.get_vgg_model()\n        return net[\'labels\']\n    elif model == \'vgg_face\':\n        net = vgg16.get_vgg_face_model()\n        return net[\'labels\']\n    else:\n        raise ValueError(""Unknown model or this model does not have labels!"")\n\n\ndef get_layer_names(model=\'inception\'):\n    """"""Retun every layer\'s index and name in the given model.\n\n    Parameters\n    ----------\n    model : str, optional\n        Which model to load. Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n\n    Returns\n    -------\n    names : list of tuples\n        The index and layer\'s name for every layer in the given model.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        if model == \'inception\':\n            net = inception.get_inception_model()\n        elif model == \'vgg_face\':\n            net = vgg16.get_vgg_face_model()\n        elif model == \'vgg16\':\n            net = vgg16.get_vgg_model()\n        elif model == \'i2v\':\n            net = i2v.get_i2v_model()\n        elif model == \'i2v-tag\':\n            net = i2v.get_i2v_tag_model()\n\n        tf.import_graph_def(net[\'graph_def\'], name=\'net\')\n        names = [(i, op.name) for i, op in enumerate(g.get_operations())]\n        return names\n\n\ndef _setup(input_img, model, downsize):\n    """"""Internal use only. Load the given model\'s graph and preprocess an image.\n\n    Parameters\n    ----------\n    input_img : np.ndarray\n        Image to process with the model\'s normalizaiton process.\n    model : str\n        Which model to load. Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n    downsize : bool\n        Optionally crop/resize the input image to the standard shape.  Only\n        applies to inception network which is all convolutional.\n\n    Returns\n    -------\n    net, img, preprocess, deprocess : dict, np.ndarray, function, function\n        net : The networks graph_def and labels\n        img : The preprocessed input image\n        preprocess: Function for preprocessing an image\n        deprocess: Function for deprocessing an image\n\n    Raises\n    ------\n    ValueError\n        If model is unknown.\n    """"""\n    if model == \'inception\':\n        net = inception.get_inception_model()\n        img = inception.preprocess(\n            input_img, resize=downsize, crop=downsize)[np.newaxis]\n        deprocess, preprocess = inception.deprocess, inception.preprocess\n    elif model == \'vgg_face\':\n        net = vgg16.get_vgg_face_model()\n        img = vgg16.preprocess(input_img)[np.newaxis]\n        deprocess, preprocess = vgg16.deprocess, vgg16.preprocess\n    elif model == \'vgg16\':\n        net = vgg16.get_vgg_model()\n        img = vgg16.preprocess(input_img)[np.newaxis]\n        deprocess, preprocess = vgg16.deprocess, vgg16.preprocess\n    elif model == \'i2v\':\n        net = i2v.get_i2v_model()\n        img = i2v.preprocess(input_img)[np.newaxis]\n        deprocess, preprocess = i2v.deprocess, i2v.preprocess\n    elif model == \'i2v_tag\':\n        net = i2v.get_i2v_tag_model()\n        img = i2v.preprocess(input_img)[np.newaxis]\n        deprocess, preprocess = i2v.deprocess, i2v.preprocess\n    else:\n        raise ValueError(""Unknown model name!  Supported: "" +\n                         ""[\'inception\', \'vgg_face\', \'vgg16\', \'i2v\', \'i2v_tag\']"")\n\n    return net, img, preprocess, deprocess\n\n\ndef _apply(img,\n           gradient,\n           it_i,\n           decay=0.998,\n           sigma=1.5,\n           blur_step=10,\n           step=1.0,\n           crop=0,\n           crop_step=1,\n           pth=0):\n    """"""Interal use only. Apply the gradient to an image with the given params.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        Tensor to apply gradient ascent to.\n    gradient : np.ndarray\n        Gradient to ascend to.\n    it_i : int\n        Current iteration (used for step modulos)\n    decay : float, optional\n        Amount to decay.\n    sigma : float, optional\n        Sigma for Gaussian Kernel.\n    blur_step : int, optional\n        How often to blur.\n    step : float, optional\n        Step for gradient ascent.\n    crop : int, optional\n        Amount to crop from each border.\n    crop_step : int, optional\n        How often to crop.\n    pth : int, optional\n        Percentile to mask out.\n\n    No Longer Returned\n    ------------------\n    img : np.ndarray\n        Ascended image.\n    """"""\n    gradient /= (np.std(gradient) + 1e-10)\n    img += gradient * step\n    img *= decay\n\n    if pth:\n        mask = (np.abs(img) < np.percentile(np.abs(img), pth))\n        img = img - img * mask\n\n    if blur_step and it_i % blur_step == 0:\n        for ch_i in range(3):\n            img[..., ch_i] = gaussian_filter(img[..., ch_i], sigma)\n\n    if crop and it_i % crop_step == 0:\n        height, width, *ch = img[0].shape\n\n        # Crop a 1 pixel border from height and width\n        img = img[:, crop:-crop, crop:-crop, :]\n\n        # Resize\n        img = resize(\n            img[0], (height, width), order=3, clip=False,\n            preserve_range=True)[np.newaxis].astype(np.float32)\n\n\ndef deep_dream(input_img,\n               downsize=False,\n               model=\'inception\',\n               layer_i=-1,\n               neuron_i=-1,\n               n_iterations=100,\n               save_gif=None,\n               save_images=\'imgs\',\n               device=\'/cpu:0\',\n               **kwargs):\n    """"""Deep Dream with the given parameters.\n\n    Parameters\n    ----------\n    input_img : np.ndarray\n        Image to apply deep dream to.  Should be 3-dimenionsal H x W x C\n        RGB uint8 or float32.\n    downsize : bool, optional\n        Whether or not to downsize the image.  Only applies to\n        model==\'inception\'.\n    model : str, optional\n        Which model to load.  Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n    layer_i : int, optional\n        Which layer to use for finding the gradient.  E.g. the softmax layer\n        for inception is -1, for vgg networks it is -2.  Use the function\n        ""get_layer_names"" to find the layer number that you need.\n    neuron_i : int, optional\n        Which neuron to use.  -1 for the entire layer.\n    n_iterations : int, optional\n        Number of iterations to dream.\n    save_gif : bool, optional\n        Save a GIF.\n    save_images : str, optional\n        Folder to save images to.\n    device : str, optional\n        Which device to use, e.g. [\'/cpu:0\'] or \'/gpu:0\'.\n    **kwargs : dict\n        See ""_apply"" for additional parameters.\n\n    Returns\n    -------\n    imgs : list of np.array\n        Images of every iteration\n    """"""\n    net, img, preprocess, deprocess = _setup(input_img, model, downsize)\n    batch, height, width, *ch = img.shape\n\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess, g.device(device):\n\n        tf.import_graph_def(net[\'graph_def\'], name=\'net\')\n        names = [op.name for op in g.get_operations()]\n        input_name = names[0] + \':0\'\n        x = g.get_tensor_by_name(input_name)\n\n        layer = g.get_tensor_by_name(names[layer_i] + \':0\')\n        layer_shape = sess.run(tf.shape(layer), feed_dict={x: img})\n        layer_vec = np.ones(layer_shape) / layer_shape[-1]\n        layer_vec[..., neuron_i] = 1.0 - (1.0 / layer_shape[-1])\n\n        ascent = tf.gradients(layer, x)\n\n        imgs = []\n        for it_i in range(n_iterations):\n            print(it_i, np.min(img), np.max(img))\n            if neuron_i == -1:\n                this_res = sess.run(ascent, feed_dict={x: img})[0]\n            else:\n                this_res = sess.run(\n                    ascent, feed_dict={x: img,\n                                       layer: layer_vec})[0]\n\n            _apply(img, this_res, it_i, **kwargs)\n            imgs.append(deprocess(img[0]))\n\n            if save_images is not None:\n                if not os.path.exists(save_images):\n                    os.mkdir(save_images)\n                imsave(\n                    os.path.join(save_images, \'frame{}.png\'.format(it_i)),\n                    imgs[-1])\n\n        if save_gif is not None:\n            gif.build_gif(imgs, saveto=save_gif)\n\n    return imgs\n\n\ndef guided_dream(input_img,\n                 guide_img=None,\n                 downsize=False,\n                 layers=[162, 183, 184, 247],\n                 label_i=962,\n                 layer_i=-1,\n                 feature_loss_weight=1.0,\n                 tv_loss_weight=1.0,\n                 l2_loss_weight=1.0,\n                 softmax_loss_weight=1.0,\n                 model=\'inception\',\n                 neuron_i=920,\n                 n_iterations=100,\n                 save_gif=None,\n                 save_images=\'imgs\',\n                 device=\'/cpu:0\',\n                 **kwargs):\n    """"""Deep Dream v2.  Use an optional guide image and other techniques.\n\n    Parameters\n    ----------\n    input_img : np.ndarray\n        Image to apply deep dream to.  Should be 3-dimenionsal H x W x C\n        RGB uint8 or float32.\n    guide_img : np.ndarray, optional\n        Optional image to find features at different layers for.  Must pass in\n        a list of layers that you want to find features for.  Then the guided\n        dream will try to match this images features at those layers.\n    downsize : bool, optional\n        Whether or not to downsize the image.  Only applies to\n        model==\'inception\'.\n    layers : list, optional\n        A list of layers to find features for in the ""guide_img"".\n    label_i : int, optional\n        Which label to use for the softmax layer.  Use the ""get_labels"" function\n        to find the index corresponding the object of interest.  If None, not\n        used.\n    layer_i : int, optional\n        Which layer to use for finding the gradient.  E.g. the softmax layer\n        for inception is -1, for vgg networks it is -2.  Use the function\n        ""get_layer_names"" to find the layer number that you need.\n    feature_loss_weight : float, optional\n        Weighting for the feature loss from the guide_img.\n    tv_loss_weight : float, optional\n        Total variational loss weighting.  Enforces smoothness.\n    l2_loss_weight : float, optional\n        L2 loss weighting.  Enforces smaller values and reduces saturation.\n    softmax_loss_weight : float, optional\n        Softmax loss weighting.  Must set label_i.\n    model : str, optional\n        Which model to load.  Must be one of: [\'inception\'], \'i2v_tag\', \'i2v\',\n        \'vgg16\', or \'vgg_face\'.\n    neuron_i : int, optional\n        Which neuron to use.  -1 for the entire layer.\n    n_iterations : int, optional\n        Number of iterations to dream.\n    save_gif : bool, optional\n        Save a GIF.\n    save_images : str, optional\n        Folder to save images to.\n    device : str, optional\n        Which device to use, e.g. [\'/cpu:0\'] or \'/gpu:0\'.\n    **kwargs : dict\n        See ""_apply"" for additional parameters.\n\n    Returns\n    -------\n    imgs : list of np.ndarray\n        Images of the dream.\n    """"""\n    net, img, preprocess, deprocess = _setup(input_img, model, downsize)\n    print(img.shape, input_img.shape)\n    print(img.min(), img.max())\n\n    if guide_img is not None:\n        guide_img = preprocess(guide_img.copy(), model)[np.newaxis]\n        assert (guide_img.shape == img.shape)\n    batch, height, width, *ch = img.shape\n\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess, g.device(device):\n        tf.import_graph_def(net[\'graph_def\'], name=\'net\')\n        names = [op.name for op in g.get_operations()]\n        input_name = names[0] + \':0\'\n        x = g.get_tensor_by_name(input_name)\n\n        features = [names[layer_i] + \':0\' for layer_i in layers]\n        feature_loss = tf.Variable(0.0)\n        for feature_i in features:\n            layer = g.get_tensor_by_name(feature_i)\n            if guide_img is None:\n                feature_loss += tf.reduce_mean(layer)\n            else:\n                # Reshape it to 2D vector\n                layer = tf.reshape(layer, [-1, 1])\n                # Do the same for our guide image\n                guide_layer = sess.run(layer, feed_dict={x: guide_img})\n                guide_layer = guide_layer.reshape(-1, 1)\n                # Now calculate their dot product\n                correlation = tf.matmul(guide_layer.T, layer)\n                feature_loss += feature_loss_weight * tf.reduce_mean(\n                    correlation)\n        softmax_loss = tf.Variable(0.0)\n        if label_i is not None:\n            layer = g.get_tensor_by_name(names[layer_i] + \':0\')\n            layer_shape = sess.run(tf.shape(layer), feed_dict={x: img})\n            layer_vec = np.ones(layer_shape) / layer_shape[-1]\n            layer_vec[..., neuron_i] = 1.0 - 1.0 / layer_shape[1]\n            softmax_loss += softmax_loss_weight * tf.reduce_mean(\n                tf.nn.l2_loss(layer - layer_vec))\n\n        dx = tf.square(x[:, :height - 1, :width - 1, :] -\n                       x[:, :height - 1, 1:, :])\n        dy = tf.square(x[:, :height - 1, :width - 1, :] -\n                       x[:, 1:, :width - 1, :])\n        tv_loss = tv_loss_weight * tf.reduce_mean(tf.pow(dx + dy, 1.2))\n        l2_loss = l2_loss_weight * tf.reduce_mean(tf.nn.l2_loss(x))\n\n        ascent = tf.gradients(feature_loss + softmax_loss + tv_loss + l2_loss,\n                              x)[0]\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        sess.run(init_op)\n        imgs = []\n        for it_i in range(n_iterations):\n            this_res, this_feature_loss, this_softmax_loss, this_tv_loss, this_l2_loss = sess.run(\n                [ascent, feature_loss, softmax_loss, tv_loss, l2_loss],\n                feed_dict={x: img})\n            print(\'feature:\', this_feature_loss, \'softmax:\', this_softmax_loss,\n                  \'tv\', this_tv_loss, \'l2\', this_l2_loss)\n\n            _apply(img, -this_res, it_i, **kwargs)\n            imgs.append(deprocess(img[0]))\n\n            if save_images is not None:\n                imsave(\n                    os.path.join(save_images, \'frame{}.png\'.format(it_i)),\n                    imgs[-1])\n\n        if save_gif is not None:\n            gif.build_gif(imgs, saveto=save_gif)\n\n    return imgs\n'"
cadl/dft.py,9,"b'""""""Utils for performing a DFT using numpy.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport numpy as np\nfrom scipy.signal import hann\nfrom math import log, exp\nfrom scipy.fftpack import dct\nfrom scipy.special import cbrt\n\n\ndef ztoc_np(re, im):\n    """"""Convert cartesian real, imaginary to polar mag, phs.\n\n    Parameters\n    ----------\n    re : TYPE\n        Description\n    im : TYPE\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    return np.sqrt(re**2 + im**2), np.angle(re + im * 1j)\n\n\ndef ctoz_np(mag, phs):\n    """"""Convert polar mag, phs to cartesian real, imaginary.\n\n    Parameters\n    ----------\n    mag : TYPE\n        Description\n    phs : TYPE\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    return mag * np.cos(phs), mag * np.sin(phs)\n\n\ndef dft_tf(s):\n    """"""Discrete Fourier Transform.\n\n    Parameters\n    ----------\n    s : TYPE\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    import tensorflow as tf\n    N = s.get_shape().as_list()[-1]\n    k = tf.reshape(tf.linspace(0.0, 2 * np.pi / N * (N // 2), N // 2), [1, N // 2])\n    x = tf.reshape(tf.linspace(0.0, N - 1, N), [N, 1])\n    freqs = tf.matmul(x, k)\n    reals = tf.matmul(s, tf.cos(freqs)) * (2.0 / N)\n    imags = tf.matmul(s, tf.sin(freqs)) * (2.0 / N)\n    return reals, imags\n\n\ndef idft_tf(reals, imags):\n    """"""Inverse Discrete Fourier Transform.\n\n    Parameters\n    ----------\n    reals : TYPE\n        Description\n    imags : TYPE\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    import tensorflow as tf\n    N = reals.get_shape().as_list()[-1] * 2\n    k = tf.reshape(tf.linspace(0.0, 2 * np.pi / N * (N // 2), N // 2), [N // 2, 1])\n    x = tf.reshape(tf.linspace(0.0, N - 1, N), [1, N])\n    freqs = tf.matmul(k, x)\n    return tf.matmul(reals, tf.cos(freqs)) + tf.matmul(imags, tf.sin(freqs))\n\n\ndef stft(signal, hop_size=512, fft_size=2048):\n    """"""Short Time Discrete Fourier Transform w/ Windowing from signal.\n\n    Parameters\n    ----------\n    signal : TYPE\n        Description\n    hop_size : int, optional\n        Description\n    fft_size : int, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    n_hops = len(signal) // hop_size\n    s = []\n    hann_win = hann(fft_size)\n    for hop_i in range(n_hops):\n        frame = signal[(hop_i * hop_size):(hop_i * hop_size + fft_size)]\n        frame = np.pad(frame, (0, fft_size - len(frame)), \'constant\')\n        frame *= hann_win\n        s.append(frame)\n    s = np.array(s)\n    N = s.shape[-1]\n    k = np.reshape(np.linspace(0.0, 2 * np.pi / N * (N // 2), N // 2), [1, N // 2])\n    x = np.reshape(np.linspace(0.0, N - 1, N), [N, 1])\n    freqs = np.dot(x, k)\n    reals = np.dot(s, np.cos(freqs)) * (2.0 / N)\n    imags = np.dot(s, np.sin(freqs)) * (2.0 / N)\n    return reals, imags\n\n\ndef istft(re, im, hop_size=512, fft_size=2048):\n    """"""Inverse Short Time Discrete Fourier Transform from real/imag.\n\n    Parameters\n    ----------\n    re : TYPE\n        Description\n    im : TYPE\n        Description\n    hop_size : int, optional\n        Description\n    fft_size : int, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    N = re.shape[1] * 2\n    k = np.reshape(np.linspace(0.0, 2 * np.pi / N * (N // 2), N // 2), [N // 2, 1])\n    x = np.reshape(np.linspace(0.0, N - 1, N), [1, N])\n    freqs = np.dot(k, x)\n    signal = np.zeros((re.shape[0] * hop_size + fft_size,))\n    recon = np.dot(re, np.cos(freqs)) + np.dot(im, np.sin(freqs))\n    for hop_i, frame in enumerate(recon):\n        signal[(hop_i * hop_size): (hop_i * hop_size + fft_size)] += frame\n    return signal\n\n\ndef forward(s, **kwargs):\n    """"""Short Time Discrete Fourier Transform w/ Windowing from signal.\n\n    Parameters\n    ----------\n    s : TYPE\n        Description\n    **kwargs : TYPE\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n\n    Deleted Parameters\n    ------------------\n    file : TYPE\n        Description\n    """"""\n    re, im = stft(s, **kwargs)\n    mag, phs = ztoc_np(re, im)\n    return mag, phs\n\n\ndef inverse(mag, phs, **kwargs):\n    """"""Inverse Short Time Discrete Fourier Transform from mag/phs.\n\n    Parameters\n    ----------\n    mag : TYPE\n        Description\n    phs : TYPE\n        Description\n    **kwargs : TYPE\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n\n    Deleted Parameters\n    ------------------\n    file : TYPE\n        Description\n    """"""\n    re, im = ctoz_np(mag, phs)\n    s = istft(re, im, **kwargs)\n    return s\n\n\ndef hz2bark(frq):\n    """"""Convert hz to bark scale.\n\n    Parameters\n    ----------\n    frq : np.ndarray, float\n        frequency\n\n    Returns\n    -------\n    bark : np.ndarray, float\n        bark value\n    """"""\n    return (26.81 * frq) / (1960 + frq) - 0.51\n\n\ndef bark2hz(bark):\n    """"""Convert bark scale to hz.\n\n    Parameters\n    ----------\n    bark : np.ndarray, float\n        bark value\n\n    Returns\n    -------\n    frq : np.ndarray, float\n        frequency\n    """"""\n    return (-19600 * bark - 9996) / (10 * bark - 263)\n\n\ndef hz2pitch(frq, beta=12, ref_frq=440.0):\n    """"""Convert frequency to pitch.\n\n    Parameters\n    ----------\n    frq : np.ndarray, float\n        frequency\n    beta : int\n        equal divisions of the octave\n    ref_frq : float, optional\n        Description\n\n    Returns\n    -------\n    p : np.ndarray, float\n        pitch, MIDI value if beta is 12\n    """"""\n    return beta * np.log2(frq / ref_frq) + 69\n\n\ndef pitch2hz(p, beta=12, ref_frq=440.0):\n    """"""Convert pitch to frequency.\n\n    Parameters\n    ----------\n    p : np.ndarray, float\n        midi pitch\n    beta : int\n        equal divisions of the octave\n    ref_frq : float, optional\n        Description\n\n    Returns\n    -------\n    frq : np.ndarray, float\n        frequency\n    """"""\n    return (2**((p - 69) / beta)) * ref_frq\n\n\ndef mel2hz(z, mode=\'htk\'):\n    """"""Convert \'mel scale\' frequencies into hz.\n\n    Parameters\n    ----------\n    z : np.ndarray, float\n        \'mel scale\' frequency\n    mode : string\n        \'htk\' uses the mel axis defined in the htkbook\n        \'slaney\' uses slaney\'s formula\n\n    Returns\n    -------\n    f : np.ndarray, float\n        frequency\n\n    Raises\n    ------\n    ValueError\n        Description\n    """"""\n    if mode == \'htk\':\n        f = 700 * (10**(z / 2595) - 1)\n\n    elif mode == \'slaney\':\n        f_0 = 0\n        f_sp = 200 / 3\n        brkfrq = 1000\n\n        # starting mel value for log region\n        brkpt = (brkfrq - f_0) / f_sp\n\n        # the magic 1.0711703 which is the ratio\n        # needed to get from 1000 hz to 6400 hz in 27 steps, and is\n        # *almost* the ratio between 1000 hz and the preceding linear\n        # filter center at 933.33333 hz (actually 1000/933.33333 =\n        # 1.07142857142857 and  exp(log(6.4)/27) = 1.07117028749447)\n        logstep = exp(log(6.4) / 27)\n\n        linpts = (z < brkpt)\n\n        if not np.isscalar(z):\n            f = 0 * z\n            # fill in parts separately\n            f[linpts] = f_0 + f_sp * z[linpts]\n            f[~linpts] = brkfrq * np.exp(log(logstep) * (z[~linpts] - brkpt))\n\n        elif linpts:\n            f = f_0 + f_sp * z\n        else:\n            f = brkfrq * exp(log(logstep) * (z - brkpt))\n\n    else:\n        raise ValueError(""{0} is not a valid mode"".format(mode))\n\n    return f\n\n\ndef hz2mel(f, mode=\'htk\'):\n    """"""Convert hz to \'mel scale\' frequencies.\n\n    Parameters\n    ----------\n    f : np.ndarray, float\n        frequency\n    mode : string\n        \'htk\' uses the mel axis defined in the htkbook\n        \'slaney\' uses slaney\'s formula\n\n    Returns\n    -------\n    z : np.ndarray, float\n        \'mel scale\' frequency\n\n    Raises\n    ------\n    ValueError\n        Description\n    """"""\n    if mode == \'htk\':\n        z = 2595 * np.log10(1 + f / 700)\n        if np.isscalar(z) and not isinstance(f, np.generic):\n            z = float(z)\n\n    elif mode == \'slaney\':\n        # 133.33333\n        f_0 = 0\n        # 66.66667\n        f_sp = 200 / 3\n        brkfrq = 1000\n\n        # starting mel value for log region\n        brkpt = (brkfrq - f_0) / f_sp\n\n        # the magic 1.0711703 which is the ratio\n        # needed to get from 1000 hz to 6400 hz in 27 steps, and is\n        # *almost* the ratio between 1000 hz and the preceding linear\n        # filter center at 933.33333 hz (actually 1000/933.33333 =\n        # 1.07142857142857 and  exp(log(6.4)/27) = 1.07117028749447)\n        logstep = exp(log(6.4) / 27)\n\n        linpts = (f < brkfrq)\n\n        if not np.isscalar(f):\n            z = 0 * f\n            # fill in parts separately\n            z[linpts] = (f[linpts] - f_0) / f_sp\n            z[~linpts] = brkpt + np.log(f[~linpts] / brkfrq) / log(logstep)\n\n        elif linpts:\n            z = (f - f_0) / f_sp\n        else:\n            z = brkpt + log(f / brkfrq) / log(logstep)\n\n    else:\n        raise ValueError(""{0} is not a valid mode"".format(mode))\n\n    return z\n\n\ndef weights(binfrqs, N, fs, width=1.0, constamp=False):\n    """"""Calculate a transformation array given the center frequencies.\n\n    Parameters\n    ----------\n    binfrqs : np.ndarray\n        center bin frequencies\n    N : int\n        fft-size\n    fs : int\n        sampling rate\n    width : float\n        width of the overlapping windows, default 1.0\n    constamp : bool\n        makes integration windows sum to 1.0, not peak at 1.0, default to False\n\n    Returns\n    -------\n    wts : np.ndarray\n        transformation matrix\n    """"""\n    # center freqs of each FFT bin\n    fftfrqs = np.arange(N // 2 + 1) / N * fs\n\n    # number of filters\n    nfilts = len(binfrqs) - 2\n\n    # weights for transformation\n    wts = np.zeros((nfilts, N))\n\n    for i in range(nfilts):\n        frqs = binfrqs[i:i + 3]\n\n        # scale by width\n        frqs = frqs[1] + width * (frqs - frqs[1])\n\n        # lower and upper slopes for all bins\n        loslope = (fftfrqs - frqs[0]) / (frqs[1] - frqs[0])\n        hislope = (frqs[2] - fftfrqs) / (frqs[2] - frqs[1])\n\n        # intersect them with each other and zero\n        wts[i, :N // 2 + 1] = np.maximum(0, np.minimum(loslope, hislope))\n\n    # constant amplitude\n    if constamp:\n        wts = np.dot(np.diag(2 / (binfrqs[2:2 + nfilts] - binfrqs[:nfilts])), wts)\n\n    return wts\n\n\ndef constantQ(N, fs, width=1.0, minpitch=45, maxpitch=117,\n              octave=12, beta=None, ref_frq=440.0, constamp=False):\n    """"""Create a constantQ weight matrix.\n\n    Parameters\n    ----------\n    N : int\n        fft-size\n    fs : int\n        rate of sampling\n    width : float, optional\n        width of triangular overlapping windows, default 1.0\n    minpitch : int, optional\n        minimum pitch of constantQ, default 45\n    maxpitch : int, optional\n        maximum pitch of constantQ, default 117\n    octave : int, optional\n        number of pitches per octave,\n        also can be considered the EDO or TET, default 12\n    beta : int, optional\n        number of divisions per octave, default octave * 3\n    ref_frq : float, optional\n        Description\n    constamp : boolean, optional\n        constant amplitude across bins, default False\n\n    Returns\n    -------\n    wts : np.ndarray\n        weight matrix\n\n    Deleted Parameters\n    ------------------\n    ref_freq : float, optional\n        frequency of A4, default 440.0\n\n    Raises\n    ------\n    ValueError\n        Description\n    """"""\n    if beta is None:\n        beta = octave * 3\n\n    if beta % octave != 0:\n        raise ValueError(""Wrap must be an equal division of beta."")\n\n    # center freqs of each FFT bin\n    fftfrqs = np.arange(N // 2 + 1) / N * fs\n    nfilts = int(maxpitch - minpitch) * int(beta / octave)\n\n    # calculates pitch values, adjusts for\n    low = hz2pitch(pitch2hz(minpitch - octave / beta), beta, ref_frq)\n    high = hz2pitch(pitch2hz(maxpitch - octave / beta), beta, ref_frq)\n\n    binfrqs = pitch2hz(np.linspace(low, high, nfilts + 2),\n                       beta=beta, ref_frq=ref_frq)\n\n    # weights for transformation\n    wts = np.zeros((nfilts, N))\n\n    for i in range(nfilts):\n        frqs = binfrqs[i:i + 3]\n\n        # scale by width\n        frqs = frqs[1] + width * (frqs - frqs[1])\n\n        # lower and upper slopes for all bins\n        loslope = (fftfrqs - frqs[0]) / (frqs[1] - frqs[0])\n        hislope = (frqs[2] - fftfrqs) / (frqs[2] - frqs[1])\n\n        # intersect them with each other and zero\n        wts[i, :N // 2 + 1] = np.maximum(0, np.minimum(loslope, hislope))\n\n    # constant amplitude\n    if constamp:\n        wts = np.dot(np.diag(2 / (binfrqs[2:2 + nfilts] - binfrqs[:nfilts])), wts)\n\n    return wts, binfrqs\n\n\ndef mel(N, fs, nfilts=24, width=1.0, minfrq=0, maxfrq=None,\n        constamp=False, mode=\'htk\'):\n    """"""Calculate a mel-band transformation matrix.\n\n    Parameters\n    ----------\n    N : int\n        fft-size\n    fs : int\n        sampling rate\n    nfilts : int\n        number of mel bands\n    width : float\n        constant width of each mel band\n    minfrq : int\n        frequency of the lowest band edge\n    maxfrq : int\n        frequency of the highest band edge, default at nyquist\n    constamp : bool\n        makes integration windows sum to 1.0, not peak at 1.0, default to False\n    mode : string\n        \'htk\' uses the mel axis defined in the HTKBook\n        \'slaney\' uses Slaney\'s formula\n        if using \'slaney\', recommended set constamp to True\n\n    Returns\n    -------\n    wts : np.ndarray\n        transformation matrix\n\n    References\n    ----------\n    .. [1] Taken from the MATLAB implementation of Dan Ellis\' automatic gain control\n        See: `documentation <http://labrosa.ee.columbia.edu/matlab/tf_agc/>`_ and\n        original MATLAB code. Pythonized 2014-05-23 by Chad Wagner chad@kadenze.com\n    """"""\n    # if maxfrq not provided, calculates based on nyquist\n    if maxfrq is None:\n        maxfrq = fs // 2\n\n    # finds min and max in mel\n    minmel = hz2mel(minfrq, mode)\n    maxmel = hz2mel(maxfrq, mode)\n\n    # \'center freqs\' of mel bands, uniformly spaced between limits\n    binfrqs = mel2hz(minmel + np.arange(nfilts + 2) /\n                     (nfilts + 1) * (maxmel - minmel), mode)\n\n    # transformation matrix\n    wts = weights(binfrqs, N, fs, width, constamp).T\n\n    return wts\n\n\ndef mfcc(X, low=2, high=13, A=1.0, C=1.0, mode=""log""):\n    """"""Returns the mel frequency cepstrum coefficients from a mel matrix\n\n    Parameters\n    ----------\n    X : np.ndarray\n        mel matrix of an stft\n    low : int\n        lowest band to return, default 2\n    high : int\n        highest band to return, default 13\n    A : float\n        addition to mel matrix before log transform\n    C : float\n        multiplication to mel matrix before log transform\n\n    Returns\n    -------\n    mfcc : np.ndarray\n        mel frequency cepstrum coefficients\n    """"""\n\n    if mode == ""log"":\n        # logs of the powers of the mel frequencies\n        X = np.log(X * C + A)\n\n    elif mode == ""cbrt"":\n        # cube root the powers of the mel frequencies\n        X = cbrt(X)\n\n    else:\n        raise ValueError(""{0} is not a valid mode"".format(mode))\n\n    # discrete cosine transform of each column\n\n    mfcc = dct(X.T, type=2, axis=0, norm=\'ortho\')[max(low - 1, 0):high, :].T\n\n    return mfcc\n'"
cadl/draw.py,110,"b'""""""Deep Recurrent Attentive Writer.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom cadl.datasets import MNIST, CIFAR10\nfrom cadl.dataset_utils import create_input_pipeline\nfrom cadl import utils, gif\nimport numpy as np\n\n\ndef linear(x, n_output):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    n_output : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    w = tf.get_variable(\n        ""w"", [x.get_shape()[1], n_output],\n        initializer=tf.contrib.layers.xavier_initializer())\n    b = tf.get_variable(\n        ""b"", [n_output], initializer=tf.constant_initializer(0.0))\n    return tf.add(tf.matmul(x, w), b)\n\n\ndef encoder(x, rnn, batch_size, state=None, n_enc=64, reuse=None):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    rnn : TYPE\n        Description\n    batch_size : TYPE\n        Description\n    state : None, optional\n        Description\n    n_enc : int, optional\n        Description\n    reuse : None, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    with tf.variable_scope(\'encoder\', reuse=reuse):\n        if state is None:\n            h_enc, state = rnn(x, rnn.zero_state(batch_size, tf.float32))\n        else:\n            h_enc, state = rnn(x, state)\n    return h_enc, state\n\n\ndef variational_layer(h_enc, noise, n_z=2, reuse=None):\n    """"""Summary\n\n    Parameters\n    ----------\n    h_enc : TYPE\n        Description\n    noise : TYPE\n        Description\n    n_z : int, optional\n        Description\n    reuse : None, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    with tf.variable_scope(\'variational\', reuse=reuse):\n        # Equation 1: use the encoder to parameterize the mean of the approximate\n        # posterior distribution Q\n        with tf.variable_scope(\'mu\', reuse=reuse):\n            h_z_mu = linear(h_enc, n_z)\n\n        # Equation 2: Similarly for the standard deviation\n        with tf.variable_scope(\'log_sigma\', reuse=reuse):\n            h_z_log_sigma = linear(h_enc, n_z)\n\n        # sample z_t ~ q(Z_t | h_enc_t)\n        z_t = h_z_mu + tf.multiply(tf.exp(h_z_log_sigma), noise)\n\n    # return the sampled value from the latent distribution and its parameters\n    return z_t, h_z_mu, h_z_log_sigma\n\n\ndef decoder(z, rnn, batch_size, state=None, n_dec=64, reuse=None):\n    """"""Summary\n\n    Parameters\n    ----------\n    z : TYPE\n        Description\n    rnn : TYPE\n        Description\n    batch_size : TYPE\n        Description\n    state : None, optional\n        Description\n    n_dec : int, optional\n        Description\n    reuse : None, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    with tf.variable_scope(\'decoder\', reuse=reuse):\n        if state is None:\n            h_dec, state = rnn(z, rnn.zero_state(batch_size, tf.float32))\n        else:\n            h_dec, state = rnn(z, state)\n    return h_dec, state\n\n\ndef create_attention_map(h_dec, reuse=None):\n    """"""Summary\n\n    Parameters\n    ----------\n    h_dec : TYPE\n        Description\n    reuse : None, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    with tf.variable_scope(""attention"", reuse=reuse):\n        p = linear(h_dec, 5)\n        g_tilde_x, g_tilde_y, log_sigma, log_delta_tilde, log_gamma = \\\n            tf.split(p, 5, axis=1)\n\n    return g_tilde_x, g_tilde_y, log_sigma, log_delta_tilde, log_gamma\n\n\ndef create_filterbank(g_x, g_y, log_sigma_sq, log_delta, A=28, B=28, C=1, N=12):\n    """"""summary\n\n    Parameters\n    ----------\n    g_x : TYPE\n        Description\n    g_y : TYPE\n        Description\n    log_sigma_sq : TYPE\n        Description\n    log_delta : TYPE\n        Description\n    A : int, optional\n        Description\n    B : int, optional\n        Description\n    C : int, optional\n        Description\n    N : int, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n\n    Deleted Parameters\n    ------------------\n    log_sigma : type\n        description\n    """"""\n    with tf.name_scope(""filterbank""):\n        # Equation 22 and 23\n        g_x = (A + 1) / 2 * (g_x + 1)\n        g_y = (B + 1) / 2 * (g_y + 1)\n\n        # The authors suggest to use a real-valued center and stride, meaning\n        # the center of this grid is not necessarily located directly on a\n        # pixel, but can be between pixels. To compute the stride, we use\n        # equation 24:\n\n        # Equation 24 delta = (max(A, B) - 1) / (N - 1) * tf.exp(log_delta)\n        delta = (max(A, B) - 1) / (N - 1) * tf.exp(log_delta)\n        # Note that we\'ve converted our `log_delta` to `delta` by taking the\n        # exponential.\n\n        # To determine the mean location of the ith and jth filter of the N x N\n        # grid of filters, we can use the formulas from the paper, equations 19\n        # and 20.  We\'ll create grid positions for the x and y positions\n        # independently.  So for each observation in our mini batch, we\'ll have\n        # N number of positions for our x and our y grid positions, or 12 x 12\n        # = 144 grid positions in total for each observation in our mini batch.\n\n        # Equations 19 and 20\n        ns = tf.expand_dims(tf.cast(tf.range(N), tf.float32), 0)\n        mu_x = tf.reshape(g_x + (ns - N / 2 - 0.5) * delta, [-1, N, 1])\n        mu_y = tf.reshape(g_y + (ns - N / 2 - 0.5) * delta, [-1, N, 1])\n\n        # Finally we\'re ready to define the filterbank matrices `F_x` and `F_y`\n        # from equations 25 and 26.  `F_x` and `F_x` require us to use $2 *\n        # \\sigma^2$.  So we\'ll calculate that first for each of our sigmas, one\n        # per observation in our mini batch.  We take exponential of\n        # `log_sigma` to get $\\sigma^2$ and then multiply by 2.  we\'ll also\n        # reshape it to the number of observations we have in the first\n        # dimension, and create singleton dimensions for broadcasting them\n        # across our filterbanks.\n        sigma_sq = tf.reshape(tf.exp(log_sigma_sq), [-1, 1, 1])\n\n        # Now we\'ll create a range for our entire image:\n        xs = tf.reshape(tf.cast(tf.range(B), tf.float32), [1, 1, -1])\n        ys = tf.reshape(tf.cast(tf.range(A), tf.float32), [1, 1, -1])\n\n        # And then using broadcasting, we can calculate the Gaussian defining\n        # the filterbank:\n        F_x = tf.exp(-tf.square(xs - mu_x) / (2 * sigma_sq))\n        F_y = tf.exp(-tf.square(ys - mu_y) / (2 * sigma_sq))\n\n        # Finally we\'ll normalize the filterbank across each location so that\n        # the sum of the energy across the x and y locations sum to 1.  We\'ll\n        # also ensure that we do not divide by zero by making sure the maximum\n        # value is at least epsilon.  There will be one filterbank defining the\n        # horizontal filters, and another for the vertical filters.  The\n        # horizontal filterbanks, `F_x[i, a]` will be N x B, so N filters\n        # across the B number of pixels.  Same for the vertical ones,\n        # `F_y[j, b]`, there will be N filters across the A number of pixels.\n\n        # Normalize\n        epsilon = 1e-10\n        F_x = F_x / tf.maximum(tf.reduce_sum(F_x, 2, keep_dims=True), epsilon)\n        F_y = F_y / tf.maximum(tf.reduce_sum(F_y, 2, keep_dims=True), epsilon)\n\n    # return the filterbanks\n    return F_x, F_y\n\n\ndef filter_image(x, F_x, F_y, log_gamma, A, B, C, N, inverted=False):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    F_x : TYPE\n        Description\n    F_y : TYPE\n        Description\n    log_gamma : TYPE\n        Description\n    A : TYPE\n        Description\n    B : TYPE\n        Description\n    C : TYPE\n        Description\n    N : TYPE\n        Description\n    inverted : bool, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n\n    Deleted Parameters\n    ------------------\n    gamma : TYPE\n        Description\n    """"""\n    with tf.name_scope(""filter""):\n        # To filter the image, we\'ll want to transpose our filterbanks\n        # dimensions allowing to to multiply the image in the next step\n        # For the read operation, we transpose X (equation 27)\n        # For write, we transpose Y, and use inverse gamma (equation 29)\n        gamma = tf.exp(log_gamma)\n        if inverted:\n            F_y = tf.transpose(F_y, perm=[0, 2, 1])\n            gamma = 1.0 / gamma\n            # Now we left and right multiply the image in `x` by each filter\n            if C == 1:\n                glimpse = tf.matmul(F_y,\n                                    tf.matmul(tf.reshape(x, [-1, N, N]), F_x))\n            else:\n                x = tf.reshape(x, [-1, N, N, C])\n                xs = tf.split(x, C, axis=3)\n                glimpses = []\n                for x_i in xs:\n                    glimpses.append(\n                        tf.matmul(F_y, tf.matmul(tf.squeeze(x_i, axis=[3]), F_x)))\n                glimpse = tf.concat(\n                    [tf.expand_dims(x_i, -1) for x_i in glimpses], axis=3)\n        else:\n            F_x = tf.transpose(F_x, perm=[0, 2, 1])\n            # Now we left and right multiply the image in `x` by each filter\n            if C == 1:\n                glimpse = tf.matmul(F_y,\n                                    tf.matmul(tf.reshape(x, [-1, A, B]), F_x))\n            else:\n                x = tf.reshape(x, [-1, A, B, C])\n                xs = tf.split(x, C, axis=3)\n                glimpses = []\n                for x_i in xs:\n                    glimpses.append(\n                        tf.matmul(F_y, tf.matmul(tf.squeeze(x_i, axis=[3]), F_x)))\n                glimpse = tf.concat(\n                    [tf.expand_dims(x_i, -1) for x_i in glimpses], axis=3)\n        # Finally, we\'ll flatten the filtered image to a vector\n        glimpse = tf.reshape(glimpse,\n                             [-1, np.prod(glimpse.get_shape().as_list()[1:])])\n\n        # And weight the filtered glimpses by gamma\n        return glimpse * tf.reshape(gamma, [-1, 1])\n\n\ndef read(x_t,\n         x_hat_t,\n         h_dec_t,\n         read_n=5,\n         A=28,\n         B=28,\n         C=1,\n         use_attention=True,\n         reuse=None):\n    """"""Read from the input image, `x`, and reconstruction error image `x_hat`.\n\n    Optionally apply a filterbank w/ `use_attention`.\n\n    Parameters\n    ----------\n    x_t : tf.Tensor\n        Input image to optionally filter\n    x_hat_t : tf.Tensor\n        Reconstruction error to optionally filter\n    h_dec_t : tf.Tensor\n        Output of the decoder of the network (could also be the encoder but the\n        authors suggest to use the decoder instead, see end of section 2.1)\n    read_n : int, optional\n        Description\n    A : int, optional\n        Description\n    B : int, optional\n        Description\n    C : int, optional\n        Description\n    use_attention : bool, optional\n        Description\n    reuse : None, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    with tf.variable_scope(\'read\', reuse=reuse):\n        if use_attention:\n            # Use the decoder\'s output to create 5 measures to define the\n            # placement and characteristics of the filterbank\n            g_x_tilde, g_y_tilde, \\\n                log_sigma_sq_tilde, log_delta_tilde, log_gamma_tilde = \\\n                create_attention_map(h_dec_t, reuse=reuse)\n\n            # Now create the filterbank\n            F_x_tilde, F_y_tilde = create_filterbank(\n                g_x_tilde,\n                g_y_tilde,\n                log_sigma_sq_tilde,\n                log_delta_tilde,\n                N=read_n,\n                A=A,\n                B=B,\n                C=C)\n\n            # And apply the filterbanks to the input image\n            x_t = filter_image(x_t, F_x_tilde, F_y_tilde, log_gamma_tilde, A, B,\n                               C, read_n)\n\n            # And similarly, apply the filterbanks to the error image\n            x_hat_t = filter_image(x_hat_t, F_x_tilde, F_y_tilde,\n                                   log_gamma_tilde, A, B, C, read_n)\n\n    # Equation 27, concat the two N x N patches from the image and the error\n    # image.  If we aren\'t using attention, these are just the unfiltered\n    # images.\n    return tf.concat([x_t, x_hat_t], axis=1)\n\n\ndef write(h_dec_t, write_n=5, A=28, B=28, C=1, use_attention=True, reuse=None):\n    """"""Summary\n\n    Parameters\n    ----------\n    h_dec_t : TYPE\n        Description\n    write_n : int, optional\n        Description\n    A : int, optional\n        Description\n    B : int, optional\n        Description\n    C : int, optional\n        Description\n    use_attention : bool, optional\n        Description\n    reuse : None, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    # Equation 28: again, like in the read layer, we can add an additional\n    # nonlinearity here to enforce the characteristics of the final activation\n    # we expect to see.  For instance, if our images are normalized 0 to 1,\n    # then we can use a sigmoid activation.\n    with tf.variable_scope(""write"", reuse=reuse):\n        # Next, we\'ll want to apply a few more additional operations if we\'re\n        # using attention\n        if use_attention:\n            w_t = linear(h_dec_t, write_n * write_n * C)\n            if C == 1:\n                w_t = tf.reshape(w_t, [-1, write_n, write_n])\n            else:\n                w_t = tf.reshape(w_t, [-1, write_n, write_n, C])\n\n            # Use the decoder\'s output to create 5 measures to define the\n            # placement and characteristics of the filterbank\n            g_x_hat, g_y_hat, log_sigma_sq_hat, log_delta_hat, log_gamma_hat = \\\n                create_attention_map(h_dec_t, reuse=reuse)\n\n            # Now create the filterbank\n            F_x_hat, F_y_hat = create_filterbank(\n                g_x_hat,\n                g_y_hat,\n                log_sigma_sq_hat,\n                log_delta_hat,\n                N=write_n,\n                A=A,\n                B=B,\n                C=C)\n\n            # And apply the filterbanks to the input image, Equation 29\n            w_t = filter_image(\n                w_t,\n                F_x_hat,\n                F_y_hat,\n                log_gamma_hat,\n                A,\n                B,\n                C,\n                write_n,\n                inverted=True)\n\n            return w_t\n        else:\n            return linear(h_dec_t, A * B * C)\n\n\ndef binary_cross_entropy(t, o, eps=1e-10):\n    """"""Summary\n\n    Parameters\n    ----------\n    t : TYPE\n        Description\n    o : TYPE\n        Description\n    eps : float, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    return -(t * tf.log(o + eps) + (1.0 - t) * tf.log(1.0 - o + eps))\n\n\ndef create_model(\n        A=28,  # img_h\n        B=28,  # img_w\n        C=1,  # img_c\n        T=16,\n        batch_size=100,\n        n_enc=128,\n        n_z=32,\n        n_dec=128,\n        read_n=12,\n        write_n=12):\n    """"""<FRESHLY_INSERTED>""""""\n    x = tf.placeholder(tf.float32, shape=[None, A * B * C], name=\'x\')\n    noise = tf.placeholder(tf.float32, shape=[None, n_z], name=\'noise\')\n    rnn_enc = tf.contrib.rnn.GRUCell(n_enc)\n    rnn_dec = tf.contrib.rnn.GRUCell(n_dec)\n    enc_state, dec_state = None, None\n\n    canvas = [tf.zeros([batch_size, A * B * C], name=\'c_0\')]\n    h_enc_t = tf.zeros([batch_size, n_dec])\n    h_dec_t = tf.zeros([batch_size, n_dec])\n\n    reuse = False\n    z_mus, z_log_sigmas = [], []\n    for t_i in range(1, T):\n        # This assumes the input image is normalized between 0 - 1\n        x_hat_t = x - tf.nn.sigmoid(canvas[t_i - 1])\n        r_t = read(\n            x_t=x,\n            x_hat_t=x_hat_t,\n            h_dec_t=h_dec_t,\n            read_n=read_n,\n            A=A,\n            B=B,\n            C=C,\n            use_attention=True,\n            reuse=reuse)\n        h_enc_t, enc_state = encoder(\n            x=tf.concat([r_t, h_dec_t], axis=1),\n            rnn=rnn_enc,\n            batch_size=batch_size,\n            state=enc_state,\n            n_enc=n_enc,\n            reuse=reuse)\n        z_t, z_mu, z_log_sigma = variational_layer(\n            h_enc=h_enc_t, noise=noise, n_z=n_z, reuse=reuse)\n\n        z_mus.append(z_mu)\n        z_log_sigmas.append(z_log_sigma)\n        h_dec_t, dec_state = decoder(\n            z=z_t,\n            rnn=rnn_dec,\n            batch_size=batch_size,\n            state=dec_state,\n            n_dec=n_dec,\n            reuse=reuse)\n        w_t = write(\n            h_dec_t=h_dec_t,\n            write_n=write_n,\n            A=A,\n            B=B,\n            C=C,\n            use_attention=True,\n            reuse=reuse)\n        c_t = canvas[-1] + w_t\n        canvas.append(c_t)\n        reuse = True\n\n    x_recon = tf.nn.sigmoid(canvas[-1])\n    with tf.variable_scope(\'loss\'):\n        loss_x = tf.reduce_mean(\n            tf.reduce_sum(binary_cross_entropy(x, x_recon), 1))\n        loss_zs = []\n        for z_mu, z_log_sigma in zip(z_mus, z_log_sigmas):\n            loss_zs.append(\n                tf.reduce_sum(\n                    tf.square(z_mu) + tf.square(tf.exp(z_log_sigma)) -\n                    2 * z_log_sigma, 1))\n        loss_z = tf.reduce_mean(0.5 * tf.reduce_sum(loss_zs, 0) - T * 0.5)\n        cost = loss_x + loss_z\n\n    return {\n        \'x\': x,\n        \'loss_x\': loss_x,\n        \'loss_z\': loss_z,\n        \'canvas\': [tf.nn.sigmoid(c_i) for c_i in canvas],\n        \'cost\': cost,\n        \'recon\': x_recon,\n        \'noise\': noise\n    }\n\n\ndef test_mnist():\n    A = 28  # img_h\n    B = 28  # img_w\n    C = 1\n    T = 10\n    n_enc = 256\n    n_z = 100\n    n_dec = 256\n    read_n = 5\n    write_n = 5\n    batch_size = 64\n    mnist = MNIST(split=[0.8, 0.1, 0.1])\n\n    n_examples = batch_size\n    zs = np.random.uniform(-1.0, 1.0, [4, n_z]).astype(np.float32)\n    zs = utils.make_latent_manifold(zs, n_examples)\n\n    # We create a session to use the graph\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        draw = create_model(\n            A=A,\n            B=B,\n            C=C,\n            T=T,\n            batch_size=batch_size,\n            n_enc=n_enc,\n            n_z=n_z,\n            n_dec=n_dec,\n            read_n=read_n,\n            write_n=write_n)\n        opt = tf.train.AdamOptimizer(learning_rate=0.0001)\n        grads = opt.compute_gradients(draw[\'cost\'])\n        for i, (g, v) in enumerate(grads):\n            if g is not None:\n                grads[i] = (tf.clip_by_norm(g, 5), v)\n        train_op = opt.apply_gradients(grads)\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        sess.run(init_op)\n        saver = tf.train.Saver()\n\n        # Fit all training data\n        batch_i = 0\n        n_epochs = 100\n        test_xs = mnist.test.images[:n_examples]\n        utils.montage(test_xs.reshape((-1, A, B)), \'test_xs.png\')\n        for epoch_i in range(n_epochs):\n            for batch_xs, _ in mnist.train.next_batch(batch_size):\n                noise = np.random.randn(batch_size, n_z)\n                lx, lz = sess.run(\n                    [draw[\'loss_x\'], draw[\'loss_z\'], train_op],\n                    feed_dict={draw[\'x\']: batch_xs,\n                               draw[\'noise\']: noise})[0:2]\n                print(\'x:\', lx, \'z:\', lz)\n                if batch_i % 1000 == 0:\n                    # Plot example reconstructions\n                    recon = sess.run(\n                        draw[\'canvas\'],\n                        feed_dict={draw[\'x\']: test_xs,\n                                   draw[\'noise\']: noise})\n                    recon = [utils.montage(r.reshape(-1, A, B)) for r in recon]\n                    gif.build_gif(\n                        recon,\n                        cmap=\'gray\',\n                        saveto=\'manifold_%08d.gif\' % batch_i)\n\n                    saver.save(sess, \'./draw.ckpt\', global_step=batch_i)\n\n                batch_i += 1\n\n\ndef train_dataset(ds,\n                  A,\n                  B,\n                  C,\n                  T=20,\n                  n_enc=512,\n                  n_z=200,\n                  n_dec=512,\n                  read_n=12,\n                  write_n=12,\n                  batch_size=100,\n                  n_epochs=100):\n\n    if ds is None:\n        ds = CIFAR10(split=[0.8, 0.1, 0.1])\n        A, B, C = (32, 32, 3)\n\n    n_examples = batch_size\n    zs = np.random.uniform(-1.0, 1.0, [4, n_z]).astype(np.float32)\n    zs = utils.make_latent_manifold(zs, n_examples)\n\n    # We create a session to use the graph\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        draw = create_model(\n            A=A,\n            B=B,\n            C=C,\n            T=T,\n            batch_size=batch_size,\n            n_enc=n_enc,\n            n_z=n_z,\n            n_dec=n_dec,\n            read_n=read_n,\n            write_n=write_n)\n        opt = tf.train.AdamOptimizer(learning_rate=0.0001)\n\n        # Clip gradients\n        grads = opt.compute_gradients(draw[\'cost\'])\n        for i, (g, v) in enumerate(grads):\n            if g is not None:\n                grads[i] = (tf.clip_by_norm(g, 5), v)\n        train_op = opt.apply_gradients(grads)\n\n        # Add summary variables\n        tf.summary.scalar(name=\'cost\', tensor=draw[\'cost\'])\n        tf.summary.scalar(name=\'loss_z\', tensor=draw[\'loss_z\'])\n        tf.summary.scalar(name=\'loss_x\', tensor=draw[\'loss_x\'])\n        tf.summary.histogram(\n            name=\'recon_t0_histogram\', values=draw[\'canvas\'][0])\n        tf.summary.histogram(\n            name=\'recon_t-1_histogram\', values=draw[\'canvas\'][-1])\n        tf.summary.image(\n            name=\'recon_t0_image\',\n            tensor=tf.reshape(draw[\'canvas\'][0], (-1, A, B, C)),\n            max_outputs=2)\n        tf.summary.image(\n            name=\'recon_t-1_image\',\n            tensor=tf.reshape(draw[\'canvas\'][-1], (-1, A, B, C)),\n            max_outputs=2)\n        sums = tf.summary.merge_all()\n        train_writer = tf.summary.FileWriter(logdir=\'draw/train\')\n        valid_writer = tf.summary.FileWriter(logdir=\'draw/valid\')\n\n        # Init\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        sess.run(init_op)\n        saver = tf.train.Saver()\n\n        # Fit all training data\n        batch_i = 0\n        test_xs = ds.test.images[:n_examples] / 255.0\n        utils.montage(test_xs.reshape((-1, A, B, C)), \'draw/test_xs.png\')\n        for epoch_i in range(n_epochs):\n            for batch_xs, _ in ds.train.next_batch(batch_size):\n                noise = np.random.randn(batch_size, n_z)\n                cost, summary = sess.run(\n                    [draw[\'cost\'], sums, train_op],\n                    feed_dict={\n                        draw[\'x\']: batch_xs / 255.0,\n                        draw[\'noise\']: noise\n                    })[0:2]\n                train_writer.add_summary(summary, batch_i)\n                print(\'train cost:\', cost)\n\n                if batch_i % 1000 == 0:\n                    # Plot example reconstructions\n                    recon = sess.run(\n                        draw[\'canvas\'],\n                        feed_dict={draw[\'x\']: test_xs,\n                                   draw[\'noise\']: noise})\n                    recon = [\n                        utils.montage(r.reshape(-1, A, B, C)) for r in recon\n                    ]\n                    gif.build_gif(\n                        recon,\n                        cmap=\'gray\',\n                        saveto=\'draw/manifold_%08d.gif\' % batch_i)\n                    saver.save(sess, \'./draw/draw.ckpt\', global_step=batch_i)\n                batch_i += 1\n\n            # Run validation\n            if batch_i % 1000 == 0:\n                for batch_xs, _ in ds.valid.next_batch(batch_size):\n                    noise = np.random.randn(batch_size, n_z)\n                    cost, summary = sess.run(\n                        [draw[\'cost\'], sums],\n                        feed_dict={\n                            draw[\'x\']: batch_xs / 255.0,\n                            draw[\'noise\']: noise\n                        })[0:2]\n                    valid_writer.add_summary(summary, batch_i)\n                    print(\'valid cost:\', cost)\n                    batch_i += 1\n\n\ndef train_input_pipeline(\n        files,\n        A,  # img_h\n        B,  # img_w\n        C,\n        T=20,\n        n_enc=512,\n        n_z=256,\n        n_dec=512,\n        read_n=15,\n        write_n=15,\n        batch_size=64,\n        n_epochs=1e9,\n        input_shape=(64, 64, 3)):\n\n    # We create a session to use the graph\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        batch = create_input_pipeline(\n            files=files,\n            batch_size=batch_size,\n            n_epochs=n_epochs,\n            crop_shape=(A, B, C),\n            shape=input_shape)\n\n        draw = create_model(\n            A=A,\n            B=B,\n            C=C,\n            T=T,\n            batch_size=batch_size,\n            n_enc=n_enc,\n            n_z=n_z,\n            n_dec=n_dec,\n            read_n=read_n,\n            write_n=write_n)\n        opt = tf.train.AdamOptimizer(learning_rate=0.0001)\n        grads = opt.compute_gradients(draw[\'cost\'])\n        for i, (g, v) in enumerate(grads):\n            if g is not None:\n                grads[i] = (tf.clip_by_norm(g, 5), v)\n        train_op = opt.apply_gradients(grads)\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        sess.run(init_op)\n        coord = tf.train.Coordinator()\n        tf.get_default_graph().finalize()\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n        # Fit all training data\n        batch_i = 0\n        epoch_i = 0\n        n_files = len(files)\n        test_xs = sess.run(batch).reshape((-1, A * B * C)) / 255.0\n        utils.montage(test_xs.reshape((-1, A, B, C)), \'test_xs.png\')\n        try:\n            while not coord.should_stop() and epoch_i < n_epochs:\n                batch_xs = sess.run(batch)\n                noise = np.random.randn(batch_size, n_z)\n                lx, lz = sess.run(\n                    [draw[\'loss_x\'], draw[\'loss_z\'], train_op],\n                    feed_dict={\n                        draw[\'x\']: batch_xs.reshape((-1, A * B * C)) / 255.0,\n                        draw[\'noise\']: noise\n                    })[0:2]\n                print(\'x:\', lx, \'z:\', lz)\n                if batch_i % n_files == 0:\n                    batch_i = 0\n                    epoch_i += 1\n                if batch_i % 1000 == 0:\n                    # Plot example reconstructions\n                    recon = sess.run(\n                        draw[\'canvas\'],\n                        feed_dict={draw[\'x\']: test_xs,\n                                   draw[\'noise\']: noise})\n                    recon = [\n                        utils.montage(r.reshape(-1, A, B, C)) for r in recon\n                    ]\n                    gif.build_gif(recon, saveto=\'manifold_%08d.gif\' % batch_i)\n                    plt.close(\'all\')\n                batch_i += 1\n        except tf.errors.OutOfRangeError:\n            print(\'Done.\')\n        finally:\n            # One of the threads has issued an exception.  So let\'s tell all the\n            # threads to shutdown.\n            coord.request_stop()\n\n        # Wait until all threads have finished.\n        coord.join(threads)\n\n        # Clean up the session.\n        sess.close()\n'"
cadl/fastwavenet.py,23,"b'""""""WaveNet Training and Fast WaveNet Decoding.\n\nFrom the following paper\n------------------------\nRamachandran, P., Le Paine, T., Khorrami, P., Babaeizadeh, M., Chang, S.,\nZhang, Y., \xe2\x80\xa6 Huang, T. (2017). Fast Generation For Convolutional\nAutoregressive Models, 1\xe2\x80\x935.\n""""""\n""""""\nWaveNet Training code and utilities are licensed under APL from the\n\nGoogle Magenta project\n----------------------\nhttps://github.com/tensorflow/magenta/blob/master/magenta/models/nsynth/wavenet\n\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom cadl import librispeech\nfrom cadl import wavenet_utils as wnu\nfrom cadl.utils import sample_categorical\nfrom scipy.io import wavfile\n\n\ndef get_sequence_length(n_stages, n_layers_per_stage):\n    """"""Summary\n\n    Parameters\n    ----------\n    n_stages : TYPE\n        Description\n    n_layers_per_stage : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    sequence_length = 2**n_layers_per_stage * 2 * n_stages\n    return sequence_length\n\n\ndef create_generation_model(n_stages=5, n_layers_per_stage=10,\n                            n_hidden=256, batch_size=1, n_skip=128,\n                            n_quantization=256, filter_length=2,\n                            onehot=False):\n    """"""Summary\n\n    Parameters\n    ----------\n    n_stages : int, optional\n        Description\n    n_layers_per_stage : int, optional\n        Description\n    n_hidden : int, optional\n        Description\n    batch_size : int, optional\n        Description\n    n_skip : int, optional\n        Description\n    n_quantization : int, optional\n        Description\n    filter_length : int, optional\n        Description\n    onehot : bool, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    offset = n_quantization / 2.0\n\n    # Encode the source with 8-bit Mu-Law.\n    X = tf.placeholder(name=\'X\', shape=[None, None], dtype=tf.float32)\n    X_quantized = wnu.mu_law(X, n_quantization)\n    if onehot:\n        X_onehot = tf.one_hot(\n            tf.cast(X_quantized + offset, tf.int32),\n            n_quantization)\n    else:\n        X_onehot = tf.expand_dims(X_quantized, 2)\n\n    push_ops, init_ops = [], []\n    h, init, push = wnu.causal_linear(\n        X=X_onehot,\n        n_inputs=256 if onehot else 1,\n        n_outputs=n_hidden,\n        name=\'startconv\',\n        rate=1,\n        batch_size=batch_size,\n        filter_length=filter_length)\n    init_ops.extend(init)\n    push_ops.extend(push)\n\n    # Set up skip connections.\n    s = wnu.linear(h, n_hidden, n_skip, name=\'skip_start\')\n\n    # Residual blocks with skip connections.\n    for i in range(n_stages * n_layers_per_stage):\n        dilation = 2**(i % n_layers_per_stage)\n\n        # dilated masked cnn\n        d, init, push = wnu.causal_linear(\n            X=h,\n            n_inputs=n_hidden,\n            n_outputs=n_hidden * 2,\n            name=\'dilatedconv_%d\' % (i + 1),\n            rate=dilation,\n            batch_size=batch_size,\n            filter_length=filter_length)\n        init_ops.extend(init)\n        push_ops.extend(push)\n\n        # gated cnn\n        assert d.get_shape().as_list()[2] % 2 == 0\n        m = d.get_shape().as_list()[2] // 2\n        d = tf.sigmoid(d[:, :, :m]) * tf.tanh(d[:, :, m:])\n\n        # residuals\n        h += wnu.linear(d, n_hidden, n_hidden, name=\'res_%d\' % (i + 1))\n\n        # skips\n        s += wnu.linear(d, n_hidden, n_skip, name=\'skip_%d\' % (i + 1))\n\n    s = tf.nn.relu(s)\n    s = wnu.linear(s, n_skip, n_skip, name=\'out1\')\n    s = tf.nn.relu(s)\n    logits = tf.clip_by_value(\n        wnu.linear(s, n_skip, n_quantization, name=\'logits_preclip\') + offset,\n        0.0, n_quantization - 1.0,\n        name=\'logits\')\n    logits = tf.reshape(logits, [-1, n_quantization])\n    probs = tf.nn.softmax(logits, name=\'softmax\')\n    synthesis = tf.reshape(\n        wnu.inv_mu_law(tf.cast(tf.argmax(probs, 1), tf.float32) - offset,\n                       n_quantization),\n        [-1, 1])\n\n    return {\n        \'X\': X,\n        \'init_ops\': init_ops,\n        \'push_ops\': push_ops,\n        \'probs\': probs,\n        \'synthesis\': synthesis\n    }\n\n\ndef train_librispeech():\n    dataset = librispeech.get_dataset(convert_to_wav=False)\n    it_i = 0\n    n_epochs = 10000\n\n    batch_size = 32\n    n_stages = 6\n    n_layers_per_stage = 9\n    n_hidden = 32\n    filter_length = 2\n    n_skip = 256\n\n    sequence_length = get_sequence_length(n_stages, n_layers_per_stage)\n    ckpt_path = \'vctk-wavenet/wavenet_filterlen{}_batchsize{}_sequencelen{}_stages{}_layers{}_hidden{}_skips{}/\'.format(\n        filter_length, batch_size, sequence_length,\n        n_stages, n_layers_per_stage, n_hidden, n_skip)\n    with tf.Graph().as_default() as g, tf.Session(graph=g) as sess:\n        net = create_generation_model(n_stages=n_stages,\n                n_layers_per_stage=n_layers_per_stage,\n                n_hidden=n_hidden,\n                batch_size=batch_size,\n                n_skip=n_skip,\n                filter_length=filter_length)\n        batch = librispeech.batch_generator\n        opt = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(\n                net[\'loss\'])\n        sess.run(tf.global_variables_initializer())\n        saver = tf.train.Saver()\n        writer = tf.summary.FileWriter(ckpt_path)\n        if tf.train.latest_checkpoint(ckpt_path) is not None:\n            saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\n        for epoch_i in range(n_epochs):\n            for batch_xs, batch_hs in batch(dataset, batch_size, sequence_length):\n                loss, _ = sess.run([net[\'loss\'], opt], feed_dict={\n                    net[\'X\']: batch_xs})\n                print(loss)\n                if it_i % 100 == 0:\n                    summary = sess.run(net[\'summaries\'], feed_dict={\n                        net[\'X\']: batch_xs})\n                    writer.add_summary(summary, it_i)\n                    saver.save(sess,\n                            os.path.join(ckpt_path, \'model.ckpt\'),\n                            global_step=it_i)\n                it_i += 1\n\n\ndef test_librispeech():\n    """"""Summary\n    """"""\n    prime_length = 6144\n    total_length = 16000 * 3\n    batch_size = 32\n    n_stages = 6\n    n_layers_per_stage = 9\n    n_hidden = 32\n    filter_length = 2\n    n_skip = 256\n    onehot = False\n\n    sequence_length = get_sequence_length(n_stages, n_layers_per_stage)\n    ckpt_path = \'vctk-wavenet/wavenet_filterlen{}_batchsize{}_sequencelen{}_stages{}_layers{}_hidden{}_skips{}/\'.format(\n        filter_length, batch_size, sequence_length,\n        n_stages, n_layers_per_stage, n_hidden, n_skip)\n\n    dataset = librispeech.get_dataset()\n    batch = next(librispeech.batch_generator(dataset,\n                                             batch_size, prime_length))[0]\n\n    with tf.Graph().as_default(), tf.Session() as sess:\n        net = create_generation_model(batch_size=batch_size,\n                                      filter_length=filter_length,\n                                      n_hidden=n_hidden,\n                                      n_skip=n_skip,\n                                      n_layers_per_stage=n_layers_per_stage,\n                                      n_stages=n_stages,\n                                      onehot=onehot)\n        saver = tf.train.Saver()\n        if tf.train.latest_checkpoint(ckpt_path) is not None:\n            saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\n        else:\n            print(\'Could not find checkpoint\')\n        sess.run(net[\'init_ops\'])\n        synth = np.zeros([batch_size, total_length], dtype=np.float32)\n        synth[:, :prime_length] = batch\n\n        print(\'Synthesize...\')\n        for sample_i in range(total_length - 1):\n            print(\'{}/{}/{}\'.format(sample_i, prime_length, total_length),\n                  end=\'\\r\')\n            probs = sess.run(\n                [net[""probs""], net[""push_ops""]],\n                feed_dict={net[""X""]: synth[:, [sample_i]]})[0]\n            idxs = sample_categorical(probs)\n            audio = wnu.inv_mu_law_numpy(idxs - 128)\n            if sample_i >= prime_length:\n                synth[:, sample_i + 1] = audio\n\n        for i in range(batch_size):\n            wavfile.write(\'synthesis-{}.wav\'.format(i),\n                          16000, synth[i])\n'"
cadl/gan.py,71,"b'""""""Generative Adversarial Network.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom cadl import utils\nfrom cadl.dataset_utils import create_input_pipeline\nfrom cadl.datasets import CELEB\n\n\ndef encoder(x,\n            dimensions=[],\n            filter_sizes=[],\n            convolutional=False,\n            activation=tf.nn.relu,\n            output_activation=tf.nn.sigmoid,\n            reuse=False):\n    """"""Encoder network codes input `x` to layers defined by dimensions.\n\n    Parameters\n    ----------\n    x : tf.Tensor\n        Input to the encoder network, e.g. tf.Placeholder or tf.Variable\n    dimensions : list, optional\n        List of the number of neurons in each layer (convolutional=False) -or-\n        List of the number of filters in each layer (convolutional=True), e.g.\n        [100, 100, 100, 100] for a 4-layer deep network with 100 in each layer.\n    filter_sizes : list, optional\n        List of the size of the kernel in each layer, e.g.:\n        [3, 3, 3, 3] is a 4-layer deep network w/ 3 x 3 kernels in every layer.\n    convolutional : bool, optional\n        Whether or not to use convolutional layers.\n    activation : fn, optional\n        Function for applying an activation, e.g. tf.nn.relu\n    output_activation : fn, optional\n        Function for applying an activation on the last layer, e.g. tf.nn.relu\n    reuse : bool, optional\n        For each layer\'s variable scope, whether to reuse existing variables.\n\n    Returns\n    -------\n    h : tf.Tensor\n        Output tensor of the encoder\n    """"""\n    # %%\n    # ensure 2-d is converted to square tensor.\n    if convolutional:\n        x_tensor = utils.to_tensor(x)\n    else:\n        x_tensor = tf.reshape(tensor=x, shape=[-1, dimensions[0]])\n        dimensions = dimensions[1:]\n    current_input = x_tensor\n\n    for layer_i, n_output in enumerate(dimensions):\n        with tf.variable_scope(str(layer_i), reuse=reuse):\n            if convolutional:\n                h, W = utils.conv2d(\n                    x=current_input,\n                    n_output=n_output,\n                    k_h=filter_sizes[layer_i],\n                    k_w=filter_sizes[layer_i],\n                    padding=\'SAME\',\n                    reuse=reuse)\n            else:\n                h, W = utils.linear(\n                    x=current_input, n_output=n_output, reuse=reuse)\n            output = activation(h)\n\n        current_input = output\n\n    flattened = utils.flatten(current_input, name=\'flatten\', reuse=reuse)\n\n    if output_activation is None:\n        return flattened\n    else:\n        return output_activation(flattened)\n\n\ndef decoder(z,\n            dimensions=[],\n            channels=[],\n            filter_sizes=[],\n            convolutional=False,\n            activation=tf.nn.relu,\n            output_activation=tf.nn.tanh,\n            reuse=None):\n    """"""Decoder network codes input `x` to layers defined by dimensions.\n\n    In contrast with `encoder`, this requires information on the number of\n    output channels in each layer for convolution.  Otherwise, it is mostly\n    the same.\n\n    Parameters\n    ----------\n    z : tf.Tensor\n        Input to the decoder network, e.g. tf.Placeholder or tf.Variable\n    dimensions : list, optional\n        List of the number of neurons in each layer (convolutional=False) -or-\n        List of the number of filters in each layer (convolutional=True), e.g.\n        [100, 100, 100, 100] for a 4-layer deep network with 100 in each layer.\n    channels : list, optional\n        For decoding when convolutional=True, require the number of output\n        channels in each layer.\n    filter_sizes : list, optional\n        List of the size of the kernel in each layer, e.g.:\n        [3, 3, 3, 3] is a 4-layer deep network w/ 3 x 3 kernels in every layer.\n    convolutional : bool, optional\n        Whether or not to use convolutional layers.\n    activation : fn, optional\n        Function for applying an activation, e.g. tf.nn.relu\n    output_activation : fn, optional\n        Function for applying an activation on the last layer, e.g. tf.nn.relu\n    reuse : bool, optional\n        For each layer\'s variable scope, whether to reuse existing variables.\n\n    Returns\n    -------\n    h : tf.Tensor\n        Output tensor of the decoder\n    """"""\n\n    if convolutional:\n        with tf.variable_scope(\'fc\', reuse=reuse):\n            z1, W = utils.linear(\n                x=z,\n                n_output=channels[0] * dimensions[0][0] * dimensions[0][1],\n                reuse=reuse)\n            rsz = tf.reshape(\n                z1, [-1, dimensions[0][0], dimensions[0][1], channels[0]])\n            current_input = activation(rsz)\n\n        dimensions = dimensions[1:]\n        channels = channels[1:]\n        filter_sizes = filter_sizes[1:]\n    else:\n        current_input = z\n\n    for layer_i, n_output in enumerate(dimensions):\n        with tf.variable_scope(str(layer_i), reuse=reuse):\n\n            if convolutional:\n                h, W = utils.deconv2d(\n                    x=current_input,\n                    n_output_h=n_output[0],\n                    n_output_w=n_output[1],\n                    n_output_ch=channels[layer_i],\n                    k_h=filter_sizes[layer_i],\n                    k_w=filter_sizes[layer_i],\n                    padding=\'SAME\',\n                    reuse=reuse)\n            else:\n                h, W = utils.linear(\n                    x=current_input, n_output=n_output, reuse=reuse)\n\n            if layer_i < len(dimensions) - 1:\n                output = activation(h)\n            else:\n                output = h\n        current_input = output\n\n    if output_activation is None:\n        return current_input\n    else:\n        return output_activation(current_input)\n\n\ndef generator(z,\n              output_h,\n              output_w,\n              convolutional=True,\n              n_features=32,\n              rgb=False,\n              reuse=None):\n    """"""Simple interface to build a decoder network given the input parameters.\n\n    Parameters\n    ----------\n    z : tf.Tensor\n        Input to the generator, i.e. tf.Placeholder of tf.Variable\n    output_h : int\n        Final generated height\n    output_w : int\n        Final generated width\n    convolutional : bool, optional\n        Whether or not to build a convolutional generative network.\n    n_features : int, optional\n        Number of channels to use in the last hidden layer.\n    rgb : bool, optional\n        Whether or not the final generated image is RGB or not.\n    reuse : None, optional\n        Whether or not to reuse the variables if they are already created.\n\n    Returns\n    -------\n    x_tilde : tf.Tensor\n        Output of the generator network.\n    """"""\n    n_channels = 3 if rgb else 1\n    with tf.variable_scope(\'generator\', reuse=reuse):\n        return decoder(\n            z=z,\n            convolutional=convolutional,\n            filter_sizes=[5, 5, 5, 5, 5],\n            channels=[\n                n_features * 8, n_features * 4, n_features * 2, n_features,\n                n_channels\n            ],\n            dimensions=[[output_h // 16,\n                         output_w // 16], [output_h // 8, output_w // 8],\n                        [output_h // 4, output_w // 4],\n                        [output_h // 2, output_w // 2], [output_h, output_w]]\n            if convolutional else [384, 512, n_features],\n            activation=tf.nn.relu6,\n            output_activation=tf.nn.tanh,\n            reuse=reuse)\n\n\ndef discriminator(x, convolutional=True, n_features=32, rgb=False, reuse=False):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    convolutional : bool, optional\n        Description\n    n_features : int, optional\n        Description\n    rgb : bool, optional\n        Description\n    reuse : bool, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    with tf.variable_scope(\'discriminator\', reuse=reuse):\n        return encoder(\n            x=x,\n            convolutional=convolutional,\n            filter_sizes=[5, 5, 5, 5],\n            dimensions=[\n                n_features, n_features * 2, n_features * 4, n_features * 8\n            ] if convolutional else [n_features, 128, 256],\n            activation=tf.nn.relu6,\n            output_activation=None,\n            reuse=reuse)\n\n\ndef GAN(input_shape, n_latent, n_features, rgb, debug=True):\n    """"""Summary\n\n    Parameters\n    ----------\n    input_shape : TYPE\n        Description\n    n_latent : TYPE\n        Description\n    n_features : TYPE\n        Description\n    rgb : TYPE\n        Description\n    debug : bool, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    # Real input samples\n    # n_features is either the image dimension or flattened number of features\n    x = tf.placeholder(tf.float32, input_shape, \'x\')\n    x = (x / 127.5) - 1.0\n    sum_x = tf.summary.image(""x"", x)\n\n    # Discriminator for real input samples\n    D_real_logits = discriminator(x, n_features=n_features, rgb=rgb)\n    D_real = tf.nn.sigmoid(D_real_logits)\n    sum_D_real = tf.summary.histogram(""D_real"", D_real)\n\n    # Generator tries to recreate input samples using latent feature vector\n    z = tf.placeholder(tf.float32, [None, n_latent], \'z\')\n    sum_z = tf.summary.histogram(""z"", z)\n    G = generator(\n        z,\n        output_h=input_shape[1],\n        output_w=input_shape[2],\n        n_features=n_features,\n        rgb=rgb)\n    sum_G = tf.summary.image(""G"", G)\n\n    # Discriminator for generated samples\n    D_fake_logits = discriminator(G, n_features=n_features, rgb=rgb, reuse=True)\n    D_fake = tf.nn.sigmoid(D_fake_logits)\n    sum_D_fake = tf.summary.histogram(""D_fake"", D_fake)\n\n    with tf.variable_scope(\'loss\'):\n        # Loss functions\n        loss_D_real = utils.binary_cross_entropy(\n            D_real, tf.ones_like(D_real), name=\'loss_D_real\')\n        loss_D_fake = utils.binary_cross_entropy(\n            D_fake, tf.zeros_like(D_fake), name=\'loss_D_fake\')\n        loss_D = tf.reduce_mean((loss_D_real + loss_D_fake) / 2)\n        loss_G = tf.reduce_mean(\n            utils.binary_cross_entropy(\n                D_fake, tf.ones_like(D_fake), name=\'loss_G\'))\n\n        # Summaries\n        sum_loss_D_real = tf.summary.histogram(""loss_D_real"", loss_D_real)\n        sum_loss_D_fake = tf.summary.histogram(""loss_D_fake"", loss_D_fake)\n        sum_loss_D = tf.summary.scalar(""loss_D"", loss_D)\n        sum_loss_G = tf.summary.scalar(""loss_G"", loss_G)\n        sum_D_real = tf.summary.histogram(""D_real"", D_real)\n        sum_D_fake = tf.summary.histogram(""D_fake"", D_fake)\n\n    return {\n        \'loss_D\': loss_D,\n        \'loss_G\': loss_G,\n        \'x\': x,\n        \'G\': G,\n        \'z\': z,\n        \'sums\': {\n            \'G\': sum_G,\n            \'D_real\': sum_D_real,\n            \'D_fake\': sum_D_fake,\n            \'loss_G\': sum_loss_G,\n            \'loss_D\': sum_loss_D,\n            \'loss_D_real\': sum_loss_D_real,\n            \'loss_D_fake\': sum_loss_D_fake,\n            \'z\': sum_z,\n            \'x\': sum_x\n        }\n    }\n\n\ndef train_input_pipeline(files,\n                         init_lr_g=1e-4,\n                         init_lr_d=1e-4,\n                         n_features=10,\n                         n_latent=100,\n                         n_epochs=1000000,\n                         batch_size=200,\n                         n_samples=15,\n                         input_shape=[218, 178, 3],\n                         crop_shape=[64, 64, 3],\n                         crop_factor=0.8):\n    """"""Summary\n\n    Parameters\n    ----------\n    files : TYPE\n        Description\n    init_lr_g : float, optional\n        Description\n    init_lr_d : float, optional\n        Description\n    n_features : int, optional\n        Description\n    n_latent : int, optional\n        Description\n    n_epochs : int, optional\n        Description\n    batch_size : int, optional\n        Description\n    n_samples : int, optional\n        Description\n    input_shape : list, optional\n        Description\n    crop_shape : list, optional\n        Description\n    crop_factor : float, optional\n        Description\n\n    No Longer Returned\n    ------------------\n    name : TYPE\n        Description\n    """"""\n\n    with tf.Graph().as_default(), tf.Session() as sess:\n        batch = create_input_pipeline(\n            files=files,\n            batch_size=batch_size,\n            n_epochs=n_epochs,\n            crop_shape=crop_shape,\n            crop_factor=crop_factor,\n            shape=input_shape)\n\n        gan = GAN(\n            input_shape=[None] + crop_shape,\n            n_features=n_features,\n            n_latent=n_latent,\n            rgb=True,\n            debug=False)\n\n        vars_d = [\n            v for v in tf.trainable_variables()\n            if v.name.startswith(\'discriminator\')\n        ]\n        print(\'Training discriminator variables:\')\n        [\n            print(v.name) for v in tf.trainable_variables()\n            if v.name.startswith(\'discriminator\')\n        ]\n\n        vars_g = [\n            v for v in tf.trainable_variables()\n            if v.name.startswith(\'generator\')\n        ]\n        print(\'Training generator variables:\')\n        [\n            print(v.name) for v in tf.trainable_variables()\n            if v.name.startswith(\'generator\')\n        ]\n        zs = np.random.uniform(-1.0, 1.0, [4, n_latent]).astype(np.float32)\n        zs = utils.make_latent_manifold(zs, n_samples)\n\n        lr_g = tf.placeholder(tf.float32, shape=[], name=\'learning_rate_g\')\n        lr_d = tf.placeholder(tf.float32, shape=[], name=\'learning_rate_d\')\n\n        try:\n            from tf.contrib.layers import apply_regularization\n            d_reg = apply_regularization(\n                tf.contrib.layers.l2_regularizer(1e-6), vars_d)\n            g_reg = apply_regularization(\n                tf.contrib.layers.l2_regularizer(1e-6), vars_g)\n        except:\n            d_reg, g_reg = 0, 0\n\n        opt_g = tf.train.AdamOptimizer(\n            lr_g, name=\'Adam_g\').minimize(\n                gan[\'loss_G\'] + g_reg, var_list=vars_g)\n        opt_d = tf.train.AdamOptimizer(\n            lr_d, name=\'Adam_d\').minimize(\n                gan[\'loss_D\'] + d_reg, var_list=vars_d)\n\n        # %%\n        # We create a session to use the graph\n\n        saver = tf.train.Saver()\n        sums = gan[\'sums\']\n        G_sum_op = tf.summary.merge([\n            sums[\'G\'], sums[\'loss_G\'], sums[\'z\'], sums[\'loss_D_fake\'],\n            sums[\'D_fake\']\n        ])\n        D_sum_op = tf.summary.merge([\n            sums[\'loss_D\'], sums[\'loss_D_real\'], sums[\'loss_D_fake\'], sums[\'z\'],\n            sums[\'x\'], sums[\'D_real\'], sums[\'D_fake\']\n        ])\n        writer = tf.summary.FileWriter(""./logs"", sess.graph_def)\n\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        sess.run(init_op)\n        coord = tf.train.Coordinator()\n        tf.get_default_graph().finalize()\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n        # g = tf.get_default_graph()\n        # [print(op.name) for op in g.get_operations()]\n\n        if os.path.exists(""gan.ckpt""):\n            saver.restore(sess, ""gan.ckpt"")\n            print(""GAN model restored."")\n\n        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n        step_i, t_i = 0, 0\n        loss_d = 1\n        loss_g = 1\n        n_loss_d, total_loss_d = 1, 1\n        n_loss_g, total_loss_g = 1, 1\n        try:\n            while not coord.should_stop():\n                batch_xs = sess.run(batch)\n                step_i += 1\n                batch_zs = np.random.uniform(\n                    -1.0, 1.0, [batch_size, n_latent]).astype(np.float32)\n\n                this_lr_g = min(1e-2,\n                                max(1e-6, init_lr_g * (loss_g / loss_d)**2))\n                this_lr_d = min(1e-2,\n                                max(1e-6, init_lr_d * (loss_d / loss_g)**2))\n                # this_lr_d *= ((1.0 - (step_i / 100000)) ** 2)\n                # this_lr_g *= ((1.0 - (step_i / 100000)) ** 2)\n\n                # if np.random.random() > (loss_g / (loss_d + loss_g)):\n                if step_i % 3 == 1:\n                    loss_d, _, sum_d = sess.run(\n                        [gan[\'loss_D\'], opt_d, D_sum_op],\n                        feed_dict={\n                            gan[\'x\']: batch_xs,\n                            gan[\'z\']: batch_zs,\n                            lr_d: this_lr_d\n                        })\n                    total_loss_d += loss_d\n                    n_loss_d += 1\n                    writer.add_summary(sum_d, step_i)\n                    print(\'%04d d* = lr: %0.08f, loss: %08.06f, \\t\' % (\n                        step_i, this_lr_d, loss_d\n                    ) + \'g  = lr: %0.08f, loss: %08.06f\' % (this_lr_g, loss_g))\n                else:\n                    loss_g, _, sum_g = sess.run(\n                        [gan[\'loss_G\'], opt_g, G_sum_op],\n                        feed_dict={gan[\'z\']: batch_zs,\n                                   lr_g: this_lr_g})\n                    total_loss_g += loss_g\n                    n_loss_g += 1\n                    writer.add_summary(sum_g, step_i)\n                    print(\'%04d d  = lr: %0.08f, loss: %08.06f, \\t\' % (\n                        step_i, this_lr_d, loss_d\n                    ) + \'g* = lr: %0.08f, loss: %08.06f\' % (this_lr_g, loss_g))\n\n                if step_i % 100 == 0:\n                    samples = sess.run(gan[\'G\'], feed_dict={gan[\'z\']: zs})\n                    utils.montage(\n                        np.clip((samples + 1) * 127.5, 0, 255).astype(np.uint8),\n                        \'imgs/gan_%08d.png\' % t_i)\n                    t_i += 1\n\n                    print(\'generator loss:\', total_loss_g / n_loss_g)\n                    print(\'discriminator loss:\', total_loss_d / n_loss_d)\n\n                    # Save the variables to disk.\n                    save_path = saver.save(\n                        sess,\n                        ""./gan.ckpt"",\n                        global_step=step_i,\n                        write_meta_graph=False)\n                    print(""Model saved in file: %s"" % save_path)\n        except tf.errors.OutOfRangeError:\n            print(\'Done training -- epoch limit reached\')\n        finally:\n            # One of the threads has issued an exception.  So let\'s tell all the\n            # threads to shutdown.\n            coord.request_stop()\n\n        # Wait until all threads have finished.\n        coord.join(threads)\n\n\nif __name__ == \'__main__\':\n    files = CELEB()\n    train_input_pipeline(files=files)\n'"
cadl/gif.py,0,"b'""""""Utility for creating a GIF.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n\ndef build_gif(imgs, interval=0.1, dpi=72,\n              save_gif=True, saveto=\'animation.gif\',\n              show_gif=False, cmap=None):\n    """"""Take an array or list of images and create a GIF.\n\n    Parameters\n    ----------\n    imgs : np.ndarray or list\n        List of images to create a GIF of\n    interval : float, optional\n        Spacing in seconds between successive images.\n    dpi : int, optional\n        Dots per inch.\n    save_gif : bool, optional\n        Whether or not to save the GIF.\n    saveto : str, optional\n        Filename of GIF to save.\n    show_gif : bool, optional\n        Whether or not to render the GIF using plt.\n    cmap : None, optional\n        Optional colormap to apply to the images.\n\n    Returns\n    -------\n    ani : matplotlib.animation.ArtistAnimation\n        The artist animation from matplotlib.  Likely not useful.\n    """"""\n    imgs = np.asarray(imgs)\n    h, w, *c = imgs[0].shape\n    fig, ax = plt.subplots(figsize=(np.round(w / dpi), np.round(h / dpi)))\n    fig.subplots_adjust(bottom=0)\n    fig.subplots_adjust(top=1)\n    fig.subplots_adjust(right=1)\n    fig.subplots_adjust(left=0)\n    ax.set_axis_off()\n\n    if cmap is not None:\n        axs = list(map(lambda x: [\n            ax.imshow(x, cmap=cmap)], imgs))\n    else:\n        axs = list(map(lambda x: [\n            ax.imshow(x)], imgs))\n\n    ani = animation.ArtistAnimation(\n        fig, axs, interval=interval * 1000, repeat_delay=0, blit=True)\n\n    if save_gif:\n        ani.save(saveto, writer=\'imagemagick\', dpi=dpi)\n\n    if show_gif:\n        plt.show()\n    else:\n        plt.close(fig)\n    return ani\n'"
cadl/glove.py,0,"b'""""""Global Vector Embeddings.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom cadl import utils\nimport zipfile\nfrom scipy.spatial import distance, distance_matrix\nfrom sklearn.decomposition import PCA\n\n\ndef get_model():\n    """"""Summary\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    # Download the glove model and open a zip file\n    file = utils.download(\'http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\')\n    zf = zipfile.ZipFile(file)\n\n    # Collect the words and their vectors\n    words = []\n    vectors = []\n    for l in zf.open(""glove.6B.300d.txt""):\n        t = l.strip().split()\n        words.append(t[0].decode())\n        vectors.append(list(map(np.double, t[1:])))\n\n    # Store as a lookup table\n    wordvecs = np.asarray(vectors, dtype=np.double)\n    word2id = {word: i for i, word in enumerate(words)}\n    return wordvecs, word2id, words\n\n\ndef course_example():\n    """"""Summary\n    """"""\n    wordvecs, word2id, words = get_model()\n\n    word = \'2000\'\n    print(word2id[word])\n\n    print(wordvecs[word2id[word]])\n\n    # Get distances to target word\n    target_vec = wordvecs[word2id[word]]\n    dists = []\n    for vec_i in wordvecs:\n        dists.append(distance.cosine(target_vec, vec_i))\n\n    k = 20\n\n    # Print top nearest words\n    idxs = np.argsort(dists)\n    for idx_i in idxs[:k]:\n        print(words[idx_i], dists[idx_i])\n\n    # Plot top nearest words\n    labels = [words[idx_i] for idx_i in idxs[:k]]\n    plt.figure()\n    plt.bar(range(k),\n            [dists[idx_i] for idx_i in idxs[:k]])\n    ax = plt.gca()\n    ax.set_xticks(range(len(labels)))\n    ax.set_xticklabels(labels, rotation=\'vertical\')\n    plt.xlabel(\'label\')\n    plt.ylabel(\'distances\')\n\n    # Create distance matrix\n    vecs = [wordvecs[idx_i] for idx_i in idxs[:k]]\n    dm = distance_matrix(vecs, vecs)\n    plt.figure()\n    plt.imshow(dm)\n    ax = plt.gca()\n    ax.set_xticks(range(len(labels)))\n    ax.set_yticks(range(len(labels)))\n    ax.set_xticklabels(labels, rotation=\'vertical\')\n    ax.set_yticklabels(labels)\n    plt.colorbar()\n\n    # Plot data points in reduced dimensionality using principal components\n    # of the distance matrix\n    res = PCA(2).fit_transform(dm / np.mean(dm, axis=0, keepdims=True))\n    pc1, pc2 = res[:, 0], res[:, 1]\n    plt.figure()\n    plt.scatter(pc1, pc2)\n    for i in range(len(labels)):\n        plt.text(pc1[i], pc2[i], labels[i])\n\n    # Let\'s stick it all in a function and explore some other words:\n    def plot_nearest_words(word, k=20):\n        """"""Summary\n\n        Parameters\n        ----------\n        word : TYPE\n            Description\n        k : int, optional\n            Description\n        """"""\n        # Get distances to target word\n        target_vec = wordvecs[word2id[word]]\n        dists = []\n        for vec_i in wordvecs:\n            dists.append(distance.cosine(target_vec, vec_i))\n        idxs = np.argsort(dists)\n        labels = [words[idx_i] for idx_i in idxs[:k]]\n        vecs = [wordvecs[idx_i] for idx_i in idxs[:k]]\n        dm = distance_matrix(vecs, vecs)\n\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n\n        # Create distance matrix\n        axs[0].imshow(dm)\n        axs[0].set_xticks(range(len(labels)))\n        axs[0].set_yticks(range(len(labels)))\n        axs[0].set_xticklabels(labels, rotation=\'vertical\')\n        axs[0].set_yticklabels(labels)\n\n        # Center the distance matrix\n        dm = dm / np.mean(dm, axis=0, keepdims=True)\n\n        # Plot data points in reduced dimensionality using principal components\n        # of the distance matrix\n        res = PCA(2).fit_transform(dm)\n        pc1, pc2 = res[:, 0], res[:, 1]\n        axs[1].scatter(pc1, pc2)\n        for i in range(len(labels)):\n            axs[1].text(pc1[i], pc2[i], labels[i])\n\n    plot_nearest_words(\'2000\')\n    plot_nearest_words(\'intelligence\')\n\n    # What else can we explore?  Well this embedding is ""linear"" meaning we can\n    # actually try performing arithmetic in this space.  A classic example is what\n    # happens when we perform: ""man"" - ""king"" + ""woman""?  Or in other words, can the\n    # word embedding understand analogies?  For instance, if man is to king as woman\n    # is to queen, then we should be able to subtract man and king, and add woman\n    # to see the result of the analogy.\n\n    # Let\'s create a function which will return us the nearest words rather than\n    # plot them:\n    def get_nearest_words(target_vec, k=20):\n        """"""Summary\n\n        Parameters\n        ----------\n        target_vec : TYPE\n            Description\n        k : int, optional\n            Description\n\n        Returns\n        -------\n        TYPE\n            Description\n        """"""\n        # Get distances to target vector\n        dists = []\n        for vec_i in wordvecs:\n            dists.append(distance.cosine(target_vec, vec_i))\n        # Get top nearest words\n        idxs = np.argsort(dists)\n        res = []\n        for idx_i in idxs[:k]:\n            res.append((words[idx_i], dists[idx_i]))\n        return res\n\n    # And a convenience function for returning a vector\n    def get_vector(word):\n        """"""Summary\n\n        Parameters\n        ----------\n        word : TYPE\n            Description\n\n        Returns\n        -------\n        TYPE\n            Description\n        """"""\n        return wordvecs[word2id[word]]\n\n    # Now we can try some word embedding arithmetic\n    get_nearest_words(get_vector(\'king\') - get_vector(\'man\') + get_vector(\'woman\'))\n    get_nearest_words(get_vector(\'france\') - get_vector(\'french\') + get_vector(\'spain\'))\n'"
cadl/i2v.py,10,"b'""""""Illustration2Vec model and preprocessing.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport json\nimport numpy as np\nfrom tensorflow.python.platform import gfile\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize as imresize\nfrom cadl.utils import download\n\n\ndef i2v_download():\n    """"""Download a pretrained i2v network.\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    model = download(\'https://s3.amazonaws.com/cadl/models/illust2vec.tfmodel\')\n    return model\n\n\ndef i2v_tag_download():\n    """"""Download a pretrained i2v network.\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    model = download(\'https://s3.amazonaws.com/cadl/models/illust2vec_tag.tfmodel\')\n    tags = download(\'https://s3.amazonaws.com/cadl/models/tag_list.json\')\n    return model, tags\n\n\ndef get_i2v_model():\n    """"""Get a pretrained i2v network.\n\n    Returns\n    -------\n    net : dict\n        {\'graph_def\': graph_def, \'labels\': synsets}\n        where the graph_def is a tf.GraphDef and the synsets\n        map an integer label from 0-1000 to a list of names\n    """"""\n    # Download the trained net\n    model = i2v_download()\n\n    # Load the saved graph\n    with gfile.GFile(model, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n\n    return {\'graph_def\': graph_def}\n\n\ndef get_i2v_tag_model():\n    """"""Get a pretrained i2v tag network.\n\n    Returns\n    -------\n    net : dict\n        {\'graph_def\': graph_def, \'labels\': synsets}\n        where the graph_def is a tf.GraphDef and the synsets\n        map an integer label from 0-1000 to a list of names\n    """"""\n    # Download the trained net\n    model, tags = i2v_tag_download()\n    tags = json.load(open(tags, \'r\'))\n\n    # Load the saved graph\n    with gfile.GFile(model, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n\n    return {\n        \'graph_def\': graph_def,\n        \'labels\': tags,\n        \'preprocess\': preprocess,\n        \'deprocess\': deprocess\n    }\n\n\ndef preprocess(img, crop=True, resize=True, dsize=(224, 224)):\n    """"""Summary\n\n    Parameters\n    ----------\n    img : TYPE\n        Description\n    crop : bool, optional\n        Description\n    resize : bool, optional\n        Description\n    dsize : tuple, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    mean_img = np.array([164.76139251, 167.47864617, 181.13838569])\n    if img.dtype == np.uint8:\n        img = (img[..., ::-1] - mean_img).astype(np.float32)\n    else:\n        img = img[..., ::-1] * 255.0 - mean_img\n\n    if crop:\n        short_edge = min(img.shape[:2])\n        yy = int((img.shape[0] - short_edge) / 2)\n        xx = int((img.shape[1] - short_edge) / 2)\n        crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n    else:\n        crop_img = img\n    if resize:\n        norm_img = imresize(crop_img, dsize, preserve_range=True)\n    else:\n        norm_img = crop_img\n    return (norm_img).astype(np.float32)\n\n\ndef deprocess(img):\n    """"""Summary\n\n    Parameters\n    ----------\n    img : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    mean_img = np.array([164.76139251, 167.47864617, 181.13838569])\n    processed = (img + mean_img)[..., ::-1]\n    return np.clip(processed, 0, 255).astype(np.uint8)\n    # return ((img / np.max(np.abs(img))) * 127.5 +\n    #         127.5).astype(np.uint8)\n\n\ndef test_i2v():\n    """"""Loads the i2v network and applies it to a test image.\n    """"""\n    with tf.Session() as sess:\n        net = get_i2v_model()\n        tf.import_graph_def(net[\'graph_def\'], name=\'i2v\')\n        g = tf.get_default_graph()\n        names = [op.name for op in g.get_operations()]\n        x = g.get_tensor_by_name(names[0] + \':0\')\n        softmax = g.get_tensor_by_name(names[-3] + \':0\')\n\n        from skimage import data\n        img = preprocess(data.coffee())[np.newaxis]\n        res = np.squeeze(softmax.eval(feed_dict={x: img}))\n        print([(res[idx], net[\'labels\'][idx])\n               for idx in res.argsort()[-5:][::-1]])\n\n        """"""Let\'s visualize the network\'s gradient activation\n        when backpropagated to the original input image.  This\n        is effectively telling us which pixels contribute to the\n        predicted class or given neuron""""""\n        pools = [name for name in names if \'pool\' in name.split(\'/\')[-1]]\n        fig, axs = plt.subplots(1, len(pools))\n        for pool_i, poolname in enumerate(pools):\n            pool = g.get_tensor_by_name(poolname + \':0\')\n            pool.get_shape()\n            neuron = tf.reduce_max(pool, 1)\n            saliency = tf.gradients(neuron, x)\n            neuron_idx = tf.arg_max(pool, 1)\n            this_res = sess.run([saliency[0], neuron_idx],\n                                feed_dict={x: img})\n\n            grad = this_res[0][0] / np.max(np.abs(this_res[0]))\n            axs[pool_i].imshow((grad * 128 + 128).astype(np.uint8))\n            axs[pool_i].set_title(poolname)\n'"
cadl/inception.py,8,"b'""""""Inception model, download, and preprocessing.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport os\nimport numpy as np\nfrom tensorflow.python.platform import gfile\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize as imresize\nfrom .utils import download_and_extract_tar, download_and_extract_zip\n\n\ndef inception_download(data_dir=\'inception\', version=\'v5\'):\n    """"""Download a pretrained inception network.\n\n    Parameters\n    ----------\n    data_dir : str, optional\n        Location of the pretrained inception network download.\n    version : str, optional\n        Version of the model: [\'v3\'] or \'v5\'.\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    if version == \'v3\':\n        download_and_extract_tar(\n            \'https://s3.amazonaws.com/cadl/models/inception-2015-12-05.tgz\',\n            data_dir)\n        return (os.path.join(data_dir, \'classify_image_graph_def.pb\'),\n                os.path.join(data_dir, \'imagenet_synset_to_human_label_map.txt\'))\n    else:\n        download_and_extract_zip(\n            \'https://s3.amazonaws.com/cadl/models/inception5h.zip\', data_dir)\n        return (os.path.join(data_dir, \'tensorflow_inception_graph.pb\'),\n                os.path.join(data_dir, \'imagenet_comp_graph_label_strings.txt\'))\n\n\ndef get_inception_model(data_dir=\'inception\', version=\'v5\'):\n    """"""Get a pretrained inception network.\n\n    Parameters\n    ----------\n    data_dir : str, optional\n        Location of the pretrained inception network download.\n    version : str, optional\n        Version of the model: [\'v3\'] or \'v5\'.\n\n    Returns\n    -------\n    net : dict\n        {\'graph_def\': graph_def, \'labels\': synsets}\n        where the graph_def is a tf.GraphDef and the synsets\n        map an integer label from 0-1000 to a list of names\n    """"""\n    # Download the trained net\n    model, labels = inception_download(data_dir, version)\n\n    # Parse the ids and synsets\n    txt = open(labels).readlines()\n    synsets = [(key, val.strip()) for key, val in enumerate(txt)]\n\n    # Load the saved graph\n    with gfile.GFile(model, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n    return {\n        \'graph_def\': graph_def,\n        \'labels\': synsets,\n        \'preprocess\': preprocess,\n        \'deprocess\': deprocess\n    }\n\n\ndef preprocess(img, crop=True, resize=True, dsize=(299, 299)):\n    """"""Summary\n\n    Parameters\n    ----------\n    img : TYPE\n        Description\n    crop : bool, optional\n        Description\n    resize : bool, optional\n        Description\n    dsize : tuple, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    if img.dtype != np.uint8:\n        img *= 255.0\n\n    if crop:\n        crop = np.min(img.shape[:2])\n        r = (img.shape[0] - crop) // 2\n        c = (img.shape[1] - crop) // 2\n        cropped = img[r: r + crop, c: c + crop]\n    else:\n        cropped = img\n\n    if resize:\n        rsz = imresize(cropped, dsize, preserve_range=True)\n    else:\n        rsz = cropped\n\n    if rsz.ndim == 2:\n        rsz = rsz[..., np.newaxis]\n\n    rsz = rsz.astype(np.float32)\n    # subtract imagenet mean\n    return (rsz - 117)\n\n\ndef deprocess(img):\n    """"""Summary\n\n    Parameters\n    ----------\n    img : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    return np.clip(img + 117, 0, 255).astype(np.uint8)\n\n\ndef test_inception():\n    """"""Loads the inception network and applies it to a test image.\n    """"""\n    with tf.Session() as sess:\n        net = get_inception_model()\n        tf.import_graph_def(net[\'graph_def\'], name=\'inception\')\n        g = tf.get_default_graph()\n        names = [op.name for op in g.get_operations()]\n        x = g.get_tensor_by_name(names[0] + \':0\')\n        softmax = g.get_tensor_by_name(names[-3] + \':0\')\n\n        from skimage import data\n        img = preprocess(data.coffee())[np.newaxis]\n        res = np.squeeze(softmax.eval(feed_dict={x: img}))\n        print([(res[idx], net[\'labels\'][idx])\n               for idx in res.argsort()[-5:][::-1]])\n\n        """"""Let\'s visualize the network\'s gradient activation\n        when backpropagated to the original input image.  This\n        is effectively telling us which pixels contribute to the\n        predicted class or given neuron""""""\n        pools = [name for name in names if \'pool\' in name.split(\'/\')[-1]]\n        fig, axs = plt.subplots(1, len(pools))\n        for pool_i, poolname in enumerate(pools):\n            pool = g.get_tensor_by_name(poolname + \':0\')\n            pool.get_shape()\n            neuron = tf.reduce_max(pool, 1)\n            saliency = tf.gradients(neuron, x)\n            neuron_idx = tf.arg_max(pool, 1)\n            this_res = sess.run([saliency[0], neuron_idx],\n                                feed_dict={x: img})\n\n            grad = this_res[0][0] / np.max(np.abs(this_res[0]))\n            axs[pool_i].imshow((grad * 128 + 128).astype(np.uint8))\n            axs[pool_i].set_title(poolname)\n'"
cadl/librispeech.py,0,"b'""""""LibriSpeech dataset, batch processing, and preprocessing.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport os\nfrom scipy.io import wavfile\nfrom cadl.utils import download_and_extract_tar\nfrom glob import glob\nimport subprocess\nimport numpy as np\n\n\ndef get_dataset(saveto=\'librispeech\', convert_to_wav=False, kind=\'dev\'):\n    """"""Download the LibriSpeech dataset and convert to wav files.\n\n    More info: http://www.openslr.org/12/\n\n    This interface downloads the LibriSpeech dataset and attempts to\n    convert the flac to wave files using ffmpeg.  If you do not have ffmpeg\n    installed, this function will not be able to convert the files to waves.\n\n    Parameters\n    ----------\n    saveto : str\n        Directory to save the resulting dataset [\'librispeech\']\n    convert_to_wav : bool, optional\n        Description\n    kind : str, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    if not os.path.exists(saveto):\n        if kind == \'dev\':\n            download_and_extract_tar(\n                \'http://www.openslr.org/resources/12/dev-clean.tar.gz\', saveto)\n        elif kind == \'train-100\':\n            download_and_extract_tar(\n                \'http://www.openslr.org/resources/12/train-clean-100.tar.gz\',\n                saveto)\n        elif kind == \'train-360\':\n            download_and_extract_tar(\n                \'http://www.openslr.org/resources/12/train-clean-360.tar.gz\',\n                saveto)\n        else:\n            print(\'Not downloading.  Pass in either [""dev""],\'\n                  \'""train-100"", or ""train-360"", in order to \'\n                  \'download the dataset.\')\n\n    wavs = glob(\'{}/**/*.wav\'.format(saveto), recursive=True)\n    if convert_to_wav:\n        if len(wavs) == 0:\n            flacs = glob(\'{}/**/*.flac\'.format(saveto), recursive=True)\n            for f in flacs:\n                subprocess.check_call(\n                    [\'ffmpeg\', \'-i\', f, \'-f\', \'wav\', \'-y\', \'%s.wav\' % f])\n            wavs = glob(\'{}/**/*.wav\'.format(saveto), recursive=True)\n        else:\n            print(\'WARNING: Found existing wave files.  Not converting!\')\n\n    dataset = []\n    for wav_i in wavs:\n        id_i, chapter_i, utter_i = wav_i.split(\'/\')[-3:]\n        dataset.append({\n            \'name\': wav_i,\n            \'id\': id_i,\n            \'chapter\': chapter_i,\n            \'utterance\': utter_i.split(\'-\')[-1].strip(\'.wav\')\n        })\n    if len(wavs) == 0:\n        print(\'LibriSpeech is a FLAC dataset.  Consider rerunning this \'\n              \'command with convert_to_wav=True, to use ffmpeg to \'\n              \'convert the flac files to wave files first. This requires \'\n              \'the use of ffmpeg and so this should be installed first.\')\n    return dataset\n\n\ndef batch_generator(dataset,\n                    batch_size=32,\n                    max_sequence_length=6144,\n                    maxval=32768.0,\n                    threshold=0.2,\n                    normalize=True):\n    """"""Summary\n\n    Parameters\n    ----------\n    dataset : TYPE\n        Description\n    batch_size : int, optional\n        Description\n    max_sequence_length : int, optional\n        Description\n    maxval : float, optional\n        Description\n    threshold : float, optional\n        Description\n    normalize : bool, optional\n        Description\n\n    Yields\n    ------\n    TYPE\n        Description\n    """"""\n    n_batches = len(dataset) // batch_size\n    for batch_i in range(n_batches):\n        cropped_wavs, ids = [], []\n        while len(cropped_wavs) < batch_size:\n            idx_i = np.random.choice(np.arange(len(dataset)))\n            fname_i = dataset[idx_i][\'name\']\n            id_i = dataset[idx_i][\'id\']\n            wav_i = wavfile.read(fname_i)[1]\n            sample = np.random.choice(range(len(wav_i) - max_sequence_length))\n            cropped_wav = wav_i[sample:sample + max_sequence_length]\n            if np.max(np.abs(cropped_wav) / maxval) > threshold:\n                if normalize:\n                    cropped_wav = cropped_wav / maxval\n                cropped_wavs.append(cropped_wav)\n                ids.append(id_i)\n        yield np.array(cropped_wavs, np.float32), np.array(ids, np.int32)\n'"
cadl/magenta_utils.py,1,"b'""""""Various utilities for working with Google\'s Magenta Library.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport os\nimport tempfile\nfrom collections import defaultdict\nimport tensorflow as tf\nimport pretty_midi\nimport magenta\nfrom magenta.music import midi_io, sequences_lib\nfrom magenta.music.melodies_lib import Melody\nfrom magenta.models.melody_rnn import melody_rnn_sequence_generator, \\\n    melody_rnn_model\nfrom magenta.protobuf import generator_pb2\n\n\ndef convert_to_monophonic(seq):\n    """"""Summary\n\n    Parameters\n    ----------\n    seq : TYPE\n        Description\n    """"""\n    note_i = 1\n    while note_i < len(seq.notes):\n        n_prev = seq.notes[note_i - 1]\n        n_curr = seq.notes[note_i]\n        if n_curr.start_time >= n_prev.start_time and \\\n                n_curr.start_time < n_prev.end_time:\n            seq.notes.remove(n_prev)\n        else:\n            note_i += 1\n\n\ndef parse_midi_file(midi_file,\n                    max_notes=float(\'Inf\'),\n                    max_time_signatures=1,\n                    max_tempos=1,\n                    ignore_polyphonic_notes=True,\n                    convert_to_drums=False,\n                    steps_per_quarter=16):\n    """"""Summary\n\n    Parameters\n    ----------\n    midi_file : TYPE\n        Description\n    max_notes : TYPE, optional\n        Description\n    max_time_signatures : int, optional\n        Description\n    max_tempos : int, optional\n        Description\n    ignore_polyphonic_notes : bool, optional\n        Description\n    convert_to_drums : bool, optional\n        Description\n    steps_per_quarter : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    seq = midi_io.midi_file_to_sequence_proto(midi_file)\n\n    while len(seq.notes) > max_notes:\n        seq.notes.pop()\n\n    while len(seq.time_signatures) > max_time_signatures:\n        seq.time_signatures.pop()\n\n    while len(seq.tempos) > max_tempos:\n        seq.tempos.pop()\n\n    if convert_to_drums:\n        for note_i in range(len(seq.notes)):\n            seq.notes[note_i].program = 10\n\n    if ignore_polyphonic_notes:\n        convert_to_monophonic(seq)\n\n    seq = sequences_lib.quantize_note_sequence(\n        seq, steps_per_quarter=steps_per_quarter)\n\n    if seq.tempos:\n        qpm = seq.tempos[0].qpm\n    else:\n        qpm = 120\n\n    melody = Melody()\n    melody.from_quantized_sequence(\n        seq, ignore_polyphonic_notes=ignore_polyphonic_notes)\n    seq = melody.to_sequence(qpm=qpm)\n\n    return seq, qpm\n\n\ndef parse_midi_dataset(directory, saveto=\'parsed\', **kwargs):\n    """"""Summary\n\n    Parameters\n    ----------\n    directory : TYPE\n        Description\n    saveto : str, optional\n        Description\n    **kwargs\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    if not os.path.exists(saveto):\n        os.mkdir(saveto)\n    failed = []\n    for root, dirs, files in os.walk(directory):\n        for name in files:\n            if name.endswith(\'mid\'):\n                fname = os.path.join(root, name)\n                seq, qpm = parse_midi_file(fname, **kwargs)\n                out_fname = os.path.join(saveto, name)\n                magenta.music.sequence_proto_to_midi_file(seq, out_fname)\n    return failed\n\n\ndef synthesize(midi_file, model=\'basic\', num_steps=2000,\n               max_primer_notes=32, temperature=1.0,\n               beam_size=1, branch_factor=1,\n               steps_per_quarter=16, **kwargs):\n    """"""Summary\n\n    Parameters\n    ----------\n    midi_file : TYPE\n        Description\n    model : str, optional\n        Description\n    num_steps : int, optional\n        Description\n    max_primer_notes : int, optional\n        Description\n    temperature : float, optional\n        Description\n    beam_size : int, optional\n        Description\n    branch_factor : int, optional\n        Description\n    steps_per_quarter : int, optional\n        Description\n    **kwargs\n        Description\n    """"""\n    config = melody_rnn_model.default_configs[\'{}_rnn\'.format(model)]\n    bundle_file = \'{}_rnn.mag\'.format(model)\n    generator = melody_rnn_sequence_generator.MelodyRnnSequenceGenerator(\n        model=melody_rnn_model.MelodyRnnModel(config),\n        details=config.details,\n        steps_per_quarter=steps_per_quarter,\n        bundle=magenta.music.read_bundle_file(bundle_file)\n    )\n\n    # Get a protobuf of the MIDI file\n    seq, qpm = parse_midi_file(midi_file, **kwargs)\n\n    opt = generator_pb2.GeneratorOptions()\n    seconds_per_step = 60.0 / qpm / steps_per_quarter\n    total_seconds = num_steps * seconds_per_step\n    last_end_time = max(n.end_time for n in seq.notes)\n    opt.generate_sections.add(\n        start_time=last_end_time + seconds_per_step,\n        end_time=total_seconds)\n    opt.args[\'temperature\'].float_value = temperature\n    opt.args[\'beam_size\'].int_value = beam_size\n    opt.args[\'branch_factor\'].int_value = branch_factor\n    opt.args[\'steps_per_iteration\'].int_value = 1\n\n    print(opt)\n    generated = generator.generate(seq, opt)\n    fname = \'primer.mid\'\n    magenta.music.sequence_proto_to_midi_file(seq, fname)\n    fname = \'synthesis.mid\'\n    magenta.music.sequence_proto_to_midi_file(generated, fname)\n\n\ndef sequence_proto_to_pretty_midi(\n        sequence, is_drum=False,\n        drop_events_n_seconds_after_last_note=None):\n    """"""Convert tensorflow.magenta.NoteSequence proto to a PrettyMIDI.\n\n    Time is stored in the NoteSequence in absolute values (seconds) as opposed to\n    relative values (MIDI ticks). When the NoteSequence is translated back to\n    PrettyMIDI the absolute time is retained. The tempo map is also recreated.\n\n    Parameters\n    ----------\n    sequence\n        A tensorfow.magenta.NoteSequence proto.\n    is_drum : bool, optional\n        Description\n    drop_events_n_seconds_after_last_note\n        Events (e.g., time signature changes)\n        that occur this many seconds after the last note will be dropped. If\n        None, then no events will be dropped.\n\n    Returns\n    -------\n    A pretty_midi.PrettyMIDI object or None if sequence could not be decoded.\n    """"""\n    from magenta.music import constants\n    _PRETTY_MIDI_MAJOR_TO_MINOR_OFFSET = 12\n\n    ticks_per_quarter = (sequence.ticks_per_quarter\n                         if sequence.ticks_per_quarter else\n                         constants.STANDARD_PPQ)\n\n    max_event_time = None\n    if drop_events_n_seconds_after_last_note is not None:\n        max_event_time = (max([n.end_time for n in sequence.notes]) +\n                          drop_events_n_seconds_after_last_note)\n\n    # Try to find a tempo at time zero. The list is not guaranteed to be in\n    # order.\n    initial_seq_tempo = None\n    for seq_tempo in sequence.tempos:\n        if seq_tempo.time == 0:\n            initial_seq_tempo = seq_tempo\n            break\n\n    kwargs = {}\n    kwargs[\'initial_tempo\'] = (initial_seq_tempo.qpm if initial_seq_tempo\n                               else constants.DEFAULT_QUARTERS_PER_MINUTE)\n    pm = pretty_midi.PrettyMIDI(resolution=ticks_per_quarter, **kwargs)\n\n    # Create an empty instrument to contain time and key signatures.\n    instrument = pretty_midi.Instrument(0, is_drum=is_drum)\n    pm.instruments.append(instrument)\n\n    # Populate time signatures.\n    for seq_ts in sequence.time_signatures:\n        if max_event_time and seq_ts.time > max_event_time:\n            continue\n        time_signature = pretty_midi.containers.TimeSignature(\n            seq_ts.numerator, seq_ts.denominator, seq_ts.time)\n        pm.time_signature_changes.append(time_signature)\n\n    # Populate key signatures.\n    for seq_key in sequence.key_signatures:\n        if max_event_time and seq_key.time > max_event_time:\n            continue\n        key_number = seq_key.key\n        if seq_key.mode == seq_key.MINOR:\n            key_number += _PRETTY_MIDI_MAJOR_TO_MINOR_OFFSET\n        key_signature = pretty_midi.containers.KeySignature(\n            key_number, seq_key.time)\n        pm.key_signature_changes.append(key_signature)\n\n    # Populate tempos.\n    # TODO(douglaseck): Update this code if pretty_midi adds the ability to\n    # write tempo.\n    for seq_tempo in sequence.tempos:\n        # Skip if this tempo was added in the PrettyMIDI constructor.\n        if seq_tempo == initial_seq_tempo:\n            continue\n        if max_event_time and seq_tempo.time > max_event_time:\n            continue\n        tick_scale = 60.0 / (pm.resolution * seq_tempo.qpm)\n        tick = pm.time_to_tick(seq_tempo.time)\n        # pylint: disable=protected-access\n        pm._tick_scales.append((tick, tick_scale))\n        pm._update_tick_to_time(0)\n        # pylint: enable=protected-access\n\n    # Populate instrument events by first gathering notes and other event types\n    # in lists then write them sorted to the PrettyMidi object.\n    instrument_events = defaultdict(lambda: defaultdict(list))\n    for seq_note in sequence.notes:\n        instrument_events[(seq_note.instrument, seq_note.program,\n                           seq_note.is_drum)][\'notes\'].append(\n            pretty_midi.Note(\n                seq_note.velocity, seq_note.pitch,\n                seq_note.start_time, seq_note.end_time))\n    for seq_bend in sequence.pitch_bends:\n        if max_event_time and seq_bend.time > max_event_time:\n            continue\n        instrument_events[(seq_bend.instrument, seq_bend.program,\n                           seq_bend.is_drum)][\'bends\'].append(\n            pretty_midi.PitchBend(seq_bend.bend, seq_bend.time))\n    for seq_cc in sequence.control_changes:\n        if max_event_time and seq_cc.time > max_event_time:\n            continue\n        instrument_events[(seq_cc.instrument, seq_cc.program,\n                           seq_cc.is_drum)][\'controls\'].append(\n            pretty_midi.ControlChange(\n                seq_cc.control_number,\n                seq_cc.control_value, seq_cc.time))\n\n    for (instr_id, prog_id, is_drum) in sorted(instrument_events.keys()):\n        # For instr_id 0 append to the instrument created above.\n        if instr_id > 0:\n            instrument = pretty_midi.Instrument(prog_id, is_drum)\n            pm.instruments.append(instrument)\n        instrument.program = prog_id\n        instrument.notes = instrument_events[\n            (instr_id, prog_id, is_drum)][\'notes\']\n        instrument.pitch_bends = instrument_events[\n            (instr_id, prog_id, is_drum)][\'bends\']\n        instrument.control_changes = instrument_events[\n            (instr_id, prog_id, is_drum)][\'controls\']\n\n    return pm\n\n\ndef sequence_proto_to_midi_file(sequence, output_file,\n                                is_drum=False,\n                                drop_events_n_seconds_after_last_note=None):\n    """"""Convert tensorflow.magenta.NoteSequence proto to a MIDI file on disk.\n\n    Time is stored in the NoteSequence in absolute values (seconds) as opposed to\n    relative values (MIDI ticks). When the NoteSequence is translated back to\n    MIDI the absolute time is retained. The tempo map is also recreated.\n\n    Parameters\n    ----------\n    sequence\n        A tensorfow.magenta.NoteSequence proto.\n    output_file\n        String path to MIDI file that will be written.\n    is_drum : bool, optional\n        Description\n    drop_events_n_seconds_after_last_note\n        Events (e.g., time signature changes)\n        that occur this many seconds after the last note will be dropped. If\n        None, then no events will be dropped.\n    """"""\n    pretty_midi_object = sequence_proto_to_pretty_midi(\n        sequence, is_drum, drop_events_n_seconds_after_last_note)\n    with tempfile.NamedTemporaryFile() as temp_file:\n        pretty_midi_object.write(temp_file.name)\n        tf.gfile.Copy(temp_file.name, output_file, overwrite=True)\n'"
cadl/mdn.py,28,"b'""""""Mixture Density Network.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.contrib.layers as tfl\nfrom skimage.data import astronaut, coffee\nfrom scipy.misc import imresize\n\n\ndef gausspdf_np(x, mean, sigma):\n    return np.exp(-(x - mean)**2 /\n                  (2 * sigma**2)) / (np.sqrt(2.0 * np.pi) * sigma)\n\n\ndef gausspdf(x, mean, sigma):\n    return tf.exp(-(x - mean)**2 /\n                  (2 * sigma**2)) / (tf.sqrt(2.0 * np.pi) * sigma)\n\n\ndef build_single_gaussian_model(n_input_features=2,\n                                n_output_features=3,\n                                n_neurons=[128, 128]):\n    X = tf.placeholder(tf.float32, shape=[None, n_input_features], name=\'X\')\n    Y = tf.placeholder(tf.float32, shape=[None, n_output_features], name=\'Y\')\n\n    current_input = X\n    for layer_i in range(len(n_neurons)):\n        current_input = tfl.linear(\n            inputs=current_input,\n            num_outputs=n_neurons[layer_i],\n            activation_fn=tf.nn.tanh,\n            scope=\'layer/\' + str(layer_i))\n\n    means = tfl.linear(\n        inputs=current_input,\n        num_outputs=n_output_features,\n        activation_fn=None,\n        scope=\'means\')\n    sigmas = tf.maximum(\n        tfl.linear(\n            inputs=current_input,\n            num_outputs=n_output_features,\n            activation_fn=tf.nn.relu,\n            scope=\'sigmas\'), 1e-10)\n\n    p = gausspdf(Y, means, sigmas)\n    negloglike = -tf.log(tf.maximum(p, 1e-10))\n    cost = tf.reduce_mean(tf.reduce_mean(negloglike, 1))\n    return X, Y, cost, means\n\n\ndef get_data(img):\n    xs = []\n    ys = []\n    for row_i in range(img.shape[0]):\n        for col_i in range(img.shape[1]):\n            xs.append([row_i, col_i])\n            ys.append(img[row_i, col_i])\n\n    xs = np.array(xs)\n    ys = np.array(ys)\n    xs = (xs - np.mean(xs)) / np.std(xs)\n    ys = (ys / 255.0)\n    return xs, ys\n\n\ndef train_single_gaussian_model():\n    img = imresize(astronaut(), (64, 64))\n    xs, ys = get_data(img)\n    n_iterations = 500\n    batch_size = 50\n    fig, ax = plt.subplots(1, 1)\n    with tf.Graph().as_default() as g, tf.Session(graph=g) as sess:\n        X, Y, cost, means = build_single_gaussian_model()\n        optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n        init_op = tf.global_variables_initializer()\n        # Here we tell tensorflow that we want to initialize all\n        # the variables in the graph so we can use them\n        # This will set W and b to their initial random normal value.\n        sess.run(init_op)\n        # We now run a loop over epochs\n        for it_i in range(n_iterations):\n            idxs = np.random.permutation(range(len(xs)))\n            n_batches = len(idxs) // batch_size\n            for batch_i in range(n_batches):\n                idxs_i = idxs[batch_i * batch_size:(batch_i + 1) * batch_size]\n                sess.run(optimizer, feed_dict={X: xs[idxs_i], Y: ys[idxs_i]})\n            this_cost = sess.run([cost], feed_dict={X: xs, Y: ys})\n            print(\'cost:\', this_cost)\n            if (it_i + 1) % 20 == 0:\n                ys_pred = means.eval(feed_dict={X: xs}, session=sess)\n                img = np.clip(ys_pred.reshape(img.shape), 0, 1)\n                plt.imshow(img)\n                plt.show()\n                fig.canvas.show()\n\n\ndef build_multiple_gaussians_model(n_input_features=2,\n                                   n_output_features=3,\n                                   n_gaussians=5,\n                                   n_neurons=[50, 50, 50, 50, 50, 50]):\n\n    X = tf.placeholder(tf.float32, shape=[None, n_input_features], name=\'X\')\n    Y = tf.placeholder(tf.float32, shape=[None, n_output_features], name=\'Y\')\n\n    current_input = X\n    for layer_i in range(len(n_neurons)):\n        current_input = tfl.linear(\n            inputs=current_input,\n            num_outputs=n_neurons[layer_i],\n            activation_fn=tf.nn.tanh,\n            scope=\'layer/\' + str(layer_i))\n\n    means = tf.reshape(\n        tfl.linear(\n            inputs=current_input,\n            num_outputs=n_output_features * n_gaussians,\n            activation_fn=None,\n            scope=\'means\'), [-1, n_output_features, n_gaussians])\n    sigmas = tf.maximum(\n        tf.reshape(\n            tfl.linear(\n                inputs=current_input,\n                num_outputs=n_output_features * n_gaussians,\n                activation_fn=tf.nn.relu,\n                scope=\'sigmas\'), [-1, n_output_features, n_gaussians]), 1e-10)\n    weights = tf.reshape(\n        tfl.linear(\n            inputs=current_input,\n            num_outputs=n_output_features * n_gaussians,\n            activation_fn=tf.nn.softmax,\n            scope=\'weights\'), [-1, n_output_features, n_gaussians])\n\n    Y_3d = tf.reshape(Y, [-1, n_output_features, 1])\n    p = gausspdf(Y_3d, means, sigmas)\n    weighted = weights * p\n    sump = tf.reduce_sum(weighted, axis=2)\n    negloglike = -tf.log(tf.maximum(sump, 1e-10))\n    cost = tf.reduce_mean(tf.reduce_mean(negloglike, 1))\n    return X, Y, cost, means, sigmas, weights\n\n\ndef train_multiple_gaussians_model():\n    img1 = imresize(astronaut(), (64, 64))\n    img2 = imresize(coffee(), (64, 64))\n    xs1, ys1 = get_data(img1)\n    xs2, ys2 = get_data(img2)\n    xs = np.r_[xs1, xs2]\n    ys = np.r_[ys1, ys2]\n    n_iterations = 500\n    batch_size = 100\n    fig, ax = plt.subplots(1, 1)\n    with tf.Graph().as_default() as g, tf.Session(graph=g) as sess:\n        X, Y, cost, means, sigmas, weights = build_multiple_gaussians_model()\n        optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n        init_op = tf.global_variables_initializer()\n        # Here we tell tensorflow that we want to initialize all\n        # the variables in the graph so we can use them\n        # This will set W and b to their initial random normal value.\n        sess.run(init_op)\n        # We now run a loop over epochs\n        for it_i in range(n_iterations):\n            idxs = np.random.permutation(range(len(xs)))\n            n_batches = len(idxs) // batch_size\n            for batch_i in range(n_batches):\n                idxs_i = idxs[batch_i * batch_size:(batch_i + 1) * batch_size]\n                sess.run(optimizer, feed_dict={X: xs[idxs_i], Y: ys[idxs_i]})\n            this_cost = sess.run([cost], feed_dict={X: xs, Y: ys})\n            print(\'cost:\', this_cost)\n            if (it_i + 1) % 20 == 0:\n                y_mu, y_dev, y_pi = sess.run(\n                    [means, sigmas, weights],\n                    feed_dict={X: xs[:np.prod(img1.shape[:2])]})\n                if False:\n                    ys_pred = np.sum(y_mu * y_pi, axis=2)\n                    img = np.clip(ys_pred, 0, 1)\n                    ax.imshow(img.reshape(img1.shape))\n                else:\n                    ys_pred = np.array([\n                        y_mu[obv, :, idx]\n                        for obv, idx in enumerate(np.argmax(y_pi.sum(1), 1))\n                    ])\n                    img = np.clip(ys_pred.reshape(img1.shape), 0, 1)\n                    ax.imshow(img.reshape(img1.shape))\n                plt.show()\n                fig.canvas.draw()\n'"
cadl/nb_utils.py,2,"b'""""""Utility for displaying Tensorflow graphs.\n""""""\n""""""\nFrom\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow\n/examples/tutorials/deepdream/deepdream.ipynb\n\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport tensorflow as tf\nimport numpy as np\nfrom IPython.display import display, HTML\n\n\ndef show_graph(graph_def):\n    """"""Summary\n\n    Parameters\n    ----------\n    graph_def : TYPE\n        Description\n    """"""\n    # Helper functions for TF Graph visualization\n    def _strip_consts(graph_def, max_const_size=32):\n        """"""Strip large constant values from graph_def.\n\n        Parameters\n        ----------\n        graph_def : TYPE\n            Description\n        max_const_size : int, optional\n            Description\n\n        Returns\n        -------\n        TYPE\n            Description\n        """"""\n        strip_def = tf.GraphDef()\n        for n0 in graph_def.node:\n            n = strip_def.node.add()\n            n.MergeFrom(n0)\n            if n.op == \'Const\':\n                tensor = n.attr[\'value\'].tensor\n                size = len(tensor.tensor_content)\n                if size > max_const_size:\n                    tensor.tensor_content = ""<stripped {} bytes>"".format(size).encode()\n        return strip_def\n\n    def _rename_nodes(graph_def, rename_func):\n        """"""Summary\n\n        Parameters\n        ----------\n        graph_def : TYPE\n            Description\n        rename_func : TYPE\n            Description\n\n        Returns\n        -------\n        TYPE\n            Description\n        """"""\n        res_def = tf.GraphDef()\n        for n0 in graph_def.node:\n            n = res_def.node.add()\n            n.MergeFrom(n0)\n            n.name = rename_func(n.name)\n            for i, s in enumerate(n.input):\n                n.input[i] = rename_func(s) if s[0] != \'^\' else \'^\' + rename_func(s[1:])\n        return res_def\n\n    def _show_entire_graph(graph_def, max_const_size=32):\n        """"""Visualize TensorFlow graph.\n\n        Parameters\n        ----------\n        graph_def : TYPE\n            Description\n        max_const_size : int, optional\n            Description\n        """"""\n        if hasattr(graph_def, \'as_graph_def\'):\n            graph_def = graph_def.as_graph_def()\n        strip_def = _strip_consts(graph_def, max_const_size=max_const_size)\n        code = """"""\n            <script>\n              function load() {{\n                document.getElementById(""{id}"").pbtxt = {data};\n              }}\n            </script>\n            <link rel=""import"" href=""https://tensorboard.appspot.com/tf-graph-basic.build.html"" onload=load()>\n            <div style=""height:600px"">\n              <tf-graph-basic id=""{id}""></tf-graph-basic>\n            </div>\n        """""".format(data=repr(str(strip_def)), id=\'graph\' + str(np.random.rand()))\n\n        iframe = """"""\n            <iframe seamless style=""width:800px;height:620px;border:0"" srcdoc=""{}""></iframe>\n        """""".format(code.replace(\'""\', \'&quot;\'))\n        display(HTML(iframe))\n    # Visualizing the network graph. Be sure expand the ""mixed"" nodes to see their\n    # internal structure. We are going to visualize ""Conv2D"" nodes.\n    tmp_def = _rename_nodes(graph_def, lambda s: ""/"".join(s.split(\'_\', 1)))\n    _show_entire_graph(tmp_def)\n'"
cadl/nsynth.py,57,"b'""""""NSynth: WaveNet Autoencoder.\n""""""\n""""""\nNSynth model code and utilities are licensed under APL from the\n\nGoogle Magenta project\n----------------------\nhttps://github.com/tensorflow/magenta/blob/master/magenta/models/nsynth\n\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport tensorflow as tf\nfrom scipy.io import wavfile\nimport numpy as np\nfrom cadl.utils import download_and_extract_tar\nfrom magenta.models.nsynth import utils\nfrom magenta.models.nsynth import reader\nfrom magenta.models.nsynth.wavenet import masked\nimport os\n\n\ndef get_model():\n    """"""Summary\n    """"""\n    download_and_extract_tar(\n        \'http://download.magenta.tensorflow.org/models/nsynth/wavenet-ckpt.tar\')\n\n\ndef causal_linear(x, n_inputs, n_outputs, name, filter_length, rate,\n                  batch_size):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    n_inputs : TYPE\n        Description\n    n_outputs : TYPE\n        Description\n    name : TYPE\n        Description\n    filter_length : TYPE\n        Description\n    rate : TYPE\n        Description\n    batch_size : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    # create queue\n    q_1 = tf.FIFOQueue(rate, dtypes=tf.float32, shapes=(batch_size, n_inputs))\n    q_2 = tf.FIFOQueue(rate, dtypes=tf.float32, shapes=(batch_size, n_inputs))\n    init_1 = q_1.enqueue_many(tf.zeros((rate, batch_size, n_inputs)))\n    init_2 = q_2.enqueue_many(tf.zeros((rate, batch_size, n_inputs)))\n    state_1 = q_1.dequeue()\n    push_1 = q_1.enqueue(x)\n    state_2 = q_2.dequeue()\n    push_2 = q_2.enqueue(state_1)\n\n    # get pretrained weights\n    W = tf.get_variable(\n        name=name + \'/W\',\n        shape=[1, filter_length, n_inputs, n_outputs],\n        dtype=tf.float32)\n    b = tf.get_variable(\n        name=name + \'/biases\', shape=[n_outputs], dtype=tf.float32)\n    W_q_2 = tf.slice(W, [0, 0, 0, 0], [-1, 1, -1, -1])\n    W_q_1 = tf.slice(W, [0, 1, 0, 0], [-1, 1, -1, -1])\n    W_x = tf.slice(W, [0, 2, 0, 0], [-1, 1, -1, -1])\n\n    # perform op w/ cached states\n    y = tf.expand_dims(\n        tf.nn.bias_add(\n            tf.matmul(state_2, W_q_2[0][0]) + tf.matmul(state_1, W_q_1[0][0]) +\n            tf.matmul(x, W_x[0][0]), b), 0)\n    return y, (init_1, init_2), (push_1, push_2)\n\n\ndef linear(x, n_inputs, n_outputs, name):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    n_inputs : TYPE\n        Description\n    n_outputs : TYPE\n        Description\n    name : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    W = tf.get_variable(\n        name=name + \'/W\', shape=[1, 1, n_inputs, n_outputs], dtype=tf.float32)\n    b = tf.get_variable(\n        name=name + \'/biases\', shape=[n_outputs], dtype=tf.float32)\n    return tf.expand_dims(tf.nn.bias_add(tf.matmul(x[0], W[0][0]), b), 0)\n\n\nclass FastGenerationConfig(object):\n    """"""Configuration object that helps manage the graph.""""""\n\n    def __init__(self, batch_size=1):\n        """""".""""""\n        self.batch_size = batch_size\n\n    def build(self, inputs):\n        """"""Build the graph for this configuration.\n        Args:\n            inputs: A dict of inputs. For training, should contain \'wav\'.\n        Returns:\n            A dict of outputs that includes the \'predictions\',\n            \'init_ops\', the \'push_ops\', and the \'quantized_input\'.\n        """"""\n        num_stages = 10\n        num_layers = 30\n        filter_length = 3\n        width = 512\n        skip_width = 256\n        num_z = 16\n\n        # Encode the source with 8-bit Mu-Law.\n        x = inputs[\'wav\']\n        batch_size = self.batch_size\n        x_quantized = utils.mu_law(x)\n        x_scaled = tf.cast(x_quantized, tf.float32) / 128.0\n        x_scaled = tf.expand_dims(x_scaled, 2)\n\n        encoding = tf.placeholder(\n                name=\'encoding\', shape=[batch_size, num_z], dtype=tf.float32)\n        en = tf.expand_dims(encoding, 1)\n\n        init_ops, push_ops = [], []\n\n        ###\n        # The WaveNet Decoder.\n        ###\n        l = x_scaled\n        l, inits, pushs = utils.causal_linear(\n                x=l,\n                n_inputs=1,\n                n_outputs=width,\n                name=\'startconv\',\n                rate=1,\n                batch_size=batch_size,\n                filter_length=filter_length)\n\n        for init in inits:\n            init_ops.append(init)\n        for push in pushs:\n            push_ops.append(push)\n\n        # Set up skip connections.\n        s = utils.linear(l, width, skip_width, name=\'skip_start\')\n\n        # Residual blocks with skip connections.\n        for i in range(num_layers):\n            dilation = 2**(i % num_stages)\n\n            # dilated masked cnn\n            d, inits, pushs = utils.causal_linear(\n                    x=l,\n                    n_inputs=width,\n                    n_outputs=width * 2,\n                    name=\'dilatedconv_%d\' % (i + 1),\n                    rate=dilation,\n                    batch_size=batch_size,\n                    filter_length=filter_length)\n\n            for init in inits:\n                init_ops.append(init)\n            for push in pushs:\n                push_ops.append(push)\n\n            # local conditioning\n            d += utils.linear(en, num_z, width * 2, name=\'cond_map_%d\' % (i + 1))\n\n            # gated cnn\n            assert d.get_shape().as_list()[2] % 2 == 0\n            m = d.get_shape().as_list()[2] // 2\n            d = tf.sigmoid(d[:, :, :m]) * tf.tanh(d[:, :, m:])\n\n            # residuals\n            l += utils.linear(d, width, width, name=\'res_%d\' % (i + 1))\n\n            # skips\n            s += utils.linear(d, width, skip_width, name=\'skip_%d\' % (i + 1))\n\n        s = tf.nn.relu(s)\n        s = (utils.linear(s, skip_width, skip_width, name=\'out1\') + utils.linear(\n                en, num_z, skip_width, name=\'cond_map_out1\'))\n        s = tf.nn.relu(s)\n\n        ###\n        # Compute the logits and get the loss.\n        ###\n        logits = utils.linear(s, skip_width, 256, name=\'logits\')\n        logits = tf.reshape(logits, [-1, 256])\n        probs = tf.nn.softmax(logits, name=\'softmax\')\n\n        return {\n                \'init_ops\': init_ops,\n                \'push_ops\': push_ops,\n                \'predictions\': probs,\n                \'encoding\': encoding,\n                \'quantized_input\': x_quantized,\n        }\n\n\nclass Config(object):\n    """"""Configuration object that helps manage the graph.""""""\n\n    def __init__(self, train_path=None):\n        self.num_iters = 200000\n        self.learning_rate_schedule = {\n                0: 2e-4,\n                90000: 4e-4 / 3,\n                120000: 6e-5,\n                150000: 4e-5,\n                180000: 2e-5,\n                210000: 6e-6,\n                240000: 2e-6,\n        }\n        self.ae_hop_length = 512\n        self.ae_bottleneck_width = 16\n        self.train_path = train_path\n\n    def get_batch(self, batch_size):\n        assert self.train_path is not None\n        data_train = reader.NSynthDataset(self.train_path, is_training=True)\n        return data_train.get_wavenet_batch(batch_size, length=6144)\n\n    @staticmethod\n    def _condition(x, encoding):\n        """"""Condition the input on the encoding.\n        Args:\n            x: The [mb, length, channels] float tensor input.\n            encoding: The [mb, encoding_length, channels] float tensor encoding.\n        Returns:\n            The output after broadcasting the encoding to x\'s shape and adding them.\n        """"""\n        mb, length, channels = x.get_shape().as_list()\n        enc_mb, enc_length, enc_channels = encoding.get_shape().as_list()\n        assert enc_mb == mb\n        assert enc_channels == channels\n\n        encoding = tf.reshape(encoding, [mb, enc_length, 1, channels])\n        x = tf.reshape(x, [mb, enc_length, -1, channels])\n        x += encoding\n        x = tf.reshape(x, [mb, length, channels])\n        x.set_shape([mb, length, channels])\n        return x\n\n    def build(self, inputs, is_training):\n        """"""Build the graph for this configuration.\n        Args:\n            inputs: A dict of inputs. For training, should contain \'wav\'.\n            is_training: Whether we are training or not. Not used in this config.\n        Returns:\n            A dict of outputs that includes the \'predictions\', \'loss\', the \'encoding\',\n            the \'quantized_input\', and whatever metrics we want to track for eval.\n        """"""\n        del is_training\n        num_stages = 10\n        num_layers = 30\n        filter_length = 3\n        width = 512\n        skip_width = 256\n        ae_num_stages = 10\n        ae_num_layers = 30\n        ae_filter_length = 3\n        ae_width = 128\n\n        # Encode the source with 8-bit Mu-Law.\n        x = inputs[\'wav\']\n        x_quantized = utils.mu_law(x)\n        x_scaled = tf.cast(x_quantized, tf.float32) / 128.0\n        x_scaled = tf.expand_dims(x_scaled, 2)\n\n        ###\n        # The Non-Causal Temporal Encoder.\n        ###\n        en = masked.conv1d(\n                x_scaled,\n                causal=False,\n                num_filters=ae_width,\n                filter_length=ae_filter_length,\n                name=\'ae_startconv\')\n\n        for num_layer in range(ae_num_layers):\n            dilation = 2**(num_layer % ae_num_stages)\n            d = tf.nn.relu(en)\n            d = masked.conv1d(\n                    d,\n                    causal=False,\n                    num_filters=ae_width,\n                    filter_length=ae_filter_length,\n                    dilation=dilation,\n                    name=\'ae_dilatedconv_%d\' % (num_layer + 1))\n            d = tf.nn.relu(d)\n            en += masked.conv1d(\n                    d,\n                    num_filters=ae_width,\n                    filter_length=1,\n                    name=\'ae_res_%d\' % (num_layer + 1))\n\n        en = masked.conv1d(\n                en,\n                num_filters=self.ae_bottleneck_width,\n                filter_length=1,\n                name=\'ae_bottleneck\')\n        en = masked.pool1d(en, self.ae_hop_length, name=\'ae_pool\', mode=\'avg\')\n        encoding = en\n\n        ###\n        # The WaveNet Decoder.\n        ###\n        l = masked.shift_right(x_scaled)\n        l = masked.conv1d(\n                l, num_filters=width, filter_length=filter_length, name=\'startconv\')\n\n        # Set up skip connections.\n        s = masked.conv1d(\n                l, num_filters=skip_width, filter_length=1, name=\'skip_start\')\n\n        # Residual blocks with skip connections.\n        for i in range(num_layers):\n            dilation = 2**(i % num_stages)\n            d = masked.conv1d(\n                    l,\n                    num_filters=2 * width,\n                    filter_length=filter_length,\n                    dilation=dilation,\n                    name=\'dilatedconv_%d\' % (i + 1))\n            d = self._condition(d,\n                                                    masked.conv1d(\n                                                            en,\n                                                            num_filters=2 * width,\n                                                            filter_length=1,\n                                                            name=\'cond_map_%d\' % (i + 1)))\n\n            assert d.get_shape().as_list()[2] % 2 == 0\n            m = d.get_shape().as_list()[2] // 2\n            d_sigmoid = tf.sigmoid(d[:, :, :m])\n            d_tanh = tf.tanh(d[:, :, m:])\n            d = d_sigmoid * d_tanh\n\n            l += masked.conv1d(\n                    d, num_filters=width, filter_length=1, name=\'res_%d\' % (i + 1))\n            s += masked.conv1d(\n                    d, num_filters=skip_width, filter_length=1, name=\'skip_%d\' % (i + 1))\n\n        s = tf.nn.relu(s)\n        s = masked.conv1d(s, num_filters=skip_width, filter_length=1, name=\'out1\')\n        s = self._condition(s,\n                                                masked.conv1d(\n                                                        en,\n                                                        num_filters=skip_width,\n                                                        filter_length=1,\n                                                        name=\'cond_map_out1\'))\n        s = tf.nn.relu(s)\n\n        ###\n        # Compute the logits and get the loss.\n        ###\n        logits = masked.conv1d(s, num_filters=256, filter_length=1, name=\'logits\')\n        logits = tf.reshape(logits, [-1, 256])\n        probs = tf.nn.softmax(logits, name=\'softmax\')\n        x_indices = tf.cast(tf.reshape(x_quantized, [-1]), tf.int32) + 128\n        loss = tf.reduce_mean(\n                tf.nn.sparse_softmax_cross_entropy_with_logits(\n                        logits=logits, labels=x_indices, name=\'nll\'),\n                0,\n                name=\'loss\')\n\n        return {\n                \'predictions\': probs,\n                \'loss\': loss,\n                \'eval\': {\n                        \'nll\': loss\n                },\n                \'quantized_input\': x_quantized,\n                \'encoding\': encoding,\n        }\n\n\ndef inv_mu_law(x, mu=255.0):\n    """"""A TF implementation of inverse Mu-Law.\n\n    Parameters\n    ----------\n    x\n        The Mu-Law samples to decode.\n    mu\n        The Mu we used to encode these samples.\n\n    Returns\n    -------\n    out\n        The decoded data.\n    """"""\n    x = np.array(x).astype(np.float32)\n    out = (x + 0.5) * 2. / (mu + 1)\n    out = np.sign(out) / mu * ((1 + mu)**np.abs(out) - 1)\n    out = np.where(np.equal(x, 0), x, out)\n    return out\n\n\ndef load_audio(wav_file, sample_length=64000):\n    """"""Summary\n\n    Parameters\n    ----------\n    wav_file : TYPE\n        Description\n    sample_length : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    wav_data = np.array([utils.load_audio(wav_file)[:sample_length]])\n    wav_data_padded = np.zeros((1, sample_length))\n    wav_data_padded[0, :wav_data.shape[1]] = wav_data\n    wav_data = wav_data_padded\n    return wav_data\n\n\ndef load_nsynth(batch_size=1, sample_length=64000):\n    """"""Summary\n\n    Parameters\n    ----------\n    encoding : bool, optional\n        Description\n    batch_size : int, optional\n        Description\n    sample_length : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    config = Config()\n    with tf.device(\'/gpu:0\'):\n        X = tf.placeholder(tf.float32, shape=[batch_size, sample_length])\n        graph = config.build({""wav"": X}, is_training=False)\n        graph.update({\'X\': X})\n    return graph\n\n\ndef load_fastgen_nsynth(batch_size=1, sample_length=64000):\n    """"""Summary\n\n    Parameters\n    ----------\n    batch_size : int, optional\n        Description\n    sample_length : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    config = FastGenerationConfig(batch_size)\n    X = tf.placeholder(tf.float32, shape=[batch_size, 1])\n    graph = config.build({""wav"": X})\n    graph.update({\'X\': X})\n    return graph\n\n\ndef sample_categorical(pmf):\n    """"""Sample from a categorical distribution.\n    Args:\n        pmf: Probablity mass function. Output of a softmax over categories.\n            Array of shape [batch_size, number of categories]. Rows sum to 1.\n    Returns:\n        idxs: Array of size [batch_size, 1]. Integer of category sampled.\n    """"""\n    if pmf.ndim == 1:\n        pmf = np.expand_dims(pmf, 0)\n    batch_size = pmf.shape[0]\n    cdf = np.cumsum(pmf, axis=1)\n    rand_vals = np.random.rand(batch_size)\n    idxs = np.zeros([batch_size, 1])\n    for i in range(batch_size):\n        idxs[i] = cdf[i].searchsorted(rand_vals[i])\n    return idxs\n\n\ndef load_batch(files, sample_length=64000):\n    """"""Load a batch of data from either .wav or .npy files.\n    Args:\n        files: A list of filepaths to .wav or .npy files\n        sample_length: Maximum sample length\n    Returns:\n        batch_data: A padded array of audio or embeddings [batch, length, (dims)]\n    """"""\n    batch_data = []\n    max_length = 0\n    is_npy = (os.path.splitext(files[0])[1] == "".npy"")\n    # Load the data\n    for f in files:\n        if is_npy:\n            data = np.load(f)\n            batch_data.append(data)\n        else:\n            data = utils.load_audio(f, sample_length, sr=16000)\n            batch_data.append(data)\n        if data.shape[0] > max_length:\n            max_length = data.shape[0]\n    # Add padding\n    for i, data in enumerate(batch_data):\n        if data.shape[0] < max_length:\n            if is_npy:\n                padded = np.zeros([max_length, +data.shape[1]])\n                padded[:data.shape[0], :] = data\n            else:\n                padded = np.zeros([max_length])\n                padded[:data.shape[0]] = data\n            batch_data[i] = padded\n    # Return arrays\n    batch_data = np.array(batch_data)\n    return batch_data\n\n\ndef save_batch(batch_audio, batch_save_paths):\n    for audio, name in zip(batch_audio, batch_save_paths):\n        tf.logging.info(""Saving: %s"" % name)\n        wavfile.write(name, 16000, audio)\n\n\ndef encode(wav_data, checkpoint_path, sample_length=64000):\n    """"""Generate an array of embeddings from an array of audio.\n    Args:\n        wav_data: Numpy array [batch_size, sample_length]\n        checkpoint_path: Location of the pretrained model.\n        sample_length: The total length of the final wave file, padded with 0s.\n    Returns:\n        encoding: a [mb, 125, 16] encoding (for 64000 sample audio file).\n    """"""\n    if wav_data.ndim == 1:\n        wav_data = np.expand_dims(wav_data, 0)\n        batch_size = 1\n    elif wav_data.ndim == 2:\n        batch_size = wav_data.shape[0]\n\n    # Load up the model for encoding and find the encoding of ""wav_data""\n    session_config = tf.ConfigProto(allow_soft_placement=True)\n    with tf.Graph().as_default(), tf.Session(config=session_config) as sess:\n        hop_length = Config().ae_hop_length\n        wav_data, sample_length = utils.trim_for_encoding(\n            wav_data, sample_length, hop_length)\n        net = load_nsynth(batch_size=batch_size, sample_length=sample_length)\n        saver = tf.train.Saver()\n        saver.restore(sess, checkpoint_path)\n        encodings = sess.run(net[""encoding""], feed_dict={net[""X""]: wav_data})\n    return encodings\n\n\ndef synthesize(encodings,\n               save_paths,\n               hop_length=None,\n               checkpoint_path=""model.ckpt-200000"",\n               samples_per_save=1000):\n    """"""Synthesize audio from an array of embeddings.\n    Args:\n        encodings: Numpy array with shape [batch_size, time, dim].\n        save_paths: Iterable of output file names.\n        checkpoint_path: Location of the pretrained model. [model.ckpt-200000]\n        samples_per_save: Save files after every amount of generated samples.\n    """"""\n    if hop_length is None:\n        hop_length = Config().ae_hop_length\n    # Get lengths\n    batch_size = encodings.shape[0]\n    encoding_length = encodings.shape[1]\n    total_length = encoding_length * hop_length\n\n    session_config = tf.ConfigProto(allow_soft_placement=True)\n    with tf.Graph().as_default(), tf.Session(config=session_config) as sess:\n        net = load_fastgen_nsynth(batch_size=batch_size)\n        saver = tf.train.Saver()\n        saver.restore(sess, checkpoint_path)\n\n        # initialize queues w/ 0s\n        sess.run(net[""init_ops""])\n\n        # Regenerate the audio file sample by sample\n        audio_batch = np.zeros((batch_size, total_length,), dtype=np.float32)\n        audio = np.zeros([batch_size, 1])\n\n        for sample_i in range(total_length):\n            enc_i = sample_i // hop_length\n            pmf = sess.run(\n                [net[""predictions""], net[""push_ops""]],\n                feed_dict={\n                    net[""X""]: audio,\n                    net[""encoding""]: encodings[:, enc_i, :]\n                })[0]\n            sample_bin = sample_categorical(pmf)\n            audio = utils.inv_mu_law_numpy(sample_bin - 128)\n            audio_batch[:, sample_i] = audio[:, 0]\n            if sample_i % 100 == 0:\n                tf.logging.info(""Sample: %d"" % sample_i)\n            if sample_i % samples_per_save == 0:\n                save_batch(audio_batch, save_paths)\n    save_batch(audio_batch, save_paths)\n'"
cadl/pixelcnn.py,89,"b'""""""Conditional Gated Pixel CNN.\n""""""\n""""""\nThanks to many reference implementations\n----------------------------------------\nhttps://github.com/anantzoid/Conditional-PixelCNN-decoder\nhttps://github.com/openai/pixel-cnn\nhttps://github.com/PrajitR/fast-pixel-cnn\n\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport os\nfrom cadl import dataset_utils as dsu\n\n\ndef gated_conv2d(X,\n                 K_h,\n                 K_w,\n                 K_c,\n                 strides=[1, 1, 1, 1],\n                 padding=\'SAME\',\n                 mask=None,\n                 cond_h=None,\n                 vertical_h=None):\n    """"""Summary\n\n    Parameters\n    ----------\n    X : TYPE\n        Description\n    K_h : TYPE\n        Description\n    K_w : TYPE\n        Description\n    K_c : TYPE\n        Description\n    strides : list, optional\n        Description\n    padding : str, optional\n        Description\n    mask : None, optional\n        Description\n    cond_h : None, optional\n        Description\n    vertical_h : None, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    with tf.variable_scope(\'masked_cnn\'):\n        W = tf.get_variable(\n            name=\'W\',\n            shape=[K_h, K_w, X.shape[-1].value, K_c * 2],\n            initializer=tf.contrib.layers.xavier_initializer_conv2d())\n        b = tf.get_variable(\n            name=\'b\', shape=[K_c * 2], initializer=tf.zeros_initializer())\n        if mask is not None:\n            W = tf.multiply(mask, W)\n        # Initial convolution with masked kernel\n        h = tf.nn.bias_add(\n            tf.nn.conv2d(X, W, strides=strides, padding=padding), b)\n\n    # Combine the horizontal stack\'s pre-activations to our hidden embedding before\n    # applying the split nonlinearities.  Check Figure 2 for details.\n    if vertical_h is not None:\n        with tf.variable_scope(\'vtoh\'):\n            W_vtoh = tf.get_variable(\n                name=\'W\',\n                shape=[1, 1, K_c * 2, K_c * 2],\n                initializer=tf.contrib.layers.xavier_initializer_conv2d())\n            b_vtob = tf.get_variable(\n                name=\'b\', shape=[K_c * 2], initializer=tf.zeros_initializer())\n            h = tf.add(h,\n                       tf.nn.bias_add(\n                           tf.nn.conv2d(\n                               vertical_h,\n                               W_vtoh,\n                               strides=strides,\n                               padding=padding), b_vtob))\n\n    # Condition on some given data\n    if cond_h is not None:\n        with tf.variable_scope(\'conditioning\'):\n            V = tf.get_variable(\n                name=\'V\',\n                shape=[cond_h.shape[1].value, K_c],\n                initializer=tf.contrib.layers.xavier_initializer_conv2d())\n            b = tf.get_variable(\n                name=\'b\', shape=[K_c], initializer=tf.zeros_initializer())\n            h = tf.add(\n                h,\n                tf.reshape(\n                    tf.nn.bias_add(tf.matmul(cond_h, V), b),\n                    tf.shape(X)[0:3] + [K_c]),\n                name=\'h\')\n\n    with tf.variable_scope(\'gated_cnn\'):\n        # Finally slice and apply gated multiplier\n        h_f = tf.slice(h, [0, 0, 0, 0], [-1, -1, -1, K_c])\n        h_g = tf.slice(h, [0, 0, 0, K_c], [-1, -1, -1, K_c])\n        y = tf.multiply(tf.nn.tanh(h_f), tf.sigmoid(h_g))\n\n    return y, h\n\n\ndef build_conditional_pixel_cnn_model(B=None,\n                                      H=32,\n                                      W=32,\n                                      C=3,\n                                      n_conditionals=None):\n    """"""Conditional Gated Pixel CNN Model.\n\n    From the paper\n    --------------\n        van den Oord, A., Kalchbrenner, N., Vinyals, O.,\n        Espeholt, L., Graves, A., & Kavukcuoglu, K. (2016).\n        Conditional Image Generation with PixelCNN Decoders.\n\n    Implements most of the paper, except for the autoencoder,\n    triplet loss of face embeddings, and pad/crop/shift ops for\n    convolution (as it is not as clear IMO from a pedagogical\n    point of view).\n\n    Parameters\n    ----------\n    B : None, optional\n        Description\n    H : int, optional\n        Description\n    W : int, optional\n        Description\n    C : int, optional\n        Description\n    n_conditionals : None, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    n_conditionals = None\n    X = tf.placeholder(name=\'X\', dtype=tf.uint8, shape=[None, H, W, C])\n\n    X_ = (tf.cast(X, tf.float32) - 127.5) / 2.0\n\n    n_layers = 10\n    D = 256\n    fmaps = 64\n\n    K_hs = [7] + [3] * (n_layers - 1)\n    K_ws = [7 * C] + [3 * C] * (n_layers - 1)\n    K_cs = [fmaps] * n_layers\n\n    if n_conditionals is not None:\n        cond_h = tf.placeholder(\n            name=\'cond_h\', dtype=tf.float32, shape=[None, n_conditionals])\n    else:\n        cond_h = None\n\n    vertical_X = X_\n    horizontal_X = X_\n    for K_h, K_w, K_c, layer_i in zip(K_hs, K_ws, K_cs, range(n_layers)):\n\n        # Create two masks: one for the first layer (a) and another for all\n        # other layers (b). Really dumb names but am just following the paper.\n        # See Figure 2 of Pixel Recurrent Neural Networks for more info.\n        if layer_i == 0:\n            mask = np.ones((K_h, K_w, 1, 1), dtype=np.float32)\n            mask[(K_h // 2 + 1):, :, :, :] = 0.0\n            mask[K_h // 2, K_w // 2:, :, :] = 0.0\n        else:\n            mask = np.ones((K_h, K_w, 1, 1), dtype=np.float32)\n            mask[(K_h // 2 + 1):, :, :, :] = 0.0\n            mask[K_h // 2, (K_w // 2 + 1):, :, :] = 0.0\n\n        with tf.variable_scope(\'layer_{}\'.format(layer_i)):\n            # Vertical layer\n            with tf.variable_scope(\'vertical\'):\n                vertical_Y, vertical_h = gated_conv2d(\n                    vertical_X, K_h, K_w, K_c, mask=mask, cond_h=cond_h)\n\n            # Horizontal layer\n            with tf.variable_scope(\'horizontal\'):\n\n                # Gated convolution adding in vertical stack information\n                horizontal_Y, horizontal_h = gated_conv2d(\n                    horizontal_X,\n                    1,\n                    K_w,\n                    K_c,\n                    mask=mask[K_h // 2, :, :, :],\n                    vertical_h=vertical_h,\n                    cond_h=cond_h)\n\n                # 1x1 to reduce channels\n                with tf.variable_scope(\'1x1\'):\n                    W_1x1 = tf.get_variable(\n                        name=\'W\',\n                        shape=[1, 1, K_c, D],\n                        initializer=tf.contrib.layers.xavier_initializer_conv2d())\n                    b_1x1 = tf.get_variable(\n                        name=\'b\', shape=[D], initializer=tf.ones_initializer())\n                    horizontal_Y = tf.nn.bias_add(\n                        tf.nn.conv2d(\n                            horizontal_Y,\n                            W_1x1,\n                            strides=[1, 1, 1, 1],\n                            padding=\'SAME\'), b_1x1)\n\n                # Add Residual\n                if layer_i > 0:\n                    with tf.variable_scope(\'residual\'):\n                        horizontal_Y = tf.add(horizontal_X, horizontal_Y)\n\n            vertical_X = vertical_Y\n            horizontal_X = horizontal_Y\n\n    # ReLu followed by 1x1 conv for 2 layers:\n    # 1x1 to reduce channels\n    Y = horizontal_X\n    with tf.variable_scope(\'output/1x1_1\'):\n        W_1x1 = tf.get_variable(\n            name=\'W\',\n            shape=[1, 1, D, D],\n            initializer=tf.contrib.layers.xavier_initializer_conv2d())\n        b_1x1 = tf.get_variable(\n            name=\'b\', shape=[D], initializer=tf.ones_initializer())\n        Y = tf.nn.relu(\n            tf.nn.bias_add(\n                tf.nn.conv2d(Y, W_1x1, strides=[1, 1, 1, 1], padding=\'SAME\'),\n                b_1x1))\n\n    with tf.variable_scope(\'output/1x1_2\'):\n        W_1x1 = tf.get_variable(\n            name=\'W\',\n            shape=[1, 1, D, D * C],\n            initializer=tf.contrib.layers.xavier_initializer_conv2d())\n        b_1x1 = tf.get_variable(\n            name=\'b\', shape=[D * C], initializer=tf.ones_initializer())\n        Y = tf.nn.bias_add(\n            tf.nn.conv2d(Y, W_1x1, strides=[1, 1, 1, 1], padding=\'SAME\'), b_1x1)\n        Y = tf.reshape(Y, [-1, D])\n\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=Y, labels=tf.cast(tf.reshape(X, [-1]), tf.int32))\n    cost = tf.reduce_mean(loss)\n    preds = tf.nn.softmax(Y)\n    sampled_preds = tf.multinomial(Y, num_samples=1)\n\n    tf.summary.image(\'actual\', X)\n    tf.summary.image(\'preds\',\n                     tf.reshape(\n                         tf.cast(tf.argmax(Y, axis=1), tf.uint8), (-1, H, W,\n                                                                   C)))\n    tf.summary.histogram(\'loss\', loss)\n    tf.summary.scalar(\'cost\', cost)\n    summaries = tf.summary.merge_all()\n\n    return {\n        \'cost\': cost,\n        \'X\': X,\n        \'preds\': preds,\n        \'sampled_preds\': sampled_preds,\n        \'summaries\': summaries\n    }\n\n\ndef train_tiny_imagenet(ckpt_path=\'pixelcnn\',\n                        n_epochs=1000,\n                        save_step=100,\n                        write_step=25,\n                        B=32,\n                        H=64,\n                        W=64,\n                        C=3):\n    """"""Summary\n\n    Parameters\n    ----------\n    ckpt_path : str, optional\n        Description\n    n_epochs : int, optional\n        Description\n    save_step : int, optional\n        Description\n    write_step : int, optional\n        Description\n    B : int, optional\n        Description\n    H : int, optional\n        Description\n    W : int, optional\n        Description\n    C : int, optional\n        Description\n    """"""\n    ckpt_name = os.path.join(ckpt_path, \'pixelcnn.ckpt\')\n\n    with tf.Graph().as_default(), tf.Session() as sess:\n        # Not actually conditioning on anything here just using the gated cnn model\n        net = build_conditional_pixel_cnn_model(B=B, H=H, W=W, C=C)\n\n        # build the optimizer (this will take a while!)\n        optimizer = tf.train.AdamOptimizer(\n            learning_rate=0.001).minimize(net[\'cost\'])\n\n        # Load a list of files for tiny imagenet, downloading if necessary\n        imagenet_files = dsu.tiny_imagenet_load()\n\n        # Create a threaded image pipeline which will load/shuffle/crop/resize\n        batch = dsu.create_input_pipeline(\n            imagenet_files[0],\n            batch_size=B,\n            n_epochs=n_epochs,\n            shape=[64, 64, 3],\n            crop_shape=[H, W, C],\n            crop_factor=1.0,\n            n_threads=8)\n\n        saver = tf.train.Saver()\n        writer = tf.summary.FileWriter(ckpt_path)\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        sess.run(init_op)\n\n        # This will handle our threaded image pipeline\n        coord = tf.train.Coordinator()\n\n        # Ensure no more changes to graph\n        tf.get_default_graph().finalize()\n\n        # Start up the queues for handling the image pipeline\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n        if os.path.exists(ckpt_name + \'.index\') or os.path.exists(ckpt_name):\n            saver.restore(sess, ckpt_name)\n            saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\n\n        epoch_i = 0\n        batch_i = 0\n        try:\n            while not coord.should_stop() and epoch_i < n_epochs:\n                batch_i += 1\n                batch_xs = sess.run(batch)\n                train_cost = sess.run(\n                    [net[\'cost\'], optimizer], feed_dict={net[\'X\']: batch_xs})[0]\n\n                print(batch_i, train_cost)\n                if batch_i % write_step == 0:\n                    summary = sess.run(\n                        net[\'summaries\'], feed_dict={net[\'X\']: batch_xs})\n                    writer.add_summary(summary, batch_i)\n\n                if batch_i % save_step == 0:\n                    # Save the variables to disk.  Don\'t write the meta graph\n                    # since we can use the code to create it, and it takes a long\n                    # time to create the graph since it is so deep\n                    saver.save(\n                        sess,\n                        ckpt_name,\n                        global_step=batch_i,\n                        write_meta_graph=True)\n        except tf.errors.OutOfRangeError:\n            print(\'Done.\')\n        finally:\n            # One of the threads has issued an exception.  So let\'s tell all the\n            # threads to shutdown.\n            coord.request_stop()\n\n        # Wait until all threads have finished.\n        coord.join(threads)\n\n\ndef generate():\n    """"""Summary\n    """"""\n    # Parameters for generation\n    ckpt_path = \'pixelcnn\'\n    B = None\n    H = 64\n    W = 64\n    C = 3\n\n    with tf.Graph().as_default(), tf.Session() as sess:\n        # Not actually conditioning on anything here just using the gated cnn model\n        net = build_conditional_pixel_cnn_model(B=B, H=H, W=W, C=C)\n\n        # Load a list of files for tiny imagenet, downloading if necessary\n        imagenet_files = dsu.tiny_imagenet_load()\n\n        saver = tf.train.Saver()\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        sess.run(init_op)\n        saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\n\n        import matplotlib.pyplot as plt\n        img = plt.imread(imagenet_files[0][1000])\n        from scipy.misc import imresize\n        og_img = imresize(img, (H, W))\n        img = og_img.copy()\n        # Zero out bottom half of image and let\'s try to synthesize it\n        img[H // 2:, :, :] = 0\n\n        for h_i in range(H // 2, H):\n            for w_i in range(W):\n                for c_i in range(C):\n                    print(h_i, w_i, c_i, end=\'\\r\')\n                    X = img.copy()\n                    preds = sess.run(\n                        net[\'sampled_preds\'],\n                        feed_dict={net[\'X\']: X[np.newaxis]})\n                    X = preds.reshape((1, H, W, C)).astype(np.uint8)\n                    img[h_i, w_i, c_i] = X[0, h_i, w_i, c_i]\n\n        fig, axs = plt.subplots(1, 2)\n        axs[0].imshow(og_img)\n        axs[1].imshow(img)\n\n\nif __name__ == \'__main__\':\n    train_tiny_imagenet()\n'"
cadl/pixelrnn.py,28,"b'""""""Basic PixelRNN i.e. CharRNN style, none of the fancy ones (i.e. Row, Diag, BiDiag).\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nAttributes\n----------\nB : int\n    Description\nC : int\n    Description\nckpt_name : str\n    Description\nH : int\n    Description\nn_epochs : int\n    Description\nn_units : int\n    Description\nW : int\n    Description\n""""""\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom cadl import dataset_utils as dsu\n\n# Parameters for training\nckpt_name = \'pixelrnn.ckpt\'\nn_epochs = 10\nn_units = 100\nB = 50\nH = 32\nW = 32\nC = 3\n\n\ndef build_pixel_rnn_basic_model(B=50, H=32, W=32, C=32, n_units=100,\n                                n_layers=2):\n    """"""Summary\n\n    Parameters\n    ----------\n    B : int, optional\n        Description\n    H : int, optional\n        Description\n    W : int, optional\n        Description\n    C : int, optional\n        Description\n    n_units : int, optional\n        Description\n    n_layers : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    # Input to the network, a batch of images\n    X = tf.placeholder(tf.float32, shape=[B, H, W, C], name=\'X\')\n    keep_prob = tf.placeholder(tf.float32, shape=1, name=\'keep_prob\')\n\n    # Flatten to 2 dimensions\n    X_2d = tf.reshape(X, [-1, H * W * C])\n\n    # Turn each pixel value into a vector of one-hot values\n    X_onehot = tf.one_hot(tf.cast(X_2d, tf.uint8), depth=256, axis=2)\n\n    # Split each pixel into its own tensor resulting in H * W * C number of\n    # Tensors each shaped as B x 256\n    pixels = [\n        tf.squeeze(p, axis=1) for p in tf.split(X_onehot, H * W * C, axis=1)\n    ]\n\n    # Create a GRU recurrent layer\n    cells = tf.contrib.rnn.GRUCell(n_units)\n    initial_state = cells.zero_state(\n        batch_size=tf.shape(X)[0], dtype=tf.float32)\n    if n_layers > 1:\n        cells = tf.contrib.rnn.MultiRNNCell(\n            [cells] * n_layers, state_is_tuple=True)\n        initial_state = cells.zero_state(tf.shape(X)[0], tf.float32)\n    cells = tf.contrib.rnn.DropoutWrapper(cells, output_keep_prob=keep_prob)\n\n    # Connect our pixel distributions (onehots) to an rnn, this will return us a\n    # list of tensors, one for each of our pixels.\n    hs, final_state = tf.contrib.rnn.static_rnn(\n        cells, pixels, initial_state=initial_state)\n\n    # Concat N pixels result back into a Tensor, B x N x n_units\n    stacked = tf.concat([tf.expand_dims(h_i, axis=1) for h_i in hs], axis=1)\n\n    # And now to 2d so we can connect to FC layer\n    stacked = tf.reshape(stacked, [-1, n_units])\n\n    # And now connect to FC layer\n    prediction = slim.linear(stacked, 256, scope=\'linear\')\n    if B * H * W * C > 1:\n        prediction = tf.slice(prediction, [0, 0],\n                              [int(prediction.shape[0] - 1), -1])\n        X_onehot_flat = tf.slice(\n            tf.reshape(X_onehot, [-1, 256]), [1, 0], [-1, -1])\n        loss = tf.nn.softmax_cross_entropy_with_logits(\n            labels=X_onehot_flat, logits=prediction)\n\n        cost = tf.reduce_mean(loss)\n    else:\n        cost = None\n\n    return {\n        \'X\': X,\n        \'recon\': prediction,\n        \'cost\': cost,\n        \'initial_state\': initial_state,\n        \'final_state\': final_state\n    }\n\n\ndef infer(sess, net, H, W, C, pixel_value=128, state=None):\n    """"""Summary\n\n    Parameters\n    ----------\n    sess : TYPE\n        Description\n    net : TYPE\n        Description\n    H : TYPE\n        Description\n    W : TYPE\n        Description\n    C : TYPE\n        Description\n    pixel_value : int, optional\n        Description\n    state : None, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    X = np.reshape(pixel_value, [1, 1, 1, 1])\n    synthesis = [pixel_value]\n    if state is None:\n        state = sess.run(net[\'initial_state\'])\n    for pixel_i in range(H * W * C - 1):\n        next, state = sess.run(\n            [net[\'recon\'], net[\'final_state\']],\n            feed_dict={net[\'X\']: X,\n                       net[\'initial_state\']: state})\n        synthesis.append(np.argmax(next))\n    return synthesis\n\n\ndef train_tiny_imagenet():\n    """"""Summary\n    """"""\n    net = build_pixel_rnn_basic_model()\n\n    # build the optimizer (this will take a while!)\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate=0.001).minimize(net[\'cost\'])\n\n    # Load a list of files for tiny imagenet, downloading if necessary\n    imagenet_files = dsu.tiny_imagenet_load()\n\n    # Create a threaded image pipeline which will load/shuffle/crop/resize\n    batch = dsu.create_input_pipeline(\n        imagenet_files,\n        batch_size=B,\n        n_epochs=n_epochs,\n        shape=[64, 64, 3],\n        crop_shape=[32, 32, 3],\n        crop_factor=0.5,\n        n_threads=8)\n\n    sess = tf.Session()\n    saver = tf.train.Saver()\n    init_op = tf.group(tf.global_variables_initializer(),\n                       tf.local_variables_initializer())\n    sess.run(init_op)\n\n    # This will handle our threaded image pipeline\n    coord = tf.train.Coordinator()\n\n    # Ensure no more changes to graph\n    tf.get_default_graph().finalize()\n\n    # Start up the queues for handling the image pipeline\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    if os.path.exists(ckpt_name + \'.index\') or os.path.exists(ckpt_name):\n        saver.restore(sess, ckpt_name)\n        saver.restore(sess, tf.train.latest_checkpoint(\'./\'))\n\n    epoch_i = 0\n    batch_i = 0\n    save_step = 100\n    try:\n        while not coord.should_stop() and epoch_i < n_epochs:\n            batch_i += 1\n            batch_xs = sess.run(batch)\n            train_cost = sess.run(\n                [net[\'cost\'], optimizer], feed_dict={net[\'X\']: batch_xs})[0]\n            print(batch_i, train_cost)\n            if batch_i % save_step == 0:\n                # Save the variables to disk.  Don\'t write the meta graph\n                # since we can use the code to create it, and it takes a long\n                # time to create the graph since it is so deep\n                saver.save(\n                    sess,\n                    ckpt_name,\n                    global_step=batch_i,\n                    write_meta_graph=False)\n    except tf.errors.OutOfRangeError:\n        print(\'Done.\')\n    finally:\n        # One of the threads has issued an exception.  So let\'s tell all the\n        # threads to shutdown.\n        coord.request_stop()\n\n    # Wait until all threads have finished.\n    coord.join(threads)\n\n    # Clean up the session.\n    sess.close()\n\n\nif __name__ == \'__main__\':\n    train_tiny_imagenet()\n'"
cadl/seq2seq.py,47,"b'""""""Sequence to Sequence models w/ Attention and BiDirectional Dynamic RNNs.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nimport nltk\nimport pickle\nfrom cadl import cornell\n\n# Special vocabulary symbols:\n# PAD is used to pad a sequence to a fixed size\n# GO is for the end of the encoding\n# EOS is for the end of decoding\n# UNK is for out of vocabulary words\n_PAD, _GO, _EOS, _UNK = ""_PAD"", ""_GO"", ""_EOS"", ""_UNK""\n_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\nPAD_ID, GO_ID, EOS_ID, UNK_ID = range(4)\n\n\ndef _create_embedding(x, vocab_size, embed_size, embed_matrix=None):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    vocab_size : TYPE\n        Description\n    embed_size : TYPE\n        Description\n    embed_matrix : None, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    # Creating an embedding matrix if one isn\'t given\n    if embed_matrix is None:\n        # This is a big matrix\n        embed_matrix = tf.get_variable(\n            name=""embedding_matrix"",\n            shape=[vocab_size, embed_size],\n            dtype=tf.float32,\n            initializer=tf.random_uniform_initializer(-1.0, 1.0))\n\n    # Perform the lookup of ids in x and perform the embedding to embed_size\n    # [batch_size, max_time, embed_size]\n    embed = tf.nn.embedding_lookup(embed_matrix, x)\n\n    return embed, embed_matrix\n\n\ndef _create_rnn_cell(n_neurons, n_layers, keep_prob):\n    """"""Summary\n\n    Parameters\n    ----------\n    n_neurons : TYPE\n        Description\n    n_layers : TYPE\n        Description\n    keep_prob : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    import tensorflow.contrib.rnn as rnn\n\n    cell_fw = rnn.LayerNormBasicLSTMCell(\n        num_units=n_neurons, dropout_keep_prob=keep_prob)\n    # Build deeper recurrent net if using more than 1 layer\n    if n_layers > 1:\n        cells = [cell_fw]\n        for layer_i in range(1, n_layers):\n            with tf.variable_scope(\'{}\'.format(layer_i)):\n                cell_fw = rnn.LayerNormBasicLSTMCell(\n                    num_units=n_neurons, dropout_keep_prob=keep_prob)\n                cells.append(cell_fw)\n        cell_fw = rnn.MultiRNNCell(cells)\n    return cell_fw\n\n\ndef _create_encoder(embed, lengths, batch_size, n_enc_neurons, n_layers,\n                    keep_prob):\n    """"""Summary\n\n    Parameters\n    ----------\n    embed : TYPE\n        Description\n    lengths : TYPE\n        Description\n    batch_size : TYPE\n        Description\n    n_enc_neurons : TYPE\n        Description\n    n_layers : TYPE\n        Description\n    keep_prob : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    # Create the RNN Cells for encoder\n    with tf.variable_scope(\'forward\'):\n        cell_fw = _create_rnn_cell(n_enc_neurons, n_layers, keep_prob)\n\n    # Create the internal multi-layer cell for the backward RNN.\n    with tf.variable_scope(\'backward\'):\n        cell_bw = _create_rnn_cell(n_enc_neurons, n_layers, keep_prob)\n\n    # Now hookup the cells to the input\n    # [batch_size, max_time, embed_size]\n    (outputs, final_state) = tf.nn.bidirectional_dynamic_rnn(\n        cell_fw=cell_fw,\n        cell_bw=cell_bw,\n        inputs=embed,\n        sequence_length=lengths,\n        time_major=False,\n        dtype=tf.float32)\n\n    return outputs, final_state\n\n\ndef _create_decoder(cells,\n                    batch_size,\n                    encoder_outputs,\n                    encoder_state,\n                    encoder_lengths,\n                    decoding_inputs,\n                    decoding_lengths,\n                    embed_matrix,\n                    target_vocab_size,\n                    scope,\n                    max_sequence_size,\n                    use_attention=True):\n    """"""Summary\n\n    Parameters\n    ----------\n    cells : TYPE\n        Description\n    batch_size : TYPE\n        Description\n    encoder_outputs : TYPE\n        Description\n    encoder_state : TYPE\n        Description\n    encoder_lengths : TYPE\n        Description\n    decoding_inputs : TYPE\n        Description\n    decoding_lengths : TYPE\n        Description\n    embed_matrix : TYPE\n        Description\n    target_vocab_size : TYPE\n        Description\n    scope : TYPE\n        Description\n    max_sequence_size : TYPE\n        Description\n    use_attention : bool, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    from tensorflow.python.layers.core import Dense\n\n    # Output projection\n    output_layer = Dense(target_vocab_size, name=\'output_projection\')\n\n    # Setup Attention\n    if use_attention:\n        attn_mech = tf.contrib.seq2seq.LuongAttention(\n            cells.output_size, encoder_outputs, encoder_lengths, scale=True)\n        cells = tf.contrib.seq2seq.AttentionWrapper(\n            cell=cells,\n            attention_mechanism=attn_mech,\n            attention_layer_size=cells.output_size,\n            alignment_history=False)\n        initial_state = cells.zero_state(\n            dtype=tf.float32, batch_size=batch_size)\n        initial_state = initial_state.clone(cell_state=encoder_state)\n\n    # Setup training a build decoder\n    helper = tf.contrib.seq2seq.TrainingHelper(\n        inputs=decoding_inputs,\n        sequence_length=decoding_lengths,\n        time_major=False)\n    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n        cell=cells,\n        helper=helper,\n        initial_state=initial_state,\n        output_layer=output_layer)\n    train_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n        train_decoder,\n        output_time_major=False,\n        impute_finished=True,\n        maximum_iterations=max_sequence_size)\n    train_logits = tf.identity(train_outputs.rnn_output, name=\'train_logits\')\n\n    # Setup inference and build decoder\n    scope.reuse_variables()\n    start_tokens = tf.tile(tf.constant([GO_ID], dtype=tf.int32), [batch_size])\n    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n        embedding=embed_matrix, start_tokens=start_tokens, end_token=EOS_ID)\n    infer_decoder = tf.contrib.seq2seq.BasicDecoder(\n        cell=cells,\n        helper=helper,\n        initial_state=initial_state,\n        output_layer=output_layer)\n    infer_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n        infer_decoder,\n        output_time_major=False,\n        impute_finished=True,\n        maximum_iterations=max_sequence_size)\n    infer_logits = tf.identity(infer_outputs.sample_id, name=\'infer_logits\')\n\n    return train_logits, infer_logits\n\n\ndef create_model(source_vocab_size=10000,\n                 target_vocab_size=10000,\n                 input_embed_size=512,\n                 target_embed_size=512,\n                 share_input_and_target_embedding=True,\n                 n_neurons=512,\n                 n_layers=4,\n                 use_attention=True,\n                 max_sequence_size=30):\n    """"""Summary\n\n    Parameters\n    ----------\n    source_vocab_size : int, optional\n        Description\n    target_vocab_size : int, optional\n        Description\n    input_embed_size : int, optional\n        Description\n    target_embed_size : int, optional\n        Description\n    share_input_and_target_embedding : bool, optional\n        Description\n    n_neurons : int, optional\n        Description\n    n_layers : int, optional\n        Description\n    use_attention : bool, optional\n        Description\n    max_sequence_size : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n\n    Raises\n    ------\n    ValueError\n        Description\n    """"""\n    n_enc_neurons = n_neurons\n    n_dec_neurons = n_neurons\n\n    # First sentence (i.e. input, original language sentence before translation)\n    # [batch_size, max_time]\n    source = tf.placeholder(tf.int32, shape=(None, None), name=\'source\')\n\n    # User should also pass in the sequence lengths\n    source_lengths = tf.placeholder(\n        tf.int32, shape=(None,), name=\'source_lengths\')\n\n    # Second sentence (i.e. reply, translation, etc...)\n    # [batch_size, max_time]\n    target = tf.placeholder(tf.int32, shape=(None, None), name=\'target\')\n\n    # User should also pass in the sequence lengths\n    target_lengths = tf.placeholder(\n        tf.int32, shape=(None,), name=\'target_lengths\')\n\n    # Dropout\n    keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n\n    # Symbolic shapes\n    batch_size, sequence_length = tf.unstack(tf.shape(source))\n\n    # Get the input to the decoder by removing last element\n    # and adding a \'go\' symbol as first element\n    with tf.variable_scope(\'target/slicing\'):\n        slice = tf.slice(target, [0, 0], [batch_size, -1])\n        decoder_input = tf.concat([tf.fill([batch_size, 1], GO_ID), slice], 1)\n\n    # Embed word ids to target embedding\n    with tf.variable_scope(\'source/embedding\'):\n        source_embed, source_embed_matrix = _create_embedding(\n            x=source, vocab_size=source_vocab_size, embed_size=input_embed_size)\n\n    # Embed word ids for target embedding\n    with tf.variable_scope(\'target/embedding\'):\n        # Check if we need a new embedding matrix or not.  If we are for\n        # instance translating to another language, then we\'d need different\n        # vocabularies for the input and outputs, and so new embeddings.\n        # However if we are for instance building a chatbot with the same\n        # language, then it doesn\'t make sense to have different embeddings and\n        # we should share them.\n        if (share_input_and_target_embedding and\n                source_vocab_size == target_vocab_size):\n            target_input_embed, target_embed_matrix = _create_embedding(\n                x=decoder_input,\n                vocab_size=target_vocab_size,\n                embed_size=target_embed_size,\n                embed_matrix=source_embed_matrix)\n        elif source_vocab_size != target_vocab_size:\n            raise ValueError(\n                \'source_vocab_size must equal target_vocab_size if \' +\n                \'sharing input and target embeddings\')\n        else:\n            target_input_embed, target_embed_matrix = _create_embedding(\n                x=target,\n                vocab_size=target_vocab_size,\n                embed_size=target_embed_size)\n\n    # Build the encoder\n    with tf.variable_scope(\'encoder\'):\n        encoder_outputs, encoder_state = _create_encoder(\n            embed=source_embed,\n            lengths=source_lengths,\n            batch_size=batch_size,\n            n_enc_neurons=n_enc_neurons,\n            n_layers=n_layers,\n            keep_prob=keep_prob)\n\n    # Build the decoder\n    with tf.variable_scope(\'decoder\') as scope:\n        cell_fw = _create_rnn_cell(n_dec_neurons, n_layers, keep_prob)\n        decoding_train_logits, decoding_infer_logits = _create_decoder(\n            cells=cell_fw,\n            batch_size=batch_size,\n            encoder_outputs=encoder_outputs[0],\n            encoder_state=encoder_state[0],\n            encoder_lengths=source_lengths,\n            decoding_inputs=target_input_embed,\n            decoding_lengths=target_lengths,\n            embed_matrix=target_embed_matrix,\n            target_vocab_size=target_vocab_size,\n            scope=scope,\n            max_sequence_size=max_sequence_size)\n\n    with tf.variable_scope(\'loss\'):\n        weights = tf.cast(tf.sequence_mask(target_lengths), tf.float32)\n        loss = tf.contrib.seq2seq.sequence_loss(\n            logits=tf.reshape(decoding_train_logits, [\n                batch_size, tf.reduce_max(target_lengths), target_vocab_size\n            ]),\n            targets=target,\n            weights=weights)\n\n    return {\n        \'loss\': loss,\n        \'source\': source,\n        \'source_lengths\': source_lengths,\n        \'target\': target,\n        \'target_lengths\': target_lengths,\n        \'keep_prob\': keep_prob,\n        \'thought_vector\': encoder_state,\n        \'decoder\': decoding_infer_logits\n    }\n\n\ndef batch_generator(sources,\n                    targets,\n                    source_lengths,\n                    target_lengths,\n                    batch_size=50):\n    """"""Summary\n\n    Parameters\n    ----------\n    sources : TYPE\n        Description\n    targets : TYPE\n        Description\n    source_lengths : TYPE\n        Description\n    target_lengths : TYPE\n        Description\n    batch_size : int, optional\n        Description\n\n    Yields\n    ------\n    TYPE\n        Description\n    """"""\n    idxs = np.random.permutation(np.arange(len(sources)))\n    n_batches = len(idxs) // batch_size\n    for batch_i in range(n_batches):\n        this_idxs = idxs[batch_i * batch_size:(batch_i + 1) * batch_size]\n        this_sources, this_targets = sources[this_idxs, :], targets[\n            this_idxs, :]\n        this_source_lengths, this_target_lengths = source_lengths[\n            this_idxs], target_lengths[this_idxs]\n        yield (this_sources[:, :np.max(this_source_lengths)],\n               this_targets[:, :np.max(this_target_lengths)],\n               this_source_lengths, this_target_lengths)\n\n\ndef preprocess(text, min_count=5, min_length=3, max_length=30):\n    """"""Summary\n\n    Parameters\n    ----------\n    text : TYPE\n        Description\n    min_count : int, optional\n        Description\n    min_length : int, optional\n        Description\n    max_length : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    sentences = [el for s in text for el in nltk.sent_tokenize(s)]\n\n    # We\'ll first tokenize each sentence into words to get a sense of\n    # how long each sentence is:\n    words = [[word.lower() for word in nltk.word_tokenize(s)]\n             for s in sentences]\n\n    # Then see how long each sentence is:\n    lengths = np.array([len(s) for s in words])\n\n    good_idxs = np.where((lengths >= min_length) & (lengths < max_length))[0]\n    dataset = [words[idx] for idx in good_idxs]\n    fdist = nltk.FreqDist([word for sentence in dataset for word in sentence])\n\n    vocab_counts = [el for el in fdist.most_common() if el[1] > min_count]\n\n    # First sort the vocabulary\n    vocab = [v[0] for v in vocab_counts]\n    vocab.sort()\n\n    # Now add the special symbols:\n    vocab = _START_VOCAB + vocab\n\n    # Then create the word to id mapping\n    vocab = {k: v for v, k in enumerate(vocab)}\n\n    with open(\'vocab.pkl\', \'wb\') as fp:\n        pickle.dump(vocab, fp)\n\n    unked = word2id(dataset, vocab)\n    return unked, vocab\n\n\ndef word2id(words, vocab):\n    """"""Summary\n\n    Parameters\n    ----------\n    words : TYPE\n        Description\n    vocab : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    unked = []\n    for s in words:\n        this_sentence = [vocab.get(w, UNK_ID) for w in s]\n        unked.append(this_sentence)\n    return unked\n\n\ndef id2word(ids, vocab):\n    """"""Summary\n\n    Parameters\n    ----------\n    ids : TYPE\n        Description\n    vocab : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    words = []\n    id2words = {v: k for k, v in vocab.items()}\n    for s in ids:\n        this_sentence = [id2words.get(w) for w in s]\n        words.append(this_sentence)\n    return words\n\n\ndef train(text,\n          max_sequence_size=20,\n          use_attention=True,\n          min_count=25,\n          min_length=5,\n          n_epochs=1000,\n          batch_size=100):\n    """"""Summary\n\n    Parameters\n    ----------\n    text : TYPE\n        Description\n    max_sequence_size : int, optional\n        Description\n    use_attention : bool, optional\n        Description\n    min_count : int, optional\n        Description\n    min_length : int, optional\n        Description\n    n_epochs : int, optional\n        Description\n    batch_size : int, optional\n        Description\n    """"""\n    # Preprocess it to word IDs including UNKs for out of vocabulary words\n    unked, vocab = preprocess(\n        text,\n        min_count=min_count,\n        min_length=min_length,\n        max_length=max_sequence_size - 1)\n\n    # Get the vocabulary size\n    vocab_size = len(vocab)\n\n    # Create input output pairs formed by neighboring sentences of dialog\n    sources_list, targets_list = unked[:-1], unked[1:]\n\n    # Store the final lengths\n    source_lengths = np.zeros((len(sources_list)), dtype=np.int32)\n    target_lengths = np.zeros((len(targets_list)), dtype=np.int32)\n    sources = np.ones(\n        (len(sources_list), max_sequence_size), dtype=np.int32) * PAD_ID\n    targets = np.ones(\n        (len(targets_list), max_sequence_size), dtype=np.int32) * PAD_ID\n\n    for i, (source_i, target_i) in enumerate(zip(sources_list, targets_list)):\n        el = source_i\n        source_lengths[i] = len(el)\n        sources[i, :len(el)] = el\n\n        el = target_i + [EOS_ID]\n        target_lengths[i] = len(el)\n        targets[i, :len(el)] = el\n\n    sess = tf.Session()\n\n    net = create_model(\n        max_sequence_size=max_sequence_size,\n        use_attention=use_attention,\n        source_vocab_size=vocab_size,\n        target_vocab_size=vocab_size)\n\n    learning_rate = tf.placeholder(tf.float32, name=\'learning_rate\')\n    opt = tf.train.AdamOptimizer(\n        learning_rate=learning_rate).minimize(net[\'loss\'])\n    init_op = tf.group(tf.global_variables_initializer(),\n                       tf.local_variables_initializer())\n    sess.run(init_op)\n    saver = tf.train.Saver()\n\n    def decode(tokens, lengths):\n        """"""Summary\n\n        Parameters\n        ----------\n        tokens : TYPE\n            Description\n        lengths : TYPE\n            Description\n        """"""\n        decoding = sess.run(\n            net[\'decoder\'],\n            feed_dict={\n                net[\'keep_prob\']: 1.0,\n                net[\'source\']: tokens,\n                net[\'source_lengths\']: lengths\n            })\n        print(\'input:\', "" "".join(id2word([tokens[0]], vocab)[0]))\n        print(\'output:\', "" "".join(id2word([decoding[0]], vocab)[0]))\n\n    current_learning_rate = 0.01\n    for epoch_i in range(n_epochs):\n        total = 0\n        for it_i, (this_sources, this_targets, this_source_lengths, this_target_lengths) \\\n            in enumerate(batch_generator(\n                sources, targets, source_lengths, target_lengths, batch_size=batch_size)):\n            if it_i % 1000 == 0:\n                current_learning_rate = max(0.0001,\n                                            current_learning_rate * 0.99)\n                print(it_i)\n                decode(this_sources[0:1], this_source_lengths[0:1])\n            l = sess.run(\n                [net[\'loss\'], opt],\n                feed_dict={\n                    learning_rate: current_learning_rate,\n                    net[\'keep_prob\']: 0.8,\n                    net[\'source\']: this_sources,\n                    net[\'target\']: this_targets,\n                    net[\'source_lengths\']: this_source_lengths,\n                    net[\'target_lengths\']: this_target_lengths\n                })[0]\n            total = total + l\n            print(\'{}: {}\'.format(it_i, total / (it_i + 1)), end=\'\\r\')\n        # End of epoch, save\n        print(\'epoch {}: {}\'.format(epoch_i, total / it_i))\n        saver.save(sess, \'./dynamic-seq2seq.ckpt\', global_step=it_i)\n\n    sess.close()\n\n\ndef train_cornell(**kwargs):\n    """"""Summary\n\n    Parameters\n    ----------\n    **kwargs\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    # Get the cornell dataset text\n    text = cornell.get_scripts()\n    return train(text, **kwargs)\n'"
cadl/squeezenet.py,28,"b'""""""SqueezeNet\n""""""\n""""""\nsqueezeNet is a much smaller convolutional network, with vastly less amount of parameters.\nqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\nAdditionally, with model compression techniques SqueezeNet can be compress to less than 0.5MB (510x smaller than AlexNet).\n\nhttp://arxiv.org/abs/1602.07360\n\nCode taken from https://github.com/Khushmeet/squeezeNet/blob/master/squeezenet/squeezenet.py\n\nMIT License\n\nCopyright (c) 2017 Khushmeet\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the ""Software""), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\n\nimport tensorflow as tf\n\n\ndef fire_module(\n        input,\n        fire_id,\n        channel,\n        s1,\n        e1,\n        e3, ):\n    """"""\n    Basic module that makes up the SqueezeNet architecture. It has two layers.\n     1. Squeeze layer (1x1 convolutions)\n     2. Expand layer (1x1 and 3x3 convolutions)\n    :param input: Tensorflow tensor\n    :param fire_id: Variable scope name\n    :param channel: Depth of the previous output\n    :param s1: Number of filters for squeeze 1x1 layer\n    :param e1: Number of filters for expand 1x1 layer\n    :param e3: Number of filters for expand 3x3 layer\n    :return: Tensorflow tensor\n\n    Parameters\n    ----------\n    input : TYPE\n        Description\n    fire_id : TYPE\n        Description\n    channel : TYPE\n        Description\n    s1 : TYPE\n        Description\n    e1 : TYPE\n        Description\n    e3 : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n\n    fire_weights = {\n        \'conv_s_1\': tf.Variable(tf.truncated_normal([1, 1, channel, s1])),\n        \'conv_e_1\': tf.Variable(tf.truncated_normal([1, 1, s1, e1])),\n        \'conv_e_3\': tf.Variable(tf.truncated_normal([3, 3, s1, e3]))\n    }\n\n    fire_biases = {\n        \'conv_s_1\': tf.Variable(tf.truncated_normal([s1])),\n        \'conv_e_1\': tf.Variable(tf.truncated_normal([e1])),\n        \'conv_e_3\': tf.Variable(tf.truncated_normal([e3]))\n    }\n\n    with tf.name_scope(fire_id):\n        output = tf.nn.conv2d(\n            input,\n            fire_weights[\'conv_s_1\'],\n            strides=[1, 1, 1, 1],\n            padding=\'SAME\',\n            name=\'conv_s_1\')\n        output = tf.nn.relu(tf.nn.bias_add(output, fire_biases[\'conv_s_1\']))\n\n        expand1 = tf.nn.conv2d(\n            output,\n            fire_weights[\'conv_e_1\'],\n            strides=[1, 1, 1, 1],\n            padding=\'SAME\',\n            name=\'conv_e_1\')\n        expand1 = tf.nn.bias_add(expand1, fire_biases[\'conv_e_1\'])\n\n        expand3 = tf.nn.conv2d(\n            output,\n            fire_weights[\'conv_e_3\'],\n            strides=[1, 1, 1, 1],\n            padding=\'SAME\',\n            name=\'conv_e_3\')\n        expand3 = tf.nn.bias_add(expand3, fire_biases[\'conv_e_3\'])\n\n        result = tf.concat([expand1, expand3], 3, name=\'concat_e1_e3\')\n        return tf.nn.relu(result)\n\n\ndef squeeze_net(input, classes):\n    """"""\n    SqueezeNet model written in tensorflow. It provides AlexNet level accuracy with 50x fewer parameters\n    and smaller model size.\n    :param input: Input tensor (4D)\n    :param classes: number of classes for classification\n    :return: Tensorflow tensor\n\n    Parameters\n    ----------\n    input : TYPE\n        Description\n    classes : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n\n    weights = {\n        \'conv1\': tf.Variable(tf.truncated_normal([7, 7, 1, 96])),\n        \'conv10\': tf.Variable(tf.truncated_normal([1, 1, 512, classes]))\n    }\n\n    biases = {\n        \'conv1\': tf.Variable(tf.truncated_normal([96])),\n        \'conv10\': tf.Variable(tf.truncated_normal([classes]))\n    }\n\n    output = tf.nn.conv2d(\n        input,\n        weights[\'conv1\'],\n        strides=[1, 2, 2, 1],\n        padding=\'SAME\',\n        name=\'conv1\')\n    output = tf.nn.bias_add(output, biases[\'conv1\'])\n\n    output = tf.nn.max_pool(\n        output,\n        ksize=[1, 3, 3, 1],\n        strides=[1, 2, 2, 1],\n        padding=\'SAME\',\n        name=\'maxpool1\')\n\n    output = fire_module(\n        output, s1=16, e1=64, e3=64, channel=96, fire_id=\'fire2\')\n    output = fire_module(\n        output, s1=16, e1=64, e3=64, channel=128, fire_id=\'fire3\')\n    output = fire_module(\n        output, s1=32, e1=128, e3=128, channel=128, fire_id=\'fire4\')\n\n    output = tf.nn.max_pool(\n        output,\n        ksize=[1, 3, 3, 1],\n        strides=[1, 2, 2, 1],\n        padding=\'SAME\',\n        name=\'maxpool4\')\n\n    output = fire_module(\n        output, s1=32, e1=128, e3=128, channel=256, fire_id=\'fire5\')\n    output = fire_module(\n        output, s1=48, e1=192, e3=192, channel=256, fire_id=\'fire6\')\n    output = fire_module(\n        output, s1=48, e1=192, e3=192, channel=384, fire_id=\'fire7\')\n    output = fire_module(\n        output, s1=64, e1=256, e3=256, channel=384, fire_id=\'fire8\')\n\n    output = tf.nn.max_pool(\n        output,\n        ksize=[1, 3, 3, 1],\n        strides=[1, 2, 2, 1],\n        padding=\'SAME\',\n        name=\'maxpool8\')\n\n    output = fire_module(\n        output, s1=64, e1=256, e3=256, channel=512, fire_id=\'fire9\')\n\n    output = tf.nn.dropout(output, keep_prob=0.5, name=\'dropout9\')\n\n    output = tf.nn.conv2d(\n        output,\n        weights[\'conv10\'],\n        strides=[1, 1, 1, 1],\n        padding=\'SAME\',\n        name=\'conv10\')\n    output = tf.nn.bias_add(output, biases[\'conv10\'])\n\n    output = tf.nn.avg_pool(\n        output,\n        ksize=[1, 13, 13, 1],\n        strides=[1, 2, 2, 1],\n        padding=\'SAME\',\n        name=\'avgpool10\')\n\n    return output\n'"
cadl/stats.py,2,"b'""""""Metrics about TensorFlow graphs.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport locale\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\n\n\ndef print_stat(prefix, statistic_type, value):\n    """"""Summary\n\n    Parameters\n    ----------\n    prefix : TYPE\n        Description\n    statistic_type : TYPE\n        Description\n    value : TYPE\n        Description\n    """"""\n    if value is None:\n        friendly_value = ""None""\n    else:\n        friendly_value = locale.format(""%d"", value, grouping=True)\n    print(""%s%s=%s"" % (prefix, statistic_type, friendly_value))\n\n\ndef calculate_graph_metrics(graph_def, statistic_types, input_layer,\n                            input_shape_override, batch_size):\n    """"""Looks at the performance statistics of all nodes in the graph.\n\n    Parameters\n    ----------\n    graph_def : TYPE\n        Description\n    statistic_types : TYPE\n        Description\n    input_layer : TYPE\n        Description\n    input_shape_override : TYPE\n        Description\n    batch_size : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n\n    Raises\n    ------\n    ValueError\n        Description\n    """"""\n    tf.import_graph_def(graph_def, name="""")\n    total_stats = {}\n    node_stats = {}\n    for statistic_type in statistic_types:\n        total_stats[statistic_type] = ops.OpStats(statistic_type)\n        node_stats[statistic_type] = {}\n    # Make sure we get pretty-printed numbers with separators.\n    locale.setlocale(locale.LC_ALL, """")\n    with tf.Session() as sess:\n        input_tensor = sess.graph.get_tensor_by_name(input_layer)\n        input_shape_tensor = input_tensor.get_shape()\n        if input_shape_tensor:\n            input_shape = input_shape_tensor.as_list()\n        else:\n            input_shape = None\n        if input_shape_override:\n            input_shape = input_shape_override\n        if input_shape is None:\n            raise ValueError(""""""No input shape was provided on the command line,""""""\n                             """""" and the input op itself had no default shape, so""""""\n                             """""" shape inference couldn\'t be performed. This is""""""\n                             """""" required for metrics calculations."""""")\n        input_shape[0] = batch_size\n        input_tensor.set_shape(input_shape)\n        for node in graph_def.node:\n            # Ensure that the updated input shape has been fully-propagated before we\n            # ask for the statistics, since they may depend on the output size.\n            op = sess.graph.get_operation_by_name(node.name)\n            ops.set_shapes_for_outputs(op)\n            for statistic_type in statistic_types:\n                current_stats = ops.get_stats_for_node_def(sess.graph, node,\n                                                           statistic_type)\n                node_stats[statistic_type][node.name] = current_stats\n                total_stats[statistic_type] += current_stats\n    return total_stats, node_stats\n\n\ndef stats(graph_def, input_layer, batch_size):\n    """"""Summary\n\n    Parameters\n    ----------\n    graph_def : TYPE\n        Description\n    input_layer : TYPE\n        Description\n    batch_size : TYPE\n        Description\n    """"""\n    statistic_types = [\'flops\']\n    input_shape_override = False\n    total_stats, node_stats = calculate_graph_metrics(\n        graph_def, statistic_types, input_layer, input_shape_override,\n        batch_size)\n    for node in graph_def.node:\n        for statistic_type in statistic_types:\n            current_stats = node_stats[statistic_type][node.name]\n            print_stat(node.name + ""("" + node.op + ""): "", statistic_type,\n                       current_stats.value)\n    for statistic_type in statistic_types:\n        value = total_stats[statistic_type].value\n        print_stat(""Total: "", statistic_type, value)\n'"
cadl/stylenet.py,15,"b'""""""Style Net w/ tests for Video Style Net.\n""""""\n""""""\nVideo Style Net requires OpenCV 3.0.0+ w/ Contrib for Python to be installed.\n\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom cadl import vgg16\nfrom cadl import gif\nfrom scipy.misc import imresize\n\n\ndef make_4d(img):\n    """"""Create a 4-dimensional N x H x W x C image.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        Given image as H x W x C or H x W.\n\n    Returns\n    -------\n    img : np.ndarray\n        N x H x W x C image.\n\n    Raises\n    ------\n    ValueError\n        Unexpected number of dimensions.\n    """"""\n    if img.ndim == 2:\n        img = np.expand_dims(img[np.newaxis], 3)\n    elif img.ndim == 3:\n        img = img[np.newaxis]\n    elif img.ndim == 4:\n        return img\n    else:\n        raise ValueError(\'Incorrect dimensions for image!\')\n    return img\n\n\ndef stylize(content_img,\n            style_img,\n            base_img=None,\n            saveto=None,\n            gif_step=5,\n            n_iterations=100,\n            style_weight=1.0,\n            content_weight=1.0):\n    """"""Stylization w/ the given content and style images.\n\n    Follows the approach in Leon Gatys et al.\n\n    Parameters\n    ----------\n    content_img : np.ndarray\n        Image to use for finding the content features.\n    style_img : TYPE\n        Image to use for finding the style features.\n    base_img : None, optional\n        Image to use for the base content.  Can be noise or an existing image.\n        If None, the content image will be used.\n    saveto : str, optional\n        Name of GIF image to write to, e.g. ""stylization.gif""\n    gif_step : int, optional\n        Modulo of iterations to save the current stylization.\n    n_iterations : int, optional\n        Number of iterations to run for.\n    style_weight : float, optional\n        Weighting on the style features.\n    content_weight : float, optional\n        Weighting on the content features.\n\n    Returns\n    -------\n    stylization : np.ndarray\n        Final iteration of the stylization.\n    """"""\n    # Preprocess both content and style images\n    content_img = vgg16.preprocess(content_img, dsize=(224, 224))[np.newaxis]\n    style_img = vgg16.preprocess(style_img, dsize=(224, 224))[np.newaxis]\n    if base_img is None:\n        base_img = content_img\n    else:\n        base_img = make_4d(vgg16.preprocess(base_img, dsize=(224, 224)))\n\n    # Get Content and Style features\n    net = vgg16.get_vgg_model()\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        tf.import_graph_def(net[\'graph_def\'], name=\'vgg\')\n        names = [op.name for op in g.get_operations()]\n        x = g.get_tensor_by_name(names[0] + \':0\')\n        content_layer = \'vgg/conv3_2/conv3_2:0\'\n        content_features = g.get_tensor_by_name(content_layer).eval(\n            feed_dict={\n                x: content_img,\n                \'vgg/dropout_1/random_uniform:0\': [[1.0] * 4096],\n                \'vgg/dropout/random_uniform:0\': [[1.0] * 4096]\n            })\n        style_layers = [\n            \'vgg/conv1_1/conv1_1:0\', \'vgg/conv2_1/conv2_1:0\',\n            \'vgg/conv3_1/conv3_1:0\', \'vgg/conv4_1/conv4_1:0\',\n            \'vgg/conv5_1/conv5_1:0\'\n        ]\n        style_activations = []\n        for style_i in style_layers:\n            style_activation_i = g.get_tensor_by_name(style_i).eval(\n                feed_dict={\n                    x: style_img,\n                    \'vgg/dropout_1/random_uniform:0\': [[1.0] * 4096],\n                    \'vgg/dropout/random_uniform:0\': [[1.0] * 4096]\n                })\n            style_activations.append(style_activation_i)\n        style_features = []\n        for style_activation_i in style_activations:\n            s_i = np.reshape(style_activation_i,\n                             [-1, style_activation_i.shape[-1]])\n            gram_matrix = np.matmul(s_i.T, s_i) / s_i.size\n            style_features.append(gram_matrix.astype(np.float32))\n\n    # Optimize both\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        net_input = tf.Variable(base_img)\n        tf.import_graph_def(\n            net[\'graph_def\'], name=\'vgg\', input_map={\'images:0\': net_input})\n\n        content_loss = tf.nn.l2_loss(\n            (g.get_tensor_by_name(content_layer) - content_features) /\n            content_features.size)\n        style_loss = np.float32(0.0)\n        for style_layer_i, style_gram_i in zip(style_layers, style_features):\n            layer_i = g.get_tensor_by_name(style_layer_i)\n            layer_shape = layer_i.get_shape().as_list()\n            layer_size = layer_shape[1] * layer_shape[2] * layer_shape[3]\n            layer_flat = tf.reshape(layer_i, [-1, layer_shape[3]])\n            gram_matrix = tf.matmul(tf.transpose(layer_flat),\n                                    layer_flat) / layer_size\n            style_loss = tf.add(style_loss,\n                                tf.nn.l2_loss((gram_matrix - style_gram_i) /\n                                              np.float32(style_gram_i.size)))\n        loss = content_weight * content_loss + style_weight * style_loss\n        optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        sess.run(init_op)\n        imgs = []\n        for it_i in range(n_iterations):\n            _, this_loss, synth = sess.run(\n                [optimizer, loss, net_input],\n                feed_dict={\n                    \'vgg/dropout_1/random_uniform:0\':\n                    np.ones(\n                        g.get_tensor_by_name(\'vgg/dropout_1/random_uniform:0\')\n                        .get_shape().as_list()),\n                    \'vgg/dropout/random_uniform:0\':\n                    np.ones(\n                        g.get_tensor_by_name(\'vgg/dropout/random_uniform:0\')\n                        .get_shape().as_list())\n                })\n            print(\n                ""iteration %d, loss: %f, range: (%f - %f)"" %\n                (it_i, this_loss, np.min(synth), np.max(synth)),\n                end=\'\\r\')\n            if it_i % gif_step == 0:\n                imgs.append(np.clip(synth[0], 0, 1))\n        if saveto is not None:\n            gif.build_gif(imgs, saveto=saveto)\n    return np.clip(synth[0], 0, 1)\n\n\ndef warp_img(img, dx, dy):\n    """"""Apply the motion vectors to the given image.\n\n    Parameters\n    ----------\n    img : np.ndarray\n        Input image to apply motion to.\n    dx : np.ndarray\n        H x W matrix defining the magnitude of the X vector\n    dy : np.ndarray\n        H x W matrix defining the magnitude of the Y vector\n\n    Returns\n    -------\n    img : np.ndarray\n        Image with pixels warped according to dx, dy.\n    """"""\n    warped = img.copy()\n    for row_i in range(img.shape[0]):\n        for col_i in range(img.shape[1]):\n            dx_i = int(np.round(dx[row_i, col_i]))\n            dy_i = int(np.round(dy[row_i, col_i]))\n            sample_dx = np.clip(dx_i + col_i, 0, img.shape[0] - 1)\n            sample_dy = np.clip(dy_i + row_i, 0, img.shape[1] - 1)\n            warped[sample_dy, sample_dx, :] = img[row_i, col_i, :]\n    return warped\n\n\ndef test_video(style_img=\'arles.jpg\', videodir=\'kurosawa\'):\n\n    has_cv2 = True\n    try:\n        import cv2\n        has_cv2 = True\n        optflow = cv2.optflow.createOptFlow_DeepFlow()\n    except ImportError:\n        has_cv2 = False\n\n    style_img = plt.imread(style_img)\n    content_files = [\n        os.path.join(videodir, f) for f in os.listdir(videodir)\n        if f.endswith(\'.png\')\n    ]\n    content_img = plt.imread(content_files[0])\n    style_img = imresize(style_img, (448, 448)).astype(np.float32) / 255.0\n    content_img = imresize(content_img, (448, 448)).astype(np.float32) / 255.0\n    if has_cv2:\n        prev_lum = cv2.cvtColor(content_img, cv2.COLOR_RGB2HSV)[:, :, 2]\n    else:\n        prev_lum = (content_img[..., 0] * 0.3 + content_img[..., 1] * 0.59 +\n                    content_img[..., 2] * 0.11)\n    imgs = []\n    stylized = stylize(\n        content_img,\n        style_img,\n        content_weight=5.0,\n        style_weight=0.5,\n        n_iterations=50)\n    plt.imsave(fname=content_files[0] + \'stylized.png\', arr=stylized)\n    imgs.append(stylized)\n    for f in content_files[1:]:\n        content_img = plt.imread(f)\n        content_img = imresize(content_img, (448,\n                                             448)).astype(np.float32) / 255.0\n        if has_cv2:\n            lum = cv2.cvtColor(content_img, cv2.COLOR_RGB2HSV)[:, :, 2]\n            flow = optflow.calc(prev_lum, lum, None)\n            warped = warp_img(stylized, flow[..., 0], flow[..., 1])\n            stylized = stylize(\n                content_img,\n                style_img,\n                content_weight=5.0,\n                style_weight=0.5,\n                base_img=warped,\n                n_iterations=50)\n        else:\n            lum = (content_img[..., 0] * 0.3 + content_img[..., 1] * 0.59 +\n                   content_img[..., 2] * 0.11)\n            stylized = stylize(\n                content_img,\n                style_img,\n                content_weight=5.0,\n                style_weight=0.5,\n                base_img=None,\n                n_iterations=50)\n        imgs.append(stylized)\n        plt.imsave(fname=f + \'stylized.png\', arr=stylized)\n        prev_lum = lum\n    return imgs\n\n\ndef test():\n    """"""Test for artistic stylization.\n    """"""\n    from six.moves import urllib\n    f = (\'https://upload.wikimedia.org/wikipedia/commons/thumb/5/54/\' +\n         \'Claude_Monet%2C_Impression%2C_soleil_levant.jpg/617px-Claude_Monet\' +\n         \'%2C_Impression%2C_soleil_levant.jpg?download\')\n    filepath, _ = urllib.request.urlretrieve(f, f.split(\'/\')[-1], None)\n    style = plt.imread(filepath).astype(np.float32) / 255.0\n\n    f = (\'https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/\' +\n         \'El_jard%C3%ADn_de_las_Delicias%2C_de_El_Bosco.jpg/640px-El_jard\' +\n         \'%C3%ADn_de_las_Delicias%2C_de_El_Bosco.jpg\')\n    filepath, _ = urllib.request.urlretrieve(f, f.split(\'/\')[-1], None)\n    content = plt.imread(filepath).astype(np.float32) / 255.0\n\n    stylize(content, style, n_iterations=20)\n\n\nif __name__ == \'__main__\':\n    test_video()\n'"
cadl/tedlium.py,0,"b'""""""TEDLium Dataset.\n""""""\n""""""\nSPH format info\n---------------\nChannels            : 1\nSample Rate     : 16000\nPrecision           : 16-bit\nBit Rate            : 256k\nSample Encoding : 16-bit Signed Integer PCM\n\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport os\n\n\ndef get_dataset():\n    """"""Summary\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    stms = []\n    for dirpath, dirnames, filenames in os.walk(\'TEDLIUM_release2\'):\n        for f in filenames:\n            if f.endswith(\'stm\'):\n                stms.append(os.path.join(dirpath, f))\n\n    data = []\n    for stm_i in stms:\n        with open(stm_i, \'r\') as fp:\n            lines = fp.readlines()\n        for line_i in lines:\n            sp = line_i.split()\n            data.append({\n                \'id\': sp[0],\n                \'num\': sp[1],\n                \'id2\': sp[2],\n                \'start_time\': sp[3],\n                \'end_time\': sp[4],\n                \'ch\': \'wideband\' if \'f0\' in sp[5] else \'telephone\',\n                \'sex\': \'male\' if \'male\' in sp[5] else \'female\',\n                \'text\': "" "".join(\n                    sp[6:]) if sp[6] != \'ignore_time_segment_in_scoring\' else \'\'})\n\n    for max_duration in range(30):\n        durations = []\n        for stm_i in stms:\n            with open(stm_i, \'r\') as fp:\n                lines = fp.readlines()\n            for line_i in lines:\n                sp = line_i.split()\n                dur = float(sp[4]) - float(sp[3])\n                if dur < max_duration:\n                    durations.append(dur)\n\n    return data, durations\n'"
cadl/timit.py,38,"b'""""""Tools for parsing TIMIT.\n\nTIMIT is described here: https://catalog.ldc.upenn.edu/docs/LDC93S1/\n\nCopyright Parag K. Mital, September 2016.\n""""""\nimport os\nimport numpy as np\nfrom scipy.io import wavfile\nfrom collections import OrderedDict\nimport tensorflow as tf\n\n\ndef parse_timit_entry(dirpath, file):\n    """"""Summary.\n\n    Parameters\n    ----------\n    dirpath : TYPE\n        Description\n    file : TYPE\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    path = os.path.join(dirpath, file)\n    phnfile = os.path.join(dirpath, file.strip(\'.wav\') + \'.PHN\')\n    with open(phnfile) as fp:\n        phones = []\n        for line_i in fp.readlines():\n            els = line_i.split(\' \')\n            phones.append({\'start_time\': els[0],\n                           \'end_time\': els[1],\n                           \'phone\': els[2].strip()})\n    wrdfile = os.path.join(dirpath, file.strip(\'.wav\') + \'.WRD\')\n    with open(wrdfile) as fp:\n        words = []\n        for line_i in fp.readlines():\n            els = line_i.split(\' \')\n            words.append({\'start_time\': els[0],\n                          \'end_time\': els[1],\n                          \'word\': els[2].strip()})\n    txtfile = os.path.join(dirpath, file.strip(\'.wav\') + \'.TXT\')\n    starttime = 0\n    endtime = 0\n    with open(txtfile) as fp:\n        lines = fp.readlines()\n        els = lines[0].split(\' \')\n        starttime = els[0]\n        endtime = els[1]\n        text = "" "".join(els[2:]).strip()\n    entry = {\n        \'path\': path,\n        \'name\': file,\n        \'phones\': phones,\n        \'words\': words,\n        \'start\': starttime,\n        \'end\': endtime,\n        \'text\': text\n    }\n    return entry\n\n\ndef parse_timit(timit_dir=\'/home/parag/kadmldb/data/speech/TIMIT/TIMIT\'):\n    """"""Summary.\n\n    Returns\n    -------\n    name : TYPE\n        Description\n\n    Parameters\n    ----------\n    timit_dir : str, optional\n        Description\n    """"""\n    timit = []\n    for dirpath, dirnames, filenames in os.walk(timit_dir):\n        for file in filenames:\n            if file.endswith(\'wav\'):\n                timit.append(parse_timit_entry(dirpath, file))\n    phones = list(set([ph[\'phone\']\n                       for t in timit for ph in t[\'phones\']]))\n    phones.sort()\n    phones = phones + [\'_\']\n    words = list(set([ph[\'word\']\n                      for t in timit for ph in t[\'words\']]))\n    words.sort()\n    encoder = OrderedDict(zip(phones, range(len(phones))))\n    decoder = OrderedDict(zip(range(len(phones)), phones))\n    return {\n        \'data\': timit,\n        \'phones\': phones,\n        \'words\': words,\n        \'encoder\': encoder,\n        \'decoder\': decoder\n    }\n\n\ndef preprocess(file):\n    """"""Summary\n\n    Parameters\n    ----------\n    file : TYPE\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    fft_size = 256\n    sr, s = wavfile.read(file)\n    s = s / np.abs(s).max()\n    mag, phs = dft.forward(s, hop_size=128, fft_size=fft_size)\n    cqft = dft.mel(fft_size, sr)\n    mel = np.dot(mag, cqft[:fft_size // 2, :])\n    mfcc = dft.mfcc(mel, 1, 13).astype(np.float32)\n    return mfcc\n\n\ndef create_observation(el, max_sequence_length, hop_size=128):\n    """"""Summary\n\n    Parameters\n    ----------\n    el : TYPE\n        Description\n    max_sequence_length : TYPE\n        Description\n    hop_size : int, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    # grab MFCCs\n    all_Xs = preprocess(el[\'path\'])\n\n    # storage for our observation\n    X, Y = [], []\n\n    # grab corresponding phones\n    for i, ph in enumerate(el[\'phones\']):\n\n        # grad start/end time in samples\n        s1, s2 = ph[\'start_time\'], ph[\'end_time\']\n\n        # convert to frames\n        f1, f2 = [min(int(np.round(float(s) / hop_size)), len(all_Xs))\n                  for s in (s1, s2)]\n        n_frames = f2 - f1\n\n        # if we have enough for this observation\n        if len(X) + n_frames > max_sequence_length:\n\n            # make sure we have data\n            if len(X) and len(Y):\n                yield (np.array(X),\n                       np.array(Y))\n                # reset for next observation\n                X, Y = [], []\n\n        # if this observation is smaller than our max sequence length\n        if n_frames < max_sequence_length:\n            # Grab the data from this phone\n            for f in range(f1, f2):\n                X.append(all_Xs[f])\n            Y.append(ph[\'phone\'])\n            Y.append(\'_\')\n\n\ndef batch_generator(timit, batch_size=10, max_sequence_length=50):\n    """"""Summary\n\n    Parameters\n    ----------\n    timit : TYPE\n        Description\n    batch_size : int, optional\n        Description\n    max_sequence_length : int, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    data = timit[\'data\']\n\n    # randomize order\n    rand_idxs = np.random.permutation(range(len(data)))\n\n    # number of batchs\n    n_batches = len(data) // batch_size\n\n    seq_lens_Xs, seq_lens_Ys = [], []\n    Xs, Ys = [], []\n    batch_i = 0\n    while batch_i < n_batches:\n        # grab next random recording\n        el = rand_idxs[batch_i]\n        batch_i += 1\n\n        for X, Y in create_observation(data[el], max_sequence_length):\n            # keep track of the original sequence lengths\n            seq_lens_Xs.append(len(X))\n            seq_lens_Ys.append(len(Y))\n\n            # encode phones to integers\n            Y_enc = np.array([timit[\'encoder\'][y_i] for y_i in Y],\n                              dtype=np.int32)[np.newaxis]\n\n            # zero-pad to max sequence length\n            if len(X) < max_sequence_length:\n                X_pad = np.zeros((1, max_sequence_length, X.shape[-1]))\n                X_pad[:, :len(X), :] = X\n            else:\n                X_pad = X[np.newaxis]\n\n            # append to minibatches\n            if len(Xs):\n                Xs = np.r_[Xs, X_pad]\n                Ys = np.r_[Ys, Y_enc]\n            else:\n                Xs = X_pad\n                Ys = Y_enc\n\n            # we\'ve got enough observations for a minibatch\n            if len(Xs) == batch_size:\n                yield Xs, Ys, seq_lens_Xs, seq_lens_Ys\n                Xs, Ys = [], []\n                seq_lens_Xs, seq_lens_Ys = [], []\n\n\ndef sparse_tuple_from(sequences, dtype=np.int32):\n    """"""Summary\n\n    Parameters\n    ----------\n    sequences : TYPE\n        Description\n    dtype : TYPE, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    indices = []\n    values = []\n\n    for n, seq in enumerate(sequences):\n        indices.extend(zip([n] * len(seq), range(len(seq))))\n        values.extend(seq)\n\n    indices = np.asarray(indices, dtype=np.int64)\n    values = np.asarray(values, dtype=dtype)\n    shape = np.asarray([len(sequences),\n                        np.asarray(indices).max(0)[1] + 1], dtype=np.int64)\n\n    return indices, np.squeeze(values), shape\n\n\ndef build_model(batch_size=100, sequence_length=50, n_features=13,\n                n_cells=100, n_layers=2, n_classes=61, bi=True):\n    """"""Summary\n\n    Parameters\n    ----------\n    batch_size : int, optional\n        Description\n    sequence_length : int, optional\n        Description\n    n_features : int, optional\n        Description\n    n_cells : int, optional\n        Description\n    n_layers : int, optional\n        Description\n    n_classes : int, optional\n        Description\n    bi : bool, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    with tf.name_scope(\'input\'):\n        X = tf.placeholder(\n            tf.float32, shape=[None, sequence_length, n_features], name=\'X\')\n        seqlens_X = tf.placeholder(tf.int32, shape=[None], name=\'seqlens\')\n\n    with tf.name_scope(\'rnn\'):\n        # Create the rnn cells\n        forward_cells = tf.nn.rnn_cell.LSTMCell(\n            n_cells, use_peepholes=True, state_is_tuple=True)\n        if bi:\n            backward_cells = tf.nn.rnn_cell.LSTMCell(\n                n_cells, use_peepholes=True, state_is_tuple=True)\n\n        # If there are many layers\n        if n_layers > 1:\n            forward_cells = tf.nn.rnn_cell.MultiRNNCell(\n                [forward_cells] * n_layers, state_is_tuple=True)\n            if bi:\n                backward_cells = tf.nn.rnn_cell.MultiRNNCell(\n                    [backward_cells] * n_layers, state_is_tuple=True)\n\n        # Initial state\n        initial_state_fw = forward_cells.zero_state(\n            tf.shape(X)[0], tf.float32)\n        if bi:\n            initial_state_bw = backward_cells.zero_state(\n                tf.shape(X)[0], tf.float32)\n\n        # Connect it to the input\n        if bi:\n            outputs, output_states = \\\n                tf.nn.bidirectional_dynamic_rnn(\n                    forward_cells, backward_cells,\n                    X, tf.cast(seqlens_X, tf.int64),\n                    initial_state_fw, initial_state_bw)\n        else:\n            outputs, output_states = tf.nn.dynamic_rnn(\n                forward_cells, X, seqlens_X, initial_state_fw)\n\n        # Pack into [timesteps, batch_size, 2 * n_cells]\n        outputs = tf.pack(outputs)\n        # Reshape to  [timesteps * batch_size, 2 * n_cells]\n        outputs = tf.reshape(outputs, [-1, 2 * n_cells if bi else n_cells])\n\n    with tf.variable_scope(\'prediction\'):\n        W = tf.get_variable(\n            ""W"",\n            shape=[2 * n_cells if bi else n_cells, n_classes],\n            initializer=tf.random_normal_initializer(stddev=0.1))\n        b = tf.get_variable(\n            ""b"",\n            shape=[n_classes],\n            initializer=tf.random_normal_initializer(stddev=0.1))\n\n        # Find the output prediction of every single character in our minibatch\n        # we denote the pre-activation prediction, logits.\n        logits = tf.matmul(outputs, W) + b\n        logits = tf.reshape(logits, [sequence_length, -1, n_classes])\n\n    with tf.name_scope(\'output\'):\n        Y = tf.sparse_placeholder(tf.int32, name=\'Y\')\n        seqlens_Y = tf.placeholder(tf.int32, shape=[batch_size], name=\'seqlen\')\n\n    with tf.name_scope(\'loss\'):\n        losses = tf.nn.ctc_loss(logits, Y, seqlens_X,\n                                preprocess_collapse_repeated=True,\n                                ctc_merge_repeated=False)\n        cost = tf.reduce_mean(losses)\n\n    with tf.name_scope(\'decoder\'):\n        decoder, log_prob = tf.nn.ctc_beam_search_decoder(\n            logits, seqlens_Y)\n        acc = tf.reduce_mean(tf.edit_distance(tf.cast(\n            decoder[0], tf.int32), Y))\n\n    return {\'X\': X, \'Y\': Y, \'cost\': cost, \'decoder\': decoder, \'acc\': acc,\n            \'seqlens_X\': seqlens_X, \'seqlens_Y\': seqlens_Y}\n\n\ndef train():\n    """"""Summary\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    n_epochs = 10000\n    batch_size = 1\n    sequence_length = 80\n    ckpt_name = \'timit.ckpt\'\n\n    timit = parse_timit()\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess, g.as_default():\n        model = build_model(batch_size=batch_size,\n                            sequence_length=sequence_length,\n                            n_classes=len(timit[\'encoder\']) + 1)\n        opt = tf.train.AdamOptimizer().minimize(model[\'cost\'])\n        sess.run(tf.initialize_all_variables())\n        saver = tf.train.Saver()\n        if os.path.exists(ckpt_name):\n            saver.restore(sess, ckpt_name)\n            print(""Model restored."")\n        for epoch_i in range(n_epochs):\n            avg_acc = 0\n            avg_cost = 0\n            it_i = 0\n            for X, Y, seqlens_X, seqlens_Y in batch_generator(timit, batch_size, sequence_length):\n                feed_dict = {\n                    model[\'X\']: X,\n                    model[\'Y\']: sparse_tuple_from(Y),\n                    model[\'seqlens_X\']: seqlens_X,\n                    model[\'seqlens_Y\']: seqlens_Y\n                }\n                this_acc, this_cost, _ = sess.run(\n                    [model[\'acc\'], model[\'cost\'], opt], feed_dict=feed_dict)\n                it_i += 1\n                avg_acc += this_acc\n                avg_cost += this_cost\n            print(epoch_i, avg_acc / it_i, avg_cost / it_i)\n            # Save the variables to disk.\n            save_path = saver.save(sess, ""./"" + ckpt_name,\n                                   global_step=epoch_i,\n                                   write_meta_graph=True)\n            print(""Model saved in file: %s"" % save_path)\n\n\nif __name__ == \'__main__\':\n    train()\n'"
cadl/utils.py,74,"b'""""""Various utilities including downloading, common layers, etc..\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport urllib\nimport requests\nimport numpy as np\nimport zipfile\nimport os\nimport sys\nfrom scipy.io import wavfile\nimport contextlib\n\n\n@contextlib.contextmanager\ndef stdout_redirect(where):\n    """"""Summary\n    \n    Parameters\n    ----------\n    where : TYPE\n        Description\n    \n    Yields\n    ------\n    TYPE\n        Description\n    """"""\n    sys.stdout = where\n    try:\n        yield where\n    finally:\n        sys.stdout = sys.__stdout__\n\n\ndef exists(site):\n    """"""Summary\n    \n    Parameters\n    ----------\n    site : TYPE\n        Description\n    \n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    res = requests.head(site)\n    return res.ok\n\n\ndef download(path):\n    """"""Use urllib to download a file.\n    \n    Parameters\n    ----------\n    path : str\n        Url to download\n    \n    Returns\n    -------\n    path : str\n        Location of downloaded file.\n    """"""\n    fname = path.split(\'/\')[-1]\n    if os.path.exists(fname):\n        return fname\n\n    print(\'Downloading \' + path)\n    with open(fname, ""wb"") as f:\n        response = requests.get(path, stream=True)\n        total_length = response.headers.get(\'content-length\')\n        if total_length is None:\n            f.write(response.content)\n        else:\n            dl = 0\n            total_length = int(total_length)\n            for data in response.iter_content(chunk_size=4096):\n                dl += len(data)\n                f.write(data)\n                done = int(50 * dl / total_length)\n                sys.stdout.write(""\\r[%s%s] %.2f of %.2f MB"" % (\n                    \'=\' * done,\n                    \' \' * (50-done),\n                    dl / 1024.0 / 1024.0,\n                    total_length / 1024.0 / 1024.0))\n                sys.stdout.flush()\n    return fname\n\n\ndef download_and_extract_tar(path, dst=\'./\'):\n    """"""Download and extract a tar file.\n    \n    Parameters\n    ----------\n    path : str\n        Url to tar file to download.\n    dst : str\n        Location to save tar file contents.\n    """"""\n    import tarfile\n    filepath = download(path)\n    if not os.path.exists(dst):\n        os.makedirs(dst)\n        tarfile.open(filepath, \'r:gz\').extractall(dst)\n\n\ndef download_and_extract_zip(path, dst=\'./\'):\n    """"""Download and extract a zip file.\n    \n    Parameters\n    ----------\n    path : str\n        Url to zip file to download.\n    dst : str\n        Location to save zip file contents.\n    """"""\n    import zipfile\n    filepath = download(path)\n    if not os.path.exists(dst):\n        os.makedirs(dst)\n        zf = zipfile.ZipFile(file=filepath)\n        zf.extractall(dst)\n\n\ndef load_audio(filename, b_normalize=True):\n    """"""Load the audiofile at the provided filename using scipy.io.wavfile.\n    \n    Optionally normalizes the audio to the maximum value.\n    \n    Parameters\n    ----------\n    filename : str\n        File to load.\n    b_normalize : bool, optional\n        Normalize to the maximum value.\n    \n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    sr, s = wavfile.read(filename)\n    if b_normalize:\n        s = s.astype(np.float32)\n        s = (s / np.max(np.abs(s)))\n        s -= np.mean(s)\n    return s\n\n\ndef corrupt(x):\n    """"""Take an input tensor and add uniform masking.\n    \n    Parameters\n    ----------\n    x : Tensor/Placeholder\n        Input to corrupt.\n    \n    Returns\n    -------\n    x_corrupted : Tensor\n        50 pct of values corrupted.\n    """"""\n    return tf.mul(x, tf.cast(tf.random_uniform(shape=tf.shape(x),\n                                               minval=0,\n                                               maxval=2,\n                                               dtype=tf.int32), tf.float32))\n\n\ndef interp(l, r, n_samples):\n    """"""Intepolate between the arrays l and r, n_samples times.\n    \n    Parameters\n    ----------\n    l : np.ndarray\n        Left edge\n    r : np.ndarray\n        Right edge\n    n_samples : int\n        Number of samples\n    \n    Returns\n    -------\n    arr : np.ndarray\n        Inteporalted array\n    """"""\n    return np.array([\n        l + step_i / (n_samples - 1) * (r - l)\n        for step_i in range(n_samples)])\n\n\ndef make_latent_manifold(corners, n_samples):\n    """"""Create a 2d manifold out of the provided corners: n_samples * n_samples.\n    \n    Parameters\n    ----------\n    corners : list of np.ndarray\n        The four corners to intepolate.\n    n_samples : int\n        Number of samples to use in interpolation.\n    \n    Returns\n    -------\n    arr : np.ndarray\n        Stacked array of all 2D interpolated samples\n    """"""\n    left = interp(corners[0], corners[1], n_samples)\n    right = interp(corners[2], corners[3], n_samples)\n\n    embedding = []\n    for row_i in range(n_samples):\n        embedding.append(interp(left[row_i], right[row_i], n_samples))\n    return np.vstack(embedding)\n\n\ndef imcrop_tosquare(img):\n    """"""Make any image a square image.\n    \n    Parameters\n    ----------\n    img : np.ndarray\n        Input image to crop, assumed at least 2d.\n    \n    Returns\n    -------\n    crop : np.ndarray\n        Cropped image.\n    """"""\n    size = np.min(img.shape[:2])\n    extra = img.shape[:2] - size\n    crop = img\n    for i in np.flatnonzero(extra):\n        crop = np.take(crop, extra[i] // 2 + np.r_[:size], axis=i)\n    return crop\n\n\ndef slice_montage(montage, img_h, img_w, n_imgs):\n    """"""Slice a montage image into n_img h x w images.\n    \n    Performs the opposite of the montage function.  Takes a montage image and\n    slices it back into a N x H x W x C image.\n    \n    Parameters\n    ----------\n    montage : np.ndarray\n        Montage image to slice.\n    img_h : int\n        Height of sliced image\n    img_w : int\n        Width of sliced image\n    n_imgs : int\n        Number of images to slice\n    \n    Returns\n    -------\n    sliced : np.ndarray\n        Sliced images as 4d array.\n    """"""\n    sliced_ds = []\n    for i in range(int(np.sqrt(n_imgs))):\n        for j in range(int(np.sqrt(n_imgs))):\n            sliced_ds.append(montage[\n                1 + i + i * img_h:1 + i + (i + 1) * img_h,\n                1 + j + j * img_w:1 + j + (j + 1) * img_w])\n    return np.array(sliced_ds)\n\n\ndef montage(images, saveto=\'montage.png\'):\n    """"""Draw all images as a montage separated by 1 pixel borders.\n    \n    Also saves the file to the destination specified by `saveto`.\n    \n    Parameters\n    ----------\n    images : numpy.ndarray\n        Input array to create montage of.  Array should be:\n        batch x height x width x channels.\n    saveto : str\n        Location to save the resulting montage image.\n    \n    Returns\n    -------\n    m : numpy.ndarray\n        Montage image.\n    """"""\n    if isinstance(images, list):\n        images = np.array(images)\n    img_h = images.shape[1]\n    img_w = images.shape[2]\n    n_plots = int(np.ceil(np.sqrt(images.shape[0])))\n    if len(images.shape) == 4 and images.shape[3] == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 3)) * 0.5\n    else:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1)) * 0.5\n    for i in range(n_plots):\n        for j in range(n_plots):\n            this_filter = i * n_plots + j\n            if this_filter < images.shape[0]:\n                this_img = images[this_filter]\n                m[1 + i + i * img_h:1 + i + (i + 1) * img_h,\n                  1 + j + j * img_w:1 + j + (j + 1) * img_w] = this_img\n    plt.imsave(arr=m, fname=saveto)\n    return m\n\n\ndef montage_filters(W):\n    """"""Draws all filters (n_input * n_output filters) as a\n    montage image separated by 1 pixel borders.\n    \n    Parameters\n    ----------\n    W : Tensor\n        Input tensor to create montage of.\n    \n    Returns\n    -------\n    m : numpy.ndarray\n        Montage image.\n    """"""\n    W = np.reshape(W, [W.shape[0], W.shape[1], 1, W.shape[2] * W.shape[3]])\n    n_plots = int(np.ceil(np.sqrt(W.shape[-1])))\n    m = np.ones(\n        (W.shape[0] * n_plots + n_plots + 1,\n         W.shape[1] * n_plots + n_plots + 1)) * 0.5\n    for i in range(n_plots):\n        for j in range(n_plots):\n            this_filter = i * n_plots + j\n            if this_filter < W.shape[-1]:\n                m[1 + i + i * W.shape[0]:1 + i + (i + 1) * W.shape[0],\n                  1 + j + j * W.shape[1]:1 + j + (j + 1) * W.shape[1]] = (\n                    np.squeeze(W[:, :, :, this_filter]))\n    return m\n\n\ndef get_celeb_files(dst=\'img_align_celeba\', max_images=100):\n    """"""Download the first 100 images of the celeb dataset.\n    \n    Files will be placed in a directory \'img_align_celeba\' if one\n    doesn\'t exist.\n    \n    Returns\n    -------\n    files : list of strings\n        Locations to the first 100 images of the celeb net dataset.\n    \n    Parameters\n    ----------\n    dst : str, optional\n        Description\n    max_images : int, optional\n        Description\n    """"""\n    # Create a directory\n    if not os.path.exists(dst):\n        os.mkdir(dst)\n\n    # Now perform the following 100 times:\n    for img_i in range(1, max_images + 1):\n\n        # create a string using the current loop counter\n        f = \'000%03d.jpg\' % img_i\n\n        if not os.path.exists(os.path.join(dst, f)):\n\n            # and get the url with that string appended the end\n            url = \'https://s3.amazonaws.com/cadl/celeb-align/\' + f\n\n            # We\'ll print this out to the console so we can see how far we\'ve gone\n            print(url, end=\'\\r\')\n\n            # And now download the url to a location inside our new directory\n            urllib.request.urlretrieve(url, os.path.join(dst, f))\n\n    files = [os.path.join(dst, file_i)\n             for file_i in os.listdir(dst)\n             if \'.jpg\' in file_i][:max_images]\n    return files\n\n\ndef get_celeb_imgs(max_images=100):\n    """"""Load the first `max_images` images of the celeb dataset.\n    \n    Returns\n    -------\n    imgs : list of np.ndarray\n        List of the first 100 images from the celeb dataset\n    \n    Parameters\n    ----------\n    max_images : int, optional\n        Description\n    """"""\n    return [plt.imread(f_i) for f_i in get_celeb_files(max_images=max_images)]\n\n\ndef gauss(mean, stddev, ksize):\n    """"""Use Tensorflow to compute a Gaussian Kernel.\n    \n    Parameters\n    ----------\n    mean : float\n        Mean of the Gaussian (e.g. 0.0).\n    stddev : float\n        Standard Deviation of the Gaussian (e.g. 1.0).\n    ksize : int\n        Size of kernel (e.g. 16).\n    \n    Returns\n    -------\n    kernel : np.ndarray\n        Computed Gaussian Kernel using Tensorflow.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        x = tf.linspace(-3.0, 3.0, ksize)\n        z = (tf.exp(tf.neg(tf.pow(x - mean, 2.0) /\n                           (2.0 * tf.pow(stddev, 2.0)))) *\n             (1.0 / (stddev * tf.sqrt(2.0 * 3.1415))))\n        return z.eval()\n\n\ndef gauss2d(mean, stddev, ksize):\n    """"""Use Tensorflow to compute a 2D Gaussian Kernel.\n    \n    Parameters\n    ----------\n    mean : float\n        Mean of the Gaussian (e.g. 0.0).\n    stddev : float\n        Standard Deviation of the Gaussian (e.g. 1.0).\n    ksize : int\n        Size of kernel (e.g. 16).\n    \n    Returns\n    -------\n    kernel : np.ndarray\n        Computed 2D Gaussian Kernel using Tensorflow.\n    """"""\n    z = gauss(mean, stddev, ksize)\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        z_2d = tf.matmul(tf.reshape(z, [ksize, 1]), tf.reshape(z, [1, ksize]))\n        return z_2d.eval()\n\n\ndef convolve(img, kernel):\n    """"""Use Tensorflow to convolve a 4D image with a 4D kernel.\n    \n    Parameters\n    ----------\n    img : np.ndarray\n        4-dimensional image shaped N x H x W x C\n    kernel : np.ndarray\n        4-dimensional image shape K_H, K_W, C_I, C_O corresponding to the\n        kernel\'s height and width, the number of input channels, and the\n        number of output channels.  Note that C_I should = C.\n    \n    Returns\n    -------\n    result : np.ndarray\n        Convolved result.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        convolved = tf.nn.conv2d(img, kernel, strides=[1, 1, 1, 1], padding=\'SAME\')\n        res = convolved.eval()\n    return res\n\n\ndef gabor(ksize=32):\n    """"""Use Tensorflow to compute a 2D Gabor Kernel.\n    \n    Parameters\n    ----------\n    ksize : int, optional\n        Size of kernel.\n    \n    Returns\n    -------\n    gabor : np.ndarray\n        Gabor kernel with ksize x ksize dimensions.\n    """"""\n    g = tf.Graph()\n    with tf.Session(graph=g):\n        z_2d = gauss2d(0.0, 1.0, ksize)\n        ones = tf.ones((1, ksize))\n        ys = tf.sin(tf.linspace(-3.0, 3.0, ksize))\n        ys = tf.reshape(ys, [ksize, 1])\n        wave = tf.matmul(ys, ones)\n        gabor = tf.mul(wave, z_2d)\n        return gabor.eval()\n\n\ndef build_submission(filename, file_list, optional_file_list=()):\n    """"""Helper utility to check homework assignment submissions and package them.\n    \n    Parameters\n    ----------\n    filename : str\n        Output zip file name\n    file_list : tuple\n        Tuple of files to include\n    optional_file_list : tuple, optional\n        Description\n    """"""\n    # check each file exists\n    for part_i, file_i in enumerate(file_list):\n        if not os.path.exists(file_i):\n            print(\'\\nYou are missing the file {}.  \'.format(file_i) +\n                  \'It does not look like you have completed Part {}.\'.format(\n                part_i + 1))\n\n    def zipdir(path, zf):\n        """"""Summary\n        \n        Parameters\n        ----------\n        path : TYPE\n            Description\n        zf : TYPE\n            Description\n        """"""\n        for root, dirs, files in os.walk(path):\n            for file in files:\n                # make sure the files are part of the necessary file list\n                if file.endswith(file_list) or file.endswith(optional_file_list):\n                    zf.write(os.path.join(root, file))\n\n    # create a zip file with the necessary files\n    zipf = zipfile.ZipFile(filename, \'w\', zipfile.ZIP_DEFLATED)\n    zipdir(\'.\', zipf)\n    zipf.close()\n    print(\'Your assignment zip file has been created!\')\n    print(\'Now submit the file:\\n{}\\nto Kadenze for grading!\'.format(\n        os.path.abspath(filename)))\n\n\ndef normalize(a, s=0.1):\n    \'\'\'Normalize the image range for visualization\n    \n    Parameters\n    ----------\n    a : TYPE\n        Description\n    s : float, optional\n        Description\n    \n    Returns\n    -------\n    TYPE\n        Description\n    \'\'\'\n    return np.uint8(np.clip(\n        (a - a.mean()) / max(a.std(), 1e-4) * s + 0.5,\n        0, 1) * 255)\n\n\n# %%\ndef weight_variable(shape, **kwargs):\n    \'\'\'Helper function to create a weight variable initialized with\n    a normal distribution\n    \n    Parameters\n    ----------\n    shape : list\n        Size of weight variable\n    **kwargs\n        Description\n    \n    Returns\n    -------\n    TYPE\n        Description\n    \'\'\'\n    if isinstance(shape, list):\n        initial = tf.random_normal(tf.stack(shape), mean=0.0, stddev=0.01)\n        initial.set_shape(shape)\n    else:\n        initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n    return tf.Variable(initial, **kwargs)\n\n\n# %%\ndef bias_variable(shape, **kwargs):\n    \'\'\'Helper function to create a bias variable initialized with\n    a constant value.\n    \n    Parameters\n    ----------\n    shape : list\n        Size of weight variable\n    **kwargs\n        Description\n    \n    Returns\n    -------\n    TYPE\n        Description\n    \'\'\'\n    if isinstance(shape, list):\n        initial = tf.random_normal(tf.stack(shape), mean=0.0, stddev=0.01)\n        initial.set_shape(shape)\n    else:\n        initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n    return tf.Variable(initial, **kwargs)\n\n\ndef binary_cross_entropy(z, x, name=None):\n    """"""Binary Cross Entropy measures cross entropy of a binary variable.\n    \n    loss(x, z) = - sum_i (x[i] * log(z[i]) + (1 - x[i]) * log(1 - z[i]))\n    \n    Parameters\n    ----------\n    z : tf.Tensor\n        A `Tensor` of the same type and shape as `x`.\n    x : tf.Tensor\n        A `Tensor` of type `float32` or `float64`.\n    name : None, optional\n        Description\n    \n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    with tf.variable_scope(name or \'bce\'):\n        eps = 1e-12\n        return (-(x * tf.log(z + eps) +\n                  (1. - x) * tf.log(1. - z + eps)))\n\n\ndef conv2d(x, n_output,\n           k_h=5, k_w=5, d_h=2, d_w=2,\n           padding=\'SAME\', name=\'conv2d\', reuse=None):\n    """"""Helper for creating a 2d convolution operation.\n    \n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to convolve.\n    n_output : int\n        Number of filters.\n    k_h : int, optional\n        Kernel height\n    k_w : int, optional\n        Kernel width\n    d_h : int, optional\n        Height stride\n    d_w : int, optional\n        Width stride\n    padding : str, optional\n        Padding type: ""SAME"" or ""VALID""\n    name : str, optional\n        Variable scope\n    reuse : None, optional\n        Description\n    \n    Returns\n    -------\n    op : tf.Tensor\n        Output of convolution\n    """"""\n    with tf.variable_scope(name or \'conv2d\', reuse=reuse):\n        W = tf.get_variable(\n            name=\'W\',\n            shape=[k_h, k_w, x.get_shape()[-1], n_output],\n            initializer=tf.contrib.layers.xavier_initializer_conv2d())\n\n        conv = tf.nn.conv2d(\n            name=\'conv\',\n            input=x,\n            filter=W,\n            strides=[1, d_h, d_w, 1],\n            padding=padding)\n\n        b = tf.get_variable(\n            name=\'b\',\n            shape=[n_output],\n            initializer=tf.constant_initializer(0.0))\n\n        h = tf.nn.bias_add(\n            name=\'h\',\n            value=conv,\n            bias=b)\n\n    return h, W\n\n\ndef deconv2d(x, n_output_h, n_output_w, n_output_ch, n_input_ch=None,\n             k_h=5, k_w=5, d_h=2, d_w=2,\n             padding=\'SAME\', name=\'deconv2d\', reuse=None):\n    """"""Deconvolution helper.\n    \n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to convolve.\n    n_output_h : int\n        Height of output\n    n_output_w : int\n        Width of output\n    n_output_ch : int\n        Number of filters.\n    n_input_ch : None, optional\n        Description\n    k_h : int, optional\n        Kernel height\n    k_w : int, optional\n        Kernel width\n    d_h : int, optional\n        Height stride\n    d_w : int, optional\n        Width stride\n    padding : str, optional\n        Padding type: ""SAME"" or ""VALID""\n    name : str, optional\n        Variable scope\n    reuse : None, optional\n        Description\n    \n    Returns\n    -------\n    op : tf.Tensor\n        Output of deconvolution\n    """"""\n    with tf.variable_scope(name or \'deconv2d\', reuse=reuse):\n        W = tf.get_variable(\n            name=\'W\',\n            shape=[k_h, k_h, n_output_ch, n_input_ch or x.get_shape()[-1]],\n            initializer=tf.contrib.layers.xavier_initializer_conv2d())\n\n        conv = tf.nn.conv2d_transpose(\n            name=\'conv_t\',\n            value=x,\n            filter=W,\n            output_shape=tf.stack(\n                [tf.shape(x)[0], n_output_h, n_output_w, n_output_ch]),\n            strides=[1, d_h, d_w, 1],\n            padding=padding)\n\n        conv.set_shape([None, n_output_h, n_output_w, n_output_ch])\n\n        b = tf.get_variable(\n            name=\'b\',\n            shape=[n_output_ch],\n            initializer=tf.constant_initializer(0.0))\n\n        h = tf.nn.bias_add(name=\'h\', value=conv, bias=b)\n\n    return h, W\n\n\ndef lrelu(features, leak=0.2):\n    """"""Leaky rectifier.\n    \n    Parameters\n    ----------\n    features : tf.Tensor\n        Input to apply leaky rectifier to.\n    leak : float, optional\n        Percentage of leak.\n    \n    Returns\n    -------\n    op : tf.Tensor\n        Resulting output of applying leaky rectifier activation.\n    """"""\n    f1 = 0.5 * (1 + leak)\n    f2 = 0.5 * (1 - leak)\n    return f1 * features + f2 * abs(features)\n\n\ndef linear(x, n_output, name=None, activation=None, reuse=None):\n    """"""Fully connected layer.\n    \n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to connect\n    n_output : int\n        Number of output neurons\n    name : None, optional\n        Scope to apply\n    activation : None, optional\n        Description\n    reuse : None, optional\n        Description\n    \n    Returns\n    -------\n    h, W : tf.Tensor, tf.Tensor\n        Output of fully connected layer and the weight matrix\n    """"""\n    if len(x.get_shape()) != 2:\n        x = flatten(x, reuse=reuse)\n\n    n_input = x.get_shape().as_list()[1]\n\n    with tf.variable_scope(name or ""fc"", reuse=reuse):\n        W = tf.get_variable(\n            name=\'W\',\n            shape=[n_input, n_output],\n            dtype=tf.float32,\n            initializer=tf.contrib.layers.xavier_initializer())\n\n        b = tf.get_variable(\n            name=\'b\',\n            shape=[n_output],\n            dtype=tf.float32,\n            initializer=tf.constant_initializer(0.0))\n\n        h = tf.nn.bias_add(\n            name=\'h\',\n            value=tf.matmul(x, W),\n            bias=b)\n\n        if activation:\n            h = activation(h)\n\n        return h, W\n\n\ndef flatten(x, name=None, reuse=None):\n    """"""Flatten Tensor to 2-dimensions.\n    \n    Parameters\n    ----------\n    x : tf.Tensor\n        Input tensor to flatten.\n    name : None, optional\n        Variable scope for flatten operations\n    reuse : None, optional\n        Description\n    \n    Returns\n    -------\n    flattened : tf.Tensor\n        Flattened tensor.\n    \n    Raises\n    ------\n    ValueError\n        Description\n    """"""\n    with tf.variable_scope(\'flatten\'):\n        dims = x.get_shape().as_list()\n        if len(dims) == 4:\n            flattened = tf.reshape(\n                x,\n                shape=[-1, dims[1] * dims[2] * dims[3]])\n        elif len(dims) == 2 or len(dims) == 1:\n            flattened = x\n        else:\n            raise ValueError(\'Expected n dimensions of 1, 2 or 4.  Found:\',\n                             len(dims))\n\n        return flattened\n\n\ndef to_tensor(x):\n    """"""Convert 2 dim Tensor to a 4 dim Tensor ready for convolution.\n    \n    Performs the opposite of flatten(x).  If the tensor is already 4-D, this\n    returns the same as the input, leaving it unchanged.\n    \n    Parameters\n    ----------\n    x : tf.Tesnor\n        Input 2-D tensor.  If 4-D already, left unchanged.\n    \n    Returns\n    -------\n    x : tf.Tensor\n        4-D representation of the input.\n    \n    Raises\n    ------\n    ValueError\n        If the tensor is not 2D or already 4D.\n    """"""\n    if len(x.get_shape()) == 2:\n        n_input = x.get_shape().as_list()[1]\n        x_dim = np.sqrt(n_input)\n        if x_dim == int(x_dim):\n            x_dim = int(x_dim)\n            x_tensor = tf.reshape(\n                x, [-1, x_dim, x_dim, 1], name=\'reshape\')\n        elif np.sqrt(n_input / 3) == int(np.sqrt(n_input / 3)):\n            x_dim = int(np.sqrt(n_input / 3))\n            x_tensor = tf.reshape(\n                x, [-1, x_dim, x_dim, 3], name=\'reshape\')\n        else:\n            x_tensor = tf.reshape(\n                x, [-1, 1, 1, n_input], name=\'reshape\')\n    elif len(x.get_shape()) == 4:\n        x_tensor = x\n    else:\n        raise ValueError(\'Unsupported input dimensions\')\n    return x_tensor\n\n\ndef sample_categorical(pmf):\n    """"""Sample from a categorical distribution.\n    \n    Parameters\n    ----------\n    pmf\n        Probablity mass function. Output of a softmax over categories.\n        Array of shape [batch_size, number of categories]. Rows sum to 1.\n    \n    Returns\n    -------\n    idxs\n        Array of size [batch_size, 1]. Integer of category sampled.\n    """"""\n    if pmf.ndim == 1:\n        pmf = np.expand_dims(pmf, 0)\n    batch_size = pmf.shape[0]\n    cdf = np.cumsum(pmf, axis=1)\n    rand_vals = np.random.rand(batch_size)\n    idxs = np.zeros([batch_size,])\n    for i in range(batch_size):\n        idxs[i] = cdf[i].searchsorted(rand_vals[i])\n    return idxs\n\n'"
cadl/vae.py,41,"b'""""""Convolutional/Variational autoencoder, including demonstration of\ntraining such a network on MNIST, CelebNet and the film, ""Sita Sings The Blues""\nusing an image pipeline.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport os\nfrom cadl.dataset_utils import create_input_pipeline\nfrom cadl.datasets import CELEB, MNIST\nfrom cadl.batch_norm import batch_norm\nfrom cadl import utils\n\n\ndef VAE(input_shape=[None, 784],\n        n_filters=[64, 64, 64],\n        filter_sizes=[4, 4, 4],\n        n_hidden=32,\n        n_code=2,\n        activation=tf.nn.tanh,\n        dropout=False,\n        denoising=False,\n        convolutional=False,\n        variational=False):\n    """"""(Variational) (Convolutional) (Denoising) Autoencoder.\n\n    Uses tied weights.\n\n    Parameters\n    ----------\n    input_shape : list, optional\n        Shape of the input to the network. e.g. for MNIST: [None, 784].\n    n_filters : list, optional\n        Number of filters for each layer.\n        If convolutional=True, this refers to the total number of output\n        filters to create for each layer, with each layer\'s number of output\n        filters as a list.\n        If convolutional=False, then this refers to the total number of neurons\n        for each layer in a fully connected network.\n    filter_sizes : list, optional\n        Only applied when convolutional=True.  This refers to the ksize (height\n        and width) of each convolutional layer.\n    n_hidden : int, optional\n        Only applied when variational=True.  This refers to the first fully\n        connected layer prior to the variational embedding, directly after\n        the encoding.  After the variational embedding, another fully connected\n        layer is created with the same size prior to decoding.  Set to 0 to\n        not use an additional hidden layer.\n    n_code : int, optional\n        Only applied when variational=True.  This refers to the number of\n        latent Gaussians to sample for creating the inner most encoding.\n    activation : function, optional\n        Activation function to apply to each layer, e.g. tf.nn.relu\n    dropout : bool, optional\n        Whether or not to apply dropout.  If using dropout, you must feed a\n        value for \'keep_prob\', as returned in the dictionary.  1.0 means no\n        dropout is used.  0.0 means every connection is dropped.  Sensible\n        values are between 0.5-0.8.\n    denoising : bool, optional\n        Whether or not to apply denoising.  If using denoising, you must feed a\n        value for \'corrupt_prob\', as returned in the dictionary.  1.0 means no\n        corruption is used.  0.0 means every feature is corrupted.  Sensible\n        values are between 0.5-0.8.\n    convolutional : bool, optional\n        Whether or not to use a convolutional network or else a fully connected\n        network will be created.  This effects the n_filters parameter\'s\n        meaning.\n    variational : bool, optional\n        Whether or not to create a variational embedding layer.  This will\n        create a fully connected layer after the encoding, if `n_hidden` is\n        greater than 0, then will create a multivariate gaussian sampling\n        layer, then another fully connected layer.  The size of the fully\n        connected layers are determined by `n_hidden`, and the size of the\n        sampling layer is determined by `n_code`.\n\n    Returns\n    -------\n    model : dict\n        {\n            \'cost\': Tensor to optimize.\n            \'Ws\': All weights of the encoder.\n            \'x\': Input Placeholder\n            \'z\': Inner most encoding Tensor (latent features)\n            \'y\': Reconstruction of the Decoder\n            \'keep_prob\': Amount to keep when using Dropout\n            \'corrupt_prob\': Amount to corrupt when using Denoising\n            \'train\': Set to True when training/Applies to Batch Normalization.\n        }\n    """"""\n    # network input / placeholders for train (bn) and dropout\n    x = tf.placeholder(tf.float32, input_shape, \'x\')\n    phase_train = tf.placeholder(tf.bool, name=\'phase_train\')\n    keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n    corrupt_prob = tf.placeholder(tf.float32, [1])\n\n    if denoising:\n        current_input = utils.corrupt(x) * corrupt_prob + x * (1 - corrupt_prob)\n\n    # 2d -> 4d if convolution\n    x_tensor = utils.to_tensor(x) if convolutional else x\n    current_input = x_tensor\n\n    Ws = []\n    shapes = []\n\n    # Build the encoder\n    for layer_i, n_output in enumerate(n_filters):\n        with tf.variable_scope(\'encoder/{}\'.format(layer_i)):\n            shapes.append(current_input.get_shape().as_list())\n            if convolutional:\n                h, W = utils.conv2d(\n                    x=current_input,\n                    n_output=n_output,\n                    k_h=filter_sizes[layer_i],\n                    k_w=filter_sizes[layer_i])\n            else:\n                h, W = utils.linear(x=current_input, n_output=n_output)\n            h = activation(batch_norm(h, phase_train, \'bn\' + str(layer_i)))\n            if dropout:\n                h = tf.nn.dropout(h, keep_prob)\n            Ws.append(W)\n            current_input = h\n\n    shapes.append(current_input.get_shape().as_list())\n\n    with tf.variable_scope(\'variational\'):\n        if variational:\n            dims = current_input.get_shape().as_list()\n            flattened = utils.flatten(current_input)\n\n            if n_hidden:\n                h = utils.linear(flattened, n_hidden, name=\'W_fc\')[0]\n                h = activation(batch_norm(h, phase_train, \'fc/bn\'))\n                if dropout:\n                    h = tf.nn.dropout(h, keep_prob)\n            else:\n                h = flattened\n\n            z_mu = utils.linear(h, n_code, name=\'mu\')[0]\n            z_log_sigma = 0.5 * utils.linear(h, n_code, name=\'log_sigma\')[0]\n\n            # Sample from noise distribution p(eps) ~ N(0, 1)\n            epsilon = tf.random_normal(tf.stack([tf.shape(x)[0], n_code]))\n\n            # Sample from posterior\n            z = z_mu + tf.multiply(epsilon, tf.exp(z_log_sigma))\n\n            if n_hidden:\n                h = utils.linear(z, n_hidden, name=\'fc_t\')[0]\n                h = activation(batch_norm(h, phase_train, \'fc_t/bn\'))\n                if dropout:\n                    h = tf.nn.dropout(h, keep_prob)\n            else:\n                h = z\n\n            size = dims[1] * dims[2] * dims[3] if convolutional else dims[1]\n            h = utils.linear(h, size, name=\'fc_t2\')[0]\n            current_input = activation(batch_norm(h, phase_train, \'fc_t2/bn\'))\n            if dropout:\n                current_input = tf.nn.dropout(current_input, keep_prob)\n\n            if convolutional:\n                current_input = tf.reshape(current_input,\n                                           tf.stack([\n                                               tf.shape(current_input)[0],\n                                               dims[1], dims[2], dims[3]\n                                           ]))\n        else:\n            z = current_input\n\n    shapes.reverse()\n    n_filters.reverse()\n    Ws.reverse()\n\n    n_filters += [input_shape[-1]]\n\n    # %%\n    # Decoding layers\n    for layer_i, n_output in enumerate(n_filters[1:]):\n        with tf.variable_scope(\'decoder/{}\'.format(layer_i)):\n            shape = shapes[layer_i + 1]\n            if convolutional:\n                h, W = utils.deconv2d(\n                    x=current_input,\n                    n_output_h=shape[1],\n                    n_output_w=shape[2],\n                    n_output_ch=shape[3],\n                    n_input_ch=shapes[layer_i][3],\n                    k_h=filter_sizes[layer_i],\n                    k_w=filter_sizes[layer_i])\n            else:\n                h, W = utils.linear(x=current_input, n_output=n_output)\n            h = activation(batch_norm(h, phase_train, \'dec/bn\' + str(layer_i)))\n            if dropout:\n                h = tf.nn.dropout(h, keep_prob)\n            current_input = h\n\n    y = current_input\n    x_flat = utils.flatten(x)\n    y_flat = utils.flatten(y)\n\n    # l2 loss\n    loss_x = tf.reduce_sum(tf.squared_difference(x_flat, y_flat), 1)\n\n    if variational:\n        # variational lower bound, kl-divergence\n        loss_z = -0.5 * tf.reduce_sum(1.0 + 2.0 * z_log_sigma - tf.square(z_mu)\n                                      - tf.exp(2.0 * z_log_sigma), 1)\n\n        # add l2 loss\n        cost = tf.reduce_mean(loss_x + loss_z)\n    else:\n        # just optimize l2 loss\n        cost = tf.reduce_mean(loss_x)\n\n    return {\n        \'cost\': cost,\n        \'Ws\': Ws,\n        \'x\': x,\n        \'z\': z,\n        \'y\': y,\n        \'keep_prob\': keep_prob,\n        \'corrupt_prob\': corrupt_prob,\n        \'train\': phase_train\n    }\n\n\ndef train_vae(files,\n              input_shape,\n              learning_rate=0.0001,\n              batch_size=100,\n              n_epochs=50,\n              n_examples=10,\n              crop_shape=[64, 64, 3],\n              crop_factor=0.8,\n              n_filters=[100, 100, 100, 100],\n              n_hidden=256,\n              n_code=50,\n              convolutional=True,\n              variational=True,\n              filter_sizes=[3, 3, 3, 3],\n              dropout=True,\n              keep_prob=0.8,\n              activation=tf.nn.relu,\n              img_step=100,\n              save_step=100,\n              ckpt_name=""vae.ckpt""):\n    """"""General purpose training of a (Variational) (Convolutional) Autoencoder.\n\n    Supply a list of file paths to images, and this will do everything else.\n\n    Parameters\n    ----------\n    files : list of strings\n        List of paths to images.\n    input_shape : list\n        Must define what the input image\'s shape is.\n    learning_rate : float, optional\n        Learning rate.\n    batch_size : int, optional\n        Batch size.\n    n_epochs : int, optional\n        Number of epochs.\n    n_examples : int, optional\n        Number of example to use while demonstrating the current training\n        iteration\'s reconstruction.  Creates a square montage, so make\n        sure int(sqrt(n_examples))**2 = n_examples, e.g. 16, 25, 36, ... 100.\n    crop_shape : list, optional\n        Size to centrally crop the image to.\n    crop_factor : float, optional\n        Resize factor to apply before cropping.\n    n_filters : list, optional\n        Same as VAE\'s n_filters.\n    n_hidden : int, optional\n        Same as VAE\'s n_hidden.\n    n_code : int, optional\n        Same as VAE\'s n_code.\n    convolutional : bool, optional\n        Use convolution or not.\n    variational : bool, optional\n        Use variational layer or not.\n    filter_sizes : list, optional\n        Same as VAE\'s filter_sizes.\n    dropout : bool, optional\n        Use dropout or not\n    keep_prob : float, optional\n        Percent of keep for dropout.\n    activation : function, optional\n        Which activation function to use.\n    img_step : int, optional\n        How often to save training images showing the manifold and\n        reconstruction.\n    save_step : int, optional\n        How often to save checkpoints.\n    ckpt_name : str, optional\n        Checkpoints will be named as this, e.g. \'model.ckpt\'\n    """"""\n    batch = create_input_pipeline(\n        files=files,\n        batch_size=batch_size,\n        n_epochs=n_epochs,\n        crop_shape=crop_shape,\n        crop_factor=crop_factor,\n        shape=input_shape)\n\n    ae = VAE(\n        input_shape=[None] + crop_shape,\n        convolutional=convolutional,\n        variational=variational,\n        n_filters=n_filters,\n        n_hidden=n_hidden,\n        n_code=n_code,\n        dropout=dropout,\n        filter_sizes=filter_sizes,\n        activation=activation)\n\n    # Create a manifold of our inner most layer to show\n    # example reconstructions.  This is one way to see\n    # what the ""embedding"" or ""latent space"" of the encoder\n    # is capable of encoding, though note that this is just\n    # a random hyperplane within the latent space, and does not\n    # encompass all possible embeddings.\n    zs = np.random.uniform(-1.0, 1.0, [4, n_code]).astype(np.float32)\n    zs = utils.make_latent_manifold(zs, n_examples)\n\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate=learning_rate).minimize(ae[\'cost\'])\n\n    # We create a session to use the graph\n    sess = tf.Session()\n    saver = tf.train.Saver()\n    init_op = tf.group(tf.global_variables_initializer(),\n                       tf.local_variables_initializer())\n    sess.run(init_op)\n\n    # This will handle our threaded image pipeline\n    coord = tf.train.Coordinator()\n\n    # Ensure no more changes to graph\n    tf.get_default_graph().finalize()\n\n    # Start up the queues for handling the image pipeline\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    if os.path.exists(ckpt_name + \'.index\') or os.path.exists(ckpt_name):\n        saver.restore(sess, ckpt_name)\n\n    # Fit all training data\n    t_i = 0\n    batch_i = 0\n    epoch_i = 0\n    cost = 0\n    n_files = len(files)\n    test_xs = sess.run(batch) / 255.0\n    utils.montage(test_xs, \'test_xs.png\')\n    try:\n        while not coord.should_stop() and epoch_i < n_epochs:\n            batch_i += 1\n            batch_xs = sess.run(batch) / 255.0\n            train_cost = sess.run(\n                [ae[\'cost\'], optimizer],\n                feed_dict={\n                    ae[\'x\']: batch_xs,\n                    ae[\'train\']: True,\n                    ae[\'keep_prob\']: keep_prob\n                })[0]\n            print(batch_i, train_cost)\n            cost += train_cost\n            if batch_i % n_files == 0:\n                print(\'epoch:\', epoch_i)\n                print(\'average cost:\', cost / batch_i)\n                cost = 0\n                batch_i = 0\n                epoch_i += 1\n\n            if batch_i % img_step == 0:\n                # Plot example reconstructions from latent layer\n                recon = sess.run(\n                    ae[\'y\'],\n                    feed_dict={\n                        ae[\'z\']: zs,\n                        ae[\'train\']: False,\n                        ae[\'keep_prob\']: 1.0\n                    })\n                utils.montage(\n                    recon.reshape([-1] + crop_shape), \'manifold_%08d.png\' % t_i)\n\n                # Plot example reconstructions\n                recon = sess.run(\n                    ae[\'y\'],\n                    feed_dict={\n                        ae[\'x\']: test_xs,\n                        ae[\'train\']: False,\n                        ae[\'keep_prob\']: 1.0\n                    })\n                print(\'reconstruction (min, max, mean):\',\n                      recon.min(), recon.max(), recon.mean())\n                utils.montage(\n                    recon.reshape([-1] + crop_shape),\n                    \'reconstruction_%08d.png\' % t_i)\n                t_i += 1\n\n            if batch_i % save_step == 0:\n                # Save the variables to disk.\n                saver.save(\n                    sess,\n                    ckpt_name,\n                    global_step=batch_i,\n                    write_meta_graph=False)\n    except tf.errors.OutOfRangeError:\n        print(\'Done.\')\n    finally:\n        # One of the threads has issued an exception.  So let\'s tell all the\n        # threads to shutdown.\n        coord.request_stop()\n\n    # Wait until all threads have finished.\n    coord.join(threads)\n\n    # Clean up the session.\n    sess.close()\n\n\n# %%\ndef test_mnist():\n    """"""Train an autoencoder on MNIST.\n\n    This function will train an autoencoder on MNIST and also\n    save many image files during the training process, demonstrating\n    the latent space of the inner most dimension of the encoder,\n    as well as reconstructions of the decoder.\n    """"""\n\n    # load MNIST\n    n_code = 2\n    mnist = MNIST(split=[0.8, 0.1, 0.1])\n    ae = VAE(\n        input_shape=[None, 784],\n        n_filters=[512, 256],\n        n_hidden=64,\n        n_code=n_code,\n        activation=tf.nn.sigmoid,\n        convolutional=False,\n        variational=True)\n\n    n_examples = 100\n    zs = np.random.uniform(-1.0, 1.0, [4, n_code]).astype(np.float32)\n    zs = utils.make_latent_manifold(zs, n_examples)\n\n    learning_rate = 0.02\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate=learning_rate).minimize(ae[\'cost\'])\n\n    # We create a session to use the graph\n    sess = tf.Session()\n    init_op = tf.group(tf.global_variables_initializer(),\n                       tf.local_variables_initializer())\n    sess.run(init_op)\n\n    # Fit all training data\n    t_i = 0\n    batch_i = 0\n    batch_size = 200\n    n_epochs = 10\n    test_xs = mnist.test.images[:n_examples]\n    utils.montage(test_xs.reshape((-1, 28, 28)), \'test_xs.png\')\n    for epoch_i in range(n_epochs):\n        train_i = 0\n        train_cost = 0\n        for batch_xs, _ in mnist.train.next_batch(batch_size):\n            train_cost += sess.run(\n                [ae[\'cost\'], optimizer],\n                feed_dict={\n                    ae[\'x\']: batch_xs,\n                    ae[\'train\']: True,\n                    ae[\'keep_prob\']: 1.0\n                })[0]\n            train_i += 1\n            if batch_i % 10 == 0:\n                # Plot example reconstructions from latent layer\n                recon = sess.run(\n                    ae[\'y\'],\n                    feed_dict={\n                        ae[\'z\']: zs,\n                        ae[\'train\']: False,\n                        ae[\'keep_prob\']: 1.0\n                    })\n                utils.montage(\n                    recon.reshape((-1, 28, 28)), \'manifold_%08d.png\' % t_i)\n                # Plot example reconstructions\n                recon = sess.run(\n                    ae[\'y\'],\n                    feed_dict={\n                        ae[\'x\']: test_xs,\n                        ae[\'train\']: False,\n                        ae[\'keep_prob\']: 1.0\n                    })\n                utils.montage(\n                    recon.reshape((-1, 28, 28)),\n                    \'reconstruction_%08d.png\' % t_i)\n                t_i += 1\n            batch_i += 1\n\n        valid_i = 0\n        valid_cost = 0\n        for batch_xs, _ in mnist.valid.next_batch(batch_size):\n            valid_cost += sess.run(\n                [ae[\'cost\']],\n                feed_dict={\n                    ae[\'x\']: batch_xs,\n                    ae[\'train\']: False,\n                    ae[\'keep_prob\']: 1.0\n                })[0]\n            valid_i += 1\n        print(\'train:\', train_cost / train_i, \'valid:\', valid_cost / valid_i)\n\n\ndef test_celeb():\n    """"""Train an autoencoder on Celeb Net.\n    """"""\n    files = CELEB()\n    train_vae(\n        files=files,\n        input_shape=[218, 178, 3],\n        batch_size=100,\n        n_epochs=50,\n        crop_shape=[64, 64, 3],\n        crop_factor=0.8,\n        convolutional=True,\n        variational=True,\n        n_filters=[100, 100, 100],\n        n_hidden=250,\n        n_code=100,\n        dropout=True,\n        filter_sizes=[3, 3, 3],\n        activation=tf.nn.sigmoid,\n        ckpt_name=\'./celeb.ckpt\')\n\n\ndef test_sita():\n    """"""Train an autoencoder on Sita Sings The Blues.\n    """"""\n    if not os.path.exists(\'sita\'):\n        os.system(\n            \'wget http://ossguy.com/sita/Sita_Sings_the_Blues_640x360_XviD.avi\')\n        os.mkdir(\'sita\')\n        os.system(\'ffmpeg -i Sita_Sings_the_Blues_640x360_XviD.avi -r 60 -f\' +\n                  \' image2 -s 160x90 sita/sita-%08d.jpg\')\n    files = [os.path.join(\'sita\', f) for f in os.listdir(\'sita\')]\n\n    train_vae(\n        files=files,\n        input_shape=[90, 160, 3],\n        batch_size=100,\n        n_epochs=50,\n        crop_shape=[90, 160, 3],\n        crop_factor=1.0,\n        convolutional=True,\n        variational=True,\n        n_filters=[100, 100, 100],\n        n_hidden=250,\n        n_code=100,\n        dropout=True,\n        filter_sizes=[3, 3, 3],\n        activation=tf.nn.sigmoid,\n        ckpt_name=\'./sita.ckpt\')\n\n\nif __name__ == \'__main__\':\n    test_celeb()\n'"
cadl/vaegan.py,63,"b'""""""Convolutional/Variational autoencoder, including demonstration of\ntraining such a network on MNIST, CelebNet and the film, ""Sita Sings The Blues""\nusing an image pipeline.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport os\nfrom cadl.dataset_utils import create_input_pipeline\nfrom cadl.datasets import CELEB\nfrom cadl import utils\n\n\ndef encoder(x,\n            n_hidden=None,\n            dimensions=[],\n            filter_sizes=[],\n            convolutional=False,\n            activation=tf.nn.relu,\n            output_activation=tf.nn.sigmoid):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    n_hidden : None, optional\n        Description\n    dimensions : list, optional\n        Description\n    filter_sizes : list, optional\n        Description\n    convolutional : bool, optional\n        Description\n    activation : TYPE, optional\n        Description\n    output_activation : TYPE, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    if convolutional:\n        x_tensor = utils.to_tensor(x)\n    else:\n        x_tensor = tf.reshape(tensor=x, shape=[-1, dimensions[0]])\n        dimensions = dimensions[1:]\n    current_input = x_tensor\n\n    Ws = []\n    hs = []\n    shapes = []\n    for layer_i, n_output in enumerate(dimensions):\n        with tf.variable_scope(str(layer_i)):\n            shapes.append(current_input.get_shape().as_list())\n            if convolutional:\n                h, W = utils.conv2d(\n                    x=current_input,\n                    n_output=n_output,\n                    k_h=filter_sizes[layer_i],\n                    k_w=filter_sizes[layer_i],\n                    padding=\'SAME\')\n            else:\n                h, W = utils.linear(x=current_input, n_output=n_output)\n            h = activation(h)\n            Ws.append(W)\n            hs.append(h)\n\n        current_input = h\n\n    shapes.append(h.get_shape().as_list())\n\n    with tf.variable_scope(\'flatten\'):\n        flattened = utils.flatten(current_input)\n\n    with tf.variable_scope(\'hidden\'):\n        if n_hidden:\n            h, W = utils.linear(flattened, n_hidden, name=\'linear\')\n            h = activation(h)\n        else:\n            h = flattened\n\n    return {\'z\': h, \'Ws\': Ws, \'hs\': hs, \'shapes\': shapes}\n\n\ndef decoder(z,\n            shapes,\n            n_hidden=None,\n            dimensions=[],\n            filter_sizes=[],\n            convolutional=False,\n            activation=tf.nn.relu,\n            output_activation=tf.nn.relu):\n    """"""Summary\n\n    Parameters\n    ----------\n    z : TYPE\n        Description\n    shapes : TYPE\n        Description\n    n_hidden : None, optional\n        Description\n    dimensions : list, optional\n        Description\n    filter_sizes : list, optional\n        Description\n    convolutional : bool, optional\n        Description\n    activation : TYPE, optional\n        Description\n    output_activation : TYPE, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    with tf.variable_scope(\'hidden/1\'):\n        if n_hidden:\n            h = utils.linear(z, n_hidden, name=\'linear\')[0]\n            h = activation(h)\n        else:\n            h = z\n\n    with tf.variable_scope(\'hidden/2\'):\n        dims = shapes[0]\n        size = dims[1] * dims[2] * dims[3] if convolutional else dims[1]\n        h = utils.linear(h, size, name=\'linear\')[0]\n        current_input = activation(h)\n        if convolutional:\n            current_input = tf.reshape(\n                current_input,\n                tf.stack(\n                    [tf.shape(current_input)[0], dims[1], dims[2], dims[3]]))\n\n    Ws = []\n    hs = []\n    for layer_i, n_output in enumerate(dimensions[1:]):\n        with tf.variable_scope(\'decoder/{}\'.format(layer_i)):\n            if convolutional:\n                shape = shapes[layer_i + 1]\n                h, W = utils.deconv2d(\n                    x=current_input,\n                    n_output_h=shape[1],\n                    n_output_w=shape[2],\n                    n_output_ch=shape[3],\n                    n_input_ch=shapes[layer_i][3],\n                    k_h=filter_sizes[layer_i],\n                    k_w=filter_sizes[layer_i])\n            else:\n                h, W = utils.linear(x=current_input, n_output=n_output)\n            if (layer_i + 1) < len(dimensions):\n                h = activation(h)\n            else:\n                h = output_activation(h)\n            Ws.append(W)\n            hs.append(h)\n            current_input = h\n\n    z = tf.identity(current_input, name=""x_tilde"")\n    return {\'x_tilde\': current_input, \'Ws\': Ws, \'hs\': hs}\n\n\ndef variational_bayes(h, n_code):\n    """"""Summary\n\n    Parameters\n    ----------\n    h : TYPE\n        Description\n    n_code : TYPE\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    z_mu = utils.linear(h, n_code, name=\'mu\')[0]\n    z_log_sigma = 0.5 * utils.linear(h, n_code, name=\'log_sigma\')[0]\n\n    # Sample from noise distribution p(eps) ~ N(0, 1)\n    epsilon = tf.random_normal(tf.stack([tf.shape(h)[0], n_code]))\n\n    # Sample from posterior\n    z = tf.add(z_mu, tf.multiply(epsilon, tf.exp(z_log_sigma)), name=\'z\')\n    # -log(p(z)/q(z|x)), bits by coding.\n    # variational bound coding costs kl(p(z|x)||q(z|x))\n    # d_kl(q(z|x)||p(z))\n    loss_z = -0.5 * tf.reduce_sum(1.0 + 2.0 * z_log_sigma - tf.square(z_mu) -\n                                  tf.exp(2.0 * z_log_sigma), 1)\n    return z, z_mu, z_log_sigma, loss_z\n\n\ndef discriminator(x,\n                  convolutional=True,\n                  filter_sizes=[5, 5, 5, 5],\n                  activation=tf.nn.relu,\n                  n_filters=[100, 100, 100, 100]):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    convolutional : bool, optional\n        Description\n    filter_sizes : list, optional\n        Description\n    activation : TYPE, optional\n        Description\n    n_filters : list, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    encoding = encoder(\n        x=x,\n        convolutional=convolutional,\n        dimensions=n_filters,\n        filter_sizes=filter_sizes,\n        activation=activation)\n\n    # flatten, then linear to 1 value\n    res = utils.flatten(encoding[\'z\'], name=\'flatten\')\n    if res.get_shape().as_list()[-1] > 1:\n        res = utils.linear(res, 1)[0]\n\n    return {\n        \'logits\': res,\n        \'probs\': tf.nn.sigmoid(res),\n        \'Ws\': encoding[\'Ws\'],\n        \'hs\': encoding[\'hs\']\n    }\n\n\ndef VAE(input_shape=[None, 784],\n        n_filters=[64, 64, 64],\n        filter_sizes=[4, 4, 4],\n        n_hidden=32,\n        n_code=2,\n        activation=tf.nn.tanh,\n        convolutional=False,\n        variational=False):\n    """"""Summary\n\n    Parameters\n    ----------\n    input_shape : list, optional\n        Description\n    n_filters : list, optional\n        Description\n    filter_sizes : list, optional\n        Description\n    n_hidden : int, optional\n        Description\n    n_code : int, optional\n        Description\n    activation : TYPE, optional\n        Description\n    convolutional : bool, optional\n        Description\n    variational : bool, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    # network input / placeholders for train (bn)\n    x = tf.placeholder(tf.float32, input_shape, \'x\')\n\n    with tf.variable_scope(\'encoder\'):\n        encoding = encoder(\n            x=x,\n            n_hidden=n_hidden,\n            convolutional=convolutional,\n            dimensions=n_filters,\n            filter_sizes=filter_sizes,\n            activation=activation)\n\n    if variational:\n        with tf.variable_scope(\'variational\'):\n            z, z_mu, z_log_sigma, loss_z = variational_bayes(\n                h=encoding[\'z\'], n_code=n_code)\n    else:\n        z = encoding[\'z\']\n        loss_z = None\n\n    shapes = encoding[\'shapes\'].copy()\n    shapes.reverse()\n    n_filters = n_filters.copy()\n    n_filters.reverse()\n    n_filters += [input_shape[-1]]\n\n    with tf.variable_scope(\'generator\'):\n        decoding = decoder(\n            z=z,\n            shapes=shapes,\n            n_hidden=n_hidden,\n            dimensions=n_filters,\n            filter_sizes=filter_sizes,\n            convolutional=convolutional,\n            activation=activation)\n\n    x_tilde = decoding[\'x_tilde\']\n    x_flat = utils.flatten(x)\n    x_tilde_flat = utils.flatten(x_tilde)\n\n    # -log(p(x|z))\n    loss_x = tf.reduce_sum(tf.squared_difference(x_flat, x_tilde_flat), 1)\n    return {\n        \'loss_x\': loss_x,\n        \'loss_z\': loss_z,\n        \'x\': x,\n        \'z\': z,\n        \'Ws\': encoding[\'Ws\'],\n        \'hs\': decoding[\'hs\'],\n        \'x_tilde\': x_tilde\n    }\n\n\ndef VAEGAN(input_shape=[None, 784],\n           n_filters=[64, 64, 64],\n           filter_sizes=[4, 4, 4],\n           n_hidden=32,\n           n_code=2,\n           activation=tf.nn.tanh,\n           convolutional=False,\n           variational=False):\n    """"""Summary\n\n    Parameters\n    ----------\n    input_shape : list, optional\n        Description\n    n_filters : list, optional\n        Description\n    filter_sizes : list, optional\n        Description\n    n_hidden : int, optional\n        Description\n    n_code : int, optional\n        Description\n    activation : TYPE, optional\n        Description\n    convolutional : bool, optional\n        Description\n    variational : bool, optional\n        Description\n\n    Returns\n    -------\n    name : TYPE\n        Description\n    """"""\n    # network input / placeholders for train (bn)\n    x = tf.placeholder(tf.float32, input_shape, \'x\')\n    z_samp = tf.placeholder(tf.float32, [None, n_code], \'z_samp\')\n\n    with tf.variable_scope(\'encoder\'):\n        encoding = encoder(\n            x=x,\n            n_hidden=n_hidden,\n            convolutional=convolutional,\n            dimensions=n_filters,\n            filter_sizes=filter_sizes,\n            activation=activation)\n\n        with tf.variable_scope(\'variational\'):\n            z, z_mu, z_log_sigma, loss_z = variational_bayes(\n                h=encoding[\'z\'], n_code=n_code)\n\n    shapes = encoding[\'shapes\'].copy()\n    shapes.reverse()\n    n_filters_decoder = n_filters.copy()\n    n_filters_decoder.reverse()\n    n_filters_decoder += [input_shape[-1]]\n\n    with tf.variable_scope(\'generator\'):\n        decoding_actual = decoder(\n            z=z,\n            shapes=shapes,\n            n_hidden=n_hidden,\n            convolutional=convolutional,\n            dimensions=n_filters_decoder,\n            filter_sizes=filter_sizes,\n            activation=activation)\n\n    with tf.variable_scope(\'generator\', reuse=True):\n        decoding_sampled = decoder(\n            z=z_samp,\n            shapes=shapes,\n            n_hidden=n_hidden,\n            convolutional=convolutional,\n            dimensions=n_filters_decoder,\n            filter_sizes=filter_sizes,\n            activation=activation)\n\n    with tf.variable_scope(\'discriminator\'):\n        D_real = discriminator(\n            x,\n            filter_sizes=filter_sizes,\n            n_filters=n_filters,\n            activation=activation)\n\n    with tf.variable_scope(\'discriminator\', reuse=True):\n        D_fake = discriminator(\n            decoding_actual[\'x_tilde\'],\n            filter_sizes=filter_sizes,\n            n_filters=n_filters,\n            activation=activation)\n\n    with tf.variable_scope(\'discriminator\', reuse=True):\n        D_samp = discriminator(\n            decoding_sampled[\'x_tilde\'],\n            filter_sizes=filter_sizes,\n            n_filters=n_filters,\n            activation=activation)\n\n    with tf.variable_scope(\'loss\'):\n        # Weights influence of content/style of decoder\n        gamma = tf.placeholder(tf.float32, name=\'gamma\')\n\n        # Discriminator_l Log Likelihood Loss\n        loss_D_llike = 0\n        for h_fake, h_real in zip(D_fake[\'hs\'][3:], D_real[\'hs\'][3:]):\n            loss_D_llike += tf.reduce_sum(0.5 * tf.squared_difference(\n                utils.flatten(h_fake), utils.flatten(h_real)), 1)\n\n        # GAN Loss\n        eps = 1e-12\n        loss_real = tf.reduce_sum(tf.log(D_real[\'probs\'] + eps), 1)\n        loss_fake = tf.reduce_sum(tf.log(1 - D_fake[\'probs\'] + eps), 1)\n        loss_samp = tf.reduce_sum(tf.log(1 - D_samp[\'probs\'] + eps), 1)\n\n        loss_GAN = (loss_real + loss_fake + loss_samp) / 3.0\n\n        loss_enc = tf.reduce_mean(loss_z + loss_D_llike)\n        loss_gen = tf.reduce_mean(gamma * loss_D_llike - loss_GAN)\n        loss_dis = -tf.reduce_mean(loss_GAN)\n\n    return {\n        \'x\': x,\n        \'z\': z,\n        \'x_tilde\': decoding_actual[\'x_tilde\'],\n        \'z_samp\': z_samp,\n        \'x_tilde_samp\': decoding_sampled[\'x_tilde\'],\n        \'loss_real\': loss_real,\n        \'loss_fake\': loss_fake,\n        \'loss_samp\': loss_samp,\n        \'loss_GAN\': loss_GAN,\n        \'loss_D_llike\': loss_D_llike,\n        \'loss_enc\': loss_enc,\n        \'loss_gen\': loss_gen,\n        \'loss_dis\': loss_dis,\n        \'gamma\': gamma\n    }\n\n\ndef train_vaegan(files,\n                 learning_rate=0.00001,\n                 batch_size=64,\n                 n_epochs=250,\n                 n_examples=10,\n                 input_shape=[218, 178, 3],\n                 crop_shape=[64, 64, 3],\n                 crop_factor=0.8,\n                 n_filters=[100, 100, 100, 100],\n                 n_hidden=None,\n                 n_code=128,\n                 convolutional=True,\n                 variational=True,\n                 filter_sizes=[3, 3, 3, 3],\n                 activation=tf.nn.elu,\n                 ckpt_name=""vaegan.ckpt""):\n    """"""Summary\n\n    Parameters\n    ----------\n    files : TYPE\n        Description\n    learning_rate : float, optional\n        Description\n    batch_size : int, optional\n        Description\n    n_epochs : int, optional\n        Description\n    n_examples : int, optional\n        Description\n    input_shape : list, optional\n        Description\n    crop_shape : list, optional\n        Description\n    crop_factor : float, optional\n        Description\n    n_filters : list, optional\n        Description\n    n_hidden : int, optional\n        Description\n    n_code : int, optional\n        Description\n    convolutional : bool, optional\n        Description\n    variational : bool, optional\n        Description\n    filter_sizes : list, optional\n        Description\n    activation : TYPE, optional\n        Description\n    ckpt_name : str, optional\n        Description\n\n    No Longer Returned\n    ------------------\n    name : TYPE\n        Description\n    """"""\n\n    ae = VAEGAN(\n        input_shape=[None] + crop_shape,\n        convolutional=convolutional,\n        variational=variational,\n        n_filters=n_filters,\n        n_hidden=n_hidden,\n        n_code=n_code,\n        filter_sizes=filter_sizes,\n        activation=activation)\n\n    batch = create_input_pipeline(\n        files=files,\n        batch_size=batch_size,\n        n_epochs=n_epochs,\n        crop_shape=crop_shape,\n        crop_factor=crop_factor,\n        shape=input_shape)\n\n    zs = np.random.randn(4, n_code).astype(np.float32)\n    zs = utils.make_latent_manifold(zs, n_examples)\n\n    opt_enc = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n        ae[\'loss_enc\'],\n        var_list=[\n            var_i for var_i in tf.trainable_variables()\n            if var_i.name.startswith(\'encoder\')\n        ])\n\n    opt_gen = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n        ae[\'loss_gen\'],\n        var_list=[\n            var_i for var_i in tf.trainable_variables()\n            if var_i.name.startswith(\'generator\')\n        ])\n\n    opt_dis = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n        ae[\'loss_dis\'],\n        var_list=[\n            var_i for var_i in tf.trainable_variables()\n            if var_i.name.startswith(\'discriminator\')\n        ])\n\n    sess = tf.Session()\n    saver = tf.train.Saver()\n    init_op = tf.group(tf.global_variables_initializer(),\n                       tf.local_variables_initializer())\n    sess.run(init_op)\n    coord = tf.train.Coordinator()\n    tf.get_default_graph().finalize()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    if os.path.exists(ckpt_name + \'.index\') or os.path.exists(ckpt_name):\n        saver.restore(sess, ckpt_name)\n        print(""VAE model restored."")\n\n    t_i = 0\n    batch_i = 0\n    epoch_i = 0\n\n    equilibrium = 0.693\n    margin = 0.4\n\n    n_files = len(files)\n    test_xs = sess.run(batch) / 255.0\n    utils.montage(test_xs, \'test_xs.png\')\n    try:\n        while not coord.should_stop() and epoch_i < n_epochs:\n            if batch_i % (n_files // batch_size) == 0:\n                batch_i = 0\n                epoch_i += 1\n                print(\'---------- EPOCH:\', epoch_i)\n\n            batch_i += 1\n            batch_xs = sess.run(batch) / 255.0\n            batch_zs = np.random.randn(batch_size, n_code).astype(np.float32)\n            real_cost, fake_cost, _ = sess.run(\n                [ae[\'loss_real\'], ae[\'loss_fake\'], opt_enc],\n                feed_dict={ae[\'x\']: batch_xs,\n                           ae[\'gamma\']: 0.5})\n            real_cost = -np.mean(real_cost)\n            fake_cost = -np.mean(fake_cost)\n            print(\'real:\', real_cost, \'/ fake:\', fake_cost)\n\n            gen_update = True\n            dis_update = True\n\n            if real_cost > (equilibrium + margin) or \\\n               fake_cost > (equilibrium + margin):\n                gen_update = False\n\n            if real_cost < (equilibrium - margin) or \\\n               fake_cost < (equilibrium - margin):\n                dis_update = False\n\n            if not (gen_update or dis_update):\n                gen_update = True\n                dis_update = True\n\n            if gen_update:\n                sess.run(\n                    opt_gen,\n                    feed_dict={\n                        ae[\'x\']: batch_xs,\n                        ae[\'z_samp\']: batch_zs,\n                        ae[\'gamma\']: 0.5\n                    })\n            if dis_update:\n                sess.run(\n                    opt_dis,\n                    feed_dict={\n                        ae[\'x\']: batch_xs,\n                        ae[\'z_samp\']: batch_zs,\n                        ae[\'gamma\']: 0.5\n                    })\n\n            if batch_i % 50 == 0:\n\n                # Plot example reconstructions from latent layer\n                recon = sess.run(ae[\'x_tilde\'], feed_dict={ae[\'z\']: zs})\n                print(\'recon:\', recon.min(), recon.max())\n                recon = np.clip(recon / recon.max(), 0, 1)\n                utils.montage(\n                    recon.reshape([-1] + crop_shape),\n                    \'imgs/manifold_%08d.png\' % t_i)\n\n                # Plot example reconstructions\n                recon = sess.run(ae[\'x_tilde\'], feed_dict={ae[\'x\']: test_xs})\n                print(\'recon:\', recon.min(), recon.max())\n                recon = np.clip(recon / recon.max(), 0, 1)\n                utils.montage(\n                    recon.reshape([-1] + crop_shape),\n                    \'imgs/reconstruction_%08d.png\' % t_i)\n                t_i += 1\n\n            if batch_i % 100 == 0:\n                # Save the variables to disk.\n                save_path = saver.save(\n                    sess,\n                    ckpt_name,\n                    global_step=batch_i,\n                    write_meta_graph=False)\n                print(""Model saved in file: %s"" % save_path)\n    except tf.errors.OutOfRangeError:\n        print(\'Done training -- epoch limit reached\')\n    finally:\n        # One of the threads has issued an exception.  So let\'s tell all the\n        # threads to shutdown.\n        coord.request_stop()\n\n    # Wait until all threads have finished.\n    coord.join(threads)\n\n    # Clean up the session.\n    sess.close()\n\n\ndef test_celeb(n_epochs=100,\n               filter_sizes=[3, 3, 3, 3],\n               n_filters=[100, 100, 100, 100],\n               crop_shape=[100, 100, 3]):\n    """"""Summary\n\n    Parameters\n    ----------\n    n_epochs : int, optional\n        Description\n\n    No Longer Returned\n    ------------------\n    name : TYPE\n        Description\n    """"""\n    files = CELEB()\n    train_vaegan(\n        files=files,\n        batch_size=64,\n        n_epochs=n_epochs,\n        crop_shape=crop_shape,\n        crop_factor=0.8,\n        input_shape=[218, 178, 3],\n        convolutional=True,\n        variational=True,\n        n_filters=n_filters,\n        n_hidden=None,\n        n_code=64,\n        filter_sizes=filter_sizes,\n        activation=tf.nn.elu,\n        ckpt_name=\'./celeb.ckpt\')\n\n\ndef test_sita(n_epochs=100):\n    """"""Summary\n\n    Parameters\n    ----------\n    n_epochs : int, optional\n        Description\n\n    No Longer Returned\n    ------------------\n    name : TYPE\n        Description\n    """"""\n    if not os.path.exists(\'sita\'):\n        os.system(\n            \'wget http://ossguy.com/sita/Sita_Sings_the_Blues_640x360_XviD.avi\')\n        os.mkdir(\'sita\')\n        os.system(\'ffmpeg -i Sita_Sings_the_Blues_640x360_XviD.avi -r 60 -f\' +\n                  \' image2 -s 160x90 sita/sita-%08d.jpg\')\n    files = [os.path.join(\'sita\', f) for f in os.listdir(\'sita\')]\n\n    train_vaegan(\n        files=files,\n        batch_size=64,\n        n_epochs=n_epochs,\n        crop_shape=[90, 160, 3],\n        crop_factor=1.0,\n        input_shape=[218, 178, 3],\n        convolutional=True,\n        variational=True,\n        n_filters=[100, 100, 100, 100, 100],\n        n_hidden=250,\n        n_code=100,\n        filter_sizes=[3, 3, 3, 3, 2],\n        activation=tf.nn.elu,\n        ckpt_name=\'./sita.ckpt\')\n\n\nif __name__ == \'__main__\':\n    test_celeb()\n'"
cadl/vctk.py,0,"b'""""""VCTK Dataset download and preprocessing.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport os\nfrom scipy.io import wavfile\nfrom cadl.utils import download_and_extract_tar\nfrom glob import glob\nimport subprocess\nimport numpy as np\n\n\ndef get_dataset(saveto=\'vctk\', convert_to_16khz=False):\n    """"""Download the VCTK dataset and convert to wav files.\n\n    More info:\n        http://homepages.inf.ed.ac.uk/jyamagis/\n        page3/page58/page58.html\n\n    This interface downloads the VCTK dataset and attempts to\n    convert the flac to wave files using ffmpeg.  If you do not have ffmpeg\n    installed, this function will not be able to convert the files to waves.\n\n    Parameters\n    ----------\n    saveto : str\n        Directory to save the resulting dataset [\'vctk\']\n    convert_to_16khz : bool, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    if not os.path.exists(saveto):\n        download_and_extract_tar(\n            \'http://homepages.inf.ed.ac.uk/jyamagis/\' +\n            \'release/VCTK-Corpus.tar.gz\',\n            saveto)\n\n    wavs = glob(\'{}/**/*.16khz.wav\'.format(saveto), recursive=True)\n    if convert_to_16khz and len(wavs) == 0:\n        wavs = glob(\'{}/**/*.wav\'.format(saveto), recursive=True)\n        for f in wavs:\n            subprocess.check_call(\n                [\'ffmpeg\', \'-i\', f, \'-f\', \'wav\', \'-ar\', \'16000\', \'-y\', \'%s.16khz.wav\' % f])\n\n    wavs = glob(\'{}/**/*.16khz.wav\'.format(saveto), recursive=True)\n\n    dataset = []\n    for wav_i in wavs:\n        chapter_i, utter_i = wav_i.split(\'/\')[-2:]\n        dataset.append({\n            \'name\': wav_i,\n            \'chapter\': chapter_i,\n            \'utterance\': utter_i.split(\'-\')[-1].strip(\'.wav\')})\n    return dataset\n\n\ndef batch_generator(dataset, batch_size=32, max_sequence_length=6144,\n                    maxval=32768.0, threshold=0.2, normalize=True):\n    """"""Summary\n\n    Parameters\n    ----------\n    dataset : TYPE\n        Description\n    batch_size : int, optional\n        Description\n    max_sequence_length : int, optional\n        Description\n    maxval : float, optional\n        Description\n    threshold : float, optional\n        Description\n    normalize : bool, optional\n        Description\n\n    Yields\n    ------\n    TYPE\n        Description\n    """"""\n    n_batches = len(dataset) // batch_size\n    for batch_i in range(n_batches):\n        cropped_wavs = []\n        while len(cropped_wavs) < batch_size:\n            idx_i = np.random.choice(np.arange(len(dataset)))\n            fname_i = dataset[idx_i][\'name\']\n            wav_i = wavfile.read(fname_i)[1]\n            if len(wav_i) > max_sequence_length:\n                sample = np.random.choice(range(len(wav_i) - max_sequence_length))\n                cropped_wav = wav_i[sample:sample + max_sequence_length]\n                if np.max(np.abs(cropped_wav) / maxval) > threshold:\n                    if normalize:\n                        cropped_wav = cropped_wav / maxval\n                    cropped_wavs.append(cropped_wav)\n        yield np.array(cropped_wavs, np.float32)\n'"
cadl/vgg16.py,19,"b'""""""VGG16 pretrained model and VGG Face model.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport tensorflow as tf\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize as imresize\nfrom .utils import download\n\n\ndef get_vgg_face_model():\n    """"""Summary\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    download(\'https://s3.amazonaws.com/cadl/models/vgg_face.tfmodel\')\n    with open(""vgg_face.tfmodel"", mode=\'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python \' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n\n    download(\'https://s3.amazonaws.com/cadl/models/vgg_face.json\')\n    labels = json.load(open(\'vgg_face.json\'))\n\n    return {\n        \'graph_def\': graph_def,\n        \'labels\': labels,\n        \'preprocess\': preprocess,\n        \'deprocess\': deprocess\n    }\n\n\ndef get_vgg_model():\n    """"""Summary\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    download(\'https://s3.amazonaws.com/cadl/models/vgg16.tfmodel\')\n    with open(""vgg16.tfmodel"", mode=\'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python \' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n\n    download(\'https://s3.amazonaws.com/cadl/models/synset.txt\')\n    with open(\'synset.txt\') as f:\n        labels = [(idx, l.strip()) for idx, l in enumerate(f.readlines())]\n\n    return {\n        \'graph_def\': graph_def,\n        \'labels\': labels,\n        \'preprocess\': preprocess,\n        \'deprocess\': deprocess\n    }\n\n\ndef preprocess(img, crop=True, resize=True, dsize=(224, 224)):\n    """"""Summary\n\n    Parameters\n    ----------\n    img : TYPE\n        Description\n    crop : bool, optional\n        Description\n    resize : bool, optional\n        Description\n    dsize : tuple, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    if img.dtype == np.uint8:\n        img = img / 255.0\n\n    if crop:\n        short_edge = min(img.shape[:2])\n        yy = int((img.shape[0] - short_edge) / 2)\n        xx = int((img.shape[1] - short_edge) / 2)\n        crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n    else:\n        crop_img = img\n\n    if resize:\n        norm_img = imresize(crop_img, dsize, preserve_range=True)\n    else:\n        norm_img = crop_img\n\n    return (norm_img).astype(np.float32)\n\n\ndef deprocess(img):\n    """"""Summary\n\n    Parameters\n    ----------\n    img : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    return np.clip(img * 255, 0, 255).astype(np.uint8)\n    # return ((img / np.max(np.abs(img))) * 127.5 +\n    #         127.5).astype(np.uint8)\n\n\ndef test_vgg():\n    """"""Loads the VGG network and applies it to a test image.\n    """"""\n    with tf.Session() as sess:\n        net = get_vgg_model()\n        tf.import_graph_def(net[\'graph_def\'], name=\'vgg\')\n        g = tf.get_default_graph()\n        names = [op.name for op in g.get_operations()]\n        input_name = names[0] + \':0\'\n        x = g.get_tensor_by_name(input_name)\n        softmax = g.get_tensor_by_name(names[-2] + \':0\')\n\n        og = plt.imread(\'bosch.png\')\n        img = preprocess(og)[np.newaxis, ...]\n        res = np.squeeze(softmax.eval(feed_dict={\n            x: img,\n            \'vgg/dropout_1/random_uniform:0\': [[1.0]],\n            \'vgg/dropout/random_uniform:0\': [[1.0]]}))\n        print([(res[idx], net[\'labels\'][idx])\n               for idx in res.argsort()[-5:][::-1]])\n\n        """"""Let\'s visualize the network\'s gradient activation\n        when backpropagated to the original input image.  This\n        is effectively telling us which pixels contribute to the\n        predicted class or given neuron""""""\n        features = [name for name in names if \'BiasAdd\' in name.split()[-1]]\n        from math import sqrt, ceil\n        n_plots = ceil(sqrt(len(features) + 1))\n        fig, axs = plt.subplots(n_plots, n_plots)\n        plot_i = 0\n        axs[0][0].imshow(img[0])\n        for feature_i, featurename in enumerate(features):\n            plot_i += 1\n            feature = g.get_tensor_by_name(featurename + \':0\')\n            neuron = tf.reduce_max(feature, 1)\n            saliency = tf.gradients(tf.reduce_sum(neuron), x)\n            neuron_idx = tf.arg_max(feature, 1)\n            this_res = sess.run([saliency[0], neuron_idx], feed_dict={\n                x: img,\n                \'vgg/dropout_1/random_uniform:0\': [[1.0]],\n                \'vgg/dropout/random_uniform:0\': [[1.0]]})\n\n            grad = this_res[0][0] / np.max(np.abs(this_res[0]))\n            ax = axs[plot_i // n_plots][plot_i % n_plots]\n            ax.imshow((grad * 127.5 + 127.5).astype(np.uint8))\n            ax.set_title(featurename)\n\n        """"""Deep Dreaming takes the backpropagated gradient activations\n        and simply adds it to the image, running the same process again\n        and again in a loop.  There are many tricks one can add to this\n        idea, such as infinitely zooming into the image by cropping and\n        scaling, adding jitter by randomly moving the image around, or\n        adding constraints on the total activations.""""""\n        og = plt.imread(\'street.png\')\n        crop = 2\n        img = preprocess(og)[np.newaxis, ...]\n        layer = g.get_tensor_by_name(features[3] + \':0\')\n        n_els = layer.get_shape().as_list()[1]\n        neuron_i = np.random.randint(1000)\n        layer_vec = np.zeros((1, n_els))\n        layer_vec[0, neuron_i] = 1\n        neuron = tf.reduce_max(layer, 1)\n        saliency = tf.gradients(tf.reduce_sum(neuron), x)\n        for it_i in range(3):\n            print(it_i)\n            this_res = sess.run(saliency[0], feed_dict={\n                x: img,\n                layer: layer_vec,\n                \'vgg/dropout_1/random_uniform:0\': [[1.0]],\n                \'vgg/dropout/random_uniform:0\': [[1.0]]})\n            grad = this_res[0] / np.mean(np.abs(grad))\n            img = img[:, crop:-crop - 1, crop:-crop - 1, :]\n            img = imresize(img[0], (224, 224))[np.newaxis]\n            img += grad\n        plt.imshow(deprocess(img[0]))\n\n\ndef test_vgg_face():\n    """"""Loads the VGG network and applies it to a test image.\n    """"""\n    with tf.Session() as sess:\n        net = get_vgg_face_model()\n        x = tf.placeholder(tf.float32, [1, 224, 224, 3], name=\'x\')\n        tf.import_graph_def(net[\'graph_def\'], name=\'vgg\',\n                            input_map={\'Placeholder:0\': x})\n        g = tf.get_default_graph()\n        names = [op.name for op in g.get_operations()]\n\n        og = plt.imread(\'bricks.png\')[..., :3]\n        img = preprocess(og)[np.newaxis, ...]\n        plt.imshow(img[0])\n        plt.show()\n\n        """"""Let\'s visualize the network\'s gradient activation\n        when backpropagated to the original input image.  This\n        is effectively telling us which pixels contribute to the\n        predicted class or given neuron""""""\n        features = [name for name in names if \'BiasAdd\' in name.split()[-1]]\n        from math import sqrt, ceil\n        n_plots = ceil(sqrt(len(features) + 1))\n        fig, axs = plt.subplots(n_plots, n_plots)\n        plot_i = 0\n        axs[0][0].imshow(img[0])\n        for feature_i, featurename in enumerate(features):\n            plot_i += 1\n            feature = g.get_tensor_by_name(featurename + \':0\')\n            neuron = tf.reduce_max(feature, 1)\n            saliency = tf.gradients(tf.reduce_sum(neuron), x)\n            neuron_idx = tf.arg_max(feature, 1)\n            this_res = sess.run([saliency[0], neuron_idx], feed_dict={x: img})\n\n            grad = this_res[0][0] / np.max(np.abs(this_res[0]))\n            ax = axs[plot_i // n_plots][plot_i % n_plots]\n            ax.imshow((grad * 127.5 + 127.5).astype(np.uint8))\n            ax.set_title(featurename)\n            plt.waitforbuttonpress()\n\n        """"""Deep Dreaming takes the backpropagated gradient activations\n        and simply adds it to the image, running the same process again\n        and again in a loop.  There are many tricks one can add to this\n        idea, such as infinitely zooming into the image by cropping and\n        scaling, adding jitter by randomly moving the image around, or\n        adding constraints on the total activations.""""""\n        og = plt.imread(\'street.png\')\n        crop = 2\n        img = preprocess(og)[np.newaxis, ...]\n        layer = g.get_tensor_by_name(features[3] + \':0\')\n        n_els = layer.get_shape().as_list()[1]\n        neuron_i = np.random.randint(1000)\n        layer_vec = np.zeros((1, n_els))\n        layer_vec[0, neuron_i] = 1\n        neuron = tf.reduce_max(layer, 1)\n        saliency = tf.gradients(tf.reduce_sum(neuron), x)\n        for it_i in range(3):\n            print(it_i)\n            this_res = sess.run(saliency[0], feed_dict={\n                x: img,\n                layer: layer_vec,\n                \'vgg/dropout_1/random_uniform:0\': [[1.0]],\n                \'vgg/dropout/random_uniform:0\': [[1.0]]})\n            grad = this_res[0] / np.mean(np.abs(grad))\n            img = img[:, crop:-crop - 1, crop:-crop - 1, :]\n            img = imresize(img[0], (224, 224))[np.newaxis]\n            img += grad\n        plt.imshow(deprocess(img[0]))\n\n\nif __name__ == \'__main__\':\n    test_vgg_face()\n'"
cadl/wavenet.py,67,"b'""""""WaveNet Autoencoder and conditional WaveNet.\n""""""\n""""""\nWaveNet Training code and utilities are licensed under APL from the\n\nGoogle Magenta project\n----------------------\nhttps://github.com/tensorflow/magenta/blob/master/magenta/models/nsynth/wavenet\n\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom cadl import librispeech, vctk\nfrom cadl import wavenet_utils as wnu\nfrom cadl.utils import sample_categorical\nfrom scipy.io import wavfile\n\n\ndef get_sequence_length(n_stages, n_layers_per_stage):\n    """"""Summary\n\n    Parameters\n    ----------\n    n_stages : TYPE\n        Description\n    n_layers_per_stage : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    sequence_length = 2**n_layers_per_stage * 2 * n_stages\n    return sequence_length\n\n\ndef condition(x, encoding):\n    """"""Summary\n\n    Parameters\n    ----------\n    x : TYPE\n        Description\n    encoding : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    batch_size, length, channels = x.get_shape().as_list()\n    enc_batch_size, enc_length, enc_channels = encoding.get_shape().as_list()\n    assert enc_batch_size == batch_size\n    assert enc_channels == channels\n    encoding = tf.reshape(encoding, [batch_size, enc_length, 1, channels])\n    x = tf.reshape(x, [batch_size, enc_length, -1, channels])\n    x += encoding\n    x = tf.reshape(x, [batch_size, length, channels])\n    x.set_shape([batch_size, length, channels])\n    return x\n\n\ndef create_wavenet_autoencoder(n_stages, n_layers_per_stage, n_hidden,\n                               batch_size, n_skip, filter_length,\n                               bottleneck_width, hop_length, n_quantization,\n                               sample_rate):\n    """"""Summary\n\n    Parameters\n    ----------\n    n_stages : TYPE\n        Description\n    n_layers_per_stage : TYPE\n        Description\n    n_hidden : TYPE\n        Description\n    batch_size : TYPE\n        Description\n    n_skip : TYPE\n        Description\n    filter_length : TYPE\n        Description\n    bottleneck_width : TYPE\n        Description\n    hop_length : TYPE\n        Description\n    n_quantization : TYPE\n        Description\n    sample_rate : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    offset = n_quantization / 2.0\n    sequence_length = 2**n_layers_per_stage * 2 * n_stages\n\n    # Encode the source with 8-bit Mu-Law.\n    X = tf.placeholder(\n        name=\'X\', shape=[batch_size, sequence_length], dtype=tf.float32)\n    X_quantized = wnu.mu_law(X, n_quantization)\n    X_scaled = tf.cast(X_quantized / offset, tf.float32)\n    X_scaled = tf.expand_dims(X_scaled, 2)\n\n    # The Non-Causal Temporal Encoder.\n    en = wnu.conv1d(\n        X=X_scaled,\n        causal=False,\n        num_filters=n_hidden,\n        filter_length=filter_length,\n        name=\'ae_startconv\')\n\n    # Residual blocks with skip connections.\n    for i in range(n_stages * n_layers_per_stage):\n        dilation = 2**(i % n_layers_per_stage)\n        print(dilation)\n        d = tf.nn.relu(en)\n        d = wnu.conv1d(\n            d,\n            causal=False,\n            num_filters=n_hidden,\n            filter_length=filter_length,\n            dilation=dilation,\n            name=\'ae_dilatedconv_%d\' % (i + 1))\n        d = tf.nn.relu(d)\n        en += wnu.conv1d(\n            d,\n            num_filters=n_hidden,\n            filter_length=1,\n            name=\'ae_res_%d\' % (i + 1))\n\n    en = wnu.conv1d(\n        en, num_filters=bottleneck_width, filter_length=1, name=\'ae_bottleneck\')\n\n    en = wnu.pool1d(en, hop_length, name=\'ae_pool\', mode=\'avg\')\n    encoding = en\n\n    # The WaveNet Decoder.\n    l = wnu.shift_right(X_scaled)\n    l = wnu.conv1d(\n        l, num_filters=n_hidden, filter_length=filter_length, name=\'startconv\')\n\n    # Set up skip connections.\n    s = wnu.conv1d(l, num_filters=n_skip, filter_length=1, name=\'skip_start\')\n\n    # Residual blocks with skip connections.\n    for i in range(n_stages * n_layers_per_stage):\n        dilation = 2**(i % n_layers_per_stage)\n        d = wnu.conv1d(\n            l,\n            num_filters=2 * n_hidden,\n            filter_length=filter_length,\n            dilation=dilation,\n            name=\'dilatedconv_%d\' % (i + 1))\n        d = condition(d,\n                      wnu.conv1d(\n                          en,\n                          num_filters=2 * n_hidden,\n                          filter_length=1,\n                          name=\'cond_map_%d\' % (i + 1)))\n        assert d.get_shape().as_list()[2] % 2 == 0\n        m = d.get_shape().as_list()[2] // 2\n        d_sigmoid = tf.sigmoid(d[:, :, :m])\n        d_tanh = tf.tanh(d[:, :, m:])\n        d = d_sigmoid * d_tanh\n        l += wnu.conv1d(\n            d, num_filters=n_hidden, filter_length=1, name=\'res_%d\' % (i + 1))\n        s += wnu.conv1d(\n            d, num_filters=n_skip, filter_length=1, name=\'skip_%d\' % (i + 1))\n\n    s = tf.nn.relu(s)\n    s = wnu.conv1d(s, num_filters=n_skip, filter_length=1, name=\'out1\')\n    s = condition(s,\n                  wnu.conv1d(\n                      en,\n                      num_filters=n_skip,\n                      filter_length=1,\n                      name=\'cond_map_out1\'))\n    s = tf.nn.relu(s)\n\n    # Compute the logits and get the loss.\n    logits = wnu.conv1d(\n        s, num_filters=n_quantization, filter_length=1, name=\'logits\')\n    logits = tf.reshape(logits, [-1, n_quantization])\n    probs = tf.nn.softmax(logits, name=\'softmax\')\n    synthesis = tf.reshape(\n        wnu.inv_mu_law(\n            tf.cast(tf.argmax(probs, 1), tf.float32) - offset, n_quantization),\n        [-1, sequence_length])\n    labels = tf.cast(tf.reshape(X_quantized, [-1]), tf.int32) + int(offset)\n    loss = tf.reduce_mean(\n        tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=logits, labels=labels, name=\'nll\'),\n        0,\n        name=\'loss\')\n\n    tf.summary.audio(""synthesis"", synthesis, sample_rate=sample_rate)\n    tf.summary.histogram(""probs"", probs)\n    tf.summary.histogram(""input_quantized"", X_quantized)\n    tf.summary.histogram(""logits"", logits)\n    tf.summary.histogram(""labels"", labels)\n    tf.summary.histogram(""synthesis"", synthesis)\n    tf.summary.scalar(""loss"", loss)\n    summaries = tf.summary.merge_all()\n\n    return {\n        \'X\': X,\n        \'quantized\': X_quantized,\n        \'encoding\': encoding,\n        \'probs\': probs,\n        \'synthesis\': synthesis,\n        \'summaries\': summaries,\n        \'loss\': loss\n    }\n\n\ndef create_wavenet(n_stages=10,\n                   n_layers_per_stage=9,\n                   n_hidden=200,\n                   batch_size=32,\n                   n_skip=100,\n                   filter_length=2,\n                   shift=True,\n                   n_quantization=256,\n                   sample_rate=16000):\n    """"""Summary\n\n    Parameters\n    ----------\n    n_stages : int, optional\n        Description\n    n_layers_per_stage : int, optional\n        Description\n    n_hidden : int, optional\n        Description\n    batch_size : int, optional\n        Description\n    n_skip : int, optional\n        Description\n    filter_length : int, optional\n        Description\n    shift : bool, optional\n        Description\n    n_quantization : int, optional\n        Description\n    sample_rate : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    offset = n_quantization / 2.0\n    sequence_length = 2**n_layers_per_stage * 2 * n_stages\n\n    # Encode the source with 8-bit Mu-Law.\n    X = tf.placeholder(\n        name=\'X\', shape=[batch_size, sequence_length], dtype=tf.float32)\n    X_quantized = wnu.mu_law(X, n_quantization)\n    X_onehot = tf.expand_dims(X_quantized, 2)\n    if shift:\n        X_onehot = wnu.shift_right(X_onehot)\n\n    h = wnu.conv1d(\n        X=X_onehot,\n        num_filters=n_hidden,\n        filter_length=filter_length,\n        name=\'startconv\')\n\n    # Set up skip connections.\n    s = wnu.conv1d(X=h, num_filters=n_skip, filter_length=1, name=\'skip_start\')\n\n    # Residual blocks with skip connections.\n    for i in range(n_stages * n_layers_per_stage):\n        dilation = 2**(i % n_layers_per_stage)\n\n        # dilated masked cnn\n        d = wnu.conv1d(\n            X=h,\n            num_filters=2 * n_hidden,\n            filter_length=filter_length,\n            dilation=dilation,\n            name=\'dilatedconv_%d\' % (i + 1))\n\n        # gated cnn\n        assert d.get_shape().as_list()[2] % 2 == 0\n        m = d.get_shape().as_list()[2] // 2\n        d = tf.sigmoid(d[:, :, :m]) * tf.tanh(d[:, :, m:])\n\n        # residuals\n        h += wnu.conv1d(\n            X=d, num_filters=n_hidden, filter_length=1, name=\'res_%d\' % (i + 1))\n\n        # skips\n        s += wnu.conv1d(\n            X=d, num_filters=n_skip, filter_length=1, name=\'skip_%d\' % (i + 1))\n\n    s = tf.nn.relu(s)\n    s = wnu.conv1d(X=s, num_filters=n_skip, filter_length=1, name=\'out1\')\n    s = tf.nn.relu(s)\n    logits = tf.clip_by_value(\n        wnu.conv1d(\n            X=s,\n            num_filters=n_quantization,\n            filter_length=1,\n            name=\'logits_preclip\') + offset,\n        0.0,\n        n_quantization - 1.0,\n        name=\'logits\')\n    logits = tf.reshape(logits, [-1, n_quantization])\n    labels = tf.cast(tf.reshape(X_quantized + offset, [-1]), tf.int32)\n    loss = tf.reduce_mean(\n        tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=logits, labels=labels, name=\'nll\'),\n        0,\n        name=\'loss\')\n\n    probs = tf.nn.softmax(logits, name=\'softmax\')\n    synthesis = tf.reshape(\n        wnu.inv_mu_law(\n            tf.cast(tf.argmax(probs, 1), tf.float32) - offset, n_quantization),\n        [-1, sequence_length])\n\n    tf.summary.audio(""synthesis"", synthesis, sample_rate=sample_rate)\n    tf.summary.histogram(""probs"", probs)\n    tf.summary.histogram(""input_quantized"", X_quantized)\n    tf.summary.histogram(""logits"", logits)\n    tf.summary.histogram(""labels"", labels)\n    tf.summary.histogram(""synthesis"", synthesis)\n    tf.summary.scalar(""loss"", loss)\n    summaries = tf.summary.merge_all()\n\n    return {\n        \'X\': X,\n        \'quantized\': X_quantized,\n        \'probs\': probs,\n        \'synthesis\': synthesis,\n        \'summaries\': summaries,\n        \'loss\': loss\n    }\n\n\ndef train_vctk():\n    """"""Summary\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    batch_size = 24\n    filter_length = 2\n    n_stages = 7\n    n_layers_per_stage = 9\n    n_hidden = 48\n    n_skip = 384\n    dataset = vctk.get_dataset()\n    it_i = 0\n    n_epochs = 1000\n    sequence_length = get_sequence_length(n_stages, n_layers_per_stage)\n    ckpt_path = \'vctk-wavenet/wavenet_filterlen{}_batchsize{}_sequencelen{}_stages{}_layers{}_hidden{}_skips{}\'.format(\n        filter_length, batch_size, sequence_length, n_stages,\n        n_layers_per_stage, n_hidden, n_skip)\n    with tf.Graph().as_default(), tf.Session() as sess:\n        net = create_wavenet(\n            batch_size=batch_size,\n            filter_length=filter_length,\n            n_hidden=n_hidden,\n            n_skip=n_skip,\n            n_stages=n_stages,\n            n_layers_per_stage=n_layers_per_stage)\n        saver = tf.train.Saver()\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        sess.run(init_op)\n        if tf.train.latest_checkpoint(ckpt_path) is not None:\n            saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\n        batch = vctk.batch_generator\n        with tf.variable_scope(\'optimizer\'):\n            opt = tf.train.AdamOptimizer(\n                learning_rate=0.0002).minimize(net[\'loss\'])\n        var_list = [\n            v for v in tf.global_variables() if v.name.startswith(\'optimizer\')\n        ]\n        sess.run(tf.variables_initializer(var_list))\n        writer = tf.summary.FileWriter(ckpt_path)\n        for epoch_i in range(n_epochs):\n            for batch_xs in batch(dataset, batch_size, sequence_length):\n                loss, quantized, _ = sess.run(\n                    [net[\'loss\'], net[\'quantized\'], opt],\n                    feed_dict={net[\'X\']: batch_xs})\n                print(loss)\n                if it_i % 100 == 0:\n                    summary = sess.run(\n                        net[\'summaries\'], feed_dict={net[\'X\']: batch_xs})\n                    writer.add_summary(summary, it_i)\n                    # save\n                    saver.save(\n                        sess,\n                        os.path.join(ckpt_path, \'model.ckpt\'),\n                        global_step=it_i)\n                it_i += 1\n\n    return loss\n\n\ndef test_librispeech():\n    """"""Summary\n    """"""\n    batch_size = 24\n    filter_length = 2\n    n_stages = 7\n    n_layers_per_stage = 9\n    n_hidden = 48\n    n_skip = 384\n    total_length = 16000\n    sequence_length = get_sequence_length(n_stages, n_layers_per_stage)\n    prime_length = sequence_length\n    ckpt_path = \'wavenet/wavenet_filterlen{}_batchsize{}_sequencelen{}_stages{}_layers{}_hidden{}_skips{}/\'.format(\n        filter_length, batch_size, sequence_length, n_stages,\n        n_layers_per_stage, n_hidden, n_skip)\n\n    dataset = librispeech.get_dataset()\n    batch = next(\n        librispeech.batch_generator(dataset, batch_size, prime_length))[0]\n\n    sess = tf.Session()\n    net = create_wavenet(\n        batch_size=batch_size,\n        filter_length=filter_length,\n        n_hidden=n_hidden,\n        n_skip=n_skip,\n        n_layers_per_stage=n_layers_per_stage,\n        n_stages=n_stages,\n        shift=False)\n    init_op = tf.group(tf.global_variables_initializer(),\n                       tf.local_variables_initializer())\n    sess.run(init_op)\n    saver = tf.train.Saver()\n    if tf.train.latest_checkpoint(ckpt_path) is not None:\n        saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\n    else:\n        print(\'Could not find checkpoint\')\n\n    synth = np.zeros([batch_size, total_length], dtype=np.float32)\n    synth[:, :prime_length] = batch\n\n    print(\'Synthesize...\')\n    for sample_i in range(0, total_length - prime_length):\n        print(\'{}/{}/{}\'.format(sample_i, prime_length, total_length), end=\'\\r\')\n        probs = sess.run(\n            net[""probs""],\n            feed_dict={net[""X""]: synth[:, sample_i:sample_i + sequence_length]})\n        idxs = sample_categorical(probs)\n        idxs = idxs.reshape((batch_size, sequence_length))\n        if sample_i == 0:\n            audio = wnu.inv_mu_law_numpy(idxs - 128)\n            synth[:, :prime_length] = audio\n        else:\n            audio = wnu.inv_mu_law_numpy(idxs[:, -1] - 128)\n            synth[:, prime_length + sample_i] = audio\n\n    for i in range(batch_size):\n        wavfile.write(\'synthesis-{}.wav\'.format(i), 16000, synth[i])\n'"
cadl/wavenet_utils.py,65,"b'""""""Various utilities for training WaveNet.\n""""""\n""""""\nWaveNet Training code and utilities are licensed under APL from the\n\nGoogle Magenta project\n----------------------\nhttps://github.com/tensorflow/magenta/blob/master/magenta/models/nsynth/wavenet\n\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport tensorflow as tf\nimport numpy as np\n\n\ndef shift_right(X):\n    """"""Shift the input over by one and a zero to the front.\n\n    Parameters\n    ----------\n    X\n        The [mb, time, channels] tensor input.\n\n    Returns\n    -------\n    x_sliced\n        The [mb, time, channels] tensor output.\n    """"""\n    shape = X.get_shape().as_list()\n    x_padded = tf.pad(X, [[0, 0], [1, 0], [0, 0]])\n    x_sliced = tf.slice(x_padded, [0, 0, 0], tf.stack([-1, np.int32(shape[1]), -1]))\n    x_sliced.set_shape(shape)\n    return x_sliced\n\n\ndef mul_or_none(a, b):\n    """"""Return the element wise multiplicative of the inputs.\n    If either input is None, we return None.\n\n    Parameters\n    ----------\n    a\n        A tensor input.\n    b\n        Another tensor input with the same type as a.\n\n    Returns\n    -------\n    None if either input is None. Otherwise returns a * b.\n    """"""\n    if a is None or b is None:\n        return None\n    return a * b\n\n\ndef time_to_batch(X, block_size):\n    """"""Splits time dimension (i.e. dimension 1) of `X` into batches.\n    Within each batch element, the `k*block_size` time steps are transposed,\n    so that the `k` time steps in each output batch element are offset by\n    `block_size` from each other.\n    The number of input time steps must be a multiple of `block_size`.\n\n    Parameters\n    ----------\n    X\n        Tensor of shape [nb, k*block_size, n] for some natural number k.\n    block_size\n        number of time steps (i.e. size of dimension 1) in the output\n        tensor.\n\n    Returns\n    -------\n    Tensor of shape [nb*block_size, k, n]\n    """"""\n    shape = X.get_shape().as_list()\n    y = tf.reshape(X, [\n        shape[0], shape[1] // block_size, block_size, shape[2]\n    ])\n    y = tf.transpose(y, [0, 2, 1, 3])\n    y = tf.reshape(y, [\n        shape[0] * block_size, shape[1] // block_size, shape[2]\n    ])\n    y.set_shape([\n        mul_or_none(shape[0], block_size), mul_or_none(shape[1], 1. / block_size),\n        shape[2]\n    ])\n    return y\n\n\ndef batch_to_time(X, block_size):\n    """"""Inverse of `time_to_batch(X, block_size)`.\n\n    Parameters\n    ----------\n    X\n        Tensor of shape [nb*block_size, k, n] for some natural number k.\n    block_size\n        number of time steps (i.e. size of dimension 1) in the output\n        tensor.\n\n    Returns\n    -------\n    Tensor of shape [nb, k*block_size, n].\n    """"""\n    shape = X.get_shape().as_list()\n    y = tf.reshape(X, [shape[0] // block_size, block_size, shape[1], shape[2]])\n    y = tf.transpose(y, [0, 2, 1, 3])\n    y = tf.reshape(y, [shape[0] // block_size, shape[1] * block_size, shape[2]])\n    y.set_shape([mul_or_none(shape[0], 1. / block_size),\n                 mul_or_none(shape[1], block_size),\n                 shape[2]])\n    return y\n\n\ndef conv1d(X,\n           num_filters,\n           filter_length,\n           name,\n           dilation=1,\n           causal=True,\n           kernel_initializer=tf.uniform_unit_scaling_initializer(1.0),\n           biases_initializer=tf.constant_initializer(0.0)):\n    """"""Fast 1D convolution that supports causal padding and dilation.\n\n    Parameters\n    ----------\n    X\n        The [mb, time, channels] float tensor that we convolve.\n    num_filters\n        The number of filter maps in the convolution.\n    filter_length\n        The integer length of the filter.\n    name\n        The name of the scope for the variables.\n    dilation\n        The amount of dilation.\n    causal\n        Whether or not this is a causal convolution.\n    kernel_initializer\n        The kernel initialization function.\n    biases_initializer\n        The biases initialization function.\n\n    Returns\n    -------\n    y\n        The output of the 1D convolution.\n    """"""\n    batch_size, length, num_input_channels = X.get_shape().as_list()\n    assert length % dilation == 0\n\n    kernel_shape = [1, filter_length, num_input_channels, num_filters]\n    strides = [1, 1, 1, 1]\n    biases_shape = [num_filters]\n    padding = \'VALID\' if causal else \'SAME\'\n\n    with tf.variable_scope(name):\n        weights = tf.get_variable(\n            \'W\', shape=kernel_shape, initializer=kernel_initializer)\n        biases = tf.get_variable(\n            \'biases\', shape=biases_shape, initializer=biases_initializer)\n\n    x_ttb = time_to_batch(X, dilation)\n    if filter_length > 1 and causal:\n        x_ttb = tf.pad(x_ttb, [[0, 0], [filter_length - 1, 0], [0, 0]])\n\n    x_ttb_shape = x_ttb.get_shape().as_list()\n    x_4d = tf.reshape(x_ttb, [x_ttb_shape[0], 1,\n                              x_ttb_shape[1], num_input_channels])\n    y = tf.nn.conv2d(x_4d, weights, strides, padding=padding)\n    y = tf.nn.bias_add(y, biases)\n    y_shape = y.get_shape().as_list()\n    y = tf.reshape(y, [y_shape[0], y_shape[2], num_filters])\n    y = batch_to_time(y, dilation)\n    y.set_shape([batch_size, length, num_filters])\n    return y\n\n\ndef pool1d(X, window_length, name, mode=\'avg\', stride=None):\n    """"""1D pooling function that supports multiple different modes.\n\n    Parameters\n    ----------\n    X\n        The [mb, time, channels] float tensor that we are going to pool over.\n    window_length\n        The amount of samples we pool over.\n    name\n        The name of the scope for the variables.\n    mode\n        The type of pooling, either avg or max.\n    stride\n        The stride length.\n\n    Returns\n    -------\n    pooled\n        The [mb, time // stride, channels] float tensor result of pooling.\n    """"""\n    if mode == \'avg\':\n        pool_fn = tf.nn.avg_pool\n    elif mode == \'max\':\n        pool_fn = tf.nn.max_pool\n\n    stride = stride or window_length\n    batch_size, length, num_channels = X.get_shape().as_list()\n    assert length % window_length == 0\n    assert length % stride == 0\n\n    window_shape = [1, 1, window_length, 1]\n    strides = [1, 1, stride, 1]\n    x_4d = tf.reshape(X, [batch_size, 1, length, num_channels])\n    pooled = pool_fn(x_4d, window_shape, strides, padding=\'SAME\', name=name)\n    return tf.reshape(pooled, [batch_size, length // stride, num_channels])\n\n\ndef mu_law(X, mu=255, int8=False):\n    """"""A TF implementation of Mu-Law encoding.\n\n    Parameters\n    ----------\n    X\n        The audio samples to encode.\n    mu\n        The Mu to use in our Mu-Law.\n    int8\n        Use int8 encoding.\n\n    Returns\n    -------\n    out\n        The Mu-Law encoded int8 data.\n    """"""\n    out = tf.sign(X) * tf.log(1 + mu * tf.abs(X)) / np.log(1 + mu)\n    out = tf.floor(out * 128)\n    if int8:\n        out = tf.cast(out, tf.int8)\n    return out\n\n\ndef mu_law_numpy(X, mu=255, int8=False):\n    """"""A TF implementation of Mu-Law encoding.\n\n    Parameters\n    ----------\n    X\n        The audio samples to encode.\n    mu\n        The Mu to use in our Mu-Law.\n    int8\n        Use int8 encoding.\n\n    Returns\n    -------\n    out\n        The Mu-Law encoded int8 data.\n    """"""\n    out = np.sign(X) * np.log(1 + mu * np.abs(X)) / np.log(1 + mu)\n    out = np.floor(out * 128)\n    if int8:\n        return out.astype(np.int8)\n    return out\n\n\ndef inv_mu_law(X, mu=255):\n    """"""A TF implementation of inverse Mu-Law.\n\n    Parameters\n    ----------\n    X\n        The Mu-Law samples to decode.\n    mu\n        The Mu we used to encode these samples.\n\n    Returns\n    -------\n    out\n        The decoded data.\n    """"""\n    X = tf.cast(X, tf.float32)\n    out = (X + 0.5) * 2. / (mu + 1)\n    out = tf.sign(out) / mu * ((1 + mu)**tf.abs(out) - 1)\n    out = tf.where(tf.equal(X, 0), X, out)\n    return out\n\n\ndef inv_mu_law_numpy(X, mu=255.0):\n    """"""A numpy implementation of inverse Mu-Law.\n\n    Parameters\n    ----------\n    X\n        The Mu-Law samples to decode.\n    mu\n        The Mu we used to encode these samples.\n\n    Returns\n    -------\n    out\n        The decoded data.\n    """"""\n    X = np.array(X).astype(np.float32)\n    out = (X + 0.5) * 2. / (mu + 1)\n    out = np.sign(out) / mu * ((1 + mu)**np.abs(out) - 1)\n    out = np.where(np.equal(X, 0), X, out)\n    return out\n\n\ndef causal_linear(X, n_inputs, n_outputs, name, filter_length, rate,\n                  batch_size, depth=1):\n    """"""Applies dilated convolution using queues.\n    Assumes a filter_length of 2 or 3.\n\n    Parameters\n    ----------\n    X\n        The [mb, time, channels] tensor input.\n    n_inputs\n        The input number of channels.\n    n_outputs\n        The output number of channels.\n    name\n        The variable scope to provide to W and biases.\n    filter_length\n        The length of the convolution, assumed to be 3.\n    rate\n        The rate or dilation\n    batch_size\n        Non-symbolic value for batch_size.\n    depth : int, optional\n        Description\n\n    Returns\n    -------\n    y\n        The output of the operation\n    (init_1, init_2)\n        Initialization operations for the queues\n    (push_1, push_2)\n        Push operations for the queues\n    """"""\n    assert filter_length == 2 or filter_length == 3\n\n    # TODO: Make generic... started something like this:\n    #    # create queue\n    #    qs = []\n    #    inits = []\n    #    states = []\n    #    pushs = []\n    #    zeros = tf.zeros((rate, batch_size, depth, n_inputs))\n    #    for f_i in range(1, filter_length):\n    #        q = tf.FIFOQueue(\n    #            rate,\n    #            dtypes=tf.float32,\n    #            shapes=(batch_size, depth, n_inputs))\n    #        qs.append(q)\n    #        inits.append(q.enqueue_many(zeros))\n    #        states.append(q.dequeue())\n    #\n    #    pushs.append(qs[0].enqueue(X))\n    #    for f_i in range(2, filter_length):\n    #        pushs.append(qs[f_i].enqueue(states[f_i - 1]))\n\n    if filter_length == 3:\n        # create queue\n        q_1 = tf.FIFOQueue(rate, dtypes=tf.float32, shapes=(batch_size, depth, n_inputs))\n        q_2 = tf.FIFOQueue(rate, dtypes=tf.float32, shapes=(batch_size, depth, n_inputs))\n        init_1 = q_1.enqueue_many(tf.zeros((rate, batch_size, depth, n_inputs)))\n        init_2 = q_2.enqueue_many(tf.zeros((rate, batch_size, depth, n_inputs)))\n        state_1 = q_1.dequeue()\n        push_1 = q_1.enqueue(X)\n        state_2 = q_2.dequeue()\n        push_2 = q_2.enqueue(state_1)\n\n        # get pretrained weights\n        w = tf.get_variable(\n            name=name + ""/W"",\n            shape=[1, filter_length, n_inputs, n_outputs],\n            dtype=tf.float32)\n        b = tf.get_variable(\n            name=name + ""/biases"", shape=[n_outputs], dtype=tf.float32)\n        w_q_2 = tf.slice(w, [0, 0, 0, 0], [-1, 1, -1, -1])\n        w_q_1 = tf.slice(w, [0, 1, 0, 0], [-1, 1, -1, -1])\n        w_x = tf.slice(w, [0, 2, 0, 0], [-1, 1, -1, -1])\n\n        # perform op w/ cached states\n        y = tf.nn.bias_add(\n            tf.matmul(state_2[:, 0, :], w_q_2[0][0]) + tf.matmul(\n                state_1[:, 0, :], w_q_1[0][0]) + tf.matmul(X[:, 0, :], w_x[0][0]), b)\n\n        y = tf.expand_dims(y, 1)\n        return y, [init_1, init_2], [push_1, push_2]\n    else:\n        # create queue\n        q = tf.FIFOQueue(\n            rate,\n            dtypes=tf.float32,\n            shapes=(batch_size, depth, n_inputs))\n        init = q.enqueue_many(\n            tf.zeros((rate, batch_size, depth, n_inputs)))\n        state = q.dequeue()\n        push = q.enqueue(X)\n\n        # get pretrained weights\n        W = tf.get_variable(\n            name=name + \'/W\',\n            shape=[1, filter_length, n_inputs, n_outputs],\n            dtype=tf.float32)\n        b = tf.get_variable(\n            name=name + \'/biases\',\n            shape=[n_outputs],\n            dtype=tf.float32)\n        W_q = tf.slice(W, [0, 0, 0, 0], [-1, 1, -1, -1])\n        W_x = tf.slice(W, [0, 1, 0, 0], [-1, 1, -1, -1])\n\n        # perform op w/ cached states\n        y = tf.nn.bias_add(\n            tf.matmul(state[:, 0, :], W_q[0][0]) +\n            tf.matmul(X[:, 0, :], W_x[0][0]),\n            b)\n        return tf.expand_dims(y, 1), [init], [push]\n\n\ndef linear(X, n_inputs, n_outputs, name):\n    """"""Summary\n\n    Parameters\n    ----------\n    X : TYPE\n        Description\n    n_inputs : TYPE\n        Description\n    n_outputs : TYPE\n        Description\n    name : TYPE\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    W = tf.get_variable(\n        name=name + \'/W\',\n        shape=[1, 1, n_inputs, n_outputs],\n        dtype=tf.float32)\n    b = tf.get_variable(\n        name=name + \'/biases\',\n        shape=[n_outputs],\n        dtype=tf.float32)\n    # ipdb.set_trace()\n    y = tf.nn.bias_add(tf.matmul(X[:, 0, :], W[0][0]), b)\n    return tf.expand_dims(y, 1)\n'"
cadl/word2vec.py,15,"b'""""""Word2Vec model.\n""""""\n""""""\nCopyright 2017 Parag K. Mital.  See also NOTICE.md.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport tensorflow as tf\n\n\ndef build_model(batch_size=128, vocab_size=50000, embedding_size=128,\n                n_neg_samples=64):\n    """"""Summary\n\n    Parameters\n    ----------\n    batch_size : int, optional\n        Description\n    vocab_size : int, optional\n        Description\n    embedding_size : int, optional\n        Description\n    n_neg_samples : int, optional\n        Description\n\n    Returns\n    -------\n    TYPE\n        Description\n    """"""\n    # Input placeholders\n    center_words = tf.placeholder(\n        tf.int32, shape=[batch_size], name=\'center_words\')\n    target_words = tf.placeholder(\n        tf.int32, shape=[batch_size, 1], name=\'target_words\')\n\n    # This is the important part of the model which will embed a word id\n    # into an embedding of size `embedding_size`\n    embed_matrix = tf.get_variable(\n        name=\'embedding\',\n        shape=[vocab_size, embedding_size],\n        dtype=tf.float32,\n        initializer=tf.random_uniform_initializer(-1.0, 1.0))\n\n    # Define the inference\n    embed = tf.nn.embedding_lookup(embed_matrix, center_words, name=\'embed\')\n\n    # Construct variables for NCE loss\n    nce_weight = tf.get_variable(\n        name=\'nce/weight\',\n        shape=[vocab_size, embedding_size],\n        dtype=tf.float32,\n        initializer=tf.truncated_normal_initializer(\n            stddev=1.0 / (embedding_size ** 0.5)))\n    nce_bias = tf.get_variable(\n        name=\'nce/bias\',\n        shape=[vocab_size],\n        dtype=tf.float32,\n        initializer=tf.constant_initializer())\n\n    # Define loss function to be NCE loss function\n    loss = tf.reduce_mean(tf.nn.nce_loss(\n        weights=nce_weight,\n        biases=nce_bias,\n        labels=target_words,\n        inputs=embed,\n        num_sampled=n_neg_samples,\n        num_classes=vocab_size), name=\'loss\')\n\n    return locals()\n'"
docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# CADL documentation build configuration file, created by\n# sphinx-quickstart on Tue Sep  5 00:50:36 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'..\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinxcontrib.napoleon\',\n    \'sphinx.ext.imgmath\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = [\'.rst\', \'.md\']\n# source_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'CADL\'\ncopyright = \'2017, Parag K. Mital\'\nauthor = \'Parag K. Mital\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'1.0.0\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'1.0.0\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nimport sphinx_rtd_theme\nhtml_theme = ""sphinx_rtd_theme""\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'CADLdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'CADL.tex\', \'CADL Documentation\',\n     \'Parag K. Mital\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'cadl\', \'CADL Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'CADL\', \'CADL Documentation\',\n     author, \'CADL\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n'"
tests/test_batch_norm.py,2,"b'import tensorflow as tf\nfrom cadl import batch_norm\nfrom numpy.testing import run_module_suite\n\n\nclass TestBatchNorm:\n    def test_batch_norm(self):\n        x = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n        phase_train = tf.placeholder(dtype=tf.bool)\n        op = batch_norm.batch_norm(x=x, phase_train=phase_train)\n        assert(op is not None)\n\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_celeb_vaegan.py,3,"b'import numpy as np\nimport tensorflow as tf\nfrom cadl import vaegan, celeb_vaegan, utils\nfrom cadl.utils import exists, stdout_redirect\nimport io\nfrom numpy.testing import run_module_suite\n\n\n\nclass TestCelebVAEGAN:\n    def test_celeb_vaegan_model_exists(self):\n        assert(exists(\'https://s3.amazonaws.com/cadl/models/celeb.vaegan.tfmodel\'))\n\n    def test_celeb_vaegan_labels_exists(self):\n        assert(exists(\'https://s3.amazonaws.com/cadl/models/list_attr_celeba.txt\'))\n\n    def test_celeb_files_exist(self):\n        assert(len(utils.get_celeb_files()) == 100)\n\n    def test_celeb_vaegan_training(self):\n        with stdout_redirect(io.StringIO()) as stdout:\n            with tf.Graph().as_default():\n                vaegan.test_celeb(n_epochs=1)\n        stdout.seek(0)\n        stdout.readlines()[-1] == ""Done training -- epoch limit reached\\n""\n\n    def test_celeb_vaegan_model(self):\n        with tf.Graph().as_default() as g, tf.Session(graph=g):\n            net = celeb_vaegan.get_celeb_vaegan_model()\n            tf.import_graph_def(\n                net[\'graph_def\'],\n                name=\'net\',\n                input_map={\n                    \'encoder/variational/random_normal:0\':\n                    np.zeros(512, dtype=np.float32)\n                }\n            )\n            names = [op.name for op in g.get_operations()]\n            assert(\'net/x\' in names)\n            assert(len(names) == 168)\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_charrnn.py,2,"b'import tensorflow as tf\nfrom cadl import charrnn\nfrom cadl.utils import stdout_redirect\nfrom numpy.testing import run_module_suite\nimport io\n\n\nclass TestCharRNN:\n    def test_alice_char_rnn_model(self):\n        with tf.Graph().as_default():\n            charrnn.test_alice()\n\n    def test_trump_char_rnn_model(self):\n        with stdout_redirect(io.StringIO()) as stdout:\n            with tf.Graph().as_default():\n                charrnn.test_trump()\n        stdout.seek(0)\n        #stdout.readlines()[-1] == ""Done training -- epoch limit reached\\n""\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_cornell.py,0,"b'from cadl import cornell\nfrom cadl.utils import exists\nfrom numpy.testing import run_module_suite\n\n\nclass TestCornell:\n    def test_cornell_exists(self):\n        assert(exists(\'https://s3.amazonaws.com/cadl/models/cornell_movie_dialogs_corpus.zip\'))\n\n    def test_cornell_download(self):\n        cornell.download_cornell()\n\n    def test_cornell_scripts(self):\n        scripts = cornell.get_scripts()\n        assert(scripts[0] == \'Can we make this quick?  \'\n                \'Roxanne Korrine and Andrew Barrett are having \'\n                \'an incredibly horrendous public break- up on the quad.  Again.\')\n        assert(len(scripts) == 304713)\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_cycle_gan.py,0,"b'import numpy as np\nfrom cadl import cycle_gan\nfrom numpy.testing import run_module_suite\n\n\nclass TestCycleGAN:\n    def test_training_dataset(self):\n        img1 = np.random.rand(100, 256, 256, 3).astype(np.float32)\n        img2 = np.random.rand(100, 256, 256, 3).astype(np.float32)\n        cycle_gan.train(img1, img2, ckpt_path=\'test\', n_epochs=1)\n\n\n    def test_training_random_crop(self):\n        img1 = np.random.rand(1024, 1024, 3).astype(np.float32)\n        img2 = np.random.rand(1024, 1024, 3).astype(np.float32)\n        cycle_gan.train(img1, img2, ckpt_path=\'test\', n_epochs=1)\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_dataset_utils.py,7,"b'import numpy as np\nimport tensorflow as tf\nfrom cadl import dataset_utils as dsu\nfrom cadl.utils import exists\nfrom numpy.testing import run_module_suite\n\n\nclass TestDatasetUtils:\n\n    def test_gtzan_exists(self):\n        assert (exists(\'http://opihi.cs.uvic.ca/sound/music_speech.tar.gz\'))\n\n    def test_gtzan_loads(self):\n        Xs, Ys = dsu.gtzan_music_speech_load()\n        assert (Xs.shape == (128, 2583, 256, 2))\n        assert (Ys.shape == (128,))\n        assert (np.sum(Ys) == 64)\n\n    def test_cifar_exists(self):\n        assert (\n            exists(\'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'))\n\n    def test_cifar_loads(self):\n        Xs, Ys = dsu.cifar10_load()\n        assert (Xs.shape == (50000, 32, 32, 3))\n        assert (Ys.shape == (50000,))\n        assert (np.mean(Ys) == 4.5)\n        assert (np.mean(Xs) == 120.70756512369792)\n\n    def test_tiny_imagenet_exists(self):\n        assert (\'http://cs231n.stanford.edu/tiny-imagenet-200.zip\')\n\n    def test_tiny_imagenet_loads(self):\n        Xs, Ys = dsu.tiny_imagenet_load()\n        assert (len(Xs) == 120000)\n        assert (Xs[0].endswith(\'n02226429_40.JPEG\'))\n        assert (Ys[0] == \'grasshopper, hopper\')\n\n    def test_input_pipeline(self):\n        Xs, Ys = dsu.tiny_imagenet_load()\n        n_batches = 0\n        batch_size = 10\n        with tf.Graph().as_default(), tf.Session() as sess:\n            batch_generator = dsu.create_input_pipeline(\n                Xs[:100],\n                batch_size=batch_size,\n                n_epochs=1,\n                shape=(64, 64, 3),\n                crop_shape=(64, 64, 3))\n            init_op = tf.group(tf.global_variables_initializer(),\n                               tf.local_variables_initializer())\n            sess.run(init_op)\n            coord = tf.train.Coordinator()\n            tf.get_default_graph().finalize()\n            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n            try:\n                while not coord.should_stop():\n                    batch = sess.run(batch_generator)\n                    assert (batch.shape == (batch_size, 64, 64, 3))\n                    n_batches += 1\n            except tf.errors.OutOfRangeError:\n                pass\n            finally:\n                coord.request_stop()\n            coord.join(threads)\n        assert (n_batches == 10)\n\n    def test_dataset(self):\n        Xs, Ys = dsu.cifar10_load()\n        ds = dsu.Dataset(Xs=Xs, ys=Ys, split=[0.8, 0.1, 0.1], one_hot=False)\n        assert (ds.X.shape == (50000, 32, 32, 3))\n        assert (ds.Y.shape == (50000,))\n\n    def test_dataset_split(self):\n        Xs, Ys = dsu.cifar10_load()\n        ds = dsu.Dataset(Xs=Xs, ys=Ys, split=[0.8, 0.1, 0.1], one_hot=False)\n        assert (ds.train.images.shape == (40000, 32, 32, 3))\n        assert (ds.valid.images.shape == (5000, 32, 32, 3))\n        assert (ds.test.images.shape == (5000, 32, 32, 3))\n\n    def test_dataset_split_batch_generator(self):\n        Xs, Ys = dsu.cifar10_load()\n        ds = dsu.Dataset(Xs=Xs, ys=Ys, split=[0.8, 0.1, 0.1], one_hot=False)\n        X_i, Y_i = next(ds.train.next_batch())\n        assert (X_i.shape == (100, 32, 32, 3))\n        assert (Y_i.shape == (100,))\n\n    def test_dataset_onehot(self):\n        Xs, Ys = dsu.cifar10_load()\n        ds = dsu.Dataset(\n            Xs=Xs, ys=Ys, split=[0.8, 0.1, 0.1], one_hot=True, n_classes=10)\n        X_i, Y_i = next(ds.train.next_batch())\n        assert (X_i.shape == (100, 32, 32, 3))\n        assert (Y_i.shape == (100, 10))\n\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_datasets.py,0,"b'from cadl import datasets\nfrom numpy.testing import run_module_suite\n\n\nclass TestDatasets:\n    def test_mnist(self):\n        ds = datasets.MNIST()\n        assert(ds.X.shape == (70000, 784))\n\n    def test_celeb(self):\n        datasets.CELEB()\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_deepdream.py,0,"b'import numpy as np\nfrom cadl import deepdream\nfrom numpy.testing import run_module_suite\n\n\nclass TestDeepDream():\n    def test_deepdream(self):\n        img = np.random.rand(256, 256, 3)\n        deepdream.deep_dream(img, n_iterations=5)\n\n    def test_guided_deepdream(self):\n        img = np.random.rand(256, 256, 3)\n        deepdream.guided_dream(img, n_iterations=5)\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_draw.py,0,"b'from numpy.testing import run_module_suite\nfrom cadl import draw, dataset_utils as dsu\n\n\nclass TestDRAW:\n    def test_draw_train_dataset(self):\n        Xs, ys = dsu.cifar10_load()\n        Xs = Xs[:100, ...].reshape((100, 3072))\n        ys = ys[:100]\n        ds = dsu.Dataset(Xs, ys, split=(0.5, 0.25, 0.25))\n        draw.train_dataset(n_epochs=1, batch_size=25, ds=ds, A=32, B=32, C=3)\n\n    def test_draw_train_input_pipeline(self):\n        Xs, ys = dsu.tiny_imagenet_load()\n        draw.train_input_pipeline(\n                Xs[:100], batch_size=20, n_epochs=1, A=64, B=64, C=3,\n                input_shape=(64, 64, 3))\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_fastwavenet.py,0,"b'from cadl import fastwavenet\nfrom numpy.testing import run_module_suite\n\n\ndef test_fastwavenet_model():\n    fastwavenet.create_generation_model()\n\n\ndef test_sequence_length():\n    res = fastwavenet.get_sequence_length(n_stages=3, n_layers_per_stage=10)\n    assert(res == 6144)\n\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_gan.py,2,"b'import tensorflow as tf\nfrom cadl import gan\nfrom cadl.utils import stdout_redirect\nfrom cadl.datasets import CELEB\nfrom numpy.testing import run_module_suite\nimport io\n\n\ndef test_gan_model():\n    with tf.Graph().as_default(), tf.Session():\n        gan.GAN(input_shape=[None, 32, 32, 3], n_latent=10, n_features=16, rgb=True)\n\ndef test_gan_training():\n    files = CELEB()[:100]\n    with stdout_redirect(io.StringIO()) as stdout:\n        with tf.Graph().as_default():\n            gan.train_input_pipeline(files=files, n_epochs=1, batch_size=10)\n    stdout.seek(0)\n    stdout.readlines()[-1] == ""Done training -- epoch limit reached\\n""\n\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_gif.py,0,"b'from cadl import gif\nfrom cadl.datasets import CELEB\nfrom numpy.testing import run_module_suite\nfrom matplotlib.pyplot import imread\nimport os\n\n\ndef test_gif():\n    files = CELEB()[:5]\n    imgs = [imread(f) for f in files]\n    gif.build_gif(imgs, saveto=\'test.gif\')\n    assert(os.path.exists(\'test.gif\'))\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_glove.py,0,"b'from cadl import glove\nfrom cadl.utils import exists\nfrom numpy.testing import run_module_suite\n\n\ndef test_glove_exists():\n    assert(exists(\'http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\'))\n\ndef test_glove_model():\n    model = glove.get_model()\n    assert(model[0].shape == (400001, 300))\n    assert(len(model[1]) == 400001)\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_librispeech.py,0,"b'from cadl import librispeech\nfrom cadl.utils import exists\nfrom numpy.testing import run_module_suite\n\n\ndef test_librispeech_exists():\n    assert(exists(\'http://www.openslr.org/resources/12/dev-clean.tar.gz\'))\n    assert(exists(\'http://www.openslr.org/resources/12/train-clean-100.tar.gz\'))\n    assert(exists(\'http://www.openslr.org/resources/12/train-clean-360.tar.gz\'))\n\ndef test_librispeech_dataset():\n    ds = librispeech.get_dataset()\n    assert(len(ds) == 106717)\n\ndef test_librispeech_batch():\n    ds = librispeech.get_dataset()\n    batch = next(librispeech.batch_generator(ds, batch_size=32))\n    assert(batch[0].shape == (32, 6144))\n    assert(batch[1].shape == (32,))\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_magenta_utils.py,0,"b'import os\nfrom cadl import magenta_utils\nfrom cadl.utils import exists, download\nfrom numpy.testing import run_module_suite\nfrom magenta.music import midi_io\n\n\ndef test_harry_potter_exists():\n    assert(exists(\'https://s3.amazonaws.com/cadl/share/27334_Harry-Potter-Theme-Monophonic.mid\'))\n    assert(exists(\'https://s3.amazonaws.com/cadl/share/21150_Harry-Potter-and-the-Philosophers-Stone.mid\'))\n\n\ndef test_basic_rnn_mag_exists():\n    assert(exists(\'https://s3.amazonaws.com/cadl/models/basic_rnn.mag\'))\n\n\ndef test_harry_potter():\n    primer_midi = download(\'https://s3.amazonaws.com/cadl/share/21150_Harry-Potter-and-the-Philosophers-Stone.mid\')\n    primer_sequence = midi_io.midi_file_to_sequence_proto(primer_midi)\n    assert(len(primer_sequence.notes) == 282)\n    assert(len(primer_sequence.time_signatures) == 1)\n\n\ndef test_convert_to_monophonic():\n    primer_midi = download(\'https://s3.amazonaws.com/cadl/share/21150_Harry-Potter-and-the-Philosophers-Stone.mid\')\n    primer_sequence = midi_io.midi_file_to_sequence_proto(primer_midi)\n    magenta_utils.convert_to_monophonic(primer_sequence)\n    assert(len(primer_sequence.notes) == 254)\n\n\ndef test_melody_rnn_generation():\n    primer_midi = download(\'https://s3.amazonaws.com/cadl/share/21150_Harry-Potter-and-the-Philosophers-Stone.mid\')\n    download(\'https://s3.amazonaws.com/cadl/models/basic_rnn.mag\')\n\n    magenta_utils.synthesize(primer_midi)\n    fname = \'primer.mid\'\n    assert(os.path.exists(\'primer.mid\'))\n    generated_primer = midi_io.midi_file_to_sequence_proto(fname)\n    assert(len(generated_primer.notes) == 14)\n\n    fname = \'synthesis.mid\'\n    assert(os.path.exists(\'synthesis.mid\'))\n    generated_synthesis = midi_io.midi_file_to_sequence_proto(fname)\n    assert(len(generated_synthesis.notes) == 243)\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_models.py,6,"b'import tensorflow as tf\nfrom cadl import inception, vgg16, i2v\nfrom cadl.utils import exists\nfrom numpy.testing import run_module_suite\n\nclass TestModels():\n    def test_inception_exists(self):\n        assert(exists(\'https://s3.amazonaws.com/cadl/models/inception-2015-12-05.tgz\'))\n\n    def test_inception_loads(self):\n        net = inception.get_inception_model()\n        with tf.Graph().as_default() as g, tf.Session():\n            tf.import_graph_def(net[\'graph_def\'])\n            ops = g.get_operations()\n            names = [op.name for op in ops]\n        assert(len(names) == 370)\n        assert(names[-1].endswith(\'output2\'))\n        assert(names[0].endswith(\'input\'))\n\n    def test_i2v_exists(self):\n        assert(exists(\'https://s3.amazonaws.com/cadl/models/illust2vec.tfmodel\'))\n\n    def test_i2v_loads(self):\n        net = i2v.get_i2v_tag_model()\n        with tf.Graph().as_default() as g, tf.Session():\n            tf.import_graph_def(net[\'graph_def\'])\n            ops = g.get_operations()\n            names = [op.name for op in ops]\n        assert(len(names) == 115)\n        assert(names[0].endswith(\'Placeholder\'))\n        assert(names[-1].endswith(\'prob\'))\n\n    def test_vgg16_exists(self):\n        assert(exists(\'https://s3.amazonaws.com/cadl/models/vgg16.tfmodel\'))\n\n    def test_vgg16_loads(self):\n        net = vgg16.get_vgg_model()\n        with tf.Graph().as_default() as g, tf.Session():\n            tf.import_graph_def(net[\'graph_def\'])\n            ops = g.get_operations()\n            names = [op.name for op in ops]\n        assert(len(names) == 129)\n        assert(names[0].endswith(\'images\'))\n        assert(names[-1].endswith(\'init\'))\n        assert(names[-2].endswith(\'prob\'))\n\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_nsynth.py,0,"b'import numpy as np\nfrom cadl import nsynth\nfrom cadl.utils import exists, download\nfrom numpy.testing import run_module_suite\n\n\ndef test_model_exists():\n    assert(exists(\'https://s3.amazonaws.com/cadl/models/model.ckpt-200000.data-00000-of-00001\'))\n    assert(exists(\'https://s3.amazonaws.com/cadl/models/model.ckpt-200000.index\'))\n    assert(exists(\'https://s3.amazonaws.com/cadl/models/model.ckpt-200000.meta\'))\n\n\ndef test_generation():\n    download(\'https://s3.amazonaws.com/cadl/models/model.ckpt-200000.data-00000-of-00001\')\n    download(\'https://s3.amazonaws.com/cadl/models/model.ckpt-200000.index\')\n    download(\'https://s3.amazonaws.com/cadl/models/model.ckpt-200000.meta\')\n    wav_file = download(\'https://s3.amazonaws.com/cadl/share/trumpet.wav\')\n    res = nsynth.synthesize(wav_file, synth_length=100)\n    max_idx = np.max(np.where(res))\n    assert(max_idx > 90 and max_idx < 100)\n    assert(np.max(res) > 0.0)\n\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_pixelcnn.py,0,"b'from cadl import pixelcnn\nfrom cadl.utils import exists\nfrom numpy.testing import run_module_suite\n\n\ndef test_pixelcnn_model():\n    model = pixelcnn.build_conditional_pixel_cnn_model()\n    assert(\'X\' in model)\n    assert(\'cost\' in model)\n    assert(\'preds\' in model)\n    assert(\'sampled_preds\' in model)\n    assert(\'summaries\' in model)\n    assert(model[\'X\'].shape.as_list() == [None, 32, 32, 3])\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_seq2seq.py,5,"b'from cadl import seq2seq, cornell\nfrom numpy.testing import run_module_suite\nimport tensorflow as tf\n\n\ndef test_embedding_layer():\n    with tf.Graph().as_default():\n        vocab_size = 100\n        embed_size = 20\n        x = tf.placeholder(name=\'x\', shape=[None, 1], dtype=tf.int32)\n        embed_lookup, embed_matrix = seq2seq._create_embedding(x, vocab_size, embed_size)\n        assert(embed_lookup.shape.as_list() == [None, 1, embed_size])\n        assert(embed_matrix.shape.as_list() == [vocab_size, embed_size])\n\ndef test_rnn_cell():\n    with tf.Graph().as_default():\n        n_neurons = 5\n        n_layers = 3\n        keep_prob = 1.0\n        cell = seq2seq._create_rnn_cell(n_neurons, n_layers, keep_prob)\n        assert(cell.output_size == n_neurons)\n        assert(len(cell.state_size) == n_layers)\n\ndef test_model():\n    with tf.Graph().as_default():\n        seq2seq.create_model()\n\ndef test_training():\n    txt = cornell.get_scripts()\n    with tf.Graph().as_default():\n        seq2seq.train(txt[:20], batch_size=10, n_epochs=1)\n\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_stylenet.py,0,b''
tests/test_tedlium.py,0,b''
tests/test_utils.py,0,"b'from cadl import librispeech\nfrom cadl.utils import exists\nfrom numpy.testing import run_module_suite\n\n\ndef test_librispeech_exists():\n    assert(exists(\'http://www.openslr.org/resources/12/dev-clean.tar.gz\'))\n    assert(exists(\'http://www.openslr.org/resources/12/train-clean-100.tar.gz\'))\n    assert(exists(\'http://www.openslr.org/resources/12/train-clean-360.tar.gz\'))\n\ndef test_librispeech_dataset():\n    ds = librispeech.get_dataset(convert_to_wav=True)\n    assert(len(ds) == 106717)\n\ndef test_librispeech_batch_generator():\n    ds = librispeech.get_dataset()\n    batch = next(librispeech.batch_generator(ds, batch_size=32))\n    assert(batch[0].shape == (32, 6144))\n\nif __name__ == ""__main__"":\n    run_module_suite()\n'"
tests/test_vae.py,0,b''
tests/test_vaegan.py,0,b''
tests/test_vctk.py,0,b''
tests/test_vgg16.py,0,b''
tests/test_wavenet.py,0,b''
tests/test_word2vec.py,0,b''
cadl/deprecated/__init__.py,0,b''
cadl/deprecated/seq2seq.py,53,"b'""""""Sequence to Sequence models w/ Attention and BiDirectional Dynamic RNNs.\n\nParag K. Mital\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nimport nltk\nimport pickle\nfrom cadl import cornell\n\n# Special vocabulary symbols:\n# PAD is used to pad a sequence to a fixed size\n# GO is for the end of the encoding\n# EOS is for the end of decoding\n# UNK is for out of vocabulary words\n_PAD, _GO, _EOS, _UNK = ""_PAD"", ""_GO"", ""_EOS"", ""_UNK""\n_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\nPAD_ID, GO_ID, EOS_ID, UNK_ID = range(4)\n\n\ndef _create_embedding(x, vocab_size, embed_size, embed_matrix=None):\n    # Creating an embedding matrix if one isn\'t given\n    if embed_matrix is None:\n        # This is a big matrix\n        embed_matrix = tf.get_variable(\n            name=""embedding_matrix"",\n            shape=[vocab_size, embed_size],\n            dtype=tf.float32,\n            initializer=tf.random_uniform_initializer(-1.0, 1.0))\n\n    # Perform the lookup of ids in x and perform the embedding to embed_size\n    # [batch_size, max_time, embed_size]\n    embed = tf.nn.embedding_lookup(embed_matrix, x)\n\n    return embed, embed_matrix\n\n\ndef _create_encoder(embed, lengths, batch_size, n_enc_neurons, n_layers,\n                    use_lstm):\n    # Create the RNN Cells for encoder\n    if use_lstm:\n        cell_fw = tf.contrib.rnn.BasicLSTMCell(n_enc_neurons)\n    else:\n        cell_fw = tf.contrib.rnn.GRUCell(n_enc_neurons)\n\n    # Build deeper recurrent net if using more than 1 layer\n    if n_layers > 1:\n        cell_fw = tf.contrib.rnn.MultiRNNCell([cell_fw] * n_layers)\n\n    # Create the internal multi-layer cell for the backward RNN.\n    if use_lstm:\n        cell_bw = tf.contrib.rnn.BasicLSTMCell(n_enc_neurons)\n    else:\n        cell_bw = tf.contrib.rnn.GRUCell(n_enc_neurons)\n\n    # Build deeper recurrent net if using more than 1 layer\n    if n_layers > 1:\n        cell_bw = tf.contrib.rnn.MultiRNNCell([cell_bw] * n_layers)\n\n    # Now hookup the cells to the input\n    # [batch_size, max_time, embed_size]\n    # We only use the forward cell\'s final state since the decoder is\n    # not a bidirectional rnn\n    (_, final_state) = tf.nn.bidirectional_dynamic_rnn(\n        cell_fw=cell_fw,\n        cell_bw=cell_bw,\n        inputs=embed,\n        sequence_length=lengths,\n        time_major=False,\n        dtype=tf.float32)\n    return final_state\n\n\ndef _create_train_decoder(cells, encoder_state, encoding_lengths, decoding,\n                          decoding_lengths, embed_matrix, batch_size,\n                          target_vocab_size, use_attention, n_dec_neurons,\n                          scope, output_fn, max_sequence_size):\n\n    if use_attention:\n        attention_states = tf.zeros([batch_size, 1, cells.output_size])\n        # Pass in the final hidden states of the encoder\'s RNN which it will\n        # attend over... thus determining which ones are useful for the\n        # decoding.\n        (attn_keys, attn_vals, attn_score_fn, attn_construct_fn) = \\\n            tf.contrib.seq2seq.prepare_attention(\n                attention_states=attention_states,\n                attention_option=\'bahdanau\',\n                num_units=n_dec_neurons)\n\n        # Use the final state of the encoder as input and build a decoder also\n        # taking information from the attention module acting on the encoder_state.\n        decoder_fn = \\\n            tf.contrib.seq2seq.attention_decoder_fn_train(\n                encoder_state=encoder_state,\n                attention_keys=attn_keys,\n                attention_values=attn_vals,\n                attention_score_fn=attn_score_fn,\n                attention_construct_fn=attn_construct_fn)\n\n    else:\n        # Build training decoder function\n        decoder_fn = \\\n            tf.contrib.seq2seq.simple_decoder_fn_train(\n                encoder_state=encoder_state)\n\n    # Build training rnn decoder\n    outputs, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(\n        cell=cells,\n        decoder_fn=decoder_fn,\n        inputs=decoding,\n        sequence_length=decoding_lengths,\n        time_major=False,\n        scope=scope)\n\n    # Convert to vocab size\n    train_logits = output_fn(outputs)\n\n    return train_logits\n\n\ndef _create_inference_decoder(cells, encoder_state, encoding_lengths, decoding,\n                              decoding_lengths, embed_matrix, batch_size,\n                              n_dec_neurons, target_vocab_size, use_attention,\n                              scope, output_fn, max_sequence_size):\n\n    if use_attention:\n        attention_states = tf.zeros([batch_size, 1, cells.output_size])\n        # Pass in the final hidden states of the encoder\'s RNN which it will\n        # attend over... thus determining which ones are useful for the\n        # decoding.\n        (attn_keys, attn_vals, attn_score_fn, attn_construct_fn) = \\\n            tf.contrib.seq2seq.prepare_attention(\n                attention_states=attention_states,\n                attention_option=\'bahdanau\',\n                num_units=n_dec_neurons)\n\n        # Build a separate inference network to use during generation.\n        decoder_fn_inference = \\\n            tf.contrib.seq2seq.attention_decoder_fn_inference(\n                output_fn=output_fn,\n                encoder_state=encoder_state,\n                attention_keys=attn_keys,\n                attention_values=attn_vals,\n                attention_score_fn=attn_score_fn,\n                attention_construct_fn=attn_construct_fn,\n                embeddings=embed_matrix,\n                start_of_sequence_id=GO_ID,\n                end_of_sequence_id=EOS_ID,\n                maximum_length=max_sequence_size,\n                num_decoder_symbols=target_vocab_size)\n    else:\n        # Build inference decoder function\n        decoder_fn_inference = \\\n            tf.contrib.seq2seq.simple_decoder_fn_inference(\n                output_fn=output_fn,\n                encoder_state=encoder_state,\n                embeddings=embed_matrix,\n                start_of_sequence_id=GO_ID,\n                end_of_sequence_id=EOS_ID,\n                maximum_length=max_sequence_size,\n                num_decoder_symbols=target_vocab_size)\n\n    # Build inference rnn decoder (handles output to vocab size, so we\n    # do not have to apply the output function).\n    (infer_logits, _, _) = tf.contrib.seq2seq.dynamic_rnn_decoder(\n        cell=cells,\n        decoder_fn=decoder_fn_inference,\n        time_major=False,\n        scope=scope)\n\n    return infer_logits\n\n\ndef create_model(source_vocab_size=20000,\n                 target_vocab_size=20000,\n                 input_embed_size=1024,\n                 target_embed_size=1024,\n                 share_input_and_target_embedding=True,\n                 n_neurons=512,\n                 n_layers=3,\n                 use_lstm=True,\n                 use_attention=True,\n                 max_sequence_size=50):\n\n    n_enc_neurons = n_neurons\n    n_dec_neurons = n_neurons\n\n    # First sentence (i.e. input, original language sentence before translation)\n    # [batch_size, max_time]\n    source = tf.placeholder(tf.int32, shape=(None, None), name=\'source\')\n\n    # User should also pass in the sequence lengths\n    source_lengths = tf.placeholder(\n        tf.int32, shape=(None), name=\'source_lengths\')\n\n    # Second sentence (i.e. reply, translation, etc...)\n    # [batch_size, max_time]\n    target = tf.placeholder(tf.int32, shape=(None, None), name=\'target\')\n\n    # User should also pass in the sequence lengths\n    target_lengths = tf.placeholder(\n        tf.int32, shape=(None), name=\'target_lengths\')\n\n    # Get symbolic shapes\n    batch_size, sequence_size = tf.unstack(tf.shape(source))\n\n    with tf.variable_scope(\'target/slicing\'):\n        slice = tf.slice(target, [0, 0], [batch_size, -1])\n        decoder_input = tf.concat([tf.fill([batch_size, 1], GO_ID), slice], 1)\n\n    with tf.variable_scope(\'source/embedding\'):\n        source_embed, source_embed_matrix = _create_embedding(\n            x=source, vocab_size=source_vocab_size, embed_size=input_embed_size)\n\n    with tf.variable_scope(\'target/embedding\'):\n        # Check if we need a new embedding matrix or not.  If we are for\n        # instance translating to another language, then we\'d need different\n        # vocabularies for the input and outputs, and so new embeddings.\n        # However if we are for instance building a chatbot with the same\n        # language, then it doesn\'t make sense to have different embeddings and\n        # we should share them.\n        if (share_input_and_target_embedding and\n                source_vocab_size == target_vocab_size):\n            target_embed, target_embed_matrix = _create_embedding(\n                x=decoder_input,\n                vocab_size=target_vocab_size,\n                embed_size=target_embed_size,\n                embed_matrix=source_embed_matrix)\n        elif source_vocab_size != target_vocab_size:\n            raise ValueError(\n                \'source_vocab_size must equal target_vocab_size if \' +\n                \'sharing input and target embeddings\')\n        else:\n            target_embed, target_embed_matrix = _create_embedding(\n                x=target,\n                vocab_size=target_vocab_size,\n                embed_size=target_embed_size)\n\n    # Build the encoder\n    with tf.variable_scope(\'encoder\'):\n        encoder_state = _create_encoder(\n            embed=source_embed,\n            lengths=source_lengths,\n            batch_size=batch_size,\n            n_enc_neurons=n_enc_neurons,\n            n_layers=n_layers,\n            use_lstm=use_lstm)\n\n    # Build the decoder\n    with tf.variable_scope(\'decoder\') as scope:\n\n        def output_fn(x):\n            return tf.contrib.layers.fully_connected(\n                inputs=x,\n                num_outputs=target_vocab_size,\n                activation_fn=None,\n                scope=scope)\n\n        # Create the RNN Cells for decoder\n        if use_lstm:\n            cells = tf.contrib.rnn.BasicLSTMCell(n_dec_neurons)\n        else:\n            cells = tf.contrib.rnn.GRUCell(n_dec_neurons)\n\n        # Build deeper recurrent net if using more than 1 layer\n        if n_layers > 1:\n            cells = tf.contrib.rnn.MultiRNNCell([cells] * n_layers)\n\n        decoding_train = _create_train_decoder(\n            cells=cells,\n            encoder_state=encoder_state[0],\n            encoding_lengths=source_lengths,\n            decoding=target_embed,\n            decoding_lengths=target_lengths,\n            embed_matrix=target_embed_matrix,\n            batch_size=batch_size,\n            target_vocab_size=target_vocab_size,\n            use_attention=use_attention,\n            scope=scope,\n            max_sequence_size=max_sequence_size,\n            n_dec_neurons=n_dec_neurons,\n            output_fn=output_fn)\n\n        # Inference model:\n        scope.reuse_variables()\n        decoding_inference = _create_inference_decoder(\n            cells=cells,\n            encoder_state=encoder_state[0],\n            encoding_lengths=source_lengths,\n            decoding=target_embed,\n            decoding_lengths=target_lengths,\n            embed_matrix=target_embed_matrix,\n            batch_size=batch_size,\n            target_vocab_size=target_vocab_size,\n            use_attention=use_attention,\n            scope=scope,\n            max_sequence_size=max_sequence_size,\n            n_dec_neurons=n_dec_neurons,\n            output_fn=output_fn)\n\n    with tf.variable_scope(\'loss\'):\n        weights = tf.ones(\n            [batch_size, tf.reduce_max(target_lengths)],\n            dtype=tf.float32,\n            name=""weights"")\n        loss = tf.contrib.seq2seq.sequence_loss(\n            logits=tf.reshape(decoding_train, [\n                batch_size, tf.reduce_max(target_lengths), target_vocab_size\n            ]),\n            targets=target,\n            weights=weights)\n\n    return {\n        \'loss\': loss,\n        \'source\': source,\n        \'source_lengths\': source_lengths,\n        \'target\': target,\n        \'target_lengths\': target_lengths,\n        \'thought_vector\': encoder_state,\n        \'decoder\': decoding_inference\n    }\n\n\ndef batch_generator(Xs, Ys, source_lengths, target_lengths, batch_size=50):\n    idxs = np.random.permutation(np.arange(len(Xs)))\n    n_batches = len(idxs) // batch_size\n    for batch_i in range(n_batches):\n        this_idxs = idxs[batch_i * batch_size:(batch_i + 1) * batch_size]\n        this_Xs, this_Ys = Xs[this_idxs, :], Ys[this_idxs, :]\n        this_source_lengths, this_target_lengths = source_lengths[\n            this_idxs], target_lengths[this_idxs]\n        yield (this_Xs[:, :np.max(this_source_lengths)],\n               this_Ys[:, :np.max(this_target_lengths)], this_source_lengths,\n               this_target_lengths)\n\n\ndef preprocess(text, min_count=10, max_length=50):\n    sentences = [el for s in text for el in nltk.sent_tokenize(s)]\n\n    # We\'ll first tokenize each sentence into words to get a sense of\n    # how long each sentence is:\n    words = [[word.lower() for word in nltk.word_tokenize(s)]\n             for s in sentences]\n\n    # Then see how long each sentence is:\n    lengths = np.array([len(s) for s in words])\n\n    good_idxs = np.where(lengths <= max_length)[0]\n    dataset = [words[idx] for idx in good_idxs]\n    fdist = nltk.FreqDist([word for sentence in dataset for word in sentence])\n\n    vocab_counts = [el for el in fdist.most_common() if el[1] > min_count]\n\n    # First sort the vocabulary\n    vocab = [v[0] for v in vocab_counts]\n    vocab.sort()\n\n    # Now add the special symbols:\n    vocab = _START_VOCAB + vocab\n\n    # Then create the word to id mapping\n    vocab = {k: v for v, k in enumerate(vocab)}\n\n    with open(\'vocab.pkl\', \'wb\') as fp:\n        pickle.dump(vocab, fp)\n\n    unked = word2id(dataset, vocab)\n    return unked, vocab\n\n\ndef word2id(words, vocab):\n    unked = []\n    for s in words:\n        this_sentence = [vocab.get(w, UNK_ID) for w in s]\n        unked.append(this_sentence)\n    return unked\n\n\ndef id2word(ids, vocab):\n    words = []\n    id2words = {v: k for k, v in vocab.items()}\n    for s in ids:\n        this_sentence = [id2words.get(w) for w in s]\n        words.append(this_sentence)\n    return words\n\n\ndef test_cornell():\n\n    # Get the cornell dataset text\n    text = cornell.get_scripts()\n\n    # Preprocess it to word IDs including UNKs for out of vocabulary words\n    max_sequence_size = 50\n    unked, vocab = preprocess(\n        text, min_count=10, max_length=max_sequence_size - 1)\n\n    # Get the vocabulary size\n    vocab_size = len(vocab)\n\n    # Create input output pairs formed by neighboring sentences of dialog\n    Xs_list, Ys_list = unked[:-1], unked[1:]\n\n    # Store the final lengths\n    source_lengths = np.zeros((len(Xs_list)), dtype=np.int32)\n    target_lengths = np.zeros((len(Ys_list)), dtype=np.int32)\n    Xs = np.ones((len(Xs_list), max_sequence_size), dtype=np.int32) * PAD_ID\n    Ys = np.ones((len(Ys_list), max_sequence_size), dtype=np.int32) * PAD_ID\n\n    for i, (source_i, target_i) in enumerate(zip(Xs_list, Ys_list)):\n        el = source_i\n        source_lengths[i] = len(el)\n        Xs[i, :len(el)] = el\n\n        el = target_i + [EOS_ID]\n        target_lengths[i] = len(el)\n        Ys[i, :len(el)] = el\n\n    sess = tf.Session()\n\n    net = create_model(\n        use_attention=True,\n        source_vocab_size=vocab_size,\n        target_vocab_size=vocab_size)\n    learning_rate = tf.placeholder(tf.float32, name=\'learning_rate\')\n    current_learning_rate = 0.01\n    opt = tf.train.AdamOptimizer(\n        learning_rate=learning_rate).minimize(net[\'loss\'])\n    init_op = tf.group(tf.global_variables_initializer(),\n                       tf.local_variables_initializer())\n    sess.run(init_op)\n    saver = tf.train.Saver()\n\n    def decode(sentence):\n        preprocessed = [\n            word\n            for s in nltk.sent_tokenize(sentence.lower())\n            for word in nltk.word_tokenize(s)\n        ][::-1]\n        tokens = cornell.word2id([preprocessed + [_GO]], vocab)\n        outputs = sess.run(\n            net[\'decoder\'],\n            feed_dict={\n                net[\'source\']: tokens,\n                net[\'source_lengths\']: [len(x_i) for x_i in tokens]\n            })\n        decoding = np.argmax(outputs, axis=2)\n        print(\'input:\', sentence, \'\\n\', \'output:\',\n              "" "".join(cornell.id2word(decoding, vocab)[0]))\n\n    n_epochs = 10\n    batch_size = 50\n    for epoch_i in range(n_epochs):\n        for it_i, (this_Xs, this_Ys, this_source_lengths, this_target_lengths) \\\n                    in enumerate(batch_generator(\n                        Xs, Ys, source_lengths, target_lengths, batch_size=batch_size)):\n            if it_i % 100 == 0:\n                current_learning_rate = current_learning_rate * 0.9\n                rand_idx = np.random.randint(0, high=len(text))\n                print(it_i)\n                decode(text[rand_idx])\n            l = sess.run(\n                [net[\'loss\'], opt],\n                feed_dict={\n                    learning_rate: current_learning_rate,\n                    net[\'source\']: this_Xs,\n                    net[\'target\']: this_Ys,\n                    net[\'source_lengths\']: this_source_lengths,\n                    net[\'target_lengths\']: this_target_lengths\n                })[0]\n            print(\'{}: {}\'.format(it_i, l), end=\'\\r\')\n        # End of epoch, save\n        saver.save(sess, \'./dynamic-seq2seq.ckpt\', global_step=it_i)\n\n    sess.close()\n'"
cadl/deprecated/seq2seq_model.py,27,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Sequence-to-sequence model with an attention mechanism.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nfrom cadl.deprecated import seq2seq_utils as data_utils\n\n\nclass Seq2SeqModel(object):\n  """"""Sequence-to-sequence model with attention and for multiple buckets.\n\n  This class implements a multi-layer recurrent neural network as encoder,\n  and an attention-based decoder. This is the same as the model described in\n  this paper: http://arxiv.org/abs/1412.7449 - please look there for details,\n  or into the seq2seq library for complete model implementation.\n  This class also allows to use GRU cells in addition to LSTM cells, and\n  sampled softmax to handle large output vocabulary size. A single-layer\n  version of this model, but with bi-directional encoder, was presented in\n    http://arxiv.org/abs/1409.0473\n  and sampled softmax is described in Section 3 of the following paper.\n    http://arxiv.org/abs/1412.2007\n  """"""\n\n  def __init__(self,\n               source_vocab_size,\n               target_vocab_size,\n               buckets,\n               size,\n               num_layers,\n               max_gradient_norm,\n               batch_size,\n               learning_rate,\n               learning_rate_decay_factor,\n               use_lstm=False,\n               num_samples=512,\n               forward_only=False,\n               dtype=tf.float32):\n    """"""Create the model.\n\n    Args:\n      source_vocab_size: size of the source vocabulary.\n      target_vocab_size: size of the target vocabulary.\n      buckets: a list of pairs (I, O), where I specifies maximum input length\n        that will be processed in that bucket, and O specifies maximum output\n        length. Training instances that have inputs longer than I or outputs\n        longer than O will be pushed to the next bucket and padded accordingly.\n        We assume that the list is sorted, e.g., [(2, 4), (8, 16)].\n      size: number of units in each layer of the model.\n      num_layers: number of layers in the model.\n      max_gradient_norm: gradients will be clipped to maximally this norm.\n      batch_size: the size of the batches used during training;\n        the model construction is independent of batch_size, so it can be\n        changed after initialization if this is convenient, e.g., for decoding.\n      learning_rate: learning rate to start with.\n      learning_rate_decay_factor: decay learning rate by this much when needed.\n      use_lstm: if true, we use LSTM cells instead of GRU cells.\n      num_samples: number of samples for sampled softmax.\n      forward_only: if set, we do not construct the backward pass in the model.\n      dtype: the data type to use to store internal variables.\n    """"""\n    self.source_vocab_size = source_vocab_size\n    self.target_vocab_size = target_vocab_size\n    self.buckets = buckets\n    self.batch_size = batch_size\n    self.learning_rate = tf.Variable(\n        float(learning_rate), trainable=False, dtype=dtype)\n    self.learning_rate_decay_op = self.learning_rate.assign(\n        self.learning_rate * learning_rate_decay_factor)\n    self.global_step = tf.Variable(0, trainable=False)\n\n    # If we use sampled softmax, we need an output projection.\n    output_projection = None\n    softmax_loss_function = None\n    # Sampled softmax only makes sense if we sample less than vocabulary size.\n    if num_samples > 0 and num_samples < self.target_vocab_size:\n      w_t = tf.get_variable(""proj_w"", [self.target_vocab_size, size], dtype=dtype)\n      w = tf.transpose(w_t)\n      b = tf.get_variable(""proj_b"", [self.target_vocab_size], dtype=dtype)\n      output_projection = (w, b)\n\n      def sampled_loss(labels, logits):\n        labels = tf.reshape(labels, [-1, 1])\n        # We need to compute the sampled_softmax_loss using 32bit floats to\n        # avoid numerical instabilities.\n        local_w_t = tf.cast(w_t, tf.float32)\n        local_b = tf.cast(b, tf.float32)\n        local_inputs = tf.cast(logits, tf.float32)\n        return tf.cast(\n            tf.nn.sampled_softmax_loss(\n                weights=local_w_t,\n                biases=local_b,\n                labels=labels,\n                inputs=local_inputs,\n                num_sampled=num_samples,\n                num_classes=self.target_vocab_size),\n            dtype)\n      softmax_loss_function = sampled_loss\n\n    # Create the internal multi-layer cell for our RNN.\n    def single_cell():\n      return tf.contrib.rnn.GRUCell(size)\n    if use_lstm:\n      def single_cell():\n        return tf.contrib.rnn.BasicLSTMCell(size)\n    cell = single_cell()\n    if num_layers > 1:\n      cell = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\n\n    # The seq2seq function: we use embedding for the input and attention.\n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n      return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n          encoder_inputs,\n          decoder_inputs,\n          cell,\n          num_encoder_symbols=source_vocab_size,\n          num_decoder_symbols=target_vocab_size,\n          embedding_size=size,\n          output_projection=output_projection,\n          feed_previous=do_decode,\n          dtype=dtype)\n\n    # Feeds for inputs.\n    self.encoder_inputs = []\n    self.decoder_inputs = []\n    self.target_weights = []\n    for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                                name=""encoder{0}"".format(i)))\n    for i in xrange(buckets[-1][1] + 1):\n      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                                name=""decoder{0}"".format(i)))\n      self.target_weights.append(tf.placeholder(dtype, shape=[None],\n                                                name=""weight{0}"".format(i)))\n\n    # Our targets are decoder inputs shifted by one.\n    targets = [self.decoder_inputs[i + 1]\n               for i in xrange(len(self.decoder_inputs) - 1)]\n\n    # Training outputs and losses.\n    if forward_only:\n      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n          self.encoder_inputs, self.decoder_inputs, targets,\n          self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n          softmax_loss_function=softmax_loss_function)\n      # If we use output projection, we need to project outputs for decoding.\n      if output_projection is not None:\n        for b in xrange(len(buckets)):\n          self.outputs[b] = [\n              tf.matmul(output, output_projection[0]) + output_projection[1]\n              for output in self.outputs[b]\n          ]\n    else:\n      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n          self.encoder_inputs, self.decoder_inputs, targets,\n          self.target_weights, buckets,\n          lambda x, y: seq2seq_f(x, y, False),\n          softmax_loss_function=softmax_loss_function)\n\n    # Gradients and SGD update operation for training the model.\n    params = tf.trainable_variables()\n    if not forward_only:\n      self.gradient_norms = []\n      self.updates = []\n      opt = tf.train.AdamOptimizer(self.learning_rate)\n      for b in xrange(len(buckets)):\n        gradients = tf.gradients(self.losses[b], params)\n        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n                                                         max_gradient_norm)\n        self.gradient_norms.append(norm)\n        self.updates.append(opt.apply_gradients(\n            zip(clipped_gradients, params), global_step=self.global_step))\n\n    self.saver = tf.train.Saver(tf.global_variables())\n\n  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n           bucket_id, forward_only):\n    """"""Run a step of the model feeding the given inputs.\n\n    Args:\n      session: tensorflow session to use.\n      encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n      decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n      target_weights: list of numpy float vectors to feed as target weights.\n      bucket_id: which bucket of the model to use.\n      forward_only: whether to do the backward step or only forward.\n\n    Returns:\n      A triple consisting of gradient norm (or None if we did not do backward),\n      average perplexity, and the outputs.\n\n    Raises:\n      ValueError: if length of encoder_inputs, decoder_inputs, or\n        target_weights disagrees with bucket size for the specified bucket_id.\n    """"""\n    # Check if the sizes match.\n    encoder_size, decoder_size = self.buckets[bucket_id]\n    if len(encoder_inputs) != encoder_size:\n      raise ValueError(""Encoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(encoder_inputs), encoder_size))\n    if len(decoder_inputs) != decoder_size:\n      raise ValueError(""Decoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(decoder_inputs), decoder_size))\n    if len(target_weights) != decoder_size:\n      raise ValueError(""Weights length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(target_weights), decoder_size))\n\n    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n    input_feed = {}\n    for l in xrange(encoder_size):\n      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n    for l in xrange(decoder_size):\n      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n      input_feed[self.target_weights[l].name] = target_weights[l]\n\n    # Since our targets are decoder inputs shifted by one, we need one more.\n    last_target = self.decoder_inputs[decoder_size].name\n    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n\n    # Output feed: depends on whether we do a backward step or not.\n    if not forward_only:\n      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                     self.gradient_norms[bucket_id],  # Gradient norm.\n                     self.losses[bucket_id]]  # Loss for this batch.\n    else:\n      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n      for l in xrange(decoder_size):  # Output logits.\n        output_feed.append(self.outputs[bucket_id][l])\n\n    outputs = session.run(output_feed, input_feed)\n    if not forward_only:\n      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n    else:\n      return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n\n  def get_batch(self, data, bucket_id):\n    """"""Get a random batch of data from the specified bucket, prepare for step.\n\n    To feed data in step(..) it must be a list of batch-major vectors, while\n    data here contains single length-major cases. So the main logic of this\n    function is to re-index data cases to be in the proper format for feeding.\n\n    Args:\n      data: a tuple of size len(self.buckets) in which each element contains\n        lists of pairs of input and output data that we use to create a batch.\n      bucket_id: integer, which bucket to get the batch for.\n\n    Returns:\n      The triple (encoder_inputs, decoder_inputs, target_weights) for\n      the constructed batch that has the proper format to call step(...) later.\n    """"""\n    encoder_size, decoder_size = self.buckets[bucket_id]\n    encoder_inputs, decoder_inputs = [], []\n\n    # Get a random batch of encoder and decoder inputs from data,\n    # pad them if needed, reverse encoder inputs and add GO to decoder.\n    for _ in xrange(self.batch_size):\n      encoder_input, decoder_input = random.choice(data[bucket_id])\n\n      # Encoder inputs are padded and then reversed.\n      encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n\n      # Decoder inputs get an extra ""GO"" symbol, and are padded then.\n      decoder_pad_size = decoder_size - len(decoder_input) - 1\n      decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n                            [data_utils.PAD_ID] * decoder_pad_size)\n\n    # Now we create batch-major vectors from the data selected above.\n    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n\n    # Batch encoder inputs are just re-indexed encoder_inputs.\n    for length_idx in xrange(encoder_size):\n      batch_encoder_inputs.append(\n          np.array([encoder_inputs[batch_idx][length_idx]\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n    for length_idx in xrange(decoder_size):\n      batch_decoder_inputs.append(\n          np.array([decoder_inputs[batch_idx][length_idx]\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n      # Create target_weights to be 0 for targets that are padding.\n      batch_weight = np.ones(self.batch_size, dtype=np.float32)\n      for batch_idx in xrange(self.batch_size):\n        # We set weight to 0 if the corresponding target is a PAD symbol.\n        # The corresponding target is decoder_input shifted by 1 forward.\n        if length_idx < decoder_size - 1:\n          target = decoder_inputs[batch_idx][length_idx + 1]\n        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n          batch_weight[batch_idx] = 0.0\n      batch_weights.append(batch_weight)\n    return batch_encoder_inputs, batch_decoder_inputs, batch_weights\n'"
cadl/deprecated/seq2seq_utils.py,3,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities for downloading data from WMT, tokenizing, vocabularies.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport re\nimport tarfile\n\nfrom six.moves import urllib\n\nfrom tensorflow.python.platform import gfile\nimport tensorflow as tf\n\n# Special vocabulary symbols - we always put them at the start.\n_PAD = b""_PAD""\n_GO = b""_GO""\n_EOS = b""_EOS""\n_UNK = b""_UNK""\n_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n\nPAD_ID = 0\nGO_ID = 1\nEOS_ID = 2\nUNK_ID = 3\n\n# Regular expressions used to tokenize.\n_WORD_SPLIT = re.compile(b""([.,!?\\""\':;)(])"")\n_DIGIT_RE = re.compile(br""\\d"")\n\n# URLs for WMT data.\n_WMT_ENFR_TRAIN_URL = ""http://www.statmt.org/wmt10/training-giga-fren.tar""\n_WMT_ENFR_DEV_URL = ""http://www.statmt.org/wmt15/dev-v2.tgz""\n\n\ndef maybe_download(directory, filename, url):\n  """"""Download filename from url unless it\'s already in directory.""""""\n  if not os.path.exists(directory):\n    print(""Creating directory %s"" % directory)\n    os.mkdir(directory)\n  filepath = os.path.join(directory, filename)\n  if not os.path.exists(filepath):\n    print(""Downloading %s to %s"" % (url, filepath))\n    filepath, _ = urllib.request.urlretrieve(url, filepath)\n    statinfo = os.stat(filepath)\n    print(""Successfully downloaded"", filename, statinfo.st_size, ""bytes"")\n  return filepath\n\n\ndef gunzip_file(gz_path, new_path):\n  """"""Unzips from gz_path into new_path.""""""\n  print(""Unpacking %s to %s"" % (gz_path, new_path))\n  with gzip.open(gz_path, ""rb"") as gz_file:\n    with open(new_path, ""wb"") as new_file:\n      for line in gz_file:\n        new_file.write(line)\n\n\ndef get_wmt_enfr_train_set(directory):\n  """"""Download the WMT en-fr training corpus to directory unless it\'s there.""""""\n  train_path = os.path.join(directory, ""giga-fren.release2.fixed"")\n  if not (gfile.Exists(train_path +"".fr"") and gfile.Exists(train_path +"".en"")):\n    corpus_file = maybe_download(directory, ""training-giga-fren.tar"",\n                                 _WMT_ENFR_TRAIN_URL)\n    print(""Extracting tar file %s"" % corpus_file)\n    with tarfile.open(corpus_file, ""r"") as corpus_tar:\n      corpus_tar.extractall(directory)\n    gunzip_file(train_path + "".fr.gz"", train_path + "".fr"")\n    gunzip_file(train_path + "".en.gz"", train_path + "".en"")\n  return train_path\n\n\ndef get_wmt_enfr_dev_set(directory):\n  """"""Download the WMT en-fr training corpus to directory unless it\'s there.""""""\n  dev_name = ""newstest2013""\n  dev_path = os.path.join(directory, dev_name)\n  if not (gfile.Exists(dev_path + "".fr"") and gfile.Exists(dev_path + "".en"")):\n    dev_file = maybe_download(directory, ""dev-v2.tgz"", _WMT_ENFR_DEV_URL)\n    print(""Extracting tgz file %s"" % dev_file)\n    with tarfile.open(dev_file, ""r:gz"") as dev_tar:\n      fr_dev_file = dev_tar.getmember(""dev/"" + dev_name + "".fr"")\n      en_dev_file = dev_tar.getmember(""dev/"" + dev_name + "".en"")\n      fr_dev_file.name = dev_name + "".fr""  # Extract without ""dev/"" prefix.\n      en_dev_file.name = dev_name + "".en""\n      dev_tar.extract(fr_dev_file, directory)\n      dev_tar.extract(en_dev_file, directory)\n  return dev_path\n\n\ndef basic_tokenizer(sentence):\n  """"""Very basic tokenizer: split the sentence into a list of tokens.""""""\n  words = []\n  for space_separated_fragment in sentence.strip().split():\n    words.extend(_WORD_SPLIT.split(space_separated_fragment))\n  return [w for w in words if w]\n\n\ndef create_vocabulary(vocabulary_path, data_path, max_vocabulary_size,\n                      tokenizer=None, normalize_digits=True):\n  """"""Create vocabulary file (if it does not exist yet) from data file.\n\n  Data file is assumed to contain one sentence per line. Each sentence is\n  tokenized and digits are normalized (if normalize_digits is set).\n  Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n  We write it to vocabulary_path in a one-token-per-line format, so that later\n  token in the first line gets id=0, second line gets id=1, and so on.\n\n  Args:\n    vocabulary_path: path where the vocabulary will be created.\n    data_path: data file that will be used to create vocabulary.\n    max_vocabulary_size: limit on the size of the created vocabulary.\n    tokenizer: a function to use to tokenize each data sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(vocabulary_path):\n    print(""Creating vocabulary %s from data %s"" % (vocabulary_path, data_path))\n    vocab = {}\n    with gfile.GFile(data_path, mode=""rb"") as f:\n      counter = 0\n      for line in f:\n        counter += 1\n        if counter % 100000 == 0:\n          print(""  processing line %d"" % counter)\n        line = tf.compat.as_bytes(line)\n        tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n        for w in tokens:\n          word = _DIGIT_RE.sub(b""0"", w) if normalize_digits else w\n          if word in vocab:\n            vocab[word] += 1\n          else:\n            vocab[word] = 1\n      vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n      if len(vocab_list) > max_vocabulary_size:\n        vocab_list = vocab_list[:max_vocabulary_size]\n      with gfile.GFile(vocabulary_path, mode=""wb"") as vocab_file:\n        for w in vocab_list:\n          vocab_file.write(w + b""\\n"")\n\n\ndef initialize_vocabulary(vocabulary_path):\n  """"""Initialize vocabulary from file.\n\n  We assume the vocabulary is stored one-item-per-line, so a file:\n    dog\n    cat\n  will result in a vocabulary {""dog"": 0, ""cat"": 1}, and this function will\n  also return the reversed-vocabulary [""dog"", ""cat""].\n\n  Args:\n    vocabulary_path: path to the file containing the vocabulary.\n\n  Returns:\n    a pair: the vocabulary (a dictionary mapping string to integers), and\n    the reversed vocabulary (a list, which reverses the vocabulary mapping).\n\n  Raises:\n    ValueError: if the provided vocabulary_path does not exist.\n  """"""\n  if gfile.Exists(vocabulary_path):\n    rev_vocab = []\n    with gfile.GFile(vocabulary_path, mode=""rb"") as f:\n      rev_vocab.extend(f.readlines())\n    rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]\n    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n    return vocab, rev_vocab\n  else:\n    raise ValueError(""Vocabulary file %s not found."", vocabulary_path)\n\n\ndef sentence_to_token_ids(sentence, vocabulary,\n                          tokenizer=None, normalize_digits=True):\n  """"""Convert a string to list of integers representing token-ids.\n\n  For example, a sentence ""I have a dog"" may become tokenized into\n  [""I"", ""have"", ""a"", ""dog""] and with vocabulary {""I"": 1, ""have"": 2,\n  ""a"": 4, ""dog"": 7""} this function will return [1, 2, 4, 7].\n\n  Args:\n    sentence: the sentence in bytes format to convert to token-ids.\n    vocabulary: a dictionary mapping tokens to integers.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n\n  Returns:\n    a list of integers, the token-ids for the sentence.\n  """"""\n\n  if tokenizer:\n    words = tokenizer(sentence)\n  else:\n    words = basic_tokenizer(sentence)\n  if not normalize_digits:\n    return [vocabulary.get(w, UNK_ID) for w in words]\n  # Normalize digits by 0 before looking words up in the vocabulary.\n  return [vocabulary.get(_DIGIT_RE.sub(b""0"", w), UNK_ID) for w in words]\n\n\ndef data_to_token_ids(data_path, target_path, vocabulary_path,\n                      tokenizer=None, normalize_digits=True):\n  """"""Tokenize data file and turn into token-ids using given vocabulary file.\n\n  This function loads data line-by-line from data_path, calls the above\n  sentence_to_token_ids, and saves the result to target_path. See comment\n  for sentence_to_token_ids on the details of token-ids format.\n\n  Args:\n    data_path: path to the data file in one-sentence-per-line format.\n    target_path: path where the file with token-ids will be created.\n    vocabulary_path: path to the vocabulary file.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(target_path):\n    print(""Tokenizing data in %s"" % data_path)\n    vocab, _ = initialize_vocabulary(vocabulary_path)\n    with gfile.GFile(data_path, mode=""rb"") as data_file:\n      with gfile.GFile(target_path, mode=""w"") as tokens_file:\n        counter = 0\n        for line in data_file:\n          counter += 1\n          if counter % 100000 == 0:\n            print(""  tokenizing line %d"" % counter)\n          token_ids = sentence_to_token_ids(tf.compat.as_bytes(line), vocab,\n                                            tokenizer, normalize_digits)\n          tokens_file.write("" "".join([str(tok) for tok in token_ids]) + ""\\n"")\n\n\ndef prepare_wmt_data(data_dir, en_vocabulary_size, fr_vocabulary_size, tokenizer=None):\n  """"""Get WMT data into data_dir, create vocabularies and tokenize data.\n\n  Args:\n    data_dir: directory in which the data sets will be stored.\n    en_vocabulary_size: size of the English vocabulary to create and use.\n    fr_vocabulary_size: size of the French vocabulary to create and use.\n    tokenizer: a function to use to tokenize each data sentence;\n      if None, basic_tokenizer will be used.\n\n  Returns:\n    A tuple of 6 elements:\n      (1) path to the token-ids for English training data-set,\n      (2) path to the token-ids for French training data-set,\n      (3) path to the token-ids for English development data-set,\n      (4) path to the token-ids for French development data-set,\n      (5) path to the English vocabulary file,\n      (6) path to the French vocabulary file.\n  """"""\n  # Get wmt data to the specified directory.\n  train_path = get_wmt_enfr_train_set(data_dir)\n  dev_path = get_wmt_enfr_dev_set(data_dir)\n\n  from_train_path = train_path + "".en""\n  to_train_path = train_path + "".fr""\n  from_dev_path = dev_path + "".en""\n  to_dev_path = dev_path + "".fr""\n  return prepare_data(data_dir, from_train_path, to_train_path, from_dev_path, to_dev_path, en_vocabulary_size,\n                      fr_vocabulary_size, tokenizer)\n\n\ndef prepare_data(data_dir, from_train_path, to_train_path, from_dev_path, to_dev_path, from_vocabulary_size,\n                 to_vocabulary_size, tokenizer=None):\n  """"""Preapre all necessary files that are required for the training.\n\n    Args:\n      data_dir: directory in which the data sets will be stored.\n      from_train_path: path to the file that includes ""from"" training samples.\n      to_train_path: path to the file that includes ""to"" training samples.\n      from_dev_path: path to the file that includes ""from"" dev samples.\n      to_dev_path: path to the file that includes ""to"" dev samples.\n      from_vocabulary_size: size of the ""from language"" vocabulary to create and use.\n      to_vocabulary_size: size of the ""to language"" vocabulary to create and use.\n      tokenizer: a function to use to tokenize each data sentence;\n        if None, basic_tokenizer will be used.\n\n    Returns:\n      A tuple of 6 elements:\n        (1) path to the token-ids for ""from language"" training data-set,\n        (2) path to the token-ids for ""to language"" training data-set,\n        (3) path to the token-ids for ""from language"" development data-set,\n        (4) path to the token-ids for ""to language"" development data-set,\n        (5) path to the ""from language"" vocabulary file,\n        (6) path to the ""to language"" vocabulary file.\n    """"""\n  # Create vocabularies of the appropriate sizes.\n  to_vocab_path = os.path.join(data_dir, ""vocab%d.to"" % to_vocabulary_size)\n  from_vocab_path = os.path.join(data_dir, ""vocab%d.from"" % from_vocabulary_size)\n  create_vocabulary(to_vocab_path, to_train_path , to_vocabulary_size, tokenizer)\n  create_vocabulary(from_vocab_path, from_train_path , from_vocabulary_size, tokenizer)\n\n  # Create token ids for the training data.\n  to_train_ids_path = to_train_path + ("".ids%d"" % to_vocabulary_size)\n  from_train_ids_path = from_train_path + ("".ids%d"" % from_vocabulary_size)\n  data_to_token_ids(to_train_path, to_train_ids_path, to_vocab_path, tokenizer)\n  data_to_token_ids(from_train_path, from_train_ids_path, from_vocab_path, tokenizer)\n\n  # Create token ids for the development data.\n  to_dev_ids_path = to_dev_path + ("".ids%d"" % to_vocabulary_size)\n  from_dev_ids_path = from_dev_path + ("".ids%d"" % from_vocabulary_size)\n  data_to_token_ids(to_dev_path, to_dev_ids_path, to_vocab_path, tokenizer)\n  data_to_token_ids(from_dev_path, from_dev_ids_path, from_vocab_path, tokenizer)\n\n  return (from_train_ids_path, to_train_ids_path,\n          from_dev_ids_path, to_dev_ids_path,\n          from_vocab_path, to_vocab_path)\n'"
docs/_themes/setup.py,0,"b'# -*- coding: utf-8 -*-\n""""""`sphinx_rtd_theme` lives on `Github`_.\n\n.. _github: https://www.github.com/snide/sphinx_rtd_theme\n\n""""""\nfrom setuptools import setup\nfrom sphinx_rtd_theme import __version__\n\n\nsetup(\n    name=\'sphinx_rtd_theme\',\n    version=__version__,\n    url=\'https://github.com/snide/sphinx_rtd_theme/\',\n    license=\'MIT\',\n    author=\'Dave Snider\',\n    author_email=\'dave.snider@gmail.com\',\n    description=\'ReadTheDocs.org theme for Sphinx, 2013 version.\',\n    long_description=open(\'README.rst\').read(),\n    zip_safe=False,\n    packages=[\'sphinx_rtd_theme\'],\n    package_data={\'sphinx_rtd_theme\': [\n        \'theme.conf\',\n        \'*.html\',\n        \'static/css/*.css\',\n        \'static/js/*.js\',\n        \'static/font/*.*\'\n    ]},\n    include_package_data=True,\n    install_requires=open(\'requirements.txt\').read().splitlines(),\n    classifiers=[\n        \'Development Status :: 3 - Alpha\',\n        \'License :: OSI Approved :: BSD License\',\n        \'Environment :: Console\',\n        \'Environment :: Web Environment\',\n        \'Intended Audience :: Developers\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Operating System :: OS Independent\',\n        \'Topic :: Documentation\',\n        \'Topic :: Software Development :: Documentation\',\n    ],\n)\n'"
docs/_themes/sphinx_rtd_theme/__init__.py,0,"b'""""""Sphinx ReadTheDocs theme.\n\nFrom https://github.com/ryan-roemer/sphinx-bootstrap-theme.\n\n""""""\nimport os\n\nVERSION = (0, 1, 8)\n\n__version__ = ""."".join(str(v) for v in VERSION)\n__version_full__ = __version__\n\n\ndef get_html_theme_path():\n    """"""Return list of HTML theme paths.""""""\n    cur_dir = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))\n    return cur_dir\n'"
