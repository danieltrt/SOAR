file_path,api_count,code
setup.py,0,"b""\nimport os\nimport subprocess\nimport multiprocessing\n\nfrom setuptools import setup\nfrom distutils.command import build as build_module\n\nbase_path = os.path.dirname(__file__)\n\nwith open(os.path.join(base_path, 'README.md')) as f:\n    long_description = '\\n' + f.read()\n\ndef build_csrc():\n    build_path = os.path.join(base_path, 'build')\n    if not os.path.exists(build_path):\n        os.makedirs(build_path)\n    subprocess.check_call(['cmake', os.path.join(base_path, 'csrc')], cwd=build_path)\n    build_cpus = min(multiprocessing.cpu_count(), 4)\n    subprocess.check_call(['make', '-j{}'.format(build_cpus)], cwd=build_path)\n\nclass CmakeAndBuild(build_module.build):\n    def run(self):\n        build_csrc()\n        build_module.build.run(self)\n\nsetup(\n    name='dirt',\n    version='0.3.0',\n    description='DIRT: Differentiable Renderer for TensorFlow',\n    long_description=long_description,\n    author='Paul Henderson',\n    author_email='paul@pmh47.net',\n    url='https://github.com/pmh47/dirt',\n    packages=['dirt'],\n    python_requires='>=2.7.0',\n    install_requires=['tensorflow-gpu>=1.6', 'numpy'],\n    package_data={'dirt': ['*.so', '*.dll']},\n    include_package_data=True,\n    cmdclass={'build': CmakeAndBuild},\n    license='MIT',\n    classifiers=[\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n    ]\n)\n"""
dirt/__init__.py,0,"b'\nfrom .rasterise_ops import rasterise, rasterise_batch, rasterise_deferred, rasterise_batch_deferred\n\n'"
dirt/lighting.py,74,"b'\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\n\n\ndef _repeat_1d(tensor, count):\n\n    assert tensor.get_shape().ndims == 1\n    return tf.reshape(tf.tile(tensor[:, tf.newaxis], tf.convert_to_tensor([1, count])), [-1])\n\n\ndef _prepare_vertices_and_faces(vertices, faces):\n\n    vertices = tf.convert_to_tensor(vertices, name=\'vertices\')\n    faces = tf.convert_to_tensor(faces, name=\'faces\')\n\n    if faces.dtype is not tf.int32:\n        assert faces.dtype is tf.int64\n        faces = tf.cast(faces, tf.int32)\n\n    return vertices, faces\n\n\ndef _get_face_normals(vertices, faces):\n\n    vertices_ndim = vertices.get_shape().ndims\n    vertices_by_index = tf.transpose(vertices, [vertices_ndim - 2] + list(range(vertices_ndim - 2)) + [vertices_ndim - 1])  # indexed by vertex-index, *, x/y/z\n    vertices_by_face = tf.gather(vertices_by_index, faces)  # indexed by face-index, vertex-in-face, *, x/y/z\n    normals_by_face = tf.cross(vertices_by_face[:, 1] - vertices_by_face[:, 0], vertices_by_face[:, 2] - vertices_by_face[:, 0])  # indexed by face-index, *, x/y/z\n    normals_by_face /= (tf.norm(normals_by_face, axis=-1, keep_dims=True) + 1.e-12)  # ditto\n    return normals_by_face, vertices_by_index\n\n\ndef vertex_normals(vertices, faces, name=None):\n    """"""Computes vertex normals for the given meshes.\n\n    This function takes a batch of meshes with common topology, and calculates vertex normals for each.\n\n    Args:\n        vertices: a `Tensor` of shape [*, vertex count, 3] or [*, vertex count, 4], where * represents arbitrarily\n            many leading (batch) dimensions.\n        faces: an int32 `Tensor` of shape [face count, 3]; each value is an index into the first dimension of `vertices`, and\n            each row defines one triangle.\n        name: an optional name for the operation\n\n    Returns:\n        a `Tensor` of shape [*, vertex count, 3], which for each vertex, gives the (normalised) average of the normals of\n        all faces that include that vertex\n    """"""\n\n    # This computes vertex normals, as the average of the normals of the faces each vertex is part of\n    # vertices is indexed by *, vertex-index, x/y/z[/w]\n    # faces is indexed by face-index, vertex-in-face\n    # result is indexed by *, vertex-index, x/y/z\n\n    with ops.name_scope(name, \'VertexNormals\', [vertices, faces]) as scope:\n\n        vertices, faces = _prepare_vertices_and_faces(vertices, faces)\n        vertices = vertices[..., :3]  # drop the w-coordinate if present\n\n        vertices_ndim = vertices.get_shape().ndims\n        normals_by_face, vertices_by_index = _get_face_normals(vertices, faces)  # normals_by_face is indexed by face-index, *, x/y/z\n\n        face_count = tf.shape(faces)[0]\n        vbi_shape = tf.shape(vertices_by_index)\n        N_extra = tf.reduce_prod(vbi_shape[1:-1])  # this is the number of \'elements\' in the * dimensions\n\n        assert vertices_ndim in {2, 3}  # ** keep it simple for now; in the general case we need a flattened outer product of ranges\n        if vertices_ndim == 2:\n            extra_indices = []\n        else:\n            extra_indices = [tf.tile(_repeat_1d(tf.range(N_extra), 3), [face_count * 3])]\n\n        normals_by_face_and_vertex = tf.SparseTensor(\n            indices=tf.cast(\n                tf.stack([  # each element of this stack is repeated a number of times matching the things after, then tiled a number of times matching the things before, so that each has the same length\n                    _repeat_1d(tf.range(face_count, dtype=tf.int32), N_extra * 9),\n                    _repeat_1d(tf.reshape(faces, [-1]), N_extra * 3)\n                ] + extra_indices + [\n                    tf.tile(tf.constant([0, 1, 2], dtype=tf.int32), tf.convert_to_tensor([face_count * N_extra * 3]))\n                ], axis=1),\n                tf.int64\n            ),\n            values=tf.reshape(tf.tile(normals_by_face[:, tf.newaxis, ...], [1, 3] + [1] * (vertices_ndim - 1)), [-1]),\n            dense_shape=tf.cast(tf.concat([[face_count], vbi_shape], axis=0), tf.int64)\n        )  # indexed by face-index, vertex-index, *, x/y/z\n\n        summed_normals_by_vertex = tf.sparse_reduce_sum(normals_by_face_and_vertex, axis=0)  # indexed by vertex-index, *, x/y/z\n        renormalised_normals_by_vertex = summed_normals_by_vertex / (tf.norm(summed_normals_by_vertex, axis=-1, keep_dims=True) + 1.e-12)  # ditto\n\n        result = tf.transpose(renormalised_normals_by_vertex, list(range(1, vertices_ndim - 1)) + [0, vertices_ndim - 1])\n        result.set_shape(vertices.get_shape())\n        return result\n\n\ndef _static_map_fn(f, elements):\n    assert elements.get_shape()[0].value is not None\n    return tf.stack([f(elements[index]) for index in range(int(elements.get_shape()[0]))])\n\n\ndef vertex_normals_pre_split(vertices, faces, name=None, static=False):\n    """"""Computes vertex normals for the given pre-split meshes.\n\n    This function is identical to `vertex_normals`, except that it assumes each vertex is used by just one face, which\n    allows a more efficient implementation.\n    """"""\n\n    # This is identical to vertex_normals, but assumes each vertex appears in exactly one face, e.g. due to having been\n    # processed by split_vertices_by_face\n    # vertices is indexed by *, vertex-index, x/y/z[/w]\n    # faces is indexed by face-index, vertex-in-face\n    # result is indexed by *, vertex-index, x/y/z\n\n    with ops.name_scope(name, \'VertexNormalsPreSplit\', [vertices, faces]) as scope:\n\n        vertices, faces = _prepare_vertices_and_faces(vertices, faces)\n        vertices = vertices[..., :3]  # drop the w-coordinate if present\n        face_count = int(faces.get_shape()[0]) if static else tf.shape(faces)[0]\n\n        normals_by_face, _ = _get_face_normals(vertices, faces)  # indexed by face-index, *, x/y/z\n        normals_by_face_flat = tf.reshape(\n            tf.transpose(normals_by_face, list(range(1, normals_by_face.get_shape().ndims - 1)) + [0, normals_by_face.get_shape().ndims - 1]),\n            [-1, face_count, 3]\n        )  # indexed by prod(*), face-index, x/y/z\n\n        normals_by_vertex_flat = (_static_map_fn if static else tf.map_fn)(lambda normals_for_iib: tf.scatter_nd(\n            indices=tf.reshape(faces, [-1, 1]),\n            updates=tf.reshape(tf.tile(normals_for_iib[:, tf.newaxis, :], [1, 3, 1]), [-1, 3]),\n            shape=tf.shape(vertices)[-2:]\n        ), normals_by_face_flat)\n        normals_by_vertex = tf.reshape(normals_by_vertex_flat, tf.shape(vertices))\n\n        return normals_by_vertex\n\n\ndef split_vertices_by_face(vertices, faces, name=None):\n    """"""Returns a new mesh where each vertex is used by exactly one face.\n\n    This function takes a batch of meshes with common topology as input, and also returns a batch of meshes\n    with common topology. The resulting meshes have the same geometry, but each vertex is used by exactly\n    one face.\n\n    Args:\n        vertices: a `Tensor` of shape [*, vertex count, 3] or [*, vertex count, 4], where * represents arbitrarily\n            many leading (batch) dimensions.\n        faces: an int32 `Tensor` of shape [face count, 3]; each value is an index into the first dimension of `vertices`, and\n            each row defines one triangle.\n\n    Returns:\n        a tuple of two tensors `new_vertices, new_faces`, where `new_vertices` has shape [*, V, 3] or [*,  V, 4], where\n        V is the new vertex count after splitting, and `new_faces` has shape [F, 3] where F is the new face count after\n        splitting.\n    """"""\n\n    # This returns an equivalent mesh, with vertices duplicated such that there is exactly one vertex per face it is used in\n    # vertices is indexed by *, vertex-index, x/y/z[/w]\n    # faces is indexed by face-index, vertex-in-face\n    # Ditto for results\n\n    with ops.name_scope(name, \'SplitVerticesByFace\', [vertices, faces]) as scope:\n\n        vertices, faces = _prepare_vertices_and_faces(vertices, faces)\n\n        vertices_shape = tf.shape(vertices)\n        face_count = tf.shape(faces)[0]\n\n        flat_vertices = tf.reshape(vertices, [-1, vertices_shape[-2], vertices_shape[-1]])\n        new_flat_vertices = tf.map_fn(lambda vertices_for_iib: tf.gather(vertices_for_iib, faces), flat_vertices)\n        new_vertices = tf.reshape(new_flat_vertices, tf.concat([vertices_shape[:-2], [face_count * 3, vertices_shape[-1]]], axis=0))\n\n        new_faces = tf.reshape(tf.range(face_count * 3), [-1, 3])\n\n        static_face_count = faces.get_shape().dims[0] if faces.get_shape().dims is not None else None\n        static_new_vertex_count = static_face_count * 3 if static_face_count is not None else None\n        if vertices.get_shape().dims is not None:\n            new_vertices.set_shape(vertices.get_shape().dims[:-2] + [static_new_vertex_count] + vertices.get_shape().dims[-1:])\n        new_faces.set_shape([static_face_count, 3])\n\n        return new_vertices, new_faces\n\n\ndef diffuse_directional(vertex_normals, vertex_colors, light_direction, light_color, double_sided=True, name=None):\n    """"""Calculate reflectance due to directional lighting on a diffuse surface.\n\n    This calculates Lambertian reflectance at points with the given normals, under a single directional\n    (parallel) light of the specified angle, mapping over leading batch/etc. dimensions. If double_sided is set,\n    then surfaces whose normal faces away from the light are still lit; otherwise, they will be black.\n\n    Note that this function may be applied to vertices of a mesh before rasterisation, or to values in a G-buffer for\n    deferred shading.\n\n    Args:\n        vertex_normals: a `Tensor` of shape [*, vertex count, 3], where * represents arbitrarily many leading (batch) dimensions..\n        vertex_colors: a `Tensor` of shape [*, vertex count, C], where * is the same as for `vertex_normals` and C is the\n            number of colour channels, defining the albedo or reflectance at each point.\n        light_direction: a `Tensor` of shape [*, 3] defining the direction of the incident light.\n        light_color: a `Tensor` of shape [*, C] defining the colour of the light.\n        double_sided: a python `bool`; if true, back faces will be shaded the same as front faces; else, they will be black\n        name: an optional name for the operation.\n\n    Returns:\n        a `Tensor` of shape [*, vertex count, C], where * is the same as for the input parameters and C is the number of channels,\n        giving the reflectance for each point.\n    """"""\n\n    # vertex_normals is indexed by *, vertex-index, x/y/z; it is assumed to be normalised\n    # vertex_colors is indexed by *, vertex-index, r/g/b\n    # light_direction is indexed by *, x/y/z; it is assumed to be normalised\n    # light_color is indexed by *, r/g/b\n    # result is indexed by *, vertex-index, r/g/b\n\n    with ops.name_scope(name, \'DiffuseDirectionalLight\', [vertex_normals, vertex_colors, light_direction, light_color]) as scope:\n\n        vertex_normals = tf.convert_to_tensor(vertex_normals, name=\'vertex_normals\')\n        vertex_colors = tf.convert_to_tensor(vertex_colors, name=\'vertex_colors\')\n        light_direction = tf.convert_to_tensor(light_direction, name=\'light_direction\')\n        light_color = tf.convert_to_tensor(light_color, name=\'light_color\')\n\n        cosines = tf.matmul(vertex_normals, -light_direction[..., tf.newaxis])  # indexed by *, vertex-index, singleton\n        if double_sided:\n            cosines = tf.abs(cosines)\n        else:\n            cosines = tf.maximum(cosines, 0.)\n\n        return light_color[..., tf.newaxis, :] * vertex_colors * cosines\n\n\ndef specular_directional(vertex_positions, vertex_normals, vertex_reflectivities, light_direction, light_color, camera_position, shininess, double_sided=True, name=None):\n    """"""Calculate reflectance due to directional lighting on a specular surface.\n\n    This calculates Phong reflectance at points with the given normals, under a single directional\n    (parallel) light of the specified angle, mapping over batch/etc. dimensions. If double_sided is set,\n    then surfaces whose normal faces away from the light are still lit; otherwise, they will be black.\n\n    Note that this function may be applied to vertices of a mesh before rasterisation, or to values in a G-buffer for\n    deferred shading.\n\n    Args:\n        vertex_positions: a `Tensor` of shape [*, vertex count, 3], where * represents arbitrarily many leading (batch) dimensions.,\n            defining the 3D location of each point.\n        vertex_normals: a `Tensor` of shape [*, vertex count, 3], where * is the same as for `vertex_positions`, defining the surface\n            normal at each point.\n        vertex_colors: a `Tensor` of shape [*, vertex count, C], where C is the number of colour channels, defining the albedo or\n            reflectance at each point.\n        light_direction: a `Tensor` of shape [*, 3] defining the direction of the incident light.\n        light_color: a `Tensor` of shape [*, C] defining the colour of the light.\n        camera_position: a `Tensor` of shape [*, 3] defining the position of the camera in 3D space.\n        shininess: a `Tensor` of shape [*] defining the specular reflectance index.\n        double_sided: a python `bool`; if true, back faces will be shaded the same as front faces; else, they will be black.\n        name: an optional name for the operation.\n\n    Returns:\n        a `Tensor` of shape [*, vertex count, C], where * is the same as for the input parameters and C is the number of channels,\n        giving the reflectance for each point.\n    """"""\n\n    # vertex_positions is indexed by *, vertex-index, x/y/z\n    # vertex_normals is indexed by *, vertex-index, x/y/z; it is assumed to be normalised\n    # vertex_reflectivities is indexed by *, vertex-index, r/g/b\n    # light_direction is indexed by *, x/y/z; it is assumed to be normalised\n    # light_color is indexed by *, r/g/b\n    # camera_position is indexed by *, x/y/z\n    # shininess is indexed by *\n    # result is indexed by *, vertex-index, r/g/b\n\n    with ops.name_scope(name, \'SpecularDirectionalLight\', [vertex_positions, vertex_normals, vertex_reflectivities, light_direction, light_color, camera_position, shininess]) as scope:\n\n        vertex_positions = tf.convert_to_tensor(vertex_positions, name=\'vertex_positions\')\n        vertex_normals = tf.convert_to_tensor(vertex_normals, name=\'vertex_normals\')\n        vertex_reflectivities = tf.convert_to_tensor(vertex_reflectivities, name=\'vertex_reflectivities\')\n        light_direction = tf.convert_to_tensor(light_direction, name=\'light_direction\')\n        light_color = tf.convert_to_tensor(light_color, name=\'light_color\')\n        camera_position = tf.convert_to_tensor(camera_position, name=\'camera_position\')\n        shininess = tf.convert_to_tensor(shininess, name=\'shininess\')\n\n        vertices_to_light_direction = -light_direction\n        reflected_directions = -vertices_to_light_direction + 2. * tf.matmul(vertex_normals, vertices_to_light_direction[..., tf.newaxis]) * vertex_normals  # indexed by *, vertex-index, x/y/z\n        vertex_to_camera_displacements = camera_position[..., tf.newaxis, :] - vertex_positions  # indexed by *, vertex-index, x/y/z\n        cosines = tf.reduce_sum(\n            (vertex_to_camera_displacements / tf.norm(vertex_to_camera_displacements, axis=-1, keep_dims=True) + 1.e-12) * reflected_directions,\n            axis=-1, keep_dims=True\n        )  # indexed by *, vertex-index, singleton\n        if double_sided:\n            cosines = tf.abs(cosines)\n        else:\n            cosines = tf.maximum(cosines, 0.)\n\n        return light_color[..., tf.newaxis, :] * vertex_reflectivities * tf.pow(cosines, shininess[..., tf.newaxis, tf.newaxis])\n\n\ndef diffuse_point(vertex_positions, vertex_normals, vertex_colors, light_position, light_color, double_sided=True, name=None):\n    """"""Calculate reflectance due to directional lighting on a diffuse surface.\n\n    This calculates Lambertian reflectance at points with the given normals, under a single point light at the\n    specified location, mapping over leading batch/etc. dimensions. If double_sided is set, then surfaces whose\n    normal faces away from the light are still lit; otherwise, they will be black.\n\n    A point light radiates uniformly in all directions from some physical location.\n\n    Note that this function may be applied to vertices of a mesh before rasterisation, or to values in a G-buffer for\n    deferred shading.\n\n    Args:\n        vertex_positions: a `Tensor` of shape [*, vertex count, 3], where * represents arbitrarily many leading (batch) dimensions.,\n            defining the 3D location of each point.\n        vertex_normals: a `Tensor` of shape [*, vertex count, 3], where * is the same as for `vertex_positions`, defining the surface\n            normal at each point.\n        vertex_colors: a `Tensor` of shape [*, vertex count, C], where C is the number of colour channels, defining the albedo or\n            reflectance at each point.\n        light_position: a `Tensor` of shape [*, 3] defining the location of the point light source.\n        light_color: a `Tensor` of shape [*, C] defining the colour of the light.\n        double_sided: a python `bool`; if true, back faces will be shaded the same as front faces; else, they will be black\n        name: an optional name for the operation.\n\n    Returns:\n        a `Tensor` of shape [*, vertex count, C], where * is the same as for the input parameters and C is the number of channels,\n        giving the reflectance for each point.\n    """"""\n\n    # vertex_positions is indexed by *, vertex-index, x/y/z\n    # vertex_normals is indexed by *, vertex-index, x/y/z; it is assumed to be normalised\n    # vertex_colors is indexed by *, vertex-index, r/g/b\n    # light_position is indexed by *, x/y/z\n    # light_color is indexed by *, r/g/b\n    # result is indexed by *, vertex-index, r/g/b\n\n    with ops.name_scope(name, \'DiffusePointLight\', [vertex_positions, vertex_normals, vertex_colors, light_position, light_color]) as scope:\n\n        vertex_positions = tf.convert_to_tensor(vertex_positions, name=\'vertex_positions\')\n        vertex_normals = tf.convert_to_tensor(vertex_normals, name=\'vertex_normals\')\n        vertex_colors = tf.convert_to_tensor(vertex_colors, name=\'vertex_colors\')\n        light_position = tf.convert_to_tensor(light_position, name=\'light_position\')\n        light_color = tf.convert_to_tensor(light_color, name=\'light_color\')\n\n        relative_positions = vertex_positions - light_position[..., tf.newaxis, :]  # indexed by *, vertex-index, x/y/z\n        incident_directions = relative_positions / (tf.norm(relative_positions, axis=-1, keep_dims=True) + 1.e-12)  # ditto\n        cosines = tf.reduce_sum(vertex_normals * incident_directions, axis=-1)  # indexed by *, vertex-index\n        if double_sided:\n            cosines = tf.abs(cosines)\n        else:\n            cosines = tf.maximum(cosines, 0.)\n\n        return light_color[..., tf.newaxis, :] * vertex_colors * cosines[..., tf.newaxis]\n\n'"
dirt/matrices.py,30,"b'\n""""""Helper functions for homogeneous transform matrices.\n\nThis module defines helper functions used to construct transform matrices.\nThese functions assume the matrices will *right*-multiply the vectors to be transformed, i.e.\nthat the inputs are row vectors -- as is the case for a matrix of vertices indexed naturally\nEquivalently, matrices are indexed by *, x/y/z[/w] (in), x/y/z[/w] (out) -- where * represents any\nsequence of indices, over all of which the operation is mapped\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\n\n\ndef rodrigues(vectors, name=None, three_by_three=False):\n    """"""Constructs a batch of angle-axis rotation matrices.\n\n    Angle-axis rotations are defined by a single 3D vector, whose direction corresponds to the axis of\n    rotation, and whose length corresponds to the rotation angle in radians.\n    This function returns  a batch of angle-axis rotation matrices, computed according to Rodrigues\'\n    formula, from a corresponding batch of 3D rotation vectors.\n\n    Args:\n        vectors: a `Tensor` of shape [*, 3], where * represents arbitrarily many leading (batch) dimensions\n        name: an optional name for the operation\n        three_by_three: return 3x3 matrices without w coordinates\n\n    Returns:\n        a `Tensor` containing rotation matrices, of shape [*, D, D], where * represents the same leading\n        dimensions as present on `vectors`, and D = 3 if three_by_three else 4\n    """"""\n\n    # vectors is indexed by *, x/y/z, so the result is indexed by *, x/y/z (in), x/y/z (out)\n    # This follows the OpenCV docs\' definition; wikipedia says slightly different...\n\n    with ops.name_scope(name, \'Rodrigues\', [vectors]) as scope:\n\n        vectors = tf.convert_to_tensor(vectors, name=\'vectors\')\n\n        vectors += 1.e-12  # for numerical stability of the derivative, which is otherwise NaN at exactly zero; also ensures norms are never zero\n        norms = tf.norm(vectors, axis=-1, keep_dims=True)  # indexed by *, singleton\n        vectors /= norms\n        norms = norms[..., 0]  # indexed by *\n\n        z = tf.zeros_like(vectors[..., 0])  # ditto\n        K = tf.convert_to_tensor([\n            [z, -vectors[..., 2], vectors[..., 1]],\n            [vectors[..., 2], z, -vectors[..., 0]],\n            [-vectors[..., 1], vectors[..., 0], z],\n        ])  # indexed by x/y/z (in), x/y/z (out), *\n        K = tf.transpose(K, list(range(2, K.get_shape().ndims)) + [0, 1])  # indexed by *, x/y/z (in), x/y/z (out)\n\n        c = tf.cos(norms)[..., tf.newaxis, tf.newaxis]\n        s = tf.sin(norms)[..., tf.newaxis, tf.newaxis]\n\n        result_3x3 = c * tf.eye(3, 3) + (1 - c) * vectors[..., :, tf.newaxis] * vectors[..., tf.newaxis, :] + s * K\n\n        if three_by_three:\n            return result_3x3\n        else:\n            return pad_3x3_to_4x4(result_3x3)\n\n\ndef translation(x, name=None):\n    """"""Constructs a batch of translation matrices.\n\n    This function returns a batch of translation matrices, from a corresponding batch of 3D displacement vectors.\n\n    Args:\n        x: a `Tensor` of shape [*, 3], where * represents arbitrarily many leading (batch) dimensions\n        name: an optional name for the operation\n\n    Returns:\n        a `Tensor` containing translation matrices, of shape [*, 4, 4], where * represents the same leading\n        dimensions as present on `x`\n    """"""\n\n    # x is indexed by *, x/y/z\n    with ops.name_scope(name, \'Translation\', []) as scope:\n        x = tf.convert_to_tensor(x, name=\'x\')\n        zeros = tf.zeros_like(x[..., 0])  # indexed by *\n        ones = tf.ones_like(zeros)\n        return tf.stack([\n            tf.stack([ones, zeros, zeros, zeros], axis=-1),  # indexed by *, x/y/z (out)\n            tf.stack([zeros, ones, zeros, zeros], axis=-1),\n            tf.stack([zeros, zeros, ones, zeros], axis=-1),\n            tf.stack([x[..., 0], x[..., 1], x[..., 2], ones], axis=-1)\n        ], axis=-2)  # indexed by *, x/y/z/w (in), x/y/z/w (out)\n\n\ndef scale(x, name=None):\n    """"""Constructs a batch of scaling matrices.\n\n    This function returns a batch of scaling matrices, from a corresponding batch of 3D scale factors.\n\n    Args:\n        x: a `Tensor` of shape [*, 3], where * represents arbitrarily many leading (batch) dimensions\n        name: an optional name for the operation\n\n    Returns:\n        a `Tensor` containing scaling matrices, of shape [*, 4, 4], where * represents the same leading\n        dimensions as present on `x`\n    """"""\n\n    with ops.name_scope(name, \'Scale\', []) as scope:\n        x = tf.convert_to_tensor(x, name=\'x\')\n        return tf.linalg.diag(tf.concat([x, tf.ones_like(x[..., :1])], axis=-1))  # indexed by *, x/y/z/w (in), x/y/z/w (out)\n\n\ndef perspective_projection(near, far, right, aspect, name=None):\n    """"""Constructs a perspective projection matrix.\n\n    This function returns a perspective projection matrix, using the OpenGL convention that the camera\n    looks along the negative-z axis in view/camera space, and the positive-z axis in clip space.\n    Multiplying view-space homogeneous coordinates by this matrix maps them into clip space.\n\n    Args:\n        near: distance to the near clipping plane; geometry nearer to the camera than this will not be rendered\n        far: distance to the far clipping plane; geometry further from the camera than this will not be rendered\n        right: distance of the right-hand edge of the view frustum from its centre at the near clipping plane\n        aspect: aspect ratio (height / width) of the viewport\n        name: an optional name for the operation\n\n    Returns:\n        a 4x4 `Tensor` containing the projection matrix\n    """"""\n\n    with ops.name_scope(name, \'PerspectiveProjection\', [near, far, right, aspect]) as scope:\n        near = tf.convert_to_tensor(near, name=\'near\')\n        far = tf.convert_to_tensor(far, name=\'far\')\n        right = tf.convert_to_tensor(right, name=\'right\')\n        aspect = tf.convert_to_tensor(aspect, name=\'aspect\')\n        top = right * aspect\n        elements = [\n            [near / right, 0., 0., 0, ],\n            [0., near / top, 0., 0.],\n            [0., 0., -(far + near) / (far - near), -2. * far * near / (far - near)],\n            [0., 0., -1., 0.]\n        ]  # indexed by x/y/z/w (out), x/y/z/w (in)\n        return tf.transpose(tf.convert_to_tensor(elements, dtype=tf.float32))\n\n\ndef pad_3x3_to_4x4(matrix, name=None):\n    """"""Pads a 3D transform matrix to a 4D homogeneous transform matrix.\n\n    This function converts a batch of 3x3 transform matrices into 4x4 equivalents that operate on\n    homogeneous coordinates.\n    To do so, for each matrix in the batch, it appends a column of zeros, a row of zeros, and a single\n    one at the bottom-right corner.\n\n    Args:\n        matrix: a `Tensor` of shape [*, 3, 3], where * represents arbitrarily many leading (batch) dimensions\n        name: an optional name for the operation\n\n    Returns:\n        a `Tensor` of shape [*, 4, 4] containing the padded matrices, where * represents the same leading\n        dimensions as present on `matrix`\n    """"""\n\n    # matrix is indexed by *, x/y/z (in), x/y/z (out)\n    # result is indexed by *, x/y/z/w (in), x/y/z/w (out)\n    with ops.name_scope(name, \'Pad3x3To4x4\', [matrix]) as scope:\n        matrix = tf.convert_to_tensor(matrix, name=\'matrix\')\n        return tf.concat([\n            tf.concat([matrix, tf.zeros_like(matrix[..., :, :1])], axis=-1),\n            tf.concat([tf.zeros_like(matrix[..., :1, :]), tf.ones_like(matrix[..., :1, :1])], axis=-1)\n        ], axis=-2)\n\n\ndef compose(*matrices):\n    """"""Composes together a sequence of matrix transformations.\n\n    This is a convenience function to multiply together a sequence of transform matrices; if `matrices`\n    is empty , it will return a single 4x4 identity matrix.\n    The evaluation order is such that the first matrix in the list is the first to be applied.\n\n    Args:\n        matrices: a list with elements of type `Tensor` and shape [*, 4, 4], where * represents arbitrarily many\n        leading (batch) dimensions\n\n    Returns:\n        a `Tensor` of shape [*, 4, 4] containing the product of the given matrices, evaluated in the same order as the\n        matrices appear in the list\n    """"""\n\n    # This applies the first matrix in the list first, i.e. compose(A, B) is \'A then B\'; as our convention is that the matrix\n    # always right-multiplies vectors, this just expands to a sequence of tf.matmul\'s in the same order as the input\n\n    if len(matrices) == 0:\n        return tf.eye(4)\n    elif len(matrices) == 1:  # special-cased to avoid an identity-multiply\n        return matrices[0]\n    else:\n        return tf.matmul(matrices[0], compose(*matrices[1:]))\n\n'"
dirt/projection.py,16,"b'\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\n\n\ndef _pixel_to_ndc(pixel_locations, image_size):\n    return (-1. + 2. * pixel_locations / image_size) * [1., -1.]\n\n\ndef _unproject_ndc_to_world(x_ndc, clip_to_world_matrix):\n    # x_ndc and result are indexed by *, x/y/z (i.e. not homogeneous)\n    # The z-coordinate of the result does not have an intuitive meaning, but is affinely related to the world-space z\n    x_world_scaled = tf.squeeze(tf.matmul(\n        tf.expand_dims(tf.concat([x_ndc, tf.ones_like(x_ndc[..., :1])], axis=-1), axis=-2),\n        clip_to_world_matrix\n    ), axis=-2)\n    x_world_scaled.set_shape(x_ndc.get_shape()[:-1].as_list() + [4])\n    return x_world_scaled[..., :3] / x_world_scaled[..., 3:]\n\n\ndef unproject_pixels_to_rays(pixel_locations, clip_to_world_matrix, image_size, name=None):\n    """"""Computes world-space start-points and directions for rays cast into the scene from the given pixel locations\n\n    Args:\n        pixel_locations: a `Tensor` of shape [A1, ..., An, B1, ..., Bm, 2] of (x, y) coordinates in pixel space, where the Ai represent\n            arbitrarily many leading (batch) dimensions., and the Bi represent arbitrarily many dimensions for which the\n            projection parameters clip_to_world_matrix and image_size are shared\n        clip_to_world_matrix: a `Tensor` of shape [A1, ..., An, 4, 4] specifying the combined transform matrix mapping from clip-space\n            to world-space. Typically this is given by inv(world-to-view-matrix * projection-matrix).\n        image_size: an int32 `Tensor` of shape [A1, ..., An, 2] specifying the width then height in pixels of the image wrt which\n            pixel_locations is given.\n        name: an optional name for the operation\n\n    Returns:\n        pixel_ray_starts_world: a `Tensor` of shape [A1, ..., An, B1, ..., Bm, 3], which for each pixel-location, gives the world-space\n            location of the intersection of the corresponding ray with the camera near-plane\n        pixel_ray_deltas_world: similar to pixel_ray_starts_world, but giving an unnormalised world-space direction vector for each\n            ray, pointing away from the camera\n    """"""\n\n    with ops.name_scope(name, \'UnprojectPixelsToRays\', [pixel_locations, clip_to_world_matrix, image_size]) as scope:\n\n        pixel_locations = tf.convert_to_tensor(pixel_locations, name=\'pixel_locations\', dtype=tf.float32)\n        clip_to_world_matrix = tf.convert_to_tensor(clip_to_world_matrix, name=\'clip_to_world_matrix\', dtype=tf.float32)\n        image_size = tf.convert_to_tensor(image_size, name=\'image_size\', dtype=tf.int32)\n\n        per_iib_dims = pixel_locations.get_shape().ndims - image_size.get_shape().ndims  # corresponds to m in the docstring\n        image_size = tf.reshape(image_size, image_size.get_shape()[:-1].as_list() + [1] * per_iib_dims + [2])\n        clip_to_world_matrix = tf.reshape(\n            clip_to_world_matrix,\n            tf.concat([\n                tf.shape(clip_to_world_matrix)[:-2],\n                [1] * per_iib_dims + [4, 4]\n            ], axis=0)\n        )\n\n        # This is needed for old versions of tensorflow as tf.matmul did not previously support broadcasting\n        version_bits = tf.version.VERSION.split(\'.\')\n        if int(version_bits[0]) <= 1 and int(version_bits[1]) < 14:\n            clip_to_world_matrix = tf.broadcast_to(\n                clip_to_world_matrix,\n                tf.concat([tf.shape(pixel_locations)[:-1], [4, 4]], axis=0)\n            )\n\n        pixel_locations_ndc = _pixel_to_ndc(pixel_locations, tf.cast(image_size, tf.float32))\n        pixel_ray_starts_world = _unproject_ndc_to_world(tf.concat([pixel_locations_ndc, -1. * tf.ones_like(pixel_locations_ndc[..., :1])], axis=-1), clip_to_world_matrix)  # indexed by A*, B*, x/y/z\n        pixel_ray_deltas_world = _unproject_ndc_to_world(tf.concat([pixel_locations_ndc, tf.zeros_like(pixel_locations_ndc[..., :1])], axis=-1), clip_to_world_matrix) - pixel_ray_starts_world\n\n    return pixel_ray_starts_world, pixel_ray_deltas_world\n\n'"
dirt/rasterise_ops.py,26,"b'import os\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\n\n_lib_path = os.path.dirname(__file__)\n_rasterise_module = tf.load_op_library(_lib_path + \'/librasterise.so\')\n\n\ndef rasterise(background, vertices, vertex_colors, faces, height=None, width=None, channels=None, name=None):\n    """"""Rasterises the given `vertices` and `faces` over `background`.\n\n    This function takes a set of vertices, vertex colors, faces (vertex indices), and a background.\n    It returns a single image, containing the faces these arrays, over the given background.\n\n    It supports single-channel (grayscale) or three-channel (RGB) image rendering, or arbitrary\n    numbers of channels for g-buffer rendering in a deferred shading pipeline (see `rasterise_deferred`).\n\n    The vertices are specified in OpenGL\'s clip space, and as such are 4D homogeneous coordinates.\n    This allows both 3D and 2D shapes to be rendered, by applying suitable projection matrices to the\n    vertices before passing them to this function.\n\n    Args:\n        background: a float32 `Tensor` of shape [height, width, channels], defining the background image to render over\n        vertices: a float32 `Tensor` of shape [vertex count, 4] defining a set of vertex locations, given in clip space\n        vertex_colors: a float32 `Tensor` of shape [vertex count, channels] defining the color of each vertex; these are\n            linearly interpolated in 3D space to calculate the color at each pixel\n        faces: an int32 `Tensor` of shape [face count, 3]; each value is an index into the first dimension of `vertices`, and\n            each row defines one triangle to rasterise. Note that each vertex may be used by several faces\n        height: a python `int` specifying the frame height; may be `None` if `background` has static shape\n        width: a python `int` specifying the frame width; may be `None` if `background` has static shape\n        channels: a python `int` specifying the number of color channels; may only be `1`or `3`. Again this may be `None`\n            if `background` has static shape\n        name: an optional name for the operation\n\n    Returns:\n        The rendered pixels, as a float32 `Tensor` of shape [height, width, channels]\n    """"""\n\n    with ops.name_scope(name, \'Rasterise\', [background, vertices, vertex_colors, faces]) as scope:\n        background = tf.convert_to_tensor(background, name=\'background\', dtype=tf.float32)\n        vertices = tf.convert_to_tensor(vertices, name=\'vertices\', dtype=tf.float32)\n        vertex_colors = tf.convert_to_tensor(vertex_colors, name=\'vertex_colors\', dtype=tf.float32)\n        faces = tf.convert_to_tensor(faces, name=\'faces\', dtype=tf.int32)\n        return rasterise_batch(background[None], vertices[None], vertex_colors[None], faces[None], height, width, channels, name)[0]\n\n\ndef rasterise_batch(background, vertices, vertex_colors, faces, height=None, width=None, channels=None, name=None):\n    """"""Rasterises a batch of meshes with the same numbers of vertices and faces.\n\n    This function takes batch-indexed `vertices`, `vertex_colors`, `faces`, and `background`.\n\n    It is conceptually equivalent to:\n    ```python\n    tf.stack([\n        rasterise(background_i, vertices_i, vertex_colors_i, faces_i)\n        for (background_i, vertices_i, vertex_colors_i, faces_i)\n        in zip(background, vertices, vertex_colors, faces)\n    ])\n    ```\n    See `rasterise` for definitions of the parameters, noting that for `rasterise_batch`, a leading dimension should be included.\n    """"""\n\n    with ops.name_scope(name, \'RasteriseBatch\', [background, vertices, vertex_colors, faces]) as scope:\n        background = tf.convert_to_tensor(background, name=\'background\', dtype=tf.float32)\n        vertices = tf.convert_to_tensor(vertices, name=\'vertices\', dtype=tf.float32)\n        vertex_colors = tf.convert_to_tensor(vertex_colors, name=\'vertex_colors\', dtype=tf.float32)\n        faces = tf.convert_to_tensor(faces, name=\'faces\', dtype=tf.int32)\n\n        if height is None:\n            height = int(background.get_shape()[1])\n        if width is None:\n            width = int(background.get_shape()[2])\n        if channels is None:\n            channels = int(background.get_shape()[3])\n\n        if channels == 1 or channels == 3:\n            return _rasterise_module.rasterise(\n                background, vertices, vertex_colors, faces,  # inputs\n                height, width, channels,  # attributes\n                name=scope\n            )\n        else:\n            assert channels > 0\n            pixels = []\n            begin_channel = 0\n            while begin_channel < channels:\n                if begin_channel + 3 <= channels:\n                    end_channel = begin_channel + 3\n                else:\n                    # in the mod-2 case, could just do a 3-channel pass (instead of two 1-channel passes), concating zeros to the background and attributes, then indexing them off the pixels\n                    end_channel = begin_channel + 1\n                with ops.name_scope(\'channels_{}_to_{}\'.format(begin_channel, end_channel)) as channel_group_scope:\n                    pixels.append(\n                        _rasterise_module.rasterise(\n                            background[..., begin_channel : end_channel],\n                            vertices,\n                            vertex_colors[..., begin_channel : end_channel],\n                            faces,\n                            height, width, end_channel - begin_channel,\n                            name=channel_group_scope\n                        )\n                    )\n                begin_channel = end_channel\n            return tf.concat(pixels, axis=-1)\n\n\n@ops.RegisterGradient(\'Rasterise\')\ndef _rasterise_grad(op, grad_pixels, name=None):\n    grad_op_result = _rasterise_module.rasterise_grad(\n        op.inputs[1], op.inputs[3],\n        op.outputs[0], grad_pixels,\n        op.get_attr(\'height\'), op.get_attr(\'width\'), op.get_attr(\'channels\'),\n        name=name\n    )\n    # tf.summary.image(\n    #     \'debug_thingy\',\n    #     (grad_op_result.debug_thingy - tf.reduce_min(grad_op_result.debug_thingy, axis=[1, 2], keep_dims=True)) /\n    #         (tf.reduce_max(grad_op_result.debug_thingy, axis=[1, 2], keep_dims=True) - tf.reduce_min(grad_op_result.debug_thingy, axis=[1, 2], keep_dims=True))\n    # )\n    return [\n        grad_op_result.grad_background,\n        grad_op_result.grad_vertices,\n        grad_op_result.grad_vertex_colors,\n        None  # wrt faces\n    ]\n\n\ndef _rasterise_grad_multichannel(vertices, faces, pixels, d_loss_by_pixels, single_or_batch):\n\n    assert single_or_batch in [\'single\', \'batch\']\n\n    if single_or_batch == \'single\':\n        vertices = vertices[None]\n        faces = faces[None]\n        pixels = pixels[None]\n        d_loss_by_pixels = d_loss_by_pixels[None]\n\n    assert len(pixels.get_shape()) == 4\n    height, width, channels = pixels.get_shape()[1], pixels.get_shape()[2], pixels.get_shape()[3]\n\n    results = []\n    begin_channel = 0\n    while begin_channel < channels:\n        if begin_channel + 3 <= channels:\n            end_channel = begin_channel + 3\n        else:\n            # in the mod-2 case, could just do a 3-channel pass (instead of two 1-channel passes), concating zeros to the background and attributes, then indexing them off the pixels\n            end_channel = begin_channel + 1\n        with ops.name_scope(\'grad_channels_{}_to_{}\'.format(begin_channel, end_channel)) as channel_group_scope:\n            results.append(_rasterise_module.rasterise_grad(\n                vertices, faces,\n                pixels[..., begin_channel : end_channel],\n                d_loss_by_pixels[..., begin_channel : end_channel],\n                height, width, end_channel - begin_channel,\n                name=channel_group_scope\n            ))\n        begin_channel = end_channel\n    # ** is the sum in the following correct?\n    grad_vertices = sum([result.grad_vertices for result in results])\n    grad_vertex_colors = tf.concat([result.grad_vertex_colors for result in results], axis=-1)\n    grad_background = tf.concat([result.grad_background for result in results], axis=-1)\n    if single_or_batch == \'single\':\n        return {\n            \'grad_vertices\': grad_vertices[0],\n            \'grad_vertex_colors\': grad_vertex_colors[0],\n            \'grad_background\': grad_background[0]\n        }\n    else:\n        return {\n            \'grad_vertices\': grad_vertices,\n            \'grad_vertex_colors\': grad_vertex_colors,\n            \'grad_background\': grad_background\n        }\n\n\ndef _rasterise_deferred_internal(background, vertices, attributes, faces, shader_fn, shader_additional_inputs, single_or_batch, name):\n\n    # ** it would be more efficient to compute both attribute and vertex gradients in one pass, modifying\n    # ** the grad op to take as inputs the loss gradients wrt both the gbuffer and the shaded pixels\n\n    # ** it would be nice to allow pixels to be a nested structure of pixel-like things\n\n    assert single_or_batch in [\'single\', \'batch\']\n\n    @tf.custom_gradient\n    def _impl(vertices, faces, attributes, background, *shader_additional_inputs):\n\n        gbuffer = (rasterise if single_or_batch == \'single\' else rasterise_batch)(background, vertices, attributes, faces, name=scope)\n\n        if tf.executing_eagerly():\n            # ** is it safe for this to be persistent, i.e. is it guaranteed that all resources will be deleted?\n            with tf.GradientTape(persistent=True) as shader_tape:\n                shader_tape.watch([gbuffer, shader_additional_inputs])\n                pixels = shader_fn(gbuffer, *shader_additional_inputs)\n        else:\n            pixels = shader_fn(gbuffer, *shader_additional_inputs)\n\n        def grad(d_loss_by_pixels):\n\n            # Calculate the derivative wrt vertices, but filtering the shaded image instead of the gbuffer -- these are the \'final\', correct\n            # gradient wrt the vertices, but for the attributes and background, we need to account for shader_fn\n            d_loss_by_vertices = _rasterise_grad_multichannel(\n                vertices, faces,\n                pixels, d_loss_by_pixels,\n                single_or_batch\n            )[\'grad_vertices\']\n\n            # For colours, need to backprop through shader_fn first; this yields the derivative of the final pixels wrt the gbuffer-pixels\n            # Then, pass these derivatives back into the rasterise-grad op to propagate to vertex/background attributes\n\n            if tf.executing_eagerly():\n                d_loss_by_gbuffer, d_loss_by_shader_additional_inputs = shader_tape.gradient(\n                    pixels,\n                    [gbuffer, shader_additional_inputs],\n                    d_loss_by_pixels\n                )\n            else:\n                d_loss_by_gbuffer_and_shader_additional_inputs = tf.gradients(\n                    pixels,\n                    [gbuffer] + list(shader_additional_inputs),\n                    d_loss_by_pixels\n                )\n                d_loss_by_gbuffer = d_loss_by_gbuffer_and_shader_additional_inputs[0]\n                d_loss_by_shader_additional_inputs = d_loss_by_gbuffer_and_shader_additional_inputs[1:]\n\n            # The attribute and background gradients computed by the following are correct; the vertex gradients are not, as they\n            # are based on filtering the gbuffer instead of the shaded output\n            d_loss_by_attributes = _rasterise_grad_multichannel(\n                vertices, faces,\n                gbuffer, d_loss_by_gbuffer,\n                single_or_batch\n            )\n\n            return [\n                d_loss_by_vertices, None, d_loss_by_attributes[\'grad_vertex_colors\'], d_loss_by_attributes[\'grad_background\']\n            ] + list(d_loss_by_shader_additional_inputs)\n\n        return pixels, grad\n\n    with ops.name_scope(name, \'RasteriseDeferred\', [background, vertices, attributes, faces] + list(shader_additional_inputs)) as scope:\n        background = tf.convert_to_tensor(background, name=\'background\', dtype=tf.float32)\n        vertices = tf.convert_to_tensor(vertices, name=\'vertices\', dtype=tf.float32)\n        attributes = tf.convert_to_tensor(attributes, name=\'vertex_attributes\', dtype=tf.float32)\n        faces = tf.convert_to_tensor(faces, name=\'faces\', dtype=tf.int32)\n\n        return _impl(vertices, faces, attributes, background, *shader_additional_inputs)\n\n\ndef rasterise_deferred(background_attributes, vertices, vertex_attributes, faces, shader_fn, shader_additional_inputs=[], name=None):\n    """"""Rasterises and shades the given `vertices` and `faces`, using the specified deferred shader function and\n    vertex/background attributes.\n\n    Deferred shading is an efficient approach to rendering images with per-pixel lighting and texturing.\n    It splits rendering into two passes. In the first pass, vertex attributes (such as colors and normals) are\n    rasterised into a pseudo-image called a G-buffer. In the second pass, shading calculations are performed\n    directly on this buffer (e.g. calculating the reflected lighting given the surface colour and normal at a pixel).\n\n    This function takes a set of vertices, vertex attributes, faces (vertex indices), background attributes, and a\n    deferred shader function. It first rasterises a G-buffer containing the attributes, then calls the given shader\n    function passing the G-buffer as input; the shader function is assumed to produce the final pixels. It is equivalent\n    to `shader_fn(rasterise(background_attributes, vertices, vertex_attributes, faces), *shader_additional_inputs)`,\n    but its gradient correctly accounts for how the approximate gradients of `rasterise` interact with `shader_fn`.\n\n    Any computation that is conceptually performed \'on the surface\' of the 3D geometry should be included\n    in `shader_fn` (typically texture-sampling and lighting); downstream 2D post-processing (e.g. blurring the\n    rendered image) should not be included.\n\n    Note that for correct gradients, `shader_fn` should not reference any tensors in enclosing scopes directly.\n    Instead, any such tensors must be passed through the list `shader_additional_inputs`, whose values are\n    forwarded as additional parameters to `shader_fn`. For example, if the pixel values depend on a non-constant\n    lighting angle, the tensor representing that lighting angle should be passed through `shader_additional_inputs`.\n\n    For usage examples, see `samples/deferred.py` and `samples/textured.py`.\n\n    Args:\n        background_attributes: a float32 `Tensor` of shape [height, width, attributes], defining the values of the\n            attributes to use for G-buffer pixels that do not intersect any triangle\n        vertices: a float32 `Tensor` of shape [vertex count, 4] defining a set of vertex locations, given as homogeneous\n            coordinates in OpenGL clip space\n        vertex_attributes: a float32 `Tensor` of shape [vertex count, attributes] defining the values of the attributes at\n            each vertex; these are linearly interpolated in 3D space to calculate the attribute value at each pixel of the\n            G-buffer\n        faces: an int32 `Tensor` of shape [face count, 3]; each value is an index into the first dimension of `vertices`, and\n            each row defines one triangle to rasterise. Note that each vertex may be used by several faces\n        shader_fn: a function that takes the G-buffer as input, and returns shaded pixels as output. Specifically, it takes\n            one or more parameters, of which the first is always the G-buffer (a float32 `Tensor` of shape\n            [height, width, attributes]), and any others are the tensors in `shader_additional_inputs`. It returns a float32\n            `Tensor` of shape [height, width, channels] containing the final image; channels is typically three (for an RGB\n            image) but this is not required\n        shader_additional_inputs: an optional list of tensors that are passed to `shader_fn` in addition to the G-buffer. Any\n            tensors required to compute the shading aside from the vertex attributes should be explicitly passed through\n            this parameter, not directly referenced in an outer scope from shader_fn, else their gradients will be incorrect.\n        name: an optional name for the operation\n\n    Returns:\n        The rendered pixels, as a float32 `Tensor` of shape [height, width, channels]\n    """"""\n\n    return _rasterise_deferred_internal(background_attributes, vertices, vertex_attributes, faces, shader_fn, shader_additional_inputs, \'single\', name)\n\n\ndef rasterise_batch_deferred(background_attributes, vertices, vertex_attributes, faces, shader_fn, shader_additional_inputs=[], name=None):\n    """"""Rasterises and shades a batch of meshes with the same numbers of vertices and faces, using the specified\n    deferred shader function.\n\n    This function takes batch-indexed `vertices`, `faces`, `vertex_attributes`, and `background_attributes`. Any\n    values in `shader_additional_inputs` may be batch-indexed or not, depending how shader_fn interprets them.\n\n    It is conceptually equivalent to:\n    ```python\n    tf.stack([\n        rasterise_deferred(background_attributes_i, vertices_i, vertex_attributes_i, faces_i, shader_fn, shader_addtional_inputs)\n        for (background_attributes_i, vertices_i, vertex_attributes_i, faces_i)\n        in zip(background_attributes, vertices, vertex_attributes, faces)\n    ])\n    ```\n    See `rasterise_deferred` for definitions of the parameters, noting that for `rasterise_batch_deferred`, a\n    leading dimension should be included.\n    """"""\n\n    return _rasterise_deferred_internal(background_attributes, vertices, vertex_attributes, faces, shader_fn, shader_additional_inputs, \'batch\', name)\n'"
samples/deferred.py,23,"b""\n# This demonstrates using Dirt for deferred shading, which allows per-pixel lighting\n\nimport tensorflow as tf\n\nimport dirt\nimport dirt.matrices as matrices\nimport dirt.lighting as lighting\n\n\nframe_width, frame_height = 640, 480\n\n\ndef build_cube():\n    vertices = [[x, y, z] for z in [-1, 1] for y in [-1, 1] for x in [-1, 1]]\n    quads = [\n        [0, 1, 3, 2], [4, 5, 7, 6],  # back, front\n        [1, 5, 4, 0], [2, 6, 7, 3],  # bottom, top\n        [4, 6, 2, 0], [3, 7, 5, 1],  # left, right\n    ]\n    triangles = sum([[[a, b, c], [c, d, a]] for [a, b, c, d] in quads], [])\n    return vertices, triangles\n\n\ndef main():\n\n    # Build the scene geometry, which is just an axis-aligned cube centred at the origin in world space\n    # We replicate vertices that are shared, so normals are effectively per-face instead of smoothed\n    cube_vertices_object, cube_faces = build_cube()\n    cube_vertices_object = tf.constant(cube_vertices_object, dtype=tf.float32)\n    cube_vertices_object, cube_faces = lighting.split_vertices_by_face(cube_vertices_object, cube_faces)\n    cube_vertex_colors = tf.ones_like(cube_vertices_object)\n\n    # Convert vertices to homogeneous coordinates\n    cube_vertices_object = tf.concat([\n        cube_vertices_object,\n        tf.ones_like(cube_vertices_object[:, -1:])\n    ], axis=1)\n\n    # Transform vertices from object to world space, by rotating around the vertical axis\n    cube_vertices_world = tf.matmul(cube_vertices_object, matrices.rodrigues([0., 0.5, 0.]))\n\n    # Calculate face normals; pre_split implies that no faces share a vertex\n    cube_normals_world = lighting.vertex_normals_pre_split(cube_vertices_world, cube_faces)\n\n    # Transform vertices from world to camera space; note that the camera points along the negative-z axis in camera space\n    view_matrix = matrices.compose(\n        matrices.translation([0., -1.5, -3.5]),  # translate it away from the camera\n        matrices.rodrigues([-0.3, 0., 0.])  # tilt the view downwards\n    )\n    cube_vertices_camera = tf.matmul(cube_vertices_world, view_matrix)\n\n    # Transform vertices from camera to clip space\n    projection_matrix = matrices.perspective_projection(near=0.1, far=20., right=0.1, aspect=float(frame_height) / frame_width)\n    cube_vertices_clip = tf.matmul(cube_vertices_camera, projection_matrix)\n\n    # The following function is applied to the G-buffer, which is a multi-channel image containing all the vertex attributes.\n    # It uses this to calculate the shading at each pixel, hence their final intensities\n    def shader_fn(gbuffer, view_matrix, light_direction):\n\n        # Unpack the different attributes from the G-buffer\n        mask = gbuffer[:, :, :1]\n        positions = gbuffer[:, :, 1:4]\n        unlit_colors = gbuffer[:, :, 4:7]\n        normals = gbuffer[:, :, 7:]\n\n        # Calculate a simple grey ambient lighting component\n        ambient_contribution = unlit_colors * [0.2, 0.2, 0.2]\n\n        # Calculate a red diffuse (Lambertian) lighting component\n        diffuse_contribution = lighting.diffuse_directional(\n            tf.reshape(normals, [-1, 3]),\n            tf.reshape(unlit_colors, [-1, 3]),\n            light_direction, light_color=[1., 0., 0.], double_sided=False\n        )\n        diffuse_contribution = tf.reshape(diffuse_contribution, [frame_height, frame_width, 3])\n\n        # Calculate a white specular (Phong) lighting component\n        camera_position_world = tf.matrix_inverse(view_matrix)[3, :3]\n        specular_contribution = lighting.specular_directional(\n            tf.reshape(positions, [-1, 3]),\n            tf.reshape(normals, [-1, 3]),\n            tf.reshape(unlit_colors, [-1, 3]),\n            light_direction, light_color=[1., 1., 1.],\n            camera_position=camera_position_world,\n            shininess=6., double_sided=False\n        )\n        specular_contribution = tf.reshape(specular_contribution, [frame_height, frame_width, 3])\n\n        # The final pixel intensities inside the shape are given by combining the three lighting components;\n        # outside the shape, they are set to a uniform background color. We clip the final values as the specular\n        # component saturates some pixels\n        pixels = tf.clip_by_value(\n            (diffuse_contribution + specular_contribution + ambient_contribution) * mask + [0., 0., 0.3] * (1. - mask),\n            0., 1.\n        )\n\n        return pixels\n\n    # Render the G-buffer channels (mask, vertex positions, vertex colours, and normals at each pixel), then perform\n    # the deferred shading calculation. In general, any tensor required by shader_fn and wrt which we need derivatives\n    # should be included in shader_additional_inputs; although in this example they are constant, we pass the view\n    # matrix and lighting direction through this route as an illustration\n    light_direction = tf.linalg.l2_normalize([1., -0.3, -0.5])\n    pixels = dirt.rasterise_deferred(\n        vertices=cube_vertices_clip,\n        vertex_attributes=tf.concat([\n            tf.ones_like(cube_vertices_object[:, :1]),  # mask\n            cube_vertices_world[:, :3],  # vertex positions\n            cube_vertex_colors,  # vertex colors\n            cube_normals_world  # normals\n        ], axis=1),\n        faces=cube_faces,\n        background_attributes=tf.zeros([frame_height, frame_width, 10]),\n        shader_fn=shader_fn,\n        shader_additional_inputs=[view_matrix, light_direction]\n    )\n\n    save_pixels = tf.write_file(\n        'deferred.jpg',\n        tf.image.encode_jpeg(tf.cast(pixels * 255, tf.uint8))\n    )\n\n    session = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))\n    with session.as_default():\n\n        save_pixels.run()\n\n\nif __name__ == '__main__':\n    main()\n\n"""
samples/simple.py,10,"b""\n# This is a simple example rendering a 3D cube in Dirt\n\nimport tensorflow as tf\nimport cv2  # OpenCV, used only to display the result\n\nimport dirt\nimport dirt.matrices as matrices\nimport dirt.lighting as lighting\n\n\nframe_width, frame_height = 640, 480\n\n\ndef build_cube():\n    vertices = [[x, y, z] for z in [-1, 1] for y in [-1, 1] for x in [-1, 1]]\n    quads = [\n        [0, 1, 3, 2], [4, 5, 7, 6],  # back, front\n        [1, 5, 4, 0], [2, 6, 7, 3],  # bottom, top\n        [4, 6, 2, 0], [3, 7, 5, 1],  # left, right\n    ]\n    triangles = sum([[[a, b, c], [c, d, a]] for [a, b, c, d] in quads], [])\n    return vertices, triangles\n\n\ndef unit(vector):\n    return tf.convert_to_tensor(vector) / tf.norm(vector)\n\n\ndef main():\n\n    # Build the scene geometry, which is just an axis-aligned cube centred at the origin in world space\n    # We replicate vertices that are shared, so normals are effectively per-face instead of smoothed\n    cube_vertices_object, cube_faces = build_cube()\n    cube_vertices_object = tf.constant(cube_vertices_object, dtype=tf.float32)\n    cube_vertices_object, cube_faces = lighting.split_vertices_by_face(cube_vertices_object, cube_faces)\n    cube_vertex_colors = tf.ones_like(cube_vertices_object)\n\n    # Convert vertices to homogeneous coordinates\n    cube_vertices_object = tf.concat([\n        cube_vertices_object,\n        tf.ones_like(cube_vertices_object[:, -1:])\n    ], axis=1)\n\n    # Transform vertices from object to world space, by rotating around the vertical axis\n    cube_vertices_world = tf.matmul(cube_vertices_object, matrices.rodrigues([0., 0.5, 0.]))\n\n    # Calculate face normals; pre_split implies that no faces share a vertex\n    cube_normals_world = lighting.vertex_normals_pre_split(cube_vertices_world, cube_faces)\n\n    # Transform vertices from world to camera space; note that the camera points along the negative-z axis in camera space\n    view_matrix = matrices.compose(\n        matrices.translation([0., -1.5, -3.5]),  # translate it away from the camera\n        matrices.rodrigues([-0.3, 0., 0.])  # tilt the view downwards\n    )\n    cube_vertices_camera = tf.matmul(cube_vertices_world, view_matrix)\n\n    # Transform vertices from camera to clip space\n    projection_matrix = matrices.perspective_projection(near=0.1, far=20., right=0.1, aspect=float(frame_height) / frame_width)\n    cube_vertices_clip = tf.matmul(cube_vertices_camera, projection_matrix)\n\n    # Calculate lighting, as combination of diffuse and ambient\n    vertex_colors_lit = lighting.diffuse_directional(\n        cube_normals_world, cube_vertex_colors,\n        light_direction=[1., 0., 0.], light_color=[1., 1., 1.]\n    ) * 0.8 + cube_vertex_colors * 0.2\n\n    pixels = dirt.rasterise(\n        vertices=cube_vertices_clip,\n        faces=cube_faces,\n        vertex_colors=vertex_colors_lit,\n        background=tf.zeros([frame_height, frame_width, 3]),\n        width=frame_width, height=frame_height, channels=3\n    )\n\n    session = tf.Session()\n    with session.as_default():\n\n        pixels_eval = pixels.eval()\n        cv2.imshow('simple.py', pixels_eval[:, :, (2, 1, 0)])\n        cv2.waitKey(0)\n\n\nif __name__ == '__main__':\n    main()\n\n"""
samples/textured.py,25,"b""\n# This demonstrates using Dirt for textured rendering with a UV map\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nimport dirt\nimport dirt.matrices as matrices\nimport dirt.lighting as lighting\n\n\nframe_width, frame_height = 640, 480\n\n\ndef uvs_to_pixel_indices(uvs, texture_shape, mode='repeat'):\n\n    # Note that this assumes u = 0, v = 0 is at the top-left of the image -- different to OpenGL!\n\n    uvs = uvs[..., ::-1]  # change x, y coordinates to y, x indices\n    texture_shape = tf.cast(texture_shape, tf.float32)\n    if mode == 'repeat':\n        return uvs % 1. * texture_shape\n    elif mode == 'clamp':\n        return tf.clip_by_value(uvs, 0., 1.) * texture_shape\n    else:\n        raise NotImplementedError\n\n\ndef sample_texture(texture, indices, mode='bilinear'):\n\n    if mode == 'nearest':\n\n        return tf.gather_nd(texture, tf.cast(indices, tf.int32))\n\n    elif mode == 'bilinear':\n\n        floor_indices = tf.floor(indices)\n        frac_indices = indices - floor_indices\n        floor_indices = tf.cast(floor_indices, tf.int32)\n\n        neighbours = tf.gather_nd(\n            texture,\n            tf.stack([\n                floor_indices,\n                floor_indices + [0, 1],\n                floor_indices + [1, 0],\n                floor_indices + [1, 1]\n            ]),\n        )\n        top_left, top_right, bottom_left, bottom_right = tf.unstack(neighbours)\n\n        return \\\n            top_left * (1. - frac_indices[..., 1:]) * (1. - frac_indices[..., :1]) + \\\n            top_right * frac_indices[..., 1:] * (1. - frac_indices[..., :1]) + \\\n            bottom_left * (1. - frac_indices[..., 1:]) * frac_indices[..., :1] + \\\n            bottom_right * frac_indices[..., 1:] * frac_indices[..., :1]\n\n    else:\n\n        raise NotImplementedError\n\n\ndef main():\n\n    # Build the scene geometry, which is just an axis-aligned cube centred at the origin in world space\n    cube_vertices_object = []\n    cube_uvs = []\n    cube_faces = []\n    def add_quad(vertices, uvs):\n        index = len(cube_vertices_object)\n        cube_faces.extend([[index + 2, index + 1, index], [index, index + 3, index + 2]])\n        cube_vertices_object.extend(vertices)\n        cube_uvs.extend(uvs)\n\n    add_quad(vertices=[[-1, -1, 1], [1, -1, 1], [1, 1, 1], [-1, 1, 1]], uvs=[[0.1, 0.9], [0.9, 0.9], [0.9, 0.1], [0.1, 0.1]])  # front\n    add_quad(vertices=[[-1, -1, -1], [1, -1, -1], [1, 1, -1], [-1, 1, -1]], uvs=[[1, 1], [0, 1], [0, 0], [1, 0]])  # back\n\n    add_quad(vertices=[[1, 1, 1], [1, 1, -1], [1, -1, -1], [1, -1, 1]], uvs=[[0.3, 0.25], [0.6, 0.25], [0.6, 0.55], [0.3, 0.55]])  # right\n    add_quad(vertices=[[-1, 1, 1], [-1, 1, -1], [-1, -1, -1], [-1, -1, 1]], uvs=[[0.4, 0.4], [0.5, 0.4], [0.5, 0.5], [0.4, 0.5]])  # left\n\n    add_quad(vertices=[[-1, 1, -1], [1, 1, -1], [1, 1, 1], [-1, 1, 1]], uvs=[[0, 0], [2, 0], [2, 2], [0, 2]])  # top\n    add_quad(vertices=[[-1, -1, -1], [1, -1, -1], [1, -1, 1], [-1, -1, 1]], uvs=[[0, 0], [2, 0], [2, 2], [0, 2]])  # bottom\n\n    cube_vertices_object = np.asarray(cube_vertices_object, np.float32)\n    cube_uvs = np.asarray(cube_uvs, np.float32)\n\n    # Load the texture image\n    texture = tf.cast(tf.image.decode_jpeg(tf.read_file(os.path.dirname(__file__) + '/cat.jpg')), tf.float32) / 255.\n\n    # Convert vertices to homogeneous coordinates\n    cube_vertices_object = tf.concat([\n        cube_vertices_object,\n        tf.ones_like(cube_vertices_object[:, -1:])\n    ], axis=1)\n\n    # Transform vertices from object to world space, by rotating around the vertical axis\n    cube_vertices_world = tf.matmul(cube_vertices_object, matrices.rodrigues([0., 0.6, 0.]))\n\n    # Calculate face normals\n    cube_normals_world = lighting.vertex_normals(cube_vertices_world, cube_faces)\n\n    # Transform vertices from world to camera space; note that the camera points along the negative-z axis in camera space\n    view_matrix = matrices.compose(\n        matrices.translation([0., -2., -3.2]),  # translate it away from the camera\n        matrices.rodrigues([-0.5, 0., 0.])  # tilt the view downwards\n    )\n    cube_vertices_camera = tf.matmul(cube_vertices_world, view_matrix)\n\n    # Transform vertices from camera to clip space\n    projection_matrix = matrices.perspective_projection(near=0.1, far=20., right=0.1, aspect=float(frame_height) / frame_width)\n    cube_vertices_clip = tf.matmul(cube_vertices_camera, projection_matrix)\n\n    # The following function is applied to the G-buffer, which is a multi-channel image containing all the vertex attributes. It\n    # uses this to calculate the shading (texture and lighting) at each pixel, hence their final intensities\n    def shader_fn(gbuffer, texture, light_direction):\n\n        # Unpack the different attributes from the G-buffer\n        mask = gbuffer[:, :, :1]\n        uvs = gbuffer[:, :, 1:3]\n        normals = gbuffer[:, :, 3:]\n\n        # Sample the texture at locations corresponding to each pixel; this defines the unlit material color at each point\n        unlit_colors = sample_texture(texture, uvs_to_pixel_indices(uvs, tf.shape(texture)[:2]))\n\n        # Calculate a simple grey ambient lighting component\n        ambient_contribution = unlit_colors * [0.4, 0.4, 0.4]\n\n        # Calculate a diffuse (Lambertian) lighting component\n        diffuse_contribution = lighting.diffuse_directional(\n            tf.reshape(normals, [-1, 3]),\n            tf.reshape(unlit_colors, [-1, 3]),\n            light_direction, light_color=[0.6, 0.6, 0.6], double_sided=True\n        )\n        diffuse_contribution = tf.reshape(diffuse_contribution, [frame_height, frame_width, 3])\n\n        # The final pixel intensities inside the shape are given by combining the ambient and diffuse components;\n        # outside the shape, they are set to a uniform background color\n        pixels = (diffuse_contribution + ambient_contribution) * mask + [0., 0., 0.3] * (1. - mask)\n\n        return pixels\n    \n    # Render the G-buffer channels (mask, UVs, and normals at each pixel), then perform the deferred shading calculation\n    # In general, any tensor required by shader_fn and wrt which we need derivatives should be included in shader_additional_inputs;\n    # although in this example they are constant, we pass the texture and lighting direction through this route as an illustration\n    light_direction = tf.linalg.l2_normalize([1., -0.3, -0.5])\n    pixels = dirt.rasterise_deferred(\n        vertices=cube_vertices_clip,\n        vertex_attributes=tf.concat([\n            tf.ones_like(cube_vertices_object[:, :1]),  # mask\n            cube_uvs,  # texture coordinates\n            cube_normals_world  # normals\n        ], axis=1),\n        faces=cube_faces,\n        background_attributes=tf.zeros([frame_height, frame_width, 6]),\n        shader_fn=shader_fn,\n        shader_additional_inputs=[texture, light_direction]\n    )\n\n    save_pixels = tf.write_file(\n        'textured.jpg',\n        tf.image.encode_jpeg(tf.cast(pixels * 255, tf.uint8))\n    )\n\n    session = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))\n    with session.as_default():\n\n        save_pixels.run()\n\n\nif __name__ == '__main__':\n    main()\n\n"""
tests/deferred_grad_test.py,59,"b""\nimport sys\nimport tensorflow as tf\n\nimport dirt\nimport dirt.matrices as matrices\nimport dirt.lighting as lighting\n\ncanvas_width, canvas_height = 32, 32\nsquare_size = 4.\n\n\ndef write_png(filename, image):\n\n    image = tf.cast(image * 255, tf.uint8)\n    return tf.write_file(filename, tf.image.encode_png(image))\n\n\ndef get_transformed_geometry(translation, rotation, scale):\n\n    # Build bent square in object space, on z = 0 plane\n    vertices_object = tf.constant([[-1, -1, 0.], [-1, 1, 0], [1, 1, 0], [1, -1, -1.3]], dtype=tf.float32) * square_size / 2\n    faces = [[0, 1, 2], [0, 2, 3]]\n\n    # ** we should add an occluding triangle!\n    # ** also a non-planar meeting-of-faces\n\n    vertices_object, faces = lighting.split_vertices_by_face(vertices_object, faces)\n\n    # Convert vertices to homogeneous coordinates\n    vertices_object = tf.concat([\n        vertices_object,\n        tf.ones_like(vertices_object[:, -1:])\n    ], axis=1)\n\n    # Transform vertices from object to world space, by rotating around the z-axis\n    vertices_world = tf.matmul(vertices_object, matrices.rodrigues([0., 0., rotation])) * scale + tf.concat([translation, [0.]], axis=0)\n\n    # Calculate face normals\n    normals_world = lighting.vertex_normals(vertices_world, faces)\n\n    # Transform vertices from world to camera space; note that the camera points along the negative-z axis in camera space\n    view_matrix = matrices.translation([-0.5, 0., -3.5])  # translate it away from the camera\n    vertices_camera = tf.matmul(vertices_world, view_matrix)\n\n    # Transform vertices from camera to clip space\n    projection_matrix = matrices.perspective_projection(near=0.1, far=20., right=0.1, aspect=float(canvas_height) / canvas_width)\n    vertices_clip = tf.matmul(vertices_camera, projection_matrix)\n\n    vertex_colours = tf.concat([\n        tf.ones([3, 3]) * [0.8, 0.5, 0.],\n        tf.ones([3, 3]) * [0.5, 0.8, 0.]\n    ], axis=0)\n\n    return vertices_clip, faces, normals_world, vertex_colours\n\n\ndef calculate_shading(colours, normals, light_intensity):\n\n    ambient = colours * [0.4, 0.4, 0.4]\n\n    light_direction = tf.linalg.l2_normalize([1., -0.3, -0.5])\n    diffuse_contribution = lighting.diffuse_directional(\n        tf.reshape(normals, [-1, 3]),\n        tf.reshape(colours, [-1, 3]),\n        light_direction, light_color=tf.constant([0., 1., 0.]) * light_intensity, double_sided=True\n    )\n    diffuse = tf.reshape(diffuse_contribution, colours.get_shape())\n\n    return ambient + diffuse\n\n\ndef get_pixels_direct(transformed_vertices, faces, vertex_normals, vertex_colours, light_intensity, background):\n\n    return dirt.rasterise(\n        vertices=transformed_vertices,\n        faces=faces,\n        vertex_colors=calculate_shading(vertex_colours, vertex_normals, light_intensity),\n        background=tf.ones([canvas_height, canvas_width, 3]) * background\n    )\n\n\ndef get_pixels_deferred_v1(transformed_vertices, faces, vertex_normals, vertex_colours, light_intensity, background):\n\n    # This is a naive implementation of deferred shading, that gives incorrect gradients. See\n    # get_pixels_deferred_v2 below for a correct implementation!\n\n    gbuffer_mask = dirt.rasterise(\n        vertices=transformed_vertices,\n        faces=faces,\n        vertex_colors=tf.ones_like(transformed_vertices[:, :1]),\n        background=tf.zeros([canvas_height, canvas_width, 1]),\n        width=canvas_width, height=canvas_height, channels=1\n    )[..., 0]\n    background_value = -1.e4\n    gbuffer_vertex_colours_world = dirt.rasterise(\n        vertices=transformed_vertices,\n        faces=faces,\n        vertex_colors=vertex_colours,\n        background=tf.ones([canvas_height, canvas_width, 3]) * background,\n        width=canvas_width, height=canvas_height, channels=3\n    )\n    gbuffer_vertex_normals_world = dirt.rasterise(\n        vertices=transformed_vertices,\n        faces=faces,\n        vertex_colors=vertex_normals,\n        background=tf.ones([canvas_height, canvas_width, 3]) * background_value,\n        width=canvas_width, height=canvas_height, channels=3\n    )\n\n    # Dilate the normals to ensure correct gradients on the silhouette\n    gbuffer_mask = gbuffer_mask[:, :, None]\n    gbuffer_vertex_normals_world_dilated = tf.nn.max_pool(gbuffer_vertex_normals_world[None, ...], ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding='SAME')[0]\n    gbuffer_vertex_normals_world = gbuffer_vertex_normals_world * gbuffer_mask + gbuffer_vertex_normals_world_dilated * (1. - gbuffer_mask)\n\n    pixels = gbuffer_mask * calculate_shading(gbuffer_vertex_colours_world, gbuffer_vertex_normals_world, light_intensity) + (1. - gbuffer_mask) * background\n\n    return pixels\n\n\ndef get_pixels_deferred_v2(transformed_vertices, faces, vertex_normals, vertex_colours, light_intensity, background):\n\n    vertex_attributes = tf.concat([tf.ones_like(transformed_vertices[:, :1]), vertex_colours, vertex_normals], axis=1)\n    background_attributes = tf.zeros([canvas_height, canvas_width, 1 + 3 + 3])\n\n    def shader_fn(gbuffer, light_intensity, background):\n        mask = gbuffer[..., :1]\n        colours = gbuffer[..., 1:4]\n        normals = gbuffer[..., 4:7]\n        pixels = mask * calculate_shading(colours, normals, light_intensity) + (1. - mask) * background\n        return pixels\n\n    pixels = dirt.rasterise_deferred(\n        background_attributes,\n        transformed_vertices,\n        vertex_attributes,\n        faces,\n        shader_fn,\n        [light_intensity, background]\n    )\n\n    return pixels\n\n\ndef prepare_gradient_images(deferred_gradients, direct_gradients):\n\n    # Concatenate then normalise, to ensure direct and deferred gradients are treated identically\n    # ** tidy up the mess of concats / transposes / reshapes here!\n    all_gradients = tf.concat([direct_gradients, deferred_gradients], axis=0)\n    all_gradients_normalised = all_gradients - tf.reduce_min(all_gradients, axis=[0, 1, 2], keepdims=True)\n    all_gradients_normalised /= tf.reduce_max(all_gradients_normalised, axis=[0, 1, 2], keepdims=True)\n\n    epsilon = 1.e-3\n    all_gradients_signs = tf.stack([\n        tf.cast(tf.greater(all_gradients[:, :, 1], epsilon), tf.float32),\n        tf.cast(tf.less(all_gradients[:, :, 1], -epsilon), tf.float32),\n        tf.zeros_like(all_gradients[:, :, 0])\n    ], axis=2)\n\n    all_gradients_images = tf.concat([\n        tf.reshape(tf.transpose(all_gradients_normalised, [0, 3, 1, 2]), [2, canvas_height, -1, 3]),\n        tf.reshape(tf.transpose(all_gradients_signs, [0, 3, 1, 2]), [2, canvas_height, -1, 3])\n    ], axis=1)\n\n    return all_gradients_images[0], all_gradients_images[1]\n\n\ndef main_graph():\n\n    translation = tf.Variable([0., 0., 0.], name='translation')\n    rotation = tf.Variable(0.5, name='rotation')\n    scale = tf.Variable(1., name='scale')\n    light_intensity = tf.Variable(0.6, name='light_intensity')\n    background = tf.Variable([0., 0., 0.2], name='background')\n\n    variables = [translation, rotation, scale, light_intensity, background]\n\n    transformed_vertices, faces, vertex_normals, vertex_colours = get_transformed_geometry(translation, rotation, scale)\n\n    pixels_direct = get_pixels_direct(transformed_vertices, faces, vertex_normals, vertex_colours, light_intensity, background)\n    save_pixels_direct = write_png('pixels_direct_graph.png', tf.tile(pixels_direct, [2, 9, 1]))\n\n    pixels_deferred = get_pixels_deferred_v2(transformed_vertices, faces, vertex_normals, vertex_colours, light_intensity, background)\n    save_pixels_deferred = write_png('pixels_deferred_graph.png', tf.tile(pixels_deferred, [2, 9, 1]))\n\n    def get_pixel_gradients(pixels):\n\n        def get_pixel_gradient(pixel_index):\n            y = pixel_index // 3 // canvas_width\n            x = pixel_index // 3 % canvas_width\n            c = pixel_index % 3\n            d_loss_by_pixels = tf.scatter_nd([[y, x, c]], [1.], shape=[canvas_height, canvas_width, 3])\n            return tf.concat([\n                tf.reshape(d_pixel_by_variables, [-1])\n                for d_pixel_by_variables\n                in tf.gradients(pixels, variables, d_loss_by_pixels)\n            ], axis=0)\n\n        d_pixels_by_variables = tf.reshape(\n            tf.map_fn(get_pixel_gradient, tf.range(canvas_height * canvas_width * 3), dtype=tf.float32),\n            [canvas_height, canvas_width, 3, -1]\n        )  # indexed by y, x, r/g/b, variable\n\n        return d_pixels_by_variables\n\n    direct_gradients = get_pixel_gradients(pixels_direct)\n    deferred_gradients = get_pixel_gradients(pixels_deferred)\n\n    direct_gradients_image, deferred_gradients_image = prepare_gradient_images(deferred_gradients, direct_gradients)\n\n    save_grads_direct = write_png('grads_direct_graph.png', direct_gradients_image)\n    save_grads_deferred = write_png('grads_deferred_graph.png', deferred_gradients_image)\n\n    session = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))\n    with session.as_default():\n\n        tf.global_variables_initializer().run()\n\n        session.run([save_pixels_direct, save_pixels_deferred, save_grads_direct, save_grads_deferred])\n\n\ndef main_eager():\n\n    tf.enable_eager_execution(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))\n\n    translation = tf.Variable([0., 0., 0.], name='translation')\n    rotation = tf.Variable(0.5, name='rotation')\n    scale = tf.Variable(1., name='scale')\n    light_intensity = tf.Variable(0.6, name='light_intensity')\n    background = tf.Variable([0., 0., 0.2], name='background')\n\n    variables = [translation, rotation, scale, light_intensity, background]\n\n    with tf.GradientTape(persistent=True) as tape:\n\n        transformed_vertices, faces, vertex_normals, vertex_colours = get_transformed_geometry(translation, rotation, scale)\n\n        pixels_direct = get_pixels_direct(transformed_vertices, faces, vertex_normals, vertex_colours, light_intensity, background)\n        pixels_deferred = get_pixels_deferred_v2(transformed_vertices, faces, vertex_normals, vertex_colours, light_intensity, background)\n\n    write_png('pixels_direct_eager.png', tf.tile(pixels_direct, [2, 9, 1]))\n    write_png('pixels_deferred_eager.png', tf.tile(pixels_deferred, [2, 9, 1]))\n\n    direct_gradients = tape.jacobian(pixels_direct, variables, experimental_use_pfor=False)  # indexed by [variable], y, x, channel, then optionally variable-dimension\n    deferred_gradients = tape.jacobian(pixels_deferred, variables, experimental_use_pfor=False)\n\n    def rearrange_gradients(gradients):\n        # Rearrange the output of GradientTape.jacobian to match that of get_pixel_gradients in main_graph\n        return tf.concat([\n            gradient if len(gradient.shape) == 4 else gradient[..., None]\n            for gradient in gradients\n        ], axis=3)\n    direct_gradients_image, deferred_gradients_image = prepare_gradient_images(\n        rearrange_gradients(deferred_gradients),\n        rearrange_gradients(direct_gradients)\n    )\n\n    write_png('grads_direct_eager.png', direct_gradients_image)\n    write_png('grads_deferred_eager.png', deferred_gradients_image)\n\n\nif __name__ == '__main__':\n\n    if len(sys.argv) != 2:\n        print('expected one argument, specifying graph or eager execution mode')\n    elif sys.argv[1] == 'graph':\n        main_graph()\n    elif sys.argv[1] == 'eager':\n        main_eager()\n    else:\n        print('invalid execution mode; should be graph or eager')\n\n"""
tests/lighting_tests.py,15,"b""\nimport numpy as np\nimport tensorflow as tf\nimport math\nimport cv2\nimport dirt.rasterise_ops\nimport dirt.matrices\nimport dirt.lighting\n\nfrom rasterise_tests import make_cylinder\n\n\ndef main():\n\n    w, h = 256, 192\n\n    vertices, faces = make_cylinder(0.2, 0.75, 0.1, 0.2, 32)\n    vertices = np.float32(np.concatenate([vertices, np.ones([len(vertices), 1])], axis=1))\n\n    rotation_xy = tf.placeholder(tf.float32, [])\n    rotation_matrix = tf.convert_to_tensor([\n        [0.5 * tf.cos(rotation_xy), 0.5 * -tf.sin(rotation_xy), 0., 0.],\n        [0.5 * tf.sin(rotation_xy), 0.5 * tf.cos(rotation_xy), 0., 0.],\n        [0., 0., 0.5, 0.],\n        [0., 0., 0., 1.]\n    ])\n\n    translation = tf.placeholder(tf.float32, [3])\n    translation_matrix = tf.stack([\n        [1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        tf.concat([translation, [1.]], axis=0)\n    ])\n\n    transformed_vertices = tf.matmul(tf.matmul(vertices, rotation_matrix), translation_matrix)\n    vertex_normals = dirt.lighting.vertex_normals(transformed_vertices[:, :3], faces)\n\n    transformed_vertices_split, faces_split = dirt.lighting.split_vertices_by_face(transformed_vertices, faces)\n    vertex_normals_split = dirt.lighting.vertex_normals_pre_split(transformed_vertices_split[:, :3], faces_split)\n\n    projection_matrix = dirt.matrices.perspective_projection(0.1, 20., 0.2, float(h) / w)\n    projected_vertices = tf.matmul(transformed_vertices, projection_matrix)\n    projected_vertices_split = tf.matmul(transformed_vertices_split, projection_matrix)\n\n    normal_im = dirt.rasterise_ops.rasterise(background=tf.zeros([h, w, 3]), vertices=projected_vertices, vertex_colors=tf.abs(vertex_normals), faces=faces, height=h, width=w, channels=3)\n\n    # ** the following two need to be careful about transformations: they use 'raw' normals and vertices, which means\n    # ** the lights are effectively transformed along with the geometry -- or equivalently, the lights are in object space\n    directional_im = dirt.rasterise_ops.rasterise(background=tf.zeros([h, w, 3]), vertices=projected_vertices, vertex_colors=tf.cast(dirt.lighting.diffuse_directional(vertex_normals, tf.ones([vertices.shape[0], 3]), [1., 0, 0], [1., 1., 0.], False) + [0, 0, 0.4], tf.float32), faces=faces, height=h, width=w, channels=3)\n    point_im = dirt.rasterise_ops.rasterise(background=tf.zeros([h, w, 3]), vertices=projected_vertices, vertex_colors=tf.cast(dirt.lighting.diffuse_point(transformed_vertices[:, :3], vertex_normals, tf.ones([vertices.shape[0], 3]), [0.5, -1., 0.5], [1., 0.5, 0.9], False) + [0, 0, 0.4], tf.float32), faces=faces, height=h, width=w, channels=3)\n    point_im_split = dirt.rasterise_ops.rasterise(background=tf.zeros([h, w, 3]), vertices=projected_vertices_split, vertex_colors=tf.cast(dirt.lighting.diffuse_point(transformed_vertices_split[:, :3], vertex_normals_split, tf.ones([transformed_vertices_split.shape[0], 3]), [0.5, -1., 0.5], [1., 0.5, 0.9], False) + [0, 0, 0.4], tf.float32), faces=faces_split, height=h, width=w, channels=3)\n\n    session = tf.Session()\n    with session.as_default():\n\n        normal_im_ = normal_im.eval({translation: [0., 0., -0.25], rotation_xy: 0.})\n        directional_im_ = directional_im.eval({translation: [0., 0., -0.25], rotation_xy: 0.})\n        point_im_ = point_im.eval({translation: [0., 0., -0.25], rotation_xy: 0.})\n        point_im_split_ = point_im_split.eval({translation: [0., 0., -0.25], rotation_xy: 0.})\n\n        cv2.imshow('normals', normal_im_[:, :, (2, 1, 0)])\n        cv2.imshow('directional', directional_im_[:, :, (2, 1, 0)])\n        cv2.imshow('point', point_im_[:, :, (2, 1, 0)])\n        cv2.imshow('point_split', point_im_split_[:, :, (2, 1, 0)])\n        cv2.waitKey(0)\n\n\nif __name__ == '__main__':\n    main()\n\n"""
tests/multi_gpu_test.py,7,"b""\nimport tensorflow as tf\nimport dirt\n\n\ndef make_pixels():\n\n    square_vertices = tf.constant([[0, 0], [0, 1], [1, 1], [1, 0]], dtype=tf.float32)\n    square_vertices = tf.concat([square_vertices, tf.zeros([4, 1]), tf.ones([4, 1])], axis=1)\n\n    return dirt.rasterise(\n        vertices=square_vertices,\n        faces=[[0, 1, 2], [0, 2, 3]],\n        vertex_colors=tf.ones([4, 3]),\n        background=tf.zeros([256, 256, 3]),\n        height=256, width=256, channels=3\n    )[:, :, 0]\n\n\ndef main():\n\n    with tf.device('/gpu:0'):\n        pixels_0 = make_pixels()\n    with tf.device('/gpu:1'):\n        pixels_1 = make_pixels()\n\n    session = tf.Session()\n    with session.as_default():\n        session.run([pixels_0, pixels_1])\n\n\nif __name__ == '__main__':\n    main()\n\n"""
tests/rasterise_tests.py,19,"b""from __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\nimport math\nimport cv2\nimport dirt.rasterise_ops\nimport dirt.lighting\nimport dirt.matrices\n\n\ndef make_cylinder(radius, height, end_offset, bevel, segments):\n\n    # Cylinder centred on the origin, with axis along y-axis, and bevelled conical ends\n\n    angles = np.linspace(0., 2 * math.pi, segments, endpoint=False, dtype=np.float32)\n    xz = np.stack([np.cos(angles), np.sin(angles)], axis=1) * radius\n    top_bevel_vertices = np.stack([xz[:, 0] * (1. - bevel), np.ones(segments) * -height / 2. - radius * bevel, xz[:, 1] * (1. - bevel)], axis=1)\n    top_vertices = np.stack([xz[:, 0], np.ones(segments) * -height / 2., xz[:, 1]], axis=1)\n    bottom_vertices = np.stack([xz[:, 0], np.ones(segments) * height / 2., xz[:, 1]], axis=1)\n    bottom_bevel_vertices = np.stack([xz[:, 0] * (1. - bevel), np.ones(segments) * height / 2. + radius * bevel, xz[:, 1] * (1. - bevel)], axis=1)\n    end_vertices = [[0., -height / 2. - end_offset, 0.], [0., height / 2. + end_offset, 0.]]\n    all_vertices = np.concatenate([top_bevel_vertices, top_vertices, bottom_vertices, bottom_bevel_vertices, end_vertices], axis=0)\n\n    faces = []\n    def make_ring(start):\n        for quad_index in range(segments):\n            upper_first = start + quad_index\n            upper_second = start + (quad_index + 1) % segments\n            lower_first = start + quad_index + segments\n            lower_second = start + (quad_index + 1) % segments + segments\n            faces.extend([\n                [upper_first, upper_second, lower_first],\n                [lower_first, upper_second, lower_second]\n            ])\n    make_ring(0)\n    make_ring(segments)\n    make_ring(segments * 2)\n    for top_first in range(segments):\n        top_second = (top_first + 1) % segments\n        bottom_first = top_first + segments * 3\n        bottom_second = (bottom_first + 1) % segments\n        faces.extend([\n            [segments * 4, top_first, top_second],\n            [segments * 4 + 1, bottom_first, bottom_second]\n        ])\n\n    return all_vertices, np.array(faces, dtype=np.int32)\n\n\ndef mesh():\n\n    vertices, faces = make_cylinder(0.2, 0.75, 0.1, 0., 10)\n    vertices = np.float32(np.concatenate([vertices, np.ones([len(vertices), 1])], axis=1))\n\n    w = 48\n    h = 36\n\n    rotation_xy = tf.placeholder(tf.float32)\n    view_matrix_1 = tf.convert_to_tensor([\n        [0.5 * tf.cos(rotation_xy), 0.5 * -tf.sin(rotation_xy), 0., 0.],\n        [0.5 * tf.sin(rotation_xy), 0.5 * tf.cos(rotation_xy), 0., 0.],\n        [0., 0., 0.5, 0.],\n        [0., 0., 0., 1.]\n    ])\n\n    translation = tf.placeholder(tf.float32)\n\n    view_matrix_2 = tf.stack([\n        [1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        tf.concat([translation, [1.]], axis=0)\n    ])\n\n    if True:  # use splitting\n        vertex_count = len(faces) * 3\n        vertices, faces = dirt.lighting.split_vertices_by_face(vertices, faces)\n    else:\n        vertex_count = len(vertices)\n\n    projection_matrix = dirt.matrices.perspective_projection(0.1, 20., 0.2, float(h) / w)\n    projected_vertices = tf.matmul(tf.matmul(tf.matmul(vertices, view_matrix_1), view_matrix_2), projection_matrix)\n\n    bgcolor = tf.placeholder(tf.float32, [3])\n    vertex_color = tf.placeholder(tf.float32, [3])\n    vertex_colors = tf.concat([tf.tile(vertex_color[np.newaxis, :], [75, 1]), np.random.uniform(size=[vertex_count - 75, 3])], axis=0)\n\n    im = dirt.rasterise_ops.rasterise(tf.concat([tf.tile(bgcolor[np.newaxis, np.newaxis, :], [h // 2, w, 1]), tf.ones([h // 2, w, 3])], axis=0), projected_vertices, vertex_colors, faces, height=h, width=w, channels=3)\n    ims = dirt.rasterise_ops.rasterise_batch(tf.tile(tf.constant([[0., 0., 0.], [0., 0., 1.]])[:, np.newaxis, np.newaxis, :], [1, h, w, 1]), tf.tile(projected_vertices[np.newaxis, ...], [2, 1, 1]), np.random.uniform(size=[2, vertex_count, 3]), tf.tile(faces[np.newaxis, ...], [2, 1, 1]), height=h, width=w, channels=3)\n\n    d_loss_by_pixels = tf.placeholder(tf.float32, [h, w, 3])\n    [gt, gr, gb, gc] = tf.gradients(im, [translation, rotation_xy, bgcolor, vertex_color], d_loss_by_pixels)\n\n    ds_loss_by_pixels = tf.placeholder(tf.float32, [2, h, w, 3])\n    [gst, gsr] = tf.gradients(ims, [translation, rotation_xy], ds_loss_by_pixels)\n\n    session = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))\n    with session.as_default():\n        tf.global_variables_initializer().run()\n\n        gx_im = np.empty([h, w, 3], dtype=np.float32)\n        gy_im = np.empty([h, w, 3], dtype=np.float32)\n        gz_im = np.empty([h, w, 3], dtype=np.float32)\n        gr_im = np.empty([h, w, 3], dtype=np.float32)\n        gb_im = np.empty([h, w, 3], dtype=np.float32)\n        gc_im = np.empty([h, w, 3], dtype=np.float32)\n\n        for y in range(h):\n            for x in range(w):\n                for c in range(3):\n                    pixel_indicator = np.zeros([h, w, 3], dtype=np.float32)\n                    pixel_indicator[y, x, c] = 1\n                    [[gx_im[y, x, c], gy_im[y, x, c], gz_im[y, x, c]], gr_im[y, x, c], [gb_im[y, x, c], _, _], [gc_im[y, x, c], _, _]] = \\\n                        session.run([gt, gr, gb, gc], {d_loss_by_pixels: pixel_indicator, translation: [0., 0., -0.25], rotation_xy: 0., bgcolor: [0.4, 0.2, 0.2], vertex_color: [0.7, 0.3, 0.6]})\n                print('.', end='')\n            print()\n\n        gsx_im = np.empty([2, h, w, 3], dtype=np.float32)\n        gsy_im = np.empty([2, h, w, 3], dtype=np.float32)\n        gsz_im = np.empty([2, h, w, 3], dtype=np.float32)\n        gsr_im = np.empty([2, h, w, 3], dtype=np.float32)\n\n        for iib in range(2):\n            print(iib + 1)\n            for y in range(h):\n                for x in range(w):\n                    for c in range(3):\n                        pixel_indicator = np.zeros([2, h, w, 3], dtype=np.float32)\n                        pixel_indicator[iib, y, x, c] = 1\n                        [[gsx_im[iib, y, x, c], gsy_im[iib, y, x, c], gsz_im[iib, y, x, c]], gsr_im[iib, y, x, c]] = session.run([gst, gsr], {ds_loss_by_pixels: pixel_indicator, translation: [0., 0., -1.], rotation_xy: 0.5})\n                    print('.', end='')\n                print()\n\n        cv2.imshow('im', im.eval({translation: [0., 0., -0.25], rotation_xy: 0., bgcolor: [0.6, 0.2, 0.2], vertex_color: [0.7, 0.3, 0.6]}))\n        cv2.imshow('ims', np.concatenate(ims.eval({translation: [0., 0., -0.25], rotation_xy: 0.5}), axis=1))\n\n        g_im = np.concatenate([gx_im, gy_im, gz_im, gr_im * 3., gb_im * 30., gc_im * 50.], axis=1)\n        g_im = (g_im - np.min(g_im)) / (np.max(g_im) - np.min(g_im))\n        cv2.imshow('grad', g_im)\n\n        gs_im = np.concatenate(np.concatenate([gsx_im, gsy_im, gsz_im, gsr_im * 3.], axis=2), axis=0)\n        gs_im = (gs_im - np.min(gs_im)) / (np.max(gs_im) - np.min(gs_im))\n        cv2.imshow('grads', gs_im)\n\n        cv2.waitKey(0)\n\n\nif __name__ == '__main__':\n    mesh()\n\n"""
tests/square_test.py,12,"b""\nimport numpy as np\nimport tensorflow as tf\nimport dirt\n\ncanvas_width, canvas_height = 128, 128\ncentre_x, centre_y = 32, 64\nsquare_size = 16\n\n\ndef get_non_dirt_pixels():\n    xs, ys = tf.meshgrid(tf.range(canvas_width), tf.range(canvas_height))\n    xs = tf.cast(xs, tf.float32) + 0.5\n    ys = tf.cast(ys, tf.float32) + 0.5\n    x_in_range = tf.less_equal(tf.abs(xs - centre_x), square_size / 2)\n    y_in_range = tf.less_equal(tf.abs(ys - centre_y), square_size / 2)\n    return tf.cast(tf.logical_and(x_in_range, y_in_range), tf.float32)\n\n\ndef get_dirt_pixels():\n\n    # Build square in screen space\n    square_vertices = tf.constant([[0, 0], [0, 1], [1, 1], [1, 0]], dtype=tf.float32) * square_size - square_size / 2.\n    square_vertices += [centre_x, centre_y]\n\n    # Transform to homogeneous coordinates in clip space\n    square_vertices = square_vertices * 2. / [canvas_width, canvas_height] - 1.\n    square_vertices = tf.concat([square_vertices, tf.zeros([4, 1]), tf.ones([4, 1])], axis=1)\n\n    return dirt.rasterise(\n        vertices=square_vertices,\n        faces=[[0, 1, 2], [0, 2, 3]],\n        vertex_colors=tf.ones([4, 1]),\n        background=tf.zeros([canvas_height, canvas_width, 1]),\n        height=canvas_height, width=canvas_width, channels=1\n    )[:, :, 0]\n\n\ndef main():\n\n    if '.' in tf.__version__ and int(tf.__version__.split('.')[0]) < 2:\n\n        session = tf.Session()\n        with session.as_default():\n\n            non_dirt_pixels = get_non_dirt_pixels().eval()\n            dirt_pixels = get_dirt_pixels().eval()\n\n    else:\n\n        non_dirt_pixels = get_non_dirt_pixels().numpy()\n        dirt_pixels = get_dirt_pixels().numpy()\n\n    if np.all(non_dirt_pixels == dirt_pixels):\n        print('successful: all pixels agree')\n    else:\n        print('failed: {} pixels disagree'.format(np.sum(non_dirt_pixels != dirt_pixels)))\n\n\nif __name__ == '__main__':\n    main()\n\n"""
