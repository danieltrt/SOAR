file_path,api_count,code
python/config.py,0,"b""# path to training data\ntraining_data_path = 'data/conversations_lenmax22_formersents2_with_former'\n\n# path to all_words\nall_words_path = 'data/all_words.txt'\n\n# training parameters \nCHECKPOINT = True\ntrain_model_path = 'model'\ntrain_model_name = 'model-55'\nstart_epoch = 56\nstart_batch = 0\nbatch_size = 25\n\n# for RL training\ntraining_type = 'normal' # 'normal' for seq2seq training, 'pg' for policy gradient\nreversed_model_path = 'Adam_encode22_decode22_reversed-maxlen22_lr0.0001_batch25_wordthres6'\nreversed_model_name = 'model-63'\n\n# data reader shuffle index list\nload_list = False\nindex_list_file = 'data/shuffle_index_list'\ncur_train_index = start_batch * batch_size\n\n# word count threshold\nWC_threshold = 20\nreversed_WC_threshold = 6\n\n# dialog simulation turns\nMAX_TURNS = 10"""
python/data_parser.py,0,"b'# coding=utf-8\n\nfrom __future__ import print_function\nimport pickle\nimport codecs\nimport re\nimport os\nimport time\nimport numpy as np\nimport config\n\ndef preProBuildWordVocab(word_count_threshold=5, all_words_path=config.all_words_path):\n    # borrowed this function from NeuralTalk\n\n    if not os.path.exists(all_words_path):\n        parse_all_words(all_words_path)\n\n    corpus = open(all_words_path, \'r\').read().split(\'\\n\')[:-1]\n    captions = np.asarray(corpus, dtype=np.object)\n\n    captions = map(lambda x: x.replace(\'.\', \'\'), captions)\n    captions = map(lambda x: x.replace(\',\', \'\'), captions)\n    captions = map(lambda x: x.replace(\'""\', \'\'), captions)\n    captions = map(lambda x: x.replace(\'\\n\', \'\'), captions)\n    captions = map(lambda x: x.replace(\'?\', \'\'), captions)\n    captions = map(lambda x: x.replace(\'!\', \'\'), captions)\n    captions = map(lambda x: x.replace(\'\\\\\', \'\'), captions)\n    captions = map(lambda x: x.replace(\'/\', \'\'), captions)\n\n    print(\'preprocessing word counts and creating vocab based on word count threshold %d\' % (word_count_threshold))\n    word_counts = {}\n    nsents = 0\n    for sent in captions:\n        nsents += 1\n        for w in sent.lower().split(\' \'):\n           word_counts[w] = word_counts.get(w, 0) + 1\n    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n    print(\'filtered words from %d to %d\' % (len(word_counts), len(vocab)))\n\n    ixtoword = {}\n    ixtoword[0] = \'<pad>\'\n    ixtoword[1] = \'<bos>\'\n    ixtoword[2] = \'<eos>\'\n    ixtoword[3] = \'<unk>\'\n\n    wordtoix = {}\n    wordtoix[\'<pad>\'] = 0\n    wordtoix[\'<bos>\'] = 1\n    wordtoix[\'<eos>\'] = 2\n    wordtoix[\'<unk>\'] = 3\n\n    for idx, w in enumerate(vocab):\n        wordtoix[w] = idx+4\n        ixtoword[idx+4] = w\n\n    word_counts[\'<pad>\'] = nsents\n    word_counts[\'<bos>\'] = nsents\n    word_counts[\'<eos>\'] = nsents\n    word_counts[\'<unk>\'] = nsents\n\n    bias_init_vector = np.array([1.0 * word_counts[ixtoword[i]] for i in ixtoword])\n    bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n    bias_init_vector = np.log(bias_init_vector)\n    bias_init_vector -= np.max(bias_init_vector) # shift to nice numeric range\n\n    return wordtoix, ixtoword, bias_init_vector\n\ndef parse_all_words(all_words_path):\n    raw_movie_lines = open(\'data/movie_lines.txt\', \'r\', encoding=\'utf-8\', errors=\'ignore\').read().split(\'\\n\')[:-1]\n\n    with codecs.open(all_words_path, ""w"", encoding=\'utf-8\', errors=\'ignore\') as f:\n        for line in raw_movie_lines:\n            line = line.split(\' +++$+++ \')\n            utterance = line[-1]\n            f.write(utterance + \'\\n\')\n\n"""""" Extract only the vocabulary part of the data """"""\ndef refine(data):\n    words = re.findall(""[a-zA-Z\'-]+"", data)\n    words = ["""".join(word.split(""\'"")) for word in words]\n    # words = ["""".join(word.split(""-"")) for word in words]\n    data = \' \'.join(words)\n    return data\n\nif __name__ == \'__main__\':\n    parse_all_words(config.all_words_path)\n\n    raw_movie_lines = open(\'data/movie_lines.txt\', \'r\', encoding=\'utf-8\', errors=\'ignore\').read().split(\'\\n\')[:-1]\n    \n    utterance_dict = {}\n    with codecs.open(\'data/tokenized_all_words.txt\', ""w"", encoding=\'utf-8\', errors=\'ignore\') as f:\n        for line in raw_movie_lines:\n            line = line.split(\' +++$+++ \')\n            line_ID = line[0]\n            utterance = line[-1]\n            utterance_dict[line_ID] = utterance\n            utterance = "" "".join([refine(w) for w in utterance.lower().split()])\n            f.write(utterance + \'\\n\')\n    pickle.dump(utterance_dict, open(\'data/utterance_dict\', \'wb\'), True)\n'"
python/data_reader.py,0,"b""# coding=utf-8\n\nfrom __future__ import print_function\nimport cPickle as pickle\nimport config\nimport random\n\nclass Data_Reader:\n    def __init__(self, cur_train_index=0, load_list=False):\n        self.training_data = pickle.load(open(config.training_data_path, 'rb'))\n        self.data_size = len(self.training_data)\n        if load_list:\n            self.shuffle_list = pickle.load(open(config.index_list_file, 'rb'))\n        else:    \n            self.shuffle_list = self.shuffle_index()\n        self.train_index = cur_train_index\n\n    def get_batch_num(self, batch_size):\n        return self.data_size // batch_size\n\n    def shuffle_index(self):\n        shuffle_index_list = random.sample(range(self.data_size), self.data_size)\n        pickle.dump(shuffle_index_list, open(config.index_list_file, 'wb'), True)\n        return shuffle_index_list\n\n    def generate_batch_index(self, batch_size):\n        if self.train_index + batch_size > self.data_size:\n            batch_index = self.shuffle_list[self.train_index:self.data_size]\n            self.shuffle_list = self.shuffle_index()\n            remain_size = batch_size - (self.data_size - self.train_index)\n            batch_index += self.shuffle_list[:remain_size]\n            self.train_index = remain_size\n        else:\n            batch_index = self.shuffle_list[self.train_index:self.train_index+batch_size]\n            self.train_index += batch_size\n\n        return batch_index\n\n    def generate_training_batch(self, batch_size):\n        batch_index = self.generate_batch_index(batch_size)\n        batch_X = [self.training_data[i][0] for i in batch_index]   # batch_size of conv_a\n        batch_Y = [self.training_data[i][1] for i in batch_index]   # batch_size of conv_b\n\n        return batch_X, batch_Y\n\n    def generate_training_batch_with_former(self, batch_size):\n        batch_index = self.generate_batch_index(batch_size)\n        batch_X = [self.training_data[i][0] for i in batch_index]   # batch_size of conv_a\n        batch_Y = [self.training_data[i][1] for i in batch_index]   # batch_size of conv_b\n        former = [self.training_data[i][2] for i in batch_index]    # batch_size of former utterance\n\n        return batch_X, batch_Y, former\n\n    def generate_testing_batch(self, batch_size):\n        batch_index = self.generate_batch_index(batch_size)\n        batch_X = [self.training_data[i][0] for i in batch_index]   # batch_size of conv_a\n\n        return batch_X\n"""
python/feature_extracter.py,0,"b'# coding=utf-8\n\nfrom __future__ import print_function\nimport cPickle as pickle\nimport time\nimport re\nimport numpy as np\nfrom gensim.models import word2vec, KeyedVectors\n\nWORD_VECTOR_SIZE = 300\n\nraw_movie_conversations = open(\'data/movie_conversations.txt\', \'r\').read().split(\'\\n\')[:-1]\n\nutterance_dict = pickle.load(open(\'data/utterance_dict\', \'rb\'))\n\nts = time.time()\ncorpus = word2vec.Text8Corpus(""data/tokenized_all_words.txt"")\nword_vector = word2vec.Word2Vec(corpus, size=WORD_VECTOR_SIZE)\nword_vector.wv.save_word2vec_format(u""model/word_vector.bin"", binary=True)\nword_vector = KeyedVectors.load_word2vec_format(\'model/word_vector.bin\', binary=True)\nprint(""Time Elapsed: {} secs\\n"".format(time.time() - ts))\n\n"""""" Extract only the vocabulary part of the data """"""\ndef refine(data):\n    words = re.findall(""[a-zA-Z\'-]+"", data)\n    words = ["""".join(word.split(""\'"")) for word in words]\n    # words = ["""".join(word.split(""-"")) for word in words]\n    data = \' \'.join(words)\n    return data\n\nts = time.time()\nconversations = []\nprint(\'len conversation\', len(raw_movie_conversations))\ncon_count = 0\ntraindata_count = 0\nfor conversation in raw_movie_conversations:\n    conversation = conversation.split(\' +++$+++ \')[-1]\n    conversation = conversation.replace(\'[\', \'\')\n    conversation = conversation.replace(\']\', \'\')\n    conversation = conversation.replace(\'\\\'\', \'\')\n    conversation = conversation.split(\', \')\n    assert len(conversation) > 1\n    for i in range(len(conversation)-1):\n        con_a = utterance_dict[conversation[i+1]].strip()\n        con_b = utterance_dict[conversation[i]].strip()\n        if len(con_a.split()) <= 22 and len(con_b.split()) <= 22:\n            con_a = [refine(w) for w in con_a.lower().split()]\n            # con_a = [word_vector[w] if w in word_vector else np.zeros(WORD_VECTOR_SIZE) for w in con_a]\n            conversations.append((con_a, con_b))\n            traindata_count += 1\n    con_count += 1\n    if con_count % 1000 == 0:\n        print(\'con_count {}, traindata_count {}\'.format(con_count, traindata_count))\npickle.dump(conversations, open(\'data/reversed_conversations_lenmax22\', \'wb\'), True)\nprint(""Time Elapsed: {} secs\\n"".format(time.time() - ts))\n\n# some statistics of training data\nmax_a = -1\nmax_b = -1\nmax_a_ind = -1\nmax_b_ind = -1\nsum_a = 0.\nsum_b = 0.\nlen_a_list = []\nlen_b_list = []\nfor i in range(len(conversations)):\n    len_a = len(conversations[i][0])\n    len_b = len(conversations[i][1].split())\n    if len_a > max_a:\n        max_a = len_a\n        max_a_ind = i\n    if len_b > max_b:\n        max_b = len_b\n        max_b_ind = i\n    sum_a += len_a\n    sum_b += len_b\n    len_a_list.append(len_a)\n    len_b_list.append(len_b)\nnp.save(""data/reversed_lenmax22_a_list"", np.array(len_a_list))\nnp.save(""data/reversed_lenmax22_b_list"", np.array(len_b_list))\nprint(""max_a_ind {}, max_b_ind {}"".format(max_a_ind, max_b_ind))\nprint(""max_a {}, max_b {}, avg_a {}, avg_b {}"".format(max_a, max_b, sum_a/len(conversations), sum_b/len(conversations)))\n\nts = time.time()\nconversations = []\n# former_sents = []\nprint(\'len conversation\', len(raw_movie_conversations))\ncon_count = 0\ntraindata_count = 0\nfor conversation in raw_movie_conversations:\n    conversation = conversation.split(\' +++$+++ \')[-1]\n    conversation = conversation.replace(\'[\', \'\')\n    conversation = conversation.replace(\']\', \'\')\n    conversation = conversation.replace(\'\\\'\', \'\')\n    conversation = conversation.split(\', \')\n    assert len(conversation) > 1\n    con_a_1 = \'\'\n    for i in range(len(conversation)-1):\n        con_a_2 = utterance_dict[conversation[i]]\n        con_b = utterance_dict[conversation[i+1]]\n        if len(con_a_1.split()) <= 22 and len(con_a_2.split()) <= 22 and len(con_b.split()) <= 22:\n            con_a = ""{} {}"".format(con_a_1, con_a_2)\n            con_a = [refine(w) for w in con_a.lower().split()]\n            # con_a = [word_vector[w] if w in word_vector else np.zeros(WORD_VECTOR_SIZE) for w in con_a]\n            conversations.append((con_a, con_b, con_a_2))\n            # former_sents.append(con_a_2)\n            traindata_count += 1\n        con_a_1 = con_a_2\n    con_count += 1\n    if con_count % 1000 == 0:\n        print(\'con_count {}, traindata_count {}\'.format(con_count, traindata_count))\npickle.dump(conversations, open(\'data/conversations_lenmax22_formersents2_with_former\', \'wb\'), True)\n# pickle.dump(former_sents, open(\'data/conversations_lenmax22_former_sents\', \'wb\'), True)\nprint(""Time Elapsed: {} secs\\n"".format(time.time() - ts))\n\nts = time.time()\nconversations = []\n# former_sents = []\nprint(\'len conversation\', len(raw_movie_conversations))\ncon_count = 0\ntraindata_count = 0\nfor conversation in raw_movie_conversations:\n    conversation = conversation.split(\' +++$+++ \')[-1]\n    conversation = conversation.replace(\'[\', \'\')\n    conversation = conversation.replace(\']\', \'\')\n    conversation = conversation.replace(\'\\\'\', \'\')\n    conversation = conversation.split(\', \')\n    assert len(conversation) > 1\n    con_a_1 = \'\'\n    for i in range(len(conversation)-1):\n        con_a_2 = utterance_dict[conversation[i]]\n        con_b = utterance_dict[conversation[i+1]]\n        if len(con_a_1.split()) <= 22 and len(con_a_2.split()) <= 22 and len(con_b.split()) <= 22:\n            con_a = ""{} {}"".format(con_a_1, con_a_2)\n            con_a = [refine(w) for w in con_a.lower().split()]\n            # con_a = [word_vector[w] if w in word_vector else np.zeros(WORD_VECTOR_SIZE) for w in con_a]\n            conversations.append((con_a, con_b))\n            # former_sents.append(con_a_2)\n            traindata_count += 1\n        con_a_1 = con_a_2\n    con_count += 1\n    if con_count % 1000 == 0:\n        print(\'con_count {}, traindata_count {}\'.format(con_count, traindata_count))\npickle.dump(conversations, open(\'data/conversations_lenmax22_former_sents2\', \'wb\'), True)\nprint(""Time Elapsed: {} secs\\n"".format(time.time() - ts))\n\nts = time.time()\nconversations = []\nprint(\'len conversation\', len(raw_movie_conversations))\ncon_count = 0\ntraindata_count = 0\nfor conversation in raw_movie_conversations:\n    conversation = conversation.split(\' +++$+++ \')[-1]\n    conversation = conversation.replace(\'[\', \'\')\n    conversation = conversation.replace(\']\', \'\')\n    conversation = conversation.replace(\'\\\'\', \'\')\n    conversation = conversation.split(\', \')\n    assert len(conversation) > 1\n    for i in range(len(conversation)-1):\n        con_a = utterance_dict[conversation[i]]\n        con_b = utterance_dict[conversation[i+1]]\n        if len(con_a.split()) <= 22 and len(con_b.split()) <= 22:\n            con_a = [refine(w) for w in con_a.lower().split()]\n            # con_a = [word_vector[w] if w in word_vector else np.zeros(WORD_VECTOR_SIZE) for w in con_a]\n            conversations.append((con_a, con_b))\n            traindata_count += 1\n    con_count += 1\n    if con_count % 1000 == 0:\n        print(\'con_count {}, traindata_count {}\'.format(con_count, traindata_count))\npickle.dump(conversations, open(\'data/conversations_lenmax22\', \'wb\'), True)\nprint(""Time Elapsed: {} secs\\n"".format(time.time() - ts))\n'"
python/model.py,59,"b'# coding=utf-8\n\nimport tensorflow as tf\nimport numpy as np\n\nclass Seq2Seq_chatbot():\n    def __init__(self, dim_wordvec, n_words, dim_hidden, batch_size, n_encode_lstm_step, n_decode_lstm_step, bias_init_vector=None, lr=0.0001):\n        self.dim_wordvec = dim_wordvec\n        self.dim_hidden = dim_hidden\n        self.batch_size = batch_size\n        self.n_words = n_words\n        self.n_encode_lstm_step = n_encode_lstm_step\n        self.n_decode_lstm_step = n_decode_lstm_step\n        self.lr = lr\n\n        with tf.device(""/cpu:0""):\n            self.Wemb = tf.Variable(tf.random_uniform([n_words, dim_hidden], -0.1, 0.1), name=\'Wemb\')\n\n        self.lstm1 = tf.contrib.rnn.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n        self.lstm2 = tf.contrib.rnn.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n\n        self.encode_vector_W = tf.Variable(tf.random_uniform([dim_wordvec, dim_hidden], -0.1, 0.1), name=\'encode_vector_W\')\n        self.encode_vector_b = tf.Variable(tf.zeros([dim_hidden]), name=\'encode_vector_b\')\n\n        self.embed_word_W = tf.Variable(tf.random_uniform([dim_hidden, n_words], -0.1, 0.1), name=\'embed_word_W\')\n        if bias_init_vector is not None:\n            self.embed_word_b = tf.Variable(bias_init_vector.astype(np.float32), name=\'embed_word_b\')\n        else:\n            self.embed_word_b = tf.Variable(tf.zeros([n_words]), name=\'embed_word_b\')\n\n    def build_model(self):\n        word_vectors = tf.placeholder(tf.float32, [self.batch_size, self.n_encode_lstm_step, self.dim_wordvec])\n\n        caption = tf.placeholder(tf.int32, [self.batch_size, self.n_decode_lstm_step+1])\n        caption_mask = tf.placeholder(tf.float32, [self.batch_size, self.n_decode_lstm_step+1])\n\n        word_vectors_flat = tf.reshape(word_vectors, [-1, self.dim_wordvec])\n        wordvec_emb = tf.nn.xw_plus_b(word_vectors_flat, self.encode_vector_W, self.encode_vector_b ) # (batch_size*n_encode_lstm_step, dim_hidden)\n        wordvec_emb = tf.reshape(wordvec_emb, [self.batch_size, self.n_encode_lstm_step, self.dim_hidden])\n\n        state1 = tf.zeros([self.batch_size, self.lstm1.state_size])\n        state2 = tf.zeros([self.batch_size, self.lstm2.state_size])\n        padding = tf.zeros([self.batch_size, self.dim_hidden])\n\n        probs = []\n        entropies = []\n        loss = 0.0\n\n        ##############################  Encoding Stage ##################################\n        for i in range(0, self.n_encode_lstm_step):\n            if i > 0:\n                tf.get_variable_scope().reuse_variables()\n\n            with tf.variable_scope(""LSTM1""):\n                output1, state1 = self.lstm1(wordvec_emb[:, i, :], state1)\n\n            with tf.variable_scope(""LSTM2""):\n                output2, state2 = self.lstm2(tf.concat([padding, output1], 1), state2)\n\n        ############################# Decoding Stage ######################################\n        for i in range(0, self.n_decode_lstm_step):\n            with tf.device(""/cpu:0""):\n                current_embed = tf.nn.embedding_lookup(self.Wemb, caption[:, i])\n\n            tf.get_variable_scope().reuse_variables()\n\n            with tf.variable_scope(""LSTM1""):\n                output1, state1 = self.lstm1(padding, state1)\n\n            with tf.variable_scope(""LSTM2""):\n                output2, state2 = self.lstm2(tf.concat([current_embed, output1], 1), state2)\n\n            labels = tf.expand_dims(caption[:, i+1], 1)\n            indices = tf.expand_dims(tf.range(0, self.batch_size, 1), 1)\n            concated = tf.concat([indices, labels], 1)\n            onehot_labels = tf.sparse_to_dense(concated, tf.stack([self.batch_size, self.n_words]), 1.0, 0.0)\n\n            logit_words = tf.nn.xw_plus_b(output2, self.embed_word_W, self.embed_word_b)\n            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=onehot_labels)\n            cross_entropy = cross_entropy * caption_mask[:, i]\n            entropies.append(cross_entropy)\n            probs.append(logit_words)\n\n            current_loss = tf.reduce_sum(cross_entropy)/self.batch_size\n            loss = loss + current_loss\n\n        with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n            train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n\n        inter_value = {\n            \'probs\': probs,\n            \'entropies\': entropies\n        }\n\n        return train_op, loss, word_vectors, caption, caption_mask, inter_value\n\n    def build_generator(self):\n        word_vectors = tf.placeholder(tf.float32, [1, self.n_encode_lstm_step, self.dim_wordvec])\n\n        word_vectors_flat = tf.reshape(word_vectors, [-1, self.dim_wordvec])\n        wordvec_emb = tf.nn.xw_plus_b(word_vectors_flat, self.encode_vector_W, self.encode_vector_b)\n        wordvec_emb = tf.reshape(wordvec_emb, [1, self.n_encode_lstm_step, self.dim_hidden])\n\n        state1 = tf.zeros([1, self.lstm1.state_size])\n        state2 = tf.zeros([1, self.lstm2.state_size])\n        padding = tf.zeros([1, self.dim_hidden])\n\n        generated_words = []\n\n        probs = []\n        embeds = []\n\n        for i in range(0, self.n_encode_lstm_step):\n            if i > 0:\n                tf.get_variable_scope().reuse_variables()\n\n            with tf.variable_scope(""LSTM1""):\n                output1, state1 = self.lstm1(wordvec_emb[:, i, :], state1)\n\n            with tf.variable_scope(""LSTM2""):\n                output2, state2 = self.lstm2(tf.concat([padding, output1], 1), state2)\n\n        for i in range(0, self.n_decode_lstm_step):\n            tf.get_variable_scope().reuse_variables()\n\n            if i == 0:\n                with tf.device(\'/cpu:0\'):\n                    current_embed = tf.nn.embedding_lookup(self.Wemb, tf.ones([1], dtype=tf.int64))\n\n            with tf.variable_scope(""LSTM1""):\n                output1, state1 = self.lstm1(padding, state1)\n\n            with tf.variable_scope(""LSTM2""):\n                output2, state2 = self.lstm2(tf.concat([current_embed, output1], 1), state2)\n\n            logit_words = tf.nn.xw_plus_b(output2, self.embed_word_W, self.embed_word_b)\n            max_prob_index = tf.argmax(logit_words, 1)[0]\n            generated_words.append(max_prob_index)\n            probs.append(logit_words)\n\n            with tf.device(""/cpu:0""):\n                current_embed = tf.nn.embedding_lookup(self.Wemb, max_prob_index)\n                current_embed = tf.expand_dims(current_embed, 0)\n\n            embeds.append(current_embed)\n\n        return word_vectors, generated_words, probs, embeds\n'"
python/simulate.py,2,"b'#-*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nfrom gensim.models import KeyedVectors\nimport data_parser\nimport config\n\nfrom model import Seq2Seq_chatbot\nimport tensorflow as tf\nimport numpy as np\n\nimport re\nimport os\nimport sys\nimport time\n\n\n#=====================================================\n# Global Parameters\n#=====================================================\ndefault_model_path = \'./model/model-20\'\ndefault_simulate_type = 1  # type 1 use one former sent, type 2 use two former sents\n\ntesting_data_path = \'sample_input.txt\' if len(sys.argv) <= 3 else sys.argv[3]\noutput_path = \'sample_dialog_output.txt\' if len(sys.argv) <= 4 else sys.argv[4]\n\nmax_turns = config.MAX_TURNS\nword_count_threshold = config.WC_threshold\n\n#=====================================================\n# Train Parameters\n#=====================================================\ndim_wordvec = 300\ndim_hidden = 1000\n\nn_encode_lstm_step = 22  # need to plus 1 later, because one random normal as the first timestep\nn_decode_lstm_step = 22\n\nbatch_size = 1\n\n"""""" Extract only the vocabulary part of the data """"""\ndef refine(data):\n    words = re.findall(""[a-zA-Z\'-]+"", data)\n    words = ["""".join(word.split(""\'"")) for word in words]\n    # words = ["""".join(word.split(""-"")) for word in words]\n    data = \' \'.join(words)\n    return data\n\ndef generate_question_vector(state, word_vector, dim_wordvec, n_encode_lstm_step):\n    state = [refine(w) for w in state.lower().split()]\n    state = [word_vector[w] if w in word_vector else np.zeros(dim_wordvec) for w in state]\n    state.insert(0, np.random.normal(size=(dim_wordvec,))) # insert random normal at the first step\n\n    if len(state) > n_encode_lstm_step:\n        state = state[:n_encode_lstm_step]\n    else:\n        for _ in range(len(state), n_encode_lstm_step):\n            state.append(np.zeros(dim_wordvec))\n\n    return np.array([state]) # 1 x n_encode_lstm_step x dim_wordvec\n\ndef generate_answer_sentence(generated_word_index, prob_logit, ixtoword):\n    # remove <unk> to second high prob. word\n    for i in range(len(generated_word_index)):\n        if generated_word_index[i] == 3:\n            sort_prob_logit = sorted(prob_logit[i][0])\n            # print(\'max val\', sort_prob_logit[-1])\n            # print(\'second max val\', sort_prob_logit[-2])\n            maxindex = np.where(prob_logit[i][0] == sort_prob_logit[-1])[0][0]\n            secmaxindex = np.where(prob_logit[i][0] == sort_prob_logit[-2])[0][0]\n            # print(\'max ind\', maxindex, ixtoword[maxindex])\n            # print(\'second max ind\', secmaxindex, ixtoword[secmaxindex])\n            generated_word_index[i] = secmaxindex\n\n    generated_words = []\n    for ind in generated_word_index:\n        generated_words.append(ixtoword[ind])\n\n    # generate sentence\n    punctuation = np.argmax(np.array(generated_words) == \'<eos>\') + 1\n    generated_words = generated_words[:punctuation]\n    generated_sentence = \' \'.join(generated_words)\n\n    # modify the output sentence \n    generated_sentence = generated_sentence.replace(\'<bos> \', \'\')\n    generated_sentence = generated_sentence.replace(\' <eos>\', \'\')\n    generated_sentence = generated_sentence.replace(\'--\', \'\')\n    generated_sentence = generated_sentence.split(\'  \')\n    for i in range(len(generated_sentence)):\n        generated_sentence[i] = generated_sentence[i].strip()\n        if len(generated_sentence[i]) > 1:\n            generated_sentence[i] = generated_sentence[i][0].upper() + generated_sentence[i][1:] + \'.\'\n        else:\n            generated_sentence[i] = generated_sentence[i].upper()\n    generated_sentence = \' \'.join(generated_sentence)\n    generated_sentence = generated_sentence.replace(\' i \', \' I \')\n    generated_sentence = generated_sentence.replace(""i\'m"", ""I\'m"")\n    generated_sentence = generated_sentence.replace(""i\'d"", ""I\'d"")\n\n    return generated_sentence\n\ndef init_history(simulate_type, start_sentence):\n    history = []\n    history += [\'\' for _ in range(simulate_type-1)]\n    history.append(start_sentence)\n    return history\n\ndef get_cur_state(simulate_type, dialog_history):\n    return \' \'.join(dialog_history[-1*simulate_type:]).strip()\n\ndef simulate(model_path=default_model_path, simulate_type=default_simulate_type):\n    \'\'\' args:\n            model_path:     <type \'str\'> the pre-trained model using for inference\n            simulate_type:  <type \'int\'> how many former sents should use as state\n    \'\'\'\n\n    testing_data = open(testing_data_path, \'r\').read().split(\'\\n\')\n\n    word_vector = KeyedVectors.load_word2vec_format(\'model/word_vector.bin\', binary=True)\n\n    _, ixtoword, bias_init_vector = data_parser.preProBuildWordVocab(word_count_threshold=word_count_threshold)\n\n    model = Seq2Seq_chatbot(\n            dim_wordvec=dim_wordvec,\n            n_words=len(ixtoword),\n            dim_hidden=dim_hidden,\n            batch_size=batch_size,\n            n_encode_lstm_step=n_encode_lstm_step,\n            n_decode_lstm_step=n_decode_lstm_step,\n            bias_init_vector=bias_init_vector)\n\n    word_vectors, caption_tf, probs, _ = model.build_generator()\n\n    sess = tf.InteractiveSession()\n\n    saver = tf.train.Saver()\n    try:\n        print(\'\\n=== Use model {} ===\\n\'.format(model_path))\n        saver.restore(sess, model_path)\n    except:\n        print(\'\\nUse default model\\n\')\n        saver.restore(sess, default_model_path)\n\n    with open(output_path, \'w\') as out:\n        for idx, start_sentence in enumerate(testing_data):\n            print(\'dialog {}\'.format(idx))\n            print(\'A => {}\'.format(start_sentence))\n            out.write(\'dialog {}\\nA: {}\\n\'.format(idx, start_sentence))\n\n            dialog_history = init_history(simulate_type, start_sentence)\n\n            for turn in range(max_turns):\n                question = generate_question_vector(state=get_cur_state(simulate_type, dialog_history), \n                                                    word_vector=word_vector, \n                                                    dim_wordvec=dim_wordvec, \n                                                    n_encode_lstm_step=n_encode_lstm_step)\n\n                generated_word_index, prob_logit = sess.run([caption_tf, probs], feed_dict={word_vectors: question})\n\n                generated_sentence = generate_answer_sentence(generated_word_index=generated_word_index, \n                                                              prob_logit=prob_logit, \n                                                              ixtoword=ixtoword)\n\n                dialog_history.append(generated_sentence)\n                print(\'B => {}\'.format(generated_sentence))\n\n                question_2 = generate_question_vector(state=get_cur_state(simulate_type, dialog_history), \n                                                    word_vector=word_vector, \n                                                    dim_wordvec=dim_wordvec, \n                                                    n_encode_lstm_step=n_encode_lstm_step)\n\n                generated_word_index, prob_logit = sess.run([caption_tf, probs], feed_dict={word_vectors: question_2})\n\n                generated_sentence_2 = generate_answer_sentence(generated_word_index=generated_word_index, \n                                                                  prob_logit=prob_logit, \n                                                                  ixtoword=ixtoword)\n\n                dialog_history.append(generated_sentence_2)\n                print(\'A => {}\'.format(generated_sentence_2))\n                out.write(\'B: {}\\nA: {}\\n\'.format(generated_sentence, generated_sentence_2))\n\n\nif __name__ == ""__main__"":\n    model_path = default_model_path if len(sys.argv) <= 1 else sys.argv[1]\n    simulate_type = default_simulate_type if len(sys.argv) <= 2 else int(sys.argv[2])\n    n_encode_lstm_step = n_encode_lstm_step * simulate_type + 1  # sent len * sent num + one random normal\n    print(\'simulate_type\', simulate_type)\n    print(\'n_encode_lstm_step\', n_encode_lstm_step)\n    simulate(model_path=model_path, simulate_type=simulate_type)\n'"
python/test.py,2,"b'#-*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nfrom gensim.models import KeyedVectors\nimport data_parser\nimport config\n\nfrom model import Seq2Seq_chatbot\nimport tensorflow as tf\nimport numpy as np\n\nimport re\nimport os\nimport sys\nimport time\n\n#=====================================================\n# Global Parameters\n#=====================================================\ndefault_model_path = \'./model/Seq2Seq/model-77\'\ntesting_data_path = \'sample_input.txt\' if len(sys.argv) <= 2 else sys.argv[2]\noutput_path = \'sample_output_S2S.txt\' if len(sys.argv) <= 3 else sys.argv[3]\n\nword_count_threshold = config.WC_threshold\n\n#=====================================================\n# Train Parameters\n#=====================================================\ndim_wordvec = 300\ndim_hidden = 1000\n\nn_encode_lstm_step = 22 + 1 # one random normal as the first timestep\nn_decode_lstm_step = 22\n\nbatch_size = 1\n\n"""""" Extract only the vocabulary part of the data """"""\ndef refine(data):\n    words = re.findall(""[a-zA-Z\'-]+"", data)\n    words = ["""".join(word.split(""\'"")) for word in words]\n    # words = ["""".join(word.split(""-"")) for word in words]\n    data = \' \'.join(words)\n    return data\n\ndef test(model_path=default_model_path):\n    testing_data = open(testing_data_path, \'r\').read().split(\'\\n\')\n\n    word_vector = KeyedVectors.load_word2vec_format(\'model/word_vector.bin\', binary=True)\n\n    _, ixtoword, bias_init_vector = data_parser.preProBuildWordVocab(word_count_threshold=word_count_threshold)\n\n    model = Seq2Seq_chatbot(\n            dim_wordvec=dim_wordvec,\n            n_words=len(ixtoword),\n            dim_hidden=dim_hidden,\n            batch_size=batch_size,\n            n_encode_lstm_step=n_encode_lstm_step,\n            n_decode_lstm_step=n_decode_lstm_step,\n            bias_init_vector=bias_init_vector)\n\n    word_vectors, caption_tf, probs, _ = model.build_generator()\n\n    sess = tf.InteractiveSession()\n\n    saver = tf.train.Saver()\n    try:\n        print(\'\\n=== Use model\', model_path, \'===\\n\')\n        saver.restore(sess, model_path)\n    except:\n        print(\'\\nUse default model\\n\')\n        saver.restore(sess, default_model_path)\n\n    with open(output_path, \'w\') as out:\n        generated_sentences = []\n        bleu_score_avg = [0., 0.]\n        for idx, question in enumerate(testing_data):\n            print(\'question =>\', question)\n\n            question = [refine(w) for w in question.lower().split()]\n            question = [word_vector[w] if w in word_vector else np.zeros(dim_wordvec) for w in question]\n            question.insert(0, np.random.normal(size=(dim_wordvec,))) # insert random normal at the first step\n\n            if len(question) > n_encode_lstm_step:\n                question = question[:n_encode_lstm_step]\n            else:\n                for _ in range(len(question), n_encode_lstm_step):\n                    question.append(np.zeros(dim_wordvec))\n\n            question = np.array([question]) # 1x22x300\n    \n            generated_word_index, prob_logit = sess.run([caption_tf, probs], feed_dict={word_vectors: question})\n            \n            # remove <unk> to second high prob. word\n            for i in range(len(generated_word_index)):\n                if generated_word_index[i] == 3:\n                    sort_prob_logit = sorted(prob_logit[i][0])\n                    maxindex = np.where(prob_logit[i][0] == sort_prob_logit[-1])[0][0]\n                    secmaxindex = np.where(prob_logit[i][0] == sort_prob_logit[-2])[0][0]\n                    generated_word_index[i] = secmaxindex\n\n            generated_words = []\n            for ind in generated_word_index:\n                generated_words.append(ixtoword[ind])\n\n            # generate sentence\n            punctuation = np.argmax(np.array(generated_words) == \'<eos>\') + 1\n            generated_words = generated_words[:punctuation]\n            generated_sentence = \' \'.join(generated_words)\n\n            # modify the output sentence \n            generated_sentence = generated_sentence.replace(\'<bos> \', \'\')\n            generated_sentence = generated_sentence.replace(\' <eos>\', \'\')\n            generated_sentence = generated_sentence.replace(\'--\', \'\')\n            generated_sentence = generated_sentence.split(\'  \')\n            for i in range(len(generated_sentence)):\n                generated_sentence[i] = generated_sentence[i].strip()\n                if len(generated_sentence[i]) > 1:\n                    generated_sentence[i] = generated_sentence[i][0].upper() + generated_sentence[i][1:] + \'.\'\n                else:\n                    generated_sentence[i] = generated_sentence[i].upper()\n            generated_sentence = \' \'.join(generated_sentence)\n            generated_sentence = generated_sentence.replace(\' i \', \' I \')\n            generated_sentence = generated_sentence.replace(""i\'m"", ""I\'m"")\n            generated_sentence = generated_sentence.replace(""i\'d"", ""I\'d"")\n            generated_sentence = generated_sentence.replace(""i\'ll"", ""I\'ll"")\n            generated_sentence = generated_sentence.replace(""i\'v"", ""I\'v"")\n            generated_sentence = generated_sentence.replace("" - "", """")\n\n            print(\'generated_sentence =>\', generated_sentence)\n            out.write(generated_sentence + \'\\n\')\n\n\nif __name__ == ""__main__"":\n    if len(sys.argv) > 1:\n        test(model_path=sys.argv[1])\n    else:\n        test()\n'"
python/train.py,3,"b'#-*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nfrom gensim.models import KeyedVectors\nfrom data_reader import Data_Reader\nimport data_parser\nimport config\n\nfrom model import Seq2Seq_chatbot\nimport tensorflow as tf\nimport numpy as np\n\nimport os\nimport time\n\n\n### Global Parameters ###\ncheckpoint = config.CHECKPOINT\nmodel_path = config.train_model_path\nmodel_name = config.train_model_name\nstart_epoch = config.start_epoch\n\nword_count_threshold = config.WC_threshold\n\n### Train Parameters ###\ndim_wordvec = 300\ndim_hidden = 1000\n\nn_encode_lstm_step = 22 + 22\nn_decode_lstm_step = 22\n\nepochs = 500\nbatch_size = 100\nlearning_rate = 0.0001\n\n\ndef pad_sequences(sequences, maxlen=None, dtype=\'int32\', padding=\'pre\', truncating=\'pre\', value=0.):\n    if not hasattr(sequences, \'__len__\'):\n        raise ValueError(\'`sequences` must be iterable.\')\n    lengths = []\n    for x in sequences:\n        if not hasattr(x, \'__len__\'):\n            raise ValueError(\'`sequences` must be a list of iterables. \'\n                             \'Found non-iterable: \' + str(x))\n        lengths.append(len(x))\n\n    num_samples = len(sequences)\n    if maxlen is None:\n        maxlen = np.max(lengths)\n\n    # take the sample shape from the first non empty sequence\n    # checking for consistency in the main loop below.\n    sample_shape = tuple()\n    for s in sequences:\n        if len(s) > 0:\n            sample_shape = np.asarray(s).shape[1:]\n            break\n\n    x = (np.ones((num_samples, maxlen) + sample_shape) * value).astype(dtype)\n    for idx, s in enumerate(sequences):\n        if not len(s):\n            continue  # empty list/array was found\n        if truncating == \'pre\':\n            trunc = s[-maxlen:]\n        elif truncating == \'post\':\n            trunc = s[:maxlen]\n        else:\n            raise ValueError(\'Truncating type ""%s"" not understood\' % truncating)\n\n        # check `trunc` has expected shape\n        trunc = np.asarray(trunc, dtype=dtype)\n        if trunc.shape[1:] != sample_shape:\n            raise ValueError(\'Shape of sample %s of sequence at position %s is different from expected shape %s\' %\n                             (trunc.shape[1:], idx, sample_shape))\n\n        if padding == \'post\':\n            x[idx, :len(trunc)] = trunc\n        elif padding == \'pre\':\n            x[idx, -len(trunc):] = trunc\n        else:\n            raise ValueError(\'Padding type ""%s"" not understood\' % padding)\n    return x\n\ndef train():\n    wordtoix, ixtoword, bias_init_vector = data_parser.preProBuildWordVocab(word_count_threshold=word_count_threshold)\n    word_vector = KeyedVectors.load_word2vec_format(\'model/word_vector.bin\', binary=True)\n\n    model = Seq2Seq_chatbot(\n            dim_wordvec=dim_wordvec,\n            n_words=len(wordtoix),\n            dim_hidden=dim_hidden,\n            batch_size=batch_size,\n            n_encode_lstm_step=n_encode_lstm_step,\n            n_decode_lstm_step=n_decode_lstm_step,\n            bias_init_vector=bias_init_vector,\n            lr=learning_rate)\n\n    train_op, tf_loss, word_vectors, tf_caption, tf_caption_mask, inter_value = model.build_model()\n\n    saver = tf.train.Saver(max_to_keep=100)\n\n    sess = tf.InteractiveSession()\n    \n    if checkpoint:\n        print(""Use Model {}."".format(model_name))\n        saver.restore(sess, os.path.join(model_path, model_name))\n        print(""Model {} restored."".format(model_name))\n    else:\n        print(""Restart training..."")\n        tf.global_variables_initializer().run()\n\n    dr = Data_Reader()\n\n    for epoch in range(start_epoch, epochs):\n        n_batch = dr.get_batch_num(batch_size)\n        for batch in range(n_batch):\n            start_time = time.time()\n\n            batch_X, batch_Y = dr.generate_training_batch(batch_size)\n\n            for i in range(len(batch_X)):\n                batch_X[i] = [word_vector[w] if w in word_vector else np.zeros(dim_wordvec) for w in batch_X[i]]\n                # batch_X[i].insert(0, np.random.normal(size=(dim_wordvec,))) # insert random normal at the first step\n                if len(batch_X[i]) > n_encode_lstm_step:\n                    batch_X[i] = batch_X[i][:n_encode_lstm_step]\n                else:\n                    for _ in range(len(batch_X[i]), n_encode_lstm_step):\n                        batch_X[i].append(np.zeros(dim_wordvec))\n\n            current_feats = np.array(batch_X)\n\n            current_captions = batch_Y\n            current_captions = map(lambda x: \'<bos> \' + x, current_captions)\n            current_captions = map(lambda x: x.replace(\'.\', \'\'), current_captions)\n            current_captions = map(lambda x: x.replace(\',\', \'\'), current_captions)\n            current_captions = map(lambda x: x.replace(\'""\', \'\'), current_captions)\n            current_captions = map(lambda x: x.replace(\'\\n\', \'\'), current_captions)\n            current_captions = map(lambda x: x.replace(\'?\', \'\'), current_captions)\n            current_captions = map(lambda x: x.replace(\'!\', \'\'), current_captions)\n            current_captions = map(lambda x: x.replace(\'\\\\\', \'\'), current_captions)\n            current_captions = map(lambda x: x.replace(\'/\', \'\'), current_captions)\n\n            for idx, each_cap in enumerate(current_captions):\n                word = each_cap.lower().split(\' \')\n                if len(word) < n_decode_lstm_step:\n                    current_captions[idx] = current_captions[idx] + \' <eos>\'\n                else:\n                    new_word = \'\'\n                    for i in range(n_decode_lstm_step-1):\n                        new_word = new_word + word[i] + \' \'\n                    current_captions[idx] = new_word + \'<eos>\'\n\n            current_caption_ind = []\n            for cap in current_captions:\n                current_word_ind = []\n                for word in cap.lower().split(\' \'):\n                    if word in wordtoix:\n                        current_word_ind.append(wordtoix[word])\n                    else:\n                        current_word_ind.append(wordtoix[\'<unk>\'])\n                current_caption_ind.append(current_word_ind)\n\n            current_caption_matrix = pad_sequences(current_caption_ind, padding=\'post\', maxlen=n_decode_lstm_step)\n            current_caption_matrix = np.hstack([current_caption_matrix, np.zeros([len(current_caption_matrix), 1])]).astype(int)\n            current_caption_masks = np.zeros((current_caption_matrix.shape[0], current_caption_matrix.shape[1]))\n            nonzeros = np.array(map(lambda x: (x != 0).sum() + 1, current_caption_matrix))\n\n            for ind, row in enumerate(current_caption_masks):\n                row[:nonzeros[ind]] = 1\n\n            if batch % 100 == 0:\n                _, loss_val = sess.run(\n                        [train_op, tf_loss],\n                        feed_dict={\n                            word_vectors: current_feats,\n                            tf_caption: current_caption_matrix,\n                            tf_caption_mask: current_caption_masks\n                        })\n                print(""Epoch: {}, batch: {}, loss: {}, Elapsed time: {}"".format(epoch, batch, loss_val, time.time() - start_time))\n            else:\n                _ = sess.run(train_op,\n                             feed_dict={\n                                word_vectors: current_feats,\n                                tf_caption: current_caption_matrix,\n                                tf_caption_mask: current_caption_masks\n                            })\n\n\n        print(""Epoch "", epoch, "" is done. Saving the model ..."")\n        saver.save(sess, os.path.join(model_path, \'model\'), global_step=epoch)\n\nif __name__ == ""__main__"":\n    train()\n'"
python/RL/rl_model.py,59,"b'# coding=utf-8\n\nimport tensorflow as tf\nimport numpy as np\n\nclass PolicyGradient_chatbot():\n    def __init__(self, dim_wordvec, n_words, dim_hidden, batch_size, n_encode_lstm_step, n_decode_lstm_step, bias_init_vector=None, lr=0.0001):\n        self.dim_wordvec = dim_wordvec\n        self.dim_hidden = dim_hidden\n        self.batch_size = batch_size\n        self.n_words = n_words\n        self.n_encode_lstm_step = n_encode_lstm_step\n        self.n_decode_lstm_step = n_decode_lstm_step\n        self.lr = lr\n\n        with tf.device(""/cpu:0""):\n            self.Wemb = tf.Variable(tf.random_uniform([n_words, dim_hidden], -0.1, 0.1), name=\'Wemb\')\n\n        self.lstm1 = tf.contrib.rnn.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n        self.lstm2 = tf.contrib.rnn.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n\n        self.encode_vector_W = tf.Variable(tf.random_uniform([dim_wordvec, dim_hidden], -0.1, 0.1), name=\'encode_vector_W\')\n        self.encode_vector_b = tf.Variable(tf.zeros([dim_hidden]), name=\'encode_vector_b\')\n\n        self.embed_word_W = tf.Variable(tf.random_uniform([dim_hidden, n_words], -0.1, 0.1), name=\'embed_word_W\')\n        if bias_init_vector is not None:\n            self.embed_word_b = tf.Variable(bias_init_vector.astype(np.float32), name=\'embed_word_b\')\n        else:\n            self.embed_word_b = tf.Variable(tf.zeros([n_words]), name=\'embed_word_b\')\n\n    def build_model(self):\n        word_vectors = tf.placeholder(tf.float32, [self.batch_size, self.n_encode_lstm_step, self.dim_wordvec])\n\n        caption = tf.placeholder(tf.int32, [self.batch_size, self.n_decode_lstm_step+1])\n        caption_mask = tf.placeholder(tf.float32, [self.batch_size, self.n_decode_lstm_step+1])\n\n        word_vectors_flat = tf.reshape(word_vectors, [-1, self.dim_wordvec])\n        wordvec_emb = tf.nn.xw_plus_b(word_vectors_flat, self.encode_vector_W, self.encode_vector_b ) # (batch_size*n_encode_lstm_step, dim_hidden)\n        wordvec_emb = tf.reshape(wordvec_emb, [self.batch_size, self.n_encode_lstm_step, self.dim_hidden])\n\n        reward = tf.placeholder(tf.float32, [self.batch_size, self.n_decode_lstm_step])\n\n        state1 = tf.zeros([self.batch_size, self.lstm1.state_size])\n        state2 = tf.zeros([self.batch_size, self.lstm2.state_size])\n        padding = tf.zeros([self.batch_size, self.dim_hidden])\n\n        entropies = []\n        loss = 0.\n        pg_loss = 0.  # policy gradient loss\n\n        ##############################  Encoding Stage ##################################\n        for i in range(0, self.n_encode_lstm_step):\n            if i > 0:\n                tf.get_variable_scope().reuse_variables()\n\n            with tf.variable_scope(""LSTM1""):\n                output1, state1 = self.lstm1(wordvec_emb[:, i, :], state1)\n                # states.append(state1)\n\n            with tf.variable_scope(""LSTM2""):\n                output2, state2 = self.lstm2(tf.concat([padding, output1], 1), state2)\n\n        ############################# Decoding Stage ######################################\n        for i in range(0, self.n_decode_lstm_step):\n            with tf.device(""/cpu:0""):\n                current_embed = tf.nn.embedding_lookup(self.Wemb, caption[:, i])\n\n            tf.get_variable_scope().reuse_variables()\n\n            with tf.variable_scope(""LSTM1""):\n                output1, state1 = self.lstm1(padding, state1)\n\n            with tf.variable_scope(""LSTM2""):\n                output2, state2 = self.lstm2(tf.concat([current_embed, output1], 1), state2)\n\n            labels = tf.expand_dims(caption[:, i+1], 1)\n            indices = tf.expand_dims(tf.range(0, self.batch_size, 1), 1)\n            concated = tf.concat([indices, labels], 1)\n            onehot_labels = tf.sparse_to_dense(concated, tf.stack([self.batch_size, self.n_words]), 1.0, 0.0)\n\n            logit_words = tf.nn.xw_plus_b(output2, self.embed_word_W, self.embed_word_b)\n\n            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=onehot_labels)\n            cross_entropy = cross_entropy * caption_mask[:, i]\n            entropies.append(cross_entropy)\n            pg_cross_entropy = cross_entropy * reward[:, i]\n\n            pg_current_loss = tf.reduce_sum(pg_cross_entropy) / self.batch_size\n            pg_loss = pg_loss + pg_current_loss\n\n        with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n            train_op = tf.train.AdamOptimizer(self.lr).minimize(pg_loss)\n\n        input_tensors = {\n            \'word_vectors\': word_vectors,\n            \'caption\': caption,\n            \'caption_mask\': caption_mask,\n            \'reward\': reward\n        }\n\n        feats = {\n            \'entropies\': entropies\n        }\n\n        return train_op, pg_loss, input_tensors, feats\n\n    def build_generator(self):\n        word_vectors = tf.placeholder(tf.float32, [self.batch_size, self.n_encode_lstm_step, self.dim_wordvec])\n\n        word_vectors_flat = tf.reshape(word_vectors, [-1, self.dim_wordvec])\n        wordvec_emb = tf.nn.xw_plus_b(word_vectors_flat, self.encode_vector_W, self.encode_vector_b)\n        wordvec_emb = tf.reshape(wordvec_emb, [self.batch_size, self.n_encode_lstm_step, self.dim_hidden])\n\n        state1 = tf.zeros([self.batch_size, self.lstm1.state_size])\n        state2 = tf.zeros([self.batch_size, self.lstm2.state_size])\n        padding = tf.zeros([self.batch_size, self.dim_hidden])\n\n        generated_words = []\n\n        probs = []\n        embeds = []\n        states = []\n\n        for i in range(0, self.n_encode_lstm_step):\n            if i > 0:\n                tf.get_variable_scope().reuse_variables()\n\n            with tf.variable_scope(""LSTM1""):\n                output1, state1 = self.lstm1(wordvec_emb[:, i, :], state1)\n                states.append(state1)\n\n            with tf.variable_scope(""LSTM2""):\n                output2, state2 = self.lstm2(tf.concat([padding, output1], 1), state2)\n\n        for i in range(0, self.n_decode_lstm_step):\n            tf.get_variable_scope().reuse_variables()\n\n            if i == 0:\n                # <bos>\n                with tf.device(\'/cpu:0\'):\n                    current_embed = tf.nn.embedding_lookup(self.Wemb, tf.ones([self.batch_size], dtype=tf.int64))\n\n            with tf.variable_scope(""LSTM1""):\n                output1, state1 = self.lstm1(padding, state1)\n\n            with tf.variable_scope(""LSTM2""):\n                output2, state2 = self.lstm2(tf.concat([current_embed, output1], 1), state2)\n\n            logit_words = tf.nn.xw_plus_b(output2, self.embed_word_W, self.embed_word_b)\n            max_prob_index = tf.argmax(logit_words, 1)\n            generated_words.append(max_prob_index)\n            probs.append(logit_words)\n\n            with tf.device(""/cpu:0""):\n                current_embed = tf.nn.embedding_lookup(self.Wemb, max_prob_index)\n\n            embeds.append(current_embed)\n\n        feats = {\n            \'probs\': probs,\n            \'embeds\': embeds,\n            \'states\': states\n        }\n\n        return word_vectors, generated_words, feats\n'"
python/RL/test.py,2,"b'#-*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nimport re\nimport os\nimport time\nimport sys\n\nsys.path.append(""python"")\nimport data_parser\nimport config\n\nfrom gensim.models import KeyedVectors\nfrom rl_model import PolicyGradient_chatbot\nimport tensorflow as tf\nimport numpy as np\n\n#=====================================================\n# Global Parameters\n#=====================================================\ndefault_model_path = \'./model/RL/model-56-3000\'\ntesting_data_path = \'sample_input.txt\' if len(sys.argv) <= 2 else sys.argv[2]\noutput_path = \'sample_output_RL.txt\' if len(sys.argv) <= 3 else sys.argv[3]\n\nword_count_threshold = config.WC_threshold\n\n#=====================================================\n# Train Parameters\n#=====================================================\ndim_wordvec = 300\ndim_hidden = 1000\n\nn_encode_lstm_step = 22 + 1 # one random normal as the first timestep\nn_decode_lstm_step = 22\n\nbatch_size = 1\n\n"""""" Extract only the vocabulary part of the data """"""\ndef refine(data):\n    words = re.findall(""[a-zA-Z\'-]+"", data)\n    words = ["""".join(word.split(""\'"")) for word in words]\n    # words = ["""".join(word.split(""-"")) for word in words]\n    data = \' \'.join(words)\n    return data\n\ndef test(model_path=default_model_path):\n    testing_data = open(testing_data_path, \'r\').read().split(\'\\n\')\n\n    word_vector = KeyedVectors.load_word2vec_format(\'model/word_vector.bin\', binary=True)\n\n    _, ixtoword, bias_init_vector = data_parser.preProBuildWordVocab(word_count_threshold=word_count_threshold)\n\n    model = PolicyGradient_chatbot(\n            dim_wordvec=dim_wordvec,\n            n_words=len(ixtoword),\n            dim_hidden=dim_hidden,\n            batch_size=batch_size,\n            n_encode_lstm_step=n_encode_lstm_step,\n            n_decode_lstm_step=n_decode_lstm_step,\n            bias_init_vector=bias_init_vector)\n\n    word_vectors, caption_tf, feats = model.build_generator()\n\n    sess = tf.InteractiveSession()\n\n    saver = tf.train.Saver()\n    try:\n        print(\'\\n=== Use model\', model_path, \'===\\n\')\n        saver.restore(sess, model_path)\n    except:\n        print(\'\\nUse default model\\n\')\n        saver.restore(sess, default_model_path)\n\n    with open(output_path, \'w\') as out:\n        generated_sentences = []\n        bleu_score_avg = [0., 0.]\n        for idx, question in enumerate(testing_data):\n            print(\'question =>\', question)\n\n            question = [refine(w) for w in question.lower().split()]\n            question = [word_vector[w] if w in word_vector else np.zeros(dim_wordvec) for w in question]\n            question.insert(0, np.random.normal(size=(dim_wordvec,))) # insert random normal at the first step\n\n            if len(question) > n_encode_lstm_step:\n                question = question[:n_encode_lstm_step]\n            else:\n                for _ in range(len(question), n_encode_lstm_step):\n                    question.append(np.zeros(dim_wordvec))\n\n            question = np.array([question]) # 1x22x300\n    \n            generated_word_index, prob_logit = sess.run([caption_tf, feats[\'probs\']], feed_dict={word_vectors: question})\n            generated_word_index = np.array(generated_word_index).reshape(batch_size, n_decode_lstm_step)[0]\n            prob_logit = np.array(prob_logit).reshape(batch_size, n_decode_lstm_step, -1)[0]\n            # print(\'generated_word_index.shape\', generated_word_index.shape)\n            # print(\'prob_logit.shape\', prob_logit.shape)\n\n            # remove <unk> to second high prob. word\n            # print(\'generated_word_index\', generated_word_index)\n            for i in range(len(generated_word_index)):\n                if generated_word_index[i] == 3:\n                    sort_prob_logit = sorted(prob_logit[i])\n                    # print(\'max val\', sort_prob_logit[-1])\n                    # print(\'second max val\', sort_prob_logit[-2])\n                    maxindex = np.where(prob_logit[i] == sort_prob_logit[-1])[0][0]\n                    secmaxindex = np.where(prob_logit[i] == sort_prob_logit[-2])[0][0]\n                    # print(\'max ind\', maxindex, ixtoword[maxindex])\n                    # print(\'second max ind\', secmaxindex, ixtoword[secmaxindex])\n                    generated_word_index[i] = secmaxindex\n            # print(\'generated_word_index\', generated_word_index)\n\n            generated_words = []\n            for ind in generated_word_index:\n                generated_words.append(ixtoword[ind])\n\n            # generate sentence\n            punctuation = np.argmax(np.array(generated_words) == \'<eos>\') + 1\n            generated_words = generated_words[:punctuation]\n            generated_sentence = \' \'.join(generated_words)\n\n            # modify the output sentence \n            generated_sentence = generated_sentence.replace(\'<bos> \', \'\')\n            generated_sentence = generated_sentence.replace(\' <eos>\', \'\')\n            generated_sentence = generated_sentence.replace(\'--\', \'\')\n            generated_sentence = generated_sentence.split(\'  \')\n            for i in range(len(generated_sentence)):\n                generated_sentence[i] = generated_sentence[i].strip()\n                if len(generated_sentence[i]) > 1:\n                    generated_sentence[i] = generated_sentence[i][0].upper() + generated_sentence[i][1:] + \'.\'\n                else:\n                    generated_sentence[i] = generated_sentence[i].upper()\n            generated_sentence = \' \'.join(generated_sentence)\n            generated_sentence = generated_sentence.replace(\' i \', \' I \')\n            generated_sentence = generated_sentence.replace(""i\'m"", ""I\'m"")\n            generated_sentence = generated_sentence.replace(""i\'d"", ""I\'d"")\n            generated_sentence = generated_sentence.replace(""i\'ll"", ""I\'ll"")\n            generated_sentence = generated_sentence.replace(""i\'v"", ""I\'v"")\n            generated_sentence = generated_sentence.replace("" - "", """")\n\n            print(\'generated_sentence =>\', generated_sentence)\n            out.write(generated_sentence + \'\\n\')\n\n\nif __name__ == ""__main__"":\n    if len(sys.argv) > 1:\n        test(model_path=sys.argv[1])\n    else:\n        test()\n'"
python/RL/train.py,8,"b'#-*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nimport os\nimport time\nimport sys\nimport copy\n\nsys.path.append(""python"")\nfrom model import Seq2Seq_chatbot\nfrom data_reader import Data_Reader\nimport data_parser\nimport config\nimport re\n\nfrom gensim.models import KeyedVectors\nfrom rl_model import PolicyGradient_chatbot\nfrom scipy import spatial\nimport tensorflow as tf\nimport numpy as np\nimport math\n\n\n### Global Parameters ###\ncheckpoint = config.CHECKPOINT\nmodel_path = config.train_model_path\nmodel_name = config.train_model_name\nstart_epoch = config.start_epoch\nstart_batch = config.start_batch\n\n# reversed model\nreversed_model_path = config.reversed_model_path\nreversed_model_name = config.reversed_model_name\n\nword_count_threshold = config.WC_threshold\nr_word_count_threshold = config.reversed_WC_threshold\n\n# dialog simulation turns\nmax_turns = config.MAX_TURNS\n\ndull_set = [""I don\'t know what you\'re talking about."", ""I don\'t know."", ""You don\'t know."", ""You know what I mean."", ""I know what you mean."", ""You know what I\'m saying."", ""You don\'t know anything.""]\n\n### Train Parameters ###\ntraining_type = config.training_type    # \'normal\' for seq2seq training, \'pg\' for policy gradient\n\ndim_wordvec = 300\ndim_hidden = 1000\n\nn_encode_lstm_step = 22 + 22\nn_decode_lstm_step = 22\n\nr_n_encode_lstm_step = 22\nr_n_decode_lstm_step = 22\n\nlearning_rate = 0.0001\nepochs = 500\nbatch_size = config.batch_size\nreversed_batch_size = config.batch_size\n\ndef pad_sequences(sequences, maxlen=None, dtype=\'int32\', padding=\'pre\', truncating=\'pre\', value=0.):\n    if not hasattr(sequences, \'__len__\'):\n        raise ValueError(\'`sequences` must be iterable.\')\n    lengths = []\n    for x in sequences:\n        if not hasattr(x, \'__len__\'):\n            raise ValueError(\'`sequences` must be a list of iterables. \'\n                             \'Found non-iterable: \' + str(x))\n        lengths.append(len(x))\n\n    num_samples = len(sequences)\n    if maxlen is None:\n        maxlen = np.max(lengths)\n\n    # take the sample shape from the first non empty sequence\n    # checking for consistency in the main loop below.\n    sample_shape = tuple()\n    for s in sequences:\n        if len(s) > 0:\n            sample_shape = np.asarray(s).shape[1:]\n            break\n\n    x = (np.ones((num_samples, maxlen) + sample_shape) * value).astype(dtype)\n    for idx, s in enumerate(sequences):\n        if not len(s):\n            continue  # empty list/array was found\n        if truncating == \'pre\':\n            trunc = s[-maxlen:]\n        elif truncating == \'post\':\n            trunc = s[:maxlen]\n        else:\n            raise ValueError(\'Truncating type ""%s"" not understood\' % truncating)\n\n        # check `trunc` has expected shape\n        trunc = np.asarray(trunc, dtype=dtype)\n        if trunc.shape[1:] != sample_shape:\n            raise ValueError(\'Shape of sample %s of sequence at position %s is different from expected shape %s\' %\n                             (trunc.shape[1:], idx, sample_shape))\n\n        if padding == \'post\':\n            x[idx, :len(trunc)] = trunc\n        elif padding == \'pre\':\n            x[idx, -len(trunc):] = trunc\n        else:\n            raise ValueError(\'Padding type ""%s"" not understood\' % padding)\n    return x\n\n"""""" Extract only the vocabulary part of the data """"""\ndef refine(data):\n    words = re.findall(""[a-zA-Z\'-]+"", data)\n    words = ["""".join(word.split(""\'"")) for word in words]\n    # words = ["""".join(word.split(""-"")) for word in words]\n    data = \' \'.join(words)\n    return data\n\ndef make_batch_X(batch_X, n_encode_lstm_step, dim_wordvec, word_vector, noise=False):\n    for i in range(len(batch_X)):\n        batch_X[i] = [word_vector[w] if w in word_vector else np.zeros(dim_wordvec) for w in batch_X[i]]\n        if noise:\n            batch_X[i].insert(0, np.random.normal(size=(dim_wordvec,))) # insert random normal at the first step\n\n        if len(batch_X[i]) > n_encode_lstm_step:\n            batch_X[i] = batch_X[i][:n_encode_lstm_step]\n        else:\n            for _ in range(len(batch_X[i]), n_encode_lstm_step):\n                batch_X[i].append(np.zeros(dim_wordvec))\n\n    current_feats = np.array(batch_X)\n    return current_feats\n\ndef make_batch_Y(batch_Y, wordtoix, n_decode_lstm_step):\n    current_captions = batch_Y\n    current_captions = map(lambda x: \'<bos> \' + x, current_captions)\n    current_captions = map(lambda x: x.replace(\'.\', \'\'), current_captions)\n    current_captions = map(lambda x: x.replace(\',\', \'\'), current_captions)\n    current_captions = map(lambda x: x.replace(\'""\', \'\'), current_captions)\n    current_captions = map(lambda x: x.replace(\'\\n\', \'\'), current_captions)\n    current_captions = map(lambda x: x.replace(\'?\', \'\'), current_captions)\n    current_captions = map(lambda x: x.replace(\'!\', \'\'), current_captions)\n    current_captions = map(lambda x: x.replace(\'\\\\\', \'\'), current_captions)\n    current_captions = map(lambda x: x.replace(\'/\', \'\'), current_captions)\n\n    for idx, each_cap in enumerate(current_captions):\n        word = each_cap.lower().split(\' \')\n        if len(word) < n_decode_lstm_step:\n            current_captions[idx] = current_captions[idx] + \' <eos>\'\n        else:\n            new_word = \'\'\n            for i in range(n_decode_lstm_step-1):\n                new_word = new_word + word[i] + \' \'\n            current_captions[idx] = new_word + \'<eos>\'\n\n    current_caption_ind = []\n    for cap in current_captions:\n        current_word_ind = []\n        for word in cap.lower().split(\' \'):\n            if word in wordtoix:\n                current_word_ind.append(wordtoix[word])\n            else:\n                current_word_ind.append(wordtoix[\'<unk>\'])\n        current_caption_ind.append(current_word_ind)\n\n    current_caption_matrix = pad_sequences(current_caption_ind, padding=\'post\', maxlen=n_decode_lstm_step)\n    current_caption_matrix = np.hstack([current_caption_matrix, np.zeros([len(current_caption_matrix), 1])]).astype(int)\n    current_caption_masks = np.zeros((current_caption_matrix.shape[0], current_caption_matrix.shape[1]))\n    nonzeros = np.array(map(lambda x: (x != 0).sum() + 1, current_caption_matrix))\n\n    for ind, row in enumerate(current_caption_masks):\n        row[:nonzeros[ind]] = 1\n\n    return current_caption_matrix, current_caption_masks\n\ndef index2sentence(generated_word_index, prob_logit, ixtoword):\n    # remove <unk> to second high prob. word\n    for i in range(len(generated_word_index)):\n        if generated_word_index[i] == 3 or generated_word_index[i] <= 1:\n            sort_prob_logit = sorted(prob_logit[i])\n            curindex = np.where(prob_logit[i] == sort_prob_logit[-2])[0][0]\n            count = 1\n            while curindex <= 3:\n                curindex = np.where(prob_logit[i] == sort_prob_logit[(-2)-count])[0][0]\n                count += 1\n\n            generated_word_index[i] = curindex\n\n    generated_words = []\n    for ind in generated_word_index:\n        generated_words.append(ixtoword[ind])\n\n    # generate sentence\n    punctuation = np.argmax(np.array(generated_words) == \'<eos>\') + 1\n    generated_words = generated_words[:punctuation]\n    generated_sentence = \' \'.join(generated_words)\n\n    # modify the output sentence \n    generated_sentence = generated_sentence.replace(\'<bos> \', \'\')\n    generated_sentence = generated_sentence.replace(\'<eos>\', \'\')\n    generated_sentence = generated_sentence.replace(\' <eos>\', \'\')\n    generated_sentence = generated_sentence.replace(\'--\', \'\')\n    generated_sentence = generated_sentence.split(\'  \')\n    for i in range(len(generated_sentence)):\n        generated_sentence[i] = generated_sentence[i].strip()\n        if len(generated_sentence[i]) > 1:\n            generated_sentence[i] = generated_sentence[i][0].upper() + generated_sentence[i][1:] + \'.\'\n        else:\n            generated_sentence[i] = generated_sentence[i].upper()\n    generated_sentence = \' \'.join(generated_sentence)\n    generated_sentence = generated_sentence.replace(\' i \', \' I \')\n    generated_sentence = generated_sentence.replace(""i\'m"", ""I\'m"")\n    generated_sentence = generated_sentence.replace(""i\'d"", ""I\'d"")\n\n    return generated_sentence\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef count_rewards(dull_loss, forward_entropy, backward_entropy, forward_target, backward_target, reward_type=\'pg\'):\n    \'\'\' args:\n            generated_word_indexs:  <type \'numpy.ndarray\'>  \n                                    word indexs generated by pre-trained model\n                                    shape: (batch_size, n_decode_lstm_step)\n            inference_feats:        <type \'dict\'>  \n                                    some features generated during inference\n                                    keys:\n                                        \'probs\': \n                                            shape: (n_decode_lstm_step, batch_size, n_words)\n                                        \'embeds\': \n                                            shape: (n_decode_lstm_step, batch_size, dim_hidden)\n                                            current word embeddings at each decode stage\n                                        \'states\': \n                                            shape: (n_encode_lstm_step, batch_size, dim_hidden)\n                                            LSTM_1\'s hidden state at each encode stage\n    \'\'\'\n\n    # normal training, rewards all equal to 1\n    if reward_type == \'normal\':\n        return np.ones([batch_size, n_decode_lstm_step])\n\n    if reward_type == \'pg\':\n        forward_entropy = np.array(forward_entropy).reshape(batch_size, n_decode_lstm_step)\n        backward_entropy = np.array(backward_entropy).reshape(batch_size, n_decode_lstm_step)\n        total_loss = np.zeros([batch_size, n_decode_lstm_step])\n\n        for i in range(batch_size):\n            # ease of answering\n            total_loss[i, :] += dull_loss[i]\n    \n            # information flow\n            # cosine_sim = 1 - spatial.distance.cosine(embeds[0][-1], embeds[1][-1])\n            # IF = cosine_sim * (-1)\n    \n            # semantic coherence\n            forward_len = len(forward_target[i].split())\n            backward_len = len(backward_target[i].split())\n            if forward_len > 0:\n                total_loss[i, :] += (np.sum(forward_entropy[i]) / forward_len)\n            if backward_len > 0:\n                total_loss[i, :] += (np.sum(backward_entropy[i]) / backward_len)\n\n        total_loss = sigmoid(total_loss) * 1.1\n\n        return total_loss\n\ndef train():\n    global dull_set\n\n    wordtoix, ixtoword, bias_init_vector = data_parser.preProBuildWordVocab(word_count_threshold=word_count_threshold)\n    word_vector = KeyedVectors.load_word2vec_format(\'model/word_vector.bin\', binary=True)\n\n    if len(dull_set) > batch_size:\n        dull_set = dull_set[:batch_size]\n    else:\n        for _ in range(len(dull_set), batch_size):\n            dull_set.append(\'\')\n    dull_matrix, dull_mask = make_batch_Y(\n                                batch_Y=dull_set, \n                                wordtoix=wordtoix, \n                                n_decode_lstm_step=n_decode_lstm_step)\n\n    ones_reward = np.ones([batch_size, n_decode_lstm_step])\n\n    g1 = tf.Graph()\n    g2 = tf.Graph()\n\n    default_graph = tf.get_default_graph() \n\n    with g1.as_default():\n        model = PolicyGradient_chatbot(\n                dim_wordvec=dim_wordvec,\n                n_words=len(wordtoix),\n                dim_hidden=dim_hidden,\n                batch_size=batch_size,\n                n_encode_lstm_step=n_encode_lstm_step,\n                n_decode_lstm_step=n_decode_lstm_step,\n                bias_init_vector=bias_init_vector,\n                lr=learning_rate)\n        train_op, loss, input_tensors, inter_value = model.build_model()\n        tf_states, tf_actions, tf_feats = model.build_generator()\n        sess = tf.InteractiveSession()\n        saver = tf.train.Saver(max_to_keep=100)\n        if checkpoint:\n            print(""Use Model {}."".format(model_name))\n            saver.restore(sess, os.path.join(model_path, model_name))\n            print(""Model {} restored."".format(model_name))\n        else:\n            print(""Restart training..."")\n            tf.global_variables_initializer().run()\n\n    r_wordtoix, r_ixtoword, r_bias_init_vector = data_parser.preProBuildWordVocab(word_count_threshold=r_word_count_threshold)\n    with g2.as_default():\n        reversed_model = Seq2Seq_chatbot(\n            dim_wordvec=dim_wordvec,\n            n_words=len(r_wordtoix),\n            dim_hidden=dim_hidden,\n            batch_size=reversed_batch_size,\n            n_encode_lstm_step=r_n_encode_lstm_step,\n            n_decode_lstm_step=r_n_decode_lstm_step,\n            bias_init_vector=r_bias_init_vector,\n            lr=learning_rate)\n        _, _, word_vectors, caption, caption_mask, reverse_inter = reversed_model.build_model()\n        sess2 = tf.InteractiveSession()\n        saver2 = tf.train.Saver()\n        saver2.restore(sess2, os.path.join(reversed_model_path, reversed_model_name))\n        print(""Reversed model {} restored."".format(reversed_model_name))\n\n\n    dr = Data_Reader(cur_train_index=config.cur_train_index, load_list=config.load_list)\n\n    for epoch in range(start_epoch, epochs):\n        n_batch = dr.get_batch_num(batch_size)\n        sb = start_batch if epoch == start_epoch else 0\n        for batch in range(sb, n_batch):\n            start_time = time.time()\n\n            batch_X, batch_Y, former = dr.generate_training_batch_with_former(batch_size)\n\n            current_feats = make_batch_X(\n                            batch_X=copy.deepcopy(batch_X), \n                            n_encode_lstm_step=n_encode_lstm_step, \n                            dim_wordvec=dim_wordvec,\n                            word_vector=word_vector)\n\n            current_caption_matrix, current_caption_masks = make_batch_Y(\n                                                                batch_Y=copy.deepcopy(batch_Y), \n                                                                wordtoix=wordtoix, \n                                                                n_decode_lstm_step=n_decode_lstm_step)\n\n            if training_type == \'pg\':\n                # action: generate batch_size sents\n                action_word_indexs, inference_feats = sess.run([tf_actions, tf_feats],\n                                                                feed_dict={\n                                                                   tf_states: current_feats\n                                                                })\n                action_word_indexs = np.array(action_word_indexs).reshape(batch_size, n_decode_lstm_step)\n                action_probs = np.array(inference_feats[\'probs\']).reshape(batch_size, n_decode_lstm_step, -1)\n\n                actions = []\n                actions_list = []\n                for i in range(len(action_word_indexs)):\n                    action = index2sentence(\n                                generated_word_index=action_word_indexs[i], \n                                prob_logit=action_probs[i],\n                                ixtoword=ixtoword)\n                    actions.append(action)\n                    actions_list.append(action.split())\n\n                action_feats = make_batch_X(\n                                batch_X=copy.deepcopy(actions_list), \n                                n_encode_lstm_step=n_encode_lstm_step, \n                                dim_wordvec=dim_wordvec,\n                                word_vector=word_vector)\n\n                action_caption_matrix, action_caption_masks = make_batch_Y(\n                                                                batch_Y=copy.deepcopy(actions), \n                                                                wordtoix=wordtoix, \n                                                                n_decode_lstm_step=n_decode_lstm_step)\n\n                # ease of answering\n                dull_loss = []\n                for vector in action_feats:\n                    action_batch_X = np.array([vector for _ in range(batch_size)])\n                    d_loss = sess.run(loss,\n                                 feed_dict={\n                                    input_tensors[\'word_vectors\']: action_batch_X,\n                                    input_tensors[\'caption\']: dull_matrix,\n                                    input_tensors[\'caption_mask\']: dull_mask,\n                                    input_tensors[\'reward\']: ones_reward\n                                })\n                    d_loss = d_loss * -1. / len(dull_set)\n                    dull_loss.append(d_loss)\n\n                # Information Flow\n                pass\n\n                # semantic coherence\n                forward_inter = sess.run(inter_value,\n                                 feed_dict={\n                                    input_tensors[\'word_vectors\']: current_feats,\n                                    input_tensors[\'caption\']: action_caption_matrix,\n                                    input_tensors[\'caption_mask\']: action_caption_masks,\n                                    input_tensors[\'reward\']: ones_reward\n                                })\n                forward_entropies = forward_inter[\'entropies\']\n                former_caption_matrix, former_caption_masks = make_batch_Y(\n                                                                batch_Y=copy.deepcopy(former), \n                                                                wordtoix=wordtoix, \n                                                                n_decode_lstm_step=n_decode_lstm_step)\n                action_feats = make_batch_X(\n                                batch_X=copy.deepcopy(actions_list), \n                                n_encode_lstm_step=r_n_encode_lstm_step, \n                                dim_wordvec=dim_wordvec,\n                                word_vector=word_vector)\n                backward_inter = sess2.run(reverse_inter,\n                                 feed_dict={\n                                    word_vectors: action_feats,\n                                    caption: former_caption_matrix,\n                                    caption_mask: former_caption_masks\n                                })\n                backward_entropies = backward_inter[\'entropies\']\n\n                # reward: count goodness of actions\n                rewards = count_rewards(dull_loss, forward_entropies, backward_entropies, actions, former, reward_type=\'pg\')\n    \n                # policy gradient: train batch with rewards\n                if batch % 10 == 0:\n                    _, loss_val = sess.run(\n                            [train_op, loss],\n                            feed_dict={\n                                input_tensors[\'word_vectors\']: current_feats,\n                                input_tensors[\'caption\']: current_caption_matrix,\n                                input_tensors[\'caption_mask\']: current_caption_masks,\n                                input_tensors[\'reward\']: rewards\n                            })\n                    print(""Epoch: {}, batch: {}, loss: {}, Elapsed time: {}"".format(epoch, batch, loss_val, time.time() - start_time))\n                else:\n                    _ = sess.run(train_op,\n                                 feed_dict={\n                                    input_tensors[\'word_vectors\']: current_feats,\n                                    input_tensors[\'caption\']: current_caption_matrix,\n                                    input_tensors[\'caption_mask\']: current_caption_masks,\n                                    input_tensors[\'reward\']: rewards\n                                })\n                if batch % 1000 == 0 and batch != 0:\n                    print(""Epoch {} batch {} is done. Saving the model ..."".format(epoch, batch))\n                    saver.save(sess, os.path.join(model_path, \'model-{}-{}\'.format(epoch, batch)))\n            if training_type == \'normal\':\n                if batch % 10 == 0:\n                    _, loss_val = sess.run(\n                            [train_op, loss],\n                            feed_dict={\n                                input_tensors[\'word_vectors\']: current_feats,\n                                input_tensors[\'caption\']: current_caption_matrix,\n                                input_tensors[\'caption_mask\']: current_caption_masks,\n                                input_tensors[\'reward\']: ones_reward\n                            })\n                    print(""Epoch: {}, batch: {}, loss: {}, Elapsed time: {}"".format(epoch, batch, loss_val, time.time() - start_time))\n                else:\n                    _ = sess.run(train_op,\n                                 feed_dict={\n                                    input_tensors[\'word_vectors\']: current_feats,\n                                    input_tensors[\'caption\']: current_caption_matrix,\n                                    input_tensors[\'caption_mask\']: current_caption_masks,\n                                    input_tensors[\'reward\']: ones_reward\n                                })\n\n        print(""Epoch "", epoch, "" is done. Saving the model ..."")\n        saver.save(sess, os.path.join(model_path, \'model\'), global_step=epoch)\n\nif __name__ == ""__main__"":\n    train()\n'"
