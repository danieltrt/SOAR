file_path,api_count,code
beam_search.py,1,"b'# -*- coding: utf-8 -*-\n#!/usr/bin/python3\n\'\'\'\ndate: 2019/5/21\nmail: cally.maxiong@gmail.com\npage: http://www.cnblogs.com/callyblog/\n\'\'\'\n\nimport tensorflow as tf\n# add self to decode memory\nclass Hypothesis:\n    """"""\n        Defines a hypothesis during beam search.\n    """"""\n    def __init__(self, tokens, log_prob, sents, normalize_by_length=True):\n        """"""\n        :param tokens: a list, which are ids in vocab\n        :param log_prob: log probability, add by beam search\n        :param sents: already decode words,\n        :param normalize_by_length: sort hypothesis by prob / len, if not, just by prob\n        """"""\n        self.tokens = tokens\n        self.log_prob = log_prob\n        self.normalize_by_length = normalize_by_length\n        self.sents = sents\n\n    def extend(self, token, log_prob, word):\n        """"""\n        Extend the hypothesis with result from latest step.\n        :param token: latest token from decoding\n        :param log_prob: log prob of the latest decoded tokens.\n        :param word: word piece by transformer decode\n        :return: new Hypothesis with the results from latest step.\n        """"""\n\n        return Hypothesis(self.tokens + [token], self.log_prob + log_prob, self.sents + word)\n\n    @property\n    def latest_token(self):\n        return self.tokens[-1]\n\n    def __str__(self):\n        return \'\'.join(list(self.sents))\n\nclass BeamSearch:\n    def __init__(self, model, beam_size, start_token, end_token, id2token, max_steps, input_x, input_y, logits,\n                 normalize_by_length=False):\n        """"""\n        :param model: transformer model\n        :param beam_size: beam size\n        :param start_token: start token\n        :param end_token: end token\n        :param id2token: id to token dict\n        :param max_steps: max steps in decode\n        :param input_x: input x\n        :param input_y: input y\n        :param logits: logits by decode\n        :param normalize_by_length: sort hypothesis by prob / len, if not, just by prob\n        """"""\n        # basic params\n        self.model = model\n        self.beam_size = beam_size\n        self.start_token = start_token\n        self.end_token = end_token\n        self.max_steps = max_steps\n        self.id2token = id2token\n\n        # placeholder\n        self.input_x = input_x\n        self.input_y = input_y\n\n        self.top_k_ = tf.nn.top_k(tf.nn.softmax(logits), k=self.beam_size * 2)\n\n        # This length normalization is only effective for the final results.\n        self.normalize_by_length = normalize_by_length\n\n    def search(self, sess, input_x, memory):\n        """"""\n        use beam search for decoding\n        :param sess: tensorflow session\n        :param input_x: article by list, and convert to id by vocab\n        :param memory: transformer encode result\n        :return: hyps: list of Hypothesis, the best hypotheses found by beam search,\n                       ordered by score\n        """"""\n        # create a list, which each element is Hypothesis\n        hyps = [Hypothesis([self.start_token], 0.0, \'\')] * self.beam_size\n\n        results = []\n        steps = 0\n        while steps < self.max_steps and len(results) < self.beam_size:\n            top_k = sess.run([self.top_k_], feed_dict={self.model.memory: [memory] * self.beam_size,\n                                                       self.input_x: [input_x] * self.beam_size,\n                                                       self.input_y: [h.tokens for h in hyps]})\n            # print(time.time() - start)\n            indices = [list(indice[-1]) for indice in top_k[0][1]]\n            probs = [list(prob[-1]) for prob in top_k[0][0]]\n\n            all_hyps = []\n\n            num_orig_hyps = 1 if steps == 0 else len(hyps)\n            for i in range(num_orig_hyps):\n                h = hyps[i]\n                for j in range(self.beam_size*2):\n                    new_h = h.extend(indices[i][j], probs[i][j], self.id2token[indices[i][j]])\n                    all_hyps.append(new_h)\n\n            # Filter and collect any hypotheses that have the end token\n            hyps = []\n            for h in self.best_hyps(all_hyps):\n                if h.latest_token == self.end_token:\n                    # Pull the hypothesis off the beam if the end token is reached.\n                    results.append(h)\n                else:\n                    # Otherwise continue to the extend the hypothesis.\n                    hyps.append(h)\n                if len(hyps) == self.beam_size or len(results) == self.beam_size:\n                    break\n\n            steps += 1\n\n            if steps == self.max_steps:\n                results.extend(hyps)\n\n        return self.best_hyps(results)\n\n    def best_hyps(self, hyps):\n        """"""\n        Sort the hyps based on log probs and length.\n        :param hyps: A list of hypothesis\n        :return: A list of sorted hypothesis in reverse log_prob order.\n        """"""\n        # This length normalization is only effective for the final results.\n        if self.normalize_by_length:\n            return sorted(hyps, key=lambda h: h.log_prob / len(h.tokens), reverse=True)\n        else:\n            return sorted(hyps, key=lambda h: h.log_prob, reverse=True)'"
data_load.py,3,"b'# -*- coding: utf-8 -*-\n#/usr/bin/python3\n\'\'\'\ndate: 2019/5/21\nmail: cally.maxiong@gmail.com\npage: http://www.cnblogs.com/callyblog/\n\'\'\'\n\nimport tensorflow as tf\nfrom utils import calc_num_batches\n\ndef _load_vocab(vocab_fpath):\n    \'\'\'Loads vocabulary file and returns idx<->token maps\n    vocab_fpath: string. vocabulary file path.\n    Note that these are reserved\n    0: <pad>, 1: <unk>, 2: <s>, 3: </s>\n\n    Returns\n    two dictionaries.\n    \'\'\'\n    vocab = []\n    with open(vocab_fpath, \'r\', encoding=\'utf-8\') as f:\n        for line in f:\n            vocab.append(line.replace(\'\\n\', \'\'))\n    token2idx = {token: idx for idx, token in enumerate(vocab)}\n    idx2token = {idx: token for idx, token in enumerate(vocab)}\n\n    return token2idx, idx2token\n\ndef load_stop(vocab_path):\n    """"""\n    load stop word\n    :param vocab_path: stop word path\n    :return: stop word list\n    """"""\n    stop_words = []\n    with open(vocab_path, \'r\', encoding=\'utf-8\') as f:\n        for line in f:\n            stop_words.append(line.replace(\'\\n\', \'\'))\n\n    return sorted(stop_words, key=lambda i: len(i), reverse=True)\n\ndef _load_data(fpaths, maxlen1, maxlen2):\n    \'\'\'Loads source and target data and filters out too lengthy samples.\n    fpath1: source file path. string.\n    fpath2: target file path. string.\n    maxlen1: source sent maximum length. scalar.\n    maxlen2: target sent maximum length. scalar.\n\n    Returns\n    sents1: list of source sents\n    sents2: list of target sents\n    \'\'\'\n    sents1, sents2 = [], []\n    for fpath in fpaths.split(\'|\'):\n        with open(fpath, \'r\', encoding=\'utf-8\') as f:\n            for line in f:\n                splits = line.split(\',\')\n                if len(splits) != 2: continue\n                sen1 = splits[1].replace(\'\\n\', \'\').strip()\n                sen2 = splits[0].strip()\n                if len(list(sen1)) + 1 > maxlen1-2: continue\n                if len(list(sen2)) + 1 > maxlen2-1: continue\n\n                sents1.append(sen1.encode(\'utf-8\'))\n                sents2.append(sen2.encode(\'utf-8\'))\n\n    return sents1[:400000], sents2[:400000]\n\ndef _encode(inp, token2idx, maxlen, type):\n    \'\'\'Converts string to number. Used for `generator_fn`.\n    inp: 1d byte array.\n    type: ""x"" (source side) or ""y"" (target side)\n    dict: token2idx dictionary\n\n    Returns\n    list of numbers\n    \'\'\'\n    inp = inp.decode(\'utf-8\')\n    if type == \'x\':\n        tokens = [\'<s>\'] + list(inp) + [\'</s>\']\n        while len(tokens) < maxlen:\n            tokens.append(\'<pad>\')\n        return [token2idx.get(token, token2idx[\'<unk>\']) for token in tokens]\n\n    else:\n        inputs = [\'<s>\'] + list(inp)\n        target = list(inp) + [\'</s>\']\n        while len(target) < maxlen:\n            inputs.append(\'<pad>\')\n            target.append(\'<pad>\')\n        return [token2idx.get(token, token2idx[\'<unk>\']) for token in inputs], [token2idx.get(token, token2idx[\'<unk>\']) for token in target]\n\ndef _generator_fn(sents1, sents2, vocab_fpath, maxlen1, maxlen2):\n    \'\'\'Generates training / evaluation data\n    sents1: list of source sents\n    sents2: list of target sents\n    vocab_fpath: string. vocabulary file path.\n\n    yields\n    xs: tuple of\n        x: list of source token ids in a sent\n        x_seqlen: int. sequence length of x\n        sent1: str. raw source (=input) sentence\n    labels: tuple of\n        decoder_input: decoder_input: list of encoded decoder inputs\n        y: list of target token ids in a sent\n        y_seqlen: int. sequence length of y\n        sent2: str. target sentence\n    \'\'\'\n    token2idx, _ = _load_vocab(vocab_fpath)\n    for sent1, sent2 in zip(sents1, sents2):\n        x = _encode(sent1, token2idx, maxlen1, ""x"")\n\n        inputs, targets = _encode(sent2, token2idx, maxlen2, ""y"")\n\n        yield (x, sent1.decode(\'utf-8\')), (inputs, targets, sent2.decode(\'utf-8\'))\n\ndef _input_fn(sents1, sents2, vocab_fpath, batch_size, gpu_nums, maxlen1, maxlen2, shuffle=False):\n    \'\'\'Batchify data\n    sents1: list of source sents\n    sents2: list of target sents\n    vocab_fpath: string. vocabulary file path.\n    batch_size: scalar\n    shuffle: boolean\n\n    Returns\n    xs: tuple of\n        x: int32 tensor. (N, T1)\n        x_seqlens: int32 tensor. (N,)\n        sents1: str tensor. (N,)\n    ys: tuple of\n        decoder_input: int32 tensor. (N, T2)\n        y: int32 tensor. (N, T2)\n        y_seqlen: int32 tensor. (N, )\n        sents2: str tensor. (N,)\n    \'\'\'\n    shapes = (([maxlen1], ()),\n              ([maxlen2], [maxlen2], ()))\n    types = ((tf.int32, tf.string),\n             (tf.int32, tf.int32, tf.string))\n\n    dataset = tf.data.Dataset.from_generator(\n        _generator_fn,\n        output_shapes=shapes,\n        output_types=types,\n        args=(sents1, sents2, vocab_fpath, maxlen1, maxlen2))  # <- arguments for generator_fn. converted to np string arrays\n\n    if shuffle: # for training\n        dataset = dataset.shuffle(128*batch_size*gpu_nums)\n\n    dataset = dataset.repeat()  # iterate forever\n    dataset = dataset.batch(batch_size*gpu_nums)\n\n    return dataset\n\ndef get_batch(fpath, maxlen1, maxlen2, vocab_fpath, batch_size, gpu_nums, shuffle=False):\n    \'\'\'Gets training / evaluation mini-batches\n    fpath: source file path. string.\n    maxlen1: source sent maximum length. scalar.\n    maxlen2: target sent maximum length. scalar.\n    vocab_fpath: string. vocabulary file path.\n    batch_size: scalar\n    shuffle: boolean\n\n    Returns\n    batches\n    num_batches: number of mini-batches\n    num_samples\n    \'\'\'\n    sents1, sents2 = _load_data(fpath, maxlen1, maxlen2)\n    batches = _input_fn(sents1, sents2, vocab_fpath, batch_size, gpu_nums, maxlen1, maxlen2, shuffle=shuffle)\n    num_batches = calc_num_batches(len(sents1), batch_size*gpu_nums)\n    return batches, num_batches, len(sents1)\n'"
hparams.py,0,"b'# -*- coding: utf-8 -*-\n#/usr/bin/python3\n\'\'\'\ndate: 2019/5/21\nmail: cally.maxiong@gmail.com\npage: http://www.cnblogs.com/callyblog/\n\'\'\'\nimport argparse\n\nclass Hparams:\n    parser = argparse.ArgumentParser()\n\n    # prepro\n    parser.add_argument(\'--vocab_size\', default=10598, type=int)\n\n    # train\n    ## files\n    parser.add_argument(\'--train\', default=\'data/test.csv\',\n                             help=""data for train"")\n\n    parser.add_argument(\'--eval\', default=\'data/test.csv\',\n                             help=""data for evaluation"")\n    parser.add_argument(\'--eval_rouge\', default=\'data/test_summary.csv\',\n                             help=""data for calculate rouge score"")\n\n    ## vocabulary\n    parser.add_argument(\'--vocab\', default=\'vocab\',\n                        help=""vocabulary file path"")\n\n    parser.add_argument(\'--stop_vocab\', default=\'stop_vocab\',\n                        help=""stop vocabulary file path"")\n\n    # training scheme\n    parser.add_argument(\'--batch_size\', default=32, type=int)\n    parser.add_argument(\'--eval_batch_size\', default=32, type=int)\n\n    parser.add_argument(\'--lr\', default=0.0005, type=float, help=""learning rate"")\n    parser.add_argument(\'--warmup_steps\', default=4000, type=int)\n    parser.add_argument(\'--logdir\', default=""log/2"", help=""log directory"")\n    parser.add_argument(\'--num_epochs\', default=5, type=int)\n    parser.add_argument(\'--evaldir\', default=""eval/1"", help=""evaluation dir"")\n\n    # model\n    parser.add_argument(\'--d_model\', default=512, type=int,\n                        help=""hidden dimension of encoder/decoder"")\n    parser.add_argument(\'--d_ff\', default=2048, type=int,\n                        help=""hidden dimension of feedforward layer"")\n    parser.add_argument(\'--num_blocks\', default=6, type=int,\n                        help=""number of encoder/decoder blocks"")\n    parser.add_argument(\'--num_heads\', default=8, type=int,\n                        help=""number of attention heads"")\n    parser.add_argument(\'--maxlen1\', default=150, type=int,\n                        help=""maximum length of a source sequence"")\n    parser.add_argument(\'--maxlen2\', default=25, type=int,\n                        help=""maximum length of a target sequence"")\n    parser.add_argument(\'--dropout_rate\', default=0.1, type=float)\n    parser.add_argument(\'--beam_size\', default=4, type=int,\n                        help=""beam size"")\n    parser.add_argument(\'--gpu_nums\', default=1, type=int,\n                        help=""gpu amount, which can allow how many gpu to train this model"")'"
model.py,60,"b'# -*- coding: utf-8 -*-\n#/usr/bin/python3\n\'\'\'\ndate: 2019/5/21\nmail: cally.maxiong@gmail.com\npage: http://www.cnblogs.com/callyblog/\n\'\'\'\nimport logging\n\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom data_load import _load_vocab\nfrom modules import get_token_embeddings, ff, positional_encoding, multihead_attention, noam_scheme\nfrom utils import convert_idx_to_token_tensor, split_input\n\nlogging.basicConfig(level=logging.INFO)\n\nclass Transformer:\n    def __init__(self, hp):\n        self.hp = hp\n        self.token2idx, self.idx2token = _load_vocab(hp.vocab)\n        self.embeddings = get_token_embeddings(self.hp.vocab_size, self.hp.d_model, zero_pad=True)\n\n    def encode(self, xs, training=True):\n        \'\'\'\n        Returns\n        memory: encoder outputs. (N, T1, d_model)\n        \'\'\'\n        with tf.variable_scope(""encoder"", reuse=tf.AUTO_REUSE):\n            self.x, sents1 = xs\n\n            # embedding\n            enc = tf.nn.embedding_lookup(self.embeddings, self.x) # (N, T1, d_model)\n            enc *= self.hp.d_model**0.5 # scale\n\n            enc += positional_encoding(enc, self.hp.maxlen1)\n            enc = tf.layers.dropout(enc, self.hp.dropout_rate, training=training)\n            ## Blocks\n            for i in range(self.hp.num_blocks):\n                with tf.variable_scope(""num_blocks_{}"".format(i), reuse=tf.AUTO_REUSE):\n                    # self-attention\n                    enc, _ = multihead_attention(queries=enc,\n                                                  keys=enc,\n                                                  values=enc,\n                                                  num_heads=self.hp.num_heads,\n                                                  dropout_rate=self.hp.dropout_rate,\n                                                  training=training,\n                                                  causality=False)\n                    # feed forward\n                    enc = ff(enc, num_units=[self.hp.d_ff, self.hp.d_model])\n        self.enc_output = enc\n        return self.enc_output, sents1\n\n    def decode(self, xs, ys, memory, training=True):\n        \'\'\'\n        memory: encoder outputs. (N, T1, d_model)\n\n        Returns\n        logits: (N, T2, V). float32.\n        y: (N, T2). int32\n        sents2: (N,). string.\n        \'\'\'\n        self.memory = memory\n        with tf.variable_scope(""decoder"", reuse=tf.AUTO_REUSE):\n            self.decoder_inputs, y, sents2 = ys\n            x, _, = xs\n\n            # embedding\n            dec = tf.nn.embedding_lookup(self.embeddings, self.decoder_inputs)  # (N, T2, d_model)\n            dec *= self.hp.d_model ** 0.5  # scale\n\n            dec += positional_encoding(dec, self.hp.maxlen2)\n\n            before_dec = dec\n\n            dec = tf.layers.dropout(dec, self.hp.dropout_rate, training=training)\n\n            attn_dists = []\n            # Blocks\n            for i in range(self.hp.num_blocks):\n                with tf.variable_scope(""num_blocks_{}"".format(i), reuse=tf.AUTO_REUSE):\n                    # Masked self-attention (Note that causality is True at this time)\n                    dec, _ = multihead_attention(queries=dec,\n                                                 keys=dec,\n                                                 values=dec,\n                                                 num_heads=self.hp.num_heads,\n                                                 dropout_rate=self.hp.dropout_rate,\n                                                 training=training,\n                                                 causality=True,\n                                                 scope=""self_attention"")\n                    # Vanilla attention\n                    dec, attn_dist = multihead_attention(queries=dec,\n                                                          keys=self.memory,\n                                                          values=self.memory,\n                                                          num_heads=self.hp.num_heads,\n                                                          dropout_rate=self.hp.dropout_rate,\n                                                          training=training,\n                                                          causality=False,\n                                                          scope=""vanilla_attention"")\n                    attn_dists.append(attn_dist)\n                    ### Feed Forward\n                    dec = ff(dec, num_units=[self.hp.d_ff, self.hp.d_model])\n\n        # Final linear projection (embedding weights are shared)\n        weights = tf.transpose(self.embeddings) # (d_model, vocab_size)\n        logits = tf.einsum(\'ntd,dk->ntk\', dec, weights) # (N, T2, vocab_size)\n\n        with tf.variable_scope(""gen"", reuse=tf.AUTO_REUSE):\n            gens = tf.layers.dense(tf.concat([before_dec, dec, attn_dists[-1]], axis=-1), units=1, activation=tf.sigmoid,\n                                   trainable=training, use_bias=False)\n\n        logits = tf.nn.softmax(logits)\n\n        # final distribution\n        self.logits = self._calc_final_dist(x, gens, logits, attn_dists[-1])\n\n        return self.logits, y, sents2\n\n    def _calc_final_dist(self, x, gens, vocab_dists, attn_dists):\n        """"""Calculate the final distribution, for the pointer-generator model\n\n        Args:\n          x: encoder input which contain oov number\n          gens: the generation, choose vocab from article or vocab\n          vocab_dists: The vocabulary distributions. List length max_dec_steps of (batch_size, vsize) arrays.\n                       The words are in the order they appear in the vocabulary file.\n          attn_dists: The attention distributions. List length max_dec_steps of (batch_size, attn_len) arrays\n\n        Returns:\n          final_dists: The final distributions. List length max_dec_steps of (batch_size, extended_vsize) arrays.\n        """"""\n        with tf.variable_scope(\'final_distribution\', reuse=tf.AUTO_REUSE):\n            # Multiply vocab dists by p_gen and attention dists by (1-p_gen)\n            vocab_dists = gens * vocab_dists\n            attn_dists = (1-gens) * attn_dists\n\n            batch_size = tf.shape(attn_dists)[0]\n            dec_t = tf.shape(attn_dists)[1]\n            attn_len = tf.shape(attn_dists)[2]\n\n            dec = tf.range(0, limit=dec_t) # [dec]\n            dec = tf.expand_dims(dec, axis=-1) # [dec, 1]\n            dec = tf.tile(dec, [1, attn_len]) # [dec, atten_len]\n            dec = tf.expand_dims(dec, axis=0) # [1, dec, atten_len]\n            dec = tf.tile(dec, [batch_size, 1, 1]) # [batch_size, dec, atten_len]\n\n            x = tf.expand_dims(x, axis=1) # [batch_size, 1, atten_len]\n            x = tf.tile(x, [1, dec_t, 1]) # [batch_size, dec, atten_len]\n            x = tf.stack([dec, x], axis=3)\n\n            attn_dists_projected = tf.map_fn(fn=lambda y: tf.scatter_nd(y[0], y[1], [dec_t, self.hp.vocab_size]),\n                                             elems=(x, attn_dists), dtype=tf.float32)\n\n            final_dists = attn_dists_projected + vocab_dists\n\n        return final_dists\n\n    def _calc_loss(self, targets, final_dists):\n        """"""\n        calculate loss\n        :param targets: reference\n        :param final_dists:  transformer decoder output add by pointer generator\n        :return: loss\n        """"""\n        with tf.name_scope(\'loss\'):\n            dec = tf.shape(targets)[1]\n            batch_nums = tf.shape(targets)[0]\n            dec = tf.range(0, limit=dec)\n            dec = tf.expand_dims(dec, axis=0)\n            dec = tf.tile(dec, [batch_nums, 1])\n            indices = tf.stack([dec, targets], axis=2) # [batch_size, dec, 2]\n\n            loss = tf.map_fn(fn=lambda x: tf.gather_nd(x[1], x[0]), elems=(indices, final_dists), dtype=tf.float32)\n            loss = tf.log(0.9) - tf.log(loss)\n\n            nonpadding = tf.to_float(tf.not_equal(targets, self.token2idx[""<pad>""]))  # 0: <pad>\n            loss = tf.reduce_sum(loss * nonpadding) / (tf.reduce_sum(nonpadding) + 1e-7)\n\n            return loss\n\n    def train(self, xs, ys):\n        """"""\n        train model\n        :param xs: dataset xs\n        :param ys: dataset ys\n        :return: loss\n                 train op\n                 global step\n                 tensorflow summary\n        """"""\n        tower_grads = []\n        global_step = tf.train.get_or_create_global_step()\n        global_step_ = global_step * self.hp.gpu_nums\n        lr = noam_scheme(self.hp.d_model, global_step_, self.hp.warmup_steps)\n        optimizer = tf.train.AdamOptimizer(lr)\n        losses = []\n        xs, ys = split_input(xs, ys, self.hp.gpu_nums)\n        with tf.variable_scope(tf.get_variable_scope()):\n            for no in range(self.hp.gpu_nums):\n                with tf.device(""/gpu:%d"" % no):\n                    with tf.name_scope(""tower_%d"" % no):\n                        memory, sents1 = self.encode(xs[no])\n                        logits, y, sents2 = self.decode(xs[no], ys[no], memory)\n                        tf.get_variable_scope().reuse_variables()\n\n                        loss = self._calc_loss(y, logits)\n                        losses.append(loss)\n                        grads = optimizer.compute_gradients(loss)\n                        tower_grads.append(grads)\n\n        with tf.device(""/cpu:0""):\n            grads = self.average_gradients(tower_grads)\n            train_op = optimizer.apply_gradients(grads, global_step=global_step)\n            loss = sum(losses) / len(losses)\n            tf.summary.scalar(\'lr\', lr)\n            tf.summary.scalar(""train_loss"", loss)\n            summaries = tf.summary.merge_all()\n\n        return loss, train_op, global_step_, summaries\n\n    def average_gradients(self, tower_grads):\n        """"""\n        average gradients of all gpu gradients\n        :param tower_grads: list, each element is a gradient of gpu\n        :return: be averaged gradient\n        """"""\n        average_grads = []\n        for grad_and_vars in zip(*tower_grads):\n            grads = []\n            for g, _ in grad_and_vars:\n                expend_g = tf.expand_dims(g, 0)\n                grads.append(expend_g)\n            grad = tf.concat(grads, 0)\n            grad = tf.reduce_mean(grad, 0)\n            v = grad_and_vars[0][1]\n            grad_and_var = (grad, v)\n            average_grads.append(grad_and_var)\n\n        return average_grads\n\n    def eval(self, xs, ys):\n        \'\'\'Predicts autoregressively\n        At inference, input ys is ignored.\n        Returns\n        y_hat: (N, T2)\n        tensorflow summary\n        \'\'\'\n        # decoder_inputs <s> sentences\n        decoder_inputs, y, sents2 = ys\n\n        # decoder_inputs shape: [batch_size, 1] [[<s>], [<s>], [<s>], [<s>]]\n        decoder_inputs = tf.ones((tf.shape(xs[0])[0], 1), tf.int32) * self.token2idx[""<s>""]\n        ys = (decoder_inputs, y, sents2)\n\n        memory, sents1 = self.encode(xs, False)\n\n        y_hat = None\n        logging.info(""Inference graph is being built. Please be patient."")\n        for _ in tqdm(range(self.hp.maxlen2)):\n            logits, y, sents2 = self.decode(xs, ys, memory, False)\n            y_hat = tf.to_int32(tf.argmax(logits, axis=-1))\n\n            if tf.reduce_sum(y_hat, 1) == self.token2idx[""<pad>""]: break\n\n            _decoder_inputs = tf.concat((decoder_inputs, y_hat), 1)\n            ys = (_decoder_inputs, y, sents2)\n\n        # monitor a random sample\n        n = tf.random_uniform((), 0, tf.shape(y_hat)[0]-1, tf.int32)\n        sent1 = sents1[n]\n        pred = convert_idx_to_token_tensor(y_hat[n], self.idx2token)\n        sent2 = sents2[n]\n\n        tf.summary.text(""sent1"", sent1)\n        tf.summary.text(""pred"", pred)\n        tf.summary.text(""sent2"", sent2)\n        summaries = tf.summary.merge_all()\n\n        return y_hat, summaries'"
modules.py,62,"b'# -*- coding: utf-8 -*-\n#/usr/bin/python3\n\'\'\'\ndate: 2019/5/21\nmail: cally.maxiong@gmail.com\npage: http://www.cnblogs.com/callyblog/\n\'\'\'\n\nimport numpy as np\nimport tensorflow as tf\n\ndef ln(inputs, epsilon = 1e-8, scope=""ln""):\n    \'\'\'Applies layer normalization. See https://arxiv.org/abs/1607.06450.\n    inputs: A tensor with 2 or more dimensions, where the first dimension has `batch_size`.\n    epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n    scope: Optional scope for `variable_scope`.\n      \n    Returns:\n      A tensor with the same shape and data dtype as `inputs`.\n    \'\'\'\n    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n        inputs_shape = inputs.get_shape()\n        params_shape = inputs_shape[-1:]\n    \n        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n        beta= tf.get_variable(""beta"", params_shape, initializer=tf.zeros_initializer())\n        gamma = tf.get_variable(""gamma"", params_shape, initializer=tf.ones_initializer())\n        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n        outputs = gamma * normalized + beta\n        \n    return outputs\n\ndef get_token_embeddings(vocab_size, num_units, zero_pad=True):\n    \'\'\'Constructs token embedding matrix.\n    Note that the column of index 0\'s are set to zeros.\n    vocab_size: scalar. V.\n    num_units: embedding dimensionalty. E.\n    zero_pad: Boolean. If True, all the values of the first row (id = 0) should be constant zero\n    To apply query/key masks easily, zero pad is turned on.\n\n    Returns\n    weight variable: (V, E)\n    \'\'\'\n    with tf.variable_scope(""shared_weight_matrix"", reuse=tf.AUTO_REUSE):\n        embeddings = tf.get_variable(\'weight_mat\',\n                                   dtype=tf.float32,\n                                   shape=(vocab_size, num_units),\n                                   initializer=tf.contrib.layers.xavier_initializer())\n        if zero_pad:\n            embeddings = tf.concat((tf.zeros(shape=[1, num_units]),\n                                    embeddings[1:, :]), 0)\n    return embeddings\n\ndef scaled_dot_product_attention(Q, K, V,\n                                 num_heads,\n                                 causality=False, dropout_rate=0.,\n                                 training=True,\n                                 scope=""scaled_dot_product_attention""):\n    \'\'\'See 3.2.1.\n    Q: Packed queries. 3d tensor. [N, T_q, d_k].\n    K: Packed keys. 3d tensor. [N, T_k, d_k].\n    V: Packed values. 3d tensor. [N, T_k, d_v].\n    causality: If True, applies masking for future blinding\n    dropout_rate: A floating point number of [0, 1].\n    training: boolean for controlling droput\n    scope: Optional scope for `variable_scope`.\n    \'\'\'\n    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n        d_k = Q.get_shape().as_list()[-1]\n\n        # dot product\n        outputs = tf.matmul(Q, tf.transpose(K, [0, 2, 1]))  # (N, T_q, T_k)\n\n        # scale\n        outputs /= d_k ** 0.5\n\n        # key masking, delete key 0\n        outputs = mask(outputs, Q, K, type=""key"")\n\n        # causality or future blinding masking\n        if causality:\n            outputs = mask(outputs, type=""future"")\n\n        # softmax\n        attn_dists = tf.nn.softmax(tf.reduce_sum(tf.split(outputs, num_heads, axis=0), axis=0))\n        outputs = tf.nn.softmax(outputs)\n        attention = tf.transpose(outputs, [0, 2, 1])\n        tf.summary.image(""attention"", tf.expand_dims(attention[:1], -1))\n\n        # query masking, delete query <pad>\n        outputs = mask(outputs, Q, K, type=""query"")\n\n        # dropout\n        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=training)\n\n        # weighted sum (context vectors)\n        outputs = tf.matmul(outputs, V)  # (N, T_q, d_v)\n\n    return outputs, attn_dists\n\ndef mask(inputs, queries=None, keys=None, type=None):\n    """"""Masks paddings on keys or queries to inputs\n    inputs: 3d tensor. (N, T_q, T_k)\n    queries: 3d tensor. (N, T_q, d)\n    keys: 3d tensor. (N, T_k, d)\n\n    e.g.,\n    >> queries = tf.constant([[[1.],\n                        [2.],\n                        [0.]]], tf.float32) # (1, 3, 1)\n    >> keys = tf.constant([[[4.],\n                     [0.]]], tf.float32)  # (1, 2, 1)\n    >> inputs = tf.constant([[[4., 0.],\n                               [8., 0.],\n                               [0., 0.]]], tf.float32)\n    >> mask(inputs, queries, keys, ""key"")\n    array([[[ 4.0000000e+00, -4.2949673e+09],\n        [ 8.0000000e+00, -4.2949673e+09],\n        [ 0.0000000e+00, -4.2949673e+09]]], dtype=float32)\n    >> inputs = tf.constant([[[1., 0.],\n                             [1., 0.],\n                              [1., 0.]]], tf.float32)\n    >> mask(inputs, queries, keys, ""query"")\n    array([[[1., 0.],\n        [1., 0.],\n        [0., 0.]]], dtype=float32)\n    """"""\n    padding_num = -2 ** 32 + 1\n    if type in (""k"", ""key"", ""keys""):\n        # Generate masks\n        masks = tf.sign(tf.reduce_sum(tf.abs(keys), axis=-1))  # (N, T_k)\n        masks = tf.expand_dims(masks, 1) # (N, 1, T_k)\n        masks = tf.tile(masks, [1, tf.shape(queries)[1], 1])  # (N, T_q, T_k)\n\n        # Apply masks to inputs\n        paddings = tf.ones_like(inputs) * padding_num\n\n        outputs = tf.where(tf.equal(masks, 0), paddings, inputs)  # (N, T_q, T_k)\n    elif type in (""q"", ""query"", ""queries""):\n        # Generate masks\n        masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=-1))  # (N, T_q)\n        masks = tf.expand_dims(masks, -1)  # (N, T_q, 1)\n        masks = tf.tile(masks, [1, 1, tf.shape(keys)[1]])  # (N, T_q, T_k)\n\n        # Apply masks to inputs\n        outputs = inputs*masks\n    elif type in (""f"", ""future"", ""right""):\n        diag_vals = tf.ones_like(inputs[0, :, :])  # (T_q, T_k)\n        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  # (T_q, T_k)\n        masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(inputs)[0], 1, 1])  # (N, T_q, T_k)\n\n        paddings = tf.ones_like(masks) * padding_num\n        outputs = tf.where(tf.equal(masks, 0), paddings, inputs)\n    else:\n        print(""Check if you entered type correctly!"")\n\n\n    return outputs\n\ndef multihead_attention(queries, keys, values,\n                        num_heads=8, \n                        dropout_rate=0,\n                        training=True,\n                        causality=False,\n                        scope=""multihead_attention""):\n    \'\'\'Applies multihead attention. See 3.2.2\n    queries: A 3d tensor with shape of [N, T_q, d_model].\n    keys: A 3d tensor with shape of [N, T_k, d_model].\n    values: A 3d tensor with shape of [N, T_k, d_model].\n    num_heads: An int. Number of heads.\n    dropout_rate: A floating point number.\n    training: Boolean. Controller of mechanism for dropout.\n    causality: Boolean. If true, units that reference the future are masked.\n    scope: Optional scope for `variable_scope`.\n        \n    Returns\n      A 3d tensor with shape of (N, T_q, C)  \n    \'\'\'\n    d_model = queries.get_shape().as_list()[-1]\n    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n        # Linear projections\n        Q = tf.layers.dense(queries, d_model, use_bias=False) # (N, T_q, d_model)\n        K = tf.layers.dense(keys, d_model, use_bias=False) # (N, T_k, d_model)\n        V = tf.layers.dense(values, d_model, use_bias=False) # (N, T_k, d_model)\n        \n        # Split and concat\n        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, d_model/h)\n        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, d_model/h)\n        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, d_model/h)\n\n        # Attention\n        outputs, attn_dists = scaled_dot_product_attention(Q_, K_, V_, num_heads, causality, dropout_rate, training)\n\n        # Restore shape\n        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2) # (N, T_q, d_model)\n        # Residual connection\n        outputs = queries + outputs\n              \n        # Normalize\n        outputs = ln(outputs)\n \n    return outputs, attn_dists\n\ndef ff(inputs, num_units, scope=""positionwise_feedforward""):\n    \'\'\'position-wise feed forward net. See 3.3\n    \n    inputs: A 3d tensor with shape of [N, T, C].\n    num_units: A list of two integers.\n    scope: Optional scope for `variable_scope`.\n\n    Returns:\n      A 3d tensor with the same shape and dtype as inputs\n    \'\'\'\n    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n        # Inner layer\n        outputs = tf.layers.dense(inputs, num_units[0], activation=tf.nn.relu)\n\n        # Outer layer\n        outputs = tf.layers.dense(outputs, num_units[1])\n\n        # Residual connection\n        outputs += inputs\n        \n        # Normalize\n        outputs = ln(outputs)\n    \n    return outputs\n\ndef label_smoothing(inputs, epsilon=0.1):\n    \'\'\'Applies label smoothing. See 5.4 and https://arxiv.org/abs/1512.00567.\n    inputs: 3d tensor. [N, T, V], where V is the number of vocabulary.\n    epsilon: Smoothing rate.\n    \n    For example,\n    \n    ```\n    import tensorflow as tf\n    inputs = tf.convert_to_tensor([[[0, 0, 1], \n       [0, 1, 0],\n       [1, 0, 0]],\n\n      [[1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0]]], tf.float32)\n       \n    outputs = label_smoothing(inputs)\n    \n    with tf.Session() as sess:\n        print(sess.run([outputs]))\n    \n    >>\n    [array([[[ 0.03333334,  0.03333334,  0.93333334],\n        [ 0.03333334,  0.93333334,  0.03333334],\n        [ 0.93333334,  0.03333334,  0.03333334]],\n\n       [[ 0.93333334,  0.03333334,  0.03333334],\n        [ 0.93333334,  0.03333334,  0.03333334],\n        [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n    ```    \n    \'\'\'\n    V = tf.cast(tf.shape(inputs)[-1], tf.float32) # number of channels\n    return ((1-epsilon) * inputs) + (epsilon / V)\n    \ndef positional_encoding(inputs,\n                        maxlen,\n                        masking=True,\n                        scope=""positional_encoding""):\n    \'\'\'Sinusoidal Positional_Encoding. See 3.5\n    inputs: 3d tensor. (N, T, E)\n    maxlen: scalar. Must be >= T\n    masking: Boolean. If True, padding positions are set to zeros.\n    scope: Optional scope for `variable_scope`.\n\n    returns\n    3d tensor that has the same shape as inputs.\n    \'\'\'\n\n    E = inputs.get_shape().as_list()[-1] # static\n    N, T = tf.shape(inputs)[0], tf.shape(inputs)[1] # dynamic\n    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n        # position indices\n        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1]) # (N, T)\n\n        # First part of the PE function: sin and cos argument\n        position_enc = np.array([\n            [pos / np.power(10000, (i-i%2)/E) for i in range(E)]\n            for pos in range(maxlen)])\n\n        # Second part, apply the cosine to even columns and sin to odds.\n        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n        position_enc = tf.convert_to_tensor(position_enc, tf.float32) # (maxlen, E)\n\n        # lookup\n        outputs = tf.nn.embedding_lookup(position_enc, position_ind)\n\n        # masks\n        if masking:\n            outputs = tf.where(tf.equal(inputs, 0), inputs, outputs)\n\n        return tf.to_float(outputs)\n\ndef noam_scheme(d_model, global_step, warmup_steps=4000.):\n    \'\'\'Noam scheme learning rate decay\n    d_model: encoder and decoder embedding\n    global_step: scalar.\n    warmup_steps: scalar. During warmup_steps, learning rate increases\n        until it reaches init_lr.\n    \'\'\'\n    step = tf.cast(global_step + 1, dtype=tf.float32)\n    return d_model ** -0.5 * tf.minimum(step * warmup_steps ** -1.5, step ** -0.5)'"
pred.py,9,"b'# -*- coding: utf-8 -*-\n#/usr/bin/python3\n\'\'\'\ndate: 2019/5/21\nmail: cally.maxiong@gmail.com\npage: http://www.cnblogs.com/callyblog/\n\'\'\'\nimport os\n\nfrom beam_search import BeamSearch\nfrom data_load import _load_vocab\nfrom hparams import Hparams\nfrom model import Transformer\n\ndef import_tf(device_id=-1, verbose=False):\n    """"""\n    import tensorflow, set tensorflow graph load device, set tensorflow log level, return tensorflow instance\n    :param device_id: GPU id\n    :param verbose: tensorflow logging level\n    :return: tensorflow instance\n    """"""\n    # set visible gpu, -1 is cpu\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'-1\' if device_id < 0 else str(device_id)\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'0\' if verbose else \'3\'\n    import tensorflow as tf\n    tf.logging.set_verbosity(tf.logging.DEBUG if verbose else tf.logging.ERROR)\n    return tf\n\nclass Prediction:\n    def __init__(self, args):\n        """"""\n        :param model_dir: model dir path\n        :param vocab_file: vocab file path\n        """"""\n        self.tf = import_tf(0)\n\n        self.args = args\n        self.model_dir = args.logdir\n        self.vocab_file = args.vocab\n        self.token2idx, self.idx2token = _load_vocab(args.vocab)\n\n        hparams = Hparams()\n        parser = hparams.parser\n        self.hp = parser.parse_args()\n\n        self.model = Transformer(self.hp)\n\n        self._add_placeholder()\n        self._init_graph()\n\n    def _init_graph(self):\n        """"""\n        init graph\n        """"""\n        self.ys = (self.input_y, None, None)\n        self.xs = (self.input_x, None)\n        self.memory = self.model.encode(self.xs, False)[0]\n        self.logits = self.model.decode(self.xs, self.ys, self.memory, False)[0]\n\n        ckpt = self.tf.train.get_checkpoint_state(self.model_dir).all_model_checkpoint_paths[-1]\n\n        graph = self.logits.graph\n        sess_config = self.tf.ConfigProto(allow_soft_placement=True)\n        sess_config.gpu_options.allow_growth = True\n\n        saver = self.tf.train.Saver()\n        self.sess = self.tf.Session(config=sess_config, graph=graph)\n\n        self.sess.run(self.tf.global_variables_initializer())\n        self.tf.reset_default_graph()\n        saver.restore(self.sess, ckpt)\n\n        self.bs = BeamSearch(self.model,\n                             self.hp.beam_size,\n                             list(self.idx2token.keys())[2],\n                             list(self.idx2token.keys())[3],\n                             self.idx2token,\n                             self.hp.maxlen2,\n                             self.input_x,\n                             self.input_y,\n                             self.logits)\n\n    def predict(self, content):\n        """"""\n        abstract prediction by beam search\n        :param content: article content\n        :return: prediction result\n        """"""\n        input_x = list(content)\n        while len(input_x) < self.args.maxlen1: input_x.append(\'<pad>\')\n        input_x = input_x[:self.args.maxlen1]\n\n        input_x = [self.token2idx.get(s, self.token2idx[\'<unk>\']) for s in input_x]\n\n        memory = self.sess.run(self.memory, feed_dict={self.input_x: [input_x]})\n\n        return self.bs.search(self.sess, input_x, memory[0])\n\n    def _add_placeholder(self):\n        """"""\n        add tensorflow placeholder\n        """"""\n        self.input_x = self.tf.placeholder(dtype=self.tf.int32, shape=[None, self.args.maxlen1], name=\'input_x\')\n        self.input_y = self.tf.placeholder(dtype=self.tf.int32, shape=[None, None], name=\'input_y\')\n\nif __name__ == \'__main__\':\n    hparams = Hparams()\n    parser = hparams.parser\n    hp = parser.parse_args()\n    preds = Prediction(hp)\n    content = \'2014\xe5\xb9\xb4\xef\xbc\x8c51\xe4\xbf\xa1\xe7\x94\xa8\xe5\x8d\xa1\xe7\xae\xa1\xe5\xae\xb6\xe8\xb7\x9f\xe5\xae\x9c\xe4\xbf\xa1\xe7\xad\x89P2P\xe5\x85\xac\xe5\x8f\xb8\xe5\x90\x88\xe4\xbd\x9c\xef\xbc\x8c\xe6\x8e\xa8\xe5\x87\xba\xe7\xba\xbf\xe4\xb8\x8a\xe4\xbf\xa1\xe8\xb4\xb7\xe4\xba\xa7\xe5\x93\x81\xe2\x80\x9c\xe7\x9e\xac\xe6\x97\xb6\xe8\xb4\xb7\xe2\x80\x9d\xef\xbc\x8c\xe5\x85\xb6\xe6\x98\xaf\xe4\xb8\x80\xe7\xa7\x8d\xe7\xba\xaf\xe5\x9c\xa8\xe7\xba\xbf\xe6\x93\x8d\xe4\xbd\x9c\xe7\x9a\x84\xe4\xbf\xa1\xe8\xb4\xb7\xe6\xa8\xa1\xe5\xbc\x8f\xe3\x80\x8251\xe4\xbf\xa1\xe7\x94\xa8\xe5\x8d\xa1\xe7\xae\xa1\xe5\xae\xb6\xe5\x88\x9b\xe5\xa7\x8b\xe4\xba\xba\xe5\xad\x99\xe6\xb5\xb7\xe6\xb6\x9b\xe8\xaf\xb4\xef\xbc\x8c51\xe7\x9b\xae\xe5\x89\x8d\xe6\xaf\x8f\xe5\xa4\xa9\xe6\x94\xbe\xe8\xb4\xb71000\xe4\xb8\x87\xef\xbc\x8c\xe9\xa2\x84\xe8\xae\xa12015\xe5\xb9\xb4\xef\xbc\x8c\xe8\x87\xaa\xe8\x90\xa5\xe4\xba\xa7\xe5\x93\x81\xe5\x8a\xa0\xe4\xb8\x8a\xe7\x9e\xac>\xe6\x97\xb6\xe8\xb4\xb7\xef\xbc\x8c\xe6\x94\xbe\xe8\xb4\xb7\xe9\xa2\x9d\xe5\xba\xa6\xe5\xb0\x86\xe8\xbf\x9c\xe8\xb6\x85\'\n    result = preds.predict(content)\n    for res in result:\n        print(res)'"
train.py,7,"b'# -*- coding: utf-8 -*-\n#/usr/bin/python3\n\'\'\'\ndate: 2019/5/21\nmail: cally.maxiong@gmail.com\npage: http://www.cnblogs.com/callyblog/\n\'\'\'\n\nimport logging\nimport os\n\nfrom sumeval.metrics.rouge import RougeCalculator\nfrom tqdm import tqdm\n\nfrom beam_search import BeamSearch\nfrom data_load import get_batch, _load_vocab\nfrom hparams import Hparams\nfrom model import Transformer\nfrom utils import save_hparams, save_variable_specs, get_hypotheses, calc_rouge, import_tf\n\nlogging.basicConfig(level=logging.INFO)\n\nrouge = RougeCalculator(stopwords=True, lang=""zh"")\n\nlogging.info(""# hparams"")\nhparams = Hparams()\nparser = hparams.parser\nhp = parser.parse_args()\n\n# import tensorflow\ngpu_list = [str(i) for i in list(range(hp.gpu_nums))]\ntf = import_tf(gpu_list)\n\nsave_hparams(hp, hp.logdir)\n\nlogging.info(""# Prepare train/eval batches"")\ntrain_batches, num_train_batches, num_train_samples = get_batch(hp.train,\n                                                                hp.maxlen1,\n                                                                hp.maxlen2,\n                                                                hp.vocab,\n                                                                hp.batch_size,\n                                                                hp.gpu_nums,\n                                                                shuffle=True)\n\neval_batches, num_eval_batches, num_eval_samples = get_batch(hp.eval,\n                                                             hp.maxlen1,\n                                                             hp.maxlen2,\n                                                             hp.vocab,\n                                                             hp.eval_batch_size,\n                                                             hp.gpu_nums,\n                                                             shuffle=False)\n\nhandle = tf.placeholder(tf.string, shape=[])\niter = tf.data.Iterator.from_string_handle(\n    handle, train_batches.output_types, train_batches.output_shapes)\n\n# create a iter of the correct shape and type\nxs, ys = iter.get_next()\n\nlogging.info(\'# init data\')\ntraining_iter = train_batches.make_one_shot_iterator()\nval_iter = eval_batches.make_initializable_iterator()\n\nlogging.info(""# Load model"")\nm = Transformer(hp)\n\n# get op\nloss, train_op, global_step, train_summaries = m.train(xs, ys)\ny_hat, eval_summaries = m.eval(xs, ys)\n\ntoken2idx, idx2token = _load_vocab(hp.vocab)\n\nbs = BeamSearch(m, hp.beam_size, list(idx2token.keys())[2], list(idx2token.keys())[3], idx2token, hp.maxlen2, m.x,\n                m.decoder_inputs, m.logits)\n\nlogging.info(""# Session"")\nsaver = tf.train.Saver(max_to_keep=hp.num_epochs)\nwith tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n    ckpt = tf.train.latest_checkpoint(hp.logdir)\n    if ckpt is None:\n        logging.info(""Initializing from scratch"")\n        sess.run(tf.global_variables_initializer())\n        save_variable_specs(os.path.join(hp.logdir, ""specs""))\n    else:\n        saver.restore(sess, ckpt)\n\n    summary_writer = tf.summary.FileWriter(hp.logdir, sess.graph)\n\n    # Iterator.string_handle() get a tensor that can be got value to feed handle placeholder\n    training_handle = sess.run(training_iter.string_handle())\n    val_handle = sess.run(val_iter.string_handle())\n\n    total_steps = hp.num_epochs * num_train_batches\n    _gs = sess.run(global_step)\n    for i in tqdm(range(_gs, total_steps+1)):\n        _, _gs, _summary = sess.run([train_op, global_step, train_summaries], feed_dict={handle: training_handle})\n        summary_writer.add_summary(_summary, _gs)\n        if _gs % (hp.gpu_nums * 5000) == 0 and _gs != 0:\n            logging.info(""steps {} is done"".format(_gs))\n\n            logging.info(""# test evaluation"")\n            sess.run(val_iter.initializer) # initial val dataset\n            _eval_summaries = sess.run(eval_summaries, feed_dict={handle: val_handle})\n            summary_writer.add_summary(_eval_summaries, _gs)\n\n            logging.info(""# beam search"")\n            hypotheses, all_targets = get_hypotheses(num_eval_batches, num_eval_samples, sess, m, bs, [xs[0], ys[2]],\n                                                     handle, val_handle)\n\n            logging.info(""# calc rouge score "")\n            if not os.path.exists(hp.evaldir): os.makedirs(hp.evaldir)\n            rouge_l = calc_rouge(rouge, all_targets, hypotheses, _gs, hp.evaldir)\n\n            model_output = ""trans_pointer%02dL%.2f"" % (_gs, rouge_l)\n\n            logging.info(\'# write hypotheses\')\n            with open(os.path.join(hp.evaldir, model_output), \'w\', encoding=\'utf-8\') as f:\n                for target, hypothes in zip(all_targets, hypotheses):\n                    f.write(\'{}-{} \\n\'.format(target, \' \'.join(hypothes)))\n\n            logging.info(""# save models"")\n\n            ckpt_name = os.path.join(hp.logdir, model_output)\n            saver.save(sess, ckpt_name, global_step=_gs)\n            logging.info(""after training of {} steps, {} has been saved."".format(_gs, ckpt_name))\n\n            logging.info(""# fall back to train mode"")\n    summary_writer.close()\n\nlogging.info(""Done"")'"
utils.py,4,"b'# -*- coding: utf-8 -*-\n#/usr/bin/python3\n\'\'\'\ndate: 2019/5/21\nmail: cally.maxiong@gmail.com\npage: http://www.cnblogs.com/callyblog/\n\'\'\'\nimport json\nimport logging\nimport os\nfrom tqdm import tqdm\n\nlogging.basicConfig(level=logging.INFO)\n\ndef calc_num_batches(total_num, batch_size):\n    \'\'\'Calculates the number of batches.\n    total_num: total sample number\n    batch_size\n\n    Returns\n    number of batches, allowing for remainders.\'\'\'\n    return total_num // batch_size + int(total_num % batch_size != 0)\n\ndef convert_idx_to_token_tensor(inputs, idx2token):\n    \'\'\'Converts int32 tensor to string tensor.\n    inputs: 1d int32 tensor. indices.\n    idx2token: dictionary\n\n    Returns\n    1d string tensor.\n    \'\'\'\n    import tensorflow as tf\n    def my_func(inputs):\n        return "" "".join(idx2token[elem] for elem in inputs)\n\n    return tf.py_func(my_func, [inputs], tf.string)\n\ndef postprocess(hypotheses):\n    \'\'\'Processes translation outputs.\n    hypotheses: list of encoded predictions\n    idx2token: dictionary\n\n    Returns\n    processed hypotheses\n    \'\'\'\n    _hypotheses = []\n    for h in hypotheses:\n        h = str(h)\n        h = h.replace(\'<s>\', \'\')\n        h = h.replace(\'</s>\', \'\')\n        h = h.replace(\'<pad>\', \'\')\n        _hypotheses.append(h)\n\n    return _hypotheses\n\ndef save_hparams(hparams, path):\n    \'\'\'Saves hparams to path\n    hparams: argsparse object.\n    path: output directory.\n\n    Writes\n    hparams as literal dictionary to path.\n    \'\'\'\n    if not os.path.exists(path): os.makedirs(path)\n    hp = json.dumps(vars(hparams))\n    with open(os.path.join(path, ""hparams""), \'w\') as fout:\n        fout.write(hp)\n\ndef load_hparams(parser, path):\n    \'\'\'Loads hparams and overrides parser\n    parser: argsparse parser\n    path: directory or file where hparams are saved\n    \'\'\'\n    if not os.path.isdir(path):\n        path = os.path.dirname(path)\n    d = open(os.path.join(path, ""hparams""), \'r\').read()\n    flag2val = json.loads(d)\n    for f, v in flag2val.items():\n        parser.f = v\n\ndef save_variable_specs(fpath):\n    \'\'\'Saves information about variables such as\n    their name, shape, and total parameter number\n    fpath: string. output file path\n\n    Writes\n    a text file named fpath.\n    \'\'\'\n    import tensorflow as tf\n    def _get_size(shp):\n        \'\'\'Gets size of tensor shape\n        shp: TensorShape\n\n        Returns\n        size\n        \'\'\'\n        size = 1\n        for d in range(len(shp)):\n            size *=shp[d]\n        return size\n\n    params, num_params = [], 0\n    for v in tf.global_variables():\n        params.append(""{}==={}"".format(v.name, v.shape))\n        num_params += _get_size(v.shape)\n    print(""num_params: "", num_params)\n    with open(fpath, \'w\') as fout:\n        fout.write(""num_params: {}\\n"".format(num_params))\n        fout.write(""\\n"".join(params))\n    logging.info(""Variables info has been saved."")\n\ndef get_hypotheses(num_batches, num_samples, sess, model, beam_search, tensor, handle_placehoder, handle):\n    \'\'\'Gets hypotheses.\n    num_batches: scalar.\n    num_samples: scalar.\n    sess: tensorflow sess object\n    tensor: target tensor to fetch\n    dict: idx2token dictionary\n\n    Returns\n    hypotheses: list of sents\n    \'\'\'\n    hypotheses, all_targets = [], []\n    for _ in tqdm(range(num_batches)):\n        articles, targets = sess.run(tensor, feed_dict={handle_placehoder: handle})\n        memories = sess.run(model.enc_output, feed_dict={model.x: articles})\n        for article, memory in zip(articles, memories):\n            summary = beam_search.search(sess, article, memory)\n            summary = postprocess(summary)\n            hypotheses.append(summary)\n        all_targets.extend([target.decode(\'utf-8\') for target in targets])\n\n    return hypotheses[:num_samples], all_targets[:num_samples]\n\ndef calc_rouge(rouge, references, models, global_step, logdir):\n    """"""\n    calculate rouge score\n    :param references: reference sentences\n    :param models: model sentences\n    :param global_step: global step\n    :param logdir: log dir\n    :return: rouge score\n    """"""\n    # delete symbol\n    references = [reference.replace(\'</s>\', \'\') for reference in references]\n\n    # calculate rouge score\n    rouge1_scores = [_rouge(rouge, model, reference, type=\'rouge1\') for model, reference in zip(models, references)]\n    rouge2_scores = [_rouge(rouge, model, reference, type=\'rouge2\') for model, reference in zip(models, references)]\n    rougel_scores = [_rouge(rouge, model, reference, type=\'rougel\') for model, reference in zip(models, references)]\n\n    # get rouge score\n    rouge1_score = sum(rouge1_scores) / len(rouge1_scores)\n    rouge2_score = sum(rouge2_scores) / len(rouge2_scores)\n    rougel_score = sum(rougel_scores) / len(rouge2_scores)\n\n    # write result\n    with open(os.path.join(logdir, \'rouge\'), \'a\', encoding=\'utf-8\') as f:\n        f.write(\'global step: {}, ROUGE 1: {}, ROUGE 2: {}, ROUGE L: {}\\n\'.format(str(global_step), str(rouge1_score),\n                                                                                  str(rouge2_score), str(rougel_score)))\n    return rouge1_score\n\ndef _rouge(rouge, model, reference, type=\'rouge1\'):\n    """"""\n    calculate rouge socore\n    :param rouge: sumeval instance\n    :param model: model prediction, list\n    :param reference: reference\n    :param type: rouge1, rouge2, rougel\n    :return: rouge 1 score\n    """"""\n    scores = None\n    if type == \'rouge1\':\n        scores = [rouge.rouge_n(summary=m, references=reference, n=1) for m in model]\n\n    if type == \'rouge2\':\n        scores = [rouge.rouge_n(summary=m, references=reference, n=2) for m in model]\n\n    if type == \'rougel\':\n        scores = [rouge.rouge_l(summary=m, references=reference) for m in model]\n\n    return max(scores)\n\ndef import_tf(gpu_list):\n    """"""\n    import tensorflow, set tensorflow graph load device\n    :param gpu_list: GPU list\n    :return: tensorflow instance\n    """"""\n    import tensorflow as tf\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \',\'.join(gpu_list)\n\n    return tf\n\ndef split_input(xs, ys, gpu_nums):\n    """"""\n    split input\n    :param xs: articles\n    :param ys: summaries\n    :param gpu_nums: gpu numbers\n    :return: split input by gpu numbers\n    """"""\n    import tensorflow as tf\n    xs = [tf.split(x, num_or_size_splits=gpu_nums, axis=0) for x in xs]\n    ys = [tf.split(y, num_or_size_splits=gpu_nums, axis=0) for y in ys]\n\n    return [(xs[0][i], xs[1][i]) for i in range(gpu_nums)], [(ys[0][i], ys[1][i], ys[2][i]) for i in range(gpu_nums)]'"
