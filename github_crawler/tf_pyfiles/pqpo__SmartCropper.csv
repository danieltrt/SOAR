file_path,api_count,code
edge_detection/const.py,0,b'#!/usr/bin/python\n#coding=utf8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimage_height = 256\nimage_width = 256\nuse_batch_norm = True\nuse_kernel_regularizer = False\n\n'
edge_detection/evaluate.py,12,"b'#!/usr/bin/python\n# coding=utf8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom util import *\nfrom hed_net import *\nimport os\n\nfrom tensorflow import flags\n\nflags.DEFINE_string(\'input_img\', \'test_image/test2.jpg\',\n                    \'Image path to run hed, must be jpg image.\')\nflags.DEFINE_string(\'checkpoint_dir\', \'./checkpoint\',\n                    \'Checkpoint directory.\')\nflags.DEFINE_string(\'output_img\', \'test_image/result.jpg\',\n                    \'Output image path.\')\nflags.DEFINE_float(\'output_threshold\', 0.0, \'output threshold, default: 0.0\')\n\nFLAGS = flags.FLAGS\n\nif not os.path.exists(FLAGS.input_img):\n    print(\'--input_img invalid\')\n    exit()\n\nif FLAGS.output_img == \'\':\n    print(\'--output_img invalid\')\n    exit()\n\nif __name__ == ""__main__"":\n    image_path_placeholder = tf.placeholder(tf.string)\n\n    feed_dict_to_use = {image_path_placeholder: FLAGS.input_img}\n\n    image_tensor = tf.read_file(image_path_placeholder)\n    image_tensor = tf.image.decode_jpeg(image_tensor, channels=3)\n    origin_tensor = tf.image.resize_images(image_tensor, [const.image_height, const.image_width])\n    image_float = tf.to_float(origin_tensor)\n    image_float = image_float / 255.0\n    image_float = tf.expand_dims(image_float, axis=0)\n\n    dsn_fuse, dsn1, dsn2, dsn3, dsn4, dsn5 = mobilenet_v2_style_hed(image_float, False)\n    # dsn_fuse = tf.reshape(dsn_fuse, shape=(const.image_height, const.image_width))\n\n    global_init = tf.global_variables_initializer()\n\n    # Saver\n    hed_weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'hed\')\n    saver = tf.train.Saver(hed_weights)\n\n    with tf.Session() as sess:\n        sess.run(global_init)\n\n        latest_ck_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n        if latest_ck_file:\n            print(\'restore from latest checkpoint file : {}\'.format(latest_ck_file))\n            saver.restore(sess, latest_ck_file)\n        else:\n            print(\'no checkpoint file to restore, exit()\')\n            exit()\n\n        _dsn_fuse = sess.run(dsn_fuse, feed_dict=feed_dict_to_use)\n\n        dsn_fuse_image = np.where(_dsn_fuse[0] > FLAGS.output_threshold, [255], [0])\n        save_img(FLAGS.output_img, dsn_fuse_image.reshape([256, 256]))\n        print(\'done! output image: {}\'.format(FLAGS.output_img))\n'"
edge_detection/finetuning.py,11,"b'#!/usr/bin/python\n# coding=utf8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom hed_net import *\nimport numpy as np\nimport os\nimport util\nfrom generate_batch_data import generate_batch_data\n\nfrom tensorflow import flags\n\nflags.DEFINE_string(\'finetuning_dir\', \'./finetuning_model\',\n                    \'finetuning directory.\')\nflags.DEFINE_string(\'checkpoint_dir\', \'./checkpoint\',\n                    \'Checkpoint directory.\')\nflags.DEFINE_string(\'image\', \'test_image/IMG_20190704_143127.jpg\', \'fine tuning image\')\nflags.DEFINE_string(\'annotation\', \'test_image/annotation_IMG_20190704_143127.png_threshold.jpg\', \'fine tuning annotation\')\nflags.DEFINE_string(\'csv\', \'\', \'fine tuning csv\')\nflags.DEFINE_integer(\'batch_size\', 4, \'batch size\')\nflags.DEFINE_float(\'lr\', 0.0005, \'learning rate\')\nflags.DEFINE_integer(\'iterations\', 15,\n                     \'Number of iterations\')\nflags.DEFINE_float(\'output_threshold\', 0.0, \'output threshold\')\n\nFLAGS = flags.FLAGS\n\nhed_ckpt_file_path = os.path.join(FLAGS.checkpoint_dir, \'hed.ckpt\')\n\ntrain_layer = [\'block0_1\', \'block1_0\', \'block2_1\', \'block3_2\', \'block4_3\', \'block5_2\']\n\nif not ((os.path.exists(FLAGS.image)) and (os.path.exists(FLAGS.annotation)) or (os.path.exists(FLAGS.csv))):\n    print(\'please add input, --img, --annotation or --csv\')\n    exit()\n\nimages = []\nannotations = []\n\nbatch_size = FLAGS.batch_size\n\nif os.path.exists(FLAGS.image) and os.path.exists(FLAGS.annotation):\n    images.append(FLAGS.image)\n    annotations.append(FLAGS.annotation)\n\nif os.path.exists(FLAGS.csv):\n    csv_img, csv_ann = util.load_sample_from_csv(FLAGS.csv)\n    if csv_img is not None and len(csv_img) > 0:\n        images.extend(csv_img)\n        annotations.extend(csv_ann)\n\nif len(images) == 0:\n    print(\'Samples is empty, exit()\')\n    exit()\n\nprint(\'fine tuning images size: {}\'.format(len(images)))\nprint(\'fine tuning annotations size: {}\'.format(len(annotations)))\n\nassert len(images) == len(annotations)\n\nimage_tensor, annotation_tensor = generate_batch_data(images, annotations, batch_size=batch_size)\n\nif __name__ == ""__main__"":\n\n    is_training = tf.placeholder(tf.bool)\n\n    dsn_fuse, dsn1, dsn2, dsn3, dsn4, dsn5 = mobilenet_v2_style_hed(image_tensor, is_training)\n\n    hed_weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'hed\')\n\n    cost = class_balanced_sigmoid_cross_entropy(dsn_fuse, annotation_tensor)\n    # cost = class_balanced_sigmoid_cross_entropy(dsn_fuse, annotation_tensor) \\\n    #     + class_balanced_sigmoid_cross_entropy(dsn1, annotation_tensor)\\\n    #     + class_balanced_sigmoid_cross_entropy(dsn2, annotation_tensor)\\\n    #     + class_balanced_sigmoid_cross_entropy(dsn3, annotation_tensor)\\\n    #     + class_balanced_sigmoid_cross_entropy(dsn4, annotation_tensor)\\\n    #     + class_balanced_sigmoid_cross_entropy(dsn5, annotation_tensor)\n\n    var_list = [v for v in tf.trainable_variables() if v.name.split(\'/\')[2] in train_layer]\n    gradients = tf.gradients(cost, var_list)\n    gradients = list(zip(gradients, var_list))\n\n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        train_step = tf.train.AdamOptimizer(learning_rate=FLAGS.lr).apply_gradients(gradients)\n        # train_step = tf.train.AdamOptimizer(learning_rate=FLAGS.lr).minimize(cost)\n\n    global_init = tf.global_variables_initializer()\n\n    # Saver\n    saver = tf.train.Saver(hed_weights)\n\n    with tf.Session() as sess:\n        sess.run(global_init)\n\n        latest_ck_file = tf.train.latest_checkpoint(FLAGS.finetuning_dir)\n        if latest_ck_file:\n            print(\'restore from latest checkpoint file : {}\'.format(latest_ck_file))\n            saver.restore(sess, latest_ck_file)\n        else:\n            print(\'no checkpoint file to restore, exit()\')\n            exit()\n\n        for epoch in range(FLAGS.iterations):\n                for step in range(len(images)):\n                    feed_dict_to_use = {is_training: True}\n                    _dsn_fuse, _ = sess.run([dsn_fuse, train_step], feed_dict=feed_dict_to_use)\n                    if epoch == FLAGS.iterations - 1:\n                        feed_dict_to_use[is_training] = False\n                        dsn_fuse_evaluate = sess.run(dsn_fuse, feed_dict=feed_dict_to_use)\n                        dsn_fuse_image = np.where(dsn_fuse_evaluate[0] > FLAGS.output_threshold, [255], [0])\n                        dsn_fuse_image_path = os.path.join(\'./test_image\', \'fine_tuning_output_img.png\')\n                        util.save_img(dsn_fuse_image_path, dsn_fuse_image.reshape([256, 256]))\n                        saver.save(sess, hed_ckpt_file_path, global_step=0)\n\n        print(""Train Finished!"")\n'"
edge_detection/freeze_model.py,9,"b'#!/usr/bin/python\n#coding=utf8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom hed_net import *\n\nfrom tensorflow import flags\nflags.DEFINE_string(\'checkpoint_dir\', \'./checkpoint\',\n                    \'Checkpoint directory.\')\nflags.DEFINE_string(\'output_file\', \'./hed_lite_model_quantize.tflite\',\n                    \'Output file\')\n\nFLAGS = flags.FLAGS\n\nif __name__ == ""__main__"":\n\n    image_input = tf.placeholder(tf.float32, shape=(const.image_height, const.image_width, 3), name=\'hed_input\')\n    image_float = image_input / 255.0\n    image_float = tf.expand_dims(image_float, axis=0)\n\n    print(\'###1 input shape is: {}, name is: {}\'.format(image_input.get_shape(), image_input.name))\n    dsn_fuse, dsn1, dsn2, dsn3, dsn4, dsn5 = mobilenet_v2_style_hed(image_float, False)\n    img_output = tf.reshape(dsn_fuse, shape=(const.image_height, const.image_width), name=""img_output"")\n    print(\'###2 output shape is: {}, name is: {}\'.format(img_output.get_shape(), img_output.name))\n\n    # Saver\n    hed_weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'hed\')\n    saver = tf.train.Saver(hed_weights)\n\n    global_init = tf.global_variables_initializer()\n\n    with tf.Session() as sess:\n        sess.run(global_init)\n\n        latest_ck_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n        if latest_ck_file:\n            print(\'restore from latest checkpoint file : {}\'.format(latest_ck_file))\n            saver.restore(sess, latest_ck_file)\n        else:\n            print(\'no checkpoint file to restore, exit()\')\n            exit()\n\n        converter = tf.contrib.lite.TFLiteConverter.from_session(sess, [image_input], [img_output])\n        converter.post_training_quantize = True\n        tflite_model = converter.convert()\n        open(FLAGS.output_file, \'wb\').write(tflite_model)\n        print(\'finished\')\n\n'"
edge_detection/generate_batch_data.py,9,"b'#!/usr/bin/python\n#coding=utf8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport multiprocessing as mt\n\nimport tensorflow as tf\nimport const\n\n\ndef generate_batch_data(images, annotations, batch_size):\n\n    def __map_fun(image_path, annotation_path):\n        image_tensor = tf.read_file(image_path)\n        image_tensor = tf.image.decode_jpeg(image_tensor, channels=3)\n        image_tensor = tf.image.resize_images(image_tensor, [const.image_height, const.image_width])\n        image_float = tf.to_float(image_tensor)\n        image_float = image_float / 255.0\n\n        annotation_content = tf.read_file(annotation_path)\n        annotation_tensor = tf.image.decode_png(annotation_content, channels=1)\n        annotation_tensor = tf.image.resize_images(annotation_tensor, [const.image_height, const.image_width])\n        annotation_float = tf.to_float(annotation_tensor)\n        annotation_float = annotation_float / 255.0\n\n        return image_float, annotation_float\n\n    data_set = tf.data.Dataset.from_tensor_slices((images, annotations))\\\n        .shuffle(100).repeat().map(__map_fun, num_parallel_calls=mt.cpu_count()).batch(batch_size)\n    iterator = data_set.make_one_shot_iterator()\n    return iterator.get_next()\n\n\n\n'"
edge_detection/hed_net.py,43,"b""#!/usr/bin/python\n#coding=utf8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport const\nimport tensorflow as tf\n\ndef class_balanced_sigmoid_cross_entropy(logits, label):\n    with tf.name_scope('class_balanced_sigmoid_cross_entropy'):\n        count_neg = tf.reduce_sum(1.0 - label) # \xe6\xa0\xb7\xe6\x9c\xac\xe4\xb8\xad0\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\n        count_pos = tf.reduce_sum(label) # \xe6\xa0\xb7\xe6\x9c\xac\xe4\xb8\xad1\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f(\xe8\xbf\x9c\xe5\xb0\x8f\xe4\xba\x8ecount_neg)\n        # print('debug, ==========================, count_pos is: {}'.format(count_pos))\n        beta = count_neg / (count_neg + count_pos)  ## e.g.  60000 / (60000 + 800) = 0.9868\n\n        pos_weight = beta / (1.0 - beta)  ## 0.9868 / (1.0 - 0.9868) = 0.9868 / 0.0132 = 74.75\n        cost = tf.nn.weighted_cross_entropy_with_logits(logits=logits, targets=label, pos_weight=pos_weight)\n        cost = tf.reduce_mean(cost * (1 - beta))\n\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe6\xa0\xb7\xe6\x9c\xac\xe4\xb8\xad1\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xe7\xad\x89\xe4\xba\x8e0\xef\xbc\x8c\xe9\x82\xa3\xe5\xb0\xb1\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xae\xa9 cost \xe4\xb8\xba 0\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba beta == 1 \xe6\x97\xb6\xef\xbc\x8c \xe9\x99\xa4\xe6\xb3\x95 pos_weight = beta / (1.0 - beta) \xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe6\x98\xaf\xe6\x97\xa0\xe7\xa9\xb7\xe5\xa4\xa7\n        zero = tf.equal(count_pos, 0.0)\n        final_cost = tf.where(zero, 0.0, cost) \n    return final_cost\n\n\ndef mobilenet_v2_style_hed(inputs, is_training):\n    assert const.use_batch_norm == True\n    assert const.use_kernel_regularizer == False\n\n    if const.use_kernel_regularizer:\n        weights_regularizer = tf.contrib.layers.l2_regularizer(scale=0.0001)\n    else:\n        weights_regularizer = None\n\n    ####################################################\n    func_blocks = mobilenet_v2_func_blocks(is_training)\n    _conv2d = func_blocks['conv2d']\n    _inverted_residual_block = func_blocks['inverted_residual_block']\n    filter_initializer = func_blocks['filter_initializer']\n    ####################################################\n\n    def _dsn_1x1_conv2d(inputs, filters):\n        kernel_size = [1, 1]\n        outputs = tf.layers.conv2d(inputs,\n                                   filters,\n                                   kernel_size, \n                                   padding='same', \n                                   activation=None, ## no activation\n                                   use_bias=False, \n                                   kernel_initializer=filter_initializer,\n                                   kernel_regularizer=weights_regularizer)\n\n        outputs = tf.layers.batch_normalization(outputs, training=is_training)\n\n        return outputs\n\n    def _output_1x1_conv2d(inputs, filters):\n        kernel_size = [1, 1]\n        outputs = tf.layers.conv2d(inputs,\n                                   filters,\n                                   kernel_size, \n                                   padding='same', \n                                   activation=None, ## no activation\n                                   use_bias=True, ## use bias\n                                   kernel_initializer=filter_initializer,\n                                   kernel_regularizer=weights_regularizer)\n\n        return outputs\n\n\n    def _dsn_deconv2d_with_upsample_factor(inputs, filters, upsample_factor):\n        ## https://github.com/s9xie/hed/blob/master/examples/hed/train_val.prototxt\n        ## \xe4\xbb\x8e\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8e\x9f\xe7\x89\x88\xe4\xbb\xa3\xe7\xa0\x81\xe9\x87\x8c\xe7\x9c\x8b\xef\xbc\x8c\xe6\x98\xaf\xe8\xbf\x99\xe6\xa0\xb7\xe8\xae\xa1\xe7\xae\x97 kernel_size \xe7\x9a\x84\n        kernel_size = [2 * upsample_factor, 2 * upsample_factor]\n        outputs = tf.layers.conv2d_transpose(inputs,\n                                             filters, \n                                             kernel_size, \n                                             strides=(upsample_factor, upsample_factor), \n                                             padding='same', \n                                             activation=None, ## no activation\n                                             use_bias=True, ## use bias\n                                             kernel_initializer=filter_initializer,\n                                             kernel_regularizer=weights_regularizer)\n\n        ## \xe6\xa6\x82\xe5\xbf\xb5\xe4\xb8\x8a\xe6\x9d\xa5\xe8\xaf\xb4\xef\xbc\x8cdeconv2d \xe5\xb7\xb2\xe7\xbb\x8f\xe6\x98\xaf\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba layer \xe4\xba\x86\xef\xbc\x8c\xe5\x8f\xaa\xe4\xb8\x8d\xe8\xbf\x87\xe6\x9c\x80\xe5\x90\x8e\xe8\xbf\x98\xe6\x9c\x89\xe4\xb8\x80\xe6\xad\xa5 1x1 \xe7\x9a\x84 conv2d \xe6\x8a\x8a 5 \xe4\xb8\xaa deconv2d \xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\x86\x8d\xe8\x9e\x8d\xe5\x90\x88\xe5\x88\xb0\xe4\xb8\x80\xe8\xb5\xb7\n        ## \xe6\x89\x80\xe4\xbb\xa5\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe5\x86\x8d\xe4\xbd\xbf\xe7\x94\xa8 batch normalization \xe4\xba\x86\n\n        return outputs\n\n\n    with tf.variable_scope('hed', 'hed', [inputs]):\n        end_points = {}\n        net = inputs\n        \n\n        ## mobilenet v2 as base net\n        with tf.variable_scope('mobilenet_v2'):\n            # \xe6\xa0\x87\xe5\x87\x86\xe7\x9a\x84 mobilenet v2 \xe9\x87\x8c\xe9\x9d\xa2\xe5\xb9\xb6\xe6\xb2\xa1\xe6\x9c\x89\xe8\xbf\x99\xe4\xb8\xa4\xe5\xb1\x82\xef\xbc\x8c\n            # \xe8\xbf\x99\xe9\x87\x8c\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe5\xbe\x97\xe5\x88\xb0\xe5\x92\x8c input image \xe7\x9b\xb8\xe5\x90\x8c size \xe7\x9a\x84 feature map \xe8\x80\x8c\xe5\xa2\x9e\xe5\x8a\xa0\xe7\x9a\x84\xe5\xb1\x82\n            net = _conv2d(net, 3, [3, 3], stride=1, scope='block0_0')\n            net = _conv2d(net, 6, [3, 3], stride=1, scope='block0_1')\n\n            dsn1 = net\n            net = _conv2d(net, 12, [3, 3], stride=2, scope='block0_2') # size/2\n\n            net = _inverted_residual_block(net, 6, stride=1, expansion=1, scope='block1_0')\n\n            dsn2 = net\n            net = _inverted_residual_block(net, 12, stride=2, scope='block2_0') # size/4\n            net = _inverted_residual_block(net, 12, stride=1, scope='block2_1')\n\n            dsn3 = net\n            net = _inverted_residual_block(net, 24, stride=2, scope='block3_0') # size/8\n            net = _inverted_residual_block(net, 24, stride=1, scope='block3_1') \n            net = _inverted_residual_block(net, 24, stride=1, scope='block3_2')\n\n            dsn4 = net\n            net = _inverted_residual_block(net, 48, stride=2, scope='block4_0') # size/16\n            net = _inverted_residual_block(net, 48, stride=1, scope='block4_1') \n            net = _inverted_residual_block(net, 48, stride=1, scope='block4_2') \n            net = _inverted_residual_block(net, 48, stride=1, scope='block4_3') \n\n            net = _inverted_residual_block(net, 64, stride=1, scope='block5_0') \n            net = _inverted_residual_block(net, 64, stride=1, scope='block5_1')\n            net = _inverted_residual_block(net, 64, stride=1, scope='block5_2')\n\n            dsn5 = net\n\n\n        ## dsn layers\n        with tf.variable_scope('dsn1'):\n            dsn1 = _dsn_1x1_conv2d(dsn1, 1)\n            # print('!! debug, dsn1 shape is: {}'.format(dsn1.get_shape()))\n            ## no need deconv2d\n\n        with tf.variable_scope('dsn2'):\n            dsn2 = _dsn_1x1_conv2d(dsn2, 1)\n            # print('!! debug, dsn2 shape is: {}'.format(dsn2.get_shape()))\n            dsn2 = _dsn_deconv2d_with_upsample_factor(dsn2, 1, upsample_factor=2)\n            # print('!! debug, dsn2 shape is: {}'.format(dsn2.get_shape()))\n\n        with tf.variable_scope('dsn3'):\n            dsn3 = _dsn_1x1_conv2d(dsn3, 1)\n            # print('!! debug, dsn3 shape is: {}'.format(dsn3.get_shape()))\n            dsn3 = _dsn_deconv2d_with_upsample_factor(dsn3, 1, upsample_factor=4)\n            # print('!! debug, dsn3 shape is: {}'.format(dsn3.get_shape()))\n\n        with tf.variable_scope('dsn4'):\n            dsn4 = _dsn_1x1_conv2d(dsn4, 1)\n            # print('!! debug, dsn4 shape is: {}'.format(dsn4.get_shape()))\n            dsn4 = _dsn_deconv2d_with_upsample_factor(dsn4, 1, upsample_factor=8)\n            # print('!! debug, dsn4 shape is: {}'.format(dsn4.get_shape()))\n\n        with tf.variable_scope('dsn5'):\n            dsn5 = _dsn_1x1_conv2d(dsn5, 1)\n            # print('!! debug, dsn5 shape is: {}'.format(dsn5.get_shape()))\n            dsn5 = _dsn_deconv2d_with_upsample_factor(dsn5, 1, upsample_factor=16)\n            # print('!! debug, dsn5 shape is: {}'.format(dsn5.get_shape()))\n\n        # dsn fuse\n        with tf.variable_scope('dsn_fuse'):\n            dsn_fuse = tf.concat([dsn1, dsn2, dsn3, dsn4, dsn5], 3)\n            # print('debug, dsn_fuse shape is: {}'.format(dsn_fuse.get_shape()))\n            dsn_fuse = _output_1x1_conv2d(dsn_fuse, 1)\n            # print('debug, dsn_fuse shape is: {}'.format(dsn_fuse.get_shape()))\n\n    return dsn_fuse, dsn1, dsn2, dsn3, dsn4, dsn5\n\n\ndef mobilenet_v2_func_blocks(is_training):\n    assert const.use_batch_norm == True\n\n    filter_initializer = tf.contrib.layers.xavier_initializer()\n    activation_func = tf.nn.relu6\n\n    def conv2d(inputs, filters, kernel_size, stride, scope=''):\n        with tf.variable_scope(scope):\n            with tf.variable_scope('conv2d'):\n                outputs = tf.layers.conv2d(inputs,\n                                           filters,\n                                           kernel_size,\n                                           strides=(stride, stride),\n                                           padding='same',\n                                           activation=None,\n                                           use_bias=False,\n                                           kernel_initializer=filter_initializer)\n                '''\n                https://github.com/udacity/deep-learning/blob/master/batch-norm/Batch_Normalization_Solutions.ipynb\n                https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\n                '''\n                outputs = tf.layers.batch_normalization(outputs, training=is_training)\n                outputs = tf.nn.relu(outputs)\n            return outputs\n\n    def _1x1_conv2d(inputs, filters, stride):\n        kernel_size = [1, 1]\n        with tf.variable_scope('1x1_conv2d'):\n            outputs = tf.layers.conv2d(inputs,\n                                       filters,\n                                       kernel_size,\n                                       strides=(stride, stride),\n                                       padding='same',\n                                       activation=None,\n                                       use_bias=False,\n                                       kernel_initializer=filter_initializer)\n\n            outputs = tf.layers.batch_normalization(outputs, training=is_training)\n            # no activation_func\n        return outputs\n\n    def expansion_conv2d(inputs, expansion, stride):\n        input_shape = inputs.get_shape().as_list()\n        assert len(input_shape) == 4\n        filters = input_shape[3] * expansion\n\n        kernel_size = [1, 1]\n        with tf.variable_scope('expansion_1x1_conv2d'):\n            outputs = tf.layers.conv2d(inputs,\n                                       filters,\n                                       kernel_size,\n                                       strides=(stride, stride),\n                                       padding='same',\n                                       activation=None,\n                                       use_bias=False,\n                                       kernel_initializer=filter_initializer)\n\n            outputs = tf.layers.batch_normalization(outputs, training=is_training)\n            outputs = activation_func(outputs)\n        return outputs\n\n    def projection_conv2d(inputs, filters, stride):\n        kernel_size = [1, 1]\n        with tf.variable_scope('projection_1x1_conv2d'):\n            outputs = tf.layers.conv2d(inputs,\n                                       filters,\n                                       kernel_size,\n                                       strides=(stride, stride),\n                                       padding='same',\n                                       activation=None,\n                                       use_bias=False,\n                                       kernel_initializer=filter_initializer)\n\n            outputs = tf.layers.batch_normalization(outputs, training=is_training)\n            # no activation_func\n        return outputs\n\n    def depthwise_conv2d(inputs,\n                         depthwise_conv_kernel_size,\n                         stride):\n        with tf.variable_scope('depthwise_conv2d'):\n            outputs = tf.contrib.layers.separable_conv2d(\n                inputs,\n                None,  # https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.py\n                depthwise_conv_kernel_size,\n                depth_multiplier=1,\n                stride=(stride, stride),\n                padding='SAME',\n                activation_fn=None,\n                weights_initializer=filter_initializer,\n                biases_initializer=None)\n\n            outputs = tf.layers.batch_normalization(outputs, training=is_training)\n            outputs = activation_func(outputs)\n\n        return outputs\n\n    def avg_pool2d(inputs, scope=''):\n        inputs_shape = inputs.get_shape().as_list()\n        assert len(inputs_shape) == 4\n\n        pool_height = inputs_shape[1]\n        pool_width = inputs_shape[2]\n\n        with tf.variable_scope(scope):\n            outputs = tf.layers.average_pooling2d(inputs,\n                                                  [pool_height, pool_width],\n                                                  strides=(1, 1),\n                                                  padding='valid')\n\n        return outputs\n\n    def inverted_residual_block(inputs,\n                                filters,\n                                stride,\n                                expansion=6,\n                                scope=''):\n        assert stride == 1 or stride == 2\n\n        depthwise_conv_kernel_size = [3, 3]\n        pointwise_conv_filters = filters\n\n        with tf.variable_scope(scope):\n            net = inputs\n            net = expansion_conv2d(net, expansion, stride=1)\n            net = depthwise_conv2d(net, depthwise_conv_kernel_size, stride=stride)\n            net = projection_conv2d(net, pointwise_conv_filters, stride=1)\n\n            if stride == 1:\n                # print('----------------- test, net.get_shape().as_list()[3] = %r' % net.get_shape().as_list()[3])\n                # print('----------------- test, inputs.get_shape().as_list()[3] = %r' % inputs.get_shape().as_list()[3])\n                # \xe5\xa6\x82\xe6\x9e\x9c net.get_shape().as_list()[3] != inputs.get_shape().as_list()[3]\n                # \xe5\x80\x9f\xe5\x8a\xa9\xe4\xb8\x80\xe4\xb8\xaa 1x1 \xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xae\xa9\xe4\xbb\x96\xe4\xbb\xac\xe7\x9a\x84 channels \xe7\x9b\xb8\xe7\xad\x89\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\x86\x8d\xe7\x9b\xb8\xe5\x8a\xa0\n                if net.get_shape().as_list()[3] != inputs.get_shape().as_list()[3]:\n                    inputs = _1x1_conv2d(inputs, net.get_shape().as_list()[3], stride=1)\n\n                net = net + inputs\n                return net\n            else:\n                # stride == 2\n                return net\n\n    func_blocks = {'conv2d': conv2d,\n                   'inverted_residual_block': inverted_residual_block,\n                   'avg_pool2d': avg_pool2d,\n                   'filter_initializer': filter_initializer, 'activation_func': activation_func}\n\n    return func_blocks\n"""
edge_detection/image_threshold.py,0,"b""#!/usr/bin/python\n#coding=utf8\n\n# \xe5\x9b\xbe\xe5\x83\x8f\xe4\xba\x8c\xe5\x80\xbc\xe5\x8c\x96\xe5\xb7\xa5\xe5\x85\xb7\n\nfrom tensorflow import flags\nfrom skimage import color\nfrom skimage import io\nfrom skimage import transform\nimport numpy as np\nfrom skimage import filters\nimport os\n\nflags.DEFINE_string('input_img', '',\n                    'input image.')\nflags.DEFINE_string('output_img', '',\n                    'output image.')\n\nFLAGS = flags.FLAGS\n\nif not os.path.exists(FLAGS.input_img):\n    print('--input_img invalid')\n    exit()\n\nif FLAGS.output_img == '':\n    print('--output_img invalid')\n    exit()\n\n\ndef threshold(ann_path, save_path):\n    img = io.imread(ann_path, mode='RGB')\n    img = color.rgb2gray(img)\n    thresh = filters.threshold_otsu(img)\n    img = np.where(img > thresh, [255], [0])\n    img = np.clip(img, 0, 255).astype(np.uint8)\n    io.imsave(save_path, img)\n\n\nif __name__ == '__main__':\n    threshold(FLAGS.input_img, FLAGS.output_img)\n"""
edge_detection/simple_maker.py,0,"b""from tensorflow import flags\nfrom skimage import color\nfrom skimage import io\nfrom skimage import transform\nimport numpy as np\nfrom skimage import filters\nimport os\nimport math\n\n# todo\n\nif __name__ == '__main__':\n    background = io.imread('make_test/background.jpg')\n    background_shape = background.shape\n    background_width = background_shape[0]\n    background_height = background_shape[1]\n\n    rect = io.imread('make_test/rect.png')\n    rect_shape = rect.shape\n    rect_width = rect_shape[0]\n    rect_height = rect_shape[1]\n\n    src = np.array([[0, 0], [400, 400], [800, 400], [400, 0]])\n    dst = np.array([[0, 0], [0, rect_height], [rect_width, rect_height], [rect_width, 0]])\n\n    tform3 = transform.ProjectiveTransform()\n    tform3.estimate(src, dst)\n\n    warped = transform.warp(rect, tform3, output_shape=(background_width, background_height))\n\n    io.imsave('make_test/rect_tr.png', warped)"""
edge_detection/util.py,0,"b""#!/usr/bin/python\n#coding=utf8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport scipy.misc\nimport numpy as np\n\n\ndef save_img(out_path, img):\n    img = np.clip(img, 0, 255).astype(np.uint8)\n    scipy.misc.imsave(out_path, img)\n\n\ndef load_sample_from_csv(csv_file):\n    images = []\n    annotations = []\n    with open(csv_file, 'r') as f:\n        for line in f.readlines():\n            a_list = line.split(', ')\n            image_l = a_list[0]\n            annotation_l = a_list[1].replace('\\n', '')\n            images.append(image_l)\n            annotations.append(annotation_l)\n    return images, annotations\n\n\n"""
