file_path,api_count,code
__init__.py,0,b'from .segmentation_models import *\n'
setup.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note: To use the \'upload\' functionality of this file, you must:\n#   $ pip install twine\n\nimport io\nimport os\nimport sys\nfrom shutil import rmtree\n\nfrom setuptools import find_packages, setup, Command\n\n# Package meta-data.\nNAME = \'segmentation_models\'\nDESCRIPTION = \'Image segmentation models with pre-trained backbones with Keras.\'\nURL = \'https://github.com/qubvel/segmentation_models\'\nEMAIL = \'qubvel@gmail.com\'\nAUTHOR = \'Pavel Yakubovskiy\'\nREQUIRES_PYTHON = \'>=3.0.0\'\nVERSION = None\n\n# The rest you shouldn\'t have to touch too much :)\n# ------------------------------------------------\n# Except, perhaps the License and Trove Classifiers!\n# If you do change the License, remember to change the Trove Classifier for that!\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\n# What packages are required for this module to be executed?\ntry:\n    with open(os.path.join(here, \'requirements.txt\'), encoding=\'utf-8\') as f:\n        REQUIRED = f.read().split(\'\\n\')\nexcept:\n    REQUIRED = []\n\n# What packages are optional?\nEXTRAS = {\n    \'tests\': [\'pytest\', \'scikit-image\'],\n}\n\n# Import the README and use it as the long-description.\n# Note: this will only work if \'README.md\' is present in your MANIFEST.in file!\ntry:\n    with io.open(os.path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n        long_description = \'\\n\' + f.read()\nexcept FileNotFoundError:\n    long_description = DESCRIPTION\n\n# Load the package\'s __version__.py module as a dictionary.\nabout = {}\nif not VERSION:\n    with open(os.path.join(here, NAME, \'__version__.py\')) as f:\n        exec(f.read(), about)\nelse:\n    about[\'__version__\'] = VERSION\n\n\nclass UploadCommand(Command):\n    """"""Support setup.py upload.""""""\n\n    description = \'Build and publish the package.\'\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        """"""Prints things in bold.""""""\n        print(s)\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status(\'Removing previous builds...\')\n            rmtree(os.path.join(here, \'dist\'))\n        except OSError:\n            pass\n\n        self.status(\'Building Source and Wheel (universal) distribution...\')\n        os.system(\'{0} setup.py sdist bdist_wheel --universal\'.format(sys.executable))\n\n        self.status(\'Uploading the package to PyPI via Twine...\')\n        os.system(\'twine upload dist/*\')\n\n        self.status(\'Pushing git tags...\')\n        os.system(\'git tag v{0}\'.format(about[\'__version__\']))\n        os.system(\'git push --tags\')\n\n        sys.exit()\n\n\n# Where the magic happens:\nsetup(\n    name=NAME,\n    version=about[\'__version__\'],\n    description=DESCRIPTION,\n    long_description=long_description,\n    long_description_content_type=\'text/x-rst\',\n    author=AUTHOR,\n    author_email=EMAIL,\n    python_requires=REQUIRES_PYTHON,\n    url=URL,\n    packages=find_packages(exclude=(\'tests\', \'docs\', \'images\', \'examples\')),\n    # If your package is a single module, use this instead of \'packages\':\n    # py_modules=[\'mypackage\'],\n\n    # entry_points={\n    #     \'console_scripts\': [\'mycli=mymodule:cli\'],\n    # },\n    install_requires=REQUIRED,\n    extras_require=EXTRAS,\n    include_package_data=True,\n    license=\'MIT\',\n    classifiers=[\n        # Trove classifiers\n        # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers\n        \'License :: OSI Approved :: MIT License\',\n        \'Programming Language :: Python\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: Implementation :: CPython\',\n        \'Programming Language :: Python :: Implementation :: PyPy\'\n    ],\n    # $ setup.py publish support.\n    cmdclass={\n        \'upload\': UploadCommand,\n    },\n)'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n# -- Project information -----------------------------------------------------\nimport sys\nsys.path.append(\'..\')\n\nproject = u\'Segmentation Models\'\ncopyright = u\'2018, Pavel Yakubovskiy\'\nauthor = u\'Pavel Yakubovskiy\'\n\n# The short X.Y version\nversion = u\'\'\n# The full version, including alpha/beta/rc tags\nrelease = u\'0.1.2\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.napoleon\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [u\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n# -- Theme setup -------------------------------------------------------------\n\nimport sphinx_rtd_theme\n\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'SegmentationModelsdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'SegmentationModels.tex\', u\'Segmentation Models Documentation\',\n     u\'Pavel Yakubovskiy\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'segmentationmodels\', u\'Segmentation Models Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'SegmentationModels\', u\'Segmentation Models Documentation\',\n     author, \'SegmentationModels\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n\n# -- Extension configuration -------------------------------------------------\n\nautodoc_mock_imports = [\n    \'skimage\',\n    \'keras\',\n    \'tensorflow\',\n    \'efficientnet\',\n    \'classification_models\',\n    \'keras_applications\',\n]\n'"
segmentation_models/__init__.py,2,"b'import os\nimport functools\nfrom .__version__ import __version__\nfrom . import base\n\n_KERAS_FRAMEWORK_NAME = \'keras\'\n_TF_KERAS_FRAMEWORK_NAME = \'tf.keras\'\n\n_DEFAULT_KERAS_FRAMEWORK = _KERAS_FRAMEWORK_NAME\n_KERAS_FRAMEWORK = None\n_KERAS_BACKEND = None\n_KERAS_LAYERS = None\n_KERAS_MODELS = None\n_KERAS_UTILS = None\n_KERAS_LOSSES = None\n\n\ndef inject_global_losses(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        kwargs[\'losses\'] = _KERAS_LOSSES\n        return func(*args, **kwargs)\n\n    return wrapper\n\n\ndef inject_global_submodules(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        kwargs[\'backend\'] = _KERAS_BACKEND\n        kwargs[\'layers\'] = _KERAS_LAYERS\n        kwargs[\'models\'] = _KERAS_MODELS\n        kwargs[\'utils\'] = _KERAS_UTILS\n        return func(*args, **kwargs)\n\n    return wrapper\n\n\ndef filter_kwargs(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        new_kwargs = {k: v for k, v in kwargs.items() if k in [\'backend\', \'layers\', \'models\', \'utils\']}\n        return func(*args, **new_kwargs)\n\n    return wrapper\n\n\ndef framework():\n    """"""Return name of Segmentation Models framework""""""\n    return _KERAS_FRAMEWORK\n\n\ndef set_framework(name):\n    """"""Set framework for Segmentation Models\n\n    Args:\n        name (str): one of ``keras``, ``tf.keras``, case insensitive.\n\n    Raises:\n        ValueError: in case of incorrect framework name.\n        ImportError: in case framework is not installed.\n\n    """"""\n    name = name.lower()\n\n    if name == _KERAS_FRAMEWORK_NAME:\n        import keras\n        import efficientnet.keras  # init custom objects\n    elif name == _TF_KERAS_FRAMEWORK_NAME:\n        from tensorflow import keras\n        import efficientnet.tfkeras  # init custom objects\n    else:\n        raise ValueError(\'Not correct module name `{}`, use `{}` or `{}`\'.format(\n            name, _KERAS_FRAMEWORK_NAME, _TF_KERAS_FRAMEWORK_NAME))\n\n    global _KERAS_BACKEND, _KERAS_LAYERS, _KERAS_MODELS\n    global _KERAS_UTILS, _KERAS_LOSSES, _KERAS_FRAMEWORK\n\n    _KERAS_FRAMEWORK = name\n    _KERAS_BACKEND = keras.backend\n    _KERAS_LAYERS = keras.layers\n    _KERAS_MODELS = keras.models\n    _KERAS_UTILS = keras.utils\n    _KERAS_LOSSES = keras.losses\n\n    # allow losses/metrics get keras submodules\n    base.KerasObject.set_submodules(\n        backend=keras.backend,\n        layers=keras.layers,\n        models=keras.models,\n        utils=keras.utils,\n    )\n\n\n# set default framework\n_framework = os.environ.get(\'SM_FRAMEWORK\', _DEFAULT_KERAS_FRAMEWORK)\ntry:\n    set_framework(_framework)\nexcept ImportError:\n    other = _TF_KERAS_FRAMEWORK_NAME if _framework == _KERAS_FRAMEWORK_NAME else _KERAS_FRAMEWORK_NAME\n    set_framework(other)\n\nprint(\'Segmentation Models: using `{}` framework.\'.format(_KERAS_FRAMEWORK))\n\n# import helper modules\nfrom . import losses\nfrom . import metrics\nfrom . import utils\n\n# wrap segmentation models with framework modules\nfrom .backbones.backbones_factory import Backbones\nfrom .models.unet import Unet as _Unet\nfrom .models.pspnet import PSPNet as _PSPNet\nfrom .models.linknet import Linknet as _Linknet\nfrom .models.fpn import FPN as _FPN\n\nUnet = inject_global_submodules(_Unet)\nPSPNet = inject_global_submodules(_PSPNet)\nLinknet = inject_global_submodules(_Linknet)\nFPN = inject_global_submodules(_FPN)\nget_available_backbone_names = Backbones.models_names\n\n\ndef get_preprocessing(name):\n    preprocess_input = Backbones.get_preprocessing(name)\n    # add bakcend, models, layers, utils submodules in kwargs\n    preprocess_input = inject_global_submodules(preprocess_input)\n    # delete other kwargs\n    # keras-applications preprocessing raise an error if something\n    # except `backend`, `layers`, `models`, `utils` passed in kwargs\n    preprocess_input = filter_kwargs(preprocess_input)\n    return preprocess_input\n\n\n__all__ = [\n    \'Unet\', \'PSPNet\', \'FPN\', \'Linknet\',\n    \'set_framework\', \'framework\',\n    \'get_preprocessing\', \'get_available_backbone_names\',\n    \'losses\', \'metrics\', \'utils\',\n    \'__version__\',\n]\n'"
segmentation_models/__version__.py,0,"b""VERSION = (1, 0, 1)\n\n__version__ = '.'.join(map(str, VERSION))\n"""
segmentation_models/losses.py,0,"b'from .base import Loss\nfrom .base import functional as F\n\nSMOOTH = 1e-5\n\n\nclass JaccardLoss(Loss):\n    r""""""Creates a criterion to measure Jaccard loss:\n\n    .. math:: L(A, B) = 1 - \\frac{A \\cap B}{A \\cup B}\n\n    Args:\n        class_weights: Array (``np.array``) of class weights (``len(weights) = num_classes``).\n        class_indexes: Optional integer or list of integers, classes to consider, if ``None`` all classes are used.\n        per_image: If ``True`` loss is calculated for each image in batch and then averaged,\n            else loss is calculated for the whole batch.\n        smooth: Value to avoid division by zero.\n\n    Returns:\n         A callable ``jaccard_loss`` instance. Can be used in ``model.compile(...)`` function\n         or combined with other losses.\n\n    Example:\n\n    .. code:: python\n\n        loss = JaccardLoss()\n        model.compile(\'SGD\', loss=loss)\n    """"""\n\n    def __init__(self, class_weights=None, class_indexes=None, per_image=False, smooth=SMOOTH):\n        super().__init__(name=\'jaccard_loss\')\n        self.class_weights = class_weights if class_weights is not None else 1\n        self.class_indexes = class_indexes\n        self.per_image = per_image\n        self.smooth = smooth\n\n    def __call__(self, gt, pr):\n        return 1 - F.iou_score(\n            gt,\n            pr,\n            class_weights=self.class_weights,\n            class_indexes=self.class_indexes,\n            smooth=self.smooth,\n            per_image=self.per_image,\n            threshold=None,\n            **self.submodules\n        )\n\n\nclass DiceLoss(Loss):\n    r""""""Creates a criterion to measure Dice loss:\n\n    .. math:: L(precision, recall) = 1 - (1 + \\beta^2) \\frac{precision \\cdot recall}\n        {\\beta^2 \\cdot precision + recall}\n\n    The formula in terms of *Type I* and *Type II* errors:\n\n    .. math:: L(tp, fp, fn) = \\frac{(1 + \\beta^2) \\cdot tp} {(1 + \\beta^2) \\cdot fp + \\beta^2 \\cdot fn + fp}\n\n    where:\n         - tp - true positives;\n         - fp - false positives;\n         - fn - false negatives;\n\n    Args:\n        beta: Float or integer coefficient for precision and recall balance.\n        class_weights: Array (``np.array``) of class weights (``len(weights) = num_classes``).\n        class_indexes: Optional integer or list of integers, classes to consider, if ``None`` all classes are used.\n        per_image: If ``True`` loss is calculated for each image in batch and then averaged,\n        else loss is calculated for the whole batch.\n        smooth: Value to avoid division by zero.\n\n    Returns:\n        A callable ``dice_loss`` instance. Can be used in ``model.compile(...)`` function`\n        or combined with other losses.\n\n    Example:\n\n    .. code:: python\n\n        loss = DiceLoss()\n        model.compile(\'SGD\', loss=loss)\n    """"""\n\n    def __init__(self, beta=1, class_weights=None, class_indexes=None, per_image=False, smooth=SMOOTH):\n        super().__init__(name=\'dice_loss\')\n        self.beta = beta\n        self.class_weights = class_weights if class_weights is not None else 1\n        self.class_indexes = class_indexes\n        self.per_image = per_image\n        self.smooth = smooth\n\n    def __call__(self, gt, pr):\n        return 1 - F.f_score(\n            gt,\n            pr,\n            beta=self.beta,\n            class_weights=self.class_weights,\n            class_indexes=self.class_indexes,\n            smooth=self.smooth,\n            per_image=self.per_image,\n            threshold=None,\n            **self.submodules\n        )\n\n\nclass BinaryCELoss(Loss):\n    """"""Creates a criterion that measures the Binary Cross Entropy between the\n    ground truth (gt) and the prediction (pr).\n\n    .. math:: L(gt, pr) = - gt \\cdot \\log(pr) - (1 - gt) \\cdot \\log(1 - pr)\n\n    Returns:\n        A callable ``binary_crossentropy`` instance. Can be used in ``model.compile(...)`` function\n        or combined with other losses.\n\n    Example:\n\n    .. code:: python\n\n        loss = BinaryCELoss()\n        model.compile(\'SGD\', loss=loss)\n    """"""\n\n    def __init__(self):\n        super().__init__(name=\'binary_crossentropy\')\n\n    def __call__(self, gt, pr):\n        return F.binary_crossentropy(gt, pr, **self.submodules)\n\n\nclass CategoricalCELoss(Loss):\n    """"""Creates a criterion that measures the Categorical Cross Entropy between the\n    ground truth (gt) and the prediction (pr).\n\n    .. math:: L(gt, pr) = - gt \\cdot \\log(pr)\n\n    Args:\n        class_weights: Array (``np.array``) of class weights (``len(weights) = num_classes``).\n        class_indexes: Optional integer or list of integers, classes to consider, if ``None`` all classes are used.\n\n    Returns:\n        A callable ``categorical_crossentropy`` instance. Can be used in ``model.compile(...)`` function\n        or combined with other losses.\n\n    Example:\n\n    .. code:: python\n\n        loss = CategoricalCELoss()\n        model.compile(\'SGD\', loss=loss)\n    """"""\n\n    def __init__(self, class_weights=None, class_indexes=None):\n        super().__init__(name=\'categorical_crossentropy\')\n        self.class_weights = class_weights if class_weights is not None else 1\n        self.class_indexes = class_indexes\n\n    def __call__(self, gt, pr):\n        return F.categorical_crossentropy(\n            gt,\n            pr,\n            class_weights=self.class_weights,\n            class_indexes=self.class_indexes,\n            **self.submodules\n        )\n\n\nclass CategoricalFocalLoss(Loss):\n    r""""""Creates a criterion that measures the Categorical Focal Loss between the\n    ground truth (gt) and the prediction (pr).\n\n    .. math:: L(gt, pr) = - gt \\cdot \\alpha \\cdot (1 - pr)^\\gamma \\cdot \\log(pr)\n\n    Args:\n        alpha: Float or integer, the same as weighting factor in balanced cross entropy, default 0.25.\n        gamma: Float or integer, focusing parameter for modulating factor (1 - p), default 2.0.\n        class_indexes: Optional integer or list of integers, classes to consider, if ``None`` all classes are used.\n\n    Returns:\n        A callable ``categorical_focal_loss`` instance. Can be used in ``model.compile(...)`` function\n        or combined with other losses.\n\n    Example:\n\n        .. code:: python\n\n            loss = CategoricalFocalLoss()\n            model.compile(\'SGD\', loss=loss)\n    """"""\n\n    def __init__(self, alpha=0.25, gamma=2., class_indexes=None):\n        super().__init__(name=\'focal_loss\')\n        self.alpha = alpha\n        self.gamma = gamma\n        self.class_indexes = class_indexes\n\n    def __call__(self, gt, pr):\n        return F.categorical_focal_loss(\n            gt,\n            pr,\n            alpha=self.alpha,\n            gamma=self.gamma,\n            class_indexes=self.class_indexes,\n            **self.submodules\n        )\n\n\nclass BinaryFocalLoss(Loss):\n    r""""""Creates a criterion that measures the Binary Focal Loss between the\n    ground truth (gt) and the prediction (pr).\n\n    .. math:: L(gt, pr) = - gt \\alpha (1 - pr)^\\gamma \\log(pr) - (1 - gt) \\alpha pr^\\gamma \\log(1 - pr)\n\n    Args:\n        alpha: Float or integer, the same as weighting factor in balanced cross entropy, default 0.25.\n        gamma: Float or integer, focusing parameter for modulating factor (1 - p), default 2.0.\n\n    Returns:\n        A callable ``binary_focal_loss`` instance. Can be used in ``model.compile(...)`` function\n        or combined with other losses.\n\n    Example:\n\n    .. code:: python\n\n        loss = BinaryFocalLoss()\n        model.compile(\'SGD\', loss=loss)\n    """"""\n\n    def __init__(self, alpha=0.25, gamma=2.):\n        super().__init__(name=\'binary_focal_loss\')\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def __call__(self, gt, pr):\n        return F.binary_focal_loss(gt, pr, alpha=self.alpha, gamma=self.gamma, **self.submodules)\n\n\n# aliases\njaccard_loss = JaccardLoss()\ndice_loss = DiceLoss()\n\nbinary_focal_loss = BinaryFocalLoss()\ncategorical_focal_loss = CategoricalFocalLoss()\n\nbinary_crossentropy = BinaryCELoss()\ncategorical_crossentropy = CategoricalCELoss()\n\n# loss combinations\nbce_dice_loss = binary_crossentropy + dice_loss\nbce_jaccard_loss = binary_crossentropy + jaccard_loss\n\ncce_dice_loss = categorical_crossentropy + dice_loss\ncce_jaccard_loss = categorical_crossentropy + jaccard_loss\n\nbinary_focal_dice_loss = binary_focal_loss + dice_loss\nbinary_focal_jaccard_loss = binary_focal_loss + jaccard_loss\n\ncategorical_focal_dice_loss = categorical_focal_loss + dice_loss\ncategorical_focal_jaccard_loss = categorical_focal_loss + jaccard_loss\n'"
segmentation_models/metrics.py,0,"b'from .base import Metric\nfrom .base import functional as F\n\nSMOOTH = 1e-5\n\n\nclass IOUScore(Metric):\n    r"""""" The `Jaccard index`_, also known as Intersection over Union and the Jaccard similarity coefficient\n    (originally coined coefficient de communaut\xc3\xa9 by Paul Jaccard), is a statistic used for comparing the\n    similarity and diversity of sample sets. The Jaccard coefficient measures similarity between finite sample sets,\n    and is defined as the size of the intersection divided by the size of the union of the sample sets:\n\n    .. math:: J(A, B) = \\frac{A \\cap B}{A \\cup B}\n\n    Args:\n        class_weights: 1. or ``np.array`` of class weights (``len(weights) = num_classes``).\n        class_indexes: Optional integer or list of integers, classes to consider, if ``None`` all classes are used.\n        smooth: value to avoid division by zero\n        per_image: if ``True``, metric is calculated as mean over images in batch (B),\n            else over whole batch\n        threshold: value to round predictions (use ``>`` comparison), if ``None`` prediction will not be round\n\n    Returns:\n       A callable ``iou_score`` instance. Can be used in ``model.compile(...)`` function.\n\n    .. _`Jaccard index`: https://en.wikipedia.org/wiki/Jaccard_index\n\n    Example:\n\n    .. code:: python\n\n        metric = IOUScore()\n        model.compile(\'SGD\', loss=loss, metrics=[metric])\n    """"""\n\n    def __init__(\n            self,\n            class_weights=None,\n            class_indexes=None,\n            threshold=None,\n            per_image=False,\n            smooth=SMOOTH,\n            name=None,\n    ):\n        name = name or \'iou_score\'\n        super().__init__(name=name)\n        self.class_weights = class_weights if class_weights is not None else 1\n        self.class_indexes = class_indexes\n        self.threshold = threshold\n        self.per_image = per_image\n        self.smooth = smooth\n\n    def __call__(self, gt, pr):\n        return F.iou_score(\n            gt,\n            pr,\n            class_weights=self.class_weights,\n            class_indexes=self.class_indexes,\n            smooth=self.smooth,\n            per_image=self.per_image,\n            threshold=self.threshold,\n            **self.submodules\n        )\n\n\nclass FScore(Metric):\n    r""""""The F-score (Dice coefficient) can be interpreted as a weighted average of the precision and recall,\n    where an F-score reaches its best value at 1 and worst score at 0.\n    The relative contribution of ``precision`` and ``recall`` to the F1-score are equal.\n    The formula for the F score is:\n\n    .. math:: F_\\beta(precision, recall) = (1 + \\beta^2) \\frac{precision \\cdot recall}\n        {\\beta^2 \\cdot precision + recall}\n\n    The formula in terms of *Type I* and *Type II* errors:\n\n    .. math:: L(tp, fp, fn) = \\frac{(1 + \\beta^2) \\cdot tp} {(1 + \\beta^2) \\cdot fp + \\beta^2 \\cdot fn + fp}\n\n    where:\n         - tp - true positives;\n         - fp - false positives;\n         - fn - false negatives;\n\n    Args:\n        beta: Integer of float f-score coefficient to balance precision and recall.\n        class_weights: 1. or ``np.array`` of class weights (``len(weights) = num_classes``)\n        class_indexes: Optional integer or list of integers, classes to consider, if ``None`` all classes are used.\n        smooth: Float value to avoid division by zero.\n        per_image: If ``True``, metric is calculated as mean over images in batch (B),\n            else over whole batch.\n        threshold: Float value to round predictions (use ``>`` comparison), if ``None`` prediction will not be round.\n        name: Optional string, if ``None`` default ``f{beta}-score`` name is used.\n\n    Returns:\n        A callable ``f_score`` instance. Can be used in ``model.compile(...)`` function.\n\n    Example:\n\n    .. code:: python\n\n        metric = FScore()\n        model.compile(\'SGD\', loss=loss, metrics=[metric])\n    """"""\n\n    def __init__(\n            self,\n            beta=1,\n            class_weights=None,\n            class_indexes=None,\n            threshold=None,\n            per_image=False,\n            smooth=SMOOTH,\n            name=None,\n    ):\n        name = name or \'f{}-score\'.format(beta)\n        super().__init__(name=name)\n        self.beta = beta\n        self.class_weights = class_weights if class_weights is not None else 1\n        self.class_indexes = class_indexes\n        self.threshold = threshold\n        self.per_image = per_image\n        self.smooth = smooth\n\n    def __call__(self, gt, pr):\n        return F.f_score(\n            gt,\n            pr,\n            beta=self.beta,\n            class_weights=self.class_weights,\n            class_indexes=self.class_indexes,\n            smooth=self.smooth,\n            per_image=self.per_image,\n            threshold=self.threshold,\n            **self.submodules\n        )\n\n\nclass Precision(Metric):\n    r""""""Creates a criterion that measures the Precision between the\n    ground truth (gt) and the prediction (pr).\n\n    .. math:: F_\\beta(tp, fp) = \\frac{tp} {(tp + fp)}\n\n    where:\n         - tp - true positives;\n         - fp - false positives;\n\n    Args:\n        class_weights: 1. or ``np.array`` of class weights (``len(weights) = num_classes``).\n        class_indexes: Optional integer or list of integers, classes to consider, if ``None`` all classes are used.\n        smooth: Float value to avoid division by zero.\n        per_image: If ``True``, metric is calculated as mean over images in batch (B),\n            else over whole batch.\n        threshold: Float value to round predictions (use ``>`` comparison), if ``None`` prediction will not be round.\n        name: Optional string, if ``None`` default ``precision`` name is used.\n\n    Returns:\n        A callable ``precision`` instance. Can be used in ``model.compile(...)`` function.\n\n    Example:\n\n    .. code:: python\n\n        metric = Precision()\n        model.compile(\'SGD\', loss=loss, metrics=[metric])\n    """"""\n\n    def __init__(\n            self,\n            class_weights=None,\n            class_indexes=None,\n            threshold=None,\n            per_image=False,\n            smooth=SMOOTH,\n            name=None,\n    ):\n        name = name or \'precision\'\n        super().__init__(name=name)\n        self.class_weights = class_weights if class_weights is not None else 1\n        self.class_indexes = class_indexes\n        self.threshold = threshold\n        self.per_image = per_image\n        self.smooth = smooth\n\n    def __call__(self, gt, pr):\n        return F.precision(\n            gt,\n            pr,\n            class_weights=self.class_weights,\n            class_indexes=self.class_indexes,\n            smooth=self.smooth,\n            per_image=self.per_image,\n            threshold=self.threshold,\n            **self.submodules\n        )\n\n\nclass Recall(Metric):\n    r""""""Creates a criterion that measures the Precision between the\n    ground truth (gt) and the prediction (pr).\n\n    .. math:: F_\\beta(tp, fn) = \\frac{tp} {(tp + fn)}\n\n    where:\n         - tp - true positives;\n         - fn - false negatives;\n\n    Args:\n        class_weights: 1. or ``np.array`` of class weights (``len(weights) = num_classes``).\n        class_indexes: Optional integer or list of integers, classes to consider, if ``None`` all classes are used.\n        smooth: Float value to avoid division by zero.\n        per_image: If ``True``, metric is calculated as mean over images in batch (B),\n            else over whole batch.\n        threshold: Float value to round predictions (use ``>`` comparison), if ``None`` prediction will not be round.\n        name: Optional string, if ``None`` default ``recall`` name is used.\n\n    Returns:\n        A callable ``recall`` instance. Can be used in ``model.compile(...)`` function.\n\n    Example:\n\n    .. code:: python\n\n        metric = Precision()\n        model.compile(\'SGD\', loss=loss, metrics=[metric])\n    """"""\n\n    def __init__(\n            self,\n            class_weights=None,\n            class_indexes=None,\n            threshold=None,\n            per_image=False,\n            smooth=SMOOTH,\n            name=None,\n    ):\n        name = name or \'recall\'\n        super().__init__(name=name)\n        self.class_weights = class_weights if class_weights is not None else 1\n        self.class_indexes = class_indexes\n        self.threshold = threshold\n        self.per_image = per_image\n        self.smooth = smooth\n\n    def __call__(self, gt, pr):\n        return F.recall(\n            gt,\n            pr,\n            class_weights=self.class_weights,\n            class_indexes=self.class_indexes,\n            smooth=self.smooth,\n            per_image=self.per_image,\n            threshold=self.threshold,\n            **self.submodules\n        )\n\n\n# aliases\niou_score = IOUScore()\nf1_score = FScore(beta=1)\nf2_score = FScore(beta=2)\nprecision = Precision()\nrecall = Recall()\n'"
segmentation_models/utils.py,0,"b'"""""" Utility functions for segmentation models """"""\n\nfrom keras_applications import get_submodules_from_kwargs\nfrom . import inject_global_submodules\n\n\ndef set_trainable(model, recompile=True, **kwargs):\n    """"""Set all layers of model trainable and recompile it\n\n    Note:\n        Model is recompiled using same optimizer, loss and metrics::\n\n            model.compile(\n                model.optimizer,\n                loss=model.loss,\n                metrics=model.metrics,\n                loss_weights=model.loss_weights,\n                sample_weight_mode=model.sample_weight_mode,\n                weighted_metrics=model.weighted_metrics,\n            )\n\n    Args:\n        model (``keras.models.Model``): instance of keras model\n\n    """"""\n    for layer in model.layers:\n        layer.trainable = True\n\n    if recompile:\n        model.compile(\n            model.optimizer,\n            loss=model.loss,\n            metrics=model.metrics,\n            loss_weights=model.loss_weights,\n            sample_weight_mode=model.sample_weight_mode,\n            weighted_metrics=model.weighted_metrics,\n        )\n\n\n@inject_global_submodules\ndef set_regularization(\n        model,\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        beta_regularizer=None,\n        gamma_regularizer=None,\n        **kwargs\n):\n    """"""Set regularizers to all layers\n\n    Note:\n       Returned model\'s config is updated correctly\n\n    Args:\n        model (``keras.models.Model``): instance of keras model\n        kernel_regularizer(``regularizer`): regularizer of kernels\n        bias_regularizer(``regularizer``): regularizer of bias\n        activity_regularizer(``regularizer``): regularizer of activity\n        gamma_regularizer(``regularizer``): regularizer of gamma of BatchNormalization\n        beta_regularizer(``regularizer``): regularizer of beta of BatchNormalization\n\n    Return:\n        out (``Model``): config updated model\n    """"""\n    _, _, models, _ = get_submodules_from_kwargs(kwargs)\n\n    for layer in model.layers:\n        # set kernel_regularizer\n        if kernel_regularizer is not None and hasattr(layer, \'kernel_regularizer\'):\n            layer.kernel_regularizer = kernel_regularizer\n        # set bias_regularizer\n        if bias_regularizer is not None and hasattr(layer, \'bias_regularizer\'):\n            layer.bias_regularizer = bias_regularizer\n        # set activity_regularizer\n        if activity_regularizer is not None and hasattr(layer, \'activity_regularizer\'):\n            layer.activity_regularizer = activity_regularizer\n\n        # set beta and gamma of BN layer\n        if beta_regularizer is not None and hasattr(layer, \'beta_regularizer\'):\n            layer.beta_regularizer = beta_regularizer\n\n        if gamma_regularizer is not None and hasattr(layer, \'gamma_regularizer\'):\n            layer.gamma_regularizer = gamma_regularizer\n\n    out = models.model_from_json(model.to_json())\n    out.set_weights(model.get_weights())\n\n    return out\n'"
tests/test_metrics.py,0,"b""import pytest\nimport numpy as np\n\nimport segmentation_models as sm\nfrom segmentation_models.metrics import IOUScore, FScore\nfrom segmentation_models.losses import JaccardLoss, DiceLoss\n\nif sm.framework() == sm._TF_KERAS_FRAMEWORK_NAME:\n    from tensorflow import keras\nelif sm.framework() == sm._KERAS_FRAMEWORK_NAME:\n    import keras\nelse:\n    raise ValueError('Incorrect framework {}'.format(sm.framework()))\n\nMETRICS = [\n    IOUScore,\n    FScore,\n]\n\nLOSSES = [\n    JaccardLoss,\n    DiceLoss,\n]\n\nGT0 = np.array(\n    [\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n    ],\n    dtype='float32',\n)\n\nGT1 = np.array(\n    [\n        [1, 1, 0],\n        [1, 1, 0],\n        [0, 0, 0],\n    ],\n    dtype='float32',\n)\n\nPR1 = np.array(\n    [\n        [0, 0, 0],\n        [1, 1, 0],\n        [0, 0, 0],\n    ],\n    dtype='float32',\n)\n\nPR2 = np.array(\n    [\n        [0, 0, 0],\n        [1, 1, 0],\n        [1, 1, 0],\n    ],\n    dtype='float32',\n)\n\nPR3 = np.array(\n    [\n        [0, 0, 0],\n        [0, 0, 0],\n        [1, 0, 0],\n    ],\n    dtype='float32',\n)\n\nIOU_CASES = (\n\n    (GT0, GT0, 1.00),\n    (GT1, GT1, 1.00),\n\n    (GT0, PR1, 0.00),\n    (GT0, PR2, 0.00),\n    (GT0, PR3, 0.00),\n\n    (GT1, PR1, 0.50),\n    (GT1, PR2, 1. / 3.),\n    (GT1, PR3, 0.00),\n)\n\nF1_CASES = (\n\n    (GT0, GT0, 1.00),\n    (GT1, GT1, 1.00),\n\n    (GT0, PR1, 0.00),\n    (GT0, PR2, 0.00),\n    (GT0, PR3, 0.00),\n\n    (GT1, PR1, 2. / 3.),\n    (GT1, PR2, 0.50),\n    (GT1, PR3, 0.00),\n)\n\nF2_CASES = (\n\n    (GT0, GT0, 1.00),\n    (GT1, GT1, 1.00),\n\n    (GT0, PR1, 0.00),\n    (GT0, PR2, 0.00),\n    (GT0, PR3, 0.00),\n\n    (GT1, PR1, 5. / 9.),\n    (GT1, PR2, 0.50),\n    (GT1, PR3, 0.00),\n)\n\n\ndef _to_4d(x):\n    if x.ndim == 2:\n        return x[None, :, :, None]\n    elif x.ndim == 3:\n        return x[None, :, :]\n\n\ndef _add_4d(x):\n    if x.ndim == 3:\n        return x[..., None]\n\n\n@pytest.mark.parametrize('case', IOU_CASES)\ndef test_iou_metric(case):\n    gt, pr, res = case\n    gt = _to_4d(gt)\n    pr = _to_4d(pr)\n    iou_score = IOUScore(smooth=10e-12)\n    score = keras.backend.eval(iou_score(gt, pr))\n    assert np.allclose(score, res)\n\n\n@pytest.mark.parametrize('case', IOU_CASES)\ndef test_jaccrad_loss(case):\n    gt, pr, res = case\n    gt = _to_4d(gt)\n    pr = _to_4d(pr)\n    jaccard_loss = JaccardLoss(smooth=10e-12)\n    score = keras.backend.eval(jaccard_loss(gt, pr))\n    assert np.allclose(score, 1 - res)\n\n\ndef _test_f_metric(case, beta=1):\n    gt, pr, res = case\n    gt = _to_4d(gt)\n    pr = _to_4d(pr)\n    f_score = FScore(beta=beta, smooth=10e-12)\n    score = keras.backend.eval(f_score(gt, pr))\n    assert np.allclose(score, res)\n\n\n@pytest.mark.parametrize('case', F1_CASES)\ndef test_f1_metric(case):\n    _test_f_metric(case, beta=1)\n\n\n@pytest.mark.parametrize('case', F2_CASES)\ndef test_f2_metric(case):\n    _test_f_metric(case, beta=2)\n\n\n@pytest.mark.parametrize('case', F1_CASES)\ndef test_dice_loss(case):\n    gt, pr, res = case\n    gt = _to_4d(gt)\n    pr = _to_4d(pr)\n    dice_loss = DiceLoss(smooth=10e-12)\n    score = keras.backend.eval(dice_loss(gt, pr))\n    assert np.allclose(score, 1 - res)\n\n\n@pytest.mark.parametrize('func', METRICS + LOSSES)\ndef test_per_image(func):\n    gt = np.stack([GT0, GT1], axis=0)\n    pr = np.stack([PR1, PR2], axis=0)\n\n    gt = _add_4d(gt)\n    pr = _add_4d(pr)\n\n    # calculate score per image\n    score_1 = keras.backend.eval(func(per_image=True, smooth=10e-12)(gt, pr))\n    score_2 = np.mean([\n        keras.backend.eval(func(smooth=10e-12)(_to_4d(GT0), _to_4d(PR1))),\n        keras.backend.eval(func(smooth=10e-12)(_to_4d(GT1), _to_4d(PR2))),\n    ])\n    assert np.allclose(score_1, score_2)\n\n\n@pytest.mark.parametrize('func', METRICS + LOSSES)\ndef test_per_batch(func):\n    gt = np.stack([GT0, GT1], axis=0)\n    pr = np.stack([PR1, PR2], axis=0)\n\n    gt = _add_4d(gt)\n    pr = _add_4d(pr)\n\n    # calculate score per batch\n    score_1 = keras.backend.eval(func(per_image=False, smooth=10e-12)(gt, pr))\n\n    gt1 = np.concatenate([GT0, GT1], axis=0)\n    pr1 = np.concatenate([PR1, PR2], axis=0)\n    score_2 = keras.backend.eval(func(per_image=True, smooth=10e-12)(_to_4d(gt1), _to_4d(pr1)))\n\n    assert np.allclose(score_1, score_2)\n\n\n@pytest.mark.parametrize('case', IOU_CASES)\ndef test_threshold_iou(case):\n    gt, pr, res = case\n    gt = _to_4d(gt)\n    pr = _to_4d(pr) * 0.51\n    iou_score = IOUScore(smooth=10e-12, threshold=0.5)\n    score = keras.backend.eval(iou_score(gt, pr))\n    assert np.allclose(score, res)\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/test_models.py,0,"b'import os\nimport pytest\nimport random\nimport six\nimport numpy as np\n\nimport segmentation_models as sm\nfrom segmentation_models import Unet\nfrom segmentation_models import Linknet\nfrom segmentation_models import PSPNet\nfrom segmentation_models import FPN\nfrom segmentation_models import get_available_backbone_names\n\nif sm.framework() == sm._TF_KERAS_FRAMEWORK_NAME:\n    from tensorflow import keras\nelif sm.framework() == sm._KERAS_FRAMEWORK_NAME:\n    import keras\nelse:\n    raise ValueError(\'Incorrect framework {}\'.format(sm.framework()))\n\ndef get_backbones():\n    is_travis = os.environ.get(\'TRAVIS\', False)\n    exclude = [\'senet154\', \'efficientnetb6\', \'efficientnetb7\']\n    backbones = get_available_backbone_names()\n\n    if is_travis:\n        backbones = [b for b in backbones if b not in exclude]\n    return backbones\n\n\nBACKBONES = get_backbones()\n\n\ndef _select_names(names):\n    is_full = os.environ.get(\'FULL_TEST\', False)\n    if not is_full:\n        return [random.choice(names)]\n    else:\n        return names\n\n\ndef keras_test(func):\n    """"""Function wrapper to clean up after TensorFlow tests.\n    # Arguments\n        func: test function to clean up after.\n    # Returns\n        A function wrapping the input function.\n    """"""\n    @six.wraps(func)\n    def wrapper(*args, **kwargs):\n        output = func(*args, **kwargs)\n        keras.backend.clear_session()\n        return output\n    return wrapper\n\n\n@keras_test\ndef _test_none_shape(model_fn, backbone, *args, **kwargs):\n\n    # define number of channels\n    input_shape = kwargs.get(\'input_shape\', None)\n    n_channels = 3 if input_shape is None else input_shape[-1]\n\n    # create test sample\n    x = np.ones((1, 32, 32, n_channels))\n\n    # define model and process sample\n    model = model_fn(backbone, *args, **kwargs)\n    y = model.predict(x)\n\n    # check output dimensions\n    assert x.shape[:-1] == y.shape[:-1]\n\n\n@keras_test\ndef _test_shape(model_fn, backbone, input_shape, *args, **kwargs):\n\n    # create test sample\n    x = np.ones((1, *input_shape))\n\n    # define model and process sample\n    model = model_fn(backbone, input_shape=input_shape, *args, **kwargs)\n    y = model.predict(x)\n\n    # check output dimensions\n    assert x.shape[:-1] == y.shape[:-1]\n\n\n@pytest.mark.parametrize(\'backbone\', _select_names(BACKBONES))\ndef test_unet(backbone):\n    _test_none_shape(\n        Unet, backbone, encoder_weights=None)\n\n    _test_none_shape(\n        Unet, backbone, encoder_weights=\'imagenet\')\n\n    _test_shape(\n        Unet, backbone, input_shape=(256, 256, 4), encoder_weights=None)\n\n\n@pytest.mark.parametrize(\'backbone\', _select_names(BACKBONES))\ndef test_linknet(backbone):\n    _test_none_shape(\n        Linknet, backbone, encoder_weights=None)\n\n    _test_none_shape(\n        Linknet, backbone, encoder_weights=\'imagenet\')\n\n    _test_shape(\n        Linknet, backbone, input_shape=(256, 256, 4), encoder_weights=None)\n\n\n@pytest.mark.parametrize(\'backbone\', _select_names(BACKBONES))\ndef test_pspnet(backbone):\n\n    _test_shape(\n        PSPNet, backbone, input_shape=(384, 384, 4), encoder_weights=None)\n\n    _test_shape(\n        PSPNet, backbone, input_shape=(384, 384, 3), encoder_weights=\'imagenet\')\n\n\n@pytest.mark.parametrize(\'backbone\', _select_names(BACKBONES))\ndef test_fpn(backbone):\n    _test_none_shape(\n        FPN, backbone, encoder_weights=None)\n\n    _test_none_shape(\n        FPN, backbone, encoder_weights=\'imagenet\')\n\n    _test_shape(\n        FPN, backbone, input_shape=(256, 256, 4), encoder_weights=None)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/test_utils.py,0,"b'import pytest\nimport numpy as np\n\nimport segmentation_models as sm\nfrom segmentation_models.utils import set_regularization\nfrom segmentation_models import Unet\n\nif sm.framework() == sm._TF_KERAS_FRAMEWORK_NAME:\n    from tensorflow import keras\nelif sm.framework() == sm._KERAS_FRAMEWORK_NAME:\n    import keras\nelse:\n    raise ValueError(\'Incorrect framework {}\'.format(sm.framework()))\n\nX1 = np.ones((1, 32, 32, 3))\nY1 = np.ones((1, 32, 32, 1))\nMODEL = Unet\nBACKBONE = \'resnet18\'\nCASE = (\n\n    (X1, Y1, MODEL, BACKBONE),\n)\n\n\ndef _test_regularizer(model, reg_model, x, y):\n\n    def zero_loss(gt, pr):\n        return pr * 0\n\n    model.compile(\'Adam\', loss=zero_loss, metrics=[\'binary_accuracy\'])\n    reg_model.compile(\'Adam\', loss=zero_loss, metrics=[\'binary_accuracy\'])\n\n    loss_1, _ = model.test_on_batch(x, y)\n    loss_2, _ = reg_model.test_on_batch(x, y)\n\n    assert loss_1 == 0\n    assert loss_2 > 0\n\n    keras.backend.clear_session()\n\n\n@pytest.mark.parametrize(\'case\', CASE)\ndef test_kernel_reg(case):\n    x, y, model_fn, backbone= case\n\n    l1_reg = keras.regularizers.l1(0.1)\n    model = model_fn(backbone)\n    reg_model = set_regularization(model, kernel_regularizer=l1_reg)\n    _test_regularizer(model, reg_model, x, y)\n\n    l2_reg = keras.regularizers.l2(0.1)\n    model = model_fn(backbone, encoder_weights=None)\n    reg_model = set_regularization(model, kernel_regularizer=l2_reg)\n    _test_regularizer(model, reg_model, x, y)\n\n\n""""""\nNote:\n    backbone resnet18 use BN after each conv layer --- so no bias used in these conv layers\n    skip the bias regularizer test\n\n@pytest.mark.parametrize(\'case\', CASE)\ndef test_bias_reg(case):\n    x, y, model_fn, backbone = case\n\n    l1_reg = regularizers.l1(1)\n    model = model_fn(backbone)\n    reg_model = set_regularization(model, bias_regularizer=l1_reg)\n    _test_regularizer(model, reg_model, x, y)\n\n    l2_reg = regularizers.l2(1)\n    model = model_fn(backbone)\n    reg_model = set_regularization(model, bias_regularizer=l2_reg)\n    _test_regularizer(model, reg_model, x, y)\n""""""\n\n\n@pytest.mark.parametrize(\'case\', CASE)\ndef test_bn_reg(case):\n    x, y, model_fn, backbone= case\n\n    l1_reg = keras.regularizers.l1(1)\n    model = model_fn(backbone)\n    reg_model = set_regularization(model, gamma_regularizer=l1_reg)\n    _test_regularizer(model, reg_model, x, y)\n\n    model = model_fn(backbone)\n    reg_model = set_regularization(model, beta_regularizer=l1_reg)\n    _test_regularizer(model, reg_model, x, y)\n\n    l2_reg = keras.regularizers.l2(1)\n    model = model_fn(backbone)\n    reg_model = set_regularization(model, gamma_regularizer=l2_reg)\n    _test_regularizer(model, reg_model, x, y)\n\n    model = model_fn(backbone)\n    reg_model = set_regularization(model, beta_regularizer=l2_reg)\n    _test_regularizer(model, reg_model, x, y)\n\n\n@pytest.mark.parametrize(\'case\', CASE)\ndef test_activity_reg(case):\n    x, y, model_fn, backbone= case\n\n    l2_reg = keras.regularizers.l2(1)\n    model = model_fn(backbone)\n    reg_model = set_regularization(model, activity_regularizer=l2_reg)\n    _test_regularizer(model, reg_model, x, y)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
segmentation_models/backbones/__init__.py,0,b''
segmentation_models/backbones/backbones_factory.py,0,"b""import copy\nimport efficientnet.model as eff\nfrom classification_models.models_factory import ModelsFactory\n\nfrom . import inception_resnet_v2 as irv2\nfrom . import inception_v3 as iv3\n\n\nclass BackbonesFactory(ModelsFactory):\n    _default_feature_layers = {\n\n        # List of layers to take features from backbone in the following order:\n        # (x16, x8, x4, x2, x1) - `x4` mean that features has 4 times less spatial\n        # resolution (Height x Width) than input image.\n\n        # VGG\n        'vgg16': ('block5_conv3', 'block4_conv3', 'block3_conv3', 'block2_conv2', 'block1_conv2'),\n        'vgg19': ('block5_conv4', 'block4_conv4', 'block3_conv4', 'block2_conv2', 'block1_conv2'),\n\n        # ResNets\n        'resnet18': ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n        'resnet34': ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n        'resnet50': ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n        'resnet101': ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n        'resnet152': ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n\n        # ResNeXt\n        'resnext50': ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n        'resnext101': ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n\n        # Inception\n        'inceptionv3': (228, 86, 16, 9),\n        'inceptionresnetv2': (594, 260, 16, 9),\n\n        # DenseNet\n        'densenet121': (311, 139, 51, 4),\n        'densenet169': (367, 139, 51, 4),\n        'densenet201': (479, 139, 51, 4),\n\n        # SE models\n        'seresnet18': ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n        'seresnet34': ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n        'seresnet50': (246, 136, 62, 4),\n        'seresnet101': (552, 136, 62, 4),\n        'seresnet152': (858, 208, 62, 4),\n        'seresnext50': (1078, 584, 254, 4),\n        'seresnext101': (2472, 584, 254, 4),\n        'senet154': (6884, 1625, 454, 12),\n\n        # Mobile Nets\n        'mobilenet': ('conv_pw_11_relu', 'conv_pw_5_relu', 'conv_pw_3_relu', 'conv_pw_1_relu'),\n        'mobilenetv2': ('block_13_expand_relu', 'block_6_expand_relu', 'block_3_expand_relu',\n                        'block_1_expand_relu'),\n\n        # EfficientNets\n        'efficientnetb0': ('block6a_expand_activation', 'block4a_expand_activation',\n                           'block3a_expand_activation', 'block2a_expand_activation'),\n        'efficientnetb1': ('block6a_expand_activation', 'block4a_expand_activation',\n                           'block3a_expand_activation', 'block2a_expand_activation'),\n        'efficientnetb2': ('block6a_expand_activation', 'block4a_expand_activation',\n                           'block3a_expand_activation', 'block2a_expand_activation'),\n        'efficientnetb3': ('block6a_expand_activation', 'block4a_expand_activation',\n                           'block3a_expand_activation', 'block2a_expand_activation'),\n        'efficientnetb4': ('block6a_expand_activation', 'block4a_expand_activation',\n                           'block3a_expand_activation', 'block2a_expand_activation'),\n        'efficientnetb5': ('block6a_expand_activation', 'block4a_expand_activation',\n                           'block3a_expand_activation', 'block2a_expand_activation'),\n        'efficientnetb6': ('block6a_expand_activation', 'block4a_expand_activation',\n                           'block3a_expand_activation', 'block2a_expand_activation'),\n        'efficientnetb7': ('block6a_expand_activation', 'block4a_expand_activation',\n                           'block3a_expand_activation', 'block2a_expand_activation'),\n\n    }\n\n    _models_update = {\n        'inceptionresnetv2': [irv2.InceptionResNetV2, irv2.preprocess_input],\n        'inceptionv3': [iv3.InceptionV3, iv3.preprocess_input],\n\n        'efficientnetb0': [eff.EfficientNetB0, eff.preprocess_input],\n        'efficientnetb1': [eff.EfficientNetB1, eff.preprocess_input],\n        'efficientnetb2': [eff.EfficientNetB2, eff.preprocess_input],\n        'efficientnetb3': [eff.EfficientNetB3, eff.preprocess_input],\n        'efficientnetb4': [eff.EfficientNetB4, eff.preprocess_input],\n        'efficientnetb5': [eff.EfficientNetB5, eff.preprocess_input],\n        'efficientnetb6': [eff.EfficientNetB6, eff.preprocess_input],\n        'efficientnetb7': [eff.EfficientNetB7, eff.preprocess_input],\n    }\n\n    # currently not supported\n    _models_delete = ['resnet50v2', 'resnet101v2', 'resnet152v2',\n                      'nasnetlarge', 'nasnetmobile', 'xception']\n\n    @property\n    def models(self):\n        all_models = copy.copy(self._models)\n        all_models.update(self._models_update)\n        for k in self._models_delete:\n            del all_models[k]\n        return all_models\n\n    def get_backbone(self, name, *args, **kwargs):\n        model_fn, _ = self.get(name)\n        model = model_fn(*args, **kwargs)\n        return model\n\n    def get_feature_layers(self, name, n=5):\n        return self._default_feature_layers[name][:n]\n\n    def get_preprocessing(self, name):\n        return self.get(name)[1]\n\n\nBackbones = BackbonesFactory()\n"""
segmentation_models/backbones/inception_resnet_v2.py,0,"b'""""""Inception-ResNet V2 model for Keras.\nModel naming and structure follows TF-slim implementation\n(which has some additional layers and different number of\nfilters from the original arXiv paper):\nhttps://github.com/tensorflow/models/blob/master/research/slim/nets/inception_resnet_v2.py\nPre-trained ImageNet weights are also converted from TF-slim,\nwhich can be found in:\nhttps://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models\n# Reference\n- [Inception-v4, Inception-ResNet and the Impact of\n   Residual Connections on Learning](https://arxiv.org/abs/1602.07261) (AAAI 2017)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom keras_applications import imagenet_utils\nfrom keras_applications import get_submodules_from_kwargs\n\nBASE_WEIGHT_URL = (\'https://github.com/fchollet/deep-learning-models/\'\n                   \'releases/download/v0.7/\')\n\nbackend = None\nlayers = None\nmodels = None\nkeras_utils = None\n\n\ndef preprocess_input(x, **kwargs):\n    """"""Preprocesses a numpy array encoding a batch of images.\n    # Arguments\n        x: a 4D numpy array consists of RGB values within [0, 255].\n    # Returns\n        Preprocessed array.\n    """"""\n    return imagenet_utils.preprocess_input(x, mode=\'tf\', **kwargs)\n\n\ndef conv2d_bn(x,\n              filters,\n              kernel_size,\n              strides=1,\n              padding=\'same\',\n              activation=\'relu\',\n              use_bias=False,\n              name=None):\n    """"""Utility function to apply conv + BN.\n    # Arguments\n        x: input tensor.\n        filters: filters in `Conv2D`.\n        kernel_size: kernel size as in `Conv2D`.\n        strides: strides in `Conv2D`.\n        padding: padding mode in `Conv2D`.\n        activation: activation in `Conv2D`.\n        use_bias: whether to use a bias in `Conv2D`.\n        name: name of the ops; will become `name + \'_ac\'` for the activation\n            and `name + \'_bn\'` for the batch norm layer.\n    # Returns\n        Output tensor after applying `Conv2D` and `BatchNormalization`.\n    """"""\n    x = layers.Conv2D(filters,\n                      kernel_size,\n                      strides=strides,\n                      padding=padding,\n                      use_bias=use_bias,\n                      name=name)(x)\n    if not use_bias:\n        bn_axis = 1 if backend.image_data_format() == \'channels_first\' else 3\n        bn_name = None if name is None else name + \'_bn\'\n        x = layers.BatchNormalization(axis=bn_axis,\n                                      scale=False,\n                                      name=bn_name)(x)\n    if activation is not None:\n        ac_name = None if name is None else name + \'_ac\'\n        x = layers.Activation(activation, name=ac_name)(x)\n    return x\n\n\ndef inception_resnet_block(x, scale, block_type, block_idx, activation=\'relu\'):\n    """"""Adds a Inception-ResNet block.\n    This function builds 3 types of Inception-ResNet blocks mentioned\n    in the paper, controlled by the `block_type` argument (which is the\n    block name used in the official TF-slim implementation):\n        - Inception-ResNet-A: `block_type=\'block35\'`\n        - Inception-ResNet-B: `block_type=\'block17\'`\n        - Inception-ResNet-C: `block_type=\'block8\'`\n    # Arguments\n        x: input tensor.\n        scale: scaling factor to scale the residuals (i.e., the output of\n            passing `x` through an inception module) before adding them\n            to the shortcut branch.\n            Let `r` be the output from the residual branch,\n            the output of this block will be `x + scale * r`.\n        block_type: `\'block35\'`, `\'block17\'` or `\'block8\'`, determines\n            the network structure in the residual branch.\n        block_idx: an `int` used for generating layer names.\n            The Inception-ResNet blocks\n            are repeated many times in this network.\n            We use `block_idx` to identify\n            each of the repetitions. For example,\n            the first Inception-ResNet-A block\n            will have `block_type=\'block35\', block_idx=0`,\n            and the layer names will have\n            a common prefix `\'block35_0\'`.\n        activation: activation function to use at the end of the block\n            (see [activations](../activations.md)).\n            When `activation=None`, no activation is applied\n            (i.e., ""linear"" activation: `a(x) = x`).\n    # Returns\n        Output tensor for the block.\n    # Raises\n        ValueError: if `block_type` is not one of `\'block35\'`,\n            `\'block17\'` or `\'block8\'`.\n    """"""\n    if block_type == \'block35\':\n        branch_0 = conv2d_bn(x, 32, 1)\n        branch_1 = conv2d_bn(x, 32, 1)\n        branch_1 = conv2d_bn(branch_1, 32, 3)\n        branch_2 = conv2d_bn(x, 32, 1)\n        branch_2 = conv2d_bn(branch_2, 48, 3)\n        branch_2 = conv2d_bn(branch_2, 64, 3)\n        branches = [branch_0, branch_1, branch_2]\n    elif block_type == \'block17\':\n        branch_0 = conv2d_bn(x, 192, 1)\n        branch_1 = conv2d_bn(x, 128, 1)\n        branch_1 = conv2d_bn(branch_1, 160, [1, 7])\n        branch_1 = conv2d_bn(branch_1, 192, [7, 1])\n        branches = [branch_0, branch_1]\n    elif block_type == \'block8\':\n        branch_0 = conv2d_bn(x, 192, 1)\n        branch_1 = conv2d_bn(x, 192, 1)\n        branch_1 = conv2d_bn(branch_1, 224, [1, 3])\n        branch_1 = conv2d_bn(branch_1, 256, [3, 1])\n        branches = [branch_0, branch_1]\n    else:\n        raise ValueError(\'Unknown Inception-ResNet block type. \'\n                         \'Expects ""block35"", ""block17"" or ""block8"", \'\n                         \'but got: \' + str(block_type))\n\n    block_name = block_type + \'_\' + str(block_idx)\n    channel_axis = 1 if backend.image_data_format() == \'channels_first\' else 3\n    mixed = layers.Concatenate(\n        axis=channel_axis, name=block_name + \'_mixed\')(branches)\n    up = conv2d_bn(mixed,\n                   backend.int_shape(x)[channel_axis],\n                   1,\n                   activation=None,\n                   use_bias=True,\n                   name=block_name + \'_conv\')\n\n    x = layers.Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n                      output_shape=backend.int_shape(x)[1:],\n                      arguments={\'scale\': scale},\n                      name=block_name)([x, up])\n    if activation is not None:\n        x = layers.Activation(activation, name=block_name + \'_ac\')(x)\n    return x\n\n\ndef InceptionResNetV2(include_top=True,\n                      weights=\'imagenet\',\n                      input_tensor=None,\n                      input_shape=None,\n                      pooling=None,\n                      classes=1000,\n                      **kwargs):\n    """"""Instantiates the Inception-ResNet v2 architecture.\n    Optionally loads weights pre-trained on ImageNet.\n    Note that the data format convention used by the model is\n    the one specified in your Keras config at `~/.keras/keras.json`.\n    # Arguments\n        include_top: whether to include the fully-connected\n            layer at the top of the network.\n        weights: one of `None` (random initialization),\n              \'imagenet\' (pre-training on ImageNet),\n              or the path to the weights file to be loaded.\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is `False` (otherwise the input shape\n            has to be `(299, 299, 3)` (with `\'channels_last\'` data format)\n            or `(3, 299, 299)` (with `\'channels_first\'` data format).\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 75.\n            E.g. `(150, 150, 3)` would be one valid value.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model will be\n                the 4D tensor output of the last convolutional block.\n            - `\'avg\'` means that global average pooling\n                will be applied to the output of the\n                last convolutional block, and thus\n                the output of the model will be a 2D tensor.\n            - `\'max\'` means that global max pooling will be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is `True`, and\n            if no `weights` argument is specified.\n    # Returns\n        A Keras `Model` instance.\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n    """"""\n    global backend, layers, models, keras_utils\n    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n\n    if not (weights in {\'imagenet\', None} or os.path.exists(weights)):\n        raise ValueError(\'The `weights` argument should be either \'\n                         \'`None` (random initialization), `imagenet` \'\n                         \'(pre-training on ImageNet), \'\n                         \'or the path to the weights file to be loaded.\')\n\n    if weights == \'imagenet\' and include_top and classes != 1000:\n        raise ValueError(\'If using `weights` as `""imagenet""` with `include_top`\'\n                         \' as true, `classes` should be 1000\')\n\n    # Determine proper input shape\n    input_shape = imagenet_utils._obtain_input_shape(\n        input_shape,\n        default_size=299,\n        min_size=32,\n        data_format=backend.image_data_format(),\n        require_flatten=include_top,\n        weights=weights)\n\n    if input_tensor is None:\n        img_input = layers.Input(shape=input_shape)\n    else:\n        if not backend.is_keras_tensor(input_tensor):\n            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    # Stem block: 35 x 35 x 192\n    x = conv2d_bn(img_input, 32, 3, strides=2, padding=\'same\')\n    x = conv2d_bn(x, 32, 3, padding=\'same\')\n    x = conv2d_bn(x, 64, 3, padding=\'same\')\n    x = layers.MaxPooling2D(3, strides=2, padding=\'same\')(x)\n    x = conv2d_bn(x, 80, 1, padding=\'same\')\n    x = conv2d_bn(x, 192, 3, padding=\'same\')\n    x = layers.MaxPooling2D(3, strides=2, padding=\'same\')(x)\n\n    # Mixed 5b (Inception-A block): 35 x 35 x 320\n    branch_0 = conv2d_bn(x, 96, 1, padding=\'same\')\n    branch_1 = conv2d_bn(x, 48, 1, padding=\'same\')\n    branch_1 = conv2d_bn(branch_1, 64, 5, padding=\'same\')\n    branch_2 = conv2d_bn(x, 64, 1, padding=\'same\')\n    branch_2 = conv2d_bn(branch_2, 96, 3, padding=\'same\')\n    branch_2 = conv2d_bn(branch_2, 96, 3, padding=\'same\')\n    branch_pool = layers.AveragePooling2D(3, strides=1, padding=\'same\')(x)\n    branch_pool = conv2d_bn(branch_pool, 64, 1, padding=\'same\')\n    branches = [branch_0, branch_1, branch_2, branch_pool]\n    channel_axis = 1 if backend.image_data_format() == \'channels_first\' else 3\n    x = layers.Concatenate(axis=channel_axis, name=\'mixed_5b\')(branches)\n\n    # 10x block35 (Inception-ResNet-A block): 35 x 35 x 320\n    for block_idx in range(1, 11):\n        x = inception_resnet_block(x,\n                                   scale=0.17,\n                                   block_type=\'block35\',\n                                   block_idx=block_idx)\n\n    # Mixed 6a (Reduction-A block): 17 x 17 x 1088\n    branch_0 = conv2d_bn(x, 384, 3, strides=2, padding=\'same\')\n    branch_1 = conv2d_bn(x, 256, 1, padding=\'same\')\n    branch_1 = conv2d_bn(branch_1, 256, 3, padding=\'same\')\n    branch_1 = conv2d_bn(branch_1, 384, 3, strides=2, padding=\'same\')\n    branch_pool = layers.MaxPooling2D(3, strides=2, padding=\'same\')(x)\n    branches = [branch_0, branch_1, branch_pool]\n    x = layers.Concatenate(axis=channel_axis, name=\'mixed_6a\')(branches)\n\n    # 20x block17 (Inception-ResNet-B block): 17 x 17 x 1088\n    for block_idx in range(1, 21):\n        x = inception_resnet_block(x,\n                                   scale=0.1,\n                                   block_type=\'block17\',\n                                   block_idx=block_idx)\n\n    # Mixed 7a (Reduction-B block): 8 x 8 x 2080\n    branch_0 = conv2d_bn(x, 256, 1, padding=\'same\')\n    branch_0 = conv2d_bn(branch_0, 384, 3, strides=2, padding=\'same\')\n    branch_1 = conv2d_bn(x, 256, 1, padding=\'same\')\n    branch_1 = conv2d_bn(branch_1, 288, 3, strides=2, padding=\'same\')\n    branch_2 = conv2d_bn(x, 256, 1, padding=\'same\')\n    branch_2 = conv2d_bn(branch_2, 288, 3, padding=\'same\')\n    branch_2 = conv2d_bn(branch_2, 320, 3, strides=2, padding=\'same\')\n    branch_pool = layers.MaxPooling2D(3, strides=2, padding=\'same\')(x)\n    branches = [branch_0, branch_1, branch_2, branch_pool]\n    x = layers.Concatenate(axis=channel_axis, name=\'mixed_7a\')(branches)\n\n    # 10x block8 (Inception-ResNet-C block): 8 x 8 x 2080\n    for block_idx in range(1, 10):\n        x = inception_resnet_block(x,\n                                   scale=0.2,\n                                   block_type=\'block8\',\n                                   block_idx=block_idx)\n    x = inception_resnet_block(x,\n                               scale=1.,\n                               activation=None,\n                               block_type=\'block8\',\n                               block_idx=10)\n\n    # Final convolution block: 8 x 8 x 1536\n    x = conv2d_bn(x, 1536, 1, name=\'conv_7b\')\n\n    if include_top:\n        # Classification block\n        x = layers.GlobalAveragePooling2D(name=\'avg_pool\')(x)\n        x = layers.Dense(classes, activation=\'softmax\', name=\'predictions\')(x)\n    else:\n        if pooling == \'avg\':\n            x = layers.GlobalAveragePooling2D()(x)\n        elif pooling == \'max\':\n            x = layers.GlobalMaxPooling2D()(x)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = keras_utils.get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n\n    # Create model.\n    model = models.Model(inputs, x, name=\'inception_resnet_v2\')\n\n    # Load weights.\n    if weights == \'imagenet\':\n        if include_top:\n            fname = \'inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5\'\n            weights_path = keras_utils.get_file(\n                fname,\n                BASE_WEIGHT_URL + fname,\n                cache_subdir=\'models\',\n                file_hash=\'e693bd0210a403b3192acc6073ad2e96\')\n        else:\n            fname = (\'inception_resnet_v2_weights_\'\n                     \'tf_dim_ordering_tf_kernels_notop.h5\')\n            weights_path = keras_utils.get_file(\n                fname,\n                BASE_WEIGHT_URL + fname,\n                cache_subdir=\'models\',\n                file_hash=\'d19885ff4a710c122648d3b5c3b684e4\')\n        model.load_weights(weights_path)\n    elif weights is not None:\n        model.load_weights(weights)\n\n    return model\n'"
segmentation_models/backbones/inception_v3.py,0,"b'""""""Inception V3 model for Keras.\nNote that the input image format for this model is different than for\nthe VGG16 and ResNet models (299x299 instead of 224x224),\nand that the input preprocessing function is also different (same as Xception).\n# Reference\n- [Rethinking the Inception Architecture for Computer Vision](\n    http://arxiv.org/abs/1512.00567) (CVPR 2016)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom keras_applications import imagenet_utils\nfrom keras_applications import get_submodules_from_kwargs\n\nWEIGHTS_PATH = (\n    \'https://github.com/fchollet/deep-learning-models/\'\n    \'releases/download/v0.5/\'\n    \'inception_v3_weights_tf_dim_ordering_tf_kernels.h5\')\nWEIGHTS_PATH_NO_TOP = (\n    \'https://github.com/fchollet/deep-learning-models/\'\n    \'releases/download/v0.5/\'\n    \'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\')\n\nbackend = None\nlayers = None\nmodels = None\nkeras_utils = None\n\n\ndef conv2d_bn(x,\n              filters,\n              num_row,\n              num_col,\n              padding=\'same\',\n              strides=(1, 1),\n              name=None):\n    """"""Utility function to apply conv + BN.\n    # Arguments\n        x: input tensor.\n        filters: filters in `Conv2D`.\n        num_row: height of the convolution kernel.\n        num_col: width of the convolution kernel.\n        padding: padding mode in `Conv2D`.\n        strides: strides in `Conv2D`.\n        name: name of the ops; will become `name + \'_conv\'`\n            for the convolution and `name + \'_bn\'` for the\n            batch norm layer.\n    # Returns\n        Output tensor after applying `Conv2D` and `BatchNormalization`.\n    """"""\n    if name is not None:\n        bn_name = name + \'_bn\'\n        conv_name = name + \'_conv\'\n    else:\n        bn_name = None\n        conv_name = None\n    if backend.image_data_format() == \'channels_first\':\n        bn_axis = 1\n    else:\n        bn_axis = 3\n    x = layers.Conv2D(\n        filters, (num_row, num_col),\n        strides=strides,\n        padding=padding,\n        use_bias=False,\n        name=conv_name)(x)\n    x = layers.BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n    x = layers.Activation(\'relu\', name=name)(x)\n    return x\n\n\ndef InceptionV3(include_top=True,\n                weights=\'imagenet\',\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n                classes=1000,\n                **kwargs):\n    """"""Instantiates the Inception v3 architecture.\n    Optionally loads weights pre-trained on ImageNet.\n    Note that the data format convention used by the model is\n    the one specified in your Keras config at `~/.keras/keras.json`.\n    # Arguments\n        include_top: whether to include the fully-connected\n            layer at the top of the network.\n        weights: one of `None` (random initialization),\n              \'imagenet\' (pre-training on ImageNet),\n              or the path to the weights file to be loaded.\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is False (otherwise the input shape\n            has to be `(299, 299, 3)` (with `channels_last` data format)\n            or `(3, 299, 299)` (with `channels_first` data format).\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 75.\n            E.g. `(150, 150, 3)` would be one valid value.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model will be\n                the 4D tensor output of the\n                last convolutional block.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional block, and thus\n                the output of the model will be a 2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is True, and\n            if no `weights` argument is specified.\n    # Returns\n        A Keras model instance.\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n    """"""\n    global backend, layers, models, keras_utils\n    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n\n    if not (weights in {\'imagenet\', None} or os.path.exists(weights)):\n        raise ValueError(\'The `weights` argument should be either \'\n                         \'`None` (random initialization), `imagenet` \'\n                         \'(pre-training on ImageNet), \'\n                         \'or the path to the weights file to be loaded.\')\n\n    if weights == \'imagenet\' and include_top and classes != 1000:\n        raise ValueError(\'If using `weights` as `""imagenet""` with `include_top`\'\n                         \' as true, `classes` should be 1000\')\n\n    # Determine proper input shape\n    input_shape = imagenet_utils._obtain_input_shape(\n        input_shape,\n        default_size=299,\n        min_size=75,\n        data_format=backend.image_data_format(),\n        require_flatten=include_top,\n        weights=weights)\n\n    if input_tensor is None:\n        img_input = layers.Input(shape=input_shape)\n    else:\n        if not backend.is_keras_tensor(input_tensor):\n            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    if backend.image_data_format() == \'channels_first\':\n        channel_axis = 1\n    else:\n        channel_axis = 3\n\n    x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding=\'same\')\n    x = conv2d_bn(x, 32, 3, 3, padding=\'same\')\n    x = conv2d_bn(x, 64, 3, 3, padding=\'same\')\n    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n\n    x = conv2d_bn(x, 80, 1, 1, padding=\'same\')\n    x = conv2d_bn(x, 192, 3, 3, padding=\'same\')\n    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n\n    # mixed 0: 35 x 35 x 256\n    branch1x1 = conv2d_bn(x, 64, 1, 1)\n\n    branch5x5 = conv2d_bn(x, 48, 1, 1)\n    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n\n    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n\n    branch_pool = layers.AveragePooling2D((3, 3),\n                                          strides=(1, 1),\n                                          padding=\'same\')(x)\n    branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n    x = layers.concatenate(\n        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n        axis=channel_axis,\n        name=\'mixed0\')\n\n    # mixed 1: 35 x 35 x 288\n    branch1x1 = conv2d_bn(x, 64, 1, 1)\n\n    branch5x5 = conv2d_bn(x, 48, 1, 1)\n    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n\n    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n\n    branch_pool = layers.AveragePooling2D((3, 3),\n                                          strides=(1, 1),\n                                          padding=\'same\')(x)\n    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n    x = layers.concatenate(\n        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n        axis=channel_axis,\n        name=\'mixed1\')\n\n    # mixed 2: 35 x 35 x 288\n    branch1x1 = conv2d_bn(x, 64, 1, 1)\n\n    branch5x5 = conv2d_bn(x, 48, 1, 1)\n    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n\n    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n\n    branch_pool = layers.AveragePooling2D((3, 3),\n                                          strides=(1, 1),\n                                          padding=\'same\')(x)\n    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n    x = layers.concatenate(\n        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n        axis=channel_axis,\n        name=\'mixed2\')\n\n    # mixed 3: 17 x 17 x 768\n    branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding=\'same\')\n\n    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n    branch3x3dbl = conv2d_bn(\n        branch3x3dbl, 96, 3, 3, strides=(2, 2), padding=\'same\')\n\n    branch_pool = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n    x = layers.concatenate(\n        [branch3x3, branch3x3dbl, branch_pool],\n        axis=channel_axis,\n        name=\'mixed3\')\n\n    # mixed 4: 17 x 17 x 768\n    branch1x1 = conv2d_bn(x, 192, 1, 1)\n\n    branch7x7 = conv2d_bn(x, 128, 1, 1)\n    branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)\n    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n\n    branch7x7dbl = conv2d_bn(x, 128, 1, 1)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n\n    branch_pool = layers.AveragePooling2D((3, 3),\n                                          strides=(1, 1),\n                                          padding=\'same\')(x)\n    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n    x = layers.concatenate(\n        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n        axis=channel_axis,\n        name=\'mixed4\')\n\n    # mixed 5, 6: 17 x 17 x 768\n    for i in range(2):\n        branch1x1 = conv2d_bn(x, 192, 1, 1)\n\n        branch7x7 = conv2d_bn(x, 160, 1, 1)\n        branch7x7 = conv2d_bn(branch7x7, 160, 1, 7)\n        branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n\n        branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7)\n        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n        branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n\n        branch_pool = layers.AveragePooling2D(\n            (3, 3), strides=(1, 1), padding=\'same\')(x)\n        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n        x = layers.concatenate(\n            [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n            axis=channel_axis,\n            name=\'mixed\' + str(5 + i))\n\n    # mixed 7: 17 x 17 x 768\n    branch1x1 = conv2d_bn(x, 192, 1, 1)\n\n    branch7x7 = conv2d_bn(x, 192, 1, 1)\n    branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)\n    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n\n    branch7x7dbl = conv2d_bn(x, 192, 1, 1)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n\n    branch_pool = layers.AveragePooling2D((3, 3),\n                                          strides=(1, 1),\n                                          padding=\'same\')(x)\n    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n    x = layers.concatenate(\n        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n        axis=channel_axis,\n        name=\'mixed7\')\n\n    # mixed 8: 8 x 8 x 1280\n    branch3x3 = conv2d_bn(x, 192, 1, 1)\n    branch3x3 = conv2d_bn(branch3x3, 320, 3, 3,\n                          strides=(2, 2), padding=\'same\')\n\n    branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n    branch7x7x3 = conv2d_bn(\n        branch7x7x3, 192, 3, 3, strides=(2, 2), padding=\'same\')\n\n    branch_pool = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n    x = layers.concatenate(\n        [branch3x3, branch7x7x3, branch_pool],\n        axis=channel_axis,\n        name=\'mixed8\')\n\n    # mixed 9: 8 x 8 x 2048\n    for i in range(2):\n        branch1x1 = conv2d_bn(x, 320, 1, 1)\n\n        branch3x3 = conv2d_bn(x, 384, 1, 1)\n        branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n        branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n        branch3x3 = layers.concatenate(\n            [branch3x3_1, branch3x3_2],\n            axis=channel_axis,\n            name=\'mixed9_\' + str(i))\n\n        branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n        branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n        branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n        branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n        branch3x3dbl = layers.concatenate(\n            [branch3x3dbl_1, branch3x3dbl_2], axis=channel_axis)\n\n        branch_pool = layers.AveragePooling2D(\n            (3, 3), strides=(1, 1), padding=\'same\')(x)\n        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n        x = layers.concatenate(\n            [branch1x1, branch3x3, branch3x3dbl, branch_pool],\n            axis=channel_axis,\n            name=\'mixed\' + str(9 + i))\n    if include_top:\n        # Classification block\n        x = layers.GlobalAveragePooling2D(name=\'avg_pool\')(x)\n        x = layers.Dense(classes, activation=\'softmax\', name=\'predictions\')(x)\n    else:\n        if pooling == \'avg\':\n            x = layers.GlobalAveragePooling2D()(x)\n        elif pooling == \'max\':\n            x = layers.GlobalMaxPooling2D()(x)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = keras_utils.get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = models.Model(inputs, x, name=\'inception_v3\')\n\n    # Load weights.\n    if weights == \'imagenet\':\n        if include_top:\n            weights_path = keras_utils.get_file(\n                \'inception_v3_weights_tf_dim_ordering_tf_kernels.h5\',\n                WEIGHTS_PATH,\n                cache_subdir=\'models\',\n                file_hash=\'9a0d58056eeedaa3f26cb7ebd46da564\')\n        else:\n            weights_path = keras_utils.get_file(\n                \'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\',\n                WEIGHTS_PATH_NO_TOP,\n                cache_subdir=\'models\',\n                file_hash=\'bcbd6486424b2319ff4ef7d526e38f63\')\n        model.load_weights(weights_path)\n    elif weights is not None:\n        model.load_weights(weights)\n\n    return model\n\n\ndef preprocess_input(x, **kwargs):\n    """"""Preprocesses a numpy array encoding a batch of images.\n    # Arguments\n        x: a 4D numpy array consists of RGB values within [0, 255].\n    # Returns\n        Preprocessed array.\n    """"""\n    return imagenet_utils.preprocess_input(x, mode=\'tf\', **kwargs)\n'"
segmentation_models/base/__init__.py,0,"b'from .objects import KerasObject, Loss, Metric\nfrom . import functional'"
segmentation_models/base/functional.py,0,"b'SMOOTH = 1e-5\n\n\n# ----------------------------------------------------------------\n#   Helpers\n# ----------------------------------------------------------------\n\ndef _gather_channels(x, indexes, **kwargs):\n    """"""Slice tensor along channels axis by given indexes""""""\n    backend = kwargs[\'backend\']\n    if backend.image_data_format() == \'channels_last\':\n        x = backend.permute_dimensions(x, (3, 0, 1, 2))\n        x = backend.gather(x, indexes)\n        x = backend.permute_dimensions(x, (1, 2, 3, 0))\n    else:\n        x = backend.permute_dimensions(x, (1, 0, 2, 3))\n        x = backend.gather(x, indexes)\n        x = backend.permute_dimensions(x, (1, 0, 2, 3))\n    return x\n\n\ndef get_reduce_axes(per_image, **kwargs):\n    backend = kwargs[\'backend\']\n    axes = [1, 2] if backend.image_data_format() == \'channels_last\' else [2, 3]\n    if not per_image:\n        axes.insert(0, 0)\n    return axes\n\n\ndef gather_channels(*xs, indexes=None, **kwargs):\n    """"""Slice tensors along channels axis by given indexes""""""\n    if indexes is None:\n        return xs\n    elif isinstance(indexes, (int)):\n        indexes = [indexes]\n    xs = [_gather_channels(x, indexes=indexes, **kwargs) for x in xs]\n    return xs\n\n\ndef round_if_needed(x, threshold, **kwargs):\n    backend = kwargs[\'backend\']\n    if threshold is not None:\n        x = backend.greater(x, threshold)\n        x = backend.cast(x, backend.floatx())\n    return x\n\n\ndef average(x, per_image=False, class_weights=None, **kwargs):\n    backend = kwargs[\'backend\']\n    if per_image:\n        x = backend.mean(x, axis=0)\n    if class_weights is not None:\n        x = x * class_weights\n    return backend.mean(x)\n\n\n# ----------------------------------------------------------------\n#   Metric Functions\n# ----------------------------------------------------------------\n\ndef iou_score(gt, pr, class_weights=1., class_indexes=None, smooth=SMOOTH, per_image=False, threshold=None, **kwargs):\n    r"""""" The `Jaccard index`_, also known as Intersection over Union and the Jaccard similarity coefficient\n    (originally coined coefficient de communaut\xc3\xa9 by Paul Jaccard), is a statistic used for comparing the\n    similarity and diversity of sample sets. The Jaccard coefficient measures similarity between finite sample sets,\n    and is defined as the size of the intersection divided by the size of the union of the sample sets:\n\n    .. math:: J(A, B) = \\frac{A \\cap B}{A \\cup B}\n\n    Args:\n        gt: ground truth 4D keras tensor (B, H, W, C) or (B, C, H, W)\n        pr: prediction 4D keras tensor (B, H, W, C) or (B, C, H, W)\n        class_weights: 1. or list of class weights, len(weights) = C\n        class_indexes: Optional integer or list of integers, classes to consider, if ``None`` all classes are used.\n        smooth: value to avoid division by zero\n        per_image: if ``True``, metric is calculated as mean over images in batch (B),\n            else over whole batch\n        threshold: value to round predictions (use ``>`` comparison), if ``None`` prediction will not be round\n\n    Returns:\n        IoU/Jaccard score in range [0, 1]\n\n    .. _`Jaccard index`: https://en.wikipedia.org/wiki/Jaccard_index\n\n    """"""\n\n    backend = kwargs[\'backend\']\n\n    gt, pr = gather_channels(gt, pr, indexes=class_indexes, **kwargs)\n    pr = round_if_needed(pr, threshold, **kwargs)\n    axes = get_reduce_axes(per_image, **kwargs)\n\n    # score calculation\n    intersection = backend.sum(gt * pr, axis=axes)\n    union = backend.sum(gt + pr, axis=axes) - intersection\n\n    score = (intersection + smooth) / (union + smooth)\n    score = average(score, per_image, class_weights, **kwargs)\n\n    return score\n\n\ndef f_score(gt, pr, beta=1, class_weights=1, class_indexes=None, smooth=SMOOTH, per_image=False, threshold=None,\n            **kwargs):\n    r""""""The F-score (Dice coefficient) can be interpreted as a weighted average of the precision and recall,\n    where an F-score reaches its best value at 1 and worst score at 0.\n    The relative contribution of ``precision`` and ``recall`` to the F1-score are equal.\n    The formula for the F score is:\n\n    .. math:: F_\\beta(precision, recall) = (1 + \\beta^2) \\frac{precision \\cdot recall}\n        {\\beta^2 \\cdot precision + recall}\n\n    The formula in terms of *Type I* and *Type II* errors:\n\n    .. math:: F_\\beta(A, B) = \\frac{(1 + \\beta^2) TP} {(1 + \\beta^2) TP + \\beta^2 FN + FP}\n\n\n    where:\n        TP - true positive;\n        FP - false positive;\n        FN - false negative;\n\n    Args:\n        gt: ground truth 4D keras tensor (B, H, W, C) or (B, C, H, W)\n        pr: prediction 4D keras tensor (B, H, W, C) or (B, C, H, W)\n        class_weights: 1. or list of class weights, len(weights) = C\n        class_indexes: Optional integer or list of integers, classes to consider, if ``None`` all classes are used.\n        beta: f-score coefficient\n        smooth: value to avoid division by zero\n        per_image: if ``True``, metric is calculated as mean over images in batch (B),\n            else over whole batch\n        threshold: value to round predictions (use ``>`` comparison), if ``None`` prediction will not be round\n\n    Returns:\n        F-score in range [0, 1]\n\n    """"""\n\n    backend = kwargs[\'backend\']\n\n    gt, pr = gather_channels(gt, pr, indexes=class_indexes, **kwargs)\n    pr = round_if_needed(pr, threshold, **kwargs)\n    axes = get_reduce_axes(per_image, **kwargs)\n\n    # calculate score\n    tp = backend.sum(gt * pr, axis=axes)\n    fp = backend.sum(pr, axis=axes) - tp\n    fn = backend.sum(gt, axis=axes) - tp\n\n    score = ((1 + beta ** 2) * tp + smooth) \\\n            / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + smooth)\n    score = average(score, per_image, class_weights, **kwargs)\n\n    return score\n\n\ndef precision(gt, pr, class_weights=1, class_indexes=None, smooth=SMOOTH, per_image=False, threshold=None, **kwargs):\n    r""""""Calculate precision between the ground truth (gt) and the prediction (pr).\n\n    .. math:: F_\\beta(tp, fp) = \\frac{tp} {(tp + fp)}\n\n    where:\n         - tp - true positives;\n         - fp - false positives;\n\n    Args:\n        gt: ground truth 4D keras tensor (B, H, W, C) or (B, C, H, W)\n        pr: prediction 4D keras tensor (B, H, W, C) or (B, C, H, W)\n        class_weights: 1. or ``np.array`` of class weights (``len(weights) = num_classes``)\n        class_indexes: Optional integer or list of integers, classes to consider, if ``None`` all classes are used.\n        smooth: Float value to avoid division by zero.\n        per_image: If ``True``, metric is calculated as mean over images in batch (B),\n            else over whole batch.\n        threshold: Float value to round predictions (use ``>`` comparison), if ``None`` prediction will not be round.\n        name: Optional string, if ``None`` default ``precision`` name is used.\n\n    Returns:\n        float: precision score\n    """"""\n    backend = kwargs[\'backend\']\n\n    gt, pr = gather_channels(gt, pr, indexes=class_indexes, **kwargs)\n    pr = round_if_needed(pr, threshold, **kwargs)\n    axes = get_reduce_axes(per_image, **kwargs)\n\n    # score calculation\n    tp = backend.sum(gt * pr, axis=axes)\n    fp = backend.sum(pr, axis=axes) - tp\n    \n    score = (tp + smooth) / (tp + fp + smooth)\n    score = average(score, per_image, class_weights, **kwargs)\n\n    return score\n\n\ndef recall(gt, pr, class_weights=1, class_indexes=None, smooth=SMOOTH, per_image=False, threshold=None, **kwargs):\n    r""""""Calculate recall between the ground truth (gt) and the prediction (pr).\n\n    .. math:: F_\\beta(tp, fn) = \\frac{tp} {(tp + fn)}\n\n    where:\n         - tp - true positives;\n         - fp - false positives;\n\n    Args:\n        gt: ground truth 4D keras tensor (B, H, W, C) or (B, C, H, W)\n        pr: prediction 4D keras tensor (B, H, W, C) or (B, C, H, W)\n        class_weights: 1. or ``np.array`` of class weights (``len(weights) = num_classes``)\n        class_indexes: Optional integer or list of integers, classes to consider, if ``None`` all classes are used.\n        smooth: Float value to avoid division by zero.\n        per_image: If ``True``, metric is calculated as mean over images in batch (B),\n            else over whole batch.\n        threshold: Float value to round predictions (use ``>`` comparison), if ``None`` prediction will not be round.\n        name: Optional string, if ``None`` default ``precision`` name is used.\n\n    Returns:\n        float: recall score\n    """"""\n    backend = kwargs[\'backend\']\n\n    gt, pr = gather_channels(gt, pr, indexes=class_indexes, **kwargs)\n    pr = round_if_needed(pr, threshold, **kwargs)\n    axes = get_reduce_axes(per_image, **kwargs)\n\n    tp = backend.sum(gt * pr, axis=axes)\n    fn = backend.sum(gt, axis=axes) - tp\n\n    score = (tp + smooth) / (tp + fn + smooth)\n    score = average(score, per_image, class_weights, **kwargs)\n\n    return score\n\n\n# ----------------------------------------------------------------\n#   Loss Functions\n# ----------------------------------------------------------------\n\ndef categorical_crossentropy(gt, pr, class_weights=1., class_indexes=None, **kwargs):\n    backend = kwargs[\'backend\']\n\n    gt, pr = gather_channels(gt, pr, indexes=class_indexes, **kwargs)\n\n    # scale predictions so that the class probas of each sample sum to 1\n    axis = 3 if backend.image_data_format() == \'channels_last\' else 1\n    pr /= backend.sum(pr, axis=axis, keepdims=True)\n\n    # clip to prevent NaN\'s and Inf\'s\n    pr = backend.clip(pr, backend.epsilon(), 1 - backend.epsilon())\n\n    # calculate loss\n    output = gt * backend.log(pr) * class_weights\n    return - backend.mean(output)\n\n\ndef binary_crossentropy(gt, pr, **kwargs):\n    backend = kwargs[\'backend\']\n    return backend.mean(backend.binary_crossentropy(gt, pr))\n\n\ndef categorical_focal_loss(gt, pr, gamma=2.0, alpha=0.25, class_indexes=None, **kwargs):\n    r""""""Implementation of Focal Loss from the paper in multiclass classification\n\n    Formula:\n        loss = - gt * alpha * ((1 - pr)^gamma) * log(pr)\n\n    Args:\n        gt: ground truth 4D keras tensor (B, H, W, C) or (B, C, H, W)\n        pr: prediction 4D keras tensor (B, H, W, C) or (B, C, H, W)\n        alpha: the same as weighting factor in balanced cross entropy, default 0.25\n        gamma: focusing parameter for modulating factor (1-p), default 2.0\n        class_indexes: Optional integer or list of integers, classes to consider, if ``None`` all classes are used.\n\n    """"""\n\n    backend = kwargs[\'backend\']\n    gt, pr = gather_channels(gt, pr, indexes=class_indexes, **kwargs)\n\n    # clip to prevent NaN\'s and Inf\'s\n    pr = backend.clip(pr, backend.epsilon(), 1.0 - backend.epsilon())\n\n    # Calculate focal loss\n    loss = - gt * (alpha * backend.pow((1 - pr), gamma) * backend.log(pr))\n\n    return backend.mean(loss)\n\n\ndef binary_focal_loss(gt, pr, gamma=2.0, alpha=0.25, **kwargs):\n    r""""""Implementation of Focal Loss from the paper in binary classification\n\n    Formula:\n        loss = - gt * alpha * ((1 - pr)^gamma) * log(pr) \\\n               - (1 - gt) * alpha * (pr^gamma) * log(1 - pr)\n\n    Args:\n        gt: ground truth 4D keras tensor (B, H, W, C) or (B, C, H, W)\n        pr: prediction 4D keras tensor (B, H, W, C) or (B, C, H, W)\n        alpha: the same as weighting factor in balanced cross entropy, default 0.25\n        gamma: focusing parameter for modulating factor (1-p), default 2.0\n\n    """"""\n    backend = kwargs[\'backend\']\n\n    # clip to prevent NaN\'s and Inf\'s\n    pr = backend.clip(pr, backend.epsilon(), 1.0 - backend.epsilon())\n\n    loss_1 = - gt * (alpha * backend.pow((1 - pr), gamma) * backend.log(pr))\n    loss_0 = - (1 - gt) * ((1 - alpha) * backend.pow((pr), gamma) * backend.log(1 - pr))\n    loss = backend.mean(loss_0 + loss_1)\n    return loss\n'"
segmentation_models/base/objects.py,0,"b""class KerasObject:\n    _backend = None\n    _models = None\n    _layers = None\n    _utils = None\n\n    def __init__(self, name=None):\n        if (self.backend is None or\n                self.utils is None or\n                self.models is None or\n                self.layers is None):\n            raise RuntimeError('You cannot use `KerasObjects` with None submodules.')\n\n        self._name = name\n\n    @property\n    def __name__(self):\n        if self._name is None:\n            return self.__class__.__name__\n        return self._name\n\n    @property\n    def name(self):\n        return self.__name__\n\n    @name.setter\n    def name(self, name):\n        self._name = name\n\n    @classmethod\n    def set_submodules(cls, backend, layers, models, utils):\n        cls._backend = backend\n        cls._layers = layers\n        cls._models = models\n        cls._utils = utils\n\n    @property\n    def submodules(self):\n        return {\n            'backend': self.backend,\n            'layers': self.layers,\n            'models': self.models,\n            'utils': self.utils,\n        }\n\n    @property\n    def backend(self):\n        return self._backend\n\n    @property\n    def layers(self):\n        return self._layers\n\n    @property\n    def models(self):\n        return self._models\n\n    @property\n    def utils(self):\n        return self._utils\n\n\nclass Metric(KerasObject):\n    pass\n\n\nclass Loss(KerasObject):\n\n    def __add__(self, other):\n        if isinstance(other, Loss):\n            return SumOfLosses(self, other)\n        else:\n            raise ValueError('Loss should be inherited from `Loss` class')\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __mul__(self, value):\n        if isinstance(value, (int, float)):\n            return MultipliedLoss(self, value)\n        else:\n            raise ValueError('Loss should be inherited from `BaseLoss` class')\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n\nclass MultipliedLoss(Loss):\n\n    def __init__(self, loss, multiplier):\n\n        # resolve name\n        if len(loss.__name__.split('+')) > 1:\n            name = '{}({})'.format(multiplier, loss.__name__)\n        else:\n            name = '{}{}'.format(multiplier, loss.__name__)\n        super().__init__(name=name)\n        self.loss = loss\n        self.multiplier = multiplier\n\n    def __call__(self, gt, pr):\n        return self.multiplier * self.loss(gt, pr)\n\n\nclass SumOfLosses(Loss):\n\n    def __init__(self, l1, l2):\n        name = '{}_plus_{}'.format(l1.__name__, l2.__name__)\n        super().__init__(name=name)\n        self.l1 = l1\n        self.l2 = l2\n\n    def __call__(self, gt, pr):\n        return self.l1(gt, pr) + self.l2(gt, pr)\n"""
segmentation_models/models/__init__.py,0,b''
segmentation_models/models/_common_blocks.py,0,"b'from keras_applications import get_submodules_from_kwargs\n\n\ndef Conv2dBn(\n        filters,\n        kernel_size,\n        strides=(1, 1),\n        padding=\'valid\',\n        data_format=None,\n        dilation_rate=(1, 1),\n        activation=None,\n        kernel_initializer=\'glorot_uniform\',\n        bias_initializer=\'zeros\',\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        use_batchnorm=False,\n        **kwargs\n):\n    """"""Extension of Conv2D layer with batchnorm""""""\n\n    conv_name, act_name, bn_name = None, None, None\n    block_name = kwargs.pop(\'name\', None)\n    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n\n    if block_name is not None:\n        conv_name = block_name + \'_conv\'\n\n    if block_name is not None and activation is not None:\n        act_str = activation.__name__ if callable(activation) else str(activation)\n        act_name = block_name + \'_\' + act_str\n\n    if block_name is not None and use_batchnorm:\n        bn_name = block_name + \'_bn\'\n\n    bn_axis = 3 if backend.image_data_format() == \'channels_last\' else 1\n\n    def wrapper(input_tensor):\n\n        x = layers.Conv2D(\n            filters=filters,\n            kernel_size=kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            activation=None,\n            use_bias=not (use_batchnorm),\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            name=conv_name,\n        )(input_tensor)\n\n        if use_batchnorm:\n            x = layers.BatchNormalization(axis=bn_axis, name=bn_name)(x)\n\n        if activation:\n            x = layers.Activation(activation, name=act_name)(x)\n\n        return x\n\n    return wrapper\n'"
segmentation_models/models/_utils.py,0,"b'from keras_applications import get_submodules_from_kwargs\n\n\ndef freeze_model(model, **kwargs):\n    """"""Set all layers non trainable, excluding BatchNormalization layers""""""\n    _, layers, _, _ = get_submodules_from_kwargs(kwargs)\n    for layer in model.layers:\n        if not isinstance(layer, layers.BatchNormalization):\n            layer.trainable = False\n    return\n\n\ndef filter_keras_submodules(kwargs):\n    """"""Selects only arguments that define keras_application submodules. """"""\n    submodule_keys = kwargs.keys() & {\'backend\', \'layers\', \'models\', \'utils\'}\n    return {key: kwargs[key] for key in submodule_keys}\n'"
segmentation_models/models/fpn.py,0,"b'from keras_applications import get_submodules_from_kwargs\n\nfrom ._common_blocks import Conv2dBn\nfrom ._utils import freeze_model, filter_keras_submodules\nfrom ..backbones.backbones_factory import Backbones\n\nbackend = None\nlayers = None\nmodels = None\nkeras_utils = None\n\n\n# ---------------------------------------------------------------------\n#  Utility functions\n# ---------------------------------------------------------------------\n\ndef get_submodules():\n    return {\n        \'backend\': backend,\n        \'models\': models,\n        \'layers\': layers,\n        \'utils\': keras_utils,\n    }\n\n\n# ---------------------------------------------------------------------\n#  Blocks\n# ---------------------------------------------------------------------\n\ndef Conv3x3BnReLU(filters, use_batchnorm, name=None):\n    kwargs = get_submodules()\n\n    def wrapper(input_tensor):\n        return Conv2dBn(\n            filters,\n            kernel_size=3,\n            activation=\'relu\',\n            kernel_initializer=\'he_uniform\',\n            padding=\'same\',\n            use_batchnorm=use_batchnorm,\n            name=name,\n            **kwargs\n        )(input_tensor)\n\n    return wrapper\n\n\ndef DoubleConv3x3BnReLU(filters, use_batchnorm, name=None):\n    name1, name2 = None, None\n    if name is not None:\n        name1 = name + \'a\'\n        name2 = name + \'b\'\n\n    def wrapper(input_tensor):\n        x = Conv3x3BnReLU(filters, use_batchnorm, name=name1)(input_tensor)\n        x = Conv3x3BnReLU(filters, use_batchnorm, name=name2)(x)\n        return x\n\n    return wrapper\n\n\ndef FPNBlock(pyramid_filters, stage):\n    conv0_name = \'fpn_stage_p{}_pre_conv\'.format(stage)\n    conv1_name = \'fpn_stage_p{}_conv\'.format(stage)\n    add_name = \'fpn_stage_p{}_add\'.format(stage)\n    up_name = \'fpn_stage_p{}_upsampling\'.format(stage)\n\n    channels_axis = 3 if backend.image_data_format() == \'channels_last\' else 1\n\n    def wrapper(input_tensor, skip):\n        # if input tensor channels not equal to pyramid channels\n        # we will not be able to sum input tensor and skip\n        # so add extra conv layer to transform it\n        input_filters = backend.int_shape(input_tensor)[channels_axis]\n        if input_filters != pyramid_filters:\n            input_tensor = layers.Conv2D(\n                filters=pyramid_filters,\n                kernel_size=(1, 1),\n                kernel_initializer=\'he_uniform\',\n                name=conv0_name,\n            )(input_tensor)\n\n        skip = layers.Conv2D(\n            filters=pyramid_filters,\n            kernel_size=(1, 1),\n            kernel_initializer=\'he_uniform\',\n            name=conv1_name,\n        )(skip)\n\n        x = layers.UpSampling2D((2, 2), name=up_name)(input_tensor)\n        x = layers.Add(name=add_name)([x, skip])\n\n        return x\n\n    return wrapper\n\n\n# ---------------------------------------------------------------------\n#  FPN Decoder\n# ---------------------------------------------------------------------\n\ndef build_fpn(\n        backbone,\n        skip_connection_layers,\n        pyramid_filters=256,\n        segmentation_filters=128,\n        classes=1,\n        activation=\'sigmoid\',\n        use_batchnorm=True,\n        aggregation=\'sum\',\n        dropout=None,\n):\n    input_ = backbone.input\n    x = backbone.output\n\n    # building decoder blocks with skip connections\n    skips = ([backbone.get_layer(name=i).output if isinstance(i, str)\n              else backbone.get_layer(index=i).output for i in skip_connection_layers])\n\n    # build FPN pyramid\n    p5 = FPNBlock(pyramid_filters, stage=5)(x, skips[0])\n    p4 = FPNBlock(pyramid_filters, stage=4)(p5, skips[1])\n    p3 = FPNBlock(pyramid_filters, stage=3)(p4, skips[2])\n    p2 = FPNBlock(pyramid_filters, stage=2)(p3, skips[3])\n\n    # add segmentation head to each\n    s5 = DoubleConv3x3BnReLU(segmentation_filters, use_batchnorm, name=\'segm_stage5\')(p5)\n    s4 = DoubleConv3x3BnReLU(segmentation_filters, use_batchnorm, name=\'segm_stage4\')(p4)\n    s3 = DoubleConv3x3BnReLU(segmentation_filters, use_batchnorm, name=\'segm_stage3\')(p3)\n    s2 = DoubleConv3x3BnReLU(segmentation_filters, use_batchnorm, name=\'segm_stage2\')(p2)\n\n    # upsampling to same resolution\n    s5 = layers.UpSampling2D((8, 8), interpolation=\'nearest\', name=\'upsampling_stage5\')(s5)\n    s4 = layers.UpSampling2D((4, 4), interpolation=\'nearest\', name=\'upsampling_stage4\')(s4)\n    s3 = layers.UpSampling2D((2, 2), interpolation=\'nearest\', name=\'upsampling_stage3\')(s3)\n\n    # aggregating results\n    if aggregation == \'sum\':\n        x = layers.Add(name=\'aggregation_sum\')([s2, s3, s4, s5])\n    elif aggregation == \'concat\':\n        concat_axis = 3 if backend.image_data_format() == \'channels_last\' else 1\n        x = layers.Concatenate(axis=concat_axis, name=\'aggregation_concat\')([s2, s3, s4, s5])\n    else:\n        raise ValueError(\'Aggregation parameter should be in (""sum"", ""concat""), \'\n                         \'got {}\'.format(aggregation))\n\n    if dropout:\n        x = layers.SpatialDropout2D(dropout, name=\'pyramid_dropout\')(x)\n\n    # final stage\n    x = Conv3x3BnReLU(segmentation_filters, use_batchnorm, name=\'final_stage\')(x)\n    x = layers.UpSampling2D(size=(2, 2), interpolation=\'bilinear\', name=\'final_upsampling\')(x)\n\n    # model head (define number of output classes)\n    x = layers.Conv2D(\n        filters=classes,\n        kernel_size=(3, 3),\n        padding=\'same\',\n        use_bias=True,\n        kernel_initializer=\'glorot_uniform\',\n        name=\'head_conv\',\n    )(x)\n    x = layers.Activation(activation, name=activation)(x)\n\n    # create keras model instance\n    model = models.Model(input_, x)\n\n    return model\n\n\n# ---------------------------------------------------------------------\n#  FPN Model\n# ---------------------------------------------------------------------\n\ndef FPN(\n        backbone_name=\'vgg16\',\n        input_shape=(None, None, 3),\n        classes=21,\n        activation=\'softmax\',\n        weights=None,\n        encoder_weights=\'imagenet\',\n        encoder_freeze=False,\n        encoder_features=\'default\',\n        pyramid_block_filters=256,\n        pyramid_use_batchnorm=True,\n        pyramid_aggregation=\'concat\',\n        pyramid_dropout=None,\n        **kwargs\n):\n    """"""FPN_ is a fully convolution neural network for image semantic segmentation\n\n    Args:\n        backbone_name: name of classification model (without last dense layers) used as feature\n                extractor to build segmentation model.\n        input_shape: shape of input data/image ``(H, W, C)``, in general\n                case you do not need to set ``H`` and ``W`` shapes, just pass ``(None, None, C)`` to make your model be\n                able to process images af any size, but ``H`` and ``W`` of input images should be divisible by factor ``32``.\n        classes: a number of classes for output (output shape - ``(h, w, classes)``).\n        weights: optional, path to model weights.\n        activation: name of one of ``keras.activations`` for last model layer (e.g. ``sigmoid``, ``softmax``, ``linear``).\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        encoder_freeze: if ``True`` set all layers of encoder (backbone model) as non-trainable.\n        encoder_features: a list of layer numbers or names starting from top of the model.\n                Each of these layers will be used to build features pyramid. If ``default`` is used\n                layer names are taken from ``DEFAULT_FEATURE_PYRAMID_LAYERS``.\n        pyramid_block_filters: a number of filters in Feature Pyramid Block of FPN_.\n        pyramid_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n                is used.\n        pyramid_aggregation: one of \'sum\' or \'concat\'. The way to aggregate pyramid blocks.\n        pyramid_dropout: spatial dropout rate for feature pyramid in range (0, 1).\n\n    Returns:\n        ``keras.models.Model``: **FPN**\n\n    .. _FPN:\n        http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n\n    """"""\n    global backend, layers, models, keras_utils\n    submodule_args = filter_keras_submodules(kwargs)\n    backend, layers, models, keras_utils = get_submodules_from_kwargs(submodule_args)\n\n    backbone = Backbones.get_backbone(\n        backbone_name,\n        input_shape=input_shape,\n        weights=encoder_weights,\n        include_top=False,\n        **kwargs,\n    )\n\n    if encoder_features == \'default\':\n        encoder_features = Backbones.get_feature_layers(backbone_name, n=4)\n\n    model = build_fpn(\n        backbone=backbone,\n        skip_connection_layers=encoder_features,\n        pyramid_filters=pyramid_block_filters,\n        segmentation_filters=pyramid_block_filters // 2,\n        use_batchnorm=pyramid_use_batchnorm,\n        dropout=pyramid_dropout,\n        activation=activation,\n        classes=classes,\n        aggregation=pyramid_aggregation,\n    )\n\n    # lock encoder weights for fine-tuning\n    if encoder_freeze:\n        freeze_model(backbone, **kwargs)\n\n    # loading model weights\n    if weights is not None:\n        model.load_weights(weights)\n\n    return model\n'"
segmentation_models/models/linknet.py,0,"b'from keras_applications import get_submodules_from_kwargs\n\nfrom ._common_blocks import Conv2dBn\nfrom ._utils import freeze_model, filter_keras_submodules\nfrom ..backbones.backbones_factory import Backbones\n\nbackend = None\nlayers = None\nmodels = None\nkeras_utils = None\n\n\n# ---------------------------------------------------------------------\n#  Utility functions\n# ---------------------------------------------------------------------\n\ndef get_submodules():\n    return {\n        \'backend\': backend,\n        \'models\': models,\n        \'layers\': layers,\n        \'utils\': keras_utils,\n    }\n\n\n# ---------------------------------------------------------------------\n#  Blocks\n# ---------------------------------------------------------------------\n\ndef Conv3x3BnReLU(filters, use_batchnorm, name=None):\n    kwargs = get_submodules()\n\n    def wrapper(input_tensor):\n        return Conv2dBn(\n            filters,\n            kernel_size=3,\n            activation=\'relu\',\n            kernel_initializer=\'he_uniform\',\n            padding=\'same\',\n            use_batchnorm=use_batchnorm,\n            name=name,\n            **kwargs\n        )(input_tensor)\n\n    return wrapper\n\n\ndef Conv1x1BnReLU(filters, use_batchnorm, name=None):\n    kwargs = get_submodules()\n\n    def wrapper(input_tensor):\n        return Conv2dBn(\n            filters,\n            kernel_size=1,\n            activation=\'relu\',\n            kernel_initializer=\'he_uniform\',\n            padding=\'same\',\n            use_batchnorm=use_batchnorm,\n            name=name,\n            **kwargs\n        )(input_tensor)\n\n    return wrapper\n\n\ndef DecoderUpsamplingX2Block(filters, stage, use_batchnorm):\n    conv_block1_name = \'decoder_stage{}a\'.format(stage)\n    conv_block2_name = \'decoder_stage{}b\'.format(stage)\n    conv_block3_name = \'decoder_stage{}c\'.format(stage)\n    up_name = \'decoder_stage{}_upsampling\'.format(stage)\n    add_name = \'decoder_stage{}_add\'.format(stage)\n\n    channels_axis = 3 if backend.image_data_format() == \'channels_last\' else 1\n\n    def wrapper(input_tensor, skip=None):\n        input_filters = backend.int_shape(input_tensor)[channels_axis]\n        output_filters = backend.int_shape(skip)[channels_axis] if skip is not None else filters\n\n        x = Conv1x1BnReLU(input_filters // 4, use_batchnorm, name=conv_block1_name)(input_tensor)\n        x = layers.UpSampling2D((2, 2), name=up_name)(x)\n        x = Conv3x3BnReLU(input_filters // 4, use_batchnorm, name=conv_block2_name)(x)\n        x = Conv1x1BnReLU(output_filters, use_batchnorm, name=conv_block3_name)(x)\n\n        if skip is not None:\n            x = layers.Add(name=add_name)([x, skip])\n        return x\n\n    return wrapper\n\n\ndef DecoderTransposeX2Block(filters, stage, use_batchnorm):\n    conv_block1_name = \'decoder_stage{}a\'.format(stage)\n    transpose_name = \'decoder_stage{}b_transpose\'.format(stage)\n    bn_name = \'decoder_stage{}b_bn\'.format(stage)\n    relu_name = \'decoder_stage{}b_relu\'.format(stage)\n    conv_block3_name = \'decoder_stage{}c\'.format(stage)\n    add_name = \'decoder_stage{}_add\'.format(stage)\n\n    channels_axis = bn_axis = 3 if backend.image_data_format() == \'channels_last\' else 1\n\n    def wrapper(input_tensor, skip=None):\n        input_filters = backend.int_shape(input_tensor)[channels_axis]\n        output_filters = backend.int_shape(skip)[channels_axis] if skip is not None else filters\n\n        x = Conv1x1BnReLU(input_filters // 4, use_batchnorm, name=conv_block1_name)(input_tensor)\n        x = layers.Conv2DTranspose(\n            filters=input_filters // 4,\n            kernel_size=(4, 4),\n            strides=(2, 2),\n            padding=\'same\',\n            name=transpose_name,\n            use_bias=not use_batchnorm,\n        )(x)\n\n        if use_batchnorm:\n            x = layers.BatchNormalization(axis=bn_axis, name=bn_name)(x)\n\n        x = layers.Activation(\'relu\', name=relu_name)(x)\n        x = Conv1x1BnReLU(output_filters, use_batchnorm, name=conv_block3_name)(x)\n\n        if skip is not None:\n            x = layers.Add(name=add_name)([x, skip])\n\n        return x\n\n    return wrapper\n\n\n# ---------------------------------------------------------------------\n#  LinkNet Decoder\n# ---------------------------------------------------------------------\n\ndef build_linknet(\n        backbone,\n        decoder_block,\n        skip_connection_layers,\n        decoder_filters=(256, 128, 64, 32, 16),\n        n_upsample_blocks=5,\n        classes=1,\n        activation=\'sigmoid\',\n        use_batchnorm=True,\n):\n    input_ = backbone.input\n    x = backbone.output\n\n    # extract skip connections\n    skips = ([backbone.get_layer(name=i).output if isinstance(i, str)\n              else backbone.get_layer(index=i).output for i in skip_connection_layers])\n\n    # add center block if previous operation was maxpooling (for vgg models)\n    if isinstance(backbone.layers[-1], layers.MaxPooling2D):\n        x = Conv3x3BnReLU(512, use_batchnorm, name=\'center_block1\')(x)\n        x = Conv3x3BnReLU(512, use_batchnorm, name=\'center_block2\')(x)\n\n    # building decoder blocks\n    for i in range(n_upsample_blocks):\n\n        if i < len(skips):\n            skip = skips[i]\n        else:\n            skip = None\n\n        x = decoder_block(decoder_filters[i], stage=i, use_batchnorm=use_batchnorm)(x, skip)\n\n    # model head (define number of output classes)\n    x = layers.Conv2D(\n        filters=classes,\n        kernel_size=(3, 3),\n        padding=\'same\',\n        use_bias=True,\n        kernel_initializer=\'glorot_uniform\'\n    )(x)\n    x = layers.Activation(activation, name=activation)(x)\n\n    # create keras model instance\n    model = models.Model(input_, x)\n\n    return model\n\n\n# ---------------------------------------------------------------------\n#  LinkNet Model\n# ---------------------------------------------------------------------\n\ndef Linknet(\n        backbone_name=\'vgg16\',\n        input_shape=(None, None, 3),\n        classes=1,\n        activation=\'sigmoid\',\n        weights=None,\n        encoder_weights=\'imagenet\',\n        encoder_freeze=False,\n        encoder_features=\'default\',\n        decoder_block_type=\'upsampling\',\n        decoder_filters=(None, None, None, None, 16),\n        decoder_use_batchnorm=True,\n        **kwargs\n):\n    """"""Linknet_ is a fully convolution neural network for fast image semantic segmentation\n\n    Note:\n        This implementation by default has 4 skip connections (original - 3).\n\n    Args:\n        backbone_name: name of classification model (without last dense layers) used as feature\n                    extractor to build segmentation model.\n        input_shape: shape of input data/image ``(H, W, C)``, in general\n                case you do not need to set ``H`` and ``W`` shapes, just pass ``(None, None, C)`` to make your model be\n                able to process images af any size, but ``H`` and ``W`` of input images should be divisible by factor ``32``.\n        classes: a number of classes for output (output shape - ``(h, w, classes)``).\n        activation: name of one of ``keras.activations`` for last model layer\n            (e.g. ``sigmoid``, ``softmax``, ``linear``).\n        weights: optional, path to model weights.\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        encoder_freeze: if ``True`` set all layers of encoder (backbone model) as non-trainable.\n        encoder_features: a list of layer numbers or names starting from top of the model.\n                    Each of these layers will be concatenated with corresponding decoder block. If ``default`` is used\n                    layer names are taken from ``DEFAULT_SKIP_CONNECTIONS``.\n        decoder_filters: list of numbers of ``Conv2D`` layer filters in decoder blocks,\n            for block with skip connection a number of filters is equal to number of filters in\n            corresponding encoder block (estimates automatically and can be passed as ``None`` value).\n        decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n                    is used.\n        decoder_block_type: one of\n                    - `upsampling`:  use ``UpSampling2D`` keras layer\n                    - `transpose`:   use ``Transpose2D`` keras layer\n\n    Returns:\n        ``keras.models.Model``: **Linknet**\n\n    .. _Linknet:\n        https://arxiv.org/pdf/1707.03718.pdf\n    """"""\n\n    global backend, layers, models, keras_utils\n    submodule_args = filter_keras_submodules(kwargs)\n    backend, layers, models, keras_utils = get_submodules_from_kwargs(submodule_args)\n\n    if decoder_block_type == \'upsampling\':\n        decoder_block = DecoderUpsamplingX2Block\n    elif decoder_block_type == \'transpose\':\n        decoder_block = DecoderTransposeX2Block\n    else:\n        raise ValueError(\'Decoder block type should be in (""upsampling"", ""transpose""). \'\n                         \'Got: {}\'.format(decoder_block_type))\n\n    backbone = Backbones.get_backbone(\n        backbone_name,\n        input_shape=input_shape,\n        weights=encoder_weights,\n        include_top=False,\n        **kwargs,\n    )\n\n    if encoder_features == \'default\':\n        encoder_features = Backbones.get_feature_layers(backbone_name, n=4)\n\n    model = build_linknet(\n        backbone=backbone,\n        decoder_block=decoder_block,\n        skip_connection_layers=encoder_features,\n        decoder_filters=decoder_filters,\n        classes=classes,\n        activation=activation,\n        n_upsample_blocks=len(decoder_filters),\n        use_batchnorm=decoder_use_batchnorm,\n    )\n\n    # lock encoder weights for fine-tuning\n    if encoder_freeze:\n        freeze_model(backbone, **kwargs)\n\n    # loading model weights\n    if weights is not None:\n        model.load_weights(weights)\n\n    return model\n'"
segmentation_models/models/pspnet.py,0,"b'from keras_applications import get_submodules_from_kwargs\n\nfrom ._common_blocks import Conv2dBn\nfrom ._utils import freeze_model, filter_keras_submodules\nfrom ..backbones.backbones_factory import Backbones\n\nbackend = None\nlayers = None\nmodels = None\nkeras_utils = None\n\n\n# ---------------------------------------------------------------------\n#  Utility functions\n# ---------------------------------------------------------------------\n\ndef get_submodules():\n    return {\n        \'backend\': backend,\n        \'models\': models,\n        \'layers\': layers,\n        \'utils\': keras_utils,\n    }\n\n\ndef check_input_shape(input_shape, factor):\n    if input_shape is None:\n        raise ValueError(""Input shape should be a tuple of 3 integers, not None!"")\n\n    h, w = input_shape[:2] if backend.image_data_format() == \'channels_last\' else input_shape[1:]\n    min_size = factor * 6\n\n    is_wrong_shape = (\n            h % min_size != 0 or w % min_size != 0 or\n            h < min_size or w < min_size\n    )\n\n    if is_wrong_shape:\n        raise ValueError(\'Wrong shape {}, input H and W should \'.format(input_shape) +\n                         \'be divisible by `{}`\'.format(min_size))\n\n\n# ---------------------------------------------------------------------\n#  Blocks\n# ---------------------------------------------------------------------\n\ndef Conv1x1BnReLU(filters, use_batchnorm, name=None):\n    kwargs = get_submodules()\n\n    def wrapper(input_tensor):\n        return Conv2dBn(\n            filters,\n            kernel_size=1,\n            activation=\'relu\',\n            kernel_initializer=\'he_uniform\',\n            padding=\'same\',\n            use_batchnorm=use_batchnorm,\n            name=name,\n            **kwargs\n        )(input_tensor)\n\n    return wrapper\n\n\ndef SpatialContextBlock(\n        level,\n        conv_filters=512,\n        pooling_type=\'avg\',\n        use_batchnorm=True,\n):\n    if pooling_type not in (\'max\', \'avg\'):\n        raise ValueError(\'Unsupported pooling type - `{}`.\'.format(pooling_type) +\n                         \'Use `avg` or `max`.\')\n\n    Pooling2D = layers.MaxPool2D if pooling_type == \'max\' else layers.AveragePooling2D\n\n    pooling_name = \'psp_level{}_pooling\'.format(level)\n    conv_block_name = \'psp_level{}\'.format(level)\n    upsampling_name = \'psp_level{}_upsampling\'.format(level)\n\n    def wrapper(input_tensor):\n        # extract input feature maps size (h, and w dimensions)\n        input_shape = backend.int_shape(input_tensor)\n        spatial_size = input_shape[1:3] if backend.image_data_format() == \'channels_last\' else input_shape[2:]\n\n        # Compute the kernel and stride sizes according to how large the final feature map will be\n        # When the kernel factor and strides are equal, then we can compute the final feature map factor\n        # by simply dividing the current factor by the kernel or stride factor\n        # The final feature map sizes are 1x1, 2x2, 3x3, and 6x6.\n        pool_size = up_size = [spatial_size[0] // level, spatial_size[1] // level]\n\n        x = Pooling2D(pool_size, strides=pool_size, padding=\'same\', name=pooling_name)(input_tensor)\n        x = Conv1x1BnReLU(conv_filters, use_batchnorm, name=conv_block_name)(x)\n        x = layers.UpSampling2D(up_size, interpolation=\'bilinear\', name=upsampling_name)(x)\n        return x\n\n    return wrapper\n\n\n# ---------------------------------------------------------------------\n#  PSP Decoder\n# ---------------------------------------------------------------------\n\ndef build_psp(\n        backbone,\n        psp_layer_idx,\n        pooling_type=\'avg\',\n        conv_filters=512,\n        use_batchnorm=True,\n        final_upsampling_factor=8,\n        classes=21,\n        activation=\'softmax\',\n        dropout=None,\n):\n    input_ = backbone.input\n    x = (backbone.get_layer(name=psp_layer_idx).output if isinstance(psp_layer_idx, str)\n         else backbone.get_layer(index=psp_layer_idx).output)\n\n    # build spatial pyramid\n    x1 = SpatialContextBlock(1, conv_filters, pooling_type, use_batchnorm)(x)\n    x2 = SpatialContextBlock(2, conv_filters, pooling_type, use_batchnorm)(x)\n    x3 = SpatialContextBlock(3, conv_filters, pooling_type, use_batchnorm)(x)\n    x6 = SpatialContextBlock(6, conv_filters, pooling_type, use_batchnorm)(x)\n\n    # aggregate spatial pyramid\n    concat_axis = 3 if backend.image_data_format() == \'channels_last\' else 1\n    x = layers.Concatenate(axis=concat_axis, name=\'psp_concat\')([x, x1, x2, x3, x6])\n    x = Conv1x1BnReLU(conv_filters, use_batchnorm, name=\'aggregation\')(x)\n\n    # model regularization\n    if dropout is not None:\n        x = layers.SpatialDropout2D(dropout, name=\'spatial_dropout\')(x)\n\n    # model head\n    x = layers.Conv2D(\n        filters=classes,\n        kernel_size=(3, 3),\n        padding=\'same\',\n        kernel_initializer=\'glorot_uniform\',\n        name=\'final_conv\',\n    )(x)\n\n    x = layers.UpSampling2D(final_upsampling_factor, name=\'final_upsampling\', interpolation=\'bilinear\')(x)\n    x = layers.Activation(activation, name=activation)(x)\n\n    model = models.Model(input_, x)\n\n    return model\n\n\n# ---------------------------------------------------------------------\n#  PSP Model\n# ---------------------------------------------------------------------\n\ndef PSPNet(\n        backbone_name=\'vgg16\',\n        input_shape=(384, 384, 3),\n        classes=21,\n        activation=\'softmax\',\n        weights=None,\n        encoder_weights=\'imagenet\',\n        encoder_freeze=False,\n        downsample_factor=8,\n        psp_conv_filters=512,\n        psp_pooling_type=\'avg\',\n        psp_use_batchnorm=True,\n        psp_dropout=None,\n        **kwargs\n):\n    """"""PSPNet_ is a fully convolution neural network for image semantic segmentation\n\n    Args:\n        backbone_name: name of classification model used as feature\n                extractor to build segmentation model.\n        input_shape: shape of input data/image ``(H, W, C)``.\n            ``H`` and ``W`` should be divisible by ``6 * downsample_factor`` and **NOT** ``None``!\n        classes: a number of classes for output (output shape - ``(h, w, classes)``).\n        activation: name of one of ``keras.activations`` for last model layer\n                (e.g. ``sigmoid``, ``softmax``, ``linear``).\n        weights: optional, path to model weights.\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        encoder_freeze: if ``True`` set all layers of encoder (backbone model) as non-trainable.\n        downsample_factor: one of 4, 8 and 16. Downsampling rate or in other words backbone depth\n            to construct PSP module on it.\n        psp_conv_filters: number of filters in ``Conv2D`` layer in each PSP block.\n        psp_pooling_type: one of \'avg\', \'max\'. PSP block pooling type (maximum or average).\n        psp_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n                is used.\n        psp_dropout: dropout rate between 0 and 1.\n\n    Returns:\n        ``keras.models.Model``: **PSPNet**\n\n    .. _PSPNet:\n        https://arxiv.org/pdf/1612.01105.pdf\n\n    """"""\n\n    global backend, layers, models, keras_utils\n    submodule_args = filter_keras_submodules(kwargs)\n    backend, layers, models, keras_utils = get_submodules_from_kwargs(submodule_args)\n\n    # control image input shape\n    check_input_shape(input_shape, downsample_factor)\n\n    backbone = Backbones.get_backbone(\n        backbone_name,\n        input_shape=input_shape,\n        weights=encoder_weights,\n        include_top=False,\n        **kwargs\n    )\n\n    feature_layers = Backbones.get_feature_layers(backbone_name, n=3)\n\n    if downsample_factor == 16:\n        psp_layer_idx = feature_layers[0]\n    elif downsample_factor == 8:\n        psp_layer_idx = feature_layers[1]\n    elif downsample_factor == 4:\n        psp_layer_idx = feature_layers[2]\n    else:\n        raise ValueError(\'Unsupported factor - `{}`, Use 4, 8 or 16.\'.format(downsample_factor))\n\n    model = build_psp(\n        backbone,\n        psp_layer_idx,\n        pooling_type=psp_pooling_type,\n        conv_filters=psp_conv_filters,\n        use_batchnorm=psp_use_batchnorm,\n        final_upsampling_factor=downsample_factor,\n        classes=classes,\n        activation=activation,\n        dropout=psp_dropout,\n    )\n\n    # lock encoder weights for fine-tuning\n    if encoder_freeze:\n        freeze_model(backbone, **kwargs)\n\n    # loading model weights\n    if weights is not None:\n        model.load_weights(weights)\n\n    return model\n'"
segmentation_models/models/unet.py,0,"b'from keras_applications import get_submodules_from_kwargs\n\nfrom ._common_blocks import Conv2dBn\nfrom ._utils import freeze_model, filter_keras_submodules\nfrom ..backbones.backbones_factory import Backbones\n\nbackend = None\nlayers = None\nmodels = None\nkeras_utils = None\n\n\n# ---------------------------------------------------------------------\n#  Utility functions\n# ---------------------------------------------------------------------\n\ndef get_submodules():\n    return {\n        \'backend\': backend,\n        \'models\': models,\n        \'layers\': layers,\n        \'utils\': keras_utils,\n    }\n\n\n# ---------------------------------------------------------------------\n#  Blocks\n# ---------------------------------------------------------------------\n\ndef Conv3x3BnReLU(filters, use_batchnorm, name=None):\n    kwargs = get_submodules()\n\n    def wrapper(input_tensor):\n        return Conv2dBn(\n            filters,\n            kernel_size=3,\n            activation=\'relu\',\n            kernel_initializer=\'he_uniform\',\n            padding=\'same\',\n            use_batchnorm=use_batchnorm,\n            name=name,\n            **kwargs\n        )(input_tensor)\n\n    return wrapper\n\n\ndef DecoderUpsamplingX2Block(filters, stage, use_batchnorm=False):\n    up_name = \'decoder_stage{}_upsampling\'.format(stage)\n    conv1_name = \'decoder_stage{}a\'.format(stage)\n    conv2_name = \'decoder_stage{}b\'.format(stage)\n    concat_name = \'decoder_stage{}_concat\'.format(stage)\n\n    concat_axis = 3 if backend.image_data_format() == \'channels_last\' else 1\n\n    def wrapper(input_tensor, skip=None):\n        x = layers.UpSampling2D(size=2, name=up_name)(input_tensor)\n\n        if skip is not None:\n            x = layers.Concatenate(axis=concat_axis, name=concat_name)([x, skip])\n\n        x = Conv3x3BnReLU(filters, use_batchnorm, name=conv1_name)(x)\n        x = Conv3x3BnReLU(filters, use_batchnorm, name=conv2_name)(x)\n\n        return x\n\n    return wrapper\n\n\ndef DecoderTransposeX2Block(filters, stage, use_batchnorm=False):\n    transp_name = \'decoder_stage{}a_transpose\'.format(stage)\n    bn_name = \'decoder_stage{}a_bn\'.format(stage)\n    relu_name = \'decoder_stage{}a_relu\'.format(stage)\n    conv_block_name = \'decoder_stage{}b\'.format(stage)\n    concat_name = \'decoder_stage{}_concat\'.format(stage)\n\n    concat_axis = bn_axis = 3 if backend.image_data_format() == \'channels_last\' else 1\n\n    def layer(input_tensor, skip=None):\n\n        x = layers.Conv2DTranspose(\n            filters,\n            kernel_size=(4, 4),\n            strides=(2, 2),\n            padding=\'same\',\n            name=transp_name,\n            use_bias=not use_batchnorm,\n        )(input_tensor)\n\n        if use_batchnorm:\n            x = layers.BatchNormalization(axis=bn_axis, name=bn_name)(x)\n\n        x = layers.Activation(\'relu\', name=relu_name)(x)\n\n        if skip is not None:\n            x = layers.Concatenate(axis=concat_axis, name=concat_name)([x, skip])\n\n        x = Conv3x3BnReLU(filters, use_batchnorm, name=conv_block_name)(x)\n\n        return x\n\n    return layer\n\n\n# ---------------------------------------------------------------------\n#  Unet Decoder\n# ---------------------------------------------------------------------\n\ndef build_unet(\n        backbone,\n        decoder_block,\n        skip_connection_layers,\n        decoder_filters=(256, 128, 64, 32, 16),\n        n_upsample_blocks=5,\n        classes=1,\n        activation=\'sigmoid\',\n        use_batchnorm=True,\n):\n    input_ = backbone.input\n    x = backbone.output\n\n    # extract skip connections\n    skips = ([backbone.get_layer(name=i).output if isinstance(i, str)\n              else backbone.get_layer(index=i).output for i in skip_connection_layers])\n\n    # add center block if previous operation was maxpooling (for vgg models)\n    if isinstance(backbone.layers[-1], layers.MaxPooling2D):\n        x = Conv3x3BnReLU(512, use_batchnorm, name=\'center_block1\')(x)\n        x = Conv3x3BnReLU(512, use_batchnorm, name=\'center_block2\')(x)\n\n    # building decoder blocks\n    for i in range(n_upsample_blocks):\n\n        if i < len(skips):\n            skip = skips[i]\n        else:\n            skip = None\n\n        x = decoder_block(decoder_filters[i], stage=i, use_batchnorm=use_batchnorm)(x, skip)\n\n    # model head (define number of output classes)\n    x = layers.Conv2D(\n        filters=classes,\n        kernel_size=(3, 3),\n        padding=\'same\',\n        use_bias=True,\n        kernel_initializer=\'glorot_uniform\',\n        name=\'final_conv\',\n    )(x)\n    x = layers.Activation(activation, name=activation)(x)\n\n    # create keras model instance\n    model = models.Model(input_, x)\n\n    return model\n\n\n# ---------------------------------------------------------------------\n#  Unet Model\n# ---------------------------------------------------------------------\n\ndef Unet(\n        backbone_name=\'vgg16\',\n        input_shape=(None, None, 3),\n        classes=1,\n        activation=\'sigmoid\',\n        weights=None,\n        encoder_weights=\'imagenet\',\n        encoder_freeze=False,\n        encoder_features=\'default\',\n        decoder_block_type=\'upsampling\',\n        decoder_filters=(256, 128, 64, 32, 16),\n        decoder_use_batchnorm=True,\n        **kwargs\n):\n    """""" Unet_ is a fully convolution neural network for image semantic segmentation\n\n    Args:\n        backbone_name: name of classification model (without last dense layers) used as feature\n            extractor to build segmentation model.\n        input_shape: shape of input data/image ``(H, W, C)``, in general\n            case you do not need to set ``H`` and ``W`` shapes, just pass ``(None, None, C)`` to make your model be\n            able to process images af any size, but ``H`` and ``W`` of input images should be divisible by factor ``32``.\n        classes: a number of classes for output (output shape - ``(h, w, classes)``).\n        activation: name of one of ``keras.activations`` for last model layer\n            (e.g. ``sigmoid``, ``softmax``, ``linear``).\n        weights: optional, path to model weights.\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        encoder_freeze: if ``True`` set all layers of encoder (backbone model) as non-trainable.\n        encoder_features: a list of layer numbers or names starting from top of the model.\n            Each of these layers will be concatenated with corresponding decoder block. If ``default`` is used\n            layer names are taken from ``DEFAULT_SKIP_CONNECTIONS``.\n        decoder_block_type: one of blocks with following layers structure:\n\n            - `upsampling`:  ``UpSampling2D`` -> ``Conv2D`` -> ``Conv2D``\n            - `transpose`:   ``Transpose2D`` -> ``Conv2D``\n\n        decoder_filters: list of numbers of ``Conv2D`` layer filters in decoder blocks\n        decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n            is used.\n\n    Returns:\n        ``keras.models.Model``: **Unet**\n\n    .. _Unet:\n        https://arxiv.org/pdf/1505.04597\n\n    """"""\n\n    global backend, layers, models, keras_utils\n    submodule_args = filter_keras_submodules(kwargs)\n    backend, layers, models, keras_utils = get_submodules_from_kwargs(submodule_args)\n\n    if decoder_block_type == \'upsampling\':\n        decoder_block = DecoderUpsamplingX2Block\n    elif decoder_block_type == \'transpose\':\n        decoder_block = DecoderTransposeX2Block\n    else:\n        raise ValueError(\'Decoder block type should be in (""upsampling"", ""transpose""). \'\n                         \'Got: {}\'.format(decoder_block_type))\n\n    backbone = Backbones.get_backbone(\n        backbone_name,\n        input_shape=input_shape,\n        weights=encoder_weights,\n        include_top=False,\n        **kwargs,\n    )\n\n    if encoder_features == \'default\':\n        encoder_features = Backbones.get_feature_layers(backbone_name, n=4)\n\n    model = build_unet(\n        backbone=backbone,\n        decoder_block=decoder_block,\n        skip_connection_layers=encoder_features,\n        decoder_filters=decoder_filters,\n        classes=classes,\n        activation=activation,\n        n_upsample_blocks=len(decoder_filters),\n        use_batchnorm=decoder_use_batchnorm,\n    )\n\n    # lock encoder weights for fine-tuning\n    if encoder_freeze:\n        freeze_model(backbone, **kwargs)\n\n    # loading model weights\n    if weights is not None:\n        model.load_weights(weights)\n\n    return model\n'"
