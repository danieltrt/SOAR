file_path,api_count,code
setup.py,0,"b""from setuptools import setup\nfrom setuptools import find_packages\n\n\nversion = '0.1'\n\nsetup(name='keras-text',\n      version=version,\n      description='Text classification library for Keras',\n      author='Raghavendra Kotikalapudi',\n      author_email='ragha@outlook.com',\n      url='https://github.com/raghakot/keras-text',\n      download_url='https://github.com/raghakot/keras-text/tarball/{}'.format(version),\n      license='MIT',\n      install_requires=['keras>=2.1.2', 'six', 'spacy>=2.0.3', 'scikit-learn'],\n      extras_require={\n          'tests': ['pytest',\n                    'pytest-pep8',\n                    'pytest-xdist',\n                    'pytest-cov'],\n      },\n      include_package_data=True,\n      packages=find_packages())\n"""
docs/__init__.py,0,b''
docs/md_autogen.py,0,"b'""""""\nParses source code to generate API docs in markdown.\n""""""\n\nimport os\nimport re\nimport inspect\nfrom inspect import getdoc, getargspec, getsourcefile, getsourcelines, getmembers\nfrom collections import defaultdict\n\nimport sys\nif sys.version[0] == \'2\':\n    reload(sys)\n    sys.setdefaultencoding(\'utf8\')\n\n_RE_BLOCKSTART = re.compile(r""(Args:|Arg:|Kwargs:|Returns:|Yields:|Kwargs:|Raises:|Notes:|Note:|Examples:|Example:)"",\n                            re.IGNORECASE)\n_RE_ARGSTART = re.compile(r""(\\w*?)\\s*?\\((.*?)\\):(.*)"", re.IGNORECASE)\n_RE_EXCSTART = re.compile(r""(\\w*?):(.*)"", re.IGNORECASE)\n\n#\n# String templates\n#\n\nFUNC_TEMPLATE = """"""-------------------\n\n{section} [{header}]({path})\n\n```python\n{funcdef}\n```\n\n{doc}\n\n""""""\n\nCLASS_TEMPLATE = """"""-------------------\n\n{section} [{header}]({path})\n\n{doc}\n\n{variables}\n\n{init}\n\n{handlers}\n\n{methods}\n\n""""""\n\nMODULE_TEMPLATE = """"""\n**Source:** {path}\n\n{global_vars}\n\n{functions}\n\n{classes}\n\n""""""\n\n\ndef make_iter(obj):\n    """""" Makes an iterable\n    """"""\n    return obj if hasattr(obj, \'__iter__\') else [obj]\n\n\ndef order_by_line_nos(objs, line_nos):\n    """"""Orders the set of `objs` by `line_nos`\n    """"""\n    ordering = sorted(range(len(line_nos)), key=line_nos.__getitem__)\n    return [objs[i] for i in ordering]\n\n\ndef to_md_file(string, filename, out_path="".""):\n    """"""Import a module path and create an api doc from it\n\n    Args:\n        string (str): string with line breaks to write to file.\n        filename (str): filename without the .md\n        out_path (str): The output directory\n    """"""\n    md_file = ""%s.md"" % filename\n    with open(os.path.join(out_path, md_file), ""w"") as f:\n        f.write(string)\n    print(""wrote {}."".format(md_file))\n\n\nclass MarkdownAPIGenerator(object):\n    def __init__(self, src_root, github_link):\n        """"""Initializes the markdown api generator.\n\n        Args:\n            src_root: The root folder name containing all the sources.\n                Ex: src\n            github_link: The base github link. Should include branch name.\n                Ex: https://github.com/raghakot/keras-vis/tree/master\n                All source links are generated with this prefix.\n        """"""\n        self.src_root = src_root\n        self.github_link = github_link\n\n    def get_line_no(self, obj):\n        """"""Gets the source line number of this object. None if `obj` code cannot be found.\n        """"""\n        try:\n            lineno = getsourcelines(obj)[1]\n        except:\n            # no code found\n            lineno = None\n        return lineno\n\n    def get_src_path(self, obj, append_base=True):\n        """"""Creates a src path string with line info for use as markdown link.\n        """"""\n        path = getsourcefile(obj)\n        if self.src_root not in path:\n            # this can happen with e.g.\n            # inlinefunc-wrapped functions\n            if hasattr(obj, ""__module__""):\n                path = ""%s.%s"" % (obj.__module__, obj.__name__)\n            else:\n                path = obj.__name__\n            path = path.replace(""."", ""/"")\n        pre, post = path.rsplit(self.src_root + ""/"", 1)\n\n        lineno = self.get_line_no(obj)\n        lineno = """" if lineno is None else ""#L{}"".format(lineno)\n\n        path = self.src_root + ""/"" + post + lineno\n        if append_base:\n            path = os.path.join(self.github_link, path)\n        return path\n\n    def doc2md(self, func):\n        """"""Parse docstring (parsed with getdoc) according to Google-style\n        formatting and convert to markdown. We support the following\n        Google style syntax:\n\n        Args, Kwargs:\n            argname (type): text\n            freeform text\n        Returns, Yields:\n            retname (type): text\n            freeform text\n        Raises:\n            exceptiontype: text\n            freeform text\n        Notes, Examples:\n            freeform text\n\n        """"""\n        doc = getdoc(func) or """"\n        blockindent = 0\n        argindent = 1\n        out = []\n\n        for line in doc.split(""\\n""):\n            indent = len(line) - len(line.lstrip())\n            line = line.lstrip()\n            if _RE_BLOCKSTART.match(line):\n                # start of a new block\n                blockindent = indent\n                out.append(""\\n*{}*\\n"".format(line))\n            elif indent > blockindent:\n                if _RE_ARGSTART.match(line):\n                    # start of new argument\n                    out.append(""\\n"" + "" "" * blockindent + "" - "" + _RE_ARGSTART.sub(r""**\\1** (\\2): \\3"", line))\n                    argindent = indent\n                elif _RE_EXCSTART.match(line):\n                    # start of an exception-type block\n                    out.append(""\\n"" + "" "" * blockindent + "" - "" + _RE_EXCSTART.sub(r""**\\1**: \\2"", line))\n                    argindent = indent\n                elif indent > argindent:\n                    out.append(""\\n"" + "" "" * (blockindent + 2) + line)\n                else:\n                    out.append(""\\n"" + line)\n            else:\n                out.append(""\\n"" + line)\n\n        return """".join(out)\n\n    def func2md(self, func, clsname=None, names=None, depth=3):\n        """"""Takes a function (or method) and documents it.\n\n        Args:\n            clsname (str, optional): class name to prepend to funcname.\n            depth (int, optional): number of ### to append to function name\n\n        """"""\n        section = ""#"" * depth\n        if names is None:\n            names = [func.__name__]\n\n        funcname = "", "".join(names)\n        escfuncname = "", "".join([""`%s`"" % funcname if funcname.startswith(""_"") else funcname for funcname in names])\n        header = ""%s%s"" % (""%s."" % clsname if clsname else """", escfuncname)\n\n        path = self.get_src_path(func)\n        doc = self.doc2md(func)\n\n        args, kwargs = [], []\n        spec = getargspec(func)\n        vargsname, kwargsname = spec.varargs, spec.keywords\n        vargs = list(make_iter(spec.args)) if spec.args else []\n        defaults = list(make_iter(spec.defaults)) if spec.defaults else []\n\n        while vargs:\n            if vargs and vargs[0] == ""self"":\n                args.append(vargs.pop(0))\n            elif len(vargs) > len(defaults):\n                args.append(vargs.pop(0))\n            else:\n                default = defaults.pop(0)\n                if isinstance(default, str):\n                    default = ""\\""%s\\"""" % default\n                else:\n                    default = ""%s"" % str(default)\n\n                kwargs.append((vargs.pop(0), default))\n\n        if args:\n            args = "", "".join(""%s"" % arg for arg in args)\n        if kwargs:\n            kwargs = "", "".join(""%s=%s"" % kwarg for kwarg in kwargs)\n            if args:\n                kwargs = "", "" + kwargs\n        if vargsname:\n            vargsname = ""*%s"" % vargsname\n            if args or kwargs:\n                vargsname = "", "" + vargsname\n        if kwargsname:\n            kwargsname = ""**%s"" % kwargsname\n            if args or kwargs or vargsname:\n                kwargsname = "", "" + kwargsname\n\n        _FUNCDEF = ""{funcname}({args}{kwargs}{vargs}{vkwargs})""\n        funcdef = _FUNCDEF.format(funcname=funcname,\n                                  args=args or """",\n                                  kwargs=kwargs or """",\n                                  vargs=vargsname or """",\n                                  vkwargs=kwargsname or """")\n\n        # split the function definition if it is too long\n        lmax = 90\n        if len(funcdef) > lmax:\n            # wrap in the args list\n            split = funcdef.split(""("", 1)\n            # we gradually build the string again\n            rest = split[1]\n            args = rest.split("", "")\n\n            funcname = ""("".join(split[:1]) + ""(""\n            lline = len(funcname)\n            parts = []\n            for arg in args:\n                larg = len(arg)\n                if larg > lmax - 5:\n                    # not much to do if arg is so long\n                    parts.append(arg)\n                elif lline + larg > lmax:\n                    # the next arg is too long, break the line\n                    parts.append(""\\\\\\n    "" + arg)\n                    lline = 0\n                else:\n                    parts.append(arg)\n                lline += len(parts[-1])\n            funcdef = funcname + "", "".join(parts)\n\n        # build the signature\n        string = FUNC_TEMPLATE.format(section=section,\n                                      header=header,\n                                      funcdef=funcdef,\n                                      path=path,\n                                      doc=doc if doc else ""*No documentation found.*"")\n        return string\n\n    def class2md(self, cls, depth=2):\n        """"""Takes a class and creates markdown text to document its methods and variables.\n        """"""\n\n        section = ""#"" * depth\n        subsection = ""#"" * (depth + 2)\n        clsname = cls.__name__\n        modname = cls.__module__\n        header = clsname\n        path = self.get_src_path(cls)\n        doc = self.doc2md(cls)\n\n        try:\n            init = self.func2md(cls.__init__, clsname=clsname)\n        except (ValueError, TypeError):\n            # this happens if __init__ is outside the repo\n            init = """"\n\n        variables = []\n        for name, obj in getmembers(cls, lambda a: not (inspect.isroutine(a) or inspect.ismethod(a))):\n            if not name.startswith(""_"") and type(obj) == property:\n                comments = self.doc2md(obj) or inspect.getcomments(obj)\n                comments = ""\\n %s"" % comments if comments else """"\n                variables.append(""\\n%s %s.%s%s\\n"" % (subsection, clsname, name, comments))\n\n        handlers = []\n        for name, obj in getmembers(cls, inspect.ismethoddescriptor):\n            if not name.startswith(""_"") and hasattr(obj, ""__module__"") and obj.__module__ == modname:\n                handlers.append(""\\n%s %s.%s\\n *Handler*"" % (subsection, clsname, name))\n\n        methods = []\n        for name, obj in getmembers(cls, inspect.ismethod):\n            if not name.startswith(""_"") and hasattr(obj,\n                                                    ""__module__"") and obj.__module__ == modname and name not in handlers:\n                methods.append(self.func2md(obj, clsname=clsname, depth=depth + 1))\n\n        string = CLASS_TEMPLATE.format(section=section,\n                                       header=header,\n                                       path=path,\n                                       doc=doc if doc else """",\n                                       init=init,\n                                       variables="""".join(variables),\n                                       handlers="""".join(handlers),\n                                       methods="""".join(methods))\n        return string\n\n    def module2md(self, module):\n        """"""Takes an imported module object and create a Markdown string containing functions and classes.\n        """"""\n        modname = module.__name__\n        path = self.get_src_path(module, append_base=False)\n        path = ""[{}]({})"".format(path, os.path.join(self.github_link, path))\n        found = set()\n\n        classes = []\n        line_nos = []\n        for name, obj in getmembers(module, inspect.isclass):\n            # handle classes\n            found.add(name)\n            if not name.startswith(""_"") and hasattr(obj, ""__module__"") and obj.__module__ == modname:\n                classes.append(self.class2md(obj))\n                line_nos.append(self.get_line_no(obj) or 0)\n        classes = order_by_line_nos(classes, line_nos)\n\n        # Since functions can have multiple aliases.\n        func2names = defaultdict(list)\n        for name, obj in getmembers(module, inspect.isfunction):\n            func2names[obj].append(name)\n\n        functions = []\n        line_nos = []\n        for obj in func2names:\n            names = func2names[obj]\n            found.update(names)\n\n            # Include if within module or included modules within __init__.py and exclude from global variables\n            is_module_within_init = \'__init__.py\' in path and obj.__module__.startswith(modname)\n            if is_module_within_init:\n                found.add(obj.__module__.replace(modname + \'.\', \'\'))\n\n            if hasattr(obj, ""__module__"") and (obj.__module__ == modname or is_module_within_init):\n                names = list(filter(lambda name: not name.startswith(""_""), names))\n                if len(names) > 0:\n                    functions.append(self.func2md(obj, names=names))\n                    line_nos.append(self.get_line_no(obj) or 0)\n        functions = order_by_line_nos(functions, line_nos)\n\n        variables = []\n        line_nos = []\n        for name, obj in module.__dict__.items():\n            if not name.startswith(""_"") and name not in found:\n                if hasattr(obj, ""__module__"") and obj.__module__ != modname:\n                    continue\n                if hasattr(obj, ""__name__"") and not obj.__name__.startswith(modname):\n                    continue\n\n                comments = inspect.getcomments(obj)\n                comments = "": %s"" % comments if comments else """"\n                variables.append(""- **%s**%s"" % (name, comments))\n                line_nos.append(self.get_line_no(obj) or 0)\n\n        variables = order_by_line_nos(variables, line_nos)\n        if variables:\n            new_list = [""**Global Variables**"", ""---------------""]\n            new_list.extend(variables)\n            variables = new_list\n\n        string = MODULE_TEMPLATE.format(path=path,\n                                        global_vars=""\\n"".join(variables) if variables else """",\n                                        functions=""\\n"".join(functions) if functions else """",\n                                        classes="""".join(classes) if classes else """")\n        return string\n'"
docs/update_docs.py,0,"b'import shutil\n\nfrom md_autogen import MarkdownAPIGenerator\nfrom md_autogen import to_md_file\n\nfrom keras_text.models import token_model\nfrom keras_text.models import sentence_model\nfrom keras_text.models import sequence_encoders\nfrom keras_text.models import layers\n\nfrom keras_text import data\nfrom keras_text import generators\nfrom keras_text import processing\nfrom keras_text import sampling\nfrom keras_text import utils\n\n\ndef generate_api_docs():\n    modules = [\n        token_model,\n        sentence_model,\n        sequence_encoders,\n        layers,\n        data,\n        generators,\n        processing,\n        sampling,\n        utils\n    ]\n\n    md_gen = MarkdownAPIGenerator(""keras_text"", ""https://github.com/raghakot/keras-text/tree/master"")\n    for m in modules:\n        md_string = md_gen.module2md(m)\n        to_md_file(md_string, m.__name__, ""sources"")\n\n\ndef update_index_md():\n    shutil.copyfile(\'../README.md\', \'sources/index.md\')\n\n\ndef copy_templates():\n    shutil.rmtree(\'sources\', ignore_errors=True)\n    shutil.copytree(\'templates\', \'sources\')\n\n\nif __name__ == ""__main__"":\n    copy_templates()\n    update_index_md()\n    generate_api_docs()\n'"
keras_text/__init__.py,0,b'import logging\nlogging.basicConfig(level=logging.INFO)\n'
keras_text/data.py,0,"b'from __future__ import absolute_import\n\nimport logging\nimport numpy as np\n\nfrom .import utils\nfrom .import sampling\n\nfrom sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Dataset(object):\n\n    def __init__(self, inputs, labels, test_indices=None, **kwargs):\n        """"""Encapsulates all pieces of data to run an experiment. This is basically a bag of items that makes it\n        easy to serialize and deserialize everything as a unit.\n\n        Args:\n            inputs: The raw model inputs. This can be set to None if you dont want\n                to serialize this value when you save the dataset.\n            labels: The raw output labels.\n            test_indices: The optional test indices to use. Ideally, this should be generated one time and reused\n                across experiments to make results comparable. `generate_test_indices` can be used generate first\n                time indices.\n            **kwargs: Additional key value items to store.\n        """"""\n        self.X = np.array(inputs)\n        self.y = np.array(labels)\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n\n        self._test_indices = None\n        self._train_indices = None\n        self.test_indices = test_indices\n\n        self.is_multi_label = isinstance(labels[0], (set, list, tuple))\n        self.label_encoder = MultiLabelBinarizer() if self.is_multi_label else LabelBinarizer()\n        self.y = self.label_encoder.fit_transform(self.y).flatten()\n\n    def update_test_indices(self, test_size=0.1):\n        """"""Updates `test_indices` property with indices of `test_size` proportion.\n\n        Args:\n            test_size: The test proportion in [0, 1] (Default value: 0.1)\n        """"""\n        if self.is_multi_label:\n            self._train_indices, self._test_indices = sampling.multi_label_train_test_split(self.y, test_size)\n        else:\n            sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size)\n            self._train_indices, self._test_indices = next(sss.split(self.X, self.y))\n\n    def save(self, file_path):\n        """"""Serializes this dataset to a file.\n\n        Args:\n            file_path: The file path to use.\n        """"""\n        utils.dump(self, file_path)\n\n    def train_val_split(self, split_ratio=0.1):\n        """"""Generates train and validation sets from the training indices.\n\n        Args:\n            split_ratio: The split proportion in [0, 1] (Default value: 0.1)\n\n        Returns:\n            The stratified train and val subsets. Multi-label outputs are handled as well.\n        """"""\n        if self.is_multi_label:\n            train_indices, val_indices = sampling.multi_label_train_test_split(self.y, split_ratio)\n        else:\n            sss = StratifiedShuffleSplit(n_splits=1, test_size=split_ratio)\n            train_indices, val_indices = next(sss.split(self.X, self.y))\n        return self.X[train_indices], self.X[val_indices], self.y[train_indices], self.y[val_indices]\n\n    @staticmethod\n    def load(file_path):\n        """"""Loads the dataset from a file.\n\n        Args:\n            file_path: The file path to use.\n\n        Returns:\n            The `Dataset` instance.\n        """"""\n        return utils.load(file_path)\n\n    @property\n    def test_indices(self):\n        return self._test_indices\n\n    @test_indices.setter\n    def test_indices(self, test_indices):\n        if test_indices is None:\n            self._train_indices = np.arange(0, len(self.y))\n        else:\n            self._test_indices = test_indices\n            self._train_indices = np.setdiff1d(np.arange(0, len(self.y)), self.test_indices)\n\n    @property\n    def train_indices(self):\n        return self._train_indices\n\n    @property\n    def labels(self):\n        return self.label_encoder.classes_\n\n    @property\n    def num_classes(self):\n        if len(self.y.shape) == 1:\n            return 1\n        else:\n            return len(self.labels)\n'"
keras_text/embeddings.py,0,"b'from __future__ import absolute_import\n\nimport os\nimport logging\nimport numpy as np\nfrom keras.utils.data_utils import get_file\n\n\nlogger = logging.getLogger(__name__)\n_EMBEDDINGS_CACHE = dict()\n\n# Add more types here as needed.\n_EMBEDDING_TYPES = {\n    \'glove.42B.300d\': {\n        \'file\': \'glove.42B.300d.txt\',\n        \'url\': \'http://nlp.stanford.edu/data/glove.42B.300d.zip\'\n    },\n\n    \'glove.6B.50d\': {\n        \'file\': \'glove.6B.50d.txt\',\n        \'url\': \'http://nlp.stanford.edu/data/glove.6B.zip\'\n    },\n\n    \'glove.6B.100d\': {\n        \'file\': \'glove.6B.100d.txt\',\n        \'url\': \'http://nlp.stanford.edu/data/glove.6B.zip\'\n    },\n\n    \'glove.6B.200d\': {\n        \'file\': \'glove.6B.200d.txt\',\n        \'url\': \'http://nlp.stanford.edu/data/glove.6B.zip\'\n    },\n\n    \'glove.6B.300d\': {\n        \'file\': \'glove.6B.300d.txt\',\n        \'url\': \'http://nlp.stanford.edu/data/glove.6B.zip\'\n    },\n\n    \'glove.840B.300d\': {\n        \'file\': \'glove.840B.300d.txt\',\n        \'url\': \'http://nlp.stanford.edu/data/glove.840B.300d.zip\'\n    }\n}\n\n\ndef _build_embeddings_index(embeddings_path):\n    logger.info(\'Building embeddings index...\')\n    index = {}\n    with open(embeddings_path, \'rb\') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vector = np.asarray(values[1:], dtype=\'float32\')\n            index[word] = vector\n    return index\n\n\ndef build_embedding_weights(word_index, embeddings_index):\n    """"""Builds an embedding matrix for all words in vocab using embeddings_index\n    """"""\n    logger.info(\'Loading embeddings for all words in the corpus\')\n    embedding_dim = embeddings_index.values()[0].shape[-1]\n\n    # +1 since tokenizer words are indexed from 1. 0 is reserved for padding and unknown words.\n    embedding_weights = np.zeros((len(word_index) + 1, embedding_dim))\n\n    for word, i in word_index.items():\n        word_vector = embeddings_index.get(word)\n        if word_vector is not None:\n            # Words not in embedding will be all zeros which can stand for padded words.\n            embedding_weights[i] = word_vector\n\n    return embedding_weights\n\n\ndef get_embeddings_index(embedding_type=\'glove.42B.300d\'):\n    """"""Retrieves embeddings index from embedding name. Will automatically download and cache as needed.\n\n    Args:\n        embedding_type: The embedding type to load.\n\n    Returns:\n        The embeddings indexed by word.\n    """"""\n\n    embeddings_index = _EMBEDDINGS_CACHE.get(embedding_type)\n    if embeddings_index is not None:\n        return embeddings_index\n\n    data_obj = _EMBEDDING_TYPES.get(embedding_type)\n    if data_obj is None:\n        raise ValueError(""Embedding name should be one of \'{}\'"".format(_EMBEDDING_TYPES.keys()))\n\n    cache_dir = os.path.expanduser(os.path.join(\'~\', \'.keras-text\'))\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    file_path = get_file(embedding_type, origin=data_obj[\'url\'], extract=True,\n                         cache_dir=cache_dir, cache_subdir=\'embeddings\')\n    file_path = os.path.join(os.path.dirname(file_path), data_obj[\'file\'])\n\n    embeddings_index = _build_embeddings_index(file_path)\n    _EMBEDDINGS_CACHE[embedding_type] = embeddings_index\n    return embeddings_index\n'"
keras_text/generators.py,0,"b'from __future__ import absolute_import\n\nimport numpy as np\nfrom keras.utils import Sequence\n\n\nclass ProcessingSequence(Sequence):\n    def __init__(self, X, y, batch_size, process_fn=None):\n        """"""A `Sequence` implementation that can pre-process a mini-batch via `process_fn`\n\n        Args:\n            X: The numpy array of inputs.\n            y: The numpy array of targets.\n            batch_size: The generator mini-batch size.\n            process_fn: The preprocessing function to apply on `X`\n        """"""\n        self.X = X\n        self.y = y\n        self.batch_size = batch_size\n        self.process_fn = process_fn or (lambda x: x)\n\n    def __len__(self):\n        return len(self.X) // self.batch_size\n\n    def on_epoch_end(self):\n        pass\n\n    def __getitem__(self, batch_idx):\n        batch_X = self.X[batch_idx * self.batch_size:(batch_idx + 1) * self.batch_size]\n        batch_y = self.y[batch_idx * self.batch_size:(batch_idx + 1) * self.batch_size]\n        return self.process_fn(batch_X), batch_y\n\n\nclass BalancedSequence(Sequence):\n    def __init__(self, X, y, batch_size, process_fn=None):\n        """"""A `Sequence` implementation that returns balanced `y` by undersampling majority class.\n\n        Args:\n            X: The numpy array of inputs.\n            y: The numpy array of targets.\n            batch_size: The generator mini-batch size.\n            process_fn: The preprocessing function to apply on `X`\n        """"""\n        self.X = X\n        self.y = y\n        self.batch_size = batch_size\n        self.process_fn = process_fn or (lambda x: x)\n\n        self.pos_indices = np.where(y == 1)[0]\n        self.neg_indices = np.where(y == 0)[0]\n        self.n = min(len(self.pos_indices), len(self.neg_indices))\n        self._index_array = None\n\n    def __len__(self):\n        # Reset batch after we are done with minority class.\n        return (self.n * 2) // self.batch_size\n\n    def on_epoch_end(self):\n        # Reset batch after all minority indices are covered.\n        self._index_array = None\n\n    def __getitem__(self, batch_idx):\n        if self._index_array is None:\n            pos_indices = self.pos_indices.copy()\n            neg_indices = self.neg_indices.copy()\n            np.random.shuffle(pos_indices)\n            np.random.shuffle(neg_indices)\n            self._index_array = np.concatenate((pos_indices[:self.n], neg_indices[:self.n]))\n            np.random.shuffle(self._index_array)\n\n        indices = self._index_array[batch_idx * self.batch_size: (batch_idx + 1) * self.batch_size]\n        return self.process_fn(self.X[indices]), self.y[indices]\n'"
keras_text/processing.py,0,"b'from __future__ import absolute_import\n\nimport abc\nimport logging\nimport spacy\n\nfrom . import utils\nimport numpy as np\n\nfrom copy import deepcopy\nfrom collections import defaultdict, OrderedDict\nfrom multiprocessing import cpu_count\n\nfrom keras.preprocessing.sequence import pad_sequences as keras_pad_sequences\nfrom keras.utils.generic_utils import Progbar\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass _CountTracker(object):\n    """"""Helper class to track counts of various document hierarchies in the corpus.\n    For example, if the tokenizer can tokenize docs as (docs, paragraph, sentences, words), then this utility\n    will track number of paragraphs, number of sentences within paragraphs and number of words within sentence.\n    """"""\n\n    def __init__(self):\n        self._prev_indices = None\n        self._local_counts = None\n        self.counts = None\n\n    def update(self, indices):\n        """"""Updates counts based on indices. The algorithm tracks the index change at i and\n        update global counts for all indices beyond i with local counts tracked so far.\n        """"""\n        # Initialize various lists for the first time based on length of indices.\n        if self._prev_indices is None:\n            self._prev_indices = indices\n\n            # +1 to track token counts in the last index.\n            self._local_counts = np.full(len(indices) + 1, 1)\n            self._local_counts[-1] = 0\n            self.counts = [[] for _ in range(len(self._local_counts))]\n\n        has_reset = False\n        for i in range(len(indices)):\n            # index value changed. Push all local values beyond i to count and reset those local_counts.\n            # For example, if document index changed, push counts on sentences and tokens and reset their local_counts\n            # to indicate that we are tracking those for new document. We need to do this at all document hierarchies.\n            if indices[i] > self._prev_indices[i]:\n                self._local_counts[i] += 1\n                has_reset = True\n                for j in range(i + 1, len(self.counts)):\n                    self.counts[j].append(self._local_counts[j])\n                    self._local_counts[j] = 1\n\n        # If none of the aux indices changed, update token count.\n        if not has_reset:\n            self._local_counts[-1] += 1\n        self._prev_indices = indices[:]\n\n    def finalize(self):\n        """"""This will add the very last document to counts. We also get rid of counts[0] since that\n        represents document level which doesnt come under anything else. We also convert all count\n        values to numpy arrays so that stats can be computed easily.\n        """"""\n        for i in range(1, len(self._local_counts)):\n            self.counts[i].append(self._local_counts[i])\n        self.counts.pop(0)\n\n        for i in range(len(self.counts)):\n            self.counts[i] = np.array(self.counts[i])\n\n\ndef _apply_generator(texts, apply_fn):\n    for text in texts:\n        yield apply_fn(text)\n\n\ndef _append(lst, indices, value):\n    """"""Adds `value` to `lst` list indexed by `indices`. Will create sub lists as required.\n    """"""\n    for i, idx in enumerate(indices):\n        # We need to loop because sometimes indices can increment by more than 1 due to missing tokens.\n        # Example: Sentence with no words after filtering words.\n        while len(lst) <= idx:\n            # Update max counts whenever a new sublist is created.\n            # There is no need to worry about indices beyond `i` since they will end up creating new lists as well.\n            lst.append([])\n        lst = lst[idx]\n\n    # Add token and update token max count.\n    lst.append(value)\n\n\ndef _recursive_apply(lst, apply_fn):\n    if len(lst) > 0 and not isinstance(lst[0], list):\n        for i in range(len(lst)):\n            lst[i] = apply_fn(lst[i])\n    else:\n        for sub_list in lst:\n            _recursive_apply(sub_list, apply_fn)\n\n\ndef _to_unicode(text):\n    if not isinstance(text, unicode):\n        text = text.decode(\'utf-8\')\n    return text\n\n\ndef _parse_spacy_kwargs(**kwargs):\n    """"""Supported args include:\n\n    Args:\n        n_threads/num_threads: Number of threads to use. Uses num_cpus - 1 by default.\n        batch_size: The number of texts to accumulate into a common working set before processing.\n            (Default value: 1000)\n    """"""\n    n_threads = kwargs.get(\'n_threads\') or kwargs.get(\'num_threads\')\n    batch_size = kwargs.get(\'batch_size\')\n\n    if n_threads is None or n_threads is -1:\n        n_threads = cpu_count() - 1\n    if batch_size is None or batch_size is -1:\n        batch_size = 1000\n    return n_threads, batch_size\n\n\ndef _pad_token_sequences(sequences, max_tokens=None,\n                         padding=\'pre\', truncating=\'pre\', value=0.):\n    return keras_pad_sequences(sequences, maxlen=max_tokens, padding=padding, truncating=truncating, value=value)\n\n\ndef _pad_sent_sequences(sequences, max_sentences=None, max_tokens=None,\n                        padding=\'pre\', truncating=\'pre\', value=0.):\n    # Infer max lengths if needed.\n    if max_sentences is None or max_tokens is None:\n        max_sentences_computed = 0\n        max_tokens_computed = 0\n        for sent_seq in sequences:\n            max_sentences_computed = max(max_sentences_computed, len(sent_seq))\n            max_tokens_computed = max(max_tokens_computed, np.max([len(token_seq) for token_seq in sent_seq]))\n\n        # Only use inferred values for None.\n        max_sentences = min(max_sentences, max_sentences_computed)\n        max_tokens = min(max_tokens, max_tokens_computed)\n\n    result = np.ones(shape=(len(sequences), max_sentences, max_tokens)) * value\n\n    for idx, sent_seq in enumerate(sequences):\n        # empty list/array was found\n        if not len(sent_seq):\n            continue\n        if truncating == \'pre\':\n            trunc = sent_seq[-max_sentences:]\n        elif truncating == \'post\':\n            trunc = sent_seq[:max_sentences]\n        else:\n            raise ValueError(\'Truncating type ""%s"" not understood\' % truncating)\n\n        # Apply padding.\n        if padding == \'post\':\n            result[idx, :len(trunc)] = _pad_token_sequences(trunc, max_tokens, padding, truncating, value)\n        elif padding == \'pre\':\n            result[idx, -len(trunc):] = _pad_token_sequences(trunc, max_tokens, padding, truncating, value)\n        else:\n            raise ValueError(\'Padding type ""%s"" not understood\' % padding)\n    return result\n\n\ndef pad_sequences(sequences, max_sentences=None, max_tokens=None,\n                  padding=\'pre\', truncating=\'post\', value=0.):\n    """"""Pads each sequence to the same length (length of the longest sequence or provided override).\n\n    Args:\n        sequences: list of list (samples, words) or list of list of list (samples, sentences, words)\n        max_sentences: The max sentence length to use. If None, largest sentence length is used.\n        max_tokens: The max word length to use. If None, largest word length is used.\n        padding: \'pre\' or \'post\', pad either before or after each sequence.\n        truncating: \'pre\' or \'post\', remove values from sequences larger than max_sentences or max_tokens\n            either in the beginning or in the end of the sentence or word sequence respectively.\n        value: The padding value.\n\n    Returns:\n        Numpy array of (samples, max_sentences, max_tokens) or (samples, max_tokens) depending on the sequence input.\n\n    Raises:\n        ValueError: in case of invalid values for `truncating` or `padding`.\n    """"""\n\n    # Determine if input is (samples, max_sentences, max_tokens) or not.\n    if isinstance(sequences[0][0], list):\n        x = _pad_sent_sequences(sequences, max_sentences, max_tokens, padding, truncating, value)\n    else:\n        x = _pad_token_sequences(sequences, max_tokens, padding, truncating, value)\n    return np.array(x, dtype=\'int32\')\n\n\n# def pad_sequences1(sequences, max_values, padding=\'pre\', truncating=\'post\', pad_value=0, dynamic_max=True):\n#     computed_max_values = [0] * len(max_values)\n#\n#     def _compute_max(lst, level):\n#         for lst in\n\n\ndef unicodify(texts):\n    """"""Encodes all text sequences as unicode. This is a python2 hassle.\n\n    Args:\n        texts: The sequence of texts.\n\n    Returns:\n        Unicode encoded sequences.\n    """"""\n    return [_to_unicode(text) for text in texts]\n\n\nclass Tokenizer(object):\n\n    def __init__(self,\n                 lang=\'en\',\n                 lower=True):\n        """"""Encodes text into `(samples, aux_indices..., token)` where each token is mapped to a unique index starting\n        from `1`. Note that `0` is a reserved for unknown tokens.\n\n        Args:\n            lang: The spacy language to use. (Default value: \'en\')\n            lower: Lower cases the tokens if True. (Default value: True)\n        """"""\n\n        self.lang = lang\n        self.lower = lower\n\n        self._token2idx = dict()\n        self._idx2token = dict()\n        self._token_counts = defaultdict(int)\n\n        self._num_texts = 0\n        self._counts = None\n\n    @abc.abstractmethod\n    def token_generator(self, texts, **kwargs):\n        """"""Generator for yielding tokens. You need to implement this method.\n\n        Args:\n            texts: list of text items to tokenize.\n            **kwargs: The kwargs propagated from `build_vocab_and_encode` or `encode_texts` call.\n\n        Returns:\n            `(text_idx, aux_indices..., token)` where aux_indices are optional. For example, if you want to vectorize\n                `texts` as `(text_idx, sentences, words), you should return `(text_idx, sentence_idx, word_token)`.\n                Similarly, you can include paragraph, page level information etc., if needed.\n        """"""\n        raise NotImplementedError()\n\n    def create_token_indices(self, tokens):\n        """"""If `apply_encoding_options` is inadequate, one can retrieve tokens from `self.token_counts`, filter with\n        a desired strategy and regenerate `token_index` using this method. The token index is subsequently used\n        when `encode_texts` or `decode_texts` methods are called.\n        """"""\n        # Since 0 is reserved.\n        indices = list(range(1, len(tokens) + 1))\n        self._token2idx = dict(list(zip(tokens, indices)))\n        self._idx2token = dict(list(zip(indices, tokens)))\n\n    def apply_encoding_options(self, min_token_count=1, max_tokens=None):\n        """"""Applies the given settings for subsequent calls to `encode_texts` and `decode_texts`. This allows you to\n        play with different settings without having to re-run tokenization on the entire corpus.\n\n        Args:\n            min_token_count: The minimum token count (frequency) in order to include during encoding. All tokens\n                below this frequency will be encoded to `0` which corresponds to unknown token. (Default value = 1)\n            max_tokens: The maximum number of tokens to keep, based their frequency. Only the most common `max_tokens`\n                tokens will be kept. Set to None to keep everything. (Default value: None)\n        """"""\n        if not self.has_vocab:\n            raise ValueError(""You need to build the vocabulary using `build_vocab` ""\n                             ""before using `apply_encoding_options`"")\n        if min_token_count < 1:\n            raise ValueError(""`min_token_count` should atleast be 1"")\n\n        # Remove tokens with freq < min_token_count\n        token_counts = list(self._token_counts.items())\n        token_counts = filter(lambda x: x[1] >= min_token_count, token_counts)\n\n        # Clip to max_tokens.\n        if max_tokens is not None:\n            token_counts.sort(key=lambda x: x[1], reverse=True)\n            filtered_tokens = zip(*token_counts)[0]\n            filtered_tokens = filtered_tokens[:max_tokens]\n        else:\n            filtered_tokens = zip(*token_counts)[0]\n\n        # Generate indices based on filtered tokens.\n        self.create_token_indices(filtered_tokens)\n\n    def encode_texts(self, texts, include_oov=False, verbose=1, **kwargs):\n        """"""Encodes the given texts using internal vocabulary with optionally applied encoding options. See\n        ``apply_encoding_options` to set various options.\n\n        Args:\n            texts: The list of text items to encode.\n            include_oov: True to map unknown (out of vocab) tokens to 0. False to exclude the token.\n            verbose: The verbosity level for progress. Can be 0, 1, 2. (Default value = 1)\n            **kwargs: The kwargs for `token_generator`.\n\n        Returns:\n            The encoded texts.\n        """"""\n        if not self.has_vocab:\n            raise ValueError(""You need to build the vocabulary using `build_vocab` before using `encode_texts`"")\n\n        progbar = Progbar(len(texts), verbose=verbose, interval=0.25)\n        encoded_texts = []\n        for token_data in self.token_generator(texts, **kwargs):\n            indices, token = token_data[:-1], token_data[-1]\n\n            token_idx = self._token2idx.get(token)\n            if token_idx is None and include_oov:\n                token_idx = 0\n\n            if token_idx is not None:\n                _append(encoded_texts, indices, token_idx)\n\n            # Update progressbar per document level.\n            progbar.update(indices[0])\n\n        # All done. Finalize progressbar.\n        progbar.update(len(texts), force=True)\n        return encoded_texts\n\n    def decode_texts(self, encoded_texts, unknown_token=""<UNK>"", inplace=True):\n        """"""Decodes the texts using internal vocabulary. The list structure is maintained.\n\n        Args:\n            encoded_texts: The list of texts to decode.\n            unknown_token: The placeholder value for unknown token. (Default value: ""<UNK>"")\n            inplace: True to make changes inplace. (Default value: True)\n\n        Returns:\n            The decoded texts.\n        """"""\n        if len(self._token2idx) == 0:\n            raise ValueError(""You need to build vocabulary using `build_vocab` before using `decode_texts`"")\n\n        if not inplace:\n            encoded_texts = deepcopy(encoded_texts)\n        _recursive_apply(encoded_texts,\n                         lambda token_id: self._idx2token.get(token_id) or unknown_token)\n        return encoded_texts\n\n    def build_vocab(self, texts, verbose=1, **kwargs):\n        """"""Builds the internal vocabulary and computes various statistics.\n\n        Args:\n            texts: The list of text items to encode.\n            verbose: The verbosity level for progress. Can be 0, 1, 2. (Default value = 1)\n            **kwargs: The kwargs for `token_generator`.\n        """"""\n        if self.has_vocab:\n            logger.warn(""Tokenizer already has existing vocabulary. Overriding and building new vocabulary."")\n\n        progbar = Progbar(len(texts), verbose=verbose, interval=0.25)\n        count_tracker = _CountTracker()\n\n        self._token_counts.clear()\n        self._num_texts = len(texts)\n\n        for token_data in self.token_generator(texts, **kwargs):\n            indices, token = token_data[:-1], token_data[-1]\n            count_tracker.update(indices)\n            self._token_counts[token] += 1\n\n            # Update progressbar per document level.\n            progbar.update(indices[0])\n\n        # Generate token2idx and idx2token.\n        self.create_token_indices(self._token_counts.keys())\n\n        # All done. Finalize progressbar update and count tracker.\n        count_tracker.finalize()\n        self._counts = count_tracker.counts\n        progbar.update(len(texts), force=True)\n\n    def get_counts(self, i):\n        """"""Numpy array of count values for aux_indices. For example, if `token_generator` generates\n        `(text_idx, sentence_idx, word)`, then `get_counts(0)` returns the numpy array of sentence lengths across\n        texts. Similarly, `get_counts(1)` will return the numpy array of token lengths across sentences.\n\n        This is useful to plot histogram or eyeball the distributions. For getting standard statistics, you can use\n        `get_stats` method.\n        """"""\n        if not self.has_vocab:\n            raise ValueError(""You need to build the vocabulary using `build_vocab` before using `get_counts`"")\n        return self._counts[i]\n\n    def get_stats(self, i):\n        """"""Gets the standard statistics for aux_index `i`. For example, if `token_generator` generates\n        `(text_idx, sentence_idx, word)`, then `get_stats(0)` will return various statistics about sentence lengths\n        across texts. Similarly, `get_counts(1)` will return statistics of token lengths across sentences.\n\n        This information can be used to pad or truncate inputs.\n        """"""\n        # OrderedDict to always show same order if printed.\n        result = OrderedDict()\n        result[\'min\'] = np.min(self._counts[i])\n        result[\'max\'] = np.max(self._counts[i])\n        result[\'std\'] = np.std(self._counts[i])\n        result[\'mean\'] = np.mean(self._counts[i])\n        return result\n\n    def save(self, file_path):\n        """"""Serializes this tokenizer to a file.\n\n        Args:\n            file_path: The file path to use.\n        """"""\n        utils.dump(self, file_path)\n\n    @staticmethod\n    def load(file_path):\n        """"""Loads the Tokenizer from a file.\n\n        Args:\n            file_path: The file path to use.\n\n        Returns:\n            The `Dataset` instance.\n        """"""\n        return utils.load(file_path)\n\n    @property\n    def has_vocab(self):\n        return len(self._token_counts) > 0 and self._counts is not None\n\n    @property\n    def token_index(self):\n        """"""Dictionary of token -> idx mappings. This can change with calls to `apply_encoding_options`.\n        """"""\n        return self._token2idx\n\n    @property\n    def token_counts(self):\n        """"""Dictionary of token -> count values for the text corpus used to `build_vocab`.\n        """"""\n        return self._token_counts\n\n    @property\n    def num_tokens(self):\n        """"""Number of unique tokens for use in enccoding/decoding.\n        This can change with calls to `apply_encoding_options`.\n        """"""\n        return len(self._token2idx)\n\n    @property\n    def num_texts(self):\n        """"""The number of texts used to build the vocabulary.\n        """"""\n        return self._num_texts\n\n\nclass WordTokenizer(Tokenizer):\n\n    def __init__(self,\n                 lang=\'en\',\n                 lower=True,\n                 lemmatize=False,\n                 remove_punct=True,\n                 remove_digits=True,\n                 remove_stop_words=False,\n                 exclude_oov=False,\n                 exclude_pos_tags=None,\n                 exclude_entities=[\'PERSON\']):\n        """"""Encodes text into `(samples, words)`\n\n        Args:\n            lang: The spacy language to use. (Default value: \'en\')\n            lower: Lower cases the tokens if True. (Default value: True)\n            lemmatize: Lemmatizes words when set to True. This also makes the word lower case\n                irrespective if the `lower` setting. (Default value: False)\n            remove_punct: Removes punct words if True. (Default value: True)\n            remove_digits: Removes digit words if True. (Default value: True)\n            remove_stop_words: Removes stop words if True. (Default value: False)\n            exclude_oov: Exclude words that are out of spacy embedding\'s vocabulary.\n                By default, GloVe 1 million, 300 dim are used. You can override spacy vocabulary with a custom\n                embedding to change this. (Default value: False)\n            exclude_pos_tags: A list of parts of speech tags to exclude. Can be any of spacy.parts_of_speech.IDS\n                (Default value: None)\n            exclude_entities: A list of entity types to be excluded.\n                Supported entity types can be found here: https://spacy.io/docs/usage/entity-recognition#entity-types\n                (Default value: [\'PERSON\'])\n        """"""\n\n        super(WordTokenizer, self).__init__(lang, lower)\n        self.lemmatize = lemmatize\n        self.remove_punct = remove_punct\n        self.remove_digits = remove_digits\n        self.remove_stop_words = remove_stop_words\n\n        self.exclude_oov = exclude_oov\n        self.exclude_pos_tags = set(exclude_pos_tags or [])\n        self.exclude_entities = set(exclude_entities or [])\n\n    def _apply_options(self, token):\n        """"""Applies various filtering and processing options on token.\n\n        Returns:\n            The processed token. None if filtered.\n        """"""\n        # Apply work token filtering.\n        if token.is_punct and self.remove_punct:\n            return None\n        if token.is_stop and self.remove_stop_words:\n            return None\n        if token.is_digit and self.remove_digits:\n            return None\n        if token.is_oov and self.exclude_oov:\n            return None\n        if token.pos_ in self.exclude_pos_tags:\n            return None\n        if token.ent_type_ in self.exclude_entities:\n            return None\n\n        # Lemmatized ones are already lowered.\n        if self.lemmatize:\n            return token.lemma_\n        if self.lower:\n            return token.lower_\n        return token.orth_\n\n    def token_generator(self, texts, **kwargs):\n        """"""Yields tokens from texts as `(text_idx, word)`\n\n        Args:\n            texts: The list of texts.\n            **kwargs: Supported args include:\n                n_threads/num_threads: Number of threads to use. Uses num_cpus - 1 by default.\n                batch_size: The number of texts to accumulate into a common working set before processing.\n                    (Default value: 1000)\n        """"""\n        # Perf optimization. Only process what is necessary.\n        n_threads, batch_size = _parse_spacy_kwargs(**kwargs)\n        nlp = spacy.load(self.lang)\n\n        disabled = [\'parser\']\n        if len(self.exclude_entities) > 0:\n            disabled.append(\'ner\')\n\n        kwargs = {\n            \'batch_size\': batch_size,\n            \'n_threads\': n_threads,\n            \'disable\': disabled\n        }\n\n        for text_idx, doc in enumerate(nlp.pipe(texts, **kwargs)):\n            for word in doc:\n                processed_word = self._apply_options(word)\n                if processed_word is not None:\n                    yield text_idx, processed_word\n\n\nclass SentenceWordTokenizer(WordTokenizer):\n\n    def __init__(self,\n                 lang=\'en\',\n                 lower=True,\n                 lemmatize=False,\n                 remove_punct=True,\n                 remove_digits=True,\n                 remove_stop_words=False,\n                 exclude_oov=False,\n                 exclude_pos_tags=None,\n                 exclude_entities=[\'PERSON\']):\n        """"""Encodes text into `(samples, sentences, words)`\n\n        Args:\n            lang: The spacy language to use. (Default value: \'en\')\n            lower: Lower cases the tokens if True. (Default value: True)\n            lemmatize: Lemmatizes words when set to True. This also makes the word lower case\n                irrespective if the `lower` setting. (Default value: False)\n            remove_punct: Removes punct words if True. (Default value: True)\n            remove_digits: Removes digit words if True. (Default value: True)\n            remove_stop_words: Removes stop words if True. (Default value: False)\n            exclude_oov: Exclude words that are out of spacy embedding\'s vocabulary.\n                By default, GloVe 1 million, 300 dim are used. You can override spacy vocabulary with a custom\n                embedding to change this. (Default value: False)\n            exclude_pos_tags: A list of parts of speech tags to exclude. Can be any of spacy.parts_of_speech.IDS\n                (Default value: None)\n            exclude_entities: A list of entity types to be excluded.\n                Supported entity types can be found here: https://spacy.io/docs/usage/entity-recognition#entity-types\n                (Default value: [\'PERSON\'])\n        """"""\n        super(SentenceWordTokenizer, self).__init__(lang,\n                                                    lower,\n                                                    lemmatize,\n                                                    remove_punct,\n                                                    remove_digits,\n                                                    remove_stop_words,\n                                                    exclude_oov,\n                                                    exclude_pos_tags,\n                                                    exclude_entities)\n\n    def token_generator(self, texts, **kwargs):\n        """"""Yields tokens from texts as `(text_idx, sent_idx, word)`\n\n        Args:\n            texts: The list of texts.\n            **kwargs: Supported args include:\n                n_threads/num_threads: Number of threads to use. Uses num_cpus - 1 by default.\n                batch_size: The number of texts to accumulate into a common working set before processing.\n                    (Default value: 1000)\n        """"""\n        # Perf optimization. Only process what is necessary.\n        n_threads, batch_size = _parse_spacy_kwargs(**kwargs)\n        nlp = spacy.load(self.lang)\n\n        disabled = []\n        if len(self.exclude_entities) > 0:\n            disabled.append(\'ner\')\n\n        kwargs = {\n            \'batch_size\': batch_size,\n            \'n_threads\': n_threads,\n            \'disable\': disabled\n        }\n\n        for text_idx, doc in enumerate(nlp.pipe(texts, **kwargs)):\n            for sent_idx, sent in enumerate(doc.sents):\n                for word in sent:\n                    processed_word = self._apply_options(word)\n                    if processed_word is not None:\n                        yield text_idx, sent_idx, processed_word\n\n\nclass CharTokenizer(Tokenizer):\n\n    def __init__(self,\n                 lang=\'en\',\n                 lower=True,\n                 charset=None):\n        """"""Encodes text into `(samples, characters)`\n\n        Args:\n            lang: The spacy language to use. (Default value: \'en\')\n            lower: Lower cases the tokens if True. (Default value: True)\n            charset: The character set to use. For example `charset = \'abc123\'`. If None, all characters will be used.\n                (Default value: None)\n        """"""\n        super(CharTokenizer, self).__init__(lang, lower)\n        self.charset = charset\n\n    def token_generator(self, texts, **kwargs):\n        """"""Yields tokens from texts as `(text_idx, character)`\n        """"""\n        for text_idx, text in enumerate(texts):\n            if self.lower:\n                text = text.lower()\n            for char in text:\n                yield text_idx, char\n\n\nclass SentenceCharTokenizer(CharTokenizer):\n\n    def __init__(self,\n                 lang=\'en\',\n                 lower=True,\n                 charset=None):\n        """"""Encodes text into `(samples, sentences, characters)`\n\n        Args:\n            lang: The spacy language to use. (Default value: \'en\')\n            lower: Lower cases the tokens if True. (Default value: True)\n            charset: The character set to use. For example `charset = \'abc123\'`. If None, all characters will be used.\n                (Default value: None)\n        """"""\n        super(SentenceCharTokenizer, self).__init__(lang, lower, charset)\n\n    def token_generator(self, texts, **kwargs):\n        """"""Yields tokens from texts as `(text_idx, sent_idx, character)`\n\n        Args:\n            texts: The list of texts.\n            **kwargs: Supported args include:\n                n_threads/num_threads: Number of threads to use. Uses num_cpus - 1 by default.\n                batch_size: The number of texts to accumulate into a common working set before processing.\n                    (Default value: 1000)\n        """"""\n        # Perf optimization. Only process what is necessary.\n        n_threads, batch_size = _parse_spacy_kwargs(**kwargs)\n        nlp = spacy.load(self.lang)\n\n        kwargs = {\n            \'batch_size\': batch_size,\n            \'n_threads\': n_threads,\n            \'disable\': [\'ner\']\n        }\n\n        # Perf optimization: Lower the entire text instead of individual tokens.\n        texts_gen = _apply_generator(texts, lambda x: x.lower()) if self.lower else texts\n        for text_idx, doc in enumerate(nlp.pipe(texts_gen, **kwargs)):\n            for sent_idx, sent in enumerate(doc.sents):\n                for word in sent:\n                    for char in word:\n                        yield text_idx, sent_idx, char\n\n\nif __name__ == \'__main__\':\n    texts = [\n        ""HELLO world hello. How are you today? Did you see the S.H.I.E.L.D?"",\n        ""Quick brown fox. Ran over the, building 1234?"",\n    ]\n\n    texts = unicodify(texts)\n    tokenizer = SentenceWordTokenizer()\n    tokenizer.build_vocab(texts)\n    tokenizer.apply_encoding_options(max_tokens=5)\n    encoded = tokenizer.encode_texts(texts)\n    decoded = tokenizer.decode_texts(encoded, inplace=False)\n    w = 1\n'"
keras_text/sampling.py,0,"b'from __future__ import absolute_import\n\nimport logging\nimport numpy as np\nfrom fractions import Fraction\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef equal_distribution_folds(y, folds=2):\n    """"""Creates `folds` number of indices that has roughly balanced multi-label distribution.\n\n    Args:\n        y: The multi-label outputs.\n        folds: The number of folds to create.\n\n    Returns:\n        `folds` number of indices that have roughly equal multi-label distributions.\n    """"""\n    n, classes = y.shape\n\n    # Compute sample distribution over classes\n    dist = y.sum(axis=0).astype(\'float\')\n    dist /= dist.sum()\n\n    index_list = []\n    fold_dist = np.zeros((folds, classes), dtype=\'float\')\n    for _ in range(folds):\n        index_list.append([])\n    for i in range(n):\n        if i < folds:\n            target_fold = i\n        else:\n            normed_folds = fold_dist.T / fold_dist.sum(axis=1)\n            how_off = normed_folds.T - dist\n            target_fold = np.argmin(np.dot((y[i] - .5).reshape(1, -1), how_off.T))\n        fold_dist[target_fold] += y[i]\n        index_list[target_fold].append(i)\n\n    logger.debug(""Fold distributions:"")\n    logger.debug(fold_dist)\n    return index_list\n\n\ndef multi_label_train_test_split(y, test_size=0.2):\n    """"""Creates a test split with roughly the same multi-label distribution in `y`.\n\n    Args:\n        y: The multi-label outputs.\n        test_size: The test size in [0, 1]\n\n    Returns:\n        The train and test indices.\n    """"""\n    if test_size <= 0 or test_size >= 1:\n        raise ValueError(""`test_size` should be between 0 and 1"")\n\n    # Find the smallest rational number.\n    frac = Fraction(test_size).limit_denominator()\n    test_folds, total_folds = frac.numerator, frac.denominator\n    logger.warn(\'Inferring test_size as {}/{}. Generating {} folds. The algorithm might fail if denominator is large.\'\n                .format(test_folds, total_folds, total_folds))\n\n    folds = equal_distribution_folds(y, folds=total_folds)\n    test_indices = np.concatenate(folds[:test_folds])\n    train_indices = np.concatenate(folds[test_folds:])\n    return train_indices, test_indices\n'"
keras_text/utils.py,0,"b""from __future__ import absolute_import\n\nimport numpy as np\nimport pickle\nimport joblib\nimport jsonpickle\n\nfrom jsonpickle.ext import numpy as jsonpickle_numpy\njsonpickle_numpy.register_handlers()\n\n\ndef dump(obj, file_name):\n    if file_name.endswith('.json'):\n        with open(file_name, 'w') as f:\n            f.write(jsonpickle.dumps(obj))\n        return\n\n    if isinstance(obj, np.ndarray):\n        np.save(file_name, obj)\n        return\n\n    # Using joblib instead of pickle because of http://bugs.python.org/issue11564\n    joblib.dump(obj, file_name, protocol=pickle.HIGHEST_PROTOCOL)\n\n\ndef load(file_name):\n    if file_name.endswith('.json'):\n        with open(file_name, 'r') as f:\n            return jsonpickle.loads(f.read())\n\n    if file_name.endswith('.npy'):\n        return np.load(file_name)\n\n    return joblib.load(file_name)\n"""
keras_text/models/__init__.py,0,b'from token_model import TokenModelFactory\nfrom sentence_model import SentenceModelFactory\nfrom sequence_encoders import *\n'
keras_text/models/layers.py,1,"b'from __future__ import absolute_import\n\nimport numpy as np\nfrom keras import backend as K\nfrom keras.layers import Layer\nfrom keras import initializers, regularizers, constraints\n\n\ndef _softmax(x, dim):\n    """"""Computes softmax along a specified dim. Keras currently lacks this feature.\n    """"""\n\n    if K.backend() == \'tensorflow\':\n        import tensorflow as tf\n        return tf.nn.softmax(x, dim)\n    elif K.backend() is \'cntk\':\n        import cntk\n        return cntk.softmax(x, dim)\n    elif K.backend() == \'theano\':\n        # Theano cannot softmax along an arbitrary dim.\n        # So, we will shuffle `dim` to -1 and un-shuffle after softmax.\n        perm = np.arange(K.ndim(x))\n        perm[dim], perm[-1] = perm[-1], perm[dim]\n        x_perm = K.permute_dimensions(x, perm)\n        output = K.softmax(x_perm)\n\n        # Permute back\n        perm[dim], perm[-1] = perm[-1], perm[dim]\n        output = K.permute_dimensions(x, output)\n        return output\n    else:\n        raise ValueError(""Backend \'{}\' not supported"".format(K.backend()))\n\n\nclass AttentionLayer(Layer):\n    """"""Attention layer that computes a learned attention over input sequence.\n\n    For details, see papers:\n    - https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf\n    - http://colinraffel.com/publications/iclr2016feed.pdf (fig 1)\n\n    Input:\n        x: Input tensor of shape `(..., time_steps, features)` where `features` must be static (known).\n\n    Output:\n        2D tensor of shape `(..., features)`. i.e., `time_steps` axis is attended over and reduced.\n    """"""\n\n    def __init__(self,\n                 kernel_initializer=\'he_normal\',\n                 kernel_regularizer=None,\n                 kernel_constraint=None,\n                 use_bias=True,\n                 bias_initializer=\'zeros\',\n                 bias_regularizer=None,\n                 bias_constraint=None,\n                 use_context=True,\n                 context_initializer=\'he_normal\',\n                 context_regularizer=None,\n                 context_constraint=None,\n                 attention_dims=None,\n                 **kwargs):\n        """"""\n        Args:\n            attention_dims: The dimensionality of the inner attention calculating neural network.\n                For input `(32, 10, 300)`, with `attention_dims` of 100, the output is `(32, 10, 100)`.\n                i.e., the attended words are 100 dimensional. This is then collapsed via summation to\n                `(32, 10, 1)` to indicate the attention weights for 10 words.\n                If set to None, `features` dims are used as `attention_dims`. (Default value: None)\n        """"""\n        if \'input_shape\' not in kwargs and \'input_dim\' in kwargs:\n            kwargs[\'input_shape\'] = (kwargs.pop(\'input_dim\'),)\n\n        super(AttentionLayer, self).__init__(**kwargs)\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n\n        self.use_bias = use_bias\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.use_context = use_context\n        self.context_initializer = initializers.get(context_initializer)\n        self.context_regularizer = regularizers.get(context_regularizer)\n        self.context_constraint = constraints.get(context_constraint)\n\n        self.attention_dims = attention_dims\n        self.supports_masking = True\n\n    def build(self, input_shape):\n        if len(input_shape) < 3:\n            raise ValueError(""Expected input shape of `(..., time_steps, features)`, found `{}`"".format(input_shape))\n\n        attention_dims = input_shape[-1] if self.attention_dims is None else self.attention_dims\n        self.kernel = self.add_weight(shape=(input_shape[-1], attention_dims),\n                                      initializer=self.kernel_initializer,\n                                      name=\'kernel\',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(attention_dims, ),\n                                        initializer=self.bias_initializer,\n                                        name=\'bias\',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        if self.use_context:\n            self.context_kernel = self.add_weight(shape=(attention_dims, ),\n                                                  initializer=self.context_initializer,\n                                                  name=\'context_kernel\',\n                                                  regularizer=self.context_regularizer,\n                                                  constraint=self.context_constraint)\n        else:\n            self.context_kernel = None\n\n        super(AttentionLayer, self).build(input_shape)\n\n    def call(self, x, mask=None):\n        # x: [..., time_steps, features]\n        # ut = [..., time_steps, attention_dims]\n        ut = K.dot(x, self.kernel)\n        if self.use_bias:\n            ut = K.bias_add(ut, self.bias)\n\n        ut = K.tanh(ut)\n        if self.use_context:\n            ut = ut * self.context_kernel\n\n        # Collapse `attention_dims` to 1. This indicates the weight for each time_step.\n        ut = K.sum(ut, axis=-1, keepdims=True)\n\n        # Convert those weights into a distribution but along time axis.\n        # i.e., sum of alphas along `time_steps` axis should be 1.\n        self.at = _softmax(ut, dim=1)\n        if mask is not None:\n            self.at *= K.cast(K.expand_dims(mask, -1), K.floatx())\n\n        # Weighted sum along `time_steps` axis.\n        return K.sum(x * self.at, axis=-2)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]\n\n    def get_attention_tensor(self):\n        if not hasattr(self, \'at\'):\n            raise ValueError(\'Attention tensor is available after calling this layer with an input\')\n        return self.at\n\n    def get_config(self):\n        config = {\n            \'kernel_initializer\': initializers.serialize(self.kernel_initializer),\n            \'kernel_regularizer\': regularizers.serialize(self.kernel_regularizer),\n            \'kernel_constraint\': constraints.serialize(self.kernel_constraint),\n            \'bias_initializer\': initializers.serialize(self.bias_initializer),\n            \'bias_regularizer\': regularizers.serialize(self.bias_regularizer),\n            \'bias_constraint\': constraints.serialize(self.bias_constraint),\n            \'context_initializer\': initializers.serialize(self.context_initializer),\n            \'context_regularizer\': regularizers.serialize(self.context_regularizer),\n            \'context_constraint\': constraints.serialize(self.context_constraint)\n        }\n        base_config = super(AttentionLayer, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass ConsumeMask(Layer):\n    """"""Layer that prevents mask propagation.\n    """"""\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        return x\n'"
keras_text/models/sentence_model.py,0,"b'from __future__ import absolute_import\n\nfrom keras.layers import Input, Embedding, Dense, TimeDistributed\nfrom keras.models import Model\n\nfrom ..embeddings import get_embeddings_index, build_embedding_weights\nfrom .sequence_encoders import SequenceEncoderBase\n\n\nclass SentenceModelFactory(object):\n    def __init__(self, num_classes, token_index, max_sents, max_tokens,\n                 embedding_type=\'glove.6B.100d\', embedding_dims=100):\n        """"""Creates a `SentenceModelFactory` instance for building various models that operate over\n        (samples, max_sentences, max_tokens) input.\n\n        Args:\n            num_classes: The number of output classes.\n            token_index: The dictionary of token and its corresponding integer index value.\n            max_sents: The max number of sentences in a document.\n            max_tokens: The max number of tokens in a sentence.\n            embedding_type: The embedding type to use. Set to None to use random embeddings.\n                (Default value: \'glove.6B.100d\')\n            embedding_dims: The number of embedding dims to use for representing a word. This argument will be ignored\n                when `embedding_type` is set. (Default value: 100)\n        """"""\n        self.num_classes = num_classes\n        self.token_index = token_index\n        self.max_sents = max_sents\n        self.max_tokens = max_tokens\n\n        # This is required to make TimeDistributed(word_encoder_model) work.\n        # TODO: Get rid of this restriction when https://github.com/fchollet/keras/issues/6917 resolves.\n        if self.max_tokens is None:\n            raise ValueError(\'`max_tokens` should be provided.\')\n\n        if embedding_type is not None:\n            self.embeddings_index = get_embeddings_index(embedding_type)\n            self.embedding_dims = self.embeddings_index.values()[0].shape[-1]\n        else:\n            self.embeddings_index = None\n            self.embedding_dims = embedding_dims\n\n    def build_model(self, token_encoder_model, sentence_encoder_model,\n                    trainable_embeddings=True, output_activation=\'softmax\'):\n        """"""Builds a model that first encodes all words within sentences using `token_encoder_model`, followed by\n        `sentence_encoder_model`.\n\n        Args:\n            token_encoder_model: An instance of `SequenceEncoderBase` for encoding tokens within sentences. This model\n                will be applied across all sentences to create a sentence encoding.\n            sentence_encoder_model: An instance of `SequenceEncoderBase` operating on sentence encoding generated by\n                `token_encoder_model`. This encoding is then fed into a final `Dense` layer for classification.\n            trainable_embeddings: Whether or not to fine tune embeddings.\n            output_activation: The output activation to use. (Default value: \'softmax\')\n                Use:\n                - `softmax` for binary or multi-class.\n                - `sigmoid` for multi-label classification.\n                - `linear` for regression output.\n\n        Returns:\n            The model output tensor.\n        """"""\n        if not isinstance(token_encoder_model, SequenceEncoderBase):\n            raise ValueError(""`token_encoder_model` should be an instance of `{}`"".format(SequenceEncoderBase))\n        if not isinstance(sentence_encoder_model, SequenceEncoderBase):\n            raise ValueError(""`sentence_encoder_model` should be an instance of `{}`"".format(SequenceEncoderBase))\n\n        if not sentence_encoder_model.allows_dynamic_length() and self.max_sents is None:\n            raise ValueError(""Sentence encoder model \'{}\' requires padding. ""\n                             ""You need to provide `max_sents`"")\n\n        if self.embeddings_index is None:\n            # The +1 is for unknown token index 0.\n            embedding_layer = Embedding(len(self.token_index) + 1,\n                                        self.embedding_dims,\n                                        input_length=self.max_tokens,\n                                        mask_zero=True,\n                                        trainable=trainable_embeddings)\n        else:\n            embedding_layer = Embedding(len(self.token_index) + 1,\n                                        self.embedding_dims,\n                                        weights=[build_embedding_weights(self.token_index, self.embeddings_index)],\n                                        input_length=self.max_tokens,\n                                        mask_zero=True,\n                                        trainable=trainable_embeddings)\n\n        word_input = Input(shape=(self.max_tokens,), dtype=\'int32\')\n        x = embedding_layer(word_input)\n        word_encoding = token_encoder_model(x)\n        token_encoder_model = Model(word_input, word_encoding, name=\'word_encoder\')\n\n        doc_input = Input(shape=(self.max_sents, self.max_tokens), dtype=\'int32\')\n        sent_encoding = TimeDistributed(token_encoder_model)(doc_input)\n        x = sentence_encoder_model(sent_encoding)\n\n        x = Dense(self.num_classes, activation=output_activation)(x)\n        return Model(doc_input, x)\n'"
keras_text/models/sequence_encoders.py,0,"b'from __future__ import absolute_import\n\nfrom keras.layers import Conv1D, Bidirectional, LSTM\nfrom keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, Dropout\nfrom keras.layers.merge import concatenate\nfrom .layers import AttentionLayer, ConsumeMask\n\n\nclass SequenceEncoderBase(object):\n\n    def __init__(self, dropout_rate=0.5):\n        """"""Creates a new instance of sequence encoder.\n\n        Args:\n            dropout_rate: The final encoded output dropout.\n        """"""\n        self.dropout_rate = dropout_rate\n\n    def __call__(self, x):\n        """"""Build the actual model here.\n\n        Args:\n            x: The encoded or embedded input sequence.\n\n        Returns:\n            The model output tensor.\n        """"""\n        # Avoid mask propagation when dynamic mini-batches are not supported.\n        if not self.allows_dynamic_length():\n            x = ConsumeMask()(x)\n\n        x = self.build_model(x)\n        if self.dropout_rate > 0:\n            x = Dropout(self.dropout_rate)(x)\n        return x\n\n    def build_model(self, x):\n        """"""Build your model graph here.\n\n        Args:\n            x: The encoded or embedded input sequence.\n\n        Returns:\n            The model output tensor without the classification block.\n        """"""\n        raise NotImplementedError()\n\n    def allows_dynamic_length(self):\n        """"""Return a boolean indicating whether this model is capable of handling variable time steps per mini-batch.\n\n        For example, this should be True for RNN models since you can use them with variable time steps per mini-batch.\n        CNNs on the other hand expect fixed time steps across all mini-batches.\n        """"""\n        # Assume default as False. Should be overridden as necessary.\n        return False\n\n\nclass YoonKimCNN(SequenceEncoderBase):\n\n    def __init__(self, num_filters=64, filter_sizes=[3, 4, 5], dropout_rate=0.5, **conv_kwargs):\n        """"""Yoon Kim\'s shallow cnn model: https://arxiv.org/pdf/1408.5882.pdf\n\n        Args:\n            num_filters: The number of filters to use per `filter_size`. (Default value = 64)\n            filter_sizes: The filter sizes for each convolutional layer. (Default value = [3, 4, 5])\n            **cnn_kwargs: Additional args for building the `Conv1D` layer.\n        """"""\n        super(YoonKimCNN, self).__init__(dropout_rate)\n        self.num_filters = num_filters\n        self.filter_sizes = filter_sizes\n        self.conv_kwargs = conv_kwargs\n\n    def build_model(self, x):\n        pooled_tensors = []\n        for filter_size in self.filter_sizes:\n            x_i = Conv1D(self.num_filters, filter_size, activation=\'elu\', **self.conv_kwargs)(x)\n            x_i = GlobalMaxPooling1D()(x_i)\n            pooled_tensors.append(x_i)\n\n        x = pooled_tensors[0] if len(self.filter_sizes) == 1 else concatenate(pooled_tensors, axis=-1)\n        return x\n\n\nclass StackedRNN(SequenceEncoderBase):\n\n    def __init__(self, rnn_class=LSTM, hidden_dims=[50, 50], bidirectional=True, dropout_rate=0.5, **rnn_kwargs):\n        """"""Creates a stacked RNN.\n\n        Args:\n            rnn_class: The type of RNN to use. (Default Value = LSTM)\n            encoder_dims: The number of hidden units of RNN. (Default Value: 50)\n            bidirectional: Whether to use bidirectional encoding. (Default Value = True)\n            **rnn_kwargs: Additional args for building the RNN.\n        """"""\n        super(StackedRNN, self).__init__(dropout_rate)\n        self.rnn_class = rnn_class\n        self.hidden_dims = hidden_dims\n        self.bidirectional = bidirectional\n        self.rnn_kwargs = rnn_kwargs\n\n    def build_model(self, x):\n        for i, n in enumerate(self.hidden_dims):\n            is_last_layer = i == len(self.hidden_dims) - 1\n            rnn = self.rnn_class(n, return_sequences=not is_last_layer, **self.rnn_kwargs)\n            if self.bidirectional:\n                x = Bidirectional(rnn)(x)\n            else:\n                x = rnn(x)\n        return x\n\n    def allows_dynamic_length(self):\n        return True\n\n\nclass AttentionRNN(SequenceEncoderBase):\n\n    def __init__(self, rnn_class=LSTM, encoder_dims=50, bidirectional=True, dropout_rate=0.5, **rnn_kwargs):\n        """"""Creates an RNN model with attention. The attention mechanism is implemented as described\n        in https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf, but without\n        sentence level attention.\n\n        Args:\n            rnn_class: The type of RNN to use. (Default Value = LSTM)\n            encoder_dims: The number of hidden units of RNN. (Default Value: 50)\n            bidirectional: Whether to use bidirectional encoding. (Default Value = True)\n            **rnn_kwargs: Additional args for building the RNN.\n        """"""\n        super(AttentionRNN, self).__init__(dropout_rate)\n        self.rnn_class = rnn_class\n        self.encoder_dims = encoder_dims\n        self.bidirectional = bidirectional\n        self.rnn_kwargs = rnn_kwargs\n\n    def build_model(self, x):\n        rnn = self.rnn_class(self.encoder_dims, return_sequences=True, **self.rnn_kwargs)\n        if self.bidirectional:\n            word_activations = Bidirectional(rnn)(x)\n        else:\n            word_activations = rnn(x)\n\n        attention_layer = AttentionLayer()\n        doc_vector = attention_layer(word_activations)\n        self.attention_tensor = attention_layer.get_attention_tensor()\n        return doc_vector\n\n    def get_attention_tensor(self):\n        if not hasattr(self, \'attention_tensor\'):\n            raise ValueError(\'You need to build the model first\')\n        return self.attention_tensor\n\n    def allows_dynamic_length(self):\n        return True\n\n\nclass AveragingEncoder(SequenceEncoderBase):\n\n    def __init__(self, dropout_rate=0):\n        """"""An encoder that averages sequence inputs.\n        """"""\n        super(AveragingEncoder, self).__init__(dropout_rate)\n\n    def build_model(self, x):\n        x = GlobalAveragePooling1D()(x)\n        return x\n'"
keras_text/models/token_model.py,0,"b'from __future__ import absolute_import\n\nfrom keras.layers import Input, Embedding, Dense\nfrom keras.models import Model\n\nfrom ..embeddings import get_embeddings_index, build_embedding_weights\nfrom .sequence_encoders import SequenceEncoderBase\n\n\nclass TokenModelFactory(object):\n    def __init__(self, num_classes, token_index, max_tokens,\n                 embedding_type=\'glove.6B.100d\', embedding_dims=100):\n        """"""Creates a `TokenModelFactory` instance for building various models that operate over\n        (samples, max_tokens) input. The token can be character, word or any other elementary token.\n\n        Args:\n            num_classes: The number of output classes.\n            token_index: The dictionary of token and its corresponding integer index value.\n            max_tokens: The max number of tokens across all documents. This can be set to None for models that\n                allow different word lengths per mini-batch.\n            embedding_type: The embedding type to use. Set to None to use random embeddings.\n                (Default value: \'glove.6B.100d\')\n            embedding_dims: The number of embedding dims to use for representing a word. This argument will be ignored\n                when `embedding_type` is set. (Default value: 100)\n        """"""\n        self.num_classes = num_classes\n        self.token_index = token_index\n        self.max_tokens = max_tokens\n\n        if embedding_type is not None:\n            self.embeddings_index = get_embeddings_index(embedding_type)\n            self.embedding_dims = self.embeddings_index.values()[0].shape[-1]\n        else:\n            self.embeddings_index = None\n            self.embedding_dims = embedding_dims\n\n    def build_model(self, token_encoder_model, trainable_embeddings=True, output_activation=\'softmax\'):\n        """"""Builds a model using the given `text_model`\n\n        Args:\n            token_encoder_model: An instance of `SequenceEncoderBase` for encoding all the tokens within a document.\n                This encoding is then fed into a final `Dense` layer for classification.\n            trainable_embeddings: Whether or not to fine tune embeddings.\n            output_activation: The output activation to use. (Default value: \'softmax\')\n                Use:\n                - `softmax` for binary or multi-class.\n                - `sigmoid` for multi-label classification.\n                - `linear` for regression output.\n\n        Returns:\n            The model output tensor.\n        """"""\n        if not isinstance(token_encoder_model, SequenceEncoderBase):\n            raise ValueError(""`token_encoder_model` should be an instance of `{}`"".format(SequenceEncoderBase))\n\n        if not token_encoder_model.allows_dynamic_length() and self.max_tokens is None:\n            raise ValueError(""The provided `token_encoder_model` does not allow variable length mini-batches. ""\n                             ""You need to provide `max_tokens`"")\n\n        if self.embeddings_index is None:\n            # The +1 is for unknown token index 0.\n            embedding_layer = Embedding(len(self.token_index) + 1,\n                                        self.embedding_dims,\n                                        input_length=self.max_tokens,\n                                        mask_zero=True,\n                                        trainable=trainable_embeddings)\n        else:\n            embedding_layer = Embedding(len(self.token_index) + 1,\n                                        self.embedding_dims,\n                                        weights=[build_embedding_weights(self.token_index, self.embeddings_index)],\n                                        input_length=self.max_tokens,\n                                        mask_zero=True,\n                                        trainable=trainable_embeddings)\n\n        sequence_input = Input(shape=(self.max_tokens,), dtype=\'int32\')\n        x = embedding_layer(sequence_input)\n        x = token_encoder_model(x)\n        x = Dense(self.num_classes, activation=output_activation)(x)\n        return Model(sequence_input, x)\n'"
tests/models/test_sentence_model.py,0,"b""import pytest\nfrom keras_text.models import SentenceModelFactory\nfrom keras_text.models import YoonKimCNN, AttentionRNN, StackedRNN, AveragingEncoder\n\n\ndef _test_build(token_encoder_model, sentence_encoder_model):\n    test_index = {'hello': 1, 'kitty': 2}\n\n    if sentence_encoder_model.allows_dynamic_length():\n        factory = SentenceModelFactory(10, test_index, max_sents=None, max_tokens=200, embedding_type=None)\n        model = factory.build_model(token_encoder_model, sentence_encoder_model)\n        model.compile(optimizer='adam', loss='categorical_crossentropy')\n        model.summary()\n    else:\n        # Should fail since this model does not allow dynamic mini-batches.\n        factory = SentenceModelFactory(10, test_index, max_sents=None, max_tokens=200, embedding_type=None)\n        with pytest.raises(ValueError):\n            factory.build_model(token_encoder_model, sentence_encoder_model)\n\n        factory = SentenceModelFactory(10, test_index, max_sents=500, max_tokens=200, embedding_type=None)\n        model = factory.build_model(token_encoder_model, sentence_encoder_model)\n        model.compile(optimizer='adam', loss='categorical_crossentropy')\n        model.summary()\n\n\ndef test_hierarchical_attention_model():\n    _test_build(AttentionRNN(), AttentionRNN())\n\n\ndef test_combinations():\n    encoders = [YoonKimCNN(), AttentionRNN(), StackedRNN(), AveragingEncoder()]\n    for word_encoder in encoders:\n        for sentence_encoder in encoders:\n            print('Testing combination {}, {}'.format(word_encoder.__class__, sentence_encoder.__class__))\n            _test_build(word_encoder, sentence_encoder)\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/models/test_token_model.py,0,"b""import pytest\nfrom keras_text.models import TokenModelFactory\nfrom keras_text.models import YoonKimCNN, AttentionRNN, StackedRNN\n\n\ndef _test_build(token_encoder_model):\n    test_index = {'hello': 1, 'kitty': 2}\n\n    if token_encoder_model.allows_dynamic_length():\n        factory = TokenModelFactory(1, test_index, max_tokens=None, embedding_type=None)\n        model = factory.build_model(token_encoder_model)\n        model.compile(optimizer='adam', loss='categorical_crossentropy')\n        model.summary()\n    else:\n        # Should fail since this model does not allow dynamic mini-batches.\n        factory = TokenModelFactory(1, test_index, max_tokens=None, embedding_type=None)\n        with pytest.raises(ValueError):\n            factory.build_model(token_encoder_model)\n\n        factory = TokenModelFactory(1, test_index, max_tokens=100, embedding_type=None)\n        model = factory.build_model(token_encoder_model)\n        model.compile(optimizer='adam', loss='categorical_crossentropy')\n        model.summary()\n\n\ndef test_yoon_kim_cnn():\n    _test_build(YoonKimCNN())\n\n\ndef test_attention_rnn():\n    _test_build(AttentionRNN())\n\n\ndef test_stacked_rnn():\n    _test_build(StackedRNN())\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
