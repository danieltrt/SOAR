file_path,api_count,code
setup.py,0,"b""from setuptools import setup\nfrom setuptools import find_packages\n\n\nversion = '0.5.0'\n\nsetup(name='keras-vis',\n      version=version,\n      description='Neural Network visualization toolkit for keras',\n      author='Raghavendra Kotikalapudi',\n      author_email='ragha@outlook.com',\n      url='https://github.com/raghakot/keras-vis',\n      download_url='https://github.com/raghakot/keras-vis/tarball/{}'.format(version),\n      license='MIT',\n      install_requires=['keras>=2.0', 'six', 'scikit-image', 'matplotlib', 'h5py'],\n      extras_require={\n          'vis_utils': ['Pillow', 'imageio'],\n          'tests': ['pytest',\n                    'pytest-pep8',\n                    'pytest-xdist',\n                    'pytest-cov'],\n      },\n      include_package_data=True,\n      packages=find_packages())\n"""
docs/__init__.py,0,b''
docs/md_autogen.py,0,"b'""""""\nParses source code to generate API docs in markdown.\n""""""\n\nimport os\nimport re\nimport inspect\nfrom inspect import getdoc, getargspec, getsourcefile, getsourcelines, getmembers\nfrom collections import defaultdict\n\nimport sys\nif sys.version[0] == \'2\':\n    reload(sys)\n    sys.setdefaultencoding(\'utf8\')\n\n_RE_BLOCKSTART = re.compile(r""(Args:|Arg:|Kwargs:|Returns:|Yields:|Kwargs:|Raises:|Notes:|Note:|Examples:|Example:)"",\n                            re.IGNORECASE)\n_RE_ARGSTART = re.compile(r""(\\w*?)\\s*?\\((.*?)\\):(.*)"", re.IGNORECASE)\n_RE_EXCSTART = re.compile(r""(\\w*?):(.*)"", re.IGNORECASE)\n\n#\n# String templates\n#\n\nFUNC_TEMPLATE = """"""-------------------\n\n{section} [{header}]({path})\n\n```python\n{funcdef}\n```\n\n{doc}\n\n""""""\n\nCLASS_TEMPLATE = """"""-------------------\n\n{section} [{header}]({path})\n\n{doc}\n\n{variables}\n\n{init}\n\n{handlers}\n\n{methods}\n\n""""""\n\nMODULE_TEMPLATE = """"""\n**Source:** {path}\n\n{global_vars}\n\n{functions}\n\n{classes}\n\n""""""\n\n\ndef make_iter(obj):\n    """""" Makes an iterable\n    """"""\n    return obj if hasattr(obj, \'__iter__\') else [obj]\n\n\ndef order_by_line_nos(objs, line_nos):\n    """"""Orders the set of `objs` by `line_nos`\n    """"""\n    ordering = sorted(range(len(line_nos)), key=line_nos.__getitem__)\n    return [objs[i] for i in ordering]\n\n\ndef to_md_file(string, filename, out_path="".""):\n    """"""Import a module path and create an api doc from it\n\n    Args:\n        string (str): string with line breaks to write to file.\n        filename (str): filename without the .md\n        out_path (str): The output directory\n    """"""\n    md_file = ""%s.md"" % filename\n    with open(os.path.join(out_path, md_file), ""w"") as f:\n        f.write(string)\n    print(""wrote {}."".format(md_file))\n\n\nclass MarkdownAPIGenerator(object):\n    def __init__(self, src_root, github_link):\n        """"""Initializes the markdown api generator.\n\n        Args:\n            src_root: The root folder name containing all the sources.\n                Ex: src\n            github_link: The base github link. Should include branch name.\n                Ex: https://github.com/raghakot/keras-vis/tree/master\n                All source links are generated with this prefix.\n        """"""\n        self.src_root = src_root\n        self.github_link = github_link\n\n    def get_line_no(self, obj):\n        """"""Gets the source line number of this object. None if `obj` code cannot be found.\n        """"""\n        try:\n            lineno = getsourcelines(obj)[1]\n        except:\n            # no code found\n            lineno = None\n        return lineno\n\n    def get_src_path(self, obj, append_base=True):\n        """"""Creates a src path string with line info for use as markdown link.\n        """"""\n        path = getsourcefile(obj)\n        if self.src_root not in path:\n            # this can happen with e.g.\n            # inlinefunc-wrapped functions\n            if hasattr(obj, ""__module__""):\n                path = ""%s.%s"" % (obj.__module__, obj.__name__)\n            else:\n                path = obj.__name__\n            path = path.replace(""."", ""/"")\n        pre, post = path.rsplit(self.src_root + ""/"", 1)\n\n        lineno = self.get_line_no(obj)\n        lineno = """" if lineno is None else ""#L{}"".format(lineno)\n\n        path = self.src_root + ""/"" + post + lineno\n        if append_base:\n            path = os.path.join(self.github_link, path)\n        return path\n\n    def doc2md(self, func):\n        """"""Parse docstring (parsed with getdoc) according to Google-style\n        formatting and convert to markdown. We support the following\n        Google style syntax:\n\n        Args, Kwargs:\n            argname (type): text\n            freeform text\n        Returns, Yields:\n            retname (type): text\n            freeform text\n        Raises:\n            exceptiontype: text\n            freeform text\n        Notes, Examples:\n            freeform text\n\n        """"""\n        # The specfication of Inspect#getdoc() was changed since version 3.5,\n        # the documentation strings are now inherited if not overridden.\n        # For details see: https://docs.python.org/3.6/library/inspect.html#inspect.getdoc\n        doc = """" if func.__doc__ is None else getdoc(func) or """"\n        blockindent = 0\n        argindent = 1\n        out = []\n\n        for line in doc.split(""\\n""):\n            indent = len(line) - len(line.lstrip())\n            line = line.lstrip()\n            if _RE_BLOCKSTART.match(line):\n                # start of a new block\n                blockindent = indent\n                out.append(""\\n*{}*\\n"".format(line))\n            elif indent > blockindent:\n                if _RE_ARGSTART.match(line):\n                    # start of new argument\n                    out.append(""\\n"" + "" "" * blockindent + "" - "" + _RE_ARGSTART.sub(r""**\\1** (\\2): \\3"", line))\n                    argindent = indent\n                elif _RE_EXCSTART.match(line):\n                    # start of an exception-type block\n                    out.append(""\\n"" + "" "" * blockindent + "" - "" + _RE_EXCSTART.sub(r""**\\1**: \\2"", line))\n                    argindent = indent\n                elif indent > argindent:\n                    out.append(""\\n"" + "" "" * (blockindent + 2) + line)\n                else:\n                    out.append(""\\n"" + line)\n            else:\n                out.append(""\\n"" + line)\n\n        return """".join(out)\n\n    def func2md(self, func, clsname=None, names=None, depth=3):\n        """"""Takes a function (or method) and documents it.\n\n        Args:\n            clsname (str, optional): class name to prepend to funcname.\n            depth (int, optional): number of ### to append to function name\n\n        """"""\n        section = ""#"" * depth\n        if names is None:\n            names = [func.__name__]\n\n        funcname = "", "".join(names)\n        escfuncname = "", "".join([""`%s`"" % funcname if funcname.startswith(""_"") else funcname for funcname in names])\n        header = ""%s%s"" % (""%s."" % clsname if clsname else """", escfuncname)\n\n        path = self.get_src_path(func)\n        doc = self.doc2md(func)\n\n        args, kwargs = [], []\n        spec = getargspec(func)\n        vargsname, kwargsname = spec.varargs, spec.keywords\n        vargs = list(make_iter(spec.args)) if spec.args else []\n        defaults = list(make_iter(spec.defaults)) if spec.defaults else []\n\n        while vargs:\n            if vargs and vargs[0] == ""self"":\n                args.append(vargs.pop(0))\n            elif len(vargs) > len(defaults):\n                args.append(vargs.pop(0))\n            else:\n                default = defaults.pop(0)\n                if isinstance(default, str):\n                    default = ""\\""%s\\"""" % default\n                else:\n                    default = ""%s"" % str(default)\n\n                kwargs.append((vargs.pop(0), default))\n\n        if args:\n            args = "", "".join(""%s"" % arg for arg in args)\n        if kwargs:\n            kwargs = "", "".join(""%s=%s"" % kwarg for kwarg in kwargs)\n            if args:\n                kwargs = "", "" + kwargs\n        if vargsname:\n            vargsname = ""*%s"" % vargsname\n            if args or kwargs:\n                vargsname = "", "" + vargsname\n        if kwargsname:\n            kwargsname = ""**%s"" % kwargsname\n            if args or kwargs or vargsname:\n                kwargsname = "", "" + kwargsname\n\n        _FUNCDEF = ""{funcname}({args}{kwargs}{vargs}{vkwargs})""\n        funcdef = _FUNCDEF.format(funcname=funcname,\n                                  args=args or """",\n                                  kwargs=kwargs or """",\n                                  vargs=vargsname or """",\n                                  vkwargs=kwargsname or """")\n\n        # split the function definition if it is too long\n        lmax = 90\n        if len(funcdef) > lmax:\n            # wrap in the args list\n            split = funcdef.split(""("", 1)\n            # we gradually build the string again\n            rest = split[1]\n            args = rest.split("", "")\n\n            funcname = ""("".join(split[:1]) + ""(""\n            lline = len(funcname)\n            parts = []\n            for arg in args:\n                larg = len(arg)\n                if larg > lmax - 5:\n                    # not much to do if arg is so long\n                    parts.append(arg)\n                elif lline + larg > lmax:\n                    # the next arg is too long, break the line\n                    parts.append(""\\\\\\n    "" + arg)\n                    lline = 0\n                else:\n                    parts.append(arg)\n                lline += len(parts[-1])\n            funcdef = funcname + "", "".join(parts)\n\n        # build the signature\n        string = FUNC_TEMPLATE.format(section=section,\n                                      header=header,\n                                      funcdef=funcdef,\n                                      path=path,\n                                      doc=doc if doc else ""*No documentation found.*"")\n        return string\n\n    def class2md(self, cls, depth=2):\n        """"""Takes a class and creates markdown text to document its methods and variables.\n        """"""\n\n        section = ""#"" * depth\n        subsection = ""#"" * (depth + 2)\n        clsname = cls.__name__\n        modname = cls.__module__\n        header = clsname\n        path = self.get_src_path(cls)\n        doc = self.doc2md(cls)\n\n        try:\n            init = self.func2md(cls.__init__, clsname=clsname)\n        except (ValueError, TypeError):\n            # this happens if __init__ is outside the repo\n            init = """"\n\n        variables = []\n        for name, obj in getmembers(cls, lambda a: not (inspect.isroutine(a) or inspect.ismethod(a))):\n            if not name.startswith(""_"") and type(obj) == property:\n                comments = self.doc2md(obj) or inspect.getcomments(obj)\n                comments = ""\\n %s"" % comments if comments else """"\n                variables.append(""\\n%s %s.%s%s\\n"" % (subsection, clsname, name, comments))\n\n        handlers = []\n        for name, obj in getmembers(cls, inspect.ismethoddescriptor):\n            if not name.startswith(""_"") and hasattr(obj, ""__module__"") and obj.__module__ == modname:\n                handlers.append(""\\n%s %s.%s\\n *Handler*"" % (subsection, clsname, name))\n\n        methods = []\n        for name, obj in getmembers(cls, lambda a: inspect.ismethod(a) or inspect.isfunction(a)):\n            if not name.startswith(""_"") and hasattr(obj,\n                                                    ""__module__"") and obj.__module__ == modname and name not in handlers:\n                methods.append(self.func2md(obj, clsname=clsname, depth=depth + 1))\n\n        string = CLASS_TEMPLATE.format(section=section,\n                                       header=header,\n                                       path=path,\n                                       doc=doc if doc else """",\n                                       init=init,\n                                       variables="""".join(variables),\n                                       handlers="""".join(handlers),\n                                       methods="""".join(methods))\n        return string\n\n    def module2md(self, module):\n        """"""Takes an imported module object and create a Markdown string containing functions and classes.\n        """"""\n        modname = module.__name__\n        path = self.get_src_path(module, append_base=False)\n        path = ""[{}]({})"".format(path, os.path.join(self.github_link, path))\n        found = set()\n\n        classes = []\n        line_nos = []\n        for name, obj in getmembers(module, inspect.isclass):\n            # handle classes\n            found.add(name)\n            if not name.startswith(""_"") and hasattr(obj, ""__module__"") and obj.__module__ == modname:\n                classes.append(self.class2md(obj))\n                line_nos.append(self.get_line_no(obj) or 0)\n        classes = order_by_line_nos(classes, line_nos)\n\n        # Since functions can have multiple aliases.\n        func2names = defaultdict(list)\n        for name, obj in getmembers(module, inspect.isfunction):\n            func2names[obj].append(name)\n\n        functions = []\n        line_nos = []\n        for obj in func2names:\n            names = func2names[obj]\n            found.update(names)\n\n            # Include if within module or included modules within __init__.py and exclude from global variables\n            is_module_within_init = \'__init__.py\' in path and obj.__module__.startswith(modname)\n            if is_module_within_init:\n                found.add(obj.__module__.replace(modname + \'.\', \'\'))\n\n            if hasattr(obj, ""__module__"") and (obj.__module__ == modname or is_module_within_init):\n                names = list(filter(lambda name: not name.startswith(""_""), names))\n                if len(names) > 0:\n                    functions.append(self.func2md(obj, names=names))\n                    line_nos.append(self.get_line_no(obj) or 0)\n        functions = order_by_line_nos(functions, line_nos)\n\n        variables = []\n        line_nos = []\n        for name, obj in module.__dict__.items():\n            if not name.startswith(""_"") and name not in found:\n                if hasattr(obj, ""__module__"") and obj.__module__ != modname:\n                    continue\n                if hasattr(obj, ""__name__"") and not obj.__name__.startswith(modname):\n                    continue\n\n                comments = inspect.getcomments(obj)\n                comments = "": %s"" % comments if comments else """"\n                variables.append(""- **%s**%s"" % (name, comments))\n                line_nos.append(self.get_line_no(obj) or 0)\n\n        variables = order_by_line_nos(variables, line_nos)\n        if variables:\n            new_list = [""**Global Variables**"", ""---------------""]\n            new_list.extend(variables)\n            variables = new_list\n\n        string = MODULE_TEMPLATE.format(path=path,\n                                        global_vars=""\\n"".join(variables) if variables else """",\n                                        functions=""\\n"".join(functions) if functions else """",\n                                        classes="""".join(classes) if classes else """")\n        return string\n'"
docs/update_docs.py,0,"b'import shutil\n\nfrom md_autogen import MarkdownAPIGenerator\nfrom md_autogen import to_md_file\n\nfrom vis import backend\nfrom vis.utils import utils\nfrom vis import visualization\nfrom vis import backprop_modifiers\nfrom vis import callbacks\nfrom vis import grad_modifiers\nfrom vis import input_modifiers\nfrom vis import losses\nfrom vis import optimizer\nfrom vis import regularizers\n\n\ndef generate_api_docs():\n    modules = [\n        backend,\n        utils,\n        visualization,\n        backprop_modifiers,\n        callbacks,\n        grad_modifiers,\n        input_modifiers,\n        losses,\n        optimizer,\n        regularizers\n    ]\n\n    md_gen = MarkdownAPIGenerator(""vis"", ""https://github.com/raghakot/keras-vis/tree/master"")\n    for m in modules:\n        md_string = md_gen.module2md(m)\n        to_md_file(md_string, m.__name__, ""sources"")\n\n\ndef update_index_md():\n    shutil.copyfile(\'../README.md\', \'sources/index.md\')\n\n\ndef copy_templates():\n    shutil.rmtree(\'sources\', ignore_errors=True)\n    shutil.copytree(\'templates\', \'sources\')\n\n\nif __name__ == ""__main__"":\n    copy_templates()\n    update_index_md()\n    generate_api_docs()\n'"
vis/__init__.py,0,b''
vis/backprop_modifiers.py,0,"b'from __future__ import absolute_import\n\nfrom . import backend\nfrom .utils import utils\n\n\ndef guided(model):\n    """"""Modifies backprop to only propagate positive gradients for positive activations.\n\n    Args:\n        model: The `keras.models.Model` instance whose gradient computation needs to be overridden.\n\n    References:\n        Details on guided back propagation can be found in paper: [String For Simplicity: The All Convolutional Net]\n        (https://arxiv.org/pdf/1412.6806.pdf)\n    """"""\n    return backend.modify_model_backprop(model, \'guided\')\n\n\ndef rectified(model):\n    """"""Modifies backprop to only propagate positive gradients.\n\n    Args:\n        model: The `keras.models.Model` instance whose gradient computation needs to be overridden.\n\n    References:\n        Details can be found in the paper: [Visualizing and Understanding Convolutional Networks]\n        (https://arxiv.org/pdf/1311.2901.pdf)\n    """"""\n    return backend.modify_model_backprop(model, \'rectified\')\n\n\n# Create aliases\nrelu = deconv = rectified\n\n\ndef get(identifier):\n    return utils.get_identifier(identifier, globals(), __name__)\n'"
vis/callbacks.py,0,"b'from __future__ import absolute_import\nimport pprint\nimport numpy as np\nfrom .utils import utils\n\ntry:\n    import imageio as imageio\nexcept ImportError:\n    imageio = None\n\n\ndef _check_imageio():\n    if not imageio:\n        raise ImportError(\'Failed to import imageio. You must install imageio\')\n\n\nclass OptimizerCallback(object):\n    """"""Abstract class for defining callbacks for use with [Optimizer.minimize](vis.optimizer.md#optimizerminimize).\n    """"""\n\n    def callback(self, i, named_losses, overall_loss, grads, wrt_value):\n        """"""This function will be called within [optimizer.minimize](vis.optimizer.md#minimize).\n\n        Args:\n            i: The optimizer iteration.\n            named_losses: List of `(loss_name, loss_value)` tuples.\n            overall_loss: Overall weighted loss.\n            grads: The gradient of input image with respect to `wrt_value`.\n            wrt_value: The current `wrt_value`.\n        """"""\n        raise NotImplementedError()\n\n    def on_end(self):\n        """"""Called at the end of optimization process. This function is typically used to cleanup / close any\n        opened resources at the end of optimization.\n        """"""\n        pass\n\n\nclass Print(OptimizerCallback):\n    """"""Callback to print values during optimization.\n    """"""\n    def callback(self, i, named_losses, overall_loss, grads, wrt_value):\n        print(\'Iteration: {}, named_losses: {}, overall loss: {}\'\n              .format(i + 1, pprint.pformat(named_losses), overall_loss))\n\n\nclass GifGenerator(OptimizerCallback):\n    """"""Callback to construct gif of optimized image.\n    """"""\n    def __init__(self, path, input_range=(0, 255)):\n        """"""\n        Args:\n            path: The file path to save gif.\n            input_range: Specifies the input range as a `(min, max)` tuple.\n                This is used to rescale the `wrt_value` passed to `callback` method\n                to the given range. (Default value=(0, 255))\n        """"""\n        self.input_range = input_range\n        _check_imageio()\n        if not path.endswith(\'.gif\'):\n            path += \'.gif\'\n        self.writer = imageio.get_writer(path, mode=\'I\', loop=1)\n\n    def callback(self, i, named_losses, overall_loss, grads, wrt_value):\n        img = utils.deprocess_input(wrt_value[0])\n        # If range has integer numbers, cast to \'uint8\'\n        if self.input_range is not None and \\\n                isinstance(self.input_range[0], int) and \\\n                isinstance(self.input_range[1], int):\n            img = np.clip(img, self.input_range[0], self.input_range[1]).astype(\'uint8\')\n        img = utils.draw_text(img, ""Step {}"".format(i + 1))\n        self.writer.append_data(img)\n\n    def on_end(self):\n        self.writer.close()\n'"
vis/grad_modifiers.py,0,"b'from __future__ import absolute_import\n\nimport numpy as np\nfrom keras import backend as K\nfrom .utils import utils\n\n\ndef negate(grads):\n    """"""Negates the gradients.\n\n    Args:\n        grads: A numpy array of grads to use.\n\n    Returns:\n        The negated gradients.\n    """"""\n    return -grads\n\n\ndef absolute(grads):\n    """"""Computes absolute gradients.\n\n    Args:\n        grads: A numpy array of grads to use.\n\n    Returns:\n        The absolute gradients.\n    """"""\n    return np.abs(grads)\n\n\ndef invert(grads):\n    """"""Inverts the gradients.\n\n    Args:\n        grads: A numpy array of grads to use.\n\n    Returns:\n        The inverted gradients.\n    """"""\n    return 1. / (grads + K.epsilon())\n\n\ndef relu(grads):\n    """"""Clips negative gradient values.\n\n    Args:\n        grads: A numpy array of grads to use.\n\n    Returns:\n        The rectified gradients.\n    """"""\n    grads[grads < 0.] = 0.\n    return grads\n\n\ndef small_values(grads):\n    """"""Can be used to highlight small gradient values.\n\n    Args:\n        grads: A numpy array of grads to use.\n\n    Returns:\n        The modified gradients that highlight small values.\n    """"""\n    return absolute(invert(grads))\n\n\ndef get(identifier):\n    return utils.get_identifier(identifier, globals(), __name__)\n'"
vis/input_modifiers.py,0,"b'from __future__ import absolute_import\n\nimport numpy as np\nfrom scipy.ndimage.interpolation import shift\nfrom .utils import utils\nfrom keras import backend as K\n\n\nclass InputModifier(object):\n    """"""Abstract class for defining an input modifier. An input modifier can be used with the\n    [Optimizer.minimize](vis.optimizer.md#optimizerminimize) to make `pre` and `post` changes to the optimized input\n    during the optimization process.\n\n    ```python\n    modifier.pre(seed_input)\n    # gradient descent update to img\n    modifier.post(seed_input)\n    ```\n    """"""\n\n    def pre(self, inp):\n        """"""Implement pre gradient descent update modification to the input. If pre-processing is not desired,\n        simply ignore the implementation. It returns the unmodified `inp` by default.\n\n        Args:\n            inp: An N-dim numpy array of shape: `(samples, channels, image_dims...)` if `image_data_format=\n                channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n\n        Returns:\n            The modified pre input.\n        """"""\n        return inp\n\n    def post(self, inp):\n        """"""Implement post gradient descent update modification to the input. If post-processing is not desired,\n        simply ignore the implementation. It returns the unmodified `inp` by default.\n\n        Args:\n            inp: An N-dim numpy array of shape: `(samples, channels, image_dims...)` if `image_data_format=\n                channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n\n        Returns:\n            The modified post input.\n        """"""\n        return inp\n\n\nclass Jitter(InputModifier):\n\n    def __init__(self, jitter=0.05):\n        """"""Implements an input modifier that introduces random jitter in `pre`.\n        Jitter has been shown to produce crisper activation maximization images.\n\n        Args:\n            jitter: The amount of jitter to apply, scalar or sequence.\n                If a scalar, same jitter is applied to all image dims. If sequence, `jitter` should contain a value\n                per image dim.\n\n                A value between `[0., 1.]` is interpreted as a percentage of the image dimension. (Default value: 0.05)\n        """"""\n        super(Jitter, self).__init__()\n        self.jitter = np.array(utils.listify(jitter))\n        if np.any(jitter < 0.):\n            raise ValueError(\'Jitter value should be positive\')\n        self._processed = False\n\n    def _process_jitter_values(self, image_dims):\n        if len(self.jitter) == 1:\n            self.jitter = np.repeat(self.jitter, len(image_dims))\n        if len(self.jitter) != len(image_dims):\n            raise RuntimeError(\'Jitter {}, does not match the number of image dims: {}\'\n                               .format(self.jitter, len(image_dims)))\n\n        # Convert percentage to absolute values.\n        for i, jitter_value in enumerate(self.jitter):\n            if jitter_value < 1.:\n                self.jitter[i] = image_dims[i] * jitter_value\n\n        # Round to int.\n        self.jitter = np.int32(self.jitter)\n        self._processed = True\n\n    def pre(self, img):\n        if not self._processed:\n            image_dims = utils.get_img_shape(img)[2:]\n            self._process_jitter_values(image_dims)\n\n        dim_offsets = [np.random.randint(-value, value + 1) for value in self.jitter]\n        if K.image_data_format() == \'channels_first\':\n            shift_vector = np.array([0, 0] + dim_offsets)\n        else:\n            shift_vector = np.array([0] + dim_offsets + [0])\n\n        return shift(img, shift_vector, mode=\'wrap\', order=0)\n'"
vis/losses.py,0,"b'from __future__ import absolute_import\n\nfrom keras import backend as K\nfrom .utils import utils\n\n\nclass Loss(object):\n    """"""Abstract class for defining the loss function to be minimized.\n    The loss function should be built by defining `build_loss` function.\n\n    The attribute `name` should be defined to identify loss function with verbose outputs.\n    Defaults to \'Unnamed Loss\' if not overridden.\n    """"""\n    def __init__(self):\n        self.name = ""Unnamed Loss""\n\n    def __str__(self):\n        return self.name\n\n    def build_loss(self):\n        """"""Implement this function to build the loss function expression.\n        Any additional arguments required to build this loss function may be passed in via `__init__`.\n\n        Ideally, the function expression must be compatible with all keras backends and `channels_first` or\n        `channels_last` image_data_format(s). `utils.slicer` can be used to define data format agnostic slices.\n        (just define it in `channels_first` format, it will automatically shuffle indices for tensorflow\n        which uses `channels_last` format).\n\n        ```python\n        # theano slice\n        conv_layer[:, filter_idx, ...]\n\n        # TF slice\n        conv_layer[..., filter_idx]\n\n        # Backend agnostic slice\n        conv_layer[utils.slicer[:, filter_idx, ...]]\n        ```\n\n        [utils.get_img_shape](vis.utils.utils.md#get_img_shape) is another optional utility that make this easier.\n\n        Returns:\n            The loss expression.\n        """"""\n        raise NotImplementedError()\n\n\nclass ActivationMaximization(Loss):\n    """"""A loss function that maximizes the activation of a set of filters within a particular layer.\n\n    Typically this loss is used to ask the reverse question - What kind of input image would increase the networks\n    confidence, for say, dog class. This helps determine what the network might be internalizing as being the \'dog\'\n    image space.\n\n    One might also use this to generate an input image that maximizes both \'dog\' and \'human\' outputs on the final\n    `keras.layers.Dense` layer.\n    """"""\n    def __init__(self, layer, filter_indices):\n        """"""\n        Args:\n            layer: The keras layer whose filters need to be maximized. This can either be a convolutional layer\n                or a dense layer.\n            filter_indices: filter indices within the layer to be maximized.\n                For `keras.layers.Dense` layer, `filter_idx` is interpreted as the output index.\n\n                If you are optimizing final `keras.layers.Dense` layer to maximize class output, you tend to get\n                better results with \'linear\' activation as opposed to \'softmax\'. This is because \'softmax\'\n                output can be maximized by minimizing scores for other classes.\n        """"""\n        super(ActivationMaximization, self).__init__()\n        self.name = ""ActivationMax Loss""\n        self.layer = layer\n        self.filter_indices = utils.listify(filter_indices)\n\n    def build_loss(self):\n        layer_output = self.layer.output\n\n        # For all other layers it is 4\n        is_dense = K.ndim(layer_output) == 2\n\n        loss = 0.\n        for idx in self.filter_indices:\n            if is_dense:\n                loss += -K.mean(layer_output[:, idx])\n            else:\n                # slicer is used to deal with `channels_first` or `channels_last` image data formats\n                # without the ugly conditional statements.\n                loss += -K.mean(layer_output[utils.slicer[:, idx, ...]])\n\n        return loss\n'"
vis/optimizer.py,0,"b'from __future__ import absolute_import\n\nimport numpy as np\nfrom keras import backend as K\n\nfrom .callbacks import Print\nfrom .grad_modifiers import get\nfrom .utils import utils\n\n\n_PRINT_CALLBACK = Print()\n\n\ndef _identity(x):\n    return x\n\n\nclass Optimizer(object):\n\n    def __init__(self, input_tensor, losses, input_range=(0, 255), wrt_tensor=None, norm_grads=True):\n        """"""Creates an optimizer that minimizes weighted loss function.\n\n        Args:\n            input_tensor: An input tensor of shape: `(samples, channels, image_dims...)` if `image_data_format=\n                channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n            losses: List of ([Loss](vis.losses.md#Loss), weight) tuples.\n            input_range: Specifies the input range as a `(min, max)` tuple. This is used to rescale the\n                final optimized input to the given range. (Default value=(0, 255))\n            wrt_tensor: Short for, with respect to. This instructs the optimizer that the aggregate loss from `losses`\n                should be minimized with respect to `wrt_tensor`.\n                `wrt_tensor` can be any tensor that is part of the model graph. Default value is set to None\n                which means that loss will simply be minimized with respect to `input_tensor`.\n            norm_grads: True to normalize gradients. Normalization avoids very small or large gradients and ensures\n                a smooth gradient gradient descent process. If you want the actual gradient\n                (for example, visualizing attention), set this to false.\n        """"""\n        self.input_tensor = input_tensor\n        self.input_range = input_range\n        self.loss_names = []\n        self.loss_functions = []\n        self.wrt_tensor = self.input_tensor if wrt_tensor is None else wrt_tensor\n        if self.input_tensor is self.wrt_tensor:\n            self.wrt_tensor_is_input_tensor = True\n            self.wrt_tensor = K.identity(self.wrt_tensor)\n        else:\n            self.wrt_tensor_is_input_tensor = False\n\n        overall_loss = None\n        for loss, weight in losses:\n            # Perf optimization. Don\'t build loss function with 0 weight.\n            if weight != 0:\n                loss_fn = weight * loss.build_loss()\n                overall_loss = loss_fn if overall_loss is None else overall_loss + loss_fn\n                self.loss_names.append(loss.name)\n                self.loss_functions.append(loss_fn)\n\n        # Compute gradient of overall with respect to `wrt` tensor.\n        if self.wrt_tensor_is_input_tensor:\n            grads = K.gradients(overall_loss, self.input_tensor)[0]\n        else:\n            grads = K.gradients(overall_loss, self.wrt_tensor)[0]\n        if norm_grads:\n            grads = K.l2_normalize(grads)\n\n        # The main function to compute various quantities in optimization loop.\n        self.compute_fn = K.function([self.input_tensor, K.learning_phase()],\n                                     self.loss_functions + [overall_loss, grads, self.wrt_tensor])\n\n    def _rmsprop(self, grads, cache=None, decay_rate=0.95):\n        """"""Uses RMSProp to compute step from gradients.\n\n        Args:\n            grads: numpy array of gradients.\n            cache: numpy array of same shape as `grads` as RMSProp cache\n            decay_rate: How fast to decay cache\n\n        Returns:\n            A tuple of\n                step: numpy array of the same shape as `grads` giving the step.\n                    Note that this does not yet take the learning rate into account.\n                cache: Updated RMSProp cache.\n        """"""\n        if cache is None:\n            cache = np.zeros_like(grads)\n        cache = decay_rate * cache + (1 - decay_rate) * grads ** 2\n        step = -grads / np.sqrt(cache + K.epsilon())\n        return step, cache\n\n    def _get_seed_input(self, seed_input):\n        """"""Creates a random `seed_input` if None. Otherwise:\n            - Ensures batch_size dim on provided `seed_input`.\n            - Shuffle axis according to expected `image_data_format`.\n        """"""\n        desired_shape = (1, ) + K.int_shape(self.input_tensor)[1:]\n        if seed_input is None:\n            return utils.random_array(desired_shape, mean=np.mean(self.input_range),\n                                      std=0.05 * (self.input_range[1] - self.input_range[0]))\n\n        # Add batch dim if needed.\n        if len(seed_input.shape) != len(desired_shape):\n            seed_input = np.expand_dims(seed_input, 0)\n\n        # Only possible if channel idx is out of place.\n        if seed_input.shape[-1] != desired_shape[-1] and \\\n           seed_input.shape[1] != desired_shape[1]:\n            seed_input = np.moveaxis(seed_input, -1, 1)\n        return seed_input.astype(K.floatx())\n\n    def minimize(self, seed_input=None, max_iter=200,\n                 input_modifiers=None, grad_modifier=None,\n                 callbacks=None, verbose=True):\n        """"""Performs gradient descent on the input image with respect to defined losses.\n\n        Args:\n            seed_input: An N-dim numpy array of shape: `(samples, channels, image_dims...)` if `image_data_format=\n                channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n                Seeded with random noise if set to None. (Default value = None)\n            max_iter: The maximum number of gradient descent iterations. (Default value = 200)\n            input_modifiers: A list of [InputModifier](vis.input_modifiers.md#inputmodifier) instances specifying\n                how to make `pre` and `post` changes to the optimized input during the optimization process.\n                `pre` is applied in list order while `post` is applied in reverse order. For example,\n                `input_modifiers = [f, g]` means that `pre_input = g(f(inp))` and `post_input = f(g(inp))`\n            grad_modifier: gradient modifier to use. See [grad_modifiers](vis.grad_modifiers.md). If you don\'t\n                specify anything, gradients are unchanged. (Default value = None)\n            callbacks: A list of [OptimizerCallback](vis.callbacks.md#optimizercallback) instances to trigger.\n            verbose: Logs individual losses at the end of every gradient descent iteration.\n                Very useful to estimate loss weight factor(s). (Default value = True)\n\n        Returns:\n            The tuple of `(optimized input, grads with respect to wrt, wrt_value)` after gradient descent iterations.\n        """"""\n        seed_input = self._get_seed_input(seed_input)\n        input_modifiers = input_modifiers or []\n        grad_modifier = _identity if grad_modifier is None else get(grad_modifier)\n\n        callbacks = callbacks or []\n        if verbose:\n            callbacks.append(_PRINT_CALLBACK)\n\n        cache = None\n        best_loss = float(\'inf\')\n        best_input = None\n\n        grads = None\n        wrt_value = None\n\n        for i in range(max_iter):\n            # Apply modifiers `pre` step\n            for modifier in input_modifiers:\n                seed_input = modifier.pre(seed_input)\n\n            # 0 learning phase for \'test\'\n            computed_values = self.compute_fn([seed_input, 0])\n            losses = computed_values[:len(self.loss_names)]\n            named_losses = list(zip(self.loss_names, losses))\n            overall_loss, grads, wrt_value = computed_values[len(self.loss_names):]\n\n            # TODO: theano grads shape is inconsistent for some reason. Patch for now and investigate later.\n            if grads.shape != wrt_value.shape:\n                grads = np.reshape(grads, wrt_value.shape)\n\n            # Apply grad modifier.\n            grads = grad_modifier(grads)\n\n            # Trigger callbacks\n            for c in callbacks:\n                c.callback(i, named_losses, overall_loss, grads, wrt_value)\n\n            # Gradient descent update.\n            # It only makes sense to do this if wrt_tensor is input_tensor. Otherwise shapes wont match for the update.\n            if self.wrt_tensor_is_input_tensor:\n                step, cache = self._rmsprop(grads, cache)\n                seed_input += step\n\n            # Apply modifiers `post` step\n            for modifier in reversed(input_modifiers):\n                seed_input = modifier.post(seed_input)\n\n            if overall_loss < best_loss:\n                best_loss = overall_loss.copy()\n                best_input = seed_input.copy()\n\n        # Trigger on_end\n        for c in callbacks:\n            c.on_end()\n\n        return utils.deprocess_input(best_input[0], self.input_range), grads, wrt_value\n'"
vis/regularizers.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\n\nimport numpy as np\nfrom keras import backend as K\n\nfrom .losses import Loss\nfrom .utils import utils\n\n\ndef normalize(input_tensor, output_tensor):\n    """"""Normalizes the `output_tensor` with respect to `input_tensor` dimensions.\n    This makes regularizer weight factor more or less uniform across various input image dimensions.\n\n    Args:\n        input_tensor: An tensor of shape: `(samples, channels, image_dims...)` if `image_data_format=\n                channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n        output_tensor: The tensor to normalize.\n\n    Returns:\n        The normalized tensor.\n    """"""\n    image_dims = utils.get_img_shape(input_tensor)[1:]\n    return output_tensor / np.prod(image_dims)\n\n\nclass TotalVariation(Loss):\n\n    def __init__(self, img_input, beta=2.):\n        """"""Total variation regularizer encourages blobbier and coherent image structures, akin to natural images.\n        See `section 3.2.2` in\n        [Visualizing deep convolutional neural networks using natural pre-images](https://arxiv.org/pdf/1512.02017v3.pdf)\n        for details.\n\n        Args:\n            img_input: An image tensor of shape: `(samples, channels, image_dims...)` if `image_data_format=`channels_first`\n                or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n            beta: Smaller values of beta give sharper but \'spikier\' images.\n                Values \\(\\in [1.5, 3.0]\\) are recommended as a reasonable compromise. (Default value = 2.)\n        """"""\n        super(TotalVariation, self).__init__()\n        self.name = ""TV({}) Loss"".format(beta)\n        self.img = img_input\n        self.beta = beta\n\n    def build_loss(self):\n        r""""""Implements the N-dim version of function\n        $$TV^{\\beta}(x) = \\sum_{whc} \\left ( \\left ( x(h, w+1, c) - x(h, w, c) \\right )^{2} +\n        \\left ( x(h+1, w, c) - x(h, w, c) \\right )^{2} \\right )^{\\frac{\\beta}{2}}$$\n        to return total variation for all images in the batch.\n        """"""\n        image_dims = K.ndim(self.img) - 2\n\n        # Constructing slice [1:] + [:-1] * (image_dims - 1) and [:-1] * (image_dims)\n        start_slice = [slice(1, None, None)] + [slice(None, -1, None) for _ in range(image_dims - 1)]\n        end_slice = [slice(None, -1, None) for _ in range(image_dims)]\n        samples_channels_slice = [slice(None, None, None), slice(None, None, None)]\n\n        # Compute pixel diffs by rolling slices to the right per image dim.\n        tv = None\n        for i in range(image_dims):\n            ss = tuple(samples_channels_slice + start_slice)\n            es = tuple(samples_channels_slice + end_slice)\n            diff_square = K.square(self.img[utils.slicer[ss]] - self.img[utils.slicer[es]])\n            tv = diff_square if tv is None else tv + diff_square\n\n            # Roll over to next image dim\n            start_slice = np.roll(start_slice, 1).tolist()\n            end_slice = np.roll(end_slice, 1).tolist()\n\n        tv = K.sum(K.pow(tv, self.beta / 2.))\n        return normalize(self.img, tv)\n\n\nclass LPNorm(Loss):\n\n    def __init__(self, img_input, p=6.):\n        """"""\n        Builds a L-p norm function. This regularizer encourages the intensity of pixels to stay bounded.\n            i.e., prevents pixels from taking on very large values.\n\n        Args:\n            img_input: 4D image input tensor to the model of shape: `(samples, channels, rows, cols)`\n                if data_format=\'channels_first\' or `(samples, rows, cols, channels)` if data_format=\'channels_last\'.\n            p: The pth norm to use. If p = float(\'inf\'), infinity-norm will be used.\n        """"""\n        super(LPNorm, self).__init__()\n        if p < 1:\n            raise ValueError(\'p value should range between [1, inf)\')\n        self.name = ""L-{} Norm Loss"".format(p)\n        self.p = p\n        self.img = img_input\n\n    def build_loss(self):\n        # Infinity norm\n        if np.isinf(self.p):\n            value = K.max(self.img)\n        else:\n            value = K.pow(K.sum(K.pow(K.abs(self.img), self.p)), 1. / self.p)\n\n        return normalize(self.img, value)\n'"
applications/self_driving/model.py,0,"b""from keras.layers.core import Dropout, Flatten\nfrom keras.layers.convolutional import MaxPooling2D, Conv2D\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\n\nFRAME_H = 70\nFRAME_W = 180\n\n\ndef build_model():\n    inp = Input(shape=(FRAME_H, FRAME_W, 3))\n    x = Conv2D(filters=8, kernel_size=(5, 5), activation='relu')(inp)\n    x = MaxPooling2D((2, 2))(x)\n\n    x = Conv2D(filters=16, kernel_size=(5, 5), activation='relu')(x)\n    x = MaxPooling2D((2, 2))(x)\n\n    x = Conv2D(filters=32, kernel_size=(5, 5), activation='relu')(x)\n    x = MaxPooling2D((2, 2))(x)\n\n    x = Flatten()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1, activation='tanh')(x)\n    return Model(inputs=[inp], outputs=[x])\n\n\nif __name__ == '__main__':\n    model = build_model()\n    model.summary()\n"""
tests/vis/test_optimizer.py,0,"b'import pytest\n\nimport keras.backend as K\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom vis.optimizer import Optimizer\nfrom vis.losses import Loss\n\n\nclass _DummyLoss(Loss):\n    def __init__(self, model):\n        self.name = \'dummy-loss\'\n        self.output = model.output\n\n    def build_loss(self):\n        return K.sum(self.output * self.output)\n\n\n@pytest.fixture(scope=""function"", autouse=True)\ndef model_and_losses():\n    model = Sequential([Dense(4, activation=\'linear\', input_shape=(2, ))])\n    losses = [(_DummyLoss(model), 1)]\n    return model, losses\n\n\ndef test_wrt_tensor_is_None(model_and_losses):\n    model, losses = model_and_losses\n    opt = Optimizer(model.input, losses, wrt_tensor=None)\n    opt.minimize()\n\n    assert opt.wrt_tensor_is_input_tensor\n    assert opt.wrt_tensor is not None\n    assert opt.wrt_tensor != opt.input_tensor\n\n\ndef test_wrt_tensor_is_input_tensor(model_and_losses):\n    model, losses = model_and_losses\n    opt = Optimizer(model.input, losses, wrt_tensor=model.input)\n    opt.minimize()\n\n    assert opt.wrt_tensor_is_input_tensor\n    assert opt.wrt_tensor is not None\n    assert opt.wrt_tensor != opt.input_tensor\n\n\ndef test_wrt_tensor_isnt_input_tensor(model_and_losses):\n    model, losses = model_and_losses\n    opt = Optimizer(model.input, losses, wrt_tensor=model.output)\n    opt.minimize()\n\n    assert not opt.wrt_tensor_is_input_tensor\n    assert opt.wrt_tensor is not None\n    assert opt.wrt_tensor != opt.input_tensor\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
vis/backend/__init__.py,0,"b'from keras import backend as K\n\n\n# Import backend depending on config\nif K.backend() == \'tensorflow\':\n    from .tensorflow_backend import *\nelif K.backend() == \'theano\':\n    from .theano_backend import *\nelse:\n    raise ValueError(""Backend \'{}\' not supported"".format(K.backend()))\n'"
vis/backend/tensorflow_backend.py,7,"b'from __future__ import absolute_import\n\nimport os\nimport tempfile\nimport inspect\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..utils import utils\nfrom tensorflow.python.framework import ops\nimport keras\nfrom keras.models import load_model\nfrom keras.layers import advanced_activations, Activation\n\n\n# Register all classes with `advanced_activations` module\n_ADVANCED_ACTIVATIONS = set()\nfor name, obj in inspect.getmembers(advanced_activations, inspect.isclass):\n    if not name.startswith(""_"") and hasattr(obj, ""__module__"") and obj.__module__ == advanced_activations.__name__:\n        _ADVANCED_ACTIVATIONS.add(obj)\n_ADVANCED_ACTIVATIONS = tuple(_ADVANCED_ACTIVATIONS)\n\n\ndef _register_guided_gradient(name):\n    if name not in ops._gradient_registry._registry:\n        @tf.RegisterGradient(name)\n        def _guided_backprop(op, grad):\n            dtype = op.outputs[0].dtype\n            gate_g = tf.cast(grad > 0., dtype)\n            gate_y = tf.cast(op.outputs[0] > 0., dtype)\n            return gate_y * gate_g * grad\n\n\ndef _register_rectified_gradient(name):\n    if name not in ops._gradient_registry._registry:\n        @tf.RegisterGradient(name)\n        def _relu_backprop(op, grad):\n            dtype = op.outputs[0].dtype\n            gate_g = tf.cast(grad > 0., dtype)\n            return gate_g * grad\n\n# Map of modifier type to registration function.\n_BACKPROP_MODIFIERS = {\n    \'guided\': _register_guided_gradient,\n    \'rectified\': _register_rectified_gradient\n}\n\n\n# Maintain a mapping of original model, backprop_modifier -> modified model as cache.\n_MODIFIED_MODEL_CACHE = dict()\n\n\ndef modify_model_backprop(model, backprop_modifier):\n    """"""Creates a copy of model by modifying all activations to use a custom op to modify the backprop behavior.\n\n    Args:\n        model:  The `keras.models.Model` instance.\n        backprop_modifier: One of `{\'guided\', \'rectified\'}`\n\n    Returns:\n        A copy of model with modified activations for backwards pass.\n    """"""\n    # The general strategy is as follows:\n    # - Save original model so that upstream callers don\'t see unexpected results with their models.\n    # - Call backend specific function that registers the custom op and loads the model under modified context manager.\n    # - Maintain cache to save this expensive process on subsequent calls.\n    # - Load model with custom context modifying backprop behavior.\n    #\n    # The reason for this round about way is because the graph needs to be rebuild when any of its layer builder\n    # functions are changed. This is very complicated to do in Keras and makes the implementation very tightly bound\n    # with keras internals. By saving and loading models, we dont have to worry about future compatibility.\n    #\n    # The only exception to this is the way advanced activations are handled which makes use of some keras internal\n    # knowledge and might break in the future.\n    # ADD on 22 Jul 2018:\n    #     In fact, it has broken. Currently, advanced activations are not supported.\n\n    # 0. Retrieve from cache if previously computed.\n    modified_model = _MODIFIED_MODEL_CACHE.get((model, backprop_modifier))\n    if modified_model is not None:\n        return modified_model\n\n    model_path = os.path.join(tempfile.gettempdir(), next(tempfile._get_candidate_names()) + \'.h5\')\n    try:\n        # 1. Save original model\n        model.save(model_path)\n\n        # 2. Register modifier and load modified model under custom context.\n        modifier_fn = _BACKPROP_MODIFIERS.get(backprop_modifier)\n        if modifier_fn is None:\n            raise ValueError(""\'{}\' modifier is not supported"".format(backprop_modifier))\n        modifier_fn(backprop_modifier)\n\n        # 3. Create graph under custom context manager.\n        with tf.get_default_graph().gradient_override_map({\'Relu\': backprop_modifier}):\n            #  This should rebuild graph with modifications.\n            modified_model = load_model(model_path)\n\n            # Cache to improve subsequent call performance.\n            _MODIFIED_MODEL_CACHE[(model, backprop_modifier)] = modified_model\n            return modified_model\n    finally:\n        os.remove(model_path)\n\n\ndef set_random_seed(seed_value=1337):\n    """"""Sets random seed value for reproducibility.\n\n    Args:\n        seed_value: The seed value to use. (Default Value = infamous 1337)\n    """"""\n    np.random.seed(seed_value)\n    tf.set_random_seed(seed_value)\n'"
vis/backend/theano_backend.py,0,"b'from __future__ import absolute_import\nimport numpy as np\n\n\ndef modify_model_backprop(model, backprop_modifier):\n    """"""Creates a copy of model by modifying all activations to use a custom op to modify the backprop behavior.\n\n   Args:\n       model:  The `keras.models.Model` instance.\n       backprop_modifier: One of `{\'guided\', \'rectified\'}`\n\n   Returns:\n       A copy of model with modified activations for backwards pass.\n   """"""\n    raise NotImplementedError(\'Theano version is not supported yet.\')\n\n\ndef set_random_seed(seed_value=1337):\n    """"""Sets random seed value for reproducibility.\n\n    Args:\n        seed_value: The seed value to use. (Default Value = infamous 1337)\n    """"""\n    np.random.seed(seed_value)\n'"
vis/utils/__init__.py,0,b''
vis/utils/test_utils.py,1,"b'from __future__ import absolute_import\n\nimport six\nimport tensorflow as tf\nfrom keras import backend as K\nfrom . import utils\n\n\ndef across_data_formats(func):\n    """"""Function wrapper to run tests on multiple keras data_format and clean up after TensorFlow tests.\n\n    Args:\n        func: test function to clean up after.\n\n    Returns:\n        A function wrapping the input function.\n    """"""\n    @six.wraps(func)\n    def wrapper(*args, **kwargs):\n        for data_format in {\'channels_first\', \'channels_last\'}:\n            K.set_image_data_format(data_format)\n            func(*args, **kwargs)\n            if K.backend() == \'tensorflow\':\n                K.clear_session()\n                tf.reset_default_graph()\n    return wrapper\n\n\ndef skip_backends(backends):\n    """"""Function wrapper to specify which backends should skip the test.\n\n    Args:\n        backends: The list of backends to skip.\n\n    Returns:\n        A function wrapping the input function.\n    """"""\n    backends = set(utils.listify(backends))\n\n    def decorator(func):\n        @six.wraps(func)\n        def wrapper(*args, **kwargs):\n            if K.backend() in backends:\n                return\n            func(*args, **kwargs)\n        return wrapper\n    return decorator\n'"
vis/utils/utils.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\n\nimport os\nimport tempfile\nimport math\nimport json\nimport six\n\nimport numpy as np\nimport matplotlib.font_manager as fontman\n\nfrom skimage import io, transform\nfrom keras import backend as K\nfrom keras.models import load_model\n\nimport logging\nlogger = logging.getLogger(__name__)\n\ntry:\n    import PIL as pil\n    from PIL import ImageFont\n    from PIL import Image\n    from PIL import ImageDraw\nexcept ImportError:\n    pil = None\n\n\n# Globals\n_CLASS_INDEX = None\n\n\ndef _check_pil():\n    if not pil:\n        raise ImportError(\'Failed to import PIL. You must install Pillow\')\n\n\ndef _find_font_file(query):\n    """"""Utility to find font file.\n    """"""\n    return list(filter(lambda path: query.lower() in os.path.basename(path).lower(), fontman.findSystemFonts()))\n\n\ndef reverse_enumerate(iterable):\n    """"""Enumerate over an iterable in reverse order while retaining proper indexes, without creating any copies.\n    """"""\n    return zip(reversed(range(len(iterable))), reversed(iterable))\n\n\ndef listify(value):\n    """"""Ensures that the value is a list. If it is not a list, it creates a new list with `value` as an item.\n    """"""\n    if not isinstance(value, list):\n        value = [value]\n    return value\n\n\ndef add_defaults_to_kwargs(defaults, **kwargs):\n    """"""Updates `kwargs` with dict of `defaults`\n\n    Args:\n        defaults: A dictionary of keys and values\n        **kwargs: The kwargs to update.\n\n    Returns:\n        The updated kwargs.\n    """"""\n    defaults = dict(defaults)\n    defaults.update(kwargs)\n    return defaults\n\n\ndef get_identifier(identifier, module_globals, module_name):\n    """"""Helper utility to retrieve the callable function associated with a string identifier.\n\n    Args:\n        identifier: The identifier. Could be a string or function.\n        module_globals: The global objects of the module.\n        module_name: The module name\n\n    Returns:\n        The callable associated with the identifier.\n    """"""\n    if isinstance(identifier, six.string_types):\n        fn = module_globals.get(identifier)\n        if fn is None:\n            raise ValueError(\'Unknown {}: {}\'.format(module_name, identifier))\n        return fn\n    elif callable(identifier):\n        return identifier\n    else:\n        raise ValueError(\'Could not interpret identifier\')\n\n\ndef apply_modifications(model, custom_objects=None):\n    """"""Applies modifications to the model layers to create a new Graph. For example, simply changing\n    `model.layers[idx].activation = new activation` does not change the graph. The entire graph needs to be updated\n    with modified inbound and outbound tensors because of change in layer building function.\n\n    Args:\n        model: The `keras.models.Model` instance.\n\n    Returns:\n        The modified model with changes applied. Does not mutate the original `model`.\n    """"""\n    # The strategy is to save the modified model and load it back. This is done because setting the activation\n    # in a Keras layer doesnt actually change the graph. We have to iterate the entire graph and change the\n    # layer inbound and outbound nodes with modified tensors. This is doubly complicated in Keras 2.x since\n    # multiple inbound and outbound nodes are allowed with the Graph API.\n    model_path = os.path.join(tempfile.gettempdir(), next(tempfile._get_candidate_names()) + \'.h5\')\n    try:\n        model.save(model_path)\n        return load_model(model_path, custom_objects=custom_objects)\n    finally:\n        os.remove(model_path)\n\n\ndef random_array(shape, mean=128., std=20.):\n    """"""Creates a uniformly distributed random array with the given `mean` and `std`.\n\n    Args:\n        shape: The desired shape\n        mean: The desired mean (Default value = 128)\n        std: The desired std (Default value = 20)\n\n    Returns: Random numpy array of given `shape` uniformly distributed with desired `mean` and `std`.\n    """"""\n    x = np.random.random(shape)\n    # normalize around mean=0, std=1\n    x = (x - np.mean(x)) / (np.std(x) + K.epsilon())\n    # and then around the desired mean/std\n    x = (x * std) + mean\n    return x\n\n\ndef find_layer_idx(model, layer_name):\n    """"""Looks up the layer index corresponding to `layer_name` from `model`.\n\n    Args:\n        model: The `keras.models.Model` instance.\n        layer_name: The name of the layer to lookup.\n\n    Returns:\n        The layer index if found. Raises an exception otherwise.\n    """"""\n    layer_idx = None\n    for idx, layer in enumerate(model.layers):\n        if layer.name == layer_name:\n            layer_idx = idx\n            break\n\n    if layer_idx is None:\n        raise ValueError(""No layer with name \'{}\' within the model"".format(layer_name))\n    return layer_idx\n\n\ndef deprocess_input(input_array, input_range=(0, 255)):\n    """"""Utility function to scale the `input_array` to `input_range` throwing away high frequency artifacts.\n\n    Args:\n        input_array: An N-dim numpy array.\n        input_range: Specifies the input range as a `(min, max)` tuple to rescale the `input_array`.\n\n    Returns:\n        The rescaled `input_array`.\n    """"""\n    # normalize tensor: center on 0., ensure std is 0.1\n    input_array = input_array.copy()\n    input_array -= input_array.mean()\n    input_array /= (input_array.std() + K.epsilon())\n    input_array *= 0.1\n\n    # clip to [0, 1]\n    input_array += 0.5\n    input_array = np.clip(input_array, 0, 1)\n\n    # Convert to `input_range`\n    return (input_range[1] - input_range[0]) * input_array + input_range[0]\n\n\ndef stitch_images(images, margin=5, cols=5):\n    """"""Utility function to stitch images together with a `margin`.\n\n    Args:\n        images: The array of 2D images to stitch.\n        margin: The black border margin size between images (Default value = 5)\n        cols: Max number of image cols. New row is created when number of images exceed the column size.\n            (Default value = 5)\n\n    Returns:\n        A single numpy image array comprising of input images.\n    """"""\n    if len(images) == 0:\n        return None\n\n    h, w, c = images[0].shape\n    n_rows = int(math.ceil(len(images) / cols))\n    n_cols = min(len(images), cols)\n\n    out_w = n_cols * w + (n_cols - 1) * margin\n    out_h = n_rows * h + (n_rows - 1) * margin\n    stitched_images = np.zeros((out_h, out_w, c), dtype=images[0].dtype)\n\n    for row in range(n_rows):\n        for col in range(n_cols):\n            img_idx = row * cols + col\n            if img_idx >= len(images):\n                break\n\n            stitched_images[(h + margin) * row: (h + margin) * row + h,\n                            (w + margin) * col: (w + margin) * col + w, :] = images[img_idx]\n\n    return stitched_images\n\n\ndef get_img_shape(img):\n    """"""Returns image shape in a backend agnostic manner.\n\n    Args:\n        img: An image tensor of shape: `(channels, image_dims...)` if data_format=\'channels_first\' or\n            `(image_dims..., channels)` if data_format=\'channels_last\'.\n\n    Returns:\n        Tuple containing image shape information in `(samples, channels, image_dims...)` order.\n    """"""\n    if isinstance(img, np.ndarray):\n        shape = img.shape\n    else:\n        shape = K.int_shape(img)\n\n    if K.image_data_format() == \'channels_last\':\n        shape = list(shape)\n        shape.insert(1, shape[-1])\n        shape = tuple(shape[:-1])\n    return shape\n\n\ndef load_img(path, grayscale=False, target_size=None):\n    """"""Utility function to load an image from disk.\n\n    Args:\n      path: The image file path.\n      grayscale: True to convert to grayscale image (Default value = False)\n      target_size: (w, h) to resize. (Default value = None)\n\n    Returns:\n        The loaded numpy image.\n    """"""\n    img = io.imread(path, grayscale)\n    if target_size:\n        img = transform.resize(img, target_size, preserve_range=True).astype(\'uint8\')\n    return img\n\n\ndef lookup_imagenet_labels(indices):\n    """"""Utility function to return the image net label for the final `dense` layer output index.\n\n    Args:\n        indices: Could be a single value or an array of indices whose labels should be looked up.\n\n    Returns:\n        Image net label corresponding to the image category.\n    """"""\n    global _CLASS_INDEX\n    if _CLASS_INDEX is None:\n        with open(os.path.join(os.path.dirname(__file__), \'../../resources/imagenet_class_index.json\')) as f:\n            _CLASS_INDEX = json.load(f)\n\n    indices = listify(indices)\n    return [_CLASS_INDEX[str(idx)][1] for idx in indices]\n\n\ndef draw_text(img, text, position=(10, 10), font=\'FreeSans.ttf\', font_size=14, color=(0, 0, 0)):\n    """"""Draws text over the image. Requires PIL.\n\n    Args:\n        img: The image to use.\n        text: The text string to overlay.\n        position: The text (x, y) position. (Default value = (10, 10))\n        font: The ttf or open type font to use. (Default value = \'FreeSans.ttf\')\n        font_size: The text font size. (Default value = 12)\n        color: The (r, g, b) values for text color. (Default value = (0, 0, 0))\n\n    Returns: Image overlayed with text.\n    """"""\n    _check_pil()\n\n    font_files = _find_font_file(font)\n    if len(font_files) == 0:\n        logger.warn(""Failed to lookup font \'{}\', falling back to default"".format(font))\n        font = ImageFont.load_default()\n    else:\n        font = ImageFont.truetype(font_files[0], font_size)\n\n    # Don\'t mutate original image\n    img = Image.fromarray(img)\n    draw = ImageDraw.Draw(img)\n    draw.text(position, text, fill=color, font=font)\n    return np.asarray(img)\n\n\ndef bgr2rgb(img):\n    """"""Converts an RGB image to BGR and vice versa\n\n    Args:\n        img: Numpy array in RGB or BGR format\n\n    Returns: The converted image format\n    """"""\n    return img[..., ::-1]\n\n\ndef normalize(array, min_value=0., max_value=1.):\n    """"""Normalizes the numpy array to (min_value, max_value)\n\n    Args:\n        array: The numpy array\n        min_value: The min value in normalized array (Default value = 0)\n        max_value: The max value in normalized array (Default value = 1)\n\n    Returns:\n        The array normalized to range between (min_value, max_value)\n    """"""\n    arr_min = np.min(array)\n    arr_max = np.max(array)\n    normalized = (array - arr_min) / (arr_max - arr_min + K.epsilon())\n    return (max_value - min_value) * normalized + min_value\n\n\nclass _BackendAgnosticImageSlice(object):\n    """"""Utility class to make image slicing uniform across various `image_data_format`.\n    """"""\n\n    def __getitem__(self, item_slice):\n        """"""Assuming a slice for shape `(samples, channels, image_dims...)`\n        """"""\n        if K.image_data_format() == \'channels_first\':\n            return item_slice\n        else:\n            # Move channel index to last position.\n            item_slice = list(item_slice)\n            item_slice.append(item_slice.pop(1))\n            return tuple(item_slice)\n\n\n""""""Slice utility to make image slicing uniform across various `image_data_format`.\nExample:\n    conv_layer[utils.slicer[:, filter_idx, :, :]] will work for both `channels_first` and `channels_last` image\n    data formats even though, in tensorflow, slice should be conv_layer[utils.slicer[:, :, :, filter_idx]]\n""""""\nslicer = _BackendAgnosticImageSlice()\n'"
vis/visualization/__init__.py,0,"b'from __future__ import absolute_import\n\n\nfrom .activation_maximization import visualize_activation_with_losses\nfrom .activation_maximization import visualize_activation\n\nfrom .saliency import visualize_saliency_with_losses\nfrom .saliency import visualize_saliency\nfrom .saliency import visualize_cam_with_losses\nfrom .saliency import visualize_cam\n\nfrom keras import backend as K\n\n\ndef get_num_filters(layer):\n    """"""Determines the number of filters within the given `layer`.\n\n    Args:\n        layer: The keras layer to use.\n\n    Returns:\n        Total number of filters within `layer`.\n        For `keras.layers.Dense` layer, this is the total number of outputs.\n    """"""\n    # Handle layers with no channels.\n    if K.ndim(layer.output) == 2:\n        return K.int_shape(layer.output)[-1]\n\n    channel_idx = 1 if K.image_data_format() == \'channels_first\' else -1\n    return K.int_shape(layer.output)[channel_idx]\n\n\ndef overlay(array1, array2, alpha=0.5):\n    """"""Overlays `array1` onto `array2` with `alpha` blending.\n\n    Args:\n        array1: The first numpy array.\n        array2: The second numpy array.\n        alpha: The alpha value of `array1` as overlayed onto `array2`. This value needs to be between [0, 1],\n            with 0 being `array2` only to 1 being `array1` only (Default value = 0.5).\n\n    Returns:\n        The `array1`, overlayed with `array2` using `alpha` blending.\n    """"""\n    if alpha < 0. or alpha > 1.:\n        raise ValueError(""`alpha` needs to be between [0, 1]"")\n    if array1.shape != array2.shape:\n        raise ValueError(\'`array1` and `array2` must have the same shapes\')\n\n    return (array1 * alpha + array2 * (1. - alpha)).astype(array1.dtype)\n'"
vis/visualization/activation_maximization.py,0,"b'from __future__ import absolute_import\n\nimport numpy as np\nfrom keras import backend as K\n\nfrom ..losses import ActivationMaximization\nfrom ..optimizer import Optimizer\nfrom ..regularizers import TotalVariation, LPNorm\nfrom ..backprop_modifiers import get\nfrom ..utils import utils\n\n\ndef visualize_activation_with_losses(input_tensor, losses, wrt_tensor=None,\n                                     seed_input=None, input_range=(0, 255),\n                                     **optimizer_params):\n    """"""Generates the `input_tensor` that minimizes the weighted `losses`. This function is intended for advanced\n    use cases where a custom loss is desired.\n\n    Args:\n        input_tensor: An input tensor of shape: `(samples, channels, image_dims...)` if `image_data_format=\n            channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n        wrt_tensor: Short for, with respect to. The gradients of losses are computed with respect to this tensor.\n            When None, this is assumed to be the same as `input_tensor` (Default value: None)\n        losses: List of ([Loss](vis.losses.md#Loss), weight) tuples.\n        seed_input: Seeds the optimization with a starting image. Initialized with a random value when set to None.\n            (Default value = None)\n        input_range: Specifies the input range as a `(min, max)` tuple. This is used to rescale the\n            final optimized input to the given range. (Default value=(0, 255))\n        optimizer_params: The **kwargs for optimizer [params](vis.optimizer.md#optimizerminimize). Will default to\n            reasonable values when required keys are not found.\n\n    Returns:\n        The model input that minimizes the weighted `losses`.\n    """"""\n    # Default optimizer kwargs.\n    optimizer_params = utils.add_defaults_to_kwargs({\n        \'seed_input\': seed_input,\n        \'max_iter\': 200,\n        \'verbose\': False\n    }, **optimizer_params)\n\n    opt = Optimizer(input_tensor, losses, input_range, wrt_tensor=wrt_tensor)\n    img = opt.minimize(**optimizer_params)[0]\n\n    # If range has integer numbers, cast to \'uint8\'\n    if isinstance(input_range[0], int) and isinstance(input_range[1], int):\n        img = np.clip(img, input_range[0], input_range[1]).astype(\'uint8\')\n\n    if K.image_data_format() == \'channels_first\':\n        img = np.moveaxis(img, 0, -1)\n    return img\n\n\ndef visualize_activation(model, layer_idx, filter_indices=None, wrt_tensor=None,\n                         seed_input=None, input_range=(0, 255),\n                         backprop_modifier=None, grad_modifier=None,\n                         act_max_weight=1, lp_norm_weight=10, tv_weight=10,\n                         **optimizer_params):\n    """"""Generates the model input that maximizes the output of all `filter_indices` in the given `layer_idx`.\n\n    Args:\n        model: The `keras.models.Model` instance. The model input shape must be: `(samples, channels, image_dims...)`\n            if `image_data_format=channels_first` or `(samples, image_dims..., channels)` if\n            `image_data_format=channels_last`.\n        layer_idx: The layer index within `model.layers` whose filters needs to be visualized.\n        filter_indices: filter indices within the layer to be maximized.\n            If None, all filters are visualized. (Default value = None)\n            For `keras.layers.Dense` layer, `filter_idx` is interpreted as the output index.\n            If you are visualizing final `keras.layers.Dense` layer, consider switching \'softmax\' activation for\n            \'linear\' using [utils.apply_modifications](vis.utils.utils.md#apply_modifications) for better results.\n        wrt_tensor: Short for, with respect to. The gradients of losses are computed with respect to this tensor.\n            When None, this is assumed to be the same as `input_tensor` (Default value: None)\n        seed_input: Seeds the optimization with a starting input. Initialized with a random value when set to None.\n            (Default value = None)\n        input_range: Specifies the input range as a `(min, max)` tuple. This is used to rescale the\n            final optimized input to the given range. (Default value=(0, 255))\n        backprop_modifier: backprop modifier to use. See [backprop_modifiers](vis.backprop_modifiers.md). If you don\'t\n            specify anything, no backprop modification is applied. (Default value = None)\n        grad_modifier: gradient modifier to use. See [grad_modifiers](vis.grad_modifiers.md). If you don\'t\n            specify anything, gradients are unchanged (Default value = None)\n        act_max_weight: The weight param for `ActivationMaximization` loss. Not used if 0 or None. (Default value = 1)\n        lp_norm_weight: The weight param for `LPNorm` regularization loss. Not used if 0 or None. (Default value = 10)\n        tv_weight: The weight param for `TotalVariation` regularization loss. Not used if 0 or None. (Default value = 10)\n        optimizer_params: The **kwargs for optimizer [params](vis.optimizer.md#optimizerminimize). Will default to\n            reasonable values when required keys are not found.\n\n    Example:\n        If you wanted to visualize the input image that would maximize the output index 22, say on\n        final `keras.layers.Dense` layer, then, `filter_indices = [22]`, `layer_idx = dense_layer_idx`.\n\n        If `filter_indices = [22, 23]`, then it should generate an input image that shows features of both classes.\n\n    Returns:\n        The model input that maximizes the output of `filter_indices` in the given `layer_idx`.\n    """"""\n    if backprop_modifier is not None:\n        modifier_fn = get(backprop_modifier)\n        model = modifier_fn(model)\n\n    losses = [\n        (ActivationMaximization(model.layers[layer_idx], filter_indices), act_max_weight),\n        (LPNorm(model.input), lp_norm_weight),\n        (TotalVariation(model.input), tv_weight)\n    ]\n\n    # Add grad_filter to optimizer_params.\n    optimizer_params = utils.add_defaults_to_kwargs({\n        \'grad_modifier\': grad_modifier\n    }, **optimizer_params)\n\n    return visualize_activation_with_losses(model.input, losses, wrt_tensor,\n                                            seed_input, input_range, **optimizer_params)\n'"
vis/visualization/saliency.py,0,"b'from __future__ import absolute_import\n\nimport numpy as np\nfrom scipy.ndimage.interpolation import zoom\n\nfrom keras.layers.convolutional import _Conv\nfrom keras.layers.pooling import _Pooling1D, _Pooling2D, _Pooling3D\nfrom keras.layers.wrappers import Wrapper\nfrom keras import backend as K\n\nfrom ..losses import ActivationMaximization\nfrom ..optimizer import Optimizer\nfrom ..backprop_modifiers import get\nfrom ..utils import utils\n\n\ndef _find_penultimate_layer(model, layer_idx, penultimate_layer_idx):\n    """"""Searches for the nearest penultimate `Conv` or `Pooling` layer.\n\n    Args:\n        model: The `keras.models.Model` instance.\n        layer_idx: The layer index within `model.layers`.\n        penultimate_layer_idx: The pre-layer to `layer_idx`. If set to None, the nearest penultimate\n            `Conv` or `Pooling` layer is used.\n\n    Returns:\n        The penultimate layer.\n    """"""\n    if penultimate_layer_idx is None:\n        for idx, layer in utils.reverse_enumerate(model.layers[:layer_idx - 1]):\n            if isinstance(layer, Wrapper):\n                layer = layer.layer\n            if isinstance(layer, (_Conv, _Pooling1D, _Pooling2D, _Pooling3D)):\n                penultimate_layer_idx = idx\n                break\n\n    if penultimate_layer_idx is None:\n        raise ValueError(\'Unable to determine penultimate `Conv` or `Pooling` \'\n                         \'layer for layer_idx: {}\'.format(layer_idx))\n\n    # Handle negative indexing otherwise the next check can fail.\n    if layer_idx < 0:\n        layer_idx = len(model.layers) + layer_idx\n    if penultimate_layer_idx > layer_idx:\n        raise ValueError(\'`penultimate_layer_idx` needs to be before `layer_idx`\')\n\n    return model.layers[penultimate_layer_idx]\n\n\ndef visualize_saliency_with_losses(input_tensor, losses, seed_input, wrt_tensor=None, grad_modifier=\'absolute\', keepdims=False):\n    """"""Generates an attention heatmap over the `seed_input` by using positive gradients of `input_tensor`\n    with respect to weighted `losses`.\n\n    This function is intended for advanced use cases where a custom loss is desired. For common use cases,\n    refer to `visualize_class_saliency` or `visualize_regression_saliency`.\n\n    For a full description of saliency, see the paper:\n    [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps]\n    (https://arxiv.org/pdf/1312.6034v2.pdf)\n\n    Args:\n        input_tensor: An input tensor of shape: `(samples, channels, image_dims...)` if `image_data_format=\n            channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n        losses: List of ([Loss](vis.losses.md#Loss), weight) tuples.\n        seed_input: The model input for which activation map needs to be visualized.\n        wrt_tensor: Short for, with respect to. The gradients of losses are computed with respect to this tensor.\n            When None, this is assumed to be the same as `input_tensor` (Default value: None)\n        grad_modifier: gradient modifier to use. See [grad_modifiers](vis.grad_modifiers.md). By default `absolute`\n            value of gradients are used. To visualize positive or negative gradients, use `relu` and `negate`\n            respectively. (Default value = \'absolute\')\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If keepdims is False, the channels axis is deleted.\n            If keepdims is True, the grad with same shape as input_tensor is returned. (Default value: False)\n\n    Returns:\n        The normalized gradients of `seed_input` with respect to weighted `losses`.\n    """"""\n    opt = Optimizer(input_tensor, losses, wrt_tensor=wrt_tensor, norm_grads=False)\n    grads = opt.minimize(seed_input=seed_input, max_iter=1, grad_modifier=grad_modifier, verbose=False)[1]\n\n    if not keepdims:\n        channel_idx = 1 if K.image_data_format() == \'channels_first\' else -1\n        grads = np.max(grads, axis=channel_idx)\n    return utils.normalize(grads)[0]\n\n\ndef visualize_saliency(model, layer_idx, filter_indices, seed_input, wrt_tensor=None,\n                       backprop_modifier=None, grad_modifier=\'absolute\', keepdims=False):\n    """"""Generates an attention heatmap over the `seed_input` for maximizing `filter_indices`\n    output in the given `layer_idx`.\n\n    Args:\n        model: The `keras.models.Model` instance. The model input shape must be: `(samples, channels, image_dims...)`\n            if `image_data_format=channels_first` or `(samples, image_dims..., channels)` if\n            `image_data_format=channels_last`.\n        layer_idx: The layer index within `model.layers` whose filters needs to be visualized.\n        filter_indices: filter indices within the layer to be maximized.\n            If None, all filters are visualized. (Default value = None)\n            For `keras.layers.Dense` layer, `filter_idx` is interpreted as the output index.\n            If you are visualizing final `keras.layers.Dense` layer, consider switching \'softmax\' activation for\n            \'linear\' using [utils.apply_modifications](vis.utils.utils.md#apply_modifications) for better results.\n        seed_input: The model input for which activation map needs to be visualized.\n        wrt_tensor: Short for, with respect to. The gradients of losses are computed with respect to this tensor.\n            When None, this is assumed to be the same as `input_tensor` (Default value: None)\n        backprop_modifier: backprop modifier to use. See [backprop_modifiers](vis.backprop_modifiers.md). If you don\'t\n            specify anything, no backprop modification is applied. (Default value = None)\n        grad_modifier: gradient modifier to use. See [grad_modifiers](vis.grad_modifiers.md). By default `absolute`\n            value of gradients are used. To visualize positive or negative gradients, use `relu` and `negate`\n            respectively. (Default value = \'absolute\')\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If keepdims is False, the channels axis is deleted.\n            If keepdims is True, the grad with same shape as input_tensor is returned. (Default value: False)\n\n    Example:\n        If you wanted to visualize attention over \'bird\' category, say output index 22 on the\n        final `keras.layers.Dense` layer, then, `filter_indices = [22]`, `layer = dense_layer`.\n\n        One could also set filter indices to more than one value. For example, `filter_indices = [22, 23]` should\n        (hopefully) show attention map that corresponds to both 22, 23 output categories.\n\n    Returns:\n        The heatmap image indicating the `seed_input` regions whose change would most contribute towards\n        maximizing the output of `filter_indices`.\n    """"""\n    if backprop_modifier is not None:\n        modifier_fn = get(backprop_modifier)\n        model = modifier_fn(model)\n\n    # `ActivationMaximization` loss reduces as outputs get large, hence negative gradients indicate the direction\n    # for increasing activations. Multiply with -1 so that positive gradients indicate increase instead.\n    losses = [\n        (ActivationMaximization(model.layers[layer_idx], filter_indices), -1)\n    ]\n    return visualize_saliency_with_losses(model.input, losses, seed_input, wrt_tensor, grad_modifier, keepdims)\n\n\ndef visualize_cam_with_losses(input_tensor, losses, seed_input, penultimate_layer, grad_modifier=None):\n    """"""Generates a gradient based class activation map (CAM) by using positive gradients of `input_tensor`\n    with respect to weighted `losses`.\n\n    For details on grad-CAM, see the paper:\n    [Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization]\n    (https://arxiv.org/pdf/1610.02391v1.pdf).\n\n    Unlike [class activation mapping](https://arxiv.org/pdf/1512.04150v1.pdf), which requires minor changes to\n    network architecture in some instances, grad-CAM has a more general applicability.\n\n    Compared to saliency maps, grad-CAM is class discriminative; i.e., the \'cat\' explanation exclusively highlights\n    cat regions and not the \'dog\' region and vice-versa.\n\n    Args:\n        input_tensor: An input tensor of shape: `(samples, channels, image_dims...)` if `image_data_format=\n            channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n        losses: List of ([Loss](vis.losses.md#Loss), weight) tuples.\n        seed_input: The model input for which activation map needs to be visualized.\n        penultimate_layer: The pre-layer to `layer_idx` whose feature maps should be used to compute gradients\n            with respect to filter output.\n        grad_modifier: gradient modifier to use. See [grad_modifiers](vis.grad_modifiers.md). If you don\'t\n            specify anything, gradients are unchanged (Default value = None)\n\n    Returns:\n        The normalized gradients of `seed_input` with respect to weighted `losses`.\n    """"""\n    penultimate_output = penultimate_layer.output\n    opt = Optimizer(input_tensor, losses, wrt_tensor=penultimate_output, norm_grads=False)\n    _, grads, penultimate_output_value = opt.minimize(seed_input, max_iter=1, grad_modifier=grad_modifier, verbose=False)\n\n    # For numerical stability. Very small grad values along with small penultimate_output_value can cause\n    # w * penultimate_output_value to zero out, even for reasonable fp precision of float32.\n    grads = grads / (np.max(grads) + K.epsilon())\n\n    # Average pooling across all feature maps.\n    # This captures the importance of feature map (channel) idx to the output.\n    channel_idx = 1 if K.image_data_format() == \'channels_first\' else -1\n    other_axis = np.delete(np.arange(len(grads.shape)), channel_idx)\n    weights = np.mean(grads, axis=tuple(other_axis))\n\n    # Generate heatmap by computing weight * output over feature maps\n    output_dims = utils.get_img_shape(penultimate_output_value)[2:]\n    heatmap = np.zeros(shape=output_dims, dtype=K.floatx())\n    for i, w in enumerate(weights):\n        if channel_idx == -1:\n            heatmap += w * penultimate_output_value[0, ..., i]\n        else:\n            heatmap += w * penultimate_output_value[0, i, ...]\n\n    # ReLU thresholding to exclude pattern mismatch information (negative gradients).\n    heatmap = np.maximum(heatmap, 0)\n\n    # The penultimate feature map size is definitely smaller than input image.\n    input_dims = utils.get_img_shape(input_tensor)[2:]\n\n    # Figure out the zoom factor.\n    zoom_factor = [i / (j * 1.0) for i, j in iter(zip(input_dims, output_dims))]\n    heatmap = zoom(heatmap, zoom_factor)\n    return utils.normalize(heatmap)\n\n\ndef visualize_cam(model, layer_idx, filter_indices,\n                  seed_input, penultimate_layer_idx=None,\n                  backprop_modifier=None, grad_modifier=None):\n    """"""Generates a gradient based class activation map (grad-CAM) that maximizes the outputs of\n    `filter_indices` in `layer_idx`.\n\n    Args:\n        model: The `keras.models.Model` instance. The model input shape must be: `(samples, channels, image_dims...)`\n            if `image_data_format=channels_first` or `(samples, image_dims..., channels)` if\n            `image_data_format=channels_last`.\n        layer_idx: The layer index within `model.layers` whose filters needs to be visualized.\n        filter_indices: filter indices within the layer to be maximized.\n            If None, all filters are visualized. (Default value = None)\n            For `keras.layers.Dense` layer, `filter_idx` is interpreted as the output index.\n            If you are visualizing final `keras.layers.Dense` layer, consider switching \'softmax\' activation for\n            \'linear\' using [utils.apply_modifications](vis.utils.utils.md#apply_modifications) for better results.\n        seed_input: The input image for which activation map needs to be visualized.\n        penultimate_layer_idx: The pre-layer to `layer_idx` whose feature maps should be used to compute gradients\n            wrt filter output. If not provided, it is set to the nearest penultimate `Conv` or `Pooling` layer.\n        backprop_modifier: backprop modifier to use. See [backprop_modifiers](vis.backprop_modifiers.md). If you don\'t\n            specify anything, no backprop modification is applied. (Default value = None)\n        grad_modifier: gradient modifier to use. See [grad_modifiers](vis.grad_modifiers.md). If you don\'t\n            specify anything, gradients are unchanged (Default value = None)\n\n     Example:\n        If you wanted to visualize attention over \'bird\' category, say output index 22 on the\n        final `keras.layers.Dense` layer, then, `filter_indices = [22]`, `layer = dense_layer`.\n\n        One could also set filter indices to more than one value. For example, `filter_indices = [22, 23]` should\n        (hopefully) show attention map that corresponds to both 22, 23 output categories.\n\n    Returns:\n        The heatmap image indicating the input regions whose change would most contribute towards\n        maximizing the output of `filter_indices`.\n    """"""\n    if backprop_modifier is not None:\n        modifier_fn = get(backprop_modifier)\n        model = modifier_fn(model)\n\n    penultimate_layer = _find_penultimate_layer(model, layer_idx, penultimate_layer_idx)\n\n    # `ActivationMaximization` outputs negative gradient values for increase in activations. Multiply with -1\n    # so that positive gradients indicate increase instead.\n    losses = [\n        (ActivationMaximization(model.layers[layer_idx], filter_indices), -1)\n    ]\n    return visualize_cam_with_losses(model.input, losses, seed_input, penultimate_layer, grad_modifier)\n'"
tests/vis/backend/test_backend.py,1,"b'import pytest\nimport numpy as np\n\nfrom vis.backend import modify_model_backprop\nfrom vis.utils.test_utils import skip_backends\n\nimport keras\nfrom keras.models import Model, Input, Sequential\nfrom keras.layers import Dense\nfrom keras.initializers import Constant\nfrom keras import backend as K\nfrom keras.activations import get\nfrom keras.layers import advanced_activations, Activation\n\n\ndef _compute_grads(model, input_array):\n    grads_fn = K.gradients(model.output, model.input)[0]\n    compute_fn = K.function([model.input, K.learning_phase()], [grads_fn])\n    return compute_fn([np.array([input_array]), 0])[0][0]\n\n\n@skip_backends(\'theano\')\ndef test_guided_grad_modifier():\n    # Create a simple 2 dense layer model.\n    simple_model = Sequential([\n        Dense(2, activation=\'relu\', use_bias=False, kernel_initializer=Constant([[-1., 1.], [-1., 1.]]), input_shape=(2,)),\n        Dense(1, activation=\'linear\', use_bias=False, kernel_initializer=Constant([-1., 1.]))\n    ])\n    simple_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam())\n\n    # Create a simple 2 dense layer model using Activation.\n    simple_model_with_activation = Sequential([\n        Dense(2, activation=\'linear\', use_bias=False, kernel_initializer=Constant([[-1., 1.], [-1., 1.]]), input_shape=(2,)),\n        Activation(\'relu\'),\n        Dense(1, activation=\'linear\', use_bias=False, kernel_initializer=Constant([-1., 1.]))\n    ])\n    simple_model_with_activation.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam())\n\n    for i, model in enumerate([simple_model, simple_model_with_activation]):\n        # Create guided backprop model\n        modified_model = modify_model_backprop(model, \'guided\')\n\n        # Gradients are zeros.\n        input_array = [0., 0.]\n        assert np.array_equal(_compute_grads(model, input_array), [0., 0.])\n        assert np.array_equal(_compute_grads(modified_model, input_array), [0., 0.])\n\n        # Below 3 cases, GuidedBackprop gradients is the same as Original gradients.\n        input_array = [1., 0.]\n        assert np.array_equal(_compute_grads(model, input_array), [1., 1.])\n        assert np.array_equal(_compute_grads(modified_model, input_array), [1., 1.])\n\n        input_array = [0., 1.]\n        assert np.array_equal(_compute_grads(model, input_array), [1., 1.])\n        assert np.array_equal(_compute_grads(modified_model, input_array), [1., 1.])\n\n        input_array = [1., 1.]\n        assert np.array_equal(_compute_grads(model, input_array), [1., 1.])\n        assert np.array_equal(_compute_grads(modified_model, input_array), [1., 1.])\n\n        # If inputs contains negative values,\n        # GuidedBackprop gradients is not the same as Original gradients.\n        input_array = [-1., 0.]\n        assert np.array_equal(_compute_grads(model, input_array), [1., 1.])\n        assert np.array_equal(_compute_grads(modified_model, input_array), [0., 0.])\n\n        input_array = [0., -1.]\n        assert np.array_equal(_compute_grads(model, input_array), [1., 1.])\n        assert np.array_equal(_compute_grads(modified_model, input_array), [0., 0.])\n\n        input_array = [-1., -1.]\n        assert np.array_equal(_compute_grads(model, input_array), [1., 1.])\n        assert np.array_equal(_compute_grads(modified_model, input_array), [0., 0.])\n\n        # Activation is not changed.\n        if i == 0:  # modified first model\n            modified_model.layers[0].activation == keras.activations.relu\n            modified_model.layers[1].activation == keras.activations.linear\n        if i == 1:  # modified second model\n            modified_model.layers[0].activation == keras.activations.linear\n            modified_model.layers[1].activation == keras.activations.relu\n            modified_model.layers[2].activation == keras.activations.linear\n\n\n# Currently, the modify_model_backprop function doesn\'t support advanced activation.\n# Therefore, this test case will temporarily comment out.\n#\n# @skip_backends(\'theano\')\n# def test_advanced_activations():\n#     """""" Tests that various ways of specifying activations in keras models are handled when replaced with Relu\n#     """"""\n#     inp = Input(shape=(2, ))\n#     x = Dense(5, activation=\'elu\')(inp)\n#     x = advanced_activations.LeakyReLU()(x)\n#     x = Activation(\'elu\')(x)\n#     model = Model(inp, x)\n#\n#     # Ensure that layer.activation, Activation and advanced activations are replaced with relu\n#     modified_model = modify_model_backprop(model, \'guided\')\n#     assert modified_model.layers[1].activation == get(\'relu\')\n#     assert modified_model.layers[2].activation == get(\'relu\')\n#     assert modified_model.layers[3].activation == get(\'relu\')\n#\n#     # Ensure that original model is unchanged.\n#     assert model.layers[1].activation == get(\'elu\')\n#     assert isinstance(model.layers[2], advanced_activations.LeakyReLU)\n#     assert model.layers[3].activation == get(\'elu\')\n\n\n# @skip_backends(\'theano\')\n# def test_rectified_grad_modifier():\n#     # Only test tensorflow implementation for now.\n#     if K.backend() == \'theano\':\n#         return\n#\n#     # Create a simple linear sequence x -> linear(w.x) with weights w1 = -1, w2 = 1.\n#     inp = Input(shape=(2, ))\n#     out = Dense(1, activation=\'linear\', use_bias=False, kernel_initializer=Constant([-1., 1.]))(inp)\n#     model = Model(inp, out)\n#\n#     # Original model gradient should be [w1, w2]\n#     assert np.array_equal(_compute_grads(model, [1., -1.]), [-1., 1.])\n#\n#     # Original gradient is [-1, 1] but new gradient should be [0, 1]\n#     # First one is clipped because of negative gradient.\n#     modified_model = modify_model_backprop(model, \'rectified\')\n#\n#     # TODO: Interestingly this does not work for some reason.\n#     # It is failing at tf.cast(grad > 0., dtype)\n#     assert np.array_equal(_compute_grads(modified_model, [1., -1.]), [0., 1.])\n#\n#     # Ensure that the original model reference remains unchanged.\n#     assert model.layers[1].activation == get(\'linear\')\n#     assert modified_model.layers[1].activation == get(\'relu\')\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/vis/utils/test_utils.py,0,"b""import pytest\nfrom vis.utils import utils\nfrom keras import backend as K\n\n\ndef test_get_img_shape_on_2d_image():\n    n = 5\n    channels = 4\n    dim1 = 1\n    dim2 = 2\n\n    K.set_image_data_format('channels_first')\n    assert (n, channels, dim1, dim2) == utils.get_img_shape(K.ones(shape=(n, channels, dim1, dim2)))\n\n    K.set_image_data_format('channels_last')\n    assert (n, channels, dim1, dim2) == utils.get_img_shape(K.ones(shape=(n, dim1, dim2, channels)))\n\n\ndef test_get_img_shape_on_3d_image():\n    n = 5\n    channels = 4\n    dim1 = 1\n    dim2 = 2\n    dim3 = 3\n\n    K.set_image_data_format('channels_first')\n    assert (n, channels, dim1, dim2, dim3) == utils.get_img_shape(K.ones(shape=(n, channels, dim1, dim2, dim3)))\n\n    K.set_image_data_format('channels_last')\n    assert (n, channels, dim1, dim2, dim3) == utils.get_img_shape(K.ones(shape=(n, dim1, dim2, dim3, channels)))\n\n\ndef test_reverse_iterable():\n    assert list(utils.reverse_enumerate('abcde')) == [(4, 'e'), (3, 'd'), (2, 'c'), (1, 'b'), (0, 'a')]\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/vis/visualization/test_saliency.py,0,"b""import pytest\n\nimport numpy as np\nimport keras.backend as K\nfrom keras.layers import Input, Dense, Flatten\nfrom keras.models import Model\nfrom vis.losses import ActivationMaximization\nfrom vis.visualization.saliency import visualize_saliency, visualize_saliency_with_losses\n\n\n@pytest.fixture(scope='function', autouse=True)\ndef model():\n    inputs = Input((28, 28, 3))\n    x = Flatten()(inputs)\n    x = Dense(100, activation='relu')(x)\n    x = Dense(1)(x)\n    return Model(inputs, x)\n\n\n@pytest.fixture(scope='function', autouse=True)\ndef data():\n    return np.random.rand(1, 28, 28, 3)\n\n\ndef test_visualize_saliency(model, data):\n    # FIXME Can't set None to filter_indices with Theano backend.\n    # To get green test, it set zero.\n    # grads = visualize_saliency(model, -1, filter_indices=None, seed_input=data)\n    grads = visualize_saliency(model, -1, filter_indices=0, seed_input=data)\n    assert grads.shape == (28, 28)\n\n\ndef test_visualize_saliency_with_unkeepdims(model, data):\n    grads = visualize_saliency(model, -1, 0, data, keepdims=True)\n    assert grads.shape == (28, 28, 3)\n\n\ndef test_visualize_saliency_with_losses(model, data):\n    losses = [\n        (ActivationMaximization(model.layers[-1], 0), -1)\n    ]\n    grads = visualize_saliency_with_losses(model.input, losses, data)\n    assert grads.shape == (28, 28)\n\n\ndef test_visualize_saliency_with_losses_with_unkeepdims(model, data):\n    losses = [\n        (ActivationMaximization(model.layers[-1], 0), -1)\n    ]\n    grads = visualize_saliency_with_losses(model.input, losses, data, keepdims=True)\n    assert grads.shape == (28, 28, 3)\n\n\ndef test_for_issues_135():\n    inputs = Input((35,))\n    x = Dense(64, activation='relu')(inputs)\n    x = Dense(100, activation='relu')(x)\n    x = Dense(50)(x)\n    model = Model(inputs, x)\n    data = np.random.rand(1, 35)\n    grads = visualize_saliency(model, -1, 0, data, keepdims=True)\n    assert grads.shape == (35,)\n"""
