file_path,api_count,code
docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# WTTE-RNN documentation build configuration file, created by\n# sphinx-quickstart on Thu Oct 13 11:28:17 2016.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath(\'.\'))\nfrom __future__ import print_function\nimport sys\nimport os\n\non_rtd = os.environ.get(\'READTHEDOCS\', None) == \'True\'\nroot_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \'..\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.coverage\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'WTTE-RNN\'\ncopyright = \'2016-2018. Egil Martinsson and contributors\'\nauthor = \'Contributors.\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\'_build\',\'*migrations*\',\'**/venv*\',\'*-env\', \'*tensorflow*\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#html_theme = \'alabaster\'\nif not on_rtd:  # only import and set the theme if we\'re building docs locally\n    try:\n        import sphinx_rtd_theme\n    except ImportError:\n        print(\'Please do pip install sphinx-rtd-theme first.\', file=sys.stderr)\n        sys.exit(1)\n    html_theme = \'sphinx_rtd_theme\'\n    html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\n# html_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'h\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'r\', \'sv\', \'tr\'\n#html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# Now only \'ja\' uses this config value\n#html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'WTTE-RNNdoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n\n# Latex figure (float) alignment\n#\'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n  (master_doc, \'WTTE-RNN.tex\', \'WTTE-RNN Documentation\',\n   \'wtte-rnn\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'WTTE-RNN\', \'WTTE-RNN Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  (master_doc, \'WTTE-RNN\', \'WTTE-RNN Documentation\',\n   author, \'WTTE-RNN\', \'Weibull Time To Event Reccurent Neural Network\',\n   \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n\ndef run_apidoc(_):\n    from sphinx.apidoc import main\n    cur_dir = os.path.abspath(os.path.dirname(__file__))\n    module_dir = os.path.join(root_dir, \'python/wtte\')\n    main([\'-M\', \'-l\', \'-o\', cur_dir, module_dir, \'--force\'])\n\ndef setup(app):\n    app.connect(\'builder-inited\', run_apidoc)\n'"
python/__init__.py,0,b''
python/setup.py,0,"b'from setuptools import setup\nfrom setuptools import find_packages\n\n\n# Used in CI and by deveopers\nbuild_requires = [\n    \'wheel\',\n    \'twine\',\n]\n\n# Used in CI and by deveopers\ntest_requires = [\n    \'pytest\',\n    \'pytest-runner\',\n    \'flake8\',\n]\n\n# Used in developers\' PCs only\ndev_requires = [\n    \'pytest-sugar\',\n    \'sphinx\',\n    \'sphinx-rtd-theme\',\n]\n\n# Used in ReadTheDocs build servers\n# (actually they already have Sphinx, but let\'s specify it explicitly.)\ndocs_requires = [\n    \'sphinx\',\n]\n\nsetup(\n    name=\'wtte\',\n    version=\'1.1.1\',\n    description=\'Weibull Time To Event model. A Deep Learning model for churn- and failure prediction and everything else.\',\n    author=\'Egil Martinsson\',\n    author_email=\'egil.martinsson@gmail.com\',\n    url=\'https://github.com/ragulpr/wtte-rnn/\',\n    license=\'MIT\',\n    install_requires=[\n        \'keras>=2.0\',\n        \'numpy\',\n        \'pandas\',\n        \'scipy\',\n        \'six==1.10.0\',\n    ],\n    extras_require={\n        \'plot\': [\'matplotlib\'],\n        \'tf\': [""tensorflow>=1.1.0""],\n        \'tf_gpu\': [""tensorflow-gpu>=1.1.0""],\n        \'build\': build_requires,\n        \'test\': test_requires,\n        \'dev\': dev_requires,\n        \'docs\': docs_requires,\n    },\n    setup_requires=[\'pytest-runner\'],\n    tests_require=[\'pytest\'],\n    packages=find_packages(\'.\', exclude=[\'examples\', \'tests\']),\n)\n'"
python/tests/__init__.py,0,b''
python/tests/test_keras.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport pytest\n\nimport numpy as np\n\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Lambda, Masking\nfrom keras.layers.wrappers import TimeDistributed\n\nfrom keras.optimizers import RMSprop\n\nfrom wtte import wtte as wtte\nfrom wtte.data_generators import generate_weibull\n\n\ndef test_keras_unstack_hack():\n    y_true_np = np.random.random([1, 3, 2])\n    y_true_np[:, :, 0] = 0\n    y_true_np[:, :, 1] = 1\n\n    y_true_keras = K.variable(y_true_np)\n\n    y, u = wtte._keras_unstack_hack(y_true_keras)\n    y_true_keras_new = K.stack([y, u], axis=-1)\n\n    np.testing.assert_array_equal(K.eval(y_true_keras_new), y_true_np)\n\n# SANITY CHECK: Use pure Weibull data censored at C(ensoring point).\n# Should converge to the generating A(alpha) and B(eta) for each timestep\n\n\ndef get_data(discrete_time):\n    y_test, y_train, u_train = generate_weibull(A=real_a,\n                                                B=real_b,\n                                                # <np.inf -> impose censoring\n                                                C=censoring_point,\n                                                shape=[n_sequences,\n                                                       n_timesteps, 1],\n                                                discrete_time=discrete_time)\n    # With random input it _should_ learn weight 0\n    x_train = x_test = np.random.uniform(\n        low=-1, high=1, size=[n_sequences, n_timesteps, n_features])\n\n    # y_test is uncencored data\n    y_test = np.append(y_test, np.ones_like(y_test), axis=-1)\n    y_train = np.append(y_train, u_train, axis=-1)\n    return y_train, x_train, y_test, x_test\n\n\nn_sequences = 10000\nn_timesteps = 2\nn_features = 1\n\nreal_a = 3.\nreal_b = 2.\ncensoring_point = real_a * 2\n\nmask_value = -10.\n\nlr = 0.02\n\n\ndef model_no_masking(discrete_time, init_alpha, max_beta):\n    model = Sequential()\n    model.add(TimeDistributed(Dense(2), input_shape=(n_timesteps, n_features)))\n\n    model.add(Lambda(wtte.output_lambda, arguments={""init_alpha"": init_alpha,\n                                                    ""max_beta_value"": max_beta}))\n\n    if discrete_time:\n        loss = wtte.loss(kind=\'discrete\').loss_function\n    else:\n        loss = wtte.loss(kind=\'continuous\').loss_function\n\n    model.compile(loss=loss, optimizer=RMSprop(lr=lr))\n\n    return model\n\n\ndef model_masking(discrete_time, init_alpha, max_beta):\n    model = Sequential()\n\n    model.add(Masking(mask_value=mask_value,\n                      input_shape=(n_timesteps, n_features)))\n    model.add(TimeDistributed(Dense(2)))\n    model.add(Lambda(wtte.output_lambda, arguments={""init_alpha"": init_alpha,\n                                                    ""max_beta_value"": max_beta}))\n\n    if discrete_time:\n        loss = wtte.loss(kind=\'discrete\', reduce_loss=False).loss_function\n    else:\n        loss = wtte.loss(kind=\'continuous\', reduce_loss=False).loss_function\n\n    model.compile(loss=loss, optimizer=RMSprop(\n        lr=lr), sample_weight_mode=\'temporal\')\n    return model\n\n\ndef keras_loglik_runner(discrete_time, add_masking):\n    np.random.seed(1)\n#    tf.set_random_seed(1)\n\n    y_train, x_train, y_test, x_test = get_data(discrete_time=discrete_time)\n\n    if add_masking:\n        # If masking doesn\'t work, it\'ll learn nonzero weights (strong signal):\n        x_train[:int(n_sequences / 2), int(n_timesteps / 2):, :] = mask_value\n        y_train[:int(n_sequences / 2), int(n_timesteps / 2):, 0] = real_a * 300.\n\n        weights = np.ones([n_sequences, n_timesteps])\n        weights[:int(n_sequences / 2), int(n_timesteps / 2):] = 0.\n\n        model = model_masking(\n            discrete_time, init_alpha=real_a, max_beta=real_b * 3)\n        model.fit(x_train, y_train,\n                  epochs=5,\n                  batch_size=100,\n                  verbose=0,\n                  sample_weight=weights,\n                  )\n    else:\n        model = model_no_masking(\n            discrete_time, init_alpha=real_a, max_beta=real_b * 3)\n\n        model.fit(x_train, y_train,\n                  epochs=5,\n                  batch_size=100,\n                  verbose=0,\n                  )\n\n    predicted = model.predict(x_test[:1, :, :])\n    a_val = predicted[:, :, 0].mean()\n    b_val = predicted[:, :, 1].mean()\n\n    print(np.abs(real_a - a_val), np.abs(real_b - b_val))\n    assert np.abs(real_a - a_val) < 0.1, \'alpha not converged\'\n    assert np.abs(real_b - b_val) < 0.1, \'beta not converged\'\n\n\ndef test_loglik_continuous():\n    keras_loglik_runner(discrete_time=False, add_masking=False)\n\n\ndef test_loglik_discrete():\n    keras_loglik_runner(discrete_time=True, add_masking=False)\n\n\ndef test_loglik_continuous_masking():\n    keras_loglik_runner(discrete_time=False, add_masking=True)\n\n\ndef test_output_lambda_initialization():\n    # Initializing beta =1 gives us a simple initialization rule for alpha.\n    # it also makes sense considering it initializes the hazard to flat\n    # and in general as a regular exponential regression model.\n    init_alpha = 5\n    n_features = 1\n    n_timesteps = 10\n    n_sequences = 5\n    np.random.seed(1)\n\n    model = Sequential()\n\n    # Identity layer\n    model.add(Lambda(lambda x: x, input_shape=(n_timesteps, n_features)))\n    model.add(Dense(2))\n    model.add(Lambda(wtte.output_lambda,\n                     arguments={""init_alpha"": init_alpha,\n                                ""max_beta_value"": 4.,\n                                ""alpha_kernel_scalefactor"": 1.\n                                }))\n\n    # Test\n    x = np.random.normal(0, 0.01, size=[n_sequences, n_timesteps, n_features])\n    predicted = model.predict(x)\n\n    # Check that it initializes +- 0.1 from init_alpha,beta=1\n    abs_error = np.abs(predicted[:, :, 0].flatten() - init_alpha).mean()\n    assert abs_error < 0.1, \'alpha initialization error\' + str(abs_error)\n    abs_error = np.abs(predicted[:, :, 1].flatten() - 1.).mean()\n    assert abs_error < 0.1, \'beta initialization error\' + str(abs_error)\n'"
python/tests/test_pipelines.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n#import pytest\n\nimport numpy as np\nimport pandas as pd\nfrom six.moves import xrange\n\nfrom wtte.transforms import padded_to_df\nfrom wtte.pipelines import data_pipeline\n\nfrom wtte.data_generators import generate_random_df\n\n\ndef run_test(\n        # \'1\' on the end dirty way to not pollute testing-namespace\n        id_col1,\n        abs_time_col1,\n        discrete_time1,\n        pad_between_steps1):\n    np.random.seed(1)\n\n    # Should fail randomly if unique_times = False since it reduces those\n    # times.\n    df = generate_random_df(n_seqs=5, max_seq_length=10, unique_times=True)\n\n    # rename the abs_time_col to something new to spot assumptions.\n    df.rename(columns={""dt"": abs_time_col1,\n                       \'id\': id_col1}, inplace=True)\n\n    column_names1 = [\'event\', \'int_column\', \'double_column\']\n    padded, padded_t, seq_ids, df_collapsed = \\\n        data_pipeline(df,\n                      id_col=id_col1,\n                      abs_time_col=abs_time_col1,\n                      column_names=column_names1,\n                      discrete_time=discrete_time1,\n                      pad_between_steps=pad_between_steps1,\n                      infer_seq_endtime=False,\n                      time_sec_interval=1,\n                      timestep_aggregation_dict=None,\n                      drop_last_timestep=False\n                      )\n\n    if pad_between_steps1:\n        df_new = padded_to_df(padded, column_names1, [\n            int, int, float], ids=seq_ids, id_col=id_col1, t_col=\'t_elapsed\')\n        df = df[[id_col1, \'t_elapsed\']+column_names1].reset_index(drop=True)\n        pd.util.testing.assert_frame_equal(df, df_new)\n    else:\n        df_new = padded_to_df(padded, column_names1, [\n            int, int, float], ids=seq_ids, id_col=id_col1, t_col=\'t_ix\')\n        df = df[[id_col1, \'t_ix\']+column_names1].reset_index(drop=True)\n\n        pd.util.testing.assert_frame_equal(df, df_new)\n\n\nclass TestPipeline():\n\n    def test_discrete_padded_pipeline(self):\n        run_test(\n            # \'1\' on the end dirty way to not pollute testing-namespace\n            id_col1=\'idnewname\',\n            abs_time_col1=\'time_int\',\n            discrete_time1=True,\n            pad_between_steps1=True)\n\n    def test_discrete_unpadded_pipeline(self):\n        run_test(\n            # \'1\' on the end dirty way to not pollute testing-namespace\n            id_col1=\'idnewname\',\n            abs_time_col1=\'time_int\',\n            discrete_time1=True,\n            pad_between_steps1=False)\n\n    def test_continuous_pipeline(self):\n        run_test(\n            # \'1\' on the end dirty way to not pollute testing-namespace\n            id_col1=\'idnewname\',\n            abs_time_col1=\'time_int\',\n            discrete_time1=True,\n            pad_between_steps1=False)\n\n    # def test_discrete_time_continous():  # TODO\n    # def test_continuous_time_discrete(): # TODO\n    # TODO test with flag infer enddtime\n    # TODO test with tte and censoring etc.\n\n'"
python/tests/test_tensorflow.py,10,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport pytest\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom wtte.objectives.tensorflow import loglik_continuous, loglik_discrete\nfrom wtte.data_generators import generate_weibull\n\n# SANITY CHECK: Use pure Weibull data censored at C(ensoring point).\n# Should converge to the generating A(alpha) and B(eta) for each timestep\n\nn_samples = 1000\nn_features = 1\nreal_a = 3.\nreal_b = 2.\ncensoring_point = real_a * 2\n\n\ndef tf_loglik_runner(loglik_fun, discrete_time):\n    sess = tf.Session()\n    np.random.seed(1)\n    tf.set_random_seed(1)\n\n    y_ = tf.placeholder(tf.float32, shape=(None, 1))\n    u_ = tf.placeholder(tf.float32, shape=(None, 1))\n\n    a = tf.exp(tf.Variable(tf.ones([1]), name='a_weight'))\n    b = tf.exp(tf.Variable(tf.ones([1]), name='b_weight'))\n\n    # testing part:\n    loglik = loglik_fun(a, b, y_, u_)\n\n    loss = -tf.reduce_mean(loglik)\n\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.005)\n\n    train_step = optimizer.minimize(loss)\n\n    # Launch the graph in a session.\n    np.random.seed(1)\n\n    # Initializes global variables in the graph.\n    sess.run(tf.global_variables_initializer())\n\n    tte_actual, tte_censored, u_train = generate_weibull(\n        A=real_a,\n        B=real_b,\n        C=censoring_point,  # <np.inf -> impose censoring\n        shape=[n_samples, n_features],\n        discrete_time=discrete_time)\n\n    # Fit\n    for step in range(1000):\n        loss_val, _, a_val, b_val = sess.run([loss, train_step, a, b], feed_dict={\n                                             y_: tte_censored, u_: u_train})\n\n        if step % 100 == 0:\n            print(step, loss_val, a_val, b_val)\n\n    print(np.abs(real_a - a_val), np.abs(real_b - b_val))\n    assert np.abs(real_a - a_val) < 0.05, 'alpha not converged'\n    assert np.abs(real_b - b_val) < 0.05, 'beta not converged'\n    sess.close()\n    tf.reset_default_graph()\n\n\ndef test_loglik_continuous():\n    tf_loglik_runner(loglik_continuous, discrete_time=False)\n\n\ndef test_loglik_discrete():\n    tf_loglik_runner(loglik_discrete, discrete_time=True)\n"""
python/tests/test_transforms.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport pytest\n\nimport numpy as np\nimport pandas as pd\nfrom six.moves import xrange\n\nfrom wtte.data_generators import generate_random_df\n\nfrom wtte.transforms import df_to_padded\nfrom wtte.transforms import padded_to_df\nfrom wtte.transforms import shift_discrete_padded_features\nfrom wtte.transforms import left_pad_to_right_pad\nfrom wtte.transforms import right_pad_to_left_pad\nfrom wtte.transforms import normalize_padded\n\n\ndef df_to_padded_padded_to_df_runner(t_col):\n    n_seqs = 5\n    max_seq_length = 10\n    ids = xrange(n_seqs)\n    cols_to_expand = [\'event\', \'int_column\', \'double_column\']\n    np.random.seed(1)\n\n    df = generate_random_df(n_seqs, max_seq_length)\n    df = df.reset_index(drop=True)\n\n    # Column names to transform to tensor\n    dtypes = df[cols_to_expand].dtypes.values\n    padded = df_to_padded(df, cols_to_expand, \'id\', t_col)\n\n    df_new = padded_to_df(padded, cols_to_expand,\n                          dtypes, ids, \'id\', t_col)\n    # Pandas is awful. Index changes when slicing\n    df = df[[\'id\', t_col] + cols_to_expand].reset_index(drop=True)\n    pd.util.testing.assert_frame_equal(df, df_new)\n\n\nclass TestDfToPaddedPaddedToDf:\n    """"""tests df_to_padded and padded_to_df\n    generates a dataframe, transforms it to tensor format then back\n    to the same df.\n    """"""\n\n    def test_record_based(self):\n        """"""here only\n        """"""\n        df_to_padded_padded_to_df_runner(t_col=\'t_ix\')\n\n    def test_padded_between(self):\n        """"""Tests df_to_padded, padded_to_df\n        """"""\n        df_to_padded_padded_to_df_runner(t_col=\'t_elapsed\')\n\n\ndef test_shift_discrete_padded_features():\n    """"""test for `discrete_padded_features`.\n        TODO buggy unit. Due to change.\n    """"""\n    x = np.array([[[1], [2], [3]]])\n    assert x.shape == (1, 3, 1)\n\n    x_shifted = shift_discrete_padded_features(x, fill=0)\n\n    np.testing.assert_array_equal(x_shifted, np.array([[[0], [1], [2]]]))\n\n\ndef test_align_padded():\n    """"""test the function for switching between left-right padd.\n    """"""\n    # For simplicity, create a realistic padded tensor\n    np.random.seed(1)\n    n_seqs = 10\n    max_seq_length = 10\n    df = generate_random_df(n_seqs, max_seq_length)\n    cols_to_expand = [\'event\', \'int_column\', \'double_column\']\n    padded = df_to_padded(df, cols_to_expand, \'id\', \'t_elapsed\')\n\n    np.testing.assert_array_equal(\n        padded, left_pad_to_right_pad(right_pad_to_left_pad(padded)))\n    padded = np.copy(np.squeeze(padded))\n    np.testing.assert_array_equal(\n        padded, left_pad_to_right_pad(right_pad_to_left_pad(padded)))\n\n\ndef test_normalize_padded():\n    """"""\n        Assume that a random normal should stay approx unchanged \n        after transformation.\n    """"""\n    # No NaNs, zeros, constant cols etc.\n    # ONLY NONZERO FALSE\n    # [batch,time,feature]\n    np.random.seed(1)\n    padded = np.random.normal(0, 1, [100000, 10, 2])\n    padded_new1, means, stds = normalize_padded(padded, only_nonzero=False)\n    padded_new2, _, _ = normalize_padded(padded, means, stds, only_nonzero=False)\n    np.testing.assert_almost_equal(padded, padded_new1, decimal=2)\n    np.testing.assert_almost_equal(padded, padded_new2, decimal=2)\n\n    # [batch,time]\n    padded_new1, means, stds = normalize_padded(padded, only_nonzero=False)\n    padded_new2, _, _ = normalize_padded(padded, means, stds, only_nonzero=False)\n    np.testing.assert_almost_equal(padded, padded_new1, decimal=2)\n    np.testing.assert_almost_equal(padded, padded_new2, decimal=2)\n\n    # ONLY NONZERO TRUE\n    # [batch,time,feature]\n    padded_new1, means, stds = normalize_padded(padded, only_nonzero=True)\n    padded_new2, _, _ = normalize_padded(padded, means, stds, only_nonzero=True)\n    np.testing.assert_almost_equal(padded, padded_new1, decimal=2)\n    np.testing.assert_almost_equal(padded, padded_new2, decimal=2)\n\n    # [batch,time]\n    padded_new1, means, stds = normalize_padded(padded, only_nonzero=True)\n    padded_new2, _, _ = normalize_padded(padded, means, stds, only_nonzero=True)\n    np.testing.assert_almost_equal(padded, padded_new1, decimal=2)\n    np.testing.assert_almost_equal(padded, padded_new2, decimal=2)\n\n    # [batch,time,feature] with nans and zeros both settings\n    np.random.seed(1)\n    padded = np.random.normal(0, 1, [100000, 10, 3])\n    # First feature constant 0, should be centered at 0 still.\n    padded[:, :, 0] = 0\n    # Last 5000 timesteps of first 5 batch masked\n    padded[5000:, 5:] = np.nan\n    padded_new1, means, stds = normalize_padded(padded, only_nonzero=False)\n    padded_new2, _, _ = normalize_padded(padded, means, stds, only_nonzero=False)\n    np.testing.assert_almost_equal(padded, padded_new1, decimal=2)\n    np.testing.assert_almost_equal(padded, padded_new2, decimal=2)\n\n    padded_new1, means, stds = normalize_padded(padded, only_nonzero=True)\n    padded_new2, _, _ = normalize_padded(padded, means, stds, only_nonzero=True)\n    np.testing.assert_almost_equal(padded, padded_new1, decimal=2)\n    np.testing.assert_almost_equal(padded, padded_new2, decimal=2)\n\n    # If some timesteps for some batch is zero, this mask should be respected and\n    # kept unchanged if only_nonzero\n    padded[:100, :5] = 0\n    padded_new1, means, stds = normalize_padded(padded, only_nonzero=True)\n    padded_new2, _, _ = normalize_padded(padded, means, stds, only_nonzero=True)\n    np.testing.assert_almost_equal(padded, padded_new1, decimal=2)\n    np.testing.assert_almost_equal(padded, padded_new2, decimal=2)\n\n    # Binary data\n    # With only_nonzero it should leave binary data unchanged\n    padded = (np.random.normal(0, 1, [100000, 10, 2]) > 2).astype(float)\n    padded[5000:, 5:] = np.nan\n\n    padded_new1, means, stds = normalize_padded(padded, only_nonzero=True)\n    padded_new2, _, _ = normalize_padded(padded, means, stds, only_nonzero=True)\n\n    np.testing.assert_almost_equal(padded_new1, padded_new2, decimal=2)\n    np.testing.assert_almost_equal(padded, padded_new1, decimal=2)\n\n    # Should map scaled binary to [0,1]\n    padded_new1, means, stds = normalize_padded(padded * 10, only_nonzero=True)\n    padded_new2, _, _ = normalize_padded(padded * 10, means, stds, only_nonzero=True)\n\n    np.testing.assert_almost_equal(padded_new1, padded_new2, decimal=2)\n    np.testing.assert_almost_equal(padded, padded_new1, decimal=2)\n\n    # If features are numerical (but zero sometimes)\n    # we should have same results and mask should be respected.\n    padded = (np.random.normal(0, 1, [100000, 10, 1])\n              > 0) * np.random.normal(0, 1, [100000, 10, 1])\n    padded[np.abs(padded) < 1e-7] = 0\n\n    padded_new1, means, stds = normalize_padded(padded, only_nonzero=True)\n    padded_new2, _, _ = normalize_padded(padded, means, stds, only_nonzero=True)\n\n    np.testing.assert_almost_equal(padded_new1, padded_new2, decimal=2)\n    np.testing.assert_almost_equal(padded, padded_new1, decimal=2)\n'"
python/tests/test_tte.py,0,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nimport pytest\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nfrom wtte.transforms import padded_events_to_tte, padded_events_to_not_censored\n\n@pytest.fixture\ndef padded_time_continuous():\n    return np.array(\n        [\n            [0, 1, 2, 3, 4],  # seq 1\n            [0, 1, 2, 3, 4],  # seq 2..\n            [0, 1, 2, 3, 4],\n            [0, 1, 2, 3, 4],\n            [0, 1, 2, 3, 4]\n        ])\n\n@pytest.fixture\ndef padded_time_discrete():\n    return np.array(\n        [\n            [0, 1, 2, 3],  # seq 1\n            [0, 1, 2, 3],  # seq 2..\n            [0, 1, 2, 3],\n            [0, 1, 2, 3],\n            [0, 1, 2, 3]\n        ])\n\n@pytest.fixture\ndef events_c():\n    return np.array(\n        # (first time is a nullity)\n        # ""time when something did or didn\'t happen""\n        # Has no future importance, possibly feature importance.\n        [\n            [1337, 0, 0, 0, 0],  # seq 1\n            [1337, 0, 0, 0, 1],  # seq 2..\n            [1337, 1, 1, 1, 0],\n            [1337, 1, 1, 0, 0],\n            [1337, 0, 1, 0, 0]\n        ])\n\n@pytest.fixture\ndef expected_is_censored_c():\n    return np.array(\n        # expected_is_censored_c[:,:-1] = expected_is_censored_d\n        # Last step is always censored\n        # (observed future interval length = 0)\n        [\n            [1, 1, 1, 1, 1],\n            [0, 0, 0, 0, 1],\n            [0, 0, 0, 1, 1],\n            [0, 0, 1, 1, 1],\n            [0, 0, 1, 1, 1]\n        ])\n\n# expected_tte_c[:,:-1] = expected_tte_d+1-expected_is_censored_d\n@pytest.fixture\ndef expected_tte_c():\n    return np.array(\n        [\n            [4, 3, 2, 1, 0],\n            [4, 3, 2, 1, 0],\n            [1, 1, 1, 1, 0],\n            [1, 1, 2, 1, 0],\n            [2, 1, 2, 1, 0]\n        ])\n\n@pytest.fixture\ndef events_d():\n    return np.array(\n        # Discrete events comes from continuous reality.\n        [\n            [0, 0, 0, 0],  # seq 1\n            [0, 0, 0, 1],  # seq 2..\n            [1, 1, 1, 0],\n            [1, 1, 0, 0],\n            [0, 1, 0, 0]\n        ])\n\n@pytest.fixture\ndef expected_is_censored_d():\n    return np.array(\n        [\n            [1, 1, 1, 1],\n            [0, 0, 0, 0],\n            [0, 0, 0, 1],\n            [0, 0, 1, 1],\n            [0, 0, 1, 1]\n        ])\n\n@pytest.fixture\ndef expected_tte_d():\n    return np.array(\n        [\n            [4, 3, 2, 1],\n            [3, 2, 1, 0],\n            [0, 0, 0, 1],\n            [0, 0, 2, 1],\n            [1, 0, 2, 1]\n        ])\n\n\nclass TestCensoringFuns:\n\n    def test_censoring_funs_no_time_discrete(self, expected_tte_d,\n                                             expected_is_censored_d, events_d):\n        times_to_event = padded_events_to_tte(events_d, discrete_time=True)\n        not_censored = padded_events_to_not_censored(events_d,\n                                                     discrete_time=True)\n \n        assert (expected_tte_d == times_to_event).all(), \'  time_to_event failed\'\n        assert (expected_is_censored_d != not_censored).all(), \'not_censored failed\'\n\n    def test_censoring_funs_no_time_continuous(\n                self, expected_tte_c, expected_is_censored_c, events_c):\n        times_to_event = padded_events_to_tte(events_c, discrete_time=False)\n        not_censored = padded_events_to_not_censored(events_c,\n                                                     discrete_time=False)\n \n        assert (expected_tte_c == times_to_event).all(), \'  time_to_event failed\'\n        assert (expected_is_censored_c != not_censored).all(), \'not_censored failed\'\n\n    def test_censoring_funs_with_time_discrete(\n                self, expected_tte_d, expected_is_censored_d, events_d,\n                padded_time_discrete):\n        times_to_event = padded_events_to_tte(events_d, discrete_time=True,\n                                              t_elapsed=padded_time_discrete)\n        not_censored = padded_events_to_not_censored(events_d,\n                                                     discrete_time=True)\n\n        assert (expected_tte_d == times_to_event).all(), \'  time_to_event failed\'\n        assert (expected_is_censored_d != not_censored).all(), \'not_censored failed\'\n\n    def test_censoring_funs_with_time_continuous(\n                self, expected_tte_c, expected_is_censored_c, events_c,\n                padded_time_continuous):\n        times_to_event = padded_events_to_tte(events_c, discrete_time=False,\n                                              t_elapsed=padded_time_continuous)\n        not_censored = padded_events_to_not_censored(events_c, discrete_time=False)\n\n        assert (expected_tte_c == times_to_event).all(), \'  time_to_event failed\'\n        assert (expected_is_censored_c != not_censored).all(), \'not_censored failed\'\n\n'"
python/wtte/__init__.py,0,b''
python/wtte/data_generators.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport pandas as pd\nfrom six.moves import xrange\n\n\ndef generate_random_df(n_seqs=5,\n                       max_seq_length=10,\n                       unique_times=True,\n                       starttimes_min=0,\n                       starttimes_max=0):\n    """"""Generates random dataframe for testing.\n\n    For every sequence:\n\n    1. generate a random seq_length from [1,`max_seq_length`]\n    2. generate the number of observations in the sequence from [1,seq_length]\n    3. randomly pick observation elapsed times from [1,`seq_length`]\n    4. randomly pick a starttime [0,`starttimes_max`]\n    5. Generate random data in the columns at these timesteps\n\n    This means that the only thing we know about a sequence is that it\'s at maximum `max_seq_length`\n\n    :param df: pandas dataframe with columns\n\n      * `id`: integer\n\n      * `t`: integer\n\n      * `dt`: integer mimmicking a global event time\n\n      * `t_ix`: integer contiguous user time count per id 0,1,2,..\n\n      * `t_elapsed`: integer the time from starttime per id ex 0,1,10,..\n\n      * `event`: 0 or 1\n\n      * `int_column`: random data\n\n      * `double_column`: dandom data\n\n    :param int unique_times: whether there id,elapsed_time has only one obs. Default true\n    :param int starttimes_min: integer to generate `dt` the absolute time\n    :param int starttimes_max: integer to generate `dt` the absolute time\n\n    :return df: A randomly generated dataframe.\n    """"""\n\n    seq_lengths = np.random.randint(max_seq_length, size=n_seqs) + 1\n    id_list = []\n    t_list = []\n    dt_list = []\n\n    if starttimes_min < starttimes_max:\n        starttimes = np.sort(np.random.randint(\n            low=starttimes_min, high=starttimes_max, size=n_seqs))\n    else:\n        starttimes = np.zeros(n_seqs)\n\n    for s in xrange(n_seqs):\n        # Each sequence consists of n_obs in the range 0-seq_lengths[s]\n        n_obs = np.sort(np.random.choice(\n            seq_lengths[s], 1, replace=False)) + 1\n\n        # Each obs occurred at random times\n        t_elapsed = np.sort(np.random.choice(\n            seq_lengths[s], n_obs, replace=not unique_times))\n\n        # there\'s always an obs at the assigned first and last timestep for\n        # this seq\n        if seq_lengths[s] - 1 not in t_elapsed:\n            t_elapsed = np.append(t_elapsed, seq_lengths[s] - 1)\n        if 0 not in t_elapsed:\n            t_elapsed = np.append(t_elapsed, 0)\n\n        t_elapsed = np.sort(t_elapsed)\n\n        id_list.append(np.repeat(s, repeats=len(t_elapsed)))\n        dt_list.append(starttimes[s] + t_elapsed)\n        t_list.append(t_elapsed)\n\n    # unlist to one array\n    id_column = [item for sublist in id_list for item in sublist]\n    dt_column = [item for sublist in dt_list for item in sublist]\n    t_column = [item for sublist in t_list for item in sublist]\n    del id_list, dt_list, t_list\n\n    # do not assume row indicates event!\n    event_column = np.random.randint(2, size=len(t_column))\n    int_column = np.random.randint(low=-5, high=5, size=len(t_column)).astype(int)\n    double_column = np.random.uniform(high=1, low=0, size=len(t_column))\n\n    df = pd.DataFrame({\'id\': id_column,\n                       \'dt\': dt_column,\n                       \'t_elapsed\': t_column,\n                       \'event\': event_column,\n                       \'int_column\': int_column,\n                       \'double_column\': double_column\n                       })\n\n    df[\'t_ix\'] = df.groupby([\'id\'])[\'t_elapsed\'].rank(\n        method=\'dense\').astype(int) - 1\n    df = df[[\'id\', \'dt\', \'t_ix\', \'t_elapsed\',\n             \'event\', \'int_column\', \'double_column\']]\n    df = df.reset_index(drop=True)\n\n    return df\n\n\ndef generate_weibull(A, B, C, shape, discrete_time):\n    """"""Generate Weibull random variables.\n\n    Inputs can be scalar or broadcastable to `shape`.\n\n    :param A: Generating alpha\n    :param B: Generating beta\n    :param C: Censoring time\n\n    :return: list of `[W, Y, U]`\n\n      * `W`: Actual TTE\n\n      * `Y`: Censored TTE\n\n      * `U`: non-censoring indicators\n\n    :rtype: ndarray\n    """"""\n    W = np.sort(A * np.power(-np.log(np.random.uniform(0, 1, shape)), 1 / B))\n\n    if discrete_time:\n        C = np.floor(C)\n        W = np.floor(W)\n\n    U = np.less_equal(W, C) * 1.\n    Y = np.minimum(W, C)\n    return W, Y, U\n'"
python/wtte/pipelines.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport pandas as pd\nfrom six.moves import xrange\n\nfrom .transforms import df_join_in_endtime\nfrom .transforms import df_to_padded\n\n\ndef data_pipeline(\n        df,\n        id_col=\'id\',\n        abs_time_col=\'time_int\',\n        column_names=[""event""],\n        constant_cols=[],\n        discrete_time=True,\n        pad_between_steps=True,\n        infer_seq_endtime=True,\n        time_sec_interval=60 * 60 * 24,\n        timestep_aggregation_dict=None,\n        drop_last_timestep=True):\n    """"""Preprocess dataframe and return it in padded tensor format.\n\n        This function is due to change alot.\n\n        1. Lowers the resolution of the (int) `abs_time_col` ex from epoch sec to epoch day by aggregating\\\n          each column using `timestep_aggregation_dict`.\n\n        2. Padds out with zeros between timesteps and fills with value of `constant_cols`.\n\n        3. Infers or adds/fills an endtime.\n\n        This outputs tensor as is and leave it to downstream to define events, disalign targets\n        and features (see `shift_discrete_padded_features`) and from that\n        censoring-indicator and tte.\n    """"""\n    if timestep_aggregation_dict is None:\n        timestep_aggregation_dict = dict.fromkeys(column_names, ""sum"")\n\n    # Asserts that column_names order is respected.\n    for key in column_names:\n        timestep_aggregation_dict[key] = timestep_aggregation_dict.pop(key)\n\n    if discrete_time:\n        # Lower resolution on unix-timestamp ex. to day\n        df[abs_time_col] = time_sec_interval * \\\n            (df[abs_time_col] // (time_sec_interval))\n        # Last timestep may be incomplete/not fully measured so drop it.\n        if drop_last_timestep:\n            df = df.loc[df[abs_time_col] < df[abs_time_col].max()]\n\n    # COLLAPSE STRATEGY : SUM by default\n    # Aggregate over the new datetime interval to get id,time_int = unique key\n    # value pair\n\n    df = df.groupby([id_col, abs_time_col], as_index=False).\\\n        agg(timestep_aggregation_dict)\n\n    if infer_seq_endtime:\n        # Assuming each sequence has its own start and is not terminated by last event:\n        # Add last time that we knew the sequence was \'alive\'.\n        df = df_join_in_endtime(df,\n                                constant_per_id_cols=[\n                                    id_col] + constant_cols,\n                                abs_time_col=abs_time_col)\n\n        # this will cast every column with NaN to float\n        df = df.fillna(0, inplace=False)\n\n    df = df.sort_values([id_col, abs_time_col],\n                        inplace=False).reset_index(drop=True)\n\n    # Add ""elapsed time"" t_elapsed = 0,3,99,179,.. for each user.\n    df[\'t_elapsed\'] = df.groupby([id_col], group_keys=False).apply(\n        lambda g: g[abs_time_col] - g[abs_time_col].min())\n\n    if discrete_time:\n        # Let each integer step stand for the time-resolution, eg. days:\n        df[\'t_elapsed\'] = df[\'t_elapsed\'].astype(int) // time_sec_interval\n    #########\n\n    if pad_between_steps:\n        # if we set \'t_elapsed\' as t_col we\'ll pad between observed steps in\n        # df_to_padded\n        t_col = \'t_elapsed\'\n    else:\n        # Add t_ix = 0,1,2,3,.. and set as primary user-time indicator.\n        # if we set \'t_ix\' as t_col no padding between steps is done in\n        # df_to_padded\n        df[\'t_ix\'] = df.groupby([id_col])[\'t_elapsed\'].\\\n            rank(method=\'dense\').astype(int) - 1\n        t_col = \'t_ix\'\n\n    # Here go over to to-tensor operations. Could be split into two funs\n\n    # Everything to tensor.\n    if pad_between_steps:\n        t_col = \'t_elapsed\'\n        # By default expands to 0,1,2,... which is fine since this is true for\n        # padded values.\n        padded_t = None\n        if not discrete_time:\n            raise ValueError(\n                ""pad_between_steps = True, discrete_time = False \\\n                \\n Can\'t padd between on a continuous scale"")\n    else:\n        # Add t_ix = 0,1,2,3,.. and set as primary user-time indicator.\n        # if we set \'t_ix\' as t_col no padding between steps is done in\n        # df_to_padded\n        t_col = \'t_ix\'\n        padded_t = df_to_padded(df, id_col=id_col, column_names=[\n            \'t_elapsed\'], t_col=t_col).squeeze()\n        if discrete_time:\n            padded_t = padded_t.astype(int)\n\n    padded = df_to_padded(\n        df, id_col=id_col, column_names=column_names, t_col=t_col)\n\n    # If some columns where specified as constant\n    # TODO carry forward in time instead.\n    if constant_cols is not None:\n        if len(constant_cols) > 0:\n            constant_cols_ix = []\n            for i in xrange(len(column_names)):\n                if column_names[i] in constant_cols:\n                    constant_cols_ix.append(i)\n\n            # Store away nan-mask to reapply it later.\n            nan_mask = np.expand_dims(padded[:, :, 0], -1) * 0\n\n            # Map data for first eventstep to every step\n            padded[:, :, constant_cols_ix] = np.expand_dims(\n                padded[:, 0, constant_cols_ix], 1)\n            # Reapply nan-mask\n            padded[:, :, constant_cols_ix] = padded[\n                :, :, constant_cols_ix] + nan_mask\n\n    seq_ids = df[id_col].unique()\n    return padded, padded_t, seq_ids, df\n'"
python/wtte/transforms.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport pandas as pd\nfrom six.moves import xrange\n\nfrom .tte_util import get_is_not_censored\nfrom .tte_util import get_tte\n\n\ndef get_padded_seq_lengths(padded):\n    """"""Returns the number of (seq_len) non-nan elements per sequence.\n\n    :param padded: 2d or 3d tensor with dim 2 the time dimension\n    """"""\n    if len(padded.shape) == 2:\n        # (n_seqs,n_timesteps)\n        seq_lengths = np.count_nonzero(~np.isnan(padded), axis=1)\n    elif len(padded.shape) == 3:\n        # (n_seqs,n_timesteps,n_features,..)\n        seq_lengths = np.count_nonzero(~np.isnan(padded[:, :, 0]), axis=1)\n    else:\n        print(\'not yet implemented\')\n        # TODO\n\n    return seq_lengths\n\n\ndef df_to_array(df, column_names, nanpad_right=True, return_lists=False,\n                id_col=\'id\', t_col=\'t\'):\n    """"""Converts flat pandas df with cols `id,t,col1,col2,..` to array indexed `[id,t,col]`. \n\n    :param df: dataframe with columns:\n\n      * `id`: Any type. A unique key for the sequence.\n\n      * `t`: integer. If `t` is a non-contiguous int vec per id then steps in\n        between t\'s are padded with zeros.\n\n      * `columns` in `column_names` (String list)\n\n    :type df: Pandas dataframe\n\n    :param Boolean nanpad_right: If `True`, sequences are `np.nan`-padded to `max_seq_len`\n    :param return_lists: Put every tensor in its own subarray\n    :param id_col: string column name for `id`\n    :param t_col: string column name for `t`\n    :return padded: With seqlen the max value of `t` per id\n\n      * if nanpad_right & !return_lists:\n        a numpy float array of dimension `[n_seqs,max_seqlen,n_features]`\n\n      * if nanpad_right & return_lists:\n        n_seqs numpy float sub-arrays of dimension `[max_seqlen,n_features]`\n\n      * if !nanpad_right & return_lists:\n        n_seqs numpy float sub-arrays of dimension `[seqlen,n_features]`\n    """"""\n\n    # Do not sort. Create a view.\n    grouped = df.groupby(id_col, sort=False)\n\n    unique_ids = list(grouped.groups.keys())\n\n    n_seqs = grouped.ngroups\n    n_features = len(column_names)\n    seq_lengths = df[[id_col, t_col]].groupby(\n        id_col).aggregate(\'max\')[t_col].values + 1\n\n    # We can\'t assume to fit varying length seqs. in flat array without\n    # padding.\n    assert nanpad_right or len(\n        set(seq_lengths)) == 1 or return_lists, \'Wont fit in flat array\'\n\n    max_seq_len = seq_lengths.max()\n\n    # Initialize the array to be filled\n    if return_lists:\n        if nanpad_right:\n            padded = np.split(np.zeros([n_seqs * max_seq_len, n_features]),\n                              np.cumsum(np.repeat(max_seq_len, n_seqs)))\n        else:\n            padded = np.split(np.zeros([sum(seq_lengths), n_features]),\n                              np.cumsum(seq_lengths))\n    else:\n        padded = np.zeros([n_seqs, max_seq_len, n_features])\n\n    # Fill it\n    for s in xrange(n_seqs):\n        # df_user is a view\n        df_group = grouped.get_group(unique_ids[s])\n\n        padded[s][df_group[t_col].values, :] = df_group[column_names].values\n        if nanpad_right and seq_lengths[s] < max_seq_len:\n            padded[s][seq_lengths[s]:, :].fill(np.nan)\n\n    return padded\n\n\ndef df_to_padded(df, column_names, id_col=\'id\', t_col=\'t\'):\n    """"""Pads pandas df to a numpy array of shape `[n_seqs,max_seqlen,n_features]`.\n        see `df_to_array` for details\n    """"""\n    return df_to_array(df, column_names, nanpad_right=True,\n                       return_lists=False, id_col=id_col, t_col=t_col)\n\n\ndef df_to_subarrays(df, column_names, id_col=\'id\', t_col=\'t\'):\n    """"""Pads pandas df to subarrays of shape `[n_seqs][seqlen[s],n_features]`.\n        see `df_to_array` for details\n    """"""\n    return df_to_array(df, column_names, nanpad_right=False,\n                       return_lists=True, id_col=id_col, t_col=t_col)\n\n\ndef padded_to_df(padded, column_names, dtypes, ids=None, id_col=\'id\', t_col=\'t\'):\n    """"""Takes padded numpy array and converts nonzero entries to pandas dataframe row.\n\n    Inverse to df_to_padded.\n\n    :param Array padded: a numpy float array of dimension `[n_seqs,max_seqlen,n_features]`.\n    :param list column_names: other columns to expand from df\n    :param list dtypes:  the type to cast the float-entries to.\n    :type dtypes: String list\n    :param ids: (optional) the ids to attach to each sequence\n    :param id_col: Column where `id` is located. Default value is `id`.\n    :param t_col: Column where `t` is located. Default value is `t`.\n\n    :return df: Dataframe with Columns\n\n      *  `id` (Integer) or the value of `ids`\n\n      *  `t` (Integer).\n\n      A row in df is the t\'th event for a `id` and has columns from `column_names`\n    """"""\n\n    def get_is_nonempty_mask(padded):\n        """""" (internal function) Non-empty masks\n\n        :return is_nonempty: True if `[i,j,:]` has non-zero non-nan - entries or\n            j is the start or endpoint of a sequence i\n        :type is_nonempty: Boolean Array\n        """"""\n\n        # If any nonzero element then nonempty:\n        is_nonempty = (padded != 0).sum(2) != 0\n\n        # first and last step in each seq is not empty\n        # (has info about from and to)\n        is_nonempty[:, 0] = True\n\n        seq_lengths = get_padded_seq_lengths(padded)\n\n        is_nonempty[xrange(n_seqs), seq_lengths - 1] = True\n\n        # nan-mask is always empty:\n        is_nonempty[np.isnan(padded.sum(2))] = False\n\n        return is_nonempty\n\n    def get_basic_df(padded, is_nonempty, ids):\n        """""" (internal function) Get basic dataframe\n\n        :return df: Dataframe with columns\n          * `id`: a column of id\n          * `t`: a column of (user/sequence) timestep\n        """"""\n        n_nonempty_steps = is_nonempty.sum(1)\n        df = pd.DataFrame(index=xrange(sum(n_nonempty_steps)))\n\n        id_vec = []\n        for s in xrange(n_seqs):\n            for reps in xrange(n_nonempty_steps[s]):\n                id_vec.append(ids[s])\n\n        df[id_col] = id_vec\n        df[t_col] = ((np.isnan(padded).sum(2) == 0).cumsum(1) - 1)[is_nonempty]\n\n        return df\n\n    if len(padded.shape) == 2:\n        padded = np.expand_dims(padded, -1)\n\n    n_seqs, max_seq_length, n_features = padded.shape\n\n    if ids is None:\n        ids = xrange(n_seqs)\n\n    is_nonempty = get_is_nonempty_mask(padded)\n\n    df_new = get_basic_df(padded, is_nonempty, ids)\n\n    for f in xrange(n_features):\n        df_new = df_new.assign(tmp=padded[:, :, f][\n                               is_nonempty].astype(dtypes[f]))\n        df_new.rename(columns={\'tmp\': column_names[f]}, inplace=True)\n\n    return df_new\n\n\ndef padded_events_to_tte(events, discrete_time, t_elapsed=None):\n    """""" computes (right censored) time to event from padded binary events.\n\n    For details see `tte_util.get_tte`\n\n    :param Array events: Events array.\n    :param Boolean discrete_time: `True` when applying discrete time scheme.\n    :param Array t_elapsed: Elapsed time. Default value is `None`.\n    :return Array time_to_events: Time-to-event tensor.\n    """"""\n    seq_lengths = get_padded_seq_lengths(events)\n    n_seqs = len(events)\n\n    times_to_event = np.zeros_like(events)\n    times_to_event[:] = np.nan\n\n    t_seq = None\n    for s in xrange(n_seqs):\n        n = seq_lengths[s]\n        if n > 0:\n            event_seq = events[s, :n]\n            if t_elapsed is not None:\n                t_seq = t_elapsed[s, :n]\n\n            times_to_event[s, :n] = get_tte(is_event=event_seq,\n                                            discrete_time=discrete_time,\n                                            t_elapsed=t_seq)\n\n    if np.isnan(times_to_event).any():\n        times_to_event[np.isnan(events)] = np.nan\n    return times_to_event\n\n\ndef padded_events_to_not_censored_vectorized(events):\n    """""" (Legacy)\n        calculates (non) right-censoring indicators from padded binary events\n    """"""\n    not_censored = np.zeros_like(events)\n    not_censored[~np.isnan(events)] = events[~np.isnan(events)]\n    # 0100 -> 0010 -> 0011 -> 1100\n    not_censored = not_censored[:, ::-1].cumsum(1)[:, ::-1]\n\n    not_censored = np.array(not_censored >= 1).astype(float)\n\n    if np.isnan(events).any():\n        not_censored[np.isnan(events)] = np.nan\n\n    return not_censored\n\n\ndef padded_events_to_not_censored(events, discrete_time):\n    seq_lengths = get_padded_seq_lengths(events)\n    n_seqs = events.shape[0]\n    is_not_censored = events.copy()\n\n    for i in xrange(n_seqs):\n        if seq_lengths[i] > 0:\n            is_not_censored[i][:seq_lengths[i]] = get_is_not_censored(\n                events[i][:seq_lengths[i]], discrete_time)\n    return is_not_censored\n\n# MISC / Data munging\n\n# def df_to_padded_memcost(df, id_col=\'id\', t_col=\'t\'):\n#     """"""\n#         Calculates memory cost of padded using the alternative routes.\n#         # number of arrays = features+tte+u = n_features+2\n#         # To list? Pad betweeen?\n#         # To array ->(pad after)\n#     """"""\n\n#     print(\'Not yet implemented\')\n#     return None\n\n\ndef _align_padded(padded, align_right):\n    """""" (Internal function) Aligns nan-padded temporal arrays to the right (align_right=True) or left.\n\n    :param Array padded: padded array\n    :param align_right: Determines padding orientation (right or left). If `True`, pads to right direction.\n    """"""\n    padded = padded.copy()\n\n    seq_lengths = get_padded_seq_lengths(padded)\n    if len(padded.shape) == 2:\n        # (n_seqs,n_timesteps)\n        is_flat = True\n        padded = np.expand_dims(padded, -1)\n    elif len(padded.shape) == 3:\n        # (n_seqs,n_timesteps,n_features)\n        is_flat = False\n    else:\n        # (n_seqs,n_timesteps,...,n_features)\n        print(\'not yet implemented\')\n        # TODO\n\n    n_seqs = padded.shape[0]\n    n_timesteps = padded.shape[1]\n\n    if align_right:\n        for i in xrange(n_seqs):\n            n = seq_lengths[i]\n            if n > 0:\n                padded[i, (n_timesteps - n):] = padded[i, :n]\n                padded[i, :(n_timesteps - n)] = np.nan\n    else:\n        for i in xrange(n_seqs):\n            n = seq_lengths[i]\n            if n > 0:\n                padded[i, :n, :] = padded[i, (n_timesteps - n):, :]\n                padded[i, n:, :] = np.nan\n\n    if is_flat:\n        padded = np.squeeze(padded)\n\n    return padded\n\n\ndef right_pad_to_left_pad(padded):\n    """""" Change right padded to left padded. """"""\n    return _align_padded(padded, align_right=True)\n\n\ndef left_pad_to_right_pad(padded):\n    """""" Change left padded to right padded. """"""\n    return _align_padded(padded, align_right=False)\n\n\ndef df_join_in_endtime(df, constant_per_id_cols=\'id\',\n                       abs_time_col=\'dt\',\n                       abs_endtime=None,\n                       fill_zeros=False):\n    """""" Join in NaN-rows at timestep of when we stopped observing non-events.\n\n        If we have a dataset consisting of events recorded until a fixed\n        timestamp, that timestamp won\'t show up in the dataset (it\'s a non-event).\n        By joining in a row with NaN data at `abs_endtime` we get a boundarytime\n        for each sequence used for TTE-calculation and padding.\n\n        This is simpler in SQL where you join `on df.dt <= df_last_timestamp.dt`\n\n        .. Protip::\n            If discrete time: filter away last interval (ex day)\n            upstream as measurements here may be incomplete, i.e if query is in\n            middle of day (we are thus always looking at yesterdays data)\n\n        :param pandas.dataframe df: Pandas dataframe\n        :param constant_per_id_cols: identifying id and\n                                   columns remaining constant per id&timestep\n        :type constant_per_id_cols: String or String list\n        :param String abs_time_col: identifying the wall-clock column df[abs_time_cols].\n        :param df[abs_time_cols]) abs_endtime: The time to join in. If None it\'s inferred.\n        :type abs_endtime: None or same as df[abs_time_cols].values.\n        :param bool fill_zeros : Whether to attempt to fill NaN with zeros after merge.\n        :return pandas.dataframe df: pandas dataframe where each `id` has rows at the endtime.\n    """"""\n    risky_columns = list(set([\'t_elapsed\', \'t\', \'t_ix\']) & set(df.columns.values))\n    if len(risky_columns):\n        print(\'Warning: df has columns \',\n              risky_columns,\n              \', call `df_join_in_endtime` before calculating any relative time.\',\n              \'( otherwise they will be replaced at last step ) \')\n\n    if type(constant_per_id_cols) is not list:\n        constant_per_id_cols = [constant_per_id_cols]\n\n    if abs_endtime is None:\n        abs_endtime = df[abs_time_col].max()\n\n    df_ids = df[constant_per_id_cols].drop_duplicates()\n\n    df_ids[abs_time_col] = abs_endtime\n\n    if fill_zeros:\n        old_dtypes = df.dtypes.values\n        cols = df.columns\n\n        df = pd.merge(df_ids, df, how=\'outer\').fillna(0)\n\n        for i in xrange(len(old_dtypes)):\n            df[cols[i]] = df[cols[i]].astype(old_dtypes[i])\n    else:\n        df = pd.merge(df_ids, df, how=\'outer\')\n\n    df = df.sort_values(by=[constant_per_id_cols[0], abs_time_col])\n\n    return df\n\n\ndef shift_discrete_padded_features(padded, fill=0):\n    """"""\n\n    :param padded: padded (np array): Array [batch,timestep,...]\n    :param float fill: value to replace nans with.\n\n    =====\n    Details\n    =====\n    For mathematical purity and to avoid confusion, in the Discrete case\n    ""2015-12-15"" means an interval ""2015-12-15 00.00 - 2015-12-15 23.59"" i.e the data\n    is accessible at ""2015-12-15 23.59""  (time when we query our database to\n    do prediction about next day.)\n\n    In the continuous case ""2015-12-15 23.59"" means exactly at\n    ""2015-12-15 23.59: 00000000"".\n\n    Discrete case\n    --------\n\n    +-+----------------------+------+\n    |t|dt                    |Event |\n    +=+======================+======+\n    |0|2015-12-15 00.00-23.59|1     |\n    +-+----------------------+------+\n    |1|2015-12-16 00.00-23.59|1     |\n    +-+----------------------+------+\n    |2|2015-12-17 00.00-23.59|0     |\n    +-+----------------------+------+\n\n    etc. In detail:\n\n    +---------+-+-+-+-+-+-+----+\n    |t        |0|1|2|3|4|5|....|\n    +=========+=+=+=+=+=+=+====+\n    |event    |1|1|0|0|1|?|....|\n    +---------+-+-+-+-+-+-+----+\n    |feature  |?|1|1|0|0|1|....|\n    +---------+-+-+-+-+-+-+----+\n    |TTE      |0|0|2|1|0|?|....|\n    +---------+-+-+-+-+-+-+----+\n    |Observed*|F|T|T|T|T|T|....|\n    +---------+-+-+-+-+-+-+----+\n\n    Continuous case\n    --------\n\n    +-+----------------+------+\n    |t|dt              |Event |\n    +=+================+======+\n    |0|2015-12-15 14.39|1     |\n    +-+----------------+------+\n    |1|2015-12-16 16.11|1     |\n    +-+----------------+------+\n    |2|2015-12-17 22.18|0     |\n    +-+----------------+------+\n\n    etc. In detail:\n\n    +---------+-+-+-+-+-+-+---+\n    |t        |0|1|2|3|4|5|...|\n    +=========+=+=+=+=+=+=+===+\n    |event    |1|1|0|0|1|?|...|\n    +---------+-+-+-+-+-+-+---+\n    |feature  |1|1|0|0|1|?|...|\n    +---------+-+-+-+-+-+-+---+\n    |TTE      |1|3|2|1|?|?|...|\n    +---------+-+-+-+-+-+-+---+\n    |Observed*|T|T|T|T|T|T|...|\n    +---------+-+-+-+-+-+-+---+\n\n\n    *Observed = Do we have feature data at this time?*\n\n        In the discrete case:\n\n        -> we need to roll data intent as features to the right.\n\n          -> First timestep typically has no measured features (and we may not even\n          know until the end of the first interval if the sequence even exists!)\n\n        So there\'s two options after rolling features to the right:\n\n        1. *Fill in 0s at t=0. (`shift_discrete_padded_features`)*\n\n            - if (data -> event) this is (randomly) leaky (potentially safe)\n            - if (data <-> event) this exposes the truth (unsafe)!\n\n        2. *Remove t=0 from target data*\n\n            - (dont learn to predict about prospective customers first purchase)\n            Safest!\n\n        note: We never have target data for the last timestep after rolling.\n\n      Example:\n      Customer has first click leading to day 0 so at day 1 we can use\n      features about that click to predict time to purchase.\n      Since click does not imply purchase we can predict time to purchase\n      at step 0 (but with no feature data, ex using zeros as input).\n    """"""\n    padded = np.roll(padded, shift=1, axis=1)\n    padded[:, 0] = fill\n    return padded\n\n\ndef normalize_padded(padded, means=None, stds=None, only_nonzero=False, epsilon=1e-6):\n    """"""Normalize by last dim of padded with means/stds or calculate them.\n\n        If `means` or `stds` is passed, it simply shifts/scales by them.\n        If only_nonzero, only normalizes nonzero entries. This should be the choice\n        for sparse event data but not default for legacy reasons.\n\n        :param Array means: location coefficients\n        :param Array stds: scale coefficients\n        :param Boolean only_nonzero: Whether to normalize non-zero elements only.\n    """"""\n\n    is_flat = len(padded.shape) == 2\n    if is_flat:\n        padded = np.expand_dims(padded, axis=-1)\n\n    n_sequences, n_timesteps, n_features = padded.shape\n    n_obs = n_sequences * n_timesteps\n\n    if only_nonzero:\n        zeros_nanmask = np.zeros_like(padded)\n        # TODO catch if no nans or zeros?\n        zeros_nanmask[np.isnan(padded)] = np.nan\n        zeros_nanmask[padded == 0] = np.nan\n\n    if means is None:\n        if only_nonzero:\n            vals = (padded + zeros_nanmask).reshape(n_obs, n_features)\n        else:\n            vals = padded.reshape(n_obs, n_features)\n        means = np.nanmean(vals, axis=0, keepdims=False, dtype=np.float128)\n        del vals\n\n        if any(np.isnan(means)):\n            means[np.isnan(means)] = 0  # If mean of empty slice.\n\n        means = means.astype(padded.dtype)\n\n    if stds is None:\n        if only_nonzero:\n            vals = (padded + zeros_nanmask).reshape(n_obs, n_features)\n        else:\n            vals = padded.reshape(n_obs, n_features)\n        stds = np.nanstd(vals, axis=0, keepdims=False, dtype=np.float128)\n        del vals\n\n        if any(np.isnan(stds)):\n            stds[np.isnan(stds)] = 1  # If no non-nan (and/or nonzero) elements\n        stds = stds.astype(padded.dtype)\n\n    if (stds < epsilon).any():\n        # Near constant features.\n        if only_nonzero:\n            # Set nonzero elements to 1\n            means[stds < epsilon] = means[stds < epsilon] - 1\n        else:\n            print(\'Warning. low-variance cols, indx: \',\n                  np.where(stds < epsilon)[0])\n            print(\'(Could be binary columns.)  stds: \', stds[stds < epsilon])\n\n        # Result is (small number)/1.0 as mean will be subtracted.\n        stds[stds < epsilon] = 1\n\n    stds = stds.reshape([1, 1, n_features])\n    means = means.reshape([1, 1, n_features])\n\n    if only_nonzero:\n        # Don\'t shift zero-elements.\n        padded = padded - means * (padded != 0)\n    else:\n        padded = padded - means\n\n    padded = padded / stds\n\n    if is_flat:\n        # Return to original shape\n        padded = padded.reshape(n_sequences, n_timesteps)\n    return padded, means, stds\n'"
python/wtte/tte_util.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom six.moves import xrange\n\n# TODO\n# - Proper tests of everything\n# - be clearer about meaning of t_elapsed, t_ix and either (t)\n# - Time Since Event is a ticking bomb. Needs better naming/definitions\n#   to ensure that it\'s either inverse TTE or a feature or if they coincide.\n\ndef roll_fun(x, size, fun=np.mean, reverse=False):\n    """"""Like cumsum but with any function `fun`. \n    """"""\n    y = np.copy(x)\n    n = len(x)\n    size = min(size, n)\n\n    if size <= 1:\n        return x\n\n    for i in xrange(size):\n        y[i] = fun(x[0:(i + 1)])\n    for i in xrange(size, n):\n        y[i] = fun(x[(i - size + 1):(i + 1)])\n    return y\n\n\ndef carry_forward_if(x, is_true):\n    """"""Locomote forward `x[i]` if `is_true[i]`.\n        remain x untouched before first pos of truth.\n\n        :param Array x: object whos elements are to carry forward\n        :param Array is_true: same length as x containing true/false boolean.\n        :return Array x: forwarded object\n    """"""\n    for i in xrange(len(x)):\n        if is_true[i]:\n            cargo = x[i]\n        if cargo is not None:\n            x[i] = cargo\n    return x\n\n\ndef carry_backward_if(x, is_true):\n    """"""Locomote backward `x[i]` if `is_true[i]`.\n        remain x untouched after last pos of truth.\n\n        :param Array x: object whos elements are to carry backward\n        :param Array is_true: same length as x containing true/false boolean.\n        :return Array x: backwarded object\n    """"""\n    for i in xrange(reversed(len(x))):\n        if is_true[i]:\n            cargo = x[i]\n        if cargo is not None:\n            x[i] = cargo\n    return x\n\n\ndef steps_since_true_minimal(is_event):\n    """"""(Time) since event over discrete (padded) event vector.\n\n        :param Array is_event: a vector of 0/1s or boolean\n        :return Array x: steps since is_event was true\n    """"""\n    n = len(is_event)\n    z = -1  # at the latest on step before\n    x = np.int32(is_event)\n    for i in xrange(n):\n        if is_event[i]:\n            z = i\n        x[i] = i - z\n    return x\n\n\ndef steps_to_true_minimal(is_event):\n    """"""(Time) to event for discrete (padded) event vector.\n\n        :param Array is_event: a vector of 0/1s or boolean\n        :return Array x: steps until is_event is true\n    """"""\n    n = len(is_event)\n    z = n  # at the earliest on step after\n    x = np.int32(is_event)\n    for i in reversed(xrange(n)):\n        if is_event[i]:\n            z = i\n        x[i] = z - i\n    return x\n\n\ndef get_tte_discrete(is_event, t_elapsed=None):\n    """"""Calculates discretely measured tte over a vector.\n\n        :param Array is_event: Boolean array\n        :param IntArray t_elapsed: integer array with same length as `is_event`. If none, it will use `xrange(len(is_event))`\n        :return Array tte: Time-to-event array (discrete version)\n\n\n        - Caveats\n            tte[i] = numb. timesteps to timestep with event\n            Step of event has tte = 0 \\\n           (event happened at time [t,t+1))\n            tte[-1]=1 if no event (censored data)\n    """"""\n    n = len(is_event)\n    tte = np.int32(is_event)\n    stepsize = 1\n    if t_elapsed is None:\n        t_elapsed = xrange(n)\n\n    t_next = t_elapsed[-1] + stepsize\n    for i in reversed(xrange(n)):\n        if is_event[i]:\n            t_next = t_elapsed[i]\n        tte[i] = t_next - t_elapsed[i]\n    return tte\n\n\ndef get_tte_continuous(is_event, t_elapsed):\n    """"""Calculates time to (pointwise measured) next event over a vector.\n\n        :param Array is_event: Boolean array\n        :param IntArray t_elapsed: integer array with same length as `is_event` that supports vectorized subtraction. If none, it will use `xrange(len(is_event))`\n        :return Array tte: Time-to-event (continuous version)\n\n        TODO::\n            Should support discretely sampled, continuously measured TTE\n\n        .. Caveats::\n            tte[i] = time to *next* event at time t[i]\n            (t[i] is exactly at event&/or query)\n            tte[-1]=0 always\n            (since last time is a *point*)\n            Last datpoints are right censored.\n    """"""\n    n = len(is_event)\n    if t_elapsed is None:\n        t_elapsed = np.int32(xrange(n))\n\n    t_next = t_elapsed[-1]\n    # lazy initialization to autoinit if difftime\n    tte = t_elapsed - t_next\n    for i in reversed(xrange(n)):\n        tte[i] = t_next - t_elapsed[i]\n        if is_event[i]:\n            t_next = t_elapsed[i]\n    return tte\n\n\ndef get_tte(is_event, discrete_time, t_elapsed=None):\n    """""" wrapper to calculate *Time To Event* for input vector.\n\n        :param Boolean discrete_time: if `True`, use `get_tte_discrete`. If `False`, use `get_tte_continuous`.\n    """"""\n    if discrete_time:\n        return get_tte_discrete(is_event, t_elapsed)\n    else:\n        return get_tte_continuous(is_event, t_elapsed)\n\n\ndef get_tse(is_event, t_elapsed=None):\n    """""" Wrapper to calculate *Time Since Event* for input vector.\n\n        Inverse of tte. Safe to use as a feature.\n        Always ""continuous"" method of calculating it.\n        tse >0 at time of event\n            (if discrete we dont know about the event yet, if continuous\n            we know at record of event so superfluous to have tse=0)\n        tse = 0 at first step\n\n        :param Array is_event: Boolean array\n        :param IntArray t_elapsed: None or integer array with same length as `is_event`.\n\n            * If none, it will use `t_elapsed.max() - t_elapsed[::-1]`.\n\n        .. TODO::\n        reverse-indexing is pretty slow and ugly and not a helpful template for implementing in other languages.\n\n    """"""\n    if t_elapsed is not None:\n        t_elapsed = t_elapsed.max() - t_elapsed[::-1]\n\n    return get_tte_continuous(is_event[::-1], t_elapsed)[::-1]\n\n\ndef get_is_not_censored(is_event, discrete_time=True):\n    """""" Calculates non-censoring indicator `u` for one vector.\n\n        :param array is_event: logical or numeric array indicating event.\n        :param Boolean discrete_time: if `True`, last observation is conditionally censored.\n    """"""\n    n = len(is_event)\n    is_not_censored = np.copy(is_event)\n\n    if discrete_time:\n        # Last obs is conditionally censored\n        event_seen = is_event[-1]\n        for i in reversed(xrange(n)):\n            if is_event[i] and not event_seen:\n                event_seen = is_event[i]\n            is_not_censored[i] = event_seen\n    else:\n        # Last obs is always censored\n        event_seen = False\n        for i in reversed(xrange(n)):\n            is_not_censored[i] = event_seen\n            if is_event[i] and not event_seen:\n                event_seen = is_event[i]\n\n    return is_not_censored\n'"
python/wtte/weibull.py,0,"b'""""""\nWrapper for Python Weibull functions\n""""""\nimport numpy as np\n\n\ndef cumulative_hazard(t, a, b):\n    """""" Cumulative hazard\n\n    :param t: Value\n    :param a: Alpha\n    :param b: Beta\n    :return: `np.power(t / a, b)`\n    """"""\n    t = np.double(t)\n    return np.power(t / a, b)\n\n\ndef hazard(t, a, b):\n    t = np.double(t)\n    return (b / a) * np.power(t / a, b - 1)\n\n\ndef cdf(t, a, b):\n    """""" Cumulative distribution function.\n\n    :param t: Value\n    :param a: Alpha\n    :param b: Beta\n    :return: `1 - np.exp(-np.power(t / a, b))`\n    """"""\n    t = np.double(t)\n    return 1 - np.exp(-np.power(t / a, b))\n\n\ndef pdf(t, a, b):\n    """""" Probability distribution function.\n\n    :param t: Value\n    :param a: Alpha\n    :param b: Beta\n    :return: `(b / a) * np.power(t / a, b - 1) * np.exp(-np.power(t / a, b))`\n    """"""\n    t = np.double(t)\n    return (b / a) * np.power(t / a, b - 1) * np.exp(-np.power(t / a, b))\n\n\ndef cmf(t, a, b):\n    """""" Cumulative Mass Function.\n\n    :param t: Value\n    :param a: Alpha\n    :param b: Beta\n    :return: `cdf(t + 1, a, b)`\n    """"""\n    t = np.double(t) + 1e-35\n    return cdf(t + 1, a, b)\n\n\ndef pmf(t, a, b):\n    """""" Probability mass function.\n\n    :param t: Value\n    :param a: Alpha\n    :param b: Beta\n    :return: `cdf(t + 1.0, a, b) - cdf(t, a, b)`\n    """"""\n    t = np.double(t) + 1e-35\n    return cdf(t + 1.0, a, b) - cdf(t, a, b)\n\n\ndef mode(a, b):\n    # Continuous mode.\n    # TODO (mathematically) prove how close it is to discretized mode\n    try:\n        mode = a * np.power((b - 1.0) / b, 1.0 / b)\n        mode[b <= 1.0] = 0.0\n    except:\n        # scalar case\n        if b <= 1.0:\n            mode = 0\n        else:\n            mode = a * np.power((b - 1.0) / b, 1.0 / b)\n    return mode\n\n\ndef mean(a, b):\n    """""" Continuous mean. at most 1 step below discretized mean \n\n    `E[T ] <= E[Td] + 1` true for positive distributions.\n    """"""\n    from scipy.special import gamma\n    return a * gamma(1.0 + 1.0 / b)\n\n\ndef quantiles(a, b, p):\n    """""" Quantiles\n\n    :param a: Alpha\n    :param b: Beta\n    :param p:\n    :return: `a * np.power(-np.log(1.0 - p), 1.0 / b)`\n    """"""\n    return a * np.power(-np.log(1.0 - p), 1.0 / b)\n\n\ndef mean(a, b):\n    """"""Continuous mean. Theoretically at most 1 step below discretized mean\n\n    `E[T ] <= E[Td] + 1` true for positive distributions.\n\n    :param a: Alpha\n    :param b: Beta\n    :return: `a * gamma(1.0 + 1.0 / b)`\n    """"""\n    from scipy.special import gamma\n    return a * gamma(1.0 + 1.0 / b)\n\n\ndef continuous_loglik(t, a, b, u=1, equality=False):\n    """"""Continous censored loglikelihood function.\n\n    :param bool equality: In ML we usually only care about the likelihood\n    with *proportionality*, removing terms not dependent on the parameters.\n    If this is set to `True` we keep those terms.\n    """"""\n    if equality:\n        loglik = u * np.log(pdf(t, a, b)) + (1 - u) * \\\n            np.log(1.0 - cdf(t, a, b))\n    else:\n        # commonly optimized over: proportional terms w.r.t alpha,beta\n        loglik = u * loglik(hazard(t, a, b)) - \\\n            loglik(cumulative_hazard(t, a, b))\n\n    return loglik\n\n\ndef discrete_loglik(t, a, b, u=1, equality=False):\n    """"""Discrete censored loglikelihood function.\n\n    :param bool equality: In ML we usually only care about the likelihood\n    with *proportionality*, removing terms not dependent on the parameters.\n    If this is set to `True` we keep those terms.\n    """"""\n    if equality:\n        # With equality instead of proportionality.\n        loglik = u * np.log(pmf(t, a, b)) + (1 - u) * \\\n            np.log(1.0 - cdf(t + 1.0, a, b))\n    else:\n        # commonly optimized over: proportional terms w.r.t alpha,beta\n        hazard0 = cumulative_hazard(t, a, b)\n        hazard1 = cumulative_hazard(t + 1., a, b)\n        loglik = u * np.log(np.exp(hazard1 - hazard0) - 1.0) - hazard1\n\n    return loglik\n\n# Conditional excess\n\n\nclass conditional_excess():\n    """""" Experimental class for conditional excess distribution.\n\n        The idea is to query `s` into the future after time `t`\n        has passed without event. Se thesis for details.\n\n        note: Note tested and may be incorrect!\n    """"""\n\n    def pdf(t, s, a, b):\n        t = np.double(t)\n        return hazard(t + s, a, b) * np.exp(-cumulative_hazard(t + s, a, b) + cumulative_hazard(t, a, b))\n\n    def cdf(t, s, a, b):\n        t = np.double(t)\n        return 1 - np.exp(-cumulative_hazard(t + s, a, b) + cumulative_hazard(t, a, b))\n\n    def quantile(t, a, b, p):\n        # TODO this is not tested yet.\n        # tests:\n        #    cequantile(0., a, b, p)==quantiles(a, b, p)\n        #    cequantile(t, a, 1., p)==cequantile(0., a, 1., p)\n        # conditional excess quantile\n        # t+s : Pr(Y<t+s|y>t)=p\n\n        print(\'not tested\')\n        L = np.power((t + .0) / a, b)\n\n        quantile = a * np.power(-np.log(1. - p) - L, 1. / b)\n\n        return quantile\n\n    def mean(t, a, b):\n        # TODO this is not tested yet.\n        # tests:\n        #    cemean(0., a, b)==mean(a, b, p)\n        #    mean(t, a, 1., p)==mean(0., a, 1., p) == a\n        # conditional excess mean\n        # E[Y|y>t]\n        # (conditional mean age at failure)\n        # http://reliabilityanalyticstoolkit.appspot.com/conditional_distribution\n        from scipy.special import gamma\n        from scipy.special import gammainc\n        # Regularized lower gamma\n        print(\'not tested\')\n\n        v = 1. + 1. / b\n        gv = gamma(v)\n        L = np.power((t + .0) / a, b)\n        cemean = a * gv * np.exp(L) * (1 - gammainc(v, t / a) / gv)\n\n        return cemean\n'"
python/wtte/wtte.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom math import log\n\nimport warnings\nimport numpy as np\n\nfrom keras import backend as K\nfrom keras.callbacks import Callback\n\n\ndef _keras_unstack_hack(ab):\n    """"""Implements tf.unstack(y_true_keras, num=2, axis=-1).\n\n       Keras-hack adopted to be compatible with Theano backend.\n\n       :param ab: stacked variables\n       :return a, b: unstacked variables\n    """"""\n    ndim = len(K.int_shape(ab))\n    if ndim == 0:\n        print(\'can not unstack with ndim=0\')\n    else:\n        a = ab[..., 0]\n        b = ab[..., 1]\n    return a, b\n\n\ndef output_lambda(x, init_alpha=1.0, max_beta_value=5.0, scalefactor=None,\n                  alpha_kernel_scalefactor=None):\n    """"""Elementwise (Lambda) computation of alpha and regularized beta.\n\n        - Alpha:\n\n            (activation)\n            Exponential units seems to give faster training than\n            the original papers softplus units. Makes sense due to logarithmic\n            effect of change in alpha.\n            (initialization)\n            To get faster training and fewer exploding gradients,\n            initialize alpha to be around its scale when beta is around 1.0,\n            approx the expected value/mean of training tte.\n            Because we\'re lazy we want the correct scale of output built\n            into the model so initialize implicitly;\n            multiply assumed exp(0)=1 by scale factor `init_alpha`.\n\n        - Beta:\n\n            (activation)\n            We want slow changes when beta-> 0 so Softplus made sense in the original\n            paper but we get similar effect with sigmoid. It also has nice features.\n            (regularization) Use max_beta_value to implicitly regularize the model\n            (initialization) Fixed to begin moving slowly around 1.0\n\n        - Usage\n            .. code-block:: python\n\n                model.add(TimeDistributed(Dense(2)))\n                model.add(Lambda(wtte.output_lambda, arguments={""init_alpha"":init_alpha, \n                                                        ""max_beta_value"":2.0\n                                                       }))\n\n\n        :param x: tensor with last dimension having length 2 with x[...,0] = alpha, x[...,1] = beta\n        :param init_alpha: initial value of `alpha`. Default value is 1.0.\n        :param max_beta_value: maximum beta value. Default value is 5.0.\n        :param max_alpha_value: maxumum alpha value. Default is `None`.\n        :type x: Array\n        :type init_alpha: Float\n        :type max_beta_value: Float\n        :type max_alpha_value: Float\n        :return x: A positive `Tensor` of same shape as input\n        :rtype: Array\n\n    """"""\n    if max_beta_value is None or max_beta_value > 3:\n        if K.epsilon() > 1e-07 and K.backend() == \'tensorflow\':\n            # TODO need to think this through lol\n            message = ""\\\n            Using tensorflow backend and allowing high `max_beta_value` may lead to\\n\\\n            gradient NaN during training unless `K.epsilon()` is small.\\n\\\n            Call `keras.backend.set_epsilon(1e-08)` to lower epsilon \\\n            ""\n            warnings.warn(message)\n    if alpha_kernel_scalefactor is not None:\n        message = ""`alpha_kernel_scalefactor` deprecated in favor of `scalefactor` scaling both.\\n Setting `scalefactor = alpha_kernel_scalefactor`""\n        warnings.warn(message)\n        scalefactor = alpha_kernel_scalefactor\n\n    a, b = _keras_unstack_hack(x)\n\n    if scalefactor is not None:\n        # Done after due to theano bug.\n        a, b = scalefactor * a, scalefactor * b\n\n    # Implicitly initialize alpha:\n    a = init_alpha * K.exp(a)\n\n    if max_beta_value > 1.05:  # some value >>1.0\n        # shift to start around 1.0\n        # assuming input is around 0.0\n        _shift = np.log(max_beta_value - 1.0)\n\n        b = b - _shift\n\n    b = max_beta_value * K.sigmoid(b)\n\n    x = K.stack([a, b], axis=-1)\n\n    return x\n\n\nclass OuputActivation(object):\n    """""" Elementwise computation of alpha and regularized beta.\n\n\n        Wrapper to `output_lambda` using keras.layers.Activation. \n        See this for details.\n\n        - Usage\n            .. code-block:: python\n\n               wtte_activation = wtte.OuputActivation(init_alpha=1.,\n                                                 max_beta_value=4.0).activation\n\n               model.add(Dense(2))\n               model.add(Activation(wtte_activation))\n\n    """"""\n\n    def __init__(self, init_alpha=1.0, max_beta_value=5.0):\n        self.init_alpha = init_alpha\n        self.max_beta_value = max_beta_value\n\n    def activation(self, ab):\n        """""" (Internal function) Activation wrapper\n\n        :param ab: original tensor with alpha and beta.\n        :return ab: return of `output_lambda` with `init_alpha` and `max_beta_value`.\n        """"""\n        ab = output_lambda(ab, init_alpha=self.init_alpha,\n                           max_beta_value=self.max_beta_value)\n\n        return ab\n\n# Backwards Compatibility\noutput_activation = OuputActivation\n\n\ndef _keras_split(y_true, y_pred):\n    """"""\n        Everything is a hack around the y_true,y_pred paradigm.\n    """"""\n    y, u = _keras_unstack_hack(y_true)\n    a, b = _keras_unstack_hack(y_pred)\n\n    return y, u, a, b\n\nkeras_split = _keras_split\n\n\ndef loglik_discrete(y, u, a, b, epsilon=K.epsilon()):\n    hazard0 = K.pow((y + epsilon) / a, b)\n    hazard1 = K.pow((y + 1.0) / a, b)\n\n    loglikelihoods = u * \\\n        K.log(K.exp(hazard1 - hazard0) - (1.0 - epsilon)) - hazard1\n    return loglikelihoods\n\n\ndef loglik_continuous(y, u, a, b, epsilon=K.epsilon()):\n    ya = (y + epsilon) / a\n    loglikelihoods = u * (K.log(b) + b * K.log(ya)) - K.pow(ya, b)\n    return loglikelihoods\n\n\ndef loglik_continuous_conditional_correction(y, u, a, b, epsilon=K.epsilon()):\n    """"""Integrated conditional excess loss.\n        Explanation TODO\n    """"""\n    ya = (y + epsilon) / a\n    loglikelihoods = y * \\\n        (u * (K.log(b) + b * K.log(ya)) - (b / (b + 1.)) * K.pow(ya, b))\n    return loglikelihoods\n\n\nclass Loss(object):\n    """""" Creates a keras WTTE-loss function.\n        - Usage\n\n            :Example:\n\n            .. code-block:: python\n               loss = wtte.Loss(kind=\'discrete\').loss_function\n               model.compile(loss=loss, optimizer=RMSprop(lr=0.01))\n\n               # And with masking:\n               loss = wtte.Loss(kind=\'discrete\',reduce_loss=False).loss_function\n               model.compile(loss=loss, optimizer=RMSprop(lr=0.01),\n                              sample_weight_mode=\'temporal\')\n\n        .. note::\n\n            With masking keras needs to access each loss-contribution individually.\n            Therefore we do not sum/reduce down to scalar (dim 1), instead return a \n            tensor (with reduce_loss=False).\n\n        :param kind:  One of \'discrete\' or \'continuous\'\n        :param reduce_loss: \n        :param clip_prob: Clip likelihood to [log(clip_prob),log(1-clip_prob)]\n        :param regularize: Deprecated.\n        :param location: Deprecated.\n        :param growth: Deprecated.\n        :type reduce_loss: Boolean\n    """"""\n\n    def __init__(self,\n                 kind,\n                 reduce_loss=True,\n                 clip_prob=1e-6,\n                 regularize=False,\n                 location=None,\n                 growth=None):\n\n        self.kind = kind\n        self.reduce_loss = reduce_loss\n        self.clip_prob = clip_prob\n\n        if regularize == True or location is not None or growth is not None:\n            raise DeprecationWarning(\'Directly penalizing beta has been found \\\n                                      to be unneccessary when using bounded activation \\\n                                      and clipping of log-likelihood.\\\n                                      Use this method instead.\')\n\n    def loss_function(self, y_true, y_pred):\n\n        y, u, a, b = _keras_split(y_true, y_pred)\n        if self.kind == \'discrete\':\n            loglikelihoods = loglik_discrete(y, u, a, b)\n        elif self.kind == \'continuous\':\n            loglikelihoods = loglik_continuous(y, u, a, b)\n\n        if self.clip_prob is not None:\n            loglikelihoods = K.clip(loglikelihoods, \n                log(self.clip_prob), log(1 - self.clip_prob))\n        if self.reduce_loss:\n            loss = -1.0 * K.mean(loglikelihoods, axis=-1)\n        else:\n            loss = -loglikelihoods\n\n        return loss\n\n# For backwards-compatibility\nloss = Loss\n\n\nclass WeightWatcher(Callback):\n    """"""Keras Callback to keep an eye on output layer weights.\n        (under development)\n\n        Usage:\n            weightwatcher = WeightWatcher(per_batch=True,per_epoch=False)\n            model.fit(...,callbacks=[weightwatcher])\n            weightwatcher.plot()\n    """"""\n\n    def __init__(self,\n                 per_batch=False,\n                 per_epoch=True\n                 ):\n        self.per_batch = per_batch\n        self.per_epoch = per_epoch\n\n    def on_train_begin(self, logs={}):\n        self.a_weights_mean = []\n        self.b_weights_mean = []\n        self.a_weights_min = []\n        self.b_weights_min = []\n        self.a_weights_max = []\n        self.b_weights_max = []\n        self.a_bias = []\n        self.b_bias = []\n\n    def append_metrics(self):\n        # Last two weightlayers in model\n\n        output_weights, output_biases = self.model.get_weights()[-2:]\n\n        a_weights_mean, b_weights_mean = output_weights.mean(0)\n        a_weights_min, b_weights_min = output_weights.min(0)\n        a_weights_max, b_weights_max = output_weights.max(0)\n\n        a_bias, b_bias = output_biases\n\n        self.a_weights_mean.append(a_weights_mean)\n        self.b_weights_mean.append(b_weights_mean)\n        self.a_weights_min.append(a_weights_min)\n        self.b_weights_min.append(b_weights_min)\n        self.a_weights_max.append(a_weights_max)\n        self.b_weights_max.append(b_weights_max)\n        self.a_bias.append(a_bias)\n        self.b_bias.append(b_bias)\n\n    def on_train_end(self, logs={}):\n        if self.per_epoch:\n            self.append_metrics()\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        if self.per_epoch:\n            self.append_metrics()\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        if self.per_batch:\n            self.append_metrics()\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        if self.per_batch:\n            self.append_metrics()\n        return\n\n    def plot(self):\n        import matplotlib.pyplot as plt\n\n        # Create axes\n        fig, ax1 = plt.subplots()\n        ax2 = ax1.twinx()\n\n        ax1.plot(self.a_bias, color=\'b\')\n        ax1.set_xlabel(\'step\')\n        ax1.set_ylabel(\'alpha\')\n\n        ax2.plot(self.b_bias, color=\'r\')\n        ax2.set_ylabel(\'beta\')\n\n        # Change color of each axis\n        def color_y_axis(ax, color):\n            """"""Color your axes.""""""\n            for t in ax.get_yticklabels():\n                t.set_color(color)\n            return None\n\n        plt.title(\'biases\')\n        color_y_axis(ax1, \'b\')\n        color_y_axis(ax2, \'r\')\n        plt.show()\n\n        ###############\n        fig, ax1 = plt.subplots()\n        ax2 = ax1.twinx()\n\n        ax1.plot(self.a_weights_min, color=\'blue\',\n                 linestyle=\'dotted\', label=\'min\', linewidth=2)\n        ax1.plot(self.a_weights_mean, color=\'blue\',\n                 linestyle=\'solid\', label=\'mean\', linewidth=1)\n        ax1.plot(self.a_weights_max, color=\'blue\',\n                 linestyle=\'dotted\', label=\'max\', linewidth=2)\n\n        ax1.set_xlabel(\'step\')\n        ax1.set_ylabel(\'alpha\')\n\n        ax2.plot(self.b_weights_min, color=\'red\',\n                 linestyle=\'dotted\', linewidth=2)\n        ax2.plot(self.b_weights_mean, color=\'red\',\n                 linestyle=\'solid\', linewidth=1)\n        ax2.plot(self.b_weights_max, color=\'red\',\n                 linestyle=\'dotted\', linewidth=2)\n        ax2.set_ylabel(\'beta\')\n\n        # Change color of each axis\n        def color_y_axis(ax, color):\n            """"""Color your axes.""""""\n            for t in ax.get_yticklabels():\n                t.set_color(color)\n            return None\n\n        plt.title(\'weights (min,mean,max)\')\n        color_y_axis(ax1, \'b\')\n        color_y_axis(ax2, \'r\')\n        plt.show()\n'"
python/wtte/objectives/__init__.py,0,b''
python/wtte/objectives/tensorflow.py,14,"b'"""""" Objective functions for TensorFlow\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport tensorflow as tf\n\n\ndef loglik_continuous(a, b, y_, u_, output_collection=(), name=None):\n    """""" Returns element-wise Weibull censored log-likelihood.\n\n    Continuous weibull log-likelihood. loss=-loglikelihood.\n    All input values must be of same type and shape.\n\n    :param a:alpha. Positive nonzero `Tensor`.\n    :type a: `float32` or `float64`.\n    :param b:beta.  Positive nonzero `Tensor`.\n    :type b: `float32` or `float64`.\n    :param y_: time to event. Positive nonzero `Tensor`\n    :type y_: `float32` or `float64`.\n    :param u_: indicator. 0.0 if right censored, 1.0 if uncensored `Tensor`\n    :type u_: `float32` or `float64`.\n    :param output_collection:name of the collection to collect result of this op.\n    :type output_collection: Tuple of Strings.\n    :param String name: name of the operation.\n    :return: A `Tensor` of log-likelihoods of same shape as a, b, y_, u_\n    """"""\n\n    with tf.name_scope(name, ""weibull_loglik_continuous"", [a, b, y_, u_]):\n\n        ya = tf.div(y_ + 1e-35, a)  # Small optimization y/a\n\n        loglik = tf.multiply(u_, tf.log(\n            b) + tf.multiply(b, tf.log(ya))) - tf.pow(ya, b)\n        tf.add_to_collection(output_collection, loglik)\n\n    return(loglik)\n\n\ndef loglik_discrete(a, b, y_, u_, output_collection=(), name=None):\n    """"""Returns element-wise Weibull censored discrete log-likelihood.\n\n    Unit-discretized weibull log-likelihood. loss=-loglikelihood.\n\n    .. note::\n        All input values must be of same type and shape.\n\n    :param a:alpha. Positive nonzero `Tensor`.\n    :type a: `float32` or `float64`.\n    :param b:beta.  Positive nonzero `Tensor`.\n    :type b: `float32` or `float64`.\n    :param y_: time to event. Positive nonzero `Tensor` \n    :type y_: `float32` or `float64`.\n    :param u_: indicator. 0.0 if right censored, 1.0 if uncensored `Tensor`\n    :type u_: `float32` or `float64`.\n    :param output_collection:name of the collection to collect result of this op.\n    :type output_collection: Tuple of Strings.\n    :param String name: name of the operation.\n    :return: A `Tensor` of log-likelihoods of same shape as a, b, y_, u_.\n    """"""\n\n    with tf.name_scope(name, ""weibull_loglik_discrete"", [a, b, y_, u_]):\n        hazard0 = tf.pow(tf.div(y_ + 1e-35, a), b)  # 1e-9 safe, really\n        hazard1 = tf.pow(tf.div(y_ + 1.0, a), b)\n        loglik = tf.multiply(u_, tf.log(\n            tf.exp(hazard1 - hazard0) - 1.0)) - hazard1\n\n        tf.add_to_collection(output_collection, loglik)\n    return(loglik)\n\n\ndef betapenalty(b, location=10.0, growth=20.0, output_collection=(), name=None):\n    """"""Returns a positive penalty term exploding when beta approaches location.\n\n    Adding this term to the loss may prevent overfitting and numerical instability\n    of large values of beta (overconfidence). Remember that loss = -loglik+penalty\n\n    :param b:beta.  Positive nonzero `Tensor`.\n    :type b: `float32` or `float64`.\n    :param output_collection:name of the collection to collect result of this op.\n    :type output_collection: Tuple of Strings.\n    :param String name: name of the operation.\n    :return:  A positive `Tensor` of same shape as `b` being a penalty term.\n    """"""\n    with tf.name_scope(name, ""weibull_betapenalty"", [b]):\n        scale = growth / location\n        penalty = tf.exp(scale * (b - location))\n        tf.add_to_collection(output_collection, penalty)\n\n    return(penalty)\n'"
python/wtte/plots/__init__.py,0,b''
python/wtte/plots/misc.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom six.moves import xrange\n\nfrom wtte import transforms as tr\n\n\ndef timeline_plot(padded, title=\'\', cmap=""jet"", plot=True, fig=None, ax=None):\n    if fig is None or ax is None:\n        fig, ax = plt.subplots(ncols=2, sharey=True, figsize=(12, 4))\n\n    ax[0].imshow(padded, interpolation=\'none\',\n                 aspect=\'auto\', cmap=cmap, origin=\'lower\')\n    ax[0].set_ylabel(\'sequence\')\n    ax[0].set_xlabel(\'sequence time\')\n\n    ax[1].imshow(tr.right_pad_to_left_pad(padded),\n                 interpolation=\'none\',\n                 aspect=\'auto\',\n                 cmap=cmap,\n                 origin=\'lower\')\n    ax[1].set_ylabel(\'sequence\')\n    ax[1].set_xlabel(\'absolute time\')  # (Assuming sequences end today)\n\n    fig.suptitle(title, fontsize=14)\n    if plot:\n        fig.show()\n        return None, None\n    else:\n        return fig, ax\n\n\ndef timeline_aggregate_plot(padded, title=\'\', cmap=""jet"", plot=True):\n    fig, ax = plt.subplots(ncols=2, nrows=2, sharex=True,\n                           sharey=False, figsize=(12, 8))\n\n    fig, ax[0] = timeline_plot(\n        padded, title, cmap=cmap, plot=False, fig=fig, ax=ax[0])\n\n    ax[1, 0].plot(np.nanmean(padded, axis=0), lw=0.5,\n                  c=\'black\', drawstyle=\'steps-post\')\n    ax[1, 0].set_title(\'mean/timestep\')\n    padded = tr.right_pad_to_left_pad(padded)\n    ax[1, 1].plot(np.nanmean(padded, axis=0), lw=0.5,\n                  c=\'black\', drawstyle=\'steps-post\')\n    ax[1, 1].set_title(\'mean/timestep\')\n\n    fig.suptitle(title, fontsize=14)\n    if plot:\n        fig.show()\n        return None, None\n    else:\n        return fig, ax\n'"
python/wtte/plots/weibull_contour.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom six.moves import xrange\n\nfrom wtte import weibull\n\n\ndef weibull_contour(Y, U, is_discrete, true_alpha, true_beta, logx=True, samples=200, lines=True):\n\n    xlist = np.linspace(true_alpha / np.e, true_alpha * np.e, samples)\n    ylist = np.linspace(true_beta / np.e, true_beta * np.e, samples)\n    x_grid, y_grid = np.meshgrid(xlist, ylist)\n\n    loglik = x_grid * 0\n\n    if is_discrete:\n        fun = weibull.discrete_loglik\n    else:\n        fun = weibull.continuous_loglik\n\n    for i in xrange(len(Y)):\n        loglik = loglik + \\\n            fun(Y[i], x_grid, y_grid, U[i])\n\n    z_grid = loglik / len(Y)\n\n    plt.figure()\n    if logx:\n        x_grid = np.log(x_grid)\n        true_alpha = np.log(true_alpha)\n        xlab = r'$\\log(\\alpha)$'\n    else:\n        xlab = r'$\\alpha$'\n\n    cp = plt.contourf(x_grid, y_grid, z_grid, 100, cmap='jet')\n    plt.colorbar(cp)\n    if lines:\n        plt.axvline(true_alpha, linestyle='dashed', c='black')\n        plt.axhline(true_beta, linestyle='dashed', c='black')\n    plt.xlabel(xlab)\n    plt.ylabel(r'$\\beta$')\n"""
python/wtte/plots/weibull_heatmap.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom six.moves import xrange\n\nfrom wtte import weibull\n\n\ndef basic_heatmap(ax, pred, max_horizon=None, resolution=None, cmap=\'jet\'):\n    if max_horizon is None:\n        max_horizon = pred.shape[0]\n\n    if resolution is None:\n        resolution = max_horizon\n\n    ax.imshow(pred.T, origin=\'lower\', interpolation=\'none\',\n              aspect=\'auto\', cmap=cmap)\n    ax.set_yticks([x * (resolution + 0.0) /\n                   max_horizon for x in [0, max_horizon / 2, max_horizon - 1]])\n    ax.set_yticklabels([0, max_horizon / 2, max_horizon - 1])\n    ax.set_ylim(-0.5, resolution - 0.5)\n    ax.set_ylabel(\'steps ahead $s$\')\n    return ax\n\n\ndef weibull_heatmap(\n    fig, ax,\n    t,\n    a,\n    b,\n    max_horizon,\n    time_to_event=None,\n    true_time_to_event=None,\n    censoring_indicator=None,\n    title=\'predicted Weibull pmf $p(t,s)$\',\n    lw=1.0,\n    is_discrete=True,\n    resolution=None,\n    xax_nbins=10,\n    yax_nbins=4,\n    cmap=\'jet\'\n):\n    """"""\n        Adds a continuous or discrete heatmap with TTE to ax.\n\n        Caveats:\n        - axis are pixels so axis\'s always discrete.\n          (so we want location of labels to be in middle)\n    """"""\n    if resolution is None:\n        # Resolution. Defaults to 1/step. Want more with pdf.\n        resolution = max_horizon\n\n    # Discrete\n    if is_discrete:\n        prob_fun = weibull.pmf\n        drawstyle = \'steps-post\'\n    else:\n        prob_fun = weibull.pdf\n        # drawstyle defaults to straight line.\n        drawstyle = None\n\n    # Number of timesteps\n    n = len(t)\n\n    # no time to event\n    # no true time to event\n    # no is_censored\n    # all is_censored\n    # ok with true_time_to_event missing but not\n    # ok with true_\n\n    if time_to_event is not None:\n        if censoring_indicator is not None and true_time_to_event is None:\n            is_censored = np.array(censoring_indicator).astype(bool)\n        if true_time_to_event is not None:\n            is_censored = (time_to_event < true_time_to_event)\n        else:\n            true_time_to_event = np.ones_like(time_to_event)\n            true_time_to_event[:] = np.nan\n            true_time_to_event[~is_censored] = time_to_event[~is_censored]\n\n    assert len(t) == n\n    assert len(a) == n\n    assert len(b) == n\n    assert len(time_to_event) == n\n    assert len(true_time_to_event) == n\n    assert len(is_censored) == n\n\n    pred = prob_fun(\n        np.tile(np.linspace(0, max_horizon - 1, resolution), (n, 1)),\n        np.tile(a.reshape(n, 1), (1, resolution)),\n        np.tile(b.reshape(n, 1), (1, resolution))\n    )\n\n    ax = basic_heatmap(ax, pred, max_horizon, resolution,\n                       cmap=cmap)\n    ax.set_title(title)\n\n    def ax_add_scaled_line(ax, t, y, y_value_max, y_n_pixels, drawstyle,\n                           linestyle=\'solid\',\n                           color=\'black\',\n                           label=None):\n        # Shifts and scales y to fit on an imshow as we expect it to be, i.e\n        # passing through middle of a pixel\n        scaled_y = ((y_n_pixels + 0.0) / y_value_max) * y\n        ax.plot(t - 0.5, scaled_y, lw=lw, linestyle=linestyle,\n                drawstyle=drawstyle, color=color, label=label)\n        # Adds last segment of steps-post that gets missing\n        ax.plot([t[-1] - 0.5, t[-1] + 0.5], [scaled_y[-1], scaled_y[-1]],\n                lw=lw,\n                linestyle=linestyle,\n                drawstyle=drawstyle,\n                color=color)\n        ax.set_xlim(-0.5, n - 0.5)\n\n    if time_to_event is not None:\n        if not all(is_censored):\n            ax_add_scaled_line(ax,\n                               t,\n                               true_time_to_event,\n                               y_value_max=max_horizon,\n                               y_n_pixels=resolution,\n                               drawstyle=drawstyle,\n                               linestyle=\'solid\',\n                               color=\'black\',\n                               label=\'time_to_event\')\n        if not all(~is_censored):\n            ax_add_scaled_line(ax,\n                               t,\n                               time_to_event,\n                               y_value_max=max_horizon,\n                               y_n_pixels=resolution,\n                               drawstyle=drawstyle,\n                               linestyle=\'dotted\',\n                               color=\'black\',\n                               label=\'(censored)\')\n\n    ax.locator_params(axis=\'y\', nbins=4)\n    ax.locator_params(axis=\'x\', nbins=10)\n\n#     [ax.axvline(x=k+1,lw=0.1,c=\'gray\') for k in xrange(n-1)]\n\n#     for k in [0,1,2]:\n#         ax[k].set_xticks(ax[5].get_xticks()-0.5)\n#         ax[k].set_xticklabels(ax[5].get_xticks().astype(int))\n\n    ax.set_xlabel(\'time\')\n\n    fig.tight_layout()\n\n    return fig, ax\n'"
