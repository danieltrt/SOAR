file_path,api_count,code
predict-server.py,0,"b'import sys, time, logging, os, argparse\n\nimport numpy as np\nfrom PIL import Image, ImageGrab\nfrom socketserver import TCPServer, StreamRequestHandler\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nfrom train import create_model, is_valid_track_code, INPUT_WIDTH, INPUT_HEIGHT, INPUT_CHANNELS\n\ndef prepare_image(im):\n    im = im.resize((INPUT_WIDTH, INPUT_HEIGHT))\n    im_arr = np.frombuffer(im.tobytes(), dtype=np.uint8)\n    im_arr = im_arr.reshape((INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS))\n    im_arr = np.expand_dims(im_arr, axis=0)\n    return im_arr\n\nclass TCPHandler(StreamRequestHandler):\n    def handle(self):\n        if args.all:\n            weights_file = \'weights/all.hdf5\'\n            logger.info(""Loading {}..."".format(weights_file))\n            model.load_weights(weights_file)\n\n        logger.info(""Handling a new connection..."")\n        for line in self.rfile:\n            message = str(line.strip(),\'utf-8\')\n            logger.debug(message)\n\n            if message.startswith(""COURSE:"") and not args.all:\n                course = message[7:].strip().lower()\n                weights_file = \'weights/{}.hdf5\'.format(course)\n                logger.info(""Loading {}..."".format(weights_file))\n                model.load_weights(weights_file)\n\n            if message.startswith(""PREDICTFROMCLIPBOARD""):\n                im = ImageGrab.grabclipboard()\n                if im != None:\n                    prediction = model.predict(prepare_image(im), batch_size=1)[0]\n                    self.wfile.write((str(prediction[0]) + ""\\n"").encode(\'utf-8\'))\n                else:\n                    self.wfile.write(""PREDICTIONERROR\\n"".encode(\'utf-8\'))\n\n            if message.startswith(""PREDICT:""):\n                im = Image.open(message[8:])\n                prediction = model.predict(prepare_image(im), batch_size=1)[0]\n                self.wfile.write((str(prediction[0]) + ""\\n"").encode(\'utf-8\'))\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Start a prediction server that other apps will call into.\')\n    parser.add_argument(\'-a\', \'--all\', action=\'store_true\', help=\'Use the combined weights for all tracks, rather than selecting the weights file based off of the course code sent by the Play.lua script.\', default=False)\n    parser.add_argument(\'-p\', \'--port\', type=int, help=\'Port number\', default=36296)\n    parser.add_argument(\'-c\', \'--cpu\', action=\'store_true\', help=\'Force Tensorflow to use the CPU.\', default=False)\n    args = parser.parse_args()\n\n    if args.cpu:\n        os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\n        os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""\n\n    logger.info(""Loading model..."")\n    model = create_model(keep_prob=1)\n\n    if args.all:\n        model.load_weights(\'weights/all.hdf5\')\n\n    logger.info(""Starting server..."")\n    server = TCPServer((\'0.0.0.0\', args.port), TCPHandler)\n\n    print(""Listening on Port: {}"".format(server.server_address[1]))\n    sys.stdout.flush()\n    server.serve_forever()\n'"
train.py,0,"b'import glob\nimport os\nimport hashlib\nimport time\nimport argparse\nfrom mkdir_p import mkdir_p\n\nfrom PIL import Image\n\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\nimport matplotlib.pyplot as plt\n\nTRACK_CODES = set(map(lambda s: s.lower(),\n    [""ALL"", ""MR"",""CM"",""BC"",""BB"",""YV"",""FS"",""KTB"",""RRy"",""LR"",""MMF"",""TT"",""KD"",""SL"",""RRd"",""WS"",\n     ""BF"",""SS"",""DD"",""DK"",""BD"",""TC""]))\n\ndef is_valid_track_code(value):\n    value = value.lower()\n    if value not in TRACK_CODES:\n        raise argparse.ArgumentTypeError(""%s is an invalid track code"" % value)\n    return value\n\nOUT_SHAPE = 1\n\nINPUT_WIDTH = 200\nINPUT_HEIGHT = 66\nINPUT_CHANNELS = 3\n\nVALIDATION_SPLIT = 0.1\nUSE_REVERSE_IMAGES = False\n\ndef customized_loss(y_true, y_pred, loss=\'euclidean\'):\n    # Simply a mean squared error that penalizes large joystick summed values\n    if loss == \'L2\':\n        L2_norm_cost = 0.001\n        val = K.mean(K.square((y_pred - y_true)), axis=-1) \\\n            + K.sum(K.square(y_pred), axis=-1) / 2 * L2_norm_cost\n    # euclidean distance loss\n    elif loss == \'euclidean\':\n        val = K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))\n    return val\n\n\ndef create_model(keep_prob=0.6):\n    model = Sequential()\n\n    # NVIDIA\'s model\n    model.add(BatchNormalization(input_shape=(INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS)))\n    model.add(Conv2D(24, kernel_size=(5, 5), strides=(2, 2), activation=\'relu\'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(36, kernel_size=(5, 5), strides=(2, 2), activation=\'relu\'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(48, kernel_size=(5, 5), strides=(2, 2), activation=\'relu\'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size=(3, 3), activation=\'relu\'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size=(3, 3), activation=\'relu\'))\n    model.add(Flatten())\n    model.add(Dense(1164, activation=\'relu\'))\n    drop_out = 1 - keep_prob\n    model.add(Dropout(drop_out))\n    model.add(Dense(100, activation=\'relu\'))\n    model.add(Dropout(drop_out))\n    model.add(Dense(50, activation=\'relu\'))\n    model.add(Dropout(drop_out))\n    model.add(Dense(10, activation=\'relu\'))\n    model.add(Dropout(drop_out))\n    model.add(Dense(OUT_SHAPE, activation=\'softsign\', name=""predictions""))\n\n    return model\n\ndef is_validation_set(string):\n    string_hash = hashlib.md5(string.encode(\'utf-8\')).digest()\n    return int.from_bytes(string_hash[:2], byteorder=\'big\') / 2**16 > VALIDATION_SPLIT\n\ndef load_training_data(track):\n    X_train, y_train = [], []\n    X_val, y_val = [], []\n\n    if track == \'all\':\n        recordings = glob.iglob(""recordings/*/*/*"")\n    else:\n        recordings = glob.iglob(""recordings/{}/*/*"".format(track))\n\n    for recording in recordings:\n        filenames = list(glob.iglob(\'{}/*.png\'.format(recording)))\n        filenames.sort(key=lambda f: int(os.path.basename(f)[:-4]))\n\n        steering = [float(line) for line in open(\n            (""{}/steering.txt"").format(recording)).read().splitlines()]\n\n        assert len(filenames) == len(steering), ""For recording %s, the number of steering values does not match the number of images."" % recording\n\n        for file, steer in zip(filenames, steering):\n            assert steer >= -1 and steer <= 1\n\n            valid = is_validation_set(file)\n            valid_reversed = is_validation_set(file + \'_flipped\')\n\n            im = Image.open(file).resize((INPUT_WIDTH, INPUT_HEIGHT))\n            im_arr = np.frombuffer(im.tobytes(), dtype=np.uint8)\n            im_arr = im_arr.reshape((INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS))\n\n            if valid:\n                X_train.append(im_arr)\n                y_train.append(steer)\n            else:\n                X_val.append(im_arr)\n                y_val.append(steer)\n\n            if USE_REVERSE_IMAGES:\n                im_reverse = im.transpose(Image.FLIP_LEFT_RIGHT)\n                im_reverse_arr = np.frombuffer(im_reverse.tobytes(), dtype=np.uint8)\n                im_reverse_arr = im_reverse_arr.reshape((INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS))\n\n                if valid_reversed:\n                    X_train.append(im_reverse_arr)\n                    y_train.append(-steer)\n                else:\n                    X_val.append(im_reverse_arr)\n                    y_val.append(-steer)\n\n    assert len(X_train) == len(y_train)\n    assert len(X_val) == len(y_val)\n\n    return np.asarray(X_train), \\\n        np.asarray(y_train).reshape((len(y_train), 1)), \\\n        np.asarray(X_val), \\\n        np.asarray(y_val).reshape((len(y_val), 1))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'track\', type=is_valid_track_code)\n    parser.add_argument(\'-c\', \'--cpu\', action=\'store_true\', help=\'Force Tensorflow to use the CPU.\', default=False)\n    args = parser.parse_args()\n\n    if args.cpu:\n        os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\n        os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""\n\n    # Load Training Data\n    X_train, y_train, X_val, y_val = load_training_data(args.track)\n\n    print(X_train.shape[0], \'training samples.\')\n    print(X_val.shape[0], \'validation samples.\')\n\n    # Training loop variables\n    epochs = 100\n    batch_size = 50\n\n    model = create_model()\n\n    mkdir_p(""weights"")\n    weights_file = ""weights/{}.hdf5"".format(args.track)\n    if os.path.isfile(weights_file):\n        model.load_weights(weights_file)\n\n    model.compile(loss=customized_loss, optimizer=optimizers.adam(lr=0.0001))\n    checkpointer = ModelCheckpoint(\n        monitor=\'val_loss\', filepath=weights_file, verbose=1, save_best_only=True, mode=\'min\')\n    earlystopping = EarlyStopping(monitor=\'val_loss\', patience=20)\n    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n              shuffle=True, validation_data=(X_val, y_val), callbacks=[checkpointer, earlystopping])\n'"
tools/plot-path.py,0,"b'import requests\nfrom PIL import Image, ImageDraw\nfrom collections import namedtuple\n\nTrackData = namedtuple(\'TrackData\', [\'url\'])\n\nTRACK_DATA = {\n    \'CM\': TrackData(\'http://www.mariouniverse.com/images/maps/n64/mk/choco-mountain.jpg\'),\n    \'LR\': TrackData(\'http://www.mariouniverse.com/images/maps/n64/mk/luigi-raceway.jpg\'),\n    \'MR\': TrackData(\'http://www.mariouniverse.com/images/maps/n64/mk/mario-raceway.jpg\')\n}\n\nim = Image.open(requests.get(TRACK_DATA[\'CM\'].url, stream=True).raw)\n\nplay_positions = []\nsearch_trajectories = []\nsearch_trajectory = []\n\nfor line in open(\'choco.log\', \'r\'):\n    if line.startswith(""Play Position:""):\n        pos = tuple(map(float, line.strip().split(\'\\t\')[1:]))\n        play_positions.append(pos)\n\n    if line.startswith(""Search Position:""):\n        pos = tuple(map(float, line.strip().split(\'\\t\')[1:]))\n        search_trajectory.append(pos)\n    else:\n        if len(search_trajectory) > 0:\n            search_trajectories.append(search_trajectory)\n            search_trajectory = []\n\nIMG_START = (652, 518)\nSTART_POS = (-5.99742794036865, -675.072143554688, 47.0080032348633)\nPOINT_RADIUS = 6\nLINE_WIDTH = 4\n\ndraw = ImageDraw.Draw(im)\nSCALE = (0.67, 0.65)\ndef project(point):\n    return (IMG_START[0] + (point[0] - START_POS[0])*SCALE[0], IMG_START[1]  + (point[1] - START_POS[1])*SCALE[1])\n\nprev_point = play_positions[0]\nproj = project(prev_point)\ndraw.ellipse([(proj[0] - POINT_RADIUS, proj[1] - POINT_RADIUS), (proj[0] + POINT_RADIUS, proj[1] + POINT_RADIUS)], fill=(0,0,255,255))\nfor point in play_positions[1:]:\n    draw.line([project(prev_point), project(point)], width = LINE_WIDTH, fill=(0,0,255,255))\n    prev_point = point\n\nfor traj in search_trajectories:\n    prev_point = traj[0]\n\n    proj = project(prev_point)\n    draw.ellipse([(proj[0] - POINT_RADIUS, proj[1] - POINT_RADIUS), (proj[0] + POINT_RADIUS, proj[1] + POINT_RADIUS)], fill=(0,255,0,255))\n\n    for point in traj[1:]:\n        draw.line([project(prev_point), project(point)], width = LINE_WIDTH, fill=(0,255,0,255))\n        prev_point = point\n\nim.save(""composite.png"")\nim.show()\n'"
tools/plot-steering.py,0,"b'import argparse\nimport os\nimport sys\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.animation as animation\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        description=\'Replay a race with steering shown.\')\n    parser.add_argument(\'recording\')\n    args = parser.parse_args()\n\n    recording = args.recording\n    if not os.path.isdir(recording):\n        print(""{} is not a folder."".format(recording))\n        sys.exit(1)\n\n    steering = [float(line) for line in open(\n        (""{}/steering.txt"").format(recording)).read().splitlines()]\n\n    plt.title(""Recording: {}"".format(recording))\n    plt.plot(steering)\n    plt.show()\n'"
tools/remove-empty-recordings.py,0,"b'import glob, os\nfor recording in glob.iglob(""recordings/*/*/*""):\n    if os.listdir(recording) == [""steering.txt""]:\n        print(recording, ""is empty. Removing..."")\n        os.remove(os.path.join(recording, \'steering.txt\'))\n        os.rmdir(recording)\n'"
tools/replay-steering.py,0,"b'import argparse\nimport os\nimport sys\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.animation as animation\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        description=\'Replay a race with steering shown.\')\n    parser.add_argument(\'recording\')\n    args = parser.parse_args()\n\n    anim_running = True\n\n    def onclick(event):\n        global anim_running\n        if anim_running:\n            anim.event_source.stop()\n            anim_running = False\n        else:\n            anim.event_source.start()\n            anim_running = True\n\n    recording = args.recording\n    if not os.path.isdir(recording):\n        print(""{} is not a folder."".format(recording))\n        sys.exit(1)\n\n    steering = [float(line) for line in open(\n        (""{}/steering.txt"").format(recording)).read().splitlines()]\n\n    fig, ax = plt.subplots(2)\n    ax[1].plot(steering)\n    ax[1].set_ylim(-1, 1)\n\n    def animate(frameno):\n        if frameno > 0:\n            ax[0].imshow(mpimg.imread(""{}/{}.png"".format(recording, frameno)))\n            ax[1].set_xlim(0, frameno)\n\n    fig.canvas.mpl_connect(\'button_press_event\', onclick)\n    fig.suptitle(""Recording: {}"".format(recording))\n    anim = animation.FuncAnimation(\n        fig, animate, blit=False, interval=10, repeat=True)\n    plt.show()\n'"
tools/show-activations.py,0,"b'from PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport easygui\nimport keras.backend as K\n\nimport itertools\nitertools.izip = zip\n\ndef prepare_image(im):\n    im = im.resize((INPUT_WIDTH, INPUT_HEIGHT))\n    im_arr = np.frombuffer(im.tobytes(), dtype=np.uint8)\n    im_arr = im_arr.reshape((INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS))\n    return im_arr\n\nfrom train import create_model, INPUT_WIDTH, INPUT_HEIGHT, INPUT_CHANNELS\n\nfrom vis.utils import utils\nfrom vis.visualization import visualize_saliency, visualize_cam, get_num_filters, visualize_activation\n\nmodel = create_model(keep_prob=1)\nmodel.load_weights(\'weights/lr.hdf5\')\n\ndef get_activations(model, model_inputs, print_shape_only=False, layer_name=None):\n    import keras.backend as K\n    print(\'----- activations -----\')\n    activations = []\n    inp = model.input\n\n    model_multi_inputs_cond = True\n    if not isinstance(inp, list):\n        # only one input! let\'s wrap it in a list.\n        inp = [inp]\n        model_multi_inputs_cond = False\n\n    outputs = [layer.output for layer in model.layers if\n               layer.name == layer_name or layer_name is None]  # all layer outputs\n\n    funcs = [K.function(inp + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n\n    if model_multi_inputs_cond:\n        list_inputs = []\n        list_inputs.extend(model_inputs)\n        list_inputs.append(1.)\n    else:\n        list_inputs = [model_inputs, 1.]\n\n    # Learning phase. 1 = Test mode (no dropout or batch normalization)\n    # layer_outputs = [func([model_inputs, 1.])[0] for func in funcs]\n    layer_outputs = [func(list_inputs)[0] for func in funcs]\n    for layer_activations in layer_outputs:\n        activations.append(layer_activations)\n        if print_shape_only:\n            print(layer_activations.shape)\n        else:\n            print(layer_activations)\n    return activations\n\nim = prepare_image(Image.open(easygui.fileopenbox()))\nim_arr = np.expand_dims(im, axis=0)\nfor activation in get_activations(model, im_arr, print_shape_only=True, layer_name=""first_layer""):\n    activations = [activation[0, :, :, i] for i in range(24)]\n    im = np.vstack((\n        np.hstack(activations[:3]), np.hstack(activations[3:6]),\n        np.hstack(activations[6:9]), np.hstack(activations[9:12]),\n        np.hstack(activations[12:15]), np.hstack(activations[15:18]),\n        np.hstack(activations[18:21]), np.hstack(activations[21:24])\n    ))\n    im = np.expand_dims(im, axis=2)\n    plt.imshow(np.concatenate((im, im, im), axis=2))\n    plt.axis(\'off\')\n    plt.show()\n'"
tools/show-saliency.py,0,"b""import os.path, sys\nsys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir))\n\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport easygui\nimport keras.backend as K\n\nimport itertools; itertools.izip = zip\n\ndef prepare_image(im):\n    im = im.resize((INPUT_WIDTH, INPUT_HEIGHT))\n    im_arr = np.frombuffer(im.tobytes(), dtype=np.uint8)\n    im_arr = im_arr.reshape((INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS))\n    return im_arr\n\nfrom train import create_model, INPUT_WIDTH, INPUT_HEIGHT, INPUT_CHANNELS\n\nfrom vis.utils import utils\nfrom vis.visualization import visualize_saliency, visualize_cam, get_num_filters, visualize_activation\n\nmodel = create_model(keep_prob=1)\nmodel.load_weights('weights/lr.hdf5')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = 'predictions'\nlayer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n\nim = prepare_image(Image.open(easygui.fileopenbox()))\n\nsaliency = visualize_saliency(model, layer_idx, [0], im, alpha=0.7)\ncam = visualize_cam(model, layer_idx, [0], im, alpha=0.7)\n\nplt.figure()\nplt.subplot(211)\nplt.axis('off')\nplt.imshow(saliency)\nplt.title('Saliency Map')\n\nplt.subplot(212)\nplt.axis('off')\nplt.imshow(cam)\nplt.title('Class Activation Map')\n\nplt.show()\n"""
