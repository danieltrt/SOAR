file_path,api_count,code
setup.py,0,"b'import ann\nfrom setuptools import setup, find_packages\n\nVERSION = ann.__version__\n\nsetup(\n    name=""ann"",\n    version=VERSION,\n    packages=find_packages(),\n    install_requires=[\'numpy>=1.10.4\'],\n    author=""Sebastian Raschka"",\n    author_email=""mail@sebastianraschka.com"",\n    description=(""Supporting package for the book ""\n                 ""\'Introduction to Artificial Neural Networks ""\n                 ""and Deep Learning: ""\n                 ""A Practical Guide with Applications in Python\'""),\n    license=""MIT"",\n    keywords=[""artificial neural networks"", ""deep learning"",\n              ""machine learning"", ""artificial intelligence"", ""data science""],\n    classifiers=[\n         \'License :: OSI Approved :: MIT License\',\n         \'Operating System :: Microsoft :: Windows\',\n         \'Operating System :: POSIX\',\n         \'Operating System :: Unix\',\n         \'Operating System :: MacOS\',\n         \'Programming Language :: Python :: 2.7\',\n         \'Programming Language :: Python :: 3.5\',\n         \'Programming Language :: Python :: 3.6\',\n         \'Topic :: Scientific/Engineering\',\n         \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n         \'Topic :: Scientific/Engineering :: Information Analysis\',\n         \'Topic :: Scientific/Engineering :: Image Recognition\',\n    ],\n    url=""https://github.com/rasbt/deep-learning-book"")\n'"
code/tests/__init__.py,0,"b'# Sebastian Raschka 2016-2017\n#\n# ann is a supporting package for the book\n# ""Introduction to Artificial Neural Networks and Deep Learning:\n#  A Practical Guide with Applications in Python""\n#\n# Author: Sebastian Raschka <sebastianraschka.com>\n#\n# License: MIT\n'"
code/tests/test_notebooks.py,0,"b'import unittest\nimport os\nimport subprocess\nimport tempfile\nimport watermark\nimport nbformat\n\n\ndef run_ipynb(path):\n    with tempfile.NamedTemporaryFile(suffix="".ipynb"") as fout:\n        args = [""python"", ""-m"", ""nbconvert"", ""--to"",\n                ""notebook"", ""--execute"", ""--output"",\n                fout.name, path]\n        subprocess.check_output(args)\n\n\ndef run_ipynb2(path):\n    args = [""python"", ""-m"", ""nbconvert"", ""--to"",\n            ""notebook"", ""--execute"", path]\n    subprocess.check_output(args)\n\n\nclass TestNotebooks(unittest.TestCase):\n\n    def test_02_perceptron(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../ch02_perceptron/\'\n                               \'ch02_perceptron.ipynb\'))\n\n    def test_appendix_f_numpy_intro(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../appendix_f_numpy-intro/\'\n                               \'appendix_f_numpy-intro.ipynb\'))\n\n    def test_appendix_g_tensorflow_basics(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../appendix_g_tensorflow-basics/\'\n                               \'appendix_g_tensorflow-basics.ipynb\'))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
code/_old-material/ch02_perceptron/solutions/01_weight_zero_learning_rate.py,0,"b""small_lr_params = perceptron_train(X_train, y_train, learning_rate=0.01,\n                                   mparams=None, zero_weights=True)\n\nfor _ in range(2):\n    _ = perceptron_train(X_train, y_train, mparams=small_lr_params)\n\nx_min_small = -2\ny_min_small = (-(small_lr_params['weights'][0] * x_min) /\n               small_lr_params['weights'][1] -\n               (small_lr_params['bias'] / small_lr_params['weights'][1]))\n\nx_max_small = 2\ny_max_small = (-(small_lr_params['weights'][0] * x_max) /\n               small_lr_params['weights'][1] -\n               (small_lr_params['bias'] / small_lr_params['weights'][1]))\n\n\nfig, ax = plt.subplots(1, 2, sharex=True, figsize=(7, 3))\n\nax[0].plot([x_min, x_max], [y_min, y_max])\nax[1].plot([x_min_small, x_max_small], [y_min_small, y_max_small])\n\nax[0].scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n              label='class 0', marker='o')\nax[0].scatter(X_train[y_trai == 1, 0], X_train[y_train == 1, 1],\n              label='class 1', marker='s')\n\nax[1].scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n              label='class 0', marker='o')\nax[1].scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n              label='class 1', marker='s')\n\nax[1].legend(loc='lower right')\n\nplt.ylim([-3, 3])\nplt.xlim([-3, 3])\nplt.show()\n\n\nprint('Learning=1. rate params:', model_params)\nprint('Learning=0.01 rate params:', small_lr_params)\n\n# As we can see, lowering the learning rate changes the model parameters.\n# But if we look closely, we can see that in the case of the perceptron\n# the learning rate is just a scaling factor of the weight & bias vector\n# if we initialize the weights to zero. Therefore, the decision region\n# is exactly the same for different learning rates.\n"""
code/_old-material/ch02_perceptron/solutions/02_random_weights_learning_rate.py,0,"b""randw_params_1 = perceptron_train(X_train, y_train, learning_rate=1.0,\n                                  mparams=None, zero_weights=False)\n\nfor _ in range(2):\n    _ = perceptron_train(X_train, y_train, mparams=randw_params_1)\n\nx_min_1 = -2\ny_min_1 = (-(randw_params_1['weights'][0] * x_min) /\n           randw_params_1['weights'][1] -\n           (randw_params_1['bias'] / randw_params_1['weights'][1]))\n\nx_max_1 = 2\ny_max_1 = (-(randw_params_1['weights'][0] * x_max) /\n           randw_params_1['weights'][1] -\n           (randw_params_1['bias'] / randw_params_1['weights'][1]))\n\n\nrandw_params_2 = perceptron_train(X_train, y_train, learning_rate=0.01,\n                                  mparams=None, zero_weights=False)\n\nfor _ in range(2):\n    _ = perceptron_train(X_train, y_train, mparams=randw_params_2)\n\nx_min_2 = -2\ny_min_2 = (-(randw_params_2['weights'][0] * x_min) /\n           randw_params_2['weights'][1] -\n            (randw_params_2['bias'] / randw_params_2['weights'][1]))\n\nx_max_2 = 2\ny_max_2 = (-(randw_params_2['weights'][0] * x_max) /\n           randw_params_2['weights'][1] -\n            (randw_params_2['bias'] / randw_params_2['weights'][1]))\n\n\nfig, ax = plt.subplots(1, 2, sharex=True, figsize=(7, 3))\n\nax[0].plot([x_min_1, x_max_1], [y_min_1, y_max_1])\nax[1].plot([x_min_2, x_max_2], [y_min_2, y_max_2])\n\nax[0].scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n              label='class 0', marker='o')\nax[0].scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n              label='class 1', marker='s')\n\nax[1].scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n              label='class 0', marker='o')\nax[1].scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n              label='class 1', marker='s')\n\nax[1].legend(loc='lower right')\n\nplt.ylim([-3, 3])\nplt.xlim([-3, 3])\nplt.show()\n\n# As we can see now, random weight initialization breaks\n# the symmetry in the weight updates if we use\n# randomly initialized weights\n"""
code/_old-material/ch02_perceptron/solutions/03_tensorflow-boundary.py,0,"b""x_min = -2\ny_min = (-(modelparams['weights'][0] * x_min) / modelparams['weights'][1] -\n         (modelparams['bias'][0] / model_params['weights'][1]))\n\nx_max = 2\ny_max = (-(modelparams['weights'][0] * x_max) / modelparams['weights'][1] -\n         (modelparams['bias'][0] / modelparams['weights'][1]))\n\n\nfig, ax = plt.subplots(1, 2, sharex=True, figsize=(7, 3))\n\nax[0].plot([x_min, x_max], [y_min, y_max])\nax[1].plot([x_min, x_max], [y_min, y_max])\n\nax[0].scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n              label='class 0', marker='o')\nax[0].scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n              label='class 1', marker='s')\n\nax[1].scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1],\n              label='class 0', marker='o')\nax[1].scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1],\n              label='class 1', marker='s')\n\nax[1].legend(loc='upper left')\nplt.show()\n\n\n# The TensorFlow model performs better on the test set just by random chance.\n# Remember, the perceptron algorithm stops learning as soon as it classifies\n# the training set perfectly.\n# Possible explanations why there is a difference between the NumPy and\n# TensorFlow outcomes could thus be numerical precision, or slight differences\n# in our implementation.\n"""
