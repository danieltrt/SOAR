file_path,api_count,code
code/.convert_notebook_to_script.py,0,"b""# Simple helper script to convert\n# a Jupyter notebook to Python\n#\n# Sebastian Raschka, 2017\n\n\nimport argparse\nimport os\nimport subprocess\n\n\ndef convert(input_path, output_path):\n    subprocess.call(['jupyter', 'nbconvert', '--to', 'script',\n                     input_path, '--output', output_path])\n\n\ndef cleanup(path):\n\n    skip_lines_startwith = ('Image(filename=',\n                            '# In[',\n                            '# <hr>',\n                            'from IPython.display import Image',\n                            'get_ipython()',\n                            '# <br>')\n\n    clean_content = []\n    imports = []\n    existing_imports = set()\n    with open(path, 'r') as f:\n        next(f)\n        next(f)\n        for line in f:\n            line = line.rstrip(' ')\n            if line.startswith(skip_lines_startwith):\n                continue\n            if line.startswith('import ') or (\n                    'from ' in line and 'import ' in line):\n                if 'from __future__ import print_function' in line:\n                    if line != imports[0]:\n                        imports.insert(0, line)\n                else:\n                    if line.strip() not in existing_imports:\n                        imports.append(line)\n                        existing_imports.add(line.strip())\n            else:\n                clean_content.append(line)\n\n    clean_content = ['# coding: utf-8\\n\\n\\n'] + imports + clean_content\n\n    with open(path, 'w') as f:\n        for line in clean_content:\n            f.write(line)\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser(\n            description='Convert Jupyter notebook to Python script.',\n            formatter_class=argparse.RawTextHelpFormatter)\n\n    parser.add_argument('-i', '--input',\n                        required=True,\n                        help='Path to the Jupyter Notebook file')\n\n    parser.add_argument('-o', '--output',\n                        required=True,\n                        help='Path to the Python script file')\n\n    parser.add_argument('-v', '--version',\n                        action='version',\n                        version='v. 0.1')\n\n    args = parser.parse_args()\n\n    convert(input_path=args.input,\n            output_path=os.path.splitext(args.output)[0])\n\n    cleanup(args.output)\n"""
code/ch02/ch02.py,0,"b'# coding: utf-8\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# *Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 2 - Training Machine Learning Algorithms for Classification\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*\n\n# ### Overview\n# \n\n# - [Artificial neurons \xe2\x80\x93 a brief glimpse into the early history of machine learning](#Artificial-neurons-a-brief-glimpse-into-the-early-history-of-machine-learning)\n#     - [The formal definition of an artificial neuron](#The-formal-definition-of-an-artificial-neuron)\n#     - [The perceptron learning rule](#The-perceptron-learning-rule)\n# - [Implementing a perceptron learning algorithm in Python](#Implementing-a-perceptron-learning-algorithm-in-Python)\n#     - [An object-oriented perceptron API](#An-object-oriented-perceptron-API)\n#     - [Training a perceptron model on the Iris dataset](#Training-a-perceptron-model-on-the-Iris-dataset)\n# - [Adaptive linear neurons and the convergence of learning](#Adaptive-linear-neurons-and-the-convergence-of-learning)\n#     - [Minimizing cost functions with gradient descent](#Minimizing-cost-functions-with-gradient-descent)\n#     - [Implementing an Adaptive Linear Neuron in Python](#Implementing-an-Adaptive-Linear-Neuron-in-Python)\n#     - [Improving gradient descent through feature scaling](#Improving-gradient-descent-through-feature-scaling)\n#     - [Large scale machine learning and stochastic gradient descent](#Large-scale-machine-learning-and-stochastic-gradient-descent)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Artificial neurons - a brief glimpse into the early history of machine learning\n\n\n\n\n\n# ## The formal definition of an artificial neuron\n\n\n\n\n\n# ## The perceptron learning rule\n\n\n\n\n\n\n\n\n\n\n# # Implementing a perceptron learning algorithm in Python\n\n# ## An object-oriented perceptron API\n\n\n\n\n\nclass Perceptron(object):\n    """"""Perceptron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n    n_iter : int\n      Passes over the training dataset.\n    random_state : int\n      Random number generator seed for random weight\n      initialization.\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n    errors_ : list\n      Number of misclassifications (updates) in each epoch.\n\n    """"""\n    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        """"""Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_samples, n_features]\n          Training vectors, where n_samples is the number of samples and\n          n_features is the number of features.\n        y : array-like, shape = [n_samples]\n          Target values.\n\n        Returns\n        -------\n        self : object\n\n        """"""\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n        self.errors_ = []\n\n        for _ in range(self.n_iter):\n            errors = 0\n            for xi, target in zip(X, y):\n                update = self.eta * (target - self.predict(xi))\n                self.w_[1:] += update * xi\n                self.w_[0] += update\n                errors += int(update != 0.0)\n            self.errors_.append(errors)\n        return self\n\n    def net_input(self, X):\n        """"""Calculate net input""""""\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def predict(self, X):\n        """"""Return class label after unit step""""""\n        return np.where(self.net_input(X) >= 0.0, 1, -1)\n\n\n\n\nv1 = np.array([1, 2, 3])\nv2 = 0.5 * v1\nnp.arccos(v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n\n\n\n# ## Training a perceptron model on the Iris dataset\n\n# ...\n\n# ### Reading-in the Iris data\n\n\n\n\ndf = pd.read_csv(\'https://archive.ics.uci.edu/ml/\'\n        \'machine-learning-databases/iris/iris.data\', header=None)\ndf.tail()\n\n\n# \n# ### Note:\n# \n# \n# You can find a copy of the Iris dataset (and all other datasets used in this book) in the code bundle of this book, which you can use if you are working offline or the UCI server at https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data is temporarily unavailable. For instance, to load the Iris dataset from a local directory, you can replace the line \n# \n#     df = pd.read_csv(\'https://archive.ics.uci.edu/ml/\'\n#         \'machine-learning-databases/iris/iris.data\', header=None)\n#  \n# by\n#  \n#     df = pd.read_csv(\'your/local/path/to/iris.data\', header=None)\n# \n\n\n\ndf = pd.read_csv(\'iris.data\', header=None)\ndf.tail()\n\n\n\n\n# ### Plotting the Iris data\n\n\n\n\n# select setosa and versicolor\ny = df.iloc[0:100, 4].values\ny = np.where(y == \'Iris-setosa\', -1, 1)\n\n# extract sepal length and petal length\nX = df.iloc[0:100, [0, 2]].values\n\n# plot data\nplt.scatter(X[:50, 0], X[:50, 1],\n            color=\'red\', marker=\'o\', label=\'setosa\')\nplt.scatter(X[50:100, 0], X[50:100, 1],\n            color=\'blue\', marker=\'x\', label=\'versicolor\')\n\nplt.xlabel(\'sepal length [cm]\')\nplt.ylabel(\'petal length [cm]\')\nplt.legend(loc=\'upper left\')\n\n# plt.savefig(\'images/02_06.png\', dpi=300)\nplt.show()\n\n\n\n# ### Training the perceptron model\n\n\n\nppn = Perceptron(eta=0.1, n_iter=10)\n\nppn.fit(X, y)\n\nplt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=\'o\')\nplt.xlabel(\'Epochs\')\nplt.ylabel(\'Number of updates\')\n\n# plt.savefig(\'images/02_07.png\', dpi=300)\nplt.show()\n\n\n\n# ### A function for plotting decision regions\n\n\n\n\n\ndef plot_decision_regions(X, y, classifier, resolution=0.02):\n\n    # setup marker generator and color map\n    markers = (\'s\', \'x\', \'o\', \'^\', \'v\')\n    colors = (\'red\', \'blue\', \'lightgreen\', \'gray\', \'cyan\')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    # plot class samples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], \n                    y=X[y == cl, 1],\n                    alpha=0.8, \n                    c=colors[idx],\n                    marker=markers[idx], \n                    label=cl, \n                    edgecolor=\'black\')\n\n\n\n\nplot_decision_regions(X, y, classifier=ppn)\nplt.xlabel(\'sepal length [cm]\')\nplt.ylabel(\'petal length [cm]\')\nplt.legend(loc=\'upper left\')\n\n\n# plt.savefig(\'images/02_08.png\', dpi=300)\nplt.show()\n\n\n\n# # Adaptive linear neurons and the convergence of learning\n\n# ...\n\n# ## Minimizing cost functions with gradient descent\n\n\n\n\n\n\n\n\n\n\n# ## Implementing an adaptive linear neuron in Python\n\n\n\nclass AdalineGD(object):\n    """"""ADAptive LInear NEuron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n    n_iter : int\n      Passes over the training dataset.\n    random_state : int\n      Random number generator seed for random weight\n      initialization.\n\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n    cost_ : list\n      Sum-of-squares cost function value in each epoch.\n\n    """"""\n    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        """""" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_samples, n_features]\n          Training vectors, where n_samples is the number of samples and\n          n_features is the number of features.\n        y : array-like, shape = [n_samples]\n          Target values.\n\n        Returns\n        -------\n        self : object\n\n        """"""\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n        self.cost_ = []\n\n        for i in range(self.n_iter):\n            net_input = self.net_input(X)\n            # Please note that the ""activation"" method has no effect\n            # in the code since it is simply an identity function. We\n            # could write `output = self.net_input(X)` directly instead.\n            # The purpose of the activation is more conceptual, i.e.,  \n            # in the case of logistic regression (as we will see later), \n            # we could change it to\n            # a sigmoid function to implement a logistic regression classifier.\n            output = self.activation(net_input)\n            errors = (y - output)\n            self.w_[1:] += self.eta * X.T.dot(errors)\n            self.w_[0] += self.eta * errors.sum()\n            cost = (errors**2).sum() / 2.0\n            self.cost_.append(cost)\n        return self\n\n    def net_input(self, X):\n        """"""Calculate net input""""""\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def activation(self, X):\n        """"""Compute linear activation""""""\n        return X\n\n    def predict(self, X):\n        """"""Return class label after unit step""""""\n        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n\nada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)\nax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker=\'o\')\nax[0].set_xlabel(\'Epochs\')\nax[0].set_ylabel(\'log(Sum-squared-error)\')\nax[0].set_title(\'Adaline - Learning rate 0.01\')\n\nada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)\nax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker=\'o\')\nax[1].set_xlabel(\'Epochs\')\nax[1].set_ylabel(\'Sum-squared-error\')\nax[1].set_title(\'Adaline - Learning rate 0.0001\')\n\n# plt.savefig(\'images/02_11.png\', dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n# ## Improving gradient descent through feature scaling\n\n\n\n\n\n\n\n# standardize features\nX_std = np.copy(X)\nX_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()\nX_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()\n\n\n\n\nada = AdalineGD(n_iter=15, eta=0.01)\nada.fit(X_std, y)\n\nplot_decision_regions(X_std, y, classifier=ada)\nplt.title(\'Adaline - Gradient Descent\')\nplt.xlabel(\'sepal length [standardized]\')\nplt.ylabel(\'petal length [standardized]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n# plt.savefig(\'images/02_14_1.png\', dpi=300)\nplt.show()\n\nplt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker=\'o\')\nplt.xlabel(\'Epochs\')\nplt.ylabel(\'Sum-squared-error\')\n\nplt.tight_layout()\n# plt.savefig(\'images/02_14_2.png\', dpi=300)\nplt.show()\n\n\n\n# ## Large scale machine learning and stochastic gradient descent\n\n\n\nclass AdalineSGD(object):\n    """"""ADAptive LInear NEuron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n    n_iter : int\n      Passes over the training dataset.\n    shuffle : bool (default: True)\n      Shuffles training data every epoch if True to prevent cycles.\n    random_state : int\n      Random number generator seed for random weight\n      initialization.\n\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n    cost_ : list\n      Sum-of-squares cost function value averaged over all\n      training samples in each epoch.\n\n        \n    """"""\n    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.w_initialized = False\n        self.shuffle = shuffle\n        self.random_state = random_state\n        \n    def fit(self, X, y):\n        """""" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_samples, n_features]\n          Training vectors, where n_samples is the number of samples and\n          n_features is the number of features.\n        y : array-like, shape = [n_samples]\n          Target values.\n\n        Returns\n        -------\n        self : object\n\n        """"""\n        self._initialize_weights(X.shape[1])\n        self.cost_ = []\n        for i in range(self.n_iter):\n            if self.shuffle:\n                X, y = self._shuffle(X, y)\n            cost = []\n            for xi, target in zip(X, y):\n                cost.append(self._update_weights(xi, target))\n            avg_cost = sum(cost) / len(y)\n            self.cost_.append(avg_cost)\n        return self\n\n    def partial_fit(self, X, y):\n        """"""Fit training data without reinitializing the weights""""""\n        if not self.w_initialized:\n            self._initialize_weights(X.shape[1])\n        if y.ravel().shape[0] > 1:\n            for xi, target in zip(X, y):\n                self._update_weights(xi, target)\n        else:\n            self._update_weights(X, y)\n        return self\n\n    def _shuffle(self, X, y):\n        """"""Shuffle training data""""""\n        r = self.rgen.permutation(len(y))\n        return X[r], y[r]\n    \n    def _initialize_weights(self, m):\n        """"""Initialize weights to small random numbers""""""\n        self.rgen = np.random.RandomState(self.random_state)\n        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=1 + m)\n        self.w_initialized = True\n        \n    def _update_weights(self, xi, target):\n        """"""Apply Adaline learning rule to update the weights""""""\n        output = self.activation(self.net_input(xi))\n        error = (target - output)\n        self.w_[1:] += self.eta * xi.dot(error)\n        self.w_[0] += self.eta * error\n        cost = 0.5 * error**2\n        return cost\n    \n    def net_input(self, X):\n        """"""Calculate net input""""""\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def activation(self, X):\n        """"""Compute linear activation""""""\n        return X\n\n    def predict(self, X):\n        """"""Return class label after unit step""""""\n        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)\n\n\n\n\nada = AdalineSGD(n_iter=15, eta=0.01, random_state=1)\nada.fit(X_std, y)\n\nplot_decision_regions(X_std, y, classifier=ada)\nplt.title(\'Adaline - Stochastic Gradient Descent\')\nplt.xlabel(\'sepal length [standardized]\')\nplt.ylabel(\'petal length [standardized]\')\nplt.legend(loc=\'upper left\')\n\nplt.tight_layout()\n# plt.savefig(\'images/02_15_1.png\', dpi=300)\nplt.show()\n\nplt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker=\'o\')\nplt.xlabel(\'Epochs\')\nplt.ylabel(\'Average Cost\')\n\nplt.tight_layout()\n# plt.savefig(\'images/02_15_2.png\', dpi=300)\nplt.show()\n\n\n\n\nada.partial_fit(X_std[0, :], y[0])\n\n\n\n# # Summary\n\n# ...\n\n# --- \n# \n# Readers may ignore the following cell\n\n\n\n\n'"
code/ch03/ch03.py,0,"b'# coding: utf-8\n\n\nfrom sklearn import __version__ as sklearn_version\nfrom distutils.version import LooseVersion\nfrom sklearn import datasets\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom pydotplus import graph_from_dot_data\nfrom sklearn.tree import export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# *Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 3 - A Tour of Machine Learning Classifiers Using Scikit-Learn\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n\n\n\nif LooseVersion(sklearn_version) < LooseVersion(\'0.18\'):\n    raise ValueError(\'Please use scikit-learn 0.18 or newer\')\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*\n\n# ### Overview\n\n# - [Choosing a classification algorithm](#Choosing-a-classification-algorithm)\n# - [First steps with scikit-learn](#First-steps-with-scikit-learn)\n#     - [Training a perceptron via scikit-learn](#Training-a-perceptron-via-scikit-learn)\n# - [Modeling class probabilities via logistic regression](#Modeling-class-probabilities-via-logistic-regression)\n#     - [Logistic regression intuition and conditional probabilities](#Logistic-regression-intuition-and-conditional-probabilities)\n#     - [Learning the weights of the logistic cost function](#Learning-the-weights-of-the-logistic-cost-function)\n#     - [Training a logistic regression model with scikit-learn](#Training-a-logistic-regression-model-with-scikit-learn)\n#     - [Tackling overfitting via regularization](#Tackling-overfitting-via-regularization)\n# - [Maximum margin classification with support vector machines](#Maximum-margin-classification-with-support-vector-machines)\n#     - [Maximum margin intuition](#Maximum-margin-intuition)\n#     - [Dealing with the nonlinearly separable case using slack variables](#Dealing-with-the-nonlinearly-separable-case-using-slack-variables)\n#     - [Alternative implementations in scikit-learn](#Alternative-implementations-in-scikit-learn)\n# - [Solving nonlinear problems using a kernel SVM](#Solving-nonlinear-problems-using-a-kernel-SVM)\n#     - [Using the kernel trick to find separating hyperplanes in higher dimensional space](#Using-the-kernel-trick-to-find-separating-hyperplanes-in-higher-dimensional-space)\n# - [Decision tree learning](#Decision-tree-learning)\n#     - [Maximizing information gain \xe2\x80\x93 getting the most bang for the buck](#Maximizing-information-gain-\xe2\x80\x93-getting-the-most-bang-for-the-buck)\n#     - [Building a decision tree](#Building-a-decision-tree)\n#     - [Combining weak to strong learners via random forests](#Combining-weak-to-strong-learners-via-random-forests)\n# - [K-nearest neighbors \xe2\x80\x93 a lazy learning algorithm](#K-nearest-neighbors-\xe2\x80\x93-a-lazy-learning-algorithm)\n# - [Summary](#Summary)\n\n\n\n\n\n\n\n# # Choosing a classification algorithm\n\n# ...\n\n# # First steps with scikit-learn\n\n# Loading the Iris dataset from scikit-learn. Here, the third column represents the petal length, and the fourth column the petal width of the flower samples. The classes are already converted to integer labels where 0=Iris-Setosa, 1=Iris-Versicolor, 2=Iris-Virginica.\n\n\n\n\niris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\n\nprint(\'Class labels:\', np.unique(y))\n\n\n# Splitting data into 70% training and 30% test data:\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=1, stratify=y)\n\n\n\n\nprint(\'Labels counts in y:\', np.bincount(y))\nprint(\'Labels counts in y_train:\', np.bincount(y_train))\nprint(\'Labels counts in y_test:\', np.bincount(y_test))\n\n\n# Standardizing the features:\n\n\n\n\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n\n\n\n# ## Training a perceptron via scikit-learn\n\n# Redefining the `plot_decision_region` function from chapter 2:\n\n\n\n\nppn = Perceptron(n_iter=40, eta0=0.1, random_state=1)\nppn.fit(X_train_std, y_train)\n\n\n# **Note**\n# \n# - You can replace `Perceptron(n_iter, ...)` by `Perceptron(max_iter, ...)` in scikit-learn >= 0.19. The `n_iter` parameter is used here deriberately, because some people still use scikit-learn 0.18.\n\n\n\ny_pred = ppn.predict(X_test_std)\nprint(\'Misclassified samples: %d\' % (y_test != y_pred).sum())\n\n\n\n\n\nprint(\'Accuracy: %.2f\' % accuracy_score(y_test, y_pred))\n\n\n\n\nprint(\'Accuracy: %.2f\' % ppn.score(X_test_std, y_test))\n\n\n\n\n\n\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n\n    # setup marker generator and color map\n    markers = (\'s\', \'x\', \'o\', \'^\', \'v\')\n    colors = (\'red\', \'blue\', \'lightgreen\', \'gray\', \'cyan\')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], \n                    y=X[y == cl, 1],\n                    alpha=0.8, \n                    c=colors[idx],\n                    marker=markers[idx], \n                    label=cl, \n                    edgecolor=\'black\')\n\n    # highlight test samples\n    if test_idx:\n        # plot all samples\n        X_test, y_test = X[test_idx, :], y[test_idx]\n\n        plt.scatter(X_test[:, 0],\n                    X_test[:, 1],\n                    c=\'\',\n                    edgecolor=\'black\',\n                    alpha=1.0,\n                    linewidth=1,\n                    marker=\'o\',\n                    s=100, \n                    label=\'test set\')\n\n\n# Training a perceptron model using the standardized training data:\n\n\n\nX_combined_std = np.vstack((X_train_std, X_test_std))\ny_combined = np.hstack((y_train, y_test))\n\nplot_decision_regions(X=X_combined_std, y=y_combined,\n                      classifier=ppn, test_idx=range(105, 150))\nplt.xlabel(\'petal length [standardized]\')\nplt.ylabel(\'petal width [standardized]\')\nplt.legend(loc=\'upper left\')\n\nplt.tight_layout()\n#plt.savefig(\'images/03_01.png\', dpi=300)\nplt.show()\n\n\n\n# # Modeling class probabilities via logistic regression\n\n# ...\n\n# ### Logistic regression intuition and conditional probabilities\n\n\n\n\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\nz = np.arange(-7, 7, 0.1)\nphi_z = sigmoid(z)\n\nplt.plot(z, phi_z)\nplt.axvline(0.0, color=\'k\')\nplt.ylim(-0.1, 1.1)\nplt.xlabel(\'z\')\nplt.ylabel(\'$\\phi (z)$\')\n\n# y axis ticks and gridline\nplt.yticks([0.0, 0.5, 1.0])\nax = plt.gca()\nax.yaxis.grid(True)\n\nplt.tight_layout()\n#plt.savefig(\'images/03_02.png\', dpi=300)\nplt.show()\n\n\n\n\n\n\n\n# ### Learning the weights of the logistic cost function\n\n\n\ndef cost_1(z):\n    return - np.log(sigmoid(z))\n\n\ndef cost_0(z):\n    return - np.log(1 - sigmoid(z))\n\nz = np.arange(-10, 10, 0.1)\nphi_z = sigmoid(z)\n\nc1 = [cost_1(x) for x in z]\nplt.plot(phi_z, c1, label=\'J(w) if y=1\')\n\nc0 = [cost_0(x) for x in z]\nplt.plot(phi_z, c0, linestyle=\'--\', label=\'J(w) if y=0\')\n\nplt.ylim(0.0, 5.1)\nplt.xlim([0, 1])\nplt.xlabel(\'$\\phi$(z)\')\nplt.ylabel(\'J(w)\')\nplt.legend(loc=\'best\')\nplt.tight_layout()\n#plt.savefig(\'images/03_04.png\', dpi=300)\nplt.show()\n\n\n\n\nclass LogisticRegressionGD(object):\n    """"""Logistic Regression Classifier using gradient descent.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n    n_iter : int\n      Passes over the training dataset.\n    random_state : int\n      Random number generator seed for random weight\n      initialization.\n\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n    cost_ : list\n      Logistic cost function value in each epoch.\n\n    """"""\n    def __init__(self, eta=0.05, n_iter=100, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        """""" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_samples, n_features]\n          Training vectors, where n_samples is the number of samples and\n          n_features is the number of features.\n        y : array-like, shape = [n_samples]\n          Target values.\n\n        Returns\n        -------\n        self : object\n\n        """"""\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n        self.cost_ = []\n\n        for i in range(self.n_iter):\n            net_input = self.net_input(X)\n            output = self.activation(net_input)\n            errors = (y - output)\n            self.w_[1:] += self.eta * X.T.dot(errors)\n            self.w_[0] += self.eta * errors.sum()\n            \n            # note that we compute the logistic `cost` now\n            # instead of the sum of squared errors cost\n            cost = -y.dot(np.log(output)) - ((1 - y).dot(np.log(1 - output)))\n            self.cost_.append(cost)\n        return self\n    \n    def net_input(self, X):\n        """"""Calculate net input""""""\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def activation(self, z):\n        """"""Compute logistic sigmoid activation""""""\n        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n\n    def predict(self, X):\n        """"""Return class label after unit step""""""\n        return np.where(self.net_input(X) >= 0.0, 1, 0)\n        # equivalent to:\n        # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)\n\n\n\n\n\nX_train_01_subset = X_train[(y_train == 0) | (y_train == 1)]\ny_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]\n\nlrgd = LogisticRegressionGD(eta=0.05, n_iter=1000, random_state=1)\nlrgd.fit(X_train_01_subset,\n         y_train_01_subset)\n\nplot_decision_regions(X=X_train_01_subset, \n                      y=y_train_01_subset,\n                      classifier=lrgd)\n\nplt.xlabel(\'petal length [standardized]\')\nplt.ylabel(\'petal width [standardized]\')\nplt.legend(loc=\'upper left\')\n\nplt.tight_layout()\n#plt.savefig(\'images/03_05.png\', dpi=300)\nplt.show()\n\n\n# ### Training a logistic regression model with scikit-learn\n\n\n\n\nlr = LogisticRegression(C=100.0, random_state=1)\nlr.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, y_combined,\n                      classifier=lr, test_idx=range(105, 150))\nplt.xlabel(\'petal length [standardized]\')\nplt.ylabel(\'petal width [standardized]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n#plt.savefig(\'images/03_06.png\', dpi=300)\nplt.show()\n\n\n\n\nlr.predict_proba(X_test_std[:3, :])\n\n\n\n\nlr.predict_proba(X_test_std[:3, :]).sum(axis=1)\n\n\n\n\nlr.predict_proba(X_test_std[:3, :]).argmax(axis=1)\n\n\n\n\nlr.predict(X_test_std[:3, :])\n\n\n\n\nlr.predict(X_test_std[0, :].reshape(1, -1))\n\n\n\n# ### Tackling overfitting via regularization\n\n\n\n\n\n\n\nweights, params = [], []\nfor c in np.arange(-5, 5):\n    lr = LogisticRegression(C=10.**c, random_state=1)\n    lr.fit(X_train_std, y_train)\n    weights.append(lr.coef_[1])\n    params.append(10.**c)\n\nweights = np.array(weights)\nplt.plot(params, weights[:, 0],\n         label=\'petal length\')\nplt.plot(params, weights[:, 1], linestyle=\'--\',\n         label=\'petal width\')\nplt.ylabel(\'weight coefficient\')\nplt.xlabel(\'C\')\nplt.legend(loc=\'upper left\')\nplt.xscale(\'log\')\n#plt.savefig(\'images/03_08.png\', dpi=300)\nplt.show()\n\n\n\n# # Maximum margin classification with support vector machines\n\n\n\n\n\n# ## Maximum margin intuition\n\n# ...\n\n# ## Dealing with the nonlinearly separable case using slack variables\n\n\n\n\n\n\n\n\nsvm = SVC(kernel=\'linear\', C=1.0, random_state=1)\nsvm.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, \n                      y_combined,\n                      classifier=svm, \n                      test_idx=range(105, 150))\nplt.xlabel(\'petal length [standardized]\')\nplt.ylabel(\'petal width [standardized]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n#plt.savefig(\'images/03_11.png\', dpi=300)\nplt.show()\n\n\n# ## Alternative implementations in scikit-learn\n\n\n\n\nppn = SGDClassifier(loss=\'perceptron\', n_iter=1000)\nlr = SGDClassifier(loss=\'log\', n_iter=1000)\nsvm = SGDClassifier(loss=\'hinge\', n_iter=1000)\n\n\n# **Note**\n# \n# - You can replace `Perceptron(n_iter, ...)` by `Perceptron(max_iter, ...)` in scikit-learn >= 0.19. The `n_iter` parameter is used here deriberately, because some people still use scikit-learn 0.18.\n\n\n# # Solving non-linear problems using a kernel SVM\n\n\n\n\nnp.random.seed(1)\nX_xor = np.random.randn(200, 2)\ny_xor = np.logical_xor(X_xor[:, 0] > 0,\n                       X_xor[:, 1] > 0)\ny_xor = np.where(y_xor, 1, -1)\n\nplt.scatter(X_xor[y_xor == 1, 0],\n            X_xor[y_xor == 1, 1],\n            c=\'b\', marker=\'x\',\n            label=\'1\')\nplt.scatter(X_xor[y_xor == -1, 0],\n            X_xor[y_xor == -1, 1],\n            c=\'r\',\n            marker=\'s\',\n            label=\'-1\')\n\nplt.xlim([-3, 3])\nplt.ylim([-3, 3])\nplt.legend(loc=\'best\')\nplt.tight_layout()\n#plt.savefig(\'images/03_12.png\', dpi=300)\nplt.show()\n\n\n\n\n\n\n\n# ## Using the kernel trick to find separating hyperplanes in higher dimensional space\n\n\n\nsvm = SVC(kernel=\'rbf\', random_state=1, gamma=0.10, C=10.0)\nsvm.fit(X_xor, y_xor)\nplot_decision_regions(X_xor, y_xor,\n                      classifier=svm)\n\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n#plt.savefig(\'images/03_14.png\', dpi=300)\nplt.show()\n\n\n\n\n\nsvm = SVC(kernel=\'rbf\', random_state=1, gamma=0.2, C=1.0)\nsvm.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, y_combined,\n                      classifier=svm, test_idx=range(105, 150))\nplt.xlabel(\'petal length [standardized]\')\nplt.ylabel(\'petal width [standardized]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n#plt.savefig(\'images/03_15.png\', dpi=300)\nplt.show()\n\n\n\n\nsvm = SVC(kernel=\'rbf\', random_state=1, gamma=100.0, C=1.0)\nsvm.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, y_combined, \n                      classifier=svm, test_idx=range(105, 150))\nplt.xlabel(\'petal length [standardized]\')\nplt.ylabel(\'petal width [standardized]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n#plt.savefig(\'images/03_16.png\', dpi=300)\nplt.show()\n\n\n\n# # Decision tree learning\n\n\n\n\n\n\n\n\n\n\n# ## Maximizing information gain - getting the most bang for the buck\n\n\n\n\n\ndef gini(p):\n    return p * (1 - p) + (1 - p) * (1 - (1 - p))\n\n\ndef entropy(p):\n    return - p * np.log2(p) - (1 - p) * np.log2((1 - p))\n\n\ndef error(p):\n    return 1 - np.max([p, 1 - p])\n\nx = np.arange(0.0, 1.0, 0.01)\n\nent = [entropy(p) if p != 0 else None for p in x]\nsc_ent = [e * 0.5 if e else None for e in ent]\nerr = [error(i) for i in x]\n\nfig = plt.figure()\nax = plt.subplot(111)\nfor i, lab, ls, c, in zip([ent, sc_ent, gini(x), err], \n                          [\'Entropy\', \'Entropy (scaled)\', \n                           \'Gini Impurity\', \'Misclassification Error\'],\n                          [\'-\', \'-\', \'--\', \'-.\'],\n                          [\'black\', \'lightgray\', \'red\', \'green\', \'cyan\']):\n    line = ax.plot(x, i, label=lab, linestyle=ls, lw=2, color=c)\n\nax.legend(loc=\'upper center\', bbox_to_anchor=(0.5, 1.15),\n          ncol=5, fancybox=True, shadow=False)\n\nax.axhline(y=0.5, linewidth=1, color=\'k\', linestyle=\'--\')\nax.axhline(y=1.0, linewidth=1, color=\'k\', linestyle=\'--\')\nplt.ylim([0, 1.1])\nplt.xlabel(\'p(i=1)\')\nplt.ylabel(\'Impurity Index\')\n#plt.savefig(\'images/03_19.png\', dpi=300, bbox_inches=\'tight\')\nplt.show()\n\n\n\n# ## Building a decision tree\n\n\n\n\ntree = DecisionTreeClassifier(criterion=\'gini\', \n                              max_depth=4, \n                              random_state=1)\ntree.fit(X_train, y_train)\n\nX_combined = np.vstack((X_train, X_test))\ny_combined = np.hstack((y_train, y_test))\nplot_decision_regions(X_combined, y_combined, \n                      classifier=tree, test_idx=range(105, 150))\n\nplt.xlabel(\'petal length [cm]\')\nplt.ylabel(\'petal width [cm]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n#plt.savefig(\'images/03_20.png\', dpi=300)\nplt.show()\n\n\n\n\n\n\ndot_data = export_graphviz(tree,\n                           filled=True, \n                           rounded=True,\n                           class_names=[\'Setosa\', \n                                        \'Versicolor\',\n                                        \'Virginica\'],\n                           feature_names=[\'petal length\', \n                                          \'petal width\'],\n                           out_file=None) \ngraph = graph_from_dot_data(dot_data) \ngraph.write_png(\'tree.png\') \n\n\n\n\n\n\n\n# ## Combining weak to strong learners via random forests\n\n\n\n\nforest = RandomForestClassifier(criterion=\'gini\',\n                                n_estimators=25, \n                                random_state=1,\n                                n_jobs=2)\nforest.fit(X_train, y_train)\n\nplot_decision_regions(X_combined, y_combined, \n                      classifier=forest, test_idx=range(105, 150))\n\nplt.xlabel(\'petal length [cm]\')\nplt.ylabel(\'petal width [cm]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n#plt.savefig(\'images/03_22.png\', dpi=300)\nplt.show()\n\n\n\n# # K-nearest neighbors - a lazy learning algorithm\n\n\n\n\n\n\n\n\nknn = KNeighborsClassifier(n_neighbors=5, \n                           p=2, \n                           metric=\'minkowski\')\nknn.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, y_combined, \n                      classifier=knn, test_idx=range(105, 150))\n\nplt.xlabel(\'petal length [standardized]\')\nplt.ylabel(\'petal width [standardized]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n#plt.savefig(\'images/03_24.png\', dpi=300)\nplt.show()\n\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
code/ch04/ch04.py,0,"b'# coding: utf-8\n\n\nimport pandas as pd\nfrom io import StringIO\nimport sys\nfrom sklearn.preprocessing import Imputer\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.base import clone\nfrom itertools import combinations\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\n# *Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 4 - Building Good Training Sets \xe2\x80\x93\xc2\xa0Data Preprocessing\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*\n\n\n# ### Overview\n\n# - [Dealing with missing data](#Dealing-with-missing-data)\n#   - [Identifying missing values in tabular data](#Identifying-missing-values-in-tabular-data)\n#   - [Eliminating samples or features with missing values](#Eliminating-samples-or-features-with-missing-values)\n#   - [Imputing missing values](#Imputing-missing-values)\n#   - [Understanding the scikit-learn estimator API](#Understanding-the-scikit-learn-estimator-API)\n# - [Handling categorical data](#Handling-categorical-data)\n#   - [Nominal and ordinal features](#Nominal-and-ordinal-features)\n#   - [Mapping ordinal features](#Mapping-ordinal-features)\n#   - [Encoding class labels](#Encoding-class-labels)\n#   - [Performing one-hot encoding on nominal features](#Performing-one-hot-encoding-on-nominal-features)\n# - [Partitioning a dataset into a separate training and test set](#Partitioning-a-dataset-into-seperate-training-and-test-sets)\n# - [Bringing features onto the same scale](#Bringing-features-onto-the-same-scale)\n# - [Selecting meaningful features](#Selecting-meaningful-features)\n#   - [L1 and L2 regularization as penalties against model complexity](#L1-and-L2-regularization-as-penalties-against-model-omplexity)\n#   - [A geometric interpretation of L2 regularization](#A-geometric-interpretation-of-L2-regularization)\n#   - [Sparse solutions with L1 regularization](#Sparse-solutions-with-L1-regularization)\n#   - [Sequential feature selection algorithms](#Sequential-feature-selection-algorithms)\n# - [Assessing feature importance with Random Forests](#Assessing-feature-importance-with-Random-Forests)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Dealing with missing data\n\n# ## Identifying missing values in tabular data\n\n\n\n\ncsv_data = \'\'\'A,B,C,D\n1.0,2.0,3.0,4.0\n5.0,6.0,,8.0\n10.0,11.0,12.0,\'\'\'\n\n# If you are using Python 2.7, you need\n# to convert the string to unicode:\n\nif (sys.version_info < (3, 0)):\n    csv_data = unicode(csv_data)\n\ndf = pd.read_csv(StringIO(csv_data))\ndf\n\n\n\n\ndf.isnull().sum()\n\n\n\n\n# access the underlying NumPy array\n# via the `values` attribute\ndf.values\n\n\n\n# ## Eliminating samples or features with missing values\n\n\n\n# remove rows that contain missing values\n\ndf.dropna(axis=0)\n\n\n\n\n# remove columns that contain missing values\n\ndf.dropna(axis=1)\n\n\n\n\n# remove columns that contain missing values\n\ndf.dropna(axis=1)\n\n\n\n\n# only drop rows where all columns are NaN\n\ndf.dropna(how=\'all\')  \n\n\n\n\n# drop rows that have less than 3 real values \n\ndf.dropna(thresh=4)\n\n\n\n\n# only drop rows where NaN appear in specific columns (here: \'C\')\n\ndf.dropna(subset=[\'C\'])\n\n\n\n# ## Imputing missing values\n\n\n\n# again: our original array\ndf.values\n\n\n\n\n# impute missing values via the column mean\n\n\nimr = Imputer(missing_values=\'NaN\', strategy=\'mean\', axis=0)\nimr = imr.fit(df.values)\nimputed_data = imr.transform(df.values)\nimputed_data\n\n\n\n# ## Understanding the scikit-learn estimator API\n\n\n\n\n\n\n\n\n\n\n# # Handling categorical data\n\n# ## Nominal and ordinal features\n\n\n\n\ndf = pd.DataFrame([[\'green\', \'M\', 10.1, \'class2\'],\n                   [\'red\', \'L\', 13.5, \'class1\'],\n                   [\'blue\', \'XL\', 15.3, \'class2\']])\n\ndf.columns = [\'color\', \'size\', \'price\', \'classlabel\']\ndf\n\n\n\n# ## Mapping ordinal features\n\n\n\nsize_mapping = {\'XL\': 3,\n                \'L\': 2,\n                \'M\': 1}\n\ndf[\'size\'] = df[\'size\'].map(size_mapping)\ndf\n\n\n\n\ninv_size_mapping = {v: k for k, v in size_mapping.items()}\ndf[\'size\'].map(inv_size_mapping)\n\n\n\n# ## Encoding class labels\n\n\n\n\n# create a mapping dict\n# to convert class labels from strings to integers\nclass_mapping = {label: idx for idx, label in enumerate(np.unique(df[\'classlabel\']))}\nclass_mapping\n\n\n\n\n# to convert class labels from strings to integers\ndf[\'classlabel\'] = df[\'classlabel\'].map(class_mapping)\ndf\n\n\n\n\n# reverse the class label mapping\ninv_class_mapping = {v: k for k, v in class_mapping.items()}\ndf[\'classlabel\'] = df[\'classlabel\'].map(inv_class_mapping)\ndf\n\n\n\n\n\n# Label encoding with sklearn\'s LabelEncoder\nclass_le = LabelEncoder()\ny = class_le.fit_transform(df[\'classlabel\'].values)\ny\n\n\n\n\n# reverse mapping\nclass_le.inverse_transform(y)\n\n\n# Note: The deprecation warning shown above is due to an implementation detail in scikit-learn. It was already addressed in a pull request (https://github.com/scikit-learn/scikit-learn/pull/9816), and the patch will be released with the next version of scikit-learn (i.e., v. 0.20.0).\n\n\n# ## Performing one-hot encoding on nominal features\n\n\n\nX = df[[\'color\', \'size\', \'price\']].values\n\ncolor_le = LabelEncoder()\nX[:, 0] = color_le.fit_transform(X[:, 0])\nX\n\n\n\n\n\nohe = OneHotEncoder(categorical_features=[0])\nohe.fit_transform(X).toarray()\n\n\n\n\n# return dense array so that we can skip\n# the toarray step\n\nohe = OneHotEncoder(categorical_features=[0], sparse=False)\nohe.fit_transform(X)\n\n\n\n\n# one-hot encoding via pandas\n\npd.get_dummies(df[[\'price\', \'color\', \'size\']])\n\n\n\n\n# multicollinearity guard in get_dummies\n\npd.get_dummies(df[[\'price\', \'color\', \'size\']], drop_first=True)\n\n\n\n\n# multicollinearity guard for the OneHotEncoder\n\nohe = OneHotEncoder(categorical_features=[0])\nohe.fit_transform(X).toarray()[:, 1:]\n\n\n\n# # Partitioning a dataset into a seperate training and test set\n\n\n\ndf_wine = pd.read_csv(\'https://archive.ics.uci.edu/\'\n                      \'ml/machine-learning-databases/wine/wine.data\',\n                      header=None)\n\n# if the Wine dataset is temporarily unavailable from the\n# UCI machine learning repository, un-comment the following line\n# of code to load the dataset from a local path:\n\n# df_wine = pd.read_csv(\'wine.data\', header=None)\n\n\ndf_wine.columns = [\'Class label\', \'Alcohol\', \'Malic acid\', \'Ash\',\n                   \'Alcalinity of ash\', \'Magnesium\', \'Total phenols\',\n                   \'Flavanoids\', \'Nonflavanoid phenols\', \'Proanthocyanins\',\n                   \'Color intensity\', \'Hue\', \'OD280/OD315 of diluted wines\',\n                   \'Proline\']\n\nprint(\'Class labels\', np.unique(df_wine[\'Class label\']))\ndf_wine.head()\n\n\n\n\n\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n\nX_train, X_test, y_train, y_test =    train_test_split(X, y, \n                     test_size=0.3, \n                     random_state=0, \n                     stratify=y)\n\n\n\n# # Bringing features onto the same scale\n\n\n\n\nmms = MinMaxScaler()\nX_train_norm = mms.fit_transform(X_train)\nX_test_norm = mms.transform(X_test)\n\n\n\n\n\nstdsc = StandardScaler()\nX_train_std = stdsc.fit_transform(X_train)\nX_test_std = stdsc.transform(X_test)\n\n\n# A visual example:\n\n\n\nex = np.array([0, 1, 2, 3, 4, 5])\n\nprint(\'standardized:\', (ex - ex.mean()) / ex.std())\n\n# Please note that pandas uses ddof=1 (sample standard deviation) \n# by default, whereas NumPy\'s std method and the StandardScaler\n# uses ddof=0 (population standard deviation)\n\n# normalize\nprint(\'normalized:\', (ex - ex.min()) / (ex.max() - ex.min()))\n\n\n\n# # Selecting meaningful features\n\n# ...\n\n# ## L1 and L2 regularization as penalties against model complexity\n\n# ## A geometric interpretation of L2 regularization\n\n\n\n\n\n\n\n\n\n# ## Sparse solutions with L1-regularization\n\n\n\n\n\n# For regularized models in scikit-learn that support L1 regularization, we can simply set the `penalty` parameter to `\'l1\'` to obtain a sparse solution:\n\n\n\nLogisticRegression(penalty=\'l1\')\n\n\n# Applied to the standardized Wine data ...\n\n\n\n\nlr = LogisticRegression(penalty=\'l1\', C=1.0)\n# Note that C=1.0 is the default. You can increase\n# or decrease it to make the regulariztion effect\n# stronger or weaker, respectively.\nlr.fit(X_train_std, y_train)\nprint(\'Training accuracy:\', lr.score(X_train_std, y_train))\nprint(\'Test accuracy:\', lr.score(X_test_std, y_test))\n\n\n\n\nlr.intercept_\n\n\n\n\nnp.set_printoptions(8)\n\n\n\n\nlr.coef_[lr.coef_!=0].shape\n\n\n\n\nlr.coef_\n\n\n\n\n\nfig = plt.figure()\nax = plt.subplot(111)\n    \ncolors = [\'blue\', \'green\', \'red\', \'cyan\', \n          \'magenta\', \'yellow\', \'black\', \n          \'pink\', \'lightgreen\', \'lightblue\', \n          \'gray\', \'indigo\', \'orange\']\n\nweights, params = [], []\nfor c in np.arange(-4., 6.):\n    lr = LogisticRegression(penalty=\'l1\', C=10.**c, random_state=0)\n    lr.fit(X_train_std, y_train)\n    weights.append(lr.coef_[1])\n    params.append(10**c)\n\nweights = np.array(weights)\n\nfor column, color in zip(range(weights.shape[1]), colors):\n    plt.plot(params, weights[:, column],\n             label=df_wine.columns[column + 1],\n             color=color)\nplt.axhline(0, color=\'black\', linestyle=\'--\', linewidth=3)\nplt.xlim([10**(-5), 10**5])\nplt.ylabel(\'weight coefficient\')\nplt.xlabel(\'C\')\nplt.xscale(\'log\')\nplt.legend(loc=\'upper left\')\nax.legend(loc=\'upper center\', \n          bbox_to_anchor=(1.38, 1.03),\n          ncol=1, fancybox=True)\n#plt.savefig(\'images/04_07.png\', dpi=300, \n#            bbox_inches=\'tight\', pad_inches=0.2)\nplt.show()\n\n\n\n# ## Sequential feature selection algorithms\n\n\n\n\n\nclass SBS():\n    def __init__(self, estimator, k_features, scoring=accuracy_score,\n                 test_size=0.25, random_state=1):\n        self.scoring = scoring\n        self.estimator = clone(estimator)\n        self.k_features = k_features\n        self.test_size = test_size\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \n        X_train, X_test, y_train, y_test =             train_test_split(X, y, test_size=self.test_size,\n                             random_state=self.random_state)\n\n        dim = X_train.shape[1]\n        self.indices_ = tuple(range(dim))\n        self.subsets_ = [self.indices_]\n        score = self._calc_score(X_train, y_train, \n                                 X_test, y_test, self.indices_)\n        self.scores_ = [score]\n\n        while dim > self.k_features:\n            scores = []\n            subsets = []\n\n            for p in combinations(self.indices_, r=dim - 1):\n                score = self._calc_score(X_train, y_train, \n                                         X_test, y_test, p)\n                scores.append(score)\n                subsets.append(p)\n\n            best = np.argmax(scores)\n            self.indices_ = subsets[best]\n            self.subsets_.append(self.indices_)\n            dim -= 1\n\n            self.scores_.append(scores[best])\n        self.k_score_ = self.scores_[-1]\n\n        return self\n\n    def transform(self, X):\n        return X[:, self.indices_]\n\n    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n        self.estimator.fit(X_train[:, indices], y_train)\n        y_pred = self.estimator.predict(X_test[:, indices])\n        score = self.scoring(y_test, y_pred)\n        return score\n\n\n\n\n\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# selecting features\nsbs = SBS(knn, k_features=1)\nsbs.fit(X_train_std, y_train)\n\n# plotting performance of feature subsets\nk_feat = [len(k) for k in sbs.subsets_]\n\nplt.plot(k_feat, sbs.scores_, marker=\'o\')\nplt.ylim([0.7, 1.02])\nplt.ylabel(\'Accuracy\')\nplt.xlabel(\'Number of features\')\nplt.grid()\nplt.tight_layout()\n# plt.savefig(\'images/04_08.png\', dpi=300)\nplt.show()\n\n\n\n\nk3 = list(sbs.subsets_[10])\nprint(df_wine.columns[1:][k3])\n\n\n\n\nknn.fit(X_train_std, y_train)\nprint(\'Training accuracy:\', knn.score(X_train_std, y_train))\nprint(\'Test accuracy:\', knn.score(X_test_std, y_test))\n\n\n\n\nknn.fit(X_train_std[:, k3], y_train)\nprint(\'Training accuracy:\', knn.score(X_train_std[:, k3], y_train))\nprint(\'Test accuracy:\', knn.score(X_test_std[:, k3], y_test))\n\n\n\n# # Assessing feature importance with Random Forests\n\n\n\n\nfeat_labels = df_wine.columns[1:]\n\nforest = RandomForestClassifier(n_estimators=500,\n                                random_state=1)\n\nforest.fit(X_train, y_train)\nimportances = forest.feature_importances_\n\nindices = np.argsort(importances)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(""%2d) %-*s %f"" % (f + 1, 30, \n                            feat_labels[indices[f]], \n                            importances[indices[f]]))\n\nplt.title(\'Feature Importance\')\nplt.bar(range(X_train.shape[1]), \n        importances[indices],\n        align=\'center\')\n\nplt.xticks(range(X_train.shape[1]), \n           feat_labels[indices], rotation=90)\nplt.xlim([-1, X_train.shape[1]])\nplt.tight_layout()\n#plt.savefig(\'images/04_09.png\', dpi=300)\nplt.show()\n\n\n\n\n\nsfm = SelectFromModel(forest, threshold=0.1, prefit=True)\nX_selected = sfm.transform(X_train)\nprint(\'Number of features that meet this threshold criterion:\', \n      X_selected.shape[1])\n\n\n# Now, let\'s print the 3 features that met the threshold criterion for feature selection that we set earlier (note that this code snippet does not appear in the actual book but was added to this notebook later for illustrative purposes):\n\n\n\nfor f in range(X_selected.shape[1]):\n    print(""%2d) %-*s %f"" % (f + 1, 30, \n                            feat_labels[indices[f]], \n                            importances[indices[f]]))\n\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
code/ch05/ch05.py,0,"b'# coding: utf-8\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy import exp\nfrom scipy.linalg import eigh\nfrom sklearn.datasets import make_moons\nfrom sklearn.datasets import make_circles\nfrom sklearn.decomposition import KernelPCA\n\n# *Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 5 - Compressing Data via Dimensionality Reduction\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*\n\n\n# ### Overview\n\n# - [Unsupervised dimensionality reduction via principal component analysis 128](#Unsupervised-dimensionality-reduction-via-principal-component-analysis-128)\n#   - [The main steps behind principal component analysis](#The-main-steps-behind-principal-component-analysis)\n#   - [Extracting the principal components step-by-step](#Extracting-the-principal-components-step-by-step)\n#   - [Total and explained variance](#Total-and-explained-variance)\n#   - [Feature transformation](#Feature-transformation)\n#   - [Principal component analysis in scikit-learn](#Principal-component-analysis-in-scikit-learn)\n# - [Supervised data compression via linear discriminant analysis](#Supervised-data-compression-via-linear-discriminant-analysis)\n#   - [Principal component analysis versus linear discriminant analysis](#Principal-component-analysis-versus-linear-discriminant-analysis)\n#   - [The inner workings of linear discriminant analysis](#The-inner-workings-of-linear-discriminant-analysis)\n#   - [Computing the scatter matrices](#Computing-the-scatter-matrices)\n#   - [Selecting linear discriminants for the new feature subspace](#Selecting-linear-discriminants-for-the-new-feature-subspace)\n#   - [Projecting samples onto the new feature space](#Projecting-samples-onto-the-new-feature-space)\n#   - [LDA via scikit-learn](#LDA-via-scikit-learn)\n# - [Using kernel principal component analysis for nonlinear mappings](#Using-kernel-principal-component-analysis-for-nonlinear-mappings)\n#   - [Kernel functions and the kernel trick](#Kernel-functions-and-the-kernel-trick)\n#   - [Implementing a kernel principal component analysis in Python](#Implementing-a-kernel-principal-component-analysis-in-Python)\n#     - [Example 1 \xe2\x80\x93 separating half-moon shapes](#Example-1:-Separating-half-moon-shapes)\n#     - [Example 2 \xe2\x80\x93 separating concentric circles](#Example-2:-Separating-concentric-circles)\n#   - [Projecting new data points](#Projecting-new-data-points)\n#   - [Kernel principal component analysis in scikit-learn](#Kernel-principal-component-analysis-in-scikit-learn)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Unsupervised dimensionality reduction via principal component analysis\n\n# ## The main steps behind principal component analysis\n\n\n\n\n\n# ## Extracting the principal components step-by-step\n\n\n\n\ndf_wine = pd.read_csv(\'https://archive.ics.uci.edu/ml/\'\n                      \'machine-learning-databases/wine/wine.data\',\n                      header=None)\n\n# if the Wine dataset is temporarily unavailable from the\n# UCI machine learning repository, un-comment the following line\n# of code to load the dataset from a local path:\n\n# df_wine = pd.read_csv(\'wine.data\', header=None)\n\ndf_wine.columns = [\'Class label\', \'Alcohol\', \'Malic acid\', \'Ash\',\n                   \'Alcalinity of ash\', \'Magnesium\', \'Total phenols\',\n                   \'Flavanoids\', \'Nonflavanoid phenols\', \'Proanthocyanins\',\n                   \'Color intensity\', \'Hue\',\n                   \'OD280/OD315 of diluted wines\', \'Proline\']\n\ndf_wine.head()\n\n\n\n# Splitting the data into 70% training and 30% test subsets.\n\n\n\n\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n\nX_train, X_test, y_train, y_test =     train_test_split(X, y, test_size=0.3, \n                     stratify=y,\n                     random_state=0)\n\n\n# Standardizing the data.\n\n\n\n\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)\n\n\n# ---\n# \n# **Note**\n# \n# Accidentally, I wrote `X_test_std = sc.fit_transform(X_test)` instead of `X_test_std = sc.transform(X_test)`. In this case, it wouldn\'t make a big difference since the mean and standard deviation of the test set should be (quite) similar to the training set. However, as remember from Chapter 3, the correct way is to re-use parameters from the training set if we are doing any kind of transformation -- the test set should basically stand for ""new, unseen"" data.\n# \n# My initial typo reflects a common mistake is that some people are *not* re-using these parameters from the model training/building and standardize the new data ""from scratch."" Here\'s simple example to explain why this is a problem.\n# \n# Let\'s assume we have a simple training set consisting of 3 samples with 1 feature (let\'s call this feature ""length""):\n# \n# - train_1: 10 cm -> class_2\n# - train_2: 20 cm -> class_2\n# - train_3: 30 cm -> class_1\n# \n# mean: 20, std.: 8.2\n# \n# After standardization, the transformed feature values are\n# \n# - train_std_1: -1.21 -> class_2\n# - train_std_2: 0 -> class_2\n# - train_std_3: 1.21 -> class_1\n# \n# Next, let\'s assume our model has learned to classify samples with a standardized length value < 0.6 as class_2 (class_1 otherwise). So far so good. Now, let\'s say we have 3 unlabeled data points that we want to classify:\n# \n# - new_4: 5 cm -> class ?\n# - new_5: 6 cm -> class ?\n# - new_6: 7 cm -> class ?\n# \n# If we look at the ""unstandardized ""length"" values in our training datast, it is intuitive to say that all of these samples are likely belonging to class_2. However, if we standardize these by re-computing standard deviation and and mean you would get similar values as before in the training set and your classifier would (probably incorrectly) classify samples 4 and 5 as class 2.\n# \n# - new_std_4: -1.21 -> class 2\n# - new_std_5: 0 -> class 2\n# - new_std_6: 1.21 -> class 1\n# \n# However, if we use the parameters from your ""training set standardization,"" we\'d get the values:\n# \n# - sample5: -18.37 -> class 2\n# - sample6: -17.15 -> class 2\n# - sample7: -15.92 -> class 2\n# \n# The values 5 cm, 6 cm, and 7 cm are much lower than anything we have seen in the training set previously. Thus, it only makes sense that the standardized features of the ""new samples"" are much lower than every standardized feature in the training set.\n# \n# ---\n\n# Eigendecomposition of the covariance matrix.\n\n\n\ncov_mat = np.cov(X_train_std.T)\neigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n\nprint(\'\\nEigenvalues \\n%s\' % eigen_vals)\n\n\n# **Note**: \n# \n# Above, I used the [`numpy.linalg.eig`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html) function to decompose the symmetric covariance matrix into its eigenvalues and eigenvectors.\n#     <pre>>>> eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)</pre>\n#     This is not really a ""mistake,"" but probably suboptimal. It would be better to use [`numpy.linalg.eigh`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html) in such cases, which has been designed for [Hermetian matrices](https://en.wikipedia.org/wiki/Hermitian_matrix). The latter always returns real  eigenvalues; whereas the numerically less stable `np.linalg.eig` can decompose nonsymmetric square matrices, you may find that it returns complex eigenvalues in certain cases. (S.R.)\n# \n\n\n# ## Total and explained variance\n\n\n\ntot = sum(eigen_vals)\nvar_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\n\n\n\n\n\n\nplt.bar(range(1, 14), var_exp, alpha=0.5, align=\'center\',\n        label=\'individual explained variance\')\nplt.step(range(1, 14), cum_var_exp, where=\'mid\',\n         label=\'cumulative explained variance\')\nplt.ylabel(\'Explained variance ratio\')\nplt.xlabel(\'Principal component index\')\nplt.legend(loc=\'best\')\nplt.tight_layout()\n# plt.savefig(\'images/05_02.png\', dpi=300)\nplt.show()\n\n\n\n# ## Feature transformation\n\n\n\n# Make a list of (eigenvalue, eigenvector) tuples\neigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n               for i in range(len(eigen_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neigen_pairs.sort(key=lambda k: k[0], reverse=True)\n\n\n\n\nw = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n               eigen_pairs[1][1][:, np.newaxis]))\nprint(\'Matrix W:\\n\', w)\n\n\n# **Note**\n# Depending on which version of NumPy and LAPACK you are using, you may obtain the Matrix W with its signs flipped. Please note that this is not an issue: If $v$ is an eigenvector of a matrix $\\Sigma$, we have\n# \n# $$\\Sigma v = \\lambda v,$$\n# \n# where $\\lambda$ is our eigenvalue,\n# \n# \n# then $-v$ is also an eigenvector that has the same eigenvalue, since\n# $$\\Sigma \\cdot (-v) = -\\Sigma v = -\\lambda v = \\lambda \\cdot (-v).$$\n\n\n\nX_train_std[0].dot(w)\n\n\n\n\nX_train_pca = X_train_std.dot(w)\ncolors = [\'r\', \'b\', \'g\']\nmarkers = [\'s\', \'x\', \'o\']\n\nfor l, c, m in zip(np.unique(y_train), colors, markers):\n    plt.scatter(X_train_pca[y_train == l, 0], \n                X_train_pca[y_train == l, 1], \n                c=c, label=l, marker=m)\n\nplt.xlabel(\'PC 1\')\nplt.ylabel(\'PC 2\')\nplt.legend(loc=\'lower left\')\nplt.tight_layout()\n# plt.savefig(\'images/05_03.png\', dpi=300)\nplt.show()\n\n\n\n# ## Principal component analysis in scikit-learn\n\n# **NOTE**\n# \n# The following four code cells has been added in addition to the content to the book, to illustrate how to replicate the results from our own PCA implementation in scikit-learn:\n\n\n\n\npca = PCA()\nX_train_pca = pca.fit_transform(X_train_std)\npca.explained_variance_ratio_\n\n\n\n\nplt.bar(range(1, 14), pca.explained_variance_ratio_, alpha=0.5, align=\'center\')\nplt.step(range(1, 14), np.cumsum(pca.explained_variance_ratio_), where=\'mid\')\nplt.ylabel(\'Explained variance ratio\')\nplt.xlabel(\'Principal components\')\n\nplt.show()\n\n\n\n\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\n\n\n\n\nplt.scatter(X_train_pca[:, 0], X_train_pca[:, 1])\nplt.xlabel(\'PC 1\')\nplt.ylabel(\'PC 2\')\nplt.show()\n\n\n\n\n\ndef plot_decision_regions(X, y, classifier, resolution=0.02):\n\n    # setup marker generator and color map\n    markers = (\'s\', \'x\', \'o\', \'^\', \'v\')\n    colors = (\'red\', \'blue\', \'lightgreen\', \'gray\', \'cyan\')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    # plot class samples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], \n                    y=X[y == cl, 1],\n                    alpha=0.6, \n                    c=cmap(idx),\n                    edgecolor=\'black\',\n                    marker=markers[idx], \n                    label=cl)\n\n\n# Training logistic regression classifier using the first 2 principal components.\n\n\n\n\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\n\nlr = LogisticRegression()\nlr = lr.fit(X_train_pca, y_train)\n\n\n\n\nplot_decision_regions(X_train_pca, y_train, classifier=lr)\nplt.xlabel(\'PC 1\')\nplt.ylabel(\'PC 2\')\nplt.legend(loc=\'lower left\')\nplt.tight_layout()\n# plt.savefig(\'images/05_04.png\', dpi=300)\nplt.show()\n\n\n\n\nplot_decision_regions(X_test_pca, y_test, classifier=lr)\nplt.xlabel(\'PC 1\')\nplt.ylabel(\'PC 2\')\nplt.legend(loc=\'lower left\')\nplt.tight_layout()\n# plt.savefig(\'images/05_05.png\', dpi=300)\nplt.show()\n\n\n\n\npca = PCA(n_components=None)\nX_train_pca = pca.fit_transform(X_train_std)\npca.explained_variance_ratio_\n\n\n\n# # Supervised data compression via linear discriminant analysis\n\n# ## Principal component analysis versus linear discriminant analysis\n\n\n\n\n\n# ## The inner workings of linear discriminant analysis\n\n\n# ## Computing the scatter matrices\n\n# Calculate the mean vectors for each class:\n\n\n\nnp.set_printoptions(precision=4)\n\nmean_vecs = []\nfor label in range(1, 4):\n    mean_vecs.append(np.mean(X_train_std[y_train == label], axis=0))\n    print(\'MV %s: %s\\n\' % (label, mean_vecs[label - 1]))\n\n\n# Compute the within-class scatter matrix:\n\n\n\nd = 13 # number of features\nS_W = np.zeros((d, d))\nfor label, mv in zip(range(1, 4), mean_vecs):\n    class_scatter = np.zeros((d, d))  # scatter matrix for each class\n    for row in X_train_std[y_train == label]:\n        row, mv = row.reshape(d, 1), mv.reshape(d, 1)  # make column vectors\n        class_scatter += (row - mv).dot((row - mv).T)\n    S_W += class_scatter                          # sum class scatter matrices\n\nprint(\'Within-class scatter matrix: %sx%s\' % (S_W.shape[0], S_W.shape[1]))\n\n\n# Better: covariance matrix since classes are not equally distributed:\n\n\n\nprint(\'Class label distribution: %s\' \n      % np.bincount(y_train)[1:])\n\n\n\n\nd = 13  # number of features\nS_W = np.zeros((d, d))\nfor label, mv in zip(range(1, 4), mean_vecs):\n    class_scatter = np.cov(X_train_std[y_train == label].T)\n    S_W += class_scatter\nprint(\'Scaled within-class scatter matrix: %sx%s\' % (S_W.shape[0],\n                                                     S_W.shape[1]))\n\n\n# Compute the between-class scatter matrix:\n\n\n\nmean_overall = np.mean(X_train_std, axis=0)\nd = 13  # number of features\nS_B = np.zeros((d, d))\nfor i, mean_vec in enumerate(mean_vecs):\n    n = X_train[y_train == i + 1, :].shape[0]\n    mean_vec = mean_vec.reshape(d, 1)  # make column vector\n    mean_overall = mean_overall.reshape(d, 1)  # make column vector\n    S_B += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)\n\nprint(\'Between-class scatter matrix: %sx%s\' % (S_B.shape[0], S_B.shape[1]))\n\n\n\n# ## Selecting linear discriminants for the new feature subspace\n\n# Solve the generalized eigenvalue problem for the matrix $S_W^{-1}S_B$:\n\n\n\neigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n\n\n# **Note**:\n#     \n# Above, I used the [`numpy.linalg.eig`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html) function to decompose the symmetric covariance matrix into its eigenvalues and eigenvectors.\n#     <pre>>>> eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)</pre>\n#     This is not really a ""mistake,"" but probably suboptimal. It would be better to use [`numpy.linalg.eigh`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html) in such cases, which has been designed for [Hermetian matrices](https://en.wikipedia.org/wiki/Hermitian_matrix). The latter always returns real  eigenvalues; whereas the numerically less stable `np.linalg.eig` can decompose nonsymmetric square matrices, you may find that it returns complex eigenvalues in certain cases. (S.R.)\n# \n\n# Sort eigenvectors in descending order of the eigenvalues:\n\n\n\n# Make a list of (eigenvalue, eigenvector) tuples\neigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n               for i in range(len(eigen_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neigen_pairs = sorted(eigen_pairs, key=lambda k: k[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n\nprint(\'Eigenvalues in descending order:\\n\')\nfor eigen_val in eigen_pairs:\n    print(eigen_val[0])\n\n\n\n\ntot = sum(eigen_vals.real)\ndiscr = [(i / tot) for i in sorted(eigen_vals.real, reverse=True)]\ncum_discr = np.cumsum(discr)\n\nplt.bar(range(1, 14), discr, alpha=0.5, align=\'center\',\n        label=\'individual ""discriminability""\')\nplt.step(range(1, 14), cum_discr, where=\'mid\',\n         label=\'cumulative ""discriminability""\')\nplt.ylabel(\'""discriminability"" ratio\')\nplt.xlabel(\'Linear Discriminants\')\nplt.ylim([-0.1, 1.1])\nplt.legend(loc=\'best\')\nplt.tight_layout()\n# plt.savefig(\'images/05_07.png\', dpi=300)\nplt.show()\n\n\n\n\nw = np.hstack((eigen_pairs[0][1][:, np.newaxis].real,\n              eigen_pairs[1][1][:, np.newaxis].real))\nprint(\'Matrix W:\\n\', w)\n\n\n\n# ## Projecting samples onto the new feature space\n\n\n\nX_train_lda = X_train_std.dot(w)\ncolors = [\'r\', \'b\', \'g\']\nmarkers = [\'s\', \'x\', \'o\']\n\nfor l, c, m in zip(np.unique(y_train), colors, markers):\n    plt.scatter(X_train_lda[y_train == l, 0],\n                X_train_lda[y_train == l, 1] * (-1),\n                c=c, label=l, marker=m)\n\nplt.xlabel(\'LD 1\')\nplt.ylabel(\'LD 2\')\nplt.legend(loc=\'lower right\')\nplt.tight_layout()\n# plt.savefig(\'images/05_08.png\', dpi=300)\nplt.show()\n\n\n\n# ## LDA via scikit-learn\n\n\n\n\nlda = LDA(n_components=2)\nX_train_lda = lda.fit_transform(X_train_std, y_train)\n\n\n\n\nlr = LogisticRegression()\nlr = lr.fit(X_train_lda, y_train)\n\nplot_decision_regions(X_train_lda, y_train, classifier=lr)\nplt.xlabel(\'LD 1\')\nplt.ylabel(\'LD 2\')\nplt.legend(loc=\'lower left\')\nplt.tight_layout()\n# plt.savefig(\'images/05_09.png\', dpi=300)\nplt.show()\n\n\n\n\nX_test_lda = lda.transform(X_test_std)\n\nplot_decision_regions(X_test_lda, y_test, classifier=lr)\nplt.xlabel(\'LD 1\')\nplt.ylabel(\'LD 2\')\nplt.legend(loc=\'lower left\')\nplt.tight_layout()\n# plt.savefig(\'images/05_10.png\', dpi=300)\nplt.show()\n\n\n\n# # Using kernel principal component analysis for nonlinear mappings\n\n\n\n\n\n\n# ## Implementing a kernel principal component analysis in Python\n\n\n\n\ndef rbf_kernel_pca(X, gamma, n_components):\n    """"""\n    RBF kernel PCA implementation.\n\n    Parameters\n    ------------\n    X: {NumPy ndarray}, shape = [n_samples, n_features]\n        \n    gamma: float\n      Tuning parameter of the RBF kernel\n        \n    n_components: int\n      Number of principal components to return\n\n    Returns\n    ------------\n     X_pc: {NumPy ndarray}, shape = [n_samples, k_features]\n       Projected dataset   \n\n    """"""\n    # Calculate pairwise squared Euclidean distances\n    # in the MxN dimensional dataset.\n    sq_dists = pdist(X, \'sqeuclidean\')\n\n    # Convert pairwise distances into a square matrix.\n    mat_sq_dists = squareform(sq_dists)\n\n    # Compute the symmetric kernel matrix.\n    K = exp(-gamma * mat_sq_dists)\n\n    # Center the kernel matrix.\n    N = K.shape[0]\n    one_n = np.ones((N, N)) / N\n    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n\n    # Obtaining eigenpairs from the centered kernel matrix\n    # scipy.linalg.eigh returns them in ascending order\n    eigvals, eigvecs = eigh(K)\n    eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]\n\n    # Collect the top k eigenvectors (projected samples)\n    X_pc = np.column_stack((eigvecs[:, i]\n                            for i in range(n_components)))\n\n    return X_pc\n\n\n\n# ### Example 1: Separating half-moon shapes\n\n\n\n\nX, y = make_moons(n_samples=100, random_state=123)\n\nplt.scatter(X[y == 0, 0], X[y == 0, 1], color=\'red\', marker=\'^\', alpha=0.5)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], color=\'blue\', marker=\'o\', alpha=0.5)\n\nplt.tight_layout()\n# plt.savefig(\'images/05_12.png\', dpi=300)\nplt.show()\n\n\n\n\n\nscikit_pca = PCA(n_components=2)\nX_spca = scikit_pca.fit_transform(X)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n\nax[0].scatter(X_spca[y == 0, 0], X_spca[y == 0, 1],\n              color=\'red\', marker=\'^\', alpha=0.5)\nax[0].scatter(X_spca[y == 1, 0], X_spca[y == 1, 1],\n              color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[1].scatter(X_spca[y == 0, 0], np.zeros((50, 1)) + 0.02,\n              color=\'red\', marker=\'^\', alpha=0.5)\nax[1].scatter(X_spca[y == 1, 0], np.zeros((50, 1)) - 0.02,\n              color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[0].set_xlabel(\'PC1\')\nax[0].set_ylabel(\'PC2\')\nax[1].set_ylim([-1, 1])\nax[1].set_yticks([])\nax[1].set_xlabel(\'PC1\')\n\nplt.tight_layout()\n# plt.savefig(\'images/05_13.png\', dpi=300)\nplt.show()\n\n\n\n\nX_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)\n\nfig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3))\nax[0].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], \n            color=\'red\', marker=\'^\', alpha=0.5)\nax[0].scatter(X_kpca[y==1, 0], X_kpca[y==1, 1],\n            color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[1].scatter(X_kpca[y==0, 0], np.zeros((50,1))+0.02, \n            color=\'red\', marker=\'^\', alpha=0.5)\nax[1].scatter(X_kpca[y==1, 0], np.zeros((50,1))-0.02,\n            color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[0].set_xlabel(\'PC1\')\nax[0].set_ylabel(\'PC2\')\nax[1].set_ylim([-1, 1])\nax[1].set_yticks([])\nax[1].set_xlabel(\'PC1\')\n\nplt.tight_layout()\n# plt.savefig(\'images/05_14.png\', dpi=300)\nplt.show()\n\n\n\n# ### Example 2: Separating concentric circles\n\n\n\n\nX, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n\nplt.scatter(X[y == 0, 0], X[y == 0, 1], color=\'red\', marker=\'^\', alpha=0.5)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], color=\'blue\', marker=\'o\', alpha=0.5)\n\nplt.tight_layout()\n# plt.savefig(\'images/05_15.png\', dpi=300)\nplt.show()\n\n\n\n\nscikit_pca = PCA(n_components=2)\nX_spca = scikit_pca.fit_transform(X)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n\nax[0].scatter(X_spca[y == 0, 0], X_spca[y == 0, 1],\n              color=\'red\', marker=\'^\', alpha=0.5)\nax[0].scatter(X_spca[y == 1, 0], X_spca[y == 1, 1],\n              color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[1].scatter(X_spca[y == 0, 0], np.zeros((500, 1)) + 0.02,\n              color=\'red\', marker=\'^\', alpha=0.5)\nax[1].scatter(X_spca[y == 1, 0], np.zeros((500, 1)) - 0.02,\n              color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[0].set_xlabel(\'PC1\')\nax[0].set_ylabel(\'PC2\')\nax[1].set_ylim([-1, 1])\nax[1].set_yticks([])\nax[1].set_xlabel(\'PC1\')\n\nplt.tight_layout()\n# plt.savefig(\'images/05_16.png\', dpi=300)\nplt.show()\n\n\n\n\nX_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\nax[0].scatter(X_kpca[y == 0, 0], X_kpca[y == 0, 1],\n              color=\'red\', marker=\'^\', alpha=0.5)\nax[0].scatter(X_kpca[y == 1, 0], X_kpca[y == 1, 1],\n              color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[1].scatter(X_kpca[y == 0, 0], np.zeros((500, 1)) + 0.02,\n              color=\'red\', marker=\'^\', alpha=0.5)\nax[1].scatter(X_kpca[y == 1, 0], np.zeros((500, 1)) - 0.02,\n              color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[0].set_xlabel(\'PC1\')\nax[0].set_ylabel(\'PC2\')\nax[1].set_ylim([-1, 1])\nax[1].set_yticks([])\nax[1].set_xlabel(\'PC1\')\n\nplt.tight_layout()\n# plt.savefig(\'images/05_17.png\', dpi=300)\nplt.show()\n\n\n\n# ## Projecting new data points\n\n\n\n\ndef rbf_kernel_pca(X, gamma, n_components):\n    """"""\n    RBF kernel PCA implementation.\n\n    Parameters\n    ------------\n    X: {NumPy ndarray}, shape = [n_samples, n_features]\n        \n    gamma: float\n      Tuning parameter of the RBF kernel\n        \n    n_components: int\n      Number of principal components to return\n\n    Returns\n    ------------\n     alphas: {NumPy ndarray}, shape = [n_samples, k_features]\n       Projected dataset \n     \n     lambdas: list\n       Eigenvalues\n\n    """"""\n    # Calculate pairwise squared Euclidean distances\n    # in the MxN dimensional dataset.\n    sq_dists = pdist(X, \'sqeuclidean\')\n\n    # Convert pairwise distances into a square matrix.\n    mat_sq_dists = squareform(sq_dists)\n\n    # Compute the symmetric kernel matrix.\n    K = exp(-gamma * mat_sq_dists)\n\n    # Center the kernel matrix.\n    N = K.shape[0]\n    one_n = np.ones((N, N)) / N\n    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n\n    # Obtaining eigenpairs from the centered kernel matrix\n    # scipy.linalg.eigh returns them in ascending order\n    eigvals, eigvecs = eigh(K)\n    eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]\n\n    # Collect the top k eigenvectors (projected samples)\n    alphas = np.column_stack((eigvecs[:, i]\n                              for i in range(n_components)))\n\n    # Collect the corresponding eigenvalues\n    lambdas = [eigvals[i] for i in range(n_components)]\n\n    return alphas, lambdas\n\n\n\n\nX, y = make_moons(n_samples=100, random_state=123)\nalphas, lambdas = rbf_kernel_pca(X, gamma=15, n_components=1)\n\n\n\n\nx_new = X[25]\nx_new\n\n\n\n\nx_proj = alphas[25] # original projection\nx_proj\n\n\n\n\ndef project_x(x_new, X, gamma, alphas, lambdas):\n    pair_dist = np.array([np.sum((x_new - row)**2) for row in X])\n    k = np.exp(-gamma * pair_dist)\n    return k.dot(alphas / lambdas)\n\n# projection of the ""new"" datapoint\nx_reproj = project_x(x_new, X, gamma=15, alphas=alphas, lambdas=lambdas)\nx_reproj \n\n\n\n\nplt.scatter(alphas[y == 0, 0], np.zeros((50)),\n            color=\'red\', marker=\'^\', alpha=0.5)\nplt.scatter(alphas[y == 1, 0], np.zeros((50)),\n            color=\'blue\', marker=\'o\', alpha=0.5)\nplt.scatter(x_proj, 0, color=\'black\',\n            label=\'original projection of point X[25]\', marker=\'^\', s=100)\nplt.scatter(x_reproj, 0, color=\'green\',\n            label=\'remapped point X[25]\', marker=\'x\', s=500)\nplt.legend(scatterpoints=1)\n\nplt.tight_layout()\n# plt.savefig(\'images/05_18.png\', dpi=300)\nplt.show()\n\n\n\n# ## Kernel principal component analysis in scikit-learn\n\n\n\n\nX, y = make_moons(n_samples=100, random_state=123)\nscikit_kpca = KernelPCA(n_components=2, kernel=\'rbf\', gamma=15)\nX_skernpca = scikit_kpca.fit_transform(X)\n\nplt.scatter(X_skernpca[y == 0, 0], X_skernpca[y == 0, 1],\n            color=\'red\', marker=\'^\', alpha=0.5)\nplt.scatter(X_skernpca[y == 1, 0], X_skernpca[y == 1, 1],\n            color=\'blue\', marker=\'o\', alpha=0.5)\n\nplt.xlabel(\'PC1\')\nplt.ylabel(\'PC2\')\nplt.tight_layout()\n# plt.savefig(\'images/05_19.png\', dpi=300)\nplt.show()\n\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
code/ch06/ch06.py,0,"b'# coding: utf-8\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import roc_curve, auc\nfrom scipy import interp\nfrom sklearn.utils import resample\n\n# *Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 6 - Learning Best Practices for Model Evaluation and Hyperparameter Tuning\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*\n\n\n# ### Overview\n\n# - [Streamlining workflows with pipelines](#Streamlining-workflows-with-pipelines)\n#   - [Loading the Breast Cancer Wisconsin dataset](#Loading-the-Breast-Cancer-Wisconsin-dataset)\n#   - [Combining transformers and estimators in a pipeline](#Combining-transformers-and-estimators-in-a-pipeline)\n# - [Using k-fold cross-validation to assess model performance](#Using-k-fold-cross-validation-to-assess-model-performance)\n#   - [The holdout method](#The-holdout-method)\n#   - [K-fold cross-validation](#K-fold-cross-validation)\n# - [Debugging algorithms with learning and validation curves](#Debugging-algorithms-with-learning-and-validation-curves)\n#   - [Diagnosing bias and variance problems with learning curves](#Diagnosing-bias-and-variance-problems-with-learning-curves)\n#   - [Addressing overfitting and underfitting with validation curves](#Addressing-overfitting-and-underfitting-with-validation-curves)\n# - [Fine-tuning machine learning models via grid search](#Fine-tuning-machine-learning-models-via-grid-search)\n#   - [Tuning hyperparameters via grid search](#Tuning-hyperparameters-via-grid-search)\n#   - [Algorithm selection with nested cross-validation](#Algorithm-selection-with-nested-cross-validation)\n# - [Looking at different performance evaluation metrics](#Looking-at-different-performance-evaluation-metrics)\n#   - [Reading a confusion matrix](#Reading-a-confusion-matrix)\n#   - [Optimizing the precision and recall of a classification model](#Optimizing-the-precision-and-recall-of-a-classification-model)\n#   - [Plotting a receiver operating characteristic](#Plotting-a-receiver-operating-characteristic)\n#   - [The scoring metrics for multiclass classification](#The-scoring-metrics-for-multiclass-classification)\n# - [Dealing with class imbalance](#Dealing-with-class-imbalance)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Streamlining workflows with pipelines\n\n# ...\n\n# ## Loading the Breast Cancer Wisconsin dataset\n\n\n\n\ndf = pd.read_csv(\'https://archive.ics.uci.edu/ml/\'\n                 \'machine-learning-databases\'\n                 \'/breast-cancer-wisconsin/wdbc.data\', header=None)\n\n# if the Breast Cancer dataset is temporarily unavailable from the\n# UCI machine learning repository, un-comment the following line\n# of code to load the dataset from a local path:\n\n# df = pd.read_csv(\'wdbc.data\', header=None)\n\ndf.head()\n\n\n\n\ndf.shape\n\n\n\n\n\n\nX = df.loc[:, 2:].values\ny = df.loc[:, 1].values\nle = LabelEncoder()\ny = le.fit_transform(y)\nle.classes_\n\n\n\n\nle.transform([\'M\', \'B\'])\n\n\n\n\n\nX_train, X_test, y_train, y_test =     train_test_split(X, y, \n                     test_size=0.20,\n                     stratify=y,\n                     random_state=1)\n\n\n\n# ## Combining transformers and estimators in a pipeline\n\n\n\n\npipe_lr = make_pipeline(StandardScaler(),\n                        PCA(n_components=2),\n                        LogisticRegression(random_state=1))\n\npipe_lr.fit(X_train, y_train)\ny_pred = pipe_lr.predict(X_test)\nprint(\'Test Accuracy: %.3f\' % pipe_lr.score(X_test, y_test))\n\n\n\n\n\n\n\n# # Using k-fold cross validation to assess model performance\n\n# ...\n\n# ## The holdout method\n\n\n\n\n\n\n# ## K-fold cross-validation\n\n\n\n\n\n\n\n    \n\nkfold = StratifiedKFold(n_splits=10,\n                        random_state=1).split(X_train, y_train)\n\nscores = []\nfor k, (train, test) in enumerate(kfold):\n    pipe_lr.fit(X_train[train], y_train[train])\n    score = pipe_lr.score(X_train[test], y_train[test])\n    scores.append(score)\n    print(\'Fold: %2d, Class dist.: %s, Acc: %.3f\' % (k+1,\n          np.bincount(y_train[train]), score))\n    \nprint(\'\\nCV accuracy: %.3f +/- %.3f\' % (np.mean(scores), np.std(scores)))\n\n\n\n\n\nscores = cross_val_score(estimator=pipe_lr,\n                         X=X_train,\n                         y=y_train,\n                         cv=10,\n                         n_jobs=1)\nprint(\'CV accuracy scores: %s\' % scores)\nprint(\'CV accuracy: %.3f +/- %.3f\' % (np.mean(scores), np.std(scores)))\n\n\n\n# # Debugging algorithms with learning curves\n\n\n# ## Diagnosing bias and variance problems with learning curves\n\n\n\n\n\n\n\n\n\npipe_lr = make_pipeline(StandardScaler(),\n                        LogisticRegression(penalty=\'l2\', random_state=1))\n\ntrain_sizes, train_scores, test_scores =                learning_curve(estimator=pipe_lr,\n                               X=X_train,\n                               y=y_train,\n                               train_sizes=np.linspace(0.1, 1.0, 10),\n                               cv=10,\n                               n_jobs=1)\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\nplt.plot(train_sizes, train_mean,\n         color=\'blue\', marker=\'o\',\n         markersize=5, label=\'training accuracy\')\n\nplt.fill_between(train_sizes,\n                 train_mean + train_std,\n                 train_mean - train_std,\n                 alpha=0.15, color=\'blue\')\n\nplt.plot(train_sizes, test_mean,\n         color=\'green\', linestyle=\'--\',\n         marker=\'s\', markersize=5,\n         label=\'validation accuracy\')\n\nplt.fill_between(train_sizes,\n                 test_mean + test_std,\n                 test_mean - test_std,\n                 alpha=0.15, color=\'green\')\n\nplt.grid()\nplt.xlabel(\'Number of training samples\')\nplt.ylabel(\'Accuracy\')\nplt.legend(loc=\'lower right\')\nplt.ylim([0.8, 1.03])\nplt.tight_layout()\n#plt.savefig(\'images/06_05.png\', dpi=300)\nplt.show()\n\n\n\n# ## Addressing over- and underfitting with validation curves\n\n\n\n\n\nparam_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\ntrain_scores, test_scores = validation_curve(\n                estimator=pipe_lr, \n                X=X_train, \n                y=y_train, \n                param_name=\'logisticregression__C\', \n                param_range=param_range,\n                cv=10)\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\nplt.plot(param_range, train_mean, \n         color=\'blue\', marker=\'o\', \n         markersize=5, label=\'training accuracy\')\n\nplt.fill_between(param_range, train_mean + train_std,\n                 train_mean - train_std, alpha=0.15,\n                 color=\'blue\')\n\nplt.plot(param_range, test_mean, \n         color=\'green\', linestyle=\'--\', \n         marker=\'s\', markersize=5, \n         label=\'validation accuracy\')\n\nplt.fill_between(param_range, \n                 test_mean + test_std,\n                 test_mean - test_std, \n                 alpha=0.15, color=\'green\')\n\nplt.grid()\nplt.xscale(\'log\')\nplt.legend(loc=\'lower right\')\nplt.xlabel(\'Parameter C\')\nplt.ylabel(\'Accuracy\')\nplt.ylim([0.8, 1.0])\nplt.tight_layout()\n# plt.savefig(\'images/06_06.png\', dpi=300)\nplt.show()\n\n\n\n# # Fine-tuning machine learning models via grid search\n\n\n# ## Tuning hyperparameters via grid search \n\n\n\n\npipe_svc = make_pipeline(StandardScaler(),\n                         SVC(random_state=1))\n\nparam_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n\nparam_grid = [{\'svc__C\': param_range, \n               \'svc__kernel\': [\'linear\']},\n              {\'svc__C\': param_range, \n               \'svc__gamma\': param_range, \n               \'svc__kernel\': [\'rbf\']}]\n\ngs = GridSearchCV(estimator=pipe_svc, \n                  param_grid=param_grid, \n                  scoring=\'accuracy\', \n                  cv=10,\n                  n_jobs=-1)\ngs = gs.fit(X_train, y_train)\nprint(gs.best_score_)\nprint(gs.best_params_)\n\n\n\n\nclf = gs.best_estimator_\n\n# clf.fit(X_train, y_train)\n# Note that the line above is not necessary, because\n# the best_estimator_ will already be refit to the complete\n# training set because of the refit=True setting in GridSearchCV\n# (refit=True by default). Thanks to a reader, German Martinez,\n# for pointing it out.\n\nprint(\'Test accuracy: %.3f\' % clf.score(X_test, y_test))\n\n\n\n# ## Algorithm selection with nested cross-validation\n\n\n\n\n\n\n\ngs = GridSearchCV(estimator=pipe_svc,\n                  param_grid=param_grid,\n                  scoring=\'accuracy\',\n                  cv=2)\n\nscores = cross_val_score(gs, X_train, y_train, \n                         scoring=\'accuracy\', cv=5)\nprint(\'CV accuracy: %.3f +/- %.3f\' % (np.mean(scores),\n                                      np.std(scores)))\n\n\n\n\n\ngs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0),\n                  param_grid=[{\'max_depth\': [1, 2, 3, 4, 5, 6, 7, None]}],\n                  scoring=\'accuracy\',\n                  cv=2)\n\nscores = cross_val_score(gs, X_train, y_train, \n                         scoring=\'accuracy\', cv=5)\nprint(\'CV accuracy: %.3f +/- %.3f\' % (np.mean(scores), \n                                      np.std(scores)))\n\n\n\n# # Looking at different performance evaluation metrics\n\n# ...\n\n# ## Reading a confusion matrix\n\n\n\n\n\n\n\n\npipe_svc.fit(X_train, y_train)\ny_pred = pipe_svc.predict(X_test)\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(confmat)\n\n\n\n\nfig, ax = plt.subplots(figsize=(2.5, 2.5))\nax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(confmat.shape[0]):\n    for j in range(confmat.shape[1]):\n        ax.text(x=j, y=i, s=confmat[i, j], va=\'center\', ha=\'center\')\n\nplt.xlabel(\'Predicted label\')\nplt.ylabel(\'True label\')\n\nplt.tight_layout()\n#plt.savefig(\'images/06_09.png\', dpi=300)\nplt.show()\n\n\n# ### Additional Note\n\n# Remember that we previously encoded the class labels so that *malignant* samples are the ""postive"" class (1), and *benign* samples are the ""negative"" class (0):\n\n\n\nle.transform([\'M\', \'B\'])\n\n\n\n\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(confmat)\n\n\n# Next, we printed the confusion matrix like so:\n\n\n\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(confmat)\n\n\n# Note that the (true) class 0 samples that are correctly predicted as class 0 (true negatives) are now in the upper left corner of the matrix (index 0, 0). In order to change the ordering so that the true negatives are in the lower right corner (index 1,1) and the true positves are in the upper left, we can use the `labels` argument like shown below:\n\n\n\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred, labels=[1, 0])\nprint(confmat)\n\n\n# We conclude:\n# \n# Assuming that class 1 (malignant) is the positive class in this example, our model correctly classified 71 of the samples that belong to class 0 (true negatives) and 40 samples that belong to class 1 (true positives), respectively. However, our model also incorrectly misclassified 1 sample from class 0 as class 1 (false positive), and it predicted that 2 samples are benign although it is a malignant tumor (false negatives).\n\n\n# ## Optimizing the precision and recall of a classification model\n\n\n\n\nprint(\'Precision: %.3f\' % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\'Recall: %.3f\' % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\'F1: %.3f\' % f1_score(y_true=y_test, y_pred=y_pred))\n\n\n\n\n\nscorer = make_scorer(f1_score, pos_label=0)\n\nc_gamma_range = [0.01, 0.1, 1.0, 10.0]\n\nparam_grid = [{\'svc__C\': c_gamma_range,\n               \'svc__kernel\': [\'linear\']},\n              {\'svc__C\': c_gamma_range,\n               \'svc__gamma\': c_gamma_range,\n               \'svc__kernel\': [\'rbf\']}]\n\ngs = GridSearchCV(estimator=pipe_svc,\n                  param_grid=param_grid,\n                  scoring=scorer,\n                  cv=10,\n                  n_jobs=-1)\ngs = gs.fit(X_train, y_train)\nprint(gs.best_score_)\nprint(gs.best_params_)\n\n\n\n# ## Plotting a receiver operating characteristic\n\n\n\n\npipe_lr = make_pipeline(StandardScaler(),\n                        PCA(n_components=2),\n                        LogisticRegression(penalty=\'l2\', \n                                           random_state=1, \n                                           C=100.0))\n\nX_train2 = X_train[:, [4, 14]]\n    \n\ncv = list(StratifiedKFold(n_splits=3, \n                          random_state=1).split(X_train, y_train))\n\nfig = plt.figure(figsize=(7, 5))\n\nmean_tpr = 0.0\nmean_fpr = np.linspace(0, 1, 100)\nall_tpr = []\n\nfor i, (train, test) in enumerate(cv):\n    probas = pipe_lr.fit(X_train2[train],\n                         y_train[train]).predict_proba(X_train2[test])\n\n    fpr, tpr, thresholds = roc_curve(y_train[test],\n                                     probas[:, 1],\n                                     pos_label=1)\n    mean_tpr += interp(mean_fpr, fpr, tpr)\n    mean_tpr[0] = 0.0\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr,\n             tpr,\n             label=\'ROC fold %d (area = %0.2f)\'\n                   % (i+1, roc_auc))\n\nplt.plot([0, 1],\n         [0, 1],\n         linestyle=\'--\',\n         color=(0.6, 0.6, 0.6),\n         label=\'random guessing\')\n\nmean_tpr /= len(cv)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, \'k--\',\n         label=\'mean ROC (area = %0.2f)\' % mean_auc, lw=2)\nplt.plot([0, 0, 1],\n         [0, 1, 1],\n         linestyle=\':\',\n         color=\'black\',\n         label=\'perfect performance\')\n\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel(\'false positive rate\')\nplt.ylabel(\'true positive rate\')\nplt.legend(loc=""lower right"")\n\nplt.tight_layout()\n# plt.savefig(\'images/06_10.png\', dpi=300)\nplt.show()\n\n\n\n# ## The scoring metrics for multiclass classification\n\n\n\npre_scorer = make_scorer(score_func=precision_score, \n                         pos_label=1, \n                         greater_is_better=True, \n                         average=\'micro\')\n\n\n# ## Dealing with class imbalance\n\n\n\nX_imb = np.vstack((X[y == 0], X[y == 1][:40]))\ny_imb = np.hstack((y[y == 0], y[y == 1][:40]))\n\n\n\n\ny_pred = np.zeros(y_imb.shape[0])\nnp.mean(y_pred == y_imb) * 100\n\n\n\n\n\nprint(\'Number of class 1 samples before:\', X_imb[y_imb == 1].shape[0])\n\nX_upsampled, y_upsampled = resample(X_imb[y_imb == 1],\n                                    y_imb[y_imb == 1],\n                                    replace=True,\n                                    n_samples=X_imb[y_imb == 0].shape[0],\n                                    random_state=123)\n\nprint(\'Number of class 1 samples after:\', X_upsampled.shape[0])\n\n\n\n\nX_bal = np.vstack((X[y == 0], X_upsampled))\ny_bal = np.hstack((y[y == 0], y_upsampled))\n\n\n\n\ny_pred = np.zeros(y_bal.shape[0])\nnp.mean(y_pred == y_bal) * 100\n\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
code/ch07/ch07.py,0,"b'# coding: utf-8\n\n\nfrom scipy.special import comb\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import ClassifierMixin\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.externals import six\nfrom sklearn.base import clone\nfrom sklearn.pipeline import _name_estimators\nimport operator\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom itertools import product\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# *Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 7 - Combining Different Models for Ensemble Learning\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*\n\n\n# ### Overview\n\n# - [Learning with ensembles](#Learning-with-ensembles)\n# - [Combining classifiers via majority vote](#Combining-classifiers-via-majority-vote)\n#     - [Implementing a simple majority vote classifier](#Implementing-a-simple-majority-vote-classifier)\n#     - [Using the majority voting principle to make predictions](#Using-the-majority-voting-principle-to-make-predictions)\n#     - [Evaluating and tuning the ensemble classifier](#Evaluating-and-tuning-the-ensemble-classifier)\n# - [Bagging \xe2\x80\x93 building an ensemble of classifiers from bootstrap samples](#Bagging----Building-an-ensemble-of-classifiers-from-bootstrap-samples)\n#     - [Bagging in a nutshell](#Bagging-in-a-nutshell)\n#     - [Applying bagging to classify samples in the Wine dataset](#Applying-bagging-to-classify-samples-in-the-Wine-dataset)\n# - [Leveraging weak learners via adaptive boosting](#Leveraging-weak-learners-via-adaptive-boosting)\n#     - [How boosting works](#How-boosting-works)\n#     - [Applying AdaBoost using scikit-learn](#Applying-AdaBoost-using-scikit-learn)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Learning with ensembles\n\n\n\n\n\n\n\n\n\n\n\n\ndef ensemble_error(n_classifier, error):\n    k_start = int(math.ceil(n_classifier / 2.))\n    probs = [comb(n_classifier, k) * error**k * (1-error)**(n_classifier - k)\n             for k in range(k_start, n_classifier + 1)]\n    return sum(probs)\n\n\n\n\nensemble_error(n_classifier=11, error=0.25)\n\n\n\n\n\nerror_range = np.arange(0.0, 1.01, 0.01)\nens_errors = [ensemble_error(n_classifier=11, error=error)\n              for error in error_range]\n\n\n\n\n\nplt.plot(error_range, \n         ens_errors, \n         label=\'Ensemble error\', \n         linewidth=2)\n\nplt.plot(error_range, \n         error_range, \n         linestyle=\'--\',\n         label=\'Base error\',\n         linewidth=2)\n\nplt.xlabel(\'Base error\')\nplt.ylabel(\'Base/Ensemble error\')\nplt.legend(loc=\'upper left\')\nplt.grid(alpha=0.5)\n#plt.savefig(\'images/07_03.png\', dpi=300)\nplt.show()\n\n\n\n# # Combining classifiers via majority vote\n\n# ## Implementing a simple majority vote classifier \n\n\n\n\nnp.argmax(np.bincount([0, 0, 1], \n                      weights=[0.2, 0.2, 0.6]))\n\n\n\n\nex = np.array([[0.9, 0.1],\n               [0.8, 0.2],\n               [0.4, 0.6]])\n\np = np.average(ex, \n               axis=0, \n               weights=[0.2, 0.2, 0.6])\np\n\n\n\n\nnp.argmax(p)\n\n\n\n\n\n\nclass MajorityVoteClassifier(BaseEstimator, \n                             ClassifierMixin):\n    """""" A majority vote ensemble classifier\n\n    Parameters\n    ----------\n    classifiers : array-like, shape = [n_classifiers]\n      Different classifiers for the ensemble\n\n    vote : str, {\'classlabel\', \'probability\'} (default=\'label\')\n      If \'classlabel\' the prediction is based on the argmax of\n        class labels. Else if \'probability\', the argmax of\n        the sum of probabilities is used to predict the class label\n        (recommended for calibrated classifiers).\n\n    weights : array-like, shape = [n_classifiers], optional (default=None)\n      If a list of `int` or `float` values are provided, the classifiers\n      are weighted by importance; Uses uniform weights if `weights=None`.\n\n    """"""\n    def __init__(self, classifiers, vote=\'classlabel\', weights=None):\n\n        self.classifiers = classifiers\n        self.named_classifiers = {key: value for key, value\n                                  in _name_estimators(classifiers)}\n        self.vote = vote\n        self.weights = weights\n\n    def fit(self, X, y):\n        """""" Fit classifiers.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Matrix of training samples.\n\n        y : array-like, shape = [n_samples]\n            Vector of target class labels.\n\n        Returns\n        -------\n        self : object\n\n        """"""\n        if self.vote not in (\'probability\', \'classlabel\'):\n            raise ValueError(""vote must be \'probability\' or \'classlabel\'""\n                             ""; got (vote=%r)""\n                             % self.vote)\n\n        if self.weights and len(self.weights) != len(self.classifiers):\n            raise ValueError(\'Number of classifiers and weights must be equal\'\n                             \'; got %d weights, %d classifiers\'\n                             % (len(self.weights), len(self.classifiers)))\n\n        # Use LabelEncoder to ensure class labels start with 0, which\n        # is important for np.argmax call in self.predict\n        self.lablenc_ = LabelEncoder()\n        self.lablenc_.fit(y)\n        self.classes_ = self.lablenc_.classes_\n        self.classifiers_ = []\n        for clf in self.classifiers:\n            fitted_clf = clone(clf).fit(X, self.lablenc_.transform(y))\n            self.classifiers_.append(fitted_clf)\n        return self\n\n    def predict(self, X):\n        """""" Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Matrix of training samples.\n\n        Returns\n        ----------\n        maj_vote : array-like, shape = [n_samples]\n            Predicted class labels.\n            \n        """"""\n        if self.vote == \'probability\':\n            maj_vote = np.argmax(self.predict_proba(X), axis=1)\n        else:  # \'classlabel\' vote\n\n            #  Collect results from clf.predict calls\n            predictions = np.asarray([clf.predict(X)\n                                      for clf in self.classifiers_]).T\n\n            maj_vote = np.apply_along_axis(\n                                      lambda x:\n                                      np.argmax(np.bincount(x,\n                                                weights=self.weights)),\n                                      axis=1,\n                                      arr=predictions)\n        maj_vote = self.lablenc_.inverse_transform(maj_vote)\n        return maj_vote\n\n    def predict_proba(self, X):\n        """""" Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        avg_proba : array-like, shape = [n_samples, n_classes]\n            Weighted average probability for each class per sample.\n\n        """"""\n        probas = np.asarray([clf.predict_proba(X)\n                             for clf in self.classifiers_])\n        avg_proba = np.average(probas, axis=0, weights=self.weights)\n        return avg_proba\n\n    def get_params(self, deep=True):\n        """""" Get classifier parameter names for GridSearch""""""\n        if not deep:\n            return super(MajorityVoteClassifier, self).get_params(deep=False)\n        else:\n            out = self.named_classifiers.copy()\n            for name, step in six.iteritems(self.named_classifiers):\n                for key, value in six.iteritems(step.get_params(deep=True)):\n                    out[\'%s__%s\' % (name, key)] = value\n            return out\n\n\n\n# ## Using the majority voting principle to make predictions\n\n\n\n\niris = datasets.load_iris()\nX, y = iris.data[50:, [1, 2]], iris.target[50:]\nle = LabelEncoder()\ny = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test =       train_test_split(X, y, \n                        test_size=0.5, \n                        random_state=1,\n                        stratify=y)\n\n\n\n\n\nclf1 = LogisticRegression(penalty=\'l2\', \n                          C=0.001,\n                          random_state=1)\n\nclf2 = DecisionTreeClassifier(max_depth=1,\n                              criterion=\'entropy\',\n                              random_state=0)\n\nclf3 = KNeighborsClassifier(n_neighbors=1,\n                            p=2,\n                            metric=\'minkowski\')\n\npipe1 = Pipeline([[\'sc\', StandardScaler()],\n                  [\'clf\', clf1]])\npipe3 = Pipeline([[\'sc\', StandardScaler()],\n                  [\'clf\', clf3]])\n\nclf_labels = [\'Logistic regression\', \'Decision tree\', \'KNN\']\n\nprint(\'10-fold cross validation:\\n\')\nfor clf, label in zip([pipe1, clf2, pipe3], clf_labels):\n    scores = cross_val_score(estimator=clf,\n                             X=X_train,\n                             y=y_train,\n                             cv=10,\n                             scoring=\'roc_auc\')\n    print(""ROC AUC: %0.2f (+/- %0.2f) [%s]""\n          % (scores.mean(), scores.std(), label))\n\n\n\n\n# Majority Rule (hard) Voting\n\nmv_clf = MajorityVoteClassifier(classifiers=[pipe1, clf2, pipe3])\n\nclf_labels += [\'Majority voting\']\nall_clf = [pipe1, clf2, pipe3, mv_clf]\n\nfor clf, label in zip(all_clf, clf_labels):\n    scores = cross_val_score(estimator=clf,\n                             X=X_train,\n                             y=y_train,\n                             cv=10,\n                             scoring=\'roc_auc\')\n    print(""ROC AUC: %0.2f (+/- %0.2f) [%s]""\n          % (scores.mean(), scores.std(), label))\n\n\n\n# # Evaluating and tuning the ensemble classifier\n\n\n\n\ncolors = [\'black\', \'orange\', \'blue\', \'green\']\nlinestyles = [\':\', \'--\', \'-.\', \'-\']\nfor clf, label, clr, ls         in zip(all_clf,\n               clf_labels, colors, linestyles):\n\n    # assuming the label of the positive class is 1\n    y_pred = clf.fit(X_train,\n                     y_train).predict_proba(X_test)[:, 1]\n    fpr, tpr, thresholds = roc_curve(y_true=y_test,\n                                     y_score=y_pred)\n    roc_auc = auc(x=fpr, y=tpr)\n    plt.plot(fpr, tpr,\n             color=clr,\n             linestyle=ls,\n             label=\'%s (auc = %0.2f)\' % (label, roc_auc))\n\nplt.legend(loc=\'lower right\')\nplt.plot([0, 1], [0, 1],\n         linestyle=\'--\',\n         color=\'gray\',\n         linewidth=2)\n\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.grid(alpha=0.5)\nplt.xlabel(\'False positive rate (FPR)\')\nplt.ylabel(\'True positive rate (TPR)\')\n\n\n#plt.savefig(\'images/07_04\', dpi=300)\nplt.show()\n\n\n\n\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\n\n\n\n\n\nall_clf = [pipe1, clf2, pipe3, mv_clf]\n\nx_min = X_train_std[:, 0].min() - 1\nx_max = X_train_std[:, 0].max() + 1\ny_min = X_train_std[:, 1].min() - 1\ny_max = X_train_std[:, 1].max() + 1\n\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nf, axarr = plt.subplots(nrows=2, ncols=2, \n                        sharex=\'col\', \n                        sharey=\'row\', \n                        figsize=(7, 5))\n\nfor idx, clf, tt in zip(product([0, 1], [0, 1]),\n                        all_clf, clf_labels):\n    clf.fit(X_train_std, y_train)\n    \n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.3)\n    \n    axarr[idx[0], idx[1]].scatter(X_train_std[y_train==0, 0], \n                                  X_train_std[y_train==0, 1], \n                                  c=\'blue\', \n                                  marker=\'^\',\n                                  s=50)\n    \n    axarr[idx[0], idx[1]].scatter(X_train_std[y_train==1, 0], \n                                  X_train_std[y_train==1, 1], \n                                  c=\'green\', \n                                  marker=\'o\',\n                                  s=50)\n    \n    axarr[idx[0], idx[1]].set_title(tt)\n\nplt.text(-3.5, -5., \n         s=\'Sepal width [standardized]\', \n         ha=\'center\', va=\'center\', fontsize=12)\nplt.text(-12.5, 4.5, \n         s=\'Petal length [standardized]\', \n         ha=\'center\', va=\'center\', \n         fontsize=12, rotation=90)\n\n#plt.savefig(\'images/07_05\', dpi=300)\nplt.show()\n\n\n\n\nmv_clf.get_params()\n\n\n\n\n\nparams = {\'decisiontreeclassifier__max_depth\': [1, 2],\n          \'pipeline-1__clf__C\': [0.001, 0.1, 100.0]}\n\ngrid = GridSearchCV(estimator=mv_clf,\n                    param_grid=params,\n                    cv=10,\n                    scoring=\'roc_auc\')\ngrid.fit(X_train, y_train)\n\nfor r, _ in enumerate(grid.cv_results_[\'mean_test_score\']):\n    print(""%0.3f +/- %0.2f %r""\n          % (grid.cv_results_[\'mean_test_score\'][r], \n             grid.cv_results_[\'std_test_score\'][r] / 2.0, \n             grid.cv_results_[\'params\'][r]))\n\n\n\n\nprint(\'Best parameters: %s\' % grid.best_params_)\nprint(\'Accuracy: %.2f\' % grid.best_score_)\n\n\n# **Note**  \n# By default, the default setting for `refit` in `GridSearchCV` is `True` (i.e., `GridSeachCV(..., refit=True)`), which means that we can use the fitted `GridSearchCV` estimator to make predictions via the `predict` method, for example:\n# \n#     grid = GridSearchCV(estimator=mv_clf, \n#                         param_grid=params, \n#                         cv=10, \n#                         scoring=\'roc_auc\')\n#     grid.fit(X_train, y_train)\n#     y_pred = grid.predict(X_test)\n# \n# In addition, the ""best"" estimator can directly be accessed via the `best_estimator_` attribute.\n\n\n\ngrid.best_estimator_.classifiers\n\n\n\n\nmv_clf = grid.best_estimator_\n\n\n\n\nmv_clf.set_params(**grid.best_estimator_.get_params())\n\n\n\n\nmv_clf\n\n\n\n# # Bagging -- Building an ensemble of classifiers from bootstrap samples\n\n\n\n\n\n# ## Bagging in a nutshell\n\n\n\n\n\n# ## Applying bagging to classify samples in the Wine dataset\n\n\n\n\ndf_wine = pd.read_csv(\'https://archive.ics.uci.edu/ml/\'\n                      \'machine-learning-databases/wine/wine.data\',\n                      header=None)\n\ndf_wine.columns = [\'Class label\', \'Alcohol\', \'Malic acid\', \'Ash\',\n                   \'Alcalinity of ash\', \'Magnesium\', \'Total phenols\',\n                   \'Flavanoids\', \'Nonflavanoid phenols\', \'Proanthocyanins\',\n                   \'Color intensity\', \'Hue\', \'OD280/OD315 of diluted wines\',\n                   \'Proline\']\n\n# if the Wine dataset is temporarily unavailable from the\n# UCI machine learning repository, un-comment the following line\n# of code to load the dataset from a local path:\n\n# df_wine = pd.read_csv(\'wine.data\', header=None)\n\n# drop 1 class\ndf_wine = df_wine[df_wine[\'Class label\'] != 1]\n\ny = df_wine[\'Class label\'].values\nX = df_wine[[\'Alcohol\', \'OD280/OD315 of diluted wines\']].values\n\n\n\n\n\n\nle = LabelEncoder()\ny = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test =            train_test_split(X, y, \n                             test_size=0.2, \n                             random_state=1,\n                             stratify=y)\n\n\n\n\n\ntree = DecisionTreeClassifier(criterion=\'entropy\', \n                              max_depth=None,\n                              random_state=1)\n\nbag = BaggingClassifier(base_estimator=tree,\n                        n_estimators=500, \n                        max_samples=1.0, \n                        max_features=1.0, \n                        bootstrap=True, \n                        bootstrap_features=False, \n                        n_jobs=1, \n                        random_state=1)\n\n\n\n\n\ntree = tree.fit(X_train, y_train)\ny_train_pred = tree.predict(X_train)\ny_test_pred = tree.predict(X_test)\n\ntree_train = accuracy_score(y_train, y_train_pred)\ntree_test = accuracy_score(y_test, y_test_pred)\nprint(\'Decision tree train/test accuracies %.3f/%.3f\'\n      % (tree_train, tree_test))\n\nbag = bag.fit(X_train, y_train)\ny_train_pred = bag.predict(X_train)\ny_test_pred = bag.predict(X_test)\n\nbag_train = accuracy_score(y_train, y_train_pred) \nbag_test = accuracy_score(y_test, y_test_pred) \nprint(\'Bagging train/test accuracies %.3f/%.3f\'\n      % (bag_train, bag_test))\n\n\n\n\n\nx_min = X_train[:, 0].min() - 1\nx_max = X_train[:, 0].max() + 1\ny_min = X_train[:, 1].min() - 1\ny_max = X_train[:, 1].max() + 1\n\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nf, axarr = plt.subplots(nrows=1, ncols=2, \n                        sharex=\'col\', \n                        sharey=\'row\', \n                        figsize=(8, 3))\n\n\nfor idx, clf, tt in zip([0, 1],\n                        [tree, bag],\n                        [\'Decision tree\', \'Bagging\']):\n    clf.fit(X_train, y_train)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    axarr[idx].contourf(xx, yy, Z, alpha=0.3)\n    axarr[idx].scatter(X_train[y_train == 0, 0],\n                       X_train[y_train == 0, 1],\n                       c=\'blue\', marker=\'^\')\n\n    axarr[idx].scatter(X_train[y_train == 1, 0],\n                       X_train[y_train == 1, 1],\n                       c=\'green\', marker=\'o\')\n\n    axarr[idx].set_title(tt)\n\naxarr[0].set_ylabel(\'Alcohol\', fontsize=12)\nplt.text(10.2, -0.5,\n         s=\'OD280/OD315 of diluted wines\',\n         ha=\'center\', va=\'center\', fontsize=12)\n\nplt.tight_layout()\n#plt.savefig(\'images/07_08.png\', dpi=300, bbox_inches=\'tight\')\nplt.show()\n\n\n\n# # Leveraging weak learners via adaptive boosting\n\n# ## How boosting works\n\n\n\n\n\n\n\n\n\n# ## Applying AdaBoost using scikit-learn\n\n\n\n\ntree = DecisionTreeClassifier(criterion=\'entropy\', \n                              max_depth=1,\n                              random_state=1)\n\nada = AdaBoostClassifier(base_estimator=tree,\n                         n_estimators=500, \n                         learning_rate=0.1,\n                         random_state=1)\n\n\n\n\ntree = tree.fit(X_train, y_train)\ny_train_pred = tree.predict(X_train)\ny_test_pred = tree.predict(X_test)\n\ntree_train = accuracy_score(y_train, y_train_pred)\ntree_test = accuracy_score(y_test, y_test_pred)\nprint(\'Decision tree train/test accuracies %.3f/%.3f\'\n      % (tree_train, tree_test))\n\nada = ada.fit(X_train, y_train)\ny_train_pred = ada.predict(X_train)\ny_test_pred = ada.predict(X_test)\n\nada_train = accuracy_score(y_train, y_train_pred) \nada_test = accuracy_score(y_test, y_test_pred) \nprint(\'AdaBoost train/test accuracies %.3f/%.3f\'\n      % (ada_train, ada_test))\n\n\n\n\nx_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\ny_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nf, axarr = plt.subplots(1, 2, sharex=\'col\', sharey=\'row\', figsize=(8, 3))\n\n\nfor idx, clf, tt in zip([0, 1],\n                        [tree, ada],\n                        [\'Decision tree\', \'AdaBoost\']):\n    clf.fit(X_train, y_train)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    axarr[idx].contourf(xx, yy, Z, alpha=0.3)\n    axarr[idx].scatter(X_train[y_train == 0, 0],\n                       X_train[y_train == 0, 1],\n                       c=\'blue\', marker=\'^\')\n    axarr[idx].scatter(X_train[y_train == 1, 0],\n                       X_train[y_train == 1, 1],\n                       c=\'green\', marker=\'o\')\n    axarr[idx].set_title(tt)\n\naxarr[0].set_ylabel(\'Alcohol\', fontsize=12)\nplt.text(10.2, -0.5,\n         s=\'OD280/OD315 of diluted wines\',\n         ha=\'center\', va=\'center\', fontsize=12)\n\nplt.tight_layout()\n#plt.savefig(\'images/07_11.png\', dpi=300, bbox_inches=\'tight\')\nplt.show()\n\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
code/ch08/ch08.py,0,"b'# coding: utf-8\n\n\nimport os\nimport sys\nimport tarfile\nimport time\nimport pyprind\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom distutils.version import LooseVersion as Version\nfrom sklearn import __version__ as sklearn_version\n\n\n# Added version check for recent scikit-learn 0.18 checks\n\n\n# *Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 8 - Applying Machine Learning To Sentiment Analysis\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*\n\n\n# ### Overview\n\n# - [Preparing the IMDb movie review data for text processing](#Preparing-the-IMDb-movie-review-data-for-text-processing)\n#   - [Obtaining the IMDb movie review dataset](#Obtaining-the-IMDb-movie-review-dataset)\n#   - [Preprocessing the movie dataset into more convenient format](#Preprocessing-the-movie-dataset-into-more-convenient-format)\n# - [Introducing the bag-of-words model](#Introducing-the-bag-of-words-model)\n#   - [Transforming words into feature vectors](#Transforming-words-into-feature-vectors)\n#   - [Assessing word relevancy via term frequency-inverse document frequency](#Assessing-word-relevancy-via-term-frequency-inverse-document-frequency)\n#   - [Cleaning text data](#Cleaning-text-data)\n#   - [Processing documents into tokens](#Processing-documents-into-tokens)\n# - [Training a logistic regression model for document classification](#Training-a-logistic-regression-model-for-document-classification)\n# - [Working with bigger data \xe2\x80\x93 online algorithms and out-of-core learning](#Working-with-bigger-data-\xe2\x80\x93-online-algorithms-and-out-of-core-learning)\n# - [Topic modeling](#Topic-modeling)\n#   - [Decomposing text documents with Latent Dirichlet Allocation](#Decomposing-text-documents-with-Latent-Dirichlet-Allocation)\n#   - [Latent Dirichlet Allocation with scikit-learn](#Latent-Dirichlet-Allocation-with-scikit-learn)\n# - [Summary](#Summary)\n\n\n# # Preparing the IMDb movie review data for text processing \n\n# ## Obtaining the IMDb movie review dataset\n\n# The IMDB movie review set can be downloaded from [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/).\n# After downloading the dataset, decompress the files.\n# \n# A) If you are working with Linux or MacOS X, open a new terminal windowm `cd` into the download directory and execute \n# \n# `tar -zxf aclImdb_v1.tar.gz`\n# \n# B) If you are working with Windows, download an archiver such as [7Zip](http://www.7-zip.org) to extract the files from the download archive.\n\n# **Optional code to download and unzip the dataset via Python:**\n\n\n\n\n\nsource = \'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\'\ntarget = \'aclImdb_v1.tar.gz\'\n\n\ndef reporthook(count, block_size, total_size):\n    global start_time\n    if count == 0:\n        start_time = time.time()\n        return\n    duration = time.time() - start_time\n    progress_size = int(count * block_size)\n    speed = progress_size / (1024.**2 * duration)\n    percent = count * block_size * 100. / total_size\n    sys.stdout.write(""\\r%d%% | %d MB | %.2f MB/s | %d sec elapsed"" %\n                    (percent, progress_size / (1024.**2), speed, duration))\n    sys.stdout.flush()\n\n\nif not os.path.isdir(\'aclImdb\') and not os.path.isfile(\'aclImdb_v1.tar.gz\'):\n    \n    if (sys.version_info < (3, 0)):\n        import urllib\n        urllib.urlretrieve(source, target, reporthook)\n    \n    else:\n        import urllib.request\n        urllib.request.urlretrieve(source, target, reporthook)\n\n\n\n\nif not os.path.isdir(\'aclImdb\'):\n\n    with tarfile.open(target, \'r:gz\') as tar:\n        tar.extractall()\n\n\n# ## Preprocessing the movie dataset into more convenient format\n\n\n\n\n# change the `basepath` to the directory of the\n# unzipped movie dataset\n\nbasepath = \'aclImdb\'\n\nlabels = {\'pos\': 1, \'neg\': 0}\npbar = pyprind.ProgBar(50000)\ndf = pd.DataFrame()\nfor s in (\'test\', \'train\'):\n    for l in (\'pos\', \'neg\'):\n        path = os.path.join(basepath, s, l)\n        for file in os.listdir(path):\n            with open(os.path.join(path, file), \n                      \'r\', encoding=\'utf-8\') as infile:\n                txt = infile.read()\n            df = df.append([[txt, labels[l]]], \n                           ignore_index=True)\n            pbar.update()\ndf.columns = [\'review\', \'sentiment\']\n\n\n# Shuffling the DataFrame:\n\n\n\n\nnp.random.seed(0)\ndf = df.reindex(np.random.permutation(df.index))\n\n\n# Optional: Saving the assembled data as CSV file:\n\n\n\ndf.to_csv(\'movie_data.csv\', index=False, encoding=\'utf-8\')\n\n\n\n\n\ndf = pd.read_csv(\'movie_data.csv\', encoding=\'utf-8\')\ndf.head(3)\n\n\n# ### Note\n# \n# If you have problems with creating the `movie_data.csv` file in the previous chapter, you can find a download a zip archive at \n# https://github.com/rasbt/python-machine-learning-book-2nd-edition/tree/master/code/ch08/\n\n\n# # Introducing the bag-of-words model\n\n# ...\n\n# ## Transforming documents into feature vectors\n\n# By calling the fit_transform method on CountVectorizer, we just constructed the vocabulary of the bag-of-words model and transformed the following three sentences into sparse feature vectors:\n# 1. The sun is shining\n# 2. The weather is sweet\n# 3. The sun is shining, the weather is sweet, and one and one is two\n# \n\n\n\n\ncount = CountVectorizer()\ndocs = np.array([\n        \'The sun is shining\',\n        \'The weather is sweet\',\n        \'The sun is shining, the weather is sweet, and one and one is two\'])\nbag = count.fit_transform(docs)\n\n\n# Now let us print the contents of the vocabulary to get a better understanding of the underlying concepts:\n\n\n\nprint(count.vocabulary_)\n\n\n# As we can see from executing the preceding command, the vocabulary is stored in a Python dictionary, which maps the unique words that are mapped to integer indices. Next let us print the feature vectors that we just created:\n\n# Each index position in the feature vectors shown here corresponds to the integer values that are stored as dictionary items in the CountVectorizer vocabulary. For example, the  rst feature at index position 0 resembles the count of the word and, which only occurs in the last document, and the word is at index position 1 (the 2nd feature in the document vectors) occurs in all three sentences. Those values in the feature vectors are also called the raw term frequencies: *tf (t,d)*\xe2\x80\x94the number of times a term t occurs in a document *d*.\n\n\n\nprint(bag.toarray())\n\n\n\n# ## Assessing word relevancy via term frequency-inverse document frequency\n\n\n\nnp.set_printoptions(precision=2)\n\n\n# When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. Those frequently occurring words typically don\'t contain useful or discriminatory information. In this subsection, we will learn about a useful technique called term frequency-inverse document frequency (tf-idf) that can be used to downweight those frequently occurring words in the feature vectors. The tf-idf can be de ned as the product of the term frequency and the inverse document frequency:\n# \n# $$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$\n# \n# Here the tf(t, d) is the term frequency that we introduced in the previous section,\n# and the inverse document frequency *idf(t, d)* can be calculated as:\n# \n# $$\\text{idf}(t,d) = \\text{log}\\frac{n_d}{1+\\text{df}(d, t)},$$\n# \n# where $n_d$ is the total number of documents, and *df(d, t)* is the number of documents *d* that contain the term *t*. Note that adding the constant 1 to the denominator is optional and serves the purpose of assigning a non-zero value to terms that occur in all training samples; the log is used to ensure that low document frequencies are not given too much weight.\n# \n# Scikit-learn implements yet another transformer, the `TfidfTransformer`, that takes the raw term frequencies from `CountVectorizer` as input and transforms them into tf-idfs:\n\n\n\n\ntfidf = TfidfTransformer(use_idf=True, \n                         norm=\'l2\', \n                         smooth_idf=True)\nprint(tfidf.fit_transform(count.fit_transform(docs))\n      .toarray())\n\n\n# As we saw in the previous subsection, the word is had the largest term frequency in the 3rd document, being the most frequently occurring word. However, after transforming the same feature vector into tf-idfs, we see that the word is is\n# now associated with a relatively small tf-idf (0.45) in document 3 since it is\n# also contained in documents 1 and 2 and thus is unlikely to contain any useful, discriminatory information.\n# \n\n# However, if we\'d manually calculated the tf-idfs of the individual terms in our feature vectors, we\'d have noticed that the `TfidfTransformer` calculates the tf-idfs slightly differently compared to the standard textbook equations that we de ned earlier. The equations for the idf and tf-idf that were implemented in scikit-learn are:\n\n# $$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$\n# \n# The tf-idf equation that was implemented in scikit-learn is as follows:\n# \n# $$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$\n# \n# While it is also more typical to normalize the raw term frequencies before calculating the tf-idfs, the `TfidfTransformer` normalizes the tf-idfs directly.\n# \n# By default (`norm=\'l2\'`), scikit-learn\'s TfidfTransformer applies the L2-normalization, which returns a vector of length 1 by dividing an un-normalized feature vector *v* by its L2-norm:\n# \n# $$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$\n# \n# To make sure that we understand how TfidfTransformer works, let us walk\n# through an example and calculate the tf-idf of the word is in the 3rd document.\n# \n# The word is has a term frequency of 3 (tf = 3) in document 3, and the document frequency of this term is 3 since the term is occurs in all three documents (df = 3). Thus, we can calculate the idf as follows:\n# \n# $$\\text{idf}(""is"", d3) = log \\frac{1+3}{1+3} = 0$$\n# \n# Now in order to calculate the tf-idf, we simply need to add 1 to the inverse document frequency and multiply it by the term frequency:\n# \n# $$\\text{tf-idf}(""is"",d3)= 3 \\times (0+1) = 3$$\n\n\n\ntf_is = 3\nn_docs = 3\nidf_is = np.log((n_docs+1) / (3+1))\ntfidf_is = tf_is * (idf_is + 1)\nprint(\'tf-idf of term ""is"" = %.2f\' % tfidf_is)\n\n\n# If we repeated these calculations for all terms in the 3rd document, we\'d obtain the following tf-idf vectors: [3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]. However, we notice that the values in this feature vector are different from the values that we obtained from the TfidfTransformer that we used previously. The  nal step that we are missing in this tf-idf calculation is the L2-normalization, which can be applied as follows:\n\n# $$\\text{tfi-df}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]}{\\sqrt{[3.39^2, 3.0^2, 3.39^2, 1.29^2, 1.29^2, 1.29^2, 2.0^2 , 1.69^2, 1.29^2]}}$$\n# \n# $$=[0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$\n# \n# $$\\Rightarrow \\text{tfi-df}_{norm}(""is"", d3) = 0.45$$\n\n# As we can see, the results match the results returned by scikit-learn\'s `TfidfTransformer` (below). Since we now understand how tf-idfs are calculated, let us proceed to the next sections and apply those concepts to the movie review dataset.\n\n\n\ntfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\nraw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1]\nraw_tfidf \n\n\n\n\nl2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\nl2_tfidf\n\n\n\n# ## Cleaning text data\n\n\n\ndf.loc[0, \'review\'][-50:]\n\n\n\n\ndef preprocessor(text):\n    text = re.sub(\'<[^>]*>\', \'\', text)\n    emoticons = re.findall(\'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\',\n                           text)\n    text = (re.sub(\'[\\W]+\', \' \', text.lower()) +\n            \' \'.join(emoticons).replace(\'-\', \'\'))\n    return text\n\n\n\n\npreprocessor(df.loc[0, \'review\'][-50:])\n\n\n\n\npreprocessor(""</a>This :) is :( a test :-)!"")\n\n\n\n\ndf[\'review\'] = df[\'review\'].apply(preprocessor)\n\n\n\n# ## Processing documents into tokens\n\n\n\n\nporter = PorterStemmer()\n\ndef tokenizer(text):\n    return text.split()\n\n\ndef tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]\n\n\n\n\ntokenizer(\'runners like running and thus they run\')\n\n\n\n\ntokenizer_porter(\'runners like running and thus they run\')\n\n\n\n\n\nnltk.download(\'stopwords\')\n\n\n\n\n\nstop = stopwords.words(\'english\')\n[w for w in tokenizer_porter(\'a runner likes running and runs a lot\')[-10:]\nif w not in stop]\n\n\n\n# # Training a logistic regression model for document classification\n\n# Strip HTML and punctuation to speed up the GridSearch later:\n\n\n\nX_train = df.loc[:25000, \'review\'].values\ny_train = df.loc[:25000, \'sentiment\'].values\nX_test = df.loc[25000:, \'review\'].values\ny_test = df.loc[25000:, \'sentiment\'].values\n\n\n\n\n\ntfidf = TfidfVectorizer(strip_accents=None,\n                        lowercase=False,\n                        preprocessor=None)\n\nparam_grid = [{\'vect__ngram_range\': [(1, 1)],\n               \'vect__stop_words\': [stop, None],\n               \'vect__tokenizer\': [tokenizer, tokenizer_porter],\n               \'clf__penalty\': [\'l1\', \'l2\'],\n               \'clf__C\': [1.0, 10.0, 100.0]},\n              {\'vect__ngram_range\': [(1, 1)],\n               \'vect__stop_words\': [stop, None],\n               \'vect__tokenizer\': [tokenizer, tokenizer_porter],\n               \'vect__use_idf\':[False],\n               \'vect__norm\':[None],\n               \'clf__penalty\': [\'l1\', \'l2\'],\n               \'clf__C\': [1.0, 10.0, 100.0]},\n              ]\n\nlr_tfidf = Pipeline([(\'vect\', tfidf),\n                     (\'clf\', LogisticRegression(random_state=0))])\n\ngs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n                           scoring=\'accuracy\',\n                           cv=5,\n                           verbose=1,\n                           n_jobs=-1)\n\n\n# **Important Note about `n_jobs`**\n# \n# Please note that it is highly recommended to use `n_jobs=-1` (instead of `n_jobs=1`) in the previous code example to utilize all available cores on your machine and speed up the grid search. However, some Windows users reported issues when running the previous code with the `n_jobs=-1` setting related to pickling the tokenizer and tokenizer_porter functions for multiprocessing on Windows. Another workaround would be to replace those two functions, `[tokenizer, tokenizer_porter]`, with `[str.split]`. However, note that the replacement by the simple `str.split` would not support stemming.\n\n# **Important Note about the running time**\n# \n# Executing the following code cell **may take up to 30-60 min** depending on your machine, since based on the parameter grid we defined, there are 2*2*2*3*5 + 2*2*2*3*5 = 240 models to fit.\n# \n# If you do not wish to wait so long, you could reduce the size of the dataset by decreasing the number of training samples, for example, as follows:\n# \n#     X_train = df.loc[:2500, \'review\'].values\n#     y_train = df.loc[:2500, \'sentiment\'].values\n#     \n# However, note that decreasing the training set size to such a small number will likely result in poorly performing models. Alternatively, you can delete parameters from the grid above to reduce the number of models to fit -- for example, by using the following:\n# \n#     param_grid = [{\'vect__ngram_range\': [(1, 1)],\n#                    \'vect__stop_words\': [stop, None],\n#                    \'vect__tokenizer\': [tokenizer],\n#                    \'clf__penalty\': [\'l1\', \'l2\'],\n#                    \'clf__C\': [1.0, 10.0]},\n#                   ]\n\n\n\n## @Readers: PLEASE IGNORE THIS CELL\n##\n## This cell is meant to generate more \n## ""logging"" output when this notebook is run \n## on the Travis Continuous Integration\n## platform to test the code as well as\n## speeding up the run using a smaller\n## dataset for debugging\n\nif \'TRAVIS\' in os.environ:\n    gs_lr_tfidf.verbose=2\n    X_train = df.loc[:250, \'review\'].values\n    y_train = df.loc[:250, \'sentiment\'].values\n    X_test = df.loc[25000:25250, \'review\'].values\n    y_test = df.loc[25000:25250, \'sentiment\'].values\n\n\n\n\ngs_lr_tfidf.fit(X_train, y_train)\n\n\n\n\nprint(\'Best parameter set: %s \' % gs_lr_tfidf.best_params_)\nprint(\'CV Accuracy: %.3f\' % gs_lr_tfidf.best_score_)\n\n\n\n\nclf = gs_lr_tfidf.best_estimator_\nprint(\'Test Accuracy: %.3f\' % clf.score(X_test, y_test))\n\n\n\n# ####  Start comment:\n#     \n# Please note that `gs_lr_tfidf.best_score_` is the average k-fold cross-validation score. I.e., if we have a `GridSearchCV` object with 5-fold cross-validation (like the one above), the `best_score_` attribute returns the average score over the 5-folds of the best model. To illustrate this with an example:\n\n\n\n\n\nnp.random.seed(0)\nnp.set_printoptions(precision=6)\ny = [np.random.randint(3) for i in range(25)]\nX = (y + np.random.randn(25)).reshape(-1, 1)\n\ncv5_idx = list(StratifiedKFold(n_splits=5, shuffle=False, random_state=0).split(X, y))\n    \ncross_val_score(LogisticRegression(random_state=123), X, y, cv=cv5_idx)\n\n\n# By executing the code above, we created a simple data set of random integers that shall represent our class labels. Next, we fed the indices of 5 cross-validation folds (`cv3_idx`) to the `cross_val_score` scorer, which returned 5 accuracy scores -- these are the 5 accuracy values for the 5 test folds.  \n# \n# Next, let us use the `GridSearchCV` object and feed it the same 5 cross-validation sets (via the pre-generated `cv3_idx` indices):\n\n\n\n\ngs = GridSearchCV(LogisticRegression(), {}, cv=cv5_idx, verbose=3).fit(X, y) \n\n\n# As we can see, the scores for the 5 folds are exactly the same as the ones from `cross_val_score` earlier.\n\n# Now, the best_score_ attribute of the `GridSearchCV` object, which becomes available after `fit`ting, returns the average accuracy score of the best model:\n\n\n\ngs.best_score_\n\n\n# As we can see, the result above is consistent with the average score computed the `cross_val_score`.\n\n\n\ncross_val_score(LogisticRegression(), X, y, cv=cv5_idx).mean()\n\n\n# #### End comment.\n# \n\n\n# # Working with bigger data - online algorithms and out-of-core learning\n\n\n\n\ndef tokenizer(text):\n    text = re.sub(\'<[^>]*>\', \'\', text)\n    emoticons = re.findall(\'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\', text.lower())\n    text = re.sub(\'[\\W]+\', \' \', text.lower()) +        \' \'.join(emoticons).replace(\'-\', \'\')\n    tokenized = [w for w in text.split() if w not in stop]\n    return tokenized\n\n\ndef stream_docs(path):\n    with open(path, \'r\', encoding=\'utf-8\') as csv:\n        next(csv)  # skip header\n        for line in csv:\n            text, label = line[:-3], int(line[-2])\n            yield text, label\n\n\n\n\nnext(stream_docs(path=\'movie_data.csv\'))\n\n\n\n\ndef get_minibatch(doc_stream, size):\n    docs, y = [], []\n    try:\n        for _ in range(size):\n            text, label = next(doc_stream)\n            docs.append(text)\n            y.append(label)\n    except StopIteration:\n        return None, None\n    return docs, y\n\n\n\n\n\nvect = HashingVectorizer(decode_error=\'ignore\', \n                         n_features=2**21,\n                         preprocessor=None, \n                         tokenizer=tokenizer)\n\nif Version(sklearn_version) < \'0.18\':\n    clf = SGDClassifier(loss=\'log\', random_state=1, n_iter=1)\nelse:\n    clf = SGDClassifier(loss=\'log\', random_state=1, max_iter=1)\n\ndoc_stream = stream_docs(path=\'movie_data.csv\')\n\n\n# **Note**\n# \n# - You can replace `Perceptron(n_iter, ...)` by `Perceptron(max_iter, ...)` in scikit-learn >= 0.19. The `n_iter` parameter is used here deriberately, because some people still use scikit-learn 0.18.\n# \n\n\n\npbar = pyprind.ProgBar(45)\n\nclasses = np.array([0, 1])\nfor _ in range(45):\n    X_train, y_train = get_minibatch(doc_stream, size=1000)\n    if not X_train:\n        break\n    X_train = vect.transform(X_train)\n    clf.partial_fit(X_train, y_train, classes=classes)\n    pbar.update()\n\n\n\n\nX_test, y_test = get_minibatch(doc_stream, size=5000)\nX_test = vect.transform(X_test)\nprint(\'Accuracy: %.3f\' % clf.score(X_test, y_test))\n\n\n\n\nclf = clf.partial_fit(X_test, y_test)\n\n\n# ## Topic modeling\n\n# ### Decomposing text documents with Latent Dirichlet Allocation\n\n# ### Latent Dirichlet Allocation with scikit-learn\n\n\n\n\ndf = pd.read_csv(\'movie_data.csv\', encoding=\'utf-8\')\ndf.head(3)\n\n\n\n\n## @Readers: PLEASE IGNORE THIS CELL\n##\n## This cell is meant to create a smaller dataset if\n## the notebook is run on the Travis Continuous Integration\n## platform to test the code on a smaller dataset\n## to prevent timeout errors and just serves a debugging tool\n## for this notebook\n\nif \'TRAVIS\' in os.environ:\n    df.loc[:500].to_csv(\'movie_data.csv\')\n    df = pd.read_csv(\'movie_data.csv\', nrows=500)\n    print(\'SMALL DATA SUBSET CREATED FOR TESTING\')\n\n\n\n\n\ncount = CountVectorizer(stop_words=\'english\',\n                        max_df=.1,\n                        max_features=5000)\nX = count.fit_transform(df[\'review\'].values)\n\n\n\n\n\nlda = LatentDirichletAllocation(n_topics=10,\n                                random_state=123,\n                                learning_method=\'batch\')\nX_topics = lda.fit_transform(X)\n\n\n\n\nlda.components_.shape\n\n\n\n\nn_top_words = 5\nfeature_names = count.get_feature_names()\n\nfor topic_idx, topic in enumerate(lda.components_):\n    print(""Topic %d:"" % (topic_idx + 1))\n    print("" "".join([feature_names[i]\n                    for i in topic.argsort()\\\n                        [:-n_top_words - 1:-1]]))\n\n\n# Based on reading the 5 most important words for each topic, we may guess that the LDA identified the following topics:\n#     \n# 1. Generally bad movies (not really a topic category)\n# 2. Movies about families\n# 3. War movies\n# 4. Art movies\n# 5. Crime movies\n# 6. Horror movies\n# 7. Comedies\n# 8. Movies somehow related to TV shows\n# 9. Movies based on books\n# 10. Action movies\n\n# To confirm that the categories make sense based on the reviews, let\'s plot 5 movies from the horror movie category (category 6 at index position 5):\n\n\n\nhorror = X_topics[:, 5].argsort()[::-1]\n\nfor iter_idx, movie_idx in enumerate(horror[:3]):\n    print(\'\\nHorror movie #%d:\' % (iter_idx + 1))\n    print(df[\'review\'][movie_idx][:300], \'...\')\n\n\n# Using the preceeding code example, we printed the first 300 characters from the top 3 horror movies and indeed, we can see that the reviews -- even though we don\'t know which exact movie they belong to -- sound like reviews of horror movies, indeed. (However, one might argue that movie #2 could also belong to topic category 1.)\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
code/ch10/ch10.py,0,"b'# coding: utf-8\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import RANSACRegressor\nfrom sklearn.model_selection import train_test_split\nimport scipy as sp\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# *Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 10 - Predicting Continuous Target Variables with Regression Analysis\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*\n\n# The seaborn package, a visualization library built on top of matplotlib, can be installed via \n# \n#     conda install seaborn\n# \n# or \n# \n#     pip install seaborn\n\n\n# ### Overview\n\n# - [Introducing regression](#Introducing-linear-regression)\n#   - [Simple linear regression](#Simple-linear-regression)\n# - [Exploring the Housing Dataset](#Exploring-the-Housing-Dataset)\n#   - [Loading the Housing dataset into a data frame](Loading-the-Housing-dataset-into-a-data-frame)\n#   - [Visualizing the important characteristics of a dataset](#Visualizing-the-important-characteristics-of-a-dataset)\n# - [Implementing an ordinary least squares linear regression model](#Implementing-an-ordinary-least-squares-linear-regression-model)\n#   - [Solving regression for regression parameters with gradient descent](#Solving-regression-for-regression-parameters-with-gradient-descent)\n#   - [Estimating the coefficient of a regression model via scikit-learn](#Estimating-the-coefficient-of-a-regression-model-via-scikit-learn)\n# - [Fitting a robust regression model using RANSAC](#Fitting-a-robust-regression-model-using-RANSAC)\n# - [Evaluating the performance of linear regression models](#Evaluating-the-performance-of-linear-regression-models)\n# - [Using regularized methods for regression](#Using-regularized-methods-for-regression)\n# - [Turning a linear regression model into a curve - polynomial regression](#Turning-a-linear-regression-model-into-a-curve---polynomial-regression)\n#   - [Modeling nonlinear relationships in the Housing Dataset](#Modeling-nonlinear-relationships-in-the-Housing-Dataset)\n#   - [Dealing with nonlinear relationships using random forests](#Dealing-with-nonlinear-relationships-using-random-forests)\n#     - [Decision tree regression](#Decision-tree-regression)\n#     - [Random forest regression](#Random-forest-regression)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Introducing linear regression\n\n# ## Simple linear regression\n\n\n\n\n\n# ## Multiple linear regression\n\n\n\n\n\n\n# # Exploring the Housing dataset\n\n# ## Loading the Housing dataset into a data frame\n\n# Description, which was previously available at: [https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)\n# \n# Attributes:\n#     \n# <pre>\n# 1. CRIM      per capita crime rate by town\n# 2. ZN        proportion of residential land zoned for lots over \n#                  25,000 sq.ft.\n# 3. INDUS     proportion of non-retail business acres per town\n# 4. CHAS      Charles River dummy variable (= 1 if tract bounds \n#                  river; 0 otherwise)\n# 5. NOX       nitric oxides concentration (parts per 10 million)\n# 6. RM        average number of rooms per dwelling\n# 7. AGE       proportion of owner-occupied units built prior to 1940\n# 8. DIS       weighted distances to five Boston employment centres\n# 9. RAD       index of accessibility to radial highways\n# 10. TAX      full-value property-tax rate per $10,000\n# 11. PTRATIO  pupil-teacher ratio by town\n# 12. B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks \n#                  by town\n# 13. LSTAT    % lower status of the population\n# 14. MEDV     Median value of owner-occupied homes in $1000s\n# </pre>\n\n\n\n\ndf = pd.read_csv(\'https://raw.githubusercontent.com/rasbt/\'\n                 \'python-machine-learning-book-2nd-edition\'\n                 \'/master/code/ch10/housing.data.txt\',\n                 header=None,\n                 sep=\'\\s+\')\n\ndf.columns = [\'CRIM\', \'ZN\', \'INDUS\', \'CHAS\', \n              \'NOX\', \'RM\', \'AGE\', \'DIS\', \'RAD\', \n              \'TAX\', \'PTRATIO\', \'B\', \'LSTAT\', \'MEDV\']\ndf.head()\n\n\n# \n# ### Note:\n# \n# \n# You can find a copy of the housing dataset (and all other datasets used in this book) in the code bundle of this book, which you can use if you are working offline or the UCI server at https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data is temporarily unavailable. For instance, to load the housing dataset from a local directory, you can replace the lines\n# df = pd.read_csv(\'https://archive.ics.uci.edu/ml/\'\n#                  \'machine-learning-databases\'\n#                  \'/housing/housing.data\',\n#                  sep=\'\\s+\')\n# in the following code example by \n# df = pd.read_csv(\'./housing.data\',\n#                  sep=\'\\s+\')\n\n\n# ## Visualizing the important characteristics of a dataset\n\n\n\n\n\n\n\ncols = [\'LSTAT\', \'INDUS\', \'NOX\', \'RM\', \'MEDV\']\n\nsns.pairplot(df[cols], size=2.5)\nplt.tight_layout()\n# plt.savefig(\'images/10_03.png\', dpi=300)\nplt.show()\n\n\n\n\n\n\ncm = np.corrcoef(df[cols].values.T)\n#sns.set(font_scale=1.5)\nhm = sns.heatmap(cm,\n                 cbar=True,\n                 annot=True,\n                 square=True,\n                 fmt=\'.2f\',\n                 annot_kws={\'size\': 15},\n                 yticklabels=cols,\n                 xticklabels=cols)\n\nplt.tight_layout()\n# plt.savefig(\'images/10_04.png\', dpi=300)\nplt.show()\n\n\n\n# # Implementing an ordinary least squares linear regression model\n\n# ...\n\n# ## Solving regression for regression parameters with gradient descent\n\n\n\nclass LinearRegressionGD(object):\n\n    def __init__(self, eta=0.001, n_iter=20):\n        self.eta = eta\n        self.n_iter = n_iter\n\n    def fit(self, X, y):\n        self.w_ = np.zeros(1 + X.shape[1])\n        self.cost_ = []\n\n        for i in range(self.n_iter):\n            output = self.net_input(X)\n            errors = (y - output)\n            self.w_[1:] += self.eta * X.T.dot(errors)\n            self.w_[0] += self.eta * errors.sum()\n            cost = (errors**2).sum() / 2.0\n            self.cost_.append(cost)\n        return self\n\n    def net_input(self, X):\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def predict(self, X):\n        return self.net_input(X)\n\n\n\n\nX = df[[\'RM\']].values\ny = df[\'MEDV\'].values\n\n\n\n\n\n\nsc_x = StandardScaler()\nsc_y = StandardScaler()\nX_std = sc_x.fit_transform(X)\ny_std = sc_y.fit_transform(y[:, np.newaxis]).flatten()\n\n\n\n\nlr = LinearRegressionGD()\nlr.fit(X_std, y_std)\n\n\n\n\nplt.plot(range(1, lr.n_iter+1), lr.cost_)\nplt.ylabel(\'SSE\')\nplt.xlabel(\'Epoch\')\n#plt.tight_layout()\n#plt.savefig(\'images/10_05.png\', dpi=300)\nplt.show()\n\n\n\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c=\'steelblue\', edgecolor=\'white\', s=70)\n    plt.plot(X, model.predict(X), color=\'black\', lw=2)    \n    return \n\n\n\n\nlin_regplot(X_std, y_std, lr)\nplt.xlabel(\'Average number of rooms [RM] (standardized)\')\nplt.ylabel(\'Price in $1000s [MEDV] (standardized)\')\n\n#plt.savefig(\'images/10_06.png\', dpi=300)\nplt.show()\n\n\n\n\nprint(\'Slope: %.3f\' % lr.w_[1])\nprint(\'Intercept: %.3f\' % lr.w_[0])\n\n\n\n\nnum_rooms_std = sc_x.transform(np.array([[5.0]]))\nprice_std = lr.predict(num_rooms_std)\nprint(""Price in $1000s: %.3f"" % sc_y.inverse_transform(price_std))\n\n\n\n# ## Estimating the coefficient of a regression model via scikit-learn\n\n\n\n\n\n\n\nslr = LinearRegression()\nslr.fit(X, y)\ny_pred = slr.predict(X)\nprint(\'Slope: %.3f\' % slr.coef_[0])\nprint(\'Intercept: %.3f\' % slr.intercept_)\n\n\n\n\nlin_regplot(X, y, slr)\nplt.xlabel(\'Average number of rooms [RM]\')\nplt.ylabel(\'Price in $1000s [MEDV]\')\n\n#plt.savefig(\'images/10_07.png\', dpi=300)\nplt.show()\n\n\n# **Normal Equations** alternative:\n\n\n\n# adding a column vector of ""ones""\nXb = np.hstack((np.ones((X.shape[0], 1)), X))\nw = np.zeros(X.shape[1])\nz = np.linalg.inv(np.dot(Xb.T, Xb))\nw = np.dot(z, np.dot(Xb.T, y))\n\nprint(\'Slope: %.3f\' % w[1])\nprint(\'Intercept: %.3f\' % w[0])\n\n\n\n# # Fitting a robust regression model using RANSAC\n\n\n\n\nransac = RANSACRegressor(LinearRegression(), \n                         max_trials=100, \n                         min_samples=50, \n                         loss=\'absolute_loss\', \n                         residual_threshold=5.0, \n                         random_state=0)\n\n\nransac.fit(X, y)\n\ninlier_mask = ransac.inlier_mask_\noutlier_mask = np.logical_not(inlier_mask)\n\nline_X = np.arange(3, 10, 1)\nline_y_ransac = ransac.predict(line_X[:, np.newaxis])\nplt.scatter(X[inlier_mask], y[inlier_mask],\n            c=\'steelblue\', edgecolor=\'white\', \n            marker=\'o\', label=\'Inliers\')\nplt.scatter(X[outlier_mask], y[outlier_mask],\n            c=\'limegreen\', edgecolor=\'white\', \n            marker=\'s\', label=\'Outliers\')\nplt.plot(line_X, line_y_ransac, color=\'black\', lw=2)   \nplt.xlabel(\'Average number of rooms [RM]\')\nplt.ylabel(\'Price in $1000s [MEDV]\')\nplt.legend(loc=\'upper left\')\n\n#plt.savefig(\'images/10_08.png\', dpi=300)\nplt.show()\n\n\n\n\nprint(\'Slope: %.3f\' % ransac.estimator_.coef_[0])\nprint(\'Intercept: %.3f\' % ransac.estimator_.intercept_)\n\n\n\n# # Evaluating the performance of linear regression models\n\n\n\n\nX = df.iloc[:, :-1].values\ny = df[\'MEDV\'].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0)\n\n\n\n\nslr = LinearRegression()\n\nslr.fit(X_train, y_train)\ny_train_pred = slr.predict(X_train)\ny_test_pred = slr.predict(X_test)\n\n\n\n\n\nary = np.array(range(100000))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.scatter(y_train_pred,  y_train_pred - y_train,\n            c=\'steelblue\', marker=\'o\', edgecolor=\'white\',\n            label=\'Training data\')\nplt.scatter(y_test_pred,  y_test_pred - y_test,\n            c=\'limegreen\', marker=\'s\', edgecolor=\'white\',\n            label=\'Test data\')\nplt.xlabel(\'Predicted values\')\nplt.ylabel(\'Residuals\')\nplt.legend(loc=\'upper left\')\nplt.hlines(y=0, xmin=-10, xmax=50, color=\'black\', lw=2)\nplt.xlim([-10, 50])\nplt.tight_layout()\n\n# plt.savefig(\'images/10_09.png\', dpi=300)\nplt.show()\n\n\n\n\n\nprint(\'MSE train: %.3f, test: %.3f\' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint(\'R^2 train: %.3f, test: %.3f\' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))\n\n\n\n# # Using regularized methods for regression\n\n\n\n\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\ny_train_pred = lasso.predict(X_train)\ny_test_pred = lasso.predict(X_test)\nprint(lasso.coef_)\n\n\n\n\nprint(\'MSE train: %.3f, test: %.3f\' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint(\'R^2 train: %.3f, test: %.3f\' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))\n\n\n# Ridge regression:\n\n\n\nridge = Ridge(alpha=1.0)\n\n\n# LASSO regression:\n\n\n\nlasso = Lasso(alpha=1.0)\n\n\n# Elastic Net regression:\n\n\n\nelanet = ElasticNet(alpha=1.0, l1_ratio=0.5)\n\n\n\n# # Turning a linear regression model into a curve - polynomial regression\n\n\n\nX = np.array([258.0, 270.0, 294.0, \n              320.0, 342.0, 368.0, \n              396.0, 446.0, 480.0, 586.0])\\\n             [:, np.newaxis]\n\ny = np.array([236.4, 234.4, 252.8, \n              298.6, 314.2, 342.2, \n              360.8, 368.0, 391.2,\n              390.8])\n\n\n\n\n\nlr = LinearRegression()\npr = LinearRegression()\nquadratic = PolynomialFeatures(degree=2)\nX_quad = quadratic.fit_transform(X)\n\n\n\n\n# fit linear features\nlr.fit(X, y)\nX_fit = np.arange(250, 600, 10)[:, np.newaxis]\ny_lin_fit = lr.predict(X_fit)\n\n# fit quadratic features\npr.fit(X_quad, y)\ny_quad_fit = pr.predict(quadratic.fit_transform(X_fit))\n\n# plot results\nplt.scatter(X, y, label=\'training points\')\nplt.plot(X_fit, y_lin_fit, label=\'linear fit\', linestyle=\'--\')\nplt.plot(X_fit, y_quad_fit, label=\'quadratic fit\')\nplt.legend(loc=\'upper left\')\n\nplt.tight_layout()\n#plt.savefig(\'images/10_10.png\', dpi=300)\nplt.show()\n\n\n\n\ny_lin_pred = lr.predict(X)\ny_quad_pred = pr.predict(X_quad)\n\n\n\n\nprint(\'Training MSE linear: %.3f, quadratic: %.3f\' % (\n        mean_squared_error(y, y_lin_pred),\n        mean_squared_error(y, y_quad_pred)))\nprint(\'Training R^2 linear: %.3f, quadratic: %.3f\' % (\n        r2_score(y, y_lin_pred),\n        r2_score(y, y_quad_pred)))\n\n\n\n# ## Modeling nonlinear relationships in the Housing Dataset\n\n\n\nX = df[[\'LSTAT\']].values\ny = df[\'MEDV\'].values\n\nregr = LinearRegression()\n\n# create quadratic features\nquadratic = PolynomialFeatures(degree=2)\ncubic = PolynomialFeatures(degree=3)\nX_quad = quadratic.fit_transform(X)\nX_cubic = cubic.fit_transform(X)\n\n# fit features\nX_fit = np.arange(X.min(), X.max(), 1)[:, np.newaxis]\n\nregr = regr.fit(X, y)\ny_lin_fit = regr.predict(X_fit)\nlinear_r2 = r2_score(y, regr.predict(X))\n\nregr = regr.fit(X_quad, y)\ny_quad_fit = regr.predict(quadratic.fit_transform(X_fit))\nquadratic_r2 = r2_score(y, regr.predict(X_quad))\n\nregr = regr.fit(X_cubic, y)\ny_cubic_fit = regr.predict(cubic.fit_transform(X_fit))\ncubic_r2 = r2_score(y, regr.predict(X_cubic))\n\n\n# plot results\nplt.scatter(X, y, label=\'training points\', color=\'lightgray\')\n\nplt.plot(X_fit, y_lin_fit, \n         label=\'linear (d=1), $R^2=%.2f$\' % linear_r2, \n         color=\'blue\', \n         lw=2, \n         linestyle=\':\')\n\nplt.plot(X_fit, y_quad_fit, \n         label=\'quadratic (d=2), $R^2=%.2f$\' % quadratic_r2,\n         color=\'red\', \n         lw=2,\n         linestyle=\'-\')\n\nplt.plot(X_fit, y_cubic_fit, \n         label=\'cubic (d=3), $R^2=%.2f$\' % cubic_r2,\n         color=\'green\', \n         lw=2, \n         linestyle=\'--\')\n\nplt.xlabel(\'% lower status of the population [LSTAT]\')\nplt.ylabel(\'Price in $1000s [MEDV]\')\nplt.legend(loc=\'upper right\')\n\n#plt.savefig(\'images/10_11.png\', dpi=300)\nplt.show()\n\n\n# Transforming the dataset:\n\n\n\nX = df[[\'LSTAT\']].values\ny = df[\'MEDV\'].values\n\n# transform features\nX_log = np.log(X)\ny_sqrt = np.sqrt(y)\n\n# fit features\nX_fit = np.arange(X_log.min()-1, X_log.max()+1, 1)[:, np.newaxis]\n\nregr = regr.fit(X_log, y_sqrt)\ny_lin_fit = regr.predict(X_fit)\nlinear_r2 = r2_score(y_sqrt, regr.predict(X_log))\n\n# plot results\nplt.scatter(X_log, y_sqrt, label=\'training points\', color=\'lightgray\')\n\nplt.plot(X_fit, y_lin_fit, \n         label=\'linear (d=1), $R^2=%.2f$\' % linear_r2, \n         color=\'blue\', \n         lw=2)\n\nplt.xlabel(\'log(% lower status of the population [LSTAT])\')\nplt.ylabel(\'$\\sqrt{Price \\; in \\; \\$1000s \\; [MEDV]}$\')\nplt.legend(loc=\'lower left\')\n\nplt.tight_layout()\n#plt.savefig(\'images/10_12.png\', dpi=300)\nplt.show()\n\n\n\n# # Dealing with nonlinear relationships using random forests\n\n# ...\n\n# ## Decision tree regression\n\n\n\n\nX = df[[\'LSTAT\']].values\ny = df[\'MEDV\'].values\n\ntree = DecisionTreeRegressor(max_depth=3)\ntree.fit(X, y)\n\nsort_idx = X.flatten().argsort()\n\nlin_regplot(X[sort_idx], y[sort_idx], tree)\nplt.xlabel(\'% lower status of the population [LSTAT]\')\nplt.ylabel(\'Price in $1000s [MEDV]\')\n#plt.savefig(\'images/10_13.png\', dpi=300)\nplt.show()\n\n\n\n# ## Random forest regression\n\n\n\nX = df.iloc[:, :-1].values\ny = df[\'MEDV\'].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.4, random_state=1)\n\n\n\n\n\nforest = RandomForestRegressor(n_estimators=1000, \n                               criterion=\'mse\', \n                               random_state=1, \n                               n_jobs=-1)\nforest.fit(X_train, y_train)\ny_train_pred = forest.predict(X_train)\ny_test_pred = forest.predict(X_test)\n\nprint(\'MSE train: %.3f, test: %.3f\' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint(\'R^2 train: %.3f, test: %.3f\' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))\n\n\n\n\nplt.scatter(y_train_pred,  \n            y_train_pred - y_train, \n            c=\'steelblue\',\n            edgecolor=\'white\',\n            marker=\'o\', \n            s=35,\n            alpha=0.9,\n            label=\'training data\')\nplt.scatter(y_test_pred,  \n            y_test_pred - y_test, \n            c=\'limegreen\',\n            edgecolor=\'white\',\n            marker=\'s\', \n            s=35,\n            alpha=0.9,\n            label=\'test data\')\n\nplt.xlabel(\'Predicted values\')\nplt.ylabel(\'Residuals\')\nplt.legend(loc=\'upper left\')\nplt.hlines(y=0, xmin=-10, xmax=50, lw=2, color=\'black\')\nplt.xlim([-10, 50])\nplt.tight_layout()\n\n# plt.savefig(\'images/10_14.png\', dpi=300)\nplt.show()\n\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
code/ch11/ch11.py,0,"b'# coding: utf-8\n\n\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport numpy as np\nfrom matplotlib import cm\nfrom sklearn.metrics import silhouette_samples\nimport pandas as pd\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\n# from scipy.cluster.hierarchy import set_link_color_palette\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.datasets import make_moons\nfrom sklearn.cluster import DBSCAN\n\n# *Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 11 - Working with Unlabeled Data \xe2\x80\x93 Clustering Analysis\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*\n\n\n# ### Overview\n\n# - [Grouping objects by similarity using k-means](#Grouping-objects-by-similarity-using-k-means)\n#   - [K-means clustering using scikit-learn](#K-means-clustering-using-scikit-learn)\n#   - [A smarter way of placing the initial cluster centroids using k-means++](#A-smarter-way-of-placing-the-initial-cluster-centroids-using-k-means++)\n#   - [Hard versus soft clustering](#Hard-versus-soft-clustering)\n#   - [Using the elbow method to find the optimal number of clusters](#Using-the-elbow-method-to-find-the-optimal-number-of-clusters)\n#   - [Quantifying the quality of clustering via silhouette plots](#Quantifying-the-quality-of-clustering-via-silhouette-plots)\n# - [Organizing clusters as a hierarchical tree](#Organizing-clusters-as-a-hierarchical-tree)\n#   - [Grouping clusters in bottom-up fashion](#Grouping-clusters-in-bottom-up-fashion)\n#   - [Performing hierarchical clustering on a distance matrix](#Performing-hierarchical-clustering-on-a-distance-matrix)\n#   - [Attaching dendrograms to a heat map](#Attaching-dendrograms-to-a-heat-map)\n#   - [Applying agglomerative clustering via scikit-learn](#Applying-agglomerative-clustering-via-scikit-learn)\n# - [Locating regions of high density via DBSCAN](#Locating-regions-of-high-density-via-DBSCAN)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Grouping objects by similarity using k-means\n\n# ## K-means clustering using scikit-learn\n\n\n\n\nX, y = make_blobs(n_samples=150, \n                  n_features=2, \n                  centers=3, \n                  cluster_std=0.5, \n                  shuffle=True, \n                  random_state=0)\n\n\n\n\n\nplt.scatter(X[:, 0], X[:, 1], \n            c=\'white\', marker=\'o\', edgecolor=\'black\', s=50)\nplt.grid()\nplt.tight_layout()\n#plt.savefig(\'images/11_01.png\', dpi=300)\nplt.show()\n\n\n\n\n\nkm = KMeans(n_clusters=3, \n            init=\'random\', \n            n_init=10, \n            max_iter=300,\n            tol=1e-04,\n            random_state=0)\n\ny_km = km.fit_predict(X)\n\n\n\n\nplt.scatter(X[y_km == 0, 0],\n            X[y_km == 0, 1],\n            s=50, c=\'lightgreen\',\n            marker=\'s\', edgecolor=\'black\',\n            label=\'cluster 1\')\nplt.scatter(X[y_km == 1, 0],\n            X[y_km == 1, 1],\n            s=50, c=\'orange\',\n            marker=\'o\', edgecolor=\'black\',\n            label=\'cluster 2\')\nplt.scatter(X[y_km == 2, 0],\n            X[y_km == 2, 1],\n            s=50, c=\'lightblue\',\n            marker=\'v\', edgecolor=\'black\',\n            label=\'cluster 3\')\nplt.scatter(km.cluster_centers_[:, 0],\n            km.cluster_centers_[:, 1],\n            s=250, marker=\'*\',\n            c=\'red\', edgecolor=\'black\',\n            label=\'centroids\')\nplt.legend(scatterpoints=1)\nplt.grid()\nplt.tight_layout()\n#plt.savefig(\'images/11_02.png\', dpi=300)\nplt.show()\n\n\n\n# ## A smarter way of placing the initial cluster centroids using k-means++\n\n# ...\n\n# ## Hard versus soft clustering\n\n# ...\n\n# ## Using the elbow method to find the optimal number of clusters \n\n\n\nprint(\'Distortion: %.2f\' % km.inertia_)\n\n\n\n\ndistortions = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters=i, \n                init=\'k-means++\', \n                n_init=10, \n                max_iter=300, \n                random_state=0)\n    km.fit(X)\n    distortions.append(km.inertia_)\nplt.plot(range(1, 11), distortions, marker=\'o\')\nplt.xlabel(\'Number of clusters\')\nplt.ylabel(\'Distortion\')\nplt.tight_layout()\n#plt.savefig(\'images/11_03.png\', dpi=300)\nplt.show()\n\n\n\n# ## Quantifying the quality of clustering  via silhouette plots\n\n\n\n\nkm = KMeans(n_clusters=3, \n            init=\'k-means++\', \n            n_init=10, \n            max_iter=300,\n            tol=1e-04,\n            random_state=0)\ny_km = km.fit_predict(X)\n\ncluster_labels = np.unique(y_km)\nn_clusters = cluster_labels.shape[0]\nsilhouette_vals = silhouette_samples(X, y_km, metric=\'euclidean\')\ny_ax_lower, y_ax_upper = 0, 0\nyticks = []\nfor i, c in enumerate(cluster_labels):\n    c_silhouette_vals = silhouette_vals[y_km == c]\n    c_silhouette_vals.sort()\n    y_ax_upper += len(c_silhouette_vals)\n    color = cm.jet(float(i) / n_clusters)\n    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, \n             edgecolor=\'none\', color=color)\n\n    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n    y_ax_lower += len(c_silhouette_vals)\n    \nsilhouette_avg = np.mean(silhouette_vals)\nplt.axvline(silhouette_avg, color=""red"", linestyle=""--"") \n\nplt.yticks(yticks, cluster_labels + 1)\nplt.ylabel(\'Cluster\')\nplt.xlabel(\'Silhouette coefficient\')\n\nplt.tight_layout()\n#plt.savefig(\'images/11_04.png\', dpi=300)\nplt.show()\n\n\n# Comparison to ""bad"" clustering:\n\n\n\nkm = KMeans(n_clusters=2,\n            init=\'k-means++\',\n            n_init=10,\n            max_iter=300,\n            tol=1e-04,\n            random_state=0)\ny_km = km.fit_predict(X)\n\nplt.scatter(X[y_km == 0, 0],\n            X[y_km == 0, 1],\n            s=50,\n            c=\'lightgreen\',\n            edgecolor=\'black\',\n            marker=\'s\',\n            label=\'cluster 1\')\nplt.scatter(X[y_km == 1, 0],\n            X[y_km == 1, 1],\n            s=50,\n            c=\'orange\',\n            edgecolor=\'black\',\n            marker=\'o\',\n            label=\'cluster 2\')\n\nplt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n            s=250, marker=\'*\', c=\'red\', label=\'centroids\')\nplt.legend()\nplt.grid()\nplt.tight_layout()\n#plt.savefig(\'images/11_05.png\', dpi=300)\nplt.show()\n\n\n\n\ncluster_labels = np.unique(y_km)\nn_clusters = cluster_labels.shape[0]\nsilhouette_vals = silhouette_samples(X, y_km, metric=\'euclidean\')\ny_ax_lower, y_ax_upper = 0, 0\nyticks = []\nfor i, c in enumerate(cluster_labels):\n    c_silhouette_vals = silhouette_vals[y_km == c]\n    c_silhouette_vals.sort()\n    y_ax_upper += len(c_silhouette_vals)\n    color = cm.jet(float(i) / n_clusters)\n    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, \n             edgecolor=\'none\', color=color)\n\n    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n    y_ax_lower += len(c_silhouette_vals)\n    \nsilhouette_avg = np.mean(silhouette_vals)\nplt.axvline(silhouette_avg, color=""red"", linestyle=""--"") \n\nplt.yticks(yticks, cluster_labels + 1)\nplt.ylabel(\'Cluster\')\nplt.xlabel(\'Silhouette coefficient\')\n\nplt.tight_layout()\n#plt.savefig(\'images/11_06.png\', dpi=300)\nplt.show()\n\n\n\n# # Organizing clusters as a hierarchical tree\n\n# ## Grouping clusters in bottom-up fashion\n\n\n\n\n\n\n\n\nnp.random.seed(123)\n\nvariables = [\'X\', \'Y\', \'Z\']\nlabels = [\'ID_0\', \'ID_1\', \'ID_2\', \'ID_3\', \'ID_4\']\n\nX = np.random.random_sample([5, 3])*10\ndf = pd.DataFrame(X, columns=variables, index=labels)\ndf\n\n\n\n# ## Performing hierarchical clustering on a distance matrix\n\n\n\n\nrow_dist = pd.DataFrame(squareform(pdist(df, metric=\'euclidean\')),\n                        columns=labels,\n                        index=labels)\nrow_dist\n\n\n# We can either pass a condensed distance matrix (upper triangular) from the `pdist` function, or we can pass the ""original"" data array and define the `metric=\'euclidean\'` argument in `linkage`. However, we should not pass the squareform distance matrix, which would yield different distance values although the overall clustering could be the same.\n\n\n\n# 1. incorrect approach: Squareform distance matrix\n\n\nrow_clusters = linkage(row_dist, method=\'complete\', metric=\'euclidean\')\npd.DataFrame(row_clusters,\n             columns=[\'row label 1\', \'row label 2\',\n                      \'distance\', \'no. of items in clust.\'],\n             index=[\'cluster %d\' % (i + 1)\n                    for i in range(row_clusters.shape[0])])\n\n\n\n\n# 2. correct approach: Condensed distance matrix\n\nrow_clusters = linkage(pdist(df, metric=\'euclidean\'), method=\'complete\')\npd.DataFrame(row_clusters,\n             columns=[\'row label 1\', \'row label 2\',\n                      \'distance\', \'no. of items in clust.\'],\n             index=[\'cluster %d\' % (i + 1) \n                    for i in range(row_clusters.shape[0])])\n\n\n\n\n# 3. correct approach: Input sample matrix\n\nrow_clusters = linkage(df.values, method=\'complete\', metric=\'euclidean\')\npd.DataFrame(row_clusters,\n             columns=[\'row label 1\', \'row label 2\',\n                      \'distance\', \'no. of items in clust.\'],\n             index=[\'cluster %d\' % (i + 1)\n                    for i in range(row_clusters.shape[0])])\n\n\n\n\n\n# make dendrogram black (part 1/2)\n# set_link_color_palette([\'black\'])\n\nrow_dendr = dendrogram(row_clusters, \n                       labels=labels,\n                       # make dendrogram black (part 2/2)\n                       # color_threshold=np.inf\n                       )\nplt.tight_layout()\nplt.ylabel(\'Euclidean distance\')\n#plt.savefig(\'images/11_11.png\', dpi=300, \n#            bbox_inches=\'tight\')\nplt.show()\n\n\n\n# ## Attaching dendrograms to a heat map\n\n\n\n# plot row dendrogram\nfig = plt.figure(figsize=(8, 8), facecolor=\'white\')\naxd = fig.add_axes([0.09, 0.1, 0.2, 0.6])\n\n# note: for matplotlib < v1.5.1, please use orientation=\'right\'\nrow_dendr = dendrogram(row_clusters, orientation=\'left\')\n\n# reorder data with respect to clustering\ndf_rowclust = df.iloc[row_dendr[\'leaves\'][::-1]]\n\naxd.set_xticks([])\naxd.set_yticks([])\n\n# remove axes spines from dendrogram\nfor i in axd.spines.values():\n    i.set_visible(False)\n\n# plot heatmap\naxm = fig.add_axes([0.23, 0.1, 0.6, 0.6])  # x-pos, y-pos, width, height\ncax = axm.matshow(df_rowclust, interpolation=\'nearest\', cmap=\'hot_r\')\nfig.colorbar(cax)\naxm.set_xticklabels([\'\'] + list(df_rowclust.columns))\naxm.set_yticklabels([\'\'] + list(df_rowclust.index))\n\n#plt.savefig(\'images/11_12.png\', dpi=300)\nplt.show()\n\n\n\n# ## Applying agglomerative clustering via scikit-learn\n\n\n\n\nac = AgglomerativeClustering(n_clusters=3, \n                             affinity=\'euclidean\', \n                             linkage=\'complete\')\nlabels = ac.fit_predict(X)\nprint(\'Cluster labels: %s\' % labels)\n\n\n\n\nac = AgglomerativeClustering(n_clusters=2, \n                             affinity=\'euclidean\', \n                             linkage=\'complete\')\nlabels = ac.fit_predict(X)\nprint(\'Cluster labels: %s\' % labels)\n\n\n\n# # Locating regions of high density via DBSCAN\n\n\n\n\n\n\n\n\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\nplt.scatter(X[:, 0], X[:, 1])\nplt.tight_layout()\n#plt.savefig(\'images/11_14.png\', dpi=300)\nplt.show()\n\n\n# K-means and hierarchical clustering:\n\n\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n\nkm = KMeans(n_clusters=2, random_state=0)\ny_km = km.fit_predict(X)\nax1.scatter(X[y_km == 0, 0], X[y_km == 0, 1],\n            edgecolor=\'black\',\n            c=\'lightblue\', marker=\'o\', s=40, label=\'cluster 1\')\nax1.scatter(X[y_km == 1, 0], X[y_km == 1, 1],\n            edgecolor=\'black\',\n            c=\'red\', marker=\'s\', s=40, label=\'cluster 2\')\nax1.set_title(\'K-means clustering\')\n\nac = AgglomerativeClustering(n_clusters=2,\n                             affinity=\'euclidean\',\n                             linkage=\'complete\')\ny_ac = ac.fit_predict(X)\nax2.scatter(X[y_ac == 0, 0], X[y_ac == 0, 1], c=\'lightblue\',\n            edgecolor=\'black\',\n            marker=\'o\', s=40, label=\'cluster 1\')\nax2.scatter(X[y_ac == 1, 0], X[y_ac == 1, 1], c=\'red\',\n            edgecolor=\'black\',\n            marker=\'s\', s=40, label=\'cluster 2\')\nax2.set_title(\'Agglomerative clustering\')\n\nplt.legend()\nplt.tight_layout()\n# plt.savefig(\'images/11_15.png\', dpi=300)\nplt.show()\n\n\n# Density-based clustering:\n\n\n\n\ndb = DBSCAN(eps=0.2, min_samples=5, metric=\'euclidean\')\ny_db = db.fit_predict(X)\nplt.scatter(X[y_db == 0, 0], X[y_db == 0, 1],\n            c=\'lightblue\', marker=\'o\', s=40,\n            edgecolor=\'black\', \n            label=\'cluster 1\')\nplt.scatter(X[y_db == 1, 0], X[y_db == 1, 1],\n            c=\'red\', marker=\'s\', s=40,\n            edgecolor=\'black\', \n            label=\'cluster 2\')\nplt.legend()\nplt.tight_layout()\n#plt.savefig(\'images/11_16.png\', dpi=300)\nplt.show()\n\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
code/ch12/ch12.py,0,"b'# coding: utf-8\n\n\nimport sys\nimport gzip\nimport shutil\nimport os\nimport struct\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# *Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 12 - Implementing a Multi-layer Artificial Neural Network from Scratch\n# \n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*\n\n# ### Overview\n\n# - [Modeling complex functions with artificial neural networks](#Modeling-complex-functions-with-artificial-neural-networks)\n#   - [Single-layer neural network recap](#Single-layer-neural-network-recap)\n#   - [Introducing the multi-layer neural network architecture](#Introducing-the-multi-layer-neural-network-architecture)\n#   - [Activating a neural network via forward propagation](#Activating-a-neural-network-via-forward-propagation)\n# - [Classifying handwritten digits](#Classifying-handwritten-digits)\n#   - [Obtaining the MNIST dataset](#Obtaining-the-MNIST-dataset)\n#   - [Implementing a multi-layer perceptron](#Implementing-a-multi-layer-perceptron)\n# - [Training an artificial neural network](#Training-an-artificial-neural-network)\n#   - [Computing the logistic cost function](#Computing-the-logistic-cost-function)\n#   - [Developing your intuition for backpropagation](#Developing-your-intuition-for-backpropagation)\n#   - [Training neural networks via backpropagation](#Training-neural-networks-via-backpropagation)\n# - [Convergence in neural networks](#Convergence-in-neural-networks)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Modeling complex functions with artificial neural networks\n\n# ...\n\n# ## Single-layer neural network recap\n\n\n\n\n\n\n# ## Introducing the multi-layer neural network architecture\n\n\n\n\n\n\n\n\n\n\n# ## Activating a neural network via forward propagation\n\n\n\n\n\n\n# # Classifying handwritten digits\n\n# ...\n\n# ## Obtaining the MNIST dataset\n\n# The MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts:\n# \n# - Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, 60,000 samples)\n# - Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, 60,000 labels)\n# - Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, 10,000 samples)\n# - Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, 10,000 labels)\n# \n# In this section, we will only be working with a subset of MNIST, thus, we only need to download the training set images and training set labels. \n# \n# After downloading the files, simply run the next code cell to unzip the files.\n# \n# \n\n\n\n# this code cell unzips mnist\n\n\nif (sys.version_info > (3, 0)):\n    writemode = \'wb\'\nelse:\n    writemode = \'w\'\n\nzipped_mnist = [f for f in os.listdir(\'./\') if f.endswith(\'ubyte.gz\')]\nfor z in zipped_mnist:\n    with gzip.GzipFile(z, mode=\'rb\') as decompressed, open(z[:-3], writemode) as outfile:\n        outfile.write(decompressed.read()) \n\n\n# ----\n# \n# IGNORE IF THE CODE CELL ABOVE EXECUTED WITHOUT PROBLEMS:\n#     \n# If you have issues with the code cell above, I recommend unzipping the files using the Unix/Linux gzip tool from the terminal for efficiency, e.g., using the command \n# \n#     gzip *ubyte.gz -d\n#  \n# in your local MNIST download directory, or, using your favorite unzipping tool if you are working with a machine running on Microsoft Windows. The images are stored in byte form, and using the following function, we will read them into NumPy arrays that we will use to train our MLP.\n# \n# Please note that if you are **not** using gzip, please make sure tha the files are named\n# \n# - train-images-idx3-ubyte\n# - train-labels-idx1-ubyte\n# - t10k-images-idx3-ubyte\n# - t10k-labels-idx1-ubyte\n# \n# If a file is e.g., named `train-images.idx3-ubyte` after unzipping (this is due to the fact that certain tools try to guess a file suffix), please rename it to `train-images-idx3-ubyte` before proceeding. \n# \n# ----\n\n\n\n \ndef load_mnist(path, kind=\'train\'):\n    """"""Load MNIST data from `path`""""""\n    labels_path = os.path.join(path, \n                               \'%s-labels-idx1-ubyte\' % kind)\n    images_path = os.path.join(path, \n                               \'%s-images-idx3-ubyte\' % kind)\n        \n    with open(labels_path, \'rb\') as lbpath:\n        magic, n = struct.unpack(\'>II\', \n                                 lbpath.read(8))\n        labels = np.fromfile(lbpath, \n                             dtype=np.uint8)\n\n    with open(images_path, \'rb\') as imgpath:\n        magic, num, rows, cols = struct.unpack("">IIII"", \n                                               imgpath.read(16))\n        images = np.fromfile(imgpath, \n                             dtype=np.uint8).reshape(len(labels), 784)\n        images = ((images / 255.) - .5) * 2\n \n    return images, labels\n\n\n\n\n\n\n\n\nX_train, y_train = load_mnist(\'\', kind=\'train\')\nprint(\'Rows: %d, columns: %d\' % (X_train.shape[0], X_train.shape[1]))\n\n\n\n\nX_test, y_test = load_mnist(\'\', kind=\'t10k\')\nprint(\'Rows: %d, columns: %d\' % (X_test.shape[0], X_test.shape[1]))\n\n\n# Visualize the first digit of each class:\n\n\n\n\nfig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,)\nax = ax.flatten()\nfor i in range(10):\n    img = X_train[y_train == i][0].reshape(28, 28)\n    ax[i].imshow(img, cmap=\'Greys\')\n\nax[0].set_xticks([])\nax[0].set_yticks([])\nplt.tight_layout()\n# plt.savefig(\'images/12_5.png\', dpi=300)\nplt.show()\n\n\n# Visualize 25 different versions of ""7"":\n\n\n\nfig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,)\nax = ax.flatten()\nfor i in range(25):\n    img = X_train[y_train == 7][i].reshape(28, 28)\n    ax[i].imshow(img, cmap=\'Greys\')\n\nax[0].set_xticks([])\nax[0].set_yticks([])\nplt.tight_layout()\n# plt.savefig(\'images/12_6.png\', dpi=300)\nplt.show()\n\n\n\n\n\nnp.savez_compressed(\'mnist_scaled.npz\', \n                    X_train=X_train,\n                    y_train=y_train,\n                    X_test=X_test,\n                    y_test=y_test)\n\n\n\n\nmnist = np.load(\'mnist_scaled.npz\')\nmnist.files\n\n\n\n\nX_train, y_train, X_test, y_test = [mnist[f] for f in [\'X_train\', \'y_train\', \n                                    \'X_test\', \'y_test\']]\n\ndel mnist\n\nX_train.shape\n\n\n\n# ## Implementing a multi-layer perceptron\n\n\n\n\n\nclass NeuralNetMLP(object):\n    """""" Feedforward neural network / Multi-layer perceptron classifier.\n\n    Parameters\n    ------------\n    n_hidden : int (default: 30)\n        Number of hidden units.\n    l2 : float (default: 0.)\n        Lambda value for L2-regularization.\n        No regularization if l2=0. (default)\n    epochs : int (default: 100)\n        Number of passes over the training set.\n    eta : float (default: 0.001)\n        Learning rate.\n    shuffle : bool (default: True)\n        Shuffles training data every epoch if True to prevent circles.\n    minibatch_size : int (default: 1)\n        Number of training samples per minibatch.\n    seed : int (default: None)\n        Random seed for initalizing weights and shuffling.\n\n    Attributes\n    -----------\n    eval_ : dict\n      Dictionary collecting the cost, training accuracy,\n      and validation accuracy for each epoch during training.\n\n    """"""\n    def __init__(self, n_hidden=30,\n                 l2=0., epochs=100, eta=0.001,\n                 shuffle=True, minibatch_size=1, seed=None):\n\n        self.random = np.random.RandomState(seed)\n        self.n_hidden = n_hidden\n        self.l2 = l2\n        self.epochs = epochs\n        self.eta = eta\n        self.shuffle = shuffle\n        self.minibatch_size = minibatch_size\n\n    def _onehot(self, y, n_classes):\n        """"""Encode labels into one-hot representation\n\n        Parameters\n        ------------\n        y : array, shape = [n_samples]\n            Target values.\n\n        Returns\n        -----------\n        onehot : array, shape = (n_samples, n_labels)\n\n        """"""\n        onehot = np.zeros((n_classes, y.shape[0]))\n        for idx, val in enumerate(y.astype(int)):\n            onehot[val, idx] = 1.\n        return onehot.T\n\n    def _sigmoid(self, z):\n        """"""Compute logistic function (sigmoid)""""""\n        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n\n    def _forward(self, X):\n        """"""Compute forward propagation step""""""\n\n        # step 1: net input of hidden layer\n        # [n_samples, n_features] dot [n_features, n_hidden]\n        # -> [n_samples, n_hidden]\n        z_h = np.dot(X, self.w_h) + self.b_h\n\n        # step 2: activation of hidden layer\n        a_h = self._sigmoid(z_h)\n\n        # step 3: net input of output layer\n        # [n_samples, n_hidden] dot [n_hidden, n_classlabels]\n        # -> [n_samples, n_classlabels]\n\n        z_out = np.dot(a_h, self.w_out) + self.b_out\n\n        # step 4: activation output layer\n        a_out = self._sigmoid(z_out)\n\n        return z_h, a_h, z_out, a_out\n\n    def _compute_cost(self, y_enc, output):\n        """"""Compute cost function.\n\n        Parameters\n        ----------\n        y_enc : array, shape = (n_samples, n_labels)\n            one-hot encoded class labels.\n        output : array, shape = [n_samples, n_output_units]\n            Activation of the output layer (forward propagation)\n\n        Returns\n        ---------\n        cost : float\n            Regularized cost\n\n        """"""\n        L2_term = (self.l2 *\n                   (np.sum(self.w_h ** 2.) +\n                    np.sum(self.w_out ** 2.)))\n\n        term1 = -y_enc * (np.log(output))\n        term2 = (1. - y_enc) * np.log(1. - output)\n        cost = np.sum(term1 - term2) + L2_term\n        \n        # If you are applying this cost function to other\n        # datasets where activation\n        # values maybe become more extreme (closer to zero or 1)\n        # you may encounter ""ZeroDivisionError""s due to numerical\n        # instabilities in Python & NumPy for the current implementation.\n        # I.e., the code tries to evaluate log(0), which is undefined.\n        # To address this issue, you could add a small constant to the\n        # activation values that are passed to the log function.\n        #\n        # For example:\n        #\n        # term1 = -y_enc * (np.log(output + 1e-5))\n        # term2 = (1. - y_enc) * np.log(1. - output + 1e-5)\n        \n        return cost\n\n    def predict(self, X):\n        """"""Predict class labels\n\n        Parameters\n        -----------\n        X : array, shape = [n_samples, n_features]\n            Input layer with original features.\n\n        Returns:\n        ----------\n        y_pred : array, shape = [n_samples]\n            Predicted class labels.\n\n        """"""\n        z_h, a_h, z_out, a_out = self._forward(X)\n        y_pred = np.argmax(z_out, axis=1)\n        return y_pred\n\n    def fit(self, X_train, y_train, X_valid, y_valid):\n        """""" Learn weights from training data.\n\n        Parameters\n        -----------\n        X_train : array, shape = [n_samples, n_features]\n            Input layer with original features.\n        y_train : array, shape = [n_samples]\n            Target class labels.\n        X_valid : array, shape = [n_samples, n_features]\n            Sample features for validation during training\n        y_valid : array, shape = [n_samples]\n            Sample labels for validation during training\n\n        Returns:\n        ----------\n        self\n\n        """"""\n        n_output = np.unique(y_train).shape[0]  # number of class labels\n        n_features = X_train.shape[1]\n\n        ########################\n        # Weight initialization\n        ########################\n\n        # weights for input -> hidden\n        self.b_h = np.zeros(self.n_hidden)\n        self.w_h = self.random.normal(loc=0.0, scale=0.1,\n                                      size=(n_features, self.n_hidden))\n\n        # weights for hidden -> output\n        self.b_out = np.zeros(n_output)\n        self.w_out = self.random.normal(loc=0.0, scale=0.1,\n                                        size=(self.n_hidden, n_output))\n\n        epoch_strlen = len(str(self.epochs))  # for progress formatting\n        self.eval_ = {\'cost\': [], \'train_acc\': [], \'valid_acc\': []}\n\n        y_train_enc = self._onehot(y_train, n_output)\n\n        # iterate over training epochs\n        for i in range(self.epochs):\n\n            # iterate over minibatches\n            indices = np.arange(X_train.shape[0])\n\n            if self.shuffle:\n                self.random.shuffle(indices)\n\n            for start_idx in range(0, indices.shape[0] - self.minibatch_size +\n                                   1, self.minibatch_size):\n                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n\n                # forward propagation\n                z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])\n\n                ##################\n                # Backpropagation\n                ##################\n\n                # [n_samples, n_classlabels]\n                sigma_out = a_out - y_train_enc[batch_idx]\n\n                # [n_samples, n_hidden]\n                sigmoid_derivative_h = a_h * (1. - a_h)\n\n                # [n_samples, n_classlabels] dot [n_classlabels, n_hidden]\n                # -> [n_samples, n_hidden]\n                sigma_h = (np.dot(sigma_out, self.w_out.T) *\n                           sigmoid_derivative_h)\n\n                # [n_features, n_samples] dot [n_samples, n_hidden]\n                # -> [n_features, n_hidden]\n                grad_w_h = np.dot(X_train[batch_idx].T, sigma_h)\n                grad_b_h = np.sum(sigma_h, axis=0)\n\n                # [n_hidden, n_samples] dot [n_samples, n_classlabels]\n                # -> [n_hidden, n_classlabels]\n                grad_w_out = np.dot(a_h.T, sigma_out)\n                grad_b_out = np.sum(sigma_out, axis=0)\n\n                # Regularization and weight updates\n                delta_w_h = (grad_w_h + self.l2*self.w_h)\n                delta_b_h = grad_b_h # bias is not regularized\n                self.w_h -= self.eta * delta_w_h\n                self.b_h -= self.eta * delta_b_h\n\n                delta_w_out = (grad_w_out + self.l2*self.w_out)\n                delta_b_out = grad_b_out  # bias is not regularized\n                self.w_out -= self.eta * delta_w_out\n                self.b_out -= self.eta * delta_b_out\n\n            #############\n            # Evaluation\n            #############\n\n            # Evaluation after each epoch during training\n            z_h, a_h, z_out, a_out = self._forward(X_train)\n            \n            cost = self._compute_cost(y_enc=y_train_enc,\n                                      output=a_out)\n\n            y_train_pred = self.predict(X_train)\n            y_valid_pred = self.predict(X_valid)\n\n            train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) /\n                         X_train.shape[0])\n            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) /\n                         X_valid.shape[0])\n\n            sys.stderr.write(\'\\r%0*d/%d | Cost: %.2f \'\n                             \'| Train/Valid Acc.: %.2f%%/%.2f%% \' %\n                             (epoch_strlen, i+1, self.epochs, cost,\n                              train_acc*100, valid_acc*100))\n            sys.stderr.flush()\n\n            self.eval_[\'cost\'].append(cost)\n            self.eval_[\'train_acc\'].append(train_acc)\n            self.eval_[\'valid_acc\'].append(valid_acc)\n\n        return self\n\n\n\n\nn_epochs = 200\n\n## @Readers: PLEASE IGNORE IF-STATEMENT BELOW\n##\n## This cell is meant to run fewer epochs when\n## the notebook is run on the Travis Continuous Integration\n## platform to test the code on a smaller dataset\n## to prevent timeout errors; it just serves a debugging tool\n\nif \'TRAVIS\' in os.environ:\n    n_epochs = 20\n\n\n\n\nnn = NeuralNetMLP(n_hidden=100, \n                  l2=0.01, \n                  epochs=n_epochs, \n                  eta=0.0005,\n                  minibatch_size=100, \n                  shuffle=True,\n                  seed=1)\n\nnn.fit(X_train=X_train[:55000], \n       y_train=y_train[:55000],\n       X_valid=X_train[55000:],\n       y_valid=y_train[55000:])\n\n\n# ---\n# **Note**\n# \n# In the fit method of the MLP example above,\n# \n# ```python\n# \n# for idx in mini:\n# ...\n#     # compute gradient via backpropagation\n#     grad1, grad2 = self._get_gradient(a1=a1, a2=a2,\n#                                       a3=a3, z2=z2,\n#                                       y_enc=y_enc[:, idx],\n#                                       w1=self.w1,\n#                                       w2=self.w2)\n# \n#     delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n#     self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n#     self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n#     delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n# ```\n# \n# `delta_w1_prev` (same applies to `delta_w2_prev`) is a memory view on `delta_w1` via  \n# \n# ```python\n# delta_w1_prev = delta_w1\n# ```\n# on the last line. This could be problematic, since updating `delta_w1 = self.eta * grad1` would change `delta_w1_prev` as well when we iterate over the for loop. Note that this is not the case here, because we assign a new array to `delta_w1` in each iteration -- the gradient array times the learning rate:\n# \n# ```python\n# delta_w1 = self.eta * grad1\n# ```\n# \n# The assignment shown above leaves the `delta_w1_prev` pointing to the ""old"" `delta_w1` array. To illustrates this with a simple snippet, consider the following example:\n# \n# \n\n\n\n\na = np.arange(5)\nb = a\nprint(\'a & b\', np.may_share_memory(a, b))\n\n\na = np.arange(5)\nprint(\'a & b\', np.may_share_memory(a, b))\n\n\n# (End of note.)\n# \n# ---\n\n\n\n\nplt.plot(range(nn.epochs), nn.eval_[\'cost\'])\nplt.ylabel(\'Cost\')\nplt.xlabel(\'Epochs\')\n#plt.savefig(\'images/12_07.png\', dpi=300)\nplt.show()\n\n\n\n\nplt.plot(range(nn.epochs), nn.eval_[\'train_acc\'], \n         label=\'training\')\nplt.plot(range(nn.epochs), nn.eval_[\'valid_acc\'], \n         label=\'validation\', linestyle=\'--\')\nplt.ylabel(\'Accuracy\')\nplt.xlabel(\'Epochs\')\nplt.legend()\n#plt.savefig(\'images/12_08.png\', dpi=300)\nplt.show()\n\n\n\n\ny_test_pred = nn.predict(X_test)\nacc = (np.sum(y_test == y_test_pred)\n       .astype(np.float) / X_test.shape[0])\n\nprint(\'Test accuracy: %.2f%%\' % (acc * 100))\n\n\n\n\nmiscl_img = X_test[y_test != y_test_pred][:25]\ncorrect_lab = y_test[y_test != y_test_pred][:25]\nmiscl_lab = y_test_pred[y_test != y_test_pred][:25]\n\nfig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,)\nax = ax.flatten()\nfor i in range(25):\n    img = miscl_img[i].reshape(28, 28)\n    ax[i].imshow(img, cmap=\'Greys\', interpolation=\'nearest\')\n    ax[i].set_title(\'%d) t: %d p: %d\' % (i+1, correct_lab[i], miscl_lab[i]))\n\nax[0].set_xticks([])\nax[0].set_yticks([])\nplt.tight_layout()\n#plt.savefig(\'images/12_09.png\', dpi=300)\nplt.show()\n\n\n\n# # Training an artificial neural network\n\n# ...\n\n# ## Computing the logistic cost function\n\n\n\n\n\n\n# ## Developing your intuition for backpropagation\n\n# ...\n\n# ## Training neural networks via backpropagation\n\n\n\n\n\n\n\n\n\n\n# # Convergence in neural networks\n\n\n\n\n\n\n# ...\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
code/ch12/neuralnet.py,0,"b'import numpy as np\nimport sys\n\n\nclass NeuralNetMLP(object):\n    """""" Feedforward neural network / Multi-layer perceptron classifier.\n\n    Parameters\n    ------------\n    n_hidden : int (default: 30)\n        Number of hidden units.\n    l2 : float (default: 0.)\n        Lambda value for L2-regularization.\n        No regularization if l2=0. (default)\n    epochs : int (default: 100)\n        Number of passes over the training set.\n    eta : float (default: 0.001)\n        Learning rate.\n    shuffle : bool (default: True)\n        Shuffles training data every epoch if True to prevent circles.\n    minibatch_size : int (default: 1)\n        Number of training samples per minibatch.\n    seed : int (default: None)\n        Random seed for initalizing weights and shuffling.\n\n    Attributes\n    -----------\n    eval_ : dict\n      Dictionary collecting the cost, training accuracy,\n      and validation accuracy for each epoch during training.\n\n    """"""\n    def __init__(self, n_hidden=30,\n                 l2=0., epochs=100, eta=0.001,\n                 shuffle=True, minibatch_size=1, seed=None):\n\n        self.random = np.random.RandomState(seed)\n        self.n_hidden = n_hidden\n        self.l2 = l2\n        self.epochs = epochs\n        self.eta = eta\n        self.shuffle = shuffle\n        self.minibatch_size = minibatch_size\n\n    def _onehot(self, y, n_classes):\n        """"""Encode labels into one-hot representation\n\n        Parameters\n        ------------\n        y : array, shape = [n_samples]\n            Target values.\n\n        Returns\n        -----------\n        onehot : array, shape = (n_samples, n_labels)\n\n        """"""\n        onehot = np.zeros((n_classes, y.shape[0]))\n        for idx, val in enumerate(y):\n            onehot[val, idx] = 1.\n        return onehot.T\n\n    def _sigmoid(self, z):\n        """"""Compute logistic function (sigmoid)""""""\n        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n\n    def _forward(self, X):\n        """"""Compute forward propagation step""""""\n\n        # step 1: net input of hidden layer\n        # [n_samples, n_features] dot [n_features, n_hidden]\n        # -> [n_samples, n_hidden]\n        z_h = np.dot(X, self.w_h) + self.b_h\n\n        # step 2: activation of hidden layer\n        a_h = self._sigmoid(z_h)\n\n        # step 3: net input of output layer\n        # [n_samples, n_hidden] dot [n_hidden, n_classlabels]\n        # -> [n_samples, n_classlabels]\n\n        z_out = np.dot(a_h, self.w_out) + self.b_out\n\n        # step 4: activation output layer\n        a_out = self._sigmoid(z_out)\n\n        return z_h, a_h, z_out, a_out\n\n    def _compute_cost(self, y_enc, output):\n        """"""Compute cost function.\n\n        Parameters\n        ----------\n        y_enc : array, shape = (n_samples, n_labels)\n            one-hot encoded class labels.\n        output : array, shape = [n_samples, n_output_units]\n            Activation of the output layer (forward propagation)\n\n        Returns\n        ---------\n        cost : float\n            Regularized cost\n\n        """"""\n        L2_term = (self.l2 *\n                   (np.sum(self.w_h ** 2.) +\n                    np.sum(self.w_out ** 2.)))\n\n        term1 = -y_enc * (np.log(output))\n        term2 = (1. - y_enc) * np.log(1. - output)\n        cost = np.sum(term1 - term2) + L2_term\n\n        # If you are applying this cost function to other\n        # datasets where activation\n        # values maybe become more extreme (closer to zero or 1)\n        # you may encounter ""ZeroDivisionError""s due to numerical\n        # instabilities in Python & NumPy for the current implementation.\n        # I.e., the code tries to evaluate log(0), which is undefined.\n        # To address this issue, you could add a small constant to the\n        # activation values that are passed to the log function.\n        #\n        # For example:\n        #\n        # term1 = -y_enc * (np.log(output + 1e-5))\n        # term2 = (1. - y_enc) * np.log(1. - output + 1e-5)\n\n        return cost\n\n    def predict(self, X):\n        """"""Predict class labels\n\n        Parameters\n        -----------\n        X : array, shape = [n_samples, n_features]\n            Input layer with original features.\n\n        Returns:\n        ----------\n        y_pred : array, shape = [n_samples]\n            Predicted class labels.\n\n        """"""\n        z_h, a_h, z_out, a_out = self._forward(X)\n        y_pred = np.argmax(z_out, axis=1)\n        return y_pred\n\n    def fit(self, X_train, y_train, X_valid, y_valid):\n        """""" Learn weights from training data.\n\n        Parameters\n        -----------\n        X_train : array, shape = [n_samples, n_features]\n            Input layer with original features.\n        y_train : array, shape = [n_samples]\n            Target class labels.\n        X_valid : array, shape = [n_samples, n_features]\n            Sample features for validation during training\n        y_valid : array, shape = [n_samples]\n            Sample labels for validation during training\n\n        Returns:\n        ----------\n        self\n\n        """"""\n        n_output = np.unique(y_train).shape[0]  # number of class labels\n        n_features = X_train.shape[1]\n\n        ########################\n        # Weight initialization\n        ########################\n\n        # weights for input -> hidden\n        self.b_h = np.zeros(self.n_hidden)\n        self.w_h = self.random.normal(loc=0.0, scale=0.1,\n                                      size=(n_features, self.n_hidden))\n\n        # weights for hidden -> output\n        self.b_out = np.zeros(n_output)\n        self.w_out = self.random.normal(loc=0.0, scale=0.1,\n                                        size=(self.n_hidden, n_output))\n\n        epoch_strlen = len(str(self.epochs))  # for progress formatting\n        self.eval_ = {\'cost\': [], \'train_acc\': [], \'valid_acc\': []}\n\n        y_train_enc = self._onehot(y_train, n_output)\n\n        # iterate over training epochs\n        for i in range(self.epochs):\n\n            # iterate over minibatches\n            indices = np.arange(X_train.shape[0])\n\n            if self.shuffle:\n                self.random.shuffle(indices)\n\n            for start_idx in range(0, indices.shape[0] - self.minibatch_size +\n                                   1, self.minibatch_size):\n                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n\n                # forward propagation\n                z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])\n\n                ##################\n                # Backpropagation\n                ##################\n\n                # [n_samples, n_classlabels]\n                sigma_out = a_out - y_train_enc[batch_idx]\n\n                # [n_samples, n_hidden]\n                sigmoid_derivative_h = a_h * (1. - a_h)\n\n                # [n_samples, n_classlabels] dot [n_classlabels, n_hidden]\n                # -> [n_samples, n_hidden]\n                sigma_h = (np.dot(sigma_out, self.w_out.T) *\n                           sigmoid_derivative_h)\n\n                # [n_features, n_samples] dot [n_samples, n_hidden]\n                # -> [n_features, n_hidden]\n                grad_w_h = np.dot(X_train[batch_idx].T, sigma_h)\n                grad_b_h = np.sum(sigma_h, axis=0)\n\n                # [n_hidden, n_samples] dot [n_samples, n_classlabels]\n                # -> [n_hidden, n_classlabels]\n                grad_w_out = np.dot(a_h.T, sigma_out)\n                grad_b_out = np.sum(sigma_out, axis=0)\n\n                # Regularization and weight updates\n                delta_w_h = (grad_w_h + self.l2*self.w_h)\n                delta_b_h = grad_b_h  # bias is not regularized\n                self.w_h -= self.eta * delta_w_h\n                self.b_h -= self.eta * delta_b_h\n\n                delta_w_out = (grad_w_out + self.l2*self.w_out)\n                delta_b_out = grad_b_out  # bias is not regularized\n                self.w_out -= self.eta * delta_w_out\n                self.b_out -= self.eta * delta_b_out\n\n            #############\n            # Evaluation\n            #############\n\n            # Evaluation after each epoch during training\n            z_h, a_h, z_out, a_out = self._forward(X_train)\n            cost = self._compute_cost(y_enc=y_train_enc,\n                                      output=a_out)\n\n            y_train_pred = self.predict(X_train)\n            y_valid_pred = self.predict(X_valid)\n\n            train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) /\n                         X_train.shape[0])\n            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) /\n                         X_valid.shape[0])\n\n            sys.stderr.write(\'\\r%0*d/%d | Cost: %.2f \'\n                             \'| Train/Valid Acc.: %.2f%%/%.2f%% \' %\n                             (epoch_strlen, i+1, self.epochs, cost,\n                              train_acc*100, valid_acc*100))\n            sys.stderr.flush()\n\n            self.eval_[\'cost\'].append(cost)\n            self.eval_[\'train_acc\'].append(train_acc)\n            self.eval_[\'valid_acc\'].append(valid_acc)\n\n        return self\n'"
code/ch13/ch13.py,43,"b'# coding: utf-8\n\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nimport gzip\nimport shutil\nimport os\nimport struct\nimport tensorflow.contrib.keras as keras\n\n# *Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com) and Vahid Mirjalili, Packt Publishing Ltd. 2017\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 13 - Parallelizing Neural Network Training with TensorFlow  \n\n# - [Building, compiling, and running machine learning models with TensorFlow](#Building,-compiling,-and-running-machine-learning-models-with-TensorFlow)\n#   - [Performance challenges of expensive computations](#Performance-challenges-of-expensive-computations)\n#   - [What is TensorFlow?](#What-is-TensorFlow?)\n#   - [First steps with TensorFlow](#First-steps-with-TensorFlow)\n#   - [Working with array structures](#Working-with-array-structures)\n#   - [Developing a simple model with low-level TensorFlow API](#Developing-a-simple-model-with-low-level-TensorFlow-API)\n# - [Training neural networks efficiently with high-level TensorFlow APIs](#Training-neural-networks-efficiently-with-high-level-TensorFlow-APIs)\n#   - [Building multilayer neural networks using TensorFlow\'s Layers API](#Building-multilayer-neural-networks-using-TensorFlow\'s-Layers-API)\n#   - [Developing Multilayer Neurals Network with Keras](#Developing-Multilayer-Neural-Network-with-Keras)\n# - [Choosing activation functions for multilayer networks](#Choosing-activation-functions-for-multilayer-networks)\n#   - [Logistic function recap](#Logistic-function-recap)\n#   - [Estimating class probabilities in multi-class classification via the softmax function](#Estimating-class-probabilities-in-multi-class-classification-via-the-softmax-function)\n#   - [Broadening the output spectrum by using a hyperbolic tangent](#Broadening-the-output-spectrum-by-using-a-hyperbolic-tangent)\n#   - [Rectified Linear Unit activation](#Rectified-Linear-Unit-activation)\n# - [Summary](#Summary)\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*\n\n\n\n\n\n\n\n\n\n# ## Building, compiling, and running machine learning models with TensorFlow\n\n# ### Performance challenges of expensive computations\n\n\n\n\n\n# ### What is TensorFlow?\n\n# ### First steps with TensorFlow\n\n\n\n\n## create a graph\ng = tf.Graph()\nwith g.as_default():\n    x = tf.placeholder(dtype=tf.float32,\n                       shape=(None), name=\'x\')\n    w = tf.Variable(2.0, name=\'weight\')\n    b = tf.Variable(0.7, name=\'bias\')\n\n    z = w*x + b\n    init = tf.global_variables_initializer()\n\n## create a session and pass in graph g\nwith tf.Session(graph=g) as sess:\n    ## initialize w and b:\n    sess.run(init)\n    ## evaluate z:\n    for t in [1.0, 0.6, -1.8]:\n        print(\'x=%4.1f --> z=%4.1f\'%(\n              t, sess.run(z, feed_dict={x:t})))\n\n\n\n\nwith tf.Session(graph=g) as sess:\n    sess.run(init)\n    print(sess.run(z, feed_dict={x:[1., 2., 3.]})) \n\n\n# ### Working with array structures\n\n\n\n\n\ng = tf.Graph()\nwith g.as_default():\n    x = tf.placeholder(dtype=tf.float32, \n                       shape=(None, 2, 3),\n                       name=\'input_x\')\n\n    x2 = tf.reshape(x, shape=(-1, 6),\n                    name=\'x2\')\n\n    ## calculate the sum of each column\n    xsum = tf.reduce_sum(x2, axis=0, name=\'col_sum\')\n\n    ## calculate the mean of each column\n    xmean = tf.reduce_mean(x2, axis=0, name=\'col_mean\')\n\n    \nwith tf.Session(graph=g) as sess:\n    x_array = np.arange(18).reshape(3, 2, 3)\n    print(\'input shape: \', x_array.shape)\n    print(\'Reshaped:\\n\', \n          sess.run(x2, feed_dict={x:x_array}))\n    print(\'Column Sums:\\n\', \n          sess.run(xsum, feed_dict={x:x_array}))\n    print(\'Column Means:\\n\', \n          sess.run(xmean, feed_dict={x:x_array}))\n\n\n# ### Developing a simple model with low-level TensorFlow API\n\n\n\n \nX_train = np.arange(10).reshape((10, 1))\ny_train = np.array([1.0, 1.3, 3.1,\n                    2.0, 5.0, 6.3, \n                    6.6, 7.4, 8.0, \n                    9.0])\n\n\n\n\nclass TfLinreg(object):\n    \n    def __init__(self, x_dim, learning_rate=0.01,\n                 random_seed=None):\n        self.x_dim = x_dim\n        self.learning_rate = learning_rate\n        self.g = tf.Graph()\n        ## build the model\n        with self.g.as_default():\n            ## set graph-level random-seed\n            tf.set_random_seed(random_seed)\n            \n            self.build()\n            ## create initializer\n            self.init_op = tf.global_variables_initializer()\n        \n    def build(self):\n        ## define placeholders for inputs\n        self.X = tf.placeholder(dtype=tf.float32,\n                                shape=(None, self.x_dim),\n                                name=\'x_input\')\n        self.y = tf.placeholder(dtype=tf.float32,\n                                shape=(None),\n                                name=\'y_input\')\n        print(self.X)\n        print(self.y)\n        ## define weight matrix and bias vector\n        w = tf.Variable(tf.zeros(shape=(1)),\n                        name=\'weight\')\n        b = tf.Variable(tf.zeros(shape=(1)), \n                        name=""bias"")\n        print(w)\n        print(b)\n\n        self.z_net = tf.squeeze(w*self.X + b,\n                                name=\'z_net\')\n        print(self.z_net)\n        \n        sqr_errors = tf.square(self.y - self.z_net, \n                               name=\'sqr_errors\')\n        print(sqr_errors)\n        self.mean_cost = tf.reduce_mean(sqr_errors,\n                                        name=\'mean_cost\')\n        \n        optimizer = tf.train.GradientDescentOptimizer(\n                    learning_rate=self.learning_rate, \n                    name=\'GradientDescent\')\n        self.optimizer = optimizer.minimize(self.mean_cost)\n\n\n\n\nlrmodel = TfLinreg(x_dim=X_train.shape[1], learning_rate=0.01)\n\n\n\n\ndef train_linreg(sess, model, X_train, y_train, num_epochs=10):\n    ## initialiaze all variables: W & b\n    sess.run(model.init_op)\n    \n    training_costs = []\n    for i in range(num_epochs):\n        _, cost = sess.run([model.optimizer, model.mean_cost], \n                           feed_dict={model.X:X_train, \n                                      model.y:y_train})\n        training_costs.append(cost)\n        \n    return training_costs\n\n\n\n\nsess = tf.Session(graph=lrmodel.g)\ntraining_costs = train_linreg(sess, lrmodel, X_train, y_train)\n\n\n\n\n\nplt.plot(range(1,len(training_costs) + 1), training_costs)\nplt.tight_layout()\nplt.xlabel(\'Epoch\')\nplt.ylabel(\'Training Cost\')\n#plt.savefig(\'images/13_01.png\', dpi=300)\nplt.show()\n\n\n\n\ndef predict_linreg(sess, model, X_test):\n    y_pred = sess.run(model.z_net, \n                      feed_dict={model.X:X_test})\n    return y_pred\n\n\n\n\nplt.scatter(X_train, y_train,\n            marker=\'s\', s=50,\n            label=\'Training Data\')\nplt.plot(range(X_train.shape[0]), \n         predict_linreg(sess, lrmodel, X_train),\n         color=\'gray\', marker=\'o\', \n         markersize=6, linewidth=3,\n         label=\'LinReg Model\')\nplt.xlabel(\'x\')\nplt.ylabel(\'y\')\nplt.legend()\nplt.tight_layout()\n#plt.savefig(\'images/13_02.png\')\nplt.show()\n\n\n# ## Training neural networks efficiently with high-level TensorFlow APIs\n\n# ### Building multilayer neural networks using TensorFlow\'s Layers API\n\n# - See Chapter 12 for details on MNIST\n\n\n\n# unzips mnist\n\n\nif (sys.version_info > (3, 0)):\n    writemode = \'wb\'\nelse:\n    writemode = \'w\'\n\nzipped_mnist = [f for f in os.listdir(\'./\') if f.endswith(\'ubyte.gz\')]\nfor z in zipped_mnist:\n    with gzip.GzipFile(z, mode=\'rb\') as decompressed, open(z[:-3], writemode) as outfile:\n        outfile.write(decompressed.read())\n\n\n\n\n \ndef load_mnist(path, kind=\'train\'):\n    """"""Load MNIST data from `path`""""""\n    labels_path = os.path.join(path, \n                               \'%s-labels-idx1-ubyte\' % kind)\n    images_path = os.path.join(path, \n                               \'%s-images-idx3-ubyte\' % kind)\n        \n    with open(labels_path, \'rb\') as lbpath:\n        magic, n = struct.unpack(\'>II\', \n                                 lbpath.read(8))\n        labels = np.fromfile(lbpath, \n                             dtype=np.uint8)\n\n    with open(images_path, \'rb\') as imgpath:\n        magic, num, rows, cols = struct.unpack("">IIII"", \n                                               imgpath.read(16))\n        images = np.fromfile(imgpath, \n                             dtype=np.uint8).reshape(len(labels), 784)\n        images = ((images / 255.) - .5) * 2\n \n    return images, labels\n\n\n\n\n## loading the data\nX_train, y_train = load_mnist(\'.\', kind=\'train\')\nprint(\'Rows: %d,  Columns: %d\' %(X_train.shape[0], \n                                 X_train.shape[1]))\n\nX_test, y_test = load_mnist(\'.\', kind=\'t10k\')\nprint(\'Rows: %d,  Columns: %d\' %(X_test.shape[0], \n                                     X_test.shape[1]))\n## mean centering and normalization:\nmean_vals = np.mean(X_train, axis=0)\nstd_val = np.std(X_train)\n\nX_train_centered = (X_train - mean_vals)/std_val\nX_test_centered = (X_test - mean_vals)/std_val\n\ndel X_train, X_test\n\nprint(X_train_centered.shape, y_train.shape)\n\nprint(X_test_centered.shape, y_test.shape)\n\n\n\n\n\nn_features = X_train_centered.shape[1]\nn_classes = 10\nrandom_seed = 123\nnp.random.seed(random_seed)\n\ng = tf.Graph()\nwith g.as_default():\n    tf.set_random_seed(random_seed)\n    tf_x = tf.placeholder(dtype=tf.float32,\n                       shape=(None, n_features),\n                       name=\'tf_x\')\n\n    tf_y = tf.placeholder(dtype=tf.int32, \n                        shape=None, name=\'tf_y\')\n    y_onehot = tf.one_hot(indices=tf_y, depth=n_classes)\n\n    h1 = tf.layers.dense(inputs=tf_x, units=50,\n                         activation=tf.tanh,\n                         name=\'layer1\')\n\n    h2 = tf.layers.dense(inputs=h1, units=50,\n                         activation=tf.tanh,\n                         name=\'layer2\')\n\n    logits = tf.layers.dense(inputs=h2, \n                             units=10,\n                             activation=None,\n                             name=\'layer3\')\n\n    predictions = {\n        \'classes\' : tf.argmax(logits, axis=1, \n                              name=\'predicted_classes\'),\n        \'probabilities\' : tf.nn.softmax(logits, \n                              name=\'softmax_tensor\')\n    }\n\n\n\n\n## define cost function and optimizer:\nwith g.as_default():\n    cost = tf.losses.softmax_cross_entropy(\n            onehot_labels=y_onehot, logits=logits)\n\n    optimizer = tf.train.GradientDescentOptimizer(\n            learning_rate=0.001)\n\n    train_op = optimizer.minimize(loss=cost)\n\n    init_op = tf.global_variables_initializer()\n\n\n\n\ndef create_batch_generator(X, y, batch_size=128, shuffle=False):\n    X_copy = np.array(X)\n    y_copy = np.array(y)\n    \n    if shuffle:\n        data = np.column_stack((X_copy, y_copy))\n        np.random.shuffle(data)\n        X_copy = data[:, :-1]\n        y_copy = data[:, -1].astype(int)\n    \n    for i in range(0, X.shape[0], batch_size):\n        yield (X_copy[i:i+batch_size, :], y_copy[i:i+batch_size])\n\n\n\n\n## create a session to launch the graph\nsess =  tf.Session(graph=g)\n## run the variable initialization operator\nsess.run(init_op)\n\n## 50 epochs of training:\ntraining_costs = []\nfor epoch in range(50):\n    training_loss = []\n    batch_generator = create_batch_generator(\n            X_train_centered, y_train, \n            batch_size=64)\n    for batch_X, batch_y in batch_generator:\n        ## prepare a dict to feed data to our network:\n        feed = {tf_x:batch_X, tf_y:batch_y}\n        _, batch_cost = sess.run([train_op, cost],\n                                 feed_dict=feed)\n        training_costs.append(batch_cost)\n    print(\' -- Epoch %2d  \'\n          \'Avg. Training Loss: %.4f\' % (\n              epoch+1, np.mean(training_costs)\n    ))\n\n\n\n\n## do prediction on the test set:\nfeed = {tf_x : X_test_centered}\ny_pred = sess.run(predictions[\'classes\'], \n                  feed_dict=feed)\n \nprint(\'Test Accuracy: %.2f%%\' % (\n      100*np.sum(y_pred == y_test)/y_test.shape[0]))\n\n\n# ### Developing Multilayer Neural Networks with Keras\n\n\n\nX_train, y_train = load_mnist(\'./\', kind=\'train\')\nprint(\'Rows: %d,  Columns: %d\' %(X_train.shape[0], \n                                 X_train.shape[1]))\nX_test, y_test = load_mnist(\'./\', kind=\'t10k\')\nprint(\'Rows: %d,  Columns: %d\' %(X_test.shape[0], \n                                 X_test.shape[1]))\n\n## mean centering and normalization:\nmean_vals = np.mean(X_train, axis=0)\nstd_val = np.std(X_train)\n\nX_train_centered = (X_train - mean_vals)/std_val\nX_test_centered = (X_test - mean_vals)/std_val\n \ndel X_train, X_test\n \nprint(X_train_centered.shape, y_train.shape)\n\nprint(X_test_centered.shape, y_test.shape)\n\n\n\n\n\n# NOTE:\n# ================================================\n# If you have TensorFlow >= v1.4 installed\n# you can use the keras API directly instead\n# of importing it from the contrib module\n# for instance, in this code cell, you can skip\n# the import, and following code cells,\n# you can use \n# `tf.keras.utils.to_categorical(y_train)`\n# instead of `keras.utils.to_categorical(y_train)`\n# and so forth.\n\nnp.random.seed(123)\ntf.set_random_seed(123)\n\n\n\n\ny_train_onehot = keras.utils.to_categorical(y_train)\n \nprint(\'First 3 labels: \', y_train[:3])\nprint(\'\\nFirst 3 labels (one-hot):\\n\', y_train_onehot[:3])\n\n\n\n\nmodel = keras.models.Sequential()\n\nmodel.add(\n    keras.layers.Dense(\n        units=50,    \n        input_dim=X_train_centered.shape[1],\n        kernel_initializer=\'glorot_uniform\',\n        bias_initializer=\'zeros\',\n        activation=\'tanh\'))\n\nmodel.add(\n    keras.layers.Dense(\n        units=50,    \n        input_dim=50,\n        kernel_initializer=\'glorot_uniform\',\n        bias_initializer=\'zeros\',\n        activation=\'tanh\'))\n\nmodel.add(\n    keras.layers.Dense(\n        units=y_train_onehot.shape[1],    \n        input_dim=50,\n        kernel_initializer=\'glorot_uniform\',\n        bias_initializer=\'zeros\',\n        activation=\'softmax\'))\n\n\nsgd_optimizer = keras.optimizers.SGD(\n        lr=0.001, decay=1e-7, momentum=.9)\n\nmodel.compile(optimizer=sgd_optimizer,\n              loss=\'categorical_crossentropy\')\n\n\n\n\nhistory = model.fit(X_train_centered, y_train_onehot,\n                    batch_size=64, epochs=50,\n                    verbose=1,\n                    validation_split=0.1)\n\n\n\n\ny_train_pred = model.predict_classes(X_train_centered, verbose=0)\nprint(\'First 3 predictions: \', y_train_pred[:3])\n\n\n\n\ny_train_pred = model.predict_classes(X_train_centered, \n                                     verbose=0)\ncorrect_preds = np.sum(y_train == y_train_pred, axis=0) \ntrain_acc = correct_preds / y_train.shape[0]\n\nprint(\'First 3 predictions: \', y_train_pred[:3])\nprint(\'Training accuracy: %.2f%%\' % (train_acc * 100))\n\n\n\n\ny_test_pred = model.predict_classes(X_test_centered, \n                                    verbose=0)\n\ncorrect_preds = np.sum(y_test == y_test_pred, axis=0) \ntest_acc = correct_preds / y_test.shape[0]\nprint(\'Test accuracy: %.2f%%\' % (test_acc * 100))\n\n\n# ## Choosing activation functions for multilayer networks\n\n# ### Logistic function recap\n\n\n\n\nX = np.array([1, 1.4, 2.5]) ## first value must be 1\nw = np.array([0.4, 0.3, 0.5])\n\ndef net_input(X, w):\n    return np.dot(X, w)\n\ndef logistic(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef logistic_activation(X, w):\n    z = net_input(X, w)\n    return logistic(z)\n\nprint(\'P(y=1|x) = %.3f\' % logistic_activation(X, w))\n\n\n\n\n# W : array with shape = (n_output_units, n_hidden_units+1)\n# \xc2\xa0 \xc2\xa0 note that the first column are the bias units\n\nW = np.array([[1.1, 1.2, 0.8, 0.4],\n              [0.2, 0.4, 1.0, 0.2],\n              [0.6, 1.5, 1.2, 0.7]])\n\n# A : data array with shape = (n_hidden_units + 1, n_samples)\n# \xc2\xa0 \xc2\xa0 note that the first column of this array must be 1\n\nA = np.array([[1, 0.1, 0.4, 0.6]])\n\nZ = np.dot(W, A[0])\ny_probas = logistic(Z)\n\nprint(\'Net Input: \\n\', Z)\n\nprint(\'Output Units:\\n\', y_probas)\n\n\n\n\ny_class = np.argmax(Z, axis=0)\nprint(\'Predicted class label: %d\' % y_class)\n\n\n# ### Estimating class probabilities in multi-class classification via the softmax function\n\n\n\ndef softmax(z):\n    return np.exp(z) / np.sum(np.exp(z))\n\ny_probas = softmax(Z)\nprint(\'Probabilities:\\n\', y_probas)\n\n\n\n\nnp.sum(y_probas)\n\n\n# ### Broadening the output spectrum by using a hyperbolic tangent\n\n\n\n\ndef tanh(z):\n    e_p = np.exp(z)\n    e_m = np.exp(-z)\n    return (e_p - e_m) / (e_p + e_m)\n\nz = np.arange(-5, 5, 0.005)\nlog_act = logistic(z)\ntanh_act = tanh(z)\n\nplt.ylim([-1.5, 1.5])\nplt.xlabel(\'net input $z$\')\nplt.ylabel(\'activation $\\phi(z)$\')\nplt.axhline(1, color=\'black\', linestyle=\':\')\nplt.axhline(0.5, color=\'black\', linestyle=\':\')\nplt.axhline(0, color=\'black\', linestyle=\':\')\nplt.axhline(-0.5, color=\'black\', linestyle=\':\')\nplt.axhline(-1, color=\'black\', linestyle=\':\')\n\nplt.plot(z, tanh_act,\n         linewidth=3, linestyle=\'--\',\n         label=\'tanh\')\n\nplt.plot(z, log_act,\n         linewidth=3,\n         label=\'logistic\')\nplt.legend(loc=\'lower right\')\nplt.tight_layout()\n#plt.savefig(\'images/13_03.png\')\nplt.show()\n\n\n# ### Rectified Linear Unit activation\n\n\n\n\n\n# ## Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
code/ch14/ch14.py,162,"b'# coding: utf-8\n\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\n\n# *Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com) and Vahid Mirjalili, Packt Publishing Ltd. 2017\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*\n\n# # Chapter 14 - Going Deeper: The Mechanics of TensorFlow\n\n# - [TensorFlow in a nutshell](#TensorFlow-in-a-nutshell)\n# - [Understanding TensorFlow\'s computation graphs](#Understanding-TensorFlow\'s-computation-graphs)\n# - [Working with TensorFlow\xe2\x80\x99s placeholders, variables, and operations](#Working-with-TensorFlow\xe2\x80\x99s-placeholders,-variables,-and-operations)\n#   - [Using placeholders for feeding data as input to a model in TensorFlow](#Using-placeholders-for-feeding-data-as-input-to-a-model-in-TensorFlow)\n#     - [Defining placeholders](#Defining-placeholders)\n#     - [Feeding placeholders with data](#Feeding-placeholders-with-data)\n#     - [Defining placeholders for data arrays with varying batchsizes](#Defining-placeholders-for-data-arrays-with-varying-batchsizes)\n#   - [Variables in TensorFlow](#Variables-in-TensorFlow)\n#     - [Defining variables](#Defining-variables)\n#     - [Initializing variables](#Initializing-variables)\n#     - [Variable scope](#Variable-scope)\n#     - [Reusing variables](#Reusing-variables)\n#   - [Building a regression model](#Building-a-regression-model)\n#   - [Executing objects in a TensorFlow graph using their names](#Executing-objects-in-a-TensorFlow-graph-using-their-names)\n# - [Saving and restoring a model in TensorFlow](#Saving-and-restoring-a-model-in-TensorFlow)\n# - [Transforming Tensors as multidimensional data arrays](#Transforming-Tensors-as-multidimensional-data-arrays)\n# - [Utilizing control flow mechanics in building graphs](#Utilizing-control-flow-mechanics-in-building-graphs)\n# - [Visualizing the graph with TensorBoard](#Visualizing-the-graph-with-TensorBoard)\n# - [Summary](#Summary)\n\n\n\n\n\n\n\n\n\n# ## TensorFlow in a nutshell\n\n\n\n\n\n# **How to get the rank and shape of a tensor**\n\n\n\ng = tf.Graph()\n\n## define the computation graph\nwith g.as_default():\n    ## define tensors t1, t2, t3:\n    t1 = tf.constant(np.pi)\n    t2 = tf.constant([1, 2, 3, 4])\n    t3 = tf.constant([[1, 2], [3, 4]])\n\n    ## get their ranks\n    r1 = tf.rank(t1)\n    r2 = tf.rank(t2)\n    r3 = tf.rank(t3)\n\n    ## get their shapes\n    s1 = t1.get_shape()\n    s2 = t2.get_shape()\n    s3 = t3.get_shape()\n    print(\'Shapes:\', s1, s2, s3)\n\nwith tf.Session(graph=g) as sess:\n    print(\'Ranks:\', \n          r1.eval(), \n          r2.eval(), \n          r3.eval())\n\n\n# ## Understanding TensorFlow\'s computation graph\n\n\n\nImage(""images/14_02.png"")\n\n\n\n\ng = tf.Graph()\n \n## add nodes to the graph\nwith g.as_default():\n    a = tf.constant(1, name=\'a\')\n    b = tf.constant(2, name=\'b\') \n    c = tf.constant(3, name=\'c\') \n\n    z = 2*(a-b) + c\n    \n## launch the graph\nwith tf.Session(graph=g) as sess:\n    print(\'2*(a-b)+c => \', sess.run(z))\n\n\n# ## Working with TensorFlow\xe2\x80\x99s placeholders, variables, and operations\n\n# ### Using placeholders for feeding data as input to a model in TensorFlow\n\n# #### Defining placeholders\n\n\n\ng = tf.Graph()\nwith g.as_default():\n    tf_a = tf.placeholder(tf.int32, shape=[],\n                          name=\'tf_a\')\n    tf_b = tf.placeholder(tf.int32, shape=[],\n                          name=\'tf_b\') \n    tf_c = tf.placeholder(tf.int32, shape=[],\n                          name=\'tf_c\') \n\n    r1 = tf_a-tf_b\n    r2 = 2*r1\n    z  = r2 + tf_c\n\n\n# #### Feeding placeholders with data\n\n\n\n## launch the previous graph\nwith tf.Session(graph=g) as sess:\n    feed = {tf_a: 1,\n            tf_b: 2,\n            tf_c: 3}\n    print(\'z:\', \n          sess.run(z, feed_dict=feed))\n\n\n# Execution with and without feeding tf_c:\n\n\n\n## launch the previous graph\nwith tf.Session(graph=g) as sess:\n    ## execution without feeding tf_c\n    feed = {tf_a: 1,\n            tf_b: 2}\n    print(\'r1:\', \n          sess.run(r1, feed_dict=feed))\n    print(\'r2:\', \n          sess.run(r2, feed_dict=feed))\n    \n    ## execution with feeding tf_c\n    feed = {tf_a: 1,\n            tf_b: 2,\n            tf_c: 3}\n    print(\'r1:\', \n          sess.run(r1, feed_dict=feed))\n    print(\'r2:\', \n          sess.run(r2, feed_dict=feed))\n\n\n# ### Defining placeholders for data arrays with varying batchsizes\n\n# Placeholder for varying batchsizes:\n\n\n\ng = tf.Graph()\n\nwith g.as_default():\n    tf_x = tf.placeholder(tf.float32, \n                          shape=[None, 2],\n                          name=\'tf_x\')\n    \n    x_mean = tf.reduce_mean(tf_x, \n                          axis=0, \n                          name=\'mean\')\n\n\nnp.random.seed(123)\nnp.set_printoptions(precision=2)\n\nwith tf.Session(graph=g) as sess:\n    x1 = np.random.uniform(low=0, high=1, \n                           size=(5,2))\n    print(\'Feeding data with shape\', x1.shape)\n    print(\'Result:\', sess.run(x_mean, \n                             feed_dict={tf_x:x1}))\n    x2 = np.random.uniform(low=0, high=1, \n                           size=(10,2))\n    print(\'Feeding data with shape\', x2.shape)\n    print(\'Result:\', sess.run(x_mean, \n                             feed_dict={tf_x:x2}))\n\n\n\n\nprint(tf_x)\n\n\n# ### Variables in TensorFlow\n# \n\n# #### Defining Variables\n\n\n\ng1 = tf.Graph()\n\nwith g1.as_default():\n    w = tf.Variable(np.array([[1, 2, 3, 4],\n                              [5, 6, 7, 8]]), name=\'w\')\n    print(w)\n\n\n# #### Initializing variables\n\n\n\n## initialize w and evaluate it\nwith tf.Session(graph=g1) as sess:\n    sess.run(tf.global_variables_initializer())\n    print(sess.run(w))\n\n\n\n\n## add the init_op to the graph\nwith g1.as_default():\n    init_op = tf.global_variables_initializer()\n    \n## initialize w with init_op and evaluate it\nwith tf.Session(graph=g1) as sess:\n    sess.run(init_op)\n    print(sess.run(w))\n\n\n\n\ng2 = tf.Graph()\n\nwith g2.as_default():\n    w1 = tf.Variable(1, name=\'w1\')\n    init_op = tf.global_variables_initializer()\n    w2 = tf.Variable(2, name=\'w2\')\n\n\n\n\nwith tf.Session(graph=g2) as sess:\n    sess.run(init_op)\n    print(\'w1:\', sess.run(w1))\n\n\n# Error if a variable is not initialized:\n\n\n\nwith tf.Session(graph=g2) as sess:\n    \n    try:\n        sess.run(init_op)\n        print(\'w2:\', sess.run(w2))\n    except tf.errors.FailedPreconditionError as e:\n        print(e)\n\n\n# #### Variable scope\n\n\n\ng = tf.Graph()\n\nwith g.as_default():\n    with tf.variable_scope(\'net_A\'):\n        with tf.variable_scope(\'layer-1\'):\n            w1 = tf.Variable(tf.random_normal(\n                shape=(10,4)), name=\'weights\')\n        with tf.variable_scope(\'layer-2\'):\n            w2 = tf.Variable(tf.random_normal(\n                shape=(20,10)), name=\'weights\')\n    with tf.variable_scope(\'net_B\'):\n        with tf.variable_scope(\'layer-1\'):\n            w3 = tf.Variable(tf.random_normal(\n                shape=(10,4)), name=\'weights\')\n\n    print(w1)\n    print(w2)\n    print(w3)\n\n\n# #### Reusing variables\n\n\n\n######################\n## Helper functions ##\n######################\n\ndef build_classifier(data, labels, n_classes=2):\n    data_shape = data.get_shape().as_list()\n    weights = tf.get_variable(name=\'weights\',\n                              shape=(data_shape[1], n_classes),\n                              dtype=tf.float32)\n    bias = tf.get_variable(name=\'bias\', \n                           initializer=tf.zeros(shape=n_classes))\n    print(weights)\n    print(bias)\n    logits = tf.add(tf.matmul(data, weights), \n                    bias, \n                    name=\'logits\')\n    print(logits)\n    return logits, tf.nn.softmax(logits)\n\ndef build_generator(data, n_hidden):\n    data_shape = data.get_shape().as_list()\n    w1 = tf.Variable(\n        tf.random_normal(shape=(data_shape[1], \n                                n_hidden)),\n        name=\'w1\')\n    b1 = tf.Variable(tf.zeros(shape=n_hidden),\n                     name=\'b1\')\n    hidden = tf.add(tf.matmul(data, w1), b1, \n                    name=\'hidden_pre-activation\')\n    hidden = tf.nn.relu(hidden, \'hidden_activation\')\n        \n    w2 = tf.Variable(\n        tf.random_normal(shape=(n_hidden, \n                                data_shape[1])),\n        name=\'w2\')\n    b2 = tf.Variable(tf.zeros(shape=data_shape[1]),\n                     name=\'b2\')\n    output = tf.add(tf.matmul(hidden, w2), b2, \n                    name = \'output\')\n    return output, tf.nn.sigmoid(output)\n    \n########################\n## Defining the graph ##\n########################\n\nbatch_size=64\ng = tf.Graph()\n\nwith g.as_default():\n    tf_X = tf.placeholder(shape=(batch_size, 100), \n                          dtype=tf.float32,\n                          name=\'tf_X\')\n    ## build the generator\n    with tf.variable_scope(\'generator\'):\n        gen_out1 = build_generator(data=tf_X, \n                                   n_hidden=50)\n    \n    ## build the classifier\n    with tf.variable_scope(\'classifier\') as scope:\n        ## classifier for the original data:\n        cls_out1 = build_classifier(data=tf_X, \n                                    labels=tf.ones(\n                                        shape=batch_size))\n        \n        ## reuse the classifier for generated data\n        scope.reuse_variables()\n        cls_out2 = build_classifier(data=gen_out1[1],\n                                    labels=tf.zeros(\n                                        shape=batch_size))\n        \n        init_op = tf.global_variables_initializer()\n\n\n\n\n## alternative way\n\ng = tf.Graph()\n\nwith g.as_default():\n    tf_X = tf.placeholder(shape=(batch_size, 100), \n                          dtype=tf.float32,\n                          name=\'tf_X\')\n    ## build the generator\n    with tf.variable_scope(\'generator\'):\n        gen_out1 = build_generator(data=tf_X, \n                                   n_hidden=50)\n    \n    ## build the classifier\n    with tf.variable_scope(\'classifier\'):\n        ## classifier for the original data:\n        cls_out1 = build_classifier(data=tf_X, \n                                    labels=tf.ones(\n                                        shape=batch_size))\n        \n    with tf.variable_scope(\'classifier\', reuse=True):\n        ## reuse the classifier for generated data\n        cls_out2 = build_classifier(data=gen_out1[1],\n                                    labels=tf.zeros(\n                                        shape=batch_size))\n        \n        init_op = tf.global_variables_initializer()\n\n\n# ### Building a regression model\n\n\n\n## define a graph\ng = tf.Graph()\n\n## define the computation graph\nwith g.as_default():\n    ## placeholders\n    tf.set_random_seed(123)\n    tf_x = tf.placeholder(shape=(None), \n                          dtype=tf.float32, \n                          name=\'tf_x\')\n    tf_y = tf.placeholder(shape=(None), \n                          dtype=tf.float32,\n                          name=\'tf_y\')\n    \n    ## define the variable (model parameters)\n    weight = tf.Variable(\n        tf.random_normal(\n            shape=(1, 1), \n            stddev=0.25),\n        name=\'weight\')\n    bias = tf.Variable(0.0, name=\'bias\')\n    \n    ## build the model\n    y_hat = tf.add(weight * tf_x, bias, \n                   name=\'y_hat\')\n    print(y_hat)\n    \n    ## compute the cost\n    cost = tf.reduce_mean(tf.square(tf_y - y_hat), \n                          name=\'cost\')\n    print(cost)\n    \n    ## train\n    optim = tf.train.GradientDescentOptimizer(\n        learning_rate=0.001)\n    train_op = optim.minimize(cost, name=\'train_op\')\n\n\n\n\n## create a random toy dataset for regression\n\n\nnp.random.seed(0)\n\ndef make_random_data():\n    x = np.random.uniform(low=-2, high=4, size=200)\n    y = []\n    for t in x:\n        r = np.random.normal(loc=0.0, \n                             scale=(0.5 + t*t/3), \n                             size=None)\n        y.append(r)\n    return  x, 1.726*x -0.84 + np.array(y)\n\n\nx, y = make_random_data() \n\nplt.plot(x, y, \'o\')\n# plt.savefig(\'images/14_03.png\', dpi=300)\nplt.show()\n\n\n# ### Executing objects in a TensorFlow graph using their names\n\n\n\n## train/test splits:\nx_train, y_train = x[:100], y[:100]\nx_test, y_test = x[100:], y[100:]\n\n\n## training the model\nn_epochs = 500\ntraining_costs = []\nwith tf.Session(graph=g) as sess:\n    ## first, run the variables initializer\n    sess.run(tf.global_variables_initializer())\n    \n    ## train the model for n_epochs\n    for e in range(n_epochs):\n        c, _ = sess.run([cost, train_op], \n                        feed_dict={tf_x: x_train,\n                                   tf_y: y_train})\n        training_costs.append(c)\n        if not e % 50:\n            print(\'Epoch %4d: %.4f\' % (e, c))\n            \n\nplt.plot(training_costs)\n# plt.savefig(\'images/14_04.png\', dpi=300)\n\n\n# Executing with variable names:\n\n\n\n## train/test splits\nx_train, y_train = x[:100], y[:100]\nx_test, y_test = x[100:], y[100:]\n\n## plot trainng data\nplt.plot(x_train, y_train, \'o\')\nplt.show()\n\n## training the model\nn_epochs = 500\ntraining_costs = []\nwith tf.Session(graph=g) as sess:\n    ## first, run the variables initializer\n    sess.run(tf.global_variables_initializer())\n    \n    ## train the model for n_eopchs\n    for e in range(n_epochs):\n        c, _ = sess.run([\'cost:0\', \'train_op\'], \n                        feed_dict={\'tf_x:0\': x_train,\n                                   \'tf_y:0\': y_train})\n        training_costs.append(c)\n        if not e % 50:\n            print(\'Epoch %4d: %.4f\' % (e, c))\n\n\n# ## Saving and restoring a model in TensorFlow\n\n\n\n## add saver to the graph\nwith g.as_default():\n    saver = tf.train.Saver()\n    \n## training the model\nn_epochs = 500\ntraining_costs = []\n\nwith tf.Session(graph=g) as sess:\n    ## first, run the variables initializer\n    sess.run(tf.global_variables_initializer())\n    \n    ## train the model for n_epochs\n    for e in range(n_epochs):\n        c, _ = sess.run([\'cost:0\', \'train_op\'], \n                        feed_dict={\'tf_x:0\':x_train,\n                                   \'tf_y:0\':y_train})\n        training_costs.append(c)\n        if not e % 50:\n            print(\'Epoch %4d: %.4f\' % (e, c))\n            \n    saver.save(sess, \'./trained-model\')\n\n\n# Restoring the saved model:\n\n\n\n## new file: loading a trained model\n## and run the model on test set\n\n\ng2 = tf.Graph()\nwith tf.Session(graph=g2) as sess:\n    new_saver = tf.train.import_meta_graph(\n        \'./trained-model.meta\')\n    new_saver.restore(sess, \'./trained-model\')\n    \n    y_pred = sess.run(\'y_hat:0\', \n                      feed_dict={\'tf_x:0\' : x_test})\n\n\n\n\nprint(\'SSE: %.4f\' % (np.sum(np.square(y_pred - y_test))))\n\n\n\n\nx_arr = np.arange(-2, 4, 0.1)\n\ng2 = tf.Graph()\nwith tf.Session(graph=g2) as sess:\n    new_saver = tf.train.import_meta_graph(\n        \'./trained-model.meta\')\n    new_saver.restore(sess, \'./trained-model\')\n    \n    y_arr = sess.run(\'y_hat:0\', \n                      feed_dict={\'tf_x:0\' : x_arr})\n\nplt.figure()\nplt.plot(x_train, y_train, \'bo\')\nplt.plot(x_test, y_test, \'bo\', alpha=0.3)\nplt.plot(x_arr, y_arr.T[:, 0], \'-r\', lw=3)\n# plt.savefig(\'images/14_05.png\', dpi=400)\nplt.show()\n\n\n# ## Transforming Tensors as multidimensional data arrays\n\n\n\ng = tf.Graph()\nwith g.as_default():\n    arr = np.array([[1., 2., 3., 3.5],\n                    [4., 5., 6., 6.5],\n                    [7., 8., 9., 9.5]])\n    T1 = tf.constant(arr, name=\'T1\')\n    print(T1)\n    s = T1.get_shape()\n    print(\'Shape of T1 is\', s)\n    T2 = tf.Variable(tf.random_normal(\n        shape=s))\n    print(T2)\n    T3 = tf.Variable(tf.random_normal(\n        shape=(s.as_list()[0],)))\n    print(T3)\n\n\n\n\nwith g.as_default():\n    T4 = tf.reshape(T1, shape=[1, 1, -1], \n                    name=\'T4\')\n    print(T4)\n    T5 = tf.reshape(T1, shape=[1, 3, -1], \n                    name=\'T5\')\n    print(T5)\n\n\n\n\nwith tf.Session(graph = g) as sess:\n    print(sess.run(T4)) \n    print()   \n    print(sess.run(T5))\n\n\n\n\nwith g.as_default():\n    T6 = tf.transpose(T5, perm=[2, 1, 0], \n                     name=\'T6\')\n    print(T6)\n    T7 = tf.transpose(T5, perm=[0, 2, 1], \n                     name=\'T7\')\n    print(T7)\n\n\n\n\nwith g.as_default():\n    t5_splt = tf.split(T5, \n                       num_or_size_splits=2, \n                       axis=2, name=\'T8\')\n    print(t5_splt)\n\n\n\n\ng = tf.Graph()\nwith g.as_default():\n    t1 = tf.ones(shape=(5, 1), \n                 dtype=tf.float32, name=\'t1\')\n    t2 = tf.zeros(shape=(5, 1),\n                 dtype=tf.float32, name=\'t2\')\n    print(t1)\n    print(t2)\n    \nwith g.as_default():\n    t3 = tf.concat([t1, t2], axis=0, name=\'t3\')\n    print(t3)\n    t4 = tf.concat([t1, t2], axis=1, name=\'t4\')\n    print(t4)\n\n\n\n\nwith tf.Session(graph = g) as sess:\n    print(t3.eval())\n    print()\n    print(t4.eval())\n\n\n# ## Utilizing control flow mechanics in building graphs\n\n\n\n## Python control flow\n\n\nx, y = 1.0, 2.0\n\ng = tf.Graph()\nwith g.as_default():\n    tf_x = tf.placeholder(dtype=tf.float32, \n                           shape=None, name=\'tf_x\')\n    tf_y = tf.placeholder(dtype=tf.float32, \n                           shape=None, name=\'tf_y\')\n    if x < y:\n        res = tf.add(tf_x, tf_y, name=\'result_add\')\n    else:\n        res = tf.subtract(tf_x, tf_y, name=\'result_sub\')\n        \n    print(\'Object: \', res)\n        \nwith tf.Session(graph=g) as sess:\n    print(\'x < y: %s -> Result:\' % (x < y), \n          res.eval(feed_dict={\'tf_x:0\': x, \n                              \'tf_y:0\': y}))\n    x, y = 2.0, 1.0\n    print(\'x < y: %s -> Result:\' % (x < y), \n          res.eval(feed_dict={\'tf_x:0\': x,\n                              \'tf_y:0\': y}))  \n    \n    ## uncomment the next line if you want to visualize the graph in TensorBoard:\n    file_writer = tf.summary.FileWriter(logdir=\'./logs/py-cflow/\', graph=g)\n\n\n\n\n## TensorFlow control flow\n\n\nx, y = 1.0, 2.0\n\ng = tf.Graph()\nwith g.as_default():\n    tf_x = tf.placeholder(dtype=tf.float32, \n                           shape=None, name=\'tf_x\')\n    tf_y = tf.placeholder(dtype=tf.float32, \n                           shape=None, name=\'tf_y\')\n    res = tf.cond(tf_x < tf_y, \n                  lambda: tf.add(tf_x, tf_y, \n                                 name=\'result_add\'),\n                  lambda: tf.subtract(tf_x, tf_y, \n                                 name=\'result_sub\'))\n    print(\'Object:\', res)\n        \nwith tf.Session(graph=g) as sess:\n    print(\'x < y: %s -> Result:\' % (x < y), \n          res.eval(feed_dict={\'tf_x:0\': x, \n                              \'tf_y:0\': y}))\n    x, y = 2.0, 1.0\n    print(\'x < y: %s -> Result:\' % (x < y), \n          res.eval(feed_dict={\'tf_x:0\': x,\n                              \'tf_y:0\': y}))  \n\n    #file_writer = tf.summary.FileWriter(logdir=\'./logs/tf-cond/\', graph=g)\n\n\n\n\n\n\n# ## Visualizing the graph with TensorBoard\n\n\n\n\n\n###########################\n##   Helper functions    ##\n###########################\n\ndef build_classifier(data, labels, n_classes=2):\n    data_shape = data.get_shape().as_list()\n    weights = tf.get_variable(name = \'weights\',\n                              shape=(data_shape[1],\n                                     n_classes),\n                              dtype=tf.float32)\n    bias = tf.get_variable(name=\'bias\', \n                           initializer=tf.zeros(\n                                     shape=n_classes))\n    print(weights)\n    print(bias)\n    logits = tf.add(tf.matmul(data, weights), \n                    bias, \n                    name=\'logits\')\n    print(logits)\n    return logits, tf.nn.softmax(logits)\n\n\ndef build_generator(data, n_hidden):\n    data_shape = data.get_shape().as_list()\n    w1 = tf.Variable(\n        tf.random_normal(shape=(data_shape[1], \n                                n_hidden)),\n        name=\'w1\')\n    b1 = tf.Variable(tf.zeros(shape=n_hidden),\n                     name=\'b1\')\n    hidden = tf.add(tf.matmul(data, w1), b1, \n                    name=\'hidden_pre-activation\')\n    hidden = tf.nn.relu(hidden, \'hidden_activation\')\n        \n    w2 = tf.Variable(\n        tf.random_normal(shape=(n_hidden, \n                                data_shape[1])),\n        name=\'w2\')\n    b2 = tf.Variable(tf.zeros(shape=data_shape[1]),\n                     name=\'b2\')\n    output = tf.add(tf.matmul(hidden, w2), b2, \n                    name = \'output\')\n    return output, tf.nn.sigmoid(output)\n\n\n###########################\n##  Building the graph   ##\n###########################\n\nbatch_size=64\ng = tf.Graph()\n\nwith g.as_default():\n    tf_X = tf.placeholder(shape=(batch_size, 100), \n                          dtype=tf.float32,\n                          name=\'tf_X\')\n    \n    ## build the generator\n    with tf.variable_scope(\'generator\'):\n        gen_out1 = build_generator(data=tf_X, \n                                   n_hidden=50)\n    \n    ## build the classifier\n    with tf.variable_scope(\'classifier\') as scope:\n        ## classifier for the original data:\n        cls_out1 = build_classifier(data=tf_X, \n                                    labels=tf.ones(\n                                        shape=batch_size))\n        \n        ## reuse the classifier for generated data\n        scope.reuse_variables()\n        cls_out2 = build_classifier(data=gen_out1[1],\n                                    labels=tf.zeros(\n                                        shape=batch_size))\n\n        \nwith tf.Session(graph = g) as sess:\n    sess.run(tf.global_variables_initializer())\n    \n    file_writer = tf.summary.FileWriter(logdir=\'logs/\', graph=g)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ## Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
code/ch15/ch15.py,95,"b'\n# coding: utf-8\n\n# *Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 15 - Classifying Images with Deep Convolutional Neural Networks\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n# In[1]:\n\n\nget_ipython().run_line_magic(\'load_ext\', \'watermark\')\nget_ipython().run_line_magic(\'watermark\', ""-a \'Sebastian Raschka & Vahid Mirjalili\' -d -p numpy,scipy,tensorflow"")\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see*: https://github.com/rasbt/watermark.\n\n# ### Overview\n\n# - [Building blocks of convolutional neural networks](#Building-blocks-of-convolutional-neural-networks)\n#   - [Understanding CNNs and learning feature hierarchies](#Understanding-CNNs-and-learning-feature-hierarchies)\n#   - [Performing discrete convolutions](#Performing-discrete-convolutions)\n#     - [Performing a discrete convolution in one dimension](#Performing-a-discrete-convolution-in-one-dimension)\n#     - [The effect of zero-padding in convolution](#The-effect-of-zero-padding-in-convolution)\n#     - [Determining the size of the convolution output](#Determining-the-size-of-the-convolution-output)\n#     - [Performing a discrete convolution in 2D](#Performing-a-discrete-convolution-in-2D)\n#     - [Sub-sampling](#Sub-sampling)\n#   - [Putting everything together to build a CNN](#Putting-everything-together-to-build-a-CNN)\n#     - [The multilayer CNN architecture](#The-multilayer-CNN-architecture)\n#     - [Loading and preprocessing the data](#Loading-and-preprocessing-the-data)\n#     - [Implementing a CNN in TensorFlow low-level API](#Implementing-a-CNN-in-TensorFlow-low-level-API)\n#     - [Implementing a CNN in the TensorFlow layers API](#Implementing-a-CNN-in-the-TensorFlow-layers-API)\n\n# In[2]:\n\n\nfrom IPython.display import Image\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\n\n\n# In[3]:\n\n\n# This is for Python 2.7 compatibility\nfrom __future__ import print_function\n\n\n# # Building blocks of convolutional neural networks \n\n# ## Understanding CNNs and learning feature hierarchies\n\n# In[4]:\n\n\nImage(filename=\'images/15_01.png\', width=700) \n\n\n# ## Performing discrete convolutions  \n\n# ###  Performing a discrete convolution in one dimension\n\n# In[5]:\n\n\nImage(filename=\'images/15_02.png\', width=700) \n\n\n# In[6]:\n\n\nImage(filename=\'images/15_03.png\', width=700) \n\n\n# ### The effect of zero-padding in convolution\n\n# In[7]:\n\n\nImage(filename=\'images/15_11.png\', width=700) \n\n\n# ### Determining the size of the convolution output\n\n# In[8]:\n\n\nimport numpy as np\n\n\ndef conv1d(x, w, p=0, s=1):\n    w_rot = np.array(w[::-1])\n    x_padded = np.array(x)\n    if p > 0:\n        zero_pad = np.zeros(shape=p)\n        x_padded = np.concatenate([zero_pad, x_padded, zero_pad])\n    res = []\n    for i in range(0, int(len(x)/s),s):\n        res.append(np.sum(x_padded[i:i+w_rot.shape[0]] * w_rot))\n    return np.array(res)\n\n## Testing:\nx = [1, 3, 2, 4, 5, 6, 1, 3]\nw = [1, 0, 3, 1, 2]\nprint(\'Conv1d Implementation: \', \n      conv1d(x, w, p=2, s=1))\nprint(\'Numpy Results:         \', \n      np.convolve(x, w, mode=\'same\'))\n\n\n# ### Performing a discrete convolution in 2D\n\n# In[9]:\n\n\nImage(filename=\'images/15_04.png\', width=700) \n\n\n# In[10]:\n\n\nImage(filename=\'images/15_05.png\', width=900) \n\n\n# In[11]:\n\n\nimport numpy as np\nimport scipy.signal\n\n\ndef conv2d(X, W, p=(0,0), s=(1,1)):\n    W_rot = np.array(W)[::-1,::-1]\n    X_orig = np.array(X)\n    n1 = X_orig.shape[0] + 2*p[0]\n    n2 = X_orig.shape[1] + 2*p[1]\n    X_padded = np.zeros(shape=(n1,n2))\n    X_padded[p[0]:p[0] + X_orig.shape[0], \n             p[1]:p[1] + X_orig.shape[1]] = X_orig\n\n    res = []\n    for i in range(0, int((X_padded.shape[0] - \n                           W_rot.shape[0])/s[0])+1, s[0]):\n        res.append([])\n        for j in range(0, int((X_padded.shape[1] - \n                               W_rot.shape[1])/s[1])+1, s[1]):\n            X_sub = X_padded[i:i+W_rot.shape[0], j:j+W_rot.shape[1]]\n            res[-1].append(np.sum(X_sub * W_rot))\n    return(np.array(res))\n    \nX = [[1, 3, 2, 4], [5, 6, 1, 3], [1 , 2,0, 2], [3, 4, 3, 2]]\nW = [[1, 0, 3], [1, 2, 1], [0, 1, 1]]\nprint(\'Conv2d Implementation: \\n\', \n      conv2d(X, W, p=(1,1), s=(1,1)))\n\nprint(\'Scipy Results:         \\n\', \n      scipy.signal.convolve2d(X, W, mode=\'same\'))\n\n\n# ## Sub-sampling\n\n# In[12]:\n\n\nImage(filename=\'images/15_06.png\', width=700) \n\n\n# # Putting everything together to build a CNN \n\n# ## Working with multiple input or color channels\n\n# In[13]:\n\n\nimport scipy.misc\n\n\ntry:\n    img = scipy.misc.imread(\'./example-image.png\', mode=\'RGB\')\nexcept AttributeError:\n    s = (""scipy.misc.imread requires Python\'s image library PIL""\n         "" You can satisfy this requirement by installing the""\n         "" userfriendly fork PILLOW via `pip install pillow`."")\n    raise AttributeError(s)\n    \n    \nprint(\'Image shape:\', img.shape)\nprint(\'Number of channels:\', img.shape[2])\nprint(\'Image data type:\', img.dtype)\n\nprint(img[100:102, 100:102, :])\n\n\n# In[14]:\n\n\nImage(filename=\'images/15_07.png\', width=800) \n\n\n# ## Regularizing a neural network with dropout\n\n# In[15]:\n\n\nImage(filename=\'images/15_08.png\', width=800) \n\n\n# # Implementing a deep convolutional neural network using TensorFlow\n\n# ## The multilayer CNN architecture \n\n# In[16]:\n\n\nImage(filename=\'images/15_09.png\', width=800) \n\n\n# ## Loading and preprocessing the data\n\n# In[17]:\n\n\n## unzips mnist\n\nimport sys\nimport gzip\nimport shutil\nimport os\n\n\nif (sys.version_info > (3, 0)):\n    writemode = \'wb\'\nelse:\n    writemode = \'w\'\n\nzipped_mnist = [f for f in os.listdir(\'./\')\n                if f.endswith(\'ubyte.gz\')]\nfor z in zipped_mnist:\n    with gzip.GzipFile(z, mode=\'rb\') as decompressed, open(z[:-3], writemode) as outfile:\n        outfile.write(decompressed.read())\n\n\n# In[18]:\n\n\nimport struct\nimport numpy as np\n\n\ndef load_mnist(path, kind=\'train\'):\n    """"""Load MNIST data from `path`""""""\n    labels_path = os.path.join(path,\n                               \'%s-labels-idx1-ubyte\'\n                                % kind)\n    images_path = os.path.join(path,\n                               \'%s-images-idx3-ubyte\'\n                               % kind)\n\n    with open(labels_path, \'rb\') as lbpath:\n        magic, n = struct.unpack(\'>II\',\n                                 lbpath.read(8))\n        labels = np.fromfile(lbpath,\n                             dtype=np.uint8)\n\n    with open(images_path, \'rb\') as imgpath:\n        magic, num, rows, cols = struct.unpack("">IIII"",\n                                               imgpath.read(16))\n        images = np.fromfile(imgpath,\n                             dtype=np.uint8).reshape(len(labels), 784)\n\n    return images, labels\n\n\nX_data, y_data = load_mnist(\'./\', kind=\'train\')\nprint(\'Rows: %d,  Columns: %d\' % (X_data.shape[0], X_data.shape[1]))\nX_test, y_test = load_mnist(\'./\', kind=\'t10k\')\nprint(\'Rows: %d,  Columns: %d\' % (X_test.shape[0], X_test.shape[1]))\n\nX_train, y_train = X_data[:50000,:], y_data[:50000]\nX_valid, y_valid = X_data[50000:,:], y_data[50000:]\n\nprint(\'Training:   \', X_train.shape, y_train.shape)\nprint(\'Validation: \', X_valid.shape, y_valid.shape)\nprint(\'Test Set:   \', X_test.shape, y_test.shape)\n\n\n# In[19]:\n\n\ndef batch_generator(X, y, batch_size=64, \n                    shuffle=False, random_seed=None):\n    \n    idx = np.arange(y.shape[0])\n    \n    if shuffle:\n        rng = np.random.RandomState(random_seed)\n        rng.shuffle(idx)\n        X = X[idx]\n        y = y[idx]\n    \n    for i in range(0, X.shape[0], batch_size):\n        yield (X[i:i+batch_size, :], y[i:i+batch_size])\n\n\n# In[20]:\n\n\nmean_vals = np.mean(X_train, axis=0)\nstd_val = np.std(X_train)\n\nX_train_centered = (X_train - mean_vals)/std_val\nX_valid_centered = (X_valid - mean_vals)/std_val\nX_test_centered = (X_test - mean_vals)/std_val\n\ndel X_data, y_data, X_train, X_valid, X_test\n\n\n# ## Implementing a CNN in TensorFlow low-level API\n\n# In[21]:\n\n\nimport tensorflow as tf\nimport numpy as np\n\n\n## wrapper functions \n\ndef conv_layer(input_tensor, name,\n               kernel_size, n_output_channels, \n               padding_mode=\'SAME\', strides=(1, 1, 1, 1)):\n    with tf.variable_scope(name):\n        ## get n_input_channels:\n        ##   input tensor shape: \n        ##   [batch x width x height x channels_in]\n        input_shape = input_tensor.get_shape().as_list()\n        n_input_channels = input_shape[-1] \n\n        weights_shape = (list(kernel_size) + \n                         [n_input_channels, n_output_channels])\n\n        weights = tf.get_variable(name=\'_weights\',\n                                  shape=weights_shape)\n        print(weights)\n        biases = tf.get_variable(name=\'_biases\',\n                                 initializer=tf.zeros(\n                                     shape=[n_output_channels]))\n        print(biases)\n        conv = tf.nn.conv2d(input=input_tensor, \n                            filter=weights,\n                            strides=strides, \n                            padding=padding_mode)\n        print(conv)\n        conv = tf.nn.bias_add(conv, biases, \n                              name=\'net_pre-activation\')\n        print(conv)\n        conv = tf.nn.relu(conv, name=\'activation\')\n        print(conv)\n        \n        return conv\n    \n\n## testing\ng = tf.Graph()\nwith g.as_default():\n    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n    conv_layer(x, name=\'convtest\', kernel_size=(3, 3), n_output_channels=32)\n    \ndel g, x\n\n\n# In[22]:\n\n\ndef fc_layer(input_tensor, name, \n             n_output_units, activation_fn=None):\n    with tf.variable_scope(name):\n        input_shape = input_tensor.get_shape().as_list()[1:]\n        n_input_units = np.prod(input_shape)\n        if len(input_shape) > 1:\n            input_tensor = tf.reshape(input_tensor, \n                                      shape=(-1, n_input_units))\n\n        weights_shape = [n_input_units, n_output_units]\n\n        weights = tf.get_variable(name=\'_weights\',\n                                  shape=weights_shape)\n        print(weights)\n        biases = tf.get_variable(name=\'_biases\',\n                                 initializer=tf.zeros(\n                                     shape=[n_output_units]))\n        print(biases)\n        layer = tf.matmul(input_tensor, weights)\n        print(layer)\n        layer = tf.nn.bias_add(layer, biases,\n                              name=\'net_pre-activation\')\n        print(layer)\n        if activation_fn is None:\n            return layer\n        \n        layer = activation_fn(layer, name=\'activation\')\n        print(layer)\n        return layer\n\n    \n## testing:\ng = tf.Graph()\nwith g.as_default():\n    x = tf.placeholder(tf.float32, \n                       shape=[None, 28, 28, 1])\n    fc_layer(x, name=\'fctest\', n_output_units=32, \n             activation_fn=tf.nn.relu)\n    \ndel g, x\n\n\n# In[23]:\n\n\ndef build_cnn(learning_rate=1e-4):\n    ## Placeholders for X and y:\n    tf_x = tf.placeholder(tf.float32, shape=[None, 784],\n                          name=\'tf_x\')\n    tf_y = tf.placeholder(tf.int32, shape=[None],\n                          name=\'tf_y\')\n\n    # reshape x to a 4D tensor: \n    # [batchsize, width, height, 1]\n    tf_x_image = tf.reshape(tf_x, shape=[-1, 28, 28, 1],\n                            name=\'tf_x_reshaped\')\n    ## One-hot encoding:\n    tf_y_onehot = tf.one_hot(indices=tf_y, depth=10,\n                             dtype=tf.float32,\n                             name=\'tf_y_onehot\')\n\n    ## 1st layer: Conv_1\n    print(\'\\nBuilding 1st layer: \')\n    h1 = conv_layer(tf_x_image, name=\'conv_1\',\n                    kernel_size=(5, 5), \n                    padding_mode=\'VALID\',\n                    n_output_channels=32)\n    ## MaxPooling\n    h1_pool = tf.nn.max_pool(h1, \n                             ksize=[1, 2, 2, 1],\n                             strides=[1, 2, 2, 1], \n                             padding=\'SAME\')\n    ## 2n layer: Conv_2\n    print(\'\\nBuilding 2nd layer: \')\n    h2 = conv_layer(h1_pool, name=\'conv_2\', \n                    kernel_size=(5,5), \n                    padding_mode=\'VALID\',\n                    n_output_channels=64)\n    ## MaxPooling \n    h2_pool = tf.nn.max_pool(h2, \n                             ksize=[1, 2, 2, 1],\n                             strides=[1, 2, 2, 1], \n                             padding=\'SAME\')\n\n    ## 3rd layer: Fully Connected\n    print(\'\\nBuilding 3rd layer:\')\n    h3 = fc_layer(h2_pool, name=\'fc_3\',\n                  n_output_units=1024, \n                  activation_fn=tf.nn.relu)\n\n    ## Dropout\n    keep_prob = tf.placeholder(tf.float32, name=\'fc_keep_prob\')\n    h3_drop = tf.nn.dropout(h3, keep_prob=keep_prob, \n                            name=\'dropout_layer\')\n\n    ## 4th layer: Fully Connected (linear activation)\n    print(\'\\nBuilding 4th layer:\')\n    h4 = fc_layer(h3_drop, name=\'fc_4\',\n                  n_output_units=10, \n                  activation_fn=None)\n\n    ## Prediction\n    predictions = {\n        \'probabilities\' : tf.nn.softmax(h4, name=\'probabilities\'),\n        \'labels\' : tf.cast(tf.argmax(h4, axis=1), tf.int32,\n                           name=\'labels\')\n    }\n    \n    ## Visualize the graph with TensorBoard:\n\n    ## Loss Function and Optimization\n    cross_entropy_loss = tf.reduce_mean(\n        tf.nn.softmax_cross_entropy_with_logits(\n            logits=h4, labels=tf_y_onehot),\n        name=\'cross_entropy_loss\')\n\n    ## Optimizer:\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n    optimizer = optimizer.minimize(cross_entropy_loss,\n                                   name=\'train_op\')\n\n    ## Computing the prediction accuracy\n    correct_predictions = tf.equal(\n        predictions[\'labels\'], \n        tf_y, name=\'correct_preds\')\n\n    accuracy = tf.reduce_mean(\n        tf.cast(correct_predictions, tf.float32),\n        name=\'accuracy\')\n\n    \ndef save(saver, sess, epoch, path=\'./model/\'):\n    if not os.path.isdir(path):\n        os.makedirs(path)\n    print(\'Saving model in %s\' % path)\n    saver.save(sess, os.path.join(path,\'cnn-model.ckpt\'),\n               global_step=epoch)\n\n    \ndef load(saver, sess, path, epoch):\n    print(\'Loading model from %s\' % path)\n    saver.restore(sess, os.path.join(\n            path, \'cnn-model.ckpt-%d\' % epoch))\n\n    \ndef train(sess, training_set, validation_set=None,\n          initialize=True, epochs=20, shuffle=True,\n          dropout=0.5, random_seed=None):\n\n    X_data = np.array(training_set[0])\n    y_data = np.array(training_set[1])\n    training_loss = []\n\n    ## initialize variables\n    if initialize:\n        sess.run(tf.global_variables_initializer())\n\n    np.random.seed(random_seed) # for shuflling in batch_generator\n    for epoch in range(1, epochs+1):\n        batch_gen = batch_generator(\n                        X_data, y_data, \n                        shuffle=shuffle)\n        avg_loss = 0.0\n        for i,(batch_x,batch_y) in enumerate(batch_gen):\n            feed = {\'tf_x:0\': batch_x, \n                    \'tf_y:0\': batch_y, \n                    \'fc_keep_prob:0\': dropout}\n            loss, _ = sess.run(\n                    [\'cross_entropy_loss:0\', \'train_op\'],\n                    feed_dict=feed)\n            avg_loss += loss\n\n        training_loss.append(avg_loss / (i+1))\n        print(\'Epoch %02d Training Avg. Loss: %7.3f\' % (\n            epoch, avg_loss), end=\' \')\n        if validation_set is not None:\n            feed = {\'tf_x:0\': validation_set[0],\n                    \'tf_y:0\': validation_set[1],\n                    \'fc_keep_prob:0\':1.0}\n            valid_acc = sess.run(\'accuracy:0\', feed_dict=feed)\n            print(\' Validation Acc: %7.3f\' % valid_acc)\n        else:\n            print()\n\n            \ndef predict(sess, X_test, return_proba=False):\n    feed = {\'tf_x:0\': X_test, \n            \'fc_keep_prob:0\': 1.0}\n    if return_proba:\n        return sess.run(\'probabilities:0\', feed_dict=feed)\n    else:\n        return sess.run(\'labels:0\', feed_dict=feed)\n\n\n# In[24]:\n\n\nimport tensorflow as tf\nimport numpy as np\n\n## Define random seed\nrandom_seed = 123\n\nnp.random.seed(random_seed)\n\n\n## create a graph\ng = tf.Graph()\nwith g.as_default():\n    tf.set_random_seed(random_seed)\n    ## build the graph\n    build_cnn()\n\n    ## saver:\n    saver = tf.train.Saver()\n\n\n# In[25]:\n\n\n## @Readers: PLEASE IGNORE THIS CELL\n##\n## This cell is meant to shrink the\n## dataset when this notebook is run \n## on the Travis Continuous Integration\n## platform to test the code as well as\n## speeding up the run using a smaller\n## dataset for debugging\n\nif \'TRAVIS\' in os.environ:\n    X_train_centered = X_train_centered[:500]\n    y_train = y_train[:500]\n    X_valid_centered = X_valid_centered[:500]\n    y_valid = y_valid[:500]\n\n\n# In[26]:\n\n\n## crearte a TF session \n## and train the CNN model\n\nwith tf.Session(graph=g) as sess:\n    train(sess, \n          training_set=(X_train_centered, y_train), \n          validation_set=(X_valid_centered, y_valid), \n          initialize=True,\n          random_seed=123)\n    save(saver, sess, epoch=20)\n\n\n# In[27]:\n\n\n### Calculate prediction accuracy\n### on test set\n### restoring the saved model\n\ndel g\n\n## create a new graph \n## and build the model\ng2 = tf.Graph()\nwith g2.as_default():\n    tf.set_random_seed(random_seed)\n    ## build the graph\n    build_cnn()\n\n    ## saver:\n    saver = tf.train.Saver()\n\n## create a new session \n## and restore the model\nwith tf.Session(graph=g2) as sess:\n    load(saver, sess, \n         epoch=20, path=\'./model/\')\n    \n    preds = predict(sess, X_test_centered, \n                    return_proba=False)\n\n    print(\'Test Accuracy: %.3f%%\' % (100*\n                np.sum(preds == y_test)/len(y_test)))\n    \n\n\n# In[28]:\n\n\n## run the prediction on \n##  some test samples\n\nnp.set_printoptions(precision=2, suppress=True)\n\nwith tf.Session(graph=g2) as sess:\n    load(saver, sess, \n         epoch=20, path=\'./model/\')\n        \n    print(predict(sess, X_test_centered[:10], \n              return_proba=False))\n    \n    print(predict(sess, X_test_centered[:10], \n                  return_proba=True))\n\n\n# In[29]:\n\n\n## continue training for 20 more epochs\n## without re-initializing :: initialize=False\n## create a new session \n## and restore the model\nwith tf.Session(graph=g2) as sess:\n    load(saver, sess, \n         epoch=20, path=\'./model/\')\n    \n    train(sess,\n          training_set=(X_train_centered, y_train), \n          validation_set=(X_valid_centered, y_valid),\n          initialize=False,\n          epochs=20,\n          random_seed=123)\n        \n    save(saver, sess, epoch=40, path=\'./model/\')\n    \n    preds = predict(sess, X_test_centered, \n                    return_proba=False)\n    \n    print(\'Test Accuracy: %.3f%%\' % (100*\n                np.sum(preds == y_test)/len(y_test)))\n\n\n# In[30]:\n\n\n## build the model\ndef build_cnn():\n    ## Placeholders for X and y:\n    tf_x = tf.placeholder(tf.float32, shape=[None, 784],\n                          name=\'tf_x\')\n    tf_y = tf.placeholder(tf.int32, shape=[None],\n                          name=\'tf_y\')\n\n    # reshape x to a 4D tensor: \n    # [batchsize, width, height, 1]\n    tf_x_image = tf.reshape(tf_x, shape=[-1, 28, 28, 1],\n                            name=\'tf_x_reshaped\')\n    ## One-hot encoding:\n    tf_y_onehot = tf.one_hot(indices=tf_y, depth=10,\n                             dtype=tf.float32,\n                             name=\'tf_y_onehot\')\n\n    ## 1st layer: Conv_1\n    print(\'\\nBuilding 1st layer: \')\n    h1 = conv_layer(tf_x_image, name=\'conv_1\',\n                    kernel_size=(5, 5), \n                    padding_mode=\'VALID\',\n                    n_output_channels=32)\n    ## MaxPooling\n    h1_pool = tf.nn.max_pool(h1, \n                             ksize=[1, 2, 2, 1],\n                             strides=[1, 2, 2, 1], \n                             padding=\'SAME\')\n    ## 2n layer: Conv_2\n    print(\'\\nBuilding 2nd layer: \')\n    h2 = conv_layer(h1_pool, name=\'conv_2\', \n                    kernel_size=(5, 5), \n                    padding_mode=\'VALID\',\n                    n_output_channels=64)\n    ## MaxPooling \n    h2_pool = tf.nn.max_pool(h2, \n                             ksize=[1, 2, 2, 1],\n                             strides=[1, 2, 2, 1], \n                             padding=\'SAME\')\n\n    ## 3rd layer: Fully Connected\n    print(\'\\nBuilding 3rd layer:\')\n    h3 = fc_layer(h2_pool, name=\'fc_3\',\n                  n_output_units=1024, \n                  activation_fn=tf.nn.relu)\n\n    ## Dropout\n    keep_prob = tf.placeholder(tf.float32, name=\'fc_keep_prob\')\n    h3_drop = tf.nn.dropout(h3, keep_prob=keep_prob, \n                            name=\'dropout_layer\')\n\n    ## 4th layer: Fully Connected (linear activation)\n    print(\'\\nBuilding 4th layer:\')\n    h4 = fc_layer(h3_drop, name=\'fc_4\',\n                  n_output_units=10, \n                  activation_fn=None)\n\n    ## Prediction\n    predictions = {\n        \'probabilities\': tf.nn.softmax(h4, name=\'probabilities\'),\n        \'labels\': tf.cast(tf.argmax(h4, axis=1), tf.int32,\n                           name=\'labels\')\n    }\n\n## create a graph\ng = tf.Graph()\nwith g.as_default():\n    tf.set_random_seed(random_seed)\n    ## build the graph\n    build_cnn()\n    \n\nwith tf.Session(graph=g) as sess:\n    file_writer = tf.summary.FileWriter(logdir=\'./tensorboard/\', graph=g)\n\n\n# #### Visualize the graph with TensorBoard\n\n# In[4]:\n\n\nImage(filename=\'images/15_10.png\', width=800) \n\n\n# ## Implementing a CNN in the TensorFlow layers API\n\n# In[32]:\n\n\nimport tensorflow as tf\nimport numpy as np\n\n\nclass ConvNN(object):\n    def __init__(self, batchsize=64,\n                 epochs=20, learning_rate=1e-4, \n                 dropout_rate=0.5,\n                 shuffle=True, random_seed=None):\n        np.random.seed(random_seed)\n        self.batchsize = batchsize\n        self.epochs = epochs\n        self.learning_rate = learning_rate\n        self.dropout_rate = dropout_rate\n        self.shuffle = shuffle\n                \n        g = tf.Graph()\n        with g.as_default():\n            ## set random-seed:\n            tf.set_random_seed(random_seed)\n            \n            ## build the network:\n            self.build()\n\n            ## initializer\n            self.init_op =                 tf.global_variables_initializer()\n\n            ## saver\n            self.saver = tf.train.Saver()\n            \n        ## create a session\n        self.sess = tf.Session(graph=g)\n                \n    def build(self):\n        \n        ## Placeholders for X and y:\n        tf_x = tf.placeholder(tf.float32, \n                              shape=[None, 784],\n                              name=\'tf_x\')\n        tf_y = tf.placeholder(tf.int32, \n                              shape=[None],\n                              name=\'tf_y\')\n        is_train = tf.placeholder(tf.bool, \n                              shape=(),\n                              name=\'is_train\')\n\n        ## reshape x to a 4D tensor: \n        ##  [batchsize, width, height, 1]\n        tf_x_image = tf.reshape(tf_x, shape=[-1, 28, 28, 1],\n                              name=\'input_x_2dimages\')\n        ## One-hot encoding:\n        tf_y_onehot = tf.one_hot(indices=tf_y, depth=10,\n                              dtype=tf.float32,\n                              name=\'input_y_onehot\')\n\n        ## 1st layer: Conv_1\n        h1 = tf.layers.conv2d(tf_x_image, \n                              kernel_size=(5, 5), \n                              filters=32, \n                              activation=tf.nn.relu)\n        ## MaxPooling\n        h1_pool = tf.layers.max_pooling2d(h1, \n                              pool_size=(2, 2), \n                              strides=(2, 2))\n        ## 2n layer: Conv_2\n        h2 = tf.layers.conv2d(h1_pool, kernel_size=(5,5), \n                              filters=64, \n                              activation=tf.nn.relu)\n        ## MaxPooling \n        h2_pool = tf.layers.max_pooling2d(h2, \n                              pool_size=(2, 2), \n                              strides=(2, 2))\n\n        ## 3rd layer: Fully Connected\n        input_shape = h2_pool.get_shape().as_list()\n        n_input_units = np.prod(input_shape[1:])\n        h2_pool_flat = tf.reshape(h2_pool, \n                              shape=[-1, n_input_units])\n        h3 = tf.layers.dense(h2_pool_flat, 1024, \n                              activation=tf.nn.relu)\n\n        ## Dropout\n        h3_drop = tf.layers.dropout(h3, \n                              rate=self.dropout_rate,\n                              training=is_train)\n        \n        ## 4th layer: Fully Connected (linear activation)\n        h4 = tf.layers.dense(h3_drop, 10, \n                              activation=None)\n\n        ## Prediction\n        predictions = {\n            \'probabilities\': tf.nn.softmax(h4, \n                              name=\'probabilities\'),\n            \'labels\': tf.cast(tf.argmax(h4, axis=1), \n                              tf.int32, name=\'labels\')}\n        \n        ## Loss Function and Optimization\n        cross_entropy_loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(\n                logits=h4, labels=tf_y_onehot),\n            name=\'cross_entropy_loss\')\n        \n        ## Optimizer\n        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n        optimizer = optimizer.minimize(cross_entropy_loss,\n                                       name=\'train_op\')\n\n        ## Finding accuracy\n        correct_predictions = tf.equal(\n            predictions[\'labels\'], \n            tf_y, name=\'correct_preds\')\n        \n        accuracy = tf.reduce_mean(\n            tf.cast(correct_predictions, tf.float32),\n            name=\'accuracy\')\n\n    def save(self, epoch, path=\'./tflayers-model/\'):\n        if not os.path.isdir(path):\n            os.makedirs(path)\n        print(\'Saving model in %s\' % path)\n        self.saver.save(self.sess, \n                        os.path.join(path, \'model.ckpt\'),\n                        global_step=epoch)\n        \n    def load(self, epoch, path):\n        print(\'Loading model from %s\' % path)\n        self.saver.restore(self.sess, \n             os.path.join(path, \'model.ckpt-%d\' % epoch))\n        \n    def train(self, training_set, \n              validation_set=None,\n              initialize=True):\n        ## initialize variables\n        if initialize:\n            self.sess.run(self.init_op)\n\n        self.train_cost_ = []\n        X_data = np.array(training_set[0])\n        y_data = np.array(training_set[1])\n\n        for epoch in range(1, self.epochs + 1):\n            batch_gen =                 batch_generator(X_data, y_data, \n                                 shuffle=self.shuffle)\n            avg_loss = 0.0\n            for i, (batch_x,batch_y) in                 enumerate(batch_gen):\n                feed = {\'tf_x:0\': batch_x, \n                        \'tf_y:0\': batch_y,\n                        \'is_train:0\': True} ## for dropout\n                loss, _ = self.sess.run(\n                        [\'cross_entropy_loss:0\', \'train_op\'], \n                        feed_dict=feed)\n                avg_loss += loss\n                \n            print(\'Epoch %02d: Training Avg. Loss: \'\n                  \'%7.3f\' % (epoch, avg_loss), end=\' \')\n            if validation_set is not None:\n                feed = {\'tf_x:0\': batch_x, \n                        \'tf_y:0\': batch_y,\n                        \'is_train:0\': False} ## for dropout\n                valid_acc = self.sess.run(\'accuracy:0\',\n                                          feed_dict=feed)\n                print(\'Validation Acc: %7.3f\' % valid_acc)\n            else:\n                print()\n                    \n    def predict(self, X_test, return_proba = False):\n        feed = {\'tf_x:0\': X_test,\n                \'is_train:0\': False} ## for dropout\n        if return_proba:\n            return self.sess.run(\'probabilities:0\',\n                                 feed_dict=feed)\n        else:\n            return self.sess.run(\'labels:0\',\n                                 feed_dict=feed)\n\n\n\n# In[33]:\n\n\ncnn = ConvNN(random_seed=123)\n\n\n# In[34]:\n\n\ncnn.train(training_set=(X_train_centered, y_train), \n          validation_set=(X_valid_centered, y_valid))\n\ncnn.save(epoch=20)\n\n\n# In[35]:\n\n\ndel cnn\n\ncnn2 = ConvNN(random_seed=123)\n\ncnn2.load(epoch=20, path=\'./tflayers-model/\')\n\nprint(cnn2.predict(X_test_centered[:10,:]))\n\n\n# In[36]:\n\n\npreds = cnn2.predict(X_test_centered)\n\nprint(\'Test Accuracy: %.2f%%\' % (100*\n      np.sum(y_test == preds)/len(y_test)))\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n# In[ ]:\n\n\nget_ipython().system(\' python ../.convert_notebook_to_script.py --input ch15.ipynb --output ch15.py\')\n\n'"
code/ch16/ch16.py,52,"b'# coding: utf-8\n\n\nimport gzip\nimport pyprind\nimport pandas as pd\nfrom string import punctuation\nimport re\nimport numpy as np\nimport os\nfrom collections import Counter\nimport tensorflow as tf\n\n# *Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com) and Vahid Mirjalili, Packt Publishing Ltd. 2017\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 16 - Modeling Sequential Data Using Recurrent Neural Networks\n# \n# \n\n# Note that the optional watermark extension is a small IPython notebook plugin that is being used to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*\n\n# - [Introducing sequential data](#Introducing-sequential-data)\n#   - [Modeling sequential data: Order matters](#Modeling-sequential-data:-Order-matters)\n#   - [Understanding the different categories of sequence modeling](#Understanding-the-different-categories-of-sequence-modeling)\n# - [Recurrent neural networks for modeling sequences](#Recurrent-neural-networks-for-modeling-sequences)\n#   - [Understanding the structure and flow of a recurrent neural network \n# ](#Understanding-the-structure-and-flow-of-a-recurrent-neural-network)\n#   - [Computing activations in an RNN](#Computing-activations-in-an-RNN)\n#   - [The challenges of learning long-range interactions](#The-challenges-of-learning-long-range-interactions)\n#   - [Long short-term memory units](#Long-short-term-memory-units)\n# - [Implementing a multilayer RNN for sequence modeling in TensorFlow](#Implementing-a-multilayer-RNN-for-sequence-modeling-in-TensorFlow)\n#   - [Performing sentiment analysis of IMDb movie reviews using multilayer RNNs](#Performing-sentiment-analysis-of-IMDb-movie-reviews-using-multilayer-RNNs)\n#     - [Preparing the data](#Preparing-the-data)\n#     - [Embedding](#Embedding)\n#     - [Building the RNN model](#Building-the-RNN-model)\n#       - [Step 1: Defining multilayer RNN cells](#Step-1:-Defining-multilayer-RNN-cells)\n#       - [Step 2: Defining the initial states for the RNN cells](#Step-2:-Defining-the-initial-states-for-the-RNN-cells)\n#       - [Step 3: Creating the recurrent neural network using the RNN cells and their states](#Step-3:-Creating-the-recurrent-neural-network-using-the-RNN-cells-and-their-states)\n#   - [Example application: character-level language modeling](#Example-application:-character-level-language-modeling)\n#     - [Preparing the data](#Preparing-the-data)\n#     - [Building the character-level RNN model](#Building-the-character-level-RNN-model)\n# - [Summary](#Summary)\n\n\n\n\n\n\n\n\n\nwith gzip.open(\'movie_data.csv.gz\') as f_in, open(\'movie_data.csv\', \'wb\') as f_out:\n    f_out.writelines(f_in)\n\n\n# # Introducing sequential data \n\n# ## Modeling sequential data: Order matters\n\n# ## Representing sequences\n\n\n\n\n\n# ## Understanding the different categories of sequence modeling\n\n\n\n\n\n# # Recurrent neural networks for modeling sequences\n\n# ## Understanding the structure and flow of a recurrent neural network \n\n\n\n\n\n\n\n\n\n# ## Computing activations in an RNN\n\n\n\n\n\n\n\n\n\n# ## The challenges of learning long-range interactions\n\n\n\n\n\n# ## Long short-term memory units\n\n\n\n\n\n# # Implementing a multilayer RNN for sequence modeling in TensorFlow\n\n# ## Performing sentiment analysis of IMDb movie reviews using multilayer RNNs\n\n# ### Preparing the data\n\n\n\n\n\n\n\n\n\ndf = pd.read_csv(\'movie_data.csv\', encoding=\'utf-8\')\nprint(df.head(3))\n\n\n\n\n## @Readers: PLEASE IGNORE THIS CELL\n##\n## This cell is meant to shrink the\n## dataset when this notebook is run \n## on the Travis Continuous Integration\n## platform to test the code as well as\n## speeding up the run using a smaller\n## dataset for debugging\n\n\n\nif \'TRAVIS\' in os.environ:\n    df = pd.read_csv(\'movie_data.csv\', encoding=\'utf-8\', nrows=500)\n\n\n\n\n## Preprocessing the data:\n## Separate words and \n## count each word\'s occurrence\n\n\n\n\ncounts = Counter()\npbar = pyprind.ProgBar(len(df[\'review\']),\n                       title=\'Counting words occurences\')\nfor i,review in enumerate(df[\'review\']):\n    text = \'\'.join([c if c not in punctuation else \' \'+c+\' \'                     for c in review]).lower()\n    df.loc[i,\'review\'] = text\n    pbar.update()\n    counts.update(text.split())\n\n\n\n\n## Create a mapping:\n## Map each unique word to an integer\n\nword_counts = sorted(counts, key=counts.get, reverse=True)\nprint(word_counts[:5])\nword_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n\n\nmapped_reviews = []\npbar = pyprind.ProgBar(len(df[\'review\']),\n                       title=\'Map reviews to ints\')\nfor review in df[\'review\']:\n    mapped_reviews.append([word_to_int[word] for word in review.split()])\n    pbar.update()\n\n\n\n\n## Define fixed-length sequences:\n## Use the last 200 elements of each sequence\n## if sequence length < 200: left-pad with zeros\n\nsequence_length = 200  ## sequence length (or T in our formulas)\nsequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\nfor i, row in enumerate(mapped_reviews):\n    review_arr = np.array(row)\n    sequences[i, -len(row):] = review_arr[-sequence_length:]\n\nX_train = sequences[:25000, :]\ny_train = df.loc[:25000, \'sentiment\'].values\nX_test = sequences[25000:, :]\ny_test = df.loc[25000:, \'sentiment\'].values\n\n\nnp.random.seed(123) # for reproducibility\n\n## Function to generate minibatches:\ndef create_batch_generator(x, y=None, batch_size=64):\n    n_batches = len(x)//batch_size\n    x= x[:n_batches*batch_size]\n    if y is not None:\n        y = y[:n_batches*batch_size]\n    for ii in range(0, len(x), batch_size):\n        if y is not None:\n            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n        else:\n            yield x[ii:ii+batch_size]\n\n\n\n\n## @Readers: PLEASE IGNORE THIS CELL\n##\n## This cell is meant to shrink the\n## dataset when this notebook is run \n## on the Travis Continuous Integration\n## platform to test the code as well as\n## speeding up the run using a smaller\n## dataset for debugging\n\nif \'TRAVIS\' in os.environ:\n    X_train = sequences[:250, :]\n    y_train = df.loc[:250, \'sentiment\'].values\n    X_test = sequences[250:500, :]\n    y_test = df.loc[250:500, \'sentiment\'].values\n\n\n# ### Embedding\n\n\n\n\n\n# ### Building the RNN model\n\n\n\n\n\nclass SentimentRNN(object):\n    def __init__(self, n_words, seq_len=200,\n                 lstm_size=256, num_layers=1, batch_size=64,\n                 learning_rate=0.0001, embed_size=200):\n        self.n_words = n_words\n        self.seq_len = seq_len\n        self.lstm_size = lstm_size   ## number of hidden units\n        self.num_layers = num_layers\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.embed_size = embed_size\n\n        self.g = tf.Graph()\n        with self.g.as_default():\n            tf.set_random_seed(123)\n            self.build()\n            self.saver = tf.train.Saver()\n            self.init_op = tf.global_variables_initializer()\n\n    def build(self):\n        ## Define the placeholders\n        tf_x = tf.placeholder(tf.int32,\n                    shape=(self.batch_size, self.seq_len),\n                    name=\'tf_x\')\n        tf_y = tf.placeholder(tf.float32,\n                    shape=(self.batch_size),\n                    name=\'tf_y\')\n        tf_keepprob = tf.placeholder(tf.float32,\n                    name=\'tf_keepprob\')\n        ## Create the embedding layer\n        embedding = tf.Variable(\n                    tf.random_uniform(\n                        (self.n_words, self.embed_size),\n                        minval=-1, maxval=1),\n                    name=\'embedding\')\n        embed_x = tf.nn.embedding_lookup(\n                    embedding, tf_x, \n                    name=\'embeded_x\')\n\n        ## Define LSTM cell and stack them together\n        cells = tf.contrib.rnn.MultiRNNCell(\n                [tf.contrib.rnn.DropoutWrapper(\n                   tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n                   output_keep_prob=tf_keepprob)\n                 for i in range(self.num_layers)])\n\n        ## Define the initial state:\n        self.initial_state = cells.zero_state(\n                 self.batch_size, tf.float32)\n        print(\'  << initial state >> \', self.initial_state)\n\n        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n                 cells, embed_x,\n                 initial_state=self.initial_state)\n        ## Note: lstm_outputs shape: \n        ##  [batch_size, max_time, cells.output_size]\n        print(\'\\n  << lstm_output   >> \', lstm_outputs)\n        print(\'\\n  << final state   >> \', self.final_state)\n\n        ## Apply a FC layer after on top of RNN output:\n        logits = tf.layers.dense(\n                 inputs=lstm_outputs[:, -1],\n                 units=1, activation=None,\n                 name=\'logits\')\n        \n        logits = tf.squeeze(logits, name=\'logits_squeezed\')\n        print (\'\\n  << logits        >> \', logits)\n        \n        y_proba = tf.nn.sigmoid(logits, name=\'probabilities\')\n        predictions = {\n            \'probabilities\': y_proba,\n            \'labels\' : tf.cast(tf.round(y_proba), tf.int32,\n                 name=\'labels\')\n        }\n        print(\'\\n  << predictions   >> \', predictions)\n\n        ## Define the cost function\n        cost = tf.reduce_mean(\n                 tf.nn.sigmoid_cross_entropy_with_logits(\n                 labels=tf_y, logits=logits),\n                 name=\'cost\')\n        \n        ## Define the optimizer\n        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n        train_op = optimizer.minimize(cost, name=\'train_op\')\n\n    def train(self, X_train, y_train, num_epochs):\n        with tf.Session(graph=self.g) as sess:\n            sess.run(self.init_op)\n            iteration = 1\n            for epoch in range(num_epochs):\n                state = sess.run(self.initial_state)\n                \n                for batch_x, batch_y in create_batch_generator(\n                            X_train, y_train, self.batch_size):\n                    feed = {\'tf_x:0\': batch_x,\n                            \'tf_y:0\': batch_y,\n                            \'tf_keepprob:0\': 0.5,\n                            self.initial_state : state}\n                    loss, _, state = sess.run(\n                            [\'cost:0\', \'train_op\', \n                             self.final_state],\n                            feed_dict=feed)\n\n                    if iteration % 20 == 0:\n                        print(""Epoch: %d/%d Iteration: %d ""\n                              ""| Train loss: %.5f"" % (\n                               epoch + 1, num_epochs,\n                               iteration, loss))\n\n                    iteration +=1\n                if (epoch+1)%10 == 0:\n                    self.saver.save(sess,\n                        ""model/sentiment-%d.ckpt"" % epoch)\n\n    def predict(self, X_data, return_proba=False):\n        preds = []\n        with tf.Session(graph = self.g) as sess:\n            self.saver.restore(\n                sess, tf.train.latest_checkpoint(\'model/\'))\n            test_state = sess.run(self.initial_state)\n            for ii, batch_x in enumerate(\n                create_batch_generator(\n                    X_data, None, batch_size=self.batch_size), 1):\n                feed = {\'tf_x:0\' : batch_x,\n                        \'tf_keepprob:0\': 1.0,\n                        self.initial_state : test_state}\n                if return_proba:\n                    pred, test_state = sess.run(\n                        [\'probabilities:0\', self.final_state],\n                        feed_dict=feed)\n                else:\n                    pred, test_state = sess.run(\n                        [\'labels:0\', self.final_state],\n                        feed_dict=feed)\n                    \n                preds.append(pred)\n                \n        return np.concatenate(preds)\n\n\n# #### Step 1: Defining multilayer RNN cells\n\n# #### Step 2: Defining the initial states for the RNN cells\n# \n\n# #### Step 3: Creating the recurrent neural network using the RNN cells and their states\n# \n# \n\n\n\n## Train:\n\nn_words = max(list(word_to_int.values())) + 1\n\nrnn = SentimentRNN(n_words=n_words, \n                   seq_len=sequence_length,\n                   embed_size=256, \n                   lstm_size=128, \n                   num_layers=1, \n                   batch_size=100, \n                   learning_rate=0.001)\n\n\n\n\nrnn.train(X_train, y_train, num_epochs=40)\n\n\n\n\n## Test: \npreds = rnn.predict(X_test)\ny_true = y_test[:len(preds)]\nprint(\'Test Acc.: %.3f\' % (\n      np.sum(preds == y_true) / len(y_true)))\n\n\n\n\n## Get probabilities:\nproba = rnn.predict(X_test, return_proba=True)\n\n\n# ## Example application: character-level language modeling\n\n\n\n\n\n# ### Preparing the data\n# \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Reading and processing text\nwith open(\'pg2265.txt\', \'r\', encoding=\'utf-8\') as f: \n    text=f.read()\n\ntext = text[15858:]\nchars = set(text)\nchar2int = {ch:i for i,ch in enumerate(chars)}\nint2char = dict(enumerate(chars))\ntext_ints = np.array([char2int[ch] for ch in text], \n                     dtype=np.int32)\n\n\n\n\n## @Readers: PLEASE IGNORE THIS CELL\n##\n## This cell is meant to shrink the\n## dataset when this notebook is run \n## on the Travis Continuous Integration\n## platform to test the code as well as\n## speeding up the run using a smaller\n## dataset for debugging\n\nif \'TRAVIS\' in os.environ:\n    text = text[:1000]\n    chars = set(text)\n    char2int = {ch:i for i,ch in enumerate(chars)}\n    int2char = dict(enumerate(chars))\n    text_ints = np.array([char2int[ch] for ch in text], \n                         dtype=np.int32)\n\n\n\n\ndef reshape_data(sequence, batch_size, num_steps):\n    mini_batch_length = batch_size * num_steps\n    num_batches = int(len(sequence) / mini_batch_length)\n    if num_batches*mini_batch_length + 1 > len(sequence):\n        num_batches = num_batches - 1\n    ## Truncate the sequence at the end to get rid of \n    ## remaining charcaters that do not make a full batch\n    x = sequence[0 : num_batches*mini_batch_length]\n    y = sequence[1 : num_batches*mini_batch_length + 1]\n    ## Split x & y into a list batches of sequences: \n    x_batch_splits = np.split(x, batch_size)\n    y_batch_splits = np.split(y, batch_size)\n    ## Stack the batches together\n    ## batch_size x mini_batch_length\n    x = np.stack(x_batch_splits)\n    y = np.stack(y_batch_splits)\n    \n    return x, y\n\n## Testing:\ntrain_x, train_y = reshape_data(text_ints, 64, 10)\nprint(train_x.shape)\nprint(train_x[0, :10])\nprint(train_y[0, :10])\nprint(\'\'.join(int2char[i] for i in train_x[0, :50]))\n\n\n\n\nnp.random.seed(123)\n\ndef create_batch_generator(data_x, data_y, num_steps):\n    batch_size, tot_batch_length = data_x.shape    \n    num_batches = int(tot_batch_length/num_steps)\n    for b in range(num_batches):\n        yield (data_x[:, b*num_steps: (b+1)*num_steps], \n               data_y[:, b*num_steps: (b+1)*num_steps])\n        \nbgen = create_batch_generator(train_x[:,:100], train_y[:,:100], 15)\nfor b in bgen:\n    print(b[0].shape, b[1].shape, end=\'  \')\n    print(\'\'.join(int2char[i] for i in b[0][0,:]).replace(\'\\n\', \'*\'), \'    \',\n          \'\'.join(int2char[i] for i in b[1][0,:]).replace(\'\\n\', \'*\'))\n\n\n# ### Building the character-level RNN model\n\n\n\n\nclass CharRNN(object):\n    def __init__(self, num_classes, batch_size=64, \n                 num_steps=100, lstm_size=128, \n                 num_layers=1, learning_rate=0.001, \n                 keep_prob=0.5, grad_clip=5, \n                 sampling=False):\n        self.num_classes = num_classes\n        self.batch_size = batch_size\n        self.num_steps = num_steps\n        self.lstm_size = lstm_size\n        self.num_layers = num_layers\n        self.learning_rate = learning_rate\n        self.keep_prob = keep_prob\n        self.grad_clip = grad_clip\n        \n        self.g = tf.Graph()\n        with self.g.as_default():\n            tf.set_random_seed(123)\n\n            self.build(sampling=sampling)\n            self.saver = tf.train.Saver()\n            self.init_op = tf.global_variables_initializer()\n            \n    def build(self, sampling):\n        if sampling == True:\n            batch_size, num_steps = 1, 1\n        else:\n            batch_size = self.batch_size\n            num_steps = self.num_steps\n\n        tf_x = tf.placeholder(tf.int32, \n                              shape=[batch_size, num_steps], \n                              name=\'tf_x\')\n        tf_y = tf.placeholder(tf.int32, \n                              shape=[batch_size, num_steps], \n                              name=\'tf_y\')\n        tf_keepprob = tf.placeholder(tf.float32, \n                              name=\'tf_keepprob\')\n\n        # One-hot encoding:\n        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n\n        ### Build the multi-layer RNN cells\n        cells = tf.contrib.rnn.MultiRNNCell(\n            [tf.contrib.rnn.DropoutWrapper(\n                tf.contrib.rnn.BasicLSTMCell(self.lstm_size), \n                output_keep_prob=tf_keepprob) \n            for _ in range(self.num_layers)])\n        \n        ## Define the initial state\n        self.initial_state = cells.zero_state(\n                    batch_size, tf.float32)\n\n        ## Run each sequence step through the RNN \n        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n                    cells, x_onehot, \n                    initial_state=self.initial_state)\n        \n        print(\'  << lstm_outputs  >>\', lstm_outputs)\n\n        seq_output_reshaped = tf.reshape(\n                    lstm_outputs, \n                    shape=[-1, self.lstm_size],\n                    name=\'seq_output_reshaped\')\n\n        logits = tf.layers.dense(\n                    inputs=seq_output_reshaped, \n                    units=self.num_classes,\n                    activation=None,\n                    name=\'logits\')\n\n        proba = tf.nn.softmax(\n                    logits, \n                    name=\'probabilities\')\n        print(proba)\n\n        y_reshaped = tf.reshape(\n                    y_onehot, \n                    shape=[-1, self.num_classes],\n                    name=\'y_reshaped\')\n        cost = tf.reduce_mean(\n                    tf.nn.softmax_cross_entropy_with_logits(\n                        logits=logits, \n                        labels=y_reshaped),\n                    name=\'cost\')\n\n        # Gradient clipping to avoid ""exploding gradients""\n        tvars = tf.trainable_variables()\n        grads, _ = tf.clip_by_global_norm(\n                    tf.gradients(cost, tvars), \n                    self.grad_clip)\n        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n        train_op = optimizer.apply_gradients(\n                    zip(grads, tvars),\n                    name=\'train_op\')\n        \n    def train(self, train_x, train_y, \n              num_epochs, ckpt_dir=\'./model/\'):\n        ## Create the checkpoint directory\n        ## if does not exists\n        if not os.path.exists(ckpt_dir):\n            os.mkdir(ckpt_dir)\n            \n        with tf.Session(graph=self.g) as sess:\n            sess.run(self.init_op)\n\n            n_batches = int(train_x.shape[1]/self.num_steps)\n            iterations = n_batches * num_epochs\n            for epoch in range(num_epochs):\n\n                # Train network\n                new_state = sess.run(self.initial_state)\n                loss = 0\n                ## Minibatch generator:\n                bgen = create_batch_generator(\n                        train_x, train_y, self.num_steps)\n                for b, (batch_x, batch_y) in enumerate(bgen, 1):\n                    iteration = epoch*n_batches + b\n                    \n                    feed = {\'tf_x:0\': batch_x,\n                            \'tf_y:0\': batch_y,\n                            \'tf_keepprob:0\': self.keep_prob,\n                            self.initial_state : new_state}\n                    batch_cost, _, new_state = sess.run(\n                            [\'cost:0\', \'train_op\', \n                                self.final_state],\n                            feed_dict=feed)\n                    if iteration % 10 == 0:\n                        print(\'Epoch %d/%d Iteration %d\'\n                              \'| Training loss: %.4f\' % (\n                              epoch + 1, num_epochs, \n                              iteration, batch_cost))\n\n                ## Save the trained model    \n                self.saver.save(\n                        sess, os.path.join(\n                            ckpt_dir, \'language_modeling.ckpt\'))\n                              \n                              \n                \n    def sample(self, output_length, \n               ckpt_dir, starter_seq=""The ""):\n        observed_seq = [ch for ch in starter_seq]        \n        with tf.Session(graph=self.g) as sess:\n            self.saver.restore(\n                sess, \n                tf.train.latest_checkpoint(ckpt_dir))\n            ## 1: run the model using the starter sequence\n            new_state = sess.run(self.initial_state)\n            for ch in starter_seq:\n                x = np.zeros((1, 1))\n                x[0,0] = char2int[ch]\n                feed = {\'tf_x:0\': x,\n                        \'tf_keepprob:0\': 1.0,\n                        self.initial_state: new_state}\n                proba, new_state = sess.run(\n                        [\'probabilities:0\', self.final_state], \n                        feed_dict=feed)\n\n            ch_id = get_top_char(proba, len(chars))\n            observed_seq.append(int2char[ch_id])\n            \n            ## 2: run the model using the updated observed_seq\n            for i in range(output_length):\n                x[0,0] = ch_id\n                feed = {\'tf_x:0\': x,\n                        \'tf_keepprob:0\': 1.0,\n                        self.initial_state: new_state}\n                proba, new_state = sess.run(\n                        [\'probabilities:0\', self.final_state], \n                        feed_dict=feed)\n\n                ch_id = get_top_char(proba, len(chars))\n                observed_seq.append(int2char[ch_id])\n\n        return \'\'.join(observed_seq)\n\n\n\n\ndef get_top_char(probas, char_size, top_n=5):\n    p = np.squeeze(probas)\n    p[np.argsort(p)[:-top_n]] = 0.0\n    p = p / np.sum(p)\n    ch_id = np.random.choice(char_size, 1, p=p)[0]\n    return ch_id\n\n\n\n\nbatch_size = 64\nnum_steps = 100 \ntrain_x, train_y = reshape_data(text_ints, \n                                batch_size, \n                                num_steps)\n\nrnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\nrnn.train(train_x, train_y, \n          num_epochs=100,\n          ckpt_dir=\'./model-100/\')\n\n\n\n\nnp.random.seed(123)\nrnn = CharRNN(len(chars), sampling=True)\n\nprint(rnn.sample(ckpt_dir=\'./model-100/\', \n                 output_length=500))\n\n\n\n\n## run for 200 epochs\nbatch_size = 64\nnum_steps = 100 \n\nrnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\nrnn.train(train_x, train_y, \n          num_epochs=200,\n          ckpt_dir=\'./model-200/\')\n\n\n\n\ndel rnn\n\nnp.random.seed(123)\nrnn = CharRNN(len(chars), sampling=True)\nprint(rnn.sample(ckpt_dir=\'./model-200/\', \n                 output_length=500))\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n\n\n\n\n\n'"
code/tests/__init__.py,0,"b'# Sebastian Raschka 2016-2017\n#\n# ann is a supporting package for the book\n# ""Introduction to Artificial Neural Networks and Deep Learning:\n#  A Practical Guide with Applications in Python""\n#\n# Author: Sebastian Raschka <sebastianraschka.com>\n#\n# License: MIT\n'"
code/tests/test_notebooks.py,0,"b'import unittest\nimport os\nimport subprocess\nimport sys\nimport logging\n\n\nLOG_FORMAT = \'[%(asctime)s %(levelname)s] %(message)s\'\nLOGGER = logging.getLogger(__file__)\n\n\ndef run_ipynb(path):\n\n    nb_dir = os.path.dirname(path)\n    nb_path = os.path.basename(path)\n    orig_dir = os.getcwd()\n    os.chdir(nb_dir)\n\n    if (sys.version_info >= (3, 0)):\n        kernel_name = \'python3\'\n    else:\n        kernel_name = \'python2\'\n    #  error_cells = []\n\n    args = [""jupyter"", ""nbconvert"",\n            ""--execute"", ""--inplace"",\n            ""--debug"",\n            ""--ExecutePreprocessor.timeout=5000"",\n            ""--ExecutePreprocessor.kernel_name=%s"" % kernel_name,\n            nb_path]\n\n    if (sys.version_info >= (3, 0)):\n        try:\n            subprocess.check_output(args)\n        except TimeoutError:\n            sys.stderr.write(\'%s timed out\\n\' % path)\n            sys.stderr.flush()\n\n    else:\n        subprocess.check_output(args)\n\n    os.chdir(orig_dir)\n\n\nclass TestNotebooks(unittest.TestCase):\n\n    def test_ch01(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../ch01/ch01.ipynb\'))\n\n    def test_ch02(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../ch02/ch02.ipynb\'))\n\n    def test_ch03(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../ch03/ch03.ipynb\'))\n\n    def test_ch04(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../ch04/ch04.ipynb\'))\n\n    def test_ch05(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../ch05/ch05.ipynb\'))\n\n    def test_ch06(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../ch06/ch06.ipynb\'))\n\n    def test_ch07(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../ch07/ch07.ipynb\'))\n\n    def test_ch08(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n\n        # run only on Py3, because of Py2 unicode handling\n        if (sys.version_info >= (3, 0)):\n            run_ipynb(os.path.join(this_dir,\n                                   \'../ch08/ch08.ipynb\'))\n\n    def test_ch09(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n\n        # run only on Py3, because of the Py3 specific pickle files\n        if (sys.version_info >= (3, 0)):\n            run_ipynb(os.path.join(this_dir, \'../ch09/ch09.ipynb\'))\n        else:\n            pass\n\n    def test_ch10(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../ch10/ch10.ipynb\'))\n\n    def test_ch11(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../ch11/ch11.ipynb\'))\n\n    def test_ch12(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../ch12/ch12.ipynb\'))\n\n    def test_ch13(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../ch13/ch13.ipynb\'))\n\n    def test_ch14(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../ch14/ch14.ipynb\'))\n\n    def test_ch15(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        run_ipynb(os.path.join(this_dir,\n                               \'../ch15/ch15.ipynb\'))\n\n    def test_ch16(self):\n        this_dir = os.path.dirname(os.path.abspath(__file__))\n        # run only on Py3, because of Py2 unicode handling\n        if (sys.version_info >= (3, 0)):\n            run_ipynb(os.path.join(this_dir,\n                                   \'../ch16/ch16.ipynb\'))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
code/ch09/1st_flask_app_1/app.py,0,"b""from flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return render_template('first_app.html')\n\nif __name__ == '__main__':\n    app.run(debug=True)"""
code/ch09/1st_flask_app_2/app.py,0,"b""from flask import Flask, render_template, request\nfrom wtforms import Form, TextAreaField, validators\n\napp = Flask(__name__)\n\nclass HelloForm(Form):\n    sayhello = TextAreaField('',[validators.DataRequired()])\n\n@app.route('/')\ndef index():\n    form = HelloForm(request.form)\n    return render_template('first_app.html', form=form)\n\n@app.route('/hello', methods=['POST'])\ndef hello():\n    form = HelloForm(request.form)\n    if request.method == 'POST' and form.validate():\n        name = request.form['sayhello']\n        return render_template('hello.html', name=name)\n    return render_template('first_app.html', form=form)\n\nif __name__ == '__main__':\n    app.run(debug=True)"""
code/ch09/movieclassifier/app.py,0,"b'from flask import Flask, render_template, request\nfrom wtforms import Form, TextAreaField, validators\nimport pickle\nimport sqlite3\nimport os\nimport numpy as np\n\n# import HashingVectorizer from local dir\nfrom vectorizer import vect\n\napp = Flask(__name__)\n\n######## Preparing the Classifier\ncur_dir = os.path.dirname(__file__)\nclf = pickle.load(open(os.path.join(cur_dir,\n                 \'pkl_objects\',\n                 \'classifier.pkl\'), \'rb\'))\ndb = os.path.join(cur_dir, \'reviews.sqlite\')\n\ndef classify(document):\n    label = {0: \'negative\', 1: \'positive\'}\n    X = vect.transform([document])\n    y = clf.predict(X)[0]\n    proba = np.max(clf.predict_proba(X))\n    return label[y], proba\n\ndef train(document, y):\n    X = vect.transform([document])\n    clf.partial_fit(X, [y])\n\ndef sqlite_entry(path, document, y):\n    conn = sqlite3.connect(path)\n    c = conn.cursor()\n    c.execute(""INSERT INTO review_db (review, sentiment, date)""\\\n    "" VALUES (?, ?, DATETIME(\'now\'))"", (document, y))\n    conn.commit()\n    conn.close()\n\n######## Flask\nclass ReviewForm(Form):\n    moviereview = TextAreaField(\'\',\n                                [validators.DataRequired(),\n                                validators.length(min=15)])\n\n@app.route(\'/\')\ndef index():\n    form = ReviewForm(request.form)\n    return render_template(\'reviewform.html\', form=form)\n\n@app.route(\'/results\', methods=[\'POST\'])\ndef results():\n    form = ReviewForm(request.form)\n    if request.method == \'POST\' and form.validate():\n        review = request.form[\'moviereview\']\n        y, proba = classify(review)\n        return render_template(\'results.html\',\n                                content=review,\n                                prediction=y,\n                                probability=round(proba*100, 2))\n    return render_template(\'reviewform.html\', form=form)\n\n@app.route(\'/thanks\', methods=[\'POST\'])\ndef feedback():\n    feedback = request.form[\'feedback_button\']\n    review = request.form[\'review\']\n    prediction = request.form[\'prediction\']\n\n    inv_label = {\'negative\': 0, \'positive\': 1}\n    y = inv_label[prediction]\n    if feedback == \'Incorrect\':\n        y = int(not(y))\n    train(review, y)\n    sqlite_entry(db, review, y)\n    return render_template(\'thanks.html\')\n\nif __name__ == \'__main__\':\n    app.run(debug=True)\n'"
code/ch09/movieclassifier/vectorizer.py,0,"b""from sklearn.feature_extraction.text import HashingVectorizer\nimport re\nimport os\nimport pickle\n\ncur_dir = os.path.dirname(__file__)\nstop = pickle.load(open(\n                os.path.join(cur_dir, \n                'pkl_objects', \n                'stopwords.pkl'), 'rb'))\n\ndef tokenizer(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n                           text.lower())\n    text = re.sub('[\\W]+', ' ', text.lower()) \\\n                   + ' '.join(emoticons).replace('-', '')\n    tokenized = [w for w in text.split() if w not in stop]\n    return tokenized\n\nvect = HashingVectorizer(decode_error='ignore',\n                         n_features=2**21,\n                         preprocessor=None,\n                         tokenizer=tokenizer)"""
code/ch09/movieclassifier_with_update/app.py,0,"b'from flask import Flask, render_template, request\nfrom wtforms import Form, TextAreaField, validators\nimport pickle\nimport sqlite3\nimport os\nimport numpy as np\n\n# import HashingVectorizer from local dir\nfrom vectorizer import vect\n\napp = Flask(__name__)\n\n######## Preparing the Classifier\ncur_dir = os.path.dirname(__file__)\nclf = pickle.load(open(os.path.join(cur_dir,\n                 \'pkl_objects\',\n                 \'classifier.pkl\'), \'rb\'))\ndb = os.path.join(cur_dir, \'reviews.sqlite\')\n\ndef classify(document):\n    label = {0: \'negative\', 1: \'positive\'}\n    X = vect.transform([document])\n    y = clf.predict(X)[0]\n    proba = np.max(clf.predict_proba(X))\n    return label[y], proba\n\ndef train(document, y):\n    X = vect.transform([document])\n    clf.partial_fit(X, [y])\n\ndef sqlite_entry(path, document, y):\n    conn = sqlite3.connect(path)\n    c = conn.cursor()\n    c.execute(""INSERT INTO review_db (review, sentiment, date)""\\\n    "" VALUES (?, ?, DATETIME(\'now\'))"", (document, y))\n    conn.commit()\n    conn.close()\n\n######## Flask\nclass ReviewForm(Form):\n    moviereview = TextAreaField(\'\',\n                                [validators.DataRequired(),\n                                validators.length(min=15)])\n\n@app.route(\'/\')\ndef index():\n    form = ReviewForm(request.form)\n    return render_template(\'reviewform.html\', form=form)\n\n@app.route(\'/results\', methods=[\'POST\'])\ndef results():\n    form = ReviewForm(request.form)\n    if request.method == \'POST\' and form.validate():\n        review = request.form[\'moviereview\']\n        y, proba = classify(review)\n        return render_template(\'results.html\',\n                                content=review,\n                                prediction=y,\n                                probability=round(proba*100, 2))\n    return render_template(\'reviewform.html\', form=form)\n\n@app.route(\'/thanks\', methods=[\'POST\'])\ndef feedback():\n    feedback = request.form[\'feedback_button\']\n    review = request.form[\'review\']\n    prediction = request.form[\'prediction\']\n\n    inv_label = {\'negative\': 0, \'positive\': 1}\n    y = inv_label[prediction]\n    if feedback == \'Incorrect\':\n        y = int(not(y))\n    train(review, y)\n    sqlite_entry(db, review, y)\n    return render_template(\'thanks.html\')\n\nif __name__ == \'__main__\':\n    app.run(debug=True)\n'"
code/ch09/movieclassifier_with_update/update.py,0,"b""import pickle\nimport sqlite3\nimport numpy as np\nimport os\n\n# import HashingVectorizer from local dir\nfrom vectorizer import vect\n\n\ndef update_model(db_path, model, batch_size=10000):\n\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n    c.execute('SELECT * from review_db')\n\n    results = c.fetchmany(batch_size)\n    while results:\n        data = np.array(results)\n        X = data[:, 0]\n        y = data[:, 1].astype(int)\n\n        classes = np.array([0, 1])\n        X_train = vect.transform(X)\n        model.partial_fit(X_train, y, classes=classes)\n        results = c.fetchmany(batch_size)\n\n    conn.close()\n    return model\n\ncur_dir = os.path.dirname(__file__)\n\nclf = pickle.load(open(os.path.join(cur_dir,\n                  'pkl_objects',\n                  'classifier.pkl'), 'rb'))\ndb = os.path.join(cur_dir, 'reviews.sqlite')\n\nclf = update_model(db_path=db, model=clf, batch_size=10000)\n\n# Uncomment the following lines if you are sure that\n# you want to update your classifier.pkl file\n# permanently.\n\n# pickle.dump(clf, open(os.path.join(cur_dir,\n#             'pkl_objects', 'classifier.pkl'), 'wb')\n#             , protocol=4)\n"""
code/ch09/movieclassifier_with_update/vectorizer.py,0,"b""from sklearn.feature_extraction.text import HashingVectorizer\nimport re\nimport os\nimport pickle\n\ncur_dir = os.path.dirname(__file__)\nstop = pickle.load(open(\n                os.path.join(cur_dir,\n                'pkl_objects',\n                'stopwords.pkl'), 'rb'))\n\ndef tokenizer(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n                           text.lower())\n    text = re.sub('[\\W]+', ' ', text.lower()) \\\n                   + ' '.join(emoticons).replace('-', '')\n    tokenized = [w for w in text.split() if w not in stop]\n    return tokenized\n\nvect = HashingVectorizer(decode_error='ignore',\n                         n_features=2**21,\n                         preprocessor=None,\n                         tokenizer=tokenizer)\n"""
code/ch09/pickle-test-scripts/pickle-dump-test.py,0,"b""import pickle\nimport os\nimport re\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.linear_model import SGDClassifier\n\n\nstop = stopwords.words('english')\n\n\ndef tokenizer(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n        ' '.join(emoticons).replace('-', '')\n    tokenized = [w for w in text.split() if w not in stop]\n    return tokenized\n\nvect = HashingVectorizer(decode_error='ignore',\n                         n_features=2**21,\n                         preprocessor=None,\n                         tokenizer=tokenizer)\n\nclf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n\n\ndf = pd.read_csv('./movie_data_small.csv', encoding='utf-8')\n\n#df.loc[:100, :].to_csv('./movie_data_small.csv', index=None)\n\n\nX_train = df['review'].values\ny_train = df['sentiment'].values\n\nX_train = vect.transform(X_train)\nclf.fit(X_train, y_train)\n\npickle.dump(stop,\n            open('stopwords.pkl', 'wb'),\n            protocol=4)\n\npickle.dump(clf,\n            open('classifier.pkl', 'wb'),\n            protocol=4)\n"""
code/ch09/pickle-test-scripts/pickle-load-test.py,0,"b""import pickle\nimport re\nimport os\nfrom vectorizer import vect\nimport numpy as np\n\nclf = pickle.load(open('classifier.pkl', 'rb'))\n\n\nlabel = {0: 'negative', 1: 'positive'}\nexample = ['I love this movie']\n\nX = vect.transform(example)\n\nprint('Prediction: %s\\nProbability: %.2f%%' %\n      (label[clf.predict(X)[0]],\n       np.max(clf.predict_proba(X)) * 100))\n"""
code/ch09/pickle-test-scripts/vectorizer.py,0,"b""from sklearn.feature_extraction.text import HashingVectorizer\nimport re\nimport os\nimport pickle\n\n\nstop = pickle.load(open('stopwords.pkl', 'rb'))\n\n\ndef tokenizer(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n                           text.lower())\n    text = re.sub('[\\W]+', ' ', text.lower()) + \\\n        ' '.join(emoticons).replace('-', '')\n    tokenized = [w for w in text.split() if w not in stop]\n    return tokenized\n\nvect = HashingVectorizer(decode_error='ignore',\n                         n_features=2**21,\n                         preprocessor=None,\n                         tokenizer=tokenizer)\n"""
