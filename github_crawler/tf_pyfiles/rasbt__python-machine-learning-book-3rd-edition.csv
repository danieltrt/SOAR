file_path,api_count,code
.convert_notebook_to_script.py,0,"b""# Simple helper script to convert\n# a Jupyter notebook to Python\n#\n# Sebastian Raschka, 2017\n\n\nimport argparse\nimport os\nimport subprocess\n\n\ndef convert(input_path, output_path):\n    subprocess.call(['jupyter', 'nbconvert', '--to', 'script',\n                     input_path, '--output', output_path])\n\n\ndef cleanup(path):\n\n    skip_lines_startwith = ('Image(filename=',\n                            '# In[',\n                            '# <hr>',\n                            'from IPython.display import Image',\n                            'get_ipython()',\n                            '# <br>')\n\n    clean_content = []\n    imports = []\n    existing_imports = set()\n    with open(path, 'r') as f:\n        next(f)\n        next(f)\n        for line in f:\n            line = line.rstrip(' ')\n            if line.startswith(skip_lines_startwith):\n                continue\n            if line.startswith('import ') or (\n                    'from ' in line and 'import ' in line):\n                if 'from __future__ import print_function' in line:\n                    if line != imports[0]:\n                        imports.insert(0, line)\n                else:\n                    if line.strip() not in existing_imports:\n                        imports.append(line)\n                        existing_imports.add(line.strip())\n            else:\n                clean_content.append(line)\n\n    clean_content = ['# coding: utf-8\\n\\n\\n'] + imports + clean_content\n\n    with open(path, 'w') as f:\n        for line in clean_content:\n            f.write(line)\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser(\n            description='Convert Jupyter notebook to Python script.',\n            formatter_class=argparse.RawTextHelpFormatter)\n\n    parser.add_argument('-i', '--input',\n                        required=True,\n                        help='Path to the Jupyter Notebook file')\n\n    parser.add_argument('-o', '--output',\n                        required=True,\n                        help='Path to the Python script file')\n\n    parser.add_argument('-v', '--version',\n                        action='version',\n                        version='v. 0.1')\n\n    args = parser.parse_args()\n\n    convert(input_path=args.input,\n            output_path=os.path.splitext(args.output)[0])\n\n    cleanup(args.output)\n"""
ch02/ch02.py,0,"b'# coding: utf-8\n\n\nimport numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# Copyright (c) 2019 [Sebastian Raschka](sebastianraschka.com)\n# \n# https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 2 - Training Machine Learning Algorithms for Classification\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this Jupyter extension via*  \n# \n#     conda install watermark -c conda-forge  \n# \n# or  \n# \n#     pip install watermark   \n# \n# *For more information, please see: https://github.com/rasbt/watermark.*\n\n# ### Overview\n# \n\n# - [Artificial neurons \xe2\x80\x93 a brief glimpse into the early history of machine learning](#Artificial-neurons-a-brief-glimpse-into-the-early-history-of-machine-learning)\n#     - [The formal definition of an artificial neuron](#The-formal-definition-of-an-artificial-neuron)\n#     - [The perceptron learning rule](#The-perceptron-learning-rule)\n# - [Implementing a perceptron learning algorithm in Python](#Implementing-a-perceptron-learning-algorithm-in-Python)\n#     - [An object-oriented perceptron API](#An-object-oriented-perceptron-API)\n#     - [Training a perceptron model on the Iris dataset](#Training-a-perceptron-model-on-the-Iris-dataset)\n# - [Adaptive linear neurons and the convergence of learning](#Adaptive-linear-neurons-and-the-convergence-of-learning)\n#     - [Minimizing cost functions with gradient descent](#Minimizing-cost-functions-with-gradient-descent)\n#     - [Implementing an Adaptive Linear Neuron in Python](#Implementing-an-Adaptive-Linear-Neuron-in-Python)\n#     - [Improving gradient descent through feature scaling](#Improving-gradient-descent-through-feature-scaling)\n#     - [Large scale machine learning and stochastic gradient descent](#Large-scale-machine-learning-and-stochastic-gradient-descent)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Artificial neurons - a brief glimpse into the early history of machine learning\n\n\n\n\n\n# ## The formal definition of an artificial neuron\n\n\n\n\n\n# ## The perceptron learning rule\n\n\n\n\n\n\n\n\n\n\n# # Implementing a perceptron learning algorithm in Python\n\n# ## An object-oriented perceptron API\n\n\n\n\n\nclass Perceptron(object):\n    """"""Perceptron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n    n_iter : int\n      Passes over the training dataset.\n    random_state : int\n      Random number generator seed for random weight\n      initialization.\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n    errors_ : list\n      Number of misclassifications (updates) in each epoch.\n\n    """"""\n    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        """"""Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_examples, n_features]\n          Training vectors, where n_examples is the number of examples and\n          n_features is the number of features.\n        y : array-like, shape = [n_examples]\n          Target values.\n\n        Returns\n        -------\n        self : object\n\n        """"""\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n        self.errors_ = []\n\n        for _ in range(self.n_iter):\n            errors = 0\n            for xi, target in zip(X, y):\n                update = self.eta * (target - self.predict(xi))\n                self.w_[1:] += update * xi\n                self.w_[0] += update\n                errors += int(update != 0.0)\n            self.errors_.append(errors)\n        return self\n\n    def net_input(self, X):\n        """"""Calculate net input""""""\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def predict(self, X):\n        """"""Return class label after unit step""""""\n        return np.where(self.net_input(X) >= 0.0, 1, -1)\n\n\n\n\nv1 = np.array([1, 2, 3])\nv2 = 0.5 * v1\nnp.arccos(v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n\n\n\n# ## Training a perceptron model on the Iris dataset\n\n# ...\n\n# ### Reading-in the Iris data\n\n\n\n\n\ns = os.path.join(\'https://archive.ics.uci.edu\', \'ml\',\n                 \'machine-learning-databases\', \'iris\',\'iris.data\')\nprint(\'URL:\', s)\n\ndf = pd.read_csv(s,\n                 header=None,\n                 encoding=\'utf-8\')\n\ndf.tail()\n\n\n# \n# ### Note:\n# \n# \n# You can find a copy of the Iris dataset (and all other datasets used in this book) in the code bundle of this book, which you can use if you are working offline or the UCI server at https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data is temporarily unavailable. For instance, to load the Iris dataset from a local directory, you can replace the line \n# \n#     df = pd.read_csv(\'https://archive.ics.uci.edu/ml/\'\n#         \'machine-learning-databases/iris/iris.data\', header=None)\n#  \n# by\n#  \n#     df = pd.read_csv(\'your/local/path/to/iris.data\', header=None)\n# \n\n\n\ndf = pd.read_csv(\'iris.data\', header=None, encoding=\'utf-8\')\ndf.tail()\n\n\n\n\n# ### Plotting the Iris data\n\n\n\n\n# select setosa and versicolor\ny = df.iloc[0:100, 4].values\ny = np.where(y == \'Iris-setosa\', -1, 1)\n\n# extract sepal length and petal length\nX = df.iloc[0:100, [0, 2]].values\n\n# plot data\nplt.scatter(X[:50, 0], X[:50, 1],\n            color=\'red\', marker=\'o\', label=\'setosa\')\nplt.scatter(X[50:100, 0], X[50:100, 1],\n            color=\'blue\', marker=\'x\', label=\'versicolor\')\n\nplt.xlabel(\'sepal length [cm]\')\nplt.ylabel(\'petal length [cm]\')\nplt.legend(loc=\'upper left\')\n\n# plt.savefig(\'images/02_06.png\', dpi=300)\nplt.show()\n\n\n\n# ### Training the perceptron model\n\n\n\nppn = Perceptron(eta=0.1, n_iter=10)\n\nppn.fit(X, y)\n\nplt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=\'o\')\nplt.xlabel(\'Epochs\')\nplt.ylabel(\'Number of updates\')\n\n# plt.savefig(\'images/02_07.png\', dpi=300)\nplt.show()\n\n\n\n# ### A function for plotting decision regions\n\n\n\n\n\ndef plot_decision_regions(X, y, classifier, resolution=0.02):\n\n    # setup marker generator and color map\n    markers = (\'s\', \'x\', \'o\', \'^\', \'v\')\n    colors = (\'red\', \'blue\', \'lightgreen\', \'gray\', \'cyan\')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    # plot class examples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], \n                    y=X[y == cl, 1],\n                    alpha=0.8, \n                    c=colors[idx],\n                    marker=markers[idx], \n                    label=cl, \n                    edgecolor=\'black\')\n\n\n\n\nplot_decision_regions(X, y, classifier=ppn)\nplt.xlabel(\'sepal length [cm]\')\nplt.ylabel(\'petal length [cm]\')\nplt.legend(loc=\'upper left\')\n\n\n# plt.savefig(\'images/02_08.png\', dpi=300)\nplt.show()\n\n\n\n# # Adaptive linear neurons and the convergence of learning\n\n# ...\n\n# ## Minimizing cost functions with gradient descent\n\n\n\n\n\n\n\n\n\n\n# ## Implementing an adaptive linear neuron in Python\n\n\n\nclass AdalineGD(object):\n    """"""ADAptive LInear NEuron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n    n_iter : int\n      Passes over the training dataset.\n    random_state : int\n      Random number generator seed for random weight\n      initialization.\n\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n    cost_ : list\n      Sum-of-squares cost function value in each epoch.\n\n    """"""\n    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        """""" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_examples, n_features]\n          Training vectors, where n_examples is the number of examples and\n          n_features is the number of features.\n        y : array-like, shape = [n_examples]\n          Target values.\n\n        Returns\n        -------\n        self : object\n\n        """"""\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n        self.cost_ = []\n\n        for i in range(self.n_iter):\n            net_input = self.net_input(X)\n            # Please note that the ""activation"" method has no effect\n            # in the code since it is simply an identity function. We\n            # could write `output = self.net_input(X)` directly instead.\n            # The purpose of the activation is more conceptual, i.e.,  \n            # in the case of logistic regression (as we will see later), \n            # we could change it to\n            # a sigmoid function to implement a logistic regression classifier.\n            output = self.activation(net_input)\n            errors = (y - output)\n            self.w_[1:] += self.eta * X.T.dot(errors)\n            self.w_[0] += self.eta * errors.sum()\n            cost = (errors**2).sum() / 2.0\n            self.cost_.append(cost)\n        return self\n\n    def net_input(self, X):\n        """"""Calculate net input""""""\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def activation(self, X):\n        """"""Compute linear activation""""""\n        return X\n\n    def predict(self, X):\n        """"""Return class label after unit step""""""\n        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n\nada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)\nax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker=\'o\')\nax[0].set_xlabel(\'Epochs\')\nax[0].set_ylabel(\'log(Sum-squared-error)\')\nax[0].set_title(\'Adaline - Learning rate 0.01\')\n\nada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)\nax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker=\'o\')\nax[1].set_xlabel(\'Epochs\')\nax[1].set_ylabel(\'Sum-squared-error\')\nax[1].set_title(\'Adaline - Learning rate 0.0001\')\n\n# plt.savefig(\'images/02_11.png\', dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n# ## Improving gradient descent through feature scaling\n\n\n\n\n\n\n\n# standardize features\nX_std = np.copy(X)\nX_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()\nX_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()\n\n\n\n\nada_gd = AdalineGD(n_iter=15, eta=0.01)\nada_gd.fit(X_std, y)\n\nplot_decision_regions(X_std, y, classifier=ada_gd)\nplt.title(\'Adaline - Gradient Descent\')\nplt.xlabel(\'sepal length [standardized]\')\nplt.ylabel(\'petal length [standardized]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n# plt.savefig(\'images/02_14_1.png\', dpi=300)\nplt.show()\n\nplt.plot(range(1, len(ada_gd.cost_) + 1), ada_gd.cost_, marker=\'o\')\nplt.xlabel(\'Epochs\')\nplt.ylabel(\'Sum-squared-error\')\n\nplt.tight_layout()\n# plt.savefig(\'images/02_14_2.png\', dpi=300)\nplt.show()\n\n\n\n# ## Large scale machine learning and stochastic gradient descent\n\n\n\nclass AdalineSGD(object):\n    """"""ADAptive LInear NEuron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n    n_iter : int\n      Passes over the training dataset.\n    shuffle : bool (default: True)\n      Shuffles training data every epoch if True to prevent cycles.\n    random_state : int\n      Random number generator seed for random weight\n      initialization.\n\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n    cost_ : list\n      Sum-of-squares cost function value averaged over all\n      training examples in each epoch.\n\n        \n    """"""\n    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.w_initialized = False\n        self.shuffle = shuffle\n        self.random_state = random_state\n        \n    def fit(self, X, y):\n        """""" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_examples, n_features]\n          Training vectors, where n_examples is the number of examples and\n          n_features is the number of features.\n        y : array-like, shape = [n_examples]\n          Target values.\n\n        Returns\n        -------\n        self : object\n\n        """"""\n        self._initialize_weights(X.shape[1])\n        self.cost_ = []\n        for i in range(self.n_iter):\n            if self.shuffle:\n                X, y = self._shuffle(X, y)\n            cost = []\n            for xi, target in zip(X, y):\n                cost.append(self._update_weights(xi, target))\n            avg_cost = sum(cost) / len(y)\n            self.cost_.append(avg_cost)\n        return self\n\n    def partial_fit(self, X, y):\n        """"""Fit training data without reinitializing the weights""""""\n        if not self.w_initialized:\n            self._initialize_weights(X.shape[1])\n        if y.ravel().shape[0] > 1:\n            for xi, target in zip(X, y):\n                self._update_weights(xi, target)\n        else:\n            self._update_weights(X, y)\n        return self\n\n    def _shuffle(self, X, y):\n        """"""Shuffle training data""""""\n        r = self.rgen.permutation(len(y))\n        return X[r], y[r]\n    \n    def _initialize_weights(self, m):\n        """"""Initialize weights to small random numbers""""""\n        self.rgen = np.random.RandomState(self.random_state)\n        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=1 + m)\n        self.w_initialized = True\n        \n    def _update_weights(self, xi, target):\n        """"""Apply Adaline learning rule to update the weights""""""\n        output = self.activation(self.net_input(xi))\n        error = (target - output)\n        self.w_[1:] += self.eta * xi.dot(error)\n        self.w_[0] += self.eta * error\n        cost = 0.5 * error**2\n        return cost\n    \n    def net_input(self, X):\n        """"""Calculate net input""""""\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def activation(self, X):\n        """"""Compute linear activation""""""\n        return X\n\n    def predict(self, X):\n        """"""Return class label after unit step""""""\n        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)\n\n\n\n\nada_sgd = AdalineSGD(n_iter=15, eta=0.01, random_state=1)\nada_sgd.fit(X_std, y)\n\nplot_decision_regions(X_std, y, classifier=ada_sgd)\nplt.title(\'Adaline - Stochastic Gradient Descent\')\nplt.xlabel(\'sepal length [standardized]\')\nplt.ylabel(\'petal length [standardized]\')\nplt.legend(loc=\'upper left\')\n\nplt.tight_layout()\n# plt.savefig(\'images/02_15_1.png\', dpi=300)\nplt.show()\n\nplt.plot(range(1, len(ada_sgd.cost_) + 1), ada_sgd.cost_, marker=\'o\')\nplt.xlabel(\'Epochs\')\nplt.ylabel(\'Average Cost\')\n\nplt.tight_layout()\n# plt.savefig(\'images/02_15_2.png\', dpi=300)\nplt.show()\n\n\n\n\nada_sgd.partial_fit(X_std[0, :], y[0])\n\n\n\n# # Summary\n\n# ...\n\n# --- \n# \n# Readers may ignore the following cell\n\n\n\n\n'"
ch03/ch03.py,0,"b'# coding: utf-8\n\n\nfrom sklearn import datasets\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom pydotplus import graph_from_dot_data\nfrom sklearn.tree import export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 3 - A Tour of Machine Learning Classifiers Using Scikit-Learn\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this Jupyter extension via*  \n# \n#     conda install watermark -c conda-forge  \n# \n# or  \n# \n#     pip install watermark   \n# \n# *For more information, please see: https://github.com/rasbt/watermark.*\n\n# ### Overview\n\n# - [Choosing a classification algorithm](#Choosing-a-classification-algorithm)\n# - [First steps with scikit-learn](#First-steps-with-scikit-learn)\n#     - [Training a perceptron via scikit-learn](#Training-a-perceptron-via-scikit-learn)\n# - [Modeling class probabilities via logistic regression](#Modeling-class-probabilities-via-logistic-regression)\n#     - [Logistic regression intuition and conditional probabilities](#Logistic-regression-intuition-and-conditional-probabilities)\n#     - [Learning the weights of the logistic cost function](#Learning-the-weights-of-the-logistic-cost-function)\n#     - [Training a logistic regression model with scikit-learn](#Training-a-logistic-regression-model-with-scikit-learn)\n#     - [Tackling overfitting via regularization](#Tackling-overfitting-via-regularization)\n# - [Maximum margin classification with support vector machines](#Maximum-margin-classification-with-support-vector-machines)\n#     - [Maximum margin intuition](#Maximum-margin-intuition)\n#     - [Dealing with the nonlinearly separable case using slack variables](#Dealing-with-the-nonlinearly-separable-case-using-slack-variables)\n#     - [Alternative implementations in scikit-learn](#Alternative-implementations-in-scikit-learn)\n# - [Solving nonlinear problems using a kernel SVM](#Solving-nonlinear-problems-using-a-kernel-SVM)\n#     - [Using the kernel trick to find separating hyperplanes in higher dimensional space](#Using-the-kernel-trick-to-find-separating-hyperplanes-in-higher-dimensional-space)\n# - [Decision tree learning](#Decision-tree-learning)\n#     - [Maximizing information gain \xe2\x80\x93 getting the most bang for the buck](#Maximizing-information-gain-\xe2\x80\x93-getting-the-most-bang-for-the-buck)\n#     - [Building a decision tree](#Building-a-decision-tree)\n#     - [Combining weak to strong learners via random forests](#Combining-weak-to-strong-learners-via-random-forests)\n# - [K-nearest neighbors \xe2\x80\x93 a lazy learning algorithm](#K-nearest-neighbors-\xe2\x80\x93-a-lazy-learning-algorithm)\n# - [Summary](#Summary)\n\n\n\n\n\n\n\n# # Choosing a classification algorithm\n\n# ...\n\n# # First steps with scikit-learn\n\n# Loading the Iris dataset from scikit-learn. Here, the third column represents the petal length, and the fourth column the petal width of the flower examples. The classes are already converted to integer labels where 0=Iris-Setosa, 1=Iris-Versicolor, 2=Iris-Virginica.\n\n\n\n\niris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\n\nprint(\'Class labels:\', np.unique(y))\n\n\n# Splitting data into 70% training and 30% test data:\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=1, stratify=y)\n\n\n\n\nprint(\'Labels counts in y:\', np.bincount(y))\nprint(\'Labels counts in y_train:\', np.bincount(y_train))\nprint(\'Labels counts in y_test:\', np.bincount(y_test))\n\n\n# Standardizing the features:\n\n\n\n\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n\n\n\n# ## Training a perceptron via scikit-learn\n\n\n\n\nppn = Perceptron(eta0=0.1, random_state=1)\nppn.fit(X_train_std, y_train)\n\n\n# **Note**\n# \n# - You can replace `Perceptron(n_iter, ...)` by `Perceptron(max_iter, ...)` in scikit-learn >= 0.19. The `n_iter` parameter is used here deliberately, because some people still use scikit-learn 0.18.\n\n\n\ny_pred = ppn.predict(X_test_std)\nprint(\'Misclassified examples: %d\' % (y_test != y_pred).sum())\n\n\n\n\n\nprint(\'Accuracy: %.3f\' % accuracy_score(y_test, y_pred))\n\n\n\n\nprint(\'Accuracy: %.3f\' % ppn.score(X_test_std, y_test))\n\n\n\n\n\n\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n\n    # setup marker generator and color map\n    markers = (\'s\', \'x\', \'o\', \'^\', \'v\')\n    colors = (\'red\', \'blue\', \'lightgreen\', \'gray\', \'cyan\')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], \n                    y=X[y == cl, 1],\n                    alpha=0.8, \n                    c=colors[idx],\n                    marker=markers[idx], \n                    label=cl, \n                    edgecolor=\'black\')\n\n    # highlight test examples\n    if test_idx:\n        # plot all examples\n        X_test, y_test = X[test_idx, :], y[test_idx]\n\n        plt.scatter(X_test[:, 0],\n                    X_test[:, 1],\n                    c=\'\',\n                    edgecolor=\'black\',\n                    alpha=1.0,\n                    linewidth=1,\n                    marker=\'o\',\n                    s=100, \n                    label=\'test set\')\n\n\n# Training a perceptron model using the standardized training data:\n\n\n\nX_combined_std = np.vstack((X_train_std, X_test_std))\ny_combined = np.hstack((y_train, y_test))\n\nplot_decision_regions(X=X_combined_std, y=y_combined,\n                      classifier=ppn, test_idx=range(105, 150))\nplt.xlabel(\'petal length [standardized]\')\nplt.ylabel(\'petal width [standardized]\')\nplt.legend(loc=\'upper left\')\n\nplt.tight_layout()\n#plt.savefig(\'images/03_01.png\', dpi=300)\nplt.show()\n\n\n\n# # Modeling class probabilities via logistic regression\n\n# ...\n\n# ### Logistic regression intuition and conditional probabilities\n\n\n\n\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\nz = np.arange(-7, 7, 0.1)\nphi_z = sigmoid(z)\n\nplt.plot(z, phi_z)\nplt.axvline(0.0, color=\'k\')\nplt.ylim(-0.1, 1.1)\nplt.xlabel(\'z\')\nplt.ylabel(\'$\\phi (z)$\')\n\n# y axis ticks and gridline\nplt.yticks([0.0, 0.5, 1.0])\nax = plt.gca()\nax.yaxis.grid(True)\n\nplt.tight_layout()\n#plt.savefig(\'images/03_02.png\', dpi=300)\nplt.show()\n\n\n\n\n\n\n\n# ### Learning the weights of the logistic cost function\n\n\n\ndef cost_1(z):\n    return - np.log(sigmoid(z))\n\n\ndef cost_0(z):\n    return - np.log(1 - sigmoid(z))\n\nz = np.arange(-10, 10, 0.1)\nphi_z = sigmoid(z)\n\nc1 = [cost_1(x) for x in z]\nplt.plot(phi_z, c1, label=\'J(w) if y=1\')\n\nc0 = [cost_0(x) for x in z]\nplt.plot(phi_z, c0, linestyle=\'--\', label=\'J(w) if y=0\')\n\nplt.ylim(0.0, 5.1)\nplt.xlim([0, 1])\nplt.xlabel(\'$\\phi$(z)\')\nplt.ylabel(\'J(w)\')\nplt.legend(loc=\'best\')\nplt.tight_layout()\n#plt.savefig(\'images/03_04.png\', dpi=300)\nplt.show()\n\n\n\n\nclass LogisticRegressionGD(object):\n    """"""Logistic Regression Classifier using gradient descent.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n    n_iter : int\n      Passes over the training dataset.\n    random_state : int\n      Random number generator seed for random weight\n      initialization.\n\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n    cost_ : list\n      Logistic cost function value in each epoch.\n\n    """"""\n    def __init__(self, eta=0.05, n_iter=100, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        """""" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_examples, n_features]\n          Training vectors, where n_examples is the number of examples and\n          n_features is the number of features.\n        y : array-like, shape = [n_examples]\n          Target values.\n\n        Returns\n        -------\n        self : object\n\n        """"""\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n        self.cost_ = []\n\n        for i in range(self.n_iter):\n            net_input = self.net_input(X)\n            output = self.activation(net_input)\n            errors = (y - output)\n            self.w_[1:] += self.eta * X.T.dot(errors)\n            self.w_[0] += self.eta * errors.sum()\n            \n            # note that we compute the logistic `cost` now\n            # instead of the sum of squared errors cost\n            cost = -y.dot(np.log(output)) - ((1 - y).dot(np.log(1 - output)))\n            self.cost_.append(cost)\n        return self\n    \n    def net_input(self, X):\n        """"""Calculate net input""""""\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def activation(self, z):\n        """"""Compute logistic sigmoid activation""""""\n        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n\n    def predict(self, X):\n        """"""Return class label after unit step""""""\n        return np.where(self.net_input(X) >= 0.0, 1, 0)\n        # equivalent to:\n        # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)\n\n\n\n\n\nX_train_01_subset = X_train_std[(y_train == 0) | (y_train == 1)]\ny_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]\n\nlrgd = LogisticRegressionGD(eta=0.05, n_iter=1000, random_state=1)\nlrgd.fit(X_train_01_subset,\n         y_train_01_subset)\n\nplot_decision_regions(X=X_train_01_subset, \n                      y=y_train_01_subset,\n                      classifier=lrgd)\n\nplt.xlabel(\'petal length [standardized]\')\nplt.ylabel(\'petal width [standardized]\')\nplt.legend(loc=\'upper left\')\n\nplt.tight_layout()\n#plt.savefig(\'images/03_05.png\', dpi=300)\nplt.show()\n\n\n# ### Training a logistic regression model with scikit-learn\n\n\n\n\nlr = LogisticRegression(C=100.0, random_state=1, solver=\'lbfgs\', multi_class=\'ovr\')\nlr.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, y_combined,\n                      classifier=lr, test_idx=range(105, 150))\nplt.xlabel(\'petal length [standardized]\')\nplt.ylabel(\'petal width [standardized]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n# plt.savefig(\'images/03_06.png\', dpi=300)\nplt.show()\n\n\n\n\nlr.predict_proba(X_test_std[:3, :])\n\n\n\n\nlr.predict_proba(X_test_std[:3, :]).sum(axis=1)\n\n\n\n\nlr.predict_proba(X_test_std[:3, :]).argmax(axis=1)\n\n\n\n\nlr.predict(X_test_std[:3, :])\n\n\n\n\nlr.predict(X_test_std[0, :].reshape(1, -1))\n\n\n\n# ### Tackling overfitting via regularization\n\n\n\n\n\n\n\nweights, params = [], []\nfor c in np.arange(-5, 5):\n    lr = LogisticRegression(C=10.**c, random_state=1,\n                            solver=\'lbfgs\',\n                            multi_class=\'ovr\')\n    lr.fit(X_train_std, y_train)\n    weights.append(lr.coef_[1])\n    params.append(10.**c)\n\nweights = np.array(weights)\nplt.plot(params, weights[:, 0],\n         label=\'petal length\')\nplt.plot(params, weights[:, 1], linestyle=\'--\',\n         label=\'petal width\')\nplt.ylabel(\'weight coefficient\')\nplt.xlabel(\'C\')\nplt.legend(loc=\'upper left\')\nplt.xscale(\'log\')\n#plt.savefig(\'images/03_08.png\', dpi=300)\nplt.show()\n\n\n\n# # Maximum margin classification with support vector machines\n\n\n\n\n\n# ## Maximum margin intuition\n\n# ...\n\n# ## Dealing with the nonlinearly separable case using slack variables\n\n\n\n\n\n\n\n\nsvm = SVC(kernel=\'linear\', C=1.0, random_state=1)\nsvm.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, \n                      y_combined,\n                      classifier=svm, \n                      test_idx=range(105, 150))\nplt.xlabel(\'petal length [standardized]\')\nplt.ylabel(\'petal width [standardized]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n#plt.savefig(\'images/03_11.png\', dpi=300)\nplt.show()\n\n\n# ## Alternative implementations in scikit-learn\n\n\n\n\nppn = SGDClassifier(loss=\'perceptron\')\nlr = SGDClassifier(loss=\'log\')\nsvm = SGDClassifier(loss=\'hinge\')\n\n\n\n# # Solving non-linear problems using a kernel SVM\n\n\n\n\nnp.random.seed(1)\nX_xor = np.random.randn(200, 2)\ny_xor = np.logical_xor(X_xor[:, 0] > 0,\n                       X_xor[:, 1] > 0)\ny_xor = np.where(y_xor, 1, -1)\n\nplt.scatter(X_xor[y_xor == 1, 0],\n            X_xor[y_xor == 1, 1],\n            c=\'b\', marker=\'x\',\n            label=\'1\')\nplt.scatter(X_xor[y_xor == -1, 0],\n            X_xor[y_xor == -1, 1],\n            c=\'r\',\n            marker=\'s\',\n            label=\'-1\')\n\nplt.xlim([-3, 3])\nplt.ylim([-3, 3])\nplt.legend(loc=\'best\')\nplt.tight_layout()\n#plt.savefig(\'images/03_12.png\', dpi=300)\nplt.show()\n\n\n\n\n\n\n\n# ## Using the kernel trick to find separating hyperplanes in higher dimensional space\n\n\n\nsvm = SVC(kernel=\'rbf\', random_state=1, gamma=0.10, C=10.0)\nsvm.fit(X_xor, y_xor)\nplot_decision_regions(X_xor, y_xor,\n                      classifier=svm)\n\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n#plt.savefig(\'images/03_14.png\', dpi=300)\nplt.show()\n\n\n\n\n\nsvm = SVC(kernel=\'rbf\', random_state=1, gamma=0.2, C=1.0)\nsvm.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, y_combined,\n                      classifier=svm, test_idx=range(105, 150))\nplt.xlabel(\'petal length [standardized]\')\nplt.ylabel(\'petal width [standardized]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n#plt.savefig(\'images/03_15.png\', dpi=300)\nplt.show()\n\n\n\n\nsvm = SVC(kernel=\'rbf\', random_state=1, gamma=100.0, C=1.0)\nsvm.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, y_combined, \n                      classifier=svm, test_idx=range(105, 150))\nplt.xlabel(\'petal length [standardized]\')\nplt.ylabel(\'petal width [standardized]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n#plt.savefig(\'images/03_16.png\', dpi=300)\nplt.show()\n\n\n\n# # Decision tree learning\n\n\n\n\n\n\n\n\n\n\n# ## Maximizing information gain - getting the most bang for the buck\n\n\n\n\n\ndef gini(p):\n    return p * (1 - p) + (1 - p) * (1 - (1 - p))\n\n\ndef entropy(p):\n    return - p * np.log2(p) - (1 - p) * np.log2((1 - p))\n\n\ndef error(p):\n    return 1 - np.max([p, 1 - p])\n\nx = np.arange(0.0, 1.0, 0.01)\n\nent = [entropy(p) if p != 0 else None for p in x]\nsc_ent = [e * 0.5 if e else None for e in ent]\nerr = [error(i) for i in x]\n\nfig = plt.figure()\nax = plt.subplot(111)\nfor i, lab, ls, c, in zip([ent, sc_ent, gini(x), err], \n                          [\'Entropy\', \'Entropy (scaled)\', \n                           \'Gini impurity\', \'Misclassification error\'],\n                          [\'-\', \'-\', \'--\', \'-.\'],\n                          [\'black\', \'lightgray\', \'red\', \'green\', \'cyan\']):\n    line = ax.plot(x, i, label=lab, linestyle=ls, lw=2, color=c)\n\nax.legend(loc=\'upper center\', bbox_to_anchor=(0.5, 1.15),\n          ncol=5, fancybox=True, shadow=False)\n\nax.axhline(y=0.5, linewidth=1, color=\'k\', linestyle=\'--\')\nax.axhline(y=1.0, linewidth=1, color=\'k\', linestyle=\'--\')\nplt.ylim([0, 1.1])\nplt.xlabel(\'p(i=1)\')\nplt.ylabel(\'impurity index\')\n#plt.savefig(\'images/03_19.png\', dpi=300, bbox_inches=\'tight\')\nplt.show()\n\n\n\n# ## Building a decision tree\n\n\n\n\ntree_model = DecisionTreeClassifier(criterion=\'gini\', \n                                    max_depth=4, \n                                    random_state=1)\ntree_model.fit(X_train, y_train)\n\nX_combined = np.vstack((X_train, X_test))\ny_combined = np.hstack((y_train, y_test))\nplot_decision_regions(X_combined, y_combined, \n                      classifier=tree_model,\n                      test_idx=range(105, 150))\n\nplt.xlabel(\'petal length [cm]\')\nplt.ylabel(\'petal width [cm]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n#plt.savefig(\'images/03_20.png\', dpi=300)\nplt.show()\n\n\n\n\n\ntree.plot_tree(tree_model)\n#plt.savefig(\'images/03_21_1.pdf\')\nplt.show()\n\n\n\n\n\n\ndot_data = export_graphviz(tree_model,\n                           filled=True, \n                           rounded=True,\n                           class_names=[\'Setosa\', \n                                        \'Versicolor\',\n                                        \'Virginica\'],\n                           feature_names=[\'petal length\', \n                                          \'petal width\'],\n                           out_file=None) \ngraph = graph_from_dot_data(dot_data) \ngraph.write_png(\'tree.png\') \n\n\n\n\n\n\n\n# ## Combining weak to strong learners via random forests\n\n\n\n\nforest = RandomForestClassifier(criterion=\'gini\',\n                                n_estimators=25, \n                                random_state=1,\n                                n_jobs=2)\nforest.fit(X_train, y_train)\n\nplot_decision_regions(X_combined, y_combined, \n                      classifier=forest, test_idx=range(105, 150))\n\nplt.xlabel(\'petal length [cm]\')\nplt.ylabel(\'petal width [cm]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n#plt.savefig(\'images/03_22.png\', dpi=300)\nplt.show()\n\n\n\n# # K-nearest neighbors - a lazy learning algorithm\n\n\n\n\n\n\n\n\nknn = KNeighborsClassifier(n_neighbors=5, \n                           p=2, \n                           metric=\'minkowski\')\nknn.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, y_combined, \n                      classifier=knn, test_idx=range(105, 150))\n\nplt.xlabel(\'petal length [standardized]\')\nplt.ylabel(\'petal width [standardized]\')\nplt.legend(loc=\'upper left\')\nplt.tight_layout()\n#plt.savefig(\'images/03_24.png\', dpi=300)\nplt.show()\n\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
ch04/ch04.py,0,"b'# coding: utf-8\n\n\nimport pandas as pd\nfrom io import StringIO\nimport sys\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.base import clone\nfrom itertools import combinations\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 4 - Building Good Training Datasets \xe2\x80\x93\xc2\xa0Data Preprocessing\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this Jupyter extension via*  \n# \n#     conda install watermark -c conda-forge  \n# \n# or  \n# \n#     pip install watermark   \n# \n# *For more information, please see: https://github.com/rasbt/watermark.*\n\n\n# ### Overview\n\n# - [Dealing with missing data](#Dealing-with-missing-data)\n#   - [Identifying missing values in tabular data](#Identifying-missing-values-in-tabular-data)\n#   - [Eliminating training examples or features with missing values](#Eliminating-training-examples-or-features-with-missing-values)\n#   - [Imputing missing values](#Imputing-missing-values)\n#   - [Understanding the scikit-learn estimator API](#Understanding-the-scikit-learn-estimator-API)\n# - [Handling categorical data](#Handling-categorical-data)\n#   - [Nominal and ordinal features](#Nominal-and-ordinal-features)\n#   - [Mapping ordinal features](#Mapping-ordinal-features)\n#   - [Encoding class labels](#Encoding-class-labels)\n#   - [Performing one-hot encoding on nominal features](#Performing-one-hot-encoding-on-nominal-features)\n# - [Partitioning a dataset into a separate training and test set](#Partitioning-a-dataset-into-seperate-training-and-test-sets)\n# - [Bringing features onto the same scale](#Bringing-features-onto-the-same-scale)\n# - [Selecting meaningful features](#Selecting-meaningful-features)\n#   - [L1 and L2 regularization as penalties against model complexity](#L1-and-L2-regularization-as-penalties-against-model-omplexity)\n#   - [A geometric interpretation of L2 regularization](#A-geometric-interpretation-of-L2-regularization)\n#   - [Sparse solutions with L1 regularization](#Sparse-solutions-with-L1-regularization)\n#   - [Sequential feature selection algorithms](#Sequential-feature-selection-algorithms)\n# - [Assessing feature importance with Random Forests](#Assessing-feature-importance-with-Random-Forests)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Dealing with missing data\n\n# ## Identifying missing values in tabular data\n\n\n\n\ncsv_data = \'\'\'A,B,C,D\n1.0,2.0,3.0,4.0\n5.0,6.0,,8.0\n10.0,11.0,12.0,\'\'\'\n\n# If you are using Python 2.7, you need\n# to convert the string to unicode:\n\nif (sys.version_info < (3, 0)):\n    csv_data = unicode(csv_data)\n\ndf = pd.read_csv(StringIO(csv_data))\ndf\n\n\n\n\ndf.isnull().sum()\n\n\n\n\n# access the underlying NumPy array\n# via the `values` attribute\ndf.values\n\n\n\n# ## Eliminating training examples or features with missing values\n\n\n\n# remove rows that contain missing values\n\ndf.dropna(axis=0)\n\n\n\n\n# remove columns that contain missing values\n\ndf.dropna(axis=1)\n\n\n\n\n# remove columns that contain missing values\n\ndf.dropna(axis=1)\n\n\n\n\n# only drop rows where all columns are NaN\n\ndf.dropna(how=\'all\')  \n\n\n\n\n# drop rows that have fewer than 3 real values \n\ndf.dropna(thresh=4)\n\n\n\n\n# only drop rows where NaN appear in specific columns (here: \'C\')\n\ndf.dropna(subset=[\'C\'])\n\n\n\n# ## Imputing missing values\n\n\n\n# again: our original array\ndf.values\n\n\n\n\n# impute missing values via the column mean\n\n\nimr = SimpleImputer(missing_values=np.nan, strategy=\'mean\')\nimr = imr.fit(df.values)\nimputed_data = imr.transform(df.values)\nimputed_data\n\n\n\n\n\ndf.fillna(df.mean())\n\n\n# ## Understanding the scikit-learn estimator API\n\n\n\n\n\n\n\n\n\n\n# # Handling categorical data\n\n# ## Nominal and ordinal features\n\n\n\n\ndf = pd.DataFrame([[\'green\', \'M\', 10.1, \'class2\'],\n                   [\'red\', \'L\', 13.5, \'class1\'],\n                   [\'blue\', \'XL\', 15.3, \'class2\']])\n\ndf.columns = [\'color\', \'size\', \'price\', \'classlabel\']\ndf\n\n\n\n# ## Mapping ordinal features\n\n\n\nsize_mapping = {\'XL\': 3,\n                \'L\': 2,\n                \'M\': 1}\n\ndf[\'size\'] = df[\'size\'].map(size_mapping)\ndf\n\n\n\n\ninv_size_mapping = {v: k for k, v in size_mapping.items()}\ndf[\'size\'].map(inv_size_mapping)\n\n\n\n# ## Encoding class labels\n\n\n\n\n# create a mapping dict\n# to convert class labels from strings to integers\nclass_mapping = {label: idx for idx, label in enumerate(np.unique(df[\'classlabel\']))}\nclass_mapping\n\n\n\n\n# to convert class labels from strings to integers\ndf[\'classlabel\'] = df[\'classlabel\'].map(class_mapping)\ndf\n\n\n\n\n# reverse the class label mapping\ninv_class_mapping = {v: k for k, v in class_mapping.items()}\ndf[\'classlabel\'] = df[\'classlabel\'].map(inv_class_mapping)\ndf\n\n\n\n\n\n# Label encoding with sklearn\'s LabelEncoder\nclass_le = LabelEncoder()\ny = class_le.fit_transform(df[\'classlabel\'].values)\ny\n\n\n\n\n# reverse mapping\nclass_le.inverse_transform(y)\n\n\n\n# ## Performing one-hot encoding on nominal features\n\n\n\nX = df[[\'color\', \'size\', \'price\']].values\ncolor_le = LabelEncoder()\nX[:, 0] = color_le.fit_transform(X[:, 0])\nX\n\n\n\n\n\nX = df[[\'color\', \'size\', \'price\']].values\ncolor_ohe = OneHotEncoder()\ncolor_ohe.fit_transform(X[:, 0].reshape(-1, 1)).toarray()\n\n\n\n\n\nX = df[[\'color\', \'size\', \'price\']].values\nc_transf = ColumnTransformer([ (\'onehot\', OneHotEncoder(), [0]),\n                               (\'nothing\', \'passthrough\', [1, 2])])\nc_transf.fit_transform(X).astype(float)\n\n\n\n\n# one-hot encoding via pandas\n\npd.get_dummies(df[[\'price\', \'color\', \'size\']])\n\n\n\n\n# multicollinearity guard in get_dummies\n\npd.get_dummies(df[[\'price\', \'color\', \'size\']], drop_first=True)\n\n\n\n\n# multicollinearity guard for the OneHotEncoder\n\ncolor_ohe = OneHotEncoder(categories=\'auto\', drop=\'first\')\nc_transf = ColumnTransformer([ (\'onehot\', color_ohe, [0]),\n                               (\'nothing\', \'passthrough\', [1, 2])])\nc_transf.fit_transform(X).astype(float)\n\n\n\n# ## Optional: Encoding Ordinal Features\n\n# If we are unsure about the numerical differences between the categories of ordinal features, or the difference between two ordinal values is not defined, we can also encode them using a threshold encoding with 0/1 values. For example, we can split the feature ""size"" with values M, L, and XL into two new features ""x > M"" and ""x > L"". Let\'s consider the original DataFrame:\n\n\n\ndf = pd.DataFrame([[\'green\', \'M\', 10.1, \'class2\'],\n                   [\'red\', \'L\', 13.5, \'class1\'],\n                   [\'blue\', \'XL\', 15.3, \'class2\']])\n\ndf.columns = [\'color\', \'size\', \'price\', \'classlabel\']\ndf\n\n\n# We can use the `apply` method of pandas\' DataFrames to write custom lambda expressions in order to encode these variables using the value-threshold approach:\n\n\n\ndf[\'x > M\'] = df[\'size\'].apply(lambda x: 1 if x in {\'L\', \'XL\'} else 0)\ndf[\'x > L\'] = df[\'size\'].apply(lambda x: 1 if x == \'XL\' else 0)\n\ndel df[\'size\']\ndf\n\n\n\n# # Partitioning a dataset into a seperate training and test set\n\n\n\ndf_wine = pd.read_csv(\'https://archive.ics.uci.edu/\'\n                      \'ml/machine-learning-databases/wine/wine.data\',\n                      header=None)\n\n# if the Wine dataset is temporarily unavailable from the\n# UCI machine learning repository, un-comment the following line\n# of code to load the dataset from a local path:\n\n# df_wine = pd.read_csv(\'wine.data\', header=None)\n\n\ndf_wine.columns = [\'Class label\', \'Alcohol\', \'Malic acid\', \'Ash\',\n                   \'Alcalinity of ash\', \'Magnesium\', \'Total phenols\',\n                   \'Flavanoids\', \'Nonflavanoid phenols\', \'Proanthocyanins\',\n                   \'Color intensity\', \'Hue\', \'OD280/OD315 of diluted wines\',\n                   \'Proline\']\n\nprint(\'Class labels\', np.unique(df_wine[\'Class label\']))\ndf_wine.head()\n\n\n\n\n\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n\nX_train, X_test, y_train, y_test =    train_test_split(X, y, \n                     test_size=0.3, \n                     random_state=0, \n                     stratify=y)\n\n\n\n# # Bringing features onto the same scale\n\n\n\n\nmms = MinMaxScaler()\nX_train_norm = mms.fit_transform(X_train)\nX_test_norm = mms.transform(X_test)\n\n\n\n\n\nstdsc = StandardScaler()\nX_train_std = stdsc.fit_transform(X_train)\nX_test_std = stdsc.transform(X_test)\n\n\n# A visual example:\n\n\n\nex = np.array([0, 1, 2, 3, 4, 5])\n\nprint(\'standardized:\', (ex - ex.mean()) / ex.std())\n\n# Please note that pandas uses ddof=1 (sample standard deviation) \n# by default, whereas NumPy\'s std method and the StandardScaler\n# uses ddof=0 (population standard deviation)\n\n# normalize\nprint(\'normalized:\', (ex - ex.min()) / (ex.max() - ex.min()))\n\n\n\n# # Selecting meaningful features\n\n# ...\n\n# ## L1 and L2 regularization as penalties against model complexity\n\n# ## A geometric interpretation of L2 regularization\n\n\n\n\n\n\n\n\n\n# ## Sparse solutions with L1-regularization\n\n\n\n\n\n# For regularized models in scikit-learn that support L1 regularization, we can simply set the `penalty` parameter to `\'l1\'` to obtain a sparse solution:\n\n\n\nLogisticRegression(penalty=\'l1\', solver=\'liblinear\', multi_class=\'ovr\')\n\n\n# Applied to the standardized Wine data ...\n\n\n\n\nlr = LogisticRegression(penalty=\'l1\', C=1.0, solver=\'liblinear\', multi_class=\'ovr\')\n# Note that C=1.0 is the default. You can increase\n# or decrease it to make the regulariztion effect\n# stronger or weaker, respectively.\nlr.fit(X_train_std, y_train)\nprint(\'Training accuracy:\', lr.score(X_train_std, y_train))\nprint(\'Test accuracy:\', lr.score(X_test_std, y_test))\n\n\n\n\nlr.intercept_\n\n\n\n\nnp.set_printoptions(8)\n\n\n\n\nlr.coef_[lr.coef_!=0].shape\n\n\n\n\nlr.coef_\n\n\n\n\n\nfig = plt.figure()\nax = plt.subplot(111)\n    \ncolors = [\'blue\', \'green\', \'red\', \'cyan\', \n          \'magenta\', \'yellow\', \'black\', \n          \'pink\', \'lightgreen\', \'lightblue\', \n          \'gray\', \'indigo\', \'orange\']\n\nweights, params = [], []\nfor c in np.arange(-4., 6.):\n    lr = LogisticRegression(penalty=\'l1\', C=10.**c, solver=\'liblinear\', \n                            multi_class=\'ovr\', random_state=0)\n    lr.fit(X_train_std, y_train)\n    weights.append(lr.coef_[1])\n    params.append(10**c)\n\nweights = np.array(weights)\n\nfor column, color in zip(range(weights.shape[1]), colors):\n    plt.plot(params, weights[:, column],\n             label=df_wine.columns[column + 1],\n             color=color)\nplt.axhline(0, color=\'black\', linestyle=\'--\', linewidth=3)\nplt.xlim([10**(-5), 10**5])\nplt.ylabel(\'weight coefficient\')\nplt.xlabel(\'C\')\nplt.xscale(\'log\')\nplt.legend(loc=\'upper left\')\nax.legend(loc=\'upper center\', \n          bbox_to_anchor=(1.38, 1.03),\n          ncol=1, fancybox=True)\n#plt.savefig(\'images/04_07.png\', dpi=300, \n#            bbox_inches=\'tight\', pad_inches=0.2)\nplt.show()\n\n\n\n# ## Sequential feature selection algorithms\n\n\n\n\n\nclass SBS():\n    def __init__(self, estimator, k_features, scoring=accuracy_score,\n                 test_size=0.25, random_state=1):\n        self.scoring = scoring\n        self.estimator = clone(estimator)\n        self.k_features = k_features\n        self.test_size = test_size\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \n        X_train, X_test, y_train, y_test =             train_test_split(X, y, test_size=self.test_size,\n                             random_state=self.random_state)\n\n        dim = X_train.shape[1]\n        self.indices_ = tuple(range(dim))\n        self.subsets_ = [self.indices_]\n        score = self._calc_score(X_train, y_train, \n                                 X_test, y_test, self.indices_)\n        self.scores_ = [score]\n\n        while dim > self.k_features:\n            scores = []\n            subsets = []\n\n            for p in combinations(self.indices_, r=dim - 1):\n                score = self._calc_score(X_train, y_train, \n                                         X_test, y_test, p)\n                scores.append(score)\n                subsets.append(p)\n\n            best = np.argmax(scores)\n            self.indices_ = subsets[best]\n            self.subsets_.append(self.indices_)\n            dim -= 1\n\n            self.scores_.append(scores[best])\n        self.k_score_ = self.scores_[-1]\n\n        return self\n\n    def transform(self, X):\n        return X[:, self.indices_]\n\n    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n        self.estimator.fit(X_train[:, indices], y_train)\n        y_pred = self.estimator.predict(X_test[:, indices])\n        score = self.scoring(y_test, y_pred)\n        return score\n\n\n\n\n\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# selecting features\nsbs = SBS(knn, k_features=1)\nsbs.fit(X_train_std, y_train)\n\n# plotting performance of feature subsets\nk_feat = [len(k) for k in sbs.subsets_]\n\nplt.plot(k_feat, sbs.scores_, marker=\'o\')\nplt.ylim([0.7, 1.02])\nplt.ylabel(\'Accuracy\')\nplt.xlabel(\'Number of features\')\nplt.grid()\nplt.tight_layout()\n# plt.savefig(\'images/04_08.png\', dpi=300)\nplt.show()\n\n\n\n\nk3 = list(sbs.subsets_[10])\nprint(df_wine.columns[1:][k3])\n\n\n\n\nknn.fit(X_train_std, y_train)\nprint(\'Training accuracy:\', knn.score(X_train_std, y_train))\nprint(\'Test accuracy:\', knn.score(X_test_std, y_test))\n\n\n\n\nknn.fit(X_train_std[:, k3], y_train)\nprint(\'Training accuracy:\', knn.score(X_train_std[:, k3], y_train))\nprint(\'Test accuracy:\', knn.score(X_test_std[:, k3], y_test))\n\n\n\n# # Assessing feature importance with Random Forests\n\n\n\n\nfeat_labels = df_wine.columns[1:]\n\nforest = RandomForestClassifier(n_estimators=500,\n                                random_state=1)\n\nforest.fit(X_train, y_train)\nimportances = forest.feature_importances_\n\nindices = np.argsort(importances)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(""%2d) %-*s %f"" % (f + 1, 30, \n                            feat_labels[indices[f]], \n                            importances[indices[f]]))\n\nplt.title(\'Feature Importance\')\nplt.bar(range(X_train.shape[1]), \n        importances[indices],\n        align=\'center\')\n\nplt.xticks(range(X_train.shape[1]), \n           feat_labels[indices], rotation=90)\nplt.xlim([-1, X_train.shape[1]])\nplt.tight_layout()\n#plt.savefig(\'images/04_09.png\', dpi=300)\nplt.show()\n\n\n\n\n\nsfm = SelectFromModel(forest, threshold=0.1, prefit=True)\nX_selected = sfm.transform(X_train)\nprint(\'Number of features that meet this threshold criterion:\', \n      X_selected.shape[1])\n\n\n# Now, let\'s print the 3 features that met the threshold criterion for feature selection that we set earlier (note that this code snippet does not appear in the actual book but was added to this notebook later for illustrative purposes):\n\n\n\nfor f in range(X_selected.shape[1]):\n    print(""%2d) %-*s %f"" % (f + 1, 30, \n                            feat_labels[indices[f]], \n                            importances[indices[f]]))\n\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
ch05/ch05.py,0,"b'# coding: utf-8\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.linalg import eigh\nfrom distutils.version import LooseVersion as Version\nfrom scipy import __version__ as scipy_version\n    from numpy import exp\n    from scipy import exp\nfrom sklearn.datasets import make_moons\nfrom sklearn.datasets import make_circles\nfrom sklearn.decomposition import KernelPCA\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 5 - Compressing Data via Dimensionality Reduction\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this Jupyter extension via*  \n# \n#     conda install watermark -c conda-forge  \n# \n# or  \n# \n#     pip install watermark   \n# \n# *For more information, please see: https://github.com/rasbt/watermark.*\n\n\n# ### Overview\n\n# - [Unsupervised dimensionality reduction via principal component analysis 128](#Unsupervised-dimensionality-reduction-via-principal-component-analysis-128)\n#   - [The main steps behind principal component analysis](#The-main-steps-behind-principal-component-analysis)\n#   - [Extracting the principal components step-by-step](#Extracting-the-principal-components-step-by-step)\n#   - [Total and explained variance](#Total-and-explained-variance)\n#   - [Feature transformation](#Feature-transformation)\n#   - [Principal component analysis in scikit-learn](#Principal-component-analysis-in-scikit-learn)\n# - [Supervised data compression via linear discriminant analysis](#Supervised-data-compression-via-linear-discriminant-analysis)\n#   - [Principal component analysis versus linear discriminant analysis](#Principal-component-analysis-versus-linear-discriminant-analysis)\n#   - [The inner workings of linear discriminant analysis](#The-inner-workings-of-linear-discriminant-analysis)\n#   - [Computing the scatter matrices](#Computing-the-scatter-matrices)\n#   - [Selecting linear discriminants for the new feature subspace](#Selecting-linear-discriminants-for-the-new-feature-subspace)\n#   - [Projecting examples onto the new feature space](#Projecting-examples-onto-the-new-feature-space)\n#   - [LDA via scikit-learn](#LDA-via-scikit-learn)\n# - [Using kernel principal component analysis for nonlinear mappings](#Using-kernel-principal-component-analysis-for-nonlinear-mappings)\n#   - [Kernel functions and the kernel trick](#Kernel-functions-and-the-kernel-trick)\n#   - [Implementing a kernel principal component analysis in Python](#Implementing-a-kernel-principal-component-analysis-in-Python)\n#     - [Example 1 \xe2\x80\x93 separating half-moon shapes](#Example-1:-Separating-half-moon-shapes)\n#     - [Example 2 \xe2\x80\x93 separating concentric circles](#Example-2:-Separating-concentric-circles)\n#   - [Projecting new data points](#Projecting-new-data-points)\n#   - [Kernel principal component analysis in scikit-learn](#Kernel-principal-component-analysis-in-scikit-learn)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Unsupervised dimensionality reduction via principal component analysis\n\n# ## The main steps behind principal component analysis\n\n\n\n\n\n# ## Extracting the principal components step-by-step\n\n\n\n\ndf_wine = pd.read_csv(\'https://archive.ics.uci.edu/ml/\'\n                      \'machine-learning-databases/wine/wine.data\',\n                      header=None)\n\n# if the Wine dataset is temporarily unavailable from the\n# UCI machine learning repository, un-comment the following line\n# of code to load the dataset from a local path:\n\n# df_wine = pd.read_csv(\'wine.data\', header=None)\n\ndf_wine.columns = [\'Class label\', \'Alcohol\', \'Malic acid\', \'Ash\',\n                   \'Alcalinity of ash\', \'Magnesium\', \'Total phenols\',\n                   \'Flavanoids\', \'Nonflavanoid phenols\', \'Proanthocyanins\',\n                   \'Color intensity\', \'Hue\',\n                   \'OD280/OD315 of diluted wines\', \'Proline\']\n\ndf_wine.head()\n\n\n\n# Splitting the data into 70% training and 30% test subsets.\n\n\n\n\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n\nX_train, X_test, y_train, y_test =     train_test_split(X, y, test_size=0.3, \n                     stratify=y,\n                     random_state=0)\n\n\n# Standardizing the data.\n\n\n\n\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)\n\n\n# ---\n# \n# **Note**\n# \n# Accidentally, I wrote `X_test_std = sc.fit_transform(X_test)` instead of `X_test_std = sc.transform(X_test)`. In this case, it wouldn\'t make a big difference since the mean and standard deviation of the test set should be (quite) similar to the training set. However, as remember from Chapter 3, the correct way is to re-use parameters from the training set if we are doing any kind of transformation -- the test set should basically stand for ""new, unseen"" data.\n# \n# My initial typo reflects a common mistake is that some people are *not* re-using these parameters from the model training/building and standardize the new data ""from scratch."" Here\'s simple example to explain why this is a problem.\n# \n# Let\'s assume we have a simple training set consisting of 3 examples with 1 feature (let\'s call this feature ""length""):\n# \n# - train_1: 10 cm -> class_2\n# - train_2: 20 cm -> class_2\n# - train_3: 30 cm -> class_1\n# \n# mean: 20, std.: 8.2\n# \n# After standardization, the transformed feature values are\n# \n# - train_std_1: -1.21 -> class_2\n# - train_std_2: 0 -> class_2\n# - train_std_3: 1.21 -> class_1\n# \n# Next, let\'s assume our model has learned to classify examples with a standardized length value < 0.6 as class_2 (class_1 otherwise). So far so good. Now, let\'s say we have 3 unlabeled data points that we want to classify:\n# \n# - new_4: 5 cm -> class ?\n# - new_5: 6 cm -> class ?\n# - new_6: 7 cm -> class ?\n# \n# If we look at the ""unstandardized ""length"" values in our training datast, it is intuitive to say that all of these examples are likely belonging to class_2. However, if we standardize these by re-computing standard deviation and and mean you would get similar values as before in the training set and your classifier would (probably incorrectly) classify examples 4 and 5 as class 2.\n# \n# - new_std_4: -1.21 -> class 2\n# - new_std_5: 0 -> class 2\n# - new_std_6: 1.21 -> class 1\n# \n# However, if we use the parameters from your ""training set standardization,"" we\'d get the values:\n# \n# - example5: -18.37 -> class 2\n# - example6: -17.15 -> class 2\n# - example7: -15.92 -> class 2\n# \n# The values 5 cm, 6 cm, and 7 cm are much lower than anything we have seen in the training set previously. Thus, it only makes sense that the standardized features of the ""new examples"" are much lower than every standardized feature in the training set.\n# \n# ---\n\n# Eigendecomposition of the covariance matrix.\n\n\n\ncov_mat = np.cov(X_train_std.T)\neigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n\nprint(\'\\nEigenvalues \\n%s\' % eigen_vals)\n\n\n# **Note**: \n# \n# Above, I used the [`numpy.linalg.eig`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html) function to decompose the symmetric covariance matrix into its eigenvalues and eigenvectors.\n#     <pre>>>> eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)</pre>\n#     This is not really a ""mistake,"" but probably suboptimal. It would be better to use [`numpy.linalg.eigh`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html) in such cases, which has been designed for [Hermetian matrices](https://en.wikipedia.org/wiki/Hermitian_matrix). The latter always returns real  eigenvalues; whereas the numerically less stable `np.linalg.eig` can decompose nonsymmetric square matrices, you may find that it returns complex eigenvalues in certain cases. (S.R.)\n# \n\n\n# ## Total and explained variance\n\n\n\ntot = sum(eigen_vals)\nvar_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\n\n\n\n\n\n\nplt.bar(range(1, 14), var_exp, alpha=0.5, align=\'center\',\n        label=\'Individual explained variance\')\nplt.step(range(1, 14), cum_var_exp, where=\'mid\',\n         label=\'Cumulative explained variance\')\nplt.ylabel(\'Explained variance ratio\')\nplt.xlabel(\'Principal component index\')\nplt.legend(loc=\'best\')\nplt.tight_layout()\n# plt.savefig(\'images/05_02.png\', dpi=300)\nplt.show()\n\n\n\n# ## Feature transformation\n\n\n\n# Make a list of (eigenvalue, eigenvector) tuples\neigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n               for i in range(len(eigen_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neigen_pairs.sort(key=lambda k: k[0], reverse=True)\n\n\n\n\nw = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n               eigen_pairs[1][1][:, np.newaxis]))\nprint(\'Matrix W:\\n\', w)\n\n\n# **Note**\n# Depending on which version of NumPy and LAPACK you are using, you may obtain the Matrix W with its signs flipped. Please note that this is not an issue: If $v$ is an eigenvector of a matrix $\\Sigma$, we have\n# \n# $$\\Sigma v = \\lambda v,$$\n# \n# where $\\lambda$ is our eigenvalue,\n# \n# \n# then $-v$ is also an eigenvector that has the same eigenvalue, since\n# $$\\Sigma \\cdot (-v) = -\\Sigma v = -\\lambda v = \\lambda \\cdot (-v).$$\n\n\n\nX_train_std[0].dot(w)\n\n\n\n\nX_train_pca = X_train_std.dot(w)\ncolors = [\'r\', \'b\', \'g\']\nmarkers = [\'s\', \'x\', \'o\']\n\nfor l, c, m in zip(np.unique(y_train), colors, markers):\n    plt.scatter(X_train_pca[y_train == l, 0], \n                X_train_pca[y_train == l, 1], \n                c=c, label=l, marker=m)\n\nplt.xlabel(\'PC 1\')\nplt.ylabel(\'PC 2\')\nplt.legend(loc=\'lower left\')\nplt.tight_layout()\n# plt.savefig(\'images/05_03.png\', dpi=300)\nplt.show()\n\n\n\n# ## Principal component analysis in scikit-learn\n\n# **NOTE**\n# \n# The following four code cells has been added in addition to the content to the book, to illustrate how to replicate the results from our own PCA implementation in scikit-learn:\n\n\n\n\npca = PCA()\nX_train_pca = pca.fit_transform(X_train_std)\npca.explained_variance_ratio_\n\n\n\n\nplt.bar(range(1, 14), pca.explained_variance_ratio_, alpha=0.5, align=\'center\')\nplt.step(range(1, 14), np.cumsum(pca.explained_variance_ratio_), where=\'mid\')\nplt.ylabel(\'Explained variance ratio\')\nplt.xlabel(\'Principal components\')\n\nplt.show()\n\n\n\n\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\n\n\n\n\nplt.scatter(X_train_pca[:, 0], X_train_pca[:, 1])\nplt.xlabel(\'PC 1\')\nplt.ylabel(\'PC 2\')\nplt.show()\n\n\n\n\n\ndef plot_decision_regions(X, y, classifier, resolution=0.02):\n\n    # setup marker generator and color map\n    markers = (\'s\', \'x\', \'o\', \'^\', \'v\')\n    colors = (\'red\', \'blue\', \'lightgreen\', \'gray\', \'cyan\')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    # plot examples by class\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], \n                    y=X[y == cl, 1],\n                    alpha=0.6, \n                    color=cmap(idx),\n                    edgecolor=\'black\',\n                    marker=markers[idx], \n                    label=cl)\n\n\n# Training logistic regression classifier using the first 2 principal components.\n\n\n\n\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\n\nlr = LogisticRegression(multi_class=\'ovr\', random_state=1, solver=\'lbfgs\')\nlr = lr.fit(X_train_pca, y_train)\n\n\n\n\nplot_decision_regions(X_train_pca, y_train, classifier=lr)\nplt.xlabel(\'PC 1\')\nplt.ylabel(\'PC 2\')\nplt.legend(loc=\'lower left\')\nplt.tight_layout()\n# plt.savefig(\'images/05_04.png\', dpi=300)\nplt.show()\n\n\n\n\nplot_decision_regions(X_test_pca, y_test, classifier=lr)\nplt.xlabel(\'PC 1\')\nplt.ylabel(\'PC 2\')\nplt.legend(loc=\'lower left\')\nplt.tight_layout()\n# plt.savefig(\'images/05_05.png\', dpi=300)\nplt.show()\n\n\n\n\npca = PCA(n_components=None)\nX_train_pca = pca.fit_transform(X_train_std)\npca.explained_variance_ratio_\n\n\n\n# # Supervised data compression via linear discriminant analysis\n\n# ## Principal component analysis versus linear discriminant analysis\n\n\n\n\n\n# ## The inner workings of linear discriminant analysis\n\n\n# ## Computing the scatter matrices\n\n# Calculate the mean vectors for each class:\n\n\n\nnp.set_printoptions(precision=4)\n\nmean_vecs = []\nfor label in range(1, 4):\n    mean_vecs.append(np.mean(X_train_std[y_train == label], axis=0))\n    print(\'MV %s: %s\\n\' % (label, mean_vecs[label - 1]))\n\n\n# Compute the within-class scatter matrix:\n\n\n\nd = 13 # number of features\nS_W = np.zeros((d, d))\nfor label, mv in zip(range(1, 4), mean_vecs):\n    class_scatter = np.zeros((d, d))  # scatter matrix for each class\n    for row in X_train_std[y_train == label]:\n        row, mv = row.reshape(d, 1), mv.reshape(d, 1)  # make column vectors\n        class_scatter += (row - mv).dot((row - mv).T)\n    S_W += class_scatter                          # sum class scatter matrices\n\nprint(\'Within-class scatter matrix: %sx%s\' % (S_W.shape[0], S_W.shape[1]))\n\n\n# Better: covariance matrix since classes are not equally distributed:\n\n\n\nprint(\'Class label distribution: %s\' \n      % np.bincount(y_train)[1:])\n\n\n\n\nd = 13  # number of features\nS_W = np.zeros((d, d))\nfor label, mv in zip(range(1, 4), mean_vecs):\n    class_scatter = np.cov(X_train_std[y_train == label].T)\n    S_W += class_scatter\nprint(\'Scaled within-class scatter matrix: %sx%s\' % (S_W.shape[0],\n                                                     S_W.shape[1]))\n\n\n# Compute the between-class scatter matrix:\n\n\n\nmean_overall = np.mean(X_train_std, axis=0)\nd = 13  # number of features\nS_B = np.zeros((d, d))\nfor i, mean_vec in enumerate(mean_vecs):\n    n = X_train_std[y_train == i + 1, :].shape[0]\n    mean_vec = mean_vec.reshape(d, 1)  # make column vector\n    mean_overall = mean_overall.reshape(d, 1)  # make column vector\n    S_B += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)\n\nprint(\'Between-class scatter matrix: %sx%s\' % (S_B.shape[0], S_B.shape[1]))\n\n\n\n# ## Selecting linear discriminants for the new feature subspace\n\n# Solve the generalized eigenvalue problem for the matrix $S_W^{-1}S_B$:\n\n\n\neigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n\n\n# **Note**:\n#     \n# Above, I used the [`numpy.linalg.eig`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html) function to decompose the symmetric covariance matrix into its eigenvalues and eigenvectors.\n#     <pre>>>> eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)</pre>\n#     This is not really a ""mistake,"" but probably suboptimal. It would be better to use [`numpy.linalg.eigh`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html) in such cases, which has been designed for [Hermetian matrices](https://en.wikipedia.org/wiki/Hermitian_matrix). The latter always returns real  eigenvalues; whereas the numerically less stable `np.linalg.eig` can decompose nonsymmetric square matrices, you may find that it returns complex eigenvalues in certain cases. (S.R.)\n# \n\n# Sort eigenvectors in descending order of the eigenvalues:\n\n\n\n# Make a list of (eigenvalue, eigenvector) tuples\neigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n               for i in range(len(eigen_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neigen_pairs = sorted(eigen_pairs, key=lambda k: k[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n\nprint(\'Eigenvalues in descending order:\\n\')\nfor eigen_val in eigen_pairs:\n    print(eigen_val[0])\n\n\n\n\ntot = sum(eigen_vals.real)\ndiscr = [(i / tot) for i in sorted(eigen_vals.real, reverse=True)]\ncum_discr = np.cumsum(discr)\n\nplt.bar(range(1, 14), discr, alpha=0.5, align=\'center\',\n        label=\'Individual ""discriminability""\')\nplt.step(range(1, 14), cum_discr, where=\'mid\',\n         label=\'Cumulative ""discriminability""\')\nplt.ylabel(\'""Discriminability"" ratio\')\nplt.xlabel(\'Linear discriminants\')\nplt.ylim([-0.1, 1.1])\nplt.legend(loc=\'best\')\nplt.tight_layout()\n# plt.savefig(\'images/05_07.png\', dpi=300)\nplt.show()\n\n\n\n\nw = np.hstack((eigen_pairs[0][1][:, np.newaxis].real,\n              eigen_pairs[1][1][:, np.newaxis].real))\nprint(\'Matrix W:\\n\', w)\n\n\n\n# ## Projecting examples onto the new feature space\n\n\n\nX_train_lda = X_train_std.dot(w)\ncolors = [\'r\', \'b\', \'g\']\nmarkers = [\'s\', \'x\', \'o\']\n\nfor l, c, m in zip(np.unique(y_train), colors, markers):\n    plt.scatter(X_train_lda[y_train == l, 0],\n                X_train_lda[y_train == l, 1] * (-1),\n                c=c, label=l, marker=m)\n\nplt.xlabel(\'LD 1\')\nplt.ylabel(\'LD 2\')\nplt.legend(loc=\'lower right\')\nplt.tight_layout()\n# plt.savefig(\'images/05_08.png\', dpi=300)\nplt.show()\n\n\n\n# ## LDA via scikit-learn\n\n\n\n\nlda = LDA(n_components=2)\nX_train_lda = lda.fit_transform(X_train_std, y_train)\n\n\n\n\n\nlr = LogisticRegression(multi_class=\'ovr\', random_state=1, solver=\'lbfgs\')\nlr = lr.fit(X_train_lda, y_train)\n\nplot_decision_regions(X_train_lda, y_train, classifier=lr)\nplt.xlabel(\'LD 1\')\nplt.ylabel(\'LD 2\')\nplt.legend(loc=\'lower left\')\nplt.tight_layout()\n# plt.savefig(\'images/05_09.png\', dpi=300)\nplt.show()\n\n\n\n\nX_test_lda = lda.transform(X_test_std)\n\nplot_decision_regions(X_test_lda, y_test, classifier=lr)\nplt.xlabel(\'LD 1\')\nplt.ylabel(\'LD 2\')\nplt.legend(loc=\'lower left\')\nplt.tight_layout()\n# plt.savefig(\'images/05_10.png\', dpi=300)\nplt.show()\n\n\n\n# # Using kernel principal component analysis for nonlinear mappings\n\n\n\n\n\n\n# ## Implementing a kernel principal component analysis in Python\n\n\n\n\n\n\n\nif scipy_version >= Version(\'1.4.1\'):\nelse:\n\n\n\n\ndef rbf_kernel_pca(X, gamma, n_components):\n    """"""\n    RBF kernel PCA implementation.\n\n    Parameters\n    ------------\n    X: {NumPy ndarray}, shape = [n_examples, n_features]\n        \n    gamma: float\n      Tuning parameter of the RBF kernel\n        \n    n_components: int\n      Number of principal components to return\n\n    Returns\n    ------------\n     X_pc: {NumPy ndarray}, shape = [n_examples, k_features]\n       Projected dataset   \n\n    """"""\n    # Calculate pairwise squared Euclidean distances\n    # in the MxN dimensional dataset.\n    sq_dists = pdist(X, \'sqeuclidean\')\n\n    # Convert pairwise distances into a square matrix.\n    mat_sq_dists = squareform(sq_dists)\n\n    # Compute the symmetric kernel matrix.\n    K = exp(-gamma * mat_sq_dists)\n\n    # Center the kernel matrix.\n    N = K.shape[0]\n    one_n = np.ones((N, N)) / N\n    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n\n    # Obtaining eigenpairs from the centered kernel matrix\n    # scipy.linalg.eigh returns them in ascending order\n    eigvals, eigvecs = eigh(K)\n    eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]\n\n    # Collect the top k eigenvectors (projected examples)\n    X_pc = np.column_stack([eigvecs[:, i]\n                            for i in range(n_components)])\n\n    return X_pc\n\n\n\n# ### Example 1: Separating half-moon shapes\n\n\n\n\nX, y = make_moons(n_samples=100, random_state=123)\n\nplt.scatter(X[y == 0, 0], X[y == 0, 1], color=\'red\', marker=\'^\', alpha=0.5)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], color=\'blue\', marker=\'o\', alpha=0.5)\n\nplt.tight_layout()\n# plt.savefig(\'images/05_12.png\', dpi=300)\nplt.show()\n\n\n\n\n\nscikit_pca = PCA(n_components=2)\nX_spca = scikit_pca.fit_transform(X)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n\nax[0].scatter(X_spca[y == 0, 0], X_spca[y == 0, 1],\n              color=\'red\', marker=\'^\', alpha=0.5)\nax[0].scatter(X_spca[y == 1, 0], X_spca[y == 1, 1],\n              color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[1].scatter(X_spca[y == 0, 0], np.zeros((50, 1)) + 0.02,\n              color=\'red\', marker=\'^\', alpha=0.5)\nax[1].scatter(X_spca[y == 1, 0], np.zeros((50, 1)) - 0.02,\n              color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[0].set_xlabel(\'PC1\')\nax[0].set_ylabel(\'PC2\')\nax[1].set_ylim([-1, 1])\nax[1].set_yticks([])\nax[1].set_xlabel(\'PC1\')\n\nplt.tight_layout()\n# plt.savefig(\'images/05_13.png\', dpi=300)\nplt.show()\n\n\n\n\nX_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\nax[0].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], \n            color=\'red\', marker=\'^\', alpha=0.5)\nax[0].scatter(X_kpca[y==1, 0], X_kpca[y==1, 1],\n            color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[1].scatter(X_kpca[y==0, 0], np.zeros((50, 1))+0.02, \n            color=\'red\', marker=\'^\', alpha=0.5)\nax[1].scatter(X_kpca[y==1, 0], np.zeros((50, 1))-0.02,\n            color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[0].set_xlabel(\'PC1\')\nax[0].set_ylabel(\'PC2\')\nax[1].set_ylim([-1, 1])\nax[1].set_yticks([])\nax[1].set_xlabel(\'PC1\')\n\nplt.tight_layout()\n# plt.savefig(\'images/05_14.png\', dpi=300)\nplt.show()\n\n\n\n# ### Example 2: Separating concentric circles\n\n\n\n\nX, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n\nplt.scatter(X[y == 0, 0], X[y == 0, 1], color=\'red\', marker=\'^\', alpha=0.5)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], color=\'blue\', marker=\'o\', alpha=0.5)\n\nplt.tight_layout()\n# plt.savefig(\'images/05_15.png\', dpi=300)\nplt.show()\n\n\n\n\nscikit_pca = PCA(n_components=2)\nX_spca = scikit_pca.fit_transform(X)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n\nax[0].scatter(X_spca[y == 0, 0], X_spca[y == 0, 1],\n              color=\'red\', marker=\'^\', alpha=0.5)\nax[0].scatter(X_spca[y == 1, 0], X_spca[y == 1, 1],\n              color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[1].scatter(X_spca[y == 0, 0], np.zeros((500, 1)) + 0.02,\n              color=\'red\', marker=\'^\', alpha=0.5)\nax[1].scatter(X_spca[y == 1, 0], np.zeros((500, 1)) - 0.02,\n              color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[0].set_xlabel(\'PC1\')\nax[0].set_ylabel(\'PC2\')\nax[1].set_ylim([-1, 1])\nax[1].set_yticks([])\nax[1].set_xlabel(\'PC1\')\n\nplt.tight_layout()\n# plt.savefig(\'images/05_16.png\', dpi=300)\nplt.show()\n\n\n\n\nX_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\nax[0].scatter(X_kpca[y == 0, 0], X_kpca[y == 0, 1],\n              color=\'red\', marker=\'^\', alpha=0.5)\nax[0].scatter(X_kpca[y == 1, 0], X_kpca[y == 1, 1],\n              color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[1].scatter(X_kpca[y == 0, 0], np.zeros((500, 1)) + 0.02,\n              color=\'red\', marker=\'^\', alpha=0.5)\nax[1].scatter(X_kpca[y == 1, 0], np.zeros((500, 1)) - 0.02,\n              color=\'blue\', marker=\'o\', alpha=0.5)\n\nax[0].set_xlabel(\'PC1\')\nax[0].set_ylabel(\'PC2\')\nax[1].set_ylim([-1, 1])\nax[1].set_yticks([])\nax[1].set_xlabel(\'PC1\')\n\nplt.tight_layout()\n# plt.savefig(\'images/05_17.png\', dpi=300)\nplt.show()\n\n\n\n# ## Projecting new data points\n\n\n\n\ndef rbf_kernel_pca(X, gamma, n_components):\n    """"""\n    RBF kernel PCA implementation.\n\n    Parameters\n    ------------\n    X: {NumPy ndarray}, shape = [n_examples, n_features]\n        \n    gamma: float\n      Tuning parameter of the RBF kernel\n        \n    n_components: int\n      Number of principal components to return\n\n    Returns\n    ------------\n     alphas: {NumPy ndarray}, shape = [n_examples, k_features]\n       Projected dataset \n     \n     lambdas: list\n       Eigenvalues\n\n    """"""\n    # Calculate pairwise squared Euclidean distances\n    # in the MxN dimensional dataset.\n    sq_dists = pdist(X, \'sqeuclidean\')\n\n    # Convert pairwise distances into a square matrix.\n    mat_sq_dists = squareform(sq_dists)\n\n    # Compute the symmetric kernel matrix.\n    K = exp(-gamma * mat_sq_dists)\n\n    # Center the kernel matrix.\n    N = K.shape[0]\n    one_n = np.ones((N, N)) / N\n    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n\n    # Obtaining eigenpairs from the centered kernel matrix\n    # scipy.linalg.eigh returns them in ascending order\n    eigvals, eigvecs = eigh(K)\n    eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]\n\n    # Collect the top k eigenvectors (projected examples)\n    alphas = np.column_stack([eigvecs[:, i]\n                              for i in range(n_components)])\n\n    # Collect the corresponding eigenvalues\n    lambdas = [eigvals[i] for i in range(n_components)]\n\n    return alphas, lambdas\n\n\n\n\nX, y = make_moons(n_samples=100, random_state=123)\nalphas, lambdas = rbf_kernel_pca(X, gamma=15, n_components=1)\n\n\n\n\nx_new = X[25]\nx_new\n\n\n\n\nx_proj = alphas[25] # original projection\nx_proj\n\n\n\n\ndef project_x(x_new, X, gamma, alphas, lambdas):\n    pair_dist = np.array([np.sum((x_new - row)**2) for row in X])\n    k = np.exp(-gamma * pair_dist)\n    return k.dot(alphas / lambdas)\n\n# projection of the ""new"" datapoint\nx_reproj = project_x(x_new, X, gamma=15, alphas=alphas, lambdas=lambdas)\nx_reproj \n\n\n\n\nplt.scatter(alphas[y == 0, 0], np.zeros((50)),\n            color=\'red\', marker=\'^\', alpha=0.5)\nplt.scatter(alphas[y == 1, 0], np.zeros((50)),\n            color=\'blue\', marker=\'o\', alpha=0.5)\nplt.scatter(x_proj, 0, color=\'black\',\n            label=\'Original projection of point X[25]\', marker=\'^\', s=100)\nplt.scatter(x_reproj, 0, color=\'green\',\n            label=\'Remapped point X[25]\', marker=\'x\', s=500)\nplt.yticks([], [])\nplt.legend(scatterpoints=1)\n\nplt.tight_layout()\n# plt.savefig(\'images/05_18.png\', dpi=300)\nplt.show()\n\n\n\n# ## Kernel principal component analysis in scikit-learn\n\n\n\n\nX, y = make_moons(n_samples=100, random_state=123)\nscikit_kpca = KernelPCA(n_components=2, kernel=\'rbf\', gamma=15)\nX_skernpca = scikit_kpca.fit_transform(X)\n\nplt.scatter(X_skernpca[y == 0, 0], X_skernpca[y == 0, 1],\n            color=\'red\', marker=\'^\', alpha=0.5)\nplt.scatter(X_skernpca[y == 1, 0], X_skernpca[y == 1, 1],\n            color=\'blue\', marker=\'o\', alpha=0.5)\n\nplt.xlabel(\'PC1\')\nplt.ylabel(\'PC2\')\nplt.tight_layout()\n# plt.savefig(\'images/05_19.png\', dpi=300)\nplt.show()\n\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
ch06/ch06.py,0,"b'# coding: utf-8\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import roc_curve, auc\nfrom distutils.version import LooseVersion as Version\nfrom scipy import __version__ as scipy_version\n    from numpy import interp\n    from scipy import interp\nfrom sklearn.utils import resample\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 6 - Learning Best Practices for Model Evaluation and Hyperparameter Tuning\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this Jupyter extension via*  \n# \n#     conda install watermark -c conda-forge  \n# \n# or  \n# \n#     pip install watermark   \n# \n# *For more information, please see: https://github.com/rasbt/watermark.*\n\n\n# ### Overview\n\n# - [Streamlining workflows with pipelines](#Streamlining-workflows-with-pipelines)\n#   - [Loading the Breast Cancer Wisconsin dataset](#Loading-the-Breast-Cancer-Wisconsin-dataset)\n#   - [Combining transformers and estimators in a pipeline](#Combining-transformers-and-estimators-in-a-pipeline)\n# - [Using k-fold cross-validation to assess model performance](#Using-k-fold-cross-validation-to-assess-model-performance)\n#   - [The holdout method](#The-holdout-method)\n#   - [K-fold cross-validation](#K-fold-cross-validation)\n# - [Debugging algorithms with learning and validation curves](#Debugging-algorithms-with-learning-and-validation-curves)\n#   - [Diagnosing bias and variance problems with learning curves](#Diagnosing-bias-and-variance-problems-with-learning-curves)\n#   - [Addressing overfitting and underfitting with validation curves](#Addressing-overfitting-and-underfitting-with-validation-curves)\n# - [Fine-tuning machine learning models via grid search](#Fine-tuning-machine-learning-models-via-grid-search)\n#   - [Tuning hyperparameters via grid search](#Tuning-hyperparameters-via-grid-search)\n#   - [Algorithm selection with nested cross-validation](#Algorithm-selection-with-nested-cross-validation)\n# - [Looking at different performance evaluation metrics](#Looking-at-different-performance-evaluation-metrics)\n#   - [Reading a confusion matrix](#Reading-a-confusion-matrix)\n#   - [Optimizing the precision and recall of a classification model](#Optimizing-the-precision-and-recall-of-a-classification-model)\n#   - [Plotting a receiver operating characteristic](#Plotting-a-receiver-operating-characteristic)\n#   - [The scoring metrics for multiclass classification](#The-scoring-metrics-for-multiclass-classification)\n# - [Dealing with class imbalance](#Dealing-with-class-imbalance)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Streamlining workflows with pipelines\n\n# ...\n\n# ## Loading the Breast Cancer Wisconsin dataset\n\n\n\n\ndf = pd.read_csv(\'https://archive.ics.uci.edu/ml/\'\n                 \'machine-learning-databases\'\n                 \'/breast-cancer-wisconsin/wdbc.data\', header=None)\n\n# if the Breast Cancer dataset is temporarily unavailable from the\n# UCI machine learning repository, un-comment the following line\n# of code to load the dataset from a local path:\n\n# df = pd.read_csv(\'wdbc.data\', header=None)\n\ndf.head()\n\n\n\n\ndf.shape\n\n\n\n\n\n\nX = df.loc[:, 2:].values\ny = df.loc[:, 1].values\nle = LabelEncoder()\ny = le.fit_transform(y)\nle.classes_\n\n\n\n\nle.transform([\'M\', \'B\'])\n\n\n\n\n\nX_train, X_test, y_train, y_test =     train_test_split(X, y, \n                     test_size=0.20,\n                     stratify=y,\n                     random_state=1)\n\n\n\n# ## Combining transformers and estimators in a pipeline\n\n\n\n\npipe_lr = make_pipeline(StandardScaler(),\n                        PCA(n_components=2),\n                        LogisticRegression(random_state=1, solver=\'lbfgs\'))\n\npipe_lr.fit(X_train, y_train)\ny_pred = pipe_lr.predict(X_test)\nprint(\'Test Accuracy: %.3f\' % pipe_lr.score(X_test, y_test))\n\n\n\n\n\n\n\n# # Using k-fold cross validation to assess model performance\n\n# ...\n\n# ## The holdout method\n\n\n\n\n\n\n# ## K-fold cross-validation\n\n\n\n\n\n\n\n    \n\nkfold = StratifiedKFold(n_splits=10).split(X_train, y_train)\n\nscores = []\nfor k, (train, test) in enumerate(kfold):\n    pipe_lr.fit(X_train[train], y_train[train])\n    score = pipe_lr.score(X_train[test], y_train[test])\n    scores.append(score)\n    print(\'Fold: %2d, Class dist.: %s, Acc: %.3f\' % (k+1,\n          np.bincount(y_train[train]), score))\n    \nprint(\'\\nCV accuracy: %.3f +/- %.3f\' % (np.mean(scores), np.std(scores)))\n\n\n\n\n\nscores = cross_val_score(estimator=pipe_lr,\n                         X=X_train,\n                         y=y_train,\n                         cv=10,\n                         n_jobs=1)\nprint(\'CV accuracy scores: %s\' % scores)\nprint(\'CV accuracy: %.3f +/- %.3f\' % (np.mean(scores), np.std(scores)))\n\n\n\n# # Debugging algorithms with learning curves\n\n\n# ## Diagnosing bias and variance problems with learning curves\n\n\n\n\n\n\n\n\n\npipe_lr = make_pipeline(StandardScaler(),\n                        LogisticRegression(penalty=\'l2\', random_state=1,\n                                           solver=\'lbfgs\', max_iter=10000))\n\ntrain_sizes, train_scores, test_scores =                learning_curve(estimator=pipe_lr,\n                               X=X_train,\n                               y=y_train,\n                               train_sizes=np.linspace(0.1, 1.0, 10),\n                               cv=10,\n                               n_jobs=1)\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\nplt.plot(train_sizes, train_mean,\n         color=\'blue\', marker=\'o\',\n         markersize=5, label=\'Training accuracy\')\n\nplt.fill_between(train_sizes,\n                 train_mean + train_std,\n                 train_mean - train_std,\n                 alpha=0.15, color=\'blue\')\n\nplt.plot(train_sizes, test_mean,\n         color=\'green\', linestyle=\'--\',\n         marker=\'s\', markersize=5,\n         label=\'Validation accuracy\')\n\nplt.fill_between(train_sizes,\n                 test_mean + test_std,\n                 test_mean - test_std,\n                 alpha=0.15, color=\'green\')\n\nplt.grid()\nplt.xlabel(\'Number of training examples\')\nplt.ylabel(\'Accuracy\')\nplt.legend(loc=\'lower right\')\nplt.ylim([0.8, 1.03])\nplt.tight_layout()\n# plt.savefig(\'images/06_05.png\', dpi=300)\nplt.show()\n\n\n\n# ## Addressing over- and underfitting with validation curves\n\n\n\n\n\nparam_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\ntrain_scores, test_scores = validation_curve(\n                estimator=pipe_lr, \n                X=X_train, \n                y=y_train, \n                param_name=\'logisticregression__C\', \n                param_range=param_range,\n                cv=10)\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\nplt.plot(param_range, train_mean, \n         color=\'blue\', marker=\'o\', \n         markersize=5, label=\'Training accuracy\')\n\nplt.fill_between(param_range, train_mean + train_std,\n                 train_mean - train_std, alpha=0.15,\n                 color=\'blue\')\n\nplt.plot(param_range, test_mean, \n         color=\'green\', linestyle=\'--\', \n         marker=\'s\', markersize=5, \n         label=\'Validation accuracy\')\n\nplt.fill_between(param_range, \n                 test_mean + test_std,\n                 test_mean - test_std, \n                 alpha=0.15, color=\'green\')\n\nplt.grid()\nplt.xscale(\'log\')\nplt.legend(loc=\'lower right\')\nplt.xlabel(\'Parameter C\')\nplt.ylabel(\'Accuracy\')\nplt.ylim([0.8, 1.0])\nplt.tight_layout()\n# plt.savefig(\'images/06_06.png\', dpi=300)\nplt.show()\n\n\n\n# # Fine-tuning machine learning models via grid search\n\n\n# ## Tuning hyperparameters via grid search \n\n\n\n\npipe_svc = make_pipeline(StandardScaler(),\n                         SVC(random_state=1))\n\nparam_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n\nparam_grid = [{\'svc__C\': param_range, \n               \'svc__kernel\': [\'linear\']},\n              {\'svc__C\': param_range, \n               \'svc__gamma\': param_range, \n               \'svc__kernel\': [\'rbf\']}]\n\ngs = GridSearchCV(estimator=pipe_svc, \n                  param_grid=param_grid, \n                  scoring=\'accuracy\', \n                  refit=True,\n                  cv=10,\n                  n_jobs=-1)\ngs = gs.fit(X_train, y_train)\nprint(gs.best_score_)\nprint(gs.best_params_)\n\n\n\n\nclf = gs.best_estimator_\n\n# clf.fit(X_train, y_train) \n# note that we do not need to refit the classifier\n# because this is done automatically via refit=True.\n\nprint(\'Test accuracy: %.3f\' % clf.score(X_test, y_test))\n\n\n\n# ## Algorithm selection with nested cross-validation\n\n\n\n\n\n\n\ngs = GridSearchCV(estimator=pipe_svc,\n                  param_grid=param_grid,\n                  scoring=\'accuracy\',\n                  cv=2)\n\nscores = cross_val_score(gs, X_train, y_train, \n                         scoring=\'accuracy\', cv=5)\nprint(\'CV accuracy: %.3f +/- %.3f\' % (np.mean(scores),\n                                      np.std(scores)))\n\n\n\n\n\ngs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0),\n                  param_grid=[{\'max_depth\': [1, 2, 3, 4, 5, 6, 7, None]}],\n                  scoring=\'accuracy\',\n                  cv=2)\n\nscores = cross_val_score(gs, X_train, y_train, \n                         scoring=\'accuracy\', cv=5)\nprint(\'CV accuracy: %.3f +/- %.3f\' % (np.mean(scores), \n                                      np.std(scores)))\n\n\n\n# # Looking at different performance evaluation metrics\n\n# ...\n\n# ## Reading a confusion matrix\n\n\n\n\n\n\n\n\npipe_svc.fit(X_train, y_train)\ny_pred = pipe_svc.predict(X_test)\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(confmat)\n\n\n\n\nfig, ax = plt.subplots(figsize=(2.5, 2.5))\nax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(confmat.shape[0]):\n    for j in range(confmat.shape[1]):\n        ax.text(x=j, y=i, s=confmat[i, j], va=\'center\', ha=\'center\')\n\nplt.xlabel(\'Predicted label\')\nplt.ylabel(\'True label\')\n\nplt.tight_layout()\n#plt.savefig(\'images/06_09.png\', dpi=300)\nplt.show()\n\n\n# ### Additional Note\n\n# Remember that we previously encoded the class labels so that *malignant* examples are the ""postive"" class (1), and *benign* examples are the ""negative"" class (0):\n\n\n\nle.transform([\'M\', \'B\'])\n\n\n\n\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(confmat)\n\n\n# Next, we printed the confusion matrix like so:\n\n\n\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(confmat)\n\n\n# Note that the (true) class 0 examples that are correctly predicted as class 0 (true negatives) are now in the upper left corner of the matrix (index 0, 0). In order to change the ordering so that the true negatives are in the lower right corner (index 1,1) and the true positves are in the upper left, we can use the `labels` argument like shown below:\n\n\n\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred, labels=[1, 0])\nprint(confmat)\n\n\n# We conclude:\n# \n# Assuming that class 1 (malignant) is the positive class in this example, our model correctly classified 71 of the examples that belong to class 0 (true negatives) and 40 examples that belong to class 1 (true positives), respectively. However, our model also incorrectly misclassified 1 example from class 0 as class 1 (false positive), and it predicted that 2 examples are benign although it is a malignant tumor (false negatives).\n\n\n# ## Optimizing the precision and recall of a classification model\n\n\n\n\nprint(\'Precision: %.3f\' % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\'Recall: %.3f\' % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\'F1: %.3f\' % f1_score(y_true=y_test, y_pred=y_pred))\n\n\n\n\n\nscorer = make_scorer(f1_score, pos_label=0)\n\nc_gamma_range = [0.01, 0.1, 1.0, 10.0]\n\nparam_grid = [{\'svc__C\': c_gamma_range,\n               \'svc__kernel\': [\'linear\']},\n              {\'svc__C\': c_gamma_range,\n               \'svc__gamma\': c_gamma_range,\n               \'svc__kernel\': [\'rbf\']}]\n\ngs = GridSearchCV(estimator=pipe_svc,\n                  param_grid=param_grid,\n                  scoring=scorer,\n                  cv=10,\n                  n_jobs=-1)\ngs = gs.fit(X_train, y_train)\nprint(gs.best_score_)\nprint(gs.best_params_)\n\n\n\n# ## Plotting a receiver operating characteristic\n\n\n\n\n\nif scipy_version >= Version(\'1.4.1\'):\nelse:\n\n\npipe_lr = make_pipeline(StandardScaler(),\n                        PCA(n_components=2),\n                        LogisticRegression(penalty=\'l2\', \n                                           random_state=1,\n                                           solver=\'lbfgs\',\n                                           C=100.0))\n\nX_train2 = X_train[:, [4, 14]]\n    \n\ncv = list(StratifiedKFold(n_splits=3).split(X_train, y_train))\n\nfig = plt.figure(figsize=(7, 5))\n\nmean_tpr = 0.0\nmean_fpr = np.linspace(0, 1, 100)\nall_tpr = []\n\nfor i, (train, test) in enumerate(cv):\n    probas = pipe_lr.fit(X_train2[train],\n                         y_train[train]).predict_proba(X_train2[test])\n\n    fpr, tpr, thresholds = roc_curve(y_train[test],\n                                     probas[:, 1],\n                                     pos_label=1)\n    mean_tpr += interp(mean_fpr, fpr, tpr)\n    mean_tpr[0] = 0.0\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr,\n             tpr,\n             label=\'ROC fold %d (area = %0.2f)\'\n                   % (i+1, roc_auc))\n\nplt.plot([0, 1],\n         [0, 1],\n         linestyle=\'--\',\n         color=(0.6, 0.6, 0.6),\n         label=\'Random guessing\')\n\nmean_tpr /= len(cv)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, \'k--\',\n         label=\'Mean ROC (area = %0.2f)\' % mean_auc, lw=2)\nplt.plot([0, 0, 1],\n         [0, 1, 1],\n         linestyle=\':\',\n         color=\'black\',\n         label=\'Perfect performance\')\n\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel(\'False positive rate\')\nplt.ylabel(\'True positive rate\')\nplt.legend(loc=""lower right"")\n\nplt.tight_layout()\n# plt.savefig(\'images/06_10.png\', dpi=300)\nplt.show()\n\n\n\n# ## The scoring metrics for multiclass classification\n\n\n\npre_scorer = make_scorer(score_func=precision_score, \n                         pos_label=1, \n                         greater_is_better=True, \n                         average=\'micro\')\n\n\n# ## Dealing with class imbalance\n\n\n\nX_imb = np.vstack((X[y == 0], X[y == 1][:40]))\ny_imb = np.hstack((y[y == 0], y[y == 1][:40]))\n\n\n\n\ny_pred = np.zeros(y_imb.shape[0])\nnp.mean(y_pred == y_imb) * 100\n\n\n\n\n\nprint(\'Number of class 1 examples before:\', X_imb[y_imb == 1].shape[0])\n\nX_upsampled, y_upsampled = resample(X_imb[y_imb == 1],\n                                    y_imb[y_imb == 1],\n                                    replace=True,\n                                    n_samples=X_imb[y_imb == 0].shape[0],\n                                    random_state=123)\n\nprint(\'Number of class 1 examples after:\', X_upsampled.shape[0])\n\n\n\n\nX_bal = np.vstack((X[y == 0], X_upsampled))\ny_bal = np.hstack((y[y == 0], y_upsampled))\n\n\n\n\ny_pred = np.zeros(y_bal.shape[0])\nnp.mean(y_pred == y_bal) * 100\n\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
ch07/ch07.py,0,"b'# coding: utf-8\n\n\nfrom scipy.special import comb\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import ClassifierMixin\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.base import clone\nfrom sklearn.pipeline import _name_estimators\nimport operator\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom itertools import product\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 7 - Combining Different Models for Ensemble Learning\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this Jupyter extension via*  \n# \n#     conda install watermark -c conda-forge  \n# \n# or  \n# \n#     pip install watermark   \n# \n# *For more information, please see: https://github.com/rasbt/watermark.*\n\n\n# ### Overview\n\n# - [Learning with ensembles](#Learning-with-ensembles)\n# - [Combining classifiers via majority vote](#Combining-classifiers-via-majority-vote)\n#     - [Implementing a simple majority vote classifier](#Implementing-a-simple-majority-vote-classifier)\n#     - [Using the majority voting principle to make predictions](#Using-the-majority-voting-principle-to-make-predictions)\n#     - [Evaluating and tuning the ensemble classifier](#Evaluating-and-tuning-the-ensemble-classifier)\n# - [Bagging \xe2\x80\x93 building an ensemble of classifiers from bootstrap samples](#Bagging----Building-an-ensemble-of-classifiers-from-bootstrap-samples)\n#     - [Bagging in a nutshell](#Bagging-in-a-nutshell)\n#     - [Applying bagging to classify examples in the Wine dataset](#Applying-bagging-to-classify-examples-in-the-Wine-dataset)\n# - [Leveraging weak learners via adaptive boosting](#Leveraging-weak-learners-via-adaptive-boosting)\n#     - [How boosting works](#How-boosting-works)\n#     - [Applying AdaBoost using scikit-learn](#Applying-AdaBoost-using-scikit-learn)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Learning with ensembles\n\n\n\n\n\n\n\n\n\n\n\n\ndef ensemble_error(n_classifier, error):\n    k_start = int(math.ceil(n_classifier / 2.))\n    probs = [comb(n_classifier, k) * error**k * (1-error)**(n_classifier - k)\n             for k in range(k_start, n_classifier + 1)]\n    return sum(probs)\n\n\n\n\nensemble_error(n_classifier=11, error=0.25)\n\n\n\n\n\nerror_range = np.arange(0.0, 1.01, 0.01)\nens_errors = [ensemble_error(n_classifier=11, error=error)\n              for error in error_range]\n\n\n\n\n\nplt.plot(error_range, \n         ens_errors, \n         label=\'Ensemble error\', \n         linewidth=2)\n\nplt.plot(error_range, \n         error_range, \n         linestyle=\'--\',\n         label=\'Base error\',\n         linewidth=2)\n\nplt.xlabel(\'Base error\')\nplt.ylabel(\'Base/Ensemble error\')\nplt.legend(loc=\'upper left\')\nplt.grid(alpha=0.5)\n#plt.savefig(\'images/07_03.png\', dpi=300)\nplt.show()\n\n\n\n# # Combining classifiers via majority vote\n\n# ## Implementing a simple majority vote classifier \n\n\n\n\nnp.argmax(np.bincount([0, 0, 1], \n                      weights=[0.2, 0.2, 0.6]))\n\n\n\n\nex = np.array([[0.9, 0.1],\n               [0.8, 0.2],\n               [0.4, 0.6]])\n\np = np.average(ex, \n               axis=0, \n               weights=[0.2, 0.2, 0.6])\np\n\n\n\n\nnp.argmax(p)\n\n\n\n\n\n\nclass MajorityVoteClassifier(BaseEstimator, \n                             ClassifierMixin):\n    """""" A majority vote ensemble classifier\n\n    Parameters\n    ----------\n    classifiers : array-like, shape = [n_classifiers]\n      Different classifiers for the ensemble\n\n    vote : str, {\'classlabel\', \'probability\'} (default=\'classlabel\')\n      If \'classlabel\' the prediction is based on the argmax of\n        class labels. Else if \'probability\', the argmax of\n        the sum of probabilities is used to predict the class label\n        (recommended for calibrated classifiers).\n\n    weights : array-like, shape = [n_classifiers], optional (default=None)\n      If a list of `int` or `float` values are provided, the classifiers\n      are weighted by importance; Uses uniform weights if `weights=None`.\n\n    """"""\n    def __init__(self, classifiers, vote=\'classlabel\', weights=None):\n\n        self.classifiers = classifiers\n        self.named_classifiers = {key: value for key, value\n                                  in _name_estimators(classifiers)}\n        self.vote = vote\n        self.weights = weights\n\n    def fit(self, X, y):\n        """""" Fit classifiers.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_examples, n_features]\n            Matrix of training examples.\n\n        y : array-like, shape = [n_examples]\n            Vector of target class labels.\n\n        Returns\n        -------\n        self : object\n\n        """"""\n        if self.vote not in (\'probability\', \'classlabel\'):\n            raise ValueError(""vote must be \'probability\' or \'classlabel\'""\n                             ""; got (vote=%r)""\n                             % self.vote)\n\n        if self.weights and len(self.weights) != len(self.classifiers):\n            raise ValueError(\'Number of classifiers and weights must be equal\'\n                             \'; got %d weights, %d classifiers\'\n                             % (len(self.weights), len(self.classifiers)))\n\n        # Use LabelEncoder to ensure class labels start with 0, which\n        # is important for np.argmax call in self.predict\n        self.lablenc_ = LabelEncoder()\n        self.lablenc_.fit(y)\n        self.classes_ = self.lablenc_.classes_\n        self.classifiers_ = []\n        for clf in self.classifiers:\n            fitted_clf = clone(clf).fit(X, self.lablenc_.transform(y))\n            self.classifiers_.append(fitted_clf)\n        return self\n\n    def predict(self, X):\n        """""" Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_examples, n_features]\n            Matrix of training examples.\n\n        Returns\n        ----------\n        maj_vote : array-like, shape = [n_examples]\n            Predicted class labels.\n            \n        """"""\n        if self.vote == \'probability\':\n            maj_vote = np.argmax(self.predict_proba(X), axis=1)\n        else:  # \'classlabel\' vote\n\n            #  Collect results from clf.predict calls\n            predictions = np.asarray([clf.predict(X)\n                                      for clf in self.classifiers_]).T\n\n            maj_vote = np.apply_along_axis(\n                                      lambda x:\n                                      np.argmax(np.bincount(x,\n                                                weights=self.weights)),\n                                      axis=1,\n                                      arr=predictions)\n        maj_vote = self.lablenc_.inverse_transform(maj_vote)\n        return maj_vote\n\n    def predict_proba(self, X):\n        """""" Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_examples, n_features]\n            Training vectors, where n_examples is the number of examples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        avg_proba : array-like, shape = [n_examples, n_classes]\n            Weighted average probability for each class per example.\n\n        """"""\n        probas = np.asarray([clf.predict_proba(X)\n                             for clf in self.classifiers_])\n        avg_proba = np.average(probas, axis=0, weights=self.weights)\n        return avg_proba\n\n    def get_params(self, deep=True):\n        """""" Get classifier parameter names for GridSearch""""""\n        if not deep:\n            return super(MajorityVoteClassifier, self).get_params(deep=False)\n        else:\n            out = self.named_classifiers.copy()\n            for name, step in self.named_classifiers.items():\n                for key, value in step.get_params(deep=True).items():\n                    out[\'%s__%s\' % (name, key)] = value\n            return out\n\n\n\n# ## Using the majority voting principle to make predictions\n\n\n\n\niris = datasets.load_iris()\nX, y = iris.data[50:, [1, 2]], iris.target[50:]\nle = LabelEncoder()\ny = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test =       train_test_split(X, y, \n                        test_size=0.5, \n                        random_state=1,\n                        stratify=y)\n\n\n\n\n\nclf1 = LogisticRegression(penalty=\'l2\', \n                          C=0.001,\n                          solver=\'lbfgs\',\n                          random_state=1)\n\nclf2 = DecisionTreeClassifier(max_depth=1,\n                              criterion=\'entropy\',\n                              random_state=0)\n\nclf3 = KNeighborsClassifier(n_neighbors=1,\n                            p=2,\n                            metric=\'minkowski\')\n\npipe1 = Pipeline([[\'sc\', StandardScaler()],\n                  [\'clf\', clf1]])\npipe3 = Pipeline([[\'sc\', StandardScaler()],\n                  [\'clf\', clf3]])\n\nclf_labels = [\'Logistic regression\', \'Decision tree\', \'KNN\']\n\nprint(\'10-fold cross validation:\\n\')\nfor clf, label in zip([pipe1, clf2, pipe3], clf_labels):\n    scores = cross_val_score(estimator=clf,\n                             X=X_train,\n                             y=y_train,\n                             cv=10,\n                             scoring=\'roc_auc\')\n    print(""ROC AUC: %0.2f (+/- %0.2f) [%s]""\n          % (scores.mean(), scores.std(), label))\n\n\n\n\n# Majority Rule (hard) Voting\n\nmv_clf = MajorityVoteClassifier(classifiers=[pipe1, clf2, pipe3])\n\nclf_labels += [\'Majority voting\']\nall_clf = [pipe1, clf2, pipe3, mv_clf]\n\nfor clf, label in zip(all_clf, clf_labels):\n    scores = cross_val_score(estimator=clf,\n                             X=X_train,\n                             y=y_train,\n                             cv=10,\n                             scoring=\'roc_auc\')\n    print(""ROC AUC: %0.2f (+/- %0.2f) [%s]""\n          % (scores.mean(), scores.std(), label))\n\n\n\n# # Evaluating and tuning the ensemble classifier\n\n\n\n\ncolors = [\'black\', \'orange\', \'blue\', \'green\']\nlinestyles = [\':\', \'--\', \'-.\', \'-\']\nfor clf, label, clr, ls         in zip(all_clf,\n               clf_labels, colors, linestyles):\n\n    # assuming the label of the positive class is 1\n    y_pred = clf.fit(X_train,\n                     y_train).predict_proba(X_test)[:, 1]\n    fpr, tpr, thresholds = roc_curve(y_true=y_test,\n                                     y_score=y_pred)\n    roc_auc = auc(x=fpr, y=tpr)\n    plt.plot(fpr, tpr,\n             color=clr,\n             linestyle=ls,\n             label=\'%s (auc = %0.2f)\' % (label, roc_auc))\n\nplt.legend(loc=\'lower right\')\nplt.plot([0, 1], [0, 1],\n         linestyle=\'--\',\n         color=\'gray\',\n         linewidth=2)\n\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.grid(alpha=0.5)\nplt.xlabel(\'False positive rate (FPR)\')\nplt.ylabel(\'True positive rate (TPR)\')\n\n\n#plt.savefig(\'images/07_04\', dpi=300)\nplt.show()\n\n\n\n\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\n\n\n\n\n\nall_clf = [pipe1, clf2, pipe3, mv_clf]\n\nx_min = X_train_std[:, 0].min() - 1\nx_max = X_train_std[:, 0].max() + 1\ny_min = X_train_std[:, 1].min() - 1\ny_max = X_train_std[:, 1].max() + 1\n\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nf, axarr = plt.subplots(nrows=2, ncols=2, \n                        sharex=\'col\', \n                        sharey=\'row\', \n                        figsize=(7, 5))\n\nfor idx, clf, tt in zip(product([0, 1], [0, 1]),\n                        all_clf, clf_labels):\n    clf.fit(X_train_std, y_train)\n    \n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.3)\n    \n    axarr[idx[0], idx[1]].scatter(X_train_std[y_train==0, 0], \n                                  X_train_std[y_train==0, 1], \n                                  c=\'blue\', \n                                  marker=\'^\',\n                                  s=50)\n    \n    axarr[idx[0], idx[1]].scatter(X_train_std[y_train==1, 0], \n                                  X_train_std[y_train==1, 1], \n                                  c=\'green\', \n                                  marker=\'o\',\n                                  s=50)\n    \n    axarr[idx[0], idx[1]].set_title(tt)\n\nplt.text(-3.5, -5., \n         s=\'Sepal width [standardized]\', \n         ha=\'center\', va=\'center\', fontsize=12)\nplt.text(-12.5, 4.5, \n         s=\'Petal length [standardized]\', \n         ha=\'center\', va=\'center\', \n         fontsize=12, rotation=90)\n\n#plt.savefig(\'images/07_05\', dpi=300)\nplt.show()\n\n\n\n\nmv_clf.get_params()\n\n\n\n\n\n\nparams = {\'decisiontreeclassifier__max_depth\': [1, 2],\n          \'pipeline-1__clf__C\': [0.001, 0.1, 100.0]}\n\ngrid = GridSearchCV(estimator=mv_clf,\n                    param_grid=params,\n                    cv=10,\n                    iid=False,\n                    scoring=\'roc_auc\')\ngrid.fit(X_train, y_train)\n\nfor r, _ in enumerate(grid.cv_results_[\'mean_test_score\']):\n    print(""%0.3f +/- %0.2f %r""\n          % (grid.cv_results_[\'mean_test_score\'][r], \n             grid.cv_results_[\'std_test_score\'][r] / 2.0, \n             grid.cv_results_[\'params\'][r]))\n\n\n\n\nprint(\'Best parameters: %s\' % grid.best_params_)\nprint(\'Accuracy: %.2f\' % grid.best_score_)\n\n\n# **Note**  \n# By default, the default setting for `refit` in `GridSearchCV` is `True` (i.e., `GridSeachCV(..., refit=True)`), which means that we can use the fitted `GridSearchCV` estimator to make predictions via the `predict` method, for example:\n# \n#     grid = GridSearchCV(estimator=mv_clf, \n#                         param_grid=params, \n#                         cv=10, \n#                         scoring=\'roc_auc\')\n#     grid.fit(X_train, y_train)\n#     y_pred = grid.predict(X_test)\n# \n# In addition, the ""best"" estimator can directly be accessed via the `best_estimator_` attribute.\n\n\n\ngrid.best_estimator_.classifiers\n\n\n\n\nmv_clf = grid.best_estimator_\n\n\n\n\nmv_clf.set_params(**grid.best_estimator_.get_params())\n\n\n\n\nmv_clf\n\n\n\n# # Bagging -- Building an ensemble of classifiers from bootstrap samples\n\n\n\n\n\n# ## Bagging in a nutshell\n\n\n\n\n\n# ## Applying bagging to classify examples in the Wine dataset\n\n\n\n\ndf_wine = pd.read_csv(\'https://archive.ics.uci.edu/ml/\'\n                      \'machine-learning-databases/wine/wine.data\',\n                      header=None)\n\ndf_wine.columns = [\'Class label\', \'Alcohol\', \'Malic acid\', \'Ash\',\n                   \'Alcalinity of ash\', \'Magnesium\', \'Total phenols\',\n                   \'Flavanoids\', \'Nonflavanoid phenols\', \'Proanthocyanins\',\n                   \'Color intensity\', \'Hue\', \'OD280/OD315 of diluted wines\',\n                   \'Proline\']\n\n# if the Wine dataset is temporarily unavailable from the\n# UCI machine learning repository, un-comment the following line\n# of code to load the dataset from a local path:\n\n# df_wine = pd.read_csv(\'wine.data\', header=None)\n\n# drop 1 class\ndf_wine = df_wine[df_wine[\'Class label\'] != 1]\n\ny = df_wine[\'Class label\'].values\nX = df_wine[[\'Alcohol\', \'OD280/OD315 of diluted wines\']].values\n\n\n\n\n\n\nle = LabelEncoder()\ny = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test =            train_test_split(X, y, \n                             test_size=0.2, \n                             random_state=1,\n                             stratify=y)\n\n\n\n\n\ntree = DecisionTreeClassifier(criterion=\'entropy\', \n                              max_depth=None,\n                              random_state=1)\n\nbag = BaggingClassifier(base_estimator=tree,\n                        n_estimators=500, \n                        max_samples=1.0, \n                        max_features=1.0, \n                        bootstrap=True, \n                        bootstrap_features=False, \n                        n_jobs=1, \n                        random_state=1)\n\n\n\n\n\ntree = tree.fit(X_train, y_train)\ny_train_pred = tree.predict(X_train)\ny_test_pred = tree.predict(X_test)\n\ntree_train = accuracy_score(y_train, y_train_pred)\ntree_test = accuracy_score(y_test, y_test_pred)\nprint(\'Decision tree train/test accuracies %.3f/%.3f\'\n      % (tree_train, tree_test))\n\nbag = bag.fit(X_train, y_train)\ny_train_pred = bag.predict(X_train)\ny_test_pred = bag.predict(X_test)\n\nbag_train = accuracy_score(y_train, y_train_pred) \nbag_test = accuracy_score(y_test, y_test_pred) \nprint(\'Bagging train/test accuracies %.3f/%.3f\'\n      % (bag_train, bag_test))\n\n\n\n\n\nx_min = X_train[:, 0].min() - 1\nx_max = X_train[:, 0].max() + 1\ny_min = X_train[:, 1].min() - 1\ny_max = X_train[:, 1].max() + 1\n\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nf, axarr = plt.subplots(nrows=1, ncols=2, \n                        sharex=\'col\', \n                        sharey=\'row\', \n                        figsize=(8, 3))\n\n\nfor idx, clf, tt in zip([0, 1],\n                        [tree, bag],\n                        [\'Decision tree\', \'Bagging\']):\n    clf.fit(X_train, y_train)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    axarr[idx].contourf(xx, yy, Z, alpha=0.3)\n    axarr[idx].scatter(X_train[y_train == 0, 0],\n                       X_train[y_train == 0, 1],\n                       c=\'blue\', marker=\'^\')\n\n    axarr[idx].scatter(X_train[y_train == 1, 0],\n                       X_train[y_train == 1, 1],\n                       c=\'green\', marker=\'o\')\n\n    axarr[idx].set_title(tt)\n\naxarr[0].set_ylabel(\'Alcohol\', fontsize=12)\n\nplt.tight_layout()\nplt.text(0, -0.2,\n         s=\'OD280/OD315 of diluted wines\',\n         ha=\'center\',\n         va=\'center\',\n         fontsize=12,\n         transform=axarr[1].transAxes)\n\n#plt.savefig(\'images/07_08.png\', dpi=300, bbox_inches=\'tight\')\nplt.show()\n\n\n\n# # Leveraging weak learners via adaptive boosting\n\n# ## How boosting works\n\n\n\n\n\n\n\n\n\n# ## Applying AdaBoost using scikit-learn\n\n\n\n\ntree = DecisionTreeClassifier(criterion=\'entropy\', \n                              max_depth=1,\n                              random_state=1)\n\nada = AdaBoostClassifier(base_estimator=tree,\n                         n_estimators=500, \n                         learning_rate=0.1,\n                         random_state=1)\n\n\n\n\ntree = tree.fit(X_train, y_train)\ny_train_pred = tree.predict(X_train)\ny_test_pred = tree.predict(X_test)\n\ntree_train = accuracy_score(y_train, y_train_pred)\ntree_test = accuracy_score(y_test, y_test_pred)\nprint(\'Decision tree train/test accuracies %.3f/%.3f\'\n      % (tree_train, tree_test))\n\nada = ada.fit(X_train, y_train)\ny_train_pred = ada.predict(X_train)\ny_test_pred = ada.predict(X_test)\n\nada_train = accuracy_score(y_train, y_train_pred) \nada_test = accuracy_score(y_test, y_test_pred) \nprint(\'AdaBoost train/test accuracies %.3f/%.3f\'\n      % (ada_train, ada_test))\n\n\n\n\nx_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\ny_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nf, axarr = plt.subplots(1, 2, sharex=\'col\', sharey=\'row\', figsize=(8, 3))\n\n\nfor idx, clf, tt in zip([0, 1],\n                        [tree, ada],\n                        [\'Decision tree\', \'AdaBoost\']):\n    clf.fit(X_train, y_train)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    axarr[idx].contourf(xx, yy, Z, alpha=0.3)\n    axarr[idx].scatter(X_train[y_train == 0, 0],\n                       X_train[y_train == 0, 1],\n                       c=\'blue\', marker=\'^\')\n    axarr[idx].scatter(X_train[y_train == 1, 0],\n                       X_train[y_train == 1, 1],\n                       c=\'green\', marker=\'o\')\n    axarr[idx].set_title(tt)\n\naxarr[0].set_ylabel(\'Alcohol\', fontsize=12)\n\nplt.tight_layout()\nplt.text(0, -0.2,\n         s=\'OD280/OD315 of diluted wines\',\n         ha=\'center\',\n         va=\'center\',\n         fontsize=12,\n         transform=axarr[1].transAxes)\n\n#plt.savefig(\'images/07_11.png\', dpi=300, bbox_inches=\'tight\')\nplt.show()\n\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
ch08/ch08.py,0,"b'# coding: utf-8\n\n\nimport os\nimport sys\nimport tarfile\nimport time\nimport urllib.request\nimport pyprind\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nimport gzip\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom distutils.version import LooseVersion as Version\nfrom sklearn import __version__ as sklearn_version\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 8 - Applying Machine Learning To Sentiment Analysis\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this Jupyter extension via*  \n# \n#     conda install watermark -c conda-forge  \n# \n# or  \n# \n#     pip install watermark   \n# \n# *For more information, please see: https://github.com/rasbt/watermark.*\n\n\n# ### Overview\n\n# - [Preparing the IMDb movie review data for text processing](#Preparing-the-IMDb-movie-review-data-for-text-processing)\n#   - [Obtaining the IMDb movie review dataset](#Obtaining-the-IMDb-movie-review-dataset)\n#   - [Preprocessing the movie dataset into more convenient format](#Preprocessing-the-movie-dataset-into-more-convenient-format)\n# - [Introducing the bag-of-words model](#Introducing-the-bag-of-words-model)\n#   - [Transforming words into feature vectors](#Transforming-words-into-feature-vectors)\n#   - [Assessing word relevancy via term frequency-inverse document frequency](#Assessing-word-relevancy-via-term-frequency-inverse-document-frequency)\n#   - [Cleaning text data](#Cleaning-text-data)\n#   - [Processing documents into tokens](#Processing-documents-into-tokens)\n# - [Training a logistic regression model for document classification](#Training-a-logistic-regression-model-for-document-classification)\n# - [Working with bigger data \xe2\x80\x93 online algorithms and out-of-core learning](#Working-with-bigger-data-\xe2\x80\x93-online-algorithms-and-out-of-core-learning)\n# - [Topic modeling](#Topic-modeling)\n#   - [Decomposing text documents with Latent Dirichlet Allocation](#Decomposing-text-documents-with-Latent-Dirichlet-Allocation)\n#   - [Latent Dirichlet Allocation with scikit-learn](#Latent-Dirichlet-Allocation-with-scikit-learn)\n# - [Summary](#Summary)\n\n\n# # Preparing the IMDb movie review data for text processing \n\n# ## Obtaining the IMDb movie review dataset\n\n# The IMDB movie review set can be downloaded from [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/).\n# After downloading the dataset, decompress the files.\n# \n# A) If you are working with Linux or MacOS X, open a new terminal windowm `cd` into the download directory and execute \n# \n# `tar -zxf aclImdb_v1.tar.gz`\n# \n# B) If you are working with Windows, download an archiver such as [7Zip](http://www.7-zip.org) to extract the files from the download archive.\n\n# **Optional code to download and unzip the dataset via Python:**\n\n\n\n\n\nsource = \'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\'\ntarget = \'aclImdb_v1.tar.gz\'\n\n\ndef reporthook(count, block_size, total_size):\n    global start_time\n    if count == 0:\n        start_time = time.time()\n        return\n    duration = time.time() - start_time\n    progress_size = int(count * block_size)\n    speed = progress_size / (1024.**2 * duration)\n    percent = count * block_size * 100. / total_size\n\n    sys.stdout.write(""\\r%d%% | %d MB | %.2f MB/s | %d sec elapsed"" %\n                    (percent, progress_size / (1024.**2), speed, duration))\n    sys.stdout.flush()\n\n\nif not os.path.isdir(\'aclImdb\') and not os.path.isfile(\'aclImdb_v1.tar.gz\'):\n    urllib.request.urlretrieve(source, target, reporthook)\n\n\n\n\nif not os.path.isdir(\'aclImdb\'):\n\n    with tarfile.open(target, \'r:gz\') as tar:\n        tar.extractall()\n\n\n# ## Preprocessing the movie dataset into more convenient format\n\n\n\n\n# change the `basepath` to the directory of the\n# unzipped movie dataset\n\nbasepath = \'aclImdb\'\n\nlabels = {\'pos\': 1, \'neg\': 0}\npbar = pyprind.ProgBar(50000)\ndf = pd.DataFrame()\nfor s in (\'test\', \'train\'):\n    for l in (\'pos\', \'neg\'):\n        path = os.path.join(basepath, s, l)\n        for file in sorted(os.listdir(path)):\n            with open(os.path.join(path, file), \n                      \'r\', encoding=\'utf-8\') as infile:\n                txt = infile.read()\n            df = df.append([[txt, labels[l]]], \n                           ignore_index=True)\n            pbar.update()\ndf.columns = [\'review\', \'sentiment\']\n\n\n# Shuffling the DataFrame:\n\n\n\n\nnp.random.seed(0)\ndf = df.reindex(np.random.permutation(df.index))\n\n\n# Optional: Saving the assembled data as CSV file:\n\n\n\ndf.to_csv(\'movie_data.csv\', index=False, encoding=\'utf-8\')\n\n\n\n\n\ndf = pd.read_csv(\'movie_data.csv\', encoding=\'utf-8\')\ndf.head(3)\n\n\n\n\ndf.shape\n\n\n# ---\n# \n# ### Note\n# \n# If you have problems with creating the `movie_data.csv`, you can find a download a zip archive at \n# https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/code/ch08/\n# \n# ---\n\n\n# # Introducing the bag-of-words model\n\n# ...\n\n# ## Transforming documents into feature vectors\n\n# By calling the fit_transform method on CountVectorizer, we just constructed the vocabulary of the bag-of-words model and transformed the following three sentences into sparse feature vectors:\n# 1. The sun is shining\n# 2. The weather is sweet\n# 3. The sun is shining, the weather is sweet, and one and one is two\n# \n\n\n\n\ncount = CountVectorizer()\ndocs = np.array([\n        \'The sun is shining\',\n        \'The weather is sweet\',\n        \'The sun is shining, the weather is sweet, and one and one is two\'])\nbag = count.fit_transform(docs)\n\n\n# Now let us print the contents of the vocabulary to get a better understanding of the underlying concepts:\n\n\n\nprint(count.vocabulary_)\n\n\n# As we can see from executing the preceding command, the vocabulary is stored in a Python dictionary, which maps the unique words that are mapped to integer indices. Next let us print the feature vectors that we just created:\n\n# Each index position in the feature vectors shown here corresponds to the integer values that are stored as dictionary items in the CountVectorizer vocabulary. For example, the  rst feature at index position 0 resembles the count of the word and, which only occurs in the last document, and the word is at index position 1 (the 2nd feature in the document vectors) occurs in all three sentences. Those values in the feature vectors are also called the raw term frequencies: *tf (t,d)*\xe2\x80\x94the number of times a term t occurs in a document *d*.\n\n\n\nprint(bag.toarray())\n\n\n\n# ## Assessing word relevancy via term frequency-inverse document frequency\n\n\n\nnp.set_printoptions(precision=2)\n\n\n# When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. Those frequently occurring words typically don\'t contain useful or discriminatory information. In this subsection, we will learn about a useful technique called term frequency-inverse document frequency (tf-idf) that can be used to downweight those frequently occurring words in the feature vectors. The tf-idf can be de ned as the product of the term frequency and the inverse document frequency:\n# \n# $$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$\n# \n# Here the tf(t, d) is the term frequency that we introduced in the previous section,\n# and the inverse document frequency *idf(t, d)* can be calculated as:\n# \n# $$\\text{idf}(t,d) = \\text{log}\\frac{n_d}{1+\\text{df}(d, t)},$$\n# \n# where $n_d$ is the total number of documents, and *df(d, t)* is the number of documents *d* that contain the term *t*. Note that adding the constant 1 to the denominator is optional and serves the purpose of assigning a non-zero value to terms that occur in all training examples; the log is used to ensure that low document frequencies are not given too much weight.\n# \n# Scikit-learn implements yet another transformer, the `TfidfTransformer`, that takes the raw term frequencies from `CountVectorizer` as input and transforms them into tf-idfs:\n\n\n\n\ntfidf = TfidfTransformer(use_idf=True, \n                         norm=\'l2\', \n                         smooth_idf=True)\nprint(tfidf.fit_transform(count.fit_transform(docs))\n      .toarray())\n\n\n# As we saw in the previous subsection, the word is had the largest term frequency in the 3rd document, being the most frequently occurring word. However, after transforming the same feature vector into tf-idfs, we see that the word is is\n# now associated with a relatively small tf-idf (0.45) in document 3 since it is\n# also contained in documents 1 and 2 and thus is unlikely to contain any useful, discriminatory information.\n# \n\n# However, if we\'d manually calculated the tf-idfs of the individual terms in our feature vectors, we\'d have noticed that the `TfidfTransformer` calculates the tf-idfs slightly differently compared to the standard textbook equations that we de ned earlier. The equations for the idf and tf-idf that were implemented in scikit-learn are:\n\n# $$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$\n# \n# The tf-idf equation that was implemented in scikit-learn is as follows:\n# \n# $$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$\n# \n# While it is also more typical to normalize the raw term frequencies before calculating the tf-idfs, the `TfidfTransformer` normalizes the tf-idfs directly.\n# \n# By default (`norm=\'l2\'`), scikit-learn\'s TfidfTransformer applies the L2-normalization, which returns a vector of length 1 by dividing an un-normalized feature vector *v* by its L2-norm:\n# \n# $$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$\n# \n# To make sure that we understand how TfidfTransformer works, let us walk\n# through an example and calculate the tf-idf of the word is in the 3rd document.\n# \n# The word is has a term frequency of 3 (tf = 3) in document 3 ($d_3$), and the document frequency of this term is 3 since the term is occurs in all three documents (df = 3). Thus, we can calculate the idf as follows:\n# \n# $$\\text{idf}(""is"", d_3) = log \\frac{1+3}{1+3} = 0$$\n# \n# Now in order to calculate the tf-idf, we simply need to add 1 to the inverse document frequency and multiply it by the term frequency:\n# \n# $$\\text{tf-idf}(""is"", d_3)= 3 \\times (0+1) = 3$$\n\n\n\ntf_is = 3\nn_docs = 3\nidf_is = np.log((n_docs+1) / (3+1))\ntfidf_is = tf_is * (idf_is + 1)\nprint(\'tf-idf of term ""is"" = %.2f\' % tfidf_is)\n\n\n# If we repeated these calculations for all terms in the 3rd document, we\'d obtain the following tf-idf vectors: [3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]. However, we notice that the values in this feature vector are different from the values that we obtained from the TfidfTransformer that we used previously. The  nal step that we are missing in this tf-idf calculation is the L2-normalization, which can be applied as follows:\n\n# $$\\text{tfi-df}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]}{\\sqrt{[3.39^2, 3.0^2, 3.39^2, 1.29^2, 1.29^2, 1.29^2, 2.0^2 , 1.69^2, 1.29^2]}}$$\n# \n# $$=[0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$\n# \n# $$\\Rightarrow \\text{tfi-df}_{norm}(""is"", d3) = 0.45$$\n\n# As we can see, the results match the results returned by scikit-learn\'s `TfidfTransformer` (below). Since we now understand how tf-idfs are calculated, let us proceed to the next sections and apply those concepts to the movie review dataset.\n\n\n\ntfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\nraw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1]\nraw_tfidf \n\n\n\n\nl2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\nl2_tfidf\n\n\n\n# ## Cleaning text data\n\n\n\ndf.loc[0, \'review\'][-50:]\n\n\n\n\ndef preprocessor(text):\n    text = re.sub(\'<[^>]*>\', \'\', text)\n    emoticons = re.findall(\'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\',\n                           text)\n    text = (re.sub(\'[\\W]+\', \' \', text.lower()) +\n            \' \'.join(emoticons).replace(\'-\', \'\'))\n    return text\n\n\n\n\npreprocessor(df.loc[0, \'review\'][-50:])\n\n\n\n\npreprocessor(""</a>This :) is :( a test :-)!"")\n\n\n\n\ndf[\'review\'] = df[\'review\'].apply(preprocessor)\n\n\n\n# ## Processing documents into tokens\n\n\n\n\nporter = PorterStemmer()\n\ndef tokenizer(text):\n    return text.split()\n\n\ndef tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]\n\n\n\n\ntokenizer(\'runners like running and thus they run\')\n\n\n\n\ntokenizer_porter(\'runners like running and thus they run\')\n\n\n\n\n\nnltk.download(\'stopwords\')\n\n\n\n\n\nstop = stopwords.words(\'english\')\n[w for w in tokenizer_porter(\'a runner likes running and runs a lot\')[-10:]\nif w not in stop]\n\n\n\n# # Training a logistic regression model for document classification\n\n# Strip HTML and punctuation to speed up the GridSearch later:\n\n\n\nX_train = df.loc[:25000, \'review\'].values\ny_train = df.loc[:25000, \'sentiment\'].values\nX_test = df.loc[25000:, \'review\'].values\ny_test = df.loc[25000:, \'sentiment\'].values\n\n\n\n\n\ntfidf = TfidfVectorizer(strip_accents=None,\n                        lowercase=False,\n                        preprocessor=None)\n\nparam_grid = [{\'vect__ngram_range\': [(1, 1)],\n               \'vect__stop_words\': [stop, None],\n               \'vect__tokenizer\': [tokenizer, tokenizer_porter],\n               \'clf__penalty\': [\'l1\', \'l2\'],\n               \'clf__C\': [1.0, 10.0, 100.0]},\n              {\'vect__ngram_range\': [(1, 1)],\n               \'vect__stop_words\': [stop, None],\n               \'vect__tokenizer\': [tokenizer, tokenizer_porter],\n               \'vect__use_idf\':[False],\n               \'vect__norm\':[None],\n               \'clf__penalty\': [\'l1\', \'l2\'],\n               \'clf__C\': [1.0, 10.0, 100.0]},\n              ]\n\nlr_tfidf = Pipeline([(\'vect\', tfidf),\n                     (\'clf\', LogisticRegression(random_state=0, solver=\'liblinear\'))])\n\ngs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n                           scoring=\'accuracy\',\n                           cv=5,\n                           verbose=2,\n                           n_jobs=-1)\n\n\n# **Important Note about `n_jobs`**\n# \n# Please note that it is highly recommended to use `n_jobs=-1` (instead of `n_jobs=1`) in the previous code example to utilize all available cores on your machine and speed up the grid search. However, some Windows users reported issues when running the previous code with the `n_jobs=-1` setting related to pickling the tokenizer and tokenizer_porter functions for multiprocessing on Windows. Another workaround would be to replace those two functions, `[tokenizer, tokenizer_porter]`, with `[str.split]`. However, note that the replacement by the simple `str.split` would not support stemming.\n\n# **Important Note about the running time**\n# \n# Executing the following code cell **may take up to 30-60 min** depending on your machine, since based on the parameter grid we defined, there are `2*2*2*3*5` + `2*2*2*3*5` = 240 models to fit.\n# \n# If you do not wish to wait so long, you could reduce the size of the dataset by decreasing the number of training examples, for example, as follows:\n# \n#     X_train = df.loc[:2500, \'review\'].values\n#     y_train = df.loc[:2500, \'sentiment\'].values\n#     \n# However, note that decreasing the training set size to such a small number will likely result in poorly performing models. Alternatively, you can delete parameters from the grid above to reduce the number of models to fit -- for example, by using the following:\n# \n#     param_grid = [{\'vect__ngram_range\': [(1, 1)],\n#                    \'vect__stop_words\': [stop, None],\n#                    \'vect__tokenizer\': [tokenizer],\n#                    \'clf__penalty\': [\'l1\', \'l2\'],\n#                    \'clf__C\': [1.0, 10.0]},\n#                   ]\n\n\n\ngs_lr_tfidf.fit(X_train, y_train)\n\n\n\n\nprint(\'Best parameter set: %s \' % gs_lr_tfidf.best_params_)\nprint(\'CV Accuracy: %.3f\' % gs_lr_tfidf.best_score_)\n\n\n\n\nclf = gs_lr_tfidf.best_estimator_\nprint(\'Test Accuracy: %.3f\' % clf.score(X_test, y_test))\n\n\n\n# ####  Start comment:\n#     \n# Please note that `gs_lr_tfidf.best_score_` is the average k-fold cross-validation score. I.e., if we have a `GridSearchCV` object with 5-fold cross-validation (like the one above), the `best_score_` attribute returns the average score over the 5-folds of the best model. To illustrate this with an example:\n\n\n\n\n\nnp.random.seed(0)\nnp.set_printoptions(precision=6)\ny = [np.random.randint(3) for i in range(25)]\nX = (y + np.random.randn(25)).reshape(-1, 1)\n\ncv5_idx = list(StratifiedKFold(n_splits=5, shuffle=False, random_state=0).split(X, y))\n    \nlr = LogisticRegression(random_state=123, multi_class=\'ovr\', solver=\'lbfgs\')\ncross_val_score(lr, X, y, cv=cv5_idx)\n\n\n# By executing the code above, we created a simple data set of random integers that shall represent our class labels. Next, we fed the indices of 5 cross-validation folds (`cv3_idx`) to the `cross_val_score` scorer, which returned 5 accuracy scores -- these are the 5 accuracy values for the 5 test folds.  \n# \n# Next, let us use the `GridSearchCV` object and feed it the same 5 cross-validation sets (via the pre-generated `cv3_idx` indices):\n\n\n\n\nlr = LogisticRegression(solver=\'lbfgs\', multi_class=\'ovr\', random_state=1)\ngs = GridSearchCV(lr, {}, cv=cv5_idx, verbose=3).fit(X, y) \n\n\n# As we can see, the scores for the 5 folds are exactly the same as the ones from `cross_val_score` earlier.\n\n# Now, the best_score_ attribute of the `GridSearchCV` object, which becomes available after `fit`ting, returns the average accuracy score of the best model:\n\n\n\ngs.best_score_\n\n\n# As we can see, the result above is consistent with the average score computed the `cross_val_score`.\n\n\n\nlr = LogisticRegression(solver=\'lbfgs\', multi_class=\'ovr\', random_state=1)\ncross_val_score(lr, X, y, cv=cv5_idx).mean()\n\n\n# #### End comment.\n# \n\n\n# # Working with bigger data - online algorithms and out-of-core learning\n\n\n\n# This cell is not contained in the book but\n# added for convenience so that the notebook\n# can be executed starting here, without\n# executing prior code in this notebook\n\n\n\nif not os.path.isfile(\'movie_data.csv\'):\n    if not os.path.isfile(\'movie_data.csv.gz\'):\n        print(\'Please place a copy of the movie_data.csv.gz\'\n              \'in this directory. You can obtain it by\'\n              \'a) executing the code in the beginning of this\'\n              \'notebook or b) by downloading it from GitHub:\'\n              \'https://github.com/rasbt/python-machine-learning-\'\n              \'book-2nd-edition/blob/master/code/ch08/movie_data.csv.gz\')\n    else:\n        with gzip.open(\'movie_data.csv.gz\', \'rb\') as in_f,                 open(\'movie_data.csv\', \'wb\') as out_f:\n            out_f.write(in_f.read())\n\n\n\n\n\n\n# The `stop` is defined as earlier in this chapter\n# Added it here for convenience, so that this section\n# can be run as standalone without executing prior code\n# in the directory\nstop = stopwords.words(\'english\')\n\n\ndef tokenizer(text):\n    text = re.sub(\'<[^>]*>\', \'\', text)\n    emoticons = re.findall(\'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\', text.lower())\n    text = re.sub(\'[\\W]+\', \' \', text.lower()) +        \' \'.join(emoticons).replace(\'-\', \'\')\n    tokenized = [w for w in text.split() if w not in stop]\n    return tokenized\n\n\ndef stream_docs(path):\n    with open(path, \'r\', encoding=\'utf-8\') as csv:\n        next(csv)  # skip header\n        for line in csv:\n            text, label = line[:-3], int(line[-2])\n            yield text, label\n\n\n\n\nnext(stream_docs(path=\'movie_data.csv\'))\n\n\n\n\ndef get_minibatch(doc_stream, size):\n    docs, y = [], []\n    try:\n        for _ in range(size):\n            text, label = next(doc_stream)\n            docs.append(text)\n            y.append(label)\n    except StopIteration:\n        return None, None\n    return docs, y\n\n\n\n\n\n\nvect = HashingVectorizer(decode_error=\'ignore\', \n                         n_features=2**21,\n                         preprocessor=None, \n                         tokenizer=tokenizer)\n\n\n\n\n\nclf = SGDClassifier(loss=\'log\', random_state=1)\n\n\ndoc_stream = stream_docs(path=\'movie_data.csv\')\n\n\n\n\npbar = pyprind.ProgBar(45)\n\nclasses = np.array([0, 1])\nfor _ in range(45):\n    X_train, y_train = get_minibatch(doc_stream, size=1000)\n    if not X_train:\n        break\n    X_train = vect.transform(X_train)\n    clf.partial_fit(X_train, y_train, classes=classes)\n    pbar.update()\n\n\n\n\nX_test, y_test = get_minibatch(doc_stream, size=5000)\nX_test = vect.transform(X_test)\nprint(\'Accuracy: %.3f\' % clf.score(X_test, y_test))\n\n\n\n\nclf = clf.partial_fit(X_test, y_test)\n\n\n# ## Topic modeling\n\n# ### Decomposing text documents with Latent Dirichlet Allocation\n\n# ### Latent Dirichlet Allocation with scikit-learn\n\n\n\n\ndf = pd.read_csv(\'movie_data.csv\', encoding=\'utf-8\')\ndf.head(3)\n\n\n\n\n\ncount = CountVectorizer(stop_words=\'english\',\n                        max_df=.1,\n                        max_features=5000)\nX = count.fit_transform(df[\'review\'].values)\n\n\n\n\n\nlda = LatentDirichletAllocation(n_components=10,\n                                random_state=123,\n                                learning_method=\'batch\')\nX_topics = lda.fit_transform(X)\n\n\n\n\nlda.components_.shape\n\n\n\n\nn_top_words = 5\nfeature_names = count.get_feature_names()\n\nfor topic_idx, topic in enumerate(lda.components_):\n    print(""Topic %d:"" % (topic_idx + 1))\n    print("" "".join([feature_names[i]\n                    for i in topic.argsort()\\\n                        [:-n_top_words - 1:-1]]))\n\n\n# Based on reading the 5 most important words for each topic, we may guess that the LDA identified the following topics:\n#     \n# 1. Generally bad movies (not really a topic category)\n# 2. Movies about families\n# 3. War movies\n# 4. Art movies\n# 5. Crime movies\n# 6. Horror movies\n# 7. Comedies\n# 8. Movies somehow related to TV shows\n# 9. Movies based on books\n# 10. Action movies\n\n# To confirm that the categories make sense based on the reviews, let\'s plot 5 movies from the horror movie category (category 6 at index position 5):\n\n\n\nhorror = X_topics[:, 5].argsort()[::-1]\n\nfor iter_idx, movie_idx in enumerate(horror[:3]):\n    print(\'\\nHorror movie #%d:\' % (iter_idx + 1))\n    print(df[\'review\'][movie_idx][:300], \'...\')\n\n\n# Using the preceeding code example, we printed the first 300 characters from the top 3 horror movies and indeed, we can see that the reviews -- even though we don\'t know which exact movie they belong to -- sound like reviews of horror movies, indeed. (However, one might argue that movie #2 could also belong to topic category 1.)\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
ch10/ch10.py,0,"b'# coding: utf-8\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mlxtend.plotting import scatterplotmatrix\nimport numpy as np\nfrom mlxtend.plotting import heatmap\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import RANSACRegressor\nfrom sklearn.model_selection import train_test_split\nimport scipy as sp\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 10 - Predicting Continuous Target Variables with Regression Analysis\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*\n\n# The mlxtend package (http://rasbt.github.io/mlxtend/), which contains a few useful functions on top of scikit-learn and matplotloib, can be installed via \n# \n#     conda install mlxtend\n# \n# or \n# \n#     pip install mlxtend\n\n\n# ### Overview\n\n# - [Introducing regression](#Introducing-linear-regression)\n#   - [Simple linear regression](#Simple-linear-regression)\n# - [Exploring the Housing Dataset](#Exploring-the-Housing-Dataset)\n#   - [Loading the Housing dataset into a data frame](Loading-the-Housing-dataset-into-a-data-frame)\n#   - [Visualizing the important characteristics of a dataset](#Visualizing-the-important-characteristics-of-a-dataset)\n# - [Implementing an ordinary least squares linear regression model](#Implementing-an-ordinary-least-squares-linear-regression-model)\n#   - [Solving regression for regression parameters with gradient descent](#Solving-regression-for-regression-parameters-with-gradient-descent)\n#   - [Estimating the coefficient of a regression model via scikit-learn](#Estimating-the-coefficient-of-a-regression-model-via-scikit-learn)\n# - [Fitting a robust regression model using RANSAC](#Fitting-a-robust-regression-model-using-RANSAC)\n# - [Evaluating the performance of linear regression models](#Evaluating-the-performance-of-linear-regression-models)\n# - [Using regularized methods for regression](#Using-regularized-methods-for-regression)\n# - [Turning a linear regression model into a curve - polynomial regression](#Turning-a-linear-regression-model-into-a-curve---polynomial-regression)\n#   - [Modeling nonlinear relationships in the Housing Dataset](#Modeling-nonlinear-relationships-in-the-Housing-Dataset)\n#   - [Dealing with nonlinear relationships using random forests](#Dealing-with-nonlinear-relationships-using-random-forests)\n#     - [Decision tree regression](#Decision-tree-regression)\n#     - [Random forest regression](#Random-forest-regression)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Introducing linear regression\n\n# ## Simple linear regression\n\n\n\n\n\n# ## Multiple linear regression\n\n\n\n\n\n\n# # Exploring the Housing dataset\n\n# ## Loading the Housing dataset into a data frame\n\n# Description, which was previously available at: [https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)\n# \n# Attributes:\n#     \n# <pre>\n# 1. CRIM      per capita crime rate by town\n# 2. ZN        proportion of residential land zoned for lots over \n#                  25,000 sq.ft.\n# 3. INDUS     proportion of non-retail business acres per town\n# 4. CHAS      Charles River dummy variable (= 1 if tract bounds \n#                  river; 0 otherwise)\n# 5. NOX       nitric oxides concentration (parts per 10 million)\n# 6. RM        average number of rooms per dwelling\n# 7. AGE       proportion of owner-occupied units built prior to 1940\n# 8. DIS       weighted distances to five Boston employment centres\n# 9. RAD       index of accessibility to radial highways\n# 10. TAX      full-value property-tax rate per $10,000\n# 11. PTRATIO  pupil-teacher ratio by town\n# 12. B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks \n#                  by town\n# 13. LSTAT    % lower status of the population\n# 14. MEDV     Median value of owner-occupied homes in $1000s\n# </pre>\n\n\n\n\ndf = pd.read_csv(\'https://raw.githubusercontent.com/rasbt/\'\n                 \'python-machine-learning-book-3rd-edition/\'\n                 \'master/ch10/housing.data.txt\',\n                 header=None,\n                 sep=\'\\s+\')\n\ndf.columns = [\'CRIM\', \'ZN\', \'INDUS\', \'CHAS\', \n              \'NOX\', \'RM\', \'AGE\', \'DIS\', \'RAD\', \n              \'TAX\', \'PTRATIO\', \'B\', \'LSTAT\', \'MEDV\']\ndf.head()\n\n\n# \n# ### Note:\n# \n# \n# You can find a copy of the housing dataset (and all other datasets used in this book) in the code bundle of this book, which you can use if you are working offline or the UCI server at https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data is temporarily unavailable. For instance, to load the housing dataset from a local directory, you can replace the lines\n# df = pd.read_csv(\'https://archive.ics.uci.edu/ml/\'\n#                  \'machine-learning-databases\'\n#                  \'/housing/housing.data\',\n#                  sep=\'\\s+\')\n# in the following code example by \n# df = pd.read_csv(\'./housing.data\',\n#                  sep=\'\\s+\')\n\n\n# ## Visualizing the important characteristics of a dataset\n\n\n\n\n\n\n\ncols = [\'LSTAT\', \'INDUS\', \'NOX\', \'RM\', \'MEDV\']\n\nscatterplotmatrix(df[cols].values, figsize=(10, 8), \n                  names=cols, alpha=0.5)\nplt.tight_layout()\n#plt.savefig(\'images/10_03.png\', dpi=300)\nplt.show()\n\n\n\n\n\n\ncm = np.corrcoef(df[cols].values.T)\nhm = heatmap(cm, row_names=cols, column_names=cols)\n\n# plt.savefig(\'images/10_04.png\', dpi=300)\nplt.show()\n\n\n\n# # Implementing an ordinary least squares linear regression model\n\n# ...\n\n# ## Solving regression for regression parameters with gradient descent\n\n\n\nclass LinearRegressionGD(object):\n\n    def __init__(self, eta=0.001, n_iter=20):\n        self.eta = eta\n        self.n_iter = n_iter\n\n    def fit(self, X, y):\n        self.w_ = np.zeros(1 + X.shape[1])\n        self.cost_ = []\n\n        for i in range(self.n_iter):\n            output = self.net_input(X)\n            errors = (y - output)\n            self.w_[1:] += self.eta * X.T.dot(errors)\n            self.w_[0] += self.eta * errors.sum()\n            cost = (errors**2).sum() / 2.0\n            self.cost_.append(cost)\n        return self\n\n    def net_input(self, X):\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def predict(self, X):\n        return self.net_input(X)\n\n\n\n\nX = df[[\'RM\']].values\ny = df[\'MEDV\'].values\n\n\n\n\n\n\nsc_x = StandardScaler()\nsc_y = StandardScaler()\nX_std = sc_x.fit_transform(X)\ny_std = sc_y.fit_transform(y[:, np.newaxis]).flatten()\n\n\n\n\nlr = LinearRegressionGD()\nlr.fit(X_std, y_std)\n\n\n\n\nplt.plot(range(1, lr.n_iter+1), lr.cost_)\nplt.ylabel(\'SSE\')\nplt.xlabel(\'Epoch\')\n#plt.tight_layout()\n#plt.savefig(\'images/10_05.png\', dpi=300)\nplt.show()\n\n\n\n\ndef lin_regplot(X, y, model):\n    plt.scatter(X, y, c=\'steelblue\', edgecolor=\'white\', s=70)\n    plt.plot(X, model.predict(X), color=\'black\', lw=2)    \n    return \n\n\n\n\nlin_regplot(X_std, y_std, lr)\nplt.xlabel(\'Average number of rooms [RM] (standardized)\')\nplt.ylabel(\'Price in $1000s [MEDV] (standardized)\')\n\n#plt.savefig(\'images/10_06.png\', dpi=300)\nplt.show()\n\n\n\n\nprint(\'Slope: %.3f\' % lr.w_[1])\nprint(\'Intercept: %.3f\' % lr.w_[0])\n\n\n\n\nnum_rooms_std = sc_x.transform(np.array([[5.0]]))\nprice_std = lr.predict(num_rooms_std)\nprint(""Price in $1000s: %.3f"" % sc_y.inverse_transform(price_std))\n\n\n\n# ## Estimating the coefficient of a regression model via scikit-learn\n\n\n\n\n\n\n\nslr = LinearRegression()\nslr.fit(X, y)\ny_pred = slr.predict(X)\nprint(\'Slope: %.3f\' % slr.coef_[0])\nprint(\'Intercept: %.3f\' % slr.intercept_)\n\n\n\n\nlin_regplot(X, y, slr)\nplt.xlabel(\'Average number of rooms [RM]\')\nplt.ylabel(\'Price in $1000s [MEDV]\')\n\n#plt.savefig(\'images/10_07.png\', dpi=300)\nplt.show()\n\n\n# **Normal Equations** alternative:\n\n\n\n# adding a column vector of ""ones""\nXb = np.hstack((np.ones((X.shape[0], 1)), X))\nw = np.zeros(X.shape[1])\nz = np.linalg.inv(np.dot(Xb.T, Xb))\nw = np.dot(z, np.dot(Xb.T, y))\n\nprint(\'Slope: %.3f\' % w[1])\nprint(\'Intercept: %.3f\' % w[0])\n\n\n\n# # Fitting a robust regression model using RANSAC\n\n\n\n\nransac = RANSACRegressor(LinearRegression(), \n                         max_trials=100, \n                         min_samples=50, \n                         loss=\'absolute_loss\', \n                         residual_threshold=5.0, \n                         random_state=0)\n\n\nransac.fit(X, y)\n\ninlier_mask = ransac.inlier_mask_\noutlier_mask = np.logical_not(inlier_mask)\n\nline_X = np.arange(3, 10, 1)\nline_y_ransac = ransac.predict(line_X[:, np.newaxis])\nplt.scatter(X[inlier_mask], y[inlier_mask],\n            c=\'steelblue\', edgecolor=\'white\', \n            marker=\'o\', label=\'Inliers\')\nplt.scatter(X[outlier_mask], y[outlier_mask],\n            c=\'limegreen\', edgecolor=\'white\', \n            marker=\'s\', label=\'Outliers\')\nplt.plot(line_X, line_y_ransac, color=\'black\', lw=2)   \nplt.xlabel(\'Average number of rooms [RM]\')\nplt.ylabel(\'Price in $1000s [MEDV]\')\nplt.legend(loc=\'upper left\')\n\n#plt.savefig(\'images/10_08.png\', dpi=300)\nplt.show()\n\n\n\n\nprint(\'Slope: %.3f\' % ransac.estimator_.coef_[0])\nprint(\'Intercept: %.3f\' % ransac.estimator_.intercept_)\n\n\n\n# # Evaluating the performance of linear regression models\n\n\n\n\nX = df.iloc[:, :-1].values\ny = df[\'MEDV\'].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0)\n\n\n\n\nslr = LinearRegression()\n\nslr.fit(X_train, y_train)\ny_train_pred = slr.predict(X_train)\ny_test_pred = slr.predict(X_test)\n\n\n\n\n\nary = np.array(range(100000))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.scatter(y_train_pred,  y_train_pred - y_train,\n            c=\'steelblue\', marker=\'o\', edgecolor=\'white\',\n            label=\'Training data\')\nplt.scatter(y_test_pred,  y_test_pred - y_test,\n            c=\'limegreen\', marker=\'s\', edgecolor=\'white\',\n            label=\'Test data\')\nplt.xlabel(\'Predicted values\')\nplt.ylabel(\'Residuals\')\nplt.legend(loc=\'upper left\')\nplt.hlines(y=0, xmin=-10, xmax=50, color=\'black\', lw=2)\nplt.xlim([-10, 50])\nplt.tight_layout()\n\n# plt.savefig(\'images/10_09.png\', dpi=300)\nplt.show()\n\n\n\n\n\nprint(\'MSE train: %.3f, test: %.3f\' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint(\'R^2 train: %.3f, test: %.3f\' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))\n\n\n\n# # Using regularized methods for regression\n\n\n\n\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\ny_train_pred = lasso.predict(X_train)\ny_test_pred = lasso.predict(X_test)\nprint(lasso.coef_)\n\n\n\n\nprint(\'MSE train: %.3f, test: %.3f\' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint(\'R^2 train: %.3f, test: %.3f\' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))\n\n\n# Ridge regression:\n\n\n\nridge = Ridge(alpha=1.0)\n\n\n# LASSO regression:\n\n\n\nlasso = Lasso(alpha=1.0)\n\n\n# Elastic Net regression:\n\n\n\nelanet = ElasticNet(alpha=1.0, l1_ratio=0.5)\n\n\n\n# # Turning a linear regression model into a curve - polynomial regression\n\n\n\nX = np.array([258.0, 270.0, 294.0, \n              320.0, 342.0, 368.0, \n              396.0, 446.0, 480.0, 586.0])\\\n             [:, np.newaxis]\n\ny = np.array([236.4, 234.4, 252.8, \n              298.6, 314.2, 342.2, \n              360.8, 368.0, 391.2,\n              390.8])\n\n\n\n\n\nlr = LinearRegression()\npr = LinearRegression()\nquadratic = PolynomialFeatures(degree=2)\nX_quad = quadratic.fit_transform(X)\n\n\n\n\n# fit linear features\nlr.fit(X, y)\nX_fit = np.arange(250, 600, 10)[:, np.newaxis]\ny_lin_fit = lr.predict(X_fit)\n\n# fit quadratic features\npr.fit(X_quad, y)\ny_quad_fit = pr.predict(quadratic.fit_transform(X_fit))\n\n# plot results\nplt.scatter(X, y, label=\'Training points\')\nplt.plot(X_fit, y_lin_fit, label=\'Linear fit\', linestyle=\'--\')\nplt.plot(X_fit, y_quad_fit, label=\'Quadratic fit\')\nplt.xlabel(\'Explanatory variable\')\nplt.ylabel(\'Predicted or known target values\')\nplt.legend(loc=\'upper left\')\n\nplt.tight_layout()\n#plt.savefig(\'images/10_11.png\', dpi=300)\nplt.show()\n\n\n\n\ny_lin_pred = lr.predict(X)\ny_quad_pred = pr.predict(X_quad)\n\n\n\n\nprint(\'Training MSE linear: %.3f, quadratic: %.3f\' % (\n        mean_squared_error(y, y_lin_pred),\n        mean_squared_error(y, y_quad_pred)))\nprint(\'Training R^2 linear: %.3f, quadratic: %.3f\' % (\n        r2_score(y, y_lin_pred),\n        r2_score(y, y_quad_pred)))\n\n\n\n# ## Modeling nonlinear relationships in the Housing Dataset\n\n\n\nX = df[[\'LSTAT\']].values\ny = df[\'MEDV\'].values\n\nregr = LinearRegression()\n\n# create quadratic features\nquadratic = PolynomialFeatures(degree=2)\ncubic = PolynomialFeatures(degree=3)\nX_quad = quadratic.fit_transform(X)\nX_cubic = cubic.fit_transform(X)\n\n# fit features\nX_fit = np.arange(X.min(), X.max(), 1)[:, np.newaxis]\n\nregr = regr.fit(X, y)\ny_lin_fit = regr.predict(X_fit)\nlinear_r2 = r2_score(y, regr.predict(X))\n\nregr = regr.fit(X_quad, y)\ny_quad_fit = regr.predict(quadratic.fit_transform(X_fit))\nquadratic_r2 = r2_score(y, regr.predict(X_quad))\n\nregr = regr.fit(X_cubic, y)\ny_cubic_fit = regr.predict(cubic.fit_transform(X_fit))\ncubic_r2 = r2_score(y, regr.predict(X_cubic))\n\n\n# plot results\nplt.scatter(X, y, label=\'Training points\', color=\'lightgray\')\n\nplt.plot(X_fit, y_lin_fit, \n         label=\'Linear (d=1), $R^2=%.2f$\' % linear_r2, \n         color=\'blue\', \n         lw=2, \n         linestyle=\':\')\n\nplt.plot(X_fit, y_quad_fit, \n         label=\'Quadratic (d=2), $R^2=%.2f$\' % quadratic_r2,\n         color=\'red\', \n         lw=2,\n         linestyle=\'-\')\n\nplt.plot(X_fit, y_cubic_fit, \n         label=\'Cubic (d=3), $R^2=%.2f$\' % cubic_r2,\n         color=\'green\', \n         lw=2, \n         linestyle=\'--\')\n\nplt.xlabel(\'% lower status of the population [LSTAT]\')\nplt.ylabel(\'Price in $1000s [MEDV]\')\nplt.legend(loc=\'upper right\')\n\n#plt.savefig(\'images/10_12.png\', dpi=300)\nplt.show()\n\n\n# Transforming the dataset:\n\n\n\nX = df[[\'LSTAT\']].values\ny = df[\'MEDV\'].values\n\n# transform features\nX_log = np.log(X)\ny_sqrt = np.sqrt(y)\n\n# fit features\nX_fit = np.arange(X_log.min()-1, X_log.max()+1, 1)[:, np.newaxis]\n\nregr = regr.fit(X_log, y_sqrt)\ny_lin_fit = regr.predict(X_fit)\nlinear_r2 = r2_score(y_sqrt, regr.predict(X_log))\n\n# plot results\nplt.scatter(X_log, y_sqrt, label=\'Training points\', color=\'lightgray\')\n\nplt.plot(X_fit, y_lin_fit, \n         label=\'Linear (d=1), $R^2=%.2f$\' % linear_r2, \n         color=\'blue\', \n         lw=2)\n\nplt.xlabel(\'log(% lower status of the population [LSTAT])\')\nplt.ylabel(\'$\\sqrt{Price \\; in \\; \\$1000s \\; [MEDV]}$\')\nplt.legend(loc=\'lower left\')\n\nplt.tight_layout()\n#plt.savefig(\'images/10_13.png\', dpi=300)\nplt.show()\n\n\n\n# # Dealing with nonlinear relationships using random forests\n\n# ...\n\n# ## Decision tree regression\n\n\n\n\nX = df[[\'LSTAT\']].values\ny = df[\'MEDV\'].values\n\ntree = DecisionTreeRegressor(max_depth=3)\ntree.fit(X, y)\n\nsort_idx = X.flatten().argsort()\n\nlin_regplot(X[sort_idx], y[sort_idx], tree)\nplt.xlabel(\'% lower status of the population [LSTAT]\')\nplt.ylabel(\'Price in $1000s [MEDV]\')\n#plt.savefig(\'images/10_14.png\', dpi=300)\nplt.show()\n\n\n\n# ## Random forest regression\n\n\n\nX = df.iloc[:, :-1].values\ny = df[\'MEDV\'].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.4, random_state=1)\n\n\n\n\n\nforest = RandomForestRegressor(n_estimators=1000, \n                               criterion=\'mse\', \n                               random_state=1, \n                               n_jobs=-1)\nforest.fit(X_train, y_train)\ny_train_pred = forest.predict(X_train)\ny_test_pred = forest.predict(X_test)\n\nprint(\'MSE train: %.3f, test: %.3f\' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint(\'R^2 train: %.3f, test: %.3f\' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))\n\n\n\n\nplt.scatter(y_train_pred,  \n            y_train_pred - y_train, \n            c=\'steelblue\',\n            edgecolor=\'white\',\n            marker=\'o\', \n            s=35,\n            alpha=0.9,\n            label=\'Training data\')\nplt.scatter(y_test_pred,  \n            y_test_pred - y_test, \n            c=\'limegreen\',\n            edgecolor=\'white\',\n            marker=\'s\', \n            s=35,\n            alpha=0.9,\n            label=\'Test data\')\n\nplt.xlabel(\'Predicted values\')\nplt.ylabel(\'Residuals\')\nplt.legend(loc=\'upper left\')\nplt.hlines(y=0, xmin=-10, xmax=50, lw=2, color=\'black\')\nplt.xlim([-10, 50])\nplt.tight_layout()\n\n#plt.savefig(\'images/10_15.png\', dpi=300)\nplt.show()\n\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
ch11/ch11.py,0,"b'# coding: utf-8\n\n\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport numpy as np\nfrom matplotlib import cm\nfrom sklearn.metrics import silhouette_samples\nimport pandas as pd\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\n# from scipy.cluster.hierarchy import set_link_color_palette\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.datasets import make_moons\nfrom sklearn.cluster import DBSCAN\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 11 - Working with Unlabeled Data \xe2\x80\x93 Clustering Analysis\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this Jupyter extension via*  \n# \n#     conda install watermark -c conda-forge  \n# \n# or  \n# \n#     pip install watermark   \n# \n# *For more information, please see: https://github.com/rasbt/watermark.*\n\n\n# ### Overview\n\n# - [Grouping objects by similarity using k-means](#Grouping-objects-by-similarity-using-k-means)\n#   - [K-means clustering using scikit-learn](#K-means-clustering-using-scikit-learn)\n#   - [A smarter way of placing the initial cluster centroids using k-means++](#A-smarter-way-of-placing-the-initial-cluster-centroids-using-k-means++)\n#   - [Hard versus soft clustering](#Hard-versus-soft-clustering)\n#   - [Using the elbow method to find the optimal number of clusters](#Using-the-elbow-method-to-find-the-optimal-number-of-clusters)\n#   - [Quantifying the quality of clustering via silhouette plots](#Quantifying-the-quality-of-clustering-via-silhouette-plots)\n# - [Organizing clusters as a hierarchical tree](#Organizing-clusters-as-a-hierarchical-tree)\n#   - [Grouping clusters in bottom-up fashion](#Grouping-clusters-in-bottom-up-fashion)\n#   - [Performing hierarchical clustering on a distance matrix](#Performing-hierarchical-clustering-on-a-distance-matrix)\n#   - [Attaching dendrograms to a heat map](#Attaching-dendrograms-to-a-heat-map)\n#   - [Applying agglomerative clustering via scikit-learn](#Applying-agglomerative-clustering-via-scikit-learn)\n# - [Locating regions of high density via DBSCAN](#Locating-regions-of-high-density-via-DBSCAN)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Grouping objects by similarity using k-means\n\n# ## K-means clustering using scikit-learn\n\n\n\n\nX, y = make_blobs(n_samples=150, \n                  n_features=2, \n                  centers=3, \n                  cluster_std=0.5, \n                  shuffle=True, \n                  random_state=0)\n\n\n\n\n\nplt.scatter(X[:, 0], X[:, 1], \n            c=\'white\', marker=\'o\', edgecolor=\'black\', s=50)\nplt.grid()\nplt.tight_layout()\n#plt.savefig(\'images/11_01.png\', dpi=300)\nplt.show()\n\n\n\n\n\nkm = KMeans(n_clusters=3, \n            init=\'random\', \n            n_init=10, \n            max_iter=300,\n            tol=1e-04,\n            random_state=0)\n\ny_km = km.fit_predict(X)\n\n\n\n\nplt.scatter(X[y_km == 0, 0],\n            X[y_km == 0, 1],\n            s=50, c=\'lightgreen\',\n            marker=\'s\', edgecolor=\'black\',\n            label=\'Cluster 1\')\nplt.scatter(X[y_km == 1, 0],\n            X[y_km == 1, 1],\n            s=50, c=\'orange\',\n            marker=\'o\', edgecolor=\'black\',\n            label=\'Cluster 2\')\nplt.scatter(X[y_km == 2, 0],\n            X[y_km == 2, 1],\n            s=50, c=\'lightblue\',\n            marker=\'v\', edgecolor=\'black\',\n            label=\'Cluster 3\')\nplt.scatter(km.cluster_centers_[:, 0],\n            km.cluster_centers_[:, 1],\n            s=250, marker=\'*\',\n            c=\'red\', edgecolor=\'black\',\n            label=\'Centroids\')\nplt.legend(scatterpoints=1)\nplt.grid()\nplt.tight_layout()\n#plt.savefig(\'images/11_02.png\', dpi=300)\nplt.show()\n\n\n\n# ## A smarter way of placing the initial cluster centroids using k-means++\n\n# ...\n\n# ## Hard versus soft clustering\n\n# ...\n\n# ## Using the elbow method to find the optimal number of clusters \n\n\n\nprint(\'Distortion: %.2f\' % km.inertia_)\n\n\n\n\ndistortions = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters=i, \n                init=\'k-means++\', \n                n_init=10, \n                max_iter=300, \n                random_state=0)\n    km.fit(X)\n    distortions.append(km.inertia_)\nplt.plot(range(1, 11), distortions, marker=\'o\')\nplt.xlabel(\'Number of clusters\')\nplt.ylabel(\'Distortion\')\nplt.tight_layout()\n#plt.savefig(\'images/11_03.png\', dpi=300)\nplt.show()\n\n\n\n# ## Quantifying the quality of clustering  via silhouette plots\n\n\n\n\nkm = KMeans(n_clusters=3, \n            init=\'k-means++\', \n            n_init=10, \n            max_iter=300,\n            tol=1e-04,\n            random_state=0)\ny_km = km.fit_predict(X)\n\ncluster_labels = np.unique(y_km)\nn_clusters = cluster_labels.shape[0]\nsilhouette_vals = silhouette_samples(X, y_km, metric=\'euclidean\')\ny_ax_lower, y_ax_upper = 0, 0\nyticks = []\nfor i, c in enumerate(cluster_labels):\n    c_silhouette_vals = silhouette_vals[y_km == c]\n    c_silhouette_vals.sort()\n    y_ax_upper += len(c_silhouette_vals)\n    color = cm.jet(float(i) / n_clusters)\n    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, \n             edgecolor=\'none\', color=color)\n\n    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n    y_ax_lower += len(c_silhouette_vals)\n    \nsilhouette_avg = np.mean(silhouette_vals)\nplt.axvline(silhouette_avg, color=""red"", linestyle=""--"") \n\nplt.yticks(yticks, cluster_labels + 1)\nplt.ylabel(\'Cluster\')\nplt.xlabel(\'Silhouette coefficient\')\n\nplt.tight_layout()\n#plt.savefig(\'images/11_04.png\', dpi=300)\nplt.show()\n\n\n# Comparison to ""bad"" clustering:\n\n\n\nkm = KMeans(n_clusters=2,\n            init=\'k-means++\',\n            n_init=10,\n            max_iter=300,\n            tol=1e-04,\n            random_state=0)\ny_km = km.fit_predict(X)\n\nplt.scatter(X[y_km == 0, 0],\n            X[y_km == 0, 1],\n            s=50,\n            c=\'lightgreen\',\n            edgecolor=\'black\',\n            marker=\'s\',\n            label=\'Cluster 1\')\nplt.scatter(X[y_km == 1, 0],\n            X[y_km == 1, 1],\n            s=50,\n            c=\'orange\',\n            edgecolor=\'black\',\n            marker=\'o\',\n            label=\'Cluster 2\')\n\nplt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n            s=250, marker=\'*\', c=\'red\', label=\'Centroids\')\nplt.legend()\nplt.grid()\nplt.tight_layout()\n#plt.savefig(\'images/11_05.png\', dpi=300)\nplt.show()\n\n\n\n\ncluster_labels = np.unique(y_km)\nn_clusters = cluster_labels.shape[0]\nsilhouette_vals = silhouette_samples(X, y_km, metric=\'euclidean\')\ny_ax_lower, y_ax_upper = 0, 0\nyticks = []\nfor i, c in enumerate(cluster_labels):\n    c_silhouette_vals = silhouette_vals[y_km == c]\n    c_silhouette_vals.sort()\n    y_ax_upper += len(c_silhouette_vals)\n    color = cm.jet(float(i) / n_clusters)\n    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, \n             edgecolor=\'none\', color=color)\n\n    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n    y_ax_lower += len(c_silhouette_vals)\n    \nsilhouette_avg = np.mean(silhouette_vals)\nplt.axvline(silhouette_avg, color=""red"", linestyle=""--"") \n\nplt.yticks(yticks, cluster_labels + 1)\nplt.ylabel(\'Cluster\')\nplt.xlabel(\'Silhouette coefficient\')\n\nplt.tight_layout()\n#plt.savefig(\'images/11_06.png\', dpi=300)\nplt.show()\n\n\n\n# # Organizing clusters as a hierarchical tree\n\n# ## Grouping clusters in bottom-up fashion\n\n\n\n\n\n\n\n\nnp.random.seed(123)\n\nvariables = [\'X\', \'Y\', \'Z\']\nlabels = [\'ID_0\', \'ID_1\', \'ID_2\', \'ID_3\', \'ID_4\']\n\nX = np.random.random_sample([5, 3])*10\ndf = pd.DataFrame(X, columns=variables, index=labels)\ndf\n\n\n\n# ## Performing hierarchical clustering on a distance matrix\n\n\n\n\nrow_dist = pd.DataFrame(squareform(pdist(df, metric=\'euclidean\')),\n                        columns=labels,\n                        index=labels)\nrow_dist\n\n\n# We can either pass a condensed distance matrix (upper triangular) from the `pdist` function, or we can pass the ""original"" data array and define the `metric=\'euclidean\'` argument in `linkage`. However, we should not pass the squareform distance matrix, which would yield different distance values although the overall clustering could be the same.\n\n\n\n# 1. incorrect approach: Squareform distance matrix\n\n\nrow_clusters = linkage(row_dist, method=\'complete\', metric=\'euclidean\')\npd.DataFrame(row_clusters,\n             columns=[\'row label 1\', \'row label 2\',\n                      \'distance\', \'no. of items in clust.\'],\n             index=[\'cluster %d\' % (i + 1)\n                    for i in range(row_clusters.shape[0])])\n\n\n\n\n# 2. correct approach: Condensed distance matrix\n\nrow_clusters = linkage(pdist(df, metric=\'euclidean\'), method=\'complete\')\npd.DataFrame(row_clusters,\n             columns=[\'row label 1\', \'row label 2\',\n                      \'distance\', \'no. of items in clust.\'],\n             index=[\'cluster %d\' % (i + 1) \n                    for i in range(row_clusters.shape[0])])\n\n\n\n\n# 3. correct approach: Input matrix\n\nrow_clusters = linkage(df.values, method=\'complete\', metric=\'euclidean\')\npd.DataFrame(row_clusters,\n             columns=[\'row label 1\', \'row label 2\',\n                      \'distance\', \'no. of items in clust.\'],\n             index=[\'cluster %d\' % (i + 1)\n                    for i in range(row_clusters.shape[0])])\n\n\n\n\n\n# make dendrogram black (part 1/2)\n# set_link_color_palette([\'black\'])\n\nrow_dendr = dendrogram(row_clusters, \n                       labels=labels,\n                       # make dendrogram black (part 2/2)\n                       # color_threshold=np.inf\n                       )\nplt.tight_layout()\nplt.ylabel(\'Euclidean distance\')\n#plt.savefig(\'images/11_11.png\', dpi=300, \n#            bbox_inches=\'tight\')\nplt.show()\n\n\n\n# ## Attaching dendrograms to a heat map\n\n\n\n# plot row dendrogram\nfig = plt.figure(figsize=(8, 8), facecolor=\'white\')\naxd = fig.add_axes([0.09, 0.1, 0.2, 0.6])\n\n# note: for matplotlib < v1.5.1, please use orientation=\'right\'\nrow_dendr = dendrogram(row_clusters, orientation=\'left\')\n\n# reorder data with respect to clustering\ndf_rowclust = df.iloc[row_dendr[\'leaves\'][::-1]]\n\naxd.set_xticks([])\naxd.set_yticks([])\n\n# remove axes spines from dendrogram\nfor i in axd.spines.values():\n    i.set_visible(False)\n\n# plot heatmap\naxm = fig.add_axes([0.23, 0.1, 0.6, 0.6])  # x-pos, y-pos, width, height\ncax = axm.matshow(df_rowclust, interpolation=\'nearest\', cmap=\'hot_r\')\nfig.colorbar(cax)\naxm.set_xticklabels([\'\'] + list(df_rowclust.columns))\naxm.set_yticklabels([\'\'] + list(df_rowclust.index))\n\n#plt.savefig(\'images/11_12.png\', dpi=300)\nplt.show()\n\n\n\n# ## Applying agglomerative clustering via scikit-learn\n\n\n\n\nac = AgglomerativeClustering(n_clusters=3, \n                             affinity=\'euclidean\', \n                             linkage=\'complete\')\nlabels = ac.fit_predict(X)\nprint(\'Cluster labels: %s\' % labels)\n\n\n\n\nac = AgglomerativeClustering(n_clusters=2, \n                             affinity=\'euclidean\', \n                             linkage=\'complete\')\nlabels = ac.fit_predict(X)\nprint(\'Cluster labels: %s\' % labels)\n\n\n\n# # Locating regions of high density via DBSCAN\n\n\n\n\n\n\n\n\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\nplt.scatter(X[:, 0], X[:, 1])\nplt.tight_layout()\n#plt.savefig(\'images/11_14.png\', dpi=300)\nplt.show()\n\n\n# K-means and hierarchical clustering:\n\n\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n\nkm = KMeans(n_clusters=2, random_state=0)\ny_km = km.fit_predict(X)\nax1.scatter(X[y_km == 0, 0], X[y_km == 0, 1],\n            edgecolor=\'black\',\n            c=\'lightblue\', marker=\'o\', s=40, label=\'cluster 1\')\nax1.scatter(X[y_km == 1, 0], X[y_km == 1, 1],\n            edgecolor=\'black\',\n            c=\'red\', marker=\'s\', s=40, label=\'cluster 2\')\nax1.set_title(\'K-means clustering\')\n\nac = AgglomerativeClustering(n_clusters=2,\n                             affinity=\'euclidean\',\n                             linkage=\'complete\')\ny_ac = ac.fit_predict(X)\nax2.scatter(X[y_ac == 0, 0], X[y_ac == 0, 1], c=\'lightblue\',\n            edgecolor=\'black\',\n            marker=\'o\', s=40, label=\'Cluster 1\')\nax2.scatter(X[y_ac == 1, 0], X[y_ac == 1, 1], c=\'red\',\n            edgecolor=\'black\',\n            marker=\'s\', s=40, label=\'Cluster 2\')\nax2.set_title(\'Agglomerative clustering\')\n\nplt.legend()\nplt.tight_layout()\n#plt.savefig(\'images/11_15.png\', dpi=300)\nplt.show()\n\n\n# Density-based clustering:\n\n\n\n\ndb = DBSCAN(eps=0.2, min_samples=5, metric=\'euclidean\')\ny_db = db.fit_predict(X)\nplt.scatter(X[y_db == 0, 0], X[y_db == 0, 1],\n            c=\'lightblue\', marker=\'o\', s=40,\n            edgecolor=\'black\', \n            label=\'Cluster 1\')\nplt.scatter(X[y_db == 1, 0], X[y_db == 1, 1],\n            c=\'red\', marker=\'s\', s=40,\n            edgecolor=\'black\', \n            label=\'Cluster 2\')\nplt.legend()\nplt.tight_layout()\n#plt.savefig(\'images/11_16.png\', dpi=300)\nplt.show()\n\n\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
ch12/ch12.py,0,"b'# coding: utf-8\n\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nimport sys\nimport gzip\nimport shutil\nimport os\nimport struct\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Python Machine Learning - Code Examples\n\n# # Chapter 12 - Implementing a Multi-layer Artificial Neural Network from Scratch\n# \n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n# *The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*\n\n# ### Overview\n\n# - [Modeling complex functions with artificial neural networks](#Modeling-complex-functions-with-artificial-neural-networks)\n#   - [Single-layer neural network recap](#Single-layer-neural-network-recap)\n#   - [Introducing the multi-layer neural network architecture](#Introducing-the-multi-layer-neural-network-architecture)\n#   - [Activating a neural network via forward propagation](#Activating-a-neural-network-via-forward-propagation)\n# - [Classifying handwritten digits](#Classifying-handwritten-digits)\n#   - [Obtaining the MNIST dataset](#Obtaining-the-MNIST-dataset)\n#   - [Implementing a multi-layer perceptron](#Implementing-a-multi-layer-perceptron)\n# - [Training an artificial neural network](#Training-an-artificial-neural-network)\n#   - [Computing the logistic cost function](#Computing-the-logistic-cost-function)\n#   - [Developing your intuition for backpropagation](#Developing-your-intuition-for-backpropagation)\n#   - [Training neural networks via backpropagation](#Training-neural-networks-via-backpropagation)\n# - [Convergence in neural networks](#Convergence-in-neural-networks)\n# - [Summary](#Summary)\n\n\n\n\n\n\n# # Modeling complex functions with artificial neural networks\n\n# ...\n\n# ## Single-layer neural network recap\n\n\n\n\n\n\n# ## Introducing the multi-layer neural network architecture\n\n\n\n\n\n\n\n\n\n\n# ## Activating a neural network via forward propagation\n\n\n\n\n\n# To use the scikit-learn API for loading MNIST, please uncomment the follwing code below.\n\n\n\n""""""\n\n\nX, y = fetch_openml(\'mnist_784\', version=1, return_X_y=True)\ny = y.astype(int)\nX = ((X / 255.) - .5) * 2\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=10000, random_state=123, stratify=y)\n""""""\n\n\n\n# # Classifying handwritten digits\n\n# ...\n\n# ## Obtaining and preparing the MNIST dataset\n\n# The MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts:\n# \n# - Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, 60,000 examples)\n# - Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, 60,000 labels)\n# - Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, 10,000 examples)\n# - Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, 10,000 labels)\n# \n# In this section, we will only be working with a subset of MNIST, thus, we only need to download the training set images and training set labels. \n# \n# After downloading the files, simply run the next code cell to unzip the files.\n# \n# \n\n\n\n# this code cell unzips mnist\n\n\nif (sys.version_info > (3, 0)):\n    writemode = \'wb\'\nelse:\n    writemode = \'w\'\n\nzipped_mnist = [f for f in os.listdir() if f.endswith(\'ubyte.gz\')]\nfor z in zipped_mnist:\n    with gzip.GzipFile(z, mode=\'rb\') as decompressed, open(z[:-3], writemode) as outfile:\n        outfile.write(decompressed.read()) \n\n\n# ----\n# \n# IGNORE IF THE CODE CELL ABOVE EXECUTED WITHOUT PROBLEMS:\n#     \n# If you have issues with the code cell above, I recommend unzipping the files using the Unix/Linux gzip tool from the terminal for efficiency, e.g., using the command \n# \n#     gzip *ubyte.gz -d\n#  \n# in your local MNIST download directory, or, using your favorite unzipping tool if you are working with a machine running on Microsoft Windows. The images are stored in byte form, and using the following function, we will read them into NumPy arrays that we will use to train our MLP.\n# \n# Please note that if you are **not** using gzip, please make sure tha the files are named\n# \n# - train-images-idx3-ubyte\n# - train-labels-idx1-ubyte\n# - t10k-images-idx3-ubyte\n# - t10k-labels-idx1-ubyte\n# \n# If a file is e.g., named `train-images.idx3-ubyte` after unzipping (this is due to the fact that certain tools try to guess a file suffix), please rename it to `train-images-idx3-ubyte` before proceeding. \n# \n# ----\n\n\n\n \ndef load_mnist(path, kind=\'train\'):\n    """"""Load MNIST data from `path`""""""\n    labels_path = os.path.join(path, \n                               \'%s-labels-idx1-ubyte\' % kind)\n    images_path = os.path.join(path, \n                               \'%s-images-idx3-ubyte\' % kind)\n        \n    with open(labels_path, \'rb\') as lbpath:\n        magic, n = struct.unpack(\'>II\', \n                                 lbpath.read(8))\n        labels = np.fromfile(lbpath, \n                             dtype=np.uint8)\n\n    with open(images_path, \'rb\') as imgpath:\n        magic, num, rows, cols = struct.unpack("">IIII"", \n                                               imgpath.read(16))\n        images = np.fromfile(imgpath, \n                             dtype=np.uint8).reshape(len(labels), 784)\n        images = ((images / 255.) - .5) * 2\n \n    return images, labels\n\n\n\n\n\n\n\n\nX_train, y_train = load_mnist(\'\', kind=\'train\')\nprint(\'Rows: %d, columns: %d\' % (X_train.shape[0], X_train.shape[1]))\n\n\n\n\nX_test, y_test = load_mnist(\'\', kind=\'t10k\')\nprint(\'Rows: %d, columns: %d\' % (X_test.shape[0], X_test.shape[1]))\n\n\n# Visualize the first digit of each class:\n\n\n\n\nfig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\nax = ax.flatten()\nfor i in range(10):\n    img = X_train[y_train == i][0].reshape(28, 28)\n    ax[i].imshow(img, cmap=\'Greys\')\n\nax[0].set_xticks([])\nax[0].set_yticks([])\nplt.tight_layout()\n# plt.savefig(\'images/12_5.png\', dpi=300)\nplt.show()\n\n\n# Visualize 25 different versions of ""7"":\n\n\n\nfig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,)\nax = ax.flatten()\nfor i in range(25):\n    img = X_train[y_train == 7][i].reshape(28, 28)\n    ax[i].imshow(img, cmap=\'Greys\')\n\nax[0].set_xticks([])\nax[0].set_yticks([])\nplt.tight_layout()\n# plt.savefig(\'images/12_6.png\', dpi=300)\nplt.show()\n\n\n\n\n\nnp.savez_compressed(\'mnist_scaled.npz\', \n                    X_train=X_train,\n                    y_train=y_train,\n                    X_test=X_test,\n                    y_test=y_test)\n\n\n\n\nmnist = np.load(\'mnist_scaled.npz\')\nmnist.files\n\n\n\n\nX_train, y_train, X_test, y_test = [mnist[f] for f in [\'X_train\', \'y_train\', \n                                    \'X_test\', \'y_test\']]\n\ndel mnist\n\nX_train.shape\n\n\n\n# ## Implementing a multi-layer perceptron\n\n\n\n\n\nclass NeuralNetMLP(object):\n    """""" Feedforward neural network / Multi-layer perceptron classifier.\n\n    Parameters\n    ------------\n    n_hidden : int (default: 30)\n        Number of hidden units.\n    l2 : float (default: 0.)\n        Lambda value for L2-regularization.\n        No regularization if l2=0. (default)\n    epochs : int (default: 100)\n        Number of passes over the training set.\n    eta : float (default: 0.001)\n        Learning rate.\n    shuffle : bool (default: True)\n        Shuffles training data every epoch if True to prevent circles.\n    minibatch_size : int (default: 1)\n        Number of training examples per minibatch.\n    seed : int (default: None)\n        Random seed for initializing weights and shuffling.\n\n    Attributes\n    -----------\n    eval_ : dict\n      Dictionary collecting the cost, training accuracy,\n      and validation accuracy for each epoch during training.\n\n    """"""\n    def __init__(self, n_hidden=30,\n                 l2=0., epochs=100, eta=0.001,\n                 shuffle=True, minibatch_size=1, seed=None):\n\n        self.random = np.random.RandomState(seed)\n        self.n_hidden = n_hidden\n        self.l2 = l2\n        self.epochs = epochs\n        self.eta = eta\n        self.shuffle = shuffle\n        self.minibatch_size = minibatch_size\n\n    def _onehot(self, y, n_classes):\n        """"""Encode labels into one-hot representation\n\n        Parameters\n        ------------\n        y : array, shape = [n_examples]\n            Target values.\n        n_classes : int\n            Number of classes\n\n        Returns\n        -----------\n        onehot : array, shape = (n_examples, n_labels)\n\n        """"""\n        onehot = np.zeros((n_classes, y.shape[0]))\n        for idx, val in enumerate(y.astype(int)):\n            onehot[val, idx] = 1.\n        return onehot.T\n\n    def _sigmoid(self, z):\n        """"""Compute logistic function (sigmoid)""""""\n        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n\n    def _forward(self, X):\n        """"""Compute forward propagation step""""""\n\n        # step 1: net input of hidden layer\n        # [n_examples, n_features] dot [n_features, n_hidden]\n        # -> [n_examples, n_hidden]\n        z_h = np.dot(X, self.w_h) + self.b_h\n\n        # step 2: activation of hidden layer\n        a_h = self._sigmoid(z_h)\n\n        # step 3: net input of output layer\n        # [n_examples, n_hidden] dot [n_hidden, n_classlabels]\n        # -> [n_examples, n_classlabels]\n\n        z_out = np.dot(a_h, self.w_out) + self.b_out\n\n        # step 4: activation output layer\n        a_out = self._sigmoid(z_out)\n\n        return z_h, a_h, z_out, a_out\n\n    def _compute_cost(self, y_enc, output):\n        """"""Compute cost function.\n\n        Parameters\n        ----------\n        y_enc : array, shape = (n_examples, n_labels)\n            one-hot encoded class labels.\n        output : array, shape = [n_examples, n_output_units]\n            Activation of the output layer (forward propagation)\n\n        Returns\n        ---------\n        cost : float\n            Regularized cost\n\n        """"""\n        L2_term = (self.l2 *\n                   (np.sum(self.w_h ** 2.) +\n                    np.sum(self.w_out ** 2.)))\n\n        term1 = -y_enc * (np.log(output))\n        term2 = (1. - y_enc) * np.log(1. - output)\n        cost = np.sum(term1 - term2) + L2_term\n        \n        # If you are applying this cost function to other\n        # datasets where activation\n        # values maybe become more extreme (closer to zero or 1)\n        # you may encounter ""ZeroDivisionError""s due to numerical\n        # instabilities in Python & NumPy for the current implementation.\n        # I.e., the code tries to evaluate log(0), which is undefined.\n        # To address this issue, you could add a small constant to the\n        # activation values that are passed to the log function.\n        #\n        # For example:\n        #\n        # term1 = -y_enc * (np.log(output + 1e-5))\n        # term2 = (1. - y_enc) * np.log(1. - output + 1e-5)\n        \n        return cost\n\n    def predict(self, X):\n        """"""Predict class labels\n\n        Parameters\n        -----------\n        X : array, shape = [n_examples, n_features]\n            Input layer with original features.\n\n        Returns:\n        ----------\n        y_pred : array, shape = [n_examples]\n            Predicted class labels.\n\n        """"""\n        z_h, a_h, z_out, a_out = self._forward(X)\n        y_pred = np.argmax(z_out, axis=1)\n        return y_pred\n\n    def fit(self, X_train, y_train, X_valid, y_valid):\n        """""" Learn weights from training data.\n\n        Parameters\n        -----------\n        X_train : array, shape = [n_examples, n_features]\n            Input layer with original features.\n        y_train : array, shape = [n_examples]\n            Target class labels.\n        X_valid : array, shape = [n_examples, n_features]\n            Sample features for validation during training\n        y_valid : array, shape = [n_examples]\n            Sample labels for validation during training\n\n        Returns:\n        ----------\n        self\n\n        """"""\n        n_output = np.unique(y_train).shape[0]  # number of class labels\n        n_features = X_train.shape[1]\n\n        ########################\n        # Weight initialization\n        ########################\n\n        # weights for input -> hidden\n        self.b_h = np.zeros(self.n_hidden)\n        self.w_h = self.random.normal(loc=0.0, scale=0.1,\n                                      size=(n_features, self.n_hidden))\n\n        # weights for hidden -> output\n        self.b_out = np.zeros(n_output)\n        self.w_out = self.random.normal(loc=0.0, scale=0.1,\n                                        size=(self.n_hidden, n_output))\n\n        epoch_strlen = len(str(self.epochs))  # for progress formatting\n        self.eval_ = {\'cost\': [], \'train_acc\': [], \'valid_acc\': []}\n\n        y_train_enc = self._onehot(y_train, n_output)\n\n        # iterate over training epochs\n        for i in range(self.epochs):\n\n            # iterate over minibatches\n            indices = np.arange(X_train.shape[0])\n\n            if self.shuffle:\n                self.random.shuffle(indices)\n\n            for start_idx in range(0, indices.shape[0] - self.minibatch_size +\n                                   1, self.minibatch_size):\n                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n\n                # forward propagation\n                z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])\n\n                ##################\n                # Backpropagation\n                ##################\n\n                # [n_examples, n_classlabels]\n                delta_out = a_out - y_train_enc[batch_idx]\n\n                # [n_examples, n_hidden]\n                sigmoid_derivative_h = a_h * (1. - a_h)\n\n                # [n_examples, n_classlabels] dot [n_classlabels, n_hidden]\n                # -> [n_examples, n_hidden]\n                delta_h = (np.dot(delta_out, self.w_out.T) *\n                           sigmoid_derivative_h)\n\n                # [n_features, n_examples] dot [n_examples, n_hidden]\n                # -> [n_features, n_hidden]\n                grad_w_h = np.dot(X_train[batch_idx].T, delta_h)\n                grad_b_h = np.sum(delta_h, axis=0)\n\n                # [n_hidden, n_examples] dot [n_examples, n_classlabels]\n                # -> [n_hidden, n_classlabels]\n                grad_w_out = np.dot(a_h.T, delta_out)\n                grad_b_out = np.sum(delta_out, axis=0)\n\n                # Regularization and weight updates\n                delta_w_h = (grad_w_h + self.l2*self.w_h)\n                delta_b_h = grad_b_h # bias is not regularized\n                self.w_h -= self.eta * delta_w_h\n                self.b_h -= self.eta * delta_b_h\n\n                delta_w_out = (grad_w_out + self.l2*self.w_out)\n                delta_b_out = grad_b_out  # bias is not regularized\n                self.w_out -= self.eta * delta_w_out\n                self.b_out -= self.eta * delta_b_out\n\n            #############\n            # Evaluation\n            #############\n\n            # Evaluation after each epoch during training\n            z_h, a_h, z_out, a_out = self._forward(X_train)\n            \n            cost = self._compute_cost(y_enc=y_train_enc,\n                                      output=a_out)\n\n            y_train_pred = self.predict(X_train)\n            y_valid_pred = self.predict(X_valid)\n\n            train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) /\n                         X_train.shape[0])\n            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) /\n                         X_valid.shape[0])\n\n            sys.stderr.write(\'\\r%0*d/%d | Cost: %.2f \'\n                             \'| Train/Valid Acc.: %.2f%%/%.2f%% \' %\n                             (epoch_strlen, i+1, self.epochs, cost,\n                              train_acc*100, valid_acc*100))\n            sys.stderr.flush()\n\n            self.eval_[\'cost\'].append(cost)\n            self.eval_[\'train_acc\'].append(train_acc)\n            self.eval_[\'valid_acc\'].append(valid_acc)\n\n        return self\n\n\n\n\nn_epochs = 200\n\n## @Readers: PLEASE IGNORE IF-STATEMENT BELOW\n##\n## This cell is meant to run fewer epochs when\n## the notebook is run on the Travis Continuous Integration\n## platform to test the code on a smaller dataset\n## to prevent timeout errors; it just serves a debugging tool\n\nif \'TRAVIS\' in os.environ:\n    n_epochs = 20\n\n\n\n\nnn = NeuralNetMLP(n_hidden=100, \n                  l2=0.01, \n                  epochs=n_epochs, \n                  eta=0.0005,\n                  minibatch_size=100, \n                  shuffle=True,\n                  seed=1)\n\nnn.fit(X_train=X_train[:55000], \n       y_train=y_train[:55000],\n       X_valid=X_train[55000:],\n       y_valid=y_train[55000:])\n\n\n\n\n\n\nplt.plot(range(nn.epochs), nn.eval_[\'cost\'])\nplt.ylabel(\'Cost\')\nplt.xlabel(\'Epochs\')\n#plt.savefig(\'images/12_07.png\', dpi=300)\nplt.show()\n\n\n\n\nplt.plot(range(nn.epochs), nn.eval_[\'train_acc\'], \n         label=\'Training\')\nplt.plot(range(nn.epochs), nn.eval_[\'valid_acc\'], \n         label=\'Validation\', linestyle=\'--\')\nplt.ylabel(\'Accuracy\')\nplt.xlabel(\'Epochs\')\nplt.legend(loc=\'lower right\')\nplt.savefig(\'images/12_08.png\', dpi=300)\nplt.show()\n\n\n\n\ny_test_pred = nn.predict(X_test)\nacc = (np.sum(y_test == y_test_pred)\n       .astype(np.float) / X_test.shape[0])\n\nprint(\'Test accuracy: %.2f%%\' % (acc * 100))\n\n\n\n\nmiscl_img = X_test[y_test != y_test_pred][:25]\ncorrect_lab = y_test[y_test != y_test_pred][:25]\nmiscl_lab = y_test_pred[y_test != y_test_pred][:25]\n\nfig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)\nax = ax.flatten()\nfor i in range(25):\n    img = miscl_img[i].reshape(28, 28)\n    ax[i].imshow(img, cmap=\'Greys\', interpolation=\'nearest\')\n    ax[i].set_title(\'%d) t: %d p: %d\' % (i+1, correct_lab[i], miscl_lab[i]))\n\nax[0].set_xticks([])\nax[0].set_yticks([])\nplt.tight_layout()\n#plt.savefig(\'images/12_09.png\', dpi=300)\nplt.show()\n\n\n\n# # Training an artificial neural network\n\n# ...\n\n# ## Computing the logistic cost function\n\n\n\n\n\n\n# ## Developing your intuition for backpropagation\n\n# ...\n\n# ## Training neural networks via backpropagation\n\n\n\n\n\n\n\n\n\n\n# # Convergence in neural networks\n\n\n\n\n\n\n# ...\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
ch12/neuralnet.py,0,"b'import numpy as np\nimport sys\n\n\nclass NeuralNetMLP(object):\n    """""" Feedforward neural network / Multi-layer perceptron classifier.\n\n    Parameters\n    ------------\n    n_hidden : int (default: 30)\n        Number of hidden units.\n    l2 : float (default: 0.)\n        Lambda value for L2-regularization.\n        No regularization if l2=0. (default)\n    epochs : int (default: 100)\n        Number of passes over the training set.\n    eta : float (default: 0.001)\n        Learning rate.\n    shuffle : bool (default: True)\n        Shuffles training data every epoch if True to prevent circles.\n    minibatch_size : int (default: 1)\n        Number of training examples per minibatch.\n    seed : int (default: None)\n        Random seed for initializing weights and shuffling.\n\n    Attributes\n    -----------\n    eval_ : dict\n      Dictionary collecting the cost, training accuracy,\n      and validation accuracy for each epoch during training.\n\n    """"""\n    def __init__(self, n_hidden=30,\n                 l2=0., epochs=100, eta=0.001,\n                 shuffle=True, minibatch_size=1, seed=None):\n\n        self.random = np.random.RandomState(seed)\n        self.n_hidden = n_hidden\n        self.l2 = l2\n        self.epochs = epochs\n        self.eta = eta\n        self.shuffle = shuffle\n        self.minibatch_size = minibatch_size\n\n    def _onehot(self, y, n_classes):\n        """"""Encode labels into one-hot representation\n\n        Parameters\n        ------------\n        y : array, shape = [n_examples]\n            Target values.\n        n_classes : int\n            Number of classes\n\n        Returns\n        -----------\n        onehot : array, shape = (n_examples, n_labels)\n\n        """"""\n        onehot = np.zeros((n_classes, y.shape[0]))\n        for idx, val in enumerate(y.astype(int)):\n            onehot[val, idx] = 1.\n        return onehot.T\n\n    def _sigmoid(self, z):\n        """"""Compute logistic function (sigmoid)""""""\n        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n\n    def _forward(self, X):\n        """"""Compute forward propagation step""""""\n\n        # step 1: net input of hidden layer\n        # [n_examples, n_features] dot [n_features, n_hidden]\n        # -> [n_examples, n_hidden]\n        z_h = np.dot(X, self.w_h) + self.b_h\n\n        # step 2: activation of hidden layer\n        a_h = self._sigmoid(z_h)\n\n        # step 3: net input of output layer\n        # [n_examples, n_hidden] dot [n_hidden, n_classlabels]\n        # -> [n_examples, n_classlabels]\n\n        z_out = np.dot(a_h, self.w_out) + self.b_out\n\n        # step 4: activation output layer\n        a_out = self._sigmoid(z_out)\n\n        return z_h, a_h, z_out, a_out\n\n    def _compute_cost(self, y_enc, output):\n        """"""Compute cost function.\n\n        Parameters\n        ----------\n        y_enc : array, shape = (n_examples, n_labels)\n            one-hot encoded class labels.\n        output : array, shape = [n_examples, n_output_units]\n            Activation of the output layer (forward propagation)\n\n        Returns\n        ---------\n        cost : float\n            Regularized cost\n\n        """"""\n        L2_term = (self.l2 *\n                   (np.sum(self.w_h ** 2.) +\n                    np.sum(self.w_out ** 2.)))\n\n        term1 = -y_enc * (np.log(output))\n        term2 = (1. - y_enc) * np.log(1. - output)\n        cost = np.sum(term1 - term2) + L2_term\n        \n        # If you are applying this cost function to other\n        # datasets where activation\n        # values maybe become more extreme (closer to zero or 1)\n        # you may encounter ""ZeroDivisionError""s due to numerical\n        # instabilities in Python & NumPy for the current implementation.\n        # I.e., the code tries to evaluate log(0), which is undefined.\n        # To address this issue, you could add a small constant to the\n        # activation values that are passed to the log function.\n        #\n        # For example:\n        #\n        # term1 = -y_enc * (np.log(output + 1e-5))\n        # term2 = (1. - y_enc) * np.log(1. - output + 1e-5)\n        \n        return cost\n\n    def predict(self, X):\n        """"""Predict class labels\n\n        Parameters\n        -----------\n        X : array, shape = [n_examples, n_features]\n            Input layer with original features.\n\n        Returns:\n        ----------\n        y_pred : array, shape = [n_examples]\n            Predicted class labels.\n\n        """"""\n        z_h, a_h, z_out, a_out = self._forward(X)\n        y_pred = np.argmax(z_out, axis=1)\n        return y_pred\n\n    def fit(self, X_train, y_train, X_valid, y_valid):\n        """""" Learn weights from training data.\n\n        Parameters\n        -----------\n        X_train : array, shape = [n_examples, n_features]\n            Input layer with original features.\n        y_train : array, shape = [n_examples]\n            Target class labels.\n        X_valid : array, shape = [n_examples, n_features]\n            Sample features for validation during training\n        y_valid : array, shape = [n_examples]\n            Sample labels for validation during training\n\n        Returns:\n        ----------\n        self\n\n        """"""\n        n_output = np.unique(y_train).shape[0]  # number of class labels\n        n_features = X_train.shape[1]\n\n        ########################\n        # Weight initialization\n        ########################\n\n        # weights for input -> hidden\n        self.b_h = np.zeros(self.n_hidden)\n        self.w_h = self.random.normal(loc=0.0, scale=0.1,\n                                      size=(n_features, self.n_hidden))\n\n        # weights for hidden -> output\n        self.b_out = np.zeros(n_output)\n        self.w_out = self.random.normal(loc=0.0, scale=0.1,\n                                        size=(self.n_hidden, n_output))\n\n        epoch_strlen = len(str(self.epochs))  # for progress formatting\n        self.eval_ = {\'cost\': [], \'train_acc\': [], \'valid_acc\': []}\n\n        y_train_enc = self._onehot(y_train, n_output)\n\n        # iterate over training epochs\n        for i in range(self.epochs):\n\n            # iterate over minibatches\n            indices = np.arange(X_train.shape[0])\n\n            if self.shuffle:\n                self.random.shuffle(indices)\n\n            for start_idx in range(0, indices.shape[0] - self.minibatch_size +\n                                   1, self.minibatch_size):\n                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n\n                # forward propagation\n                z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])\n\n                ##################\n                # Backpropagation\n                ##################\n\n                # [n_examples, n_classlabels]\n                delta_out = a_out - y_train_enc[batch_idx]\n\n                # [n_examples, n_hidden]\n                sigmoid_derivative_h = a_h * (1. - a_h)\n\n                # [n_examples, n_classlabels] dot [n_classlabels, n_hidden]\n                # -> [n_examples, n_hidden]\n                delta_h = (np.dot(delta_out, self.w_out.T) *\n                           sigmoid_derivative_h)\n\n                # [n_features, n_examples] dot [n_examples, n_hidden]\n                # -> [n_features, n_hidden]\n                grad_w_h = np.dot(X_train[batch_idx].T, delta_h)\n                grad_b_h = np.sum(delta_h, axis=0)\n\n                # [n_hidden, n_examples] dot [n_examples, n_classlabels]\n                # -> [n_hidden, n_classlabels]\n                grad_w_out = np.dot(a_h.T, delta_out)\n                grad_b_out = np.sum(delta_out, axis=0)\n\n                # Regularization and weight updates\n                delta_w_h = (grad_w_h + self.l2*self.w_h)\n                delta_b_h = grad_b_h # bias is not regularized\n                self.w_h -= self.eta * delta_w_h\n                self.b_h -= self.eta * delta_b_h\n\n                delta_w_out = (grad_w_out + self.l2*self.w_out)\n                delta_b_out = grad_b_out  # bias is not regularized\n                self.w_out -= self.eta * delta_w_out\n                self.b_out -= self.eta * delta_b_out\n\n            #############\n            # Evaluation\n            #############\n\n            # Evaluation after each epoch during training\n            z_h, a_h, z_out, a_out = self._forward(X_train)\n            \n            cost = self._compute_cost(y_enc=y_train_enc,\n                                      output=a_out)\n\n            y_train_pred = self.predict(X_train)\n            y_valid_pred = self.predict(X_valid)\n\n            train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) /\n                         X_train.shape[0])\n            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) /\n                         X_valid.shape[0])\n\n            sys.stderr.write(\'\\r%0*d/%d | Cost: %.2f \'\n                             \'| Train/Valid Acc.: %.2f%%/%.2f%% \' %\n                             (epoch_strlen, i+1, self.epochs, cost,\n                              train_acc*100, valid_acc*100))\n            sys.stderr.flush()\n\n            self.eval_[\'cost\'].append(cost)\n            self.eval_[\'train_acc\'].append(train_acc)\n            self.eval_[\'valid_acc\'].append(valid_acc)\n\n        return self'"
ch13/ch13_part1.py,57,"b'# coding: utf-8\n\n\nimport tensorflow as tf\nimport numpy as np\nimport pathlib\nimport matplotlib.pyplot as plt\nimport os\nimport tensorflow_datasets as tfds\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com) & [Vahid Mirjalili](http://vahidmirjalili.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Chapter 13: Parallelizing Neural Network Training with TensorFlow (Part 1/2)\n# \n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n\n\n\n\n# ## TensorFlow and training performance\n\n# ### Performance challenges\n\n\n\n\n\n# ### What is TensorFlow?\n\n\n\n\n\n# ### How we will learn TensorFlow\n\n# ## First steps with TensorFlow\n\n# ### Installing TensorFlow\n\n\n\n#! pip install tensorflow\n\n\n\n\nprint(\'TensorFlow version:\', tf.__version__)\n\nnp.set_printoptions(precision=3)\n\n\n\n\n\n\n# ### Creating tensors in TensorFlow\n\n\n\na = np.array([1, 2, 3], dtype=np.int32)\nb = [4, 5, 6]\n\nt_a = tf.convert_to_tensor(a)\nt_b = tf.convert_to_tensor(b)\n\nprint(t_a)\nprint(t_b)\n\n\n\n\ntf.is_tensor(a), tf.is_tensor(t_a)\n\n\n\n\nt_ones = tf.ones((2, 3))\n\nt_ones.shape\n\n\n\n\nt_ones.numpy()\n\n\n\n\nconst_tensor = tf.constant([1.2, 5, np.pi], dtype=tf.float32)\n\nprint(const_tensor)\n\n\n# ### Manipulating the data type and shape of a tensor\n\n\n\nt_a_new = tf.cast(t_a, tf.int64)\n\nprint(t_a_new.dtype)\n\n\n\n\nt = tf.random.uniform(shape=(3, 5))\n\nt_tr = tf.transpose(t)\nprint(t.shape, \' --> \', t_tr.shape)\n\n\n\n\nt = tf.zeros((30,))\n\nt_reshape = tf.reshape(t, shape=(5, 6))\n\nprint(t_reshape.shape)\n\n\n\n\nt = tf.zeros((1, 2, 1, 4, 1))\n\nt_sqz = tf.squeeze(t, axis=(2, 4))\n\nprint(t.shape, \' --> \', t_sqz.shape)\n\n\n# ### Applying mathematical operations to tensors\n\n\n\ntf.random.set_seed(1)\n\nt1 = tf.random.uniform(shape=(5, 2), \n                       minval=-1.0,\n                       maxval=1.0)\n\nt2 = tf.random.normal(shape=(5, 2), \n                      mean=0.0,\n                      stddev=1.0)\n\n\n\n\nt3 = tf.multiply(t1, t2).numpy()\nprint(t3)\n\n\n\n\nt4 = tf.math.reduce_mean(t1, axis=0)\n\nprint(t4)\n\n\n\n\nt5 = tf.linalg.matmul(t1, t2, transpose_b=True)\n\nprint(t5.numpy())\n\n\n\n\nt6 = tf.linalg.matmul(t1, t2, transpose_a=True)\n\nprint(t6.numpy())\n\n\n\n\nnorm_t1 = tf.norm(t1, ord=2, axis=1).numpy()\n\nprint(norm_t1)\n\n\n\n\nnp.sqrt(np.sum(np.square(t1), axis=1))\n\n\n# ### Split, stack, and concatenate tensors\n\n\n\ntf.random.set_seed(1)\n\nt = tf.random.uniform((6,))\n\nprint(t.numpy())\n\nt_splits = tf.split(t, 3)\n\n[item.numpy() for item in t_splits]\n\n\n\n\ntf.random.set_seed(1)\nt = tf.random.uniform((5,))\n\nprint(t.numpy())\n\nt_splits = tf.split(t, num_or_size_splits=[3, 2])\n\n[item.numpy() for item in t_splits]\n\n\n\n\nA = tf.ones((3,))\nB = tf.zeros((2,))\n\nC = tf.concat([A, B], axis=0)\nprint(C.numpy())\n\n\n\n\nA = tf.ones((3,))\nB = tf.zeros((3,))\n\nS = tf.stack([A, B], axis=1)\nprint(S.numpy())\n\n\n# ## Building input pipelines using tf.data: The TensorFlow Dataset API\n\n# ### Creating a TensorFlow Dataset from existing tensors \n\n\n\na = [1.2, 3.4, 7.5, 4.1, 5.0, 1.0]\n\nds = tf.data.Dataset.from_tensor_slices(a)\n\nprint(ds)\n\n\n\n\nfor item in ds:\n    print(item)\n\n\n\n\nds_batch = ds.batch(3)\n\nfor i, elem in enumerate(ds_batch, 1):\n    print(\'batch {}:\'.format(i), elem.numpy())\n\n\n# ### Combining two tensors into a joint dataset\n\n\n\ntf.random.set_seed(1)\n\nt_x = tf.random.uniform([4, 3], dtype=tf.float32)\nt_y = tf.range(4)\n\n\n\n\nds_x = tf.data.Dataset.from_tensor_slices(t_x)\nds_y = tf.data.Dataset.from_tensor_slices(t_y)\n    \nds_joint = tf.data.Dataset.zip((ds_x, ds_y))\n\nfor example in ds_joint:\n    print(\'  x: \', example[0].numpy(), \n          \'  y: \', example[1].numpy())\n\n\n\n\n## method 2:\nds_joint = tf.data.Dataset.from_tensor_slices((t_x, t_y))\n\nfor example in ds_joint:\n    print(\'  x: \', example[0].numpy(), \n          \'  y: \', example[1].numpy())\n\n\n\n\nds_trans = ds_joint.map(lambda x, y: (x*2-1.0, y))\n\nfor example in ds_trans:\n    print(\'  x: \', example[0].numpy(), \n          \'  y: \', example[1].numpy())\n\n\n# ### Shuffle, batch, and repeat\n\n\n\ntf.random.set_seed(1)\nds = ds_joint.shuffle(buffer_size=len(t_x))\n\nfor example in ds:\n    print(\'  x: \', example[0].numpy(), \n          \'  y: \', example[1].numpy())\n\n\n\n\nds = ds_joint.batch(batch_size=3,\n                    drop_remainder=False)\n\nbatch_x, batch_y = next(iter(ds))\n\nprint(\'Batch-x: \\n\', batch_x.numpy())\n\nprint(\'Batch-y:   \', batch_y.numpy())\n\n\n\n\nds = ds_joint.batch(3).repeat(count=2)\n\nfor i,(batch_x, batch_y) in enumerate(ds):\n    print(i, batch_x.shape, batch_y.numpy())\n\n\n\n\nds = ds_joint.repeat(count=2).batch(3)\n\nfor i,(batch_x, batch_y) in enumerate(ds):\n    print(i, batch_x.shape, batch_y.numpy())\n\n\n\n\ntf.random.set_seed(1)\n\n## Order 1: shuffle -> batch -> repeat\nds = ds_joint.shuffle(4).batch(2).repeat(3)\n\nfor i,(batch_x, batch_y) in enumerate(ds):\n    print(i, batch_x.shape, batch_y.numpy())\n\n\n\n\ntf.random.set_seed(1)\n\n## Order 1: shuffle -> batch -> repeat\nds = ds_joint.shuffle(4).batch(2).repeat(20)\n\nfor i,(batch_x, batch_y) in enumerate(ds):\n    print(i, batch_x.shape, batch_y.numpy())\n\n\n\n\ntf.random.set_seed(1)\n\n## Order 2: batch -> shuffle -> repeat\nds = ds_joint.batch(2).shuffle(4).repeat(3)\n\nfor i,(batch_x, batch_y) in enumerate(ds):\n    print(i, batch_x.shape, batch_y.numpy())\n\n\n\n\ntf.random.set_seed(1)\n\n## Order 2: batch -> shuffle -> repeat\nds = ds_joint.batch(2).shuffle(4).repeat(20)\n\nfor i,(batch_x, batch_y) in enumerate(ds):\n    print(i, batch_x.shape, batch_y.numpy())\n\n\n# ### Creating a dataset from files on your local storage disk\n\n\n\n\nimgdir_path = pathlib.Path(\'cat_dog_images\')\n\nfile_list = sorted([str(path) for path in imgdir_path.glob(\'*.jpg\')])\n\nprint(file_list)\n\n\n\n\n\n\nfig = plt.figure(figsize=(10, 5))\nfor i,file in enumerate(file_list):\n    img_raw = tf.io.read_file(file)\n    img = tf.image.decode_image(img_raw)\n    print(\'Image shape: \', img.shape)\n    ax = fig.add_subplot(2, 3, i+1)\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(img)\n    ax.set_title(os.path.basename(file), size=15)\n    \n# plt.savefig(\'ch13-catdot-examples.pdf\')\nplt.tight_layout()\nplt.show()\n\n\n\n\nlabels = [1 if \'dog\' in os.path.basename(file) else 0\n          for file in file_list]\nprint(labels)\n\n\n\n\nds_files_labels = tf.data.Dataset.from_tensor_slices(\n    (file_list, labels))\n\nfor item in ds_files_labels:\n    print(item[0].numpy(), item[1].numpy())\n\n\n\n\ndef load_and_preprocess(path, label):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [img_height, img_width])\n    image /= 255.0\n\n    return image, label\n\nimg_width, img_height = 120, 80\n\nds_images_labels = ds_files_labels.map(load_and_preprocess)\n\nfig = plt.figure(figsize=(10, 5))\nfor i,example in enumerate(ds_images_labels):\n    print(example[0].shape, example[1].numpy())\n    ax = fig.add_subplot(2, 3, i+1)\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(example[0])\n    ax.set_title(\'{}\'.format(example[1].numpy()), \n                 size=15)\n    \nplt.tight_layout()\n#plt.savefig(\'ch13-catdog-dataset.pdf\')\nplt.show()\n\n\n# ### Fetching available datasets from the tensorflow_datasets library\n\n\n\n\n\n\n\n\nprint(len(tfds.list_builders()))\nprint(tfds.list_builders()[:5])\n\n\n\n\n## Run this to see the full list:\ntfds.list_builders()\n\n\n# Fetching CelebA dataset\n\n\n\nceleba_bldr = tfds.builder(\'celeb_a\')\n\nprint(celeba_bldr.info.features)\nprint(\'\\n\', 30*""="", \'\\n\')\nprint(celeba_bldr.info.features.keys())\nprint(\'\\n\', 30*""="", \'\\n\')\nprint(celeba_bldr.info.features[\'image\'])\nprint(\'\\n\', 30*""="", \'\\n\')\nprint(celeba_bldr.info.features[\'attributes\'].keys())\nprint(\'\\n\', 30*""="", \'\\n\')\nprint(celeba_bldr.info.citation)\n\n\n\n\n# Download the data, prepare it, and write it to disk\nceleba_bldr.download_and_prepare()\n\n\n\n\n# Load data from disk as tf.data.Datasets\ndatasets = celeba_bldr.as_dataset(shuffle_files=False)\n\ndatasets.keys()\n\n\n\n\n#import tensorflow as tf\nds_train = datasets[\'train\']\nassert isinstance(ds_train, tf.data.Dataset)\n\nexample = next(iter(ds_train))\nprint(type(example))\nprint(example.keys())\n\n\n\n\nds_train = ds_train.map(lambda item: \n     (item[\'image\'], tf.cast(item[\'attributes\'][\'Male\'], tf.int32)))\n\n\n\n\nds_train = ds_train.batch(18)\nimages, labels = next(iter(ds_train))\n\nprint(images.shape, labels)\n\n\n\n\nfig = plt.figure(figsize=(12, 8))\nfor i,(image,label) in enumerate(zip(images, labels)):\n    ax = fig.add_subplot(3, 6, i+1)\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(image)\n    ax.set_title(\'{}\'.format(label), size=15)\n    \n\nplt.show()\n\n\n# Alternative ways for loading a dataset\n\n\n\nmnist, mnist_info = tfds.load(\'mnist\', with_info=True,\n                              shuffle_files=False)\n\nprint(mnist_info)\n\nprint(mnist.keys())\n\n\n\n\nds_train = mnist[\'train\']\n\nassert isinstance(ds_train, tf.data.Dataset)\n\nds_train = ds_train.map(lambda item: \n     (item[\'image\'], item[\'label\']))\n\nds_train = ds_train.batch(10)\nbatch = next(iter(ds_train))\nprint(batch[0].shape, batch[1])\n\nfig = plt.figure(figsize=(15, 6))\nfor i,(image,label) in enumerate(zip(batch[0], batch[1])):\n    ax = fig.add_subplot(2, 5, i+1)\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(image[:, :, 0], cmap=\'gray_r\')\n    ax.set_title(\'{}\'.format(label), size=15)\n    \nplt.show()\n\n\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n\n\n\n\n\n'"
ch13/ch13_part2.py,27,"b""# coding: utf-8\n\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow_datasets as tfds\nfrom scipy.special import expit\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com) & [Vahid Mirjalili](http://vahidmirjalili.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Chapter 13: Parallelizing Neural Network Training with TensorFlow  (Part 2/2)\n# \n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n\n\n\n\n# ## Building a neural network model in TensorFlow\n\n# ### The TensorFlow Keras API (tf.keras)\n\n# ### Building a linear regression model\n\n\n\n\n\n\n\nX_train = np.arange(10).reshape((10, 1))\ny_train = np.array([1.0, 1.3, 3.1,\n                    2.0, 5.0, 6.3,\n                    6.6, 7.4, 8.0,\n                    9.0])\n\n\nplt.plot(X_train, y_train, 'o', markersize=10)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n\n\n\n\nX_train_norm = (X_train - np.mean(X_train))/np.std(X_train)\n\nds_train_orig = tf.data.Dataset.from_tensor_slices(\n    (tf.cast(X_train_norm, tf.float32),\n     tf.cast(y_train, tf.float32)))\n\n\n\n\nclass MyModel(tf.keras.Model):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.w = tf.Variable(0.0, name='weight')\n        self.b = tf.Variable(0.0, name='bias')\n\n    def call(self, x):\n        return self.w*x + self.b\n\n\nmodel = MyModel()\n\nmodel.build(input_shape=(None, 1))\nmodel.summary()\n\n\n\n\ndef loss_fn(y_true, y_pred):\n    return tf.reduce_mean(tf.square(y_true - y_pred))\n\n\n## testing the function:\nyt = tf.convert_to_tensor([1.0])\nyp = tf.convert_to_tensor([1.5])\n\nloss_fn(yt, yp)\n\n\n\n\ndef train(model, inputs, outputs, learning_rate):\n    with tf.GradientTape() as tape:\n        current_loss = loss_fn(model(inputs), outputs)\n    dW, db = tape.gradient(current_loss, [model.w, model.b])\n    model.w.assign_sub(learning_rate * dW)\n    model.b.assign_sub(learning_rate * db)\n\n\n\n\ntf.random.set_seed(1)\n\nnum_epochs = 200\nlog_steps = 100\nlearning_rate = 0.001\nbatch_size = 1\nsteps_per_epoch = int(np.ceil(len(y_train) / batch_size))\n\n\nds_train = ds_train_orig.shuffle(buffer_size=len(y_train))\nds_train = ds_train.repeat(count=None)\nds_train = ds_train.batch(1)\n\nWs, bs = [], []\n\nfor i, batch in enumerate(ds_train):\n    if i >= steps_per_epoch * num_epochs:\n        break\n    Ws.append(model.w.numpy())\n    bs.append(model.b.numpy())\n\n    bx, by = batch\n    loss_val = loss_fn(model(bx), by)\n\n    train(model, bx, by, learning_rate=learning_rate)\n    if i%log_steps==0:\n        print('Epoch {:4d} Step {:2d} Loss {:6.4f}'.format(\n              int(i/steps_per_epoch), i, loss_val))\n\n\n\n\nprint('Final Parameters:', model.w.numpy(), model.b.numpy())\n\n\nX_test = np.linspace(0, 9, num=100).reshape(-1, 1)\nX_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n\ny_pred = model(tf.cast(X_test_norm, dtype=tf.float32))\n\n\nfig = plt.figure(figsize=(13, 5))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(X_train_norm, y_train, 'o', markersize=10)\nplt.plot(X_test_norm, y_pred, '--', lw=3)\nplt.legend(['Training examples', 'Linear Reg.'], fontsize=15)\nax.set_xlabel('x', size=15)\nax.set_ylabel('y', size=15)\nax.tick_params(axis='both', which='major', labelsize=15)\n\nax = fig.add_subplot(1, 2, 2)\nplt.plot(Ws, lw=3)\nplt.plot(bs, lw=3)\nplt.legend(['Weight w', 'Bias unit b'], fontsize=15)\nax.set_xlabel('Iteration', size=15)\nax.set_ylabel('Value', size=15)\nax.tick_params(axis='both', which='major', labelsize=15)\n#plt.savefig('ch13-linreg-1.pdf')\n\nplt.show()\n\n\n# ### Model training via the .compile() and .fit() methods\n\n\n\ntf.random.set_seed(1)\nmodel = MyModel()\n#model.build((None, 1))\n\nmodel.compile(optimizer='sgd', \n              loss=loss_fn,\n              metrics=['mae', 'mse'])\n\n\n\n\nmodel.fit(X_train_norm, y_train, \n          epochs=num_epochs, batch_size=batch_size,\n          verbose=1)\n\n\n\n\nprint(model.w.numpy(), model.b.numpy())\n\n\nX_test = np.linspace(0, 9, num=100).reshape(-1, 1)\nX_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n\ny_pred = model(tf.cast(X_test_norm, dtype=tf.float32))\n\n\nfig = plt.figure(figsize=(13, 5))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(X_train_norm, y_train, 'o', markersize=10)\nplt.plot(X_test_norm, y_pred, '--', lw=3)\nplt.legend(['Training Samples', 'Linear Regression'], fontsize=15)\n\nax = fig.add_subplot(1, 2, 2)\nplt.plot(Ws, lw=3)\nplt.plot(bs, lw=3)\nplt.legend(['W', 'bias'], fontsize=15)\nplt.show()\n\n\n# ## Building a multilayer perceptron for classifying flowers in the Iris dataset\n\n\n\n\n\n\niris, iris_info = tfds.load('iris', with_info=True)\n\nprint(iris_info)\n\n\n\n\ntf.random.set_seed(1)\n\nds_orig = iris['train']\nds_orig = ds_orig.shuffle(150, reshuffle_each_iteration=False)\n\nprint(next(iter(ds_orig)))\n\nds_train_orig = ds_orig.take(100)\nds_test = ds_orig.skip(100)\n\n\n\n\n## checking the number of examples:\n\nn = 0\nfor example in ds_train_orig:\n    n += 1\nprint(n)\n\n\nn = 0\nfor example in ds_test:\n    n += 1\nprint(n)\n\n\n\n\nds_train_orig = ds_train_orig.map(\n    lambda x: (x['features'], x['label']))\n\nds_test = ds_test.map(\n    lambda x: (x['features'], x['label']))\n\nnext(iter(ds_train_orig))\n\n\n\n\niris_model = tf.keras.Sequential([\n    tf.keras.layers.Dense(16, activation='sigmoid', \n                          name='fc1', input_shape=(4,)),\n    tf.keras.layers.Dense(3, name='fc2', activation='softmax')])\n\niris_model.summary()\n\n\n\n\niris_model.compile(optimizer='adam',\n                   loss='sparse_categorical_crossentropy',\n                   metrics=['accuracy'])\n\n\n\n\nnum_epochs = 100\ntraining_size = 100\nbatch_size = 2\nsteps_per_epoch = np.ceil(training_size / batch_size)\n\nds_train = ds_train_orig.shuffle(buffer_size=training_size)\nds_train = ds_train.repeat()\nds_train = ds_train.batch(batch_size=batch_size)\nds_train = ds_train.prefetch(buffer_size=1000)\n\n\nhistory = iris_model.fit(ds_train, epochs=num_epochs,\n                         steps_per_epoch=steps_per_epoch, \n                         verbose=0)\n\n\n\n\nhist = history.history\n\nfig = plt.figure(figsize=(12, 5))\nax = fig.add_subplot(1, 2, 1)\nax.plot(hist['loss'], lw=3)\nax.set_title('Training loss', size=15)\nax.set_xlabel('Epoch', size=15)\nax.tick_params(axis='both', which='major', labelsize=15)\n\nax = fig.add_subplot(1, 2, 2)\nax.plot(hist['accuracy'], lw=3)\nax.set_title('Training accuracy', size=15)\nax.set_xlabel('Epoch', size=15)\nax.tick_params(axis='both', which='major', labelsize=15)\nplt.tight_layout()\n#plt.savefig('ch13-cls-learning-curve.pdf')\n\nplt.show()\n\n\n# ### Evaluating the trained model on the test dataset\n\n\n\nresults = iris_model.evaluate(ds_test.batch(50), verbose=0)\nprint('Test loss: {:.4f}   Test Acc.: {:.4f}'.format(*results))\n\n\n# ### Saving and reloading the trained model\n\n\n\niris_model.save('iris-classifier.h5', \n                overwrite=True,\n                include_optimizer=True,\n                save_format='h5')\n\n\n\n\niris_model_new = tf.keras.models.load_model('iris-classifier.h5')\n\niris_model_new.summary()\n\n\n\n\nresults = iris_model_new.evaluate(ds_test.batch(50), verbose=0)\nprint('Test loss: {:.4f}   Test Acc.: {:.4f}'.format(*results))\n\n\n\n\nlabels_train = []\nfor i,item in enumerate(ds_train_orig):\n    labels_train.append(item[1].numpy())\n    \nlabels_test = []\nfor i,item in enumerate(ds_test):\n    labels_test.append(item[1].numpy())\nprint('Training Set: ',len(labels_train), 'Test Set: ', len(labels_test))\n\n\n\n\niris_model_new.to_json()\n\n\n# ## Choosing activation functions for multilayer neural networks\n# \n\n# ### Logistic function recap\n\n\n\n\nX = np.array([1, 1.4, 2.5]) ## first value must be 1\nw = np.array([0.4, 0.3, 0.5])\n\ndef net_input(X, w):\n    return np.dot(X, w)\n\ndef logistic(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef logistic_activation(X, w):\n    z = net_input(X, w)\n    return logistic(z)\n\nprint('P(y=1|x) = %.3f' % logistic_activation(X, w)) \n\n\n\n\n# W : array with shape = (n_output_units, n_hidden_units+1)\n# note that the first column are the bias units\n\nW = np.array([[1.1, 1.2, 0.8, 0.4],\n              [0.2, 0.4, 1.0, 0.2],\n              [0.6, 1.5, 1.2, 0.7]])\n\n# A : data array with shape = (n_hidden_units + 1, n_samples)\n# note that the first column of this array must be 1\n\nA = np.array([[1, 0.1, 0.4, 0.6]])\nZ = np.dot(W, A[0])\ny_probas = logistic(Z)\nprint('Net Input: \\n', Z)\n\nprint('Output Units:\\n', y_probas) \n\n\n\n\ny_class = np.argmax(Z, axis=0)\nprint('Predicted class label: %d' % y_class) \n\n\n# ### Estimating class probabilities in multiclass classification via the softmax function\n\n\n\ndef softmax(z):\n    return np.exp(z) / np.sum(np.exp(z))\n\ny_probas = softmax(Z)\nprint('Probabilities:\\n', y_probas)\n\nnp.sum(y_probas)\n\n\n\n\n\nZ_tensor = tf.expand_dims(Z, axis=0)\ntf.keras.activations.softmax(Z_tensor)\n\n\n# ### Broadening the output spectrum using a hyperbolic tangent\n\n\n\n\ndef tanh(z):\n    e_p = np.exp(z)\n    e_m = np.exp(-z)\n    return (e_p - e_m) / (e_p + e_m)\n\nz = np.arange(-5, 5, 0.005)\nlog_act = logistic(z)\ntanh_act = tanh(z)\nplt.ylim([-1.5, 1.5])\nplt.xlabel('Net input $z$')\nplt.ylabel('Activation $\\phi(z)$')\nplt.axhline(1, color='black', linestyle=':')\nplt.axhline(0.5, color='black', linestyle=':')\nplt.axhline(0, color='black', linestyle=':')\nplt.axhline(-0.5, color='black', linestyle=':')\nplt.axhline(-1, color='black', linestyle=':')\nplt.plot(z, tanh_act,\n    linewidth=3, linestyle='--',\n    label='Tanh')\nplt.plot(z, log_act,\n    linewidth=3,\n    label='Logistic')\nplt.legend(loc='lower right')\nplt.tight_layout()\nplt.show()\n\n\n\n\nnp.tanh(z)\n\n\n\n\n\ntf.keras.activations.tanh(z)\n\n\n\n\n\nexpit(z)\n\n\n\n\ntf.keras.activations.sigmoid(z)\n\n\n# ### Rectified linear unit activation\n\n\n\n\ntf.keras.activations.relu(z)\n\n\n# ## Summary\n\n# # Appendix\n# \n# ## Splitting a dataset: danger of mixing train/test examples\n\n\n\n## the correct way:\nds = tf.data.Dataset.range(15)\nds = ds.shuffle(15, reshuffle_each_iteration=False)\n\n\nds_train = ds.take(10)\nds_test = ds.skip(10)\n\nds_train = ds_train.shuffle(10).repeat(10)\nds_test = ds_test.shuffle(5)\nds_test = ds_test.repeat(10)\n\nset_train = set()\nfor i,item in enumerate(ds_train):\n    set_train.add(item.numpy())\n\nset_test = set()\nfor i,item in enumerate(ds_test):\n    set_test.add(item.numpy())\n\nprint(set_train, set_test)\n\n\n\n\n## The wrong way:\nds = tf.data.Dataset.range(15)\nds = ds.shuffle(15, reshuffle_each_iteration=True)\n\n\nds_train = ds.take(10)\nds_test = ds.skip(10)\n\nds_train = ds_train.shuffle(10).repeat(10)\nds_test = ds_test.shuffle(5)\nds_test = ds_test.repeat(10)\n\nset_train = set()\nfor i,item in enumerate(ds_train):\n    set_train.add(item.numpy())\n\nset_test = set()\nfor i,item in enumerate(ds_test):\n    set_test.add(item.numpy())\n\nprint(set_train, set_test)\n\n\n# ### Splitting a dataset using `tfds.Split`\n\n\n\n\n##--------------------------- Attention ------------------------##\n##                                                              ##\n##     Note: currently, tfds.Split has a bug  in TF 2.0.0       ##\n##                                                              ##\n##  I.e., splitting [2, 1] is expected to result in             ##\n##      100 train and 50 test examples                          ##\n##                                                              ##\n##  but instead, it results in 116 train and 34 test examples   ##\n##                                                              ##\n##--------------------------------------------------------------##\n\n\n##  method 1: specifying percentage:\n#first_67_percent = tfds.Split.TRAIN.subsplit(tfds.percent[:67])\n#last_33_percent = tfds.Split.TRAIN.subsplit(tfds.percent[-33:])\n\n#ds_train_orig = tfds.load('iris', split=first_67_percent)\n#ds_test = tfds.load('iris', split=last_33_percent)\n\n\n##  method 2: specifying the weights\nsplit_train, split_test = tfds.Split.TRAIN.subsplit([2, 1])\n\nds_train_orig = tfds.load('iris', split=split_train)\nds_test = tfds.load('iris', split=split_test)\n\nprint(next(iter(ds_train_orig)))\nprint()\nprint(next(iter(ds_test)))\n\n\nds_train_orig = ds_train_orig.shuffle(100, reshuffle_each_iteration=True)\nds_test  = ds_test.shuffle(50, reshuffle_each_iteration=False)\n\nds_train_orig = ds_train_orig.map(\n    lambda x: (x['features'], x['label']))\n\nds_test = ds_test.map(\n    lambda x: (x['features'], x['label']))\n\nprint(next(iter(ds_train_orig)))\n\n\nfor j in range(5):\n    labels_train = []\n    for i,item in enumerate(ds_train_orig):\n        labels_train.append(item[1].numpy())\n\n    labels_test = []\n    for i,item in enumerate(ds_test):\n        labels_test.append(item[1].numpy())\n    print('Training Set: ',len(labels_train), 'Test Set: ', len(labels_test))\n\n    labels_test = np.array(labels_test)\n\n    print(np.sum(labels_test == 0), np.sum(labels_test == 1), np.sum(labels_test == 2))\n\n\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n"""
ch14/ch14_part1.py,163,"b""# coding: utf-8\n\n\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mlxtend.plotting import plot_decision_regions\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com) & [Vahid Mirjalili](http://vahidmirjalili.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Chapter 14: Going Deeper -- the Mechanics of TensorFlow (Part 1/3)\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n\n\n\n\n# ## The key features of TensorFlow\n# \n# ### TensorFlow's computation graphs: migrating to TensorFlow v2\n# \n# ### Understanding computation graphs\n# \n# \n\n\n\n\n\n# ### Creating a graph in TensorFlow v1.x\n# \n# \n\n\n\n\n\n\n\n## TF-v1.x style\n\ng = tf.Graph()\nwith g.as_default():\n    a = tf.constant(1, name='a')\n    b = tf.constant(2, name='b')\n    c = tf.constant(3, name='c')\n    z = 2*(a - b) + c\n    \nwith tf.compat.v1.Session(graph=g) as sess:\n    print('Result: z =', sess.run(z))\n    print('Result: z =', z.eval())\n\n\n# ### Migrating a graph to TensorFlow v2\n\n\n\n## TF v2 style\na = tf.constant(1, name='a')\nb = tf.constant(2, name='b')\nc = tf.constant(3, name='c')\n\nz = 2*(a - b) + c\ntf.print('Result: z =', z)\n\n\n# ### Loading input data into a model: TensorFlow v1.x style\n\n\n\n## TF-v1.x style\ng = tf.Graph()\nwith g.as_default():\n    a = tf.compat.v1.placeholder(shape=None, dtype=tf.int32, name='tf_a')\n    b = tf.compat.v1.placeholder(shape=None, dtype=tf.int32, name='tf_b')\n    c = tf.compat.v1.placeholder(shape=None, dtype=tf.int32, name='tf_c')\n    z = 2*(a - b) + c\n    \nwith tf.compat.v1.Session(graph=g) as sess:\n    feed_dict = {a:1, b:2, c:3}\n    print('Result: z =', sess.run(z, feed_dict=feed_dict))\n\n\n# ### Loading input data into a model: TensorFlow v2 style\n\n\n\n## TF-v2 style\ndef compute_z(a, b, c):\n    r1 = tf.subtract(a, b)\n    r2 = tf.multiply(2, r1)\n    z = tf.add(r2, c)\n    return z\n\ntf.print('Scalar Inputs:', compute_z(1, 2, 3))\ntf.print('Rank 1 Inputs:', compute_z([1], [2], [3]))\ntf.print('Rank 2 Inputs:', compute_z([[1]], [[2]], [[3]]))\n\n\n# ### Improving computational performance with function decorators\n\n\n\n@tf.function\ndef compute_z(a, b, c):\n    r1 = tf.subtract(a, b)\n    r2 = tf.multiply(2, r1)\n    z = tf.add(r2, c)\n    return z\n\ntf.print('Scalar Inputs:', compute_z(1, 2, 3))\ntf.print('Rank 1 Inputs:', compute_z([1], [2], [3]))\ntf.print('Rank 2 Inputs:', compute_z([[1]], [[2]], [[3]]))\n\n\n\n\n@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),\n                              tf.TensorSpec(shape=[None], dtype=tf.int32),\n                              tf.TensorSpec(shape=[None], dtype=tf.int32),))\ndef compute_z(a, b, c):\n    r1 = tf.subtract(a, b)\n    r2 = tf.multiply(2, r1)\n    z = tf.add(r2, c)\n    return z\n\ntf.print('Rank 1 Inputs:', compute_z([1], [2], [3]))\ntf.print('Rank 1 Inputs:', compute_z([1, 2], [2, 4], [3, 6]))\n\n\n# ```python\n# ## we expect this to result in an error\n# tf.print('Rank 2 Inputs:', compute_z([[1], [2]], [[2], [4]], [[3], [6]]))\n# \n# \n# ## >> Error:\n# #ValueError: Python inputs incompatible with input_signature: \n# #inputs (([[1], [2]], [[2], [4]], [[3], [6]])), input_signature \n# #((TensorSpec(shape=(None,), dtype=tf.int32, name=None), \n# #  TensorSpec(shape=(None,), dtype=tf.int32, name=None), \n# #  TensorSpec(shape=(None,), dtype=tf.int32, name=None)))\n# ```\n\n\n\ntf.TensorSpec(shape=[None], dtype=tf.int32)\n\n\n# ## TensorFlow Variable objects for storing and updating model parameters\n\n\n\na = tf.Variable(initial_value=3.14, name='var_a')\nb = tf.Variable(initial_value=[1, 2, 3], name='var_b')\nc = tf.Variable(initial_value=[True, False], dtype=tf.bool)\nd = tf.Variable(initial_value=['abc'], dtype=tf.string)\nprint(a)\nprint(b)\nprint(c)\nprint(d)\n\n\n\n\na.trainable\n\n\n\n\nw = tf.Variable([1, 2, 3], trainable=False)\n\nprint(w.trainable)\n\n\n\n\nprint(w.assign([3, 1, 4], read_value=True))\nw.assign_add([2, -1, 2], read_value=False)\n\nprint(w.value())\n\n\n\n\ntf.random.set_seed(1)\ninit = tf.keras.initializers.GlorotNormal()\n\ntf.print(init(shape=(3,)))\n\n\n\n\nv = tf.Variable(init(shape=(2, 3)))\ntf.print(v)\n\n\n\n\nclass MyModule(tf.Module):\n    def __init__(self):\n        init = tf.keras.initializers.GlorotNormal()\n        self.w1 = tf.Variable(init(shape=(2, 3)), trainable=True)\n        self.w2 = tf.Variable(init(shape=(1, 2)), trainable=False)\n                \nm = MyModule()\nprint('All module variables: ', [v.shape for v in m.variables])\nprint('Trainable variable:   ', [v.shape for v in\n                                 m.trainable_variables])\n\n\n# #### Variables with tf.function\n\n# ```python\n# \n# ## this will produce an error\n# ## ==> you cannot create a varibale inside a\n# ##     decorated function\n# \n# @tf.function\n# def f(x):\n#     w = tf.Variable([1, 2, 3])\n# \n# f([1])\n# \n# ## ==> results in error\n# ## ValueError: tf.function-decorated function tried to create variables on non-first call.\n# \n# ```\n\n\n\n\ntf.random.set_seed(1)\nw = tf.Variable(tf.random.uniform((3, 3)))\n\n@tf.function\ndef compute_z(x):    \n    return tf.matmul(w, x)\n\nx = tf.constant([[1], [2], [3]], dtype=tf.float32)\ntf.print(compute_z(x))\n\n\n# ## Computing gradients via automatic differentiation and GradientTape\n# \n\n# ### Computing the gradients of the loss with respect to trainable variables\n\n\n\n\nw = tf.Variable(1.0)\nb = tf.Variable(0.5)\nprint(w.trainable, b.trainable)\n\nx = tf.convert_to_tensor([1.4])\ny = tf.convert_to_tensor([2.1])\n\nwith tf.GradientTape() as tape:\n    z = tf.add(tf.multiply(w, x), b)\n    loss = tf.reduce_sum(tf.square(y - z))\n\ndloss_dw = tape.gradient(loss, w)\n\ntf.print('dL/dw : ', dloss_dw)\n\n\n\n\n# verifying the computed gradient\n#tf.print(-2*x * (-b - w*x + y))\n\ntf.print(2*x * ((w*x + b) - y))\n\n\n# ### Computing gradients with respect to non-trainable tensors\n# \n#  Monitoring the non-trainable tensors via `tape.watch()`\n\n\n\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    z = tf.add(tf.multiply(w, x), b)\n    loss = tf.square(y - z)\n\ndloss_dx = tape.gradient(loss, x)\n\ntf.print('dL/dx:', dloss_dx)\n\n\n\n\n# verifying the computed gradient\ntf.print(2*w * ((w*x + b) - y))\n\n\n# ### Keeping resources for multiple gradient computations \n# \n# via `persistent=True`\n\n\n\nwith tf.GradientTape(persistent=True) as tape:\n    z = tf.add(tf.multiply(w, x), b)\n    loss = tf.reduce_sum(tf.square(y - z))\n\ndloss_dw = tape.gradient(loss, w)\ndloss_db = tape.gradient(loss, b)\n\ntf.print('dL/dw:', dloss_dw)\ntf.print('dL/db:', dloss_db)\n\n\n# #### Updating variables: `optimizer.apply_gradients()`\n\n\n\noptimizer = tf.keras.optimizers.SGD()\n\noptimizer.apply_gradients(zip([dloss_dw, dloss_db], [w, b]))\n\ntf.print('Updated w:', w)\ntf.print('Updated bias:', b)\n\n\n# ## Simplifying implementations of common architectures via the Keras API\n# \n# \n\n\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(units=16, activation='relu'))\nmodel.add(tf.keras.layers.Dense(units=32, activation='relu'))\n\n## late variable creation\nmodel.build(input_shape=(None, 4))\nmodel.summary()\n\n\n\n\n## printing variables of the model\nfor v in model.variables:\n    print('{:20s}'.format(v.name), v.trainable, v.shape)\n\n\n# #### Configuring layers\n# \n#  * Keras Initializers `tf.keras.initializers`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers  \n#  * Keras Regularizers `tf.keras.regularizers`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/regularizers  \n#  * Activations `tf.keras.activations`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations  \n\n\n\nmodel = tf.keras.Sequential()\n\nmodel.add(\n    tf.keras.layers.Dense(\n        units=16, \n        activation=tf.keras.activations.relu,\n        kernel_initializer=tf.keras.initializers.GlorotNormal(),\n        bias_initializer=tf.keras.initializers.Constant(2.0)\n    ))\n\nmodel.add(\n    tf.keras.layers.Dense(\n        units=32, \n        activation=tf.keras.activations.sigmoid,\n        kernel_regularizer=tf.keras.regularizers.l1\n    ))\n\nmodel.build(input_shape=(None, 4))\nmodel.summary()\n\n\n# #### Compiling a model\n# \n#  * Keras Optimizers `tf.keras.optimizers`:  https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers\n#  * Keras Loss Functins `tf.keras.losses`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses\n#  * Keras Metrics `tf.keras.metrics`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics\n\n\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    metrics=[tf.keras.metrics.Accuracy(), \n             tf.keras.metrics.Precision(),\n             tf.keras.metrics.Recall(),])\n\n\n# ## Solving an XOR classification problem\n\n\n\ntf.random.set_seed(1)\nnp.random.seed(1)\n\nx = np.random.uniform(low=-1, high=1, size=(200, 2))\ny = np.ones(len(x))\ny[x[:, 0] * x[:, 1]<0] = 0\n\nx_train = x[:100, :]\ny_train = y[:100]\nx_valid = x[100:, :]\ny_valid = y[100:]\n\nfig = plt.figure(figsize=(6, 6))\nplt.plot(x[y==0, 0], \n         x[y==0, 1], 'o', alpha=0.75, markersize=10)\nplt.plot(x[y==1, 0], \n         x[y==1, 1], '<', alpha=0.75, markersize=10)\nplt.xlabel(r'$x_1$', size=15)\nplt.ylabel(r'$x_2$', size=15)\nplt.show()\n\n\n\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(units=1, \n                                input_shape=(2,), \n                                activation='sigmoid'))\n\nmodel.summary()\n\n\n\n\nmodel.compile(optimizer=tf.keras.optimizers.SGD(),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[tf.keras.metrics.BinaryAccuracy()])\n\n\n\n\nhist = model.fit(x_train, y_train, \n                 validation_data=(x_valid, y_valid), \n                 epochs=200, batch_size=2, verbose=0)\n\n\n\n\n\nhistory = hist.history\n\nfig = plt.figure(figsize=(16, 4))\nax = fig.add_subplot(1, 3, 1)\nplt.plot(history['loss'], lw=4)\nplt.plot(history['val_loss'], lw=4)\nplt.legend(['Train loss', 'Validation loss'], fontsize=15)\nax.set_xlabel('Epochs', size=15)\n\nax = fig.add_subplot(1, 3, 2)\nplt.plot(history['binary_accuracy'], lw=4)\nplt.plot(history['val_binary_accuracy'], lw=4)\nplt.legend(['Train Acc.', 'Validation Acc.'], fontsize=15)\nax.set_xlabel('Epochs', size=15)\n\nax = fig.add_subplot(1, 3, 3)\nplot_decision_regions(X=x_valid, y=y_valid.astype(np.integer),\n                      clf=model)\nax.set_xlabel(r'$x_1$', size=15)\nax.xaxis.set_label_coords(1, -0.025)\nax.set_ylabel(r'$x_2$', size=15)\nax.yaxis.set_label_coords(-0.025, 1)\nplt.show()\n\n\n\n\ntf.random.set_seed(1)\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(units=4, input_shape=(2,), activation='relu'))\nmodel.add(tf.keras.layers.Dense(units=4, activation='relu'))\nmodel.add(tf.keras.layers.Dense(units=4, activation='relu'))\nmodel.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n\nmodel.summary()\n\n## compile:\nmodel.compile(optimizer=tf.keras.optimizers.SGD(),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[tf.keras.metrics.BinaryAccuracy()])\n\n## train:\nhist = model.fit(x_train, y_train, \n                 validation_data=(x_valid, y_valid), \n                 epochs=200, batch_size=2, verbose=0)\n\nhistory = hist.history\n\n\n\n\nfig = plt.figure(figsize=(16, 4))\nax = fig.add_subplot(1, 3, 1)\nplt.plot(history['loss'], lw=4)\nplt.plot(history['val_loss'], lw=4)\nplt.legend(['Train loss', 'Validation loss'], fontsize=15)\nax.set_xlabel('Epochs', size=15)\n\nax = fig.add_subplot(1, 3, 2)\nplt.plot(history['binary_accuracy'], lw=4)\nplt.plot(history['val_binary_accuracy'], lw=4)\nplt.legend(['Train Acc.', 'Validation Acc.'], fontsize=15)\nax.set_xlabel('Epochs', size=15)\n\nax = fig.add_subplot(1, 3, 3)\nplot_decision_regions(X=x_valid, y=y_valid.astype(np.integer),\n                      clf=model)\nax.set_xlabel(r'$x_1$', size=15)\nax.xaxis.set_label_coords(1, -0.025)\nax.set_ylabel(r'$x_2$', size=15)\nax.yaxis.set_label_coords(-0.025, 1)\nplt.show()\n\n\n# ## Making model building more flexible with Keras' functional API\n# \n# \n\n\n\ntf.random.set_seed(1)\n\n## input layer:\ninputs = tf.keras.Input(shape=(2,))\n\n## hidden layers\nh1 = tf.keras.layers.Dense(units=4, activation='relu')(inputs)\nh2 = tf.keras.layers.Dense(units=4, activation='relu')(h1)\nh3 = tf.keras.layers.Dense(units=4, activation='relu')(h2)\n\n## output:\noutputs = tf.keras.layers.Dense(units=1, activation='sigmoid')(h3)\n\n## construct a model:\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.summary()\n\n\n\n\n## compile:\nmodel.compile(optimizer=tf.keras.optimizers.SGD(),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[tf.keras.metrics.BinaryAccuracy()])\n\n## train:\nhist = model.fit(x_train, y_train, \n                 validation_data=(x_valid, y_valid), \n                 epochs=200, batch_size=2, verbose=0)\n\n## Plotting\nhistory = hist.history\n\nfig = plt.figure(figsize=(16, 4))\nax = fig.add_subplot(1, 3, 1)\nplt.plot(history['loss'], lw=4)\nplt.plot(history['val_loss'], lw=4)\nplt.legend(['Train loss', 'Validation loss'], fontsize=15)\nax.set_xlabel('Epochs', size=15)\n\nax = fig.add_subplot(1, 3, 2)\nplt.plot(history['binary_accuracy'], lw=4)\nplt.plot(history['val_binary_accuracy'], lw=4)\nplt.legend(['Train Acc.', 'Validation Acc.'], fontsize=15)\nax.set_xlabel('Epochs', size=15)\n\nax = fig.add_subplot(1, 3, 3)\nplot_decision_regions(X=x_valid, y=y_valid.astype(np.integer),\n                      clf=model)\nax.set_xlabel(r'$x_1$', size=15)\nax.xaxis.set_label_coords(1, -0.025)\nax.set_ylabel(r'$x_2$', size=15)\nax.yaxis.set_label_coords(-0.025, 1)\nplt.show()\n\n\n# ## Implementing models based on Keras' Model class\n# \n# #### Sub-classing `tf.keras.Model`\n# \n#  * define `__init__()`\n#  * define `call()`\n\n\n\nclass MyModel(tf.keras.Model):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.hidden_1 = tf.keras.layers.Dense(units=4, activation='relu')\n        self.hidden_2 = tf.keras.layers.Dense(units=4, activation='relu')\n        self.hidden_3 = tf.keras.layers.Dense(units=4, activation='relu')\n        self.output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')\n        \n    def call(self, inputs):\n        h = self.hidden_1(inputs)\n        h = self.hidden_2(h)\n        h = self.hidden_3(h)\n        return self.output_layer(h)\n    \ntf.random.set_seed(1)\n\n## testing:\nmodel = MyModel()\nmodel.build(input_shape=(None, 2))\n\nmodel.summary()\n\n## compile:\nmodel.compile(optimizer=tf.keras.optimizers.SGD(),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[tf.keras.metrics.BinaryAccuracy()])\n\n## train:\nhist = model.fit(x_train, y_train, \n                 validation_data=(x_valid, y_valid), \n                 epochs=200, batch_size=2, verbose=0)\n\n## Plotting\nhistory = hist.history\n\nfig = plt.figure(figsize=(16, 4))\nax = fig.add_subplot(1, 3, 1)\nplt.plot(history['loss'], lw=4)\nplt.plot(history['val_loss'], lw=4)\nplt.legend(['Train loss', 'Validation loss'], fontsize=15)\nax.set_xlabel('Epochs', size=15)\n\nax = fig.add_subplot(1, 3, 2)\nplt.plot(history['binary_accuracy'], lw=4)\nplt.plot(history['val_binary_accuracy'], lw=4)\nplt.legend(['Train Acc.', 'Validation Acc.'], fontsize=15)\nax.set_xlabel('Epochs', size=15)\n\nax = fig.add_subplot(1, 3, 3)\nplot_decision_regions(X=x_valid, y=y_valid.astype(np.integer),\n                      clf=model)\nax.set_xlabel(r'$x_1$', size=15)\nax.xaxis.set_label_coords(1, -0.025)\nax.set_ylabel(r'$x_2$', size=15)\nax.yaxis.set_label_coords(-0.025, 1)\nplt.show()\n\n\n# ## Writing custom Keras layers\n# \n# \n# #### Defining a custom layer:\n#  * Define `__init__()`\n#  * Define `build()` for late-variable creation\n#  * Define `call()`\n#  * Define `get_config()` for serialization\n\n\n\nclass NoisyLinear(tf.keras.layers.Layer):\n    def __init__(self, output_dim, noise_stddev=0.1, **kwargs):\n        self.output_dim = output_dim\n        self.noise_stddev = noise_stddev\n        super(NoisyLinear, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.w = self.add_weight(name='weights',\n                                 shape=(input_shape[1], self.output_dim),\n                                 initializer='random_normal',\n                                 trainable=True)\n        \n        self.b = self.add_weight(shape=(self.output_dim,),\n                                 initializer='zeros',\n                                 trainable=True)\n\n    def call(self, inputs, training=False):\n        if training:\n            batch = tf.shape(inputs)[0]\n            dim = tf.shape(inputs)[1]\n            noise = tf.random.normal(shape=(batch, dim),\n                                     mean=0.0,\n                                     stddev=self.noise_stddev)\n\n            noisy_inputs = tf.add(inputs, noise)\n        else:\n            noisy_inputs = inputs\n        z = tf.matmul(noisy_inputs, self.w) + self.b\n        return tf.keras.activations.relu(z)\n    \n    def get_config(self):\n        config = super(NoisyLinear, self).get_config()\n        config.update({'output_dim': self.output_dim,\n                       'noise_stddev': self.noise_stddev})\n        return config\n\n\n## testing:\n\ntf.random.set_seed(1)\n\nnoisy_layer = NoisyLinear(4)\nnoisy_layer.build(input_shape=(None, 4))\n\nx = tf.zeros(shape=(1, 4))\ntf.print(noisy_layer(x, training=True))\n\n## re-building from config:\nconfig = noisy_layer.get_config()\nnew_layer = NoisyLinear.from_config(config)\ntf.print(new_layer(x, training=True))\n\n\n\n\ntf.random.set_seed(1)\n\nmodel = tf.keras.Sequential([\n    NoisyLinear(4, noise_stddev=0.1),\n    tf.keras.layers.Dense(units=4, activation='relu'),\n    tf.keras.layers.Dense(units=4, activation='relu'),\n    tf.keras.layers.Dense(units=1, activation='sigmoid')])\n\nmodel.build(input_shape=(None, 2))\nmodel.summary()\n\n## compile:\nmodel.compile(optimizer=tf.keras.optimizers.SGD(),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[tf.keras.metrics.BinaryAccuracy()])\n\n## train:\nhist = model.fit(x_train, y_train, \n                 validation_data=(x_valid, y_valid), \n                 epochs=200, batch_size=2, \n                 verbose=0)\n\n## Plotting\nhistory = hist.history\n\nfig = plt.figure(figsize=(16, 4))\nax = fig.add_subplot(1, 3, 1)\nplt.plot(history['loss'], lw=4)\nplt.plot(history['val_loss'], lw=4)\nplt.legend(['Train loss', 'Validation loss'], fontsize=15)\nax.set_xlabel('Epochs', size=15)\n\nax = fig.add_subplot(1, 3, 2)\nplt.plot(history['binary_accuracy'], lw=4)\nplt.plot(history['val_binary_accuracy'], lw=4)\nplt.legend(['Train Acc.', 'Validation Acc.'], fontsize=15)\nax.set_xlabel('Epochs', size=15)\n\nax = fig.add_subplot(1, 3, 3)\nplot_decision_regions(X=x_valid, y=y_valid.astype(np.integer),\n                      clf=model)\nax.set_xlabel(r'$x_1$', size=15)\nax.xaxis.set_label_coords(1, -0.025)\nax.set_ylabel(r'$x_2$', size=15)\nax.yaxis.set_label_coords(-0.025, 1)\nplt.show()\n\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n"""
ch14/ch14_part2.py,12,"b'# coding: utf-8\n\n\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport sklearn\nimport sklearn.model_selection\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com) & [Vahid Mirjalili](http://vahidmirjalili.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Chapter 14: Going Deeper -- the Mechanics of TensorFlow (Part 2/3)\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n\n\n\n\n\n# ## TensorFlow Estimators\n# \n# ##### Steps for using pre-made estimators\n# \n#  * **Step 1:** Define the input function for importing the data   \n#  * **Step 2:**  Define the feature columns to bridge between the estimator and the data   \n#  * **Step 3:** Instantiate an estimator or convert a Keras model to an estimator   \n#  * **Step 4:** Use the estimator: train() evaluate() predict()   \n\n\n\ntf.random.set_seed(1)\nnp.random.seed(1)\n\n\n# ### Working with feature columns\n# \n# \n#  * See definition: https://developers.google.com/machine-learning/glossary/#feature_columns\n#  * Documentation: https://www.tensorflow.org/api_docs/python/tf/feature_column\n\n\n\n\n\n\n\ndataset_path = tf.keras.utils.get_file(""auto-mpg.data"", \n                                       (""http://archive.ics.uci.edu/ml/machine-learning-databases""\n                                        ""/auto-mpg/auto-mpg.data""))\n\ncolumn_names = [\'MPG\', \'Cylinders\', \'Displacement\', \'Horsepower\',\n                \'Weight\', \'Acceleration\', \'ModelYear\', \'Origin\']\n\ndf = pd.read_csv(dataset_path, names=column_names,\n                 na_values = ""?"", comment=\'\\t\',\n                 sep="" "", skipinitialspace=True)\n\ndf.tail()\n\n\n\n\nprint(df.isna().sum())\n\ndf = df.dropna()\ndf = df.reset_index(drop=True)\ndf.tail()\n\n\n\n\n\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df, train_size=0.8)\ntrain_stats = df_train.describe().transpose()\ntrain_stats\n\n\n\n\nnumeric_column_names = [\'Cylinders\', \'Displacement\', \'Horsepower\', \'Weight\', \'Acceleration\']\n\ndf_train_norm, df_test_norm = df_train.copy(), df_test.copy()\n\nfor col_name in numeric_column_names:\n    mean = train_stats.loc[col_name, \'mean\']\n    std  = train_stats.loc[col_name, \'std\']\n    df_train_norm.loc[:, col_name] = (df_train_norm.loc[:, col_name] - mean)/std\n    df_test_norm.loc[:, col_name] = (df_test_norm.loc[:, col_name] - mean)/std\n    \ndf_train_norm.tail()\n\n\n# #### Numeric Columns\n\n\n\nnumeric_features = []\n\nfor col_name in numeric_column_names:\n    numeric_features.append(tf.feature_column.numeric_column(key=col_name))\n    \nnumeric_features\n\n\n\n\nfeature_year = tf.feature_column.numeric_column(key=""ModelYear"")\n\nbucketized_features = []\n\nbucketized_features.append(tf.feature_column.bucketized_column(\n    source_column=feature_year,\n    boundaries=[73, 76, 79]))\n\nprint(bucketized_features)\n\n\n\n\nfeature_origin = tf.feature_column.categorical_column_with_vocabulary_list(\n    key=\'Origin\',\n    vocabulary_list=[1, 2, 3])\n\ncategorical_indicator_features = []\ncategorical_indicator_features.append(tf.feature_column.indicator_column(feature_origin))\n\nprint(categorical_indicator_features)\n\n\n# ### Machine learning with pre-made Estimators\n\n\n\ndef train_input_fn(df_train, batch_size=8):\n    df = df_train.copy()\n    train_x, train_y = df, df.pop(\'MPG\')\n    dataset = tf.data.Dataset.from_tensor_slices((dict(train_x), train_y))\n\n    # shuffle, repeat, and batch the examples\n    return dataset.shuffle(1000).repeat().batch(batch_size)\n\n## inspection\nds = train_input_fn(df_train_norm)\nbatch = next(iter(ds))\nprint(\'Keys:\', batch[0].keys())\nprint(\'Batch Model Years:\', batch[0][\'ModelYear\'])\n\n\n\n\nall_feature_columns = (numeric_features + \n                       bucketized_features + \n                       categorical_indicator_features)\n\nprint(all_feature_columns)\n\n\n\n\nregressor = tf.estimator.DNNRegressor(\n    feature_columns=all_feature_columns,\n    hidden_units=[32, 10],\n    model_dir=\'models/autompg-dnnregressor/\')\n\n\n\n\nEPOCHS = 1000\nBATCH_SIZE = 8\ntotal_steps = EPOCHS * int(np.ceil(len(df_train) / BATCH_SIZE))\nprint(\'Training Steps:\', total_steps)\n\nregressor.train(\n    input_fn=lambda:train_input_fn(df_train_norm, batch_size=BATCH_SIZE),\n    steps=total_steps)\n\n\n\n\nreloaded_regressor = tf.estimator.DNNRegressor(\n    feature_columns=all_feature_columns,\n    hidden_units=[32, 10],\n    warm_start_from=\'models/autompg-dnnregressor/\',\n    model_dir=\'models/autompg-dnnregressor/\')\n\n\n\n\ndef eval_input_fn(df_test, batch_size=8):\n    df = df_test.copy()\n    test_x, test_y = df, df.pop(\'MPG\')\n    dataset = tf.data.Dataset.from_tensor_slices((dict(test_x), test_y))\n\n    return dataset.batch(batch_size)\n\neval_results = reloaded_regressor.evaluate(\n    input_fn=lambda:eval_input_fn(df_test_norm, batch_size=8))\n\nfor key in eval_results:\n    print(\'{:15s} {}\'.format(key, eval_results[key]))\n    \nprint(\'Average-Loss {:.4f}\'.format(eval_results[\'average_loss\']))\n\n\n\n\npred_res = regressor.predict(input_fn=lambda: eval_input_fn(df_test_norm, batch_size=8))\n\nprint(next(iter(pred_res)))\n\n\n# #### Boosted Tree Regressor\n\n\n\nboosted_tree = tf.estimator.BoostedTreesRegressor(\n    feature_columns=all_feature_columns,\n    n_batches_per_layer=20,\n    n_trees=200)\n\nboosted_tree.train(\n    input_fn=lambda:train_input_fn(df_train_norm, batch_size=BATCH_SIZE))\n\neval_results = boosted_tree.evaluate(\n    input_fn=lambda:eval_input_fn(df_test_norm, batch_size=8))\n\nprint(eval_results)\n\nprint(\'Average-Loss {:.4f}\'.format(eval_results[\'average_loss\']))\n\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n'"
ch14/ch14_part3.py,21,"b""# coding: utf-8\n\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com) & [Vahid Mirjalili](http://vahidmirjalili.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Chapter 14: Going Deeper -- the Mechanics of TensorFlow (Part 3/3)\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n\n\n\n\n# ### Using Estimators for MNIST hand-written digit classification\n\n\n\nBUFFER_SIZE = 10000\nBATCH_SIZE = 64\nNUM_EPOCHS = 20\nsteps_per_epoch = np.ceil(60000 / BATCH_SIZE)\n\n\n\n\ndef preprocess(item):\n    image = item['image']\n    label = item['label']\n    image = tf.image.convert_image_dtype(\n        image, tf.float32)\n    image = tf.reshape(image, (-1,))\n\n    return {'image-pixels':image}, label[..., tf.newaxis]\n\n#Step 1: Defining the input functions (one for training and one for evaluation)\n## Step 1: Define the input function for training\ndef train_input_fn():\n    datasets = tfds.load(name='mnist')\n    mnist_train = datasets['train']\n\n    dataset = mnist_train.map(preprocess)\n    dataset = dataset.shuffle(BUFFER_SIZE)\n    dataset = dataset.batch(BATCH_SIZE)\n    return dataset.repeat()\n\n## define input-function for evaluation:\ndef eval_input_fn():\n    datasets = tfds.load(name='mnist')\n    mnist_test = datasets['test']\n    dataset = mnist_test.map(preprocess).batch(BATCH_SIZE)\n    return dataset\n\n\n\n\n## Step 2: feature column\nimage_feature_column = tf.feature_column.numeric_column(\n    key='image-pixels', shape=(28*28))\n\n\n\n\n## Step 3: instantiate the estimator\ndnn_classifier = tf.estimator.DNNClassifier(\n    feature_columns=[image_feature_column],\n    hidden_units=[32, 16],\n    n_classes=10,\n    model_dir='models/mnist-dnn/')\n\n\n## Step 4: train\ndnn_classifier.train(\n    input_fn=train_input_fn,\n    steps=NUM_EPOCHS * steps_per_epoch)\n\n\n\n\neval_result = dnn_classifier.evaluate(\n    input_fn=eval_input_fn)\n\nprint(eval_result)\n\n\n# ### Creating a custom Estimator from an existing Keras model\n\n\n\n## Set random seeds for reproducibility\ntf.random.set_seed(1)\nnp.random.seed(1)\n\n## Create the data\nx = np.random.uniform(low=-1, high=1, size=(200, 2))\ny = np.ones(len(x))\ny[x[:, 0] * x[:, 1]<0] = 0\n\nx_train = x[:100, :]\ny_train = y[:100]\nx_valid = x[100:, :]\ny_valid = y[100:]\n\n\n\n\n## Step 1: Define the input functions\ndef train_input_fn(x_train, y_train, batch_size=8):\n    dataset = tf.data.Dataset.from_tensor_slices(\n        ({'input-features':x_train}, y_train.reshape(-1, 1)))\n\n    # Shuffle, repeat, and batch the examples.\n    return dataset.shuffle(100).repeat().batch(batch_size)\n\ndef eval_input_fn(x_test, y_test=None, batch_size=8):\n    if y_test is None:\n        dataset = tf.data.Dataset.from_tensor_slices(\n            {'input-features':x_test})\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices(\n            ({'input-features':x_test}, y_test.reshape(-1, 1)))\n\n\n    # Shuffle, repeat, and batch the examples.\n    return dataset.batch(batch_size)\n\n\n\n\n## Step 2: Define the feature columns\nfeatures = [\n    tf.feature_column.numeric_column(\n        key='input-features:', shape=(2,))\n]\n    \nfeatures\n\n\n\n\n## Step 3: Create the estimator: convert from a Keras model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(2,), name='input-features'),\n    tf.keras.layers.Dense(units=4, activation='relu'),\n    tf.keras.layers.Dense(units=4, activation='relu'),\n    tf.keras.layers.Dense(units=4, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.summary()\n\nmodel.compile(optimizer=tf.keras.optimizers.SGD(),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[tf.keras.metrics.BinaryAccuracy()])\n\nmy_estimator = tf.keras.estimator.model_to_estimator(\n    keras_model=model,\n    model_dir='models/estimator-for-XOR/')\n\n\n\n\n## Step 4: use the estimator: train/evaluate/predict\n\nnum_epochs = 200\nbatch_size = 2\nsteps_per_epoch = np.ceil(len(x_train) / batch_size)\n\nmy_estimator.train(\n    input_fn=lambda: train_input_fn(x_train, y_train, batch_size),\n    steps=num_epochs * steps_per_epoch)\n\n\n\n\nmy_estimator.evaluate(\n    input_fn=lambda: eval_input_fn(x_valid, y_valid, batch_size))\n\n\n# ...\n\n# # Summary\n\n# ...\n\n# ---\n# \n# Readers may ignore the next cell.\n\n\n\n\n"""
ch15/ch15_part1.py,47,"b""# coding: utf-8\n\n\nimport tensorflow as tf\nimport numpy as np\nimport scipy.signal\nimport imageio\nfrom tensorflow import keras\nimport tensorflow_datasets as tfds\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com) & [Vahid Mirjalili](http://vahidmirjalili.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Chapter 15: Classifying Images with Deep Convolutional Neural Networks (Part 1/2)\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n\n\n\n\n# ##  The building blocks of convolutional neural networks\n# \n# ### Understanding CNNs and feature hierarchies\n\n\n\n\n\n# \n# \n# ### Performing discrete convolutions\n# \n# ### Discrete convolutions in one dimension\n# \n# \n\n\n\n\n\n\n\n\n\n# ### Padding inputs to control the size of the output feature maps\n# \n# \n\n\n\n\n\n# ### Determining the size of the convolution output\n\n\n\n\nprint('TensorFlow version:', tf.__version__)\nprint('NumPy version: ', np.__version__)\n\n\n\n\ndef conv1d(x, w, p=0, s=1):\n    w_rot = np.array(w[::-1])\n    x_padded = np.array(x)\n    if p > 0:\n        zero_pad = np.zeros(shape=p)\n        x_padded = np.concatenate(\n            [zero_pad, x_padded, zero_pad])\n    res = []\n    for i in range(0, int((len(x_padded) - len(w_rot)) / s) + 1, s):\n        res.append(np.sum(\n            x_padded[i:i+w_rot.shape[0]] * w_rot))\n    return np.array(res)\n\n\n## Testing:\nx = [1, 3, 2, 4, 5, 6, 1, 3]\nw = [1, 0, 3, 1, 2]\n\nprint('Conv1d Implementation:',\n      conv1d(x, w, p=2, s=1))\n\nprint('Numpy Results:',\n      np.convolve(x, w, mode='same')) \n\n\n# ### Performing a discrete convolution in 2D\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef conv2d(X, W, p=(0, 0), s=(1, 1)):\n    W_rot = np.array(W)[::-1,::-1]\n    X_orig = np.array(X)\n    n1 = X_orig.shape[0] + 2*p[0]\n    n2 = X_orig.shape[1] + 2*p[1]\n    X_padded = np.zeros(shape=(n1, n2))\n    X_padded[p[0]:p[0]+X_orig.shape[0],\n    p[1]:p[1]+X_orig.shape[1]] = X_orig\n\n    res = []\n    for i in range(0, int((X_padded.shape[0] - \n                           W_rot.shape[0])/s[0])+1, s[0]):\n        res.append([])\n        for j in range(0, int((X_padded.shape[1] - \n                               W_rot.shape[1])/s[1])+1, s[1]):\n            X_sub = X_padded[i:i+W_rot.shape[0],\n                             j:j+W_rot.shape[1]]\n            res[-1].append(np.sum(X_sub * W_rot))\n    return(np.array(res))\n\nX = [[1, 3, 2, 4], [5, 6, 1, 3], [1, 2, 0, 2], [3, 4, 3, 2]]\nW = [[1, 0, 3], [1, 2, 1], [0, 1, 1]]\n\nprint('Conv2d Implementation:\\n',\n    conv2d(X, W, p=(1, 1), s=(1, 1)))\n\n\nprint('SciPy Results:\\n',\n    scipy.signal.convolve2d(X, W, mode='same'))\n\n\n# ## Subsampling layers\n\n\n\n\n\n# ## Putting everything together \xe2\x80\x93 implementing a CNN\n# \n# ### Working with multiple input or color channels\n# \n# \n\n\n\n\n\n# **TIP: Reading an image file**\n\n\n\n\n\nimg_raw = tf.io.read_file('example-image.png')\nimg = tf.image.decode_image(img_raw)\nprint('Image shape:', img.shape)\nprint('Number of channels:', img.shape[2])\nprint('Image data type:', img.dtype)\nprint(img[100:102, 100:102, :])\n\n\n\n\n\n\nimg = imageio.imread('example-image.png')\nprint('Image shape:', img.shape)\nprint('Number of channels:', img.shape[2])\nprint('Image data type:', img.dtype)\nprint(img[100:102, 100:102, :])\n\n\n# **INFO-BOX: The rank of a grayscale image for input to a CNN**\n\n\n\nimg_raw = tf.io.read_file('example-image-gray.png')\nimg = tf.image.decode_image(img_raw)\ntf.print('Rank:', tf.rank(img))\ntf.print('Shape:', img.shape)\n\n\n\n\nimg = imageio.imread('example-image-gray.png')\ntf.print('Rank:', tf.rank(img))\ntf.print('Shape:', img.shape)\n\nimg_reshaped = tf.reshape(img, (img.shape[0], img.shape[1], 1))\ntf.print('New Shape:', img_reshaped.shape)\n\n\n# ## Regularizing a neural network with dropout\n# \n# \n\n\n\n\n\n\n\n\n\nconv_layer = keras.layers.Conv2D(\n    filters=16, kernel_size=(3, 3),\n    kernel_regularizer=keras.regularizers.l2(0.001))\n\nfc_layer = keras.layers.Dense(\n    units=16, kernel_regularizer=keras.regularizers.l2(0.001))\n\n\n# ## Loss Functions for Classification\n# \n#  * **`BinaryCrossentropy()`**\n#    * `from_logits=False` \n#    * `from_logits=True`\n# \n#  * **`CategoricalCrossentropy()`**\n#    * `from_logits=False`\n#    * `from_logits=True`\n#    \n#  * **`SparseCategoricalCrossentropy()`**\n#    * `from_logits=False`\n#    * `from_logits=True`\n# \n\n\n\n\n\n\n\n####### Binary Crossentropy\nbce_probas = tf.keras.losses.BinaryCrossentropy(from_logits=False)\nbce_logits = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\nlogits = tf.constant([0.8])\nprobas = tf.keras.activations.sigmoid(logits)\n\ntf.print(\n    'BCE (w Probas): {:.4f}'.format(\n    bce_probas(y_true=[1], y_pred=probas)),\n    '(w Logits): {:.4f}'.format(\n    bce_logits(y_true=[1], y_pred=logits)))\n\n\n####### Categorical Crossentropy\ncce_probas = tf.keras.losses.CategoricalCrossentropy(\n    from_logits=False)\ncce_logits = tf.keras.losses.CategoricalCrossentropy(\n    from_logits=True)\n\nlogits = tf.constant([[1.5, 0.8, 2.1]])\nprobas = tf.keras.activations.softmax(logits)\n\ntf.print(\n    'CCE (w Probas): {:.4f}'.format(\n    cce_probas(y_true=[0, 0, 1], y_pred=probas)),\n    '(w Logits): {:.4f}'.format(\n    cce_logits(y_true=[0, 0, 1], y_pred=logits)))\n\n####### Sparse Categorical Crossentropy\nsp_cce_probas = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=False)\nsp_cce_logits = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True)\n\ntf.print(\n    'Sparse CCE (w Probas): {:.4f}'.format(\n    sp_cce_probas(y_true=[2], y_pred=probas)),\n    '(w Logits): {:.4f}'.format(\n    sp_cce_logits(y_true=[2], y_pred=logits)))\n\n\n# ## Implementing a deep convolutional neural network using TensorFlow\n# \n# ### The multilayer CNN architecture\n\n\n\n\n\n# ### Loading and preprocessing the data\n\n\n\n\n\n\n\n\n## MNIST dataset\n\nmnist_bldr = tfds.builder('mnist')\nmnist_bldr.download_and_prepare()\ndatasets = mnist_bldr.as_dataset(shuffle_files=False)\nprint(datasets.keys())\nmnist_train_orig, mnist_test_orig = datasets['train'], datasets['test']\n\n\n\n\nBUFFER_SIZE = 10000\nBATCH_SIZE = 64\nNUM_EPOCHS = 20\n\n\n\n\nmnist_train = mnist_train_orig.map(\n    lambda item: (tf.cast(item['image'], tf.float32)/255.0, \n                  tf.cast(item['label'], tf.int32)))\n\nmnist_test = mnist_test_orig.map(\n    lambda item: (tf.cast(item['image'], tf.float32)/255.0, \n                  tf.cast(item['label'], tf.int32)))\n\ntf.random.set_seed(1)\n\nmnist_train = mnist_train.shuffle(buffer_size=BUFFER_SIZE,\n                                  reshuffle_each_iteration=False)\n\nmnist_valid = mnist_train.take(10000).batch(BATCH_SIZE)\nmnist_train = mnist_train.skip(10000).batch(BATCH_SIZE)\n\n\n# ### Implementing a CNN using the TensorFlow Keras API\n# \n# #### Configuring CNN layers in Keras\n# \n#  * **Conv2D:** `tf.keras.layers.Conv2D`\n#    * `filters`\n#    * `kernel_size`\n#    * `strides`\n#    * `padding`\n#    \n#    \n#  * **MaxPool2D:** `tf.keras.layers.MaxPool2D`\n#    * `pool_size`\n#    * `strides`\n#    * `padding`\n#    \n#    \n#  * **Dropout** `tf.keras.layers.Dropout2D`\n#    * `rate`\n\n# ### Constructing a CNN in Keras\n\n\n\nmodel = tf.keras.Sequential()\n\nmodel.add(tf.keras.layers.Conv2D(\n    filters=32, kernel_size=(5, 5),\n    strides=(1, 1), padding='same',\n    data_format='channels_last',\n    name='conv_1', activation='relu'))\n\nmodel.add(tf.keras.layers.MaxPool2D(\n    pool_size=(2, 2), name='pool_1'))\n    \nmodel.add(tf.keras.layers.Conv2D(\n    filters=64, kernel_size=(5, 5),\n    strides=(1, 1), padding='same',\n    name='conv_2', activation='relu'))\n\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), name='pool_2'))\n\n\n\n\nmodel.compute_output_shape(input_shape=(16, 28, 28, 1))\n\n\n\n\nmodel.add(tf.keras.layers.Flatten())\n\nmodel.compute_output_shape(input_shape=(16, 28, 28, 1))\n\n\n\n\nmodel.add(tf.keras.layers.Dense(\n    units=1024, name='fc_1', \n    activation='relu'))\n\nmodel.add(tf.keras.layers.Dropout(\n    rate=0.5))\n    \nmodel.add(tf.keras.layers.Dense(\n    units=10, name='fc_2',\n    activation='softmax'))\n\n\n\n\ntf.random.set_seed(1)\nmodel.build(input_shape=(None, 28, 28, 1))\n\nmodel.compute_output_shape(input_shape=(16, 28, 28, 1))\n\n\n\n\nmodel.summary()\n\n\n\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics=['accuracy']) # same as `tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')`\n\nhistory = model.fit(mnist_train, epochs=NUM_EPOCHS, \n                    validation_data=mnist_valid, \n                    shuffle=True)\n\n\n\n\nhist = history.history\nx_arr = np.arange(len(hist['loss'])) + 1\n\nfig = plt.figure(figsize=(12, 4))\nax = fig.add_subplot(1, 2, 1)\nax.plot(x_arr, hist['loss'], '-o', label='Train loss')\nax.plot(x_arr, hist['val_loss'], '--<', label='Validation loss')\nax.set_xlabel('Epoch', size=15)\nax.set_ylabel('Loss', size=15)\nax.legend(fontsize=15)\nax = fig.add_subplot(1, 2, 2)\nax.plot(x_arr, hist['accuracy'], '-o', label='Train acc.')\nax.plot(x_arr, hist['val_accuracy'], '--<', label='Validation acc.')\nax.legend(fontsize=15)\nax.set_xlabel('Epoch', size=15)\nax.set_ylabel('Accuracy', size=15)\n\n#plt.savefig('figures/15_12.png', dpi=300)\nplt.show()\n\n\n\ntest_results = model.evaluate(mnist_test.batch(20))\nprint('\\nTest Acc. {:.2f}%'.format(test_results[1]*100))\n\n\n\n\nbatch_test = next(iter(mnist_test.batch(12)))\n\npreds = model(batch_test[0])\n\ntf.print(preds.shape)\npreds = tf.argmax(preds, axis=1)\nprint(preds)\n\nfig = plt.figure(figsize=(12, 4))\nfor i in range(12):\n    ax = fig.add_subplot(2, 6, i+1)\n    ax.set_xticks([]); ax.set_yticks([])\n    img = batch_test[0][i, :, :, 0]\n    ax.imshow(img, cmap='gray_r')\n    ax.text(0.9, 0.1, '{}'.format(preds[i]), \n            size=15, color='blue',\n            horizontalalignment='center',\n            verticalalignment='center', \n            transform=ax.transAxes)\n    \n#plt.savefig('figures/15_13.png', dpi=300)\nplt.show()\n\n\n\n\n\nif not os.path.exists('models'):\n    os.mkdir('models')\n\n\nmodel.save('models/mnist-cnn.h5')\n\n\n# ----\n# \n# Readers may ignore the next cell.\n\n\n\n\n"""
ch15/ch15_part2.py,46,"b""# coding: utf-8\n\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom collections import Counter\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com) & [Vahid Mirjalili](http://vahidmirjalili.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Chapter 15: Classifying Images with Deep Convolutional Neural Networks (Part 2/2)\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n\n\n\n\n\n\n# ## Gender classification from face images using CNN\n# \n\n# ### Loading the CelebA dataset\n\n\n\nceleba_bldr = tfds.builder('celeb_a')\nceleba_bldr.download_and_prepare()\nceleba = celeba_bldr.as_dataset(shuffle_files=False)\nprint(celeba.keys())\n\nceleba_train = celeba['train']\nceleba_valid = celeba['validation']\nceleba_test = celeba['test']\n\ndef count_items(ds):\n    n = 0\n    for _ in ds:\n        n += 1\n    return n\n\nprint('Train set:  {}'.format(count_items(celeba_train)))\nprint('Validation: {}'.format(count_items(celeba_valid)))\nprint('Test set:   {}'.format(count_items(celeba_test)))\n\n\n\n\nceleba_train = celeba_train.take(16000)\nceleba_valid = celeba_valid.take(1000)\n\nprint('Train set:  {}'.format(count_items(celeba_train)))\nprint('Validation: {}'.format(count_items(celeba_valid)))\n\n\n# ### Image transformation and data augmentation\n\n\n\n## take 5 examples:\nexamples = []\nfor example in celeba_train.take(5):\n    examples.append(example['image'])\n\nfig = plt.figure(figsize=(16, 8.5))\n\n## Column 1: cropping to a bounding-box\nax = fig.add_subplot(2, 5, 1)\nax.imshow(examples[0])\nax = fig.add_subplot(2, 5, 6)\nax.set_title('Crop to a \\nbounding-box', size=15)\nimg_cropped = tf.image.crop_to_bounding_box(\n    examples[0], 50, 20, 128, 128)\nax.imshow(img_cropped)\n\n## Column 2: flipping (horizontally)\nax = fig.add_subplot(2, 5, 2)\nax.imshow(examples[1])\nax = fig.add_subplot(2, 5, 7)\nax.set_title('Flip (horizontal)', size=15)\nimg_flipped = tf.image.flip_left_right(examples[1])\nax.imshow(img_flipped)\n\n## Column 3: adjust contrast\nax = fig.add_subplot(2, 5, 3)\nax.imshow(examples[2])\nax = fig.add_subplot(2, 5, 8)\nax.set_title('Adjust constrast', size=15)\nimg_adj_contrast = tf.image.adjust_contrast(\n    examples[2], contrast_factor=2)\nax.imshow(img_adj_contrast)\n\n## Column 4: adjust brightness\nax = fig.add_subplot(2, 5, 4)\nax.imshow(examples[3])\nax = fig.add_subplot(2, 5, 9)\nax.set_title('Adjust brightness', size=15)\nimg_adj_brightness = tf.image.adjust_brightness(\n    examples[3], delta=0.3)\nax.imshow(img_adj_brightness)\n\n## Column 5: cropping from image center \nax = fig.add_subplot(2, 5, 5)\nax.imshow(examples[4])\nax = fig.add_subplot(2, 5, 10)\nax.set_title('Centeral crop\\nand resize', size=15)\nimg_center_crop = tf.image.central_crop(\n    examples[4], 0.7)\nimg_resized = tf.image.resize(\n    img_center_crop, size=(218, 178))\nax.imshow(img_resized.numpy().astype('uint8'))\n\n# plt.savefig('figures/15_14.png', dpi=300)\nplt.show()\n\n\n\n\ntf.random.set_seed(1)\n\nfig = plt.figure(figsize=(14, 12))\n\nfor i,example in enumerate(celeba_train.take(3)):\n    image = example['image']\n\n    ax = fig.add_subplot(3, 4, i*4+1)\n    ax.imshow(image)\n    if i == 0:\n        ax.set_title('Orig.', size=15)\n\n    ax = fig.add_subplot(3, 4, i*4+2)\n    img_crop = tf.image.random_crop(image, size=(178, 178, 3))\n    ax.imshow(img_crop)\n    if i == 0:\n        ax.set_title('Step 1: Random crop', size=15)\n\n    ax = fig.add_subplot(3, 4, i*4+3)\n    img_flip = tf.image.random_flip_left_right(img_crop)\n    ax.imshow(tf.cast(img_flip, tf.uint8))\n    if i == 0:\n        ax.set_title('Step 2: Random flip', size=15)\n\n    ax = fig.add_subplot(3, 4, i*4+4)\n    img_resize = tf.image.resize(img_flip, size=(128, 128))\n    ax.imshow(tf.cast(img_resize, tf.uint8))\n    if i == 0:\n        ax.set_title('Step 3: Resize', size=15)\n\n# plt.savefig('figures/15_15.png', dpi=300)\nplt.show()\n\n\n\n\ndef preprocess(example, size=(64, 64), mode='train'):\n    image = example['image']\n    label = example['attributes']['Male']\n    if mode == 'train':\n        image_cropped = tf.image.random_crop(\n            image, size=(178, 178, 3))\n        image_resized = tf.image.resize(\n            image_cropped, size=size)\n        image_flip = tf.image.random_flip_left_right(\n            image_resized)\n        return (image_flip/255.0, tf.cast(label, tf.int32))\n    \n    else:\n        image_cropped = tf.image.crop_to_bounding_box(\n            image, offset_height=20, offset_width=0,\n            target_height=178, target_width=178)\n        image_resized = tf.image.resize(\n            image_cropped, size=size)\n        return (image_resized/255.0, tf.cast(label, tf.int32))\n\n## testing:\n#item = next(iter(celeba_train))\n#preprocess(item, mode='train')\n\n\n\n\ntf.random.set_seed(1)\n\nds = celeba_train.shuffle(1000, reshuffle_each_iteration=False)\nds = ds.take(2).repeat(5)\n\nds = ds.map(lambda x:preprocess(x, size=(178, 178), mode='train'))\n\nfig = plt.figure(figsize=(15, 6))\nfor j,example in enumerate(ds):\n    ax = fig.add_subplot(2, 5, j//2+(j%2)*5+1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(example[0])\n    \n#plt.savefig('figures/15_16.png', dpi=300)\nplt.show()\n\n\n\n\nBATCH_SIZE = 32\nBUFFER_SIZE = 1000\nIMAGE_SIZE = (64, 64)\nsteps_per_epoch = np.ceil(16000/BATCH_SIZE)\nprint(steps_per_epoch)\n\nds_train = celeba_train.map(\n    lambda x: preprocess(x, size=IMAGE_SIZE, mode='train'))\nds_train = ds_train.shuffle(buffer_size=BUFFER_SIZE).repeat()\nds_train = ds_train.batch(BATCH_SIZE)\n\nds_valid = celeba_valid.map(\n    lambda x: preprocess(x, size=IMAGE_SIZE, mode='eval'))\nds_valid = ds_valid.batch(BATCH_SIZE)\n\n\n# ### Training a CNN gender classifier\n# \n# * **Global Average Pooling**\n\n\n\n\n\n\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(\n        32, (3, 3), padding='same', activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(rate=0.5),\n    \n    tf.keras.layers.Conv2D(\n        64, (3, 3), padding='same', activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(rate=0.5),\n    \n    tf.keras.layers.Conv2D(\n        128, (3, 3), padding='same', activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    \n    tf.keras.layers.Conv2D(\n        256, (3, 3), padding='same', activation='relu'),\n])\n\n\n\n\nmodel.compute_output_shape(input_shape=(None, 64, 64, 3))\n\n\n\n\nmodel.add(tf.keras.layers.GlobalAveragePooling2D())\nmodel.compute_output_shape(input_shape=(None, 64, 64, 3))\n\n\n\n\nmodel.add(tf.keras.layers.Dense(1, activation=None))\n\n\n\n\ntf.random.set_seed(1)\n\nmodel.build(input_shape=(None, 64, 64, 3))\n\nmodel.summary()\n\n\n\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory = model.fit(ds_train, validation_data=ds_valid, \n                    epochs=20, steps_per_epoch=steps_per_epoch)\n\n\n\n\nhist = history.history\nx_arr = np.arange(len(hist['loss'])) + 1\n\nfig = plt.figure(figsize=(12, 4))\nax = fig.add_subplot(1, 2, 1)\nax.plot(x_arr, hist['loss'], '-o', label='Train loss')\nax.plot(x_arr, hist['val_loss'], '--<', label='Validation loss')\nax.legend(fontsize=15)\nax.set_xlabel('Epoch', size=15)\nax.set_ylabel('Loss', size=15)\n\nax = fig.add_subplot(1, 2, 2)\nax.plot(x_arr, hist['accuracy'], '-o', label='Train acc.')\nax.plot(x_arr, hist['val_accuracy'], '--<', label='Validation acc.')\nax.legend(fontsize=15)\nax.set_xlabel('Epoch', size=15)\nax.set_ylabel('Accuracy', size=15)\n\n#plt.savefig('figures/15_18.png', dpi=300)\nplt.show()\n\n\n\n\nds_test = celeba_test.map(\n    lambda x:preprocess(x, size=IMAGE_SIZE, mode='eval')).batch(32)\nresults = model.evaluate(ds_test, verbose=0)\nprint('Test Acc: {:.2f}%'.format(results[1]*100))\n\n\n\n\nhistory = model.fit(ds_train, validation_data=ds_valid, \n                    epochs=30, initial_epoch=20,\n                    steps_per_epoch=steps_per_epoch)\n\n\n\n\nhist2 = history.history\nx_arr = np.arange(len(hist['loss'] + hist2['loss']))\n\n\nfig = plt.figure(figsize=(12, 4))\nax = fig.add_subplot(1, 2, 1)\nax.plot(x_arr, hist['loss']+hist2['loss'], \n        '-o', label='Train Loss')\nax.plot(x_arr, hist['val_loss']+hist2['val_loss'],\n        '--<', label='Validation Loss')\nax.legend(fontsize=15)\n\n\nax = fig.add_subplot(1, 2, 2)\nax.plot(x_arr, hist['accuracy']+hist2['accuracy'], \n        '-o', label='Train Acc.')\nax.plot(x_arr, hist['val_accuracy']+hist2['val_accuracy'], \n        '--<', label='Validation Acc.')\nax.legend(fontsize=15)\nplt.show()\n\n\n\n\nds_test = celeba_test.map(\n    lambda x:preprocess(x, size=IMAGE_SIZE, mode='eval')).batch(32)\nresults = model.evaluate(ds_test, verbose=0)\nprint('Test Acc: {:.2f}%'.format(results[1]*100))\n\n\n\n\nds = ds_test.unbatch().take(10)\n\npred_logits = model.predict(ds.batch(10))\nprobas = tf.sigmoid(pred_logits)\nprobas = probas.numpy().flatten()*100\n\nfig = plt.figure(figsize=(15, 7))\nfor j,example in enumerate(ds):\n    ax = fig.add_subplot(2, 5, j+1)\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(example[0])\n    if example[1].numpy() == 1:\n        label='Male'\n    else:\n        label = 'Female'\n    ax.text(\n        0.5, -0.15, \n        'GT: {:s}\\nPr(Male)={:.0f}%'.format(label, probas[j]), \n        size=16, \n        horizontalalignment='center',\n        verticalalignment='center', \n        transform=ax.transAxes)\n    \n#plt.savefig('figures/figures-15_19.png', dpi=300)\nplt.show()\n\n\n\n\nmodel.save('models/celeba-cnn.h5')\n\n\n# ...\n# \n# \n# ## Summary\n# \n# ...\n# \n# \n\n# ## Appendix:\n# \n# ### The effect of initial shuffling\n\n\n\n\n\n## MNIST dataset\n#datasets = tfds.load(name='mnist')\nmnist_bldr = tfds.builder('mnist')\nmnist_bldr.download_and_prepare()\ndatasets = mnist_bldr.as_dataset(shuffle_files=False)\nmnist_train_orig, mnist_test_orig = datasets['train'], datasets['test']\n\n\nmnist_train = mnist_train_orig.map(\n    lambda item: (tf.cast(item['image'], tf.float32)/255.0, \n                  tf.cast(item['label'], tf.int32)))\n\nmnist_test = mnist_test_orig.map(\n    lambda item: (tf.cast(item['image'], tf.float32)/255.0, \n                  tf.cast(item['label'], tf.int32)))\n\ntf.random.set_seed(1)\n\nmnist_train = mnist_train.shuffle(buffer_size=10000,\n                                  reshuffle_each_iteration=False)\n\nmnist_valid = mnist_train.take(100)#.batch(BATCH_SIZE)\nmnist_train = mnist_train.skip(100)#.batch(BATCH_SIZE)\n\n\n# **Notice that count-of-labels in mnist_valid did not stay the same when the dataset is loaded with using Builder and specifying `mnist_bldr.as_dataset(shuffle_files=False)`**\n\n\n\n\ndef count_labels(ds):\n    counter = Counter()\n    for example in ds:\n        counter.update([example[1].numpy()])\n    return counter\n    \nprint('Count of labels:', count_labels(mnist_valid))\nprint('Count of labels:', count_labels(mnist_valid))\n\n\n\n\n\n\n## MNIST dataset\ndatasets = tfds.load(name='mnist')\n#mnist_bldr = tfds.builder('mnist')\n#mnist_bldr.download_and_prepare()\n#datasets = mnist_bldr.as_dataset(shuffle_files=False)\nmnist_train_orig, mnist_test_orig = datasets['train'], datasets['test']\n\n\nmnist_train = mnist_train_orig.map(\n    lambda item: (tf.cast(item['image'], tf.float32)/255.0, \n                  tf.cast(item['label'], tf.int32)))\n\nmnist_test = mnist_test_orig.map(\n    lambda item: (tf.cast(item['image'], tf.float32)/255.0, \n                  tf.cast(item['label'], tf.int32)))\n\ntf.random.set_seed(1)\n\nmnist_train = mnist_train.shuffle(buffer_size=10000,\n                                  reshuffle_each_iteration=False)\n\nmnist_valid = mnist_train.take(100)#.batch(BATCH_SIZE)\nmnist_train = mnist_train.skip(100)#.batch(BATCH_SIZE)\n\n\n# **Notice that count-of-labels in mnist_valid did not stay the same when the dataset is loaded with `tfds.load()`**\n\n\n\n\ndef count_labels(ds):\n    counter = Counter()\n    for example in ds:\n        counter.update([example[1].numpy()])\n    return counter\n    \nprint('Count of labels:', count_labels(mnist_valid))\nprint('Count of labels:', count_labels(mnist_valid))\n\n\n# ----\n# \n# Readers may ignore the next cell.\n\n\n\n\n"""
ch16/ch16_part1.py,48,"b""# coding: utf-8\n\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport pandas as pd\nimport os\nimport gzip\nimport shutil\nfrom collections import Counter\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import SimpleRNN\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import GRU\nfrom tensorflow.keras.layers import Bidirectional\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com) & [Vahid Mirjalili](http://vahidmirjalili.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Chapter 16: Modeling Sequential Data Using Recurrent Neural Networks (Part 1/2)\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n\n\n\n\n# # Introducing sequential data\n# \n# ## Modeling sequential data\xe2\x81\xa0\xe2\x80\x94order matters\n# \n# ## Representing sequences\n# \n# \n\n\n\n\n\n# ## The different categories of sequence modeling\n\n\n\n\n\n# # RNNs for modeling sequences\n# \n# ## Understanding the RNN looping mechanism\n# \n\n\n\n\n\n\n\n\n\n# ## Computing activations in an RNN\n# \n\n\n\n\n\n\n\n\n\n# ## Hidden-recurrence vs. output-recurrence\n\n\n\n\n\n\n\ntf.random.set_seed(1)\n\nrnn_layer = tf.keras.layers.SimpleRNN(\n    units=2, use_bias=True, \n    return_sequences=True)\nrnn_layer.build(input_shape=(None, None, 5))\n\nw_xh, w_oo, b_h = rnn_layer.weights\n\nprint('W_xh shape:', w_xh.shape)\nprint('W_oo shape:', w_oo.shape)\nprint('b_h shape:', b_h.shape)\n\n\n\n\nx_seq = tf.convert_to_tensor(\n    [[1.0]*5, [2.0]*5, [3.0]*5],\n    dtype=tf.float32)\n\n\n## output of SimepleRNN:\noutput = rnn_layer(tf.reshape(x_seq, shape=(1, 3, 5)))\n\n## manually computing the output:\nout_man = []\nfor t in range(len(x_seq)):\n    xt = tf.reshape(x_seq[t], (1, 5))\n    print('Time step {} =>'.format(t))\n    print('   Input           :', xt.numpy())\n    \n    ht = tf.matmul(xt, w_xh) + b_h    \n    print('   Hidden          :', ht.numpy())\n    \n    if t>0:\n        prev_o = out_man[t-1]\n    else:\n        prev_o = tf.zeros(shape=(ht.shape))\n        \n    ot = ht + tf.matmul(prev_o, w_oo)\n    ot = tf.math.tanh(ot)\n    out_man.append(ot)\n    print('   Output (manual) :', ot.numpy())\n    print('   SimpleRNN output:'.format(t), output[0][t].numpy())\n    print()\n\n\n# ## The challenges of learning long-range interactions\n# \n\n\n\n\n\n# \n# ## Long Short-Term Memory cells \n\n\n\n\n\n# # Implementing RNNs for sequence modeling in TensorFlow\n# \n# ## Project one: predicting the sentiment of IMDb movie reviews\n# \n# ### Preparing the movie review data\n# \n# \n\n\n\n\n\n\n\n\n\nwith gzip.open('../ch08/movie_data.csv.gz', 'rb') as f_in, open('movie_data.csv', 'wb') as f_out:\n    shutil.copyfileobj(f_in, f_out)\n\n\n\n\ndf = pd.read_csv('movie_data.csv', encoding='utf-8')\n\ndf.tail()\n\n\n\n\n# Step 1: Create a dataset\n\ntarget = df.pop('sentiment')\n\nds_raw = tf.data.Dataset.from_tensor_slices(\n    (df.values, target.values))\n\n## inspection:\nfor ex in ds_raw.take(3):\n    tf.print(ex[0].numpy()[0][:50], ex[1])\n\n\n#  * **Train/validaiton/test splits**\n\n\n\ntf.random.set_seed(1)\n\nds_raw = ds_raw.shuffle(\n    50000, reshuffle_each_iteration=False)\n\nds_raw_test = ds_raw.take(25000)\nds_raw_train_valid = ds_raw.skip(25000)\nds_raw_train = ds_raw_train_valid.take(20000)\nds_raw_valid = ds_raw_train_valid.skip(20000)\n\n\n#  * **Tokenizer and Encoder**\n#    * `tfds.features.text.Tokenizer`: https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer\n#    * `tfds.features.text.TokenTextEncoder`: https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/TokenTextEncoder\n\n#  * **Encoding sequences: keeping the last 100 items in each sequence**\n\n\n\n## Step 2: find unique words\n\n\ntokenizer = tfds.features.text.Tokenizer()\ntoken_counts = Counter()\n\nfor example in ds_raw_train:\n    tokens = tokenizer.tokenize(example[0].numpy()[0])\n    token_counts.update(tokens)\n    \nprint('Vocab-size:', len(token_counts))\n\n\n\n\n## Step 3: endoding each unique token into integers\n\nencoder = tfds.features.text.TokenTextEncoder(token_counts)\n\nexample_str = 'This is an example!'\nencoder.encode(example_str)\n\n\n\n\n## Step 3-A: define the function for transformation\n\ndef encode(text_tensor, label):\n    text = text_tensor.numpy()[0]\n    encoded_text = encoder.encode(text)\n    return encoded_text, label\n\n## Step 3-B: wrap the encode function to a TF Op.\ndef encode_map_fn(text, label):\n    return tf.py_function(encode, inp=[text, label], \n                          Tout=(tf.int64, tf.int64))\n\n\n\n\nds_train = ds_raw_train.map(encode_map_fn)\nds_valid = ds_raw_valid.map(encode_map_fn)\nds_test = ds_raw_test.map(encode_map_fn)\n\ntf.random.set_seed(1)\nfor example in ds_train.shuffle(1000).take(5):\n    print('Sequence length:', example[0].shape)\n    \nexample\n\n\n#  * **batch() vs. padded_batch()**\n\n# ```python\n# \n# # this will result in error\n# \n# \n# BATCH_SIZE = 32\n# train_data = all_encoded_data.batch(BATCH_SIZE)\n# \n# next(iter(train_data))\n# \n# # Running this will result in error\n# # We cannot apply .batch() to this dataset\n# ```\n\n\n\n## Take a small subset\n\nds_subset = ds_train.take(8)\nfor example in ds_subset:\n    print('Individual Shape:', example[0].shape)\n\n## batching the datasets\nds_batched = ds_subset.padded_batch(\n    4, padded_shapes=([-1], []))\n\nfor batch in ds_batched:\n    print('Batch Shape:', batch[0].shape)\n\n\n\n\n## batching the datasets\ntrain_data = ds_train.padded_batch(\n    32, padded_shapes=([-1],[]))\n\nvalid_data = ds_valid.padded_batch(\n    32, padded_shapes=([-1],[]))\n\ntest_data = ds_test.padded_batch(\n    32, padded_shapes=([-1],[]))\n\n\n# ### Embedding layers for sentence encoding\n# \n# \n#  * `input_dim`: number of words, i.e. maximum integer index + 1.\n#  * `output_dim`: \n#  * `input_length`: the length of (padded) sequence\n#     * for example, `'This is an example' -> [0, 0, 0, 0, 0, 0, 3, 1, 8, 9]`   \n#     => input_lenght is 10\n#  \n#  \n# \n#  * When calling the layer, takes integr values as input,   \n#  the embedding layer convert each interger into float vector of size `[output_dim]`\n#    * If input shape is `[BATCH_SIZE]`, output shape will be `[BATCH_SIZE, output_dim]`\n#    * If input shape is `[BATCH_SIZE, 10]`, output shape will be `[BATCH_SIZE, 10, output_dim]`\n\n\n\n\n\n\n\n\n\nmodel = tf.keras.Sequential()\n\nmodel.add(Embedding(input_dim=100,\n                    output_dim=6,\n                    input_length=20,\n                    name='embed-layer'))\n\nmodel.summary()\n\n\n# ### Building an RNN model\n# \n# * **Keras RNN layers:**\n#   * `tf.keras.layers.SimpleRNN(units, return_sequences=False)`\n#   * `tf.keras.layers.LSTM(..)`\n#   * `tf.keras.layers.GRU(..)`\n#   * `tf.keras.layers.Bidirectional()`\n#  \n# * **Determine `return_sequenes=?`**\n#   * In a multi-layer RNN, all RNN layers except the last one should have `return_sequenes=True`\n#   * For the last RNN layer, decide based on the type of problem: \n#      * many-to-many: -> `return_sequences=True`\n#      * many-to-one : -> `return_sequenes=False`\n#      * ..\n#     \n\n\n\n## An example of building a RNN model\n## with SimpleRNN layer\n\n\nmodel = Sequential()\nmodel.add(Embedding(1000, 32))\nmodel.add(SimpleRNN(32, return_sequences=True))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1))\nmodel.summary()\n\n\n\n\n## An example of building a RNN model\n## with LSTM layer\n\n\n\n\nmodel = Sequential()\nmodel.add(Embedding(10000, 32))\nmodel.add(LSTM(32, return_sequences=True))\nmodel.add(LSTM(32))\nmodel.add(Dense(1))\nmodel.summary()\n\n\n\n\n## An example of building a RNN model\n## with GRU layer\n\nmodel = Sequential()\nmodel.add(Embedding(10000, 32))\nmodel.add(GRU(32, return_sequences=True))\nmodel.add(GRU(32))\nmodel.add(Dense(1))\nmodel.summary()\n\n\n# ### Building an RNN model for the sentiment analysis task\n\n\n\nembedding_dim = 20\nvocab_size = len(token_counts) + 2\n\ntf.random.set_seed(1)\n\n## build the model\nbi_lstm_model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(\n        input_dim=vocab_size,\n        output_dim=embedding_dim,\n        name='embed-layer'),\n    \n    tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(64, name='lstm-layer'),\n        name='bidir-lstm'), \n\n    tf.keras.layers.Dense(64, activation='relu'),\n    \n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nbi_lstm_model.summary()\n\n## compile and train:\nbi_lstm_model.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-3),\n    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n    metrics=['accuracy'])\n\nhistory = bi_lstm_model.fit(\n    train_data, \n    validation_data=valid_data, \n    epochs=10)\n\n## evaluate on the test data\ntest_results= bi_lstm_model.evaluate(test_data)\nprint('Test Acc.: {:.2f}%'.format(test_results[1]*100))\n\n\n\n\nif not os.path.exists('models'):\n    os.mkdir('models')\n\n\nbi_lstm_model.save('models/Bidir-LSTM-full-length-seq.h5')\n\n\n#  * **Trying SimpleRNN with short sequences**\n\n\n\ndef preprocess_datasets(\n    ds_raw_train, \n    ds_raw_valid, \n    ds_raw_test,\n    max_seq_length=None,\n    batch_size=32):\n    \n    ## Step 1: (already done => creating a dataset)\n    ## Step 2: find unique tokens\n    tokenizer = tfds.features.text.Tokenizer()\n    token_counts = Counter()\n\n    for example in ds_raw_train:\n        tokens = tokenizer.tokenize(example[0].numpy()[0])\n        if max_seq_length is not None:\n            tokens = tokens[-max_seq_length:]\n        token_counts.update(tokens)\n\n    print('Vocab-size:', len(token_counts))\n\n\n    ## Step 3: encoding the texts\n    encoder = tfds.features.text.TokenTextEncoder(token_counts)\n    def encode(text_tensor, label):\n        text = text_tensor.numpy()[0]\n        encoded_text = encoder.encode(text)\n        if max_seq_length is not None:\n            encoded_text = encoded_text[-max_seq_length:]\n        return encoded_text, label\n\n    def encode_map_fn(text, label):\n        return tf.py_function(encode, inp=[text, label], \n                              Tout=(tf.int64, tf.int64))\n\n    ds_train = ds_raw_train.map(encode_map_fn)\n    ds_valid = ds_raw_valid.map(encode_map_fn)\n    ds_test = ds_raw_test.map(encode_map_fn)\n\n    ## Step 4: batching the datasets\n    train_data = ds_train.padded_batch(\n        batch_size, padded_shapes=([-1],[]))\n\n    valid_data = ds_valid.padded_batch(\n        batch_size, padded_shapes=([-1],[]))\n\n    test_data = ds_test.padded_batch(\n        batch_size, padded_shapes=([-1],[]))\n\n    return (train_data, valid_data, \n            test_data, len(token_counts))\n\n\n\n\ndef build_rnn_model(embedding_dim, vocab_size,\n                    recurrent_type='SimpleRNN',\n                    n_recurrent_units=64,\n                    n_recurrent_layers=1,\n                    bidirectional=True):\n\n    tf.random.set_seed(1)\n\n    # build the model\n    model = tf.keras.Sequential()\n    \n    model.add(\n        Embedding(\n            input_dim=vocab_size,\n            output_dim=embedding_dim,\n            name='embed-layer')\n    )\n    \n    for i in range(n_recurrent_layers):\n        return_sequences = (i < n_recurrent_layers-1)\n            \n        if recurrent_type == 'SimpleRNN':\n            recurrent_layer = SimpleRNN(\n                units=n_recurrent_units, \n                return_sequences=return_sequences,\n                name='simprnn-layer-{}'.format(i))\n        elif recurrent_type == 'LSTM':\n            recurrent_layer = LSTM(\n                units=n_recurrent_units, \n                return_sequences=return_sequences,\n                name='lstm-layer-{}'.format(i))\n        elif recurrent_type == 'GRU':\n            recurrent_layer = GRU(\n                units=n_recurrent_units, \n                return_sequences=return_sequences,\n                name='gru-layer-{}'.format(i))\n        \n        if bidirectional:\n            recurrent_layer = Bidirectional(\n                recurrent_layer, name='bidir-'+recurrent_layer.name)\n            \n        model.add(recurrent_layer)\n\n    model.add(tf.keras.layers.Dense(64, activation='relu'))\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n    \n    return model\n\n\n\n\n\n\nbatch_size = 32\nembedding_dim = 20\nmax_seq_length = 100\n\ntrain_data, valid_data, test_data, n = preprocess_datasets(\n    ds_raw_train, ds_raw_valid, ds_raw_test, \n    max_seq_length=max_seq_length, \n    batch_size=batch_size\n)\n\n\nvocab_size = n + 2\n\nrnn_model = build_rnn_model(\n    embedding_dim, vocab_size,\n    recurrent_type='SimpleRNN', \n    n_recurrent_units=64,\n    n_recurrent_layers=1,\n    bidirectional=True)\n\nrnn_model.summary()\n\n\n\n\nrnn_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                  metrics=['accuracy'])\n\n\nhistory = rnn_model.fit(\n    train_data, \n    validation_data=valid_data, \n    epochs=10)\n\n\n\n\nresults = rnn_model.evaluate(test_data)\n\n\n\n\nprint('Test Acc.: {:.2f}%'.format(results[1]*100))\n\n\n# ## Optional exercise: \n# \n# ### Uni-directional SimpleRNN with full-length sequences\n\n\n\nbatch_size = 32\nembedding_dim = 20\nmax_seq_length = None\n\ntrain_data, valid_data, test_data, n = preprocess_datasets(\n    ds_raw_train, ds_raw_valid, ds_raw_test, \n    max_seq_length=max_seq_length, \n    batch_size=batch_size\n)\n\n\nvocab_size = n + 2\n\nrnn_model = build_rnn_model(\n    embedding_dim, vocab_size,\n    recurrent_type='SimpleRNN', \n    n_recurrent_units=64,\n    n_recurrent_layers=1,\n    bidirectional=False)\n\nrnn_model.summary()\n\n\n\n\nrnn_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                  metrics=['accuracy'])\n\nhistory = rnn_model.fit(\n    train_data, \n    validation_data=valid_data, \n    epochs=10)\n\n\n# # Appendix\n# \n\n# ### A -- An alternative way to get the dataset: using tensorflow_datasets\n\n\n\nimdb_bldr = tfds.builder('imdb_reviews')\nprint(imdb_bldr.info)\n\nimdb_bldr.download_and_prepare()\n\ndatasets = imdb_bldr.as_dataset(shuffle_files=False)\n\ndatasets.keys()\n\n\n\n\nimdb_train = datasets['train']\nimdb_train = datasets['test']\n\n\n# ### B -- Tokenizer and Encoder\n# \n#  * `tfds.features.text.Tokenizer`: https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer\n#  * `tfds.features.text.TokenTextEncoder`: https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/TokenTextEncoder\n# \n# \n\n\n\nvocab_set = {'a', 'b', 'c', 'd'}\nencoder = tfds.features.text.TokenTextEncoder(vocab_set)\nprint(encoder)\n\nprint(encoder.encode(b'a b c d, , : .'))\n\nprint(encoder.encode(b'a b c d e f g h i z'))\n\n\n# ### C -- Text Pre-processing with Keras \n\n\n\nTOP_K = 200\nMAX_LEN = 10\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=TOP_K)\n\ntokenizer.fit_on_texts(['this is an example', 'je suis en forme '])\nsequences = tokenizer.texts_to_sequences(['this is an example', 'je suis en forme '])\nprint(sequences)\n\ntf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_LEN)\n\n\n\n\nTOP_K = 20000\nMAX_LEN = 500\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=TOP_K)\n\ntokenizer.fit_on_texts(\n    [example['text'].numpy().decode('utf-8') \n     for example in imdb_train])\n\nx_train = tokenizer.texts_to_sequences(\n    [example['text'].numpy().decode('utf-8')\n     for example in imdb_train])\n\nprint(len(x_train))\n\n\nx_train_padded = tf.keras.preprocessing.sequence.pad_sequences(\n    x_train, maxlen=MAX_LEN)\n\nprint(x_train_padded.shape)\n\n\n# ### D -- Embedding\n# \n# \n\n\n\n\n\ntf.random.set_seed(1)\nembed = Embedding(input_dim=100, output_dim=4)\n\ninp_arr = np.array([1, 98, 5, 6, 67, 45])\ntf.print(embed(inp_arr))\ntf.print(embed(inp_arr).shape)\n\ntf.print(embed(np.array([1])))\n\n\n# \n# ---\n\n# \n# \n# Readers may ignore the next cell.\n# \n\n\n\n\n"""
ch16/ch16_part2.py,28,"b""# coding: utf-8\n\n\nimport numpy as np\nimport tensorflow as tf\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com) & [Vahid Mirjalili](http://vahidmirjalili.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# Chapter 16: Modeling Sequential Data Using Recurrent Neural Networks (part 2/2)\n# ========\n# \n# \n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n\n\n\n\n# ## Project two: character-level language modeling in TensorFlow\n# \n\n\n\n\n\n# ### Preprocessing the dataset\n\n\n\n\n\n\n\n\n\n## Reading and processing text\nwith open('1268-0.txt', 'r') as fp:\n    text=fp.read()\n    \nstart_indx = text.find('THE MYSTERIOUS ISLAND')\nend_indx = text.find('End of the Project Gutenberg')\nprint(start_indx, end_indx)\n\ntext = text[start_indx:end_indx]\nchar_set = set(text)\nprint('Total Length:', len(text))\nprint('Unique Characters:', len(char_set))\n\n\n\n\n\n\n\n\nchars_sorted = sorted(char_set)\nchar2int = {ch:i for i,ch in enumerate(chars_sorted)}\nchar_array = np.array(chars_sorted)\n\ntext_encoded = np.array(\n    [char2int[ch] for ch in text],\n    dtype=np.int32)\n\nprint('Text encoded shape: ', text_encoded.shape)\n\nprint(text[:15], '     == Encoding ==> ', text_encoded[:15])\nprint(text_encoded[15:21], ' == Reverse  ==> ', ''.join(char_array[text_encoded[15:21]]))\n\n\n\n\n\n\n\n\n\n\nds_text_encoded = tf.data.Dataset.from_tensor_slices(text_encoded)\n\nfor ex in ds_text_encoded.take(5):\n    print('{} -> {}'.format(ex.numpy(), char_array[ex.numpy()]))\n\n\n\n\nseq_length = 40\nchunk_size = seq_length + 1\n\nds_chunks = ds_text_encoded.batch(chunk_size, drop_remainder=True)\n\n## inspection:\nfor seq in ds_chunks.take(1):\n    input_seq = seq[:seq_length].numpy()\n    target = seq[seq_length].numpy()\n    print(input_seq, ' -> ', target)\n    print(repr(''.join(char_array[input_seq])), \n          ' -> ', repr(''.join(char_array[target])))\n\n\n\n\n\n\n\n\n## define the function for splitting x & y\ndef split_input_target(chunk):\n    input_seq = chunk[:-1]\n    target_seq = chunk[1:]\n    return input_seq, target_seq\n\nds_sequences = ds_chunks.map(split_input_target)\n\n## inspection:\nfor example in ds_sequences.take(2):\n    print(' Input (x):', repr(''.join(char_array[example[0].numpy()])))\n    print('Target (y):', repr(''.join(char_array[example[1].numpy()])))\n    print()\n\n\n\n\n# Batch size\nBATCH_SIZE = 64\nBUFFER_SIZE = 10000\n\ntf.random.set_seed(1)\nds = ds_sequences.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)# drop_remainder=True)\n\nds\n\n\n# ### Building a character-level RNN model\n\n\n\ndef build_model(vocab_size, embedding_dim, rnn_units):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n        tf.keras.layers.LSTM(\n            rnn_units, return_sequences=True),\n        tf.keras.layers.Dense(vocab_size)\n    ])\n    return model\n\n\ncharset_size = len(char_array)\nembedding_dim = 256\nrnn_units = 512\n\ntf.random.set_seed(1)\n\nmodel = build_model(\n    vocab_size = charset_size,\n    embedding_dim=embedding_dim,\n    rnn_units=rnn_units)\n\nmodel.summary()\n\n\n\n\nmodel.compile(\n    optimizer='adam', \n    loss=tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True\n    ))\n\nmodel.fit(ds, epochs=20)\n\n\n# ### Evaluation phase: generating new text passages\n\n\n\ntf.random.set_seed(1)\n\nlogits = [[1.0, 1.0, 1.0]]\nprint('Probabilities:', tf.math.softmax(logits).numpy()[0])\n\nsamples = tf.random.categorical(\n    logits=logits, num_samples=10)\ntf.print(samples.numpy())\n\n\n\n\ntf.random.set_seed(1)\n\nlogits = [[1.0, 1.0, 3.0]]\nprint('Probabilities:', tf.math.softmax(logits).numpy()[0])\n\nsamples = tf.random.categorical(\n    logits=logits, num_samples=10)\ntf.print(samples.numpy())\n\n\n\n\ndef sample(model, starting_str, \n           len_generated_text=500, \n           max_input_length=40,\n           scale_factor=1.0):\n    encoded_input = [char2int[s] for s in starting_str]\n    encoded_input = tf.reshape(encoded_input, (1, -1))\n\n    generated_str = starting_str\n\n    model.reset_states()\n    for i in range(len_generated_text):\n        logits = model(encoded_input)\n        logits = tf.squeeze(logits, 0)\n\n        scaled_logits = logits * scale_factor\n        new_char_indx = tf.random.categorical(\n            scaled_logits, num_samples=1)\n        \n        new_char_indx = tf.squeeze(new_char_indx)[-1].numpy()    \n\n        generated_str += str(char_array[new_char_indx])\n        \n        new_char_indx = tf.expand_dims([new_char_indx], 0)\n        encoded_input = tf.concat(\n            [encoded_input, new_char_indx],\n            axis=1)\n        encoded_input = encoded_input[:, -max_input_length:]\n\n    return generated_str\n\ntf.random.set_seed(1)\nprint(sample(model, starting_str='The island'))\n\n\n# * **Predictability vs. randomness**\n\n\n\nlogits = np.array([[1.0, 1.0, 3.0]])\n\nprint('Probabilities before scaling:        ', tf.math.softmax(logits).numpy()[0])\n\nprint('Probabilities after scaling with 0.5:', tf.math.softmax(0.5*logits).numpy()[0])\n\nprint('Probabilities after scaling with 0.1:', tf.math.softmax(0.1*logits).numpy()[0])\n\n\n\n\ntf.random.set_seed(1)\nprint(sample(model, starting_str='The island', \n             scale_factor=2.0))\n\n\n\n\ntf.random.set_seed(1)\nprint(sample(model, starting_str='The island', \n             scale_factor=0.5))\n\n\n# # Understanding language with the Transformer model\n# \n# ## Understanding the self-attention mechanism\n# \n# ## A basic version of self-attention\n# \n# \n\n\n\n\n\n# ### Parameterizing the self-attention mechanism with query, key, and value weights\n# \n\n# \n# ## Multi-head attention and the Transformer block\n\n\n\n\n\n# \n# ...\n# \n# \n# # Summary\n# \n# ...\n# \n\n# \n# \n# Readers may ignore the next cell.\n# \n\n\n\n\n"""
ch17/ch17_part1.py,37,"b'# coding: utf-8\n\n\nimport tensorflow as tf\n#from google.colab import drive\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport itertools\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com) & [Vahid Mirjalili](http://vahidmirjalili.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Chapter 17 - Generative Adversarial Networks for Synthesizing New Data (Part 1/2)\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n\n\n\n\n# # Introducing generative adversarial networks\n# \n# ## Starting with autoencoders\n\n\n\n\n\n# ## Generative models for synthesizing new data\n\n\n\n\n\n# ## Generating new samples with GANs\n\n\n\n\n\n# ## Understanding the loss functions for the generator and discriminator networks in a GAN model\n\n\n\n\n\n# # Implementing a GAN from scratch\n# \n\n# ## Training GAN models on Google Colab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Uncomment the following line if running this notebook on Google Colab\n#! pip install -q tensorflow-gpu==2.0.0\n\n\n\n\nprint(tf.__version__)\n\nprint(""GPU Available:"", tf.test.is_gpu_available())\n\nif tf.test.is_gpu_available():\n    device_name = tf.test.gpu_device_name()\n\nelse:\n    device_name = \'cpu:0\'\n    \nprint(device_name)\n\n\n\n\n#drive.mount(\'/content/drive/\')\n\n\n# ## Implementing the generator and the discriminator networks \n\n\n\n\n\n\n\n\n\n\n\n## define a function for the generator:\ndef make_generator_network(\n        num_hidden_layers=1,\n        num_hidden_units=100,\n        num_output_units=784):\n    model = tf.keras.Sequential()\n    for i in range(num_hidden_layers):\n        model.add(\n            tf.keras.layers.Dense(\n                units=num_hidden_units, \n                use_bias=False)\n            )\n        model.add(tf.keras.layers.LeakyReLU())\n        \n    model.add(tf.keras.layers.Dense(\n        units=num_output_units, activation=\'tanh\'))\n    return model\n\n## define a function for the discriminator:\ndef make_discriminator_network(\n        num_hidden_layers=1,\n        num_hidden_units=100,\n        num_output_units=1):\n    model = tf.keras.Sequential()\n    for i in range(num_hidden_layers):\n        model.add(tf.keras.layers.Dense(units=num_hidden_units))\n        model.add(tf.keras.layers.LeakyReLU())\n        model.add(tf.keras.layers.Dropout(rate=0.5))\n        \n    model.add(\n        tf.keras.layers.Dense(\n            units=num_output_units, \n            activation=None)\n        )\n    return model\n\n\n\n\nimage_size = (28, 28)\nz_size = 20\nmode_z = \'uniform\'  # \'uniform\' vs. \'normal\'\ngen_hidden_layers = 1\ngen_hidden_size = 100\ndisc_hidden_layers = 1\ndisc_hidden_size = 100\n\ntf.random.set_seed(1)\n\ngen_model = make_generator_network(\n    num_hidden_layers=gen_hidden_layers, \n    num_hidden_units=gen_hidden_size,\n    num_output_units=np.prod(image_size))\n\ngen_model.build(input_shape=(None, z_size))\ngen_model.summary()\n\n\n\n\ndisc_model = make_discriminator_network(\n    num_hidden_layers=disc_hidden_layers,\n    num_hidden_units=disc_hidden_size)\n\ndisc_model.build(input_shape=(None, np.prod(image_size)))\ndisc_model.summary()\n\n\n# ## Defining the training dataset\n\n\n\nmnist_bldr = tfds.builder(\'mnist\')\nmnist_bldr.download_and_prepare()\nmnist = mnist_bldr.as_dataset(shuffle_files=False)\n\ndef preprocess(ex, mode=\'uniform\'):\n    image = ex[\'image\']\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.reshape(image, [-1])\n    image = image*2 - 1.0\n    if mode == \'uniform\':\n        input_z = tf.random.uniform(\n            shape=(z_size,), minval=-1.0, maxval=1.0)\n    elif mode == \'normal\':\n        input_z = tf.random.normal(shape=(z_size,))\n    return input_z, image\n\n\n\nmnist_trainset = mnist[\'train\']\n\nprint(\'Before preprocessing:  \')\nexample = next(iter(mnist_trainset))[\'image\']\nprint(\'dtype: \', example.dtype, \' Min: {} Max: {}\'.format(np.min(example), np.max(example)))\n\nmnist_trainset = mnist_trainset.map(preprocess)\n\nprint(\'After preprocessing:  \')\nexample = next(iter(mnist_trainset))[0]\nprint(\'dtype: \', example.dtype, \' Min: {} Max: {}\'.format(np.min(example), np.max(example)))\n\n\n#  * **Step-by-step walk through the data-flow**\n\n\n\nmnist_trainset = mnist_trainset.batch(32, drop_remainder=True)\ninput_z, input_real = next(iter(mnist_trainset))\nprint(\'input-z -- shape:\', input_z.shape)\nprint(\'input-real -- shape:\', input_real.shape)\n\ng_output = gen_model(input_z)\nprint(\'Output of G -- shape:\', g_output.shape)\n\nd_logits_real = disc_model(input_real)\nd_logits_fake = disc_model(g_output)\nprint(\'Disc. (real) -- shape:\', d_logits_real.shape)\nprint(\'Disc. (fake) -- shape:\', d_logits_fake.shape)\n\n\n# ## Training the GAN model\n\n\n\nloss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n## Loss for the Generator\ng_labels_real = tf.ones_like(d_logits_fake)\ng_loss = loss_fn(y_true=g_labels_real, y_pred=d_logits_fake)\nprint(\'Generator Loss: {:.4f}\'.format(g_loss))\n\n## Loss for the Discriminator\nd_labels_real = tf.ones_like(d_logits_real)\nd_labels_fake = tf.zeros_like(d_logits_fake)\n\nd_loss_real = loss_fn(y_true=d_labels_real, y_pred=d_logits_real)\nd_loss_fake = loss_fn(y_true=d_labels_fake, y_pred=d_logits_fake)\nprint(\'Discriminator Losses: Real {:.4f} Fake {:.4f}\'\n      .format(d_loss_real.numpy(), d_loss_fake.numpy()))\n\n\n#  * **Final training**\n\n\n\n\n\nnum_epochs = 100\nbatch_size = 64\nimage_size = (28, 28)\nz_size = 20\nmode_z = \'uniform\'\ngen_hidden_layers = 1\ngen_hidden_size = 100\ndisc_hidden_layers = 1\ndisc_hidden_size = 100\n\ntf.random.set_seed(1)\nnp.random.seed(1)\n\n\nif mode_z == \'uniform\':\n    fixed_z = tf.random.uniform(\n        shape=(batch_size, z_size),\n        minval=-1, maxval=1)\nelif mode_z == \'normal\':\n    fixed_z = tf.random.normal(\n        shape=(batch_size, z_size))\n\n\ndef create_samples(g_model, input_z):\n    g_output = g_model(input_z, training=False)\n    images = tf.reshape(g_output, (batch_size, *image_size))    \n    return (images+1)/2.0\n\n## Set-up the dataset\nmnist_trainset = mnist[\'train\']\nmnist_trainset = mnist_trainset.map(\n    lambda ex: preprocess(ex, mode=mode_z))\n\nmnist_trainset = mnist_trainset.shuffle(10000)\nmnist_trainset = mnist_trainset.batch(\n    batch_size, drop_remainder=True)\n\n## Set-up the model\nwith tf.device(device_name):\n    gen_model = make_generator_network(\n        num_hidden_layers=gen_hidden_layers, \n        num_hidden_units=gen_hidden_size,\n        num_output_units=np.prod(image_size))\n    gen_model.build(input_shape=(None, z_size))\n\n    disc_model = make_discriminator_network(\n        num_hidden_layers=disc_hidden_layers,\n        num_hidden_units=disc_hidden_size)\n    disc_model.build(input_shape=(None, np.prod(image_size)))\n\n## Loss function and optimizers:\nloss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\ng_optimizer = tf.keras.optimizers.Adam()\nd_optimizer = tf.keras.optimizers.Adam()\n\nall_losses = []\nall_d_vals = []\nepoch_samples = []\n\nstart_time = time.time()\nfor epoch in range(1, num_epochs+1):\n    epoch_losses, epoch_d_vals = [], []\n    for i,(input_z,input_real) in enumerate(mnist_trainset):\n        \n        ## Compute generator\'s loss\n        with tf.GradientTape() as g_tape:\n            g_output = gen_model(input_z)\n            d_logits_fake = disc_model(g_output, training=True)\n            labels_real = tf.ones_like(d_logits_fake)\n            g_loss = loss_fn(y_true=labels_real, y_pred=d_logits_fake)\n            \n        g_grads = g_tape.gradient(g_loss, gen_model.trainable_variables)\n        g_optimizer.apply_gradients(\n            grads_and_vars=zip(g_grads, gen_model.trainable_variables))\n\n        ## Compute discriminator\'s loss\n        with tf.GradientTape() as d_tape:\n            d_logits_real = disc_model(input_real, training=True)\n\n            d_labels_real = tf.ones_like(d_logits_real)\n            \n            d_loss_real = loss_fn(\n                y_true=d_labels_real, y_pred=d_logits_real)\n\n            d_logits_fake = disc_model(g_output, training=True)\n            d_labels_fake = tf.zeros_like(d_logits_fake)\n\n            d_loss_fake = loss_fn(\n                y_true=d_labels_fake, y_pred=d_logits_fake)\n\n            d_loss = d_loss_real + d_loss_fake\n\n        ## Compute the gradients of d_loss\n        d_grads = d_tape.gradient(d_loss, disc_model.trainable_variables)\n        \n        ## Optimization: Apply the gradients\n        d_optimizer.apply_gradients(\n            grads_and_vars=zip(d_grads, disc_model.trainable_variables))\n                           \n        epoch_losses.append(\n            (g_loss.numpy(), d_loss.numpy(), \n             d_loss_real.numpy(), d_loss_fake.numpy()))\n        \n        d_probs_real = tf.reduce_mean(tf.sigmoid(d_logits_real))\n        d_probs_fake = tf.reduce_mean(tf.sigmoid(d_logits_fake))\n        epoch_d_vals.append((d_probs_real.numpy(), d_probs_fake.numpy()))        \n    all_losses.append(epoch_losses)\n    all_d_vals.append(epoch_d_vals)\n    print(\n        \'Epoch {:03d} | ET {:.2f} min | Avg Losses >>\'\n        \' G/D {:.4f}/{:.4f} [D-Real: {:.4f} D-Fake: {:.4f}]\'\n        .format(\n            epoch, (time.time() - start_time)/60, \n            *list(np.mean(all_losses[-1], axis=0))))\n    epoch_samples.append(\n        create_samples(gen_model, fixed_z).numpy())\n\n\n\n\n#import pickle\n# pickle.dump({\'all_losses\':all_losses, \n#              \'all_d_vals\':all_d_vals,\n#              \'samples\':epoch_samples}, \n#             open(\'/content/drive/My Drive/Colab Notebooks/PyML-3rd-edition/ch17-vanila-learning.pkl\', \'wb\'))\n\n#gen_model.save(\'/content/drive/My Drive/Colab Notebooks/PyML-3rd-edition/ch17-vanila-gan_gen.h5\')\n#disc_model.save(\'/content/drive/My Drive/Colab Notebooks/PyML-3rd-edition/ch17-vanila-gan_disc.h5\')\n\n\n\n\n\n\nfig = plt.figure(figsize=(16, 6))\n\n## Plotting the losses\nax = fig.add_subplot(1, 2, 1)\ng_losses = [item[0] for item in itertools.chain(*all_losses)]\nd_losses = [item[1]/2.0 for item in itertools.chain(*all_losses)]\nplt.plot(g_losses, label=\'Generator loss\', alpha=0.95)\nplt.plot(d_losses, label=\'Discriminator loss\', alpha=0.95)\nplt.legend(fontsize=20)\nax.set_xlabel(\'Iteration\', size=15)\nax.set_ylabel(\'Loss\', size=15)\n\nepochs = np.arange(1, 101)\nepoch2iter = lambda e: e*len(all_losses[-1])\nepoch_ticks = [1, 20, 40, 60, 80, 100]\nnewpos = [epoch2iter(e) for e in epoch_ticks]\nax2 = ax.twiny()\nax2.set_xticks(newpos)\nax2.set_xticklabels(epoch_ticks)\nax2.xaxis.set_ticks_position(\'bottom\')\nax2.xaxis.set_label_position(\'bottom\')\nax2.spines[\'bottom\'].set_position((\'outward\', 60))\nax2.set_xlabel(\'Epoch\', size=15)\nax2.set_xlim(ax.get_xlim())\nax.tick_params(axis=\'both\', which=\'major\', labelsize=15)\nax2.tick_params(axis=\'both\', which=\'major\', labelsize=15)\n\n## Plotting the outputs of the discriminator\nax = fig.add_subplot(1, 2, 2)\nd_vals_real = [item[0] for item in itertools.chain(*all_d_vals)]\nd_vals_fake = [item[1] for item in itertools.chain(*all_d_vals)]\nplt.plot(d_vals_real, alpha=0.75, label=r\'Real: $D(\\mathbf{x})$\')\nplt.plot(d_vals_fake, alpha=0.75, label=r\'Fake: $D(G(\\mathbf{z}))$\')\nplt.legend(fontsize=20)\nax.set_xlabel(\'Iteration\', size=15)\nax.set_ylabel(\'Discriminator output\', size=15)\n\nax2 = ax.twiny()\nax2.set_xticks(newpos)\nax2.set_xticklabels(epoch_ticks)\nax2.xaxis.set_ticks_position(\'bottom\')\nax2.xaxis.set_label_position(\'bottom\')\nax2.spines[\'bottom\'].set_position((\'outward\', 60))\nax2.set_xlabel(\'Epoch\', size=15)\nax2.set_xlim(ax.get_xlim())\nax.tick_params(axis=\'both\', which=\'major\', labelsize=15)\nax2.tick_params(axis=\'both\', which=\'major\', labelsize=15)\n\n\n#plt.savefig(\'images/ch17-gan-learning-curve.pdf\')\nplt.show()\n\n\n\n\nselected_epochs = [1, 2, 4, 10, 50, 100]\nfig = plt.figure(figsize=(10, 14))\nfor i,e in enumerate(selected_epochs):\n    for j in range(5):\n        ax = fig.add_subplot(6, 5, i*5+j+1)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        if j == 0:\n            ax.text(\n                -0.06, 0.5, \'Epoch {}\'.format(e),\n                rotation=90, size=18, color=\'red\',\n                horizontalalignment=\'right\',\n                verticalalignment=\'center\', \n                transform=ax.transAxes)\n        \n        image = epoch_samples[e-1][j]\n        ax.imshow(image, cmap=\'gray_r\')\n    \n#plt.savefig(\'images/ch17-vanila-gan-samples.pdf\')\nplt.show()\n\n\n# \n# ----\n\n# \n# \n# Readers may ignore the next cell.\n# \n\n\n\n\n'"
ch17/ch17_part2.py,47,"b'# coding: utf-8\n\n\n#from google.colab import drive\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport itertools\n\n# *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com) & [Vahid Mirjalili](http://vahidmirjalili.com), Packt Publishing Ltd. 2019\n# \n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n# \n# Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n# # Chapter 17 - Generative Adversarial Networks for Synthesizing New Data (Part 2/2)\n\n# Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n\n\n\n\n\n\n\n\n# # Improving the quality of synthesized images using a convolutional and Wasserstein GAN\n\n# ## Transposed convolution\n\n\n\n\n\n\n\n\n\n# ## Batch normalization\n\n\n\n\n\n# ## Implementing the generator and discriminator\n\n\n\n\n\n\n\n\n\n#  * **Setting up the Google Colab**\n\n\n\n#! pip install -q tensorflow-gpu==2.0.0-beta1\n\n\n\n\n#drive.mount(\'/content/drive/\')\n\n\n\n\n\n\nprint(tf.__version__)\n\nprint(""GPU Available:"", tf.test.is_gpu_available())\n\nif tf.test.is_gpu_available():\n    device_name = tf.test.gpu_device_name()\n\nelse:\n    device_name = \'CPU:0\'\n    \nprint(device_name)\n\n\n\n\n\n\n\n\ndef make_dcgan_generator(\n        z_size=20, \n        output_size=(28, 28, 1),\n        n_filters=128, \n        n_blocks=2):\n    size_factor = 2**n_blocks\n    hidden_size = (\n        output_size[0]//size_factor, \n        output_size[1]//size_factor\n    )\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=(z_size,)),\n        \n        tf.keras.layers.Dense(\n            units=n_filters*np.prod(hidden_size), \n            use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.LeakyReLU(),\n        tf.keras.layers.Reshape(\n            (hidden_size[0], hidden_size[1], n_filters)),\n    \n        tf.keras.layers.Conv2DTranspose(\n            filters=n_filters, kernel_size=(5, 5), strides=(1, 1),\n            padding=\'same\', use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.LeakyReLU()\n    ])\n        \n    nf = n_filters\n    for i in range(n_blocks):\n        nf = nf // 2\n        model.add(\n            tf.keras.layers.Conv2DTranspose(\n                filters=nf, kernel_size=(5, 5), strides=(2, 2),\n                padding=\'same\', use_bias=False))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.LeakyReLU())\n                \n    model.add(\n        tf.keras.layers.Conv2DTranspose(\n            filters=output_size[2], kernel_size=(5, 5), \n            strides=(1, 1), padding=\'same\', use_bias=False, \n            activation=\'tanh\'))\n        \n    return model\n\ndef make_dcgan_discriminator(\n        input_size=(28, 28, 1),\n        n_filters=64, \n        n_blocks=2):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=input_size),\n        tf.keras.layers.Conv2D(\n            filters=n_filters, kernel_size=5, \n            strides=(1, 1), padding=\'same\'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.LeakyReLU()\n    ])\n    \n    nf = n_filters\n    for i in range(n_blocks):\n        nf = nf*2\n        model.add(\n            tf.keras.layers.Conv2D(\n                filters=nf, kernel_size=(5, 5), \n                strides=(2, 2),padding=\'same\'))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.LeakyReLU())\n        model.add(tf.keras.layers.Dropout(0.3))\n        \n    model.add(tf.keras.layers.Conv2D(\n            filters=1, kernel_size=(7, 7), padding=\'valid\'))\n    \n    model.add(tf.keras.layers.Reshape((1,)))\n    \n    return model\n\n\n\n\ngen_model = make_dcgan_generator()\ngen_model.summary()\n\ndisc_model = make_dcgan_discriminator()\ndisc_model.summary()\n\n\n# ## Dissimilarity measures between two distributions \n\n\n\n\n\n\n\n\n\n# ## Using EM distance in practice for GANs\n\n# ## Gradient penalty \n\n# ## Implementing WGAN-GP to train the DCGAN model\n\n\n\nmnist_bldr = tfds.builder(\'mnist\')\nmnist_bldr.download_and_prepare()\nmnist = mnist_bldr.as_dataset(shuffle_files=False)\n\ndef preprocess(ex, mode=\'uniform\'):\n    image = ex[\'image\']\n    image = tf.image.convert_image_dtype(image, tf.float32)\n\n    image = image*2 - 1.0\n    if mode == \'uniform\':\n        input_z = tf.random.uniform(\n            shape=(z_size,), minval=-1.0, maxval=1.0)\n    elif mode == \'normal\':\n        input_z = tf.random.normal(shape=(z_size,))\n    return input_z, image\n\n\n\n\nnum_epochs = 100\nbatch_size = 128\nimage_size = (28, 28)\nz_size = 20\nmode_z = \'uniform\'\nlambda_gp = 10.0\n\ntf.random.set_seed(1)\nnp.random.seed(1)\n\n## Set-up the dataset\nmnist_trainset = mnist[\'train\']\nmnist_trainset = mnist_trainset.map(preprocess)\n\nmnist_trainset = mnist_trainset.shuffle(10000)\nmnist_trainset = mnist_trainset.batch(\n    batch_size, drop_remainder=True)\n\n## Set-up the model\nwith tf.device(device_name):\n    gen_model = make_dcgan_generator()\n    gen_model.build(input_shape=(None, z_size))\n    gen_model.summary()\n\n    disc_model = make_dcgan_discriminator()\n    disc_model.build(input_shape=(None, np.prod(image_size)))\n    disc_model.summary()\n\n\n\n\n\n\n## optimizers:\ng_optimizer = tf.keras.optimizers.Adam(0.0002)\nd_optimizer = tf.keras.optimizers.Adam(0.0002)\n\nif mode_z == \'uniform\':\n    fixed_z = tf.random.uniform(\n        shape=(batch_size, z_size),\n        minval=-1, maxval=1)\nelif mode_z == \'normal\':\n    fixed_z = tf.random.normal(\n        shape=(batch_size, z_size))\n\ndef create_samples(g_model, input_z):\n    g_output = g_model(input_z, training=False)\n    images = tf.reshape(g_output, (batch_size, *image_size))    \n    return (images+1)/2.0\n\nall_losses = []\nepoch_samples = []\n\nstart_time = time.time()\n\nfor epoch in range(1, num_epochs+1):\n    epoch_losses = []\n    for i,(input_z,input_real) in enumerate(mnist_trainset):\n        \n        ## Compute discriminator\'s loss and gradients:\n        with tf.GradientTape() as d_tape, tf.GradientTape() as g_tape:\n            g_output = gen_model(input_z, training=True)\n            \n            d_critics_real = disc_model(input_real, training=True)\n            d_critics_fake = disc_model(g_output, training=True)\n\n            ## Compute generator\'s loss:\n            g_loss = -tf.math.reduce_mean(d_critics_fake)\n\n            ## Compute discriminator\'s losses\n            d_loss_real = -tf.math.reduce_mean(d_critics_real)\n            d_loss_fake =  tf.math.reduce_mean(d_critics_fake)\n            d_loss = d_loss_real + d_loss_fake\n\n            ## Gradient penalty:\n            with tf.GradientTape() as gp_tape:\n                alpha = tf.random.uniform(\n                    shape=[d_critics_real.shape[0], 1, 1, 1], \n                    minval=0.0, maxval=1.0)\n                interpolated = (\n                    alpha*input_real + (1-alpha)*g_output)\n                gp_tape.watch(interpolated)\n                d_critics_intp = disc_model(interpolated)\n            \n            grads_intp = gp_tape.gradient(\n                d_critics_intp, [interpolated,])[0]\n            grads_intp_l2 = tf.sqrt(\n                tf.reduce_sum(tf.square(grads_intp), axis=[1, 2, 3]))\n            grad_penalty = tf.reduce_mean(tf.square(grads_intp_l2 - 1.0))\n        \n            d_loss = d_loss + lambda_gp*grad_penalty\n        \n        ## Optimization: Compute the gradients apply them\n        d_grads = d_tape.gradient(d_loss, disc_model.trainable_variables)\n        d_optimizer.apply_gradients(\n            grads_and_vars=zip(d_grads, disc_model.trainable_variables))\n        \n        g_grads = g_tape.gradient(g_loss, gen_model.trainable_variables)\n        g_optimizer.apply_gradients(\n            grads_and_vars=zip(g_grads, gen_model.trainable_variables))\n\n        epoch_losses.append(\n            (g_loss.numpy(), d_loss.numpy(), \n             d_loss_real.numpy(), d_loss_fake.numpy()))\n                    \n    all_losses.append(epoch_losses)\n    \n    print(\'Epoch {:-3d} | ET {:.2f} min | Avg Losses >>\'\n          \' G/D {:6.2f}/{:6.2f} [D-Real: {:6.2f} D-Fake: {:6.2f}]\'\n          .format(epoch, (time.time() - start_time)/60, \n                  *list(np.mean(all_losses[-1], axis=0)))\n    )\n    \n    epoch_samples.append(\n        create_samples(gen_model, fixed_z).numpy()\n    )\n\n\n\n\n#import pickle\n#pickle.dump({\'all_losses\':all_losses, \n#             \'samples\':epoch_samples}, \n#            open(\'/content/drive/My Drive/Colab Notebooks/PyML-3rd-edition/ch17-WDCGAN-learning.pkl\', \'wb\'))\n\n#gen_model.save(\'/content/drive/My Drive/Colab Notebooks/PyML-3rd-edition/ch17-WDCGAN-gan_gen.h5\')\n#disc_model.save(\'/content/drive/My Drive/Colab Notebooks/PyML-3rd-edition/ch17-WDCGAN-gan_disc.h5\')\n\n\n\n\n\n\nfig = plt.figure(figsize=(8, 6))\n\n## Plotting the losses\nax = fig.add_subplot(1, 1, 1)\ng_losses = [item[0] for item in itertools.chain(*all_losses)]\nd_losses = [item[1] for item in itertools.chain(*all_losses)]\nplt.plot(g_losses, label=\'Generator loss\', alpha=0.95)\nplt.plot(d_losses, label=\'Discriminator loss\', alpha=0.95)\nplt.legend(fontsize=20)\nax.set_xlabel(\'Iteration\', size=15)\nax.set_ylabel(\'Loss\', size=15)\n\nepochs = np.arange(1, 101)\nepoch2iter = lambda e: e*len(all_losses[-1])\nepoch_ticks = [1, 20, 40, 60, 80, 100]\nnewpos   = [epoch2iter(e) for e in epoch_ticks]\nax2 = ax.twiny()\nax2.set_xticks(newpos)\nax2.set_xticklabels(epoch_ticks)\nax2.xaxis.set_ticks_position(\'bottom\')\nax2.xaxis.set_label_position(\'bottom\')\nax2.spines[\'bottom\'].set_position((\'outward\', 60))\nax2.set_xlabel(\'Epoch\', size=15)\nax2.set_xlim(ax.get_xlim())\nax.tick_params(axis=\'both\', which=\'major\', labelsize=15)\nax2.tick_params(axis=\'both\', which=\'major\', labelsize=15)\n\n#plt.savefig(\'images/ch17-wdcgan-learning-curve.pdf\')\nplt.show()\n\n\n\n\nselected_epochs = [1, 2, 4, 10, 50, 100]\nfig = plt.figure(figsize=(10, 14))\nfor i,e in enumerate(selected_epochs):\n    for j in range(5):\n        ax = fig.add_subplot(6, 5, i*5+j+1)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        if j == 0:\n            ax.text(\n                -0.06, 0.5, \'Epoch {}\'.format(e),\n                rotation=90, size=18, color=\'red\',\n                horizontalalignment=\'right\',\n                verticalalignment=\'center\', \n                transform=ax.transAxes)\n        \n        image = epoch_samples[e-1][j]\n        ax.imshow(image, cmap=\'gray_r\')\n    \n#plt.savefig(\'images/ch17-wdcgan-samples.pdf\')\nplt.show()\n\n\n# ## Mode collapse\n\n\n\n\n\n# \n# ----\n\n# \n# \n# Readers may ignore the next cell.\n# \n# \n\n\n\n\n'"
ch09/1st_flask_app_1/app.py,0,"b""from flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return render_template('first_app.html')\n\nif __name__ == '__main__':\n    app.run(debug=True)"""
ch09/1st_flask_app_2/app.py,0,"b""from flask import Flask, render_template, request\nfrom wtforms import Form, TextAreaField, validators\n\napp = Flask(__name__)\n\nclass HelloForm(Form):\n    sayhello = TextAreaField('',[validators.DataRequired()])\n\n@app.route('/')\ndef index():\n    form = HelloForm(request.form)\n    return render_template('first_app.html', form=form)\n\n@app.route('/hello', methods=['POST'])\ndef hello():\n    form = HelloForm(request.form)\n    if request.method == 'POST' and form.validate():\n        name = request.form['sayhello']\n        return render_template('hello.html', name=name)\n    return render_template('first_app.html', form=form)\n\nif __name__ == '__main__':\n    app.run(debug=True)"""
ch09/movieclassifier/app.py,0,"b'from flask import Flask, render_template, request\nfrom wtforms import Form, TextAreaField, validators\nimport pickle\nimport sqlite3\nimport os\nimport numpy as np\n\n# import HashingVectorizer from local dir\nfrom vectorizer import vect\n\napp = Flask(__name__)\n\n######## Preparing the Classifier\ncur_dir = os.path.dirname(__file__)\nclf = pickle.load(open(os.path.join(cur_dir,\n                 \'pkl_objects\',\n                 \'classifier.pkl\'), \'rb\'))\ndb = os.path.join(cur_dir, \'reviews.sqlite\')\n\ndef classify(document):\n    label = {0: \'negative\', 1: \'positive\'}\n    X = vect.transform([document])\n    y = clf.predict(X)[0]\n    proba = np.max(clf.predict_proba(X))\n    return label[y], proba\n\ndef train(document, y):\n    X = vect.transform([document])\n    clf.partial_fit(X, [y])\n\ndef sqlite_entry(path, document, y):\n    conn = sqlite3.connect(path)\n    c = conn.cursor()\n    c.execute(""INSERT INTO review_db (review, sentiment, date)""\\\n    "" VALUES (?, ?, DATETIME(\'now\'))"", (document, y))\n    conn.commit()\n    conn.close()\n\n######## Flask\nclass ReviewForm(Form):\n    moviereview = TextAreaField(\'\',\n                                [validators.DataRequired(),\n                                validators.length(min=15)])\n\n@app.route(\'/\')\ndef index():\n    form = ReviewForm(request.form)\n    return render_template(\'reviewform.html\', form=form)\n\n@app.route(\'/results\', methods=[\'POST\'])\ndef results():\n    form = ReviewForm(request.form)\n    if request.method == \'POST\' and form.validate():\n        review = request.form[\'moviereview\']\n        y, proba = classify(review)\n        return render_template(\'results.html\',\n                                content=review,\n                                prediction=y,\n                                probability=round(proba*100, 2))\n    return render_template(\'reviewform.html\', form=form)\n\n@app.route(\'/thanks\', methods=[\'POST\'])\ndef feedback():\n    feedback = request.form[\'feedback_button\']\n    review = request.form[\'review\']\n    prediction = request.form[\'prediction\']\n\n    inv_label = {\'negative\': 0, \'positive\': 1}\n    y = inv_label[prediction]\n    if feedback == \'Incorrect\':\n        y = int(not(y))\n    train(review, y)\n    sqlite_entry(db, review, y)\n    return render_template(\'thanks.html\')\n\nif __name__ == \'__main__\':\n    app.run(debug=True)\n'"
ch09/movieclassifier/vectorizer.py,0,"b""from sklearn.feature_extraction.text import HashingVectorizer\nimport re\nimport os\nimport pickle\n\ncur_dir = os.path.dirname(__file__)\nstop = pickle.load(open(\n                os.path.join(cur_dir, \n                'pkl_objects', \n                'stopwords.pkl'), 'rb'))\n\ndef tokenizer(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n                           text.lower())\n    text = re.sub('[\\W]+', ' ', text.lower()) \\\n                   + ' '.join(emoticons).replace('-', '')\n    tokenized = [w for w in text.split() if w not in stop]\n    return tokenized\n\nvect = HashingVectorizer(decode_error='ignore',\n                         n_features=2**21,\n                         preprocessor=None,\n                         tokenizer=tokenizer)\n"""
ch09/movieclassifier_with_update/app.py,0,"b'from flask import Flask, render_template, request\nfrom wtforms import Form, TextAreaField, validators\nimport pickle\nimport sqlite3\nimport os\nimport numpy as np\n\n# import HashingVectorizer from local dir\nfrom vectorizer import vect\n\napp = Flask(__name__)\n\n######## Preparing the Classifier\ncur_dir = os.path.dirname(__file__)\nclf = pickle.load(open(os.path.join(cur_dir,\n                 \'pkl_objects\',\n                 \'classifier.pkl\'), \'rb\'))\ndb = os.path.join(cur_dir, \'reviews.sqlite\')\n\ndef classify(document):\n    label = {0: \'negative\', 1: \'positive\'}\n    X = vect.transform([document])\n    y = clf.predict(X)[0]\n    proba = np.max(clf.predict_proba(X))\n    return label[y], proba\n\ndef train(document, y):\n    X = vect.transform([document])\n    clf.partial_fit(X, [y])\n\ndef sqlite_entry(path, document, y):\n    conn = sqlite3.connect(path)\n    c = conn.cursor()\n    c.execute(""INSERT INTO review_db (review, sentiment, date)""\\\n    "" VALUES (?, ?, DATETIME(\'now\'))"", (document, y))\n    conn.commit()\n    conn.close()\n\n######## Flask\nclass ReviewForm(Form):\n    moviereview = TextAreaField(\'\',\n                                [validators.DataRequired(),\n                                validators.length(min=15)])\n\n@app.route(\'/\')\ndef index():\n    form = ReviewForm(request.form)\n    return render_template(\'reviewform.html\', form=form)\n\n@app.route(\'/results\', methods=[\'POST\'])\ndef results():\n    form = ReviewForm(request.form)\n    if request.method == \'POST\' and form.validate():\n        review = request.form[\'moviereview\']\n        y, proba = classify(review)\n        return render_template(\'results.html\',\n                                content=review,\n                                prediction=y,\n                                probability=round(proba*100, 2))\n    return render_template(\'reviewform.html\', form=form)\n\n@app.route(\'/thanks\', methods=[\'POST\'])\ndef feedback():\n    feedback = request.form[\'feedback_button\']\n    review = request.form[\'review\']\n    prediction = request.form[\'prediction\']\n\n    inv_label = {\'negative\': 0, \'positive\': 1}\n    y = inv_label[prediction]\n    if feedback == \'Incorrect\':\n        y = int(not(y))\n    train(review, y)\n    sqlite_entry(db, review, y)\n    return render_template(\'thanks.html\')\n\nif __name__ == \'__main__\':\n    app.run(debug=True)\n'"
ch09/movieclassifier_with_update/update.py,0,"b""import pickle\nimport sqlite3\nimport numpy as np\nimport os\n\n# import HashingVectorizer from local dir\nfrom vectorizer import vect\n\n\ndef update_model(db_path, model, batch_size=10000):\n\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n    c.execute('SELECT * from review_db')\n\n    results = c.fetchmany(batch_size)\n    while results:\n        data = np.array(results)\n        X = data[:, 0]\n        y = data[:, 1].astype(int)\n\n        classes = np.array([0, 1])\n        X_train = vect.transform(X)\n        model.partial_fit(X_train, y, classes=classes)\n        results = c.fetchmany(batch_size)\n\n    conn.close()\n    return model\n\ncur_dir = os.path.dirname(__file__)\n\nclf = pickle.load(open(os.path.join(cur_dir,\n                  'pkl_objects',\n                  'classifier.pkl'), 'rb'))\ndb = os.path.join(cur_dir, 'reviews.sqlite')\n\nclf = update_model(db_path=db, model=clf, batch_size=10000)\n\n# Uncomment the following lines if you are sure that\n# you want to update your classifier.pkl file\n# permanently.\n\n# pickle.dump(clf, open(os.path.join(cur_dir,\n#             'pkl_objects', 'classifier.pkl'), 'wb')\n#             , protocol=4)\n"""
ch09/movieclassifier_with_update/vectorizer.py,0,"b""from sklearn.feature_extraction.text import HashingVectorizer\nimport re\nimport os\nimport pickle\n\ncur_dir = os.path.dirname(__file__)\nstop = pickle.load(open(\n                os.path.join(cur_dir,\n                'pkl_objects',\n                'stopwords.pkl'), 'rb'))\n\ndef tokenizer(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n                           text.lower())\n    text = re.sub('[\\W]+', ' ', text.lower()) \\\n                   + ' '.join(emoticons).replace('-', '')\n    tokenized = [w for w in text.split() if w not in stop]\n    return tokenized\n\nvect = HashingVectorizer(decode_error='ignore',\n                         n_features=2**21,\n                         preprocessor=None,\n                         tokenizer=tokenizer)\n"""
ch09/pickle-test-scripts/pickle-dump-test.py,0,"b""import pickle\nimport os\nimport re\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.linear_model import SGDClassifier\n\n\nstop = stopwords.words('english')\n\n\ndef tokenizer(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n        ' '.join(emoticons).replace('-', '')\n    tokenized = [w for w in text.split() if w not in stop]\n    return tokenized\n\nvect = HashingVectorizer(decode_error='ignore',\n                         n_features=2**21,\n                         preprocessor=None,\n                         tokenizer=tokenizer)\n\nclf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n\n\ndf = pd.read_csv('./movie_data_small.csv', encoding='utf-8')\n\n#df.loc[:100, :].to_csv('./movie_data_small.csv', index=None)\n\n\nX_train = df['review'].values\ny_train = df['sentiment'].values\n\nX_train = vect.transform(X_train)\nclf.fit(X_train, y_train)\n\npickle.dump(stop,\n            open('stopwords.pkl', 'wb'),\n            protocol=4)\n\npickle.dump(clf,\n            open('classifier.pkl', 'wb'),\n            protocol=4)\n"""
ch09/pickle-test-scripts/pickle-load-test.py,0,"b""import pickle\nimport re\nimport os\nfrom vectorizer import vect\nimport numpy as np\n\nclf = pickle.load(open('classifier.pkl', 'rb'))\n\n\nlabel = {0: 'negative', 1: 'positive'}\nexample = ['I love this movie']\n\nX = vect.transform(example)\n\nprint('Prediction: %s\\nProbability: %.2f%%' %\n      (label[clf.predict(X)[0]],\n       np.max(clf.predict_proba(X)) * 100))\n"""
ch09/pickle-test-scripts/vectorizer.py,0,"b""from sklearn.feature_extraction.text import HashingVectorizer\nimport re\nimport os\nimport pickle\n\n\nstop = pickle.load(open('stopwords.pkl', 'rb'))\n\n\ndef tokenizer(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n                           text.lower())\n    text = re.sub('[\\W]+', ' ', text.lower()) + \\\n        ' '.join(emoticons).replace('-', '')\n    tokenized = [w for w in text.split() if w not in stop]\n    return tokenized\n\nvect = HashingVectorizer(decode_error='ignore',\n                         n_features=2**21,\n                         preprocessor=None,\n                         tokenizer=tokenizer)\n"""
ch18/cartpole/main.py,6,"b""# coding: utf-8\n\n# Python Machine Learning 3rd Edition by\n# Sebastian Raschka (https://sebastianraschka.com) & Vahid Mirjalili](http://vahidmirjalili.com)\n# Packt Publishing Ltd. 2019\n#\n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n#\n# Code License: MIT License (https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n#################################################################################\n# Chapter 18 - Reinforcement Learning for Decision Making in Complex Environments\n#################################################################################\n\n# Script: carpole/main.py\n\nimport gym\nimport numpy as np\nimport tensorflow as tf\nimport random\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple\nfrom collections import deque\n\nnp.random.seed(1)\ntf.random.set_seed(1)\n\nTransition = namedtuple(\n    'Transition', ('state', 'action', 'reward',\n                   'next_state', 'done'))\n\n\nclass DQNAgent:\n    def __init__(\n            self, env, discount_factor=0.95,\n            epsilon_greedy=1.0, epsilon_min=0.01,\n            epsilon_decay=0.995, learning_rate=1e-3,\n            max_memory_size=2000):\n        self.env = env\n        self.state_size = env.observation_space.shape[0]\n        self.action_size = env.action_space.n\n\n        self.memory = deque(maxlen=max_memory_size)\n\n        self.gamma = discount_factor\n        self.epsilon = epsilon_greedy\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.lr = learning_rate\n        self._build_nn_model()\n\n    def _build_nn_model(self, n_layers=3):\n        self.model = tf.keras.Sequential()\n\n        # Hidden layers\n        for n in range(n_layers - 1):\n            self.model.add(tf.keras.layers.Dense(\n                units=32, activation='relu'))\n            self.model.add(tf.keras.layers.Dense(\n                units=32, activation='relu'))\n\n        # Last layer\n        self.model.add(tf.keras.layers.Dense(\n            units=self.action_size))\n\n        # Build & compile model\n        self.model.build(input_shape=(None, self.state_size))\n        self.model.compile(\n            loss='mse',\n            optimizer=tf.keras.optimizers.Adam(lr=self.lr))\n\n    def remember(self, transition):\n        self.memory.append(transition)\n\n    def choose_action(self, state):\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_size)\n        q_values = self.model.predict(state)[0]\n        return np.argmax(q_values)  # returns action\n\n    def _learn(self, batch_samples):\n        batch_states, batch_targets = [], []\n        for transition in batch_samples:\n            s, a, r, next_s, done = transition\n            if done:\n                target = r\n            else:\n                target = (r +\n                          self.gamma * np.amax(\n                            self.model.predict(next_s)[0]\n                            )\n                          )\n            target_all = self.model.predict(s)[0]\n            target_all[a] = target\n            batch_states.append(s.flatten())\n            batch_targets.append(target_all)\n            self._adjust_epsilon()\n        return self.model.fit(x=np.array(batch_states),\n                              y=np.array(batch_targets),\n                              epochs=1,\n                              verbose=0)\n\n    def _adjust_epsilon(self):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n    def replay(self, batch_size):\n        samples = random.sample(self.memory, batch_size)\n        history = self._learn(samples)\n        return history.history['loss'][0]\n\n\ndef plot_learning_history(history):\n    fig = plt.figure(1, figsize=(14, 5))\n    ax = fig.add_subplot(1, 1, 1)\n    episodes = np.arange(len(history[0])) + 1\n    plt.plot(episodes, history[0], lw=4,\n             marker='o', markersize=10)\n    ax.tick_params(axis='both', which='major', labelsize=15)\n    plt.xlabel('Episodes', size=20)\n    plt.ylabel('# Total Rewards', size=20)\n    plt.show()\n\n\n# General settings\nEPISODES = 200\nbatch_size = 32\ninit_replay_memory_size = 500\n\nif __name__ == '__main__':\n    env = gym.make('CartPole-v1')\n    agent = DQNAgent(env)\n    state = env.reset()\n    state = np.reshape(state, [1, agent.state_size])\n\n    # Filling up the replay-memory\n    for i in range(init_replay_memory_size):\n        action = agent.choose_action(state)\n        next_state, reward, done, _ = env.step(action)\n        next_state = np.reshape(next_state, [1, agent.state_size])\n        agent.remember(Transition(state, action, reward,\n                                  next_state, done))\n        if done:\n            state = env.reset()\n            state = np.reshape(state, [1, agent.state_size])\n        else:\n            state = next_state\n\n    total_rewards, losses = [], []\n    for e in range(EPISODES):\n        state = env.reset()\n        if e % 10 == 0:\n            env.render()\n        state = np.reshape(state, [1, agent.state_size])\n        for i in range(500):\n            action = agent.choose_action(state)\n            next_state, reward, done, _ = env.step(action)\n            next_state = np.reshape(next_state,\n                                    [1, agent.state_size])\n            agent.remember(Transition(state, action, reward,\n                                      next_state, done))\n            state = next_state\n            if e % 10 == 0:\n                env.render()\n            if done:\n                total_rewards.append(i)\n                print('Episode: %d/%d, Total reward: %d'\n                      % (e, EPISODES, i))\n                break\n            loss = agent.replay(batch_size)\n            losses.append(loss)\n    plot_learning_history(total_rewards)\n"""
ch18/gridworld/agent.py,0,"b'# coding: utf-8\n\n# Python Machine Learning 3rd Edition by\n# Sebastian Raschka (https://sebastianraschka.com) & Vahid Mirjalili](http://vahidmirjalili.com)\n# Packt Publishing Ltd. 2019\n#\n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n#\n# Code License: MIT License (https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n#################################################################################\n# Chapter 18 - Reinforcement Learning for Decision Making in Complex Environments\n#################################################################################\n\n# Script: agent.py\n\nfrom collections import defaultdict\nimport numpy as np\n\n\nclass Agent(object):\n    def __init__(\n            self, env,\n            learning_rate=0.01,\n            discount_factor=0.9,\n            epsilon_greedy=0.9,\n            epsilon_min=0.1,\n            epsilon_decay=0.95):\n        self.env = env\n        self.lr = learning_rate\n        self.gamma = discount_factor\n        self.epsilon = epsilon_greedy\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n\n        # Define the q_table\n        self.q_table = defaultdict(lambda: np.zeros(self.env.nA))\n\n    def choose_action(self, state):\n        if np.random.uniform() < self.epsilon:\n            action = np.random.choice(self.env.nA)\n        else:\n            q_vals = self.q_table[state]\n            perm_actions = np.random.permutation(self.env.nA)\n            q_vals = [q_vals[a] for a in perm_actions]\n            perm_q_argmax = np.argmax(q_vals)\n            action = perm_actions[perm_q_argmax]\n        return action\n\n    def _learn(self, transition):\n        s, a, r, next_s, done = transition\n        q_val = self.q_table[s][a]\n        if done:\n            q_target = r\n        else:\n            q_target = r + self.gamma*np.max(self.q_table[next_s])\n\n        # Update the q_table\n        self.q_table[s][a] += self.lr * (q_target - q_val)\n\n        # Adjust the epislon\n        self._adjust_epsilon()\n\n    def _adjust_epsilon(self):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n'"
ch18/gridworld/gridworld_env.py,0,"b""# coding: utf-8\n\n# Python Machine Learning 3rd Edition by\n# Sebastian Raschka (https://sebastianraschka.com) & Vahid Mirjalili](http://vahidmirjalili.com)\n# Packt Publishing Ltd. 2019\n#\n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n#\n# Code License: MIT License (https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n#################################################################################\n# Chapter 18 - Reinforcement Learning for Decision Making in Complex Environments\n#################################################################################\n\n# Script: gridworld_env.py\n\nimport numpy as np\nfrom gym.envs.toy_text import discrete\nfrom collections import defaultdict\nimport time\nimport pickle\nimport os\n\nfrom gym.envs.classic_control import rendering\n\nCELL_SIZE = 100\nMARGIN = 10\n\n\ndef get_coords(row, col, loc='center'):\n    xc = (col + 1.5) * CELL_SIZE\n    yc = (row + 1.5) * CELL_SIZE\n    if loc == 'center':\n        return xc, yc\n    elif loc == 'interior_corners':\n        half_size = CELL_SIZE//2 - MARGIN\n        xl, xr = xc - half_size, xc + half_size\n        yt, yb = xc - half_size, xc + half_size\n        return [(xl, yt), (xr, yt), (xr, yb), (xl, yb)]\n    elif loc == 'interior_triangle':\n        x1, y1 = xc, yc + CELL_SIZE//3\n        x2, y2 = xc + CELL_SIZE//3, yc - CELL_SIZE//3\n        x3, y3 = xc - CELL_SIZE//3, yc - CELL_SIZE//3\n        return [(x1, y1), (x2, y2), (x3, y3)]\n\n\ndef draw_object(coords_list):\n    if len(coords_list) == 1:  # -> circle\n        obj = rendering.make_circle(int(0.45*CELL_SIZE))\n        obj_transform = rendering.Transform()\n        obj.add_attr(obj_transform)\n        obj_transform.set_translation(*coords_list[0])\n        obj.set_color(0.2, 0.2, 0.2)  # -> black\n    elif len(coords_list) == 3:  # -> triangle\n        obj = rendering.FilledPolygon(coords_list)\n        obj.set_color(0.9, 0.6, 0.2)  # -> yellow\n    elif len(coords_list) > 3:  # -> polygon\n        obj = rendering.FilledPolygon(coords_list)\n        obj.set_color(0.4, 0.4, 0.8)  # -> blue\n    return obj\n\n\nclass GridWorldEnv(discrete.DiscreteEnv):\n    def __init__(self, num_rows=4, num_cols=6, delay=0.05):\n        self.num_rows = num_rows\n        self.num_cols = num_cols\n\n        self.delay = delay\n\n        move_up = lambda row, col: (max(row - 1, 0), col)\n        move_down = lambda row, col: (min(row + 1, num_rows - 1), col)\n        move_left = lambda row, col: (row, max(col - 1, 0))\n        move_right = lambda row, col: (row, min(col + 1, num_cols - 1))\n\n        self.action_defs = {0: move_up, 1: move_right,\n                            2: move_down, 3: move_left}\n\n        # Number of states/actions\n        nS = num_cols * num_rows\n        nA = len(self.action_defs)\n        self.grid2state_dict = {(s // num_cols, s % num_cols): s\n                                for s in range(nS)}\n        self.state2grid_dict = {s: (s // num_cols, s % num_cols)\n                                for s in range(nS)}\n\n        # Gold state\n        gold_cell = (num_rows // 2, num_cols - 2)\n\n        # Trap states\n        trap_cells = [((gold_cell[0] + 1), gold_cell[1]),\n                      (gold_cell[0], gold_cell[1] - 1),\n                      ((gold_cell[0] - 1), gold_cell[1])]\n\n        gold_state = self.grid2state_dict[gold_cell]\n        trap_states = [self.grid2state_dict[(r, c)]\n                       for (r, c) in trap_cells]\n        self.terminal_states = [gold_state] + trap_states\n        print(self.terminal_states)\n\n        # Build the transition probability\n        P = defaultdict(dict)\n        for s in range(nS):\n            row, col = self.state2grid_dict[s]\n            P[s] = defaultdict(list)\n            for a in range(nA):\n                action = self.action_defs[a]\n                next_s = self.grid2state_dict[action(row, col)]\n\n                # Terminal state\n                if self.is_terminal(next_s):\n                    r = (1.0 if next_s == self.terminal_states[0]\n                         else -1.0)\n                else:\n                    r = 0.0\n                if self.is_terminal(s):\n                    done = True\n                    next_s = s\n                else:\n                    done = False\n                P[s][a] = [(1.0, next_s, r, done)]\n\n        # Initial state distribution\n        isd = np.zeros(nS)\n        isd[0] = 1.0\n\n        super(GridWorldEnv, self).__init__(nS, nA, P, isd)\n\n        self.viewer = None\n        self._build_display(gold_cell, trap_cells)\n\n    def is_terminal(self, state):\n        return state in self.terminal_states\n\n    def _build_display(self, gold_cell, trap_cells):\n\n        screen_width = (self.num_cols + 2) * CELL_SIZE\n        screen_height = (self.num_rows + 2) * CELL_SIZE\n        self.viewer = rendering.Viewer(screen_width,\n                                       screen_height)\n\n        all_objects = []\n\n        # List of border points' coordinates\n        bp_list = [\n            (CELL_SIZE - MARGIN, CELL_SIZE - MARGIN),\n            (screen_width - CELL_SIZE + MARGIN, CELL_SIZE - MARGIN),\n            (screen_width - CELL_SIZE + MARGIN,\n             screen_height - CELL_SIZE + MARGIN),\n            (CELL_SIZE - MARGIN, screen_height - CELL_SIZE + MARGIN)\n        ]\n        border = rendering.PolyLine(bp_list, True)\n        border.set_linewidth(5)\n        all_objects.append(border)\n\n        # Vertical lines\n        for col in range(self.num_cols + 1):\n            x1, y1 = (col + 1) * CELL_SIZE, CELL_SIZE\n            x2, y2 = (col + 1) * CELL_SIZE, \\\n                     (self.num_rows + 1) * CELL_SIZE\n            line = rendering.PolyLine([(x1, y1), (x2, y2)], False)\n            all_objects.append(line)\n\n        # Horizontal lines\n        for row in range(self.num_rows + 1):\n            x1, y1 = CELL_SIZE, (row + 1) * CELL_SIZE\n            x2, y2 = (self.num_cols + 1) * CELL_SIZE, \\\n                     (row + 1) * CELL_SIZE\n            line = rendering.PolyLine([(x1, y1), (x2, y2)], False)\n            all_objects.append(line)\n\n        # Traps: --> circles\n        for cell in trap_cells:\n            trap_coords = get_coords(*cell, loc='center')\n            all_objects.append(draw_object([trap_coords]))\n\n        # Gold:  --> triangle\n        gold_coords = get_coords(*gold_cell,\n                                 loc='interior_triangle')\n        all_objects.append(draw_object(gold_coords))\n\n        # Agent --> square or robot\n        if (os.path.exists('robot-coordinates.pkl') and CELL_SIZE == 100):\n            agent_coords = pickle.load(\n                open('robot-coordinates.pkl', 'rb'))\n            starting_coords = get_coords(0, 0, loc='center')\n            agent_coords += np.array(starting_coords)\n        else:\n            agent_coords = get_coords(0, 0, loc='interior_corners')\n        agent = draw_object(agent_coords)\n        self.agent_trans = rendering.Transform()\n        agent.add_attr(self.agent_trans)\n        all_objects.append(agent)\n\n        for obj in all_objects:\n            self.viewer.add_geom(obj)\n\n    def render(self, mode='human', done=False):\n        if done:\n            sleep_time = 1\n        else:\n            sleep_time = self.delay\n        x_coord = self.s % self.num_cols\n        y_coord = self.s // self.num_cols\n        x_coord = (x_coord + 0) * CELL_SIZE\n        y_coord = (y_coord + 0) * CELL_SIZE\n        self.agent_trans.set_translation(x_coord, y_coord)\n        rend = self.viewer.render(\n            return_rgb_array=(mode == 'rgb_array'))\n        time.sleep(sleep_time)\n        return rend\n\n    def close(self):\n        if self.viewer:\n            self.viewer.close()\n            self.viewer = None\n\n\nif __name__ == '__main__':\n    env = GridWorldEnv(5, 6)\n    for i in range(1):\n        s = env.reset()\n        env.render(mode='human', done=False)\n\n        while True:\n            action = np.random.choice(env.nA)\n            res = env.step(action)\n            print('Action ', env.s, action, ' -> ', res)\n            env.render(mode='human', done=res[2])\n            if res[2]:\n                break\n\n    env.close()\n"""
ch18/gridworld/qlearning.py,0,"b'# coding: utf-8\n\n# Python Machine Learning 3rd Edition by\n# Sebastian Raschka (https://sebastianraschka.com) & Vahid Mirjalili](http://vahidmirjalili.com)\n# Packt Publishing Ltd. 2019\n#\n# Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n#\n# Code License: MIT License (https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)\n\n#################################################################################\n# Chapter 18 - Reinforcement Learning for Decision Making in Complex Environments\n#################################################################################\n\n# Script: qlearning.py\n\nfrom gridworld_env import GridWorldEnv\nfrom agent import Agent\nfrom collections import namedtuple\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nnp.random.seed(1)\n\nTransition = namedtuple(\n    \'Transition\', (\'state\', \'action\', \'reward\', \'next_state\', \'done\'))\n\n\ndef run_qlearning(agent, env, num_episodes=50):\n    history = []\n    for episode in range(num_episodes):\n        state = env.reset()\n        env.render(mode=\'human\')\n        final_reward, n_moves = 0.0, 0\n        while True:\n            action = agent.choose_action(state)\n            next_s, reward, done, _ = env.step(action)\n            agent._learn(Transition(state, action, reward,\n                                    next_s, done))\n            env.render(mode=\'human\', done=done)\n            state = next_s\n            n_moves += 1\n            if done:\n                break\n            final_reward = reward\n        history.append((n_moves, final_reward))\n        print(\'Episode %d: Reward %.1f #Moves %d\'\n              % (episode, final_reward, n_moves))\n\n    return history\n\n\ndef plot_learning_history(history):\n    fig = plt.figure(1, figsize=(14, 10))\n    ax = fig.add_subplot(2, 1, 1)\n    episodes = np.arange(len(history))\n    moves = np.array([h[0] for h in history])\n    plt.plot(episodes, moves, lw=4,\n             marker=""o"", markersize=10)\n    ax.tick_params(axis=\'both\', which=\'major\', labelsize=15)\n    plt.xlabel(\'Episodes\', size=20)\n    plt.ylabel(\'# moves\', size=20)\n\n    ax = fig.add_subplot(2, 1, 2)\n    rewards = np.array([h[1] for h in history])\n    plt.step(episodes, rewards, lw=4)\n    ax.tick_params(axis=\'both\', which=\'major\', labelsize=15)\n    plt.xlabel(\'Episodes\', size=20)\n    plt.ylabel(\'Final rewards\', size=20)\n    plt.savefig(\'q-learning-history.png\', dpi=300)\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    env = GridWorldEnv(num_rows=5, num_cols=6)\n    agent = Agent(env)\n    history = run_qlearning(agent, env)\n    env.close()\n\n    plot_learning_history(history)\n'"
