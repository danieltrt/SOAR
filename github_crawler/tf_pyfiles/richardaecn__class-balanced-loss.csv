file_path,api_count,code
src/cifar.py,13,"b'""""""CIFAR-10 data set.\n\nSee http://www.cs.toronto.edu/~kriz/cifar.html.\n""""""\nimport os\n\nimport tensorflow as tf\nfrom data_utils import get_img_num_per_cls\n\nHEIGHT = 32\nWIDTH = 32\nDEPTH = 3\n\n\nclass CifarDataSet(object):\n    """"""Cifar data set.""""""\n\n    def __init__(self,\n                 data_dir,\n                 data_version=\'10\',\n                 subset=\'train\',\n                 imb_factor=None,\n                 use_distortion=True):\n        self.data_dir = data_dir\n        self.data_version = data_version\n        self.subset = subset\n        self.imb_factor = imb_factor\n        self.use_distortion = use_distortion\n\n    def get_filenames(self):\n        if self.subset == \'train_offline\':  # so avoid shuffle during make_batch\n            return [os.path.join(self.data_dir, \'train\' + \'.tfrecords\')]\n        if self.subset in [\'train\', \'eval\']:\n            return [os.path.join(self.data_dir, self.subset + \'.tfrecords\')]\n        else:\n            raise ValueError(\'Invalid data subset ""%s""\' % self.subset)\n\n    def parser(self, serialized_example):\n        """"""Parses a single tf.Example into image and label tensors.""""""\n        # Dimensions of the images in the CIFAR-10 dataset.\n        # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of\n        # the input format.\n        features = tf.parse_single_example(\n            serialized_example,\n            features={\n                \'image\': tf.FixedLenFeature([], tf.string),\n                \'label\': tf.FixedLenFeature([], tf.int64),\n            })\n        image = tf.decode_raw(features[\'image\'], tf.uint8)\n        image.set_shape([DEPTH * HEIGHT * WIDTH])\n\n        # Reshape from [depth * height * width] to [depth, height, width].\n        image = tf.cast(\n            tf.transpose(tf.reshape(image, [DEPTH, HEIGHT, WIDTH]), [1, 2, 0]),\n            tf.float32)\n        label = tf.cast(features[\'label\'], tf.int32)\n\n        # Custom preprocessing.\n        image = self.preprocess(image)\n\n        return image, label\n\n    def make_batch(self, batch_size):\n        """"""Read the images and labels from \'filenames\'.""""""\n        filenames = self.get_filenames()\n        # Repeat infinitely.\n        dataset = tf.data.TFRecordDataset(filenames).repeat()\n\n        # Parse records.\n        dataset = dataset.map(self.parser, num_parallel_calls=batch_size)\n\n        # Potentially shuffle records.\n        if self.subset == \'train\':\n            min_queue_examples = int(\n                CifarDataSet.num_examples_per_epoch(\n                    self.subset, self.imb_factor, self.data_version) * 0.4)\n            # Ensure that the capacity is sufficiently large to provide good\n            # random shuffling.\n            dataset = dataset.shuffle(buffer_size=min_queue_examples +\n                                      3 * batch_size)\n\n        # Batch it up.\n        dataset = dataset.batch(batch_size)\n        iterator = dataset.make_one_shot_iterator()\n        image_batch, label_batch = iterator.get_next()\n\n        return image_batch, label_batch\n\n    def preprocess(self, image):\n        """"""Preprocess a single image in [height, width, depth] layout.""""""\n        if self.subset == \'train\' and self.use_distortion:\n            # Pad 4 pixels on each dimension of feature map, done in mini-batch\n            image = tf.image.resize_image_with_crop_or_pad(image, 40, 40)\n            image = tf.random_crop(image, [HEIGHT, WIDTH, DEPTH])\n            image = tf.image.random_flip_left_right(image)\n        return image\n\n    @staticmethod\n    def num_examples_per_epoch(subset=\'train\',\n                               imb_factor=None,\n                               cifar_version=\'10\'):\n        if subset == \'train\':\n            if imb_factor is None:\n                return 50000\n            else:\n                return sum(get_img_num_per_cls(cifar_version, imb_factor))\n        elif subset == \'eval\':\n            return 10000\n        else:\n            raise ValueError(\'Invalid data subset ""%s""\' % subset)\n'"
src/cifar_main.py,83,"b'""""""Training and evaluation for CIFAR image classification.""""""\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport functools\nimport itertools\nimport os\nimport time\n\nimport cifar\nimport cifar_model\nimport cifar_utils\nimport data_utils\nimport numpy as np\nimport six\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef dir2version(data_dir):\n  return data_dir.split(\'/\')[-1].split(\'-\')[1]  # string, 10, 20 or 100\n\n\ndef learning_rate_schedule(current_epoch,\n                           base_learning_rate,\n                           lr_boundaries,\n                           lr_multiplier):\n  """"""Handles linear scaling rule, gradual warmup, and LR decay.\n  The learning rate starts at 0, then it increases linearly per epoch.\n  After 5 epochs we reach the base learning rate.\n\n  Args:\n    current_epoch: `Tensor` for current epoch.\n    base_learning_rate: initial learning rate after warmup.\n    lr_boundaries: a list of training epochs.\n    lr_multiplier: a list of learing rate multipliers.\n  Returns:\n    A scaled `Tensor` for current learning rate.\n  """"""\n  staged_lr = [base_learning_rate * x for x in lr_multiplier]\n  decay_rate = (base_learning_rate * current_epoch / lr_boundaries[0])\n  for st_lr, start_epoch in zip(staged_lr, lr_boundaries):\n    decay_rate = tf.where(current_epoch < start_epoch,\n                          decay_rate, st_lr)\n  return decay_rate\n\n\ndef get_model_fn(num_gpus, variable_strategy, num_workers):\n  """"""Returns a function that will build the resnet model.""""""\n\n  def _resnet_model_fn(features, labels, mode, params):\n    """"""Resnet model body.\n\n    Support single host, one or more GPU training. Parameter distribution can\n    be either one of the following scheme.\n    1. CPU is the parameter server and manages gradient updates.\n    2. Parameters are distributed evenly across all GPUs, and the first GPU\n       manages gradient updates.\n\n    Args:\n      features: a list of tensors, one for each tower\n      labels: a list of tensors, one for each tower\n      mode: ModeKeys.TRAIN or EVAL\n      params: Hyperparameters suitable for tuning\n    Returns:\n      A EstimatorSpec object.\n    """"""\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n    weight_decay = params.weight_decay\n    momentum = params.momentum\n    data_version = params.data_version\n    imb_factor = params.imb_factor\n\n    if num_gpus == 0:\n      num_devices = 1\n      device_type = \'cpu\'\n    else:\n      num_devices = num_gpus\n      device_type = \'gpu\'\n\n    tower_features = features\n    # for estimator mode =PREDICT\n    tower_labels = labels if labels is not None else [labels] * num_devices\n    tower_losses = []\n    reg_losses = []\n    tower_gradvars = []\n    tower_preds = []\n\n    # channels first (NCHW) is normally optimal on GPU and channels last (NHWC)\n    # on CPU. The exception is Intel MKL on CPU which is optimal with\n    # channels_last.\n    data_format = params.data_format\n    if not data_format:\n      if num_gpus == 0:\n        data_format = \'channels_last\'\n      else:\n        data_format = \'channels_first\'\n\n    for i in range(num_devices):\n      worker_device = \'/{}:{}\'.format(device_type, i)\n      if variable_strategy == \'CPU\':\n        device_setter = cifar_utils.local_device_setter(\n            worker_device=worker_device)\n      elif variable_strategy == \'GPU\':\n        device_setter = cifar_utils.local_device_setter(\n            ps_device_type=\'gpu\',\n            worker_device=worker_device,\n            ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(\n                num_gpus, tf.contrib.training.byte_size_load_fn))\n      with tf.variable_scope(\'resnet\', reuse=bool(i != 0)):\n        with tf.name_scope(\'tower_%d\' % i) as name_scope:\n          with tf.device(device_setter):\n            loss_list, gradvars, preds = _tower_fn(\n                is_training, weight_decay, tower_features[i], tower_labels[i],\n                data_version, data_format, params.num_layers,\n                params.batch_norm_decay, params.batch_norm_epsilon,\n                params.resnet_version, params.loss_type, params.gamma,\n                params.weights)\n            if mode != tf.estimator.ModeKeys.PREDICT:\n              tower_losses.append(loss_list[0])\n              reg_losses.append(loss_list[1])\n              tower_gradvars.append(gradvars)\n            tower_preds.append(preds)\n            if i == 0:\n              # Only trigger batch_norm moving mean and variance update from\n              # the 1st tower. Ideally, we should grab the updates from all\n              # towers but these stats accumulate extremely fast so we can\n              # ignore the other stats from the other towers without\n              # significant detriment.\n              update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS,\n                                             name_scope)\n\n    if mode != tf.estimator.ModeKeys.PREDICT:\n      # Now compute global loss and gradients.\n      gradvars = []\n      with tf.name_scope(\'gradient_averaging\'):\n        all_grads = {}\n        for grad, var in itertools.chain(*tower_gradvars):\n          if grad is not None:\n            all_grads.setdefault(var, []).append(grad)\n        for var, grads in six.iteritems(all_grads):\n          # Average gradients on the same device as the variables\n          # to which they apply.\n          with tf.device(var.device):\n            if len(grads) == 1:\n              avg_grad = grads[0]\n            else:\n              avg_grad = tf.multiply(tf.add_n(grads), 1. / len(grads))\n          gradvars.append((avg_grad, var))\n\n      # Device that runs the ops to apply global gradient updates.\n      consolid_device = \'/gpu:0\' if variable_strategy == \'GPU\' else \'/cpu:0\'\n      with tf.device(consolid_device):\n        num_train_examples = cifar.CifarDataSet.num_examples_per_epoch(\n            \'train\', imb_factor, data_version)\n        train_batch_size = params.train_batch_size * num_workers\n        num_per_batch = train_batch_size / num_train_examples\n        current_epoch = tf.cast(\n            tf.train.get_global_step(), tf.float32) * num_per_batch\n        boundaries = np.asarray(params.learning_rate_schedule, dtype=np.int64)\n        multipliers = np.asarray(\n            params.learning_rate_multiplier, dtype=np.float32)\n        # Linear scaling of base learning rate.\n        base_lr = params.learning_rate * train_batch_size / 128\n        learning_rate = learning_rate_schedule(\n            current_epoch, base_lr, boundaries, multipliers)\n\n        loss = tf.reduce_mean(tower_losses, name=\'loss\')\n        reg_loss = tf.reduce_mean(reg_losses, name=\'regularization_loss\')\n\n        optimizer = tf.train.MomentumOptimizer(\n            learning_rate=learning_rate, momentum=momentum)\n\n        if params.sync:\n          optimizer = tf.train.SyncReplicasOptimizer(\n              optimizer, replicas_to_aggregate=num_workers)\n\n        # Create single grouped train op\n        train_op = [\n            optimizer.apply_gradients(\n                gradvars, global_step=tf.train.get_global_step())\n        ]\n        train_op.extend(update_ops)\n        train_op = tf.group(*train_op)\n    else:\n      train_op = None\n      loss = None\n      # train_hooks = None\n\n    predictions = {\n        \'classes\':\n            tf.concat([p[\'classes\'] for p in tower_preds], axis=0),\n        \'probabilities\':\n            tf.concat([p[\'probabilities\'] for p in tower_preds], axis=0),\n        \'logits\':\n            tf.concat([p[\'logits\'] for p in tower_preds], axis=0),\n    }\n\n    if mode != tf.estimator.ModeKeys.PREDICT:\n      stacked_labels = tf.concat(labels, axis=0)\n      accuracy = tf.metrics.accuracy(stacked_labels, predictions[\'classes\'])\n\n      metrics = {\'accuracy\': accuracy}\n\n      tf.summary.scalar(\'learning_rate\', learning_rate)\n      tf.summary.scalar(\'regularization_loss\', reg_loss)\n      tf.summary.scalar(\'network_loss\', loss - reg_loss)\n      tf.summary.scalar(\'epoch\', current_epoch)\n    else:\n      metrics = {}\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=predictions,\n        loss=loss,\n        train_op=train_op,\n        # training_hooks=train_hooks,\n        eval_metric_ops=metrics)\n\n  return _resnet_model_fn\n\n\ndef focal_loss(labels, logits, alpha, gamma):\n  """"""Compute the focal loss between `logits` and the ground truth `labels`.\n\n  Focal loss = -alpha_t * (1-pt)^gamma * log(pt)\n  where pt is the probability of being classified to the true class.\n  pt = p (if true class), otherwise pt = 1 - p. p = sigmoid(logit).\n\n  Args:\n    labels: A float32 tensor of size [batch, num_classes].\n    logits: A float32 tensor of size [batch, num_classes].\n    alpha: A float32 tensor of size [batch_size]\n      specifying per-example weight for balanced cross entropy.\n    gamma: A float32 scalar modulating loss from hard and easy examples.\n  Returns:\n    focal_loss: A float32 scalar representing normalized total loss.\n  """"""\n  with tf.name_scope(\'focal_loss\'):\n    logits = tf.cast(logits, dtype=tf.float32)\n    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=labels, logits=logits)\n\n    # positive_label_mask = tf.equal(labels, 1.0)\n    # probs = tf.sigmoid(logits)\n    # probs_gt = tf.where(positive_label_mask, probs, 1.0 - probs)\n    # # With gamma < 1, the implementation could produce NaN during back prop.\n    # modulator = tf.pow(1.0 - probs_gt, gamma)\n\n    # A numerically stable implementation of modulator.\n    if gamma == 0.0:\n      modulator = 1.0\n    else:\n      modulator = tf.exp(-gamma * labels * logits - gamma * tf.log1p(\n          tf.exp(-1.0 * logits)))\n\n    loss = modulator * cross_entropy\n\n    weighted_loss = alpha * loss\n    focal_loss = tf.reduce_sum(weighted_loss)\n    # Normalize by the total number of positive samples.\n    focal_loss /= tf.reduce_sum(labels)\n  return focal_loss\n\n\ndef _tower_fn(is_training, weight_decay, feature, label, data_version,\n              data_format, num_layers, batch_norm_decay, batch_norm_epsilon,\n              resnet_version, loss_type, gamma, weights):\n  """"""Build computation tower (Resnet).\n\n  Args:\n    is_training: true if is training graph.\n    weight_decay: weight regularization strength, a float.\n    feature: a Tensor.\n    label: a Tensor.\n    data_version: a str, \'10\' or \'100\'\n    data_format: channels_last (NHWC) or channels_first (NCHW).\n    num_layers: number of layers, an int.\n    batch_norm_decay: decay for batch normalization, a float.\n    batch_norm_epsilon: epsilon for batch normalization, a float.\n    resnet_version: preactivation or postactivation\n    loss_type: Loss type (\'softmax\', \'sigmoid\', \'focal\').\n    gamma: gamma for focal loss.\n    weights: weights per class.\n\n  Returns:\n    A tuple with the loss for the tower, the gradients and parameters, and\n    predictions.\n\n  """"""\n  num_classes = int(data_version)\n  model = cifar_model.ResNetCifar(\n      num_layers,\n      batch_norm_decay=batch_norm_decay,\n      batch_norm_epsilon=batch_norm_epsilon,\n      is_training=is_training,\n      version=resnet_version,\n      num_classes=num_classes,\n      data_format=data_format,\n      loss_type=loss_type)\n  logits = model.forward_pass(feature, input_data_format=\'channels_last\')\n  if loss_type == \'softmax\':\n    tower_pred = {\n        \'classes\': tf.argmax(input=logits, axis=1),\n        \'probabilities\': tf.nn.softmax(logits),\n        \'logits\': logits,  # a tensor,\n        \'labels\': label,\n    }\n  elif loss_type == \'sigmoid\' or loss_type == \'focal\':\n    tower_pred = {\n        \'classes\': tf.argmax(input=logits, axis=1),\n        \'probabilities\': tf.sigmoid(logits),\n        \'logits\': logits,  # a tensor,\n        \'labels\': label,\n    }\n\n  if label is None:   # for classifier.predict\n    return None, None, tower_pred\n\n  one_hot_labels = tf.one_hot(label, num_classes)\n\n  weights = tf.cast(weights, dtype=tf.float32)\n  weights = tf.expand_dims(weights, 0)\n  weights = tf.tile(weights, [tf.shape(one_hot_labels)[0], 1]) * one_hot_labels\n  weights = tf.reduce_sum(weights, axis=1)\n  weights = tf.expand_dims(weights, 1)\n  weights = tf.tile(weights, [1, num_classes])\n\n  if loss_type == \'softmax\':\n    tower_loss = tf.losses.softmax_cross_entropy(\n        one_hot_labels, logits, weights=tf.reduce_mean(weights, axis=1))\n    tower_loss = tf.reduce_mean(tower_loss)\n  elif loss_type == \'sigmoid\':\n    tower_loss = weights * tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=one_hot_labels, logits=logits)\n    # Normalize by the total number of positive samples.\n    tower_loss = tf.reduce_sum(tower_loss) / tf.reduce_sum(one_hot_labels)\n  elif loss_type == \'focal\':\n    tower_loss = focal_loss(one_hot_labels, logits, weights, gamma)\n\n  model_params = tf.trainable_variables()\n  if loss_type == \'softmax\':\n    reg_loss = weight_decay * tf.add_n(\n        [tf.nn.l2_loss(v) for v in model_params])\n  elif loss_type == \'sigmoid\' or loss_type == \'focal\':\n    # no regularization (weight decay) for last layer\'s bias.\n    reg_loss = weight_decay * tf.add_n(\n        [tf.nn.l2_loss(v) for v in model_params if \'dense/bias\' not in v.name])\n\n  tower_loss += reg_loss\n  tower_grad = tf.gradients(tower_loss, model_params)\n\n  return [tower_loss, reg_loss], zip(tower_grad, model_params), tower_pred\n\n\ndef input_fn(data_dir,\n             subset,\n             imbalance_factor,\n             num_shards,\n             batch_size,\n             use_distortion_for_training=True):\n  """"""Create input graph for model.\n\n  Args:\n    data_dir: Directory where TFRecords representing the dataset are located.\n    subset: one of \'train\', \'validate\' and \'eval\'.\n    imbalance_factor: float, None if this dataset is not long tailed.\n    num_shards: num of towers participating in data-parallel training.\n    batch_size: total batch size for training to be divided by the number of\n    shards.\n    use_distortion_for_training: True to use distortions.\n  Returns:\n    two lists of tensors for features and labels, each of num_shards length.\n  """"""\n  with tf.device(\'/cpu:0\'):\n    use_distortion = subset == \'train\' and use_distortion_for_training\n    dataset = cifar.CifarDataSet(\n        data_dir, dir2version(data_dir),\n        subset, imbalance_factor, use_distortion)\n    image_batch, label_batch = dataset.make_batch(batch_size)\n    if num_shards <= 1:\n      # No GPU available or only 1 GPU.\n      return [image_batch], [label_batch]\n\n    # Note that passing num=batch_size is safe here, even though\n    # dataset.batch(batch_size) can, in some cases, return fewer than batch_size\n    # examples. This is because it does so only when repeating for a limited\n    # number of epochs, but our dataset repeats forever.\n    image_batch = tf.unstack(image_batch, num=batch_size, axis=0)\n    label_batch = tf.unstack(label_batch, num=batch_size, axis=0)\n    feature_shards = [[] for i in range(num_shards)]\n    label_shards = [[] for i in range(num_shards)]\n    for i in xrange(batch_size):\n      idx = i % num_shards\n      feature_shards[idx].append(image_batch[i])\n      label_shards[idx].append(label_batch[i])\n    feature_shards = [tf.parallel_stack(x) for x in feature_shards]\n    label_shards = [tf.parallel_stack(x) for x in label_shards]\n    return feature_shards, label_shards\n\n\ndef main(job_dir, data_dir, num_gpus, variable_strategy,\n         use_distortion_for_training, log_device_placement, num_intra_threads,\n         **hparams):\n  # The env variable is on deprecation path, default is set to off.\n  os.environ[\'TF_SYNC_ON_FINISH\'] = \'0\'\n  os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n  # Session configuration.\n  sess_config = tf.ConfigProto(\n      allow_soft_placement=True,\n      log_device_placement=log_device_placement,\n      intra_op_parallelism_threads=num_intra_threads,\n      gpu_options=tf.GPUOptions(force_gpu_compatible=True, allow_growth=True)\n  )\n\n  config = cifar_utils.RunConfig(\n      session_config=sess_config,\n      model_dir=job_dir,\n      save_summary_steps=100)\n\n  # Normalized weights based on inverse number of effective data per class.\n  img_num_per_cls = data_utils.get_img_num_per_cls(\n      hparams[\'data_version\'], hparams[\'imb_factor\'])\n  effective_num = 1.0 - np.power(hparams[\'beta\'], img_num_per_cls)\n  weights = (1.0 - hparams[\'beta\']) / np.array(effective_num)\n  weights = weights / np.sum(weights) * int(hparams[\'data_version\'])\n\n  hparams = tf.contrib.training.HParams(\n      is_chief=config.is_chief,\n      weights=weights,\n      **hparams)\n\n  train_input_fn = functools.partial(\n      input_fn,\n      data_dir,\n      subset=\'train\',\n      imbalance_factor=hparams.imb_factor,\n      num_shards=num_gpus,\n      batch_size=hparams.train_batch_size,\n      use_distortion_for_training=use_distortion_for_training)\n\n  eval_input_fn = functools.partial(\n      input_fn,\n      data_dir,\n      subset=\'eval\',\n      imbalance_factor=hparams.imb_factor,\n      batch_size=hparams.eval_batch_size,\n      num_shards=num_gpus)\n\n  num_eval_examples = cifar.CifarDataSet.num_examples_per_epoch(\'eval\')\n  if num_eval_examples % hparams.eval_batch_size != 0:\n    raise ValueError(\n        \'validation set size must be multiple of eval_batch_size\')\n\n  num_workers = config.num_worker_replicas or 1\n  num_train_examples = cifar.CifarDataSet.num_examples_per_epoch(\n      \'train\', hparams.imb_factor, hparams.data_version)\n  train_batch_size = hparams.train_batch_size * num_workers\n  train_steps = num_train_examples * hparams.train_epochs // train_batch_size\n  eval_steps = num_eval_examples // hparams.eval_batch_size\n  classifier = tf.estimator.Estimator(\n      model_fn=get_model_fn(num_gpus, variable_strategy, num_workers),\n      model_dir=job_dir,\n      config=config,\n      params=hparams)\n\n  ckpt = tf.train.get_checkpoint_state(job_dir)\n  if ckpt is None:\n    current_step = 0\n  else:\n    current_step = int(\n        os.path.basename(ckpt.model_checkpoint_path).split(\'-\')[1])\n\n  steps_per_eval = num_train_examples * hparams.eval_epochs // train_batch_size\n  tf.logging.info(\'Training for %d steps. Current step %d.\',\n                  train_steps,\n                  current_step)\n  start_timestamp = time.time()  # This time will include compilation time\n\n  while current_step < train_steps:\n    # Train for up to steps_per_eval number of steps.\n    # At the end of training, a checkpoint will be written to --job_dir.\n    next_checkpoint = min(current_step + steps_per_eval, train_steps)\n    classifier.train(\n        input_fn=train_input_fn, max_steps=next_checkpoint)\n    current_step = next_checkpoint\n\n    tf.logging.info(\'Finished training up to step %d. Elapsed seconds %d.\',\n                    next_checkpoint, int(time.time() - start_timestamp))\n\n    # Evaluate the model on the most recent model in --job_dir.\n    # Since evaluation happens in batches of --eval_batch_size, some images\n    # may be excluded modulo the batch size. As long as the batch size is\n    # consistent, the evaluated images are also consistent.\n    tf.logging.info(\'Starting to evaluate.\')\n    eval_results = classifier.evaluate(\n        input_fn=eval_input_fn, steps=eval_steps)\n    tf.logging.info(\'Eval results at step %d: %s\',\n                    next_checkpoint, eval_results)\n\n  elapsed_time = int(time.time() - start_timestamp)\n  tf.logging.info(\'Finished training up to step %d. Elapsed seconds %d.\',\n                  train_steps, elapsed_time)\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \'--data-dir\',\n      type=str,\n      required=True,\n      help=\'The directory where the CIFAR-10 input data is stored.\')\n  parser.add_argument(\n      \'--job-dir\',\n      type=str,\n      required=True,\n      help=\'The directory where the model will be stored.\')\n  parser.add_argument(\n      \'--data-version\',\n      type=str,\n      default=\'10\',\n      help=\'cifar dataset version, 10, 20 or 100\')\n  parser.add_argument(\n      \'--variable-strategy\',\n      choices=[\'CPU\', \'GPU\'],\n      type=str,\n      default=\'GPU\',\n      help=\'Where to locate variable operations\')\n  parser.add_argument(\n      \'--num-gpus\',\n      type=int,\n      default=1,\n      help=\'The number of gpus used. Uses only CPU if set to 0.\')\n  parser.add_argument(\n      \'--num-layers\',\n      type=int,\n      default=32,\n      help=\'The number of layers of the model.\')\n  parser.add_argument(\n      \'--resnet-version\',\n      type=str,\n      default=\'v1\',\n      help=""""""\\\n      The version of resnet, \\\n      v1 : use basic (non-bottleneck) block and ResNet V1 (post-activation). \\\n      v2: Use basic (non-bottleneck) block and ResNet V2 (pre-activation). \\\n      bv2: Use bottleneck block and ResNet V2 (pre-activation).\\\n      """""")\n  parser.add_argument(\n      \'--train-epochs\',\n      type=int,\n      default=200,\n      help=\'The number of epochs to use for training.\')\n  parser.add_argument(\n      \'--train-batch-size\',\n      type=int,\n      default=128,\n      help=\'Batch size for training.\')\n  parser.add_argument(\n      \'--eval-batch-size\',\n      type=int,\n      default=100,\n      help=\'Batch size for validation.\')\n  parser.add_argument(\n      \'--eval-epochs\',\n      type=int,\n      default=2,\n      help=\'The number of epochs between evaluations.\')\n  parser.add_argument(\n      \'--momentum\',\n      type=float,\n      default=0.9,\n      help=\'Momentum for MomentumOptimizer.\')\n  parser.add_argument(\n      \'--weight-decay\',\n      type=float,\n      default=2e-4,\n      help=\'Weight decay for convolutions.\')\n  parser.add_argument(\n      \'--learning-rate\',\n      type=float,\n      default=0.1,\n      help=""""""\\\n      This is the inital learning rate value. The learning rate will decrease\n      during training. For more details check the model_fn implementation in\n      this file.\\\n      """""")\n  parser.add_argument(\n      \'--learning-rate-schedule\',\n      nargs=\'+\',\n      type=int,\n      default=[5, 160, 180],\n      help=\'Schedule of learning rate decay\')\n  parser.add_argument(\n      \'--learning-rate-multiplier\',\n      nargs=\'+\',\n      type=float,\n      default=[1, 0.1, 0.01],\n      help=\'Schedule of learning rate decay\')\n  parser.add_argument(\n      \'--use-distortion-for-training\',\n      type=bool,\n      default=True,\n      help=\'If doing image distortion for training.\')\n  parser.add_argument(\n      \'--sync\',\n      action=\'store_true\',\n      default=False,\n      help=""""""\\\n      If present, running in a distributed environment will run on sync mode.\\\n      """""")\n  parser.add_argument(\n      \'--num-intra-threads\',\n      type=int,\n      default=0,\n      help=""""""\\\n      Number of threads to use for intra-op parallelism. When training on CPU\n      set to 0 to have the system pick the appropriate number or alternatively\n      set it to the number of physical CPU cores.\\\n      """""")\n  parser.add_argument(\n      \'--num-inter-threads\',\n      type=int,\n      default=0,\n      help=""""""\\\n      Number of threads to use for inter-op parallelism. If set to 0, the\n      system will pick an appropriate number.\\\n      """""")\n  parser.add_argument(\n      \'--data-format\',\n      type=str,\n      default=None,\n      help=""""""\\\n      If not set, the data format best for the training device is used.\n      Allowed values: channels_first (NCHW) channels_last (NHWC).\\\n      """""")\n  parser.add_argument(\n      \'--log-device-placement\',\n      action=\'store_true\',\n      default=False,\n      help=\'Whether to log device placement.\')\n  parser.add_argument(\n      \'--batch-norm-decay\',\n      type=float,\n      default=0.9,\n      help=\'Decay for batch norm.\')\n  parser.add_argument(\n      \'--batch-norm-epsilon\',\n      type=float,\n      default=1e-5,\n      help=\'Epsilon for batch norm.\')\n  parser.add_argument(\n      \'--imb-factor\',\n      type=float,\n      default=None,\n      help=\'Imbalance factor, None if the dataset is default.\')\n  parser.add_argument(\n      \'--loss-type\',\n      type=str,\n      default=\'softmax\',\n      help=""""""\\\n      Loss type for training the network (\'softmax\', \'sigmoid\', \'focal\').\\\n      """""")\n  parser.add_argument(\n      \'--gamma\',\n      type=float,\n      default=1.0,\n      help=\'Gamma for focal loss.\')\n  parser.add_argument(\n      \'--beta\',\n      type=float,\n      default=0.0,\n      help=\'Beta for class balanced loss.\')\n\n  args = parser.parse_args()\n\n  if args.num_gpus > 0:\n    assert tf.test.is_gpu_available(), ""Requested GPUs but none found.""\n  if args.num_gpus < 0:\n    raise ValueError(\n        \'Invalid GPU count: \\""--num-gpus\\"" must be 0 or a positive integer.\')\n  if args.num_gpus == 0 and args.variable_strategy == \'GPU\':\n    raise ValueError(\'num-gpus=0, CPU must be used as parameter server. Set\'\n                     \'--variable-strategy=CPU.\')\n  if (args.num_layers - 2) % 6 != 0:\n    raise ValueError(\'Invalid --num-layers parameter.\')\n  if args.num_gpus != 0 and args.train_batch_size % args.num_gpus != 0:\n    raise ValueError(\'--train-batch-size must be multiple of --num-gpus.\')\n  if args.num_gpus != 0 and args.eval_batch_size % args.num_gpus != 0:\n    raise ValueError(\'--eval-batch-size must be multiple of --num-gpus.\')\n  if args.resnet_version not in [\'v1\', \'v2\', \'bv2\']:\n    raise ValueError(\'--resnet-version: must be one of v1, v2, bv2.\')\n  if args.loss_type not in [\'softmax\', \'sigmoid\', \'focal\']:\n    raise ValueError(\'--loss-type must be one of softmax, sigmoid, focal.\')\n  if len(args.learning_rate_schedule) != len(args.learning_rate_multiplier):\n    raise ValueError(\'The length of --learning-rate-multiplier and \'\n                     \'--learning-rate-schedule must be same.\')\n\n  main(**vars(args))\n'"
src/cifar_model.py,3,"b'""""""Model class for Cifar Dataset.""""""\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport model_base\n\n\nclass ResNetCifar(model_base.ResNet):\n  """"""Cifar model with ResNetV1 and basic residual block.""""""\n\n  def __init__(self,\n               num_layers,\n               is_training,\n               batch_norm_decay,\n               batch_norm_epsilon,\n               version=\'v1\',\n               num_classes=10,\n               data_format=\'channels_first\',\n               loss_type=\'softmax\'):\n    super(ResNetCifar, self).__init__(\n        is_training,\n        data_format,\n        batch_norm_decay,\n        batch_norm_epsilon\n    )\n    self.n = (num_layers - 2) // 6\n    self.num_classes = num_classes\n    self.filters = [16, 16, 32, 64]\n    self.strides = [1, 2, 2]\n    self.version = version\n    self.loss_type = loss_type\n\n  def forward_pass(self, x, input_data_format=\'channels_last\'):\n    """"""Build the core model within the graph.""""""\n    if self._data_format != input_data_format:\n      if input_data_format == \'channels_last\':\n        # Computation requires channels_first.\n        x = tf.transpose(x, [0, 3, 1, 2])\n      else:\n        # Computation requires channels_last.\n        x = tf.transpose(x, [0, 2, 3, 1])\n\n    # Image standardization.\n    x = x / 128 - 1\n\n    x = self._conv(x, 3, 16, 1)\n    x = self._batch_norm(x)\n    x = self._relu(x)\n\n    if self.version == \'v1\':\n      # Use basic (non-bottleneck) block and ResNet V1 (post-activation).\n      res_func = self._residual_v1\n    elif self.version == \'v2\':\n      # Use basic (non-bottleneck) block and ResNet V2 (pre-activation).\n      res_func = self._residual_v2\n    else:  # \'bv2\'\n      # Use bottleneck block and ResNet V2 (pre-activation).\n      res_func = self._bottleneck_residual_v2\n\n    # 3 stages of block stacking.\n    for i in range(3):\n      with tf.name_scope(\'stage\'):\n        for j in range(self.n):\n          if j == 0:\n            # First block in a stage, filters and strides may change.\n            x = res_func(x, self.filters[i], self.filters[i + 1],\n                         self.strides[i])\n          else:\n            # Following blocks in a stage, constant filters and unit stride.\n            x = res_func(x, self.filters[i + 1], self.filters[i + 1], 1)\n\n    x = self._global_avg_pool(x)\n    x = self._fully_connected(x, self.num_classes, self.loss_type)\n\n    return x\n'"
src/cifar_utils.py,1,"b'import collections\nimport six\nimport json\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.core.framework import node_def_pb2\nfrom tensorflow.python.framework import device as pydev\nfrom tensorflow.python.training import basic_session_run_hooks\nfrom tensorflow.python.training import session_run_hook\nfrom tensorflow.python.training import training_util\nfrom tensorflow.python.training import device_setter\nfrom tensorflow.contrib.learn.python.learn import run_config\n\n\nclass RunConfig(tf.contrib.learn.RunConfig):\n  def uid(self, whitelist=None):\n    """"""Generates a \'Unique Identifier\' based on all internal fields.\n    Caller should use the uid string to check `RunConfig` instance integrity\n    in one session use, but should not rely on the implementation details, which\n    is subject to change.\n    Args:\n      whitelist: A list of the string names of the properties uid should not\n        include. If `None`, defaults to `_DEFAULT_UID_WHITE_LIST`, which\n        includes most properties user allowes to change.\n    Returns:\n      A uid string.\n    """"""\n    if whitelist is None:\n      whitelist = run_config._DEFAULT_UID_WHITE_LIST\n\n    state = {k: v for k, v in self.__dict__.items() if not k.startswith(\'__\')}\n    # Pop out the keys in whitelist.\n    for k in whitelist:\n      state.pop(\'_\' + k, None)\n\n    ordered_state = collections.OrderedDict(\n        sorted(state.items(), key=lambda t: t[0]))\n    # For class instance without __repr__, some special cares are required.\n    # Otherwise, the object address will be used.\n    if \'_cluster_spec\' in ordered_state:\n      ordered_state[\'_cluster_spec\'] = collections.OrderedDict(\n          sorted(ordered_state[\'_cluster_spec\'].as_dict().items(),\n                 key=lambda t: t[0])\n      )\n    return \', \'.join(\n        \'%s=%r\' % (k, v) for (k, v) in six.iteritems(ordered_state))\n\n\nclass ExamplesPerSecondHook(session_run_hook.SessionRunHook):\n  """"""Hook to print out examples per second.\n\n    Total time is tracked and then divided by the total number of steps\n    to get the average step time and then batch_size is used to determine\n    the running average of examples per second. The examples per second for the\n    most recent interval is also logged.\n  """"""\n\n  def __init__(\n          self,\n          batch_size,\n          every_n_steps=100,\n          every_n_secs=None,):\n    """"""Initializer for ExamplesPerSecondHook.\n\n      Args:\n      batch_size: Total batch size used to calculate examples/second from\n      global time.\n      every_n_steps: Log stats every n steps.\n      every_n_secs: Log stats every n seconds.\n    """"""\n    if (every_n_steps is None) == (every_n_secs is None):\n      raise ValueError(\'exactly one of every_n_steps\'\n                       \' and every_n_secs should be provided.\')\n    self._timer = basic_session_run_hooks.SecondOrStepTimer(\n        every_steps=every_n_steps, every_secs=every_n_secs)\n\n    self._step_train_time = 0\n    self._total_steps = 0\n    self._batch_size = batch_size\n\n  def begin(self):\n    self._global_step_tensor = training_util.get_global_step()\n    if self._global_step_tensor is None:\n      raise RuntimeError(\n          \'Global step should be created to use StepCounterHook.\')\n\n  def before_run(self, run_context):  # pylint: disable=unused-argument\n    return basic_session_run_hooks.SessionRunArgs(self._global_step_tensor)\n\n  def after_run(self, run_context, run_values):\n    _ = run_context\n\n    global_step = run_values.results\n    if self._timer.should_trigger_for_step(global_step):\n      elapsed_time, elapsed_steps = self._timer.update_last_triggered_step(\n          global_step)\n      if elapsed_time is not None:\n        steps_per_sec = elapsed_steps / elapsed_time\n        self._step_train_time += elapsed_time\n        self._total_steps += elapsed_steps\n\n        average_examples_per_sec = self._batch_size * (\n            self._total_steps / self._step_train_time)\n        current_examples_per_sec = steps_per_sec * self._batch_size\n        # Average examples/sec followed by current examples/sec\n        logging.info(\'%s: %g (%g), step = %g\', \'Average examples/sec\',\n                     average_examples_per_sec, current_examples_per_sec,\n                     self._total_steps)\n\n\ndef local_device_setter(num_devices=1,\n                        ps_device_type=\'cpu\',\n                        worker_device=\'/cpu:0\',\n                        ps_ops=None,\n                        ps_strategy=None):\n  if ps_ops == None:\n    ps_ops = [\'Variable\', \'VariableV2\', \'VarHandleOp\']\n\n  if ps_strategy is None:\n    ps_strategy = device_setter._RoundRobinStrategy(num_devices)\n  if not six.callable(ps_strategy):\n    raise TypeError(""ps_strategy must be callable"")\n\n  def _local_device_chooser(op):\n    current_device = pydev.DeviceSpec.from_string(op.device or """")\n\n    node_def = op if isinstance(op, node_def_pb2.NodeDef) else op.node_def\n    if node_def.op in ps_ops:\n      ps_device_spec = pydev.DeviceSpec.from_string(\n          \'/{}:{}\'.format(ps_device_type, ps_strategy(op)))\n\n      ps_device_spec.merge_from(current_device)\n      return ps_device_spec.to_string()\n    else:\n      worker_device_spec = pydev.DeviceSpec.from_string(worker_device or """")\n      worker_device_spec.merge_from(current_device)\n      return worker_device_spec.to_string()\n  return _local_device_chooser\n'"
src/data_utils.py,1,"b'from six.moves import cPickle as pickle\nimport json\nimport sys\nimport os\nimport numpy as np\nimport tensorflow as tf\n\n\n# ========================== Read/Write Data ===================================\nclass MyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return super(MyEncoder, self).default(obj)\n\n\ndef write_json(data, outfile):\n    json_dir, _ = os.path.split(outfile)\n    if not os.path.exists(json_dir):\n        os.makedirs(json_dir)\n\n    with open(outfile, \'w\') as f:\n        json.dump(data, f, ensure_ascii=False, indent=2, cls=MyEncoder)\n\n\ndef read_json(data_dir):\n    with open(data_dir, \'r\') as f:\n        output = json.load(f)\n    return output\n\n\ndef read_pickle_from_file(filename):\n    with tf.gfile.Open(filename, \'rb\') as f:\n        if sys.version_info >= (3, 0):\n            data_dict = pickle.load(f, encoding=\'bytes\')\n        else:\n            data_dict = pickle.load(f)\n    return data_dict\n\n\n# ========================== Original Cifar Data ===============================\ndef check_version(cifar_version):\n    if cifar_version not in [\'10\', \'100\', \'20\']:\n        raise ValueError(\'cifar version must be one of 10, 20, 100.\')\n\n\ndef img_num(cifar_version):\n    check_version(cifar_version)\n    dt = {\'10\': 5000, \'100\': 500, \'20\': 2500}\n    return dt[cifar_version]\n\n\ndef local_folder(cifar_version):\n    check_version(cifar_version)\n    dt = {\'10\': \'cifar-10-batches-py\',\n          \'100\': \'cifar-100-python\',\n          \'20\': \'cifar-100-python\'}\n    return dt[cifar_version]\n\n\ndef test_file(cifar_version):\n    check_version(cifar_version)\n    dt = {\'10\': \'test_batch\',\n          \'100\': \'test\',\n          \'20\': \'test\'}\n    return dt[cifar_version]\n\n\ndef local_label_names(cifar_version):\n    check_version(cifar_version)\n    dt = {\n        \'10\': \'cifar-10-batches-py/batches.meta\',\n        \'100\': \'cifar-100-python/meta\',\n        \'20\': \'cifar-100-python/meta\'\n    }\n    return dt[cifar_version]\n\n\ndef label_names(cifar_version):\n    check_version(cifar_version)\n    label_name = {\'10\': b\'labels\',\n                  \'100\': b\'fine_labels\',\n                  \'20\': b\'coarse_labels\'}\n    return label_name[cifar_version]\n\n\n# ======================== Imbalanced Cifar Data ===============================\ndef get_img_num_per_cls(cifar_version, imb_factor=None):\n    """"""\n    Get a list of image numbers for each class, given cifar version\n    Num of imgs follows emponential distribution\n    img max: 5000 / 500 * e^(-lambda * 0);\n    img min: 5000 / 500 * e^(-lambda * int(cifar_version - 1))\n    exp(-lambda * (int(cifar_version) - 1)) = img_max / img_min\n    args:\n      cifar_version: str, \'10\', \'100\', \'20\'\n      imb_factor: float, imbalance factor: img_min/img_max,\n        None if geting default cifar data number\n    output:\n      img_num_per_cls: a list of number of images per class\n    """"""\n    cls_num = int(cifar_version)\n    img_max = img_num(cifar_version)\n    if imb_factor is None:\n        return [img_max] * cls_num\n    img_num_per_cls = []\n    for cls_idx in range(cls_num):\n        num = img_max * (imb_factor**(cls_idx / (cls_num - 1.0)))\n        img_num_per_cls.append(int(num))\n    return img_num_per_cls\n'"
src/generate_cifar_tfrecords.py,8,"b'""""""Read CIFAR-10 data from pickled numpy arrays and writes TFRecords.\n\nGenerates tf.train.Example protos and writes them to TFRecord files from the\npython version of the CIFAR-10 dataset downloaded from\nhttps://www.cs.toronto.edu/~kriz/cifar.html.\nhttps://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\n\nimport tarfile\nfrom six.moves import cPickle as pickle\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\n\ndef download_and_extract(data_dir, data_version):\n  print(\'=\' * 80)\n  if data_version == \'10\':\n    print(\'cifar-10 dataset\')\n    CIFAR_FILENAME = \'cifar-10-python.tar.gz\'\n  else:\n    print(\'cifar-100 dataset\')\n    CIFAR_FILENAME = \'cifar-100-python.tar.gz\'\n  print(\'=\' * 80)\n\n  CIFAR_DOWNLOAD_URL = \'https://www.cs.toronto.edu/~kriz/\' + CIFAR_FILENAME\n\n  print(\'Download from {} and extract.\'.format(CIFAR_DOWNLOAD_URL))\n  # download CIFAR-10 if not already downloaded.\n  tf.contrib.learn.datasets.base.maybe_download(CIFAR_FILENAME, data_dir,\n                                                CIFAR_DOWNLOAD_URL)\n  tarfile.open(os.path.join(data_dir, CIFAR_FILENAME),\n               \'r:gz\').extractall(data_dir)\n\n\ndef _int64_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _bytes_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _get_file_names(data_version):\n  """"""Returns the file names expected to exist in the input_dir.""""""\n  file_names = {}\n  if data_version == \'10\':\n    file_names[\'train\'] = [\'data_batch_%d\' % i for i in xrange(1, 6)]\n    # file_names[\'validation\'] = [\'data_batch_5\']\n    file_names[\'eval\'] = [\'test_batch\']\n  else:\n    file_names[\'train\'] = [\'train\']\n    file_names[\'eval\'] = [\'test\']\n  return file_names\n\n\ndef read_pickle_from_file(filename):\n  with tf.gfile.Open(filename, \'rb\') as f:\n    if sys.version_info >= (3, 0):\n      data_dict = pickle.load(f, encoding=\'bytes\')\n    else:\n      data_dict = pickle.load(f)\n  return data_dict\n\n\ndef convert_to_tfrecord(input_files, output_file, data_version):\n  """"""Converts a file to TFRecords.""""""\n  print(\'Generating %s\' % output_file)\n  with tf.python_io.TFRecordWriter(output_file) as record_writer:\n    for input_file in input_files:\n      data_dict = read_pickle_from_file(input_file)\n      data = data_dict[b\'data\']\n      if data_version == \'10\':\n        labels = data_dict[b\'labels\']\n      elif data_version == \'100\': # cifar-100\n        labels = data_dict[b\'fine_labels\']\n      else: # cifar-20\n        labels = data_dict[b\'coarse_labels\']\n\n      num_entries_in_batch = len(labels)\n      for i in range(num_entries_in_batch):\n        example = tf.train.Example(features=tf.train.Features(\n            feature={\n                \'image\': _bytes_feature(data[i].tobytes()),\n                \'label\': _int64_feature(labels[i])\n            }))\n        record_writer.write(example.SerializeToString())\n\n\ndef main(args):\n  data_dir = args.data_dir\n  data_ver = args.CIFAR_data_version\n\n  download_and_extract(data_dir, data_ver)\n  file_names = _get_file_names(data_ver)\n\n  if data_ver == \'10\':\n    CIFAR_LOCAL_FOLDER = \'cifar-10-batches-py\'\n  else:\n    CIFAR_LOCAL_FOLDER = \'cifar-100-python\'\n  input_dir = os.path.join(data_dir, CIFAR_LOCAL_FOLDER)\n\n  for mode, files in file_names.items():\n    input_files = [os.path.join(input_dir, f) for f in files]\n    output_file = os.path.join(data_dir, mode + \'.tfrecords\')\n    try:\n      os.remove(output_file)\n    except OSError:\n      pass\n    # Convert to tf.train.Example and write the to TFRecords.\n    convert_to_tfrecord(input_files, output_file, data_ver)\n  print(\'Done!\')\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \'--data-dir\',\n      type=str,\n      default=\'\',\n      help=\'Directory to download and extract CIFAR-10/100 to.\')\n  parser.add_argument(\n      \'--CIFAR-data-version\',\n      type=str,\n      default=\'10\',\n      help=\'CIFAR data version, 10, 20, or 100\')\n\n  args = parser.parse_args()\n\n  if args.CIFAR_data_version not in [\'10\', \'20\', \'100\']:\n    raise ValueError(\'--CIFAR-data-version: must be one of 10, 20, and 100\')\n\n  main(args)\n'"
src/generate_cifar_tfrecords_im.py,7,"b'""""""Read CIFAR-10 data from pickled numpy arrays and writes\nTFRecords with imblanced classes.\n\nGenerates tf.train.Example protos and writes them to TFRecord files from the\npython version of the CIFAR-10 dataset downloaded from\nhttps://www.cs.toronto.edu/~kriz/cifar.html.\nhttps://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nimport random\n\nfrom data_utils import *\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _get_file_names(data_version):\n    """"""Returns the file names expected to exist in the input_dir.""""""\n    file_names = {}\n    if data_version == \'10\':\n        file_names[\'train\'] = [\'data_batch_%d\' % i for i in xrange(1, 6)]\n        # file_names[\'validation\'] = [\'data_batch_5\']\n        file_names[\'eval\'] = [\'test_batch\']\n    else:\n        file_names[\'train\'] = [\'train\']\n        file_names[\'eval\'] = [\'test\']\n    return file_names\n\n\ndef get_traindata_list(data_dir, data_version):\n    """"""Returns a dict contain a list of file names for training data.""""""\n    training_data = {}\n    for cls_idx in range(int(data_version)):\n        training_data[str(cls_idx)] = []\n    file_names = _get_file_names(data_version)[\'train\']\n\n    cifar_local = local_folder(data_version)\n    input_dir = os.path.join(data_dir, cifar_local)\n\n    for f in file_names:\n        data_path = os.path.join(input_dir, f)\n        data_dict = read_pickle_from_file(data_path)\n        for idx, label in enumerate(data_dict[label_names(data_version)]):\n            training_data[str(label)].append(f + \'/\' + str(idx))\n\n    return training_data\n\n\ndef get_imbalanced_data(training_data, img_num_per_cls):\n    """"""Get a list of imbalanced training data, store it into im_data dict.""""""\n    im_data = {}\n    for cls_idx, img_id_list in training_data.items():\n        random.shuffle(img_id_list)\n        img_num = img_num_per_cls[int(cls_idx)]\n        im_data[cls_idx] = img_id_list[:img_num]\n    return im_data\n\n\ndef sort_input(im_data, cifar_version):\n    """"""Sort data into batch - idx_list for faster tfrecord writing.""""""\n    data_to_write = []\n    for cls_idx, img_id_list in im_data.items():\n        data_to_write.extend(img_id_list)\n\n    print(\'number of training images are {}\'.format(len(data_to_write)))\n    if cifar_version == \'10\':\n        data_sorted = {\n            \'data_batch_1\': [],\n            \'data_batch_2\': [],\n            \'data_batch_3\': [],\n            \'data_batch_4\': [],\n            \'data_batch_5\': [],\n        }\n    else:\n        data_sorted = {\'train\': []}\n\n    for img_id in data_to_write:\n        image_id = img_id.split(\'/\')\n        data_sorted[image_id[0]].append(int(image_id[1]))\n    return data_sorted\n\n\ndef convert_to_tfrecord(input_dir, output_file, data_version, im_sorted_data):\n    """"""Converts a file to tfrecord.""""""\n    with tf.python_io.TFRecordWriter(output_file) as record_writer:\n        for folder_name, img_list in im_sorted_data.items():\n            # Convert to tf.train.Example and write the to TFRecords.\n            print(\'converting data from {}\'.format(folder_name))\n\n            input_path = os.path.join(input_dir, folder_name)\n            data_dict = read_pickle_from_file(input_path)\n            data = data_dict[b\'data\']\n            labels = data_dict[label_names(data_version)]\n\n            for img_id in img_list:\n                example = tf.train.Example(\n                    features=tf.train.Features(\n                        feature={\n                            \'image\': _bytes_feature(data[img_id].tobytes()),\n                            \'label\': _int64_feature(labels[img_id])\n                        }))\n                record_writer.write(example.SerializeToString())\n\n\ndef main(imb_factor):\n    data_ver = args.CIFAR_data_version\n\n    print(\'=\' * 80)\n    if data_ver == \'10\':\n        print(\'cifar-10, imbalance factor = \' + str(1.0 / imb_factor) + \'%\')\n    elif data_ver == \'20\':\n        print(\'cifar-20, imbalance factor = \' + str(1.0 / imb_factor) + \'%\')\n    elif data_ver == \'100\':\n        print(\'cifar-100, imbalance factor = \' + str(1.0 / imb_factor) + \'%\')\n    print(\'=\' * 80)\n\n    ori_data_dir = \'data/cifar-\' + data_ver + \'-data\'\n    im_data_dir = \'data/cifar-\' + data_ver + \'-data-im-\' + str(imb_factor)\n\n    orig_data = get_traindata_list(ori_data_dir, data_ver)\n    img_num_list = get_img_num_per_cls(data_ver, imb_factor)\n    im_data = get_imbalanced_data(orig_data, img_num_list)\n\n    # write_json(orig_data, os.path.join(ori_data_dir, \'train_img_id.json\'))\n    # write_json(im_data, os.path.join(im_data_dir, \'train_img_id.json\'))\n\n    im_sorted_data = sort_input(im_data, data_ver)\n\n    input_dir = os.path.join(ori_data_dir, local_folder(data_ver))\n    output_file = os.path.join(im_data_dir, \'train.tfrecords\')\n    print(\'writing data to \' + output_file)\n\n    try:\n        os.remove(output_file)\n    except OSError:\n        pass\n    convert_to_tfrecord(input_dir, output_file, data_ver, im_sorted_data)\n\n    print(\'Done!\')\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--CIFAR-data-version\',\n        type=str,\n        default=\'10\',\n        help=\'CIFAR data version, 10, 20, or 100\')\n\n    imb_factor_list = [0.005, 0.01, 0.02, 0.05, 0.1]\n\n    for imb_factor in imb_factor_list:\n        main(root, data_version, imb_factor)\n'"
src/model_base.py,30,"b'""""""ResNet model.\n\nRelated papers:\nhttps://arxiv.org/pdf/1603.05027v2.pdf\nhttps://arxiv.org/pdf/1512.03385v1.pdf\nhttps://arxiv.org/pdf/1605.07146v1.pdf\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass ResNet(object):\n  """"""ResNet model.""""""\n\n  def __init__(self,\n               is_training,\n               data_format,\n               batch_norm_decay,\n               batch_norm_epsilon):\n    """"""ResNet constructor.\n\n    Args:\n      is_training: if build training or inference model.\n      data_format: the data_format used during computation.\n                   one of \'channels_first\' or \'channels_last\'.\n    """"""\n    self._batch_norm_decay = batch_norm_decay\n    self._batch_norm_epsilon = batch_norm_epsilon\n    self._is_training = is_training\n    assert data_format in (\'channels_first\', \'channels_last\')\n    self._data_format = data_format\n\n  def forward_pass(self, x):\n    raise NotImplementedError(\n        \'forward_pass() is implemented in ResNet sub classes\')\n\n  def _residual_v1(self,\n                   x,\n                   in_filter,\n                   out_filter,\n                   stride,\n                   activate_before_residual=False):\n    """"""Residual unit with 2 sub layers, using Plan A for shortcut connection.""""""\n\n    del activate_before_residual\n    with tf.name_scope(\'residual_v1\') as name_scope:\n      orig_x = x\n\n      x = self._conv(x, 3, out_filter, stride)\n      x = self._batch_norm(x)\n      x = self._relu(x)\n\n      x = self._conv(x, 3, out_filter, 1)\n      x = self._batch_norm(x)\n\n      if in_filter != out_filter:\n        orig_x = self._avg_pool(orig_x, stride, stride)\n        pad = (out_filter - in_filter) // 2\n        if self._data_format == \'channels_first\':\n          orig_x = tf.pad(orig_x, [[0, 0], [pad, pad], [0, 0], [0, 0]])\n        else:\n          orig_x = tf.pad(orig_x, [[0, 0], [0, 0], [0, 0], [pad, pad]])\n\n      x = self._relu(tf.add(x, orig_x))\n\n      tf.logging.info(\'image after unit %s: %s\', name_scope, x.get_shape())\n      return x\n\n  def _residual_v2(self,\n                   x,\n                   in_filter,\n                   out_filter,\n                   stride,\n                   activate_before_residual=False):\n    """"""Residual unit with 2 sub layers with preactivation, plan A shortcut.""""""\n\n    with tf.name_scope(\'residual_v2\') as name_scope:\n      if activate_before_residual:\n        x = self._batch_norm(x)\n        x = self._relu(x)\n        orig_x = x\n      else:\n        orig_x = x\n        x = self._batch_norm(x)\n        x = self._relu(x)\n\n      x = self._conv(x, 3, out_filter, stride)\n\n      x = self._batch_norm(x)\n      x = self._relu(x)\n      x = self._conv(x, 3, out_filter, 1)\n      # x = self._conv(x, 3, out_filter, [1, 1, 1, 1])\n\n      if in_filter != out_filter:\n        pad = (out_filter - in_filter) // 2\n        orig_x = self._avg_pool(orig_x, stride, stride)\n        if self._data_format == \'channels_first\':\n          orig_x = tf.pad(orig_x, [[0, 0], [pad, pad], [0, 0], [0, 0]])\n        else:\n          orig_x = tf.pad(orig_x, [[0, 0], [0, 0], [0, 0], [pad, pad]])\n\n      x = tf.add(x, orig_x)\n\n      tf.logging.info(\'image after unit %s: %s\', name_scope, x.get_shape())\n      return x\n\n  def _bottleneck_residual_v2(self,\n                              x,\n                              in_filter,\n                              out_filter,\n                              stride,\n                              activate_before_residual=False):\n    """"""Bottleneck residual unit with 3 sub layers, plan B shortcut.""""""\n\n    with tf.name_scope(\'bottle_residual_v2\') as name_scope:\n      if activate_before_residual:\n        x = self._batch_norm(x)\n        x = self._relu(x)\n        orig_x = x\n      else:\n        orig_x = x\n        x = self._batch_norm(x)\n        x = self._relu(x)\n\n      x = self._conv(x, 1, out_filter // 4, stride, is_atrous=True)\n\n      x = self._batch_norm(x)\n      x = self._relu(x)\n      # pad when stride isn\'t unit\n      x = self._conv(x, 3, out_filter // 4, 1, is_atrous=True)\n\n      x = self._batch_norm(x)\n      x = self._relu(x)\n      x = self._conv(x, 1, out_filter, 1, is_atrous=True)\n\n      if in_filter != out_filter:\n        orig_x = self._conv(orig_x, 1, out_filter, stride, is_atrous=True)\n      x = tf.add(x, orig_x)\n\n      tf.logging.info(\'image after unit %s: %s\', name_scope, x.get_shape())\n      return x\n\n  def _conv(self, x, kernel_size, filters, strides, is_atrous=False):\n    """"""Convolution.""""""\n\n    padding = \'SAME\'\n    if not is_atrous and strides > 1:\n      pad = kernel_size - 1\n      pad_beg = pad // 2\n      pad_end = pad - pad_beg\n      if self._data_format == \'channels_first\':\n        x = tf.pad(x, [[0, 0], [0, 0], [pad_beg, pad_end], [pad_beg, pad_end]])\n      else:\n        x = tf.pad(x, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n      padding = \'VALID\'\n    return tf.layers.conv2d(\n        inputs=x,\n        kernel_size=kernel_size,\n        filters=filters,\n        strides=strides,\n        padding=padding,\n        use_bias=False,\n        data_format=self._data_format)\n\n  def _batch_norm(self, x):\n    if self._data_format == \'channels_first\':\n      data_format = \'NCHW\'\n    else:\n      data_format = \'NHWC\'\n    return tf.contrib.layers.batch_norm(\n        x,\n        decay=self._batch_norm_decay,\n        center=True,\n        scale=True,\n        epsilon=self._batch_norm_epsilon,\n        is_training=self._is_training,\n        fused=True,\n        data_format=data_format)\n\n  def _relu(self, x):\n    return tf.nn.relu(x)\n\n  def _fully_connected(self, x, out_dim, loss_type):\n    with tf.name_scope(\'fully_connected\') as name_scope:\n      if loss_type == \'softmax\':\n        x = tf.layers.dense(x, out_dim)\n      elif loss_type == \'sigmoid\' or loss_type == \'focal\':\n        x = tf.layers.dense(\n            inputs=x,\n            units=out_dim,\n            bias_initializer=tf.constant_initializer(-np.log(out_dim - 1))\n        )\n\n    tf.logging.info(\'image after unit %s: %s\', name_scope, x.get_shape())\n    return x\n\n  def _avg_pool(self, x, pool_size, stride):\n    with tf.name_scope(\'avg_pool\') as name_scope:\n      x = tf.layers.average_pooling2d(\n          x, pool_size, stride, \'SAME\', data_format=self._data_format)\n\n    tf.logging.info(\'image after unit %s: %s\', name_scope, x.get_shape())\n    return x\n\n  def _global_avg_pool(self, x):\n    with tf.name_scope(\'global_avg_pool\') as name_scope:\n      assert x.get_shape().ndims == 4\n      if self._data_format == \'channels_first\':\n        x = tf.reduce_mean(x, [2, 3])\n      else:\n        x = tf.reduce_mean(x, [1, 2])\n    tf.logging.info(\'image after unit %s: %s\', name_scope, x.get_shape())\n    return x\n'"
tpu/models/setup.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Cloud TPU samples.""""""\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nsetup(\n    name=\'cloud_tpu_samples\',\n    packages=find_packages()\n)\n'"
tpu/models/official/__init__.py,0,b''
tpu/tools/datasets/create_coco_tf_record.py,18,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Convert raw COCO dataset to TFRecord for object_detection.\n\nExample usage:\n    python create_coco_tf_record.py --logtostderr \\\n      --train_image_dir=""${TRAIN_IMAGE_DIR}"" \\\n      --val_image_dir=""${VAL_IMAGE_DIR}"" \\\n      --test_image_dir=""${TEST_IMAGE_DIR}"" \\\n      --train_annotations_file=""${TRAIN_ANNOTATIONS_FILE}"" \\\n      --val_annotations_file=""${VAL_ANNOTATIONS_FILE}"" \\\n      --testdev_annotations_file=""${TESTDEV_ANNOTATIONS_FILE}"" \\\n      --output_dir=""${OUTPUT_DIR}""\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport hashlib\nimport io\nimport json\nimport multiprocessing\nimport os\nfrom absl import flags\nimport numpy as np\nimport PIL.Image\n\nfrom pycocotools import mask\nfrom research.object_detection.utils import dataset_util\nfrom research.object_detection.utils import label_map_util\n\nimport tensorflow as tf\nflags.DEFINE_boolean(\'include_masks\', False,\n                     \'Whether to include instance segmentations masks \'\n                     \'(PNG encoded) in the result. default: False.\')\nflags.DEFINE_string(\'train_image_dir\', \'\', \'Training image directory.\')\nflags.DEFINE_string(\'val_image_dir\', \'\', \'Validation image directory.\')\nflags.DEFINE_string(\'test_image_dir\', \'\', \'Test image directory.\')\nflags.DEFINE_string(\'train_annotations_file\', \'\',\n                    \'Training annotations JSON file.\')\nflags.DEFINE_string(\'val_annotations_file\', \'\',\n                    \'Validation annotations JSON file.\')\nflags.DEFINE_string(\'testdev_annotations_file\', \'\',\n                    \'Test-dev annotations JSON file.\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/\', \'Output data directory.\')\n\nFLAGS = flags.FLAGS\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef create_tf_example(image,\n                      annotations_list,\n                      image_dir,\n                      category_index,\n                      include_masks=False):\n  """"""Converts image and annotations to a tf.Example proto.\n\n  Args:\n    image: dict with keys:\n      [u\'license\', u\'file_name\', u\'coco_url\', u\'height\', u\'width\',\n      u\'date_captured\', u\'flickr_url\', u\'id\']\n    annotations_list:\n      list of dicts with keys:\n      [u\'segmentation\', u\'area\', u\'iscrowd\', u\'image_id\',\n      u\'bbox\', u\'category_id\', u\'id\']\n      Notice that bounding box coordinates in the official COCO dataset are\n      given as [x, y, width, height] tuples using absolute coordinates where\n      x, y represent the top-left (0-indexed) corner.  This function converts\n      to the format expected by the Tensorflow Object Detection API (which is\n      which is [ymin, xmin, ymax, xmax] with coordinates normalized relative\n      to image size).\n    image_dir: directory containing the image files.\n    category_index: a dict containing COCO category information keyed\n      by the \'id\' field of each category.  See the\n      label_map_util.create_category_index function.\n    include_masks: Whether to include instance segmentations masks\n      (PNG encoded) in the result. default: False.\n  Returns:\n    example: The converted tf.Example\n    num_annotations_skipped: Number of (invalid) annotations that were ignored.\n\n  Raises:\n    ValueError: if the image pointed to by data[\'filename\'] is not a valid JPEG\n  """"""\n  image_height = image[\'height\']\n  image_width = image[\'width\']\n  filename = image[\'file_name\']\n  image_id = image[\'id\']\n\n  full_path = os.path.join(image_dir, filename)\n  with tf.gfile.GFile(full_path, \'rb\') as fid:\n    encoded_jpg = fid.read()\n  encoded_jpg_io = io.BytesIO(encoded_jpg)\n  image = PIL.Image.open(encoded_jpg_io)\n  key = hashlib.sha256(encoded_jpg).hexdigest()\n\n  xmin = []\n  xmax = []\n  ymin = []\n  ymax = []\n  is_crowd = []\n  category_names = []\n  category_ids = []\n  area = []\n  encoded_mask_png = []\n  num_annotations_skipped = 0\n  for object_annotations in annotations_list:\n    (x, y, width, height) = tuple(object_annotations[\'bbox\'])\n    if width <= 0 or height <= 0:\n      num_annotations_skipped += 1\n      continue\n    if x + width > image_width or y + height > image_height:\n      num_annotations_skipped += 1\n      continue\n    xmin.append(float(x) / image_width)\n    xmax.append(float(x + width) / image_width)\n    ymin.append(float(y) / image_height)\n    ymax.append(float(y + height) / image_height)\n    is_crowd.append(object_annotations[\'iscrowd\'])\n    category_id = int(object_annotations[\'category_id\'])\n    category_ids.append(category_id)\n    category_names.append(category_index[category_id][\'name\'].encode(\'utf8\'))\n    area.append(object_annotations[\'area\'])\n\n    if include_masks:\n      run_len_encoding = mask.frPyObjects(object_annotations[\'segmentation\'],\n                                          image_height, image_width)\n      binary_mask = mask.decode(run_len_encoding)\n      if not object_annotations[\'iscrowd\']:\n        binary_mask = np.amax(binary_mask, axis=2)\n      pil_image = PIL.Image.fromarray(binary_mask)\n      output_io = io.BytesIO()\n      pil_image.save(output_io, format=\'PNG\')\n      encoded_mask_png.append(output_io.getvalue())\n  feature_dict = {\n      \'image/height\':\n          dataset_util.int64_feature(image_height),\n      \'image/width\':\n          dataset_util.int64_feature(image_width),\n      \'image/filename\':\n          dataset_util.bytes_feature(filename.encode(\'utf8\')),\n      \'image/source_id\':\n          dataset_util.bytes_feature(str(image_id).encode(\'utf8\')),\n      \'image/key/sha256\':\n          dataset_util.bytes_feature(key.encode(\'utf8\')),\n      \'image/encoded\':\n          dataset_util.bytes_feature(encoded_jpg),\n      \'image/format\':\n          dataset_util.bytes_feature(\'jpeg\'.encode(\'utf8\')),\n      \'image/object/bbox/xmin\':\n          dataset_util.float_list_feature(xmin),\n      \'image/object/bbox/xmax\':\n          dataset_util.float_list_feature(xmax),\n      \'image/object/bbox/ymin\':\n          dataset_util.float_list_feature(ymin),\n      \'image/object/bbox/ymax\':\n          dataset_util.float_list_feature(ymax),\n      \'image/object/class/text\':\n          dataset_util.bytes_list_feature(category_names),\n      \'image/object/class/label\':\n          dataset_util.int64_list_feature(category_ids),\n      \'image/object/is_crowd\':\n          dataset_util.int64_list_feature(is_crowd),\n      \'image/object/area\':\n          dataset_util.float_list_feature(area),\n  }\n  if include_masks:\n    feature_dict[\'image/object/mask\'] = (\n        dataset_util.bytes_list_feature(encoded_mask_png))\n  example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n  return key, example, num_annotations_skipped\n\n\ndef _pool_create_tf_example(args):\n  return create_tf_example(*args)\n\n\ndef _create_tf_record_from_coco_annotations(\n    annotations_file, image_dir, output_path, include_masks, num_shards):\n  """"""Loads COCO annotation json files and converts to tf.Record format.\n\n  Args:\n    annotations_file: JSON file containing bounding box annotations.\n    image_dir: Directory containing the image files.\n    output_path: Path to output tf.Record file.\n    include_masks: Whether to include instance segmentations masks\n      (PNG encoded) in the result. default: False.\n    num_shards: Number of output files to create.\n  """"""\n  with tf.gfile.GFile(annotations_file, \'r\') as fid:\n    groundtruth_data = json.load(fid)\n\n  images = groundtruth_data[\'images\']\n  category_index = label_map_util.create_category_index(\n      groundtruth_data[\'categories\'])\n\n  annotations_index = {}\n  if \'annotations\' in groundtruth_data:\n    tf.logging.info(\n        \'Found groundtruth annotations. Building annotations index.\')\n    for annotation in groundtruth_data[\'annotations\']:\n      image_id = annotation[\'image_id\']\n      if image_id not in annotations_index:\n        annotations_index[image_id] = []\n      annotations_index[image_id].append(annotation)\n  missing_annotation_count = 0\n  for image in images:\n    image_id = image[\'id\']\n    if image_id not in annotations_index:\n      missing_annotation_count += 1\n      annotations_index[image_id] = []\n\n  tf.logging.info(\'%d images are missing annotations.\',\n                  missing_annotation_count)\n\n  tf.logging.info(\'writing to output path: %s\', output_path)\n  writers = [\n      tf.python_io.TFRecordWriter(output_path + \'-%05d-of-%05d.tfrecord\' %\n                                  (i, num_shards)) for i in range(num_shards)\n  ]\n\n  pool = multiprocessing.Pool()\n  total_num_annotations_skipped = 0\n  for idx, (_, tf_example, num_annotations_skipped) in enumerate(\n      pool.imap(_pool_create_tf_example,\n                [(image, annotations_index[image[\'id\']], image_dir,\n                  category_index, include_masks)\n                 for image in images])):\n    if idx % 100 == 0:\n      tf.logging.info(\'On image %d of %d\', idx, len(images))\n\n    total_num_annotations_skipped += num_annotations_skipped\n    writers[idx % num_shards].write(tf_example.SerializeToString())\n\n  pool.close()\n  pool.join()\n\n  for writer in writers:\n    writer.close()\n\n  tf.logging.info(\'Finished writing, skipped %d annotations.\',\n                  total_num_annotations_skipped)\n\n\ndef main(_):\n  assert FLAGS.train_image_dir, \'`train_image_dir` missing.\'\n  assert FLAGS.val_image_dir, \'`val_image_dir` missing.\'\n  assert FLAGS.test_image_dir, \'`test_image_dir` missing.\'\n  assert FLAGS.train_annotations_file, \'`train_annotations_file` missing.\'\n  assert FLAGS.val_annotations_file, \'`val_annotations_file` missing.\'\n  assert FLAGS.testdev_annotations_file, \'`testdev_annotations_file` missing.\'\n\n  if not tf.gfile.IsDirectory(FLAGS.output_dir):\n    tf.gfile.MakeDirs(FLAGS.output_dir)\n  train_output_path = os.path.join(FLAGS.output_dir, \'train\')\n  val_output_path = os.path.join(FLAGS.output_dir, \'val\')\n  testdev_output_path = os.path.join(FLAGS.output_dir, \'test-dev\')\n\n  _create_tf_record_from_coco_annotations(\n      FLAGS.train_annotations_file,\n      FLAGS.train_image_dir,\n      train_output_path,\n      FLAGS.include_masks,\n      num_shards=256)\n  _create_tf_record_from_coco_annotations(\n      FLAGS.val_annotations_file,\n      FLAGS.val_image_dir,\n      val_output_path,\n      FLAGS.include_masks,\n      num_shards=32)\n  _create_tf_record_from_coco_annotations(\n      FLAGS.testdev_annotations_file,\n      FLAGS.test_image_dir,\n      testdev_output_path,\n      FLAGS.include_masks,\n      num_shards=32)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/tools/datasets/dataset_to_gcs.py,25,"b'r""""""Script to convert dataset and upload to gcs.\n\n```\npython dataset_to_gcs.py \\\n  --project=tpu-training-221714 \\\n  --gcs_output_path=gs://tpu_training \\\n  --local_scratch_dir=/media/yincui/HardDrive/data/inat2018/tfrecord \\\n  --raw_data_dir=/media/yincui/HardDrive/data/inat2018\n```\n""""""\n\nimport math\nimport os\nimport random\nimport tarfile\nimport urllib\n\nfrom absl import app\nfrom absl import flags\nimport tensorflow as tf\n\nfrom google.cloud import storage\n\nflags.DEFINE_string(\n    \'project\', None, \'Google cloud project id for uploading the dataset.\')\nflags.DEFINE_string(\n    \'gcs_output_path\', None, \'GCS path for uploading the dataset.\')\nflags.DEFINE_string(\n    \'local_scratch_dir\', None, \'Scratch directory path for temporary files.\')\nflags.DEFINE_string(\n    \'raw_data_dir\', None, \'Directory path for raw dataset. \'\n    \'Should have train and validation subdirectories inside it.\')\nflags.DEFINE_boolean(\n    \'gcs_upload\', True, \'Set to false to not upload to gcs.\')\n\nFLAGS = flags.FLAGS\n\nTRAINING_SHARDS = 1000\nVALIDATION_SHARDS = 100\n\nTRAINING_DIRECTORY = \'train\'\nVALIDATION_DIRECTORY = \'validation\'\n\n\ndef _check_or_create_dir(directory):\n  """"""Check if directory exists otherwise create it.""""""\n  if not tf.gfile.Exists(directory):\n    tf.gfile.MakeDirs(directory)\n\n\ndef _int64_feature(value):\n  """"""Wrapper for inserting int64 features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef _bytes_feature(value):\n  """"""Wrapper for inserting bytes features into Example proto.""""""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _convert_to_example(filename, image_buffer, label, height, width):\n  """"""Build an Example proto for an example.\n\n  Args:\n    filename: string, path to an image file, e.g., \'/path/to/example.JPG\'\n    image_buffer: string, JPEG encoding of RGB image\n    label: integer, identifier for the ground truth for the network\n    height: integer, image height in pixels\n    width: integer, image width in pixels\n  Returns:\n    Example proto\n  """"""\n  image_format = b\'JPEG\'\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n      \'image/height\': _int64_feature(height),\n      \'image/width\': _int64_feature(width),\n      \'image/class/label\': _int64_feature(label),\n      \'image/format\': _bytes_feature(image_format),\n      \'image/encoded\': _bytes_feature(image_buffer)}))\n  return example\n\n\ndef _is_png(filename):\n  """"""Determine if a file contains a PNG format image.\n\n  Args:\n    filename: string, path of the image file.\n\n  Returns:\n    boolean indicating if the image is a PNG.\n  """"""\n  # File list from:\n  # https://github.com/cytsai/ilsvrc-cmyk-image-list\n  return \'n02105855_2933.JPEG\' in filename\n\n\ndef _is_cmyk(filename):\n  """"""Determine if file contains a CMYK JPEG format image.\n\n  Args:\n    filename: string, path of the image file.\n\n  Returns:\n    boolean indicating if the image is a JPEG encoded with CMYK color space.\n  """"""\n  # File list from:\n  # https://github.com/cytsai/ilsvrc-cmyk-image-list\n  blacklist = set([\'n01739381_1309.JPEG\', \'n02077923_14822.JPEG\',\n                   \'n02447366_23489.JPEG\', \'n02492035_15739.JPEG\',\n                   \'n02747177_10752.JPEG\', \'n03018349_4028.JPEG\',\n                   \'n03062245_4620.JPEG\', \'n03347037_9675.JPEG\',\n                   \'n03467068_12171.JPEG\', \'n03529860_11437.JPEG\',\n                   \'n03544143_17228.JPEG\', \'n03633091_5218.JPEG\',\n                   \'n03710637_5125.JPEG\', \'n03961711_5286.JPEG\',\n                   \'n04033995_2932.JPEG\', \'n04258138_17003.JPEG\',\n                   \'n04264628_27969.JPEG\', \'n04336792_7448.JPEG\',\n                   \'n04371774_5854.JPEG\', \'n04596742_4225.JPEG\',\n                   \'n07583066_647.JPEG\', \'n13037406_4650.JPEG\'])\n  return os.path.basename(filename) in blacklist\n\n\nclass ImageCoder(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Create a single Session to run all image coding calls.\n    self._sess = tf.Session()\n\n    # Initializes function that converts PNG to JPEG data.\n    self._png_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_png(self._png_data, channels=3)\n    self._png_to_jpeg = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that converts CMYK JPEG data to RGB JPEG data.\n    self._cmyk_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_jpeg(self._cmyk_data, channels=0)\n    self._cmyk_to_rgb = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def png_to_jpeg(self, image_data):\n    return self._sess.run(self._png_to_jpeg,\n                          feed_dict={self._png_data: image_data})\n\n  def cmyk_to_rgb(self, image_data):\n    return self._sess.run(self._cmyk_to_rgb,\n                          feed_dict={self._cmyk_data: image_data})\n\n  def decode_jpeg(self, image_data):\n    image = self._sess.run(self._decode_jpeg,\n                           feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\ndef _process_image(filename, coder):\n  """"""Process a single image file.\n\n  Args:\n    filename: string, path to an image file e.g., \'/path/to/example.JPG\'.\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n  Returns:\n    image_buffer: string, JPEG encoding of RGB image.\n    height: integer, image height in pixels.\n    width: integer, image width in pixels.\n  """"""\n  # Read the image file.\n  with tf.gfile.FastGFile(filename, \'rb\') as f:\n    image_data = f.read()\n\n  # Clean the dirty data.\n  if _is_png(filename):\n    # 1 image is a PNG.\n    tf.logging.info(\'Converting PNG to JPEG for %s\' % filename)\n    image_data = coder.png_to_jpeg(image_data)\n  elif _is_cmyk(filename):\n    # 22 JPEG images are in CMYK colorspace.\n    tf.logging.info(\'Converting CMYK to RGB for %s\' % filename)\n    image_data = coder.cmyk_to_rgb(image_data)\n\n  # Decode the RGB JPEG.\n  image = coder.decode_jpeg(image_data)\n\n  # Check that image converted to RGB\n  assert len(image.shape) == 3\n  height = image.shape[0]\n  width = image.shape[1]\n  assert image.shape[2] == 3\n\n  return image_data, height, width\n\n\ndef _process_image_files_batch(coder, output_file, filenames, labels):\n  """"""Processes and saves list of images as TFRecords.\n\n  Args:\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    output_file: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    labels: labels\n  """"""\n  writer = tf.python_io.TFRecordWriter(output_file)\n\n  for i in range(len(filenames)):\n    filename = filenames[i]\n    label = labels[i]\n    image_buffer, height, width = _process_image(filename, coder)\n    example = _convert_to_example(filename, image_buffer, label, height, width)\n    writer.write(example.SerializeToString())\n\n  writer.close()\n\n\ndef _process_dataset(filenames, labels, output_directory, prefix, num_shards):\n  """"""Processes and saves list of images as TFRecords.\n\n  Args:\n    filenames: list of strings; each string is a path to an image file\n    labels: labels\n    output_directory: path where output files should be created\n    prefix: string; prefix for each file\n    num_shards: number of chucks to split the filenames into\n\n  Returns:\n    files: list of tf-record filepaths created from processing the dataset.\n  """"""\n  _check_or_create_dir(output_directory)\n  chunksize = int(math.ceil(len(filenames) / num_shards))\n  coder = ImageCoder()\n\n  files = []\n\n  for shard in range(num_shards):\n    chunk_files = filenames[shard * chunksize : (shard + 1) * chunksize]\n    chunk_labels = labels[shard * chunksize : (shard + 1) * chunksize]\n    output_file = os.path.join(\n        output_directory, \'%s-%.5d-of-%.5d\' % (prefix, shard, num_shards))\n    _process_image_files_batch(coder, output_file, chunk_files, chunk_labels)\n    tf.logging.info(\'Finished writing file: %s\' % output_file)\n    files.append(output_file)\n  return files\n\n\ndef convert_to_tf_records(raw_data_dir):\n  """"""Convert the dataset into TF-Record dumps.""""""\n\n  # Glob all the training files\n  training_files = []\n  training_labels = []\n  for line in open(os.path.join(raw_data_dir, \'train.txt\'), \'r\'):\n    line_list = line.strip().split(\': \')\n    training_files.append(os.path.join(raw_data_dir, line_list[0]))\n    training_labels.append(int(line_list[1]))\n\n  # Shuffle training records to ensure we are distributing classes\n  # across the batches.\n  training_shuffle_idx = list(zip(training_files, training_labels))\n  random.shuffle(training_shuffle_idx)\n  training_files, training_labels = zip(*training_shuffle_idx)\n\n  # Glob all the validation files\n  validation_files = []\n  validation_labels = []\n  for line in open(os.path.join(raw_data_dir, \'val.txt\'), \'r\'):\n    line_list = line.strip().split(\': \')\n    validation_files.append(os.path.join(raw_data_dir, line_list[0]))\n    validation_labels.append(int(line_list[1]))\n\n  # Create training data\n  tf.logging.info(\'Processing the training data.\')\n  training_records = _process_dataset(\n      training_files, training_labels,\n      os.path.join(FLAGS.local_scratch_dir, TRAINING_DIRECTORY),\n      TRAINING_DIRECTORY, TRAINING_SHARDS)\n\n  # Create validation data\n  tf.logging.info(\'Processing the validation data.\')\n  validation_records = _process_dataset(\n      validation_files, validation_labels,\n      os.path.join(FLAGS.local_scratch_dir, VALIDATION_DIRECTORY),\n      VALIDATION_DIRECTORY, VALIDATION_SHARDS)\n\n  return training_records, validation_records\n\n\ndef upload_to_gcs(training_records, validation_records):\n  """"""Upload TF-Record files to GCS, at provided path.""""""\n\n  # Find the GCS bucket_name and key_prefix for dataset files\n  path_parts = FLAGS.gcs_output_path[5:].split(\'/\', 1)\n  bucket_name = path_parts[0]\n  if len(path_parts) == 1:\n    key_prefix = \'\'\n  elif path_parts[1].endswith(\'/\'):\n    key_prefix = path_parts[1]\n  else:\n    key_prefix = path_parts[1] + \'/\'\n\n  client = storage.Client(project=FLAGS.project)\n  bucket = client.get_bucket(bucket_name)\n\n  def _upload_files(filenames):\n    """"""Upload a list of files into a specifc subdirectory.""""""\n    for i, filename in enumerate(sorted(filenames)):\n      blob = bucket.blob(key_prefix + os.path.basename(filename))\n      blob.upload_from_filename(filename)\n      if not i % 20:\n        tf.logging.info(\'Finished uploading file: %s\' % filename)\n\n  # Upload training dataset\n  tf.logging.info(\'Uploading the training data.\')\n  _upload_files(training_records)\n\n  # Upload validation dataset\n  tf.logging.info(\'Uploading the validation data.\')\n  _upload_files(validation_records)\n\n\ndef main(argv):  # pylint: disable=unused-argument\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  if FLAGS.gcs_upload and FLAGS.project is None:\n    raise ValueError(\'GCS Project must be provided.\')\n\n  if FLAGS.gcs_upload and FLAGS.gcs_output_path is None:\n    raise ValueError(\'GCS output path must be provided.\')\n  elif FLAGS.gcs_upload and not FLAGS.gcs_output_path.startswith(\'gs://\'):\n    raise ValueError(\'GCS output path must start with gs://\')\n\n  if FLAGS.local_scratch_dir is None:\n    raise ValueError(\'Scratch directory path must be provided.\')\n\n  raw_data_dir = FLAGS.raw_data_dir\n\n  # Convert the raw data into tf-records\n  training_records, validation_records = convert_to_tf_records(raw_data_dir)\n\n  # Upload to GCS\n  if FLAGS.gcs_upload:\n    upload_to_gcs(training_records, validation_records)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tpu/tools/datasets/imagenet_to_gcs.py,33,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Script to download the Imagenet dataset and upload to gcs.\n\nTo run the script setup a virtualenv with the following libraries installed.\n- `gcloud`: Follow the instructions on\n  [cloud SDK docs](https://cloud.google.com/sdk/downloads) followed by\n  installing the python api using `pip install gcloud`.\n- `google-cloud-storage`: Install with `pip install google-cloud-storage`\n- `tensorflow`: Install with `pip install tensorflow`\n\nOnce you have all the above libraries setup, you should register on the\n[Imagenet website](http://image-net.org/download-images) to get your\nusername and access_key.\n\nMake sure you have around 300GB of disc space available on the machine where\nyou\'re running this script. You can run the script using the following command.\n```\npython imagenet_to_gcs.py \\\n  --project=""TEST_PROJECT"" \\\n  --gcs_output_path=""gs://TEST_BUCKET/IMAGENET_DIR"" \\\n  --local_scratch_dir=""./imagenet"" \\\n  --imagenet_username=FILL_ME_IN \\\n  --imagenet_access_key=FILL_ME_IN \\\n```\n\nOptionally if the raw data has already been downloaded you can provide a direct\n`raw_data_directory` path. If raw data directory is provided it should be in\nthe format:\n- Training images: train/n03062245/n03062245_4620.JPEG\n- Validation Images: validation/ILSVRC2012_val_00000001.JPEG\n- Validation Labels: synset_labels.txt\n""""""\n\nimport math\nimport os\nimport random\nimport tarfile\nimport urllib\n\nfrom absl import flags\nimport tensorflow as tf\n\nfrom google.cloud import storage\n\nflags.DEFINE_string(\n    \'project\', None, \'Google cloud project id for uploading the dataset.\')\nflags.DEFINE_string(\n    \'gcs_output_path\', None, \'GCS path for uploading the dataset.\')\nflags.DEFINE_string(\n    \'local_scratch_dir\', None, \'Scratch directory path for temporary files.\')\nflags.DEFINE_string(\n    \'raw_data_dir\', None, \'Directory path for raw Imagenet dataset. \'\n    \'Should have train and validation subdirectories inside it.\')\nflags.DEFINE_string(\n    \'imagenet_username\', None, \'Username for Imagenet.org account\')\nflags.DEFINE_string(\n    \'imagenet_access_key\', None, \'Access Key for Imagenet.org account\')\nflags.DEFINE_boolean(\n    \'gcs_upload\', True, \'Set to false to not upload to gcs.\')\n\nFLAGS = flags.FLAGS\n\nBASE_URL = \'http://www.image-net.org/challenges/LSVRC/2012/nnoupb/\'\nLABELS_URL = \'https://raw.githubusercontent.com/tensorflow/models/master/research/inception/inception/data/imagenet_2012_validation_synset_labels.txt\'  # pylint: disable=line-too-long\n\nTRAINING_FILE = \'ILSVRC2012_img_train.tar\'\nVALIDATION_FILE = \'ILSVRC2012_img_val.tar\'\nLABELS_FILE = \'synset_labels.txt\'\n\nTRAINING_SHARDS = 1024\nVALIDATION_SHARDS = 128\n\nTRAINING_DIRECTORY = \'train\'\nVALIDATION_DIRECTORY = \'validation\'\n\n\ndef _check_or_create_dir(directory):\n  """"""Check if directory exists otherwise create it.""""""\n  if not tf.gfile.Exists(directory):\n    tf.gfile.MakeDirs(directory)\n\n\ndef download_dataset(raw_data_dir):\n  """"""Download the Imagenet dataset into the temporary directory.""""""\n  def _download(url, filename):\n    """"""Download the dataset at the provided filepath.""""""\n    urllib.urlretrieve(url, filename)\n\n  def _get_members(filename):\n    """"""Get all members of a tarfile.""""""\n    tar = tarfile.open(filename)\n    members = tar.getmembers()\n    tar.close()\n    return members\n\n  def _untar_file(filename, directory, member=None):\n    """"""Untar a file at the provided directory path.""""""\n    _check_or_create_dir(directory)\n    tar = tarfile.open(filename)\n    if member is None:\n      tar.extractall(path=directory)\n    else:\n      tar.extract(member, path=directory)\n    tar.close()\n\n  # Check if raw_data_dir exists\n  _check_or_create_dir(raw_data_dir)\n\n  # Download the training data\n  tf.logging.info(\'Downloading the training set. This may take a few hours.\')\n  directory = os.path.join(raw_data_dir, TRAINING_DIRECTORY)\n  filename = os.path.join(raw_data_dir, TRAINING_FILE)\n  _download(BASE_URL + TRAINING_FILE, filename)\n\n  # The training tarball contains multiple tar balls inside it. Extract them\n  # in order to create a clean directory structure.\n  for member in _get_members(filename):\n    subdirectory = os.path.join(directory, member.name.split(\'.\')[0])\n    sub_tarfile = os.path.join(subdirectory, member.name)\n\n    _untar_file(filename, subdirectory, member)\n    _untar_file(sub_tarfile, subdirectory)\n    os.remove(sub_tarfile)\n\n  # Download synset_labels for validation set\n  tf.logging.info(\'Downloading the validation labels.\')\n  _download(LABELS_URL, os.path.join(raw_data_dir, LABELS_FILE))\n\n  # Download the validation data\n  tf.logging.info(\'Downloading the validation set. This may take a few hours.\')\n  directory = os.path.join(raw_data_dir, VALIDATION_DIRECTORY)\n  filename = os.path.join(raw_data_dir, VALIDATION_FILE)\n  _download(BASE_URL + VALIDATION_FILE, filename)\n  _untar_file(filename, directory)\n\n\ndef _int64_feature(value):\n  """"""Wrapper for inserting int64 features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef _bytes_feature(value):\n  """"""Wrapper for inserting bytes features into Example proto.""""""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _convert_to_example(filename, image_buffer, label, synset, height, width):\n  """"""Build an Example proto for an example.\n\n  Args:\n    filename: string, path to an image file, e.g., \'/path/to/example.JPG\'\n    image_buffer: string, JPEG encoding of RGB image\n    label: integer, identifier for the ground truth for the network\n    synset: string, unique WordNet ID specifying the label, e.g., \'n02323233\'\n    height: integer, image height in pixels\n    width: integer, image width in pixels\n  Returns:\n    Example proto\n  """"""\n  colorspace = \'RGB\'\n  channels = 3\n  image_format = \'JPEG\'\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n      \'image/height\': _int64_feature(height),\n      \'image/width\': _int64_feature(width),\n      \'image/colorspace\': _bytes_feature(colorspace),\n      \'image/channels\': _int64_feature(channels),\n      \'image/class/label\': _int64_feature(label),\n      \'image/class/synset\': _bytes_feature(synset),\n      \'image/format\': _bytes_feature(image_format),\n      \'image/filename\': _bytes_feature(os.path.basename(filename)),\n      \'image/encoded\': _bytes_feature(image_buffer)}))\n  return example\n\n\ndef _is_png(filename):\n  """"""Determine if a file contains a PNG format image.\n\n  Args:\n    filename: string, path of the image file.\n\n  Returns:\n    boolean indicating if the image is a PNG.\n  """"""\n  # File list from:\n  # https://github.com/cytsai/ilsvrc-cmyk-image-list\n  return \'n02105855_2933.JPEG\' in filename\n\n\ndef _is_cmyk(filename):\n  """"""Determine if file contains a CMYK JPEG format image.\n\n  Args:\n    filename: string, path of the image file.\n\n  Returns:\n    boolean indicating if the image is a JPEG encoded with CMYK color space.\n  """"""\n  # File list from:\n  # https://github.com/cytsai/ilsvrc-cmyk-image-list\n  blacklist = set([\'n01739381_1309.JPEG\', \'n02077923_14822.JPEG\',\n                   \'n02447366_23489.JPEG\', \'n02492035_15739.JPEG\',\n                   \'n02747177_10752.JPEG\', \'n03018349_4028.JPEG\',\n                   \'n03062245_4620.JPEG\', \'n03347037_9675.JPEG\',\n                   \'n03467068_12171.JPEG\', \'n03529860_11437.JPEG\',\n                   \'n03544143_17228.JPEG\', \'n03633091_5218.JPEG\',\n                   \'n03710637_5125.JPEG\', \'n03961711_5286.JPEG\',\n                   \'n04033995_2932.JPEG\', \'n04258138_17003.JPEG\',\n                   \'n04264628_27969.JPEG\', \'n04336792_7448.JPEG\',\n                   \'n04371774_5854.JPEG\', \'n04596742_4225.JPEG\',\n                   \'n07583066_647.JPEG\', \'n13037406_4650.JPEG\'])\n  return os.path.basename(filename) in blacklist\n\n\nclass ImageCoder(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Create a single Session to run all image coding calls.\n    self._sess = tf.Session()\n\n    # Initializes function that converts PNG to JPEG data.\n    self._png_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_png(self._png_data, channels=3)\n    self._png_to_jpeg = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that converts CMYK JPEG data to RGB JPEG data.\n    self._cmyk_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_jpeg(self._cmyk_data, channels=0)\n    self._cmyk_to_rgb = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def png_to_jpeg(self, image_data):\n    return self._sess.run(self._png_to_jpeg,\n                          feed_dict={self._png_data: image_data})\n\n  def cmyk_to_rgb(self, image_data):\n    return self._sess.run(self._cmyk_to_rgb,\n                          feed_dict={self._cmyk_data: image_data})\n\n  def decode_jpeg(self, image_data):\n    image = self._sess.run(self._decode_jpeg,\n                           feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\ndef _process_image(filename, coder):\n  """"""Process a single image file.\n\n  Args:\n    filename: string, path to an image file e.g., \'/path/to/example.JPG\'.\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n  Returns:\n    image_buffer: string, JPEG encoding of RGB image.\n    height: integer, image height in pixels.\n    width: integer, image width in pixels.\n  """"""\n  # Read the image file.\n  with tf.gfile.FastGFile(filename, \'r\') as f:\n    image_data = f.read()\n\n  # Clean the dirty data.\n  if _is_png(filename):\n    # 1 image is a PNG.\n    tf.logging.info(\'Converting PNG to JPEG for %s\' % filename)\n    image_data = coder.png_to_jpeg(image_data)\n  elif _is_cmyk(filename):\n    # 22 JPEG images are in CMYK colorspace.\n    tf.logging.info(\'Converting CMYK to RGB for %s\' % filename)\n    image_data = coder.cmyk_to_rgb(image_data)\n\n  # Decode the RGB JPEG.\n  image = coder.decode_jpeg(image_data)\n\n  # Check that image converted to RGB\n  assert len(image.shape) == 3\n  height = image.shape[0]\n  width = image.shape[1]\n  assert image.shape[2] == 3\n\n  return image_data, height, width\n\n\ndef _process_image_files_batch(coder, output_file, filenames, synsets, labels):\n  """"""Processes and saves list of images as TFRecords.\n\n  Args:\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    output_file: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    synsets: list of strings; each string is a unique WordNet ID\n    labels: map of string to integer; id for all synset labels\n  """"""\n  writer = tf.python_io.TFRecordWriter(output_file)\n\n  for filename, synset in zip(filenames, synsets):\n    image_buffer, height, width = _process_image(filename, coder)\n    label = labels[synset]\n    example = _convert_to_example(filename, image_buffer, label,\n                                  synset, height, width)\n    writer.write(example.SerializeToString())\n\n  writer.close()\n\n\ndef _process_dataset(filenames, synsets, labels, output_directory, prefix,\n                     num_shards):\n  """"""Processes and saves list of images as TFRecords.\n\n  Args:\n    filenames: list of strings; each string is a path to an image file\n    synsets: list of strings; each string is a unique WordNet ID\n    labels: map of string to integer; id for all synset labels\n    output_directory: path where output files should be created\n    prefix: string; prefix for each file\n    num_shards: number of chucks to split the filenames into\n\n  Returns:\n    files: list of tf-record filepaths created from processing the dataset.\n  """"""\n  _check_or_create_dir(output_directory)\n  chunksize = int(math.ceil(len(filenames) / num_shards))\n  coder = ImageCoder()\n\n  files = []\n\n  for shard in range(num_shards):\n    chunk_files = filenames[shard * chunksize : (shard + 1) * chunksize]\n    chunk_synsets = synsets[shard * chunksize : (shard + 1) * chunksize]\n    output_file = os.path.join(\n        output_directory, \'%s-%.5d-of-%.5d\' % (prefix, shard, num_shards))\n    _process_image_files_batch(coder, output_file, chunk_files,\n                               chunk_synsets, labels)\n    tf.logging.info(\'Finished writing file: %s\' % output_file)\n    files.append(output_file)\n  return files\n\n\ndef convert_to_tf_records(raw_data_dir):\n  """"""Convert the Imagenet dataset into TF-Record dumps.""""""\n\n  # Shuffle training records to ensure we are distributing classes\n  # across the batches.\n  random.seed(0)\n  def make_shuffle_idx(n):\n    order = range(n)\n    random.shuffle(order)\n    return order\n\n  # Glob all the training files\n  training_files = tf.gfile.Glob(\n      os.path.join(raw_data_dir, TRAINING_DIRECTORY, \'*\', \'*.JPEG\'))\n\n  # Get training file synset labels from the directory name\n  training_synsets = [\n      os.path.basename(os.path.dirname(f)) for f in training_files]\n\n  training_shuffle_idx = make_shuffle_idx(len(training_files))\n  training_files = [training_files[i] for i in training_shuffle_idx]\n  training_synsets = [training_synsets[i] for i in training_shuffle_idx]\n\n  # Glob all the validation files\n  validation_files = sorted(tf.gfile.Glob(\n      os.path.join(raw_data_dir, VALIDATION_DIRECTORY, \'*.JPEG\')))\n\n  # Get validation file synset labels from labels.txt\n  validation_synsets = tf.gfile.FastGFile(\n      os.path.join(raw_data_dir, LABELS_FILE), \'r\').read().splitlines()\n\n  # Create unique ids for all synsets\n  labels = {v: k + 1 for k, v in enumerate(\n      sorted(set(validation_synsets + training_synsets)))}\n\n  # Create training data\n  tf.logging.info(\'Processing the training data.\')\n  training_records = _process_dataset(\n      training_files, training_synsets, labels,\n      os.path.join(FLAGS.local_scratch_dir, TRAINING_DIRECTORY),\n      TRAINING_DIRECTORY, TRAINING_SHARDS)\n\n  # Create validation data\n  tf.logging.info(\'Processing the validation data.\')\n  validation_records = _process_dataset(\n      validation_files, validation_synsets, labels,\n      os.path.join(FLAGS.local_scratch_dir, VALIDATION_DIRECTORY),\n      VALIDATION_DIRECTORY, VALIDATION_SHARDS)\n\n  return training_records, validation_records\n\n\ndef upload_to_gcs(training_records, validation_records):\n  """"""Upload TF-Record files to GCS, at provided path.""""""\n\n  # Find the GCS bucket_name and key_prefix for dataset files\n  path_parts = FLAGS.gcs_output_path[5:].split(\'/\', 1)\n  bucket_name = path_parts[0]\n  if len(path_parts) == 1:\n    key_prefix = \'\'\n  elif path_parts[1].endswith(\'/\'):\n    key_prefix = path_parts[1]\n  else:\n    key_prefix = path_parts[1] + \'/\'\n\n  client = storage.Client(project=FLAGS.project)\n  bucket = client.get_bucket(bucket_name)\n\n  def _upload_files(filenames):\n    """"""Upload a list of files into a specifc subdirectory.""""""\n    for i, filename in enumerate(sorted(filenames)):\n      blob = bucket.blob(key_prefix + os.path.basename(filename))\n      blob.upload_from_filename(filename)\n      if not i % 20:\n        tf.logging.info(\'Finished uploading file: %s\' % filename)\n\n  # Upload training dataset\n  tf.logging.info(\'Uploading the training data.\')\n  _upload_files(training_records)\n\n  # Upload validation dataset\n  tf.logging.info(\'Uploading the validation data.\')\n  _upload_files(validation_records)\n\n\ndef main(argv):  # pylint: disable=unused-argument\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  if FLAGS.gcs_upload and FLAGS.project is None:\n    raise ValueError(\'GCS Project must be provided.\')\n\n  if FLAGS.gcs_upload and FLAGS.gcs_output_path is None:\n    raise ValueError(\'GCS output path must be provided.\')\n  elif FLAGS.gcs_upload and not FLAGS.gcs_output_path.startswith(\'gs://\'):\n    raise ValueError(\'GCS output path must start with gs://\')\n\n  if FLAGS.local_scratch_dir is None:\n    raise ValueError(\'Scratch directory path must be provided.\')\n\n  # Download the dataset if it is not present locally\n  raw_data_dir = FLAGS.raw_data_dir\n  if raw_data_dir is None:\n    raw_data_dir = os.path.join(FLAGS.local_scratch_dir, \'raw_data\')\n    tf.logging.info(\'Downloading data to raw_data_dir: %s\' % raw_data_dir)\n    download_dataset(raw_data_dir)\n\n  # Convert the raw data into tf-records\n  training_records, validation_records = convert_to_tf_records(raw_data_dir)\n\n  # Upload to GCS\n  if FLAGS.gcs_upload:\n    upload_to_gcs(training_records, validation_records)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
tpu/tools/datasets/jpeg_to_tf_record.py,9,"b'# Copyright 2018 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nr""""""Beam pipeline to create TFRecord files from JPEG files stored on GCS.\n\nThese are the TFRecord format expected by  the resnet and amoebanet models.\nExample usage:\npython -m jpeg_to_tf_record.py \\\n       --train_csv gs://cloud-ml-data/img/flower_photos/train_set.csv \\\n       --validation_csv gs://cloud-ml-data/img/flower_photos/eval_set.csv \\\n       --labels_file /tmp/labels.txt \\\n       --project_id $PROJECT \\\n       --output_dir gs://${BUCKET}/tpu/imgclass/data\n\nThe format of the CSV files is:\n    URL-of-image,label\nAnd the format of the labels_file is simply a list of strings one-per-line.\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nimport datetime\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport apache_beam as beam\nimport tensorflow as tf\n\n\ndef _int64_feature(value):\n  """"""Wrapper for inserting int64 features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef _bytes_feature(value):\n  """"""Wrapper for inserting bytes features into Example proto.""""""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _convert_to_example(filename, image_buffer, label_int, label_str, height,\n                        width):\n  """"""Build an Example proto for an example.\n\n  Args:\n    filename: string, path to an image file, e.g., \'/path/to/example.JPG\'\n    image_buffer: string, JPEG encoding of RGB image\n    label_int: integer, identifier for ground truth (0-based)\n    label_str: string, identifier for ground truth, e.g., \'daisy\'\n    height: integer, image height in pixels\n    width: integer, image width in pixels\n  Returns:\n    Example proto\n  """"""\n  colorspace = \'RGB\'\n  channels = 3\n  image_format = \'JPEG\'\n\n  example = tf.train.Example(\n      features=tf.train.Features(\n          feature={\n              \'image/height\': _int64_feature(height),\n              \'image/width\': _int64_feature(width),\n              \'image/colorspace\': _bytes_feature(colorspace),\n              \'image/channels\': _int64_feature(channels),\n              \'image/class/label\': _int64_feature(label_int +\n                                                  1),  # model expects 1-based\n              \'image/class/synset\': _bytes_feature(label_str),\n              \'image/format\': _bytes_feature(image_format),\n              \'image/filename\': _bytes_feature(os.path.basename(filename)),\n              \'image/encoded\': _bytes_feature(image_buffer)\n          }))\n  return example\n\n\nclass ImageCoder(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Create a single Session to run all image coding calls.\n    self._sess = tf.Session()\n\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def decode_jpeg(self, image_data):\n    image = self._sess.run(\n        self._decode_jpeg, feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n  def __del__(self):\n    self._sess.close()\n\n\ndef _get_image_data(filename, coder):\n  """"""Process a single image file.\n\n  Args:\n    filename: string, path to an image file e.g., \'/path/to/example.JPG\'.\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n  Returns:\n    image_buffer: string, JPEG encoding of RGB image.\n    height: integer, image height in pixels.\n    width: integer, image width in pixels.\n  """"""\n  # Read the image file.\n  with tf.gfile.FastGFile(filename, \'r\') as ifp:\n    image_data = ifp.read()\n\n  # Decode the RGB JPEG.\n  image = coder.decode_jpeg(image_data)\n\n  # Check that image converted to RGB\n  assert len(image.shape) == 3\n  height = image.shape[0]\n  width = image.shape[1]\n  assert image.shape[2] == 3\n\n  return image_data, height, width\n\n\ndef convert_to_example(csvline, categories):\n  """"""Parse a line of CSV file and convert to TF Record.\n\n  Args:\n    csvline: line from input CSV file\n    categories: list of labels\n  Yields:\n    serialized TF example if the label is in categories\n  """"""\n  filename, label = csvline.encode(\'ascii\', \'ignore\').split(\',\')\n  if label in categories:\n    # ignore labels not in categories list\n    coder = ImageCoder()\n    image_buffer, height, width = _get_image_data(filename, coder)\n    del coder\n    example = _convert_to_example(filename, image_buffer,\n                                  categories.index(label), label, height, width)\n    yield example.SerializeToString()\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \'--train_csv\',\n      # pylint: disable=line-too-long\n      help=\n      \'Path to input.  Each line of input has two fields  image-file-name and label separated by a comma\',\n      required=True)\n  parser.add_argument(\n      \'--validation_csv\',\n      # pylint: disable=line-too-long\n      help=\n      \'Path to input.  Each line of input has two fields  image-file-name and label separated by a comma\',\n      required=True)\n  parser.add_argument(\n      \'--labels_file\',\n      help=\'Path to file containing list of labels, one per line\',\n      required=True)\n  parser.add_argument(\n      \'--project_id\',\n      help=\'ID (not name) of your project. Ignored by DirectRunner\',\n      required=True)\n  parser.add_argument(\n      \'--runner\',\n      help=\'If omitted, uses DataFlowRunner if output_dir starts with gs://\',\n      default=None)\n  parser.add_argument(\n      \'--output_dir\', help=\'Top-level directory for TF Records\', required=True)\n\n  args = parser.parse_args()\n  arguments = args.__dict__\n\n  JOBNAME = (\n      \'preprocess-images-\' + datetime.datetime.now().strftime(\'%y%m%d-%H%M%S\'))\n\n  PROJECT = arguments[\'project_id\']\n  OUTPUT_DIR = arguments[\'output_dir\']\n\n  # set RUNNER using command-line arg or based on output_dir path\n  on_cloud = OUTPUT_DIR.startswith(\'gs://\')\n  if arguments[\'runner\']:\n    RUNNER = arguments[\'runner\']\n  else:\n    RUNNER = \'DataflowRunner\' if on_cloud else \'DirectRunner\'\n\n  # clean-up output directory since Beam will name files 0000-of-0004 etc.\n  # and this could cause confusion if earlier run has 0000-of-0005, for eg\n  if on_cloud:\n    try:\n      subprocess.check_call(\'gsutil -m rm -r {}\'.format(OUTPUT_DIR).split())\n    except subprocess.CalledProcessError:\n      pass\n  else:\n    shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n    os.makedirs(OUTPUT_DIR)\n\n  # read list of labels\n  with tf.gfile.FastGFile(arguments[\'labels_file\'], \'r\') as f:\n    LABELS = [line.rstrip() for line in f]\n  print(\'Read in {} labels, from {} to {}\'.format(\n      len(LABELS), LABELS[0], LABELS[-1]))\n  if len(LABELS) < 2:\n    print(\'Require at least two labels\')\n    sys.exit(-1)\n\n  # set up Beam pipeline to convert images to TF Records\n  options = {\n      \'staging_location\': os.path.join(OUTPUT_DIR, \'tmp\', \'staging\'),\n      \'temp_location\': os.path.join(OUTPUT_DIR, \'tmp\'),\n      \'job_name\': JOBNAME,\n      \'project\': PROJECT,\n      \'teardown_policy\': \'TEARDOWN_ALWAYS\',\n      \'save_main_session\': True\n  }\n  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n\n  with beam.Pipeline(RUNNER, options=opts) as p:\n    # BEAM tasks\n    for step in [\'train\', \'validation\']:\n      _ = (\n          p\n          | \'{}_read_csv\'.format(step) >> beam.io.ReadFromText(\n              arguments[\'{}_csv\'.format(step)])\n          | \'{}_convert\'.format(step) >>\n          beam.FlatMap(lambda line: convert_to_example(line, LABELS))\n          | \'{}_write_tfr\'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n              os.path.join(OUTPUT_DIR, step)))\n'"
tpu/tools/datasets/tfrecords_to_bigtable.py,15,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Script to transfer a set of TFRecord files to Cloud Bigtable.\n\nGoogle Cloud Bigtable is a high performance storage system, and can be very\nuseful for serving data to high performance accelerators in a cost effective\nfashion.\n\nSample usage:\n\n```\npython tfrecords_to_bigtable.py --source_glob=gs://my_bucket/path/to/files/* \\\n   --bigtable_instance=my_bigtable_instance --bigtable=my_table_name         \\\n   [ --project=my_project_id_or_number ] [ --num_records=50000 ]  # Optional.\n```\n\nBy default, the script will write entries into sequential rows with row keys\nnumbered based on a sequential counting index. It\'s common to want to have\nmultiple datasets in a single large Bigtable instance (even in a single table),\nso you can use the --row_prefix flag to set a prefix. For example if the flag\n`--row_prefix=test_`, row keys would look as follows:\n - test_00000000\n - [...]\n - test_12345678\n\nIf you have more than 100000000 records in your dataset, be sure to increase the\nvalue of the `--num_records` flag appropriately.\n\n> Note: assigning sequentially increasing row keys is a known performance\n> anti-pattern. This script is not designed for high-speed data loading (it is\n> single-threaded after all!). For large datasets, please use high-scale data\n> processing frameworks such as Apache Beam / Cloud Dataflow / Cloud Dataproc,\n> etc.\n\nThis script by default writes into the column family `ds` (dataset). This can\nbe changed by using the `--column_family` flag. You can create the `ds` column\nfamily using the `cbt` tool as follows:\n\n```\ncbt -project=$MY_PROJECT -instance=$MY_INSTANCE createfamily $TABLE_NAME ds\n```\n\nYou can make a super simple test TFRecord dataset by doing the following in an\ninteractive python terminal:\n\n```\npython\n>>> import tensorflow as tf\n>>> from tensorflow.contrib.data.python.ops.writers import TFRecordWriter\n>>> ds = tf.data.Dataset.range(10)\n>>> ds = ds.map(lambda x: tf.as_string(x))\n>>> writer = TFRecordWriter(\'/tmp/testdata.tfrecord\')\n>>> op = writer.write(ds)\n>>> sess = tf.Session()\n>>> sess.run(op)\n```\n\n> Note: there are a few more options available to tune performance. To see all\n> flags, run `python tf_records_to_bigtable.py --help`.\n\nThis script is designed to be re-used both as-is as well as modified to suit\nyour data loading needs. If you want to load data from a data source other than\nTFRecord files, simply modify the `build_source_dataset` function.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nfrom six.moves.urllib.request import Request\nfrom six.moves.urllib.request import urlopen\nimport tensorflow as tf\n\n\nflags.DEFINE_string(\'source_glob\', None, \'The source TFRecord files to read \'\n                    \'from and push into Cloud Bigtable.\')\nflags.DEFINE_string(\'bigtable_instance\', None, \'The Cloud Bigtable instance.\')\nflags.DEFINE_string(\'bigtable\', None, \'The table within the instance to write \'\n                    \'to.\')\nflags.DEFINE_string(\'project\', None, \'The Project to use. (Optional if running \'\n                    \'on a Compute Engine VM, as it can be auto-determined from \'\n                    \'the metadata service.)\')\nflags.DEFINE_integer(\n    \'num_records\', None, \'The approximate dataset size (used for padding \'\n    \'the appropriate number of zeros when constructing row keys). It should \'\n    \'not be smaller than the actual number of records.\')\nflags.DEFINE_integer(\'num_parallel_reads\', None, \'The number of parallel reads \'\n                     \'from the source file system.\')\nflags.DEFINE_string(\'column_family\', \'ds\', \'The column family to write the \'\n                    \'data into.\')\nflags.DEFINE_string(\'column\', \'d\', \'The column name (qualifier) to write the \'\n                    \'data into.\')\nflags.DEFINE_string(\'row_prefix\', None, \'A prefix for each row key.\')\n\nFLAGS = flags.FLAGS\n\n\ndef request_gce_metadata(path):\n  req = Request(\'http://metadata/computeMetadata/v1/%s\' % path,\n                headers={\'Metadata-Flavor\': \'Google\'})\n  resp = urlopen(req, timeout=2)\n  return tf.compat.as_str(resp.read())\n\n\ndef project_from_metadata():\n  return request_gce_metadata(\'project/project-id\')\n\n\ndef print_sources():\n  all_files = tf.gfile.Glob(FLAGS.source_glob)\n  # TODO(saeta): consider stat\'ing all files to determine total dataset size.\n  print(\'Found %d files (from ""%s"" to ""%s"")\' % (len(all_files), all_files[0],\n                                                all_files[-1]))\n\n\ndef validate_source_flags():\n  if FLAGS.source_glob is None:\n    raise ValueError(\'--source_glob must be specified.\')\n\n\ndef build_source_dataset():\n  validate_source_flags()\n  print_sources()\n  files = tf.data.Dataset.list_files(FLAGS.source_glob)\n  dataset = tf.data.TFRecordDataset(files,\n                                    num_parallel_reads=FLAGS.num_parallel_reads)\n  return dataset\n\n\ndef pad_width(num_records):\n  return len(\'%d\' % (num_records - 1))\n\n\ndef build_row_key_dataset(num_records, row_prefix):\n  if num_records is not None:\n    ds = tf.data.Dataset.range(num_records)\n  else:\n    ds = tf.contrib.data.Counter()\n  if num_records is None:\n    width = 10\n  else:\n    width = pad_width(num_records)\n  ds = ds.map(lambda idx: tf.as_string(idx, width=width, fill=\'0\'))\n  if row_prefix is not None:\n    ds = ds.map(lambda idx: tf.string_join([row_prefix, idx]))\n  return ds\n\n\ndef make_bigtable_client_and_table():\n  project = FLAGS.project\n  if project is None:\n    print(\'--project was not set on the command line, attempting to infer it \'\n          \'from the metadata service...\')\n    project = project_from_metadata()\n  if project is None:\n    raise ValueError(\'Please set a project on the command line.\')\n  instance = FLAGS.bigtable_instance\n  if instance is None:\n    raise ValueError(\'Please set an instance on the command line.\')\n  table_name = FLAGS.bigtable\n  if table_name is None:\n    raise ValueError(\'Please set a table on the command line.\')\n  client = tf.contrib.cloud.BigtableClient(project, instance)\n  table = client.table(table_name)\n  return (client, table)\n\n\ndef write_to_bigtable_op(aggregate_dataset, bigtable):\n  return bigtable.write(aggregate_dataset,\n                        column_families=[FLAGS.column_family],\n                        columns=[FLAGS.column])\n\n\ndef main(argv):\n  if len(argv) > 1:\n    raise ValueError(\'Too many command-line arguments.\')\n  source_dataset = build_source_dataset()\n  row_key_dataset = build_row_key_dataset(FLAGS.num_records, FLAGS.row_prefix)\n  aggregate_dataset = tf.data.Dataset.zip((row_key_dataset, source_dataset))\n  _, table = make_bigtable_client_and_table()\n  write_op = write_to_bigtable_op(aggregate_dataset, table)\n\n  print(\'Dataset ops created; about to create the session.\')\n  sess = tf.Session()\n  print(\'Starting transfer...\')\n  sess.run(write_op)\n  print(\'Complete!\')\n\n\nif __name__ == \'__main__\':\n  tf.app.run(main)\n'"
tpu/tools/diagnostics/diagnostics.py,11,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Script for debugging Cloud TPU errors.""""""\n\nimport argparse\nfrom datetime import datetime\nimport logging\nimport socket\nimport subprocess\n\n# pylint: disable=g-import-not-at-top\ntry:\n  import tensorflow as tf\n  TF_VERSION = tf.__version__\nexcept ImportError:\n  logging.error(\'Failed to import tensorflow\')\n  TF_VERSION = None\n\ntry:\n  from tensorflow.contrib.tpu.python.tpu import tpu\nexcept ImportError:\n  logging.error(\'Failed to import TPU module, make sure you are \'\n                \'using version 1.3 or above\')\n\ntry:\n  # Try to import urllib.request module in Python 3.x\n  from urllib.request import Request\n  from urllib.request import urlopen\n  from urllib.error import URLError\nexcept ImportError:\n  # Running Python 2.x so import urllib2 instead\n  from urllib2 import Request\n  from urllib2 import urlopen\n  from urllib2 import URLError\n\n\n# pylint: enable=g-import-not-at-top\n# Constants\nMETADATA_URL = \'http://metadata/computeMetadata/v1/\'\nMETADATA_HEADERS = {\'Metadata-Flavor\': \'Google\'}\n\n\ndef _call_metadata(suffix):\n  """"""Return the response of the metadata service for the provided suffix.""""""\n  request = Request(METADATA_URL + suffix, headers=METADATA_HEADERS)\n  return urlopen(request).read().decode(\'utf-8\')\n\n\ndef call_instance_metadata(suffix):\n  """"""Return the response of the instance metadata service for the suffix.""""""\n  return _call_metadata(\'instance/\' + suffix)\n\n\ndef call_project_metadata(suffix):\n  """"""Return the response of the project metadata service for the suffix.""""""\n  return _call_metadata(\'project/\' + suffix)\n\n\nclass Diagnostics(object):\n  """"""Class containing information needed for creating the diagnostics report.""""""\n\n  def __init__(self, tpu_name, project_id):\n    self.current_time = datetime.utcnow().isoformat()\n    self.project_id = project_id\n\n    # GCE VM Information\n    self.gce_vm_id = None\n    self.gce_vm_name = None\n    self.gce_vm_ip = None\n    self.gce_vm_zone = None\n\n    # TPU Information\n    self.tpu_name = tpu_name\n    self.tpu_ip = None\n    self.tpu_version = None\n    self.tpu_zone = None\n\n    # Run Information\n    self.is_running_on_gce = None\n    self.tensorflow_version = TF_VERSION\n    self.connected_to_tpu = None\n\n    # TPU tests\n    self.cpu_hello_world = None\n    self.tpu_initialization = None\n    self.tpu_computation = None\n\n  def __str__(self):\n    return """"""\n      TPU DIAGNOSTICS REPORT:\n\n      Current Time     : {current_time}\n      Project Id       : {project_id}\n\n      GCE VM ID        : {gce_vm_id}\n      GCE VM Name      : {gce_vm_name}\n      GCE VM IP        : {gce_vm_ip}\n      GCE VM Zone      : {gce_vm_zone}\n\n      TPU Name         : {tpu_name}\n      TPU IP           : {tpu_ip}\n      TPU Version      : {tpu_version}\n      TPU Zone         : {tpu_zone}\n\n      Running on GCE   : {is_running_on_gce}\n      TF Version       : {tensorflow_version}\n      TPU Connected    : {connected_to_tpu}\n\n      CPU HelloWorld     : {cpu_hello_world}\n      TPU Initialization : {tpu_initialization}\n      TPU Computation    : {tpu_computation}\n    """""".format(**self.__dict__)\n\n  def _gather_vm_stats(self):\n    """"""Information about the host VM.""""""\n    try:\n      self.gce_vm_id = call_instance_metadata(\'id\')\n      self.gce_vm_zone = call_instance_metadata(\'zone\').split(\'/\')[-1]\n      self.gce_vm_name = call_instance_metadata(\'hostname\'),\n      self.gce_vm_ip = call_instance_metadata(\n          \'network-interfaces/0/access-configs/0/external-ip\')\n      self.is_running_on_gce = True\n      logging.info(\'Finished collecing information about the GCE VM\')\n    except URLError:\n      self.is_running_on_gce = False\n      logging.error(\n          \'Failed to get the instance info from the metadata service\')\n\n  def _gather_tpu_stats(self):\n    """"""Information about the TPU.""""""\n    output = subprocess.check_output(\n        [\'gcloud\', \'alpha\', \'compute\', \'tpus\', \'list\',\n         \'--zone=%s\' % self.gce_vm_zone, \'--project=%s\' % self.project_id])\n\n    tpu_found = False\n    for row in output.decode(\'utf-8\').split(\'\\n\'):\n      if row and self.tpu_name == row.split()[0]:\n        tpu_instance_metadata = row.split()\n        tpu_found = True\n\n    if not tpu_found:\n      logging.error(\n          \'TPU with name: %s does not seem to be running in project %s\',\n          self.tpu_name, self.project_id)\n      return self\n\n    self.tpu_ip = tpu_instance_metadata[4].split(\':\')[0]\n    self.tpu_version = tpu_instance_metadata[3]\n    self.tpu_zone = tpu_instance_metadata[1]\n    logging.info(\'Finished collecing information about the TPU\')\n\n  def _check_network_with_tpu(self):\n    """"""Check if can open a connection to the cloud TPU.""""""\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n      s.connect((self.tpu_ip, 8470))\n      self.connected_to_tpu = True\n      logging.info(\'Successfully connected to TPU Instance\')\n    except Exception:  # pylint: disable=broad-except\n      self.connected_to_tpu = False\n      logging.error(\'Failed to connect to TPU Instance\')\n    finally:\n      s.shutdown(2)\n\n  def _run_cpu_hello_world(self):\n    """"""Try running CPU based tensorflow.""""""\n    hello = tf.constant(\'Hello, TensorFlow!\')\n    with tf.Session() as sess:\n      logging.info(sess.run(hello))\n    self.cpu_hello_world = \'Passed\'\n    logging.info(\'Successfully ran the HelloWorld program on the VM\')\n\n  def _run_tpu_initialization(self):\n    """"""Test TPU system initialization.""""""\n    with tf.Session(\'grpc://{0}:8470\'.format(self.tpu_ip)) as sess:\n      sess.run(tpu.initialize_system())\n      sess.run(tpu.shutdown_system())\n      logging.info(\'Successfully initialized and shutdown the tpu\')\n    self.tpu_initialization = \'Passed\'\n\n  def _run_tpu_computation(self):\n    """"""Attempt to run computation graph directly on TPU.""""""\n    def _computation_fn(alpha, x, y):\n      return alpha * x + y\n\n    alpha = tf.Variable(3.0, name=\'alpha\')\n    x = tf.Variable(tf.ones([3, 3], tf.float32), name=\'x\')\n    y = tf.Variable(tf.ones([3, 3], tf.float32), name=\'y\')\n\n    result = tf.contrib.tpu.rewrite(_computation_fn, [alpha, x, y])\n\n    with tf.Session(\'grpc://{0}:8470\'.format(self.tpu_ip)) as sess:\n      sess.run(tf.contrib.tpu.initialize_system())\n      sess.run(tf.global_variables_initializer())\n      logging.info(sess.run(result))\n      sess.run(tpu.shutdown_system())\n      logging.info(\'Output should be a 3x3 matrix with all 4s.\')\n    self.tpu_computation = \'Passed\'\n    logging.info(\'Successfully ran a computation on the TPU\')\n\n  def diagnose(self):\n    """"""Run all applicable diagnostic test.""""""\n\n    try:\n      # Get basic information about the enviornment\n      self._gather_vm_stats()\n      self._gather_tpu_stats()\n      self._check_network_with_tpu()\n\n      if not self.connected_to_tpu or self.tensorflow_version is None:\n        # We shouldn\'t do more tests if we can\'t reach the TPU\n        return self\n\n      # Test running basic jobs on the TPU\n      self._run_cpu_hello_world()\n      self._run_tpu_initialization()\n      self._run_tpu_computation()\n    except Exception:  # pylint: disable=broad-except\n      logging.exception(\'Saw an unexpected error in running diagnostics\')\n\n\ndef main(argv=None):\n  """"""Main Script for TPU diagnostics.""""""\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--project_id\', default=None,\n                      help=\'ProjectId of the current job\')\n\n  parser.add_argument(\'--tpu_name\', required=True,\n                      help=\'Name of the TPU being diagnosed\')\n\n  args, _ = parser.parse_known_args(argv)\n\n  if args.project_id is None:\n    try:\n      project_id = call_project_metadata(\'project-id\')\n    except URLError:\n      raise RuntimeError(\'Please provide the project_id input\')\n  else:\n    project_id = args.project_id\n\n  report = Diagnostics(args.tpu_name, project_id)\n  report.diagnose()\n  logging.info(report)\n\n\nif __name__ == \'__main__\':\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
tpu/models/experimental/cifar_keras/cifar_keras.py,26,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Cifar example using Keras for model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport absl.logging as _logging  # pylint: disable=unused-import\nimport tensorflow as tf\n\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'tpu\', default=None,\n    help=\'The Cloud TPU to use for training. This should be either the name \'\n    \'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\')\nflags.DEFINE_string(\n    ""gcp_project"", default=None,\n    help=""Project name for the Cloud TPU-enabled project. If not specified, we ""\n    ""will attempt to automatically detect the GCE project from metadata."")\nflags.DEFINE_string(\n    ""tpu_zone"", default=None,\n    help=""GCE zone where the Cloud TPU is located in. If not specified, we ""\n    ""will attempt to automatically detect the GCE project from metadata."")\n\n# Model specific paramenters\nflags.DEFINE_integer(""batch_size"", 128,\n                     ""Mini-batch size for the computation. Note that this ""\n                     ""is the global batch size and not the per-shard batch."")\nflags.DEFINE_float(""learning_rate"", 0.05, ""Learning rate."")\nflags.DEFINE_string(""train_file"", """", ""Path to cifar10 training data."")\nflags.DEFINE_integer(""train_steps"", 100000,\n                     ""Total number of steps. Note that the actual number of ""\n                     ""steps is the next multiple of --iterations greater ""\n                     ""than this value."")\nflags.DEFINE_bool(""use_tpu"", True, ""Use TPUs rather than plain CPUs"")\nflags.DEFINE_string(""model_dir"", None, ""Estimator model_dir"")\nflags.DEFINE_integer(""iterations_per_loop"", 100,\n                     ""Number of iterations per TPU training loop."")\nflags.DEFINE_integer(""num_shards"", 8, ""Number of shards (TPU chips)."")\n\n\nFLAGS = flags.FLAGS\n\n\ndef model_fn(features, labels, mode, params):\n  """"""Define a CIFAR model in Keras.""""""\n  del params  # unused\n  layers = tf.contrib.keras.layers\n\n  # Pass our input tensor to initialize the Keras input layer.\n  v = layers.Input(tensor=features)\n  v = layers.Conv2D(filters=32, kernel_size=5,\n                    activation=""relu"", padding=""same"")(v)\n  v = layers.MaxPool2D(pool_size=2)(v)\n  v = layers.Conv2D(filters=64, kernel_size=5,\n                    activation=""relu"", padding=""same"")(v)\n  v = layers.MaxPool2D(pool_size=2)(v)\n  v = layers.Flatten()(v)\n  fc1 = layers.Dense(units=512, activation=""relu"")(v)\n  logits = layers.Dense(units=10)(fc1)\n\n  # Instead of constructing a Keras model for training, build our loss function\n  # and optimizer in Tensorflow.\n  #\n  # N.B.  This construction omits some features that are important for more\n  # complex models (e.g. regularization, batch-norm).  Once\n  # `model_to_estimator` support is added for TPUs, it should be used instead.\n  loss = tf.reduce_mean(\n      tf.nn.sparse_softmax_cross_entropy_with_logits(\n          logits=logits, labels=labels\n      )\n  )\n  optimizer = tf.train.AdamOptimizer()\n  if FLAGS.use_tpu:\n    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n  train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n\n  return tf.contrib.tpu.TPUEstimatorSpec(\n      mode=mode,\n      loss=loss,\n      train_op=train_op,\n      predictions={\n          ""classes"": tf.argmax(input=logits, axis=1),\n          ""probabilities"": tf.nn.softmax(logits, name=""softmax_tensor"")\n      }\n  )\n\n\ndef input_fn(params):\n  """"""Read CIFAR input data from a TFRecord dataset.""""""\n  del params\n  batch_size = FLAGS.batch_size\n  def parser(serialized_example):\n    """"""Parses a single tf.Example into image and label tensors.""""""\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            ""image"": tf.FixedLenFeature([], tf.string),\n            ""label"": tf.FixedLenFeature([], tf.int64),\n        })\n    image = tf.decode_raw(features[""image""], tf.uint8)\n    image.set_shape([3*32*32])\n    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\n    image = tf.transpose(tf.reshape(image, [3, 32, 32]))\n    label = tf.cast(features[""label""], tf.int32)\n    return image, label\n\n  dataset = tf.data.TFRecordDataset([FLAGS.train_file])\n  dataset = dataset.map(parser, num_parallel_calls=batch_size)\n  dataset = dataset.prefetch(4 * batch_size).cache().repeat()\n  dataset = dataset.apply(\n      tf.contrib.data.batch_and_drop_remainder(FLAGS.batch_size)\n  )\n  dataset = dataset.prefetch(1)\n  return dataset\n\n\ndef main(argv):\n  del argv  # Unused.\n\n  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n      FLAGS.tpu,\n      zone=FLAGS.tpu_zone,\n      project=FLAGS.gcp_project)\n\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_secs=3600,\n      session_config=tf.ConfigProto(\n          allow_soft_placement=True, log_device_placement=True),\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=FLAGS.iterations_per_loop,\n          num_shards=FLAGS.num_shards),\n  )\n\n  estimator = tf.contrib.tpu.TPUEstimator(\n      model_fn=model_fn,\n      use_tpu=FLAGS.use_tpu,\n      config=run_config,\n      train_batch_size=FLAGS.batch_size)\n  estimator.train(input_fn=input_fn, max_steps=FLAGS.train_steps)\n\n\nif __name__ == ""__main__"":\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
tpu/models/experimental/dcgan/cifar_input.py,12,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""CIFAR example using input pipelines.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'cifar_train_data_file\', \'\',\n                    \'Path to CIFAR10 training data.\')\nflags.DEFINE_string(\'cifar_test_data_file\', \'\', \'Path to CIFAR10 test data.\')\n\n\ndef parser(serialized_example):\n  """"""Parses a single tf.Example into image and label tensors.""""""\n  features = tf.parse_single_example(\n      serialized_example,\n      features={\n          \'image\': tf.FixedLenFeature([], tf.string),\n          \'label\': tf.FixedLenFeature([], tf.int64),\n      })\n  image = tf.decode_raw(features[\'image\'], tf.uint8)\n  image.set_shape([3*32*32])\n  # Normalize the values of the image from the range [0, 255] to [-1.0, 1.0]\n  image = tf.cast(image, tf.float32) * (2.0 / 255) - 1.0\n  image = tf.transpose(tf.reshape(image, [3, 32*32]))\n  label = tf.cast(features[\'label\'], tf.int32)\n  return image, label\n\n\nclass InputFunction(object):\n  """"""Wrapper class that is passed as callable to Estimator.""""""\n\n  def __init__(self, is_training, noise_dim):\n    self.is_training = is_training\n    self.noise_dim = noise_dim\n    self.data_file = (FLAGS.cifar_train_data_file if is_training\n                      else FLAGS.cifar_test_data_file)\n\n  def __call__(self, params):\n    batch_size = params[\'batch_size\']\n    dataset = tf.data.TFRecordDataset([self.data_file])\n    dataset = dataset.map(parser, num_parallel_calls=batch_size)\n    dataset = dataset.prefetch(4 * batch_size).cache().repeat()\n    dataset = dataset.apply(\n        tf.contrib.data.batch_and_drop_remainder(batch_size))\n    dataset = dataset.prefetch(2)\n    images, labels = dataset.make_one_shot_iterator().get_next()\n\n    # Reshape to give inputs statically known shapes.\n    images = tf.reshape(images, [batch_size, 32, 32, 3])\n\n    random_noise = tf.random_normal([batch_size, self.noise_dim])\n\n    features = {\n        \'real_images\': images,\n        \'random_noise\': random_noise}\n\n    return features, labels\n\n\ndef convert_array_to_image(array):\n  """"""Converts a numpy array to a PIL Image and undoes any rescaling.""""""\n  img = Image.fromarray(np.uint8((array + 1.0) / 2.0 * 255), mode=\'RGB\')\n  return img\n'"
tpu/models/experimental/dcgan/cifar_model.py,16,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Simple generator and discriminator models.\n\nBased on the convolutional and ""deconvolutional"" models presented in\n""Unsupervised Representation Learning with Deep Convolutional Generative\nAdversarial Networks"" by A. Radford et. al.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef _leaky_relu(x):\n  return tf.nn.leaky_relu(x, alpha=0.2)\n\n\ndef _batch_norm(x, is_training, name):\n  return tf.layers.batch_normalization(\n      x, momentum=0.9, epsilon=1e-5, training=is_training, name=name)\n\n\ndef _dense(x, channels, name):\n  return tf.layers.dense(\n      x, channels,\n      kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n      name=name)\n\n\ndef _conv2d(x, filters, kernel_size, stride, name):\n  return tf.layers.conv2d(\n      x, filters, [kernel_size, kernel_size],\n      strides=[stride, stride], padding=\'same\',\n      kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n      name=name)\n\n\ndef _deconv2d(x, filters, kernel_size, stride, name):\n  return tf.layers.conv2d_transpose(\n      x, filters, [kernel_size, kernel_size],\n      strides=[stride, stride], padding=\'same\',\n      kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n      name=name)\n\n\ndef discriminator(x, is_training=True, scope=\'Discriminator\'):\n  with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n    x = _conv2d(x, 64, 5, 2, name=\'d_conv1\')\n    x = _leaky_relu(x)\n\n    x = _conv2d(x, 128, 5, 2, name=\'d_conv2\')\n    x = _leaky_relu(_batch_norm(x, is_training, name=\'d_bn2\'))\n\n    x = _conv2d(x, 256, 5, 2, name=\'d_conv3\')\n    x = _leaky_relu(_batch_norm(x, is_training, name=\'d_bn3\'))\n\n    x = tf.reshape(x, [-1, 4 * 4 * 256])\n\n    x = _dense(x, 1, name=\'d_fc_4\')\n\n    return x\n\n\ndef generator(x, is_training=True, scope=\'Generator\'):\n  with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n    x = _dense(x, 4096, name=\'g_fc1\')\n    x = tf.nn.relu(_batch_norm(x, is_training, name=\'g_bn1\'))\n\n    x = tf.reshape(x, [-1, 4, 4, 256])\n\n    x = _deconv2d(x, 128, 5, 2, name=\'g_dconv2\')\n    x = tf.nn.relu(_batch_norm(x, is_training, name=\'g_bn2\'))\n\n    x = _deconv2d(x, 64, 4, 2, name=\'g_dconv3\')\n    x = tf.nn.relu(_batch_norm(x, is_training, name=\'g_bn3\'))\n\n    x = _deconv2d(x, 3, 4, 2, name=\'g_dconv4\')\n    x = tf.tanh(x)\n\n    return x\n\n'"
tpu/models/experimental/dcgan/dcgan_main.py,46,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Runs a DCGAN model on MNIST dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\n# Standard Imports\nfrom absl import flags\nimport absl.logging as _logging  # pylint: disable=unused-import\nimport numpy as np\nimport tensorflow as tf\n\nimport cifar_input\nimport cifar_model\nimport mnist_input\nimport mnist_model\nfrom tensorflow.python.estimator import estimator\n\nFLAGS = flags.FLAGS\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'tpu\', default=None,\n    help=\'The Cloud TPU to use for training. This should be either the name \'\n    \'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\')\nflags.DEFINE_string(\n    \'gcp_project\', default=None,\n    help=\'Project name for the Cloud TPU-enabled project. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\nflags.DEFINE_string(\n    \'tpu_zone\', default=None,\n    help=\'GCE zone where the Cloud TPU is located in. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\n\n# Model specific paramenters\nflags.DEFINE_string(\'dataset\', \'mnist\',\n                    \'One of [""mnist"", ""cifar""]. Requires additional flags\')\nflags.DEFINE_string(\'model_dir\', \'\', \'Output model directory\')\nflags.DEFINE_integer(\'noise_dim\', 64,\n                     \'Number of dimensions for the noise vector\')\nflags.DEFINE_integer(\'batch_size\', 1024,\n                     \'Batch size for both generator and discriminator\')\nflags.DEFINE_integer(\'num_shards\', None, \'Number of TPU chips\')\nflags.DEFINE_integer(\'train_steps\', 10000, \'Number of training steps\')\nflags.DEFINE_integer(\'train_steps_per_eval\', 1000,\n                     \'Steps per eval and image generation\')\nflags.DEFINE_integer(\'iterations_per_loop\', 100,\n                     \'Steps per interior TPU loop. Should be less than\'\n                     \' --train_steps_per_eval\')\nflags.DEFINE_float(\'learning_rate\', 0.0002, \'LR for both D and G\')\nflags.DEFINE_boolean(\'eval_loss\', False,\n                     \'Evaluate discriminator and generator loss during eval\')\nflags.DEFINE_boolean(\'use_tpu\', True, \'Use TPU for training\')\n\n_NUM_VIZ_IMAGES = 100   # For generating a 10x10 grid of generator samples\n\n# Global variables for data and model\ndataset = None\nmodel = None\n\n\ndef model_fn(features, labels, mode, params):\n  """"""Constructs DCGAN from individual generator and discriminator networks.""""""\n  del labels    # Unconditional GAN does not use labels\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    ###########\n    # PREDICT #\n    ###########\n    # Pass only noise to PREDICT mode\n    random_noise = features[\'random_noise\']\n    predictions = {\n        \'generated_images\': model.generator(random_noise, is_training=False)\n    }\n\n    return tf.contrib.tpu.TPUEstimatorSpec(mode=mode, predictions=predictions)\n\n  # Use params[\'batch_size\'] for the batch size inside model_fn\n  batch_size = params[\'batch_size\']   # pylint: disable=unused-variable\n  real_images = features[\'real_images\']\n  random_noise = features[\'random_noise\']\n\n  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n  generated_images = model.generator(random_noise,\n                                     is_training=is_training)\n\n  # Get logits from discriminator\n  d_on_data_logits = tf.squeeze(model.discriminator(real_images))\n  d_on_g_logits = tf.squeeze(model.discriminator(generated_images))\n\n  # Calculate discriminator loss\n  d_loss_on_data = tf.nn.sigmoid_cross_entropy_with_logits(\n      labels=tf.ones_like(d_on_data_logits),\n      logits=d_on_data_logits)\n  d_loss_on_gen = tf.nn.sigmoid_cross_entropy_with_logits(\n      labels=tf.zeros_like(d_on_g_logits),\n      logits=d_on_g_logits)\n\n  d_loss = d_loss_on_data + d_loss_on_gen\n\n  # Calculate generator loss\n  g_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n      labels=tf.ones_like(d_on_g_logits),\n      logits=d_on_g_logits)\n\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    #########\n    # TRAIN #\n    #########\n    d_loss = tf.reduce_mean(d_loss)\n    g_loss = tf.reduce_mean(g_loss)\n    d_optimizer = tf.train.AdamOptimizer(\n        learning_rate=FLAGS.learning_rate, beta1=0.5)\n    g_optimizer = tf.train.AdamOptimizer(\n        learning_rate=FLAGS.learning_rate, beta1=0.5)\n\n    if FLAGS.use_tpu:\n      d_optimizer = tf.contrib.tpu.CrossShardOptimizer(d_optimizer)\n      g_optimizer = tf.contrib.tpu.CrossShardOptimizer(g_optimizer)\n\n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n      d_step = d_optimizer.minimize(\n          d_loss,\n          var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n                                     scope=\'Discriminator\'))\n      g_step = g_optimizer.minimize(\n          g_loss,\n          var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n                                     scope=\'Generator\'))\n\n      increment_step = tf.assign_add(tf.train.get_or_create_global_step(), 1)\n      joint_op = tf.group([d_step, g_step, increment_step])\n\n      return tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=g_loss,\n          train_op=joint_op)\n\n  elif mode == tf.estimator.ModeKeys.EVAL:\n    ########\n    # EVAL #\n    ########\n    def _eval_metric_fn(d_loss, g_loss):\n      # When using TPUs, this function is run on a different machine than the\n      # rest of the model_fn and should not capture any Tensors defined there\n      return {\n          \'discriminator_loss\': tf.metrics.mean(d_loss),\n          \'generator_loss\': tf.metrics.mean(g_loss)}\n\n    return tf.contrib.tpu.TPUEstimatorSpec(\n        mode=mode,\n        loss=tf.reduce_mean(g_loss),\n        eval_metrics=(_eval_metric_fn, [d_loss, g_loss]))\n\n  # Should never reach here\n  raise ValueError(\'Invalid mode provided to model_fn\')\n\n\ndef generate_input_fn(is_training):\n  """"""Creates input_fn depending on whether the code is training or not.""""""\n  return dataset.InputFunction(is_training, FLAGS.noise_dim)\n\n\ndef noise_input_fn(params):\n  """"""Input function for generating samples for PREDICT mode.\n\n  Generates a single Tensor of fixed random noise. Use tf.data.Dataset to\n  signal to the estimator when to terminate the generator returned by\n  predict().\n\n  Args:\n    params: param `dict` passed by TPUEstimator.\n\n  Returns:\n    1-element `dict` containing the randomly generated noise.\n  """"""\n  np.random.seed(0)\n  noise_dataset = tf.data.Dataset.from_tensors(tf.constant(\n      np.random.randn(params[\'batch_size\'], FLAGS.noise_dim), dtype=tf.float32))\n  noise = noise_dataset.make_one_shot_iterator().get_next()\n  return {\'random_noise\': noise}, None\n\n\ndef main(argv):\n  del argv\n  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n      FLAGS.tpu,\n      zone=FLAGS.tpu_zone,\n      project=FLAGS.gcp_project)\n\n  config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      model_dir=FLAGS.model_dir,\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          num_shards=FLAGS.num_shards,\n          iterations_per_loop=FLAGS.iterations_per_loop))\n\n  # Set module-level global variable so that model_fn and input_fn can be\n  # identical for each different kind of dataset and model\n  global dataset, model\n  if FLAGS.dataset == \'mnist\':\n    dataset = mnist_input\n    model = mnist_model\n  elif FLAGS.dataset == \'cifar\':\n    dataset = cifar_input\n    model = cifar_model\n  else:\n    raise ValueError(\'Invalid dataset: %s\' % FLAGS.dataset)\n\n  # TPU-based estimator used for TRAIN and EVAL\n  est = tf.contrib.tpu.TPUEstimator(\n      model_fn=model_fn,\n      use_tpu=FLAGS.use_tpu,\n      config=config,\n      train_batch_size=FLAGS.batch_size,\n      eval_batch_size=FLAGS.batch_size)\n\n  # CPU-based estimator used for PREDICT (generating images)\n  cpu_est = tf.contrib.tpu.TPUEstimator(\n      model_fn=model_fn,\n      use_tpu=False,\n      config=config,\n      predict_batch_size=_NUM_VIZ_IMAGES)\n\n  tf.gfile.MakeDirs(os.path.join(FLAGS.model_dir, \'generated_images\'))\n\n  current_step = estimator._load_global_step_from_checkpoint_dir(FLAGS.model_dir)   # pylint: disable=protected-access,line-too-long\n  tf.logging.info(\'Starting training for %d steps, current step: %d\' %\n                  (FLAGS.train_steps, current_step))\n  while current_step < FLAGS.train_steps:\n    next_checkpoint = min(current_step + FLAGS.train_steps_per_eval,\n                          FLAGS.train_steps)\n    est.train(input_fn=generate_input_fn(True),\n              max_steps=next_checkpoint)\n    current_step = next_checkpoint\n    tf.logging.info(\'Finished training step %d\' % current_step)\n\n    if FLAGS.eval_loss:\n      # Evaluate loss on test set\n      metrics = est.evaluate(input_fn=generate_input_fn(False),\n                             steps=dataset.NUM_EVAL_IMAGES // FLAGS.batch_size)\n      tf.logging.info(\'Finished evaluating\')\n      tf.logging.info(metrics)\n\n    # Render some generated images\n    generated_iter = cpu_est.predict(input_fn=noise_input_fn)\n    images = [p[\'generated_images\'][:, :, :] for p in generated_iter]\n    assert len(images) == _NUM_VIZ_IMAGES\n    image_rows = [np.concatenate(images[i:i+10], axis=0)\n                  for i in range(0, _NUM_VIZ_IMAGES, 10)]\n    tiled_image = np.concatenate(image_rows, axis=1)\n\n    img = dataset.convert_array_to_image(tiled_image)\n\n    step_string = str(current_step).zfill(5)\n    file_obj = tf.gfile.Open(\n        os.path.join(FLAGS.model_dir,\n                     \'generated_images\', \'gen_%s.png\' % (step_string)), \'w\')\n    img.save(file_obj, format=\'png\')\n    tf.logging.info(\'Finished generating images\')\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
tpu/models/experimental/dcgan/mnist_input.py,11,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Read MNIST data as TFRecords and create a tf.data.Dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'mnist_train_data_file\', \'\', \'Training .tfrecord data file\')\nflags.DEFINE_string(\'mnist_test_data_file\', \'\', \'Test .tfrecord data file\')\n\nNUM_TRAIN_IMAGES = 60000\nNUM_EVAL_IMAGES = 10000\n\n\ndef parser(serialized_example):\n  """"""Parses a single Example into image and label tensors.""""""\n  features = tf.parse_single_example(\n      serialized_example,\n      features={\n          \'image_raw\': tf.FixedLenFeature([], tf.string),\n          \'label\': tf.FixedLenFeature([], tf.int64)   # label is unused\n      })\n  image = tf.decode_raw(features[\'image_raw\'], tf.uint8)\n  image.set_shape([28 * 28])\n  image = tf.reshape(image, [28, 28, 1])\n\n  # Normalize the values of the image from [0, 255] to [-1.0, 1.0]\n  image = tf.cast(image, tf.float32) * (2.0 / 255) - 1.0\n\n  label = tf.cast(tf.reshape(features[\'label\'], shape=[]), dtype=tf.int32)\n  return image, label\n\n\nclass InputFunction(object):\n  """"""Wrapper class that is passed as callable to Estimator.""""""\n\n  def __init__(self, is_training, noise_dim):\n    self.is_training = is_training\n    self.noise_dim = noise_dim\n    self.data_file = (FLAGS.mnist_train_data_file if is_training\n                      else FLAGS.mnist_test_data_file)\n\n  def __call__(self, params):\n    """"""Creates a simple Dataset pipeline.""""""\n\n    batch_size = params[\'batch_size\']\n    dataset = tf.data.TFRecordDataset(self.data_file)\n    dataset = dataset.map(parser).cache()\n    if self.is_training:\n      dataset = dataset.repeat()\n    dataset = dataset.shuffle(1024)\n    dataset = dataset.prefetch(batch_size)\n    dataset = dataset.apply(\n        tf.contrib.data.batch_and_drop_remainder(batch_size))\n    dataset = dataset.prefetch(2)    # Prefetch overlaps in-feed with training\n    images, labels = dataset.make_one_shot_iterator().get_next()\n\n    random_noise = tf.random_normal([batch_size, self.noise_dim])\n\n    features = {\n        \'real_images\': images,\n        \'random_noise\': random_noise}\n\n    return features, labels\n\n\ndef convert_array_to_image(array):\n  """"""Converts a numpy array to a PIL Image and undoes any rescaling.""""""\n  array = array[:, :, 0]\n  img = Image.fromarray(np.uint8((array + 1.0) / 2.0 * 255), mode=\'L\')\n  return img\n'"
tpu/models/experimental/dcgan/mnist_model.py,16,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Simple generator and discriminator models.\n\nBased on the convolutional and ""deconvolutional"" models presented in\n""Unsupervised Representation Learning with Deep Convolutional Generative\nAdversarial Networks"" by A. Radford et. al.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef _leaky_relu(x):\n  return tf.nn.leaky_relu(x, alpha=0.2)\n\n\ndef _batch_norm(x, is_training, name):\n  return tf.layers.batch_normalization(\n      x, momentum=0.9, epsilon=1e-5, training=is_training, name=name)\n\n\ndef _dense(x, channels, name):\n  return tf.layers.dense(\n      x, channels,\n      kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n      name=name)\n\n\ndef _conv2d(x, filters, kernel_size, stride, name):\n  return tf.layers.conv2d(\n      x, filters, [kernel_size, kernel_size],\n      strides=[stride, stride], padding=\'same\',\n      kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n      name=name)\n\n\ndef _deconv2d(x, filters, kernel_size, stride, name):\n  return tf.layers.conv2d_transpose(\n      x, filters, [kernel_size, kernel_size],\n      strides=[stride, stride], padding=\'same\',\n      kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n      name=name)\n\n\ndef discriminator(x, is_training=True, scope=\'Discriminator\'):\n  # conv64-lrelu + conv128-bn-lrelu + fc1024-bn-lrelu + fc1\n  with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n    x = _conv2d(x, 64, 4, 2, name=\'d_conv1\')\n    x = _leaky_relu(x)\n\n    x = _conv2d(x, 128, 4, 2, name=\'d_conv2\')\n    x = _leaky_relu(_batch_norm(x, is_training, name=\'d_bn2\'))\n\n    x = tf.reshape(x, [-1, 7 * 7 * 128])\n\n    x = _dense(x, 1024, name=\'d_fc3\')\n    x = _leaky_relu(_batch_norm(x, is_training, name=\'d_bn3\'))\n\n    x = _dense(x, 1, name=\'d_fc4\')\n\n    return x\n\n\ndef generator(x, is_training=True, scope=\'Generator\'):\n  # fc1024-bn-relu + fc6272-bn-relu + deconv64-bn-relu + deconv1-tanh\n  with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n    x = _dense(x, 1024, name=\'g_fc1\')\n    x = tf.nn.relu(_batch_norm(x, is_training, name=\'g_bn1\'))\n\n    x = _dense(x, 7 * 7 * 128, name=\'g_fc2\')\n    x = tf.nn.relu(_batch_norm(x, is_training, name=\'g_bn2\'))\n\n    x = tf.reshape(x, [-1, 7, 7, 128])\n\n    x = _deconv2d(x, 64, 4, 2, name=\'g_dconv3\')\n    x = tf.nn.relu(_batch_norm(x, is_training, name=\'g_bn3\'))\n\n    x = _deconv2d(x, 1, 4, 2, name=\'g_dconv4\')\n    x = tf.tanh(x)\n\n    return x\n\n# TODO(chrisying): objective score (e.g. MNIST score)\n\n'"
tpu/models/experimental/deeplab/data_pipeline.py,7,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Prepare the data used for Deeplab training/evaluation.\n\nBased on third_party/tensorflow_models/deeplab/utils/input_generator.py.\n""""""\n\nimport tensorflow as tf\nfrom deeplab import input_preprocess\n\nslim = tf.contrib.slim\n\ndataset_data_provider = slim.dataset_data_provider\n\n\ndef get_data(data_provider, dataset_split):\n  """"""Gets data from data provider.\n\n  Args:\n    data_provider: An object of slim.data_provider.\n    dataset_split: Dataset split.\n\n  Returns:\n    image: Image Tensor.\n    label: Label Tensor storing segmentation annotations.\n    image_name: Image name.\n    height: Image height.\n    width: Image width.\n\n  Raises:\n    ValueError: Failed to find label.\n  """"""\n  if \'labels_class\' not in data_provider.list_items():\n    raise ValueError(\'Failed to find labels.\')\n\n  image, = data_provider.get([\'image\'])\n\n  # Some datasets do not contain image_name.\n  if \'image_name\' in data_provider.list_items():\n    image_name, = data_provider.get([\'image_name\'])\n  else:\n    image_name = tf.constant(\'\')\n\n  # Some datasets do not contain image_height and image_width. We just infer the\n  # shape from image.\n  height = tf.shape(image)[0]\n  width = tf.shape(image)[1]\n\n  label = None\n  if dataset_split != \'test\':\n    label, = data_provider.get([\'labels_class\'])\n\n  return image, label, image_name, height, width\n\n\nclass InputReader(object):\n  """"""Prepares data for TPUEstimator.""""""\n\n  def __init__(self,\n               dataset,\n               split_name,\n               is_training,\n               model_variant):\n    """"""Initializes slim Dataset etc.\n\n    Args:\n      dataset: slim Dataset.\n      split_name: String, the name of train/eval/test split.\n      is_training: Boolean, whether the data is used for training.\n      model_variant: String, model variant for choosing how to mean-subtract the\n        images.\n    """"""\n    self._dataset = dataset\n    self._split_name = split_name\n    self._is_training = is_training\n    self._model_variant = model_variant\n\n    self._num_readers = 8\n    self._num_threads = 64\n\n  def __call__(self, params):\n    """"""Reads, preprocesses and batches data for TPUEstimator.""""""\n    data_provider = dataset_data_provider.DatasetDataProvider(\n        self._dataset,\n        num_readers=self._num_readers,\n        shuffle=self._is_training,\n        num_epochs=None if self._is_training else 1\n    )\n    image, label, image_name, height, width = get_data(\n        data_provider, self._split_name)\n\n    if label is not None:\n      if label.shape.ndims == 2:\n        label = tf.expand_dims(label, 2)\n      elif label.shape.ndims == 3 and label.shape.dims[2] == 1:\n        pass\n      else:\n        raise ValueError(\'Input label shape must be [height, width], or \'\n                         \'[height, width, 1].\')\n    label.set_shape([None, None, 1])\n\n    crop_height, crop_width = params[\'crop_size\']\n    original_image, image, label = input_preprocess.preprocess_image_and_label(\n        image,\n        label,\n        crop_height=crop_height,\n        crop_width=crop_width,\n        min_resize_value=params[\'min_resize_value\'],\n        max_resize_value=params[\'max_resize_value\'],\n        resize_factor=params[\'resize_factor\'],\n        min_scale_factor=params[\'min_scale_factor\'],\n        max_scale_factor=params[\'max_scale_factor\'],\n        scale_factor_step_size=params[\'scale_factor_step_size\'],\n        ignore_label=self._dataset.ignore_label,\n        is_training=self._is_training,\n        model_variant=self._model_variant)\n\n    sample = {\n        \'image\': image,\n        \'image_name\': image_name,\n        \'height\': height,\n        \'width\': width\n    }\n    if label is not None:\n      sample[\'label\'] = label\n\n    num_threads = self._num_threads\n    if not self._is_training:\n      # Original image is only used during visualization.\n      sample[\'original_image\'] = original_image,\n      num_threads = 1\n\n    # TODO(shizhiw): switch to tf.data.\n    batch = tf.train.batch(\n        sample,\n        batch_size=params[\'batch_size\'],\n        num_threads=num_threads,\n        capacity=32 * params[\'batch_size\'],\n        allow_smaller_final_batch=False,\n        dynamic_pad=True)\n\n    return batch[\'image\'], batch[\'label\']\n'"
tpu/models/experimental/deeplab/main.py,22,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""DeepLab V3 training and evaluation loops.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import flags\nimport tensorflow as tf\n\nimport data_pipeline\nimport model\nfrom tensorflow.python.estimator import estimator\nfrom deeplab import common\nfrom deeplab.deprecated import segmentation_dataset\n\n# Dataset settings.\nflags.DEFINE_string(\'dataset_name\', \'pascal_voc_seg\',\n                    \'Name of the segmentation dataset.\')\nflags.DEFINE_string(\'train_split\', \'train_aug\',\n                    \'Which split of the dataset to be used for training\')\nflags.DEFINE_string(\'eval_split\', \'val\',\n                    \'Which split of the dataset used for evaluation\')\nflags.DEFINE_string(\'dataset_dir\', None, \'Where the dataset reside.\')\n\n# Preprocess settings.\nflags.DEFINE_multi_integer(\'crop_size\', [513, 513],\n                           \'Image crop size [height, width].\')\nflags.DEFINE_float(\'min_scale_factor\', 0.5,\n                   \'Mininum scale factor for data augmentation.\')\nflags.DEFINE_float(\'max_scale_factor\', 2,\n                   \'Maximum scale factor for data augmentation.\')\nflags.DEFINE_float(\'scale_factor_step_size\',\n                   0.25,\n                   \'Scale factor step size for data augmentation.\')\n\n# Model settings.\nflags.DEFINE_multi_integer(\'atrous_rates\', [6, 12, 18],\n                           \'Atrous rates for atrous spatial pyramid pooling.\')\nflags.DEFINE_integer(\'output_stride\', 16,\n                     \'The ratio of input to output spatial resolution.\')\nflags.DEFINE_boolean(\'fine_tune_batch_norm\',\n                     True,\n                     \'Fine tune the batch norm parameters or not.\')\nflags.DEFINE_boolean(\'upsample_logits\', True,\n                     \'Upsample logits during training.\')\n\n# Training and evaluation settings.\nflags.DEFINE_enum(\'learning_policy\', \'poly\', [\'poly\', \'step\'],\n                  \'Learning rate policy for training.\')\nflags.DEFINE_float(\'weight_decay\', 0.0001,\n                   \'The value of the weight decay for training.\')\nflags.DEFINE_float(\'learning_rate\', 0.01,\n                   \'The learning rate for model training.\')\nflags.DEFINE_float(\'learning_rate_decay\', 0.97,\n                   \'The rate of decay for learning rate.\')\nflags.DEFINE_float(\'learning_power\', 0.9,\n                   \'The power value used in the poly learning policy.\')\nflags.DEFINE_string(\'optimizer\', \'momentum\',\n                    \'Optimizer to use.\')\nflags.DEFINE_float(\'momentum\', 0.9,\n                   \'momentum for momentum optimizer\')\n# 8x num_shards to exploit TPU memory chunks.\nflags.DEFINE_integer(\'eval_batch_size\', 8, \'Batch size for evaluation.\')\nflags.DEFINE_integer(\'train_batch_size\', 64, \'Batch size for training.\')\nflags.DEFINE_integer(\'train_steps\', 20000,\n                     \'The number of steps to use for training.\')\nflags.DEFINE_integer(\'steps_per_eval\', 2000,\n                     (\'Controls how often evaluation is performed.\'))\nflags.DEFINE_string(\n    \'mode\', \'train_and_eval\',\n    (\'Train, or eval, or interleave train & eval.\'))\nflags.DEFINE_integer(\'save_checkpoints_steps\', 2000,\n                     \'Number of steps between checkpoint saves\')\nflags.DEFINE_string(\'model_dir\', None, \'Estimator model_dir\')\nflags.DEFINE_string(\'init_checkpoint\', None,\n                    \'Location of the checkpoint for seeding \'\n                    \'the backbone network\')\nflags.DEFINE_integer(\n    \'eval_timeout\',\n    default=None,\n    help=(\n        \'Maximum seconds between checkpoints before evaluation terminates.\'))\n# TODO(b/111116845, b/79915673): `use_host_call` must be `True`.\nflags.DEFINE_bool(\n    \'use_host_call\', default=True,\n    help=(\'Call host_call which is executed every training step. This is\'\n          \' generally used for generating training summaries (train loss,\'\n          \' learning rate, etc...). When --use_host_call=false, there could\'\n          \' be a performance drop if host_call function is slow and cannot\'\n          \' keep up with the TPU-side computation.\'))\nflags.DEFINE_integer(\n    \'iterations_per_loop\', default=2000,\n    help=(\'Number of steps to run on TPU before outfeeding metrics to the CPU.\'\n          \' If the number of iterations in the loop would exceed the number of\'\n          \' train steps, the loop will exit before reaching\'\n          \' --iterations_per_loop. The larger this value is, the higher the\'\n          \' utilization on the TPU.\'))\n\n# TPU settings.\nflags.DEFINE_integer(\'num_shards\', 8, \'Number of shards (TPU chips).\')\nflags.DEFINE_bool(\'use_tpu\', True, \'Use TPUs rather than plain CPUs\')\nflags.DEFINE_bool(\'use_bfloat16\', False, \'Use bfloat16 for training\')\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'tpu\', default=None,\n    help=\'The Cloud TPU to use for training. This should be either the name \'\n    \'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\')\n\nflags.DEFINE_string(\n    \'gcp_project\', default=None,\n    help=\'Project name for the Cloud TPU-enabled project. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\n\nflags.DEFINE_string(\n    \'tpu_zone\', default=None,\n    help=\'GCE zone where the Cloud TPU is located in. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\n\nslim = tf.contrib.slim\nFLAGS = flags.FLAGS\n\n\ndef train_and_eval(deeplab_estimator, train_dataset, eval_dataset,\n                   num_batches_per_epoch):\n  """"""Interleaves training and evaluation.""""""\n  # pylint: disable=protected-access\n  current_step = estimator._load_global_step_from_checkpoint_dir(\n      FLAGS.model_dir)\n  tf.logging.info(\'Training for %d steps (%.2f epochs in total). Current\'\n                  \' step %d.\' %\n                  (FLAGS.train_steps,\n                   FLAGS.train_steps / num_batches_per_epoch,\n                   current_step))\n  start_timestamp = time.time()\n  while current_step < FLAGS.train_steps:\n    # Train for up to steps_per_eval number of steps. At the end of training,\n    # a checkpoint will be written to --model_dir.\n    next_checkpoint = min(current_step + FLAGS.steps_per_eval,\n                          FLAGS.train_steps)\n\n    train_input_fn = data_pipeline.InputReader(\n        train_dataset,\n        FLAGS.train_split,\n        is_training=True,\n        model_variant=FLAGS.model_variant\n    )\n    deeplab_estimator.train(\n        input_fn=train_input_fn,\n        max_steps=next_checkpoint\n    )\n    current_step = next_checkpoint\n\n    elapsed_time = int(time.time() - start_timestamp)\n    tf.logging.info(\'Finished training up to step %d. Elapsed seconds %d.\' %\n                    (current_step, elapsed_time))\n\n    tf.logging.info(\'Starting to evaluate.\')\n\n    eval_input_fn = data_pipeline.InputReader(\n        eval_dataset,\n        FLAGS.eval_split,\n        is_training=False,\n        model_variant=FLAGS.model_variant\n    )\n    eval_results = deeplab_estimator.evaluate(\n        input_fn=eval_input_fn,\n        steps=eval_dataset.num_samples // FLAGS.eval_batch_size\n    )\n    tf.logging.info(\'Eval results: %s\' % eval_results)\n\n\ndef get_params(ignore_label, num_classes, num_batches_per_epoch):\n  """"""Build a dict of parameters from command line args.""""""\n  params = {k: FLAGS[k].value for k in FLAGS}\n\n  outputs_to_num_classes = {common.OUTPUT_TYPE: num_classes}\n  model_options = common.ModelOptions(\n      outputs_to_num_classes, FLAGS.crop_size, FLAGS.atrous_rates,\n      FLAGS.output_stride,\n      preprocessed_images_dtype=(\n          tf.bfloat16 if params[\'use_bfloat16\'] else tf.float32))\n  params.update({\'ignore_label\': ignore_label,\n                 \'model_options\': model_options,\n                 \'num_batches_per_epoch\': num_batches_per_epoch,\n                 \'num_classes\': num_classes,\n                 \'outputs_to_num_classes\': outputs_to_num_classes})\n\n  tf.logging.debug(\'Params: \')\n  for k, v in sorted(params.items()):\n    tf.logging.debug(\'%s: %s\', k, v)\n  return params\n\n\ndef main(unused_argv):\n  train_dataset = segmentation_dataset.get_dataset(\n      FLAGS.dataset_name, FLAGS.train_split, dataset_dir=FLAGS.dataset_dir)\n  eval_dataset = segmentation_dataset.get_dataset(\n      FLAGS.dataset_name, FLAGS.eval_split, dataset_dir=FLAGS.dataset_dir)\n\n  num_train_images = train_dataset.num_samples\n  num_classes = train_dataset.num_classes\n  ignore_label = train_dataset.ignore_label\n\n  num_batches_per_epoch = num_train_images / FLAGS.train_batch_size\n\n  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n      FLAGS.tpu,\n      zone=FLAGS.tpu_zone,\n      project=FLAGS.gcp_project)\n  config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=FLAGS.iterations_per_loop,\n          num_shards=FLAGS.num_shards))\n\n  params = get_params(ignore_label, num_classes, num_batches_per_epoch)\n\n  deeplab_estimator = tf.contrib.tpu.TPUEstimator(\n      use_tpu=FLAGS.use_tpu,\n      model_fn=model.model_fn,\n      config=config,\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=FLAGS.eval_batch_size,\n      params=params)\n\n  if FLAGS.mode == \'train\':\n    tf.logging.info(\'Training for %d steps (%.2f epochs in total).\' %\n                    (FLAGS.train_steps,\n                     FLAGS.train_steps / num_batches_per_epoch))\n    train_input_fn = data_pipeline.InputReader(\n        train_dataset,\n        FLAGS.train_split,\n        is_training=True,\n        model_variant=FLAGS.model_variant)\n    deeplab_estimator.train(\n        input_fn=train_input_fn,\n        max_steps=FLAGS.train_steps)\n  elif FLAGS.mode == \'train_and_eval\':\n    train_and_eval(deeplab_estimator, train_dataset, eval_dataset,\n                   num_batches_per_epoch)\n  elif FLAGS.mode == \'eval\':\n\n    eval_input_fn = data_pipeline.InputReader(\n        eval_dataset,\n        FLAGS.eval_split,\n        is_training=False,\n        model_variant=FLAGS.model_variant\n    )\n\n    # Run evaluation when there\'s a new checkpoint\n    for ckpt in tf.contrib.training.checkpoints_iterator(\n        FLAGS.model_dir, timeout=FLAGS.eval_timeout):\n\n      tf.logging.info(\'Starting to evaluate.\')\n      try:\n        eval_results = deeplab_estimator.evaluate(\n            input_fn=eval_input_fn,\n            steps=eval_dataset.num_samples // FLAGS.eval_batch_size\n        )\n        tf.logging.info(\'Eval results: %s\' % eval_results)\n\n        # Terminate eval job when final checkpoint is reached\n        current_step = int(os.path.basename(ckpt).split(\'-\')[1])\n        if current_step >= FLAGS.train_steps:\n          tf.logging.info(\'Evaluation finished after training step %d\' %\n                          current_step)\n          break\n\n      except tf.errors.NotFoundError:\n        # Since the coordinator is on a different job than the TPU worker,\n        # sometimes the TPU worker does not finish initializing until long after\n        # the CPU job tells it to start evaluating. In this case, the checkpoint\n        # file could have been deleted already.\n        tf.logging.info(\'Checkpoint %s no longer exists, skipping checkpoint\' %\n                        ckpt)\n  else:\n    tf.logging.error(\'Mode not found.\')\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/models/experimental/deeplab/model.py,50,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provide model_fn for TPUEstimator training and evaluation.""""""\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib.tpu.python.tpu import bfloat16\nfrom deeplab import common\nfrom deeplab.model import multi_scale_logits\nfrom deeplab.utils.train_utils import add_softmax_cross_entropy_loss_for_each_scale\n\n\nslim = tf.contrib.slim\n\n# Scope for the merged multi-scale logits.\n_MERGED_LOGITS_SCOPE = \'merged_logits\'\n\n\ndef _build_network(features, mode, params):\n  """"""Builds the network for different values of params[\'use_bfloat16\'].""""""\n  if params[\'use_bfloat16\']:\n    with bfloat16.bfloat16_scope():\n      outputs_to_scales_to_logits = multi_scale_logits(\n          features,\n          params[\'model_options\'],\n          params[\'image_pyramid\'],\n          weight_decay=0.0,\n          is_training=mode == tf.estimator.ModeKeys.TRAIN,\n          fine_tune_batch_norm=(\n              params[\'fine_tune_batch_norm\']\n              if mode == tf.estimator.ModeKeys.TRAIN else False)\n      )\n    for level, output in outputs_to_scales_to_logits.iteritems():\n      for scale, logits in output.iteritems():\n        outputs_to_scales_to_logits[level][scale] = tf.cast(logits, tf.float32)\n  else:\n    outputs_to_scales_to_logits = multi_scale_logits(\n        features,\n        params[\'model_options\'],\n        params[\'image_pyramid\'],\n        weight_decay=params[\'weight_decay\'],\n        is_training=mode == tf.estimator.ModeKeys.TRAIN,\n        fine_tune_batch_norm=(\n            params[\'fine_tune_batch_norm\']\n            if mode == tf.estimator.ModeKeys.TRAIN else False)\n    )\n  return outputs_to_scales_to_logits\n\n\ndef loss_fn(features, labels, mode, params):\n  """"""Computes label predictions and cross entropy loss against labels.""""""\n  outputs_to_scales_to_logits = _build_network(features, mode, params)\n\n  for output, num_classes in params[\'outputs_to_num_classes\'].items():\n    add_softmax_cross_entropy_loss_for_each_scale(\n        outputs_to_scales_to_logits[output],\n        labels,\n        num_classes,\n        ignore_label=params[\'ignore_label\'],\n        loss_weight=1.0,\n        upsample_logits=params[\'upsample_logits\'],\n        scope=output)\n\n  losses = tf.add_n(tf.losses.get_losses())\n  l2_loss = []\n  for v in tf.trainable_variables():\n    if \'BatchNorm\' not in v.name and \'weights\' in v.name:\n      l2_loss.append(tf.nn.l2_loss(v))\n  loss = losses + params[\'weight_decay\'] * tf.add_n(l2_loss)\n  return loss\n\n\ndef _create_eval_metric(features, labels, params):\n  """"""Creates eval_metric for model_fn.""""""\n  outputs_to_scales_to_logits = _build_network(\n      features, tf.estimator.ModeKeys.EVAL, params)\n\n  semantic_merged_logits = (\n      outputs_to_scales_to_logits[common.OUTPUT_TYPE][_MERGED_LOGITS_SCOPE])\n\n  def metric_fn(semantic_merged_logits, labels):\n    """"""Creates metric_fn for TPUEstimatorSpec.""""""\n    logits = tf.image.resize_bilinear(\n        semantic_merged_logits, params[\'crop_size\'], align_corners=True)\n    predictions_with_shape = tf.argmax(logits, 3, output_type=tf.int32)\n    predictions = tf.reshape(predictions_with_shape, shape=[-1])\n\n    labels = tf.reshape(labels, shape=[-1])\n    weights = tf.to_float(tf.not_equal(labels, params[\'ignore_label\']))\n\n    # Set ignore_label regions to label 0, because metrics.mean_iou requires\n    # range of labels = [0, dataset.num_classes). Note the ignore_lable regions\n    # are not evaluated since the corresponding regions contain weights = 0.\n    labels = tf.where(\n        tf.equal(labels, params[\'ignore_label\']), tf.zeros_like(labels), labels)\n\n    return {\n        \'miou\':\n            tf.metrics.mean_iou(\n                predictions, labels, params[\'num_classes\'], weights=weights),\n    }\n\n  return metric_fn, [semantic_merged_logits, labels]\n\n\ndef _get_optimizer(params, learning_rate):\n  """"""Gets optimizer based on params.""""""\n  if params[\'optimizer\'] == \'momentum\':\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate=learning_rate,\n        momentum=params[\'momentum\'],\n        use_nesterov=True)\n  elif params[\'optimizer\'] == \'sgd\':\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n  elif params[\'optimizer\'] == \'rmsprop\':\n    optimizer = tf.train.RMSPropOptimizer(\n        learning_rate=learning_rate,\n        epsilon=params[\'rmsprop_epsilon\'],\n        momentum=params[\'rmsprop_momentum\'])\n  else:\n    raise KeyError(\'Unknown optimizer: %s\' % params[\'optimizer\'])\n\n  return optimizer\n\n\ndef _get_learning_rate(params, global_step, num_batches_per_epoch):\n  """"""Gets learning rate based on params.""""""\n  learning_policy = params[\'learning_policy\']\n  if learning_policy == \'poly\':\n    learning_rate = tf.train.polynomial_decay(\n        params[\'learning_rate\'],\n        global_step,\n        params[\'train_steps\'],\n        end_learning_rate=0,\n        power=params[\'learning_power\'])\n  elif learning_policy == \'step\':\n    learning_rate = tf.train.exponential_decay(\n        params[\'learning_rate\'],\n        global_step,\n        decay_rate=params[\'learning_rate_decay\'],\n        decay_steps=num_batches_per_epoch,\n        staircase=True,\n    )\n  else:\n    raise KeyError(\'Unknown learning policy: %s\' % learning_policy)\n\n  return learning_rate\n\n\ndef model_fn(features, labels, mode, params):\n  """"""TPUEstimator compatible model function.""""""\n  loss = loss_fn(features, labels, mode, params)\n\n  host_call = None\n  train_op = None\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    num_batches_per_epoch = params[\'num_batches_per_epoch\']\n    global_step = tf.train.get_global_step()\n    current_epoch = tf.cast(global_step, tf.float32) / num_batches_per_epoch\n\n    learning_rate = _get_learning_rate(\n        params, global_step, num_batches_per_epoch)\n    optimizer = _get_optimizer(params, learning_rate)\n    if params[\'use_tpu\']:\n      optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n    # Batch norm requires update_ops to be added as a train_op dependency.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.minimize(loss, tf.train.get_global_step())\n\n    if params[\'use_host_call\']:\n      def host_call_fn(global_step, loss, learning_rate, current_epoch):\n        """"""Training host call. Creates scalar summaries for training metrics.\n\n        This function is executed on the CPU and should not directly reference\n        any Tensors in the rest of the `model_fn`. To pass Tensors from the\n        model to the `metric_fn`, provide as part of the `host_call`. See\n        https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n        for more information.\n\n        Arguments should match the list of `Tensor` objects passed as the second\n        element in the tuple passed to `host_call`.\n\n        Args:\n          global_step: `Tensor with shape `[batch, ]` for the global_step.\n          loss: `Tensor` with shape `[batch, ]` for the training loss.\n          learning_rate: `Tensor` with shape `[batch, ]` for the learning_rate.\n          current_epoch: `Tensor` with shape `[batch, ]` for the current_epoch.\n\n        Returns:\n          List of summary ops to run on the CPU host.\n        """"""\n        # Outfeed supports int32 but global_step is expected to be int64.\n        global_step = tf.reduce_mean(global_step)\n        with (tf.contrib.summary.create_file_writer(\n            params[\'model_dir\']).as_default()):\n          with tf.contrib.summary.always_record_summaries():\n            tf.contrib.summary.scalar(\n                \'loss\', tf.reduce_mean(loss), step=global_step)\n            tf.contrib.summary.scalar(\n                \'learning_rate\', tf.reduce_mean(learning_rate),\n                step=global_step)\n            tf.contrib.summary.scalar(\n                \'current_epoch\', tf.reduce_mean(current_epoch),\n                step=global_step)\n\n            return tf.contrib.summary.all_summary_ops()\n\n      # To log the loss, current learning rate, and epoch for Tensorboard, the\n      # summary op needs to be run on the host CPU via host_call. host_call\n      # expects [batch_size, ...] Tensors, thus reshape to introduce a batch\n      # dimension. These Tensors are implicitly concatenated to\n      # [params[\'batch_size\']].\n      global_step_t = tf.reshape(global_step, [1])\n      loss_t = tf.reshape(loss, [1])\n      learning_rate_t = tf.reshape(learning_rate, [1])\n      current_epoch_t = tf.reshape(current_epoch, [1])\n\n      host_call = (host_call_fn,\n                   [global_step_t, loss_t, learning_rate_t, current_epoch_t])\n\n  eval_metrics = None\n  if mode == tf.estimator.ModeKeys.EVAL:\n    eval_metrics = _create_eval_metric(features, labels, params)\n\n  # Restore from checkpoint if available.\n  if params[\'init_checkpoint\'] and mode == tf.estimator.ModeKeys.TRAIN:\n    def scaffold_fn():\n      """"""Create Scaffold for initialization, etc.""""""\n      tf.train.init_from_checkpoint(params[\'init_checkpoint\'], {\n          \'resnet_v1_101/\': \'resnet_v1_101/\',\n      })\n      return tf.train.Scaffold()\n  else:\n    scaffold_fn = None\n  return tf.contrib.tpu.TPUEstimatorSpec(\n      mode=mode,\n      loss=loss,\n      train_op=train_op,\n      scaffold_fn=scaffold_fn,\n      host_call=host_call,\n      eval_metrics=eval_metrics,\n  )\n'"
tpu/models/experimental/distribution_strategy/imagenet_input.py,40,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Efficient ImageNet input pipeline using tf.data.Dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nfrom collections import namedtuple\nimport functools\nimport os\nimport tensorflow as tf\nimport resnet_preprocessing\n\n\ndef image_serving_input_fn():\n  """"""Serving input fn for raw images.""""""\n\n  def _preprocess_image(image_bytes):\n    """"""Preprocess a single raw image.""""""\n    image = resnet_preprocessing.preprocess_image(\n        image_bytes=image_bytes, is_training=False)\n    return image\n\n  image_bytes_list = tf.placeholder(\n      shape=[None],\n      dtype=tf.string,\n  )\n  images = tf.map_fn(\n      _preprocess_image, image_bytes_list, back_prop=False, dtype=tf.float32)\n  return tf.estimator.export.ServingInputReceiver(\n      images, {\'image_bytes\': image_bytes_list})\n\n\nclass ImageNetTFExampleInput(object):\n  """"""Base class for ImageNet input_fn generator.\n\n  Args:\n    is_training: `bool` for whether the input is for training\n    use_bfloat16: If True, use bfloat16 precision; else use float32.\n    transpose_input: \'bool\' for whether to use the double transpose trick\n    num_cores: `int` for the number of TPU cores\n  """"""\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self,\n               is_training,\n               use_bfloat16,\n               num_cores=8,\n               image_size=224,\n               transpose_input=False):\n    self.image_preprocessing_fn = resnet_preprocessing.preprocess_image\n    self.is_training = is_training\n    self.use_bfloat16 = use_bfloat16\n    self.num_cores = num_cores\n    self.transpose_input = transpose_input\n    self.image_size = image_size\n\n  def set_shapes(self, batch_size, images, labels):\n    """"""Statically set the batch_size dimension.""""""\n    if self.transpose_input:\n      images.set_shape(images.get_shape().merge_with(\n          tf.TensorShape([None, None, None, batch_size])))\n      labels.set_shape(labels.get_shape().merge_with(\n          tf.TensorShape([batch_size])))\n    else:\n      images.set_shape(images.get_shape().merge_with(\n          tf.TensorShape([batch_size, None, None, None])))\n      labels.set_shape(labels.get_shape().merge_with(\n          tf.TensorShape([batch_size])))\n\n    return images, labels\n\n  def dataset_parser(self, value):\n    """"""Parses an image and its label from a serialized ResNet-50 TFExample.\n\n    Args:\n      value: serialized string containing an ImageNet TFExample.\n\n    Returns:\n      Returns a tuple of (image, label) from the TFExample.\n    """"""\n    keys_to_features = {\n        \'image/encoded\': tf.FixedLenFeature((), tf.string, \'\'),\n        \'image/format\': tf.FixedLenFeature((), tf.string, \'jpeg\'),\n        \'image/class/label\': tf.FixedLenFeature([], tf.int64, -1),\n        \'image/class/text\': tf.FixedLenFeature([], tf.string, \'\'),\n        \'image/object/bbox/xmin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/class/label\': tf.VarLenFeature(dtype=tf.int64),\n    }\n\n    parsed = tf.parse_single_example(value, keys_to_features)\n    image_bytes = tf.reshape(parsed[\'image/encoded\'], shape=[])\n\n    image = self.image_preprocessing_fn(\n        image_bytes=image_bytes,\n        is_training=self.is_training,\n        image_size=self.image_size,\n        use_bfloat16=self.use_bfloat16)\n\n    # Subtract one so that labels are in [0, 1000).\n    label = tf.cast(\n        tf.reshape(parsed[\'image/class/label\'], shape=[]), dtype=tf.int32) - 1\n\n    return image, label\n\n  @abc.abstractmethod\n  def make_source_dataset(self):\n    """"""Makes dataset of serialized TFExamples.\n\n    The returned dataset will contain `tf.string` tensors, but these strings are\n    serialized `TFExample` records that will be parsed by `dataset_parser`.\n\n    If self.is_training, the dataset should be infinite.\n\n    Returns:\n      A `tf.data.Dataset` object.\n    """"""\n    return\n\n  def input_fn(self, params):\n    """"""Input function which provides a single batch for train or eval.\n\n    Args:\n      params: `dict` of parameters passed from the `TPUEstimator`.\n          `params[\'batch_size\']` is always provided and should be used as the\n          effective batch size.\n\n    Returns:\n      A `tf.data.Dataset` object.\n    """"""\n\n    # Retrieves the batch size for the current shard. The # of shards is\n    # computed according to the input pipeline deployment. See\n    # tf.contrib.tpu.RunConfig for details.\n    batch_size = params[\'batch_size\']\n\n    dataset = self.make_source_dataset()\n\n    # Use the fused map-and-batch operation.\n    #\n    # For XLA, we must used fixed shapes. Because we repeat the source training\n    # dataset indefinitely, we can use `drop_remainder=True` to get fixed-size\n    # batches without dropping any training examples.\n    #\n    # When evaluating, `drop_remainder=True` prevents accidentally evaluating\n    # the same image twice by dropping the final batch if it is less than a full\n    # batch size. As long as this validation is done with consistent batch size,\n    # exactly the same images will be used.\n    dataset = dataset.apply(\n        tf.contrib.data.map_and_batch(\n            self.dataset_parser, batch_size=batch_size,\n            num_parallel_batches=self.num_cores, drop_remainder=True))\n\n    # Transpose for performance on TPU\n    if self.transpose_input:\n      dataset = dataset.map(\n          lambda images, labels: (tf.transpose(images, [1, 2, 3, 0]), labels),\n          num_parallel_calls=self.num_cores)\n\n    # Assign static batch size dimension\n    dataset = dataset.map(functools.partial(self.set_shapes, batch_size))\n\n    # Prefetch overlaps in-feed with training\n    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n    return dataset\n\n\nclass ImageNetInput(ImageNetTFExampleInput):\n  """"""Generates ImageNet input_fn from a series of TFRecord files.\n\n  The training data is assumed to be in TFRecord format with keys as specified\n  in the dataset_parser below, sharded across 1024 files, named sequentially:\n\n      train-00000-of-01024\n      train-00001-of-01024\n      ...\n      train-01023-of-01024\n\n  The validation data is in the same format but sharded in 128 files.\n\n  The format of the data required is created by the script at:\n      https://github.com/tensorflow/tpu/blob/master/tools/datasets/imagenet_to_gcs.py\n  """"""\n\n  def __init__(self,\n               is_training,\n               use_bfloat16,\n               transpose_input,\n               data_dir,\n               image_size=224,\n               num_parallel_calls=64,\n               cache=False,\n               num_replicas=None,\n               replica=0):\n    """"""Create an input from TFRecord files.\n\n    Args:\n      is_training: `bool` for whether the input is for training\n      use_bfloat16: If True, use bfloat16 precision; else use float32.\n      transpose_input: \'bool\' for whether to use the double transpose trick\n      data_dir: `str` for the directory of the training and validation data;\n          if \'null\' (the literal string \'null\') or implicitly False\n          then construct a null pipeline, consisting of empty images\n          and blank labels.\n      num_parallel_calls: concurrency level to use when reading data from disk.\n      cache: if true, fill the dataset by repeating from its cache\n      num_replicas: `int` for the number of model replicas this dataset should\n          be sharded onto, or `None` if this dataset should not be sharded.\n      replica: `int` for the replica that input_fn should produce data for\n    """"""\n    super(ImageNetInput, self).__init__(\n        is_training=is_training,\n        image_size=image_size,\n        use_bfloat16=use_bfloat16,\n        transpose_input=transpose_input)\n    self.data_dir = data_dir\n    # TODO(b/112427086):  simplify the choice of input source\n    if self.data_dir == \'null\' or not self.data_dir:\n      self.data_dir = None\n    self.num_parallel_calls = num_parallel_calls\n    self.cache = cache\n    self.num_replicas = num_replicas\n    self.replica = replica\n\n  def _get_null_input(self, data):\n    """"""Returns a null image (all black pixels).\n\n    Args:\n      data: element of a dataset, ignored in this method, since it produces\n          the same null image regardless of the element.\n\n    Returns:\n      a tensor representing a null image.\n    """"""\n    del data  # Unused since output is constant regardless of input\n    return tf.zeros([self.image_size, self.image_size, 3], tf.bfloat16\n                    if self.use_bfloat16 else tf.float32)\n\n  def dataset_parser(self, value):\n    """"""See base class.""""""\n    if not self.data_dir:\n      return value, tf.constant(0, tf.int32)\n    return super(ImageNetInput, self).dataset_parser(value)\n\n  def make_source_dataset(self):\n    """"""See base class.""""""\n    if not self.data_dir:\n      tf.logging.info(\'Undefined data_dir implies null input\')\n      return tf.data.Dataset.range(1).repeat().map(self._get_null_input)\n\n    # Shuffle the filenames to ensure better randomization.\n    file_pattern = os.path.join(\n        self.data_dir, \'train-*\' if self.is_training else \'validation-*\')\n    dataset = tf.data.Dataset.list_files(file_pattern, shuffle=self.is_training)\n\n    # Shard the data into `num_replicas` parts, get the part for `replica`\n    if self.num_replicas:\n      dataset = dataset.shard(self.num_replicas, self.replica)\n\n    if self.is_training and not self.cache:\n      dataset = dataset.repeat()\n\n    def fetch_dataset(filename):\n      buffer_size = 8 * 1024 * 1024  # 8 MiB per file\n      dataset = tf.data.TFRecordDataset(filename, buffer_size=buffer_size)\n      return dataset\n\n    # Read the data from disk in parallel\n    dataset = dataset.apply(\n        tf.contrib.data.parallel_interleave(\n            fetch_dataset, cycle_length=self.num_parallel_calls, sloppy=True))\n\n    if self.cache:\n      dataset = dataset.cache().apply(\n          tf.contrib.data.shuffle_and_repeat(1024 * 16))\n    else:\n      dataset = dataset.shuffle(1024)\n    return dataset\n\n\n# Defines a selection of data from a Cloud Bigtable.\nBigtableSelection = namedtuple(\'BigtableSelection\',\n                               [\'project\',\n                                \'instance\',\n                                \'table\',\n                                \'prefix\',\n                                \'column_family\',\n                                \'column_qualifier\'])\n\n\nclass ImageNetBigtableInput(ImageNetTFExampleInput):\n  """"""Generates ImageNet input_fn from a Bigtable for training or evaluation.\n  """"""\n\n  def __init__(self, is_training, use_bfloat16, transpose_input, selection):\n    """"""Constructs an ImageNet input from a BigtableSelection.\n\n    Args:\n      is_training: `bool` for whether the input is for training\n      use_bfloat16: If True, use bfloat16 precision; else use float32.\n      transpose_input: \'bool\' for whether to use the double transpose trick\n      selection: a BigtableSelection specifying a part of a Bigtable.\n    """"""\n    super(ImageNetBigtableInput, self).__init__(\n        is_training=is_training,\n        use_bfloat16=use_bfloat16,\n        transpose_input=transpose_input)\n    self.selection = selection\n\n  def make_source_dataset(self):\n    """"""See base class.""""""\n    data = self.selection\n    client = tf.contrib.cloud.BigtableClient(data.project, data.instance)\n    table = client.table(data.table)\n    ds = table.parallel_scan_prefix(data.prefix,\n                                    columns=[(data.column_family,\n                                              data.column_qualifier)])\n    # The Bigtable datasets will have the shape (row_key, data)\n    ds_data = ds.map(lambda index, data: data)\n\n    if self.is_training:\n      ds_data = ds_data.repeat()\n\n    return ds_data\n'"
tpu/models/experimental/distribution_strategy/imagenet_input_keras.py,26,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Efficient ImageNet input pipeline using tf.data.Dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\n\nimport resnet_preprocessing\n\n\nclass ImageNetInput(object):\n  """"""Generates ImageNet input_fn for training or evaluation.\n\n  The training data is assumed to be in TFRecord format with keys as specified\n  in the dataset_parser below, sharded across 1024 files, named sequentially:\n      train-00000-of-01024\n      train-00001-of-01024\n      ...\n      train-01023-of-01024\n\n  The validation data is in the same format but sharded in 128 files.\n\n  The format of the data required is created by the script at:\n      https://github.com/tensorflow/tpu/blob/master/tools/datasets/imagenet_to_gcs.py\n\n  Args:\n    is_training: `bool` for whether the input is for training\n    data_dir: `str` for the directory of the training and validation data;\n        if \'null\' (the literal string \'null\', not None), then construct a null\n        pipeline, consisting of empty images.\n    per_core_batch_size: The per-TPU-core batch size to use.\n  """"""\n\n  def __init__(self, is_training, data_dir, per_core_batch_size=128):\n    self.image_preprocessing_fn = resnet_preprocessing.preprocess_image\n    self.is_training = is_training\n    self.data_dir = data_dir\n    if self.data_dir == \'null\' or self.data_dir == \'\':\n      self.data_dir = None\n    self.per_core_batch_size = per_core_batch_size\n\n  def dataset_parser(self, value):\n    """"""Parse an ImageNet record from a serialized string Tensor.""""""\n    keys_to_features = {\n        \'image/encoded\':\n            tf.FixedLenFeature((), tf.string, \'\'),\n        \'image/format\':\n            tf.FixedLenFeature((), tf.string, \'jpeg\'),\n        \'image/class/label\':\n            tf.FixedLenFeature([], tf.int64, -1),\n        \'image/class/text\':\n            tf.FixedLenFeature([], tf.string, \'\'),\n        \'image/object/bbox/xmin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/class/label\':\n            tf.VarLenFeature(dtype=tf.int64),\n    }\n\n    parsed = tf.parse_single_example(value, keys_to_features)\n    image_bytes = tf.reshape(parsed[\'image/encoded\'], shape=[])\n\n    image = self.image_preprocessing_fn(\n        image_bytes=image_bytes,\n        is_training=self.is_training,\n        use_bfloat16=False)\n\n    # Subtract one so that labels are in [0, 1000), and cast to float32 for\n    # Keras model.\n    label = tf.cast(tf.cast(\n        tf.reshape(parsed[\'image/class/label\'], shape=[1]), dtype=tf.int32) - 1,\n                    dtype=tf.float32)\n\n    return image, label\n\n  def input_fn(self):\n    """"""Input function which provides a single batch for train or eval.\n\n    Returns:\n      A `tf.data.Dataset` object.\n    """"""\n    if self.data_dir is None:\n      tf.logging.info(\'Using fake input.\')\n      return self.input_fn_null()\n\n    # Shuffle the filenames to ensure better randomization.\n    file_pattern = os.path.join(\n        self.data_dir, \'train-*\' if self.is_training else \'validation-*\')\n    dataset = tf.data.Dataset.list_files(file_pattern, shuffle=self.is_training)\n\n    if self.is_training:\n      dataset = dataset.repeat()\n\n    def fetch_dataset(filename):\n      buffer_size = 8 * 1024 * 1024     # 8 MiB per file\n      dataset = tf.data.TFRecordDataset(filename, buffer_size=buffer_size)\n      return dataset\n\n    # Read the data from disk in parallel\n    dataset = dataset.apply(\n        tf.contrib.data.parallel_interleave(\n            fetch_dataset, cycle_length=16, sloppy=True))\n    dataset = dataset.shuffle(1024)\n\n    # Parse, pre-process, and batch the data in parallel\n    dataset = dataset.apply(\n        tf.contrib.data.map_and_batch(\n            self.dataset_parser, batch_size=self.per_core_batch_size,\n            num_parallel_batches=2,\n            drop_remainder=True))\n\n    # Prefetch overlaps in-feed with training\n    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n    return dataset\n\n  def input_fn_null(self):\n    """"""Input function which provides null (black) images.""""""\n    dataset = tf.data.Dataset.range(1).repeat().map(self._get_null_input)\n    dataset = dataset.prefetch(self.per_core_batch_size)\n\n    dataset = dataset.batch(self.per_core_batch_size, drop_remainder=True)\n\n    dataset = dataset.prefetch(32)     # Prefetch overlaps in-feed with training\n    tf.logging.info(\'Input dataset: %s\', str(dataset))\n    return dataset\n\n  def _get_null_input(self, _):\n    null_image = tf.zeros([224, 224, 3], tf.float32)\n    return null_image, tf.constant(0, tf.float32)\n'"
tpu/models/experimental/distribution_strategy/resnet_estimator.py,48,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""TF Estimator-based ResNet compatible with TPU Distribution Strategy.\n\nThis is an implementation of TensorFlow Estimator-based ResNet and is\nadditionally compatible with TPU Distribution Strategy.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\nimport tensorflow as tf\n\nimport imagenet_input\nimport resnet_model\nfrom tensorflow.contrib.distribute.python import tpu_strategy as tpu_lib\n\ntf.flags.DEFINE_string(\'tpu\', None, \'Name of TPU to run this against\')\ntf.flags.DEFINE_string(\'gcp_project\', None, \'GCP project containing the TPU\')\ntf.flags.DEFINE_string(\'tpu_zone\', None, \'GCP zone of the TPU\')\n\ntf.flags.DEFINE_integer(\'num_cores\', 8, \'Number of cores in TPU\')\ntf.flags.DEFINE_string(\'data_dir\', None, \'Directory containing ImageNet data\')\ntf.flags.DEFINE_string(\'model_dir\', \'\',\n                       \'Directory containing model data and checkpoints\')\ntf.flags.DEFINE_integer(\'train_batch_size\', 128, \'Per core batch size\')\ntf.flags.DEFINE_integer(\'eval_batch_size\', 125, \'Per core batch size\')\ntf.flags.DEFINE_string(\'precision\', \'bfloat16\',\n                       \'Precision to use; one of: {bfloat16, float32}\')\ntf.flags.DEFINE_bool(\'use_keras_model\', False,\n                     \'Whether to use Keras implementation of ResNet model\')\ntf.flags.DEFINE_bool(\'transpose_input\', True,\n                     \'Whether to transpose input for better performance\')\ntf.flags.DEFINE_string(\'optimizer\', \'momentum\',\n                       \'Optimizer to use; one of: {momentum, sgd}\')\n\n\nFLAGS = tf.flags.FLAGS\n\n_NUM_TRAIN_IMAGES = 1281167\n_NUM_EVAL_IMAGES = 50000\n_NUM_CLASSES = 1000\n_RESNET_DEPTH = 50\n_LEARNING_RATE = 0.1\n_MOMENTUM = 0.9\n_WEIGHT_DECAY = 1e-4\n\n_TRAIN_STEPS = 112590\n\nMEAN_RGB = [0.485, 0.456, 0.406]\nSTDDEV_RGB = [0.229, 0.224, 0.225]\n\n# Learning rate schedule (much more aggressive for testing)\nLR_SCHEDULE = [    # (multiplier, epoch to start) tuples\n    (1.0, 5), (0.1, 30), (0.01, 60), (0.001, 80)\n]\n\n\ndef learning_rate_schedule(current_epoch):\n  scaled_lr = (_LEARNING_RATE *\n               (FLAGS.train_batch_size * FLAGS.num_cores / 256.0))\n  decay_rate = (scaled_lr * LR_SCHEDULE[0][0] *\n                tf.to_float(current_epoch) / LR_SCHEDULE[0][1])\n  for mult, start_epoch in LR_SCHEDULE:\n    decay_rate = tf.where(current_epoch < start_epoch,\n                          decay_rate, scaled_lr * mult)\n  return decay_rate\n\n\ndef model_fn(features, labels, mode):\n  """"""Definition for ResNet model.""""""\n  is_training = mode == tf.estimator.ModeKeys.TRAIN\n\n  if FLAGS.transpose_input:\n    features = tf.transpose(features, [3, 0, 1, 2])  # Double-transpose trick\n\n  # Normalize the image to zero mean and unit variance.\n  features -= tf.constant(MEAN_RGB, shape=[1, 1, 3], dtype=features.dtype)\n  features /= tf.constant(STDDEV_RGB, shape=[1, 1, 3], dtype=features.dtype)\n\n  def create_model():\n    """"""Create the model and compute the logits.""""""\n    if FLAGS.use_keras_model:\n      model = tf.keras.applications.resnet50.ResNet50(\n          include_top=True,\n          weights=None,\n          input_tensor=None,\n          input_shape=None,\n          pooling=None,\n          classes=_NUM_CLASSES)\n      return model(features, training=is_training)\n    else:\n      model = resnet_model.resnet_v1(\n          resnet_depth=_RESNET_DEPTH,\n          num_classes=_NUM_CLASSES,\n          data_format=\'channels_last\')\n      return model(inputs=features, is_training=is_training)\n\n  if FLAGS.precision == \'bfloat16\':\n    with tf.contrib.tpu.bfloat16_scope():\n      logits = create_model()\n  else:\n    logits = create_model()\n\n  logits = tf.cast(logits, tf.float32)\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    assert False, \'Not implemented correctly right now!\'\n    predictions = {\'logits\': logits}\n    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n  cross_entropy = tf.losses.sparse_softmax_cross_entropy(\n      labels=labels, logits=logits)\n\n  loss = cross_entropy + _WEIGHT_DECAY * tf.add_n(\n      [tf.nn.l2_loss(v) for v in tf.trainable_variables()\n       if \'batch_normalization\' not in v.name])\n\n  if mode == tf.estimator.ModeKeys.EVAL:\n    predictions = tf.argmax(logits, axis=1)\n    top_1_accuracy = tf.metrics.accuracy(labels, predictions)\n    # TODO(priyag): Add this back when in_top_k is supported on TPU.\n    # in_top_5 = tf.cast(tf.nn.in_top_k(logits, labels, 5), tf.float32)\n    # top_5_accuracy = tf.metrics.mean(in_top_5)\n\n    eval_metric_ops = {\n        \'top_1_accuracy\': top_1_accuracy,\n        # \'top_5_accuracy\': top_5_accuracy,\n    }\n\n    return tf.estimator.EstimatorSpec(\n        mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\n  assert mode == tf.estimator.ModeKeys.TRAIN\n\n  global_step = tf.train.get_or_create_global_step()\n  batches_per_epoch = (_NUM_TRAIN_IMAGES /\n                       (FLAGS.train_batch_size * FLAGS.num_cores))\n  current_epoch = (tf.cast(global_step, tf.float32) / batches_per_epoch)\n  learning_rate = learning_rate_schedule(current_epoch)\n\n  if FLAGS.optimizer == \'sgd\':\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n  else:\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate=learning_rate,\n        momentum=_MOMENTUM,\n        use_nesterov=True)\n\n  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n  with tf.control_dependencies(update_ops):\n    train_op = optimizer.minimize(loss, global_step=global_step)\n  return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n\n\ndef main(unused_argv):\n  """"""Starts a ResNet training session.""""""\n  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n      FLAGS.tpu,\n      zone=FLAGS.tpu_zone,\n      project=FLAGS.gcp_project)\n\n  # Estimator looks at the master it connects to for MonitoredTrainingSession\n  # by reading the `TF_CONFIG` environment variable.\n  tf_config_env = {\n      \'session_master\': tpu_cluster_resolver.get_master(),\n      \'eval_session_master\': tpu_cluster_resolver.get_master()\n  }\n  os.environ[\'TF_CONFIG\'] = json.dumps(tf_config_env)\n\n  steps_per_run_train = _NUM_TRAIN_IMAGES // (\n      FLAGS.train_batch_size * FLAGS.num_cores)\n  steps_per_run_eval = _NUM_EVAL_IMAGES // (\n      FLAGS.eval_batch_size * FLAGS.num_cores)\n  steps_per_eval = steps_per_run_train\n\n  train_distribution = tpu_lib.TPUStrategy(tpu_cluster_resolver,\n                                           steps_per_run=steps_per_run_train)\n  eval_distribution = tpu_lib.TPUStrategy(tpu_cluster_resolver,\n                                          steps_per_run=steps_per_run_eval)\n  config = tf.estimator.RunConfig(\n      model_dir=FLAGS.model_dir,\n      train_distribute=train_distribution,\n      eval_distribute=eval_distribution,\n      save_checkpoints_steps=steps_per_eval,\n      save_checkpoints_secs=None,\n      keep_checkpoint_max=1000)\n\n  resnet_estimator = tf.estimator.Estimator(\n      model_fn=model_fn, config=config)\n\n  train_input, eval_input = [\n      imagenet_input.ImageNetInput(\n          is_training=is_training,\n          data_dir=FLAGS.data_dir,\n          transpose_input=FLAGS.transpose_input,\n          use_bfloat16=(FLAGS.precision == \'bfloat16\'))\n      for is_training in [True, False]\n  ]\n\n  try:\n    current_step = resnet_estimator.get_variable_value(tf.GraphKeys.GLOBAL_STEP)\n  except ValueError:\n    current_step = 0\n\n  while current_step < _TRAIN_STEPS:\n    next_checkpoint = min(current_step + steps_per_eval, _TRAIN_STEPS)\n\n    resnet_estimator.train(\n        input_fn=lambda: train_input.input_fn(  # pylint: disable=g-long-lambda\n            {\'batch_size\': FLAGS.train_batch_size}),\n        max_steps=next_checkpoint)\n    current_step = next_checkpoint\n\n    eval_results = resnet_estimator.evaluate(\n        input_fn=lambda: eval_input.input_fn(  # pylint: disable=g-long-lambda\n            {\'batch_size\': FLAGS.eval_batch_size}),\n        steps=_NUM_EVAL_IMAGES // (FLAGS.eval_batch_size * FLAGS.num_cores))\n\n    tf.logging.info(\'Eval results: %s\' % eval_results)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/models/experimental/distribution_strategy/resnet_keras.py,6,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""ResNet-50 implemented with Keras running on Cloud TPUs.\n\nThis file shows how you can run ResNet-50 on a Cloud TPU using the TensorFlow\nKeras support. This is configured for ImageNet (e.g. 1000 classes), but you can\neasily adapt to your own datasets by changing the code appropriately.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nfrom absl import logging\nimport tensorflow as tf\nimport numpy as np\nimport os\n\nimport imagenet_input_keras as imagenet_input\nfrom tensorflow.contrib.distribute.python import tpu_strategy as tpu_lib\n\ntry:\n  import h5py as _  # pylint: disable=g-import-not-at-top\n  HAS_H5PY = True\nexcept ImportError:\n  logging.warning(\'`h5py` is not installed. Please consider installing it \'\n                  \'to save weights for long-running training.\')\n  HAS_H5PY = False\n\n\nflags.DEFINE_bool(\'use_tpu\', True, \'Use TPU model instead of CPU.\')\nflags.DEFINE_string(\'tpu\', None, \'Name of the TPU to use.\')\nflags.DEFINE_string(\'data_dir\', None, \'Directory of training and testing data.\')\ntf.flags.DEFINE_string(\'model_dir\', \'\',\n                       \'Directory containing model data and checkpoints\')\n\nFLAGS = flags.FLAGS\n\nPER_CORE_BATCH_SIZE = 128\nNUM_CLASSES = 1000\nIMAGE_SIZE = 224\nAPPROX_IMAGENET_TRAINING_IMAGES = 1280000  # Approximate number of images.\nAPPROX_IMAGENET_TEST_IMAGES = 48000  # Approximate number of images.\n\nWEIGHTS_TXT = \'resnet50_weights.h5\'\n\n\ndef main(argv):\n  logging.info(\'Building Keras ResNet-50 model.\')\n  model = tf.keras.applications.resnet50.ResNet50(\n      include_top=True,\n      weights=None,\n      input_tensor=None,\n      input_shape=None,\n      pooling=None,\n      classes=NUM_CLASSES)\n\n  num_cores = 8\n  batch_size = PER_CORE_BATCH_SIZE * num_cores\n\n  if FLAGS.use_tpu:\n    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    strategy = tpu_lib.TPUStrategy(resolver, steps_per_run=100)\n  else:\n    strategy = None\n\n  logging.info(\'Compiling model.\')\n  model.compile(\n      optimizer=tf.train.GradientDescentOptimizer(learning_rate=1.0),\n      loss=\'sparse_categorical_crossentropy\',\n      metrics=[\'sparse_categorical_accuracy\'],\n      distribute=strategy)\n\n  # TODO(sourabhbajaj): Add support for synthetic dataset.\n  if FLAGS.data_dir is None:\n    raise ValueError(\'data_dir must be provided to train the model.\')\n\n  imagenet_train, imagenet_eval = [imagenet_input.ImageNetInput(\n      is_training=is_training,\n      data_dir=FLAGS.data_dir,\n      per_core_batch_size=PER_CORE_BATCH_SIZE)\n                                   for is_training in [True, False]]\n  logging.info(\'Training model using real data in directory ""%s"".\',\n               FLAGS.data_dir)\n  num_epochs = 90  # Standard imagenet training regime.\n  model.fit(imagenet_train.input_fn(),\n            epochs=num_epochs,\n            steps_per_epoch=int(APPROX_IMAGENET_TRAINING_IMAGES / batch_size))\n\n  if HAS_H5PY:\n    weights_path = os.path.join(FLAGS.model_dir, WEIGHTS_TXT)\n    logging.info(\'Save weights into %s\', weights_path)\n    model.save_weights(weights_path, overwrite=True)\n\n  logging.info(\'Evaluating the model on the validation dataset.\')\n  score = model.evaluate(\n      imagenet_eval.input_fn(),\n      steps=int(APPROX_IMAGENET_TEST_IMAGES // batch_size),\n      verbose=1)\n  logging.info(\'Evaluation score: %s\', score)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/models/experimental/distribution_strategy/resnet_model.py,21,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the post-activation form of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nBATCH_NORM_DECAY = 0.9\nBATCH_NORM_EPSILON = 1e-5\n\n\ndef batch_norm_relu(inputs, is_training, relu=True, init_zero=False,\n                    data_format=\'channels_first\'):\n  """"""Performs a batch normalization followed by a ReLU.\n\n  Args:\n    inputs: `Tensor` of shape `[batch, channels, ...]`.\n    is_training: `bool` for whether the model is training.\n    relu: `bool` if False, omits the ReLU operation.\n    init_zero: `bool` if True, initializes scale parameter of batch\n        normalization with 0 instead of 1 (default).\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    A normalized `Tensor` with the same `data_format`.\n  """"""\n  if init_zero:\n    gamma_initializer = tf.zeros_initializer()\n  else:\n    gamma_initializer = tf.ones_initializer()\n\n  if data_format == \'channels_first\':\n    axis = 1\n  else:\n    axis = 3\n\n  inputs = tf.layers.batch_normalization(\n      inputs=inputs,\n      axis=axis,\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      center=True,\n      scale=True,\n      training=is_training,\n      fused=True,\n      gamma_initializer=gamma_initializer)\n\n  if relu:\n    inputs = tf.nn.relu(inputs)\n  return inputs\n\n\ndef fixed_padding(inputs, kernel_size, data_format=\'channels_first\'):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Args:\n    inputs: `Tensor` of size `[batch, channels, height, width]` or\n        `[batch, height, width, channels]` depending on `data_format`.\n    kernel_size: `int` kernel size to be used for `conv2d` or max_pool2d`\n        operations. Should be a positive integer.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    A padded `Tensor` of the same `data_format` with size either intact\n    (if `kernel_size == 1`) or padded (if `kernel_size > 1`).\n  """"""\n  pad_total = kernel_size - 1\n  pad_beg = pad_total // 2\n  pad_end = pad_total - pad_beg\n  if data_format == \'channels_first\':\n    padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                    [pad_beg, pad_end], [pad_beg, pad_end]])\n  else:\n    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                    [pad_beg, pad_end], [0, 0]])\n\n  return padded_inputs\n\n\ndef conv2d_fixed_padding(inputs, filters, kernel_size, strides,\n                         data_format=\'channels_first\'):\n  """"""Strided 2-D convolution with explicit padding.\n\n  The padding is consistent and is based only on `kernel_size`, not on the\n  dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n\n  Args:\n    inputs: `Tensor` of size `[batch, channels, height_in, width_in]`.\n    filters: `int` number of filters in the convolution.\n    kernel_size: `int` size of the kernel to be used in the convolution.\n    strides: `int` strides of the convolution.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    A `Tensor` of shape `[batch, filters, height_out, width_out]`.\n  """"""\n  if strides > 1:\n    inputs = fixed_padding(inputs, kernel_size, data_format=data_format)\n\n  return tf.layers.conv2d(\n      inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n      padding=(\'SAME\' if strides == 1 else \'VALID\'), use_bias=False,\n      kernel_initializer=tf.variance_scaling_initializer(),\n      data_format=data_format)\n\n\ndef residual_block(inputs, filters, is_training, strides,\n                   use_projection=False, data_format=\'channels_first\'):\n  """"""Standard building block for residual networks with BN after convolutions.\n\n  Args:\n    inputs: `Tensor` of size `[batch, channels, height, width]`.\n    filters: `int` number of filters for the first two convolutions. Note that\n        the third and final convolution will use 4 times as many filters.\n    is_training: `bool` for whether the model is in training.\n    strides: `int` block stride. If greater than 1, this block will ultimately\n        downsample the input.\n    use_projection: `bool` for whether this block should use a projection\n        shortcut (versus the default identity shortcut). This is usually `True`\n        for the first block of a block group, which may change the number of\n        filters and the resolution.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    The output `Tensor` of the block.\n  """"""\n  shortcut = inputs\n  if use_projection:\n    # Projection shortcut in first layer to match filters and strides\n    shortcut = conv2d_fixed_padding(\n        inputs=inputs, filters=filters, kernel_size=1, strides=strides,\n        data_format=data_format)\n    shortcut = batch_norm_relu(shortcut, is_training, relu=False,\n                               data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n      data_format=data_format)\n  inputs = batch_norm_relu(inputs, is_training, data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=3, strides=1,\n      data_format=data_format)\n  inputs = batch_norm_relu(inputs, is_training, relu=False, init_zero=True,\n                           data_format=data_format)\n\n  return tf.nn.relu(inputs + shortcut)\n\n\ndef bottleneck_block(inputs, filters, is_training, strides,\n                     use_projection=False, data_format=\'channels_first\'):\n  """"""Bottleneck block variant for residual networks with BN after convolutions.\n\n  Args:\n    inputs: `Tensor` of size `[batch, channels, height, width]`.\n    filters: `int` number of filters for the first two convolutions. Note that\n        the third and final convolution will use 4 times as many filters.\n    is_training: `bool` for whether the model is in training.\n    strides: `int` block stride. If greater than 1, this block will ultimately\n        downsample the input.\n    use_projection: `bool` for whether this block should use a projection\n        shortcut (versus the default identity shortcut). This is usually `True`\n        for the first block of a block group, which may change the number of\n        filters and the resolution.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    The output `Tensor` of the block.\n  """"""\n  shortcut = inputs\n  if use_projection:\n    # Projection shortcut only in first block within a group. Bottleneck blocks\n    # end with 4 times the number of filters.\n    filters_out = 4 * filters\n    shortcut = conv2d_fixed_padding(\n        inputs=inputs, filters=filters_out, kernel_size=1, strides=strides,\n        data_format=data_format)\n    shortcut = batch_norm_relu(shortcut, is_training, relu=False,\n                               data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=1, strides=1,\n      data_format=data_format)\n  inputs = batch_norm_relu(inputs, is_training, data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n      data_format=data_format)\n  inputs = batch_norm_relu(inputs, is_training, data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=4 * filters, kernel_size=1, strides=1,\n      data_format=data_format)\n  inputs = batch_norm_relu(inputs, is_training, relu=False, init_zero=True,\n                           data_format=data_format)\n\n  return tf.nn.relu(inputs + shortcut)\n\n\ndef block_group(inputs, filters, block_fn, blocks, strides, is_training, name,\n                data_format=\'channels_first\'):\n  """"""Creates one group of blocks for the ResNet model.\n\n  Args:\n    inputs: `Tensor` of size `[batch, channels, height, width]`.\n    filters: `int` number of filters for the first convolution of the layer.\n    block_fn: `function` for the block to use within the model\n    blocks: `int` number of blocks contained in the layer.\n    strides: `int` stride to use for the first convolution of the layer. If\n        greater than 1, this layer will downsample the input.\n    is_training: `bool` for whether the model is training.\n    name: `str`name for the Tensor output of the block layer.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    The output `Tensor` of the block layer.\n  """"""\n  # Only the first block per block_group uses projection shortcut and strides.\n  inputs = block_fn(inputs, filters, is_training, strides,\n                    use_projection=True, data_format=data_format)\n\n  for _ in range(1, blocks):\n    inputs = block_fn(inputs, filters, is_training, 1,\n                      data_format=data_format)\n\n  return tf.identity(inputs, name)\n\n\ndef resnet_v1_generator(block_fn, layers, num_classes,\n                        data_format=\'channels_first\'):\n  """"""Generator for ResNet v1 models.\n\n  Args:\n    block_fn: `function` for the block to use within the model. Either\n        `residual_block` or `bottleneck_block`.\n    layers: list of 4 `int`s denoting the number of blocks to include in each\n      of the 4 block groups. Each group consists of blocks that take inputs of\n      the same resolution.\n    num_classes: `int` number of possible classes for image classification.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    Model `function` that takes in `inputs` and `is_training` and returns the\n    output `Tensor` of the ResNet model.\n  """"""\n  def model(inputs, is_training):\n    """"""Creation of the model graph.""""""\n    inputs = conv2d_fixed_padding(\n        inputs=inputs, filters=64, kernel_size=7, strides=2,\n        data_format=data_format)\n    inputs = tf.identity(inputs, \'initial_conv\')\n    inputs = batch_norm_relu(inputs, is_training, data_format=data_format)\n\n    inputs = tf.layers.max_pooling2d(\n        inputs=inputs, pool_size=3, strides=2, padding=\'SAME\',\n        data_format=data_format)\n    inputs = tf.identity(inputs, \'initial_max_pool\')\n\n    inputs = block_group(\n        inputs=inputs, filters=64, block_fn=block_fn, blocks=layers[0],\n        strides=1, is_training=is_training, name=\'block_group1\',\n        data_format=data_format)\n    inputs = block_group(\n        inputs=inputs, filters=128, block_fn=block_fn, blocks=layers[1],\n        strides=2, is_training=is_training, name=\'block_group2\',\n        data_format=data_format)\n    inputs = block_group(\n        inputs=inputs, filters=256, block_fn=block_fn, blocks=layers[2],\n        strides=2, is_training=is_training, name=\'block_group3\',\n        data_format=data_format)\n    inputs = block_group(\n        inputs=inputs, filters=512, block_fn=block_fn, blocks=layers[3],\n        strides=2, is_training=is_training, name=\'block_group4\',\n        data_format=data_format)\n\n    # The activation is 7x7 so this is a global average pool.\n    # TODO(huangyp): reduce_mean will be faster.\n    pool_size = (inputs.shape[1], inputs.shape[2])\n    inputs = tf.layers.average_pooling2d(\n        inputs=inputs, pool_size=pool_size, strides=1, padding=\'VALID\',\n        data_format=data_format)\n    inputs = tf.identity(inputs, \'final_avg_pool\')\n    inputs = tf.reshape(\n        inputs, [-1, 2048 if block_fn is bottleneck_block else 512])\n    inputs = tf.layers.dense(\n        inputs=inputs,\n        units=num_classes,\n        kernel_initializer=tf.random_normal_initializer(stddev=.01))\n    inputs = tf.identity(inputs, \'final_dense\')\n    return inputs\n\n  model.default_image_size = 224\n  return model\n\n\ndef resnet_v1(resnet_depth, num_classes, data_format=\'channels_first\'):\n  """"""Returns the ResNet model for a given size and number of output classes.""""""\n  model_params = {\n      18: {\'block\': residual_block, \'layers\': [2, 2, 2, 2]},\n      34: {\'block\': residual_block, \'layers\': [3, 4, 6, 3]},\n      50: {\'block\': bottleneck_block, \'layers\': [3, 4, 6, 3]},\n      101: {\'block\': bottleneck_block, \'layers\': [3, 4, 23, 3]},\n      152: {\'block\': bottleneck_block, \'layers\': [3, 8, 36, 3]},\n      200: {\'block\': bottleneck_block, \'layers\': [3, 24, 36, 3]}\n  }\n\n  if resnet_depth not in model_params:\n    raise ValueError(\'Not a valid resnet_depth:\', resnet_depth)\n\n  params = model_params[resnet_depth]\n  return resnet_v1_generator(\n      params[\'block\'], params[\'layers\'], num_classes, data_format)\n'"
tpu/models/experimental/distribution_strategy/resnet_preprocessing.py,30,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""ImageNet preprocessing for ResNet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nIMAGE_SIZE = 224\nCROP_PADDING = 32\n\n\ndef distorted_bounding_box_crop(image_bytes,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image_bytes: `Tensor` of binary image data.\n    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n        where each coordinate is [0, 1) and the coordinates are arranged\n        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n        image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n        area of the image must contain at least this fraction of any bounding\n        box supplied.\n    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n        image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `float`s. The cropped area of the image\n        must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n        region of the image of the specified constraints. After `max_attempts`\n        failures, return the entire image.\n    scope: Optional `str` for name scope.\n  Returns:\n    cropped image `Tensor`\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image_bytes, bbox]):\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        shape,\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n    target_height, target_width, _ = tf.unstack(bbox_size)\n    crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n\n    return image\n\n\ndef _at_least_x_are_equal(a, b, x):\n  """"""At least `x` of `a` and `b` `Tensors` are equal.""""""\n  match = tf.equal(a, b)\n  match = tf.cast(match, tf.int32)\n  return tf.greater_equal(tf.reduce_sum(match), x)\n\n\ndef _decode_and_random_crop(image_bytes, image_size):\n  """"""Make a random crop of image_size.""""""\n  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n  image = distorted_bounding_box_crop(\n      image_bytes,\n      bbox,\n      min_object_covered=0.1,\n      aspect_ratio_range=(3. / 4, 4. / 3.),\n      area_range=(0.08, 1.0),\n      max_attempts=10,\n      scope=None)\n  original_shape = tf.image.extract_jpeg_shape(image_bytes)\n  bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)\n\n  image = tf.cond(\n      bad,\n      lambda: _decode_and_center_crop(image_bytes, image_size),\n      lambda: tf.image.resize_bicubic([image],  # pylint: disable=g-long-lambda\n                                      [image_size, image_size])[0])\n\n  return image\n\n\ndef _decode_and_center_crop(image_bytes, image_size):\n  """"""Crops to center of image with padding then scales image_size.""""""\n  shape = tf.image.extract_jpeg_shape(image_bytes)\n  image_height = shape[0]\n  image_width = shape[1]\n\n  padded_center_crop_size = tf.cast(\n      ((image_size / (image_size + CROP_PADDING)) *\n       tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n      tf.int32)\n\n  offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n  offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n  crop_window = tf.stack([offset_height, offset_width,\n                          padded_center_crop_size, padded_center_crop_size])\n  image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n  image = tf.image.resize_bicubic([image], [image_size, image_size])[0]\n\n  return image\n\n\ndef _flip(image):\n  """"""Random horizontal image flip.""""""\n  image = tf.image.random_flip_left_right(image)\n  return image\n\n\ndef preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.\n    use_bfloat16: `bool` for whether to use bfloat16.\n    image_size: image size.\n\n  Returns:\n    A preprocessed image `Tensor`.\n  """"""\n  image = _decode_and_random_crop(image_bytes, image_size)\n  image = _flip(image)\n  image = tf.reshape(image, [image_size, image_size, 3])\n  image = tf.image.convert_image_dtype(\n      image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n  return image\n\n\ndef preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.\n    use_bfloat16: `bool` for whether to use bfloat16.\n    image_size: image size.\n\n  Returns:\n    A preprocessed image `Tensor`.\n  """"""\n  image = _decode_and_center_crop(image_bytes, image_size)\n  image = tf.reshape(image, [image_size, image_size, 3])\n  image = tf.image.convert_image_dtype(\n      image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n  return image\n\n\ndef preprocess_image(image_bytes, is_training=False, use_bfloat16=False,\n      image_size=IMAGE_SIZE):\n  """"""Preprocesses the given image.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.\n    is_training: `bool` for whether the preprocessing is for training.\n    use_bfloat16: `bool` for whether to use bfloat16.\n    image_size: image size.\n\n  Returns:\n    A preprocessed image `Tensor` with value range of [0, 255].\n  """"""\n  if is_training:\n    return preprocess_for_train(image_bytes, use_bfloat16, image_size)\n  else:\n    return preprocess_for_eval(image_bytes, use_bfloat16, image_size)\n'"
tpu/models/experimental/inception/imagenet.py,21,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the ImageNet ILSVRC 2012 Dataset plus some bounding boxes.\n\nSome images have one or more bounding boxes associated with the label of the\nimage. See details here: http://image-net.org/download-bboxes\n\nImageNet is based upon WordNet 3.0. To uniquely identify a synset, we use\n""WordNet ID"" (wnid), which is a concatenation of POS ( i.e. part of speech )\nand SYNSET OFFSET of WordNet. For more information, please refer to the\nWordNet documentation[http://wordnet.princeton.edu/wordnet/documentation/].\n\n""There are bounding boxes for over 3000 popular synsets available.\nFor each synset, there are on average 150 images with bounding boxes.""\n\nWARNING: Don\'t use for object detection, in this case all the bounding boxes\nof the image belong to just one class.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n# TODO(nsilberman): Add tfrecord file type once the script is updated.\n_FILE_PATTERN = \'%s-*\'\n\n_SPLITS_TO_SIZES = {\n    \'train\': 1281167,\n    \'validation\': 50000,\n}\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying height and width.\',\n    \'label\': \'The label id of the image, integer between 0 and 999\',\n    \'label_text\': \'The text of the label.\',\n    \'object/bbox\': \'A list of bounding boxes.\',\n    \'object/label\': \'A list of labels, one per each object.\',\n}\n\n_NUM_CLASSES = 1001\n\n_KEYS_TO_FEATURES = {\n    \'image/encoded\': tf.FixedLenFeature(\n        (), tf.string, default_value=\'\'),\n    \'image/format\': tf.FixedLenFeature(\n        (), tf.string, default_value=\'jpeg\'),\n    \'image/class/label\': tf.FixedLenFeature(\n        [], dtype=tf.int64, default_value=-1),\n    \'image/class/text\': tf.FixedLenFeature(\n        [], dtype=tf.string, default_value=\'\'),\n    \'image/object/bbox/xmin\': tf.VarLenFeature(\n        dtype=tf.float32),\n    \'image/object/bbox/ymin\': tf.VarLenFeature(\n        dtype=tf.float32),\n    \'image/object/bbox/xmax\': tf.VarLenFeature(\n        dtype=tf.float32),\n    \'image/object/bbox/ymax\': tf.VarLenFeature(\n        dtype=tf.float32),\n    \'image/object/class/label\': tf.VarLenFeature(\n        dtype=tf.int64),\n}\n\n_ITEMS_TO_HANDLERS = {\n    \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n    \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n    \'label_text\': slim.tfexample_decoder.Tensor(\'image/class/text\'),\n    \'object/bbox\': slim.tfexample_decoder.BoundingBox(\n        [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\'),\n    \'object/label\': slim.tfexample_decoder.Tensor(\'image/object/class/label\'),\n}\n\n\ndef get_split_size(set_name):\n  """"""Return size of train/validation set.""""""\n  return _SPLITS_TO_SIZES.get(set_name)\n\n\ndef get_decoder():\n  return slim.tfexample_decoder.TFExampleDecoder(\n      _KEYS_TO_FEATURES, _ITEMS_TO_HANDLERS)\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None,\n              reader=None, use_slim=True):\n  """"""Gets a dataset tuple with instructions for reading ImageNet.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n    use_slim: Boolean to decide dataset type\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  if use_slim:\n    # Allowing None in signature so that dataset_factory can use the default\n    if reader is None:\n      reader = tf.TFRecordReader\n\n    dataset = slim.dataset.Dataset(\n        data_sources=file_pattern,\n        reader=reader,\n        decoder=get_decoder(),\n        num_samples=_SPLITS_TO_SIZES[split_name],\n        items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n        num_classes=_NUM_CLASSES)\n  else:\n    dataset = tf.data.Dataset.list_files(file_pattern, shuffle=False)\n\n  return dataset\n'"
tpu/models/experimental/inception/inception_preprocessing.py,69,"b'# Copyright 2016 Google. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the Inception networks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# Standard Imports\nfrom absl import flags\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import random_ops\n\n\nflags.DEFINE_float(\n    \'cb_distortion_range\', 0.1, \'Cb distortion range +/-\')\n\nflags.DEFINE_float(\n    \'cr_distortion_range\', 0.1, \'Cr distortion range +/-\')\n\nflags.DEFINE_boolean(\n    \'use_fast_color_distort\', True,\n    \'apply fast color/chroma distortion if True, else apply\'\n    \'brightness/saturation/hue/contrast distortion\')\n\nFLAGS = flags.FLAGS\n\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distort_color_fast(image, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Distort brightness and chroma values of input image\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    br_delta = random_ops.random_uniform([], -32./255., 32./255., seed=None)\n    cb_factor = random_ops.random_uniform(\n        [], -FLAGS.cb_distortion_range, FLAGS.cb_distortion_range, seed=None)\n    cr_factor = random_ops.random_uniform(\n        [], -FLAGS.cr_distortion_range, FLAGS.cr_distortion_range, seed=None)\n\n    channels = tf.split(axis=2, num_or_size_splits=3, value=image)\n    red_offset = 1.402 * cr_factor + br_delta\n    green_offset = -0.344136 * cb_factor - 0.714136 * cr_factor + br_delta\n    blue_offset = 1.772 * cb_factor + br_delta\n    channels[0] += red_offset\n    channels[1] += green_offset\n    channels[2] += blue_offset\n    image = tf.concat(axis=2, values=channels)\n    image = tf.clip_by_value(image, 0., 1.)\n\n    return image\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(3./4., 4./3.),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n      image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n      area of the image must contain at least this fraction of any bounding box\n      supplied.\n    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n      image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `floats`. The cropped area of the image\n      must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n      region of the image of the specified constraints. After `max_attempts`\n      failures, return the entire image.\n    scope: Optional scope for name_scope.\n  Returns:\n    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # A large fraction of image datasets contain a human-annotated bounding\n    # box delineating the region of the image containing the object of interest.\n    # We choose to create a new bounding box for the object which is a randomly\n    # distorted version of the human-annotated bounding box that obeys an\n    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n    # bounding box. If no box is supplied, then we assume the bounding box is\n    # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        tf.shape(image),\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n    return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         fast_mode=True,\n                         scope=None,\n                         add_image_summaries=True):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Additionally it would create image_summaries to display the different\n  transformations applied to the image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n      bi-cubic resizing, random_hue or random_contrast).\n    scope: Optional scope for name_scope.\n    add_image_summaries: Enable image summaries.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].\n  """"""\n  with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n    if bbox is None:\n      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                         dtype=tf.float32,\n                         shape=[1, 1, 4])\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    if add_image_summaries:\n      # Each bounding box has shape [1, num_boxes, box coords] and\n      # the coordinates are ordered [ymin, xmin, ymax, xmax].\n      image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                    bbox)\n      tf.summary.image(\'image_with_bounding_boxes\', image_with_box)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n    if add_image_summaries:\n      image_with_distorted_box = tf.image.draw_bounding_boxes(\n          tf.expand_dims(image, 0), distorted_bbox)\n      tf.summary.image(\'images_with_distorted_bounding_box\',\n                       image_with_distorted_box)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize_images(x, [height, width], method),\n        num_cases=num_resize_cases)\n\n    if add_image_summaries:\n      tf.summary.image(\'cropped_resized_image\',\n                       tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors. There are 1 or 4 ways to do it.\n    if FLAGS.use_fast_color_distort:\n      distorted_image = distort_color_fast(distorted_image)\n    else:\n      num_distort_cases = 1 if fast_mode else 4\n      distorted_image = apply_with_random_selector(\n          distorted_image,\n          lambda x, ordering: distort_color(x, ordering, fast_mode),\n          num_cases=num_distort_cases)\n\n    if add_image_summaries:\n      tf.summary.image(\'final_distorted_image\',\n                       tf.expand_dims(distorted_image, 0))\n    distorted_image = tf.subtract(distorted_image, 0.5)\n    distorted_image = tf.multiply(distorted_image, 2.0)\n    return distorted_image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n  """"""Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n\n  If central_fraction is specified it would crop the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n  with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n      # Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(image, [height, width],\n                                       align_corners=False)\n      image = tf.squeeze(image, [0])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    image.set_shape([height, width, 3])\n    return image\n\n\ndef preprocess_image(image, output_height, output_width,\n                     is_training=False,\n                     bbox=None,\n                     fast_mode=True,\n                     add_image_summaries=False):\n  """"""Pre-process one image for training or evaluation.\n\n  Args:\n    image: 3-D Tensor [height, width, channels] with the image. If dtype is\n      tf.float32 then the range should be [0, 1], otherwise it would converted\n      to tf.float32 assuming that the range is [0, MAX], where MAX is largest\n      positive representable number for int(8/16/32) data type (see\n      `tf.image.convert_image_dtype` for details).\n    output_height: integer, image expected height.\n    output_width: integer, image expected width.\n    is_training: Boolean. If true it would transform an image for train,\n      otherwise it would transform it for evaluation.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations.\n    add_image_summaries: Enable image summaries.\n\n  Returns:\n    3-D float Tensor containing an appropriately scaled image\n\n  Raises:\n    ValueError: if user does not provide bounding box\n  """"""\n  if is_training:\n    return preprocess_for_train(image, output_height, output_width, bbox,\n                                fast_mode,\n                                add_image_summaries=add_image_summaries)\n  else:\n    return preprocess_for_eval(image, output_height, output_width)\n'"
tpu/models/experimental/inception/inception_v2.py,102,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Open-source TensorFlow Inception v2 Example.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nfrom absl import flags\nimport absl.logging as _logging  # pylint: disable=unused-import\nimport tensorflow as tf\n\nimport inception_preprocessing\nimport inception_v2_tpu_model as inception\nimport vgg_preprocessing\n\nfrom tensorflow.contrib import summary\nfrom tensorflow.contrib.framework.python.ops import arg_scope\nfrom tensorflow.contrib.training.python.training import evaluation\n\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'tpu\', default=None,\n    help=\'The Cloud TPU to use for training. This should be either the name \'\n    \'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\')\nflags.DEFINE_string(\n    \'gcp_project\', default=None,\n    help=\'Project name for the Cloud TPU-enabled project. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\nflags.DEFINE_string(\n    \'tpu_zone\', default=None,\n    help=\'GCE zone where the Cloud TPU is located in. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\n\n# Model specific paramenters\nflags.DEFINE_string(\n    \'data_dir\', \'\',\n    \'Directory where input data is stored\')\n\nflags.DEFINE_string(\n    \'model_dir\', None,\n    \'Directory where model output is stored\')\n\nflags.DEFINE_integer(\n    \'num_shards\', 8,\n    \'Number of shards (workers).\')\n\nflags.DEFINE_integer(\n    \'iterations\', 100,\n    \'Number of iterations per TPU training loop.\')\n\nflags.DEFINE_integer(\n    \'train_batch_size\', 1024,\n    \'Global (not per-shard) batch size for training\')\n\nflags.DEFINE_integer(\n    \'eval_total_size\', 0,\n    \'Total batch size for evaluation, use the entire validation set if 0\')\n\nflags.DEFINE_integer(\n    \'eval_batch_size\', 1024,\n    \'Global (not per-shard) batch size for evaluation\')\n\nflags.DEFINE_integer(\n    \'train_steps\', 200000,\n    \'Number of steps use for training.\')\n\nflags.DEFINE_integer(\n    \'train_steps_per_eval\', 2000,\n    \'Number of training steps to run between evaluations.\')\n\nflags.DEFINE_string(\n    \'mode\', \'train_and_eval\',\n    \'Mode to run: train, eval, train_and_eval\')\n\nflags.DEFINE_integer(\n    \'min_eval_interval\', 180,\n    \'Minimum number of seconds between evaluations\')\n\nflags.DEFINE_integer(\n    \'eval_timeout\', None,\n    \'Evaluation timeout: Maximum number of seconds that \'\n    \'may elapse while no new checkpoints are observed\')\n\nflags.DEFINE_bool(\n    \'use_tpu\', True,\n    \'Use TPUs rather than plain CPUs\')\n\nflags.DEFINE_boolean(\n    \'per_host_input_for_training\', True,\n    \'If true, input_fn is invoked per host rather than per shard.\')\n\nflags.DEFINE_string(\n    \'use_data\', \'real\',\n    \'One of ""fake"",""real""\')\n\nflags.DEFINE_float(\n    \'learning_rate\', 0.165,\n    \'Learning rate.\')\n\nflags.DEFINE_string(\n    \'optimizer\', \'RMS\',\n    \'Optimizer (one of sgd, RMS, momentum)\')\n\nflags.DEFINE_integer(\n    \'num_classes\', 1001,\n    \'Number of classes to distinguish\')\n\nflags.DEFINE_integer(\n    \'width\', 224,\n    \'Width of input image\')\n\nflags.DEFINE_integer(\n    \'height\', 224,\n    \'Height of input image\')\n\nflags.DEFINE_bool(\n    \'transpose_enabled\', False,\n    \'Boolean to enable/disable explicit I/O transpose\')\n\nflags.DEFINE_bool(\n    \'log_device_placement\', False,\n    \'Boolean to enable/disable log device placement\')\n\nflags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'Number of steps which must have run before showing summaries.\')\n\nflags.DEFINE_integer(\n    \'save_checkpoints_secs\', 1000,\n    \'Interval (in seconds) at which the model data \'\n    \'should be checkpointed. Set to 0 to disable.\')\n\nflags.DEFINE_bool(\n    \'moving_average\', True,\n    \'Whether to enable moving average computation on variables\')\n\nflags.DEFINE_string(\n    \'preprocessing\', \'inception\',\n    \'Preprocessing stage to use: one of inception or vgg\')\n\nflags.DEFINE_bool(\n    \'use_annotated_bbox\', False,\n    \'If true, use annotated bounding box as input to cropping function, \'\n    \'else use full image size\')\n\nflags.DEFINE_float(\n    \'learning_rate_decay\', 0.94,\n    \'Exponential decay rate used in learning rate adjustment\')\n\nflags.DEFINE_integer(\n    \'learning_rate_decay_epochs\', 3,\n    \'Exponential decay epochs used in learning rate adjustment\')\n\nflags.DEFINE_bool(\n    \'display_tensors\', False,\n    \'Whether to dump prediction tensors for comparison\')\n\nflags.DEFINE_bool(\n    \'clear_update_collections\', True,\n    \'Set batchnorm update_collections to None if true, else use default value\')\n\nflags.DEFINE_integer(\n    \'cold_epochs\', 2,\n    \'Number of epochs using cold learning rate\')\n\nflags.DEFINE_integer(\n    \'warmup_epochs\', 7,\n    \'Number of epochs using linearly increasing learning rate\')\n\nflags.DEFINE_bool(\n    \'use_learning_rate_warmup\', False,\n    \'Apply learning rate warmup if true\')\n\n# Dataset specific paramenters\nflags.DEFINE_bool(\n    \'prefetch_enabled\', True,\n    \'Boolean to enable/disable prefetching\')\n\nflags.DEFINE_integer(\n    \'prefetch_dataset_buffer_size\', 8*1024*1024,\n    \'Number of bytes in read buffer. 0 means no buffering.\')\n\nflags.DEFINE_integer(\n    \'num_files_infeed\', 8,\n    \'Number of training files to read in parallel.\')\n\nflags.DEFINE_integer(\n    \'num_parallel_calls\', 64,\n    \'Number of elements to process in parallel (by mapper)\')\n\nflags.DEFINE_integer(\n    \'initial_shuffle_buffer_size\', 1024,\n    \'Number of elements from dataset that shuffler will sample from. \'\n    \'This shuffling is done before any other operations. \'\n    \'Set to 0 to disable\')\n\nflags.DEFINE_integer(\n    \'followup_shuffle_buffer_size\', 1000,\n    \'Number of elements from dataset that shuffler will sample from. \'\n    \'This shuffling is done after prefetching is done. \'\n    \'Set to 0 to disable\')\n\n\nFLAGS = flags.FLAGS\n\n# Dataset constants\n_NUM_TRAIN_IMAGES = 1281167\n_NUM_EVAL_IMAGES = 50000\n\n# Random cropping constants\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n# Constants dictating the learning rate schedule.\nRMSPROP_DECAY = 0.9                # Decay term for RMSProp.\nRMSPROP_MOMENTUM = 0.9             # Momentum in RMSProp.\nRMSPROP_EPSILON = 1.0              # Epsilon term for RMSProp.\n\n# Constants dictating moving average.\nMOVING_AVERAGE_DECAY = 0.995\n\n# Batchnorm moving mean/variance parameters\nBATCH_NORM_DECAY = 0.996\nBATCH_NORM_EPSILON = 1e-3\n\n\nclass InputPipeline(object):\n  """"""Generates ImageNet input_fn for training or evaluation.\n\n  The training data is assumed to be in TFRecord format with keys as specified\n  in the dataset_parser below, sharded across 1024 files, named sequentially:\n      train-00000-of-01024\n      train-00001-of-01024\n      ...\n      train-01023-of-01024\n\n  The validation data is in the same format but sharded in 128 files.\n\n  The format of the data required is created by the script at:\n      https://github.com/tensorflow/tpu/blob/master/tools/datasets/imagenet_to_gcs.py\n\n  Args:\n    is_training: `bool` for whether the input is for training\n  """"""\n\n  def __init__(self, is_training, data_dir):\n    self.is_training = is_training\n    self.data_dir = data_dir\n\n  def dataset_parser(self, serialized_proto):\n    """"""Parse an Imagenet record from value.""""""\n    keys_to_features = {\n        \'image/encoded\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\':\n            tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/class/label\':\n            tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),\n        \'image/class/text\':\n            tf.FixedLenFeature([], dtype=tf.string, default_value=\'\'),\n        \'image/object/bbox/xmin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/class/label\':\n            tf.VarLenFeature(dtype=tf.int64),\n    }\n\n    features = tf.parse_single_example(serialized_proto, keys_to_features)\n\n    bbox = None\n    if FLAGS.use_annotated_bbox:\n      xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n      ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n      xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n      ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n      # Note that we impose an ordering of (y, x) just to make life difficult.\n      bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n      # Force the variable number of bounding boxes into the shape\n      # [1, num_boxes, coords].\n      bbox = tf.expand_dims(bbox, 0)\n      bbox = tf.transpose(bbox, [0, 2, 1])\n\n    image = features[\'image/encoded\']\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    if FLAGS.preprocessing == \'vgg\':\n      image = vgg_preprocessing.preprocess_image(\n          image=image,\n          output_height=FLAGS.height,\n          output_width=FLAGS.width,\n          is_training=self.is_training,\n          resize_side_min=_RESIZE_SIDE_MIN,\n          resize_side_max=_RESIZE_SIDE_MAX)\n    elif FLAGS.preprocessing == \'inception\':\n      image = inception_preprocessing.preprocess_image(\n          image=image,\n          output_height=FLAGS.height,\n          output_width=FLAGS.width,\n          is_training=self.is_training,\n          bbox=bbox)\n\n    label = tf.cast(\n        tf.reshape(features[\'image/class/label\'], shape=[]), dtype=tf.int32)\n\n    return image, label\n\n  def input_fn(self, params):\n    """"""Input function which provides a single batch for train or eval.\n\n    Args:\n      params: `dict` of parameters passed from the `TPUEstimator`.\n          `params[\'batch_size\']` is always provided and should be used as the\n          effective batch size.\n\n    Returns:\n      A (images, labels) tuple of `Tensor`s for a batch of samples.\n    """"""\n    batch_size = params[\'batch_size\']\n\n    if FLAGS.use_data == \'real\':\n      file_pattern = os.path.join(\n          self.data_dir, \'train-*\' if self.is_training else \'validation-*\')\n      dataset = tf.data.Dataset.list_files(file_pattern,\n                                           shuffle=self.is_training)\n\n      if self.is_training:\n        dataset = dataset.repeat()\n\n      def prefetch_dataset(filename):\n        dataset = tf.data.TFRecordDataset(\n            filename, buffer_size=FLAGS.prefetch_dataset_buffer_size)\n        return dataset\n\n      dataset = dataset.apply(\n          tf.contrib.data.parallel_interleave(\n              prefetch_dataset,\n              cycle_length=FLAGS.num_files_infeed,\n              sloppy=True))\n\n      if FLAGS.followup_shuffle_buffer_size > 0:\n        dataset = dataset.shuffle(\n            buffer_size=FLAGS.followup_shuffle_buffer_size)\n\n      dataset = dataset.map(\n          self.dataset_parser,\n          num_parallel_calls=FLAGS.num_parallel_calls)\n\n      dataset = dataset.prefetch(batch_size)\n\n      dataset = dataset.apply(\n          tf.contrib.data.batch_and_drop_remainder(batch_size))\n\n      dataset = dataset.prefetch(2)  # Prefetch overlaps in-feed with training\n\n      images, labels = dataset.make_one_shot_iterator().get_next()\n    else:\n      images = tf.random_uniform(\n          [batch_size, FLAGS.height, FLAGS.width, 3], minval=-1, maxval=1)\n      labels = tf.random_uniform(\n          [batch_size], minval=0, maxval=999, dtype=tf.int32)\n\n    images = tensor_transform_fn(images, params[\'output_perm\'])\n    return images, labels\n\n\ndef tensor_transform_fn(data, perm):\n  """"""Transpose function.\n\n  This function is used to transpose an image tensor on the host and then\n  perform an inverse transpose on the TPU. The transpose on the TPU gets\n  effectively elided thus voiding any associated computational cost.\n\n  NOTE: Eventually the compiler will be able to detect when this kind of\n  operation may prove beneficial and perform these types of transformations\n  implicitly, voiding the need for user intervention\n\n  Args:\n    data: Tensor to be transposed\n    perm: New ordering of dimensions\n\n  Returns:\n    Transposed tensor\n  """"""\n  if FLAGS.transpose_enabled:\n    return tf.transpose(data, perm)\n  return data\n\n\ndef inception_model_fn(features, labels, mode, params):\n  """"""Inception v2 model using Estimator API.""""""\n  num_classes = FLAGS.num_classes\n  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n  is_eval = (mode == tf.estimator.ModeKeys.EVAL)\n  features = tensor_transform_fn(features, params[\'input_perm\'])\n\n  if FLAGS.clear_update_collections:\n    # updates_collections must be set to None in order to use fused batchnorm\n    with arg_scope(inception.inception_v2_arg_scope(\n        batch_norm_decay=BATCH_NORM_DECAY,\n        batch_norm_epsilon=BATCH_NORM_EPSILON,\n        updates_collections=None)):\n      logits, end_points = inception.inception_v2(\n          features,\n          num_classes,\n          is_training=is_training,\n          replace_separable_convolution=True)\n  else:\n    with arg_scope(inception.inception_v2_arg_scope(\n        batch_norm_decay=BATCH_NORM_DECAY,\n        batch_norm_epsilon=BATCH_NORM_EPSILON)):\n      logits, end_points = inception.inception_v2(\n          features,\n          num_classes,\n          is_training=is_training,\n          replace_separable_convolution=True)\n\n  predictions = end_points\n  predictions.update({\n      \'classes\': tf.argmax(input=logits, axis=1),\n      \'probabilities\': tf.nn.softmax(logits, name=\'softmax_tensor\')\n  })\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n  if mode == tf.estimator.ModeKeys.EVAL and FLAGS.display_tensors and (\n      not FLAGS.use_tpu):\n    with tf.control_dependencies([\n        tf.Print(\n            predictions[\'classes\'], [predictions[\'classes\']],\n            summarize=FLAGS.eval_batch_size,\n            message=\'prediction: \')\n    ]):\n      labels = tf.Print(\n          labels, [labels], summarize=FLAGS.eval_batch_size, message=\'label: \')\n\n  one_hot_labels = tf.one_hot(labels, FLAGS.num_classes, dtype=tf.int32)\n\n  tf.losses.softmax_cross_entropy(\n      onehot_labels=one_hot_labels,\n      logits=logits,\n      weights=1.0,\n      label_smoothing=0.1)\n  loss = tf.losses.get_total_loss(add_regularization_losses=True)\n\n  initial_learning_rate = FLAGS.learning_rate * FLAGS.train_batch_size / 256\n  if FLAGS.use_learning_rate_warmup:\n    # Adjust initial learning rate to match final warmup rate\n    warmup_decay = FLAGS.learning_rate_decay**(\n        (FLAGS.warmup_epochs + FLAGS.cold_epochs) /\n        FLAGS.learning_rate_decay_epochs)\n    adj_initial_learning_rate = initial_learning_rate * warmup_decay\n\n  final_learning_rate = 0.0001 * initial_learning_rate\n\n  host_call = None\n  train_op = None\n  if is_training:\n    batches_per_epoch = _NUM_TRAIN_IMAGES / FLAGS.train_batch_size\n    global_step = tf.train.get_or_create_global_step()\n    current_epoch = tf.cast(\n        (tf.cast(global_step, tf.float32) / batches_per_epoch), tf.int32)\n\n    learning_rate = tf.train.exponential_decay(\n        learning_rate=initial_learning_rate,\n        global_step=global_step,\n        decay_steps=int(FLAGS.learning_rate_decay_epochs * batches_per_epoch),\n        decay_rate=FLAGS.learning_rate_decay,\n        staircase=True)\n\n    if FLAGS.use_learning_rate_warmup:\n      wlr = 0.1 * adj_initial_learning_rate\n      wlr_height = tf.cast(\n          0.9 * adj_initial_learning_rate /\n          (FLAGS.warmup_epochs + FLAGS.learning_rate_decay_epochs - 1),\n          tf.float32)\n      epoch_offset = tf.cast(FLAGS.cold_epochs - 1, tf.int32)\n      exp_decay_start = (FLAGS.warmup_epochs + FLAGS.cold_epochs +\n                         FLAGS.learning_rate_decay_epochs)\n      lin_inc_lr = tf.add(\n          wlr, tf.multiply(\n              tf.cast(tf.subtract(current_epoch, epoch_offset), tf.float32),\n              wlr_height))\n      learning_rate = tf.where(\n          tf.greater_equal(current_epoch, FLAGS.cold_epochs),\n          (tf.where(tf.greater_equal(current_epoch, exp_decay_start),\n                    learning_rate, lin_inc_lr)),\n          wlr)\n\n    # Set a minimum boundary for the learning rate.\n    learning_rate = tf.maximum(\n        learning_rate, final_learning_rate, name=\'learning_rate\')\n\n    if FLAGS.optimizer == \'sgd\':\n      tf.logging.info(\'Using SGD optimizer\')\n      optimizer = tf.train.GradientDescentOptimizer(\n          learning_rate=learning_rate)\n    elif FLAGS.optimizer == \'momentum\':\n      tf.logging.info(\'Using Momentum optimizer\')\n      optimizer = tf.train.MomentumOptimizer(\n          learning_rate=learning_rate, momentum=0.9)\n    elif FLAGS.optimizer == \'RMS\':\n      tf.logging.info(\'Using RMS optimizer\')\n      optimizer = tf.train.RMSPropOptimizer(\n          learning_rate,\n          RMSPROP_DECAY,\n          momentum=RMSPROP_MOMENTUM,\n          epsilon=RMSPROP_EPSILON)\n    else:\n      tf.logging.fatal(\'Unknown optimizer:\', FLAGS.optimizer)\n\n    if FLAGS.use_tpu:\n      optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.minimize(loss, global_step=global_step)\n    if FLAGS.moving_average:\n      ema = tf.train.ExponentialMovingAverage(\n          decay=MOVING_AVERAGE_DECAY, num_updates=global_step)\n      variables_to_average = (\n          tf.trainable_variables() + tf.moving_average_variables())\n      with tf.control_dependencies([train_op]), tf.name_scope(\'moving_average\'):\n        train_op = ema.apply(variables_to_average)\n\n    # To log the loss, current learning rate, and epoch for Tensorboard, the\n    # summary op needs to be run on the host CPU via host_call. host_call\n    # expects [batch_size, ...] Tensors, thus reshape to introduce a batch\n    # dimension. These Tensors are implicitly concatenated to\n    # [params[\'batch_size\']].\n    gs_t = tf.reshape(global_step, [1])\n    loss_t = tf.reshape(loss, [1])\n    lr_t = tf.reshape(learning_rate, [1])\n    ce_t = tf.reshape(current_epoch, [1])\n\n    def host_call_fn(gs, loss, lr, ce):\n      """"""Training host call. Creates scalar summaries for training metrics.\n\n      This function is executed on the CPU and should not directly reference\n      any Tensors in the rest of the `model_fn`. To pass Tensors from the model\n      to the `metric_fn`, provide as part of the `host_call`. See\n      https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n      for more information.\n\n      Arguments should match the list of `Tensor` objects passed as the second\n      element in the tuple passed to `host_call`.\n\n      Args:\n        gs: `Tensor with shape `[batch]` for the global_step\n        loss: `Tensor` with shape `[batch]` for the training loss.\n        lr: `Tensor` with shape `[batch]` for the learning_rate.\n        ce: `Tensor` with shape `[batch]` for the current_epoch.\n\n      Returns:\n        List of summary ops to run on the CPU host.\n      """"""\n      gs = gs[0]\n      with summary.create_file_writer(FLAGS.model_dir).as_default():\n        with summary.always_record_summaries():\n          summary.scalar(\'loss\', tf.reduce_mean(loss), step=gs)\n          summary.scalar(\'learning_rate\', tf.reduce_mean(lr), step=gs)\n          summary.scalar(\'current_epoch\', tf.reduce_mean(ce), step=gs)\n\n          return summary.all_summary_ops()\n\n    host_call = (host_call_fn, [gs_t, loss_t, lr_t, ce_t])\n\n  eval_metrics = None\n  if is_eval:\n    def metric_fn(labels, logits):\n      """"""Evaluation metric function. Evaluates accuracy.\n\n      This function is executed on the CPU and should not directly reference\n      any Tensors in the rest of the `model_fn`. To pass Tensors from the model\n      to the `metric_fn`, provide as part of the `eval_metrics`. See\n      https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n      for more information.\n\n      Arguments should match the list of `Tensor` objects passed as the second\n      element in the tuple passed to `eval_metrics`.\n\n      Args:\n        labels: `Tensor` with shape `[batch, ]`.\n        logits: `Tensor` with shape `[batch, num_classes]`.\n\n      Returns:\n        A dict of the metrics to return from evaluation.\n      """"""\n      predictions = tf.argmax(logits, axis=1)\n      top_1_accuracy = tf.metrics.accuracy(labels, predictions)\n      in_top_5 = tf.cast(tf.nn.in_top_k(logits, labels, 5), tf.float32)\n      top_5_accuracy = tf.metrics.mean(in_top_5)\n\n      return {\n          \'accuracy\': top_1_accuracy,\n          \'accuracy@5\': top_5_accuracy,\n      }\n\n    eval_metrics = (metric_fn, [labels, logits])\n\n  return tf.contrib.tpu.TPUEstimatorSpec(\n      mode=mode,\n      loss=loss,\n      train_op=train_op,\n      host_call=host_call,\n      eval_metrics=eval_metrics)\n\n\nclass LoadEMAHook(tf.train.SessionRunHook):\n  """"""Hook to load exponential moving averages into corresponding variables.""""""\n\n  def __init__(self, model_dir):\n    super(LoadEMAHook, self).__init__()\n    self._model_dir = model_dir\n\n  def begin(self):\n    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n    variables_to_restore = ema.variables_to_restore()\n    self._load_ema = tf.contrib.framework.assign_from_checkpoint_fn(\n        tf.train.latest_checkpoint(self._model_dir), variables_to_restore)\n\n  def after_create_session(self, sess, coord):\n    tf.logging.info(\'Reloading EMA...\')\n    self._load_ema(sess)\n\n\ndef main(unused_argv):\n  del unused_argv  # Unused\n\n  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n      FLAGS.tpu,\n      zone=FLAGS.tpu_zone,\n      project=FLAGS.gcp_project)\n\n  params = {\n      \'input_perm\': [0, 1, 2, 3],\n      \'output_perm\': [0, 1, 2, 3],\n  }\n\n  batch_axis = 0\n  if FLAGS.transpose_enabled:\n    params[\'input_perm\'] = [3, 0, 1, 2]\n    params[\'output_perm\'] = [1, 2, 3, 0]\n    batch_axis = 3\n\n  if FLAGS.eval_total_size > 0:\n    eval_size = FLAGS.eval_total_size\n  else:\n    eval_size = _NUM_EVAL_IMAGES\n  eval_steps = eval_size // FLAGS.eval_batch_size\n\n  iterations = (eval_steps if FLAGS.mode == \'eval\' else\n                FLAGS.iterations)\n\n  eval_batch_size = (None if FLAGS.mode == \'train\' else\n                     FLAGS.eval_batch_size)\n\n  per_host_input_for_training = (\n      FLAGS.num_shards <= 8 if FLAGS.mode == \'train\' else True)\n\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_secs=FLAGS.save_checkpoints_secs,\n      save_summary_steps=FLAGS.save_summary_steps,\n      session_config=tf.ConfigProto(\n          allow_soft_placement=True,\n          log_device_placement=FLAGS.log_device_placement),\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=iterations,\n          num_shards=FLAGS.num_shards,\n          per_host_input_for_training=per_host_input_for_training))\n\n  inception_classifier = tf.contrib.tpu.TPUEstimator(\n      model_fn=inception_model_fn,\n      use_tpu=FLAGS.use_tpu,\n      config=run_config,\n      params=params,\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=eval_batch_size,\n      batch_axis=(batch_axis, 0))\n\n  # Input pipelines are slightly different (with regards to shuffling and\n  # preprocessing) between training and evaluation.\n  imagenet_train = InputPipeline(\n      is_training=True,\n      data_dir=FLAGS.data_dir)\n  imagenet_eval = InputPipeline(\n      is_training=False,\n      data_dir=FLAGS.data_dir)\n\n  if FLAGS.moving_average:\n    eval_hooks = [LoadEMAHook(FLAGS.model_dir)]\n  else:\n    eval_hooks = []\n\n  if FLAGS.mode == \'eval\':\n    # Run evaluation when there is a new checkpoint\n    for checkpoint in evaluation.checkpoints_iterator(FLAGS.model_dir):\n      tf.logging.info(\'Starting to evaluate.\')\n      try:\n        start_timestamp = time.time()  # Includes compilation time\n        eval_results = inception_classifier.evaluate(\n            input_fn=imagenet_eval.input_fn,\n            steps=eval_steps,\n            hooks=eval_hooks,\n            checkpoint_path=checkpoint)\n        elapsed_time = int(time.time() - start_timestamp)\n        tf.logging.info(\n            \'Eval results: %s. Elapsed seconds: %d\', eval_results, elapsed_time)\n\n        # Terminate eval job when final checkpoint is reached\n        current_step = int(os.path.basename(checkpoint).split(\'-\')[1])\n        if current_step >= FLAGS.train_steps:\n          tf.logging.info(\n              \'Evaluation finished after training step %d\', current_step)\n          break\n      except tf.errors.NotFoundError:\n        # Since the coordinator is on a different job than the TPU worker,\n        # sometimes the TPU worker does not finish initializing until long after\n        # the CPU job tells it to start evaluating. In this case, the checkpoint\n        # file could have been deleted already.\n        tf.logging.info(\n            \'Checkpoint %s no longer exists, skipping checkpoint\', checkpoint)\n\n  elif FLAGS.mode == \'train_and_eval\':\n    for cycle in range(FLAGS.train_steps // FLAGS.train_steps_per_eval):\n      tf.logging.info(\'Starting training cycle %d.\' % cycle)\n      inception_classifier.train(\n          input_fn=imagenet_train.input_fn, steps=FLAGS.train_steps_per_eval)\n\n      tf.logging.info(\'Starting evaluation cycle %d .\' % cycle)\n      eval_results = inception_classifier.evaluate(\n          input_fn=imagenet_eval.input_fn, steps=eval_steps, hooks=eval_hooks)\n      tf.logging.info(\'Evaluation results: %s\' % eval_results)\n\n  else:\n    tf.logging.info(\'Starting training ...\')\n    inception_classifier.train(\n        input_fn=imagenet_train.input_fn, steps=FLAGS.train_steps)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/models/experimental/inception/inception_v2_tpu_model.py,4,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v2 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport absl.logging as _logging  # pylint: disable=unused-import\n\nfrom tensorflow.contrib import layers\nfrom tensorflow.contrib.framework.python.ops import arg_scope\nfrom tensorflow.contrib.layers.python.layers import initializers\nfrom tensorflow.contrib.layers.python.layers import layers as layers_lib\nfrom tensorflow.contrib.layers.python.layers import regularizers\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import variable_scope\n\ntrunc_normal = lambda stddev: init_ops.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v2_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      replace_separable_convolution=False,\n                      scope=None):\n  """"""Inception v2 (6a2).\n\n  Constructs an Inception v2 network from inputs to the given final endpoint.\n  This method can construct the network up to the layer inception(5b) as\n  described in http://arxiv.org/abs/1502.03167.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\',\n      \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\', \'Mixed_5b\',\n      \'Mixed_5c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    replace_separable_convolution: Replace the separable convolution in the\n      layer Conv2d_1a_7x7 with a normal convolution.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with variable_scope.variable_scope(scope, \'InceptionV2\', [inputs]):\n    with arg_scope(\n        [\n            layers.conv2d, layers_lib.max_pool2d, layers_lib.avg_pool2d,\n            layers.separable_conv2d\n        ],\n        stride=1,\n        padding=\'SAME\'):\n\n      # Note that sizes in the comments below assume an input spatial size of\n      # 224x224, however, the inputs can be of any size greater 32x32.\n\n      # 224 x 224 x 3\n      end_point = \'Conv2d_1a_7x7\'\n\n      if replace_separable_convolution:\n        # Use a normal convolution instead of a separable convolution as it\n        # provides better performance on some devices.\n        net = layers.conv2d(\n            inputs,\n            depth(64), [7, 7],\n            stride=2,\n            weights_initializer=trunc_normal(1.0),\n            scope=end_point)\n      else:\n        # depthwise_multiplier here is different from depth_multiplier.\n        # depthwise_multiplier determines the output channels of the initial\n        # depthwise conv (see docs for tf.nn.separable_conv2d), while\n        # depth_multiplier controls the # channels of the subsequent 1x1\n        # convolution. Must have\n        #   in_channels * depthwise_multipler <= out_channels\n        # so that the separable convolution is not overparameterized.\n        depthwise_multiplier = min(int(depth(64) / 3), 8)\n        net = layers.separable_conv2d(\n            inputs,\n            depth(64), [7, 7],\n            depth_multiplier=depthwise_multiplier,\n            stride=2,\n            weights_initializer=trunc_normal(1.0),\n            scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint:\n        return net, end_points\n      # 112 x 112 x 64\n      end_point = \'MaxPool_2a_3x3\'\n      net = layers_lib.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint:\n        return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2b_1x1\'\n      net = layers.conv2d(\n          net,\n          depth(64), [1, 1],\n          scope=end_point,\n          weights_initializer=trunc_normal(0.1))\n      end_points[end_point] = net\n      if end_point == final_endpoint:\n        return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2c_3x3\'\n      net = layers.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint:\n        return net, end_points\n      # 56 x 56 x 192\n      end_point = \'MaxPool_3a_3x3\'\n      net = layers_lib.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint:\n        return net, end_points\n      # 28 x 28 x 192\n      # Inception module.\n      end_point = \'Mixed_3b\'\n      with variable_scope.variable_scope(end_point):\n        with variable_scope.variable_scope(\'Branch_0\'):\n          branch_0 = layers.conv2d(\n              net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with variable_scope.variable_scope(\'Branch_1\'):\n          branch_1 = layers.conv2d(\n              net,\n              depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = layers.conv2d(\n              branch_1, depth(64), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with variable_scope.variable_scope(\'Branch_2\'):\n          branch_2 = layers.conv2d(\n              net,\n              depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(96), [3, 3], scope=\'Conv2d_0c_3x3\')\n        with variable_scope.variable_scope(\'Branch_3\'):\n          branch_3 = layers_lib.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = layers.conv2d(\n              branch_3,\n              depth(32), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = array_ops.concat([branch_0, branch_1, branch_2, branch_3], 3)\n        end_points[end_point] = net\n        if end_point == final_endpoint:\n          return net, end_points\n      # 28 x 28 x 256\n      end_point = \'Mixed_3c\'\n      with variable_scope.variable_scope(end_point):\n        with variable_scope.variable_scope(\'Branch_0\'):\n          branch_0 = layers.conv2d(\n              net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with variable_scope.variable_scope(\'Branch_1\'):\n          branch_1 = layers.conv2d(\n              net,\n              depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = layers.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with variable_scope.variable_scope(\'Branch_2\'):\n          branch_2 = layers.conv2d(\n              net,\n              depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(96), [3, 3], scope=\'Conv2d_0c_3x3\')\n        with variable_scope.variable_scope(\'Branch_3\'):\n          branch_3 = layers_lib.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = layers.conv2d(\n              branch_3,\n              depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = array_ops.concat([branch_0, branch_1, branch_2, branch_3], 3)\n        end_points[end_point] = net\n        if end_point == final_endpoint:\n          return net, end_points\n      # 28 x 28 x 320\n      end_point = \'Mixed_4a\'\n      with variable_scope.variable_scope(end_point):\n        with variable_scope.variable_scope(\'Branch_0\'):\n          branch_0 = layers.conv2d(\n              net,\n              depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = layers.conv2d(\n              branch_0, depth(160), [3, 3], stride=2, scope=\'Conv2d_1a_3x3\')\n        with variable_scope.variable_scope(\'Branch_1\'):\n          branch_1 = layers.conv2d(\n              net,\n              depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = layers.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_1 = layers.conv2d(\n              branch_1, depth(96), [3, 3], stride=2, scope=\'Conv2d_1a_3x3\')\n        with variable_scope.variable_scope(\'Branch_2\'):\n          branch_2 = layers_lib.max_pool2d(\n              net, [3, 3], stride=2, scope=\'MaxPool_1a_3x3\')\n        net = array_ops.concat([branch_0, branch_1, branch_2], 3)\n        end_points[end_point] = net\n        if end_point == final_endpoint:\n          return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4b\'\n      with variable_scope.variable_scope(end_point):\n        with variable_scope.variable_scope(\'Branch_0\'):\n          branch_0 = layers.conv2d(\n              net, depth(224), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with variable_scope.variable_scope(\'Branch_1\'):\n          branch_1 = layers.conv2d(\n              net,\n              depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = layers.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with variable_scope.variable_scope(\'Branch_2\'):\n          branch_2 = layers.conv2d(\n              net,\n              depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(128), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(128), [3, 3], scope=\'Conv2d_0c_3x3\')\n        with variable_scope.variable_scope(\'Branch_3\'):\n          branch_3 = layers_lib.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = layers.conv2d(\n              branch_3,\n              depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = array_ops.concat([branch_0, branch_1, branch_2, branch_3], 3)\n        end_points[end_point] = net\n        if end_point == final_endpoint:\n          return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4c\'\n      with variable_scope.variable_scope(end_point):\n        with variable_scope.variable_scope(\'Branch_0\'):\n          branch_0 = layers.conv2d(\n              net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with variable_scope.variable_scope(\'Branch_1\'):\n          branch_1 = layers.conv2d(\n              net,\n              depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = layers.conv2d(\n              branch_1, depth(128), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with variable_scope.variable_scope(\'Branch_2\'):\n          branch_2 = layers.conv2d(\n              net,\n              depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(128), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(128), [3, 3], scope=\'Conv2d_0c_3x3\')\n        with variable_scope.variable_scope(\'Branch_3\'):\n          branch_3 = layers_lib.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = layers.conv2d(\n              branch_3,\n              depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = array_ops.concat([branch_0, branch_1, branch_2, branch_3], 3)\n        end_points[end_point] = net\n        if end_point == final_endpoint:\n          return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4d\'\n      with variable_scope.variable_scope(end_point):\n        with variable_scope.variable_scope(\'Branch_0\'):\n          branch_0 = layers.conv2d(\n              net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with variable_scope.variable_scope(\'Branch_1\'):\n          branch_1 = layers.conv2d(\n              net,\n              depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = layers.conv2d(\n              branch_1, depth(160), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with variable_scope.variable_scope(\'Branch_2\'):\n          branch_2 = layers.conv2d(\n              net,\n              depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(160), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(160), [3, 3], scope=\'Conv2d_0c_3x3\')\n        with variable_scope.variable_scope(\'Branch_3\'):\n          branch_3 = layers_lib.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = layers.conv2d(\n              branch_3,\n              depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = array_ops.concat([branch_0, branch_1, branch_2, branch_3], 3)\n        end_points[end_point] = net\n        if end_point == final_endpoint:\n          return net, end_points\n\n      # 14 x 14 x 576\n      end_point = \'Mixed_4e\'\n      with variable_scope.variable_scope(end_point):\n        with variable_scope.variable_scope(\'Branch_0\'):\n          branch_0 = layers.conv2d(\n              net, depth(96), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with variable_scope.variable_scope(\'Branch_1\'):\n          branch_1 = layers.conv2d(\n              net,\n              depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = layers.conv2d(\n              branch_1, depth(192), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with variable_scope.variable_scope(\'Branch_2\'):\n          branch_2 = layers.conv2d(\n              net,\n              depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(192), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(192), [3, 3], scope=\'Conv2d_0c_3x3\')\n        with variable_scope.variable_scope(\'Branch_3\'):\n          branch_3 = layers_lib.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = layers.conv2d(\n              branch_3,\n              depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = array_ops.concat([branch_0, branch_1, branch_2, branch_3], 3)\n        end_points[end_point] = net\n        if end_point == final_endpoint:\n          return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_5a\'\n      with variable_scope.variable_scope(end_point):\n        with variable_scope.variable_scope(\'Branch_0\'):\n          branch_0 = layers.conv2d(\n              net,\n              depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = layers.conv2d(\n              branch_0, depth(192), [3, 3], stride=2, scope=\'Conv2d_1a_3x3\')\n        with variable_scope.variable_scope(\'Branch_1\'):\n          branch_1 = layers.conv2d(\n              net,\n              depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = layers.conv2d(\n              branch_1, depth(256), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_1 = layers.conv2d(\n              branch_1, depth(256), [3, 3], stride=2, scope=\'Conv2d_1a_3x3\')\n        with variable_scope.variable_scope(\'Branch_2\'):\n          branch_2 = layers_lib.max_pool2d(\n              net, [3, 3], stride=2, scope=\'MaxPool_1a_3x3\')\n        net = array_ops.concat([branch_0, branch_1, branch_2], 3)\n        end_points[end_point] = net\n        if end_point == final_endpoint:\n          return net, end_points\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5b\'\n      with variable_scope.variable_scope(end_point):\n        with variable_scope.variable_scope(\'Branch_0\'):\n          branch_0 = layers.conv2d(\n              net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with variable_scope.variable_scope(\'Branch_1\'):\n          branch_1 = layers.conv2d(\n              net,\n              depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = layers.conv2d(\n              branch_1, depth(320), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with variable_scope.variable_scope(\'Branch_2\'):\n          branch_2 = layers.conv2d(\n              net,\n              depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(224), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(224), [3, 3], scope=\'Conv2d_0c_3x3\')\n        with variable_scope.variable_scope(\'Branch_3\'):\n          branch_3 = layers_lib.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = layers.conv2d(\n              branch_3,\n              depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = array_ops.concat([branch_0, branch_1, branch_2, branch_3], 3)\n        end_points[end_point] = net\n        if end_point == final_endpoint:\n          return net, end_points\n\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5c\'\n      with variable_scope.variable_scope(end_point):\n        with variable_scope.variable_scope(\'Branch_0\'):\n          branch_0 = layers.conv2d(\n              net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with variable_scope.variable_scope(\'Branch_1\'):\n          branch_1 = layers.conv2d(\n              net,\n              depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = layers.conv2d(\n              branch_1, depth(320), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with variable_scope.variable_scope(\'Branch_2\'):\n          branch_2 = layers.conv2d(\n              net,\n              depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(224), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = layers.conv2d(\n              branch_2, depth(224), [3, 3], scope=\'Conv2d_0c_3x3\')\n        with variable_scope.variable_scope(\'Branch_3\'):\n          branch_3 = layers_lib.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n          branch_3 = layers.conv2d(\n              branch_3,\n              depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = array_ops.concat([branch_0, branch_1, branch_2, branch_3], 3)\n        end_points[end_point] = net\n        if end_point == final_endpoint:\n          return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v2(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 replace_separable_convolution=False,\n                 prediction_fn=layers_lib.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV2\'):\n  """"""Inception v2 model for classification.\n\n  Constructs an Inception v2 network for classification as described in\n  http://arxiv.org/abs/1502.03167.\n\n  The recommended image size used to train this network is 224x224. For image\n  sizes that differ substantially, it is recommended to use inception_v2_base()\n  and connect custom final layers to the output.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    replace_separable_convolution: Replace the separable convolution in the\n      layer Conv2d_1a_7x7 with a normal convolution.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n        Note that input image sizes other than 224x224 might lead to different\n        spatial dimensions, and hence cannot be squeezed. In this event,\n        it is best to set spatial_squeeze as False, and perform a reduce_mean\n        over the resulting spatial dimensions with sizes exceeding 1.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if depth_multiplier <= 0.\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  # Final pooling and prediction\n  with variable_scope.variable_scope(\n      scope, \'InceptionV2\', [inputs, num_classes], reuse=reuse) as scope:\n    with arg_scope(\n        [layers_lib.batch_norm, layers_lib.dropout], is_training=is_training):\n      net, end_points = inception_v2_base(\n          inputs,\n          scope=scope,\n          min_depth=min_depth,\n          depth_multiplier=depth_multiplier,\n          replace_separable_convolution=replace_separable_convolution)\n      with variable_scope.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n        net = layers_lib.avg_pool2d(\n            net,\n            kernel_size,\n            padding=\'VALID\',\n            scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 1024\n        net = layers_lib.dropout(\n            net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = layers.conv2d(\n            net,\n            num_classes, [1, 1],\n            activation_fn=None,\n            normalizer_fn=None,\n            scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = array_ops.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\n\n\ninception_v2.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.stack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [\n        min(shape[1], kernel_size[0]), min(shape[2], kernel_size[1])\n    ]\n  return kernel_size_out\n\n\ndef inception_v2_arg_scope(weight_decay=0.00004,\n                           batch_norm_var_collection=\'moving_vars\',\n                           batch_norm_decay=0.9997,\n                           batch_norm_epsilon=0.001,\n                           updates_collections=ops.GraphKeys.UPDATE_OPS,\n                           use_fused_batchnorm=True):\n  """"""Defines the default InceptionV2 arg scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_var_collection: The name of the collection for the batch norm\n      variables.\n    batch_norm_decay: Decay for batch norm moving average\n    batch_norm_epsilon: Small float added to variance to avoid division by zero\n    updates_collections: Collections for the update ops of the layer\n    use_fused_batchnorm: Enable fused batchnorm.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      # collection containing update_ops.\n      \'updates_collections\': updates_collections,\n      # Enable fused batchnorm.\n      \'fused\': use_fused_batchnorm,\n      # collection containing the moving mean and moving variance.\n      \'variables_collections\': {\n          \'beta\': None,\n          \'gamma\': None,\n          \'moving_mean\': [batch_norm_var_collection],\n          \'moving_variance\': [batch_norm_var_collection],\n      }\n  }\n\n  # Set weight_decay for weights in Conv and FC layers.\n  with arg_scope(\n      [layers.conv2d, layers_lib.fully_connected],\n      weights_regularizer=regularizers.l2_regularizer(weight_decay)):\n    with arg_scope(\n        [layers.conv2d],\n        weights_initializer=initializers.variance_scaling_initializer(),\n        activation_fn=nn_ops.relu,\n        normalizer_fn=layers_lib.batch_norm,\n        normalizer_params=batch_norm_params) as sc:\n      return sc\n'"
tpu/models/experimental/inception/inception_v3.py,112,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Open-source TensorFlow Inception v3 Example.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nfrom absl import flags\nimport absl.logging as _logging  # pylint: disable=unused-import\nimport tensorflow as tf\n\nimport inception_preprocessing\nimport vgg_preprocessing\n\nfrom tensorflow.contrib import summary\nfrom tensorflow.contrib.framework.python.ops import arg_scope\nfrom tensorflow.contrib.slim.nets import inception\nfrom tensorflow.contrib.training.python.training import evaluation\n\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'tpu\', default=None,\n    help=\'The Cloud TPU to use for training. This should be either the name \'\n    \'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\')\nflags.DEFINE_string(\n    \'gcp_project\', default=None,\n    help=\'Project name for the Cloud TPU-enabled project. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\nflags.DEFINE_string(\n    \'tpu_zone\', default=None,\n    help=\'GCE zone where the Cloud TPU is located in. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\n\n# Model specific paramenters\nflags.DEFINE_string(\n    \'data_dir\', \'\',\n    \'Directory where input data is stored\')\n\nflags.DEFINE_string(\n    \'model_dir\', None,\n    \'Directory where model output is stored\')\n\nflags.DEFINE_integer(\n    \'num_shards\', 8,\n    \'Number of shards (workers).\')\n\nflags.DEFINE_integer(\n    \'iterations\', 100,\n    \'Number of iterations per TPU training loop.\')\n\nflags.DEFINE_bool(\n    \'skip_host_call\', default=True,\n    help=(\'Skip the host call which is executed every training step. This is\'\n          \' generally used for generating training summaries (train loss,\'\n          \' learning rate, etc...). When --skip_host_call=false, there could\'\n          \' be a performance drop if host_call function is slow and cannot\'\n          \' keep up with the computation running on the TPU.\'))\n\nflags.DEFINE_integer(\n    \'train_batch_size\', 1024,\n    \'Global (not per-shard) batch size for training\')\n\nflags.DEFINE_integer(\n    \'eval_total_size\', 0,\n    \'Total batch size for evaluation, use the entire validation set if 0\')\n\nflags.DEFINE_integer(\n    \'eval_batch_size\', 1024,\n    \'Global (not per-shard) batch size for evaluation\')\n\nflags.DEFINE_integer(\n    \'train_steps\', 213000,\n    \'Number of steps use for training.\')\n\nflags.DEFINE_integer(\n    \'train_steps_per_eval\', 2000,\n    \'Number of training steps to run between evaluations.\')\n\nflags.DEFINE_string(\n    \'mode\', \'train_and_eval\',\n    \'Mode to run: train, eval, train_and_eval\')\n\nflags.DEFINE_integer(\n    \'min_eval_interval\', 180,\n    \'Minimum number of seconds between evaluations\')\n\nflags.DEFINE_integer(\n    \'eval_timeout\', None,\n    \'Evaluation timeout: Maximum number of seconds that \'\n    \'may elapse while no new checkpoints are observed\')\n\nflags.DEFINE_bool(\n    \'use_tpu\', True,\n    \'Use TPUs rather than plain CPUs\')\n\nflags.DEFINE_boolean(\n    \'per_host_input_for_training\', True,\n    \'If true, input_fn is invoked per host rather than per shard.\')\n\nflags.DEFINE_string(\n    \'use_data\', \'real\',\n    \'One of ""fake"",""real""\')\n\nflags.DEFINE_float(\n    \'learning_rate\', 0.165,\n    \'Learning rate.\')\n\nflags.DEFINE_string(\n    \'optimizer\', \'RMS\',\n    \'Optimizer (one of sgd, RMS, momentum)\')\n\nflags.DEFINE_integer(\n    \'num_classes\', 1001,\n    \'Number of classes to distinguish\')\n\nflags.DEFINE_integer(\n    \'width\', 299,\n    \'Width of input image\')\n\nflags.DEFINE_integer(\n    \'height\', 299,\n    \'Height of input image\')\n\nflags.DEFINE_bool(\n    \'transpose_enabled\', False,\n    \'Boolean to enable/disable explicit I/O transpose\')\n\nflags.DEFINE_bool(\n    \'log_device_placement\', False,\n    \'Boolean to enable/disable log device placement\')\n\nflags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'Number of steps which must have run before showing summaries.\')\n\nflags.DEFINE_integer(\n    \'save_checkpoints_secs\', 1000,\n    \'Interval (in seconds) at which the model data \'\n    \'should be checkpointed. Set to 0 to disable.\')\n\nflags.DEFINE_bool(\n    \'moving_average\', True,\n    \'Whether to enable moving average computation on variables\')\n\nflags.DEFINE_string(\n    \'preprocessing\', \'inception\',\n    \'Preprocessing stage to use: one of inception or vgg\')\n\nflags.DEFINE_bool(\n    \'use_annotated_bbox\', False,\n    \'If true, use annotated bounding box as input to cropping function, \'\n    \'else use full image size\')\n\nflags.DEFINE_float(\n    \'learning_rate_decay\', 0.94,\n    \'Exponential decay rate used in learning rate adjustment\')\n\nflags.DEFINE_integer(\n    \'learning_rate_decay_epochs\', 3,\n    \'Exponential decay epochs used in learning rate adjustment\')\n\nflags.DEFINE_bool(\n    \'display_tensors\', False,\n    \'Whether to dump prediction tensors for comparison\')\n\nflags.DEFINE_bool(\n    \'clear_update_collections\', True,\n    \'Set batchnorm update_collections to None if true, else use default value\')\n\nflags.DEFINE_integer(\n    \'cold_epochs\', 2,\n    \'Number of epochs using cold learning rate\')\n\nflags.DEFINE_integer(\n    \'warmup_epochs\', 7,\n    \'Number of epochs using linearly increasing learning rate\')\n\nflags.DEFINE_bool(\n    \'use_learning_rate_warmup\', False,\n    \'Apply learning rate warmup if true\')\n\n# Dataset specific paramenters\nflags.DEFINE_bool(\n    \'prefetch_enabled\', True,\n    \'Boolean to enable/disable prefetching\')\n\nflags.DEFINE_integer(\n    \'prefetch_dataset_buffer_size\', 8*1024*1024,\n    \'Number of bytes in read buffer. 0 means no buffering.\')\n\nflags.DEFINE_integer(\n    \'num_files_infeed\', 8,\n    \'Number of training files to read in parallel.\')\n\nflags.DEFINE_integer(\n    \'num_parallel_calls\', 64,\n    \'Number of elements to process in parallel (by mapper)\')\n\nflags.DEFINE_integer(\n    \'initial_shuffle_buffer_size\', 1024,\n    \'Number of elements from dataset that shuffler will sample from. \'\n    \'This shuffling is done before any other operations. \'\n    \'Set to 0 to disable\')\n\nflags.DEFINE_integer(\n    \'followup_shuffle_buffer_size\', 1000,\n    \'Number of elements from dataset that shuffler will sample from. \'\n    \'This shuffling is done after prefetching is done. \'\n    \'Set to 0 to disable\')\n\nflags.DEFINE_string(\n    \'precision\', \'float32\',\n    help=(\'Precision to use; one of: {bfloat16, float32}\'))\n\nFLAGS = flags.FLAGS\n\n# Dataset constants\n_NUM_TRAIN_IMAGES = 1281167\n_NUM_EVAL_IMAGES = 50000\n\n# Random cropping constants\n_RESIZE_SIDE_MIN = 300\n_RESIZE_SIDE_MAX = 600\n\n# Constants dictating the learning rate schedule.\nRMSPROP_DECAY = 0.9                # Decay term for RMSProp.\nRMSPROP_MOMENTUM = 0.9             # Momentum in RMSProp.\nRMSPROP_EPSILON = 1.0              # Epsilon term for RMSProp.\n\n# Constants dictating moving average.\nMOVING_AVERAGE_DECAY = 0.995\n\n# Batchnorm moving mean/variance parameters\nBATCH_NORM_DECAY = 0.996\nBATCH_NORM_EPSILON = 1e-3\n\nWEIGHT_DECAY = 0.00004\n\n\nclass InputPipeline(object):\n  """"""Generates ImageNet input_fn for training or evaluation.\n\n  The training data is assumed to be in TFRecord format with keys as specified\n  in the dataset_parser below, sharded across 1024 files, named sequentially:\n      train-00000-of-01024\n      train-00001-of-01024\n      ...\n      train-01023-of-01024\n\n  The validation data is in the same format but sharded in 128 files.\n\n  The format of the data required is created by the script at:\n      https://github.com/tensorflow/tpu/blob/master/tools/datasets/imagenet_to_gcs.py\n\n  Args:\n    is_training: `bool` for whether the input is for training\n  """"""\n\n  def __init__(self, is_training, data_dir, use_bfloat16):\n    self.is_training = is_training\n    self.data_dir = data_dir\n    self.use_bfloat16 = use_bfloat16\n\n  def dataset_parser(self, serialized_proto):\n    """"""Parse an Imagenet record from value.""""""\n    keys_to_features = {\n        \'image/encoded\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\':\n            tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/class/label\':\n            tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),\n        \'image/class/text\':\n            tf.FixedLenFeature([], dtype=tf.string, default_value=\'\'),\n        \'image/object/bbox/xmin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/class/label\':\n            tf.VarLenFeature(dtype=tf.int64),\n    }\n\n    features = tf.parse_single_example(serialized_proto, keys_to_features)\n\n    bbox = None\n    if FLAGS.use_annotated_bbox:\n      xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n      ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n      xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n      ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n      # Note that we impose an ordering of (y, x) just to make life difficult.\n      bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n      # Force the variable number of bounding boxes into the shape\n      # [1, num_boxes, coords].\n      bbox = tf.expand_dims(bbox, 0)\n      bbox = tf.transpose(bbox, [0, 2, 1])\n\n    image = features[\'image/encoded\']\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    if FLAGS.preprocessing == \'vgg\':\n      image = vgg_preprocessing.preprocess_image(\n          image=image,\n          output_height=FLAGS.height,\n          output_width=FLAGS.width,\n          is_training=self.is_training,\n          resize_side_min=_RESIZE_SIDE_MIN,\n          resize_side_max=_RESIZE_SIDE_MAX)\n    elif FLAGS.preprocessing == \'inception\':\n      image = inception_preprocessing.preprocess_image(\n          image=image,\n          output_height=FLAGS.height,\n          output_width=FLAGS.width,\n          is_training=self.is_training,\n          bbox=bbox)\n\n    label = tf.cast(\n        tf.reshape(features[\'image/class/label\'], shape=[]), dtype=tf.int32)\n\n    if self.use_bfloat16:\n      image = tf.cast(image, tf.bfloat16)\n\n    return image, label\n\n  def dataset_iterator(self, batch_size, shuffle):\n    """"""Constructs a real-data iterator over batches for train or eval.\n\n    Args:\n      batch_size: The effective batch size.\n      shuffle: Whether or not to shuffle the data.\n\n    Returns:\n      A tf.data iterator.\n    """"""\n    file_pattern = os.path.join(self.data_dir, \'train-*\'\n                                if self.is_training else \'validation-*\')\n    dataset = tf.data.Dataset.list_files(file_pattern, shuffle=self.is_training)\n\n    if self.is_training:\n      dataset = dataset.repeat()\n\n    def prefetch_dataset(filename):\n      dataset = tf.data.TFRecordDataset(\n          filename, buffer_size=FLAGS.prefetch_dataset_buffer_size)\n      return dataset\n\n    dataset = dataset.apply(\n        tf.contrib.data.parallel_interleave(\n            prefetch_dataset, cycle_length=FLAGS.num_files_infeed, sloppy=True))\n\n    if shuffle and FLAGS.followup_shuffle_buffer_size > 0:\n      dataset = dataset.shuffle(buffer_size=FLAGS.followup_shuffle_buffer_size)\n\n    dataset = dataset.map(\n        self.dataset_parser, num_parallel_calls=FLAGS.num_parallel_calls)\n\n    dataset = dataset.prefetch(batch_size)\n\n    dataset = dataset.apply(\n        tf.contrib.data.batch_and_drop_remainder(batch_size))\n\n    dataset = dataset.prefetch(2)  # Prefetch overlaps in-feed with training\n\n    return dataset.make_one_shot_iterator()\n\n  def input_fn(self, params):\n    """"""Input function which provides a single batch for train or eval.\n\n    Args:\n      params: `dict` of parameters passed from the `TPUEstimator`.\n          `params[\'batch_size\']` is always provided and should be used as the\n          effective batch size.\n\n    Returns:\n      A (images, labels) tuple of `Tensor`s for a batch of samples.\n    """"""\n    batch_size = params[\'batch_size\']\n\n    if FLAGS.use_data == \'real\':\n      images, labels = self.dataset_iterator(batch_size,\n                                             self.is_training).get_next()\n    else:\n      images = tf.random_uniform(\n          [batch_size, FLAGS.height, FLAGS.width, 3], minval=-1, maxval=1)\n      labels = tf.random_uniform(\n          [batch_size], minval=0, maxval=999, dtype=tf.int32)\n\n    images = tensor_transform_fn(images, params[\'output_perm\'])\n    return images, labels\n\n\ndef tensor_transform_fn(data, perm):\n  """"""Transpose function.\n\n  This function is used to transpose an image tensor on the host and then\n  perform an inverse transpose on the TPU. The transpose on the TPU gets\n  effectively elided thus voiding any associated computational cost.\n\n  NOTE: Eventually the compiler will be able to detect when this kind of\n  operation may prove beneficial and perform these types of transformations\n  implicitly, voiding the need for user intervention\n\n  Args:\n    data: Tensor to be transposed\n    perm: New ordering of dimensions\n\n  Returns:\n    Transposed tensor\n  """"""\n  if FLAGS.transpose_enabled:\n    return tf.transpose(data, perm)\n  return data\n\n\ndef inception_model_fn(features, labels, mode, params):\n  """"""Inception v3 model using Estimator API.""""""\n  num_classes = FLAGS.num_classes\n  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n  is_eval = (mode == tf.estimator.ModeKeys.EVAL)\n  features = tensor_transform_fn(features, params[\'input_perm\'])\n\n  # This nested function allows us to avoid duplicating the logic which\n  # builds the network, for different values of --precision.\n  def build_network():\n    if FLAGS.precision == \'bfloat16\':\n      with tf.contrib.tpu.bfloat16_scope():\n        logits, end_points = inception.inception_v3(\n            features,\n            num_classes,\n            is_training=is_training)\n      logits = tf.cast(logits, tf.float32)\n    elif FLAGS.precision == \'float32\':\n      logits, end_points = inception.inception_v3(\n          features,\n          num_classes,\n          is_training=is_training)\n    return logits, end_points\n\n  if FLAGS.clear_update_collections:\n    # updates_collections must be set to None in order to use fused batchnorm\n    with arg_scope(inception.inception_v3_arg_scope(\n        weight_decay=0.0,\n        batch_norm_decay=BATCH_NORM_DECAY,\n        batch_norm_epsilon=BATCH_NORM_EPSILON,\n        updates_collections=None)):\n      logits, end_points = build_network()\n  else:\n    with arg_scope(inception.inception_v3_arg_scope(\n        batch_norm_decay=BATCH_NORM_DECAY,\n        batch_norm_epsilon=BATCH_NORM_EPSILON)):\n      logits, end_points = build_network()\n\n  predictions = end_points\n  predictions.update({\n      \'classes\': tf.argmax(input=logits, axis=1),\n      \'probabilities\': tf.nn.softmax(logits, name=\'softmax_tensor\')\n  })\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n  if mode == tf.estimator.ModeKeys.EVAL and FLAGS.display_tensors and (\n      not FLAGS.use_tpu):\n    with tf.control_dependencies([\n        tf.Print(\n            predictions[\'classes\'], [predictions[\'classes\']],\n            summarize=FLAGS.eval_batch_size,\n            message=\'prediction: \')\n    ]):\n      labels = tf.Print(\n          labels, [labels], summarize=FLAGS.eval_batch_size, message=\'label: \')\n\n  one_hot_labels = tf.one_hot(labels, FLAGS.num_classes, dtype=tf.int32)\n\n  if \'AuxLogits\' in end_points:\n    tf.losses.softmax_cross_entropy(\n        onehot_labels=one_hot_labels,\n        logits=tf.cast(end_points[\'AuxLogits\'], tf.float32),\n        weights=0.4,\n        label_smoothing=0.1,\n        scope=\'aux_loss\')\n\n  tf.losses.softmax_cross_entropy(\n      onehot_labels=one_hot_labels,\n      logits=logits,\n      weights=1.0,\n      label_smoothing=0.1)\n\n  losses = tf.add_n(tf.losses.get_losses())\n  l2_loss = []\n  for v in tf.trainable_variables():\n    if \'BatchNorm\' not in v.name and \'weights\' in v.name:\n      l2_loss.append(tf.nn.l2_loss(v))\n  loss = losses + WEIGHT_DECAY * tf.add_n(l2_loss)\n\n  initial_learning_rate = FLAGS.learning_rate * FLAGS.train_batch_size / 256\n  if FLAGS.use_learning_rate_warmup:\n    # Adjust initial learning rate to match final warmup rate\n    warmup_decay = FLAGS.learning_rate_decay**(\n        (FLAGS.warmup_epochs + FLAGS.cold_epochs) /\n        FLAGS.learning_rate_decay_epochs)\n    adj_initial_learning_rate = initial_learning_rate * warmup_decay\n\n  final_learning_rate = 0.0001 * initial_learning_rate\n\n  host_call = None\n  train_op = None\n  if is_training:\n    batches_per_epoch = _NUM_TRAIN_IMAGES / FLAGS.train_batch_size\n    global_step = tf.train.get_or_create_global_step()\n    current_epoch = tf.cast(\n        (tf.cast(global_step, tf.float32) / batches_per_epoch), tf.int32)\n\n    learning_rate = tf.train.exponential_decay(\n        learning_rate=initial_learning_rate,\n        global_step=global_step,\n        decay_steps=int(FLAGS.learning_rate_decay_epochs * batches_per_epoch),\n        decay_rate=FLAGS.learning_rate_decay,\n        staircase=True)\n\n    if FLAGS.use_learning_rate_warmup:\n      wlr = 0.1 * adj_initial_learning_rate\n      wlr_height = tf.cast(\n          0.9 * adj_initial_learning_rate /\n          (FLAGS.warmup_epochs + FLAGS.learning_rate_decay_epochs - 1),\n          tf.float32)\n      epoch_offset = tf.cast(FLAGS.cold_epochs - 1, tf.int32)\n      exp_decay_start = (FLAGS.warmup_epochs + FLAGS.cold_epochs +\n                         FLAGS.learning_rate_decay_epochs)\n      lin_inc_lr = tf.add(\n          wlr, tf.multiply(\n              tf.cast(tf.subtract(current_epoch, epoch_offset), tf.float32),\n              wlr_height))\n      learning_rate = tf.where(\n          tf.greater_equal(current_epoch, FLAGS.cold_epochs),\n          (tf.where(tf.greater_equal(current_epoch, exp_decay_start),\n                    learning_rate, lin_inc_lr)),\n          wlr)\n\n    # Set a minimum boundary for the learning rate.\n    learning_rate = tf.maximum(\n        learning_rate, final_learning_rate, name=\'learning_rate\')\n\n    if FLAGS.optimizer == \'sgd\':\n      tf.logging.info(\'Using SGD optimizer\')\n      optimizer = tf.train.GradientDescentOptimizer(\n          learning_rate=learning_rate)\n    elif FLAGS.optimizer == \'momentum\':\n      tf.logging.info(\'Using Momentum optimizer\')\n      optimizer = tf.train.MomentumOptimizer(\n          learning_rate=learning_rate, momentum=0.9)\n    elif FLAGS.optimizer == \'RMS\':\n      tf.logging.info(\'Using RMS optimizer\')\n      optimizer = tf.train.RMSPropOptimizer(\n          learning_rate,\n          RMSPROP_DECAY,\n          momentum=RMSPROP_MOMENTUM,\n          epsilon=RMSPROP_EPSILON)\n    else:\n      tf.logging.fatal(\'Unknown optimizer:\', FLAGS.optimizer)\n\n    if FLAGS.use_tpu:\n      optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.minimize(loss, global_step=global_step)\n    if FLAGS.moving_average:\n      ema = tf.train.ExponentialMovingAverage(\n          decay=MOVING_AVERAGE_DECAY, num_updates=global_step)\n      variables_to_average = (\n          tf.trainable_variables() + tf.moving_average_variables())\n      with tf.control_dependencies([train_op]), tf.name_scope(\'moving_average\'):\n        train_op = ema.apply(variables_to_average)\n\n    # To log the loss, current learning rate, and epoch for Tensorboard, the\n    # summary op needs to be run on the host CPU via host_call. host_call\n    # expects [batch_size, ...] Tensors, thus reshape to introduce a batch\n    # dimension. These Tensors are implicitly concatenated to\n    # [params[\'batch_size\']].\n    gs_t = tf.reshape(global_step, [1])\n    loss_t = tf.reshape(loss, [1])\n    lr_t = tf.reshape(learning_rate, [1])\n    ce_t = tf.reshape(current_epoch, [1])\n\n    if not FLAGS.skip_host_call:\n      def host_call_fn(gs, loss, lr, ce):\n        """"""Training host call. Creates scalar summaries for training metrics.\n\n        This function is executed on the CPU and should not directly reference\n        any Tensors in the rest of the `model_fn`. To pass Tensors from the\n        model to the `metric_fn`, provide them as part of the `host_call`. See\n        https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n        for more information.\n\n        Arguments should match the list of `Tensor` objects passed as the second\n        element in the tuple passed to `host_call`.\n\n        Args:\n          gs: `Tensor with shape `[batch]` for the global_step\n          loss: `Tensor` with shape `[batch]` for the training loss.\n          lr: `Tensor` with shape `[batch]` for the learning_rate.\n          ce: `Tensor` with shape `[batch]` for the current_epoch.\n\n        Returns:\n          List of summary ops to run on the CPU host.\n        """"""\n        gs = gs[0]\n        with summary.create_file_writer(FLAGS.model_dir).as_default():\n          with summary.always_record_summaries():\n            summary.scalar(\'loss\', tf.reduce_mean(loss), step=gs)\n            summary.scalar(\'learning_rate\', tf.reduce_mean(lr), step=gs)\n            summary.scalar(\'current_epoch\', tf.reduce_mean(ce), step=gs)\n\n            return summary.all_summary_ops()\n\n      host_call = (host_call_fn, [gs_t, loss_t, lr_t, ce_t])\n\n  eval_metrics = None\n  if is_eval:\n    def metric_fn(labels, logits):\n      """"""Evaluation metric function. Evaluates accuracy.\n\n      This function is executed on the CPU and should not directly reference\n      any Tensors in the rest of the `model_fn`. To pass Tensors from the model\n      to the `metric_fn`, provide as part of the `eval_metrics`. See\n      https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n      for more information.\n\n      Arguments should match the list of `Tensor` objects passed as the second\n      element in the tuple passed to `eval_metrics`.\n\n      Args:\n        labels: `Tensor` with shape `[batch, ]`.\n        logits: `Tensor` with shape `[batch, num_classes]`.\n\n      Returns:\n        A dict of the metrics to return from evaluation.\n      """"""\n      predictions = tf.argmax(logits, axis=1)\n      top_1_accuracy = tf.metrics.accuracy(labels, predictions)\n      in_top_5 = tf.cast(tf.nn.in_top_k(logits, labels, 5), tf.float32)\n      top_5_accuracy = tf.metrics.mean(in_top_5)\n\n      return {\n          \'accuracy\': top_1_accuracy,\n          \'accuracy@5\': top_5_accuracy,\n      }\n\n    eval_metrics = (metric_fn, [labels, logits])\n\n  return tf.contrib.tpu.TPUEstimatorSpec(\n      mode=mode,\n      loss=loss,\n      train_op=train_op,\n      host_call=host_call,\n      eval_metrics=eval_metrics)\n\n\nclass LoadEMAHook(tf.train.SessionRunHook):\n  """"""Hook to load exponential moving averages into corresponding variables.""""""\n\n  def __init__(self, model_dir):\n    super(LoadEMAHook, self).__init__()\n    self._model_dir = model_dir\n\n  def begin(self):\n    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n    variables_to_restore = ema.variables_to_restore()\n    self._load_ema = tf.contrib.framework.assign_from_checkpoint_fn(\n        tf.train.latest_checkpoint(self._model_dir), variables_to_restore)\n\n  def after_create_session(self, sess, coord):\n    tf.logging.info(\'Reloading EMA...\')\n    self._load_ema(sess)\n\n\ndef main(unused_argv):\n  del unused_argv  # Unused\n\n  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n      FLAGS.tpu,\n      zone=FLAGS.tpu_zone,\n      project=FLAGS.gcp_project)\n\n  assert FLAGS.precision == \'bfloat16\' or FLAGS.precision == \'float32\', (\n      \'Invalid value for --precision flag; must be bfloat16 or float32.\')\n  tf.logging.info(\'Precision: %s\', FLAGS.precision)\n\n  params = {\n      \'input_perm\': [0, 1, 2, 3],\n      \'output_perm\': [0, 1, 2, 3],\n  }\n\n  batch_axis = 0\n  if FLAGS.transpose_enabled:\n    params[\'input_perm\'] = [3, 0, 1, 2]\n    params[\'output_perm\'] = [1, 2, 3, 0]\n    batch_axis = 3\n\n  if FLAGS.eval_total_size > 0:\n    eval_size = FLAGS.eval_total_size\n  else:\n    eval_size = _NUM_EVAL_IMAGES\n  eval_steps = eval_size // FLAGS.eval_batch_size\n\n  iterations = (eval_steps if FLAGS.mode == \'eval\' else\n                FLAGS.iterations)\n\n  eval_batch_size = (None if FLAGS.mode == \'train\' else\n                     FLAGS.eval_batch_size)\n\n  per_host_input_for_training = (\n      FLAGS.num_shards <= 8 if FLAGS.mode == \'train\' else True)\n\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_secs=FLAGS.save_checkpoints_secs,\n      save_summary_steps=FLAGS.save_summary_steps,\n      session_config=tf.ConfigProto(\n          allow_soft_placement=True,\n          log_device_placement=FLAGS.log_device_placement),\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=iterations,\n          num_shards=FLAGS.num_shards,\n          per_host_input_for_training=per_host_input_for_training))\n\n  inception_classifier = tf.contrib.tpu.TPUEstimator(\n      model_fn=inception_model_fn,\n      use_tpu=FLAGS.use_tpu,\n      config=run_config,\n      params=params,\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=eval_batch_size,\n      batch_axis=(batch_axis, 0))\n\n  # Input pipelines are slightly different (with regards to shuffling and\n  # preprocessing) between training and evaluation.\n  use_bfloat16 = FLAGS.precision == \'bfloat16\'\n  imagenet_train = InputPipeline(\n      is_training=True,\n      data_dir=FLAGS.data_dir,\n      use_bfloat16=use_bfloat16)\n  imagenet_eval = InputPipeline(\n      is_training=False,\n      data_dir=FLAGS.data_dir,\n      use_bfloat16=use_bfloat16)\n\n  if FLAGS.moving_average:\n    eval_hooks = [LoadEMAHook(FLAGS.model_dir)]\n  else:\n    eval_hooks = []\n\n  if FLAGS.mode == \'eval\':\n    # Run evaluation when there is a new checkpoint\n    for checkpoint in evaluation.checkpoints_iterator(FLAGS.model_dir):\n      tf.logging.info(\'Starting to evaluate.\')\n      try:\n        start_timestamp = time.time()  # Includes compilation time\n        eval_results = inception_classifier.evaluate(\n            input_fn=imagenet_eval.input_fn,\n            steps=eval_steps,\n            hooks=eval_hooks,\n            checkpoint_path=checkpoint)\n        elapsed_time = int(time.time() - start_timestamp)\n        tf.logging.info(\n            \'Eval results: %s. Elapsed seconds: %d\', eval_results, elapsed_time)\n\n        # Terminate eval job when final checkpoint is reached\n        current_step = int(os.path.basename(checkpoint).split(\'-\')[1])\n        if current_step >= FLAGS.train_steps:\n          tf.logging.info(\n              \'Evaluation finished after training step %d\', current_step)\n          break\n      except tf.errors.NotFoundError:\n        # Since the coordinator is on a different job than the TPU worker,\n        # sometimes the TPU worker does not finish initializing until long after\n        # the CPU job tells it to start evaluating. In this case, the checkpoint\n        # file could have been deleted already.\n        tf.logging.info(\n            \'Checkpoint %s no longer exists, skipping checkpoint\', checkpoint)\n\n  elif FLAGS.mode == \'train_and_eval\':\n    for cycle in range(FLAGS.train_steps // FLAGS.train_steps_per_eval):\n      tf.logging.info(\'Starting training cycle %d.\' % cycle)\n      inception_classifier.train(\n          input_fn=imagenet_train.input_fn, steps=FLAGS.train_steps_per_eval)\n\n      tf.logging.info(\'Starting evaluation cycle %d .\' % cycle)\n      eval_results = inception_classifier.evaluate(\n          input_fn=imagenet_eval.input_fn, steps=eval_steps, hooks=eval_hooks)\n      tf.logging.info(\'Evaluation results: %s\' % eval_results)\n\n  else:\n    tf.logging.info(\'Starting training ...\')\n    inception_classifier.train(\n        input_fn=imagenet_train.input_fn, max_steps=FLAGS.train_steps)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/models/experimental/inception/inception_v3_old.py,63,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Open-source TensorFlow Inception v3 Example.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import flags\nimport tensorflow as tf\n\nfrom tensorflow.contrib import slim\n\nfrom tensorflow.contrib.slim.nets import inception\n\n\n\nflags.DEFINE_float(\'learning_rate\', 0.02, \'Learning rate.\')\nflags.DEFINE_float(\'depth_multiplier\', 1.0, \'Depth Multiplier on Inception\')\nflags.DEFINE_integer(\'train_steps\', 800,\n                     \'Total number of steps. Note that the actual number of \'\n                     \'steps is the next multiple of --iterations greater \'\n                     \'than this value.\')\nflags.DEFINE_integer(\'save_checkpoints_secs\', None,\n                     \'Seconds between checkpoint saves\')\nflags.DEFINE_bool(\'use_tpu\', True, \'Use TPUs rather than plain CPUs\')\nflags.DEFINE_string(\'use_data\', \'fake\', \'Data from ""fake"",""real""\')\nflags.DEFINE_string(\'data_dir\', \'\', \'Path of the data (for use_data=real)\')\nflags.DEFINE_string(\'master\', \'local\',\n                    \'BNS name of the TensorFlow master to use.\')\nflags.DEFINE_string(\'model_dir\', None, \'Estimator model_dir\')\nflags.DEFINE_integer(\'iterations\', 40,\n                     \'Number of iterations per TPU training loop.\')\nflags.DEFINE_string(\'optimizer\', \'momentum\',\n                    \'optimizer (one of sgd, rms, momentum)\')\nflags.DEFINE_integer(\'num_shards\', 8, \'Number of shards (TPU chips).\')\nflags.DEFINE_integer(\'batch_size\', 64,\n                     \'Global batch_size, not the per-shard batch_size\')\nflags.DEFINE_integer(\'num_labels\', 1024, \'number of classes to distinguish\')\nflags.DEFINE_integer(\'width\', 304, \'width of input image\')\nflags.DEFINE_integer(\'height\', 304, \'height of input image\')\n\nFLAGS = flags.FLAGS\n\n\ndef inception_v3_arg_scope(is_training=True,\n                           weight_decay=0.00004,\n                           stddev=0.1,\n                           batch_norm_var_collection=\'moving_vars\'):\n  """"""Defines the default InceptionV3 arg scope.\n\n  Args:\n    is_training: Whether or not we\'re training the model.\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: The standard deviation of the trunctated normal weight initializer.\n    batch_norm_var_collection: The name of the collection for the batch norm\n      variables.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  batch_norm_params = {\n      \'is_training\': is_training,\n      # Decay for the moving averages.\n      \'decay\': 0.9997,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': 0.001,\n      # collection containing the moving mean and moving variance.\n      \'variables_collections\': {\n          \'beta\': None,\n          \'gamma\': None,\n          \'moving_mean\': [batch_norm_var_collection],\n          \'moving_variance\': [batch_norm_var_collection],\n      }\n  }\n\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=tf.truncated_normal_initializer(stddev=stddev),\n        activation_fn=tf.nn.relu,\n        normalizer_fn=slim.batch_norm,\n        normalizer_params=batch_norm_params) as sc:\n      return sc\n\n\ndef model_fn(features, labels, mode, params):\n  """"""Inception v3 model using Estimator API.""""""\n  del params\n\n  if mode != tf.estimator.ModeKeys.TRAIN:\n    raise RuntimeError(\'mode {} is not supported yet\'.format(mode))\n\n  num_labels = FLAGS.num_labels\n\n  with slim.arg_scope(inception_v3_arg_scope(is_training=True)):\n    logits, end_points = inception.inception_v3(\n        features,\n        num_labels,\n        is_training=True,\n        depth_multiplier=FLAGS.depth_multiplier)\n\n  onehot_labels = tf.one_hot(\n      indices=tf.cast(labels, tf.int32), depth=num_labels)\n\n  if \'AuxLogits\' in end_points:\n    tf.losses.softmax_cross_entropy(end_points[\'AuxLogits\'],\n                                    onehot_labels,\n                                    label_smoothing=0.1,\n                                    weights=0.4,\n                                    scope=\'aux_loss\')\n  tf.losses.softmax_cross_entropy(logits,\n                                  onehot_labels,\n                                  label_smoothing=0.1,\n                                  weights=1.0)\n  loss = tf.losses.get_total_loss()\n\n  if FLAGS.optimizer == \'sgd\':\n    tf.logging.info(\'Using SGD optimizer\')\n    optimizer = tf.train.GradientDescentOptimizer(\n        learning_rate=FLAGS.learning_rate)\n  elif FLAGS.optimizer == \'momentum\':\n    tf.logging.info(\'Using Momentum optimizer\')\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate=FLAGS.learning_rate, momentum=0.9)\n  else:\n    tf.logging.fatal(\'Unknown optimizer:\', FLAGS.optimizer)\n\n  if FLAGS.use_tpu:\n    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n  train_op = optimizer.minimize(\n      loss, global_step=tf.train.get_or_create_global_step())\n\n  return tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n\n\ndef input_fn(params):\n  """"""Create a single batch of input data for the model.""""""\n  batch_size = params[\'batch_size\']\n  height = FLAGS.height\n  width = FLAGS.width\n\n  def preprocess(image, bbox):\n    """"""Preprocesses the image by resizing and rescaling it.""""""\n    del bbox\n\n    # Convert to float32\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    # TODO(jhseu): Distortion\n    image = tf.image.central_crop(image, central_fraction=0.875)\n\n    # Resize the image to the original height and width.\n    image = tf.expand_dims(image, 0)\n    image = tf.image.resize_bilinear(image, [height, width],\n                                     align_corners=False)\n    image = tf.squeeze(image, [0])\n\n    # Rescale to [-1,1] instead of [0, 1)\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    return image\n\n  def parser(value):\n    """"""Parse an Imagenet record from value.""""""\n    keys_to_features = {\n        \'image/encoded\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\':\n            tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/class/label\':\n            tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),\n        \'image/class/text\':\n            tf.FixedLenFeature([], dtype=tf.string, default_value=\'\'),\n        \'image/object/bbox/xmin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/class/label\':\n            tf.VarLenFeature(dtype=tf.int64),\n    }\n\n    parsed = tf.parse_single_example(value, keys_to_features)\n    encoded_image = tf.reshape(\n        parsed[\'image/encoded\'], shape=[], name=\'encoded_image\')\n    image_format = parsed[\'image/format\']\n    xmin = tf.expand_dims(parsed[\'image/object/bbox/xmin\'].values, 0)\n    ymin = tf.expand_dims(parsed[\'image/object/bbox/ymin\'].values, 0)\n    xmax = tf.expand_dims(parsed[\'image/object/bbox/xmax\'].values, 0)\n    ymax = tf.expand_dims(parsed[\'image/object/bbox/ymax\'].values, 0)\n\n    # Note that we impose an ordering of (y, x) just to make life difficult.\n    bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n    # Force the variable number of bounding boxes into the shape\n    # [1, num_boxes, coords].\n    bbox = tf.expand_dims(bbox, 0)\n    bbox = tf.transpose(bbox, [0, 2, 1])\n\n    def decode_png():\n      return tf.image.decode_png(encoded_image, 3)\n\n    def decode_jpg():\n      return tf.image.decode_jpeg(encoded_image, 3)\n\n    # If image format is PNG, use decode_png, default to jpg.\n    pred_fn_pairs = {\n        tf.logical_or(\n            tf.equal(image_format, \'png\'), tf.equal(image_format, \'PNG\')):\n        decode_png\n    }\n\n    image = tf.case(pred_fn_pairs, default=decode_jpg, exclusive=True)\n    image.set_shape([None, None, 3])\n\n    image = preprocess(image, bbox)\n\n    label = tf.cast(\n        tf.reshape(parsed[\'image/class/label\'], shape=[]),\n        dtype=tf.int32,\n        name=\'cast_label\')\n    label = tf.reshape(label, [1])\n    return tf.cast(image, tf.float32), label\n\n  if FLAGS.use_data == \'real\':\n    data_dir = FLAGS.data_dir\n    filenames = [\n        os.path.join(data_dir, \'train-%05d-of-01024\' % i)\n        for i in range(0, 984)\n    ]\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.repeat().map(parser).batch(batch_size)\n    images, labels = dataset.make_one_shot_iterator().get_next()\n  else:\n    images = tf.random_uniform(\n        [batch_size, height, width, 3], minval=-1, maxval=1)\n    labels = tf.random_uniform(\n        [batch_size], minval=0, maxval=999, dtype=tf.int32)\n\n  # Reshape to give inputs statically known shapes.\n  return (\n      tf.reshape(images, [batch_size, height, width, 3]),\n      tf.reshape(labels, [batch_size]),\n  )\n\n\ndef main(unused_argv):\n  del unused_argv  # Unused\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  run_config = tf.contrib.tpu.RunConfig(\n      master=FLAGS.master,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_secs=FLAGS.save_checkpoints_secs,\n      session_config=tf.ConfigProto(),\n      tpu_config=tf.contrib.tpu.TPUConfig(FLAGS.iterations, FLAGS.num_shards),\n  )\n\n  estimator = tf.contrib.tpu.TPUEstimator(\n      model_fn=model_fn,\n      use_tpu=FLAGS.use_tpu,\n      config=run_config,\n      train_batch_size=FLAGS.batch_size)\n\n  estimator.train(input_fn=input_fn, max_steps=FLAGS.train_steps)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
tpu/models/experimental/inception/inception_v4.py,109,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Open-source TensorFlow Inception V4 Example.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nfrom absl import flags\nimport absl.logging as _logging  # pylint: disable=unused-import\nimport tensorflow as tf\n\nimport inception_preprocessing\nimport inception_v4_model as inception\nimport vgg_preprocessing\n\nfrom tensorflow.contrib import summary\nfrom tensorflow.contrib.framework.python.ops import arg_scope\nfrom tensorflow.contrib.training.python.training import evaluation\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'tpu\', default=None,\n    help=\'The Cloud TPU to use for training. This should be either the name \'\n    \'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\')\nflags.DEFINE_string(\n    \'gcp_project\', default=None,\n    help=\'Project name for the Cloud TPU-enabled project. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\nflags.DEFINE_string(\n    \'tpu_zone\', default=None,\n    help=\'GCE zone where the Cloud TPU is located in. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\n\n# Model specific paramenters\nflags.DEFINE_string(\n    \'data_dir\', \'\',\n    \'Directory where input data is stored\')\n\nflags.DEFINE_string(\n    \'model_dir\', None,\n    \'Directory where model output is stored\')\n\nflags.DEFINE_integer(\n    \'num_shards\', 8,\n    \'Number of shards (workers).\')\n\nflags.DEFINE_integer(\n    \'iterations\', 100,\n    \'Number of iterations per TPU training loop.\')\n\nflags.DEFINE_bool(\n    \'skip_host_call\', default=True,\n    help=(\'Skip the host call which is executed every training step. This is\'\n          \' generally used for generating training summaries (train loss,\'\n          \' learning rate, etc...). When --skip_host_call=false, there could\'\n          \' be a performance drop if host_call function is slow and cannot\'\n          \' keep up with the computation running on the TPU.\'))\n\nflags.DEFINE_integer(\n    \'train_batch_size\', 256,\n    \'Global (not per-shard) batch size for training\')\n\nflags.DEFINE_integer(\n    \'eval_total_size\', 0,\n    \'Total batch size for evaluation, use the entire validation set if 0\')\n\nflags.DEFINE_integer(\n    \'eval_batch_size\', 1024,\n    \'Global (not per-shard) batch size for evaluation\')\n\nflags.DEFINE_integer(\n    \'train_steps\', 80000,\n    \'Number of steps use for training.\')\n\nflags.DEFINE_integer(\n    \'train_steps_per_eval\', 2000,\n    \'Number of training steps to run between evaluations.\')\n\nflags.DEFINE_string(\n    \'mode\', \'train_and_eval\',\n    \'Mode to run: train, eval, train_and_eval\')\n\nflags.DEFINE_integer(\n    \'min_eval_interval\', 180,\n    \'Minimum number of seconds between evaluations\')\n\nflags.DEFINE_integer(\n    \'eval_timeout\', None,\n    \'Evaluation timeout: Maximum number of seconds that \'\n    \'may elapse while no new checkpoints are observed\')\n\nflags.DEFINE_bool(\n    \'use_tpu\', True,\n    \'Use TPUs rather than plain CPUs\')\n\nflags.DEFINE_boolean(\n    \'per_host_input_for_training\', True,\n    \'If true, input_fn is invoked per host rather than per shard.\')\n\nflags.DEFINE_string(\n    \'use_data\', \'real\',\n    \'One of ""fake"",""real""\')\n\nflags.DEFINE_float(\n    \'learning_rate\', 0.15,\n    \'Learning rate.\')\n\nflags.DEFINE_string(\n    \'optimizer\', \'RMS\',\n    \'Optimizer (one of sgd, RMS, momentum)\')\n\nflags.DEFINE_integer(\n    \'num_classes\', 1001,\n    \'Number of classes to distinguish\')\n\nflags.DEFINE_integer(\n    \'width\', 299,\n    \'Width of input image\')\n\nflags.DEFINE_integer(\n    \'height\', 299,\n    \'Height of input image\')\n\nflags.DEFINE_bool(\n    \'transpose_enabled\', False,\n    \'Boolean to enable/disable explicit I/O transpose\')\n\nflags.DEFINE_bool(\n    \'log_device_placement\', False,\n    \'Boolean to enable/disable log device placement\')\n\nflags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'Number of steps which must have run before showing summaries.\')\n\nflags.DEFINE_integer(\n    \'save_checkpoints_secs\', 1000,\n    \'Interval (in seconds) at which the model data \'\n    \'should be checkpointed. Set to 0 to disable.\')\n\nflags.DEFINE_bool(\n    \'moving_average\', True,\n    \'Whether to enable moving average computation on variables\')\n\nflags.DEFINE_string(\n    \'preprocessing\', \'inception\',\n    \'Preprocessing stage to use: one of inception or vgg\')\n\nflags.DEFINE_bool(\n    \'use_annotated_bbox\', True,\n    \'If true, use annotated bounding box as input to cropping function, \'\n    \'else use full image size\')\n\nflags.DEFINE_float(\n    \'learning_rate_decay\', 0.94,\n    \'Exponential decay rate used in learning rate adjustment\')\n\nflags.DEFINE_integer(\n    \'learning_rate_decay_epochs\', 2,\n    \'Exponential decay epochs used in learning rate adjustment\')\n\nflags.DEFINE_bool(\n    \'display_tensors\', False,\n    \'Whether to dump prediction tensors for comparison\')\n\nflags.DEFINE_bool(\n    \'clear_update_collections\', True,\n    \'Set batchnorm update_collections to None if true, else use default value\')\n\nflags.DEFINE_float(\n    \'cold_learning_rate\', 0.001,\n    \'Very initial learning rate\')\n\nflags.DEFINE_integer(\n    \'cold_epochs\', 1,\n    \'Epochs with cold learning rate\')\n\nflags.DEFINE_integer(\n    \'warmup_epochs\', 2,\n    \'Number of epochs of warmup at low learning rates\')\n\n# Dataset specific paramenters\nflags.DEFINE_bool(\n    \'prefetch_enabled\', True,\n    \'Boolean to enable/disable prefetching\')\n\nflags.DEFINE_integer(\n    \'prefetch_dataset_buffer_size\', 8*1024*1024,\n    \'Number of bytes in read buffer. 0 means no buffering.\')\n\nflags.DEFINE_integer(\n    \'num_files_infeed\', 8,\n    \'Number of training files to read in parallel.\')\n\nflags.DEFINE_integer(\n    \'num_parallel_calls\', 64,\n    \'Number of elements to process in parallel (by mapper)\')\n\nflags.DEFINE_integer(\n    \'initial_shuffle_buffer_size\', 1024,\n    \'Number of elements from dataset that shuffler will sample from. \'\n    \'This shuffling is done before any other operations. \'\n    \'Set to 0 to disable\')\n\nflags.DEFINE_integer(\n    \'followup_shuffle_buffer_size\', 1000,\n    \'Number of elements from dataset that shuffler will sample from. \'\n    \'This shuffling is done after prefetching is done. \'\n    \'Set to 0 to disable\')\n\nflags.DEFINE_string(\n    \'precision\', \'float32\',\n    help=(\'Precision to use; one of: {bfloat16, float32}\'))\n\nFLAGS = flags.FLAGS\n\n# Dataset constants\n_NUM_TRAIN_IMAGES = 1281167\n_NUM_EVAL_IMAGES = 50000\n\n# Random cropping constants\n_RESIZE_SIDE_MIN = 300\n_RESIZE_SIDE_MAX = 600\n\n# Constants dictating the learning rate schedule.\nRMSPROP_DECAY = 0.9                # Decay term for RMSProp.\nRMSPROP_MOMENTUM = 0.9             # Momentum in RMSProp.\nRMSPROP_EPSILON = 1.0              # Epsilon term for RMSProp.\n\n# Constants dictating moving average.\nMOVING_AVERAGE_DECAY = 0.995\n\n# Batchnorm moving mean/variance parameters\nBATCH_NORM_DECAY = 0.995\nBATCH_NORM_EPSILON = 1e-3\n\nWEIGHT_DECAY = 0.00004\n\n\nclass InputPipeline(object):\n  """"""Generates ImageNet input_fn for training or evaluation.\n\n  The training data is assumed to be in TFRecord format with keys as specified\n  in the dataset_parser below, sharded across 1024 files, named sequentially:\n      train-00000-of-01024\n      train-00001-of-01024\n      ...\n      train-01023-of-01024\n\n  The validation data is in the same format but sharded in 128 files.\n\n  The format of the data required is created by the script at:\n      https://github.com/tensorflow/tpu/blob/master/tools/datasets/imagenet_to_gcs.py\n\n  Args:\n    is_training: `bool` for whether the input is for training\n  """"""\n\n  def __init__(self, is_training, data_dir, use_bfloat16):\n    self.is_training = is_training\n    self.data_dir = data_dir\n    self.use_bfloat16 = use_bfloat16\n\n  def dataset_parser(self, serialized_proto):\n    """"""Parse an Imagenet record from value.""""""\n    keys_to_features = {\n        \'image/encoded\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\':\n            tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/class/label\':\n            tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),\n        \'image/class/text\':\n            tf.FixedLenFeature([], dtype=tf.string, default_value=\'\'),\n        \'image/object/bbox/xmin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/class/label\':\n            tf.VarLenFeature(dtype=tf.int64),\n    }\n\n    features = tf.parse_single_example(serialized_proto, keys_to_features)\n\n    bbox = None\n    if FLAGS.use_annotated_bbox:\n      xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n      ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n      xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n      ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n      # Note that we impose an ordering of (y, x) just to make life difficult.\n      bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n      # Force the variable number of bounding boxes into the shape\n      # [1, num_boxes, coords].\n      bbox = tf.expand_dims(bbox, 0)\n      bbox = tf.transpose(bbox, [0, 2, 1])\n\n    image = features[\'image/encoded\']\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    if FLAGS.preprocessing == \'vgg\':\n      image = vgg_preprocessing.preprocess_image(\n          image=image,\n          output_height=FLAGS.height,\n          output_width=FLAGS.width,\n          is_training=self.is_training,\n          resize_side_min=_RESIZE_SIDE_MIN,\n          resize_side_max=_RESIZE_SIDE_MAX)\n    elif FLAGS.preprocessing == \'inception\':\n      image = inception_preprocessing.preprocess_image(\n          image=image,\n          output_height=FLAGS.height,\n          output_width=FLAGS.width,\n          is_training=self.is_training,\n          bbox=bbox,\n          fast_mode=False)\n\n    label = tf.cast(\n        tf.reshape(features[\'image/class/label\'], shape=[]), dtype=tf.int32)\n\n    if self.use_bfloat16:\n      image = tf.cast(image, tf.bfloat16)\n\n    return image, label\n\n  def input_fn(self, params):\n    """"""Input function which provides a single batch for train or eval.\n\n    Args:\n      params: `dict` of parameters passed from the `TPUEstimator`.\n          `params[\'batch_size\']` is always provided and should be used as the\n          effective batch size.\n\n    Returns:\n      A (images, labels) tuple of `Tensor`s for a batch of samples.\n    """"""\n    batch_size = params[\'batch_size\']\n\n    if FLAGS.use_data == \'real\':\n      file_pattern = os.path.join(\n          self.data_dir, \'train-*\' if self.is_training else \'validation-*\')\n      dataset = tf.data.Dataset.list_files(file_pattern,\n                                           shuffle=self.is_training)\n\n      if self.is_training:\n        dataset = dataset.repeat()\n\n      def prefetch_dataset(filename):\n        dataset = tf.data.TFRecordDataset(\n            filename, buffer_size=FLAGS.prefetch_dataset_buffer_size)\n        return dataset\n\n      dataset = dataset.apply(\n          tf.contrib.data.parallel_interleave(\n              prefetch_dataset,\n              cycle_length=FLAGS.num_files_infeed,\n              sloppy=True))\n\n      if FLAGS.followup_shuffle_buffer_size > 0:\n        dataset = dataset.shuffle(\n            buffer_size=FLAGS.followup_shuffle_buffer_size)\n\n      dataset = dataset.map(\n          self.dataset_parser,\n          num_parallel_calls=FLAGS.num_parallel_calls)\n\n      dataset = dataset.prefetch(batch_size)\n\n      dataset = dataset.apply(\n          tf.contrib.data.batch_and_drop_remainder(batch_size))\n\n      dataset = dataset.prefetch(2)  # Prefetch overlaps in-feed with training\n\n      images, labels = dataset.make_one_shot_iterator().get_next()\n    else:\n      images = tf.random_uniform(\n          [batch_size, FLAGS.height, FLAGS.width, 3], minval=-1, maxval=1)\n      labels = tf.random_uniform(\n          [batch_size], minval=0, maxval=999, dtype=tf.int32)\n\n    images = tensor_transform_fn(images, params[\'pipeline_transpose_dims\'])\n    return images, labels\n\n\ndef tensor_transform_fn(data, perm):\n  """"""Transpose function.\n\n  This function is used to transpose an image tensor on the host and then\n  perform an inverse transpose on the TPU. The transpose on the TPU gets\n  effectively elided thus voiding any associated computational cost.\n\n  NOTE: Eventually the compiler will be able to detect when this kind of\n  operation may prove beneficial and perform these types of transformations\n  implicitly, voiding the need for user intervention\n\n  Args:\n    data: Tensor to be transposed\n    perm: New ordering of dimensions\n\n  Returns:\n    Transposed tensor\n  """"""\n  if FLAGS.transpose_enabled:\n    return tf.transpose(data, perm)\n  return data\n\n\ndef inception_model_fn(features, labels, mode, params):\n  """"""Inception v4 model using Estimator API.""""""\n  num_classes = FLAGS.num_classes\n  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n  is_eval = (mode == tf.estimator.ModeKeys.EVAL)\n  features = tensor_transform_fn(features, params[\'model_transpose_dims\'])\n\n  # This nested function allows us to avoid duplicating the logic which\n  # builds the network, for different values of --precision.\n  def build_network():\n    if FLAGS.precision == \'bfloat16\':\n      with tf.contrib.tpu.bfloat16_scope():\n        logits, end_points = inception.inception_v4(\n            features,\n            num_classes,\n            is_training=is_training)\n      logits = tf.cast(logits, tf.float32)\n    elif FLAGS.precision == \'float32\':\n      logits, end_points = inception.inception_v4(\n          features,\n          num_classes,\n          is_training=is_training)\n    return logits, end_points\n\n  if FLAGS.clear_update_collections:\n    with arg_scope(inception.inception_v4_arg_scope(\n        weight_decay=0.0,\n        batch_norm_decay=BATCH_NORM_DECAY,\n        batch_norm_epsilon=BATCH_NORM_EPSILON,\n        updates_collections=None)):\n      logits, end_points = build_network()\n  else:\n    with arg_scope(inception.inception_v4_arg_scope(\n        batch_norm_decay=BATCH_NORM_DECAY,\n        batch_norm_epsilon=BATCH_NORM_EPSILON)):\n      logits, end_points = build_network()\n\n  predictions = end_points\n  predictions.update({\n      \'classes\': tf.argmax(input=logits, axis=1),\n      \'probabilities\': tf.nn.softmax(logits, name=\'softmax_tensor\')\n  })\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n  if mode == tf.estimator.ModeKeys.EVAL and FLAGS.display_tensors and (\n      not FLAGS.use_tpu):\n    with tf.control_dependencies([\n        tf.Print(\n            predictions[\'classes\'], [predictions[\'classes\']],\n            summarize=FLAGS.eval_batch_size,\n            message=\'prediction: \')\n    ]):\n      labels = tf.Print(\n          labels, [labels], summarize=FLAGS.eval_batch_size, message=\'label: \')\n\n  one_hot_labels = tf.one_hot(labels, FLAGS.num_classes, dtype=tf.int32)\n\n  if \'AuxLogits\' in end_points:\n    tf.losses.softmax_cross_entropy(\n        onehot_labels=one_hot_labels,\n        logits=tf.cast(end_points[\'AuxLogits\'], tf.float32),\n        weights=0.4,\n        label_smoothing=0.1,\n        scope=\'aux_loss\')\n\n  tf.losses.softmax_cross_entropy(\n      onehot_labels=one_hot_labels,\n      logits=logits,\n      weights=1.0,\n      label_smoothing=0.1)\n\n  losses = tf.add_n(tf.losses.get_losses())\n  l2_loss = []\n  for v in tf.trainable_variables():\n    tf.logging.info(v.name)\n    if \'BatchNorm\' not in v.name and \'weights\' in v.name:\n      l2_loss.append(tf.nn.l2_loss(v))\n    tf.logging.info(len(l2_loss))\n  loss = losses + WEIGHT_DECAY * tf.add_n(l2_loss)\n\n  initial_learning_rate = FLAGS.learning_rate * FLAGS.train_batch_size / 256\n  # Adjust the initial learning rate for warmup\n  initial_learning_rate /= (\n      FLAGS.learning_rate_decay**((FLAGS.warmup_epochs + FLAGS.cold_epochs) /\n                                  FLAGS.learning_rate_decay_epochs))\n  final_learning_rate = 0.0001 * initial_learning_rate\n\n  host_call = None\n  train_op = None\n  if is_training:\n    batches_per_epoch = _NUM_TRAIN_IMAGES / FLAGS.train_batch_size\n    global_step = tf.train.get_or_create_global_step()\n    current_epoch = tf.cast(\n        (tf.cast(global_step, tf.float32) / batches_per_epoch), tf.int32)\n\n    clr = FLAGS.cold_learning_rate\n    wlr = initial_learning_rate / (FLAGS.warmup_epochs + FLAGS.cold_epochs)\n    learning_rate = tf.where(\n        tf.greater_equal(current_epoch, FLAGS.cold_epochs), (tf.where(\n            tf.greater_equal(current_epoch,\n                             FLAGS.warmup_epochs + FLAGS.cold_epochs),\n            tf.train.exponential_decay(\n                learning_rate=initial_learning_rate,\n                global_step=global_step,\n                decay_steps=int(FLAGS.learning_rate_decay_epochs *\n                                batches_per_epoch),\n                decay_rate=FLAGS.learning_rate_decay,\n                staircase=True), tf.multiply(\n                    tf.cast(current_epoch, tf.float32), wlr))), clr)\n\n    # Set a minimum boundary for the learning rate.\n    learning_rate = tf.maximum(\n        learning_rate, final_learning_rate, name=\'learning_rate\')\n\n    if FLAGS.optimizer == \'sgd\':\n      tf.logging.info(\'Using SGD optimizer\')\n      optimizer = tf.train.GradientDescentOptimizer(\n          learning_rate=learning_rate)\n    elif FLAGS.optimizer == \'momentum\':\n      tf.logging.info(\'Using Momentum optimizer\')\n      optimizer = tf.train.MomentumOptimizer(\n          learning_rate=learning_rate, momentum=0.9)\n    elif FLAGS.optimizer == \'RMS\':\n      tf.logging.info(\'Using RMS optimizer\')\n      optimizer = tf.train.RMSPropOptimizer(\n          learning_rate,\n          RMSPROP_DECAY,\n          momentum=RMSPROP_MOMENTUM,\n          epsilon=RMSPROP_EPSILON)\n    else:\n      tf.logging.fatal(\'Unknown optimizer:\', FLAGS.optimizer)\n\n    if FLAGS.use_tpu:\n      optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.minimize(loss, global_step=global_step)\n    if FLAGS.moving_average:\n      ema = tf.train.ExponentialMovingAverage(\n          decay=MOVING_AVERAGE_DECAY, num_updates=global_step)\n      variables_to_average = (\n          tf.trainable_variables() + tf.moving_average_variables())\n      with tf.control_dependencies([train_op]), tf.name_scope(\'moving_average\'):\n        train_op = ema.apply(variables_to_average)\n\n    # To log the loss, current learning rate, and epoch for Tensorboard, the\n    # summary op needs to be run on the host CPU via host_call. host_call\n    # expects [batch_size, ...] Tensors, thus reshape to introduce a batch\n    # dimension. These Tensors are implicitly concatenated to\n    # [params[\'batch_size\']].\n    gs_t = tf.reshape(global_step, [1])\n    loss_t = tf.reshape(loss, [1])\n    lr_t = tf.reshape(learning_rate, [1])\n    ce_t = tf.reshape(current_epoch, [1])\n\n    if not FLAGS.skip_host_call:\n      def host_call_fn(gs, loss, lr, ce):\n        """"""Training host call. Creates scalar summaries for training metrics.\n\n        This function is executed on the CPU and should not directly reference\n        any Tensors in the rest of the `model_fn`. To pass Tensors from the\n        model to the `metric_fn`, provide as part of the `host_call`. See\n        https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n        for more information.\n\n        Arguments should match the list of `Tensor` objects passed as the second\n        element in the tuple passed to `host_call`.\n\n        Args:\n          gs: `Tensor with shape `[batch]` for the global_step\n          loss: `Tensor` with shape `[batch]` for the training loss.\n          lr: `Tensor` with shape `[batch]` for the learning_rate.\n          ce: `Tensor` with shape `[batch]` for the current_epoch.\n\n        Returns:\n          List of summary ops to run on the CPU host.\n        """"""\n        gs = gs[0]\n        with summary.create_file_writer(FLAGS.model_dir).as_default():\n          with summary.always_record_summaries():\n            summary.scalar(\'loss\', tf.reduce_mean(loss), step=gs)\n            summary.scalar(\'learning_rate\', tf.reduce_mean(lr), step=gs)\n            summary.scalar(\'current_epoch\', tf.reduce_mean(ce), step=gs)\n\n            return summary.all_summary_ops()\n\n      host_call = (host_call_fn, [gs_t, loss_t, lr_t, ce_t])\n\n  eval_metrics = None\n  if is_eval:\n    def metric_fn(labels, logits):\n      """"""Evaluation metric function. Evaluates accuracy.\n\n      This function is executed on the CPU and should not directly reference\n      any Tensors in the rest of the `model_fn`. To pass Tensors from the model\n      to the `metric_fn`, provide as part of the `eval_metrics`. See\n      https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n      for more information.\n\n      Arguments should match the list of `Tensor` objects passed as the second\n      element in the tuple passed to `eval_metrics`.\n\n      Args:\n        labels: `Tensor` with shape `[batch, ]`.\n        logits: `Tensor` with shape `[batch, num_classes]`.\n\n      Returns:\n        A dict of the metrics to return from evaluation.\n      """"""\n      predictions = tf.argmax(logits, axis=1)\n      top_1_accuracy = tf.metrics.accuracy(labels, predictions)\n      in_top_5 = tf.cast(tf.nn.in_top_k(logits, labels, 5), tf.float32)\n      top_5_accuracy = tf.metrics.mean(in_top_5)\n\n      return {\n          \'accuracy\': top_1_accuracy,\n          \'accuracy@5\': top_5_accuracy,\n      }\n\n    eval_metrics = (metric_fn, [labels, logits])\n\n  return tf.contrib.tpu.TPUEstimatorSpec(\n      mode=mode,\n      loss=loss,\n      train_op=train_op,\n      host_call=host_call,\n      eval_metrics=eval_metrics)\n\n\nclass LoadEMAHook(tf.train.SessionRunHook):\n  """"""Hook to load exponential moving averages into corresponding variables.""""""\n\n  def __init__(self, model_dir):\n    super(LoadEMAHook, self).__init__()\n    self._model_dir = model_dir\n\n  def begin(self):\n    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n    variables_to_restore = ema.variables_to_restore()\n    self._load_ema = tf.contrib.framework.assign_from_checkpoint_fn(\n        tf.train.latest_checkpoint(self._model_dir), variables_to_restore)\n\n  def after_create_session(self, sess, coord):\n    tf.logging.info(\'Reloading EMA...\')\n    self._load_ema(sess)\n\n\ndef main(unused_argv):\n  del unused_argv  # Unused\n\n  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n      FLAGS.tpu,\n      zone=FLAGS.tpu_zone,\n      project=FLAGS.gcp_project)\n\n  assert FLAGS.precision == \'bfloat16\' or FLAGS.precision == \'float32\', (\n      \'Invalid value for --precision flag; must be bfloat16 or float32.\')\n  tf.logging.info(\'Precision: %s\', FLAGS.precision)\n\n  batch_size_per_shard = FLAGS.train_batch_size // FLAGS.num_shards\n  params = {\n      \'model_transpose_dims\': [0, 1, 2, 3],\n      \'pipeline_transpose_dims\': [0, 1, 2, 3],\n  }\n\n  batch_axis = 0\n  if FLAGS.transpose_enabled:\n    # On the TPU, convolutions are executed with a different leading\n    # dimension when batch size per shard is less than 64. By\n    # default images are loaded in NHWC order. For optimal performance,\n    # we want to use CHWN order while training wjem batch size per\n    # worker is smaller than 64 per shard.\n\n    if batch_size_per_shard >= 64:\n      params[\'model_transpose_dims\'] = [3, 0, 1, 2]\n      params[\'pipeline_transpose_dims\'] = [1, 2, 3, 0]\n      batch_axis = 3\n    else:\n      params[\'model_transpose_dims\'] = [2, 0, 1, 3]\n      params[\'pipeline_transpose_dims\'] = [1, 2, 0, 3]\n      batch_axis = 2\n\n  if FLAGS.eval_total_size > 0:\n    eval_size = FLAGS.eval_total_size\n  else:\n    eval_size = _NUM_EVAL_IMAGES\n  eval_steps = eval_size // FLAGS.eval_batch_size\n\n  iterations = (eval_steps if FLAGS.mode == \'eval\' else\n                FLAGS.iterations)\n\n  eval_batch_size = (None if FLAGS.mode == \'train\' else\n                     FLAGS.eval_batch_size)\n\n  per_host_input_for_training = (\n      FLAGS.num_shards <= 8 if FLAGS.mode == \'train\' else True)\n\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_secs=FLAGS.save_checkpoints_secs,\n      save_summary_steps=FLAGS.save_summary_steps,\n      session_config=tf.ConfigProto(\n          allow_soft_placement=True,\n          log_device_placement=FLAGS.log_device_placement),\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=iterations,\n          num_shards=FLAGS.num_shards,\n          per_host_input_for_training=per_host_input_for_training))\n\n  inception_classifier = tf.contrib.tpu.TPUEstimator(\n      model_fn=inception_model_fn,\n      use_tpu=FLAGS.use_tpu,\n      config=run_config,\n      params=params,\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=eval_batch_size,\n      batch_axis=(batch_axis, 0))\n\n  # Input pipelines are slightly different (with regards to shuffling and\n  # preprocessing) between training and evaluation.\n  use_bfloat16 = FLAGS.precision == \'bfloat16\'\n  imagenet_train = InputPipeline(\n      is_training=True,\n      data_dir=FLAGS.data_dir,\n      use_bfloat16=use_bfloat16)\n  imagenet_eval = InputPipeline(\n      is_training=False,\n      data_dir=FLAGS.data_dir,\n      use_bfloat16=use_bfloat16)\n\n  if FLAGS.moving_average:\n    eval_hooks = [LoadEMAHook(FLAGS.model_dir)]\n  else:\n    eval_hooks = []\n\n  if FLAGS.mode == \'eval\':\n    # Run evaluation when there is a new checkpoint\n    for checkpoint in evaluation.checkpoints_iterator(FLAGS.model_dir):\n      tf.logging.info(\'Starting to evaluate.\')\n      try:\n        start_timestamp = time.time()  # Includes compilation time\n        eval_results = inception_classifier.evaluate(\n            input_fn=imagenet_eval.input_fn,\n            steps=eval_steps,\n            hooks=eval_hooks,\n            checkpoint_path=checkpoint)\n        elapsed_time = int(time.time() - start_timestamp)\n        tf.logging.info(\n            \'Eval results: %s. Elapsed seconds: %d\', eval_results, elapsed_time)\n\n        # Terminate eval job when final checkpoint is reached\n        current_step = int(os.path.basename(checkpoint).split(\'-\')[1])\n        if current_step >= FLAGS.train_steps:\n          tf.logging.info(\n              \'Evaluation finished after training step %d\', current_step)\n          break\n      except tf.errors.NotFoundError:\n        # Since the coordinator is on a different job than the TPU worker,\n        # sometimes the TPU worker does not finish initializing until long after\n        # the CPU job tells it to start evaluating. In this case, the checkpoint\n        # file could have been deleted already.\n        tf.logging.info(\n            \'Checkpoint %s no longer exists, skipping checkpoint\', checkpoint)\n\n  elif FLAGS.mode == \'train_and_eval\':\n    for cycle in range(FLAGS.train_steps // FLAGS.train_steps_per_eval):\n      tf.logging.info(\'Starting training cycle %d.\' % cycle)\n      inception_classifier.train(\n          input_fn=imagenet_train.input_fn, steps=FLAGS.train_steps_per_eval)\n\n      tf.logging.info(\'Starting evaluation cycle %d .\' % cycle)\n      eval_results = inception_classifier.evaluate(\n          input_fn=imagenet_eval.input_fn, steps=eval_steps, hooks=eval_hooks)\n      tf.logging.info(\'Evaluation results: %s\' % eval_results)\n\n  else:\n    tf.logging.info(\'Starting training ...\')\n    inception_classifier.train(\n        input_fn=imagenet_train.input_fn, max_steps=FLAGS.train_steps)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/models/experimental/inception/inception_v4_model.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception V4 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.contrib import slim\nfrom tensorflow.contrib.layers.python.layers import layers as layers_lib\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import variable_scope\n\n\ndef block_inception_a(inputs, scope=None, reuse=None):\n  """"""Builds Inception-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope(\n      [slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1,\n      padding=\'SAME\'):\n    with variable_scope.variable_scope(\n        scope, \'BlockInceptionA\', [inputs], reuse=reuse):\n      with variable_scope.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with variable_scope.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n      with variable_scope.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0c_3x3\')\n      with variable_scope.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return array_ops.concat(\n          axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_a(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope(\n      [slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1,\n      padding=\'SAME\'):\n    with variable_scope.variable_scope(\n        scope, \'BlockReductionA\', [inputs], reuse=reuse):\n      with variable_scope.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(\n            inputs,\n            384, [3, 3],\n            stride=2,\n            padding=\'VALID\',\n            scope=\'Conv2d_1a_3x3\')\n      with variable_scope.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_1 = slim.conv2d(\n            branch_1,\n            256, [3, 3],\n            stride=2,\n            padding=\'VALID\',\n            scope=\'Conv2d_1a_3x3\')\n      with variable_scope.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(\n            inputs, [3, 3], stride=2, padding=\'VALID\', scope=\'MaxPool_1a_3x3\')\n      return array_ops.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_b(inputs, scope=None, reuse=None):\n  """"""Builds Inception-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope(\n      [slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1,\n      padding=\'SAME\'):\n    with variable_scope.variable_scope(\n        scope, \'BlockInceptionB\', [inputs], reuse=reuse):\n      with variable_scope.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with variable_scope.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope=\'Conv2d_0c_7x1\')\n      with variable_scope.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope=\'Conv2d_0b_7x1\')\n        branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope=\'Conv2d_0c_1x7\')\n        branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope=\'Conv2d_0d_7x1\')\n        branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope=\'Conv2d_0e_1x7\')\n      with variable_scope.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return array_ops.concat(\n          axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_b(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope(\n      [slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1,\n      padding=\'SAME\'):\n    with variable_scope.variable_scope(\n        scope, \'BlockReductionB\', [inputs], reuse=reuse):\n      with variable_scope.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_0 = slim.conv2d(\n            branch_0,\n            192, [3, 3],\n            stride=2,\n            padding=\'VALID\',\n            scope=\'Conv2d_1a_3x3\')\n      with variable_scope.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope=\'Conv2d_0c_7x1\')\n        branch_1 = slim.conv2d(\n            branch_1,\n            320, [3, 3],\n            stride=2,\n            padding=\'VALID\',\n            scope=\'Conv2d_1a_3x3\')\n      with variable_scope.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(\n            inputs, [3, 3], stride=2, padding=\'VALID\', scope=\'MaxPool_1a_3x3\')\n      return array_ops.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_c(inputs, scope=None, reuse=None):\n  """"""Builds Inception-C block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope(\n      [slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1,\n      padding=\'SAME\'):\n    with variable_scope.variable_scope(\n        scope, \'BlockInceptionC\', [inputs], reuse=reuse):\n      with variable_scope.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with variable_scope.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = array_ops.concat(\n            axis=3,\n            values=[\n                slim.conv2d(branch_1, 256, [1, 3], scope=\'Conv2d_0b_1x3\'),\n                slim.conv2d(branch_1, 256, [3, 1], scope=\'Conv2d_0c_3x1\')\n            ])\n      with variable_scope.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope=\'Conv2d_0b_3x1\')\n        branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope=\'Conv2d_0c_1x3\')\n        branch_2 = array_ops.concat(\n            axis=3,\n            values=[\n                slim.conv2d(branch_2, 256, [1, 3], scope=\'Conv2d_0d_1x3\'),\n                slim.conv2d(branch_2, 256, [3, 1], scope=\'Conv2d_0e_3x1\')\n            ])\n      with variable_scope.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return array_ops.concat(\n          axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef inception_v4_base(inputs, final_endpoint=\'Mixed_7d\', scope=None):\n  """"""Creates the Inception V4 network up to the given final endpoint.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    final_endpoint: specifies the endpoint to construct the network up to.\n      It can be one of [ \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'Mixed_3a\', \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n      \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\', \'Mixed_6e\',\n      \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\',\n      \'Mixed_7d\']\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n  """"""\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with variable_scope.variable_scope(scope, \'InceptionV4\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n        stride=1,\n        padding=\'SAME\'):\n      # 299 x 299 x 3\n      net = slim.conv2d(\n          inputs, 32, [3, 3], stride=2, padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      if add_and_check_final(\'Conv2d_1a_3x3\', net):\n        return net, end_points\n      # 149 x 149 x 32\n      net = slim.conv2d(net, 32, [3, 3], padding=\'VALID\', scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net):\n        return net, end_points\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 64, [3, 3], scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net):\n        return net, end_points\n      # 147 x 147 x 64\n      with variable_scope.variable_scope(\'Mixed_3a\'):\n        with variable_scope.variable_scope(\'Branch_0\'):\n          branch_0 = slim.max_pool2d(\n              net, [3, 3], stride=2, padding=\'VALID\', scope=\'MaxPool_0a_3x3\')\n        with variable_scope.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, 96, [3, 3], stride=2, padding=\'VALID\', scope=\'Conv2d_0a_3x3\')\n        net = array_ops.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_3a\', net):\n          return net, end_points\n\n      # 73 x 73 x 160\n      with variable_scope.variable_scope(\'Mixed_4a\'):\n        with variable_scope.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(\n              branch_0, 96, [3, 3], padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with variable_scope.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(\n              branch_1, 96, [3, 3], padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        net = array_ops.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_4a\', net):\n          return net, end_points\n\n      # 71 x 71 x 192\n      with variable_scope.variable_scope(\'Mixed_5a\'):\n        with variable_scope.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net,\n              192, [3, 3],\n              stride=2,\n              padding=\'VALID\',\n              scope=\'Conv2d_1a_3x3\')\n        with variable_scope.variable_scope(\'Branch_1\'):\n          branch_1 = slim.max_pool2d(\n              net, [3, 3], stride=2, padding=\'VALID\', scope=\'MaxPool_1a_3x3\')\n        net = array_ops.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_5a\', net):\n          return net, end_points\n\n      # 35 x 35 x 384\n      # 4 x Inception-A blocks\n      for idx in range(4):\n        block_scope = \'Mixed_5\' + chr(ord(\'b\') + idx)\n        net = block_inception_a(net, block_scope)\n        if add_and_check_final(block_scope, net):\n          return net, end_points\n\n      # 35 x 35 x 384\n      # Reduction-A block\n      net = block_reduction_a(net, \'Mixed_6a\')\n      if add_and_check_final(\'Mixed_6a\', net):\n        return net, end_points\n\n      # 17 x 17 x 1024\n      # 7 x Inception-B blocks\n      for idx in range(7):\n        block_scope = \'Mixed_6\' + chr(ord(\'b\') + idx)\n        net = block_inception_b(net, block_scope)\n        if add_and_check_final(block_scope, net):\n          return net, end_points\n\n      # 17 x 17 x 1024\n      # Reduction-B block\n      net = block_reduction_b(net, \'Mixed_7a\')\n      if add_and_check_final(\'Mixed_7a\', net):\n        return net, end_points\n\n      # 8 x 8 x 1536\n      # 3 x Inception-C blocks\n      for idx in range(3):\n        block_scope = \'Mixed_7\' + chr(ord(\'b\') + idx)\n        net = block_inception_c(net, block_scope)\n        if add_and_check_final(block_scope, net):\n          return net, end_points\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v4(inputs,\n                 num_classes=1001,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 reuse=None,\n                 scope=\'InceptionV4\',\n                 create_aux_logits=True):\n  """"""Creates the Inception V4 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxiliary logits.\n\n  Returns:\n    net: a Tensor with the logits (pre-softmax activations) if num_classes\n      is a non-zero integer, or the non-dropped input to the logits layer\n      if num_classes is 0 or None.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n  with variable_scope.variable_scope(\n      scope, \'InceptionV4\', [inputs], reuse=reuse) as scope:\n    with slim.arg_scope(\n        [slim.batch_norm, slim.dropout], is_training=is_training):\n      net, end_points = inception_v4_base(inputs, scope=scope)\n\n      with slim.arg_scope(\n          [slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n          stride=1,\n          padding=\'SAME\'):\n        # Auxiliary Head logits\n        if create_aux_logits and num_classes:\n          with variable_scope.variable_scope(\'AuxLogits\'):\n            # 17 x 17 x 1024\n            aux_logits = end_points[\'Mixed_6h\']\n            aux_logits = slim.avg_pool2d(\n                aux_logits, [5, 5],\n                stride=3,\n                padding=\'VALID\',\n                scope=\'AvgPool_1a_5x5\')\n            aux_logits = slim.conv2d(\n                aux_logits, 128, [1, 1], scope=\'Conv2d_1b_1x1\')\n            aux_logits = slim.conv2d(\n                aux_logits,\n                768,\n                aux_logits.get_shape()[1:3],\n                padding=\'VALID\',\n                scope=\'Conv2d_2a\')\n            aux_logits = slim.flatten(aux_logits)\n            aux_logits = slim.fully_connected(\n                aux_logits, num_classes, activation_fn=None, scope=\'Aux_logits\')\n            end_points[\'AuxLogits\'] = aux_logits\n\n        # Final pooling and prediction\n        # TODO(sguada,arnoegw): Consider adding a parameter global_pool which\n        # can be set to False to disable pooling here (as in resnet_*()).\n        with variable_scope.variable_scope(\'Logits\'):\n          # 8 x 8 x 1536\n          kernel_size = net.get_shape()[1:3]\n          if kernel_size.is_fully_defined():\n            net = slim.avg_pool2d(\n                net, kernel_size, padding=\'VALID\', scope=\'AvgPool_1a\')\n          else:\n            net = math_ops.reduce_mean(\n                net, [1, 2], keep_dims=True, name=\'global_pool\')\n          end_points[\'global_pool\'] = net\n          if not num_classes:\n            return net, end_points\n          # 1 x 1 x 1536\n          net = slim.dropout(net, dropout_keep_prob, scope=\'Dropout_1b\')\n          net = slim.flatten(net, scope=\'PreLogitsFlatten\')\n          end_points[\'PreLogitsFlatten\'] = net\n          # 1536\n          logits = slim.fully_connected(\n              net, num_classes, activation_fn=None, scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = layers_lib.softmax(\n              logits, scope=\'Predictions\')\n    return logits, end_points\n\n\ninception_v4.default_image_size = 299\n\n\ndef inception_v4_arg_scope(weight_decay=0.00004,\n                           batch_norm_var_collection=\'moving_vars\',\n                           batch_norm_decay=0.9997,\n                           batch_norm_epsilon=0.001,\n                           updates_collections=ops.GraphKeys.UPDATE_OPS,\n                           use_fused_batchnorm=True,\n                           activation_fn=nn_ops.relu):\n  """"""Defines the default InceptionV3 arg scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_var_collection: The name of the collection for the batch norm\n      variables.\n    batch_norm_decay: Decay for batch norm moving average\n    batch_norm_epsilon: Small float added to variance to avoid division by zero\n    updates_collections: Collections for the update ops of the layer\n    use_fused_batchnorm: Enable fused batchnorm.\n    activation_fn: Activation function for conv2d.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      # collection containing update_ops.\n      \'updates_collections\': updates_collections,\n      # Use fused batch norm if possible.\n      \'fused\': use_fused_batchnorm,\n      # collection containing the moving mean and moving variance.\n      \'variables_collections\': {\n          \'beta\': None,\n          \'gamma\': None,\n          \'moving_mean\': [batch_norm_var_collection],\n          \'moving_variance\': [batch_norm_var_collection],\n      }\n  }\n\n  normalizer_fn = slim.batch_norm\n  normalizer_params = batch_norm_params\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected],\n      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=slim.variance_scaling_initializer(),\n        activation_fn=activation_fn,\n        normalizer_fn=normalizer_fn,\n        normalizer_params=normalizer_params) as sc:\n      return sc\n'"
tpu/models/experimental/inception/vgg_preprocessing.py,60,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nThe preprocessing steps for VGG were introduced in the following technical\nreport:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n  """"""Crops the given image using the provided offsets and sizes.\n\n  Note that the method doesn\'t assume we know the input image size but it does\n  assume we know the input image rank.\n\n  Args:\n    image: an image of shape [height, width, channels].\n    offset_height: a scalar tensor indicating the height offset.\n    offset_width: a scalar tensor indicating the width offset.\n    crop_height: the height of the cropped image.\n    crop_width: the width of the cropped image.\n\n  Returns:\n    the cropped (and resized) image.\n\n  Raises:\n    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n      less than the crop size.\n  """"""\n  original_shape = tf.shape(image)\n\n  rank_assertion = tf.Assert(\n      tf.equal(tf.rank(image), 3),\n      [\'Rank of image must be equal to 3.\'])\n  with tf.control_dependencies([rank_assertion]):\n    cropped_shape = tf.stack([crop_height, crop_width, original_shape[2]])\n\n  size_assertion = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(original_shape[0], crop_height),\n          tf.greater_equal(original_shape[1], crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))\n\n  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n  # define the crop size.\n  with tf.control_dependencies([size_assertion]):\n    image = tf.slice(image, offsets, cropped_shape)\n  return tf.reshape(image, cropped_shape)\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n  """"""Crops the given list of images.\n\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n\n  Returns:\n    the image_list with cropped images.\n\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  """"""\n  if not image_list:\n    raise ValueError(\'Empty image_list.\')\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n        tf.equal(image_rank, 3),\n        [\'Wrong rank for tensor  %s [expected] [actual]\',\n         image_list[i].name, 3, image_rank])\n    rank_assertions.append(rank_assert)\n\n  with tf.control_dependencies([rank_assertions[0]]):\n    image_shape = tf.shape(image_list[0])\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(image_height, crop_height),\n          tf.greater_equal(image_width, crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    with tf.control_dependencies([rank_assertions[i]]):\n      shape = tf.shape(image)\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n        tf.equal(height, image_height),\n        [\'Wrong height for tensor %s [expected][actual]\',\n         image.name, height, image_height])\n    width_assert = tf.Assert(\n        tf.equal(width, image_width),\n        [\'Wrong width for tensor %s [expected][actual]\',\n         image.name, width, image_width])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  with tf.control_dependencies(asserts):\n    max_offset_height = tf.reshape(image_height - crop_height + 1, [])\n  with tf.control_dependencies(asserts):\n    max_offset_width = tf.reshape(image_width - crop_width + 1, [])\n  offset_height = tf.random_uniform(\n      [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random_uniform(\n      [], maxval=max_offset_width, dtype=tf.int32)\n\n  return [_crop(image, offset_height, offset_width,\n                crop_height, crop_width) for image in image_list]\n\n\ndef _central_crop(image_list, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    the list of cropped images.\n  """"""\n  outputs = []\n  for image in image_list:\n    image_height = tf.shape(image)[0]\n    image_width = tf.shape(image)[1]\n\n    offset_height = (image_height - crop_height) / 2\n    offset_width = (image_width - crop_width) / 2\n\n    outputs.append(_crop(image, offset_height, offset_width,\n                         crop_height, crop_width))\n  return outputs\n\n\ndef _mean_image_subtraction(image, means):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n  num_channels = image.get_shape().as_list()[-1]\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] -= means[i]\n  return tf.concat(axis=2, values=channels)\n\n\ndef _smallest_size_at_least(height, width, smallest_side):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: and int32 scalar tensor indicating the new width.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  height = tf.to_float(height)\n  width = tf.to_float(width)\n  smallest_side = tf.to_float(smallest_side)\n\n  scale = tf.cond(tf.greater(height, width),\n                  lambda: smallest_side / width,\n                  lambda: smallest_side / height)\n  new_height = tf.to_int32(height * scale)\n  new_width = tf.to_int32(width * scale)\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, smallest_side):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  shape = tf.shape(image)\n  height = shape[0]\n  width = shape[1]\n  new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n  image = tf.expand_dims(image, 0)\n  resized_image = tf.image.resize_bilinear(image, [new_height, new_width],\n                                           align_corners=False)\n  resized_image = tf.squeeze(resized_image)\n  resized_image.set_shape([None, None, 3])\n  return resized_image\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         resize_side_min=_RESIZE_SIDE_MIN,\n                         resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  resize_side = tf.random_uniform(\n      [], minval=resize_side_min, maxval=resize_side_max+1, dtype=tf.int32)\n\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _random_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  image = tf.image.random_flip_left_right(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_for_eval(image, output_height, output_width, resize_side):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side: The smallest side of the image for aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _central_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_image(image, output_height, output_width, is_training=False,\n                     resize_side_min=_RESIZE_SIDE_MIN,\n                     resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, then this value\n      is used for rescaling.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, this value is\n      ignored. Otherwise, the resize side is sampled from\n        [resize_size_min, resize_size_max].\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    image = preprocess_for_train(image, output_height, output_width,\n                                 resize_side_min, resize_side_max)\n  else:\n    image = preprocess_for_eval(image, output_height, output_width,\n                                resize_side_min)\n  # Scale to (-1,1). TODO(currutia): check whether this is actually needed\n  image = tf.multiply(image, 1. / 128.)\n  return image\n'"
tpu/models/experimental/keras/imagenet_input.py,33,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Efficient ImageNet input pipeline using tf.data.Dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\n\nimport resnet_preprocessing\n\n\ndef image_serving_input_fn():\n  """"""Serving input fn for raw images.""""""\n\n  def _preprocess_image(image_bytes):\n    """"""Preprocess a single raw image.""""""\n    image = resnet_preprocessing.preprocess_image(\n        image_bytes=image_bytes, is_training=False)\n    return image\n\n  image_bytes_list = tf.placeholder(\n      shape=[None],\n      dtype=tf.string,\n  )\n  images = tf.map_fn(\n      _preprocess_image, image_bytes_list, back_prop=False, dtype=tf.float32)\n  return tf.estimator.export.ServingInputReceiver(\n      images, {\'image_bytes\': image_bytes_list})\n\nclass ImageNetInput(object):\n  """"""Generates ImageNet input_fn for training or evaluation.\n\n  The training data is assumed to be in TFRecord format with keys as specified\n  in the dataset_parser below, sharded across 1024 files, named sequentially:\n      train-00000-of-01024\n      train-00001-of-01024\n      ...\n      train-01023-of-01024\n\n  The validation data is in the same format but sharded in 128 files.\n\n  The format of the data required is created by the script at:\n      https://github.com/tensorflow/tpu/blob/master/tools/datasets/imagenet_to_gcs.py\n\n  Args:\n    is_training: `bool` for whether the input is for training\n    data_dir: `str` for the directory of the training and validation data;\n        if \'null\' (the literal string \'null\', not None), then construct a null\n        pipeline, consisting of empty images.\n    per_core_batch_size: The per-TPU-core batch size to use.\n  """"""\n\n  def __init__(self, is_training, data_dir, per_core_batch_size=128):\n    self.image_preprocessing_fn = resnet_preprocessing.preprocess_image\n    self.is_training = is_training\n    self.data_dir = data_dir\n    if self.data_dir == \'null\' or self.data_dir == \'\':\n      self.data_dir = None\n    self.per_core_batch_size = per_core_batch_size\n\n  def dataset_parser(self, value):\n    """"""Parse an ImageNet record from a serialized string Tensor.""""""\n    keys_to_features = {\n        \'image/encoded\':\n            tf.FixedLenFeature((), tf.string, \'\'),\n        \'image/format\':\n            tf.FixedLenFeature((), tf.string, \'jpeg\'),\n        \'image/class/label\':\n            tf.FixedLenFeature([], tf.int64, -1),\n        \'image/class/text\':\n            tf.FixedLenFeature([], tf.string, \'\'),\n        \'image/object/bbox/xmin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/class/label\':\n            tf.VarLenFeature(dtype=tf.int64),\n    }\n\n    parsed = tf.parse_single_example(value, keys_to_features)\n    image_bytes = tf.reshape(parsed[\'image/encoded\'], shape=[])\n\n    image = self.image_preprocessing_fn(\n        image_bytes=image_bytes,\n        is_training=self.is_training,\n        use_bfloat16=False)\n\n    # Subtract one so that labels are in [0, 1000), and cast to float32 for\n    # Keras model.\n    label = tf.cast(tf.cast(\n        tf.reshape(parsed[\'image/class/label\'], shape=[1]), dtype=tf.int32) - 1,\n                    dtype=tf.float32)\n\n    return image, label\n\n  def input_fn(self):\n    """"""Input function which provides a single batch for train or eval.\n\n    Returns:\n      A `tf.data.Dataset` object.\n    """"""\n    if self.data_dir is None:\n      tf.logging.info(\'Using fake input.\')\n      return self.input_fn_null()\n\n    # Shuffle the filenames to ensure better randomization.\n    file_pattern = os.path.join(\n        self.data_dir, \'train-*\' if self.is_training else \'validation-*\')\n    dataset = tf.data.Dataset.list_files(file_pattern, shuffle=self.is_training)\n\n    if self.is_training:\n      dataset = dataset.repeat()\n\n    def fetch_dataset(filename):\n      buffer_size = 8 * 1024 * 1024     # 8 MiB per file\n      dataset = tf.data.TFRecordDataset(filename, buffer_size=buffer_size)\n      return dataset\n\n    # Read the data from disk in parallel\n    dataset = dataset.apply(\n        tf.contrib.data.parallel_interleave(\n            fetch_dataset, cycle_length=16, sloppy=True))\n    dataset = dataset.shuffle(1024)\n\n    # Parse, pre-process, and batch the data in parallel\n    dataset = dataset.apply(\n        tf.contrib.data.map_and_batch(\n            self.dataset_parser, batch_size=self.per_core_batch_size,\n            num_parallel_batches=2,\n            drop_remainder=True))\n\n    # Prefetch overlaps in-feed with training\n    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n    return dataset\n\n  # TODO(xiejw): Remove this generator once evaluation with dataset is ready.\n  def evaluation_generator(self, master):\n    """"""Creates a generator for evaluation.""""""\n    next_batch = self.input_fn().make_one_shot_iterator().get_next()\n    with tf.Session(master) as sess:\n      while True:\n        try:\n          yield sess.run(next_batch)\n        except tf.errors.OutOfRangeError:\n          return\n\n  def input_fn_null(self):\n    """"""Input function which provides null (black) images.""""""\n    dataset = tf.data.Dataset.range(1).repeat().map(self._get_null_input)\n    dataset = dataset.prefetch(self.per_core_batch_size)\n\n    dataset = dataset.batch(self.per_core_batch_size, drop_remainder=True)\n\n    dataset = dataset.prefetch(32)     # Prefetch overlaps in-feed with training\n    tf.logging.info(\'Input dataset: %s\', str(dataset))\n    return dataset\n\n  def _get_null_input(self, _):\n    null_image = tf.zeros([224, 224, 3], tf.float32)\n    return null_image, tf.constant(0, tf.float32)\n'"
tpu/models/experimental/keras/mnist.py,19,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Experimental Keras MNIST Example.\n\nTo test on CPU:\n    python mnist.py --use_tpu=False [--fake_data=true]\n\nTo test on TPU:\n    python mnist.py --use_tpu=True [--tpu=$TPU_NAME]\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# Standard Imports\nfrom absl import flags\nimport numpy as np\nimport tensorflow as tf\n\nflags.DEFINE_bool(\'use_tpu\', True, \'Use TPU model instead of CPU.\')\nflags.DEFINE_string(\'tpu\', None, \'Name of the TPU to use.\')\n\nflags.DEFINE_bool(\'fake_data\', False, \'Use fake data to test functionality.\')\n\nBATCH_SIZE = 128\nNUM_CLASSES = 10\nEPOCHS = 12\n\n# input image dimensions\nIMG_ROWS, IMG_COLS = 28, 28\n\nFLAGS = flags.FLAGS\n\n\ndef mnist_model(input_shape):\n  """"""Creates a MNIST model.""""""\n  model = tf.keras.models.Sequential()\n  model.add(\n      tf.keras.layers.Conv2D(\n          32, kernel_size=(3, 3), activation=\'relu\', input_shape=input_shape))\n  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation=\'relu\'))\n  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n  model.add(tf.keras.layers.Dropout(0.25))\n  model.add(tf.keras.layers.Flatten())\n  model.add(tf.keras.layers.Dense(128, activation=\'relu\'))\n  model.add(tf.keras.layers.Dropout(0.5))\n  model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=\'softmax\'))\n  return model\n\n\ndef main(unused_dev):\n  use_tpu = FLAGS.use_tpu\n\n  print(\'Mode:\', \'TPU\' if use_tpu else \'CPU\')\n\n  if FLAGS.fake_data:\n    print(\'Using fake data\')\n    x_train = np.random.random((128, IMG_ROWS, IMG_COLS))\n    y_train = np.zeros([128, 1], dtype=np.int32)\n    x_test, y_test = x_train, y_train\n  else:\n    # the data, split between train and test sets\n    print(\'Using real data\')\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n  x_train = x_train.reshape(x_train.shape[0], IMG_ROWS, IMG_COLS, 1)\n  x_test = x_test.reshape(x_test.shape[0], IMG_ROWS, IMG_COLS, 1)\n  input_shape = (IMG_ROWS, IMG_COLS, 1)\n\n  x_train = x_train.astype(\'float32\')\n  x_test = x_test.astype(\'float32\')\n  x_train /= 255\n  x_test /= 255\n  print(\'x_train shape:\', x_train.shape)\n  print(x_train.shape[0], \'train samples\')\n  print(x_test.shape[0], \'test samples\')\n\n  # convert class vectors to binary class matrices\n  y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n  y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n\n  model = mnist_model(input_shape)\n\n  if use_tpu:\n    strategy = tf.contrib.tpu.TPUDistributionStrategy(\n        tf.contrib.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    )\n    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n\n  model.compile(\n      loss=tf.keras.losses.categorical_crossentropy,\n      optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05),\n      metrics=[\'accuracy\'])\n\n  model.fit(\n      x_train,\n      y_train,\n      batch_size=BATCH_SIZE,\n      epochs=EPOCHS,\n      verbose=1,\n      validation_data=(x_test, y_test))\n  score = model.evaluate(x_test, y_test, verbose=0)\n  print(\'Loss for final step:\', score[0])\n  print(\'Accuracy \', score[1])\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/models/experimental/keras/resnet50.py,7,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""ResNet-50 implemented with Keras running on Cloud TPUs.\n\nThis file shows how you can run ResNet-50 on a Cloud TPU using the TensorFlow\nKeras support. This is configured for ImageNet (e.g. 1000 classes), but you can\neasily adapt to your own datasets by changing the code appropriately.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nfrom absl import logging\nimport tensorflow as tf\nimport numpy as np\n\nimport imagenet_input\n\ntry:\n  import h5py as _  # pylint: disable=g-import-not-at-top\n  HAS_H5PY = True\nexcept ImportError:\n  logging.warning(\'`h5py` is not installed. Please consider installing it \'\n                  \'to save weights for long-running training.\')\n  HAS_H5PY = False\n\n\nflags.DEFINE_bool(\'use_tpu\', True, \'Use TPU model instead of CPU.\')\nflags.DEFINE_string(\'tpu\', None, \'Name of the TPU to use.\')\nflags.DEFINE_string(\'data\', None, \'Path to training and testing data.\')\n\nFLAGS = flags.FLAGS\n\nPER_CORE_BATCH_SIZE = 128\nNUM_CLASSES = 1000\nIMAGE_SIZE = 224\nEPOCHS = 90  # Standard imagenet training regime.\nAPPROX_IMAGENET_TRAINING_IMAGES = 1280000  # Approximate number of images.\nAPPROX_IMAGENET_TEST_IMAGES = 48000  # Approximate number of images.\n\nBASE_LEARNING_RATE = 0.4\n\nWEIGHTS_TXT = \'/tmp/resnet50_weights.h5\'\n\n\ndef main(argv):\n  logging.info(\'Building Keras ResNet-50 model.\')\n  model = tf.keras.applications.resnet50.ResNet50(\n      include_top=True,\n      weights=None,\n      input_tensor=None,\n      input_shape=None,\n      pooling=None,\n      classes=NUM_CLASSES)\n\n  num_cores = 8\n  batch_size = PER_CORE_BATCH_SIZE * num_cores\n\n  if FLAGS.use_tpu:\n    logging.info(\'Converting from CPU to TPU model.\')\n    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n    session_master = resolver.master()\n  else:\n    session_master = \'\'\n\n  logging.info(\'Compiling model.\')\n  model.compile(\n      optimizer=tf.keras.optimizers.SGD(lr=BASE_LEARNING_RATE,\n                                        momentum=0.9,\n                                        nesterov=True),\n      loss=\'sparse_categorical_crossentropy\',\n      metrics=[\'sparse_categorical_accuracy\'])\n\n  if FLAGS.data is None:\n    training_images = np.random.randn(\n        batch_size, IMAGE_SIZE, IMAGE_SIZE, 3).astype(np.float32)\n    training_labels = np.random.randint(NUM_CLASSES, size=batch_size,\n                                        dtype=np.int32)\n    logging.info(\'Training model using synthetica data.\')\n    model.fit(training_images, training_labels, epochs=EPOCHS,\n              batch_size=batch_size)\n    logging.info(\'Evaluating the model on synthetic data.\')\n    model.evaluate(training_images, training_labels, verbose=0)\n  else:\n    imagenet_train, imagenet_eval = [imagenet_input.ImageNetInput(\n        is_training=is_training,\n        data_dir=FLAGS.data,\n        per_core_batch_size=PER_CORE_BATCH_SIZE)\n                                     for is_training in [True, False]]\n    logging.info(\'Training model using real data in directory ""%s"".\',\n                 FLAGS.data)\n    model.fit(imagenet_train.input_fn,\n              epochs=EPOCHS,\n              steps_per_epoch=int(APPROX_IMAGENET_TRAINING_IMAGES / batch_size))\n\n    if HAS_H5PY:\n      logging.info(\'Save weights into %s\', WEIGHTS_TXT)\n      model.save_weights(WEIGHTS_TXT, overwrite=True)\n\n    logging.info(\'Evaluating the model on the validation dataset.\')\n    score = model.evaluate(\n        imagenet_eval.input_fn,\n        steps=int(APPROX_IMAGENET_TEST_IMAGES // batch_size),\n        verbose=1)\n    print(\'Evaluation score\', score)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/models/experimental/keras/resnet_preprocessing.py,30,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""ImageNet preprocessing for ResNet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nIMAGE_SIZE = 224\nCROP_PADDING = 32\n\n\ndef distorted_bounding_box_crop(image_bytes,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image_bytes: `Tensor` of binary image data.\n    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n        where each coordinate is [0, 1) and the coordinates are arranged\n        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n        image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n        area of the image must contain at least this fraction of any bounding\n        box supplied.\n    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n        image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `float`s. The cropped area of the image\n        must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n        region of the image of the specified constraints. After `max_attempts`\n        failures, return the entire image.\n    scope: Optional `str` for name scope.\n  Returns:\n    (cropped image `Tensor`, distorted bbox `Tensor`).\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image_bytes, bbox]):\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        shape,\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n    target_height, target_width, _ = tf.unstack(bbox_size)\n    crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n\n    return image\n\n\ndef _at_least_x_are_equal(a, b, x):\n  """"""At least `x` of `a` and `b` `Tensors` are equal.""""""\n  match = tf.equal(a, b)\n  match = tf.cast(match, tf.int32)\n  return tf.greater_equal(tf.reduce_sum(match), x)\n\n\ndef _decode_and_random_crop(image_bytes):\n  """"""Make a random crop of IMAGE_SIZE.""""""\n  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n  image = distorted_bounding_box_crop(\n      image_bytes,\n      bbox,\n      min_object_covered=0.1,\n      aspect_ratio_range=(3. / 4, 4. / 3.),\n      area_range=(0.08, 1.0),\n      max_attempts=10,\n      scope=None)\n  original_shape = tf.image.extract_jpeg_shape(image_bytes)\n  bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)\n\n  image = tf.cond(\n      bad,\n      lambda: _decode_and_center_crop(image_bytes),\n      lambda: tf.image.resize_bicubic([image],  # pylint: disable=g-long-lambda\n                                      [IMAGE_SIZE, IMAGE_SIZE])[0])\n\n  return image\n\n\ndef _decode_and_center_crop(image_bytes):\n  """"""Crops to center of image with padding then scales IMAGE_SIZE.""""""\n  shape = tf.image.extract_jpeg_shape(image_bytes)\n  image_height = shape[0]\n  image_width = shape[1]\n\n  padded_center_crop_size = tf.cast(\n      ((IMAGE_SIZE / (IMAGE_SIZE + CROP_PADDING)) *\n       tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n      tf.int32)\n\n  offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n  offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n  crop_window = tf.stack([offset_height, offset_width,\n                          padded_center_crop_size, padded_center_crop_size])\n  image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n  image = tf.image.resize_bicubic([image], [IMAGE_SIZE, IMAGE_SIZE])[0]\n\n  return image\n\n\ndef _flip(image):\n  """"""Random horizontal image flip.""""""\n  image = tf.image.random_flip_left_right(image)\n  return image\n\n\ndef preprocess_for_train(image_bytes, use_bfloat16):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.\n    use_bfloat16: `bool` for whether to use bfloat16.\n\n  Returns:\n    A preprocessed image `Tensor`.\n  """"""\n  image = _decode_and_random_crop(image_bytes)\n  image = _flip(image)\n  image = tf.reshape(image, [IMAGE_SIZE, IMAGE_SIZE, 3])\n  image = tf.image.convert_image_dtype(\n      image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n  return image\n\n\ndef preprocess_for_eval(image_bytes, use_bfloat16):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.\n    use_bfloat16: `bool` for whether to use bfloat16.\n\n  Returns:\n    A preprocessed image `Tensor`.\n  """"""\n  image = _decode_and_center_crop(image_bytes)\n  image = tf.reshape(image, [IMAGE_SIZE, IMAGE_SIZE, 3])\n  image = tf.image.convert_image_dtype(\n      image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n  return image\n\n\ndef preprocess_image(image_bytes, is_training=False, use_bfloat16=False):\n  """"""Preprocesses the given image.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.\n    is_training: `bool` for whether the preprocessing is for training.\n    use_bfloat16: `bool` for whether to use bfloat16.\n\n  Returns:\n    A preprocessed image `Tensor`.\n  """"""\n  if is_training:\n    return preprocess_for_train(image_bytes, use_bfloat16)\n  else:\n    return preprocess_for_eval(image_bytes, use_bfloat16)\n'"
tpu/models/experimental/keras_colab/shakespeare_lstm.py,17,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Colab Example for Shakespeare LSTM example.\n\nTo test on TPU:\n    python shapespear_lstm.py --use_tpu=True [--tpu=$TPU_NAME]\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nfrom absl import app\nfrom absl import flags\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nflags.DEFINE_bool(\'use_tpu\', True, \'Use TPU model instead of CPU.\')\nflags.DEFINE_string(\'tpu\', None, \'Name of the TPU to use.\')\n\nFLAGS = flags.FLAGS\n\n\n# The data can be obtained from http://www.gutenberg.org/files/100/100-0.txt\nSHAKESPEARE_TXT = \'gs://cloud-tpu-artifacts/shakespeare/shakespeare.txt\'\n\nWEIGHTS_TXT = \'/tmp/bard.h5\'\n\nEMBEDDING_DIM = 512\n\n\ndef transform(txt, pad_to=None):\n  """"""Transforms the input `txt` to model sequence data.""""""\n  # drop any non-ascii characters\n  output = np.asarray([ord(c) for c in txt if ord(c) < 255],\n                      dtype=np.int32)\n  if pad_to is not None:\n    output = output[:pad_to]\n    output = np.concatenate([\n        np.zeros([pad_to - len(txt)], dtype=np.int32),\n        output,\n    ])\n  return output\n\n\ndef training_generator(data, seq_len=100, batch_size=1024):\n  """"""A generator yields (seq, target) arrays for training.""""""\n  while True:\n    offsets = np.random.randint(0, len(data) - seq_len, batch_size)\n\n    # Our model uses sparse crossentropy loss, but Keras requires labels\n    # to have the same rank as the input logits.  We add an empty final\n    # dimension to account for this.\n    yield (\n        np.stack([data[idx:idx + seq_len] for idx in offsets]),\n        np.expand_dims(\n            np.stack([data[idx + 1:idx + seq_len + 1] for idx in offsets]),\n            -1),\n    )\n\n\ndef lstm_model(seq_len=100, batch_size=None, stateful=True):\n  """"""Language model: predict the next char given the current sequence.""""""\n  source = tf.keras.Input(\n      name=\'seed\', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n\n  embedding = tf.keras.layers.Embedding(\n      input_dim=256, output_dim=EMBEDDING_DIM)(source)\n  lstm_1 = tf.keras.layers.LSTM(\n      EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n  lstm_2 = tf.keras.layers.LSTM(\n      EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n  predicted_char = tf.keras.layers.TimeDistributed(\n      tf.keras.layers.Dense(256, activation=\'softmax\'))(lstm_2)\n\n  model = tf.keras.Model(\n      inputs=[source], outputs=[predicted_char],\n  )\n  model.compile(\n      optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01),\n      loss=\'sparse_categorical_crossentropy\',\n      metrics=[\'sparse_categorical_accuracy\'])\n  return model\n\n\ndef main(unused_dev):\n  with tf.gfile.GFile(SHAKESPEARE_TXT, \'r\') as f:\n    txt = f.read()\n\n  print(\'Input text [{}]: {}\'.format(len(txt), txt[:50]))\n  data = transform(txt)\n\n  seq_len = 10\n  x, y = six.next(training_generator(data, seq_len=seq_len, batch_size=1))\n  print(\'Random sample of the data (seq_len={}):\'.format(seq_len))\n  print(\'  x:\', x)\n  print(\'  y:\', y)\n\n  seq_len = 100\n  training_model = lstm_model(seq_len=seq_len, batch_size=None, stateful=False)\n\n  print()\n  print(\'Model Summary\')\n  training_model.summary()\n\n  if FLAGS.use_tpu:\n    strategy = tf.contrib.tpu.TPUDistributionStrategy(\n        tf.contrib.cluster_resolver.TPUClusterResolver(tpu=flags.FLAGS.tpu)\n    )\n    training_model = tf.contrib.tpu.keras_to_tpu_model(\n        training_model, strategy=strategy)\n\n  print(\'Training on\', \'TPU\' if FLAGS.use_tpu else \'CPU\')\n  training_model.fit_generator(\n      training_generator(data, seq_len=seq_len, batch_size=1024),\n      steps_per_epoch=100,\n      epochs=10,\n  )\n  training_model.save_weights(WEIGHTS_TXT, overwrite=True)\n\n  print(\'Running inference on the CPU.\')\n  batch_size = 5\n  predict_len = 500\n\n  # We seed the model with our initial string, copied batch_size times\n  seed_txt = \'Looks it not like the king?  Verily, we must go! \'\n  print(\'Seed:\', seed_txt)\n\n  seed = transform(seed_txt)\n  seed = np.repeat(np.expand_dims(seed, 0), batch_size, axis=0)\n\n  # Keras requires the batch size be specified ahead of time for stateful\n  # models.  We use a sequence length of 1, as we will be feeding in one\n  # character at a time and predicting the next character.\n  prediction_model = lstm_model(seq_len=1, batch_size=batch_size, stateful=True)\n  prediction_model.load_weights(WEIGHTS_TXT)\n  if FLAGS.use_tpu:\n    strategy = tf.contrib.tpu.TPUDistributionStrategy(\n        tf.contrib.cluster_resolver.TPUClusterResolver(tpu=flags.FLAGS.tpu))\n    prediction_model = tf.contrib.tpu.keras_to_tpu_model(\n        prediction_model, strategy=strategy)\n\n  # First, run the seed forward to prime the state of the model.\n  prediction_model.reset_states()\n  for i in range(len(seed_txt) - 1):\n    prediction_model.predict(seed[:, i:i + 1])\n\n  # Now we can accumulate predictions!\n  predictions = [seed[:, -1:]]\n  for i in range(predict_len):\n    last_word = predictions[-1]\n    next_probits = prediction_model.predict(last_word)[:, 0, :]\n\n    # sample from our output distribution\n    next_idx = [\n        np.random.choice(256, p=next_probits[i])\n        for i in range(batch_size)\n    ]\n    predictions.append(np.asarray(next_idx, dtype=np.int32))\n\n  for i in range(batch_size):\n    print(\'\\nPREDICTION %d\\n\\n\' % i)\n    p = [predictions[j][i] for j in range(predict_len)]\n    print(\'\'.join([chr(c) for c in p]))\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  app.run(main)\n'"
tpu/models/experimental/mnist_keras/mnist.py,19,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Experimental Keras MNIST Example.\n\nTo test on CPU:\n    python mnist.py --use_tpu=False [--fake_data=true]\n\nTo test on TPU:\n    python mnist.py --use_tpu=True [--tpu=$TPU_NAME]\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# Standard Imports\nfrom absl import app\nfrom absl import flags\nimport numpy as np\nimport tensorflow as tf\n\nflags.DEFINE_bool(\'use_tpu\', True, \'Use TPU model instead of CPU.\')\nflags.DEFINE_string(\'tpu\', None, \'Name of the TPU to use.\')\nflags.DEFINE_string(\n    \'model_dir\', None,\n    (\'The directory where the model and training/evaluation summaries \'\n     \'are stored. If unset, no summaries will be stored.\'))\n\nflags.DEFINE_bool(\'fake_data\', False, \'Use fake data to test functionality.\')\n\nBATCH_SIZE = 128\nNUM_CLASSES = 10\nEPOCHS = 12\n\n# input image dimensions\nIMG_ROWS, IMG_COLS = 28, 28\n\nFLAGS = flags.FLAGS\n\n\ndef mnist_model(input_shape):\n  """"""Creates a MNIST model.""""""\n  model = tf.keras.models.Sequential()\n  model.add(\n      tf.keras.layers.Conv2D(\n          32, kernel_size=(3, 3), activation=\'relu\', input_shape=input_shape))\n  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation=\'relu\'))\n  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n  model.add(tf.keras.layers.Dropout(0.25))\n  model.add(tf.keras.layers.Flatten())\n  model.add(tf.keras.layers.Dense(128, activation=\'relu\'))\n  model.add(tf.keras.layers.Dropout(0.5))\n  model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=\'softmax\'))\n  return model\n\n\ndef main(unused_dev):\n  use_tpu = FLAGS.use_tpu\n\n  print(\'Mode:\', \'TPU\' if use_tpu else \'CPU\')\n\n  if FLAGS.fake_data:\n    print(\'Using fake data\')\n    x_train = np.random.random((128, IMG_ROWS, IMG_COLS))\n    y_train = np.zeros([128, 1], dtype=np.int32)\n    x_test, y_test = x_train, y_train\n  else:\n    # the data, split between train and test sets\n    print(\'Using real data\')\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n  x_train = x_train.reshape(x_train.shape[0], IMG_ROWS, IMG_COLS, 1)\n  x_test = x_test.reshape(x_test.shape[0], IMG_ROWS, IMG_COLS, 1)\n  input_shape = (IMG_ROWS, IMG_COLS, 1)\n\n  x_train = x_train.astype(\'float32\')\n  x_test = x_test.astype(\'float32\')\n  x_train /= 255\n  x_test /= 255\n  print(\'x_train shape:\', x_train.shape)\n  print(x_train.shape[0], \'train samples\')\n  print(x_test.shape[0], \'test samples\')\n\n  # convert class vectors to binary class matrices\n  y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n  y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n\n  model = mnist_model(input_shape)\n\n  if use_tpu:\n    strategy = tf.contrib.tpu.TPUDistributionStrategy(\n        tf.contrib.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    )\n    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n\n  model.compile(\n      loss=tf.keras.losses.categorical_crossentropy,\n      optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05),\n      metrics=[\'accuracy\'])\n\n  callbacks = []\n  if FLAGS.model_dir:\n    callbacks = [tf.keras.callbacks.TensorBoard(log_dir=FLAGS.model_dir)]\n\n  model.fit(\n      x_train,\n      y_train,\n      batch_size=BATCH_SIZE,\n      callbacks=callbacks,\n      epochs=EPOCHS,\n      verbose=1,\n      validation_data=(x_test, y_test))\n  score = model.evaluate(x_test, y_test, verbose=0)\n  print(\'Loss for final step:\', score[0])\n  print(\'Accuracy \', score[1])\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  app.run(main)\n'"
tpu/models/experimental/ncf/ncf_main.py,58,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""NCF recommendation model with TPU embedding.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport threading\n\nfrom absl import app as absl_app\nfrom absl import flags\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.contrib import tpu\nimport tpu_embedding\nfrom official.datasets import movielens\nfrom official.recommendation import constants as rconst\nfrom official.recommendation import data_preprocessing\nfrom official.recommendation import neumf_model\nfrom official.utils.flags import core as flags_core\n\n_TOP_K = 10  # Top-k list for evaluation\n\n# keys for evaluation metrics\n_HR_KEY = ""HR""\n_NDCG_KEY = ""NDCG""\n\n_NUM_EPOCHS = 15\n\nGraphSpec = collections.namedtuple(""GraphSpec"", [\n    ""graph"", ""embedding"", ""run_tpu_loop"", ""get_infeed_thread_fn"", ""hook_before"",\n    ""hook_after""\n])\n\n\ndef main(_):\n  """"""Train NCF model and evaluate its hit rate (HR) metric.""""""\n  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n      FLAGS.tpu,\n      zone=FLAGS.tpu_zone,\n      project=FLAGS.gcp_project)\n  master = tpu_cluster_resolver.master()\n\n  ncf_dataset, cleanup_fn = data_preprocessing.instantiate_pipeline(\n      dataset=FLAGS.dataset,\n      data_dir=FLAGS.data_dir,\n      # TODO(shizhiw): support multihost.\n      batch_size=FLAGS.batch_size,\n      eval_batch_size=FLAGS.eval_batch_size,\n      num_neg=FLAGS.num_neg,\n      epochs_per_cycle=1,\n      match_mlperf=FLAGS.ml_perf,\n      use_subprocess=FLAGS.use_subprocess,\n      cache_id=FLAGS.cache_id)\n\n  train_params, eval_params = create_params(ncf_dataset)\n\n  eval_graph_spec = build_graph(\n      eval_params, ncf_dataset, tpu_embedding.INFERENCE)\n\n  for epoch in range(_NUM_EPOCHS):\n    tf.logging.info(""Training {}..."".format(epoch))\n    # build training graph each epoch as number of batches per epoch\n    # i.e. batch_count might change by 1 between epochs.\n    train_graph_spec = build_graph(\n        train_params, ncf_dataset, tpu_embedding.TRAINING)\n\n    run_graph(master, train_graph_spec, epoch)\n\n    tf.logging.info(""Evaluating {}..."".format(epoch))\n    run_graph(master, eval_graph_spec, epoch)\n\n  cleanup_fn()  # Cleanup data construction artifacts and subprocess.\n\n\ndef create_params(ncf_dataset):\n  """"""Create params for the model.""""""\n  learning_rate = FLAGS.learning_rate\n  beta1 = FLAGS.beta1\n  beta2 = FLAGS.beta2\n  epsilon = FLAGS.epsilon\n  model_dir = FLAGS.model_dir\n\n  params = {\n      ""learning_rate"": learning_rate,\n      ""num_users"": ncf_dataset.num_users,  # 138493 for 20m, 6040 for 1m.\n      ""num_items"": ncf_dataset.num_items,  # 26744 for 20m\n      ""mf_dim"": FLAGS.num_factors,\n      ""model_layers"": [int(layer) for layer in FLAGS.layers],\n      ""mf_regularization"": FLAGS.mf_regularization,\n      ""mlp_reg_layers"": [float(reg) for reg in FLAGS.mlp_regularization],\n      ""use_tpu"": True,\n      ""beta1"": beta1,\n      ""beta2"": beta2,\n      ""epsilon"": epsilon,\n      ""model_dir"": model_dir,\n  }\n\n  train_params = copy.copy(params)\n  train_params[""batch_size""] = FLAGS.batch_size\n  eval_params = copy.copy(params)\n  eval_params[""batch_size""] = FLAGS.eval_batch_size\n\n  return train_params, eval_params\n\n\ndef run_graph(master, graph_spec, epoch):\n  """"""Run graph_spec.graph with master.""""""\n  tf.logging.info(""Running graph for epoch {}..."".format(epoch))\n  with tf.Session(master, graph_spec.graph) as sess:\n    tf.logging.info(""Initializing system for epoch {}..."".format(epoch))\n    sess.run(tpu.initialize_system(\n        embedding_config=graph_spec.embedding.config_proto))\n\n    tf.logging.info(""Running before hook for epoch {}..."".format(epoch))\n    graph_spec.hook_before(sess, epoch)\n\n    tf.logging.info(""Running infeed for epoch {}..."".format(epoch))\n    infeed_thread_fn = graph_spec.get_infeed_thread_fn(sess)\n    infeed_thread = threading.Thread(target=infeed_thread_fn)\n    tf.logging.info(""Staring infeed thread..."")\n    infeed_thread.start()\n\n    tf.logging.info(""Running TPU loop for epoch {}..."".format(epoch))\n    graph_spec.run_tpu_loop(sess, epoch)\n\n    tf.logging.info(""Joining infeed thread..."")\n    infeed_thread.join()\n\n    tf.logging.info(""Running after hook for epoch {}..."".format(epoch))\n    graph_spec.hook_after(sess, epoch)\n\n\ndef build_graph(params, ncf_dataset, mode):\n  """"""Build graph_spec with graph and some useful handles.""""""\n  tf.logging.info(""building graph for mode {}."".format(mode))\n\n  with tf.Graph().as_default() as graph:\n    embedding = get_embedding(params, mode)\n    tf.logging.info(""tpu_embedding_config_proto: {}.""\n                    .format(embedding.config_proto))\n    if mode == tpu_embedding.INFERENCE:\n      assert (params[""batch_size""] % (embedding.num_cores *\n                                      (1 + rconst.NUM_EVAL_NEGATIVES))) == 0\n\n    input_fn, train_record_dir, batch_count = data_preprocessing.make_input_fn(\n        ncf_dataset=ncf_dataset, is_training=(mode == tpu_embedding.TRAINING))\n\n    get_infeed_thread_fn, infeed_queue = (\n        build_infeed(input_fn, params, batch_count, embedding, mode))\n\n    tpu_loop = build_tpu_loop(infeed_queue, params, batch_count, embedding,\n                              mode)\n\n    def run_tpu_loop(sess, epoch):\n      if mode == tpu_embedding.TRAINING:\n        sess.run(tpu_loop)\n      else:\n        total_values, count_values = (sess.run(tpu_loop))\n        hr = np.sum(total_values) / np.sum(count_values)\n        tf.logging.info(""HR = {} after epoch {}."".format(hr, epoch))\n\n    hook_before, hook_after = build_hooks(\n        mode, embedding, params, train_record_dir)\n\n    return GraphSpec(graph, embedding, run_tpu_loop, get_infeed_thread_fn,\n                     hook_before, hook_after)\n\n\ndef build_infeed(input_fn, params, batch_count, embedding, mode):\n  """"""Build infeed.""""""\n  if mode == tpu_embedding.TRAINING:\n    infeed_queue = tpu.InfeedQueue(\n        tuple_types=[tf.int32], tuple_shapes=[[params[""batch_size""], 1]])\n    infeed_queue.set_number_of_shards(embedding.num_cores)\n  else:\n    infeed_queue = tpu.InfeedQueue(\n        tuple_types=[tf.float32], tuple_shapes=[[params[""batch_size""], 1]])\n    infeed_queue.set_number_of_shards(embedding.num_cores)\n\n  def enqueue_ops_fn():\n    """"""Create enqueue ops.""""""\n    ds = input_fn(params)\n    iterator = ds.make_one_shot_iterator()\n    if mode == tpu_embedding.TRAINING:\n      features, labels = iterator.get_next()\n    else:\n      features = iterator.get_next()\n\n    # TODO(shizhiw): speed up input pipeline by avoiding splitting and\n    # sparse tensor.\n    # TPU embedding enqueue.\n    users = features[movielens.USER_COLUMN]\n    items = features[movielens.ITEM_COLUMN]\n\n    sparse_features_list = []\n    users_per_core_list = tf.split(users,\n                                   embedding.num_cores_per_host)\n    items_per_core_list = tf.split(items,\n                                   embedding.num_cores_per_host)\n    for j in range(embedding.num_cores_per_host):\n      users_sparse = tf.SparseTensor(\n          indices=[[i, 0] for i in range(\n              embedding.batch_size_per_core)],\n          values=users_per_core_list[j],\n          dense_shape=[embedding.batch_size_per_core, 1])\n      items_sparse = tf.SparseTensor(\n          indices=[[i, 0] for i in range(\n              embedding.batch_size_per_core)],\n          values=items_per_core_list[j],\n          dense_shape=[embedding.batch_size_per_core, 1])\n      sparse_features = {\n          ""mf_user"": users_sparse,\n          ""mlp_user"": users_sparse,\n          ""mf_item"": items_sparse,\n          ""mlp_item"": items_sparse,\n      }\n      sparse_features_list.append(sparse_features)\n    enqueue_ops = embedding.generate_enqueue_ops(\n        sparse_features_list)\n\n    # TPU dense enqueue.\n    if mode == tpu_embedding.TRAINING:\n      # Infeed does not support bool.\n      labels = tf.cast(labels, tf.int32)\n      enqueue_ops.extend(\n          infeed_queue.split_inputs_and_generate_enqueue_ops([labels]))\n    else:\n      duplicate_mask = tf.cast(features[rconst.DUPLICATE_MASK], tf.float32)\n      enqueue_ops.extend(\n          infeed_queue.split_inputs_and_generate_enqueue_ops([duplicate_mask]))\n\n    return enqueue_ops\n\n  if len(embedding.hosts) != 1:\n    raise ValueError(""len(embedding.hosts) should be 1, but got {}.""\n                     .format(embedding.hosts))\n  # TODO(shizhiw): check enqueue op location in tpu_embedding.py as user\n  # might fail to specify device for enqueue ops.\n  with tf.device(embedding.hosts[0]):\n    wrapped_enqueue_ops = wrap_computation_in_while_loop(\n        enqueue_ops_fn, n=batch_count, parallel_iterations=1)\n\n  def get_infeed_thread_fn(sess):\n    def infeed_thread_fn():\n      tf.logging.info(""Enqueueing..."")\n      sess.run(wrapped_enqueue_ops)\n    return infeed_thread_fn\n\n  return get_infeed_thread_fn, infeed_queue\n\n\ndef build_tpu_loop(infeed_queue, params, batch_count, embedding, mode):\n  """"""Build op to run loops on TPU.""""""\n  if mode == tpu_embedding.TRAINING:\n    def tpu_step_fn(labels):\n      """"""Create one step in training.""""""\n      logits = logits_fn(embedding, params)\n\n      if FLAGS.lazy_adam:\n        optimizer = tf.train.AdamOptimizer(\n            learning_rate=params[""learning_rate""],\n            beta1=params[""beta1""],\n            beta2=params[""beta2""],\n            epsilon=params[""epsilon""])\n      else:\n        optimizer = tf.contrib.opt.LazyAdamOptimizer(\n            learning_rate=params[""learning_rate""],\n            beta1=params[""beta1""],\n            beta2=params[""beta2""],\n            epsilon=params[""epsilon""])\n      optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n      # Softmax with the first column of ones is equivalent to sigmoid.\n      softmax_logits = tf.concat(\n          [tf.ones(logits.shape, dtype=logits.dtype), logits], axis=1)\n\n      loss = tf.losses.sparse_softmax_cross_entropy(\n          labels=labels, logits=softmax_logits)\n\n      minimize_op = optimizer.minimize(loss)\n      with tf.control_dependencies([minimize_op]):\n        send_gradient_op = embedding.generate_send_gradients_op()\n\n      return send_gradient_op\n\n    def tpu_loop_fn():\n      return tpu.repeat(batch_count, tpu_step_fn, infeed_queue=infeed_queue)\n\n    tpu_loop = tpu.shard(tpu_loop_fn, num_shards=embedding.num_cores)\n    return tpu_loop\n  else:\n\n    def tpu_step_fn(total, count, duplicate_mask):\n      """"""One step in evaluation.""""""\n      logits = logits_fn(embedding, params)\n      in_top_k, _, metric_weights, _ = neumf_model.compute_top_k_and_ndcg(\n          logits, duplicate_mask, FLAGS.ml_perf)\n      metric_weights = tf.cast(metric_weights, tf.float32)\n      total += tf.reduce_sum(tf.multiply(in_top_k, metric_weights))\n      count += tf.reduce_sum(metric_weights)\n      return total, count\n\n    inputs = [tf.constant(0.), tf.constant(0.)]\n\n    def tpu_loop_fn():\n      return tpu.repeat(\n          batch_count, tpu_step_fn, inputs, infeed_queue=infeed_queue)\n\n    tpu_loop = tpu.shard(tpu_loop_fn, num_shards=embedding.num_cores)\n    return tpu_loop\n\n\ndef build_hooks(mode, embedding, params, train_record_dir):\n  """"""Build `hook_before` and `hook_after` for `graph_spec`.""""""\n  saver = tf.train.Saver()\n  if mode == tpu_embedding.TRAINING:\n    def hook_before(sess, epoch):\n      if epoch == 0:\n        sess.run(tf.global_variables_initializer())\n      else:\n        saver.restore(sess,\n                      ""{}/model.ckpt.{}"".format(\n                          params[""model_dir""], epoch-1))\n      sess.run(embedding.init_ops)\n\n    def hook_after(sess, epoch):\n      sess.run(embedding.retrieve_parameters_ops)\n      ckpt_path = saver.save(sess,\n                             ""{}/model.ckpt.{}"".format(\n                                 params[""model_dir""], epoch))\n      tf.logging.info(""Model saved in path: {}.""\n                      .format(ckpt_path))\n      # must delete; otherwise the first epoch\'s data will always be used.\n      tf.gfile.DeleteRecursively(train_record_dir)\n  else:\n    def hook_before(sess, epoch):\n      saver.restore(sess,\n                    ""{}/model.ckpt.{}"".format(\n                        params[""model_dir""], epoch))\n      sess.run(embedding.init_ops)\n\n    def hook_after(sess, epoch):\n      del sess, epoch\n\n  return hook_before, hook_after\n\n\ndef get_embedding(params, mode):\n  """"""Create `TPUEmbedding` object.""""""\n  initializer = tf.random_normal_initializer(0., 0.01)\n  mlp_dim = params[""model_layers""][0]//2\n  table_mf_user = tpu_embedding.TableConfig(\n      vocabulary_size=params[""num_users""],\n      dimension=params[""mf_dim""],\n      initializer=initializer, combiner=""sum"")\n  table_mlp_user = tpu_embedding.TableConfig(\n      vocabulary_size=params[""num_users""],\n      dimension=mlp_dim,\n      initializer=initializer, combiner=""sum"")\n  table_mf_item = tpu_embedding.TableConfig(\n      vocabulary_size=params[""num_items""],\n      dimension=params[""mf_dim""],\n      initializer=initializer, combiner=""sum"")\n  table_mlp_item = tpu_embedding.TableConfig(\n      vocabulary_size=params[""num_items""],\n      dimension=mlp_dim,\n      initializer=initializer, combiner=""sum"")\n  table_to_config_dict = {\n      ""mf_user"": table_mf_user,\n      ""mlp_user"": table_mlp_user,\n      ""mf_item"": table_mf_item,\n      ""mlp_item"": table_mlp_item,\n  }\n  feature_to_table_dict = {\n      ""mf_user"": ""mf_user"",\n      ""mlp_user"": ""mlp_user"",\n      ""mf_item"": ""mf_item"",\n      ""mlp_item"": ""mlp_item"",\n  }\n\n  learning_rate = params[""learning_rate""]\n  if mode == tpu_embedding.TRAINING:\n    optimization_parameters = tpu_embedding.AdamParameters(\n        learning_rate, beta1=params[""beta1""], beta2=params[""beta2""],\n        epsilon=params[""epsilon""],\n        lazy_adam=FLAGS.lazy_adam,\n        sum_inside_sqrt=FLAGS.adam_sum_inside_sqrt,\n        use_gradient_accumulation=FLAGS.use_gradient_accumulation,\n        pipeline_execution_with_tensor_core=(\n            FLAGS.pipeline_execution_with_tensor_core))\n  else:\n    optimization_parameters = None\n\n  embedding = tpu_embedding.TPUEmbedding(\n      table_to_config_dict,\n      feature_to_table_dict,\n      params[""batch_size""],\n      num_hosts=1,\n      mode=mode,\n      optimization_parameters=optimization_parameters)\n\n  return embedding\n\n\ndef logits_fn(embedding, params):\n  """"""Calculate logits.""""""\n  input_layer = embedding.get_activations()\n\n  # TODO(shizhiw): support one feature to multiple tables in tpu_embedding.py.\n  input_layer_mf_user = input_layer[""mf_user""]\n  input_layer_mf_item = input_layer[""mf_item""]\n  input_layer_mlp_user = input_layer[""mlp_user""]\n  input_layer_mlp_item = input_layer[""mlp_item""]\n\n  mf_user_input = tf.keras.layers.Input(tensor=input_layer_mf_user)\n  mf_item_input = tf.keras.layers.Input(tensor=input_layer_mf_item)\n  mlp_user_input = tf.keras.layers.Input(tensor=input_layer_mlp_user)\n  mlp_item_input = tf.keras.layers.Input(tensor=input_layer_mlp_item)\n\n  model_layers = params[""model_layers""]\n  mlp_reg_layers = params[""mlp_reg_layers""]\n\n  if model_layers[0] % 2 != 0:\n    raise ValueError(""The first layer size should be multiple of 2!"")\n\n  # GMF part\n  # Element-wise multiply\n  mf_vector = tf.keras.layers.multiply([mf_user_input, mf_item_input])\n\n  # MLP part\n  # Concatenation of two latent features\n  mlp_vector = tf.keras.layers.concatenate([mlp_user_input, mlp_item_input])\n\n  num_layer = len(model_layers)  # Number of layers in the MLP\n  for layer in range(1, num_layer):\n    model_layer = tf.keras.layers.Dense(\n        model_layers[layer],\n        kernel_regularizer=tf.keras.regularizers.l2(mlp_reg_layers[layer]),\n        activation=""relu"")\n    mlp_vector = model_layer(mlp_vector)\n\n  # Concatenate GMF and MLP parts\n  predict_vector = tf.keras.layers.concatenate([mf_vector, mlp_vector])\n\n  # Final prediction layer\n  logits = tf.keras.layers.Dense(\n      1, activation=None, kernel_initializer=""lecun_uniform"",\n      name=movielens.RATING_COLUMN)(predict_vector)\n\n  return logits\n\n\ndef wrap_computation_in_while_loop(op_fn, n, parallel_iterations=10):\n  """"""Wraps the ops generated by `op_fn` in tf.while_loop.""""""\n\n  def computation(i):\n    ops = op_fn()\n    if not isinstance(ops, list):\n      ops = [ops]\n    with tf.control_dependencies(ops):\n      return i + 1\n\n  return tf.while_loop(\n      lambda i: tf.less(i, n),\n      computation, [tf.constant(0)],\n      parallel_iterations=parallel_iterations)\n\n\ndef define_ncf_flags():\n  """"""Add flags for running ncf_main.""""""\n  flags.DEFINE_enum(\n      name=""dataset"", default=""ml-20m"",\n      enum_values=[""ml-1m"", ""ml-20m""], case_sensitive=False,\n      help=flags_core.help_wrap(\n          ""Dataset to be trained and evaluated.""))\n\n  flags.DEFINE_string(\n      ""data_dir"", default=None,\n      help=(""The directory where movielens data is stored.""))\n\n  flags.DEFINE_integer(\n      ""batch_size"", default=2048*16, help=""Batch size."")\n\n  flags.DEFINE_string(\n      ""model_dir"", default=None,\n      help=(""The directory where the model and summaries are stored.""))\n\n  flags.DEFINE_string(\n      ""tpu"", default=None,\n      help=""The Cloud TPU to use for training. This should be either the name ""\n      ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 ""\n      ""url."")\n\n  flags.DEFINE_string(\n      ""gcp_project"", default=None,\n      help=""Project name for the Cloud TPU-enabled project. If not specified, ""\n      ""we will attempt to automatically detect the GCE project from metadata."")\n\n  flags.DEFINE_string(\n      ""tpu_zone"", default=None,\n      help=""GCE zone where the Cloud TPU is located in. If not specified, we ""\n      ""will attempt to automatically detect the zone from metadata."")\n\n  flags.DEFINE_boolean(\n      name=""download_if_missing"", default=True, help=flags_core.help_wrap(\n          ""Download data to data_dir if it is not already present.""))\n\n  flags.DEFINE_integer(\n      name=""eval_batch_size"",\n      default=80000,\n      help=flags_core.help_wrap(\n          ""The batch size used for evaluation. This should generally be larger""\n          ""than the training batch size as the lack of back propagation during""\n          ""evaluation can allow for larger batch sizes to fit in memory. If not""\n          ""specified, the training batch size (--batch_size) will be used.""))\n\n  flags.DEFINE_integer(\n      name=""num_factors"", default=64,\n      help=flags_core.help_wrap(""The Embedding size of MF model.""))\n\n  flags.DEFINE_list(\n      name=""layers"", default=[256, 256, 128, 64],\n      help=flags_core.help_wrap(\n          ""The sizes of hidden layers for MLP. Example ""\n          ""to specify different sizes of MLP layers: --layers=32,16,8,4""))\n\n  flags.DEFINE_float(\n      name=""mf_regularization"", default=0.,\n      help=flags_core.help_wrap(\n          ""The regularization factor for MF embeddings. The factor is used by ""\n          ""regularizer which allows to apply penalties on layer parameters or ""\n          ""layer activity during optimization.""))\n\n  flags.DEFINE_list(\n      name=""mlp_regularization"", default=[""0."", ""0."", ""0."", ""0.""],\n      help=flags_core.help_wrap(\n          ""The regularization factor for each MLP layer. See mf_regularization ""\n          ""help for more info about regularization factor.""))\n\n  flags.DEFINE_integer(\n      name=""num_neg"", default=4,\n      help=flags_core.help_wrap(\n          ""The Number of negative instances to pair with a positive instance.""))\n\n  flags.DEFINE_float(\n      name=""learning_rate"", default=0.0005,\n      help=flags_core.help_wrap(""The learning rate.""))\n\n  flags.DEFINE_bool(\n      name=""ml_perf"", default=True,\n      help=flags_core.help_wrap(\n          ""If set, changes the behavior of the model slightly to match the ""\n          ""MLPerf reference implementations here: \\n""\n          ""https://github.com/mlperf/reference/tree/master/recommendation/""\n          ""pytorch\\n""\n          ""The two changes are:\\n""\n          ""1. When computing the HR and NDCG during evaluation, remove ""\n          ""duplicate user-item pairs before the computation. This results in ""\n          ""better HRs and NDCGs.\\n""\n          ""2. Use a different sorting algorithm when sorting the input data, ""\n          ""which performs better due to the fact the sorting algorithms are ""\n          ""not stable.""))\n\n  flags.DEFINE_float(\n      name=""beta1"", default=0.9,\n      help=flags_core.help_wrap(""AdamOptimizer parameter hyperparam beta1.""))\n\n  flags.DEFINE_float(\n      name=""beta2"", default=0.999,\n      help=flags_core.help_wrap(""AdamOptimizer parameter hyperparam beta2.""))\n\n  flags.DEFINE_float(\n      name=""epsilon"", default=1e-08,\n      help=flags_core.help_wrap(""AdamOptimizer parameter hyperparam epsilon.""))\n\n  flags.DEFINE_bool(\n      name=""use_gradient_accumulation"", default=True,\n      help=flags_core.help_wrap(\n          ""setting this to `True` makes embedding ""\n          ""gradients calculation more accurate but slower. Please see ""\n          "" `optimization_parameters.proto` for details.""))\n\n  flags.DEFINE_bool(\n      name=""pipeline_execution_with_tensor_core"", default=False,\n      help=flags_core.help_wrap(\n          ""setting this to `True` makes training ""\n          ""faster, but trained model will be different if step N and step N+1 ""\n          ""involve the same set of embedding ID. Please see ""\n          ""`tpu_embedding_configuration.proto` for details""))\n\n  flags.DEFINE_bool(\n      name=""use_subprocess"", default=True, help=flags_core.help_wrap(\n          ""By default, ncf_main.py starts async data generation process as a ""\n          ""subprocess. If set to False, ncf_main.py will assume the async data ""\n          ""generation process has already been started by the user.""))\n\n  flags.DEFINE_integer(name=""cache_id"", default=None, help=flags_core.help_wrap(\n      ""Use a specified cache_id rather than using a timestamp. This is only ""\n      ""needed to synchronize across multiple workers. Generally this flag will ""\n      ""not need to be set.""\n  ))\n\n  flags.DEFINE_bool(\n      name=""lazy_adam"", default=False, help=flags_core.help_wrap(\n          ""By default, use Adam optimizer. If True, use Lazy Adam optimizer, ""\n          ""which will be faster but might need tuning for convergence.""))\n\n  flags.DEFINE_bool(\n      name=""adam_sum_inside_sqrt"", default=True, help=flags_core.help_wrap(\n          ""If True, Adam or lazy Adam updates on TPU embedding will be faster. ""\n          ""For details, see ""\n          ""tensorflow/contrib/tpu/proto/optimization_parameters.proto.""))\n\n\nif __name__ == ""__main__"":\n  tf.logging.set_verbosity(tf.logging.INFO)\n  define_ncf_flags()\n  FLAGS = flags.FLAGS\n  absl_app.run(main)\n'"
tpu/models/experimental/qanet/data.py,67,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Implements data loaders and metrics for the SQuAD dataset.""""""\nimport collections\nimport json\nimport os\nimport re\nimport string\n# Standard Imports\nimport numpy as np\nimport tensorflow as tf\n\n\ndef build_dataset(cfg, is_tpu):\n  """"""Construct train and eval inputs_fn.""""""\n  load_tfrecord = cfg.load_tfrecord\n  if is_tpu:\n    load_tfrecord = True\n  # TODO(ddohan): Share the common args more clearly\n  train_input = get_input_fn(\n      split=cfg.train_split,\n      max_length=cfg.max_length,\n      # TPUs don\'t handle OutOfRange exceptions from data pipelines, so we\n      # repeat indefinitely and handle setting number of training steps\n      # manually. This is handled by the tpu.steps_per_epoch setting.\n      # On a GPU, we are able to be more exact about the exact boundary between\n      # epochs and avoid reasoning in terms of step counts.\n      # If 0, repeat indefinitely. Otherwise repeat N times.\n      num_repeats=0 if is_tpu else cfg.num_repeats,\n      shuffle=cfg.train_shuffle,\n      cache=cfg.cache,\n      limit=None,\n      data_path=cfg.data_path,\n      vocab_path=cfg.vocab_path,\n      is_tpu=is_tpu,\n      use_generator=not load_tfrecord,\n      resample_too_long=cfg.resample_too_long,\n      is_training=True)\n  eval_input = get_input_fn(\n      split=cfg.eval_split,\n      max_length=None,  # Never do any filtering at eval\n      limit=None,\n      num_repeats=1,\n      shuffle=False,\n      cache=cfg.cache,\n      data_path=cfg.data_path,\n      vocab_path=cfg.vocab_path,\n      is_tpu=False,  # Never eval on TPU because of py_func\n      use_generator=not load_tfrecord,\n      is_training=False)\n  return train_input, eval_input\n\n\ndef word_tokenize(text):\n  """"""Split on whitespace and punctuation.""""""\n  return re.findall(r\'\\w+|[^\\w\\s]\', text, re.UNICODE)\n\n\ndef utf_encode_list(text):\n  """"""utf encode every element of a list.""""""\n  return [x.encode(\'utf-8\') for x in text]\n\n\ndef convert_to_spans(raw_text, tokens):\n  """"""Convert tokenized version of `raw_text` to character spans into it.\n\n  Args:\n    raw_text: The raw string\n    tokens: The tokenized version of the string\n\n\n  Returns:\n    [list of (start, end) tuples] mapping each token to corresponding indices\n    in the text.\n  """"""\n  cur_idx = 0\n  spans = []\n  for token in tokens:\n    tmp = raw_text.find(token, cur_idx)\n    l = len(token)\n    cur_idx = tmp\n    spans.append((cur_idx, cur_idx + l))\n    cur_idx += l\n  return spans\n\n\ndef get_answer_tokens(context, context_tokens, answer_start, answer_end):\n  """"""Get answer given context, context_words, and span.\n\n  Args:\n    context: A list of bytes, to be decoded with utf-8.\n    context_tokens: A list of a list of bytes, to be decoded with utf-8.\n    answer_start: An int for answer start.\n    answer_end: An int for answer end.\n  Returns:\n    A list of bytes, encoded with utf-8, for the answer.\n  """"""\n  # Word level\n  context = context.decode(\'utf-8\')\n  spans = convert_to_spans(\n      context, [token.decode(\'utf-8\') for token in context_tokens])\n  start_char = spans[answer_start][0]\n  end_char = spans[answer_end][1]\n  result = context[start_char:end_char]\n  return result\n\n\ndef get_embedding_map(embeddings_path, draft=False, word_subset=None):\n  """"""Get an `OrderedDict` that maps word to vector.\n\n  Args:\n    embeddings_path: `str` value,\n      path to the embeddings file (e.g. `glove.6B.50d.txt`)\n    draft: `bool` value, whether to only load first 99 for draft mode.\n  Returns:\n    `OrderedDict` object, mapping word to vector.\n  """"""\n  embeddings = collections.OrderedDict()\n  with tf.gfile.GFile(embeddings_path, \'r\') as fp:\n    for idx, line in enumerate(fp):\n      if len(line) < 30 and idx == 0:\n        # In fasttext, the first line is the # of vocab words.\n        continue\n      line = line.decode(\'utf-8\')\n      tokens = line.strip().split(u\' \')\n      word = tokens[0]\n      vec = list(map(float, tokens[1:]))\n      if draft and idx > 99:\n        break\n      if word_subset:\n        if word not in word_subset:\n          continue\n      embeddings[word] = vec\n  return embeddings\n\n\n# Global state to do some basic caching - avoid reloading\n_GLOBAL_VOCAB_CACHE = dict()\n\n\ndef get_pretrained_embeddings_cache(embeddings_path):\n  """"""Get pretrained vocab embeddings.""""""\n  if embeddings_path in _GLOBAL_VOCAB_CACHE:\n    return _GLOBAL_VOCAB_CACHE[embeddings_path]\n  else:\n    tf.logging.info(\'Loading pretrained embeddings from %s\', embeddings_path)\n    embeddings = get_embedding_map(embeddings_path)\n    # OrderedDict, so keys and values are ordered.\n    assert isinstance(embeddings, collections.OrderedDict)\n    words = embeddings.keys()\n    embeddings_as_arr = np.array(embeddings.values())\n  _GLOBAL_VOCAB_CACHE[embeddings_path] = (words, embeddings_as_arr)\n  return _GLOBAL_VOCAB_CACHE[embeddings_path]\n\n\ndef squad_generator(path, tokenizer_fn=word_tokenize, as_np=False):\n  """"""Generate SQuAD data from the raw json file.""""""\n\n  with tf.gfile.GFile(path) as f:\n    squad = json.load(f)\n  for article in squad[\'data\']:\n\n    for paragraph in article[\'paragraphs\']:\n      context = paragraph[\'context\'].strip()\n      context_enc = context.encode(\'utf-8\')\n      context_tokens = tokenizer_fn(context)\n      for qa in paragraph[\'qas\']:\n        question = qa[\'question\'].strip()\n        id_ = qa[\'id\']\n\n        answer_starts = [answer[\'answer_start\'] for answer in qa[\'answers\']]\n        answers = [answer[\'text\'].strip() for answer in qa[\'answers\']]\n        answer_ends = [\n            start + len(answer)\n            for start, answer in zip(answer_starts, answers)\n        ]\n\n        feats = {}\n        feats[\'id\'] = id_\n        feats[\'answers\'] = utf_encode_list(answers)\n        feats[\'num_answers\'] = len(answers)\n\n        feats[\'context\'] = context_enc\n        feats[\'context_tokens\'] = context_tokens\n        feats[\'context_length\'] = len(context_tokens)\n\n        feats[\'question\'] = question.encode(\'utf-8\')\n        feats[\'question_tokens\'] = utf_encode_list(tokenizer_fn(question))\n        feats[\'question_length\'] = len(feats[\'question_tokens\'])\n\n        spans = convert_to_spans(context, feats[\'context_tokens\'])\n        starts = []\n        ends = []\n        for answer_start, answer_end in zip(answer_starts, answer_ends):\n          start, end = get_span(spans, answer_start, answer_end)\n          starts.append(start)\n          ends.append(end)\n\n        feats[\'answers_start_token\'] = starts\n        feats[\'answers_end_token\'] = ends\n        feats[\'context_tokens\'] = utf_encode_list(feats[\'context_tokens\'])\n        if as_np:\n          out = dict()\n          for feat in feats:\n            out[feat] = np.array(feats[feat])\n          yield out\n        else:\n          yield feats\n\n\ndef get_span(spans, answer_start, answer_end):\n  """"""Get the start/end index that contains the (start,end) interval.\n\n  Args:\n    spans: [List of (start, end) tuples]\n    answer_start: Start index\n    answer_end: End index\n\n  Returns:\n    tuple of (start, end) indices into spans such that\n    spans[start][0] <= answer_start <= answer_end <= spans[end][1]\n\n  Raises:\n    ValueError: if either the start or end position is not found.\n  """"""\n  word_answer_start = None\n  word_answer_end = None\n  for word_idx, span in enumerate(spans):\n    if span[0] <= answer_start <= span[1]:\n      word_answer_start = word_idx\n    if span[0] <= answer_end <= span[1]:\n      word_answer_end = word_idx\n    if word_answer_start and word_answer_end:\n      break\n  if word_answer_end is None and word_answer_start is not None:\n    # TODO(ddohan): Figure out why this is sometimes necessary\n    if answer_end > spans[-1][-1]:\n      word_answer_end = len(spans) - 1\n  if word_answer_end is None or word_answer_start is None:\n    raise ValueError\n  assert word_answer_end >= word_answer_start\n  return word_answer_start, word_answer_end\n\n\nFIELDS = [\'context\', \'question\', \'answers\']\n\n\ndef build_tfrecord_pipeline(filenames):\n  """"""Read TFRecords from disk to create data pipeline.""""""\n  sequence_feature = tf.FixedLenSequenceFeature(\n      [], tf.int64, allow_missing=True)\n  str_sequence_feature = tf.FixedLenSequenceFeature(\n      [], tf.string, allow_missing=True)\n  int_feature = tf.FixedLenFeature([], tf.int64)\n  str_feature = tf.FixedLenFeature([], tf.string)\n  features = {\n      \'id\': str_feature,\n      \'num_answers\': int_feature,\n      \'answers\': str_sequence_feature,\n      \'answers_start_token\': sequence_feature,\n      \'answers_end_token\': sequence_feature,\n      \'context\': str_feature,\n      \'context_length\': int_feature,\n      \'context_tokens\': str_sequence_feature,\n      \'question\': str_feature,\n      \'question_length\': int_feature,\n      \'question_tokens\': str_sequence_feature,\n  }\n\n  def _parse(proto):\n    return tf.parse_single_example(proto, features=features)\n\n  ds = tf.data.TFRecordDataset(\n      filenames,\n      # 1 GB\n      buffer_size=1024 * 1024 * 1024,\n      num_parallel_reads=8)\n\n  ds = ds.map(_parse, num_parallel_calls=16)\n  return ds\n\n\ndef build_generator_pipeline(data_path,\n                             split,\n                             tokenizer_fn=word_tokenize):\n  """"""Build a data pipeline from raw json SQuAD file.""""""\n  shapes, types = get_shapes_and_types(is_tpu=False, max_length=None)\n\n  def generator():\n    path = os.path.join(data_path, \'%s-v1.1.json\' % split)\n    return squad_generator(path=path, tokenizer_fn=tokenizer_fn)\n\n  ds = tf.data.Dataset.from_generator(\n      generator, output_types=types, output_shapes=shapes)\n  return ds\n\n\ndef get_shapes_and_types(is_tpu=False, max_length=None):\n  """"""Build tuple of (shapes, types) dictionaries specifying the dataset.""""""\n  # TODO(ddohan): Explicitly list types & shapes instead of creating in a loop\n  types = {}\n  shapes = {}\n  length = None\n  if is_tpu:\n    assert max_length\n    length = max_length\n\n  for k in FIELDS:\n    if not is_tpu:\n      types[k] = tf.string\n      types[\'%s_tokens\' % k] = tf.string\n      shapes[k] = []\n      shapes[\'%s_tokens\' % k] = [length]\n    types[\'%s_length\' % k] = tf.int64\n    shapes[\'%s_length\' % k] = []\n  for k in [\'answers_tokens\', \'answers_length\']:\n    if k in types:\n      del types[k]\n      del shapes[k]\n\n  types[\'num_answers\'] = tf.int64\n  types[\'answers_start_token\'] = tf.int64\n  types[\'answers_end_token\'] = tf.int64\n\n  shapes[\'num_answers\'] = []\n  shapes[\'answers_start_token\'] = []\n  shapes[\'answers_end_token\'] = []\n\n  if not is_tpu:\n    types[\'id\'] = tf.string\n    shapes[\'id\'] = []\n\n  for k in shapes:\n    if k.startswith(\'answer\'):\n      # TODO(ddohan): Handle multiple answers\n      shapes[k] = [1 if is_tpu else None] + shapes[k]\n\n  return shapes, types\n\n\ndef resample_example(example, max_length=256):\n  """"""Given an example and max length, resample the context to that length.\n\n  Start position randomly chosen from [0, answer_start]. Assumes a single\n    answer per context, which is true for the SQuAD training set.\n\n  Args:\n    example: A single example containing at least these fields:\n      [\'answers_start_token\', \'answers_end_token\', \'context_tokens\',\n      \'context_length\']\n    max_length: Maximum length. Contexts are resampled to this length.\n\n  Returns:\n    Resampled example.\n  """"""\n\n  # TODO(ddohan): Consider randomly cropping to shorter lengths\n  # TODO(ddohan): Figure out how to resample the raw text as well. Not necessary\n  # for training\n  def _resample():\n    """"""Helper method for resampling inside cond.""""""\n    x = example\n    ans_start = tf.to_int64(x[\'answers_start_token\'][0])\n    ans_end = tf.to_int64(x[\'answers_end_token\'][0])\n    min_start = tf.maximum(tf.to_int64(0), ans_end - max_length + 1)\n    max_start = ans_start\n    start_idx = tf.random_uniform([],\n                                  min_start,\n                                  max_start + 1, dtype=tf.int64)\n    for k in [\'answers_start_token\', \'answers_end_token\']:\n      x[k] -= start_idx\n    x[\'context_tokens\'] = x[\'context_tokens\'][start_idx:start_idx + max_length]\n    x[\'context_length\'] = tf.to_int64(tf.shape(x[\'context_tokens\'])[0])\n    return x\n\n  def identity():\n    return example\n\n  return tf.cond(\n      tf.greater_equal(\n          tf.to_int32(max_length), tf.to_int32(example[\'context_length\'])),\n      true_fn=identity,\n      false_fn=_resample)\n\n\ndef get_input_fn(split=\'dev\',\n                 shuffle=False,\n                 num_repeats=False,\n                 limit=None,\n                 do_embedding=True,\n                 cache=True,\n                 max_length=None,\n                 resample_too_long=True,\n                 data_path=None,\n                 vocab_path=None,\n                 is_tpu=False,\n                 use_generator=True,\n                 is_training=False):\n  """"""Build input function.""""""\n  if is_tpu:\n    assert max_length\n\n  # Do the GLOVE embedding lookups in the data loader\n  if do_embedding:\n    # Load and package into the graph directly\n    # Vocab is about ~200MB total once filtered down\n    embedding_words, embedding_vectors = get_pretrained_embeddings_cache(\n        embeddings_path=vocab_path)\n\n  def _input_fn(params=None):\n    """"""Input function compatible with `Experiment` object.\n\n    Args:\n      params: Params passed to the estimator. Contains \'batch_size\'.\n\n    Returns:\n      A tuple of feature tensors and target tensors.\n\n    Raises:\n      ValueError: If filtering by length is set during eval mode.\n    """"""\n    if not is_training:\n      assert not is_tpu\n    tf.logging.info(\'Data pipeline given params:\\n%s\' % params)\n    if is_training:\n      batch_size = params.dataset.train_batch_size\n    else:\n      batch_size = params.dataset.eval_batch_size\n\n    if use_generator:\n      tf.logging.info(\'Building generator data pipeline.\')\n      ds = build_generator_pipeline(\n          data_path=data_path,\n          split=split,\n          tokenizer_fn=word_tokenize)\n    else:\n      tf.logging.info(\'Loading TFRecords from %s\' % data_path)\n      filenames = tf.gfile.Glob(os.path.join(data_path, \'%s_*\' % split))\n      tf.logging.info(filenames)\n      ds = build_tfrecord_pipeline(filenames=filenames)\n\n    if max_length:\n      if not is_training:\n        raise ValueError(\'Unable to filter or resample examples at eval time.\')\n      if resample_too_long:\n\n        tf.logging.info(\'Resampling with max length %s\', max_length)\n        def _resample(x):\n          return resample_example(x, max_length=max_length)\n\n        ds = ds.map(_resample, num_parallel_calls=16)\n      else:\n        # Filter out examples over our max length to avoid an error downstream.\n        tf.logging.info(\'Filtering out examples over max length %s\', max_length)\n        def _not_too_long(x):\n          return tf.greater_equal(\n              tf.to_int32(max_length), tf.to_int32(x[\'context_length\']))\n\n        ds = ds.filter(_not_too_long)\n\n    if limit:\n      # Take the first N examples\n      ds = ds.take(limit)\n\n    if cache:\n      # Cache dataset to avoid hitting the python generator after first epoch\n      ds = ds.cache()\n\n    # Subset that we should actually pass back to the caller\n    # This is required to filter out tf.string fields which are not TPU\n    # compatible\n    # Specifically: id, context, question, context_tokens and question_tokens\n    # are all string fields that will be removed.\n    shapes, _ = get_shapes_and_types(is_tpu=is_tpu, max_length=max_length)\n\n    if do_embedding:\n      # Embed tokens with pretrained word vectors\n\n      # Add in shape info before batching\n      shapes[\'context_vecs\'] = [max_length if is_tpu else None, 300]\n      shapes[\'question_vecs\'] = [max_length if is_tpu else None, 300]\n\n      vocab_table = tf.contrib.lookup.index_table_from_tensor(\n          embedding_words, default_value=0)\n      vocab_vectors = tf.constant(embedding_vectors, dtype=tf.float32)\n\n      def lookup(words):\n        ids = vocab_table.lookup(words)\n        result = tf.nn.embedding_lookup(params=vocab_vectors, ids=ids)\n        return result\n\n      def lookup_fields(d):\n        d[\'context_vecs\'] = lookup(d[\'context_tokens\'])\n        d[\'question_vecs\'] = lookup(d[\'question_tokens\'])\n        return d\n\n      ds = ds.map(lookup_fields, num_parallel_calls=16)\n\n    buffer_size = 5000  # Magic number TUNE ME\n    repeats = num_repeats if num_repeats else None\n    if shuffle and repeats != 1:\n      tf.logging.info(\'Shuffle and repeat size: %s\' % buffer_size)\n      ds = ds.apply(\n          tf.contrib.data.shuffle_and_repeat(\n              buffer_size=buffer_size,\n              count=repeats))\n    elif repeats != 1:\n      tf.logging.info(\'Repeating\')\n      ds = ds.repeat(count=repeats)\n    elif shuffle:\n      tf.logging.info(\'Shuffle size: %s\' % buffer_size)\n      ds = ds.shuffle(buffer_size=buffer_size)\n\n    def filter_fields(example):\n      out = {}\n      for k in shapes:\n        out[k] = example[k]\n      return out\n\n    ds = ds.map(filter_fields, num_parallel_calls=16)\n\n    if is_training:\n      ds = ds.apply(\n          tf.contrib.data.padded_batch_and_drop_remainder(\n              batch_size, padded_shapes=shapes))\n    else:\n      # Never want to ignore values at eval time\n      ds = ds.padded_batch(batch_size, padded_shapes=shapes)\n    ds = ds.prefetch(tf.contrib.data.AUTOTUNE)  # Buffer a few batches ahead\n    if do_embedding:\n      iterator = ds.make_initializable_iterator()\n      # Must be initialized when the graph is initialized and before the\n      # dataset tensors are evaluated.\n      # Run `tf.tables_initializer()` before getting first batch\n      tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS,\n                           iterator.initializer)\n    else:\n      iterator = ds.make_one_shot_iterator()\n    batch = iterator.get_next()\n    return batch, batch\n\n  return _input_fn\n\n# Metrics\n\n\ndef metric_fn(answers, prediction, start, end, yp1, yp2, num_answers):\n  """"""Compute span accuracies and token F1/EM scores.""""""\n\n  yp1 = tf.expand_dims(yp1, -1)\n  yp2 = tf.expand_dims(yp2, -1)\n  answer_mask = tf.sequence_mask(num_answers)\n\n  start = tf.to_int64(start)\n  end = tf.to_int64(end)\n  start_correct = tf.reduce_any(tf.equal(start, yp1) & answer_mask, 1)\n  end_correct = tf.reduce_any(tf.equal(end, yp2) & answer_mask, 1)\n  correct = start_correct & end_correct\n\n  em = tf.py_func(\n      enum_fn(_exact_match_score, dtype=\'float32\'),\n      [prediction, answers, answer_mask], \'float32\')\n  f1 = tf.py_func(\n      enum_fn(_f1_score, dtype=\'float32\'), [prediction, answers, answer_mask],\n      \'float32\')\n\n  eval_metric_ops = {\n      # TODO(ddohan): Add other useful metrics\n      \'acc_start\':\n          tf.metrics.mean(tf.cast(start_correct, \'float\')),\n      \'acc_end\':\n          tf.metrics.mean(tf.cast(end_correct, \'float\')),\n      \'acc_span\':\n          tf.metrics.mean(tf.cast(correct, \'float\')),\n      \'em\':\n          tf.metrics.mean(em),\n      \'f1\':\n          tf.metrics.mean(f1),\n      # Number of questions processed\n      \'num_question\':\n          tf.metrics.true_positives(\n              tf.ones([tf.shape(prediction)][0]),\n              tf.ones([tf.shape(prediction)][0]))\n  }\n  return eval_metric_ops\n\n\ndef _f1_score(prediction, ground_truths, answer_mask=None):\n  prediction = prediction.decode(\'utf-8\', errors=\'ignore\')\n  ground_truths = [\n      ground_truth.decode(\'utf-8\', errors=\'ignore\')\n      for ground_truth in ground_truths\n  ]\n  scores = np.array(\n      [_f1_score_(prediction, ground_truth) for ground_truth in ground_truths])\n  return max(scores * answer_mask.astype(float))\n\n\ndef _exact_match_score(prediction, ground_truths, answer_mask=None):\n  prediction = prediction.decode(\'utf-8\', errors=\'ignore\')\n  ground_truths = [\n      ground_truth.decode(\'utf-8\') for ground_truth in ground_truths\n  ]\n  scores = np.array([\n      float(_exact_match_score_(prediction, ground_truth))\n      for ground_truth in ground_truths\n  ])\n  return max(scores * answer_mask.astype(float))\n\n\ndef enum_fn(fn, dtype=\'object\'):\n  # Map function across a batch\n\n  def new_fn(*args):\n    return np.array([fn(*each_args) for each_args in zip(*args)], dtype=dtype)\n\n  return new_fn\n\n\n# Functions below are copied from official SQuAD eval script and SHOULD NOT\n# BE MODIFIED.\n\n\ndef _normalize_answer(s):\n  """"""Lower text and remove punctuation, articles and extra whitespace.\n\n  Directly copied from official SQuAD eval script, SHOULD NOT BE MODIFIED.\n\n  Args:\n    s: Input text.\n  Returns:\n    Normalized text.\n  """"""\n\n  def remove_articles(text):\n    return re.sub(r\'\\b(a|an|the)\\b\', \' \', text)\n\n  def white_space_fix(text):\n    return \' \'.join(text.split())\n\n  def remove_punc(text):\n    exclude = set(string.punctuation)\n    return \'\'.join(ch for ch in text if ch not in exclude)\n\n  def lower(text):\n    return text.lower()\n\n  return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef _f1_score_(prediction, ground_truth):\n  """"""Directly copied from official SQuAD eval script, SHOULD NOT BE MODIFIED.""""""\n  # tf.logging.info(\'%s | %s\' % (prediction, ground_truth))\n  prediction_tokens = _normalize_answer(prediction).split()\n  ground_truth_tokens = _normalize_answer(ground_truth).split()\n  common = collections.Counter(prediction_tokens) & collections.Counter(\n      ground_truth_tokens)\n  num_same = sum(common.values())\n  if num_same == 0:\n    return 0\n  precision = 1.0 * num_same / len(prediction_tokens)\n  recall = 1.0 * num_same / len(ground_truth_tokens)\n  f1 = (2 * precision * recall) / (precision + recall)\n  return f1\n\n\ndef _exact_match_score_(prediction, ground_truth):\n  """"""Directly copied from official SQuAD eval script, SHOULD NOT BE MODIFIED.""""""\n  return _normalize_answer(prediction) == _normalize_answer(ground_truth)\n'"
tpu/models/experimental/qanet/model.py,91,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements the QANet question answering model for SQuAD.""""""\nimport os\n# Standard Imports\nimport numpy as np\nfrom tensor2tensor.layers import common_attention\nfrom tensor2tensor.layers import common_layers\nimport tensorflow as tf\nimport data\nimport utils\n\n\ndef build_config(model_dir, data_path):\n  return utils.Config(\n      # Practicalities\n      model_dir=model_dir,\n      master=\'\',\n\n      # Data\n      num_epochs=30,\n      # If 0, run until OurOfRange is raised by dataset.\n      # Must be nonzero on TPU.\n      steps_per_epoch=0,\n      dataset=utils.Config(\n          train_split=\'train\',\n          eval_split=\'dev\',\n          load_tfrecord=False,  # If false, generate on the fly\n          # Number of times to repeat dataset per call.\n          # If 0, repeat indefinitely.\n          num_repeats=1,\n          train_shuffle=True,\n          cache=True,\n          max_length=256,\n          data_path=data_path,\n          vocab_path=os.path.join(data_path, \'vocab.vec\'),\n          train_batch_size=32,\n          eval_batch_size=16,\n          resample_too_long=True,\n      ),\n\n      # Optimizer\n      opt=utils.Config(\n          lr=0.001,\n          warmup_steps=1000,\n          beta1=0.9,\n          beta2=0.999,\n          epsilon=1e-7,\n          grad_clip_norm=10.0,\n          l2_reg=5e-8,\n          ema_decay=0.999),\n      # Encoder structure\n      encoder_emb=utils.Config(\n          layers=1,\n          kernel_size=7,\n          hidden_size=128,\n          ffn_multiplier=2.0,\n          attention_heads=4,\n          attention_dropout=0.1,\n          layer_dropout=0.25,\n          attention_type=\'dot_product\',\n          structure=\'conv,conv,conv,conv,att,ffn\',\n          separable_conv=True,\n          timing_signal=True,\n      ),\n      encoder_model=utils.Config(\n          layers=1,\n          hidden_size=128,\n          kernel_size=7,\n          ffn_multiplier=2.0,\n          attention_heads=4,\n          attention_dropout=0.1,\n          layer_dropout=0.25,\n          attention_type=\'dot_product\',\n          structure=\'conv,conv,att,ffn\',\n          separable_conv=True,\n          timing_signal=True,\n      ),\n      output_dropout_rate=0.3,\n      embedding=utils.Config(dropout_rate=0.15),\n      tpu=utils.Config(enable=False,),\n  )\n\n\ndef get_loss(answer_start,\n             answer_end,\n             logits_start,\n             logits_end,\n             label_smoothing=0.0):\n  """"""Get loss given answer and logits.\n\n  Args:\n    answer_start: [batch_size, num_answers] shaped tensor\n    answer_end: Similar to `answer_start` but for end.\n    logits_start: [batch_size, context_size]-shaped tensor for answer start\n      logits.\n    logits_end: Similar to `logits_start`, but for end. This tensor can be also\n      [batch_size, context_size, context_size], in which case the true answer\n      start is used to index on dim 1 (context_size).\n    label_smoothing: whether to use label smoothing or not.\n  Returns:\n    Float loss tensor.\n  """"""\n  length = logits_start.shape.as_list()[1] or tf.shape(logits_start)[1]\n  start = tf.one_hot(answer_start, length)\n  end = tf.one_hot(answer_end, length)\n  loss_start = tf.losses.softmax_cross_entropy(\n      onehot_labels=start, logits=logits_start, label_smoothing=label_smoothing)\n  loss_end = tf.losses.softmax_cross_entropy(\n      onehot_labels=end, logits=logits_end, label_smoothing=label_smoothing)\n\n  return tf.reduce_mean(loss_start) + tf.reduce_mean(loss_end)\n\n\ndef bi_attention_memory_efficient_dcn(\n    a,\n    b,\n    mask_a=None,\n    # TODO(ddohan): Should the other mask be used?\n    mask_b=None,  # pylint: disable=unused-argument\n):\n  """"""Biattention between question (a) and document (b).""""""\n  logits = tf.transpose(\n      trilinear_memory_efficient(a, b), perm=[0, 2, 1])  # [bs,len_b,len_a]\n  b2a = b2a_attention(logits, a, mask_a)\n  a2b = a2b_attention_dcn(logits, b)\n  return b2a, a2b\n\n\ndef trilinear_memory_efficient(a, b, use_activation=False):\n  """"""W1a + W2b + aW3b.""""""\n  d = a.shape.as_list()[-1]\n  n = tf.shape(a)[0]\n\n  len_a = tf.shape(a)[1]\n  len_b = tf.shape(b)[1]\n\n  w1 = tf.get_variable(\'w1\', shape=[d, 1], dtype=tf.float32)\n  w2 = tf.get_variable(\'w2\', shape=[d, 1], dtype=tf.float32)\n  w3 = tf.get_variable(\'w3\', shape=[1, 1, d], dtype=tf.float32)\n\n  a_reshape = tf.reshape(a, [-1, d])  # [bs*len_a, d]\n  b_reshape = tf.reshape(b, [-1, d])  # [bs*len_b, d]\n\n  part_1 = tf.reshape(tf.matmul(a_reshape, w1), [n, len_a])  # [bs, len_a]\n  part_1 = tf.tile(tf.expand_dims(part_1, 2),\n                   [1, 1, len_b])  # [bs, len_a, len_b]\n\n  part_2 = tf.reshape(tf.matmul(b_reshape, w2), [n, len_b])  # [bs, len_b]\n  part_2 = tf.tile(tf.expand_dims(part_2, 1),\n                   [1, len_a, 1])  # [bs, len_a, len_b]\n\n  a_w3 = a * w3  # [bs, len_a, d]\n  part_3 = tf.matmul(a_w3, tf.transpose(b, perm=[0, 2, 1]))  # [bs,len_a,len_b]\n\n  ## return the unnormalized logits matrix : [bs,len_a,len_b]\n  if use_activation:\n    return tf.nn.relu(part_1 + part_2 + part_3)\n  return part_1 + part_2 + part_3\n\n\ndef b2a_attention(b, a, mask_a=None):\n  """"""Attention of document (b) over question (a).\n\n  Args:\n    b: [bs, len_b, depth]\n    a: [bs, len_a, depth]\n    mask_a: Mask over elem\n\n  Returns:\n    logits: [batch, len_b, len_a]\n\n  """"""\n  if len(mask_a.get_shape()) == 1:\n    mask_a = tf.sequence_mask(mask_a, tf.shape(a)[1])\n  if len(mask_a.get_shape()) == 2:\n    mask_a = tf.expand_dims(mask_a, 1)\n  logits = exp_mask(b, mask_a, mask_is_length=False)\n  probabilities = tf.nn.softmax(logits)  # [bs,len_b,len_a]\n  b2a = tf.matmul(probabilities, a)  # [bs, len_b, d]\n  return b2a\n\n\ndef a2b_attention_dcn(logits, b):\n  """"""Attention of question (a) over document (b).\n\n  Args:\n    logits: [bs, len_b, len_a] tensor\n    b: [bs, len_b, depth] tensor\n\n  Returns:\n    logits: [batch, len_b, depth]\n\n  """"""\n  prob1 = tf.nn.softmax(logits)  # [bs,len_b,len_a]\n  prob2 = tf.nn.softmax(tf.transpose(logits, perm=[0, 2,\n                                                   1]))  # [bs,len_a,len_b]\n  a2b = tf.matmul(tf.matmul(prob1, prob2), b)  # [bs,len_b,d]\n  return a2b\n\n\nVERY_LARGE_NEGATIVE_VALUE = -1e12\n\n\ndef exp_mask(logits, mask, mask_is_length=True):\n  """"""Exponential mask for logits.\n\n  Logits cannot be masked with 0 (i.e. multiplying boolean mask)\n  because expnentiating 0 becomes 1. `exp_mask` adds very large negative value\n  to `False` portion of `mask` so that the portion is effectively ignored\n  when exponentiated, e.g. softmaxed.\n\n  Args:\n    logits: Arbitrary-rank logits tensor to be masked.\n    mask: `boolean` type mask tensor.\n      Could be same shape as logits (`mask_is_length=False`)\n      or could be length tensor of the logits (`mask_is_length=True`).\n    mask_is_length: `bool` value. whether `mask` is boolean mask.\n  Returns:\n    Masked logits with the same shape of `logits`.\n  """"""\n  if mask_is_length:\n    mask = tf.sequence_mask(mask, maxlen=tf.shape(logits)[-1])\n  return logits + (1.0 - tf.cast(mask, \'float\')) * VERY_LARGE_NEGATIVE_VALUE\n\n\n# Metrics\n\n\ndef get_answer_op(context, context_words, answer_start, answer_end):\n  return tf.py_func(\n      data.enum_fn(data.get_answer_tokens),\n      [context, context_words, answer_start, answer_end], \'string\')\n\n\n# token level - Empirically 870 in train set, 700 in dev\nMAX_CONTEXT_SIZE = 900\n\n\ndef get_predictions(context,\n                    context_tokens,\n                    logits_start,\n                    logits_end,\n                    max_answer_size=30):\n  """"""Get prediction op dictionary given start & end logits.\n\n  This dictionary will contain predictions as well as everything needed\n  to produce the nominal answer and identifier (ids).\n\n  Args:\n    context: The original context string\n    context_tokens: Tokens to index into\n    logits_start: [batch_size, context_size]-shaped tensor of start logits\n    logits_end: [batch_size, context_size]-shaped tensor of end logits\n    max_answer_size: Maximum length (in tokens) of a valid answer.\n  Returns:\n    A dictionary of prediction tensors.\n  """"""\n  prob_start = tf.nn.softmax(logits_start)\n  prob_end = tf.nn.softmax(logits_end)\n\n  max_x_len = tf.shape(context_tokens)[1]\n  # This is only for computing span accuracy and not used for training.\n  # Masking with `upper_triangular_matrix` only allows valid spans,\n  # i.e. `answer_pred_start` <= `answer_pred_end`.\n  upper_tri_mat = tf.slice(\n      np.triu(\n          np.ones([MAX_CONTEXT_SIZE, MAX_CONTEXT_SIZE], dtype=\'float32\') -\n          np.triu(\n              np.ones([MAX_CONTEXT_SIZE, MAX_CONTEXT_SIZE], dtype=\'float32\'),\n              k=max_answer_size)), [0, 0], [max_x_len, max_x_len])\n  # Outer product\n  prob_mat = tf.expand_dims(prob_start, -1) * tf.expand_dims(prob_end, 1)\n\n  # Mask out\n  prob_mat *= tf.expand_dims(upper_tri_mat, 0)\n\n  answer_pred_start = tf.argmax(tf.reduce_max(prob_mat, 2), 1)\n  answer_pred_end = tf.argmax(tf.reduce_max(prob_mat, 1), 1)\n  answer_prob = tf.reduce_max(prob_mat, [1, 2])\n\n  predictions = {\n      \'yp1\': answer_pred_start,\n      \'yp2\': answer_pred_end,\n      \'p1\': prob_start,\n      \'p2\': prob_end,\n      \'answer_prob\': answer_prob,\n  }\n\n  answer = get_answer_op(\n      context,\n      context_tokens,\n      answer_pred_start,\n      answer_pred_end,\n  )\n  predictions[\'answer\'] = answer\n  return predictions\n\n\ndef get_attention_bias(sequence_length, maxlen=None):\n  """"""Create attention bias so attention is not applied at padding position.""""""\n  # attention_bias: [batch, 1, 1, memory_length]\n  mask = tf.sequence_mask(sequence_length, maxlen=maxlen)\n  nonpadding = tf.to_float(mask)\n  invert_sequence_mask = tf.to_float(tf.logical_not(mask))\n  attention_bias = common_attention.attention_bias_ignore_padding(\n      invert_sequence_mask)\n  return nonpadding, attention_bias\n\n\ndef separable_conv(x, filters, kernel_size, activation):\n  """"""Apply a depthwise separable 1d convolution.""""""\n  tf.assert_rank(x, 3)\n  net = tf.expand_dims(x, 2)\n  net = tf.layers.separable_conv2d(\n      net,\n      filters=filters,\n      kernel_size=(kernel_size, 1),\n      padding=\'same\',\n      activation=activation)\n  net = tf.squeeze(net, axis=2)\n  return net\n\n\ndef sequence_encoder(inputs, length, is_training, cfg):\n  """"""Encode a sequence using self attention, convolutions, and dense layers.\n\n  Args:\n    inputs: [batch x length x depth] tensor to encode\n    length: [batch] tensor containing length of each sequence as an int\n    is_training: bool indicating whether we are training\n    cfg: Layer configuration\n\n  Returns:\n    Encoded sequence\n\n  Raises:\n    ValueError: If cfg.structure is invalid.\n  """"""\n  cfg = utils.Config(cfg)\n  assert length is not None\n  assert is_training in [False, True]\n\n  # Turn off dropout at test time.\n  if not is_training:\n    for k in cfg:\n      if \'dropout\' in k:\n        cfg[k] = 0.0\n\n  # Mask out padding tokens during attention\n  maxlen = None\n  if is_training:\n    # All dimensions must be static on a TPU\n    maxlen = inputs.shape.as_list()[1]\n  _, attention_bias = get_attention_bias(length, maxlen=maxlen)\n\n  if inputs.shape.as_list()[-1] != cfg.hidden_size:\n    # Project to internal size\n    inputs = common_layers.conv1d(\n        inputs=inputs,\n        filters=cfg.hidden_size,\n        kernel_size=1,\n        activation=None,\n        padding=\'SAME\')\n  net = inputs\n  if cfg.timing_signal:\n    net = common_attention.add_timing_signal_nd(net)\n  structure = cfg.structure.split(\',\') * cfg.layers\n  for layer_id, layer_type in enumerate(structure):\n    with tf.variable_scope(\'%s_%d\' % (layer_type, layer_id)):\n      layer_input = net\n      net = common_layers.layer_norm(net)\n      if layer_type == \'att\':\n        net = common_attention.multihead_attention(\n            query_antecedent=net,\n            memory_antecedent=None,\n            bias=attention_bias,\n            total_key_depth=cfg.hidden_size,\n            total_value_depth=cfg.hidden_size,\n            output_depth=cfg.hidden_size,\n            num_heads=cfg.attention_heads,\n            dropout_rate=cfg.attention_dropout,\n            attention_type=cfg.attention_type,\n            make_image_summary=False)\n      elif layer_type == \'conv\':\n        if cfg.separable_conv:\n          net = separable_conv(\n              net,\n              filters=cfg.hidden_size,\n              kernel_size=cfg.kernel_size,\n              activation=tf.nn.relu)\n        else:\n          net = common_layers.conv1d(\n              inputs=net,\n              filters=cfg.hidden_size,\n              kernel_size=cfg.kernel_size,\n              activation=tf.nn.relu,\n              padding=\'SAME\')\n      elif layer_type == \'ffn\':\n        # TODO(ddohan): See how expert_utils used to do the dense layer\n        net = tf.layers.dense(\n            net,\n            units=int(cfg.ffn_multiplier * cfg.hidden_size),\n            activation=tf.nn.relu)\n        net = tf.layers.dense(net, units=cfg.hidden_size, activation=None)\n      else:\n        raise ValueError(\'Unknown layer type %s\' % layer_type)\n\n      if cfg.layer_dropout:\n        net = tf.nn.dropout(net, keep_prob=1.0 - cfg.layer_dropout)\n      net += layer_input\n  net = common_layers.layer_norm(net)\n  return net\n\n\ndef create_eval_scaffold_fn(ema_decay, model_dir):\n  """"""Returns scaffold function for evaluation step.""""""\n\n  def scaffold_fn():\n    """"""Scaffold function for Estimator, used for restoring EMA vars.""""""\n    # If we used EMA for training, load the exponential moving\n    # average variables for evaluations.  Otherwise, load the\n    # normal variables.\n    ema = tf.train.ExponentialMovingAverage(decay=ema_decay)\n    var_dict = ema.variables_to_restore(tf.trainable_variables())\n    tf.logging.info(model_dir)\n    if model_dir is None:\n      saver_filename = None\n    else:\n      saver_filename = os.path.join(model_dir, \'model_saver_file\')\n    saver = tf.train.Saver(var_dict,\n                           max_to_keep=2,\n                           filename=saver_filename)\n    return tf.train.Scaffold(saver=saver)\n\n  return scaffold_fn\n\n\ndef build_train_op(loss, is_tpu, opt_cfg, trainable_vars=None):\n  """"""Build the optimizer, compute gradients, and build training op.""""""\n  trainable_vars = trainable_vars or tf.trainable_variables()\n\n  def decay_fn(learning_rate, global_step):\n    # Linear warmup from 0.\n\n    lr_decay = tf.minimum(1.0, tf.to_float(global_step) / opt_cfg.warmup_steps)\n    return lr_decay * learning_rate\n\n  def optimizer(lr):\n    opt = tf.train.AdamOptimizer(\n        learning_rate=lr,\n        beta1=opt_cfg.beta1,\n        beta2=opt_cfg.beta2,\n        epsilon=opt_cfg.epsilon)\n    if is_tpu:\n      opt = tf.contrib.tpu.CrossShardOptimizer(opt)\n    return opt\n\n  if opt_cfg.l2_reg:\n    tf.logging.info(\'Applying l2 regularization of %s\', opt_cfg.l2_reg)\n    decay_costs = []\n    for var in trainable_vars:\n      decay_costs.append(tf.nn.l2_loss(var))\n\n    loss += tf.multiply(opt_cfg.l2_reg, tf.add_n(decay_costs))\n\n  train_op = tf.contrib.layers.optimize_loss(\n      loss=loss,\n      global_step=tf.train.get_global_step(),\n      learning_rate=opt_cfg.lr,\n      learning_rate_decay_fn=decay_fn,\n      clip_gradients=opt_cfg.grad_clip_norm,\n      optimizer=optimizer,\n      variables=trainable_vars,\n      gradient_noise_scale=None,\n      summaries=[] if is_tpu else [\n          \'learning_rate\',\n          \'loss\',\n      ])\n\n  if opt_cfg.ema_decay < 1.0:\n    ema = tf.train.ExponentialMovingAverage(decay=opt_cfg.ema_decay)\n    maintain_average_op = ema.apply(trainable_vars)\n    with tf.control_dependencies([train_op]):\n      train_op = tf.group(maintain_average_op)\n\n  return train_op\n\n\ndef model_fn(\n    features,  # This is batch_features from input_fn\n    labels,    # This is batch_labels from input_fn\n    mode,      # An instance of tf.estimator.ModeKeys\n    params):   # Additional configuration\n  """"""Model function defining a QANet model.""""""\n  is_training = mode == \'train\'\n  cfg = utils.Config(params)\n\n  def make_encoder(encoder_cfg, name):\n\n    def call_encoder(inputs, length):\n      return sequence_encoder(\n          inputs=inputs,\n          length=length,\n          is_training=is_training,\n          cfg=encoder_cfg)\n\n    return tf.make_template(name, call_encoder)\n\n  xl = features[\'context_length\']\n  ql = features[\'question_length\']\n\n  x = features[\'context_vecs\']\n  q = features[\'question_vecs\']\n\n  def emb_dropout(net):\n    return tf.layers.dropout(\n        net, rate=cfg.embedding.dropout_rate, training=is_training)\n\n  def output_dropout(net):\n    return tf.layers.dropout(\n        net, rate=cfg.output_dropout_rate, training=is_training)\n\n  encoder_emb = make_encoder(cfg.encoder_emb, \'encoder_emb\')\n  encoder_model = make_encoder(cfg.encoder_model, \'encoder_model\')\n\n  def encode_emb(net, length):\n    net = emb_dropout(net)\n    net = encoder_emb(net, length)\n    net = emb_dropout(net)\n    return net\n\n  x = encode_emb(x, xl)\n  q = encode_emb(q, ql)\n\n  with tf.variable_scope(\'attention\'):\n    xq, qx = bi_attention_memory_efficient_dcn(a=q, b=x, mask_a=ql, mask_b=xl)\n\n  net = tf.concat([x, xq, x * xq, x * qx], 2)\n  net = output_dropout(encoder_model(net, xl))\n  x_enc = output_dropout(encoder_model(net, xl))\n  start = output_dropout(encoder_model(x_enc, xl))\n  end = output_dropout(encoder_model(start, xl))\n\n  logits_start = exp_mask(\n      tf.squeeze(\n          tf.layers.dense(tf.concat([x_enc, start], 2), 1, name=\'logits_start\'),\n          2), xl)\n  logits_end = exp_mask(\n      tf.squeeze(\n          tf.layers.dense(tf.concat([x_enc, end], 2), 1, name=\'logits_end\'), 2),\n      xl)\n  predictions = {\n      \'logits_start\': logits_start,\n      \'logits_end\': logits_end,\n  }\n\n  starts = features[\'answers_start_token\']\n  ends = features[\'answers_end_token\']\n  loss = get_loss(\n      answer_start=starts[:, 0],\n      answer_end=ends[:, 0],\n      logits_start=logits_start,\n      logits_end=logits_end)\n\n  # Eval never runs on TPU\n  if not is_training:\n    orig_context = features[\'context_tokens\']\n    predictions[\'id\'] = features[\'id\']\n    predictions.update(\n        get_predictions(\n            context=features[\'context\'],\n            context_tokens=orig_context,\n            logits_start=logits_start,\n            logits_end=logits_end))\n\n  train_op = None\n  scaffold_fn = tf.train.Scaffold\n  if is_training:\n    train_op = build_train_op(loss, is_tpu=cfg.tpu.enable, opt_cfg=cfg.opt)\n\n  eval_metrics = None\n  if mode == \'eval\':\n    if cfg.opt.ema_decay < 1.0:\n      scaffold_fn = create_eval_scaffold_fn(cfg.opt.ema_decay, cfg.model_dir)\n    eval_metrics = (data.metric_fn,\n                    dict(\n                        answers=labels[\'answers\'],\n                        prediction=predictions[\'answer\'],\n                        start=starts,\n                        end=ends,\n                        yp1=predictions[\'yp1\'],\n                        yp2=predictions[\'yp2\'],\n                        num_answers=labels[\'num_answers\']))\n\n  # Run eval on CPU/GPU always\n  if cfg.tpu.enable and is_training:\n    return tf.contrib.tpu.TPUEstimatorSpec(\n        mode,\n        loss=loss,\n        predictions=predictions,\n        train_op=train_op,\n        eval_metrics=eval_metrics,\n        scaffold_fn=scaffold_fn)\n  else:\n    eval_metric_ops = None\n    if mode == \'eval\':\n      eval_metric_ops = eval_metrics[0](**eval_metrics[1])\n\n    return tf.estimator.EstimatorSpec(\n        mode,\n        loss=loss,\n        predictions=predictions,\n        train_op=train_op,\n        eval_metric_ops=eval_metric_ops,\n        scaffold=scaffold_fn())\n\n\ndef get_estimator(**kwargs):\n  """"""Construct an estimator.""""""\n  cfg = utils.Config(kwargs)\n\n  if cfg.tpu.get(\'name\'):\n    tf.logging.info(\'Using cluster resolver.\')\n    cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n        cfg.tpu.name, zone=cfg.tpu.zone, project=cfg.tpu.gcp_project)\n    master = None\n  else:\n    cluster_resolver = None\n    master = cfg.master\n\n  tf.logging.info(\'Config:\\n %s\' % cfg)\n  if cfg.tpu.enable:\n    if not cfg.steps_per_epoch:\n      raise ValueError(\'steps_per_epoch must be nonzero on TPU.\')\n    exp = tf.contrib.tpu.TPUEstimator(\n        model_fn=model_fn,\n        config=tf.contrib.tpu.RunConfig(\n            cluster=cluster_resolver,\n            master=master,\n            model_dir=cfg.model_dir,\n            tpu_config=tf.contrib.tpu.TPUConfig(\n                iterations_per_loop=cfg.steps_per_epoch)),\n        use_tpu=True,\n        eval_on_tpu=False,\n        # TPU requires these args, but they are ignored inside the input\n        # function, which directly get train_batch_size or eval_batch_size.\n        train_batch_size=cfg.dataset.train_batch_size,\n        eval_batch_size=cfg.dataset.eval_batch_size,\n        params=cfg,\n    )\n  else:\n    exp = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=cfg.model_dir, params=cfg)\n\n  return exp\n'"
tpu/models/experimental/qanet/preprocess.py,13,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Preprocess data into TFRecords and construct pretrained embedding set.\n\nIf embedding_path is provided, then also filter down the vocab to only words\npresent in the dataset.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf\n\nimport data\n\nflags.DEFINE_string(\'input_path\', \'\', \'Comma separated path to JSON files.\')\nflags.DEFINE_integer(\'max_shard_size\', 11000, \'Number of examples per shard.\')\nflags.DEFINE_string(\'output_path\', \'/tmp\', \'TFRecord path/name prefix. \')\nflags.DEFINE_string(\'embedding_path\', \'\', \'Path to embeddings in GLOVE format.\')\n\nFLAGS = flags.FLAGS\n\n\ndef get_tf_example(example):\n  """"""Get `tf.train.Example` object from example dict.\n\n  Args:\n    example: tokenized, indexed example.\n  Returns:\n    `tf.train.Example` object corresponding to the example.\n  Raises:\n    ValueError: if a key in `example` is invalid.\n  """"""\n  feature = {}\n  for key, val in sorted(example.items()):\n    if not isinstance(val, list):\n      val = [val]\n\n    if isinstance(val[0], str) or isinstance(val[0], unicode):\n      dtype = \'bytes\'\n    elif isinstance(val[0], int):\n      dtype = \'int64\'\n    else:\n      raise TypeError(\'`%s` has an invalid type: %r\' % (key, type(val[0])))\n\n    if dtype == \'bytes\':\n      # Transform unicode into bytes if necessary.\n      if isinstance(val[0], unicode):\n        val = [each.encode(\'utf-8\') for each in val]\n      feature[key] = tf.train.Feature(bytes_list=tf.train.BytesList(value=val))\n    elif dtype == \'int64\':\n      feature[key] = tf.train.Feature(int64_list=tf.train.Int64List(value=val))\n    else:\n      raise TypeError(\'`%s` has an invalid type: %r\' % (key, type(val[0])))\n  return tf.train.Example(features=tf.train.Features(feature=feature))\n\n\ndef write_as_tf_records(out_dir, name, examples):\n  """"""Dumps examples as TFRecord files.\n\n  Args:\n    out_dir: Output directory.\n    name: Name of this split.\n    examples: a `list` of `dict`, where each dict is indexed example.\n  """"""\n  tf.gfile.MakeDirs(out_dir)\n\n  writer = None\n  counter = 0\n  num_shards = 0\n  for example in examples:\n    if writer is None:\n      path = os.path.join(\n          out_dir, \'{name}_{shards}.tfrecord\'.format(\n              name=name, shards=str(num_shards).zfill(4)))\n      writer = tf.python_io.TFRecordWriter(path)\n    tf_example = get_tf_example(example)\n    writer.write(tf_example.SerializeToString())\n    counter += 1\n    if counter == FLAGS.max_shard_size:\n      counter = 0\n      writer.close()\n      writer = None\n      num_shards += 1\n  if writer is not None:\n    writer.close()\n\n\ndef main(argv):\n  del argv  # Unused.\n  paths = FLAGS.input_path.split(\',\')\n  tf.logging.info(\'Loading data from: %s\', paths)\n  vocab = set()\n\n  for path in paths:\n    _, name = os.path.split(path)\n    tf.logging.info(name)\n    if \'-v1.1.json\' not in name:\n      raise ValueError(\'Input must be named <split_name>-v1.1.json\')\n    name = name.split(\'-\')[0]\n    generator = data.squad_generator(path=path)\n    examples = list(generator)\n    write_as_tf_records(FLAGS.output_path, name, examples)\n    for example in examples:\n      for k in [\'question_tokens\', \'context_tokens\']:\n        for word in example[k]:\n          # The decode to utf-8 is important to ensure the comparisons occur\n          # properly when we filter below.\n          vocab.add(word.decode(\'utf-8\'))\n  del examples\n\n  if FLAGS.embedding_path:\n    tf.logging.info(\'Filtering down embeddings from: %s\' % FLAGS.embedding_path)\n    filtered = data.get_embedding_map(FLAGS.embedding_path, word_subset=vocab)\n\n    ordered = []\n    if \'UNK\' not in filtered:\n      # We add a fixed UNK token to the vocab consisting of all zeros.\n      # Get the embedding size by looking at one of the embeddings we already\n      # have.\n      embed_size = len(filtered[filtered.keys()[0]])\n      ordered.append((\'UNK\', [0.0] * embed_size))\n    else:\n      ordered.append((\'UNK\', filtered[\'UNK\']))\n      del filtered[\'UNK\']\n\n    for k, v in filtered.iteritems():\n      ordered.append((k, v))\n\n    tf.logging.info(\'Vocab filtered to %s tokens.\' % len(filtered))\n    tf.logging.info(\'Writing out vocab.\')\n    with tf.gfile.Open(os.path.join(FLAGS.output_path, \'vocab.vec\'), \'w\') as f:\n      for k, v in ordered:\n        f.write(\'%s %s\\n\' % (k, \' \'.join(str(x) for x in v)))\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tpu/models/experimental/qanet/run.py,1,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Main launcher for QANet train/eval/predict.""""""\nimport tensorflow as tf\n\nimport run_lib\n\n\ndef main(_):\n  run_lib.run()\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
tpu/models/experimental/qanet/run_lib.py,26,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Library with train/eval/predict functions.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\nimport pprint\nfrom absl import flags\nimport tensorflow as tf\n\nimport data\nimport model\nimport utils\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    ""tpu"",\n    default=None,\n    help=""The Cloud TPU to use for training. This should be either the name ""\n    ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url."")\nflags.DEFINE_string(\n    ""gcp_project"",\n    default=None,\n    help=""Project name for the Cloud TPU-enabled project. If not specified, we ""\n    ""will attempt to automatically detect the GCE project from metadata."")\nflags.DEFINE_string(\n    ""tpu_zone"",\n    default=None,\n    help=""GCE zone where the Cloud TPU is located in. If not specified, we ""\n    ""will attempt to automatically detect the GCE project from metadata."")\nflags.DEFINE_boolean(""enable_tpu"", False,\n                     ""Use an attached TPU based on master address."")\n\n# Model settings\n\nflags.DEFINE_string(""model_dir"", None, ""Estimator model_dir."")\nflags.DEFINE_string(""mode"", ""train"", ""One of {train, eval, predict}."")\nflags.DEFINE_string(""predict_path"", ""/tmp/qanet_predictions.json"",\n                    ""Path to write predictions to."")\nflags.DEFINE_string(""master"", """", ""Master"")\n\nflags.DEFINE_string(""data_path"", """", ""Data directory."")\nflags.DEFINE_string(""config_file"", """", ""Path to config file."")\nflags.DEFINE_string(""config"", """", ""Config overrides"")\n\nFLAGS = flags.FLAGS\n\n\ndef _load_config(model_dir):\n  tf.logging.info(""model_dir = "" + model_dir)\n  with tf.gfile.GFile(os.path.join(model_dir, ""config.json"")) as f:\n    cfg = json.load(f)\n    cfg = utils.to_config(cfg)\n  return cfg\n\n\ndef train_and_eval(cfg, do_eval=True, report_fn=None):\n  """"""Run training (and evaluation if on a GPU).""""""\n  tf.logging.info(""cfg.model_dir = "" + cfg.model_dir)\n  # Save out config to model directory\n  assert FLAGS.mode == ""train""\n  tf.gfile.MakeDirs(cfg.model_dir)\n  with tf.gfile.GFile(os.path.join(cfg.model_dir, ""config.json""), ""w"") as f:\n    json.dump(cfg, f)\n\n  if not cfg.dataset.num_repeats and not cfg.steps_per_epoch:\n    raise ValueError(""Must have a fixed num repeats or epoch step size."")\n\n  # Construct inputs and estimator\n  train_input, eval_input = data.build_dataset(\n      cfg.dataset, is_tpu=cfg.tpu.enable)\n  estimator = model.get_estimator(**cfg)\n\n  if do_eval:\n    eval_metrics = None\n    for i in range(cfg.num_epochs):\n      tf.logging.info(""Starting epoch %s/%s"" % (i + 1, cfg.num_epochs))\n      train_metrics = estimator.train(\n          input_fn=train_input, steps=cfg.steps_per_epoch or None)\n      tf.logging.info(pprint.pformat(train_metrics))\n      eval_metrics = estimator.evaluate(input_fn=eval_input)\n      tf.logging.info(pprint.pformat(eval_metrics))\n      if report_fn:\n        report_fn(eval_metrics)\n    return eval_metrics\n  else:\n    for i in range(cfg.num_epochs):\n      tf.logging.info(""Starting epoch %s/%s"" % (i + 1, cfg.num_epochs))\n      train_metrics = estimator.train(\n          input_fn=train_input, steps=cfg.steps_per_epoch)\n      tf.logging.info(pprint.pformat(train_metrics))\n    return dict()\n\n\ndef evaluate(override_cfg, model_dir, continuous=True):\n  """"""Run training and evaluation.""""""\n  tf.logging.info(""model_dir = "" + model_dir)\n  try:\n    cfg = _load_config(model_dir)\n  except tf.errors.NotFoundError:\n    tf.logging.info(""Model directory does not exist yet. Creating new config."")\n    cfg = model.build_config(model_dir=model_dir, data_path=FLAGS.data_path)\n  tf.logging.info(cfg)\n  tf.logging.info(override_cfg)\n  cfg = utils.merge(cfg, override_cfg)\n\n  cfg.tpu.enable = False\n  cfg.dataset.max_length = None\n\n  # Construct inputs and estimator\n  _, eval_input = data.build_dataset(cfg.dataset, is_tpu=cfg.tpu.enable)\n  estimator = model.get_estimator(**cfg)\n  if continuous:\n    checkpoints_iterator = tf.contrib.training.checkpoints_iterator(\n        cfg.model_dir)\n    eval_metrics = None\n    for ckpt_path in checkpoints_iterator:\n      eval_metrics = estimator.evaluate(\n          input_fn=eval_input, checkpoint_path=ckpt_path)\n      tf.logging.info(pprint.pformat(eval_metrics))\n    return eval_metrics\n  else:\n    eval_metrics = estimator.evaluate(input_fn=eval_input)\n    return eval_metrics\n\n\ndef predict(override_cfg, model_dir):\n  """"""Run model over a dataset and dump predictions to json file.""""""\n  assert FLAGS.predict_path\n  cfg = _load_config(model_dir)\n  cfg = utils.merge(cfg, override_cfg)\n  input_fn = data.get_input_fn(\n      split=cfg.dataset.eval_split,\n      max_length=None,\n      repeat=False,\n      shuffle=False,\n      cache=False,\n      limit=None,\n      data_path=cfg.dataset.data_path,\n      vocab_path=cfg.dataset.vocab_path,\n      is_tpu=False,\n      use_generator=True,\n      is_training=False)\n  estimator = model.get_estimator(**cfg)\n  predictions = dict()\n  for i, prediction in enumerate(estimator.predict(input_fn)):\n    predictions[prediction[""id""]] = prediction[""answer""]\n    if i % 100 == 0:\n      tf.logging.info(""Prediction %s | %s: %s"" % (i, prediction[""id""],\n                                                  prediction[""answer""]))\n\n  # Dump results to a file\n  with tf.gfile.GFile(FLAGS.predict_path, ""w"") as f:\n    json.dump(predictions, f)\n\n\ndef create_config(model_dir, hparams=None):\n  """"""Creates config instance.""""""\n  tf.logging.info(""model_dir = "" + model_dir)\n  assert model_dir\n\n  if hparams:\n    tf.logging.info(""Given override cfg:\\n%s"" % pprint.pformat(hparams))\n  else:\n    hparams = dict()\n\n  # Build the default config\n  cfg = model.build_config(model_dir=model_dir, data_path=FLAGS.data_path)\n\n  if FLAGS.config_file:\n    with tf.gfile.GFile(FLAGS.config_file) as f:\n      file_cfg = json.load(f)\n      file_cfg = utils.to_config(file_cfg)\n    tf.logging.info(""Loaded config from file:\\n%s"" % file_cfg)\n    cfg = utils.merge_fixed_structure(cfg, file_cfg)\n\n  # Override from flags\n  overrides = dict()\n  if FLAGS.config:\n    overrides = utils.parse_config_string(FLAGS.config)\n    tf.logging.info(""Parsed config overrides:\\n%s"" % overrides)\n    cfg = utils.merge_fixed_structure(cfg, overrides)\n\n  if FLAGS.master:\n    cfg.master = FLAGS.master\n\n  cfg = utils.merge_fixed_structure(cfg, utils.unflatten_dict(hparams))\n\n  tf.logging.info(""Operative config:\\n%s"" % cfg)\n\n  return cfg\n\ndef run():\n  """"""Runs train/eval/predict depends on mode flag.""""""\n  tf.logging.set_verbosity(tf.logging.INFO)\n  cfg = create_config(model_dir=FLAGS.model_dir)\n\n  if FLAGS.tpu:\n    cfg.tpu.name = FLAGS.tpu\n    cfg.tpu.zone = FLAGS.tpu_zone\n    cfg.tpu.gcp_project = FLAGS.gcp_project\n    cfg.tpu.enable = True\n  else:\n    # Toggle TPU relevant settings\n    if FLAGS.enable_tpu:\n      cfg.tpu.enable = True\n    else:\n      cfg.tpu.enable = False\n\n  if cfg.tpu.enable:\n    assert FLAGS.mode == ""train""\n\n  if ""train"" in FLAGS.mode:\n    return train_and_eval(cfg, do_eval=not cfg.tpu.enable)\n  elif FLAGS.mode == ""eval"":\n    return evaluate(override_cfg=cfg, model_dir=FLAGS.model_dir)\n  elif FLAGS.mode == ""predict"":\n    return predict(override_cfg=cfg, model_dir=FLAGS.model_dir)\n  else:\n    raise NotImplementedError(""Unknown run mode %s"" % FLAGS.mode)\n'"
tpu/models/experimental/qanet/utils.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Config utilities.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport math\nimport pprint\n\n\n# TODO(ddohan): FrozenConfig type\nclass Config(dict):\n  """"""a dictionary that supports dot and dict notation.\n\n  Create:\n    d = Config()\n    d = Config({\'val1\':\'first\'})\n\n  Get:\n    d.val2\n    d[\'val2\']\n\n  Set:\n    d.val2 = \'second\'\n    d[\'val2\'] = \'second\'\n  """"""\n  __getattr__ = dict.__getitem__\n  __setattr__ = dict.__setitem__\n  __delattr__ = dict.__delitem__\n\n  def __str__(self):\n    return pprint.pformat(self)\n\n  def __deepcopy__(self, memo):\n    return self.__class__([(copy.deepcopy(k, memo), copy.deepcopy(v, memo))\n                           for k, v in self.items()])\n\n\ndef to_config(mapping):\n  out = Config(copy.deepcopy(mapping))\n  for k, v in out.iteritems():\n    if isinstance(v, dict):\n      out[k] = to_config(v)\n  return out\n\n\ndef unflatten_dict(flat_dict):\n  """"""Convert a flattened dict to a nested dict.\n\n  Inverse of flatten_config.\n\n  Args:\n    flat_dict: A dictionary to unflatten.\n\n  Returns:\n    A dictionary with all keys containing `.` split into nested dicts.\n    {\'a.b.c\': 1} --> {\'a\': {\'b\': {\'c\': 1}}}\n  """"""\n  result = {}\n  for key, val in flat_dict.iteritems():\n    parts = key.split(\'.\')\n    cur = result\n    for part in parts[:-1]:\n      if part not in cur:\n        cur[part] = Config()\n      cur = cur[part]\n    cur[parts[-1]] = val\n  return Config(result)\n\n\ndef parse_config_string(string):\n  """"""Parse a config string such as one produced by `config_to_string`.\n\n  example:\n\n  A config of:\n  ```\n  {\n    \'model\': {\n      \'fn\': \'RNN\'\n    }\n    \'train_steps\': 500\n  }\n  ```\n\n  Yields a serialized string of: `model.fn=RNN,train_steps=500`\n\n  Args:\n   string: String to parse.\n\n  Returns:\n    dict resulting from parsing the string. Keys are split on `.`s.\n\n  """"""\n  result = {}\n  for entry in string.split(\',\'):\n    try:\n      key, val = entry.split(\'=\')\n    except ValueError:\n      raise ValueError(\'Error parsing entry %s\' % entry)\n    val = _try_numeric(val)\n    result[key] = val\n  return unflatten_dict(result)\n\n\ndef _try_numeric(string):\n  """"""Attempt to convert a string to an int then a float.\n\n\n  Args:\n    string: String to convert\n\n  Returns:\n    Attempts, in order, to return an integer, a float, and finally the original\n    string.\n  """"""\n  try:\n    float_val = float(string)\n    if math.floor(float_val) == float_val:\n      return int(float_val)\n    return float_val\n  except ValueError:\n    return string\n\n\ndef _convert_type(val, tp):\n  """"""Attempt to convert given value to type.\n\n  This is used when trying to convert an input value to fit the desired type.\n\n  Args:\n    val: Value to convert.\n    tp: Type to convert to.\n\n  Returns:\n    Value after type conversion.\n\n  Raises:\n    ValueError: If the conversion fails.\n  """"""\n  if tp in [int, float, str, unicode, bool, tuple, list]:\n    in_type = type(val)\n    cast = tp(val)\n    if in_type(cast) != val:\n      raise TypeError(\n          \'Type conversion between %s (%s) and %s (%s) loses information.\' %\n          (val, type(val), cast, tp))\n    return cast\n  raise ValueError(\n      \'Cannot convert %s (type %s) to type %s\' % (val, type(val), tp))\n\n\ndef merge_fixed_structure(*args, **kwargs):\n  kwargs[\'_merge_validate\'] = True\n  return merge(*args, **kwargs)\n\n\ndef merge(*args, **kwargs):\n  """"""Merge together an iterable of configs in order.\n\n  The first instance of a key determines its type. The last instance of a key\n  determines its value.\n\n  Example:\n\n  args[0] = {\n    \'layers\': 1,\n    \'dropout\': 0.5\n  }\n  kwargs = {\n    dropout\': 0\n  }\n\n  Final result:\n  {\n    \'layers\': 1,\n    \'dropout\': 0.0  # note the type change\n  }\n\n  Args:\n    *args: List of dict-like objects to merge in order.\n    **kwargs: Any additional values to add. Treated like as a final additional\n      dict to merge.\n\n  Returns:\n    dict resulting from merging all configs together.\n\n  Raises:\n    TypeError: if there is a type mismatch between the same key across dicts.\n    ValueError: If _merge_validate is specified and there is a type mismatch.\n  """"""\n  assert args\n  validate = False\n  if \'_merge_validate\' in kwargs:\n    validate = kwargs[\'_merge_validate\']\n    del kwargs[\'_merge_validate\']\n  config = copy.deepcopy(args[0])\n  configs = list(args)\n  configs.append(kwargs)\n  for c in configs[1:]:\n    for k, v in c.iteritems():\n      if isinstance(v, dict):\n        v = copy.deepcopy(v)\n      if k in config:\n        value_type = type(config[k])\n\n        if config[k] is not None and v is not None and not isinstance(\n            v, value_type):\n          v = value_type(v)\n\n        if isinstance(v, dict):\n          config[k] = merge(config[k], v, _merge_validate=validate)\n        else:\n          config[k] = v\n      else:\n        if validate:\n          raise ValueError(\'Target did not contain key %s\' % k)\n        config[k] = v\n\n  return copy.deepcopy(config)\n'"
tpu/models/experimental/resnet50_keras/eval_utils.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Evaluation utils for `KerasTPUmodel`.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom six.moves import xrange\n\n\ndef multi_top_k_accuracy(model, evaluation_generator, eval_steps, ks=(1, 5)):\n  """"""Calculates top k accuracy for the given `k` values.\n\n  Args:\n    model: `KerasTPUModel` to evaluate.\n    evaluation_generator: a Python generator to generate (features, labels) for\n                          evaluation.\n    eval_steps: int, number of evaluation steps.\n    ks: a tuple of int, position values to calculate top k accurary.\n\n  Returns:\n    A dictionary containing top k accuracy for the given `k` values.\n  """"""\n  def _count_matched(predictions, labels, ks):\n    """"""Count number of pairs with label in any of top k predictions.""""""\n    top_k_matched = dict.fromkeys(ks, 0)\n    for prediction, label in zip(predictions, labels):\n      for k in ks:\n        top_k_predictions = np.argpartition(prediction, -k)[-k:]\n        if label in top_k_predictions:\n          top_k_matched[k] += 1\n    return top_k_matched\n\n  total = 0\n  top_k_matched = dict.fromkeys(ks, 0)\n  for _ in xrange(eval_steps):\n    (features, labels) = next(evaluation_generator)\n    predictions = model.predict_on_batch(features)\n    batch_top_k_matched = _count_matched(predictions, labels, ks)\n    for k, matched in batch_top_k_matched.items():\n      top_k_matched[k] += matched\n    total += len(labels)\n\n  return dict([(""top_{0}_accuracy"".format(k), matched / float(total))\n               for k, matched in top_k_matched.items()])\n'"
tpu/models/experimental/resnet50_keras/imagenet_input.py,32,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Efficient ImageNet input pipeline using tf.data.Dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\n\nimport resnet_preprocessing\n\n\ndef image_serving_input_fn():\n  """"""Serving input fn for raw images.""""""\n\n  def _preprocess_image(image_bytes):\n    """"""Preprocess a single raw image.""""""\n    image = resnet_preprocessing.preprocess_image(\n        image_bytes=image_bytes, is_training=False)\n    return image\n\n  image_bytes_list = tf.placeholder(\n      shape=[None],\n      dtype=tf.string,\n  )\n  images = tf.map_fn(\n      _preprocess_image, image_bytes_list, back_prop=False, dtype=tf.float32)\n  return tf.estimator.export.ServingInputReceiver(\n      images, {\'image_bytes\': image_bytes_list})\n\nclass ImageNetInput(object):\n  """"""Generates ImageNet input_fn for training or evaluation.\n\n  The training data is assumed to be in TFRecord format with keys as specified\n  in the dataset_parser below, sharded across 1024 files, named sequentially:\n      train-00000-of-01024\n      train-00001-of-01024\n      ...\n      train-01023-of-01024\n\n  The validation data is in the same format but sharded in 128 files.\n\n  The format of the data required is created by the script at:\n      https://github.com/tensorflow/tpu/blob/master/tools/datasets/imagenet_to_gcs.py\n\n  Args:\n    is_training: `bool` for whether the input is for training.\n    data_dir: `str` for the directory of the training and validation data;\n        if \'null\' (the literal string \'null\', not None), then construct a null\n        pipeline, consisting of empty images.\n    use_bfloat16: If True, use bfloat16 precision; else use float32.\n    per_core_batch_size: The per-TPU-core batch size to use.\n  """"""\n\n  def __init__(self,\n               is_training,\n               data_dir,\n               use_bfloat16=False,\n               per_core_batch_size=128):\n    self.image_preprocessing_fn = resnet_preprocessing.preprocess_image\n    self.is_training = is_training\n    self.use_bfloat16 = use_bfloat16\n    self.data_dir = data_dir\n    if self.data_dir == \'null\' or self.data_dir == \'\':\n      self.data_dir = None\n    self.per_core_batch_size = per_core_batch_size\n\n  def dataset_parser(self, value):\n    """"""Parse an ImageNet record from a serialized string Tensor.""""""\n    keys_to_features = {\n        \'image/encoded\':\n            tf.FixedLenFeature((), tf.string, \'\'),\n        \'image/format\':\n            tf.FixedLenFeature((), tf.string, \'jpeg\'),\n        \'image/class/label\':\n            tf.FixedLenFeature([], tf.int64, -1),\n        \'image/class/text\':\n            tf.FixedLenFeature([], tf.string, \'\'),\n        \'image/object/bbox/xmin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/class/label\':\n            tf.VarLenFeature(dtype=tf.int64),\n    }\n\n    parsed = tf.parse_single_example(value, keys_to_features)\n    image_bytes = tf.reshape(parsed[\'image/encoded\'], shape=[])\n\n    image = self.image_preprocessing_fn(\n        image_bytes=image_bytes,\n        is_training=self.is_training,\n        use_bfloat16=self.use_bfloat16)\n\n    # Subtract one so that labels are in [0, 1000), and cast to float32 for\n    # Keras model.\n    label = tf.cast(tf.cast(\n        tf.reshape(parsed[\'image/class/label\'], shape=[1]), dtype=tf.int32) - 1,\n                    dtype=tf.float32)\n\n    return image, label\n\n  def input_fn(self):\n    """"""Input function which provides a single batch for train or eval.\n\n    Returns:\n      A `tf.data.Dataset` object.\n    """"""\n    if self.data_dir is None:\n      tf.logging.info(\'Using fake input.\')\n      return self.input_fn_null()\n\n    # Shuffle the filenames to ensure better randomization.\n    file_pattern = os.path.join(\n        self.data_dir, \'train-*\' if self.is_training else \'validation-*\')\n    dataset = tf.data.Dataset.list_files(file_pattern, shuffle=self.is_training)\n\n    if self.is_training:\n      dataset = dataset.repeat()\n\n    def fetch_dataset(filename):\n      buffer_size = 8 * 1024 * 1024     # 8 MiB per file\n      dataset = tf.data.TFRecordDataset(filename, buffer_size=buffer_size)\n      return dataset\n\n    # Read the data from disk in parallel\n    dataset = dataset.apply(\n        tf.contrib.data.parallel_interleave(\n            fetch_dataset, cycle_length=16, sloppy=self.is_training))\n    if self.is_training:\n      dataset = dataset.shuffle(1024)\n\n    # Parse, pre-process, and batch the data in parallel\n    dataset = dataset.apply(\n        tf.contrib.data.map_and_batch(\n            self.dataset_parser, batch_size=self.per_core_batch_size,\n            num_parallel_batches=2,\n            drop_remainder=True))\n\n    # Prefetch overlaps in-feed with training\n    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n    return dataset\n\n  # TODO(xiejw): Remove this generator when we have support for top_k\n  # evaluation.\n  def evaluation_generator(self, sess):\n    """"""Creates a generator for evaluation.""""""\n    next_batch = self.input_fn().make_one_shot_iterator().get_next()\n    while True:\n      try:\n        yield sess.run(next_batch)\n      except tf.errors.OutOfRangeError:\n        return\n\n  def input_fn_null(self):\n    """"""Input function which provides null (black) images.""""""\n    dataset = tf.data.Dataset.range(1).repeat().map(self._get_null_input)\n    dataset = dataset.prefetch(self.per_core_batch_size)\n\n    dataset = dataset.batch(self.per_core_batch_size, drop_remainder=True)\n\n    dataset = dataset.prefetch(32)     # Prefetch overlaps in-feed with training\n    tf.logging.info(\'Input dataset: %s\', str(dataset))\n    return dataset\n\n  def _get_null_input(self, _):\n    null_image = tf.zeros([224, 224, 3], tf.float32)\n    return null_image, tf.constant(0, tf.float32)\n'"
tpu/models/experimental/resnet50_keras/resnet50.py,7,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""ResNet-50 implemented with Keras running on Cloud TPUs.\n\nThis file shows how you can run ResNet-50 on a Cloud TPU using the TensorFlow\nKeras support. This is configured for ImageNet (e.g. 1000 classes), but you can\neasily adapt to your own datasets by changing the code appropriately.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nimport numpy as np\nimport tensorflow as tf\n\nimport eval_utils\nimport imagenet_input\nimport resnet_model\nfrom tensorflow.python.keras import backend as K\n\ntry:\n  import h5py as _  # pylint: disable=g-import-not-at-top\n  HAS_H5PY = True\nexcept ImportError:\n  logging.warning(\'`h5py` is not installed. Please consider installing it \'\n                  \'to save weights for long-running training.\')\n  HAS_H5PY = False\n\n\nflags.DEFINE_bool(\'use_tpu\', True, \'Use TPU model instead of CPU.\')\nflags.DEFINE_string(\'tpu\', None, \'Name of the TPU to use.\')\nflags.DEFINE_string(\'data\', None, \'Path to training and testing data.\')\nflags.DEFINE_string(\n    \'model_dir\', None,\n    (\'The directory where the model weights and training/evaluation summaries \'\n     \'are stored. If unset, model weights will be saved to /tmp and no \'\n     \'summaries will be stored.\'))\nflags.DEFINE_bool(\n    \'eval_top_5_accuracy\', False,\n    \'Eval both top 1 and top 5 accuracy. Otherwise, only eval top 1 accuracy. \'\n    \'N.B. enabling this would slow down the eval time due to using python \'\n    \'generator for evaluation input. Will be deprecated once we have support \'\n    \'for top_k accuracy evaluation.\')\n\nFLAGS = flags.FLAGS\n\n# Imagenet training and test data sets.\nNUM_CLASSES = 1000\nIMAGE_SIZE = 224\nEPOCHS = 90  # Standard imagenet training regime.\nAPPROX_IMAGENET_TRAINING_IMAGES = 1280000  # Approximate number of images.\nAPPROX_IMAGENET_TEST_IMAGES = 48000  # Approximate number of images.\n\n# Training hyperparameters.\nNUM_CORES = 8\nPER_CORE_BATCH_SIZE = 128\nBATCH_SIZE = NUM_CORES * PER_CORE_BATCH_SIZE\nTRAINING_STEPS_PER_EPOCH = int(APPROX_IMAGENET_TRAINING_IMAGES / BATCH_SIZE)\nBASE_LEARNING_RATE = 0.4\n# Learning rate schedule\nLR_SCHEDULE = [    # (multiplier, epoch to start) tuples\n    (1.0, 5), (0.1, 30), (0.01, 60), (0.001, 80)\n]\n\nEVAL_STEPS = int(APPROX_IMAGENET_TEST_IMAGES // BATCH_SIZE)\nWEIGHTS_TXT = \'resnet50_weights.h5\'\n\n\ndef learning_rate_schedule(current_epoch, current_batch):\n  """"""Handles linear scaling rule, gradual warmup, and LR decay.\n\n  The learning rate starts at 0, then it increases linearly per step.\n  After 5 epochs we reach the base learning rate (scaled to account\n    for batch size).\n  After 30, 60 and 80 epochs the learning rate is divided by 10.\n  After 90 epochs training stops and the LR is set to 0. This ensures\n    that we train for exactly 90 epochs for reproducibility.\n\n  Args:\n    current_epoch: integer, current epoch indexed from 0.\n    current_batch: integer, current batch in the current epoch, indexed from 0.\n\n  Returns:\n    Adjusted learning rate.\n  """"""\n  epoch = current_epoch + float(current_batch) / TRAINING_STEPS_PER_EPOCH\n  warmup_lr_multiplier, warmup_end_epoch = LR_SCHEDULE[0]\n  if epoch < warmup_end_epoch:\n    # Learning rate increases linearly per step.\n    return BASE_LEARNING_RATE * warmup_lr_multiplier * epoch / warmup_end_epoch\n  for mult, start_epoch in LR_SCHEDULE:\n    if epoch >= start_epoch:\n      learning_rate = BASE_LEARNING_RATE * mult\n    else:\n      break\n  return learning_rate\n\n\nclass LearningRateBatchScheduler(tf.keras.callbacks.Callback):\n  """"""Callback to update learning rate on every batch (not epoch boundaries).\n\n  N.B. Only support Keras optimizers, not TF optimizers.\n\n  Args:\n      schedule: a function that takes an epoch index and a batch index as input\n          (both integer, indexed from 0) and returns a new learning rate as\n          output (float).\n  """"""\n\n  def __init__(self, schedule):\n    super(LearningRateBatchScheduler, self).__init__()\n    self.schedule = schedule\n    self.epochs = -1\n    self.prev_lr = -1\n\n  def on_epoch_begin(self, epoch, logs=None):\n    if not hasattr(self.model.optimizer, \'lr\'):\n      raise ValueError(\'Optimizer must have a ""lr"" attribute.\')\n    self.epochs += 1\n\n  def on_batch_begin(self, batch, logs=None):\n    lr = self.schedule(self.epochs, batch)\n    if not isinstance(lr, (float, np.float32, np.float64)):\n      raise ValueError(\'The output of the ""schedule"" function should be float.\')\n    if lr != self.prev_lr:\n      K.set_value(self.model.optimizer.lr, lr)\n      self.prev_lr = lr\n      logging.debug(\'Epoch %05d Batch %05d: LearningRateBatchScheduler change \'\n                    \'learning rate to %s.\', self.epochs, batch, lr)\n\n\ndef main(argv):\n  logging.info(\'Building Keras ResNet-50 model\')\n  model = resnet_model.ResNet50(num_classes=NUM_CLASSES)\n\n  if FLAGS.use_tpu:\n    logging.info(\'Converting from CPU to TPU model.\')\n    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    strategy = tf.contrib.tpu.TPUDistributionStrategy(resolver)\n    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n    session_master = resolver.master()\n  else:\n    session_master = \'\'\n\n  logging.info(\'Compiling model.\')\n  model.compile(\n      optimizer=tf.keras.optimizers.SGD(lr=BASE_LEARNING_RATE,\n                                        momentum=0.9,\n                                        nesterov=True),\n      loss=\'sparse_categorical_crossentropy\',\n      metrics=[\'sparse_categorical_accuracy\'])\n\n  callbacks = [LearningRateBatchScheduler(schedule=learning_rate_schedule)]\n  if FLAGS.model_dir:\n    callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=FLAGS.model_dir))\n\n  if FLAGS.data is None:\n    training_images = np.random.randn(\n        BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3).astype(np.float32)\n    training_labels = np.random.randint(NUM_CLASSES, size=BATCH_SIZE,\n                                        dtype=np.int32)\n    logging.info(\'Training model using synthetica data.\')\n    model.fit(\n        training_images,\n        training_labels,\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        callbacks=callbacks)\n    logging.info(\'Evaluating the model on synthetic data.\')\n    model.evaluate(training_images, training_labels, verbose=0)\n  else:\n    imagenet_train = imagenet_input.ImageNetInput(\n        is_training=True,\n        data_dir=FLAGS.data,\n        per_core_batch_size=PER_CORE_BATCH_SIZE)\n    logging.info(\'Training model using real data in directory ""%s"".\',\n                 FLAGS.data)\n    model.fit(imagenet_train.input_fn,\n              epochs=EPOCHS,\n              steps_per_epoch=TRAINING_STEPS_PER_EPOCH,\n              callbacks=callbacks)\n\n    logging.info(\'Evaluating the model on the validation dataset.\')\n    if FLAGS.eval_top_5_accuracy:\n      logging.info(\'Evaluating top 1 and top 5 accuracy using a Python \'\n                   \'generator.\')\n      # We feed the inputs from a Python generator, so we need to build a single\n      # batch for all of the cores, which will be split on TPU.\n      imagenet_eval = imagenet_input.ImageNetInput(\n          is_training=False,\n          data_dir=FLAGS.data,\n          per_core_batch_size=BATCH_SIZE)\n      score = eval_utils.multi_top_k_accuracy(\n          model, imagenet_eval.evaluation_generator(K.get_session()),\n          EVAL_STEPS)\n    else:\n      imagenet_eval = imagenet_input.ImageNetInput(\n          is_training=False,\n          data_dir=FLAGS.data,\n          per_core_batch_size=PER_CORE_BATCH_SIZE)\n      score = model.evaluate(imagenet_eval.input_fn,\n                             steps=EVAL_STEPS,\n                             verbose=1)\n    print(\'Evaluation score\', score)\n\n    if HAS_H5PY:\n      weights_file = os.path.join(\n          FLAGS.model_dir if FLAGS.model_dir else \'/tmp\', WEIGHTS_TXT)\n      logging.info(\'Save weights into %s\', weights_file)\n      model.save_weights(weights_file, overwrite=True)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  app.run(main)\n'"
tpu/models/experimental/resnet50_keras/resnet_model.py,1,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""ResNet50 model for Keras.\n\nAdapted from tf.keras.applications.resnet50.ResNet50().\n\nRelated papers/blogs:\n- https://arxiv.org/abs/1512.03385\n- https://arxiv.org/pdf/1603.05027v2.pdf\n- http://torch.ch/blog/2016/02/04/resnets.html\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport warnings\n\nfrom tensorflow.python.keras import backend\nfrom tensorflow.python.keras import layers\nfrom tensorflow.python.keras import models\nfrom tensorflow.python.keras import regularizers\nfrom tensorflow.python.keras import utils\n\n\nL2_WEIGHT_DECAY = 1e-4\nBATCH_NORM_DECAY = 0.9\nBATCH_NORM_EPSILON = 1e-5\n\n\ndef identity_block(input_tensor, kernel_size, filters, stage, block):\n  """"""The identity block is the block that has no conv layer at shortcut.\n\n  # Arguments\n      input_tensor: input tensor\n      kernel_size: default 3, the kernel size of\n          middle conv layer at main path\n      filters: list of integers, the filters of 3 conv layer at main path\n      stage: integer, current stage label, used for generating layer names\n      block: \'a\',\'b\'..., current block label, used for generating layer names\n\n  # Returns\n      Output tensor for the block.\n  """"""\n  filters1, filters2, filters3 = filters\n  if backend.image_data_format() == \'channels_last\':\n    bn_axis = 3\n  else:\n    bn_axis = 1\n  conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n  bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n  x = layers.Conv2D(filters1, (1, 1),\n                    kernel_initializer=\'he_normal\',\n                    kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                    bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                    name=conv_name_base + \'2a\')(input_tensor)\n  x = layers.BatchNormalization(axis=bn_axis,\n                                momentum=BATCH_NORM_DECAY,\n                                epsilon=BATCH_NORM_EPSILON,\n                                name=bn_name_base + \'2a\')(x)\n  x = layers.Activation(\'relu\')(x)\n\n  x = layers.Conv2D(filters2, kernel_size,\n                    padding=\'same\',\n                    kernel_initializer=\'he_normal\',\n                    kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                    bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                    name=conv_name_base + \'2b\')(x)\n  x = layers.BatchNormalization(axis=bn_axis,\n                                momentum=BATCH_NORM_DECAY,\n                                epsilon=BATCH_NORM_EPSILON,\n                                name=bn_name_base + \'2b\')(x)\n  x = layers.Activation(\'relu\')(x)\n\n  x = layers.Conv2D(filters3, (1, 1),\n                    kernel_initializer=\'he_normal\',\n                    kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                    bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                    name=conv_name_base + \'2c\')(x)\n  x = layers.BatchNormalization(axis=bn_axis,\n                                momentum=BATCH_NORM_DECAY,\n                                epsilon=BATCH_NORM_EPSILON,\n                                name=bn_name_base + \'2c\')(x)\n\n  x = layers.add([x, input_tensor])\n  x = layers.Activation(\'relu\')(x)\n  return x\n\n\ndef conv_block(input_tensor,\n               kernel_size,\n               filters,\n               stage,\n               block,\n               strides=(2, 2)):\n  """"""A block that has a conv layer at shortcut.\n\n  # Arguments\n      input_tensor: input tensor\n      kernel_size: default 3, the kernel size of\n          middle conv layer at main path\n      filters: list of integers, the filters of 3 conv layer at main path\n      stage: integer, current stage label, used for generating layer names\n      block: \'a\',\'b\'..., current block label, used for generating layer names\n      strides: Strides for the second conv layer in the block.\n\n  # Returns\n      Output tensor for the block.\n\n  Note that from stage 3,\n  the second conv layer at main path is with strides=(2, 2)\n  And the shortcut should have strides=(2, 2) as well\n  """"""\n  filters1, filters2, filters3 = filters\n  if backend.image_data_format() == \'channels_last\':\n    bn_axis = 3\n  else:\n    bn_axis = 1\n  conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n  bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n  x = layers.Conv2D(filters1, (1, 1), kernel_initializer=\'he_normal\',\n                    kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                    bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                    name=conv_name_base + \'2a\')(input_tensor)\n  x = layers.BatchNormalization(axis=bn_axis,\n                                momentum=BATCH_NORM_DECAY,\n                                epsilon=BATCH_NORM_EPSILON,\n                                name=bn_name_base + \'2a\')(x)\n  x = layers.Activation(\'relu\')(x)\n\n  x = layers.Conv2D(filters2, kernel_size, strides=strides, padding=\'same\',\n                    kernel_initializer=\'he_normal\',\n                    kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                    bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                    name=conv_name_base + \'2b\')(x)\n  x = layers.BatchNormalization(axis=bn_axis,\n                                momentum=BATCH_NORM_DECAY,\n                                epsilon=BATCH_NORM_EPSILON,\n                                name=bn_name_base + \'2b\')(x)\n  x = layers.Activation(\'relu\')(x)\n\n  x = layers.Conv2D(filters3, (1, 1),\n                    kernel_initializer=\'he_normal\',\n                    kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                    bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                    name=conv_name_base + \'2c\')(x)\n  x = layers.BatchNormalization(axis=bn_axis,\n                                momentum=BATCH_NORM_DECAY,\n                                epsilon=BATCH_NORM_EPSILON,\n                                name=bn_name_base + \'2c\')(x)\n\n  shortcut = layers.Conv2D(filters3, (1, 1), strides=strides,\n                           kernel_initializer=\'he_normal\',\n                           kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                           bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                           name=conv_name_base + \'1\')(input_tensor)\n  shortcut = layers.BatchNormalization(axis=bn_axis,\n                                       momentum=BATCH_NORM_DECAY,\n                                       epsilon=BATCH_NORM_EPSILON,\n                                       name=bn_name_base + \'1\')(shortcut)\n\n  x = layers.add([x, shortcut])\n  x = layers.Activation(\'relu\')(x)\n  return x\n\n\ndef ResNet50(num_classes):\n  """"""Instantiates the ResNet50 architecture.\n\n  Args:\n    num_classes: `int` number of classes for image classification.\n\n  Returns:\n      A Keras model instance.\n  """"""\n  # Determine proper input shape\n  if backend.image_data_format() == \'channels_first\':\n    input_shape = (3, 224, 224)\n    bn_axis = 1\n  else:\n    input_shape = (224, 224, 3)\n    bn_axis = 3\n\n  img_input = layers.Input(shape=input_shape)\n  x = layers.ZeroPadding2D(padding=(3, 3), name=\'conv1_pad\')(img_input)\n  x = layers.Conv2D(64, (7, 7),\n                    strides=(2, 2),\n                    padding=\'valid\',\n                    kernel_initializer=\'he_normal\',\n                    kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                    bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n                    name=\'conv1\')(x)\n  x = layers.BatchNormalization(axis=bn_axis,\n                                momentum=BATCH_NORM_DECAY,\n                                epsilon=BATCH_NORM_EPSILON,\n                                name=\'bn_conv1\')(x)\n  x = layers.Activation(\'relu\')(x)\n  x = layers.ZeroPadding2D(padding=(1, 1), name=\'pool1_pad\')(x)\n  x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n  x = conv_block(x, 3, [64, 64, 256], stage=2, block=\'a\', strides=(1, 1))\n  x = identity_block(x, 3, [64, 64, 256], stage=2, block=\'b\')\n  x = identity_block(x, 3, [64, 64, 256], stage=2, block=\'c\')\n\n  x = conv_block(x, 3, [128, 128, 512], stage=3, block=\'a\')\n  x = identity_block(x, 3, [128, 128, 512], stage=3, block=\'b\')\n  x = identity_block(x, 3, [128, 128, 512], stage=3, block=\'c\')\n  x = identity_block(x, 3, [128, 128, 512], stage=3, block=\'d\')\n\n  x = conv_block(x, 3, [256, 256, 1024], stage=4, block=\'a\')\n  x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'b\')\n  x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'c\')\n  x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'d\')\n  x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'e\')\n  x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'f\')\n\n  x = conv_block(x, 3, [512, 512, 2048], stage=5, block=\'a\')\n  x = identity_block(x, 3, [512, 512, 2048], stage=5, block=\'b\')\n  x = identity_block(x, 3, [512, 512, 2048], stage=5, block=\'c\')\n\n  x = layers.GlobalAveragePooling2D(name=\'avg_pool\')(x)\n  x = layers.Dense(\n      num_classes, activation=\'softmax\',\n      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n      bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n      name=\'fc1000\')(x)\n\n  # Create model.\n  return models.Model(img_input, x, name=\'resnet50\')\n'"
tpu/models/experimental/resnet50_keras/resnet_preprocessing.py,30,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""ImageNet preprocessing for ResNet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nIMAGE_SIZE = 224\nCROP_PADDING = 32\n\n\ndef distorted_bounding_box_crop(image_bytes,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image_bytes: `Tensor` of binary image data.\n    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n        where each coordinate is [0, 1) and the coordinates are arranged\n        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n        image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n        area of the image must contain at least this fraction of any bounding\n        box supplied.\n    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n        image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `float`s. The cropped area of the image\n        must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n        region of the image of the specified constraints. After `max_attempts`\n        failures, return the entire image.\n    scope: Optional `str` for name scope.\n  Returns:\n    (cropped image `Tensor`, distorted bbox `Tensor`).\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image_bytes, bbox]):\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        shape,\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n    target_height, target_width, _ = tf.unstack(bbox_size)\n    crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n\n    return image\n\n\ndef _at_least_x_are_equal(a, b, x):\n  """"""At least `x` of `a` and `b` `Tensors` are equal.""""""\n  match = tf.equal(a, b)\n  match = tf.cast(match, tf.int32)\n  return tf.greater_equal(tf.reduce_sum(match), x)\n\n\ndef _decode_and_random_crop(image_bytes):\n  """"""Make a random crop of IMAGE_SIZE.""""""\n  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n  image = distorted_bounding_box_crop(\n      image_bytes,\n      bbox,\n      min_object_covered=0.1,\n      aspect_ratio_range=(3. / 4, 4. / 3.),\n      area_range=(0.08, 1.0),\n      max_attempts=10,\n      scope=None)\n  original_shape = tf.image.extract_jpeg_shape(image_bytes)\n  bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)\n\n  image = tf.cond(\n      bad,\n      lambda: _decode_and_center_crop(image_bytes),\n      lambda: tf.image.resize_bicubic([image],  # pylint: disable=g-long-lambda\n                                      [IMAGE_SIZE, IMAGE_SIZE])[0])\n\n  return image\n\n\ndef _decode_and_center_crop(image_bytes):\n  """"""Crops to center of image with padding then scales IMAGE_SIZE.""""""\n  shape = tf.image.extract_jpeg_shape(image_bytes)\n  image_height = shape[0]\n  image_width = shape[1]\n\n  padded_center_crop_size = tf.cast(\n      ((IMAGE_SIZE / (IMAGE_SIZE + CROP_PADDING)) *\n       tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n      tf.int32)\n\n  offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n  offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n  crop_window = tf.stack([offset_height, offset_width,\n                          padded_center_crop_size, padded_center_crop_size])\n  image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n  image = tf.image.resize_bicubic([image], [IMAGE_SIZE, IMAGE_SIZE])[0]\n\n  return image\n\n\ndef _flip(image):\n  """"""Random horizontal image flip.""""""\n  image = tf.image.random_flip_left_right(image)\n  return image\n\n\ndef preprocess_for_train(image_bytes, use_bfloat16):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.\n    use_bfloat16: `bool` for whether to use bfloat16.\n\n  Returns:\n    A preprocessed image `Tensor`.\n  """"""\n  image = _decode_and_random_crop(image_bytes)\n  image = _flip(image)\n  image = tf.reshape(image, [IMAGE_SIZE, IMAGE_SIZE, 3])\n  image = tf.image.convert_image_dtype(\n      image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n  return image\n\n\ndef preprocess_for_eval(image_bytes, use_bfloat16):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.\n    use_bfloat16: `bool` for whether to use bfloat16.\n\n  Returns:\n    A preprocessed image `Tensor`.\n  """"""\n  image = _decode_and_center_crop(image_bytes)\n  image = tf.reshape(image, [IMAGE_SIZE, IMAGE_SIZE, 3])\n  image = tf.image.convert_image_dtype(\n      image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n  return image\n\n\ndef preprocess_image(image_bytes, is_training=False, use_bfloat16=False):\n  """"""Preprocesses the given image.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.\n    is_training: `bool` for whether the preprocessing is for training.\n    use_bfloat16: `bool` for whether to use bfloat16.\n\n  Returns:\n    A preprocessed image `Tensor`.\n  """"""\n  if is_training:\n    return preprocess_for_train(image_bytes, use_bfloat16)\n  else:\n    return preprocess_for_eval(image_bytes, use_bfloat16)\n'"
tpu/models/experimental/show_and_tell/configuration.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Image-to-text model and training configurations.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass ModelConfig(object):\n  """"""Wrapper class for model hyperparameters.""""""\n\n  def __init__(self):\n    """"""Sets the default model hyperparameters.""""""\n    # File pattern of sharded TFRecord file containing SequenceExample protos.\n    # Must be provided in training and evaluation modes.\n    self.input_file_pattern = None\n\n    # Image format (""jpeg"" or ""png"").\n    self.image_format = ""jpeg""\n\n    # Approximate number of values per input shard. Used to ensure sufficient\n    # mixing between shards in training.\n    self.values_per_input_shard = 2300\n    # Minimum number of shards to keep in the input queue.\n    self.input_queue_capacity_factor = 2\n    # Number of threads for prefetching SequenceExample protos.\n    self.num_input_reader_threads = 1\n\n    # Name of the context feature containing image data.\n    self.image_feature_name = ""image/encoded""\n    # Name of the feature containing string captions.\n    self.caption_feature_name = ""image/caption""\n\n    # Number of unique words in the vocab (plus 1, for <UNK>).\n    # The default value is larger than the expected actual vocab size to allow\n    # for differences between tokenizer versions used in preprocessing. There is\n    # no harm in using a value greater than the actual vocab size, but using a\n    # value less than the actual vocab size will result in an error.\n    self.vocab_size = 10002\n\n    # Number of threads for image preprocessing. Should be a multiple of 2.\n    self.num_preprocess_threads = 8\n\n    # Batch size.\n    self.batch_size = 32\n\n    # File containing an Inception v3 checkpoint to initialize the variables\n    # of the Inception model. Must be provided when starting training for the\n    # first time.\n    self.inception_checkpoint_file = None\n\n    # Dimensions of Inception v3 input images.\n    self.image_height = 299\n    self.image_width = 299\n\n    # Scale used to initialize model variables.\n    self.initializer_scale = 0.08\n\n    # LSTM input and output dimensionality, respectively.\n    self.embedding_size = 512\n    self.num_lstm_units = 512\n\n    # If < 1.0, the dropout keep probability applied to LSTM variables.\n    self.lstm_dropout_keep_prob = 0.7\n\n\nclass TrainingConfig(object):\n  """"""Wrapper class for training hyperparameters.""""""\n\n  def __init__(self):\n    """"""Sets the default training hyperparameters.""""""\n    # Number of examples per epoch of training data.\n    self.num_examples_per_epoch = 586363\n\n    # Optimizer for training the model.\n    self.optimizer = ""SGD""\n\n    # Learning rate for the initial phase of training.\n    self.initial_learning_rate = 2.0\n    self.learning_rate_decay_factor = 0.5\n    self.num_epochs_per_decay = 8.0\n\n    # Learning rate when fine tuning the Inception v3 parameters.\n    self.train_inception_learning_rate = 0.0005\n\n    # If not None, clip gradients to this value.\n    self.clip_gradients = 5.0\n\n    # How many model checkpoints to keep.\n    self.max_checkpoints_to_keep = 5\n'"
tpu/models/experimental/show_and_tell/image_embedding.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Image embedding ops.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3_base\n\nslim = tf.contrib.slim\n\n\n\ndef inception_v3(images,\n                 trainable=True,\n                 is_training=True,\n                 weight_decay=0.00004,\n                 stddev=0.1,\n                 dropout_keep_prob=0.8,\n                 use_batch_norm=True,\n                 batch_norm_params=None,\n                 add_summaries=True,\n                 scope=""InceptionV3""):\n  """"""Builds an Inception V3 subgraph for image embeddings.\n\n  Args:\n    images: A float32 Tensor of shape [batch, height, width, channels].\n    trainable: Whether the inception submodel should be trainable or not.\n    is_training: Boolean indicating training mode or not.\n    weight_decay: Coefficient for weight regularization.\n    stddev: The standard deviation of the trunctated normal weight initializer.\n    dropout_keep_prob: Dropout keep probability.\n    use_batch_norm: Whether to use batch normalization.\n    batch_norm_params: Parameters for batch normalization. See\n      tf.contrib.layers.batch_norm for details.\n    add_summaries: Whether to add activation summaries.\n    scope: Optional Variable scope.\n\n  Returns:\n    end_points: A dictionary of activations from inception_v3 layers.\n  """"""\n  # Only consider the inception model to be in training mode if it\'s trainable.\n  is_inception_model_training = trainable and is_training\n\n  if use_batch_norm:\n    # Default parameters for batch normalization.\n    if not batch_norm_params:\n      batch_norm_params = {\n          ""is_training"": is_inception_model_training,\n          ""trainable"": trainable,\n          # Decay for the moving averages.\n          ""decay"": 0.9997,\n          # Epsilon to prevent 0s in variance.\n          ""epsilon"": 0.001,\n          # Collection containing the moving mean and moving variance.\n          ""variables_collections"": {\n              ""beta"": None,\n              ""gamma"": None,\n              ""moving_mean"": [""moving_vars""],\n              ""moving_variance"": [""moving_vars""],\n          }\n      }\n  else:\n    batch_norm_params = None\n\n  if trainable:\n    weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n  else:\n    weights_regularizer = None\n\n  with tf.variable_scope(scope, ""InceptionV3"", [images]) as scope:\n    with slim.arg_scope(\n        [slim.conv2d, slim.fully_connected],\n        weights_regularizer=weights_regularizer,\n        trainable=trainable):\n      with slim.arg_scope(\n          [slim.conv2d],\n          weights_initializer=tf.truncated_normal_initializer(stddev=stddev),\n          activation_fn=tf.nn.relu,\n          normalizer_fn=slim.batch_norm,\n          normalizer_params=batch_norm_params):\n        net, end_points = inception_v3_base(\n            images, scope=scope)\n        with tf.variable_scope(""logits""):\n          shape = net.get_shape()\n          net = slim.avg_pool2d(net, shape[1:3], padding=""VALID"", scope=""pool"")\n          net = slim.dropout(\n              net,\n              keep_prob=dropout_keep_prob,\n              is_training=is_inception_model_training,\n              scope=""dropout"")\n          net = slim.flatten(net, scope=""flatten"")\n\n  # Add summaries.\n  if add_summaries:\n    for v in end_points.values():\n      tf.contrib.layers.summaries.summarize_activation(v)\n\n  return net\n'"
tpu/models/experimental/show_and_tell/image_processing.py,27,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Helper functions for image preprocessing.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# Standard Imports\nimport tensorflow as tf\n\n\ndef distort_image(images, seed):\n  """"""Perform random distortions on a batch of images.\n\n  Args:\n    images: A float32 Tensor of shape [batch_size, height, width, 3] with values in [0, 1).\n    seed: Tensor (scalar) for seeding the order of the random pertubations.\n\n  Returns:\n    distorted_image: A float32 Tensor of shape [height, width, 3] with values in\n      [0, 1].\n  """"""\n  color_ordering = tf.contrib.stateless.stateless_random_normal(\n      shape=[images.shape[0]],\n      seed=tf.cast(tf.stack([0, seed]), tf.int32),\n      dtype=tf.float32)\n\n  # random flip doesn\'t work on a batch, and running it inside of a map_fn\n  # triggers a memory error; skip it for now: we could alternatively move it\n  # to the CPU.\n  #\n  # image = tf.image.random_flip_left_right(image)\n\n  with tf.name_scope(""distort_color"", values=[images]):\n    def _a(image):\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.032)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      return image\n\n    def _b(image):\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.032)\n      return image\n\n    images = tf.where(\n        tf.less(color_ordering, 0),\n        _a(images),\n        _b(images)\n        # tf.map_fn(_a, images),\n        # tf.map_fn(_b, images)\n    )\n\n    # The random_* ops do not necessarily clamp.\n    images = tf.clip_by_value(images, 0.0, 1.0)\n\n  return images\n\n\ndef process_image(encoded_image,\n                  is_training,\n                  height,\n                  width,\n                  resize_height=346,\n                  resize_width=346,\n                  thread_id=0,\n                  image_format=""jpeg""):\n  """"""Decode an image, resize and apply random distortions.\n\n  In training, images are distorted slightly differently depending on thread_id.\n\n  Args:\n    encoded_image: String Tensor containing the image.\n    is_training: Boolean; whether preprocessing for training or eval.\n    height: Height of the output image.\n    width: Width of the output image.\n    resize_height: If > 0, resize height before crop to final dimensions.\n    resize_width: If > 0, resize width before crop to final dimensions.\n    thread_id: Preprocessing thread id used to select the ordering of color\n      distortions. There should be a multiple of 2 preprocessing threads.\n    image_format: ""jpeg"" or ""png"".\n\n  Returns:\n    A float32 Tensor of shape [height, width, 3] with values in [-1, 1].\n\n  Raises:\n    ValueError: If image_format is invalid.\n  """"""\n  # Helper function to log an image summary to the visualizer. Summaries are\n  # only logged in thread 0.\n  def image_summary(name, image):\n    if not thread_id:\n      tf.summary.image(name, tf.expand_dims(image, 0))\n\n  # Decode image into a float32 Tensor of shape [?, ?, 3] with values in [0, 1).\n  with tf.name_scope(""decode"", values=[encoded_image]):\n    if image_format == ""jpeg"":\n      image = tf.image.decode_jpeg(encoded_image, channels=3)\n    elif image_format == ""png"":\n      image = tf.image.decode_png(encoded_image, channels=3)\n    else:\n      raise ValueError(""Invalid image format: %s"" % image_format)\n  image_summary(""original_image"", image)\n\n  # Resize image.\n  assert (resize_height > 0) == (resize_width > 0)\n  if resize_height:\n    image = tf.image.resize_images(image,\n                                   size=[resize_height, resize_width],\n                                   method=tf.image.ResizeMethod.BILINEAR)\n\n  # Crop to final dimensions.\n  if is_training:\n    image = tf.random_crop(image, [height, width, 3])\n  else:\n    # Central crop, assuming resize_height > height, resize_width > width.\n    image = tf.image.resize_image_with_crop_or_pad(image, height, width)\n\n  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n  image_summary(""resized_image"", image)\n  return image\n'"
tpu/models/experimental/show_and_tell/inputs.py,32,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Input ops.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# Standard Imports\nimport tensorflow as tf\n\n\ndef parse_example(serialized, image_feature, caption_feature):\n  """"""Parses a tensorflow.SequenceExample into an image and caption.\n\n  Args:\n    serialized: A scalar string Tensor; a single serialized SequenceExample.\n    image_feature: Name of SequenceExample context feature containing image\n      data.\n    caption_feature: Name of SequenceExample feature list containing integer\n      captions.\n\n  Returns:\n    encoded_image: A scalar string Tensor containing a JPEG encoded image.\n    caption: A 1-D uint64 Tensor with dynamically specified length.\n  """"""\n  parsed = tf.parse_single_example(\n      serialized,\n      features={\n          image_feature: tf.FixedLenFeature([], dtype=tf.string),\n          caption_feature: tf.VarLenFeature(dtype=tf.string),\n      })\n\n  encoded_image = parsed[image_feature]\n  caption = parsed[caption_feature]\n\n  # Just take the first caption\n  caption = tf.sparse_tensor_to_dense(caption, default_value=\'\')[0]\n  return encoded_image, caption\n\n\ndef prefetch_input_data(reader,\n                        file_pattern,\n                        is_training,\n                        batch_size,\n                        values_per_shard,\n                        input_queue_capacity_factor=16,\n                        num_reader_threads=1,\n                        shard_queue_name=""filename_queue"",\n                        value_queue_name=""input_queue""):\n  """"""Prefetches string values from disk into an input queue.\n\n  In training the capacity of the queue is important because a larger queue\n  means better mixing of training examples between shards. The minimum number of\n  values kept in the queue is values_per_shard * input_queue_capacity_factor,\n  where input_queue_memory factor should be chosen to trade-off better mixing\n  with memory usage.\n\n  Args:\n    reader: Instance of tf.ReaderBase.\n    file_pattern: Comma-separated list of file patterns (e.g.\n        /tmp/train_data-?????-of-00100).\n    is_training: Boolean; whether prefetching for training or eval.\n    batch_size: Model batch size used to determine queue capacity.\n    values_per_shard: Approximate number of values per shard.\n    input_queue_capacity_factor: Minimum number of values to keep in the queue\n      in multiples of values_per_shard. See comments above.\n    num_reader_threads: Number of reader threads to fill the queue.\n    shard_queue_name: Name for the shards filename queue.\n    value_queue_name: Name for the values input queue.\n\n  Returns:\n    A Queue containing prefetched string values.\n  """"""\n  data_files = []\n  for pattern in file_pattern.split("",""):\n    data_files.extend(tf.gfile.Glob(pattern))\n  if not data_files:\n    tf.logging.fatal(""Found no input files matching %s"", file_pattern)\n  else:\n    tf.logging.info(""Prefetching values from %d files matching %s"",\n                    len(data_files), file_pattern)\n\n  if is_training:\n    filename_queue = tf.train.string_input_producer(\n        data_files, shuffle=True, capacity=16, name=shard_queue_name)\n    min_queue_examples = values_per_shard * input_queue_capacity_factor\n    capacity = min_queue_examples + 100 * batch_size\n    values_queue = tf.RandomShuffleQueue(\n        capacity=capacity,\n        min_after_dequeue=min_queue_examples,\n        dtypes=[tf.string],\n        name=""random_"" + value_queue_name)\n  else:\n    filename_queue = tf.train.string_input_producer(\n        data_files, shuffle=False, capacity=1, name=shard_queue_name)\n    capacity = values_per_shard + 3 * batch_size\n    values_queue = tf.FIFOQueue(\n        capacity=capacity, dtypes=[tf.string], name=""fifo_"" + value_queue_name)\n\n  enqueue_ops = []\n  for _ in range(num_reader_threads):\n    _, value = reader.read(filename_queue)\n    enqueue_ops.append(values_queue.enqueue([value]))\n  tf.train.queue_runner.add_queue_runner(\n      tf.train.queue_runner.QueueRunner(values_queue, enqueue_ops))\n  tf.summary.scalar(\n      ""queue/%s/fraction_of_%d_full"" % (values_queue.name, capacity),\n      tf.cast(values_queue.size(), tf.float32) * (1. / capacity))\n\n  return values_queue\n\ndef pad_caption_to_input(caption, max_caption_len=64):\n  # clip long captions\n  caption = caption[0:max_caption_len]\n  caption = tf.cast(caption, tf.int32)\n  caption = tf.where(tf.equal(caption, -1),\n                     tf.zeros_like(caption),\n                     caption)\n\n  # pad short captions up\n  caption_len = tf.shape(caption)[0]\n  caption = tf.pad(caption,\n                   [(0, tf.maximum(max_caption_len - caption_len, 0))])\n\n  input_seq = caption[0:-1]\n  target_seq = caption[1:]\n\n  indicator = tf.pad(\n      tf.ones(caption_len - 1),\n      [(0, tf.maximum(max_caption_len - caption_len, 0))]\n  )\n\n  input_seq.set_shape(max_caption_len - 1)\n  target_seq.set_shape(max_caption_len - 1)\n  indicator.set_shape(max_caption_len - 1)\n  return input_seq, target_seq, indicator\n\ndef batch_with_dynamic_pad(images_and_captions,\n                           batch_size,\n                           queue_capacity,\n                           add_summaries=True):\n  """"""Batches input images and captions.\n\n  This function splits the caption into an input sequence and a target sequence,\n  where the target sequence is the input sequence right-shifted by 1. Input and\n  target sequences are batched and padded up to the maximum length of sequences\n  in the batch. A mask is created to distinguish real words from padding words.\n\n  Example:\n    Actual captions in the batch (\'-\' denotes padded character):\n      [\n        [ 1 2 5 4 5 ],\n        [ 1 2 3 4 - ],\n        [ 1 2 3 - - ],\n      ]\n\n    input_seqs:\n      [\n        [ 1 2 3 4 ],\n        [ 1 2 3 - ],\n        [ 1 2 - - ],\n      ]\n\n    target_seqs:\n      [\n        [ 2 3 4 5 ],\n        [ 2 3 4 - ],\n        [ 2 3 - - ],\n      ]\n\n    mask:\n      [\n        [ 1 1 1 1 ],\n        [ 1 1 1 0 ],\n        [ 1 1 0 0 ],\n      ]\n\n  Args:\n    images_and_captions: A list of pairs [image, caption], where image is a\n      Tensor of shape [height, width, channels] and caption is a 1-D Tensor of\n      any length. Each pair will be processed and added to the queue in a\n      separate thread.\n    batch_size: Batch size.\n    queue_capacity: Queue capacity.\n    add_summaries: If true, add caption length summaries.\n\n  Returns:\n    images: A Tensor of shape [batch_size, height, width, channels].\n    input_seqs: An int32 Tensor of shape [batch_size, padded_length].\n    target_seqs: An int32 Tensor of shape [batch_size, padded_length].\n    mask: An int32 0/1 Tensor of shape [batch_size, padded_length].\n  """"""\n  enqueue_list = []\n  for image, caption in images_and_captions:\n    enqueue_list.append([image, input_seq, target_seq, indicator])\n\n  images, input_seqs, target_seqs, mask = tf.train.batch_join(\n      enqueue_list,\n      batch_size=batch_size,\n      capacity=queue_capacity,\n      dynamic_pad=True,\n      name=""batch_and_pad"")\n\n  if add_summaries:\n    lengths = tf.add(tf.reduce_sum(mask, 1), 1)\n    tf.summary.scalar(""caption_length/batch_min"", tf.reduce_min(lengths))\n    tf.summary.scalar(""caption_length/batch_max"", tf.reduce_max(lengths))\n    tf.summary.scalar(""caption_length/batch_mean"", tf.reduce_mean(lengths))\n\n  return images, input_seqs, target_seqs, mask\n'"
tpu/models/experimental/show_and_tell/show_and_tell_model.py,52,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Image-to-text implementation based on http://arxiv.org/abs/1411.4555.\n\n""Show and Tell: A Neural Image Caption Generator""\nOriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# Standard Imports\nimport tensorflow as tf\n\nimport image_embedding\nimport image_processing\nimport inputs as input_ops\n\n\nclass ShowAndTellModel(object):\n  """"""Image-to-text implementation based on http://arxiv.org/abs/1411.4555.\n\n  ""Show and Tell: A Neural Image Caption Generator""\n  Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan\n  """"""\n\n  def __init__(self, config, mode, train_inception=False):\n    """"""Basic setup.\n\n    Args:\n      config: Object containing configuration parameters.\n      mode: ""train"", ""eval"" or ""inference"".\n      train_inception: Whether the inception submodel variables are trainable.\n    """"""\n    assert mode in [""train"", ""eval"", ""inference""]\n    self.config = config\n    self.mode = mode\n    self.train_inception = train_inception\n\n    # To match the ""Show and Tell"" paper we initialize all variables with a\n    # random uniform initializer.\n    self.initializer = tf.random_uniform_initializer(\n        minval=-self.config.initializer_scale,\n        maxval=self.config.initializer_scale)\n\n    # A float32 Tensor with shape [batch_size, height, width, channels].\n    self.images = None\n\n    # An int32 Tensor with shape [batch_size, padded_length].\n    self.input_seqs = None\n\n    # An int32 Tensor with shape [batch_size, padded_length].\n    self.target_seqs = None\n\n    # An int32 0/1 Tensor with shape [batch_size, padded_length].\n    self.input_mask = None\n\n    # A float32 Tensor with shape [batch_size, embedding_size].\n    self.image_embeddings = None\n\n    # A float32 Tensor with shape [batch_size, padded_length, embedding_size].\n    self.seq_embeddings = None\n\n    # A float32 scalar Tensor; the total loss for the trainer to optimize.\n    self.total_loss = None\n\n    # A float32 Tensor with shape [batch_size * padded_length].\n    self.target_cross_entropy_losses = None\n\n    # A float32 Tensor with shape [batch_size * padded_length].\n    self.target_cross_entropy_loss_weights = None\n\n    # Collection of variables from the inception submodel.\n    self.inception_variables = []\n\n    # Function to restore the inception submodel from checkpoint.\n    self.init_fn = None\n\n    # Global step Tensor.\n    self.global_step = None\n\n  def is_training(self):\n    """"""Returns true if the model is built for training mode.""""""\n    return self.mode == ""train""\n\n  def load_image(self, encoded_image, thread_id=0):\n    """"""Decodes and processes an image string.\n\n    Args:\n      encoded_image: A scalar string Tensor; the encoded image.\n      thread_id: Preprocessing thread id used to select the ordering of color\n        distortions.\n\n    Returns:\n      A float32 Tensor of shape [height, width, 3]; the processed image.\n    """"""\n    return image_processing.process_image(\n        encoded_image,\n        is_training=self.is_training(),\n        height=self.config.image_height,\n        width=self.config.image_width,\n        thread_id=thread_id,\n        image_format=self.config.image_format)\n\n  def distort_images(self, images, seed):\n    """"""Distort a batch of images.\n\n    (Processing a batch allows us to easily switch between TPU and CPU\n    execution).\n    """"""\n    if self.mode == ""train"":\n      images = image_processing.distort_image(images, seed)\n\n    # Rescale to [-1,1] instead of [0, 1]\n    images = tf.subtract(images, 0.5)\n    images = tf.multiply(images, 2.0)\n    return images\n\n  def build_inputs(self):\n    """"""Input prefetching, preprocessing and batching.\n\n    Outputs:\n      self.images\n      self.input_seqs\n      self.target_seqs (training and eval only)\n      self.input_mask (training and eval only)\n    """"""\n    if self.mode == ""inference"":\n      # In inference mode, images and inputs are fed via placeholders.\n      image_feed = tf.placeholder(dtype=tf.string, shape=[], name=""image_feed"")\n      input_feed = tf.placeholder(\n          dtype=tf.int64,\n          shape=[None],  # batch_size\n          name=""input_feed"")\n\n      # Process image and insert batch dimensions.\n      images = tf.expand_dims(self.load_image(image_feed), 0)\n      input_seqs = tf.expand_dims(input_feed, 1)\n\n      # No target sequences or input mask in inference mode.\n      target_seqs = None\n      input_mask = None\n    else:\n\n      def _load_example(serialized_example):\n        encoded_image, caption = input_ops.parse_example(\n            serialized_example,\n            image_feature=self.config.image_feature_name,\n            caption_feature=self.config.caption_feature_name)\n        image = self.load_image(encoded_image)\n        # strings.split expects a batch\n        words = tf.strings.split(tf.reshape(caption, [1]), sep="" "")\n        words = tf.sparse_tensor_to_dense(words, default_value="""")[0]\n        word_idx = tf.strings.to_hash_bucket(words, self.config.vocab_size)\n        input_seqs, target_seqs, input_mask = input_ops.pad_caption_to_input(\n            word_idx)\n        return image, input_seqs, target_seqs, input_mask\n\n      def _load_dataset(filename):\n        return tf.data.TFRecordDataset(filename, buffer_size=16 * 1024 * 1024)\n\n      df = tf.data.Dataset.list_files(\n          self.config.input_file_pattern, shuffle=self.mode == ""train"")\n      df = df.apply(\n          tf.data.experimental.parallel_interleave(\n              _load_dataset, cycle_length=64, sloppy=True))\n\n      if self.mode == ""train"":\n        df = df.repeat()\n        df = df.shuffle(1024)\n\n      df = df.apply(\n          tf.data.experimental.map_and_batch(\n              _load_example,\n              self.config.batch_size,\n              num_parallel_batches=8,\n              drop_remainder=True))\n      df = df.prefetch(8)\n      images, input_seqs, target_seqs, input_mask = df.make_one_shot_iterator(\n      ).get_next()\n\n    self.images = images\n    self.input_seqs = input_seqs\n    self.target_seqs = target_seqs\n    self.input_mask = input_mask\n\n  def build_image_embeddings(self, images):\n    """"""Builds the image model subgraph and generates image embeddings.\n\n    Inputs:\n      images\n\n    Outputs:\n      self.image_embeddings\n    """"""\n    images = self.distort_images(images, tf.train.get_or_create_global_step())\n    inception_output = image_embedding.inception_v3(\n        images,\n        trainable=self.train_inception,\n        is_training=self.is_training(),\n        add_summaries=False)\n\n    self.inception_variables = tf.get_collection(\n        tf.GraphKeys.GLOBAL_VARIABLES, scope=""InceptionV3"")\n\n    # Map inception output into embedding space.\n    with tf.variable_scope(""image_embedding"") as scope:\n      image_embeddings = tf.contrib.layers.fully_connected(\n          inputs=inception_output,\n          num_outputs=self.config.embedding_size,\n          activation_fn=None,\n          weights_initializer=self.initializer,\n          biases_initializer=None,\n          scope=scope)\n\n    # Save the embedding size in the graph.\n    tf.constant(self.config.embedding_size, name=""embedding_size"")\n\n    return image_embeddings\n\n  def build_seq_embeddings(self, input_seqs):\n    """"""Builds the input sequence embeddings.\n\n    Inputs:\n      input_seqs\n\n    Outputs:\n      self.seq_embeddings\n    """"""\n    with tf.variable_scope(""seq_embedding""), tf.device(""/cpu:0""):\n      embedding_map = tf.get_variable(\n          name=""map"",\n          shape=[self.config.vocab_size, self.config.embedding_size],\n          initializer=self.initializer)\n      seq_embeddings = tf.nn.embedding_lookup(embedding_map, input_seqs)\n\n    return seq_embeddings\n\n  def build_model(self):\n    """"""Builds the model.\n\n    Inputs:\n      self.image_embeddings\n      self.seq_embeddings\n      self.target_seqs (training and eval only)\n      self.input_mask (training and eval only)\n\n    Outputs:\n      self.total_loss (training and eval only)\n      self.target_cross_entropy_losses (training and eval only)\n      self.target_cross_entropy_loss_weights (training and eval only)\n    """"""\n    # This LSTM cell has biases and outputs tanh(new_c) * sigmoid(o), but the\n    # modified LSTM in the ""Show and Tell"" paper has no biases and outputs\n    # new_c * sigmoid(o).\n    lstm_cell = tf.contrib.rnn.BasicLSTMCell(\n        num_units=self.config.num_lstm_units, state_is_tuple=True)\n    if self.mode == ""train"":\n      lstm_cell = tf.contrib.rnn.DropoutWrapper(\n          lstm_cell,\n          input_keep_prob=self.config.lstm_dropout_keep_prob,\n          output_keep_prob=self.config.lstm_dropout_keep_prob)\n\n    with tf.variable_scope(""lstm"", initializer=self.initializer) as lstm_scope:\n      # Feed the image embeddings to set the initial LSTM state.\n      zero_state = lstm_cell.zero_state(\n          batch_size=self.image_embeddings.get_shape()[0], dtype=tf.float32)\n      _, initial_state = lstm_cell(self.image_embeddings, zero_state)\n\n      # Allow the LSTM variables to be reused.\n      lstm_scope.reuse_variables()\n\n      if self.mode == ""inference"":\n        # In inference mode, use concatenated states for convenient feeding and\n        # fetching.\n        tf.concat(initial_state, 1, name=""initial_state"")\n\n        # Placeholder for feeding a batch of concatenated states.\n        state_feed = tf.placeholder(\n            dtype=tf.float32,\n            shape=[None, sum(lstm_cell.state_size)],\n            name=""state_feed"")\n        state_tuple = tf.split(value=state_feed, num_or_size_splits=2, axis=1)\n\n        # Run a single LSTM step.\n        lstm_outputs, state_tuple = lstm_cell(\n            inputs=tf.squeeze(self.seq_embeddings, squeeze_dims=[1]),\n            state=state_tuple)\n\n        # Concatentate the resulting state.\n        tf.concat(state_tuple, 1, name=""state"")\n      else:\n        # Run the batch of sequence embeddings through the LSTM.\n        sequence_length = tf.reduce_sum(self.input_mask, 1)\n        lstm_outputs, _ = tf.nn.dynamic_rnn(\n            cell=lstm_cell,\n            inputs=self.seq_embeddings,\n            sequence_length=sequence_length,\n            initial_state=initial_state,\n            dtype=tf.float32,\n            scope=lstm_scope)\n\n    # Stack batches vertically.\n    lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\n\n    with tf.variable_scope(""logits"") as logits_scope:\n      logits = tf.contrib.layers.fully_connected(\n          inputs=lstm_outputs,\n          num_outputs=self.config.vocab_size,\n          activation_fn=None,\n          weights_initializer=self.initializer,\n          scope=logits_scope)\n\n    if self.mode == ""inference"":\n      tf.nn.softmax(logits, name=""softmax"")\n    else:\n      targets = tf.reshape(self.target_seqs, [-1])\n      weights = tf.to_float(tf.reshape(self.input_mask, [-1]))\n\n      # Compute losses.\n      losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n          labels=targets, logits=logits)\n      batch_loss = tf.div(\n          tf.reduce_sum(tf.multiply(losses, weights)),\n          tf.reduce_sum(weights),\n          name=""batch_loss"")\n      tf.losses.add_loss(batch_loss)\n      total_loss = tf.losses.get_total_loss()\n\n      self.total_loss = total_loss\n      self.target_cross_entropy_losses = losses  # Used in evaluation.\n      self.target_cross_entropy_loss_weights = weights  # Used in evaluation.\n\n  def setup_inception_initializer(self):\n    """"""Sets up the function to restore inception variables from checkpoint.""""""\n    if self.mode != ""inference"":\n      # Restore inception variables only.\n      saver = tf.train.Saver(self.inception_variables)\n\n      def restore_fn(sess):\n        tf.logging.info(""Restoring Inception variables from checkpoint file %s"",\n                        self.config.inception_checkpoint_file)\n        saver.restore(sess, self.config.inception_checkpoint_file)\n\n      self.init_fn = restore_fn\n\n  def setup_global_step(self):\n    """"""Sets up the global step Tensor.""""""\n    self.global_step = tf.train.get_or_create_global_step()\n\n  def build_model_for_tpu(self, images, input_seqs, target_seqs, input_mask):\n    self.image_embeddings = self.build_image_embeddings(images)\n    self.seq_embeddings = self.build_seq_embeddings(target_seqs)\n    self.target_seqs = target_seqs\n    self.input_mask = input_mask\n    self.build_model()\n\n  def build(self):\n    """"""Creates all ops for training and evaluation.""""""\n    self.build_inputs()\n    self.image_embeddings = self.build_image_embeddings(self.images)\n    self.seq_embeddings = self.build_seq_embeddings(self.input_seqs)\n    self.build_model()\n    self.setup_inception_initializer()\n    self.setup_global_step()\n'"
tpu/models/experimental/show_and_tell/show_and_tell_tpu_test.py,14,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Test Keras TPU interface support.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\n\n# Standard Imports\nimport numpy as np\nimport tensorflow.google as tf\n\nimport configuration\nimport show_and_tell_model\nimport train\n\ntpu = tf.contrib.tpu\n\nINPUT_FILE_PATTERN = \'gs://your-gs-bucket/train-00000*\'\n\n@contextlib.contextmanager\ndef _reset_for_test():\n  tf.reset_default_graph()\n  yield tf.Session(\'\')\n\n\nclass ShowAndTellTPUTest(tf.test.TestCase):\n\n  def testCallModelFnWithPlaceholders(self):\n    with _reset_for_test() as session:\n      config = configuration.ModelConfig()\n      model = show_and_tell_model.ShowAndTellModel(config, mode=\'train\')\n\n      def model_fn(images, input_seq, target_seq, input_mask):\n        model.build_model_for_tpu(images, input_seq, target_seq, input_mask)\n        return model.total_loss\n\n      images = tf.placeholder(tf.float32, shape=(1, 224, 224, 3))\n      input_seq = tf.placeholder(tf.int32, shape=(1, 128))\n      target_seq = tf.placeholder(tf.int32, shape=(1, 128))\n      input_mask = tf.placeholder(tf.int32, shape=(1, 128))\n\n      tpu_model_fn = tpu.rewrite(model_fn,\n                                 [images, input_seq, target_seq, input_mask])\n      caption = np.random.randint(low=0, high=1000, size=128).reshape((1, 128))\n      session.run(tpu.initialize_system())\n      session.run(tf.global_variables_initializer())\n      inputs = {\n          images: np.random.randn(1, 224, 224, 3),\n          input_seq: caption,\n          target_seq: caption,\n          input_mask: np.random.random_integers(0, 1, size=128).reshape(1, 128),\n      }\n      session.run(tpu_model_fn, inputs)\n      session.run(tpu.shutdown_system())\n\n  def testCallInputFn(self):\n    with _reset_for_test() as session:\n      outputs = train.input_fn({\n        \'batch_size\': 4,\n        \'input_file_pattern\': INPUT_FILE_PATTERN,\n        \'mode\': \'train\',\n      })\n      tf.logging.info(\'Outputs: %s\', outputs)\n      self.assertEqual(outputs[\'images\'].shape.as_list(), [4, 299, 299, 3])\n      self.assertEqual(outputs[\'input_mask\'].shape[0], 4)\n      for k, v in outputs.items():\n        tf.logging.info(\'Output %s\', k)\n        tf.logging.info(session.run(v).shape)\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.test.main()\n'"
tpu/models/experimental/show_and_tell/train.py,34,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Train the model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\n# Standard Imports\nfrom absl import app\nimport tensorflow as tf\n\nimport configuration\nimport show_and_tell_model\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.flags.DEFINE_string(\n    ""tpu"", default=None,\n    help=""The Cloud TPU to use for training. This should be either the name ""\n         ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 ""\n         ""url."")\ntf.flags.DEFINE_string(\n    ""gcp_project"", default=None,\n    help=""Project name for the Cloud TPU-enabled project. If not specified, we ""\n         ""will attempt to automatically detect the GCE project from metadata."")\ntf.flags.DEFINE_string(\n    ""tpu_zone"", default=None,\n    help=""GCE zone where the Cloud TPU is located in. If not specified, we ""\n         ""will attempt to automatically detect the GCE project from metadata."")\ntf.flags.DEFINE_bool(""use_tpu"", True, ""If true, use TPU"")\ntf.flags.DEFINE_string(""mode"", ""train"",\n                       ""Execution mode: one of train|evaluate ."")\ntf.flags.DEFINE_string(""input_file_pattern"", """",\n                       ""File pattern of sharded TFRecord input files."")\ntf.flags.DEFINE_string(""inception_checkpoint_file"", """",\n                       ""Path to a pretrained inception_v3 model."")\ntf.flags.DEFINE_string(""model_dir"", """",\n                       ""Directory for saving and loading model checkpoints."")\ntf.flags.DEFINE_boolean(""train_inception"", False,\n                        ""Whether to train inception submodel variables."")\ntf.flags.DEFINE_integer(""train_steps"", 10000, ""Number of batches for training."")\ntf.flags.DEFINE_integer(""train_batch_size"", 1024, ""Batch size for training."")\ntf.flags.DEFINE_integer(""eval_batch_size"", 1024, ""Batch size for evaluation."")\ntf.flags.DEFINE_integer(""iterations_per_loop"", 100,\n                        ""TPU batch iterations per loop."")\n\nMODEKEY_TO_MODE = {\n    tf.estimator.ModeKeys.PREDICT: ""inference"",\n    tf.estimator.ModeKeys.EVAL: ""evaluate"",\n    tf.estimator.ModeKeys.TRAIN: ""train"",\n}\n\n\ndef model_fn(features, labels, mode, params):\n  im_mode = MODEKEY_TO_MODE[mode]\n  model_config = configuration.ModelConfig()\n  training_config = configuration.TrainingConfig()\n  model = show_and_tell_model.ShowAndTellModel(\n      model_config, mode=im_mode, train_inception=FLAGS.train_inception)\n  model.build_model_for_tpu(\n      images=features[""images""],\n      input_seqs=features[""input_seqs""],\n      target_seqs=features[""target_seqs""],\n      input_mask=features[""input_mask""])\n\n  optimizer = tf.train.GradientDescentOptimizer(\n      learning_rate=training_config.initial_learning_rate)\n  optimizer = tf.contrib.estimator.clip_gradients_by_norm(\n      optimizer, training_config.clip_gradients)\n  if FLAGS.use_tpu:\n    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n  train_op = optimizer.minimize(\n      model.total_loss, global_step=tf.train.get_or_create_global_step())\n\n  def scaffold_fn():\n    """"""Load pretrained Inception checkpoint at initialization time.""""""\n    return tf.train.Scaffold(init_fn=model.init_fn)\n\n  return tf.contrib.tpu.TPUEstimatorSpec(\n      mode=mode,\n      loss=model.total_loss,\n      train_op=train_op,\n      scaffold_fn=scaffold_fn)\n\n\ndef input_fn(params):\n  model_config = configuration.ModelConfig()\n  model_config.input_file_pattern = params[""input_file_pattern""]\n  model_config.batch_size = params[""batch_size""]\n  model_config.mode = params[""mode""]\n  model = show_and_tell_model.ShowAndTellModel(model_config, mode=""train"")\n  model.build_inputs()\n  return {\n      ""images"": model.images,\n      ""input_seqs"": model.input_seqs,\n      ""target_seqs"": model.target_seqs,\n      ""input_mask"": model.input_mask\n  }\n\n\ndef main(unused_argv):\n  assert FLAGS.input_file_pattern, ""--input_file_pattern is required""\n  assert FLAGS.model_dir, ""--model_dir is required""\n\n  if FLAGS.use_tpu:\n    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu,\n        zone=FLAGS.tpu_zone,\n        project=FLAGS.gcp_project)\n    tpu_grpc_url = tpu_cluster_resolver.get_master()\n  else:\n    tpu_grpc_url = \'\'\n\n  run_config = tf.contrib.tpu.RunConfig(\n      master=tpu_grpc_url,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_steps=1000,\n      keep_checkpoint_max=None,\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=FLAGS.iterations_per_loop,))\n\n  estimator = tf.contrib.tpu.TPUEstimator(\n      use_tpu=FLAGS.use_tpu,\n      model_fn=model_fn,\n      config=run_config,\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=FLAGS.eval_batch_size,\n      params={\n          ""input_file_pattern"": FLAGS.input_file_pattern,\n          ""use_tpu"": FLAGS.use_tpu,\n          ""mode"": FLAGS.mode,\n      })\n\n  training_config = configuration.TrainingConfig()\n\n  if FLAGS.mode == ""train"":\n    estimator.train(\n        input_fn=input_fn,\n        max_steps=FLAGS.train_steps,\n    )\n  else:\n    # Run evaluation when there""s a new checkpoint\n    for ckpt in tf.contrib.training.checkpoints_iterator(FLAGS.model_dir):\n      tf.logging.info(""Starting to evaluate."")\n      try:\n        eval_results = estimator.evaluate(\n            input_fn=input_fn,\n            steps=(\n                training_config.num_examples_per_epoch // FLAGS.eval_batch_size\n            ),\n            checkpoint_path=ckpt)\n        tf.logging.info(""Eval results: %s"", eval_results)\n\n        current_step = int(os.path.basename(ckpt).split(""-"")[1])\n        if current_step >= FLAGS.train_steps:\n          tf.logging.info(\n              ""Evaluation finished after training step %d"" % current_step)\n          break\n\n      except tf.errors.NotFoundError:\n        tf.logging.info(\n            ""Checkpoint %s no longer exists, skipping checkpoint"" % ckpt)\n\n\nif __name__ == ""__main__"":\n  tf.logging.set_verbosity(tf.logging.INFO)\n  app.run(main)\n'"
tpu/models/official/amoeba_net/amoeba_net.py,27,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# pylint: disable=line-too-long\nr""""""TensorFlow AmoebaNet Example.\n\nGCP Run Example\npython amoeba_net.py --data_dir=gs://cloud-tpu-datasets/imagenet-data --model_dir=gs://cloud-tpu-ckpts/models/ameoba_net_x/ \\\n--drop_connect_keep_prob=1.0 --cell_name=evol_net_x --num_cells=12 --reduction_size=256 --image_size=299 --num_epochs=48 \\\n--train_batch_size=256 --num_epochs_per_eval=4.0 --lr_decay_value=0.89 --lr_num_epochs_per_decay=1 --alsologtostderr \\\n--tpu=huangyp-tpu-0\n""""""\n# pylint: enable=line-too-long\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nimport math\nfrom absl import flags\nimport absl.logging as _logging  # pylint: disable=unused-import\nimport tensorflow as tf\n\nimport amoeba_net_model as model_lib\nimport model_specs\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'tpu\', default=None,\n    help=\'The Cloud TPU to use for training. This should be either the name \'\n    \'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\')\n\nflags.DEFINE_string(\n    \'gcp_project\', default=None,\n    help=\'Project name for the Cloud TPU-enabled project. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\n\nflags.DEFINE_string(\n    \'tpu_zone\', default=None,\n    help=\'GCE zone where the Cloud TPU is located in. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\n\n# General Parameters\nflags.DEFINE_integer(\n    \'num_shards\', 8,\n    \'Number of shards (TPU cores).\')\n\nflags.DEFINE_integer(\n    \'distributed_group_size\', 1,\n    help=\'Size of the distributed batch norm. group.\'\n    \'Default is normalization over local examples only.\'\n    \'When set to a value greater than 1, it will enable\'\n    \'a distribtued batch norm. To enable a global batch norm.\'\n    \'set distributed_group_size to FLAGS.num_shards\')\n\nflags.DEFINE_bool(\n    \'use_tpu\', True,\n    \'Use TPUs rather than CPU or GPU.\')\n\nflags.DEFINE_string(\n    \'data_dir\', \'\',\n    \'Directory where input data is stored\')\n\nflags.DEFINE_string(\n    \'model_dir\', None,\n    \'Directory where model output is stored\')\n\nflags.DEFINE_integer(\n    \'iterations_per_loop\', 500,\n    \'Number of iterations per TPU training loop.\')\n\nflags.DEFINE_integer(\n    \'train_batch_size\', 256,\n    \'Global (not per-shard) batch size for training\')\n\nflags.DEFINE_integer(\n    \'eval_batch_size\', 256,\n    \'Global (not per-shard) batch size for evaluation\')\n\nflags.DEFINE_float(\n    \'num_epochs\', 48.,\n    \'Number of steps use for training.\')\n\nflags.DEFINE_float(\n    \'num_epochs_per_eval\', 1.,\n    \'Number of training epochs to run between evaluations.\')\n\nflags.DEFINE_string(\n    \'mode\', \'train_and_eval\',\n    \'Mode to run: train, eval, train_and_eval\')\n\nflags.DEFINE_integer(\n    \'save_checkpoints_steps\', None,\n    \'Interval (in steps) at which the model data \'\n    \'should be checkpointed. Set to 0 to disable.\')\n\nflags.DEFINE_bool(\n    \'enable_hostcall\', True,\n    \'Skip the host_call which is executed every training step. This is\'\n    \' generally used for generating training summaries (train loss,\'\n    \' learning rate, etc...). When --enable_hostcall=True, there could\'\n    \' be a performance drop if host_call function is slow and cannot\'\n    \' keep up with the TPU-side computation.\')\n\n# Model specific parameters\nflags.DEFINE_bool(\'use_aux_head\', True, \'Include aux head or not.\')\nflags.DEFINE_float(\n    \'aux_scaling\', 0.4, \'Scaling factor of aux_head\')\nflags.DEFINE_float(\n    \'batch_norm_decay\', 0.9, \'Batch norm decay.\')\nflags.DEFINE_float(\n    \'batch_norm_epsilon\', 1e-5, \'Batch norm epsilon.\')\nflags.DEFINE_float(\n    \'dense_dropout_keep_prob\', None, \'Dense dropout keep probability.\')\nflags.DEFINE_float(\n    \'drop_connect_keep_prob\', 1.0, \'Drop connect keep probability.\')\nflags.DEFINE_string(\n    \'drop_connect_version\', None, \'Drop connect version.\')\nflags.DEFINE_string(\n    \'cell_name\', \'amoeba_net_d\', \'Which network to run.\')\nflags.DEFINE_integer(\n    \'num_cells\', 12, \'Total number of cells.\')\nflags.DEFINE_integer(\n    \'reduction_size\', 256, \'Default cell reduction size.\')\nflags.DEFINE_integer(\n    \'stem_reduction_size\', 32, \'Stem filter size.\')\nflags.DEFINE_float(\n    \'weight_decay\', 4e-05, \'Weight decay for slim model.\')\n\n# Training hyper-parameters\nflags.DEFINE_float(\n    \'lr\', 0.64, \'Learning rate.\')\nflags.DEFINE_string(\n    \'optimizer\', \'rmsprop\',\n    \'Optimizer (one of sgd, rmsprop, momentum)\')\nflags.DEFINE_float(\n    \'moving_average_decay\', 0.9999,\n    \'moving average decay rate\')\nflags.DEFINE_float(\n    \'lr_decay_value\', 0.9,\n    \'Exponential decay rate used in learning rate adjustment\')\nflags.DEFINE_integer(\n    \'lr_num_epochs_per_decay\', 1,\n    \'Exponential decay epochs used in learning rate adjustment\')\nflags.DEFINE_string(\n    \'lr_decay_method\', \'exponential\',\n    \'Method of decay: exponential, cosine, constant, stepwise\')\nflags.DEFINE_float(\n    \'lr_warmup_epochs\', 3.0,\n    \'Learning rate increased from zero linearly to lr for the first \'\n    \'lr_warmup_epochs.\')\nflags.DEFINE_float(\'gradient_clipping_by_global_norm\', 0,\n                   \'gradient_clipping_by_global_norm\')\n\nflags.DEFINE_integer(\n    \'image_size\', 299, \'Size of image, assuming image height and width.\')\n\nflags.DEFINE_bool(\n    \'use_bp16\', True, \'If True, use bfloat16 for activations\')\n\nflags.DEFINE_integer(\n    \'eval_timeout\', 60*60*24,\n    \'Maximum seconds between checkpoints before evaluation terminates.\')\n\nFLAGS = flags.FLAGS\n\n\ndef build_run_config():\n  """"""Return RunConfig for TPU estimator.""""""\n  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n      FLAGS.tpu,\n      zone=FLAGS.tpu_zone,\n      project=FLAGS.gcp_project)\n\n  eval_steps = model_lib.NUM_EVAL_IMAGES // FLAGS.eval_batch_size\n  iterations_per_loop = (eval_steps if FLAGS.mode == \'eval\'\n                         else FLAGS.iterations_per_loop)\n  save_checkpoints_steps = FLAGS.save_checkpoints_steps or iterations_per_loop\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_steps=save_checkpoints_steps,\n      keep_checkpoint_max=None,\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=iterations_per_loop,\n          num_shards=FLAGS.num_shards,\n          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n      ))\n  return run_config\n\n\n# TODO(ereal): simplify this.\ndef override_with_flags(hparams):\n  """"""Overrides parameters with flag values.""""""\n  override_flag_names = [\n      \'aux_scaling\',\n      \'train_batch_size\',\n      \'batch_norm_decay\',\n      \'batch_norm_epsilon\',\n      \'dense_dropout_keep_prob\',\n      \'drop_connect_keep_prob\',\n      \'drop_connect_version\',\n      \'eval_batch_size\',\n      \'gradient_clipping_by_global_norm\',\n      \'lr\',\n      \'lr_decay_method\',\n      \'lr_decay_value\',\n      \'lr_num_epochs_per_decay\',\n      \'moving_average_decay\',\n      \'image_size\',\n      \'num_cells\',\n      \'num_epochs\',\n      \'num_epochs_per_eval\',\n      \'optimizer\',\n      \'enable_hostcall\',\n      \'use_aux_head\',\n      \'use_bp16\',\n      \'use_tpu\',\n      \'lr_warmup_epochs\',\n      \'weight_decay\',\n      \'num_shards\',\n      \'distributed_group_size\',\n  ]\n  for flag_name in override_flag_names:\n    flag_value = getattr(FLAGS, flag_name, \'INVALID\')\n    if flag_value == \'INVALID\':\n      tf.logging.fatal(\'Unknown flag %s.\' % str(flag_name))\n    if flag_value is not None:\n      _set_or_add_hparam(hparams, flag_name, flag_value)\n\n\ndef build_hparams():\n  """"""Build tf.Hparams for training Amoeba Net.""""""\n  hparams = model_lib.imagenet_hparams()\n  hparams.add_hparam(\'reduction_size\', FLAGS.reduction_size)\n  operations, hiddenstate_indices, used_hiddenstates = (\n      model_specs.get_normal_cell(FLAGS.cell_name))\n  hparams.add_hparam(\'normal_cell_operations\', operations)\n  hparams.add_hparam(\'normal_cell_hiddenstate_indices\',\n                     hiddenstate_indices)\n  hparams.add_hparam(\'normal_cell_used_hiddenstates\',\n                     used_hiddenstates)\n  operations, hiddenstate_indices, used_hiddenstates = (\n      model_specs.get_reduction_cell(FLAGS.cell_name))\n  hparams.add_hparam(\'reduction_cell_operations\',\n                     operations)\n  hparams.add_hparam(\'reduction_cell_hiddenstate_indices\',\n                     hiddenstate_indices)\n  hparams.add_hparam(\'reduction_cell_used_hiddenstates\',\n                     used_hiddenstates)\n  hparams.add_hparam(\'stem_reduction_size\', FLAGS.stem_reduction_size)\n\n  hparams.set_hparam(\'data_format\', \'NHWC\')\n  override_with_flags(hparams)\n\n  return hparams\n\n\ndef _terminate_eval():\n  tf.logging.info(\'Timeout passed with no new checkpoints ... terminating eval\')\n  return True\n\n\ndef _get_next_checkpoint():\n  return tf.contrib.training.checkpoints_iterator(\n      FLAGS.model_dir,\n      timeout=FLAGS.eval_timeout,\n      timeout_fn=_terminate_eval)\n\n\ndef _set_or_add_hparam(hparams, name, value):\n  if getattr(hparams, name, None) is None:\n    hparams.add_hparam(name, value)\n  else:\n    hparams.set_hparam(name, value)\n\n\ndef _load_global_step_from_checkpoint_dir(checkpoint_dir):\n  try:\n    checkpoint_reader = tf.train.NewCheckpointReader(\n        tf.train.latest_checkpoint(checkpoint_dir))\n    return checkpoint_reader.get_tensor(tf.GraphKeys.GLOBAL_STEP)\n  except:  # pylint: disable=bare-except\n    return 0\n\n\ndef main(_):\n  mode = FLAGS.mode\n  data_dir = FLAGS.data_dir\n  model_dir = FLAGS.model_dir\n  hparams = build_hparams()\n\n  estimator_parmas = {}\n\n  train_steps_per_epoch = int(\n      math.ceil(model_lib.NUM_TRAIN_IMAGES / float(hparams.train_batch_size)))\n  eval_steps = model_lib.NUM_EVAL_IMAGES // hparams.eval_batch_size\n  eval_batch_size = (None if mode == \'train\' else\n                     hparams.eval_batch_size)\n\n  model = model_lib.AmoebaNetEstimatorModel(hparams, model_dir)\n\n  if hparams.use_tpu:\n    run_config = build_run_config()\n    image_classifier = tf.contrib.tpu.TPUEstimator(\n        model_fn=model.model_fn,\n        use_tpu=True,\n        config=run_config,\n        params=estimator_parmas,\n        train_batch_size=hparams.train_batch_size,\n        eval_batch_size=eval_batch_size)\n  else:\n    save_checkpoints_steps = (FLAGS.save_checkpoints_steps or\n                              FLAGS.iterations_per_loop)\n    run_config = tf.estimator.RunConfig(\n        model_dir=FLAGS.model_dir,\n        save_checkpoints_steps=save_checkpoints_steps)\n    image_classifier = tf.estimator.Estimator(\n        model_fn=model.model_fn,\n        config=run_config,\n        params=estimator_parmas)\n\n  # Input pipelines are slightly different (with regards to shuffling and\n  # preprocessing) between training and evaluation.\n  imagenet_train = model_lib.InputPipeline(\n      is_training=True, data_dir=data_dir, hparams=hparams)\n  imagenet_eval = model_lib.InputPipeline(\n      is_training=False, data_dir=data_dir, hparams=hparams)\n\n  if hparams.moving_average_decay < 1:\n    eval_hooks = [model_lib.LoadEMAHook(model_dir,\n                                        hparams.moving_average_decay)]\n  else:\n    eval_hooks = []\n\n  if mode == \'eval\':\n    for checkpoint in _get_next_checkpoint():\n      tf.logging.info(\'Starting to evaluate.\')\n      try:\n        eval_results = image_classifier.evaluate(\n            input_fn=imagenet_eval.input_fn,\n            steps=eval_steps,\n            hooks=eval_hooks,\n            checkpoint_path=checkpoint)\n        tf.logging.info(\'Evaluation results: %s\' % eval_results)\n      except tf.errors.NotFoundError:\n        # skip checkpoint if it gets deleted prior to evaluation\n        tf.logging.info(\'Checkpoint %s no longer exists ... skipping\')\n  elif mode == \'train_and_eval\':\n    current_step = _load_global_step_from_checkpoint_dir(model_dir)\n    tf.logging.info(\'Starting training at step=%d.\' % current_step)\n    train_steps_per_eval = int(\n        hparams.num_epochs_per_eval * train_steps_per_epoch)\n    # Final Evaluation if training is finished.\n    if current_step >= hparams.num_epochs * train_steps_per_epoch:\n      eval_results = image_classifier.evaluate(\n          input_fn=imagenet_eval.input_fn, steps=eval_steps, hooks=eval_hooks)\n      tf.logging.info(\'Evaluation results: %s\' % eval_results)\n    while current_step < hparams.num_epochs * train_steps_per_epoch:\n      image_classifier.train(\n          input_fn=imagenet_train.input_fn, steps=train_steps_per_eval)\n      current_step += train_steps_per_eval\n      tf.logging.info(\'Starting evaluation at step=%d.\' % current_step)\n      eval_results = image_classifier.evaluate(\n          input_fn=imagenet_eval.input_fn, steps=eval_steps, hooks=eval_hooks)\n      tf.logging.info(\'Evaluation results: %s\' % eval_results)\n  elif mode == \'predict\':\n    tf.logging.info(\'Starting prediction ...\')\n    time_hook = model_lib.SessionTimingHook()\n    eval_hooks.append(time_hook)\n    result_iter = image_classifier.predict(\n        input_fn=imagenet_eval.input_fn,\n        hooks=eval_hooks,\n        yield_single_examples=False)\n    results = list(itertools.islice(result_iter, eval_steps))\n    tf.logging.info(\'Inference speed = {} images per second.\'.format(\n        time_hook.compute_speed(len(results) * eval_batch_size)))\n  else:  # default to train mode.\n    current_step = _load_global_step_from_checkpoint_dir(model_dir)\n    total_step = int(hparams.num_epochs * train_steps_per_epoch)\n    if current_step < total_step:\n      tf.logging.info(\'Starting training ...\')\n      image_classifier.train(\n          input_fn=imagenet_train.input_fn,\n          steps=total_step-current_step)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/models/official/amoeba_net/amoeba_net_model.py,74,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""AmoebaNet ImageNet model functions.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport os\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport inception_preprocessing\nimport model_builder\n\n# Dataset constants\nNUM_TRAIN_IMAGES = 1281167\nNUM_EVAL_IMAGES = 50000\nLABEL_CLASSES = 1001\n\n# Random cropping constants\n_RESIZE_SIDE_MIN = 300\n_RESIZE_SIDE_MAX = 600\n\n# Constants dictating the learning rate schedule.\nRMSPROP_DECAY = 0.9                # Decay term for RMSProp.\nRMSPROP_MOMENTUM = 0.9             # Momentum in RMSProp.\nRMSPROP_EPSILON = 1.0              # Epsilon term for RMSProp.\n\n\ndef imagenet_hparams():\n  """"""Returns default ImageNet training params.\n\n  These defaults are for full training. For search training, some should be\n  modified to increase the speed of the search.\n  """"""\n  return tf.contrib.training.HParams(\n      ##########################################################################\n      # Input pipeline params. #################################################\n      ##########################################################################\n\n      image_size=299,\n\n      ##########################################################################\n      # Architectural params. ##################################################\n      ##########################################################################\n\n      # The total number of regular cells (summed across all stacks). Reduction\n      # cells are not included.\n      num_cells=18,\n\n      # How many reduction cells to use between the stacks of regular cells.\n      num_reduction_layers=2,\n\n      # Stem.\n      stem_type=\'imagenet\',  # \'imagenet\' or others\n      num_stem_cells=2,  # 2 if stem_type == \'imagenet\' else 0\n\n      # Implementation details.\n      data_format=\'NCHW\',  # \'NHWC\' or \'NCHW\'.\n\n      ##########################################################################\n      # Training params. #######################################################\n      ##########################################################################\n\n      # Summed across all TPU cores training a model.\n      train_batch_size=32,\n\n      num_epochs=100.,\n\n      # Auxiliary head.\n      use_aux_head=True,\n      aux_scaling=0.4,\n\n      # Regularization.\n      l1_decay_rate=0.0,\n      label_smoothing=0.1,\n      drop_connect_keep_prob=0.7,\n      # `drop_connect_version` determines how the drop_connect probabilites are\n      # set/increased over time:\n      # -v1: increase dropout probability over training,\n      # -v2: increase dropout probability as you increase the number of cells,\n      #      so the top cell has the highest dropout and the lowest cell has the\n      #      lowest dropout,\n      # -v3: Do both v1 and v2.\n      drop_connect_version=\'v1\',\n      # `drop_connect_condition` determines under what conditions drop_connect\n      # is used:\n      # -identity: Dropout all paths except identity connections,\n      # -all: Dropout all paths,\n      # -separable: Dropout only paths containing a separable conv operation.\n      dense_dropout_keep_prob=0.5,\n      batch_norm_epsilon=0.001,\n      batch_norm_decay=0.9997,\n      shuffle_buffer=20000,\n\n      # Any value <= 0 means it is unused\n      gradient_clipping_by_global_norm=10.0,\n\n      # Learning rate schedule.\n      lr=0.015,\n      lr_decay_method=\'exponential\',\n      lr_decay_value=0.97,\n      lr_num_epochs_per_decay=2.4,\n      lr_warmup_epochs=3.0,\n\n      # Optimizer.\n      optimizer=\'rmsprop\',  # \'sgd\', \'mom\', \'adam\' or \'rmsprop\'\n      rmsprop_decay=0.9,\n      rmsprop_momentum_rate=0.9,\n      rmsprop_epsilon=1.0,\n      momentum_rate=0.9,\n      use_nesterov=1,\n\n      ##########################################################################\n      # Eval and reporting params. #############################################\n      ##########################################################################\n\n      # This number should be a multiple of the number of TPU shards\n      # used for eval (e.g., 2 for a 1x1 or 8 for a 2x2).\n      eval_batch_size=40,\n\n      # How many different crops are fed into one model. Also affects training.\n      num_input_images=1,\n\n      moving_average_decay=0.9999,\n\n      write_summaries=0,\n\n      ##########################################################################\n      # Other params. ##########################################################\n      ##########################################################################\n\n      use_tpu=False)\n\n\ndef formatted_hparams(hparams):\n  """"""Formatts the hparams into a readable string.\n\n  Also looks for attributes that have not correctly been added to the hparams\n  and prints the keys as ""bad keys"". These bad keys may be left out of iterators\n  and cirumvent type checking.\n\n  Args:\n    hparams: an HParams instance.\n\n  Returns:\n    A string.\n  """"""\n  # Look for bad keys (see docstring).\n  good_keys = set(hparams.values().keys())\n  bad_keys = []\n  for key in hparams.__dict__:\n    if key not in good_keys and not key.startswith(\'_\'):\n      bad_keys.append(key)\n  bad_keys.sort()\n\n  # Format hparams.\n  readable_items = [\n      \'%s: %s\' % (k, v) for k, v in sorted(hparams.values().iteritems())]\n  readable_items.append(\'Bad keys: %s\' % \',\'.join(bad_keys))\n  readable_string = (\'\\n\'.join(readable_items))\n  return readable_string\n\n\nclass AmoebaNetEstimatorModel(object):\n  """"""Definition of AmoebaNet.""""""\n\n  def __init__(self, hparams, model_dir):\n    self.hparams = hparams\n    self.model_dir = model_dir\n\n  def _calc_num_trainable_params(self):\n    self.num_trainable_params = np.sum([\n        np.prod(var.get_shape().as_list()) for var in tf.trainable_variables()\n    ])\n    tf.logging.info(\n        \'number of trainable params: {}\'.format(self.num_trainable_params))\n\n  def _build_learning_rate_schedule(self, global_step):\n    """"""Build learning rate.""""""\n    steps_per_epoch = NUM_TRAIN_IMAGES // self.hparams.train_batch_size\n    lr_warmup_epochs = 0\n    if self.hparams.lr_decay_method == \'exponential\':\n      lr_warmup_epochs = self.hparams.lr_warmup_epochs\n    learning_rate = model_builder.build_learning_rate(\n        self.hparams.lr,\n        self.hparams.lr_decay_method,\n        global_step,\n        total_steps=steps_per_epoch * self.hparams.num_epochs,\n        decay_steps=steps_per_epoch * self.hparams.lr_num_epochs_per_decay,\n        decay_factor=self.hparams.lr_decay_value,\n        add_summary=False,\n        warmup_steps=int(lr_warmup_epochs * steps_per_epoch))\n\n    learning_rate = tf.maximum(\n        learning_rate, 0.0001 * self.hparams.lr, name=\'learning_rate\')\n    return learning_rate\n\n  def _build_network(self, features, labels, mode):\n    """"""Build a network that returns loss and logits from features and labels.""""""\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n    is_predict = (mode == tf.estimator.ModeKeys.PREDICT)\n    steps_per_epoch = float(NUM_TRAIN_IMAGES) / self.hparams.train_batch_size\n    num_total_steps = int(steps_per_epoch * self.hparams.num_epochs)\n    if getattr(self.hparams, \'num_total_steps\', None) is None:\n      self.hparams.add_hparam(\'num_total_steps\', num_total_steps)\n    else:\n      self.hparams.set_hparam(\'num_total_steps\', num_total_steps)\n\n    hparams = copy.deepcopy(self.hparams)\n    if not is_training:\n      hparams.set_hparam(\'use_aux_head\', False)\n\n    tf.logging.info(\n        \'Amoeba net received hparams for {}:\\n{}\'.format(\n            \'training\' if is_training else \'eval\',\n            formatted_hparams(hparams)))\n\n    logits, end_points = model_builder.build_network(\n        features, LABEL_CLASSES, is_training, hparams)\n\n    if not is_predict:\n      labels = tf.one_hot(labels, LABEL_CLASSES)\n      loss = model_builder.build_softmax_loss(\n          logits,\n          end_points,\n          labels,\n          label_smoothing=hparams.label_smoothing,\n          add_summary=False)\n\n    # Calculate and print the number of trainable parameters in the model\n    if is_training:\n      flops = model_builder.compute_flops_per_example(hparams.train_batch_size)\n    else:\n      flops = model_builder.compute_flops_per_example(hparams.eval_batch_size)\n    tf.logging.info(\'number of flops: {}\'.format(flops))\n    self._calc_num_trainable_params()\n\n    if is_predict:\n      return None, logits\n\n    return loss, logits\n\n  def _build_optimizer(self, learning_rate):\n    """"""Build optimizer.""""""\n    if self.hparams.optimizer == \'sgd\':\n      tf.logging.info(\'Using SGD optimizer\')\n      optimizer = tf.train.GradientDescentOptimizer(\n          learning_rate=learning_rate)\n    elif self.hparams.optimizer == \'momentum\':\n      tf.logging.info(\'Using Momentum optimizer\')\n      optimizer = tf.train.MomentumOptimizer(\n          learning_rate=learning_rate,\n          momentum=self.hparams.momentum_rate)\n    elif self.hparams.optimizer == \'rmsprop\':\n      tf.logging.info(\'Using RMSProp optimizer\')\n      optimizer = tf.train.RMSPropOptimizer(\n          learning_rate,\n          RMSPROP_DECAY,\n          momentum=RMSPROP_MOMENTUM,\n          epsilon=RMSPROP_EPSILON)\n    else:\n      tf.logging.fatal(\'Unknown optimizer:\', self.hparams.optimizer)\n\n    if self.hparams.use_tpu:\n      optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n    return optimizer\n\n  def _build_train_op(self, optimizer, loss, global_step):\n    """"""Build train_op from optimizer and loss.""""""\n    grads_and_vars = optimizer.compute_gradients(loss)\n    if self.hparams.gradient_clipping_by_global_norm > 0.0:\n      g, v = zip(*grads_and_vars)\n      g, _ = tf.clip_by_global_norm(\n          g, self.hparams.gradient_clipping_by_global_norm)\n      grads_and_vars = zip(g, v)\n\n    return optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n\n  def model_fn(self, features, labels, mode, params):\n    """"""Build the model based on features, labels, and mode.\n\n    Args:\n      features: The features dictionary containing the data Tensor\n        and the number of examples.\n      labels: The labels Tensor resulting from calling the model.\n      mode: A string indicating the training mode.\n      params: A dictionary of hyperparameters.\n\n    Returns:\n      A tf.estimator.EstimatorSpec.\n    """"""\n    del params\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n    eval_active = (mode == tf.estimator.ModeKeys.EVAL)\n    is_predict = (mode == tf.estimator.ModeKeys.PREDICT)\n    features = tf.transpose(features, [3, 0, 1, 2])  # HWCN to NHWC\n    loss, logits = self._build_network(features, labels, mode)\n\n    if is_predict:\n      predictions = {\'logits\': logits}\n      if self.hparams.use_tpu:\n        return  tf.contrib.tpu.TPUEstimatorSpec(mode=mode,\n                                                predictions=predictions)\n      else:\n        return tf.estimator.EstimatorSpec(mode=mode,\n                                          predictions=predictions)\n    host_call = None\n    train_op = None\n\n    if is_training:\n      global_step = tf.train.get_or_create_global_step()\n      gs_t = tf.reshape(tf.cast(global_step, tf.int32), [1])\n\n      # Setup learning rate schedule\n      learning_rate = self._build_learning_rate_schedule(global_step)\n\n      # Setup optimizer.\n      optimizer = self._build_optimizer(learning_rate)\n\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        train_op = self._build_train_op(optimizer, loss,\n                                        global_step=global_step)\n      if self.hparams.moving_average_decay > 0:\n        ema = tf.train.ExponentialMovingAverage(\n            decay=self.hparams.moving_average_decay, num_updates=global_step)\n        variables_to_average = (tf.trainable_variables() +\n                                tf.moving_average_variables())\n        with tf.control_dependencies([train_op]):\n          with tf.name_scope(\'moving_average\'):\n            train_op = ema.apply(variables_to_average)\n\n      lr_t = tf.reshape(learning_rate, [1])\n      host_call = None\n      if self.hparams.enable_hostcall:\n        def host_call_fn(gs, lr):\n          # Outfeed supports int32 but global_step is expected to be int64.\n          gs = tf.cast(tf.reduce_mean(gs), tf.int64)\n          with tf.contrib.summary.create_file_writer(\n              self.model_dir).as_default():\n            with tf.contrib.summary.always_record_summaries():\n              tf.contrib.summary.scalar(\'learning_rate\', tf.reduce_mean(lr),\n                                        step=gs)\n              return tf.contrib.summary.all_summary_ops()\n        host_call = (host_call_fn, [gs_t, lr_t])\n\n    eval_metrics = None\n    eval_metric_ops = None\n    if eval_active:\n      def metric_fn(labels, logits):\n        """"""Evaluation metric fn. Performed on CPU, do not reference TPU ops.""""""\n        # Outfeed supports int32 but global_step is expected to be int64.\n        predictions = tf.argmax(logits, axis=1)\n        categorical_labels = labels\n        top_1_accuracy = tf.metrics.accuracy(categorical_labels, predictions)\n        in_top_5 = tf.cast(tf.nn.in_top_k(logits, categorical_labels, 5),\n                           tf.float32)\n        top_5_accuracy = tf.metrics.mean(in_top_5)\n\n        return {\n            \'top_1_accuracy\': top_1_accuracy,\n            \'top_5_accuracy\': top_5_accuracy,\n        }\n\n      eval_metrics = (metric_fn, [labels, logits])\n      eval_metric_ops = metric_fn(labels, logits)\n\n    if self.hparams.use_tpu:\n      return tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode, loss=loss, train_op=train_op,\n          host_call=host_call, eval_metrics=eval_metrics)\n    return tf.estimator.EstimatorSpec(\n        mode=mode, loss=loss, train_op=train_op,\n        eval_metric_ops=eval_metric_ops)\n\n\nclass InputPipeline(object):\n  """"""Generates ImageNet input_fn for training or evaluation.\n\n  The training data is assumed to be in TFRecord format with keys as specified\n  in the dataset_parser below, sharded across 1024 files, named sequentially:\n      train-00000-of-01024\n      train-00001-of-01024\n      ...\n      train-01023-of-01024\n\n  The validation data is in the same format but sharded in 128 files.\n\n  The format of the data required is created by the script at:\n      https://github.com/tensorflow/tpu/blob/master/tools/datasets/imagenet_to_gcs.py\n\n  Args:\n    is_training: `bool` for whether the input is for training\n  """"""\n\n  def __init__(self, is_training, data_dir, hparams):\n    self.is_training = is_training\n    self.data_dir = data_dir\n    self.hparams = hparams\n    self.num_classes = 1001\n\n  def _dataset_parser(self, serialized_proto):\n    """"""Parse an Imagenet record from value.""""""\n    keys_to_features = {\n        \'image/encoded\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\':\n            tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/class/label\':\n            tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),\n        \'image/class/text\':\n            tf.FixedLenFeature([], dtype=tf.string, default_value=\'\'),\n        \'image/object/bbox/xmin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/class/label\':\n            tf.VarLenFeature(dtype=tf.int64),\n    }\n\n    features = tf.parse_single_example(serialized_proto, keys_to_features)\n\n    bbox = None\n\n    image = features[\'image/encoded\']\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    image = inception_preprocessing.preprocess_image(\n        image=image,\n        output_height=self.hparams.image_size,\n        output_width=self.hparams.image_size,\n        is_training=self.is_training,\n        bbox=bbox)\n\n    label = tf.cast(\n        tf.reshape(features[\'image/class/label\'], shape=[]), dtype=tf.int32)\n\n    return image, label\n\n  def input_fn(self, params):\n    """"""Input function which provides a single batch for train or eval.\n\n    Args:\n      params: `dict` of parameters passed from the `TPUEstimator`.\n\n    Returns:\n      A callable dataset object.\n    """"""\n    # Retrieves the batch size for the current shard. The # of shards is\n    # computed according to the input pipeline deployment. See\n    # tf.contrib.tpu.RunConfig for details.\n    if \'batch_size\' in params:\n      batch_size = params[\'batch_size\']\n    else:\n      batch_size = (self.hparams.train_batch_size if self.is_training\n                    else self.hparams.eval_batch_size)\n    file_pattern = os.path.join(\n        self.data_dir, \'train-*\' if self.is_training else \'validation-*\')\n    dataset = tf.data.Dataset.list_files(file_pattern, shuffle=self.is_training)\n    if self.is_training:\n      dataset = dataset.repeat()\n\n    def fetch_dataset(filename):\n      buffer_size = 8 * 1024 * 1024  # 8 MiB per file\n      dataset = tf.data.TFRecordDataset(filename, buffer_size=buffer_size)\n      return dataset\n\n    dataset = dataset.apply(\n        tf.contrib.data.parallel_interleave(\n            fetch_dataset, cycle_length=64, sloppy=True))\n    dataset = dataset.shuffle(1024)\n\n    # Use the fused map-and-batch operation.\n    #\n    # For XLA, we must used fixed shapes. Because we repeat the source training\n    # dataset indefinitely, we can use `drop_remainder=True` to get fixed-size\n    # batches without dropping any training examples.\n    #\n    # When evaluating, `drop_remainder=True` prevents accidentally evaluating\n    # the same image twice by dropping the final batch if it is less than a full\n    # batch size. As long as this validation is done with consistent batch size,\n    # exactly the same images will be used.\n    dataset = dataset.apply(\n        tf.contrib.data.map_and_batch(\n            self._dataset_parser, batch_size=batch_size,\n            num_parallel_batches=8, drop_remainder=True))\n\n    dataset = dataset.map(\n        lambda images, labels: (tf.transpose(images, [1, 2, 3, 0]), labels),\n        num_parallel_calls=8)\n\n    dataset = dataset.prefetch(32)  # Prefetch overlaps in-feed with training\n    return dataset  # Must return the dataset and not tensors for high perf!\n\n\nclass LoadEMAHook(tf.train.SessionRunHook):\n  """"""Hook to load EMA into their corresponding variables.""""""\n\n  def __init__(self, model_dir, moving_average_decay):\n    super(LoadEMAHook, self).__init__()\n    self._model_dir = model_dir\n    self.moving_average_decay = moving_average_decay\n\n  def begin(self):\n    ema = tf.train.ExponentialMovingAverage(self.moving_average_decay)\n    variables_to_restore = ema.variables_to_restore()\n    self._load_ema = tf.contrib.framework.assign_from_checkpoint_fn(\n        tf.train.latest_checkpoint(self._model_dir), variables_to_restore)\n\n  def after_create_session(self, sess, coord):\n    tf.logging.info(\'Reloading EMA...\')\n    self._load_ema(sess)\n\n\nclass SessionTimingHook(tf.train.SessionRunHook):\n  """"""Hook that computes speed based on session run time.""""""\n\n  def __init__(self):\n    # Lists of walltime.\n    self._before_runs = []\n    self._after_runs = []\n\n  def before_run(self, run_context):\n    self._before_runs.append(time.time())\n\n  def after_run(self, run_context, results):\n    self._after_runs.append(time.time())\n\n  def compute_speed(self, num_samples):\n    """"""Returns speed, in number of samples per second.""""""\n    num_runs = len(self._before_runs)\n    if num_runs == 0:\n      raise ValueError(\'Session run time never recorded\')\n    if len(self._after_runs) != num_runs:\n      raise ValueError(\n          \'Number of before_run events (%d) does not match \'\n          \'number of after_run events (%d)\' %\n          (len(self._before_runs), len(self._after_runs)))\n    total_eval_time = sum(self._after_runs[i] - self._before_runs[i]\n                          for i in range(num_runs))\n    if num_runs <= 1:\n      tf.logging.warn(\n          \'Speed will be inaccurate with only one session run\')\n    else:\n      # Exclude the first run, which tends to take much longer than other runs.\n      total_eval_time -= (self._after_runs[0] - self._before_runs[0])\n      # We assume num_samples are evenly distributed across runs.\n      num_samples *= (float(num_runs - 1) / num_runs)\n    return num_samples / max(total_eval_time, 1e-6)\n'"
tpu/models/official/amoeba_net/inception_preprocessing.py,69,"b'# Copyright 2018 Google. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the Inception networks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import random_ops\n\n\nflags.DEFINE_float(\n    \'cb_distortion_range\', 0.1, \'Cb distortion range +/-\')\n\nflags.DEFINE_float(\n    \'cr_distortion_range\', 0.1, \'Cr distortion range +/-\')\n\nflags.DEFINE_boolean(\n    \'use_fast_color_distort\', True,\n    \'apply fast color/chroma distortion if True, else apply\'\n    \'brightness/saturation/hue/contrast distortion\')\n\nFLAGS = flags.FLAGS\n\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.minimum(tf.maximum(image, 0.0), 1.0)\n\n\ndef distort_color_fast(image, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Distort brightness and chroma values of input image\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    br_delta = random_ops.random_uniform([], -32./255., 32./255., seed=None)\n    cb_factor = random_ops.random_uniform(\n        [], -FLAGS.cb_distortion_range, FLAGS.cb_distortion_range, seed=None)\n    cr_factor = random_ops.random_uniform(\n        [], -FLAGS.cr_distortion_range, FLAGS.cr_distortion_range, seed=None)\n\n    channels = tf.split(axis=2, num_or_size_splits=3, value=image)\n    red_offset = 1.402 * cr_factor + br_delta\n    green_offset = -0.344136 * cb_factor - 0.714136 * cr_factor + br_delta\n    blue_offset = 1.772 * cb_factor + br_delta\n    channels[0] += red_offset\n    channels[1] += green_offset\n    channels[2] += blue_offset\n    image = tf.concat(axis=2, values=channels)\n    image = tf.minimum(tf.maximum(image, 0.), 1.)\n    return image\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(3./4., 4./3.),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n      image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n      area of the image must contain at least this fraction of any bounding box\n      supplied.\n    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n      image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `floats`. The cropped area of the image\n      must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n      region of the image of the specified constraints. After `max_attempts`\n      failures, return the entire image.\n    scope: Optional scope for name_scope.\n  Returns:\n    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # A large fraction of image datasets contain a human-annotated bounding\n    # box delineating the region of the image containing the object of interest.\n    # We choose to create a new bounding box for the object which is a randomly\n    # distorted version of the human-annotated bounding box that obeys an\n    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n    # bounding box. If no box is supplied, then we assume the bounding box is\n    # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        tf.shape(image),\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n    return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         fast_mode=True,\n                         scope=None,\n                         add_image_summaries=True):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Additionally it would create image_summaries to display the different\n  transformations applied to the image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n      bi-cubic resizing, random_hue or random_contrast).\n    scope: Optional scope for name_scope.\n    add_image_summaries: Enable image summaries.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].\n  """"""\n  with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n    if bbox is None:\n      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                         dtype=tf.float32,\n                         shape=[1, 1, 4])\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    if add_image_summaries:\n      # Each bounding box has shape [1, num_boxes, box coords] and\n      # the coordinates are ordered [ymin, xmin, ymax, xmax].\n      image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                    bbox)\n      tf.summary.image(\'image_with_bounding_boxes\', image_with_box)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n    if add_image_summaries:\n      image_with_distorted_box = tf.image.draw_bounding_boxes(\n          tf.expand_dims(image, 0), distorted_bbox)\n      tf.summary.image(\'images_with_distorted_bounding_box\',\n                       image_with_distorted_box)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize_images(x, [height, width], method),\n        num_cases=num_resize_cases)\n\n    if add_image_summaries:\n      tf.summary.image(\'cropped_resized_image\',\n                       tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors. There are 1 or 4 ways to do it.\n    if FLAGS.use_fast_color_distort:\n      distorted_image = distort_color_fast(distorted_image)\n    else:\n      num_distort_cases = 1 if fast_mode else 4\n      distorted_image = apply_with_random_selector(\n          distorted_image,\n          lambda x, ordering: distort_color(x, ordering, fast_mode),\n          num_cases=num_distort_cases)\n\n    if add_image_summaries:\n      tf.summary.image(\'final_distorted_image\',\n                       tf.expand_dims(distorted_image, 0))\n    distorted_image = tf.subtract(distorted_image, 0.5)\n    distorted_image = tf.multiply(distorted_image, 2.0)\n    return distorted_image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n  """"""Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n\n  If central_fraction is specified it would crop the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n  with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n      # Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(image, [height, width],\n                                       align_corners=False)\n      image = tf.squeeze(image, [0])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    image.set_shape([height, width, 3])\n    return image\n\n\ndef preprocess_image(image, output_height, output_width,\n                     is_training=False,\n                     bbox=None,\n                     fast_mode=True,\n                     add_image_summaries=False):\n  """"""Pre-process one image for training or evaluation.\n\n  Args:\n    image: 3-D Tensor [height, width, channels] with the image. If dtype is\n      tf.float32 then the range should be [0, 1], otherwise it would converted\n      to tf.float32 assuming that the range is [0, MAX], where MAX is largest\n      positive representable number for int(8/16/32) data type (see\n      `tf.image.convert_image_dtype` for details).\n    output_height: integer, image expected height.\n    output_width: integer, image expected width.\n    is_training: Boolean. If true it would transform an image for train,\n      otherwise it would transform it for evaluation.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations.\n    add_image_summaries: Enable image summaries.\n\n  Returns:\n    3-D float Tensor containing an appropriately scaled image\n\n  Raises:\n    ValueError: if user does not provide bounding box\n  """"""\n  if is_training:\n    return preprocess_for_train(image, output_height, output_width, bbox,\n                                fast_mode,\n                                add_image_summaries=add_image_summaries)\n  else:\n    return preprocess_for_eval(image, output_height, output_width)\n'"
tpu/models/official/amoeba_net/model_builder.py,48,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Constructs a generic image model based on the hparams the user passes in.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nimport numpy as np\nimport tensorflow as tf\n\nimport network_utils\n\n\narg_scope = tf.contrib.framework.arg_scope\nslim = tf.contrib.slim\n\n\ndef _build_loss(loss_fn, loss_name, logits, end_points, labels,\n                add_summary=False):\n  """"""Compute total loss based on the specified loss function.""""""\n  # Collect all losses explicitly to sum up the total_loss.\n  losses = []\n\n  # Whethere to add aux loss is controled in network_fn. Once an aux head is\n  # built, an aux loss would be added here automatically.\n  aux_head_endpoint = None\n  if \'AuxLogits\' in end_points:\n    # For Inception/Genet aux head.\n    aux_head_endpoint = end_points[\'AuxLogits\']\n  elif \'aux_logits\' in end_points:\n    # For AmoebaNet aux head.\n    aux_head_endpoint = end_points[\'aux_logits\'],\n  if aux_head_endpoint:\n    aux_loss = loss_fn(\n        labels,\n        tf.squeeze(aux_head_endpoint, axis=[0]),\n        weights=0.4,\n        scope=\'aux_loss\')\n    tf.logging.info(\'Adding to aux loss.\')\n    if add_summary:\n      tf.summary.scalar(\'losses/aux_loss\', aux_loss)\n\n    losses.append(aux_loss)\n\n  # Add the empirical loss.\n  primary_loss = loss_fn(labels, logits, weights=1.0, scope=loss_name)\n  losses.append(primary_loss)\n\n  # Add regularization losses.\n  reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n  if reg_losses:\n    fp32_reg_losses = []\n    for reg_loss in reg_losses:\n      fp32_reg_losses.append(tf.cast(reg_loss, tf.float32))\n    reg_loss = tf.add_n(fp32_reg_losses, name=\'regularization_loss\')\n    losses.append(reg_loss)\n\n  total_loss = tf.add_n(losses, name=\'total_loss\')\n  if add_summary:\n    tf.summary.scalar(\'losses/\' + loss_name, primary_loss)\n    tf.summary.scalar(\'losses/regularization_loss\', reg_loss)\n    tf.summary.scalar(\'losses/total_loss\', total_loss)\n\n  return total_loss\n\n\ndef build_softmax_loss(logits,\n                       end_points,\n                       labels,\n                       label_smoothing=0.1,\n                       add_summary=True):\n  loss_fn = functools.partial(\n      tf.losses.softmax_cross_entropy, label_smoothing=label_smoothing)\n  return _build_loss(\n      loss_fn=loss_fn,\n      loss_name=\'softmax_loss\',\n      logits=logits,\n      end_points=end_points,\n      labels=labels,\n      add_summary=add_summary)\n\n\ndef compute_flops_per_example(batch_size):\n  # TODO(ereal): remove this function and other unnecessary reporting.\n  options = tf.profiler.ProfileOptionBuilder.float_operation()\n  options[\'output\'] = \'none\'\n  flops = (\n      tf.profiler.profile(\n          tf.get_default_graph(),\n          options=options\n          ).total_float_ops / batch_size)\n  return flops\n\n\ndef build_learning_rate(initial_lr,\n                        lr_decay_type,\n                        global_step,\n                        decay_factor=None,\n                        decay_steps=None,\n                        stepwise_epoch=None,\n                        total_steps=None,\n                        add_summary=True,\n                        warmup_steps=0):\n  """"""Build learning rate.""""""\n  if lr_decay_type == \'exponential\':\n    assert decay_steps is not None\n    assert decay_factor is not None\n    lr = tf.train.exponential_decay(\n        initial_lr, global_step, decay_steps, decay_factor, staircase=True)\n  elif lr_decay_type == \'cosine\':\n    assert total_steps is not None\n    lr = 0.5 * initial_lr * (\n        1 + tf.cos(np.pi * tf.cast(global_step, tf.float32) / total_steps))\n  elif lr_decay_type == \'constant\':\n    lr = initial_lr\n  elif lr_decay_type == \'stepwise\':\n    assert stepwise_epoch is not None\n    boundaries = [\n        10 * stepwise_epoch,\n        20 * stepwise_epoch,\n    ]\n    values = [initial_lr, initial_lr * 0.1, initial_lr * 0.01]\n    lr = tf.train.piecewise_constant(global_step, boundaries, values)\n  else:\n    assert False, \'Unknown lr_decay_type : %s\' % lr_decay_type\n\n  # By default, warmup_steps_fraction = 0.0 which means no warmup steps.\n  tf.logging.info(\'Learning rate warmup_steps: %d\' % warmup_steps)\n  warmup_lr = (\n      initial_lr * tf.cast(global_step, tf.float32) / tf.cast(\n          warmup_steps, tf.float32))\n  lr = tf.cond(global_step < warmup_steps, lambda: warmup_lr, lambda: lr)\n\n  if add_summary:\n    tf.summary.scalar(\'learning_rate\', lr)\n\n  return lr\n\n\ndef _build_aux_head(net, end_points, num_classes, hparams, scope):\n  """"""Auxiliary head used for all models across all datasets.""""""\n  aux_scaling = 1.0\n  # TODO(huangyp): double check aux_scaling with vrv@.\n  if hasattr(hparams, \'aux_scaling\'):\n    aux_scaling = hparams.aux_scaling\n  tf.logging.info(\'aux scaling: {}\'.format(aux_scaling))\n  with tf.variable_scope(scope, custom_getter=network_utils.bp16_getter):\n    aux_logits = tf.identity(net)\n    with tf.variable_scope(\'aux_logits\'):\n      aux_logits = slim.avg_pool2d(\n          aux_logits, [5, 5], stride=3, padding=\'VALID\')\n      aux_logits = slim.conv2d(aux_logits, int(128 * aux_scaling),\n                               [1, 1], scope=\'proj\')\n      aux_logits = network_utils.batch_norm(aux_logits, scope=\'aux_bn0\')\n      aux_logits = tf.nn.relu(aux_logits)\n      # Shape of feature map before the final layer.\n      shape = aux_logits.shape\n      if hparams.data_format == \'NHWC\':\n        shape = shape[1:3]\n      else:\n        shape = shape[2:4]\n      aux_logits = slim.conv2d(aux_logits, int(768 * aux_scaling),\n                               shape, padding=\'VALID\')\n      aux_logits = network_utils.batch_norm(aux_logits, scope=\'aux_bn1\')\n      aux_logits = tf.nn.relu(aux_logits)\n      aux_logits = tf.contrib.layers.flatten(aux_logits)\n      aux_logits = slim.fully_connected(aux_logits, num_classes)\n      end_point_name = (\n          \'aux_logits\' if \'aux_logits\' not in end_points else \'aux_logits_2\')\n      end_points[end_point_name] = tf.cast(aux_logits, tf.float32)\n\n\ndef _imagenet_stem(inputs, hparams, stem_cell, filter_scaling_rate):\n  """"""Stem used for models trained on ImageNet.""""""\n\n  # 149 x 149 x 32\n  num_stem_filters = hparams.stem_reduction_size\n  with tf.variable_scope(\'stem\', custom_getter=network_utils.bp16_getter):\n    net = slim.conv2d(\n        inputs, num_stem_filters, [3, 3], stride=2, scope=\'conv0\',\n        padding=\'VALID\')\n    net = network_utils.batch_norm(net, scope=\'conv0_bn\')\n    tf.logging.info(\'imagenet_stem shape: {}\'.format(net.shape))\n  # Run the reduction cells\n  cell_outputs = [None, net]\n  filter_scaling = 1.0 / (filter_scaling_rate**hparams.num_stem_cells)\n  for cell_num in range(hparams.num_stem_cells):\n    net = stem_cell(\n        net,\n        scope=\'cell_stem_{}\'.format(cell_num),\n        filter_scaling=filter_scaling,\n        stride=2,\n        prev_layer=cell_outputs[-2],\n        cell_num=cell_num)\n    cell_outputs.append(net)\n    filter_scaling *= filter_scaling_rate\n    tf.logging.info(\'imagenet_stem net shape at reduction layer {}: {}\'.format(\n        cell_num, net.shape))\n  # Only include cells in the cell_outputs.\n  return net, cell_outputs\n\n\ndef _basic_stem(inputs, hparams):\n  num_stem_filters = hparams.stem_reduction_size\n  with tf.variable_scope(\'stem\', custom_getter=network_utils.bp16_getter):\n    net = slim.conv2d(\n        inputs, num_stem_filters, [3, 3], stride=1, scope=\'conv0\',\n        padding=\'VALID\')\n    net = network_utils.batch_norm(net, scope=\'conv0_bn\')\n    tf.logging.info(\'basic_stem shape: {}\'.format(net.shape))\n  return net, [None, net]\n\n\ndef network_arg_scope(weight_decay=5e-5,\n                      batch_norm_decay=0.9997,\n                      batch_norm_epsilon=1e-3,\n                      is_training=True,\n                      data_format=\'NHWC\',\n                      num_shards=None,\n                      distributed_group_size=1):\n  """"""Defines the default arg scope for the AmoebaNet ImageNet model.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n    is_training: whether is training or not.\n      Useful for fine-tuning a model with different num_classes.\n    data_format: Input data format.\n    num_shards: Number of shards in the job\n    distributed_group_size: Size of the group to average for batch norm.\n  Returns:\n    An `arg_scope` to use for the AmoebaNet Model.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': True,\n      \'moving_vars\': \'moving_vars\',\n      \'is_training\': is_training,\n      \'data_format\': data_format,\n      \'num_shards\': num_shards,\n      \'distributed_group_size\': distributed_group_size,\n  }\n  weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n  weights_initializer = tf.contrib.layers.variance_scaling_initializer(\n      mode=\'FAN_OUT\')\n  with arg_scope([slim.fully_connected, slim.conv2d, slim.separable_conv2d],\n                 weights_regularizer=weights_regularizer,\n                 weights_initializer=weights_initializer):\n    with arg_scope([slim.fully_connected],\n                   activation_fn=None, scope=\'FC\'):\n      with arg_scope([slim.conv2d, slim.separable_conv2d],\n                     activation_fn=None, biases_initializer=None):\n        with arg_scope([network_utils.batch_norm], **batch_norm_params):\n          with arg_scope(\n              [slim.dropout, network_utils.drop_path], is_training=is_training):\n            with arg_scope([slim.avg_pool2d,\n                            slim.max_pool2d,\n                            slim.conv2d,\n                            slim.separable_conv2d,\n                            network_utils.factorized_reduction,\n                            network_utils.global_avg_pool,\n                            network_utils.get_channel_index,\n                            network_utils.get_channel_dim],\n                           data_format=data_format) as sc:\n              return sc\n\n\ndef build_network(inputs,\n                  num_classes,\n                  is_training=True,\n                  hparams=None):\n  """"""Builds an image model.\n\n  Builds a model the takes inputs and return logits and endpoints.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of classes needed to be predicted by the model.\n    is_training: whether is training or not.\n      Useful for fine-tuning a model with different num_classes.\n    hparams: hparams used to construct the imagenet model.\n\n  Returns:\n    a list containing \'logits\', \'aux_logits\' Tensors.\n\n  Raises:\n    ValueError: upon invalid hparams.\n  """"""\n  total_num_cells = (hparams.num_cells +\n                     hparams.num_reduction_layers +\n                     hparams.num_stem_cells)\n  normal_cell = network_utils.BaseCell(\n      hparams.reduction_size,\n      hparams.normal_cell_operations,\n      hparams.normal_cell_used_hiddenstates,\n      hparams.normal_cell_hiddenstate_indices,\n      hparams.drop_connect_keep_prob,\n      total_num_cells,\n      hparams.num_total_steps)\n  reduction_cell = network_utils.BaseCell(\n      hparams.reduction_size,\n      hparams.reduction_cell_operations,\n      hparams.reduction_cell_used_hiddenstates,\n      hparams.reduction_cell_hiddenstate_indices,\n      hparams.drop_connect_keep_prob,\n      total_num_cells,\n      hparams.num_total_steps)\n  num_shards = hparams.num_shards\n  distributed_group_size = hparams.distributed_group_size\n  assert distributed_group_size == 1 or hparams.use_tpu\n  sc = network_arg_scope(weight_decay=hparams.weight_decay,\n                         batch_norm_decay=hparams.batch_norm_decay,\n                         batch_norm_epsilon=hparams.batch_norm_epsilon,\n                         is_training=is_training,\n                         data_format=hparams.data_format,\n                         num_shards=num_shards,\n                         distributed_group_size=distributed_group_size)\n  with arg_scope(sc):\n    return _build_network_base(inputs,\n                               normal_cell=normal_cell,\n                               reduction_cell=reduction_cell,\n                               num_classes=num_classes,\n                               hparams=hparams,\n                               is_training=is_training)\n\n\ndef _build_network_base(images,\n                        normal_cell,\n                        reduction_cell,\n                        num_classes,\n                        hparams,\n                        is_training):\n  """"""Constructs a AmoebaNet image model.""""""\n  if hparams.get(\'use_bp16\', False) and hparams.get(\'use_tpu\', False):\n    images = tf.cast(images, dtype=tf.bfloat16)\n  end_points = {}\n  filter_scaling_rate = 2\n  # Find where to place the reduction cells or stride normal cells\n  reduction_indices = network_utils.calc_reduction_layers(\n      hparams.num_cells, hparams.num_reduction_layers)\n  stem_cell = reduction_cell\n\n  if hparams.stem_type == \'imagenet\':\n    net, cell_outputs = _imagenet_stem(images, hparams, stem_cell,\n                                       filter_scaling_rate)\n  else:\n    net, cell_outputs = _basic_stem(images, hparams)\n\n  # Setup for building in the auxiliary head.\n  aux_head_cell_idxes = []\n  if len(reduction_indices) >= 2:\n    aux_head_cell_idxes.append(reduction_indices[1] - 1)\n\n  # Run the cells\n  filter_scaling = 1.0\n  # true_cell_num accounts for the stem cells\n  true_cell_num = hparams.num_stem_cells\n  for cell_num in range(hparams.num_cells):\n    tf.logging.info(\'Current cell num: {}\'.format(true_cell_num))\n\n    if cell_num in reduction_indices:\n      filter_scaling *= filter_scaling_rate\n      net = reduction_cell(\n          net,\n          scope=\'reduction_cell_{}\'.format(reduction_indices.index(cell_num)),\n          filter_scaling=filter_scaling,\n          stride=2,\n          prev_layer=cell_outputs[-2],\n          cell_num=true_cell_num)\n      cell_outputs.append(net)\n      tf.logging.info(\'Reduction cell shape at layer {}: {}\'.format(\n          true_cell_num, net.shape))\n      true_cell_num += 1\n    net = normal_cell(\n        net,\n        scope=\'cell_{}\'.format(cell_num),\n        filter_scaling=filter_scaling,\n        stride=1,\n        prev_layer=cell_outputs[-2],\n        cell_num=true_cell_num)\n    if (hparams.use_aux_head and cell_num in aux_head_cell_idxes and\n        num_classes and is_training):\n      aux_net = tf.nn.relu(net)\n      _build_aux_head(aux_net, end_points, num_classes, hparams,\n                      scope=\'aux_{}\'.format(cell_num))\n    cell_outputs.append(net)\n    tf.logging.info(\'Normal net shape at layer {}: {}\'.format(\n        true_cell_num, net.shape))\n    true_cell_num += 1\n\n  # Final softmax layer\n  with tf.variable_scope(\'final_layer\',\n                         custom_getter=network_utils.bp16_getter):\n    net = tf.nn.relu(net)\n    net = network_utils.global_avg_pool(net)\n    end_points[\'global_pool\'] = net\n    net = slim.dropout(net, hparams.dense_dropout_keep_prob, scope=\'dropout\')\n    logits = slim.fully_connected(net, num_classes)\n  logits = tf.cast(logits, tf.float32)\n  predictions = tf.nn.softmax(logits, name=\'predictions\')\n  end_points[\'logits\'] = logits\n  end_points[\'predictions\'] = predictions\n  return logits, end_points\n'"
tpu/models/official/amoeba_net/model_specs.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Configs for various AmoebaNet architectures.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\ndef get_normal_cell(cell_name):\n  """"""Return normal cell spec.""""""\n  operations = []\n  hiddenstate_indices = []\n  used_hiddenstates = []\n  if cell_name == \'evol_net_g\' or cell_name == \'amoeba_net_a\':\n    operations = [\'avg_pool_3x3\', \'max_pool_3x3\', \'separable_3x3_2\', \'none\',\n                  \'none\', \'avg_pool_3x3\', \'separable_3x3_2\', \'separable_5x5_2\',\n                  \'avg_pool_3x3\', \'separable_3x3_2\']\n    hiddenstate_indices = [0, 0, 1, 1, 0, 1, 0, 2, 5, 0]\n    used_hiddenstates = [1, 0, 1, 0, 0, 1, 0]\n  elif cell_name == \'evol_net_h\' or cell_name == \'amoeba_net_b\':\n    operations = [\'1x1\', \'max_pool_3x3\', \'none\', \'separable_3x3_2\', \'1x1\',\n                  \'separable_3x3_2\', \'1x1\', \'none\', \'avg_pool_3x3\', \'1x1\']\n    hiddenstate_indices = [1, 1, 1, 0, 1, 0, 2, 2, 1, 5]\n    used_hiddenstates = [0, 1, 1, 0, 0, 1, 0]\n  elif cell_name == \'evol_net_a\' or cell_name == \'amoeba_net_c\':\n    operations = [\'avg_pool_3x3\', \'separable_3x3_2\', \'none\', \'separable_3x3_2\',\n                  \'avg_pool_3x3\', \'separable_3x3_2\', \'none\', \'separable_3x3_2\',\n                  \'avg_pool_3x3\', \'separable_3x3_2\']\n    hiddenstate_indices = [0, 0, 0, 0, 2, 1, 0, 1, 3, 0]\n    used_hiddenstates = [1, 0, 0, 1, 0, 0, 0]\n  elif cell_name == \'evol_net_x\' or cell_name == \'amoeba_net_d\':\n    operations = [\'1x1\', \'max_pool_3x3\', \'none\', \'1x7_7x1\', \'1x1\', \'1x7_7x1\',\n                  \'max_pool_3x3\', \'none\', \'avg_pool_3x3\', \'1x1\']\n    hiddenstate_indices = [1, 1, 1, 0, 0, 0, 2, 2, 1, 5]\n    used_hiddenstates = [0, 1, 1, 0, 0, 1, 0]\n  else:\n    raise ValueError(\'Unsupported cell name: %s.\' % cell_name)\n  return operations, hiddenstate_indices, used_hiddenstates\n\n\ndef get_reduction_cell(cell_name):\n  """"""Return reduction cell spec.""""""\n  operations = []\n  hiddenstate_indices = []\n  used_hiddenstates = []\n  if cell_name == \'evol_net_g\' or cell_name == \'amoeba_net_a\':\n    operations = [\'separable_3x3_2\', \'avg_pool_3x3\', \'max_pool_3x3\',\n                  \'separable_7x7_2\', \'max_pool_3x3\', \'max_pool_3x3\',\n                  \'separable_3x3_2\', \'1x7_7x1\', \'avg_pool_3x3\',\n                  \'separable_7x7_2\']\n    hiddenstate_indices = [1, 0, 0, 2, 1, 0, 4, 0, 1, 0]\n    used_hiddenstates = [1, 1, 0, 0, 0, 0, 0]\n  elif cell_name == \'evol_net_h\' or cell_name == \'amoeba_net_b\':\n    operations = [\'max_pool_2x2\', \'max_pool_3x3\', \'none\', \'3x3\',\n                  \'dil_2_separable_5x5_2\', \'max_pool_3x3\', \'none\',\n                  \'separable_3x3_2\', \'avg_pool_3x3\', \'1x1\']\n    hiddenstate_indices = [0, 0, 2, 1, 2, 2, 3, 1, 4, 3]\n    used_hiddenstates = [1, 1, 1, 1, 1, 0, 0]\n  elif cell_name == \'evol_net_a\' or cell_name == \'amoeba_net_c\':\n    operations = [\'max_pool_3x3\', \'max_pool_3x3\', \'separable_7x7_2\',\n                  \'separable_3x3_2\', \'separable_7x7_2\', \'max_pool_3x3\',\n                  \'separable_5x5_2\', \'separable_5x5_2\', \'max_pool_3x3\',\n                  \'separable_3x3_2\']\n    hiddenstate_indices = [0, 0, 2, 0, 0, 1, 4, 4, 1, 1]\n    used_hiddenstates = [0, 1, 0, 0, 0, 0, 0]\n  elif cell_name == \'evol_net_x\' or cell_name == \'amoeba_net_d\':\n    operations = [\'max_pool_2x2\', \'max_pool_3x3\', \'none\', \'3x3\', \'1x7_7x1\',\n                  \'max_pool_3x3\', \'none\', \'max_pool_2x2\', \'avg_pool_3x3\',\n                  \'1x1\']\n    hiddenstate_indices = [0, 0, 2, 1, 2, 2, 3, 1, 2, 3]\n    used_hiddenstates = [1, 1, 1, 1, 0, 0, 0]\n  else:\n    raise ValueError(\'Unsupported cell name: %s.\' % cell_name)\n  return operations, hiddenstate_indices, used_hiddenstates\n'"
tpu/models/official/amoeba_net/network_utils.py,82,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A custom module for some common operations used by AmoebaNet.\n\nFunctions exposed in this file:\n- bp16_getter\n- calc_reduction_layers\n- get_channel_index\n- get_channel_dim\n- global_avg_pool\n- factorized_reduction\n- drop_path\n\nClasses exposed in this file:\n- BaseCell\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib.tpu.python.ops import tpu_ops\nfrom tensorflow.python.training import moving_averages\n\narg_scope = tf.contrib.framework.arg_scope\nslim = tf.contrib.slim\n\nDATA_FORMAT_NCHW = \'NCHW\'\nDATA_FORMAT_NHWC = \'NHWC\'\nINVALID = \'null\'\n\n\ndef cross_replica_average(inputs, num_shards, distributed_group_size):\n  """"""Calculates the average value of inputs tensor across TPU replicas.""""""\n  group_assignment = None\n  if num_shards is not None and distributed_group_size != num_shards:\n    group_size = distributed_group_size\n    group_assignment = []\n    for g in range(num_shards // group_size):\n      replica_ids = [g * group_size + i for i in range(group_size)]\n      group_assignment.append(replica_ids)\n\n  return tpu_ops.cross_replica_sum(inputs, group_assignment) / tf.cast(\n      distributed_group_size, inputs.dtype)\n\n\ndef bp16_getter(getter, *args, **kwargs):\n  """"""Returns a custom getter that this class\'s methods must be called.""""""\n  cast_to_bfloat16 = False\n  requested_dtype = kwargs[\'dtype\']\n  if requested_dtype == tf.bfloat16:\n    # Keep a master copy of weights in fp32 and cast to bp16 when the weights\n    # are used.\n    kwargs[\'dtype\'] = tf.float32\n    cast_to_bfloat16 = True\n  var = getter(*args, **kwargs)\n  # This if statement is needed to guard the cast, because batch norm\n  # assigns directly to the return value of this custom getter. The cast\n  # makes the return value not a variable so it cannot be assigned. Batch\n  # norm variables are always in fp32 so this if statement is never\n  # triggered for them.\n  if cast_to_bfloat16:\n    var = tf.cast(var, tf.bfloat16)\n  return var\n\n\ndef calc_reduction_layers(num_cells, num_reduction_layers):\n  """"""Figure out what layers should have reductions.""""""\n  reduction_layers = []\n  for pool_num in range(1, num_reduction_layers + 1):\n    layer_num = (float(pool_num) / (num_reduction_layers + 1)) * num_cells\n    layer_num = int(layer_num)\n    reduction_layers.append(layer_num)\n  return reduction_layers\n\n\n@tf.contrib.framework.add_arg_scope\ndef get_channel_index(data_format=INVALID):\n  assert data_format != INVALID\n  axis = 3 if data_format == \'NHWC\' else 1\n  return axis\n\n\n@tf.contrib.framework.add_arg_scope\ndef get_channel_dim(shape, data_format=INVALID):\n  assert data_format != INVALID\n  assert len(shape) == 4\n  if data_format == \'NHWC\':\n    return int(shape[3])\n  elif data_format == \'NCHW\':\n    return int(shape[1])\n  else:\n    raise ValueError(\'Not a valid data_format\', data_format)\n\n\n@tf.contrib.framework.add_arg_scope\ndef global_avg_pool(x, data_format=INVALID):\n  """"""Average pool away the height and width spatial dimensions of x.""""""\n  assert data_format != INVALID\n  assert data_format in [\'NHWC\', \'NCHW\']\n  assert x.shape.ndims == 4\n  if data_format == \'NHWC\':\n    return tf.reduce_mean(x, [1, 2])\n  else:\n    return tf.reduce_mean(x, [2, 3])\n\n\n@tf.contrib.framework.add_arg_scope\ndef factorized_reduction(net, output_filters, stride, data_format=INVALID):\n  """"""Reduces the shape of net without information loss due to striding.""""""\n  assert output_filters % 2 == 0, (\n      \'Need even number of filters when using this factorized reduction.\')\n  assert data_format != INVALID\n  if stride == 1:\n    net = slim.conv2d(net, output_filters, 1, scope=\'path_conv\')\n    net = batch_norm(net, scope=\'path_bn\')\n    return net\n  if data_format == \'NHWC\':\n    stride_spec = [1, stride, stride, 1]\n  else:\n    stride_spec = [1, 1, stride, stride]\n\n  # Skip path 1\n  path1 = tf.nn.avg_pool(\n      net, [1, 1, 1, 1], stride_spec, \'VALID\', data_format=data_format)\n  path1 = slim.conv2d(path1, int(output_filters / 2), 1, scope=\'path1_conv\')\n\n  # Skip path 2\n  # First pad with 0\'s on the right and bottom, then shift the filter to\n  # include those 0\'s that were added.\n  if data_format == \'NHWC\':\n    pad_arr = [[0, 0], [0, 1], [0, 1], [0, 0]]\n    path2 = tf.pad(net, pad_arr)[:, 1:, 1:, :]\n    concat_axis = 3\n  else:\n    pad_arr = [[0, 0], [0, 0], [0, 1], [0, 1]]\n    path2 = tf.pad(net, pad_arr)[:, :, 1:, 1:]\n    concat_axis = 1\n\n  path2 = tf.nn.avg_pool(\n      path2, [1, 1, 1, 1], stride_spec, \'VALID\', data_format=data_format)\n  path2 = slim.conv2d(path2, int(output_filters / 2), 1, scope=\'path2_conv\')\n\n  # Concat and apply BN\n  final_path = tf.concat(values=[path1, path2], axis=concat_axis)\n  final_path = batch_norm(final_path, scope=\'final_path_bn\')\n  return final_path\n\n\n@tf.contrib.framework.add_arg_scope\ndef drop_path(net, keep_prob, is_training=True):\n  """"""Drops out a whole example hiddenstate with the specified probability.""""""\n  if is_training:\n    batch_size = tf.shape(net)[0]\n    noise_shape = [batch_size, 1, 1, 1]\n    keep_prob = tf.cast(keep_prob, dtype=net.dtype)\n    random_tensor = keep_prob\n    random_tensor += tf.random_uniform(noise_shape, dtype=net.dtype)\n    binary_tensor = tf.floor(random_tensor)\n    net = tf.div(net, keep_prob) * binary_tensor\n  return net\n\n\ndef _operation_to_filter_shape(operation):\n  splitted_operation = operation.split(\'x\')\n  filter_shape = int(splitted_operation[0][-1])\n  assert filter_shape == int(\n      splitted_operation[1][0]), \'Rectangular filters not supported.\'\n  return filter_shape\n\n\ndef _operation_to_num_layers(operation):\n  splitted_operation = operation.split(\'_\')\n  if \'x\' in splitted_operation[-1]:\n    return 1\n  return int(splitted_operation[-1])\n\n\ndef _operation_to_info(operation):\n  """"""Takes in operation name and returns meta information.\n\n  An example would be \'separable_3x3_4\' -> (3, 4).\n\n  Args:\n    operation: String that corresponds to convolution operation.\n\n  Returns:\n    Tuple of (filter shape, num layers).\n  """"""\n  num_layers = _operation_to_num_layers(operation)\n  filter_shape = _operation_to_filter_shape(operation)\n  return num_layers, filter_shape\n\n\ndef _stacked_separable_conv(net, stride, operation, filter_size):\n  """"""Takes in an operations and parses it to the correct sep operation.""""""\n  num_layers, kernel_size = _operation_to_info(operation)\n  for layer_num in range(num_layers - 1):\n    net = tf.nn.relu(net)\n    net = slim.separable_conv2d(\n        net,\n        filter_size,\n        kernel_size,\n        depth_multiplier=1,\n        scope=\'separable_{0}x{0}_{1}\'.format(kernel_size, layer_num + 1),\n        stride=stride)\n    net = batch_norm(\n        net, scope=\'bn_sep_{0}x{0}_{1}\'.format(kernel_size, layer_num + 1))\n    stride = 1\n  net = tf.nn.relu(net)\n  net = slim.separable_conv2d(\n      net,\n      filter_size,\n      kernel_size,\n      depth_multiplier=1,\n      scope=\'separable_{0}x{0}_{1}\'.format(kernel_size, num_layers),\n      stride=stride)\n  net = batch_norm(\n      net, scope=\'bn_sep_{0}x{0}_{1}\'.format(kernel_size, num_layers))\n  return net\n\n\ndef _operation_to_pooling_type(operation):\n  """"""Takes in the operation string and returns the pooling type.""""""\n  splitted_operation = operation.split(\'_\')\n  return splitted_operation[0]\n\n\ndef _operation_to_pooling_shape(operation):\n  """"""Takes in the operation string and returns the pooling kernel shape.""""""\n  splitted_operation = operation.split(\'_\')\n  shape = splitted_operation[-1]\n  assert \'x\' in shape\n  filter_height, filter_width = shape.split(\'x\')\n  assert filter_height == filter_width\n  return int(filter_height)\n\n\ndef _operation_to_pooling_info(operation):\n  """"""Parses the pooling operation string to return its type and shape.""""""\n  pooling_type = _operation_to_pooling_type(operation)\n  pooling_shape = _operation_to_pooling_shape(operation)\n  return pooling_type, pooling_shape\n\n\ndef _pooling(net, stride, operation):\n  """"""Parses operation and performs the correct pooling operation on net.""""""\n  padding = \'SAME\'\n  pooling_type, pooling_shape = _operation_to_pooling_info(operation)\n  if pooling_type == \'avg\':\n    net = slim.avg_pool2d(net, pooling_shape, stride=stride, padding=padding)\n  elif pooling_type == \'max\':\n    net = slim.max_pool2d(net, pooling_shape, stride=stride, padding=padding)\n  elif pooling_type == \'min\':\n    net = slim.max_pool2d(-1 * net, pooling_shape, stride=stride,\n                          padding=padding)\n    net = -1 * net\n  else:\n    raise NotImplementedError(\'Unimplemented pooling type: \', pooling_type)\n  return net\n\n\nclass BaseCell(object):\n  """"""Base Cell class that is used as a \'layer\' in image architectures.\n\n  Args:\n    num_conv_filters: The number of filters for each convolution operation.\n    operations: List of operations that are performed in the AmoebaNet Cell in\n      order.\n    used_hiddenstates: Binary array that signals if the hiddenstate was used\n      within the cell. This is used to determine what outputs of the cell\n      should be concatenated together.\n    hiddenstate_indices: Determines what hiddenstates should be combined\n      together with the specified operations to create the AmoebaNet cell.\n  """"""\n\n  def __init__(self, num_conv_filters, operations, used_hiddenstates,\n               hiddenstate_indices, drop_path_keep_prob, total_num_cells,\n               total_training_steps):\n    self._num_conv_filters = num_conv_filters\n    self._operations = operations\n    self._used_hiddenstates = used_hiddenstates\n    self._hiddenstate_indices = hiddenstate_indices\n    self._drop_path_keep_prob = drop_path_keep_prob\n    self._total_num_cells = total_num_cells\n    self._total_training_steps = total_training_steps\n\n  def _reduce_prev_layer(self, prev_layer, curr_layer):\n    """"""Matches dimension of prev_layer to the curr_layer.""""""\n    # Set the prev layer to the current layer if it is none\n    if prev_layer is None:\n      return curr_layer\n    curr_num_filters = self._filter_size\n    prev_num_filters = get_channel_dim(prev_layer.shape)\n    curr_filter_shape = int(curr_layer.shape[2])\n    prev_filter_shape = int(prev_layer.shape[2])\n    if curr_filter_shape != prev_filter_shape:\n      prev_layer = tf.nn.relu(prev_layer)\n      prev_layer = factorized_reduction(\n          prev_layer, curr_num_filters, stride=2)\n    elif curr_num_filters != prev_num_filters:\n      prev_layer = tf.nn.relu(prev_layer)\n      prev_layer = slim.conv2d(\n          prev_layer, curr_num_filters, 1, scope=\'prev_1x1\')\n      prev_layer = batch_norm(prev_layer, scope=\'prev_bn\')\n    return prev_layer\n\n  def _cell_base(self, net, prev_layer):\n    """"""Runs the beginning of the conv cell before the predicted ops are run.""""""\n    num_filters = self._filter_size\n\n    # Check to be sure prev layer stuff is setup correctly\n    prev_layer = self._reduce_prev_layer(prev_layer, net)\n\n    net = tf.nn.relu(net)\n    net = slim.conv2d(net, num_filters, 1, scope=\'1x1\')\n    net = batch_norm(net, scope=\'beginning_bn\')\n    split_axis = get_channel_index()\n    net = tf.split(axis=split_axis, num_or_size_splits=1, value=net)\n    for split in net:\n      assert int(split.shape[split_axis] == int(self._num_conv_filters *\n                                                self._filter_scaling))\n    net.append(prev_layer)\n    return net\n\n  def __call__(self, net, scope=None, filter_scaling=1, stride=1,\n               prev_layer=None, cell_num=-1):\n    """"""Runs the conv cell.""""""\n    self._cell_num = cell_num\n    self._filter_scaling = filter_scaling\n    self._filter_size = int(self._num_conv_filters * filter_scaling)\n\n    i = 0\n    with tf.variable_scope(scope, custom_getter=bp16_getter):\n      net = self._cell_base(net, prev_layer)\n      for iteration in range(5):\n        with tf.variable_scope(\'comb_iter_{}\'.format(iteration)):\n          left_hiddenstate_idx, right_hiddenstate_idx = (\n              self._hiddenstate_indices[i],\n              self._hiddenstate_indices[i + 1])\n          original_input_left = left_hiddenstate_idx < 2\n          original_input_right = right_hiddenstate_idx < 2\n          h1 = net[left_hiddenstate_idx]\n          h2 = net[right_hiddenstate_idx]\n\n          operation_left = self._operations[i]\n          operation_right = self._operations[i+1]\n          i += 2\n          # Apply conv operations\n          with tf.variable_scope(\'left\'):\n            h1 = self._apply_operation(h1, operation_left,\n                                       stride, original_input_left)\n          with tf.variable_scope(\'right\'):\n            h2 = self._apply_operation(h2, operation_right,\n                                       stride, original_input_right)\n\n          # Combine hidden states using \'add\'.\n          with tf.variable_scope(\'combine\'):\n            h = h1 + h2\n\n          # Add hiddenstate to the list of hiddenstates we can choose from\n          net.append(h)\n\n      with tf.variable_scope(\'cell_output\'):\n        net = self._combine_unused_states(net)\n\n      return net\n\n  def _apply_conv_operation(self, net, operation, stride, filter_size):\n    """"""Takes in a hiddenstate and applies an operation to it.\n\n    Args:\n      net: This is a hiddenstate from the network that will have an operation\n        applied to it.\n      operation: This is a string that specifies what operations should be\n        applied to net.\n      stride: Stride for the operations being passed in.\n      filter_size: Number of filters output from this operation.\n\n    Returns:\n      The hiddenstate net after it had the operation passed in applied to it.\n    """"""\n\n    if operation == \'1x1\':\n      net = slim.conv2d(net, filter_size, 1, scope=\'1x1\')\n    elif operation == \'3x3\':\n      net = slim.conv2d(net, filter_size, 3, scope=\'3x3\', stride=stride)\n    elif operation == \'1x7_7x1\':\n      net = slim.conv2d(net, filter_size, [1, 7], scope=\'1x7\',\n                        stride=[1, stride])\n      net = batch_norm(net, scope=\'bn_1x7_7x1\')\n      net = tf.nn.relu(net)\n      net = slim.conv2d(net, filter_size, [7, 1], scope=\'7x1\',\n                        stride=[stride, 1])\n    elif operation == \'1x3_3x1\':\n      net = slim.conv2d(net, filter_size, [1, 3], scope=\'1x3\',\n                        stride=[1, stride])\n      net = batch_norm(net, scope=\'bn_1x3_3x1\')\n      net = tf.nn.relu(net)\n      net = slim.conv2d(net, filter_size, [3, 1], scope=\'3x1\',\n                        stride=[stride, 1])\n    elif operation in [\'dilated_3x3_rate_2\', \'dilated_3x3_rate_4\',\n                       \'dilated_3x3_rate_6\']:\n      dilation_rate = int(operation.split(\'_\')[-1])\n      net = slim.conv2d(\n          net,\n          filter_size,\n          3,\n          rate=dilation_rate,\n          stride=stride,\n          scope=\'dilated_3x3\')\n    else:\n      raise NotImplementedError(\'Unimplemented conv operation: \', operation)\n    return net\n\n  def _apply_operation(self, net, operation,\n                       stride, is_from_original_input):\n    """"""Applies the predicted conv operation to net.""""""\n    # Dont stride if this is not one of the original hiddenstates\n    if stride > 1 and not is_from_original_input:\n      stride = 1\n    input_filters = get_channel_dim(net.shape)\n    filter_size = self._filter_size\n    if \'separable\' in operation:\n      net = _stacked_separable_conv(net, stride, operation, filter_size)\n    elif operation in [\'dilated_3x3_rate_2\', \'dilated_3x3_rate_4\',\n                       \'dilated_3x3_rate_6\', \'3x3\', \'1x7_7x1\', \'1x3_3x1\']:\n      if operation == \'1x3_3x1\':\n        reduced_filter_size = int(3 * filter_size / 8)\n      else:\n        reduced_filter_size = int(filter_size / 4)\n      if reduced_filter_size < 1:\n        # If the intermediate number of channels would be too small, then don\'t\n        # use a bottleneck layer.\n        net = tf.nn.relu(net)\n        net = self._apply_conv_operation(net, operation, stride, filter_size)\n        net = batch_norm(net, scope=\'bn\')\n      else:\n        # Use a bottleneck layer.\n        net = tf.nn.relu(net)\n        net = slim.conv2d(net, reduced_filter_size, 1, scope=\'pre_1x1\')\n        net = batch_norm(net, scope=\'pre_bn\')\n        net = tf.nn.relu(net)\n        net = self._apply_conv_operation(\n            net, operation, stride, reduced_filter_size)\n        net = batch_norm(net, scope=\'bn\')\n        net = tf.nn.relu(net)\n        net = slim.conv2d(net, filter_size, 1, scope=\'post_1x1\')\n        net = batch_norm(net, scope=\'post_bn\')\n    elif operation in [\'none\', \'1x1\']:\n      # Check if a stride is needed, then use a strided 1x1 here\n      if stride > 1 or operation == \'1x1\' or (input_filters != filter_size):\n        net = tf.nn.relu(net)\n        net = slim.conv2d(net, filter_size, 1, stride=stride, scope=\'1x1\')\n        net = batch_norm(net, scope=\'bn_1\')\n    elif \'pool\' in operation:\n      net = _pooling(net, stride, operation)\n      if input_filters != filter_size:\n        net = slim.conv2d(net, filter_size, 1, stride=1, scope=\'1x1\')\n        net = batch_norm(net, scope=\'bn_1\')\n    else:\n      raise ValueError(\'Unimplemented operation\', operation)\n\n    if operation != \'none\':\n      net = self._apply_drop_path(net)\n\n    tf.logging.info(\'Net shape after {}: {}\'.format(operation, net.shape))\n    return net\n\n  def _combine_unused_states(self, net):\n    """"""Concatenate the unused hidden states of the cell.""""""\n    used_hiddenstates = self._used_hiddenstates\n\n    final_height = int(net[-1].shape[2])\n    final_num_filters = get_channel_dim(net[-1].shape)\n    assert len(used_hiddenstates) == len(net)\n    for idx, used_h in enumerate(used_hiddenstates):\n      curr_height = int(net[idx].shape[2])\n      curr_num_filters = get_channel_dim(net[idx].shape)\n\n      # Determine if a reduction should be applied to make the number of\n      # filters match.\n      should_reduce = final_num_filters != curr_num_filters\n      should_reduce = (final_height != curr_height) or should_reduce\n      should_reduce = should_reduce and not used_h\n      if should_reduce:\n        stride = 2 if final_height != curr_height else 1\n        with tf.variable_scope(\'reduction_{}\'.format(idx)):\n          net[idx] = factorized_reduction(\n              net[idx], final_num_filters, stride)\n\n    states_to_combine = (\n        [h for h, is_used in zip(net, used_hiddenstates) if not is_used])\n\n    # Return the concat of all the states\n    concat_axis = get_channel_index()\n    net = tf.concat(values=states_to_combine, axis=concat_axis)\n    return net\n\n  @tf.contrib.framework.add_arg_scope  # No public API. For internal use only.\n  def _apply_drop_path(self, net, current_step=None,\n                       drop_connect_version=\'v1\'):\n    """"""Apply drop_path regularization.\n\n    Args:\n      net: the Tensor that gets drop_path regularization applied.\n      current_step: a float32 Tensor with the current global_step value,\n        to be divided by hparams.total_training_steps. Usually None, which\n        defaults to tf.train.get_or_create_global_step() properly casted.\n      drop_connect_version: one of \'v1\', \'v2\', \'v3\', controlling whether\n        the dropout rate is scaled by current_step (v1), layer (v2), or\n        both (v1, the default).\n\n    Returns:\n      The dropped-out value of `net`.\n    """"""\n    drop_path_keep_prob = self._drop_path_keep_prob\n    if drop_path_keep_prob < 1.0:\n      assert drop_connect_version in [\'v1\', \'v2\', \'v3\']\n      if drop_connect_version in [\'v2\', \'v3\']:\n        # Scale keep prob by layer number\n        assert self._cell_num != -1\n        # The added 2 is for the reduction cells\n        num_cells = self._total_num_cells\n        layer_ratio = (self._cell_num + 1)/float(num_cells)\n        drop_path_keep_prob = 1 - layer_ratio * (1 - drop_path_keep_prob)\n      if drop_connect_version in [\'v1\', \'v3\']:\n        # Decrease the keep probability over time\n        if not current_step:\n          current_step = tf.cast(tf.train.get_or_create_global_step(),\n                                 tf.float32)\n        drop_path_burn_in_steps = self._total_training_steps\n        current_ratio = current_step / drop_path_burn_in_steps\n        current_ratio = tf.minimum(1.0, current_ratio)\n        drop_path_keep_prob = (1 - current_ratio * (1 - drop_path_keep_prob))\n      net = drop_path(net, drop_path_keep_prob)\n    return net\n\n\n# TODO(huangyp): find out the difference and use the layers batch_norm.\n@tf.contrib.framework.add_arg_scope\ndef batch_norm(inputs,\n               decay=0.999,\n               center=True,\n               scale=False,\n               epsilon=0.001,\n               moving_vars=\'moving_vars\',\n               activation_fn=None,\n               is_training=True,\n               data_format=\'NHWC\',\n               reuse=None,\n               num_shards=None,\n               distributed_group_size=1,\n               scope=None):\n  """"""Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167.\n\n    ""Batch Normalization: Accelerating Deep Network Training by Reducing\n    Internal Covariate Shift""\n\n    Sergey Ioffe, Christian Szegedy\n\n  Can be used as a normalizer function for conv2d and fully_connected.\n\n  Note: When is_training is True the moving_mean and moving_variance need to be\n  updated, by default the update_ops are placed in `tf.GraphKeys.UPDATE_OPS` so\n  they need to be added as a dependency to the `train_op`, example:\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    if update_ops:\n      updates = tf.group(*update_ops)\n      total_loss = control_flow_ops.with_dependencies([updates], total_loss)\n\n  One can set updates_collections=None to force the updates in place, but that\n  can have speed penalty, especially in distributed settings.\n\n  Args:\n    inputs: A tensor with 2 or more dimensions, where the first dimension has\n      `batch_size`. The normalization is over all but the last dimension if\n      `data_format` is `NHWC` and the second dimension if `data_format` is\n      `NCHW`.\n    decay: Decay for the moving average. Reasonable values for `decay` are close\n      to 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc.\n      Lower `decay` value (recommend trying `decay`=0.9) if model experiences\n      reasonably good training performance but poor validation and/or test\n      performance.\n    center: If True, add offset of `beta` to normalized tensor.  If False,\n      `beta` is ignored.\n    scale: If True, multiply by `gamma`. If False, `gamma` is\n      not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n      disabled since the scaling can be done by the next layer.\n    epsilon: Small float added to variance to avoid dividing by zero.\n    moving_vars: Name of collection created for moving variables.\n    activation_fn: Activation function, default set to None to skip it and\n      maintain a linear activation.\n    is_training: Whether or not the layer is in training mode. In training mode\n      it would accumulate the statistics of the moments into `moving_mean` and\n      `moving_variance` using an exponential moving average with the given\n      `decay`. When it is not in training mode then it would use the values of\n      the `moving_mean` and the `moving_variance`.\n    data_format: input data format. NHWC or NCHW\n    reuse: Whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    num_shards: Number of shards that participate in the global\n      reduction. Default is set to None, that will skip the cross replica sum in\n      and normalize across local examples only.\n    distributed_group_size: Number of replicas to normalize across in the\n      distributed batch normalization.\n    scope: Optional scope for `variable_scope`.\n\n  Returns:\n    A `Tensor` representing the output of the operation.\n\n  Raises:\n    ValueError: If the rank of `inputs` is undefined.\n    ValueError: If the rank of `inputs` is neither 2 or 4.\n    ValueError: If rank or `C` dimension of `inputs` is undefined.\n  """"""\n  trainable = True\n\n  with tf.variable_scope(scope, \'BatchNorm\', [inputs], reuse=reuse):\n    inputs = tf.convert_to_tensor(inputs)\n    original_shape = inputs.get_shape()\n    original_rank = original_shape.ndims\n    if original_rank is None:\n      raise ValueError(\'Inputs %s has undefined rank\' % inputs.name)\n    elif original_rank not in [2, 4]:\n      raise ValueError(\'Inputs %s has unsupported rank.\'\n                       \' Expected 2 or 4 but got %d\' % (inputs.name,\n                                                        original_rank))\n    if original_rank == 2:\n      channels = inputs.get_shape()[-1].value\n      if channels is None:\n        raise ValueError(\'`C` dimension must be known but is None\')\n      new_shape = [-1, 1, 1, channels]\n      if data_format == \'NCHW\':\n        new_shape = [-1, channels, 1, 1]\n      inputs = tf.reshape(inputs, new_shape)\n    inputs_shape = inputs.get_shape()\n    if data_format == \'NHWC\':\n      params_shape = inputs_shape[-1:]\n    else:\n      params_shape = inputs_shape[1:2]\n    if not params_shape.is_fully_defined():\n      raise ValueError(\'Inputs %s has undefined `C` dimension %s.\' %\n                       (inputs.name, params_shape))\n\n    # Allocate parameters for the beta and gamma of the normalization.\n    trainable_beta = trainable and center\n    collections = [tf.GraphKeys.MODEL_VARIABLES, tf.GraphKeys.GLOBAL_VARIABLES]\n    beta = tf.contrib.framework.variable(\n        \'beta\',\n        params_shape,\n        collections=collections,\n        initializer=tf.zeros_initializer(),\n        trainable=trainable_beta)\n    trainable_gamma = trainable and scale\n    gamma = tf.contrib.framework.variable(\n        \'gamma\',\n        params_shape,\n        collections=collections,\n        initializer=tf.ones_initializer(),\n        trainable=trainable_gamma)\n\n    # Create moving_mean and moving_variance variables and add them to the\n    # appropiate collections.\n    moving_collections = [moving_vars,\n                          tf.GraphKeys.MOVING_AVERAGE_VARIABLES,\n                          tf.GraphKeys.MODEL_VARIABLES,\n                          tf.GraphKeys.GLOBAL_VARIABLES]\n    # Disable partition setting for moving_mean and moving_variance\n    # as assign_moving_average op below doesn\'t support partitioned variable.\n    scope = tf.get_variable_scope()\n    partitioner = scope.partitioner\n    scope.set_partitioner(None)\n    moving_mean = tf.contrib.framework.variable(\n        \'moving_mean\',\n        params_shape,\n        initializer=tf.zeros_initializer(),\n        trainable=False,\n        collections=moving_collections)\n    moving_variance = tf.contrib.framework.variable(\n        \'moving_variance\',\n        params_shape,\n        initializer=tf.ones_initializer(),\n        trainable=False,\n        collections=moving_collections)\n    # Restore scope\'s partitioner setting.\n    scope.set_partitioner(partitioner)\n\n    # Add cross replica sum to do subset mean and variance calculation\n    # First compute mean and variance\n    if is_training:\n      if distributed_group_size > 1:\n        # Execute a distributed batch normalization\n        if data_format == \'NCHW\':\n          axis = 1\n        else:\n          axis = 3\n        input_shape = inputs.get_shape()\n        inputs_dtype = inputs.dtype\n        inputs = tf.cast(inputs, tf.float32)\n        ndims = len(input_shape)\n        reduction_axes = [i for i in range(ndims) if i != axis]\n        counts, mean_ss, variance_ss, _ = tf.nn.sufficient_statistics(\n            inputs, reduction_axes, keep_dims=False)\n        mean_ss = cross_replica_average(mean_ss, num_shards,\n                                        distributed_group_size)\n        variance_ss = cross_replica_average(variance_ss, num_shards,\n                                            distributed_group_size)\n        mean, variance = tf.nn.normalize_moments(\n            counts, mean_ss, variance_ss, shift=None)\n        outputs = tf.nn.batch_normalization(inputs, mean, variance, beta, gamma,\n                                            epsilon)\n        outputs = tf.cast(outputs, inputs_dtype)\n      else:\n        outputs, mean, variance = tf.nn.fused_batch_norm(\n            inputs, gamma, beta, epsilon=epsilon, data_format=data_format)\n    else:\n      outputs, mean, variance = tf.nn.fused_batch_norm(\n          inputs,\n          gamma,\n          beta,\n          mean=moving_mean,\n          variance=moving_variance,\n          epsilon=epsilon,\n          is_training=False,\n          data_format=data_format)\n\n    if is_training:\n      update_moving_mean = moving_averages.assign_moving_average(\n          moving_mean,\n          tf.cast(mean, moving_mean.dtype),\n          decay,\n          zero_debias=False)\n      update_moving_variance = moving_averages.assign_moving_average(\n          moving_variance,\n          tf.cast(variance, moving_variance.dtype),\n          decay,\n          zero_debias=False)\n      tf.add_to_collection(\'update_ops\', update_moving_mean)\n      tf.add_to_collection(\'update_ops\', update_moving_variance)\n\n    outputs.set_shape(inputs_shape)\n    if original_shape.ndims == 2:\n      outputs = tf.reshape(outputs, original_shape)\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return outputs\n'"
tpu/models/official/amoeba_net/network_utils_test.py,3,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.nasnet.nasnet_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport network_utils\n\n\nclass NetworkUtilsTest(tf.test.TestCase):\n\n  def testCalcReductionLayers(self):\n    num_cells = 18\n    num_reduction_layers = 2\n    reduction_layers = network_utils.calc_reduction_layers(\n        num_cells, num_reduction_layers)\n    self.assertEqual(len(reduction_layers), 2)\n    self.assertEqual(reduction_layers[0], 6)\n    self.assertEqual(reduction_layers[1], 12)\n\n  def testGetChannelIndex(self):\n    data_formats = [\'NHWC\', \'NCHW\']\n    for data_format in data_formats:\n      index = network_utils.get_channel_index(data_format)\n      correct_index = 3 if data_format == \'NHWC\' else 1\n      self.assertEqual(index, correct_index)\n\n  def testGetChannelDim(self):\n    data_formats = [\'NHWC\', \'NCHW\']\n    shape = [10, 20, 30, 40]\n    for data_format in data_formats:\n      dim = network_utils.get_channel_dim(shape, data_format)\n      correct_dim = shape[3] if data_format == \'NHWC\' else shape[1]\n      self.assertEqual(dim, correct_dim)\n\n  def testGlobalAvgPool(self):\n    data_formats = [\'NHWC\', \'NCHW\']\n    inputs = tf.placeholder(tf.float32, (5, 10, 20, 10))\n    for data_format in data_formats:\n      output = network_utils.global_avg_pool(\n          inputs, data_format)\n      self.assertEqual(output.shape, [5, 10])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tpu/models/official/amoeba_net/tf_hub.py,33,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Provide functionalities to export and eval tf_hub module.\n\nExample use of export_to_hub:\n\n  tf_hub.py --tf_hub_mode=\'export_to_hub\' --cell_name=amoeba_net_a  \\\n  --reduction_size=448 --num_cells=18 --image_size=331 \\\n  --drop_connect_keep_prob=0.7 \\\n  --export_path=/tmp/module_export \\\n  --model_dir=/ADD_PATH_WITH_1001_CLASSES_HERE \\\n  --alsologtostderr\n\nExample use of eval_from_hub\n  tf_hub.py --tf_hub_mode=\'eval_from_hub\' --cell_name=amoeba_net_a  \\\n  --reduction_size=448 --num_cells=18 --image_size=331 \\\n  --export_path=/tmp/module_export \\\n  --data_dir=/ADD_DATA_PATH_HERE \\\n  --model_dir=/ADD_PATH_WITH_1001_CLASSES_HERE \\\n  --eval_batch_size=40 --alsologtostderr\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\n\n# Standard Imports\nfrom absl import app\nfrom absl import flags\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nimport amoeba_net\nimport amoeba_net_model as model_lib\nimport model_builder\n\nflags.DEFINE_string(\'tf_hub_mode\', \'export_to_hub\',\n                    \'export_to_hub|eval_from_hub\')\n\nflags.DEFINE_string(\'export_path\', None,\n                    \'Directory where export output is stored\')\n\nflags.DEFINE_bool(\n    \'export_feature_vector\', False,\n    \'If true, network builder only returns feature vector after global_pool \'\n    \'without the fully connected layer.\')\n\nflags.DEFINE_bool(\n    \'dryrun_with_untrained_weights\', None,\n    \'FOR TESTING USE ONLY. If set, export_to_hub is done without restoring \'\n    \'the model\\\'s trained weights. This helps test the Python code quickly but \'\n    \'makes the resulting module useless.\')\n\n\nFLAGS = flags.FLAGS\nslim = tf.contrib.slim\n\n\ndef _make_module_fn(hparams, num_classes):\n  """"""Returns a module_fn for use with hub.create_module_spec().""""""\n\n  def _module_fn(is_training):\n    """"""A module_fn for use with hub.create_module_spec().\n\n    Args:\n      is_training: a boolean, passed to the config.network_fn.\n          This is meant to control whether batch norm, dropout etc. are built\n          in training or inference mode for this graph version.\n\n    Raises:\n      ValueError: if network_fn outputs are not as expected.\n    """"""\n    # Set up the module input, and attach an ImageModuleInfo about it.\n    with tf.name_scope(\'hub_input\'):\n      default_size = (hparams.image_size,) * 2\n      image_module_info = hub.ImageModuleInfo()\n      size_info = image_module_info.default_image_size\n      size_info.height, size_info.width = default_size\n      # TODO(b/72731449): Support variable input size.\n      shape = (None,) + default_size + (3,)\n      images = tf.placeholder(dtype=tf.float32, shape=shape, name=\'images\')\n      hub.attach_image_module_info(image_module_info)\n      # The input is expected to have RGB color values in the range [0,1]\n      # and gets converted for AmoebaNet to the Inception-style range [-1,+1].\n      scaled_images = tf.multiply(images, 2.0)\n      scaled_images = tf.subtract(scaled_images, 1.0)\n\n    # Build the net.\n    logits, end_points = model_builder.build_network(scaled_images, num_classes,\n                                                     is_training, hparams)\n\n    with tf.name_scope(\'hub_output\'):\n      # Extract the feature_vectors output.\n      try:\n        feature_vectors = end_points[\'global_pool\']\n      except KeyError:\n        tf.logging.error(\'Valid keys of end_points are:\', \', \'.join(end_points))\n        raise\n      with tf.name_scope(\'feature_vector\'):\n        if feature_vectors.shape.ndims != 2:\n          raise ValueError(\n              \'Wrong rank (expected 2 after squeeze) \'\n              \'in feature_vectors:\', feature_vectors)\n      # Extract the logits output (if applicable).\n      if num_classes:\n        with tf.name_scope(\'classification\'):\n          if logits.shape.ndims != 2:\n            raise ValueError(\'Wrong rank (expected 2) in logits:\', logits)\n\n    # Add named signatures.\n    hub.add_signature(\'image_feature_vector\', dict(images=images),\n                      dict(end_points, default=feature_vectors))\n    if num_classes:\n      hub.add_signature(\'image_classification\', dict(images=images),\n                        dict(end_points, default=logits))\n    # Add the default signature.\n    if num_classes:\n      hub.add_signature(\'default\', dict(images=images), dict(default=logits))\n    else:\n      hub.add_signature(\'default\', dict(images=images),\n                        dict(default=feature_vectors))\n  return _module_fn\n\n\ndef export_to_hub(checkpoint_path, export_path, num_classes, hparams):\n  """"""Exports the network as a TF-Hub Module.\n\n  If a positive integer num_classes is given, a module for image classification\n  is exported. If num_classes is 0 or None, a module for feature vector\n  extraction is exported. In both cases, the default signature returns\n  a default output that matches the Python slim API  net, _ = network_fn(...).\n\n  Args:\n    checkpoint_path: a string with the file name of the checkpoint from which\n        the trained weights are copied into the Module.\n        FOR TESTING USE ONLY, this can be set to empty or None, to skip\n        restoring weights, which ignores the checkpoint and copies the random\n        initializer values of the weights instead.\n    export_path: a string with the directory to pass to hub.Module.export().\n    num_classes: an integer with the number of classes for which the given\n        checkpoint has been trained. If 0 or None, the classification layer\n        is omitted.\n    hparams: hyper parameters.\n  """"""\n  module_fn = _make_module_fn(hparams, num_classes)\n  tags_and_args = [\n      # The default graph is built with batch_norm, dropout etc. in inference\n      # mode. This graph version is good for inference, not training.\n      ([], {\n          \'is_training\': False\n      }),\n      # A separate \'train\' graph builds batch_norm, dropout etc. in training\n      # mode.\n      ([\'train\'], {\n          \'is_training\': True\n      }),\n  ]\n  drop_collections = [\n      \'moving_vars\', tf.GraphKeys.GLOBAL_STEP,\n      tf.GraphKeys.MOVING_AVERAGE_VARIABLES\n  ]\n  spec = hub.create_module_spec(module_fn, tags_and_args, drop_collections)\n\n  with tf.Graph().as_default():\n    module = hub.Module(spec)\n    init_fn = _get_init_fn(\n        module,\n        checkpoint_path,\n        hparams.moving_average_decay > 0,\n        moving_averages_blacklist_regex=\'global_step\')\n    with tf.Session() as session:\n      init_fn(session)\n      module.export(export_path, session=session)\n\n  tf.logging.info(\'Export to {} succeeded.\'.format(export_path))\n\n\ndef _get_init_fn(module,\n                 checkpoint_path,\n                 export_moving_averages=False,\n                 moving_averages_blacklist_regex=None):\n  """"""Returns init_fn for the session that calls hub.Module.export().""""""\n  if not checkpoint_path:\n    tf.logging.warn(\'DRYRUN: using random weight initializers, no checkpoint\')\n    return lambda session: session.run(tf.global_variables_initializer())\n\n  # Build `variables_to_restore` as a map from names in the checkpoint to the\n  # variable in the instantiated module.\n  if export_moving_averages:\n    variables_to_restore = {}\n    num_averaged = num_blacklisted = 0\n    for variable_name, variable in module.variable_map.items():\n      if (moving_averages_blacklist_regex and\n          re.match(moving_averages_blacklist_regex, variable_name)):\n        num_blacklisted += 1\n      else:\n        variable_name += \'/ExponentialMovingAverage\'\n        num_averaged += 1\n      variables_to_restore[variable_name] = variable\n    tf.logging.info(\'Export of moving averages is applied to %d variables \'\n                    \'with %d exempted by matching the blacklist_regex\' %\n                    (num_averaged, num_blacklisted))\n  else:\n    variables_to_restore = module.variable_map\n    tf.logging.info(\'Export of moving averages is disabled\')\n\n  unchecked_init_fn = slim.assign_from_checkpoint_fn(checkpoint_path,\n                                                     variables_to_restore)\n  def init_fn(session):\n    unchecked_init_fn(session)\n    _check_shapes_of_restored_variables(session, variables_to_restore)\n\n  return init_fn\n\n\ndef _check_shapes_of_restored_variables(session, variables_to_restore):\n  """"""Raises TypeError if restored variables have unexpected shapes.""""""\n  num_errors = 0\n  for variable_name, variable in variables_to_restore.items():\n    graph_shape = variable.value().shape\n    # Values are big, but tf.shape(..) whould echo graph_shape if fully defined.\n    checkpoint_shape = session.run(variable.value()).shape\n    if not graph_shape.is_compatible_with(checkpoint_shape):\n      tf.logging.error(\'Shape mismatch for variable %s: \'\n                       \'graph expects %s but checkpoint has %s\' %\n                       (variable_name, graph_shape, checkpoint_shape))\n      num_errors += 1\n  if num_errors:\n    raise TypeError(\n        \'Shape mismatch for %d variables, see error log for list.\' % num_errors)\n\n\ndef _make_model_fn(hub_module_spec):\n  """"""Returns a model_fn for estimator using hub_module.""""""\n\n  def _model_fn(features, labels, mode, params):\n    """"""model_fn for estimator.""""""\n    del params\n    features = tf.transpose(features, [3, 0, 1, 2])  # HWCN to NHWC\n    hub_module = hub.Module(spec=hub_module_spec, trainable=False)\n    logits = hub_module(features)\n    labels_onehot = tf.one_hot(labels, logits.shape[1])\n    loss = tf.losses.softmax_cross_entropy(labels_onehot, logits)\n\n    eval_metric_ops = None\n\n    def metric_fn(labels, logits):\n      """"""Evaluation metric fn. Performed on CPU, do not reference TPU ops.""""""\n      predictions = tf.argmax(logits, axis=1)\n      top_1_accuracy = tf.metrics.accuracy(labels, predictions)\n      in_top_5 = tf.cast(tf.nn.in_top_k(logits, labels, 5), tf.float32)\n      top_5_accuracy = tf.metrics.mean(in_top_5)\n\n      return {\n          \'top_1_accuracy\': top_1_accuracy,\n          \'top_5_accuracy\': top_5_accuracy,\n      }\n\n    eval_metric_ops = metric_fn(labels, logits)\n    return tf.estimator.EstimatorSpec(\n        mode=mode, loss=loss, train_op=None, eval_metric_ops=eval_metric_ops)\n\n  return _model_fn\n\n\ndef eval_from_hub(model_dir, input_fn, eval_steps):\n  """"""Eval using hub module.""""""\n  hub_module_spec = hub.load_module_spec(model_dir)\n  run_config = tf.estimator.RunConfig(model_dir=model_dir)\n  image_classifier = tf.estimator.Estimator(\n      model_fn=_make_model_fn(hub_module_spec), config=run_config, params={})\n  eval_results = image_classifier.evaluate(input_fn=input_fn, steps=eval_steps)\n  tf.logging.info(\'Evaluation results: %s\' % eval_results)\n\n\ndef main(_):\n  mode = FLAGS.tf_hub_mode\n  data_dir = amoeba_net.FLAGS.data_dir\n  model_dir = amoeba_net.FLAGS.model_dir\n  hparams = amoeba_net.build_hparams()\n  hparams.add_hparam(\'drop_path_burn_in_steps\', 0)\n  hparams.set_hparam(\'use_tpu\', False)\n  # So far, there is no standardized way of exposing aux heads for\n  # fine-tuning Hub image modules. Disable aux heads to avoid putting unused\n  # variables and ops into the module.\n  hparams.set_hparam(\'use_aux_head\', False)\n  eval_steps = FLAGS.num_eval_images // FLAGS.eval_batch_size\n  export_path = FLAGS.export_path or (model_dir + \'/export\')\n\n  input_pipeline = model_lib.InputPipeline(\n      is_training=False, data_dir=data_dir, hparams=hparams, eval_from_hub=True)\n\n  if mode == \'eval_from_hub\':\n    eval_from_hub(export_path, input_pipeline.input_fn, eval_steps=eval_steps)\n  elif mode == \'export_to_hub\':\n    num_classes = (None if FLAGS.export_feature_vector else\n                   input_pipeline.num_classes)\n\n    if FLAGS.dryrun_with_untrained_weights:\n      checkpoint_path = None\n    else:\n      checkpoint_path = tf.train.latest_checkpoint(model_dir)\n      if not checkpoint_path:\n        raise IOError(\'No checkpoint found.\')\n    export_to_hub(\n        checkpoint_path, export_path, num_classes, hparams)\n  else:\n    raise ValueError(\'Unsupported tf_hub_mode = {}\'.format(mode))\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  app.run(main)\n'"
tpu/models/official/densenet/densenet_imagenet.py,69,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""DenseNet implementation with TPU support.\n\nOriginal paper: (https://arxiv.org/abs/1608.06993)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom absl import flags\nimport absl.logging as _logging  # pylint: disable=unused-import\nimport tensorflow as tf\n\nimport densenet_model\nimport vgg_preprocessing\nfrom tensorflow.contrib.training.python.training import evaluation\n\nFLAGS = flags.FLAGS\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'tpu\', default=None,\n    help=\'The Cloud TPU to use for training. This should be either the name \'\n    \'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\')\nflags.DEFINE_string(\n    ""gcp_project"", default=None,\n    help=""Project name for the Cloud TPU-enabled project. If not specified, we ""\n    ""will attempt to automatically detect the GCE project from metadata."")\nflags.DEFINE_string(\n    ""tpu_zone"", default=None,\n    help=""GCE zone where the Cloud TPU is located in. If not specified, we ""\n    ""will attempt to automatically detect the GCE project from metadata."")\n\n# Model specific paramenters\nflags.DEFINE_string(\n    ""data_dir"",\n    default="""",\n    help=""The directory where the ImageNet input data is stored."")\n\nflags.DEFINE_string(\n    ""model_dir"",\n    default="""",\n    help=""The directory where the model will be stored."")\n\nflags.DEFINE_integer(\n    ""train_batch_size"", default=1024, help=""Batch size for training."")\n\nflags.DEFINE_integer(\n    ""eval_batch_size"", default=1024, help=""Batch size for evaluation."")\n\nflags.DEFINE_integer(\n    ""num_shards"", default=8, help=""Number of shards (TPU cores)."")\n\nflags.DEFINE_integer(\n    ""iterations_per_loop"",\n    default=None,\n    help=(""Number of interior TPU cycles to run before returning to the host. ""\n          ""This is different from the number of steps run before each eval ""\n          ""and should primarily be used only if you need more incremental ""\n          ""logging during training. Setting this to None (default) will ""\n          ""set the iterations_per_loop to be as large as possible (i.e. ""\n          ""perform every call to train in a single TPU loop.""))\n\nflags.DEFINE_integer(\n    ""prefetch_dataset_buffer_size"", 8 * 1024 * 1024,\n    ""Number of bytes prefetched in read buffer. 0 means no buffering."")\n\nflags.DEFINE_integer(""num_files_infeed"", 8,\n                     ""Number of training files to read in parallel."")\n\nflags.DEFINE_integer(""shuffle_buffer_size"", 1000,\n                     ""Size of the shuffle buffer used to randomize ordering"")\n\n# For mode=train and mode=train_and_eval\nflags.DEFINE_integer(\n    ""steps_per_checkpoint"",\n    default=1000,\n    help=(""Controls how often checkpoints are generated. More steps per ""\n          ""checkpoint = higher utilization of TPU and generally higher ""\n          ""steps/sec""))\n\n# For mode=eval\nflags.DEFINE_integer(\n    ""min_eval_interval"",\n    default=180,\n    help=""Minimum seconds between evaluations."")\n\n# For mode=eval\nflags.DEFINE_integer(\n    ""eval_timeout"",\n    default=None,\n    help=""Maximum seconds between checkpoints before evaluation terminates."")\n\nflags.DEFINE_integer(\n    ""network_depth"",\n    default=121,\n    help=""Number of levels in the Densenet network"")\n\nflags.DEFINE_integer(\n    ""train_steps"",\n    default=130000,  # Roughly 100 epochs\n    help=""The number of steps to use for training."")\n\n# For mode=train_and_eval, evaluation occurs at each steps_per_checkpoint\n# Note: independently of steps_per_checkpoint, estimator will save the most\n# recent checkpoint every 10 minutes by default for train_and_eval\nflags.DEFINE_string(\n    ""mode"",\n    default=""train_and_eval"",\n    help=(""Mode to run: train, eval, train_and_eval ""\n          ""(default, interleaved train & eval).""))\n\n# Dataset constants\n_LABEL_CLASSES = 1001\n_NUM_CHANNELS = 3\n_NUM_TRAIN_IMAGES = 1281167\n_NUM_EVAL_IMAGES = 50000\n_MOMENTUM = 0.9\n_WEIGHT_DECAY = 1e-4\n\n# Learning hyperaparmeters\n_BASE_LR = 0.1\n_LR_SCHEDULE = [  # (LR multiplier, epoch to start)\n    (1.0 / 6, 0), (2.0 / 6, 1), (3.0 / 6, 2), (4.0 / 6, 3), (5.0 / 6, 4),\n    (1.0, 5), (0.1, 30), (0.01, 60), (0.001, 80), (0.0001, 90)\n]\n\n\ndef learning_rate_schedule(current_epoch):\n  """"""Handles linear scaling rule, gradual warmup, and LR decay.""""""\n  scaled_lr = _BASE_LR * (FLAGS.train_batch_size / 256.0)\n\n  decay_rate = scaled_lr\n  for mult, start_epoch in _LR_SCHEDULE:\n    decay_rate = tf.where(current_epoch < start_epoch, decay_rate,\n                          scaled_lr * mult)\n\n  return decay_rate\n\n\nclass ImageNetInput(object):\n  """"""Wrapper class that acts as the input_fn to TPUEstimator.""""""\n\n  def __init__(self, is_training, data_dir=None):\n    self.is_training = is_training\n    if data_dir:\n      self.data_dir = data_dir\n    elif FLAGS.data_dir:\n      self.data_dir = FLAGS.data_dir\n    else:\n      self.data_dir = None\n\n  def dataset_parser(self, value):\n    """"""Parse an Imagenet record from value.""""""\n    keys_to_features = {\n        ""image/encoded"": tf.FixedLenFeature((), tf.string, """"),\n        ""image/format"": tf.FixedLenFeature((), tf.string, ""jpeg""),\n        ""image/class/label"": tf.FixedLenFeature([], tf.int64, -1),\n        ""image/class/text"": tf.FixedLenFeature([], tf.string, """"),\n        ""image/object/bbox/xmin"": tf.VarLenFeature(dtype=tf.float32),\n        ""image/object/bbox/ymin"": tf.VarLenFeature(dtype=tf.float32),\n        ""image/object/bbox/xmax"": tf.VarLenFeature(dtype=tf.float32),\n        ""image/object/bbox/ymax"": tf.VarLenFeature(dtype=tf.float32),\n        ""image/object/class/label"": tf.VarLenFeature(dtype=tf.int64),\n    }\n\n    parsed = tf.parse_single_example(value, keys_to_features)\n\n    image = tf.image.decode_image(\n        tf.reshape(parsed[""image/encoded""], shape=[]), _NUM_CHANNELS)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    # TODO(shivaniagrawal): height and width of image from model\n    image = vgg_preprocessing.preprocess_image(\n        image=image,\n        output_height=224,\n        output_width=224,\n        is_training=self.is_training)\n\n    label = tf.cast(\n        tf.reshape(parsed[""image/class/label""], shape=[]), dtype=tf.int32)\n\n    return image, tf.one_hot(label, _LABEL_CLASSES)\n\n  def __call__(self, params):\n    """"""Input function which provides a single batch for train or eval.""""""\n    if self.data_dir is None:\n      tf.logging.info(\'Using fake input.\')\n      return self._input_fn_null(params)\n\n    # Retrieves the batch size for the current shard. The # of shards is\n    # computed according to the input pipeline deployment. See\n    # `tf.contrib.tpu.RunConfig` for details.\n    batch_size = params[""batch_size""]\n\n    # Shuffle the filenames to ensure better randomization\n    file_pattern = os.path.join(self.data_dir, ""train-*""\n                                if self.is_training else ""validation-*"")\n    dataset = tf.data.Dataset.list_files(file_pattern, shuffle=False)\n    if self.is_training:\n      dataset = dataset.shuffle(buffer_size=1024)  # 1024 files in dataset\n\n    if self.is_training:\n      dataset = dataset.repeat()\n\n    def prefetch_dataset(filename):\n      buffer_size = FLAGS.prefetch_dataset_buffer_size\n      dataset = tf.data.TFRecordDataset(filename, buffer_size=buffer_size)\n      return dataset\n\n    dataset = dataset.apply(\n        tf.contrib.data.parallel_interleave(\n            prefetch_dataset, cycle_length=FLAGS.num_files_infeed, sloppy=True))\n    dataset = dataset.shuffle(FLAGS.shuffle_buffer_size)\n\n    dataset = dataset.map(self.dataset_parser, num_parallel_calls=128)\n    dataset = dataset.prefetch(batch_size)\n    dataset = dataset.apply(\n        tf.contrib.data.batch_and_drop_remainder(batch_size))\n    dataset = dataset.prefetch(2)  # Prefetch overlaps in-feed with training\n    return dataset\n\n  def _input_fn_null(self, params):\n    """"""Input function which provides null (black) images.""""""\n    batch_size = params[""batch_size""]\n    null_image = tf.zeros([224, 224, 3], tf.float32)\n    null_label = tf.one_hot(tf.constant(0, tf.int32), _LABEL_CLASSES)\n    dataset = tf.data.Dataset.from_tensors((null_image, null_label))\n    dataset = dataset.repeat(batch_size).batch(batch_size, drop_remainder=True)\n    dataset = dataset.take(1).cache().repeat()\n    tf.logging.info(""Input dataset: %s"", str(dataset))\n    return dataset\n\n\ndef model_fn(features, labels, mode, params):\n  """"""Our model_fn for Densenet to be used with our Estimator.""""""\n  tf.logging.info(""model_fn"")\n\n  if FLAGS.network_depth == 169:\n    logits = densenet_model.densenet_imagenet_169(\n        features, is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n  elif FLAGS.network_depth == 201:\n    logits = densenet_model.densenet_imagenet_201(\n        features, is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n  elif FLAGS.network_depth == 121:\n    logits = densenet_model.densenet_imagenet_121(\n        features, is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n  else:\n    tf.logging.info(""Number of layers not supported, revert to 121"")\n    logits = densenet_model.densenet_imagenet_121(\n        features, is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n\n  # Calculate loss, which includes softmax cross entropy and L2 regularization.\n  cross_entropy = tf.losses.softmax_cross_entropy(\n      logits=logits, onehot_labels=labels)\n\n  # Add weight decay to the loss. We exclude weight decay on the batch\n  # normalization variables because it slightly improves accuracy.\n  loss = cross_entropy + _WEIGHT_DECAY * tf.add_n([\n      tf.nn.l2_loss(v)\n      for v in tf.trainable_variables()\n      if ""batch_normalization"" not in v.name\n  ])\n\n  global_step = tf.train.get_global_step()\n  current_epoch = (\n      tf.cast(global_step, tf.float32) / params[""batches_per_epoch""])\n  learning_rate = learning_rate_schedule(current_epoch)\n\n  # TODO(chrisying): this is a hack to get the LR and epoch for Tensorboard.\n  # Reimplement this when TPU training summaries are supported.\n  lr_repeat = tf.reshape(\n      tf.tile(tf.expand_dims(learning_rate, 0), [\n          params[""batch_size""],\n      ]), [params[""batch_size""], 1])\n  ce_repeat = tf.reshape(\n      tf.tile(tf.expand_dims(current_epoch, 0), [\n          params[""batch_size""],\n      ]), [params[""batch_size""], 1])\n\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate=learning_rate, momentum=_MOMENTUM)\n    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n    # Batch norm requires update_ops to be added as a train_op dependency.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.minimize(loss, global_step)\n  else:\n    train_op = None\n\n  eval_metrics = None\n  if mode == tf.estimator.ModeKeys.EVAL:\n\n    def metric_fn(labels, logits, lr_repeat, ce_repeat):\n      """"""Evaluation metric fn. Performed on CPU, do not reference TPU ops.""""""\n      predictions = tf.argmax(logits, axis=1)\n      accuracy = tf.metrics.accuracy(tf.argmax(labels, axis=1), predictions)\n      lr = tf.metrics.mean(lr_repeat)\n      ce = tf.metrics.mean(ce_repeat)\n      return {""accuracy"": accuracy, ""learning_rate"": lr, ""current_epoch"": ce}\n\n    eval_metrics = (metric_fn, [labels, logits, lr_repeat, ce_repeat])\n\n  return tf.contrib.tpu.TPUEstimatorSpec(\n      mode=mode, loss=loss, train_op=train_op, eval_metrics=eval_metrics)\n\n\ndef main(unused_argv):\n  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n      FLAGS.tpu,\n      zone=FLAGS.tpu_zone,\n      project=FLAGS.gcp_project)\n\n  batches_per_epoch = _NUM_TRAIN_IMAGES / FLAGS.train_batch_size\n  steps_per_checkpoint = FLAGS.steps_per_checkpoint\n  iterations_per_loop = FLAGS.iterations_per_loop\n  eval_steps = _NUM_EVAL_IMAGES // FLAGS.eval_batch_size\n  if iterations_per_loop is None or steps_per_checkpoint < iterations_per_loop:\n    iterations_per_loop = steps_per_checkpoint\n  if FLAGS.mode == ""eval"":\n    iterations_per_loop = eval_steps\n  params = {\n      ""batches_per_epoch"": batches_per_epoch,\n  }\n\n  config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_steps=steps_per_checkpoint,\n      log_step_count_steps=iterations_per_loop,\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=iterations_per_loop, num_shards=FLAGS.num_shards))\n\n  densenet_estimator = tf.contrib.tpu.TPUEstimator(\n      model_fn=model_fn,\n      config=config,\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=FLAGS.eval_batch_size,\n      params=params)\n\n  if FLAGS.mode == ""train"":\n    tf.logging.info(""Training for %d steps (%.2f epochs in total)."" %\n                    (FLAGS.train_steps, FLAGS.train_steps / batches_per_epoch))\n    densenet_estimator.train(\n        input_fn=ImageNetInput(True), max_steps=FLAGS.train_steps)\n\n  elif FLAGS.mode == ""train_and_eval"":\n    current_step = 0\n    tf.logging.info(""Training for %d steps (%.2f epochs in total). Current ""\n                    ""step %d"" %\n                    (FLAGS.train_steps, FLAGS.train_steps / batches_per_epoch,\n                     current_step))\n    while current_step < FLAGS.train_steps:\n      next_checkpoint = min(current_step + steps_per_checkpoint,\n                            FLAGS.train_steps)\n      num_steps = next_checkpoint - current_step\n      current_step = next_checkpoint\n      densenet_estimator.train(input_fn=ImageNetInput(True), steps=num_steps)\n\n      tf.logging.info(""Starting to evaluate."")\n      eval_results = densenet_estimator.evaluate(\n          input_fn=ImageNetInput(False),\n          steps=_NUM_EVAL_IMAGES // FLAGS.eval_batch_size)\n      tf.logging.info(""Eval results: %s"" % eval_results)\n\n  else:\n\n    def terminate_eval():\n      tf.logging.info(""Terminating eval after %d seconds of no checkpoints"" %\n                      FLAGS.eval_timeout)\n      return True\n\n    # Run evaluation when there""s a new checkpoint\n    # If the evaluation worker is delayed in processing a new checkpoint,\n    # the checkpoint file may be deleted by the trainer before it can be\n    # evaluated.\n    # Ignore the error in this case.\n    for ckpt in evaluation.checkpoints_iterator(\n        FLAGS.model_dir,\n        min_interval_secs=FLAGS.min_eval_interval,\n        timeout=FLAGS.eval_timeout,\n        timeout_fn=terminate_eval):\n\n      tf.logging.info(""Starting to evaluate."")\n      try:\n        eval_results = densenet_estimator.evaluate(\n            input_fn=ImageNetInput(False),\n            steps=eval_steps,\n            checkpoint_path=ckpt)\n        tf.logging.info(""Eval results: %s"" % eval_results)\n      except tf.errors.NotFoundError:\n        tf.logging.info(""Checkpoint %s no longer exists, skipping checkpoint"")\n\n\nif __name__ == ""__main__"":\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/models/official/densenet/densenet_model.py,29,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""DenseNet implementation with TPU support.\n\nOriginal paper: (https://arxiv.org/abs/1608.06993)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport tensorflow as tf\n\n# Learning hyperaparmeters\n_BATCH_NORM_DECAY = 0.997\n_BATCH_NORM_EPSILON = 1e-5\n\nFLAGS = flags.FLAGS\nflags.DEFINE_bool(""use_bottleneck"", False, ""Use bottleneck convolution layers"")\n\n\ndef conv(image, filters, strides=1, kernel_size=3):\n  """"""Convolution with default options from the densenet paper.""""""\n  # Use initialization from https://arxiv.org/pdf/1502.01852.pdf\n\n  return tf.layers.conv2d(\n      inputs=image,\n      filters=filters,\n      kernel_size=kernel_size,\n      strides=strides,\n      activation=tf.identity,\n      use_bias=False,\n      padding=""same"",\n      kernel_initializer=tf.variance_scaling_initializer(),\n  )\n\n\ndef dense_block(image, filters, is_training):\n  """"""Standard BN+Relu+conv block for DenseNet.""""""\n  image = tf.layers.batch_normalization(\n      inputs=image,\n      axis=-1,\n      training=is_training,\n      fused=True,\n      center=True,\n      scale=True,\n      momentum=_BATCH_NORM_DECAY,\n      epsilon=_BATCH_NORM_EPSILON,\n  )\n\n  if FLAGS.use_bottleneck:\n    # Add bottleneck layer to optimize computation and reduce HBM space\n    image = tf.nn.relu(image)\n    image = conv(image, 4 * filters, strides=1, kernel_size=1)\n    image = tf.layers.batch_normalization(\n        inputs=image,\n        axis=-1,\n        training=is_training,\n        fused=True,\n        center=True,\n        scale=True,\n        momentum=_BATCH_NORM_DECAY,\n        epsilon=_BATCH_NORM_EPSILON,\n    )\n\n  image = tf.nn.relu(image)\n  return conv(image, filters)\n\n\ndef transition_layer(image, filters, is_training):\n  """"""Construct the transition layer with specified growth rate.""""""\n\n  image = tf.layers.batch_normalization(\n      inputs=image,\n      axis=-1,\n      training=is_training,\n      fused=True,\n      center=True,\n      scale=True,\n      momentum=_BATCH_NORM_DECAY,\n      epsilon=_BATCH_NORM_EPSILON,\n  )\n  image = tf.nn.relu(image)\n  conv_img = conv(image, filters=filters, kernel_size=1)\n  return tf.layers.average_pooling2d(\n      conv_img, pool_size=2, strides=2, padding=""same"")\n\n\ndef _int_shape(layer):\n  return layer.get_shape().as_list()\n\n\n# Definition of the CIFAR-10 network\ndef densenet_cifar_model(image,\n                         k,\n                         layers,\n                         is_training=True,\n                         num_blocks=3,\n                         num_classes=10):\n  """"""Construct a DenseNet with the specified growth size and layers.""""""\n  layers_per_block = int((layers - 4) / num_blocks)\n\n  v = conv(image, filters=2*k, strides=(1, 1), kernel_size=(3, 3))\n  for i in range(num_blocks):\n    with tf.variable_scope(""block-%d"" % i):\n      for j in range(layers_per_block):\n        with tf.variable_scope(""conv-%d-%d"" % (i, j)):\n          dv = dense_block(v, k, is_training)\n          v = tf.concat([v, dv], axis=3)\n    if i != num_blocks - 1:\n      with tf.variable_scope(""transition-%d"" % i):\n        v = transition_layer(v, _int_shape(v)[3], is_training)\n\n  global_pool = tf.reduce_sum(v, axis=(2, 3), name=""global_pool"")\n  logits = tf.layers.dense(\n      global_pool,\n      units=num_classes,\n      activation=tf.identity,\n      kernel_initializer=tf.random_normal_initializer(stddev=2.0 / (\n          _int_shape(global_pool)[1] * 10)),\n      kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\n      bias_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\n  )\n  return logits\n\n\n# Definition of the Imagenet network\ndef densenet_imagenet_model(image, k, depths, num_classes, is_training=True):\n  """"""Construct a DenseNet with the specified growth size and layers.""""""\n\n  num_channels = 2 * k\n  v = conv(image, filters=2 * k, strides=2, kernel_size=7)\n  v = tf.layers.batch_normalization(\n      inputs=v,\n      axis=-1,\n      training=is_training,\n      fused=True,\n      center=True,\n      scale=True,\n      momentum=_BATCH_NORM_DECAY,\n      epsilon=_BATCH_NORM_EPSILON,\n  )\n  v = tf.nn.relu(v)\n  v = tf.layers.max_pooling2d(v, pool_size=3, strides=2, padding=""same"")\n  for i, depth in enumerate(depths):\n    with tf.variable_scope(""block-%d"" % i):\n      for j in range(depth):\n        with tf.variable_scope(""denseblock-%d-%d"" % (i, j)):\n          output = dense_block(v, k, is_training)\n          v = tf.concat([v, output], axis=3)\n          num_channels += k\n      if i != len(depths) - 1:\n        num_channels /= 2\n        v = transition_layer(v, num_channels, is_training)\n\n  global_pool = tf.reduce_mean(v, axis=(1, 2), name=""global_pool"")\n  dense_layer = tf.layers.dense(global_pool, units=num_classes)\n  logits = tf.identity(dense_layer, ""final_dense"")\n\n  return logits\n\n\ndef densenet_imagenet_121(inputs, is_training=True, num_classes=1001):\n  """"""DenseNet 121.""""""\n  depths = [6, 12, 24, 16]\n  growth_rate = 32\n  return densenet_imagenet_model(inputs, growth_rate, depths, num_classes,\n                                 is_training)\n\n\ndef densenet_imagenet_169(inputs, is_training=True, num_classes=1001):\n  """"""DenseNet 121.""""""\n  depths = [6, 12, 32, 32]\n  growth_rate = 32\n  return densenet_imagenet_model(inputs, growth_rate, depths, num_classes,\n                                 is_training)\n\n\ndef densenet_imagenet_201(inputs, is_training=True, num_classes=1001):\n  """"""DenseNet 121.""""""\n  depths = [6, 12, 48, 32]\n  growth_rate = 32\n  return densenet_imagenet_model(inputs, growth_rate, depths, num_classes,\n                                 is_training)\n'"
tpu/models/official/densenet/vgg_preprocessing.py,59,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nThe preprocessing steps for VGG were introduced in the following technical\nreport:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n_R_MEAN = 123.68 / 255\n_G_MEAN = 116.78 / 255\n_B_MEAN = 103.94 / 255\n\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n  """"""Crops the given image using the provided offsets and sizes.\n\n  Note that the method doesn\'t assume we know the input image size but it does\n  assume we know the input image rank.\n\n  Args:\n    image: an image of shape [height, width, channels].\n    offset_height: a scalar tensor indicating the height offset.\n    offset_width: a scalar tensor indicating the width offset.\n    crop_height: the height of the cropped image.\n    crop_width: the width of the cropped image.\n\n  Returns:\n    the cropped (and resized) image.\n\n  Raises:\n    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n      less than the crop size.\n  """"""\n  original_shape = tf.shape(image)\n\n  rank_assertion = tf.Assert(\n      tf.equal(tf.rank(image), 3),\n      [\'Rank of image must be equal to 3.\'])\n  with tf.control_dependencies([rank_assertion]):\n    cropped_shape = tf.stack([crop_height, crop_width, original_shape[2]])\n\n  size_assertion = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(original_shape[0], crop_height),\n          tf.greater_equal(original_shape[1], crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))\n\n  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n  # define the crop size.\n  with tf.control_dependencies([size_assertion]):\n    image = tf.slice(image, offsets, cropped_shape)\n  return tf.reshape(image, cropped_shape)\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n  """"""Crops the given list of images.\n\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n\n  Returns:\n    the image_list with cropped images.\n\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  """"""\n  if not image_list:\n    raise ValueError(\'Empty image_list.\')\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n        tf.equal(image_rank, 3),\n        [\'Wrong rank for tensor  %s [expected] [actual]\',\n         image_list[i].name, 3, image_rank])\n    rank_assertions.append(rank_assert)\n\n  with tf.control_dependencies([rank_assertions[0]]):\n    image_shape = tf.shape(image_list[0])\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(image_height, crop_height),\n          tf.greater_equal(image_width, crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    with tf.control_dependencies([rank_assertions[i]]):\n      shape = tf.shape(image)\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n        tf.equal(height, image_height),\n        [\'Wrong height for tensor %s [expected][actual]\',\n         image.name, height, image_height])\n    width_assert = tf.Assert(\n        tf.equal(width, image_width),\n        [\'Wrong width for tensor %s [expected][actual]\',\n         image.name, width, image_width])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  with tf.control_dependencies(asserts):\n    max_offset_height = tf.reshape(image_height - crop_height + 1, [])\n  with tf.control_dependencies(asserts):\n    max_offset_width = tf.reshape(image_width - crop_width + 1, [])\n  offset_height = tf.random_uniform(\n      [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random_uniform(\n      [], maxval=max_offset_width, dtype=tf.int32)\n\n  return [_crop(image, offset_height, offset_width,\n                crop_height, crop_width) for image in image_list]\n\n\ndef _central_crop(image_list, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    the list of cropped images.\n  """"""\n  outputs = []\n  for image in image_list:\n    image_height = tf.shape(image)[0]\n    image_width = tf.shape(image)[1]\n\n    offset_height = (image_height - crop_height) / 2\n    offset_width = (image_width - crop_width) / 2\n\n    outputs.append(_crop(image, offset_height, offset_width,\n                         crop_height, crop_width))\n  return outputs\n\n\ndef _mean_image_subtraction(image, means):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n  num_channels = image.get_shape().as_list()[-1]\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] -= means[i]\n  return tf.concat(axis=2, values=channels)\n\n\ndef _smallest_size_at_least(height, width, smallest_side):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: and int32 scalar tensor indicating the new width.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  height = tf.to_float(height)\n  width = tf.to_float(width)\n  smallest_side = tf.to_float(smallest_side)\n\n  scale = tf.cond(tf.greater(height, width),\n                  lambda: smallest_side / width,\n                  lambda: smallest_side / height)\n  new_height = tf.to_int32(height * scale)\n  new_width = tf.to_int32(width * scale)\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, smallest_side):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  shape = tf.shape(image)\n  height = shape[0]\n  width = shape[1]\n  new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n  image = tf.expand_dims(image, 0)\n  resized_image = tf.image.resize_bilinear(image, [new_height, new_width],\n                                           align_corners=False)\n  resized_image = tf.squeeze(resized_image)\n  resized_image.set_shape([None, None, 3])\n  return resized_image\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         resize_side_min=_RESIZE_SIDE_MIN,\n                         resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  resize_side = tf.random_uniform(\n      [], minval=resize_side_min, maxval=resize_side_max+1, dtype=tf.int32)\n\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _random_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  image = tf.image.random_flip_left_right(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_for_eval(image, output_height, output_width, resize_side):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side: The smallest side of the image for aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _central_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_image(image, output_height, output_width, is_training=False,\n                     resize_side_min=_RESIZE_SIDE_MIN,\n                     resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, then this value\n      is used for rescaling.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, this value is\n      ignored. Otherwise, the resize side is sampled from\n        [resize_size_min, resize_size_max].\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    return preprocess_for_train(image, output_height, output_width,\n                                resize_side_min, resize_side_max)\n  else:\n    return preprocess_for_eval(image, output_height, output_width,\n                               resize_side_min)\n'"
tpu/models/official/mobilenet/inception_preprocessing.py,69,"b'# Copyright 2016 Google. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the Inception networks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# Standard Imports\nfrom absl import flags\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import random_ops\n\n\nflags.DEFINE_float(\n    \'cb_distortion_range\', 0.1, \'Cb distortion range +/-\')\n\nflags.DEFINE_float(\n    \'cr_distortion_range\', 0.1, \'Cr distortion range +/-\')\n\nflags.DEFINE_boolean(\n    \'use_fast_color_distort\', True,\n    \'apply fast color/chroma distortion if True, else apply\'\n    \'brightness/saturation/hue/contrast distortion\')\n\nFLAGS = flags.FLAGS\n\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distort_color_fast(image, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Distort brightness and chroma values of input image\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    br_delta = random_ops.random_uniform([], -32./255., 32./255., seed=None)\n    cb_factor = random_ops.random_uniform(\n        [], -FLAGS.cb_distortion_range, FLAGS.cb_distortion_range, seed=None)\n    cr_factor = random_ops.random_uniform(\n        [], -FLAGS.cr_distortion_range, FLAGS.cr_distortion_range, seed=None)\n\n    channels = tf.split(axis=2, num_or_size_splits=3, value=image)\n    red_offset = 1.402 * cr_factor + br_delta\n    green_offset = -0.344136 * cb_factor - 0.714136 * cr_factor + br_delta\n    blue_offset = 1.772 * cb_factor + br_delta\n    channels[0] += red_offset\n    channels[1] += green_offset\n    channels[2] += blue_offset\n    image = tf.concat(axis=2, values=channels)\n    image = tf.clip_by_value(image, 0., 1.)\n\n    return image\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(3./4., 4./3.),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n      image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n      area of the image must contain at least this fraction of any bounding box\n      supplied.\n    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n      image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `floats`. The cropped area of the image\n      must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n      region of the image of the specified constraints. After `max_attempts`\n      failures, return the entire image.\n    scope: Optional scope for name_scope.\n  Returns:\n    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # A large fraction of image datasets contain a human-annotated bounding\n    # box delineating the region of the image containing the object of interest.\n    # We choose to create a new bounding box for the object which is a randomly\n    # distorted version of the human-annotated bounding box that obeys an\n    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n    # bounding box. If no box is supplied, then we assume the bounding box is\n    # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        tf.shape(image),\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n    return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         fast_mode=True,\n                         scope=None,\n                         add_image_summaries=True):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Additionally it would create image_summaries to display the different\n  transformations applied to the image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n      bi-cubic resizing, random_hue or random_contrast).\n    scope: Optional scope for name_scope.\n    add_image_summaries: Enable image summaries.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].\n  """"""\n  with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n    if bbox is None:\n      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                         dtype=tf.float32,\n                         shape=[1, 1, 4])\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    if add_image_summaries:\n      # Each bounding box has shape [1, num_boxes, box coords] and\n      # the coordinates are ordered [ymin, xmin, ymax, xmax].\n      image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                    bbox)\n      tf.summary.image(\'image_with_bounding_boxes\', image_with_box)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n    if add_image_summaries:\n      image_with_distorted_box = tf.image.draw_bounding_boxes(\n          tf.expand_dims(image, 0), distorted_bbox)\n      tf.summary.image(\'images_with_distorted_bounding_box\',\n                       image_with_distorted_box)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize_images(x, [height, width], method),\n        num_cases=num_resize_cases)\n\n    if add_image_summaries:\n      tf.summary.image(\'cropped_resized_image\',\n                       tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors. There are 1 or 4 ways to do it.\n    if FLAGS.use_fast_color_distort:\n      distorted_image = distort_color_fast(distorted_image)\n    else:\n      num_distort_cases = 1 if fast_mode else 4\n      distorted_image = apply_with_random_selector(\n          distorted_image,\n          lambda x, ordering: distort_color(x, ordering, fast_mode),\n          num_cases=num_distort_cases)\n\n    if add_image_summaries:\n      tf.summary.image(\'final_distorted_image\',\n                       tf.expand_dims(distorted_image, 0))\n    distorted_image = tf.subtract(distorted_image, 0.5)\n    distorted_image = tf.multiply(distorted_image, 2.0)\n    return distorted_image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n  """"""Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n\n  If central_fraction is specified it would crop the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n  with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n      # Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(image, [height, width],\n                                       align_corners=False)\n      image = tf.squeeze(image, [0])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    image.set_shape([height, width, 3])\n    return image\n\n\ndef preprocess_image(image, output_height, output_width,\n                     is_training=False,\n                     bbox=None,\n                     fast_mode=True,\n                     add_image_summaries=False):\n  """"""Pre-process one image for training or evaluation.\n\n  Args:\n    image: 3-D Tensor [height, width, channels] with the image. If dtype is\n      tf.float32 then the range should be [0, 1], otherwise it would converted\n      to tf.float32 assuming that the range is [0, MAX], where MAX is largest\n      positive representable number for int(8/16/32) data type (see\n      `tf.image.convert_image_dtype` for details).\n    output_height: integer, image expected height.\n    output_width: integer, image expected width.\n    is_training: Boolean. If true it would transform an image for train,\n      otherwise it would transform it for evaluation.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations.\n    add_image_summaries: Enable image summaries.\n\n  Returns:\n    3-D float Tensor containing an appropriately scaled image\n\n  Raises:\n    ValueError: if user does not provide bounding box\n  """"""\n  if is_training:\n    return preprocess_for_train(image, output_height, output_width, bbox,\n                                fast_mode,\n                                add_image_summaries=add_image_summaries)\n  else:\n    return preprocess_for_eval(image, output_height, output_width)\n'"
tpu/models/official/mobilenet/mobilenet.py,82,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Training harness for MobileNet v1.\n\nThis demonstrates how to train the Mobilenet model without any modifications to\nthe original model definition.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom absl import flags\nimport tensorflow as tf\n\nimport inception_preprocessing\nimport mobilenet_model as mobilenet_v1\nimport vgg_preprocessing\n\nfrom tensorflow.contrib.framework.python.ops import arg_scope\nfrom tensorflow.contrib.training.python.training import evaluation\n\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'tpu\', default=None,\n    help=\'The Cloud TPU to use for training. This should be either the name \'\n    \'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\')\nflags.DEFINE_string(\n    \'gcp_project\', default=None,\n    help=\'Project name for the Cloud TPU-enabled project. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\nflags.DEFINE_string(\n    \'tpu_zone\', default=None,\n    help=\'GCE zone where the Cloud TPU is located in. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\n\n# Model specific paramenters\nflags.DEFINE_string(\n    \'data_dir\', \'\',\n    \'Directory where input data is stored\')\n\nflags.DEFINE_string(\n    \'model_dir\', None,\n    \'Directory where model output is stored\')\n\nflags.DEFINE_integer(\n    \'num_shards\', 8,\n    \'Number of shards (workers).\')\n\nflags.DEFINE_integer(\n    \'iterations\', 100,\n    \'Number of iterations per TPU training loop.\')\n\nflags.DEFINE_integer(\n    \'train_batch_size\', 1024,\n    \'Global (not per-shard) batch size for training\')\n\nflags.DEFINE_integer(\n    \'eval_total_size\', 0,\n    \'Total batch size for evaluation, use the entire validation set if 0\')\n\nflags.DEFINE_integer(\n    \'eval_batch_size\', 1024,\n    \'Global (not per-shard) batch size for evaluation\')\n\nflags.DEFINE_integer(\n    \'train_steps\', 8000000,\n    \'Number of steps use for training.\')\n\nflags.DEFINE_integer(\n    \'train_steps_per_eval\', 2000,\n    \'Number of training steps to run between evaluations.\')\n\nflags.DEFINE_string(\n    \'mode\', \'train_and_eval\',\n    \'Mode to run: train, eval, train_and_eval\')\n\nflags.DEFINE_integer(\n    \'min_eval_interval\', 180,\n    \'Minimum number of seconds between evaluations\')\n\nflags.DEFINE_integer(\n    \'eval_timeout\', None,\n    \'Evaluation timeout: Maximum number of seconds that \'\n    \'may elapse while no new checkpoints are observed\')\n\nflags.DEFINE_bool(\n    \'use_tpu\', True,\n    \'Use TPUs rather than plain CPUs\')\n\nflags.DEFINE_boolean(\n    \'per_host_input_for_training\', True,\n    \'If true, input_fn is invoked per host rather than per shard.\')\n\nflags.DEFINE_string(\n    \'use_data\', \'real\',\n    \'One of ""fake"",""real""\')\n\nflags.DEFINE_float(\n    \'learning_rate\', 0.165,\n    \'Learning rate.\')\n\nflags.DEFINE_float(\n    \'depth_multiplier\', 1.0,\n    \'Depth Multiplier on Inception\')\n\nflags.DEFINE_string(\n    \'optimizer\', \'RMS\',\n    \'Optimizer (one of sgd, RMS, momentum)\')\n\nflags.DEFINE_integer(\n    \'num_classes\', 1001,\n    \'Number of classes to distinguish\')\n\nflags.DEFINE_integer(\n    \'width\', 224,\n    \'Width of input image\')\n\nflags.DEFINE_integer(\n    \'height\', 224,\n    \'Height of input image\')\n\nflags.DEFINE_bool(\n    \'transpose_enabled\', False,\n    \'Boolean to enable/disable explicit I/O transpose\')\n\nflags.DEFINE_bool(\n    \'use_fused_batchnorm\', True,\n    \'Enable fused batchrnom\')\n\nflags.DEFINE_bool(\n    \'log_device_placement\', False,\n    \'Boolean to enable/disable log device placement\')\n\nflags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'Number of steps which must have run before showing summaries.\')\n\nflags.DEFINE_integer(\n    \'save_checkpoints_secs\', 1000,\n    \'Interval (in seconds) at which the model data \'\n    \'should be checkpointed. Set to 0 to disable.\')\n\nflags.DEFINE_bool(\n    \'moving_average\', True,\n    \'Whether to enable moving average computation on variables\')\n\nflags.DEFINE_string(\n    \'preprocessing\', \'inception\',\n    \'Preprocessing stage to use: one of inception or vgg\')\n\nflags.DEFINE_bool(\n    \'use_annotated_bbox\', False,\n    \'If true, use annotated bounding box as input to cropping function, \'\n    \'else use full image size\')\n\nflags.DEFINE_float(\n    \'learning_rate_decay\', 0.94,\n    \'Exponential decay rate used in learning rate adjustment\')\n\nflags.DEFINE_integer(\n    \'learning_rate_decay_epochs\', 3,\n    \'Exponential decay epochs used in learning rate adjustment\')\n\nflags.DEFINE_bool(\n    \'use_logits\', True,\n    \'Use logits if true, else use predictions\')\n\nflags.DEFINE_bool(\n    \'display_tensors\', False,\n    \'Whether to dump prediction tensors for comparison\')\n\nflags.DEFINE_bool(\n    \'clear_update_collections\', True,\n    \'Set batchnorm update_collections to None if true, else use default value\')\n\n# Dataset specific paramenters\nflags.DEFINE_bool(\n    \'prefetch_enabled\', True,\n    \'Boolean to enable/disable prefetching\')\n\nflags.DEFINE_integer(\n    \'prefetch_dataset_buffer_size\', 8*1024*1024,\n    \'Number of bytes in read buffer. 0 means no buffering.\')\n\nflags.DEFINE_integer(\n    \'num_files_infeed\', 8,\n    \'Number of training files to read in parallel.\')\n\nflags.DEFINE_integer(\n    \'num_parallel_calls\', 64,\n    \'Number of elements to process in parallel (by mapper)\')\n\nflags.DEFINE_integer(\n    \'initial_shuffle_buffer_size\', 1024,\n    \'Number of elements from dataset that shuffler will sample from. \'\n    \'This shuffling is done before any other operations. \'\n    \'Set to 0 to disable\')\n\nflags.DEFINE_integer(\n    \'followup_shuffle_buffer_size\', 1000,\n    \'Number of elements from dataset that shuffler will sample from. \'\n    \'This shuffling is done after prefetching is done. \'\n    \'Set to 0 to disable\')\n\n\nFLAGS = flags.FLAGS\n\n# Dataset constants\n_NUM_TRAIN_IMAGES = 1281167\n_NUM_EVAL_IMAGES = 50000\n\n# Random cropping constants\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n# Constants dictating the learning rate schedule.\nRMSPROP_DECAY = 0.9                # Decay term for RMSProp.\nRMSPROP_MOMENTUM = 0.9             # Momentum in RMSProp.\nRMSPROP_EPSILON = 1.0              # Epsilon term for RMSProp.\n\n# Constants dictating moving average.\nMOVING_AVERAGE_DECAY = 0.995\n\n# Batchnorm moving mean/variance parameters\nBATCH_NORM_DECAY = 0.996\nBATCH_NORM_EPSILON = 1e-3\n\n\nclass InputPipeline(object):\n  """"""Generates ImageNet input_fn for training or evaluation.\n\n  The training data is assumed to be in TFRecord format with keys as specified\n  in the dataset_parser below, sharded across 1024 files, named sequentially:\n      train-00000-of-01024\n      train-00001-of-01024\n      ...\n      train-01023-of-01024\n\n  The validation data is in the same format but sharded in 128 files.\n\n  The fortmat of the data required is created by the script at:\n      https://github.com/tensorflow/tpu/blob/master/tools/datasets/imagenet_to_gcs.py\n\n  Args:\n    is_training: `bool` for whether the input is for training\n  """"""\n\n  def __init__(self, is_training, data_dir):\n    self.is_training = is_training\n    self.data_dir = data_dir\n\n  def dataset_parser(self, serialized_proto):\n    """"""Parse an Imagenet record from value.""""""\n    keys_to_features = {\n        \'image/encoded\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\':\n            tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/class/label\':\n            tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),\n        \'image/class/text\':\n            tf.FixedLenFeature([], dtype=tf.string, default_value=\'\'),\n        \'image/object/bbox/xmin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/class/label\':\n            tf.VarLenFeature(dtype=tf.int64),\n    }\n\n    features = tf.parse_single_example(serialized_proto, keys_to_features)\n\n    bbox = None\n    if FLAGS.use_annotated_bbox:\n      xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n      ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n      xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n      ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n      # Note that we impose an ordering of (y, x) just to make life difficult.\n      bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n      # Force the variable number of bounding boxes into the shape\n      # [1, num_boxes, coords].\n      bbox = tf.expand_dims(bbox, 0)\n      bbox = tf.transpose(bbox, [0, 2, 1])\n\n    image = features[\'image/encoded\']\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    if FLAGS.preprocessing == \'vgg\':\n      image = vgg_preprocessing.preprocess_image(\n          image=image,\n          output_height=FLAGS.height,\n          output_width=FLAGS.width,\n          is_training=self.is_training,\n          resize_side_min=_RESIZE_SIDE_MIN,\n          resize_side_max=_RESIZE_SIDE_MAX)\n    elif FLAGS.preprocessing == \'inception\':\n      image = inception_preprocessing.preprocess_image(\n          image=image,\n          output_height=FLAGS.height,\n          output_width=FLAGS.width,\n          is_training=self.is_training,\n          bbox=bbox)\n    else:\n      assert False, \'Unknown preprocessing type: %s\' % FLAGS.preprocessing\n\n    label = tf.cast(\n        tf.reshape(features[\'image/class/label\'], shape=[]), dtype=tf.int32)\n\n    return image, label\n\n  def input_fn(self, params):\n    """"""Input function which provides a single batch for train or eval.\n\n    Args:\n      params: `dict` of parameters passed from the `TPUEstimator`.\n          `params[\'batch_size\']` is always provided and should be used as the\n          effective batch size.\n\n    Returns:\n      A (images, labels) tuple of `Tensor`s for a batch of samples.\n    """"""\n    batch_size = params[\'batch_size\']\n\n    if FLAGS.use_data == \'real\':\n      file_pattern = os.path.join(\n          self.data_dir, \'train-*\' if self.is_training else \'validation-*\')\n      dataset = tf.data.Dataset.list_files(file_pattern,\n                                           shuffle=self.is_training)\n\n      if self.is_training:\n        dataset = dataset.repeat()\n\n      def prefetch_dataset(filename):\n        dataset = tf.data.TFRecordDataset(\n            filename, buffer_size=FLAGS.prefetch_dataset_buffer_size)\n        return dataset\n\n      dataset = dataset.apply(\n          tf.contrib.data.parallel_interleave(\n              prefetch_dataset,\n              cycle_length=FLAGS.num_files_infeed,\n              sloppy=True))\n\n      if FLAGS.followup_shuffle_buffer_size > 0:\n        dataset = dataset.shuffle(\n            buffer_size=FLAGS.followup_shuffle_buffer_size)\n\n      dataset = dataset.map(\n          self.dataset_parser,\n          num_parallel_calls=FLAGS.num_parallel_calls)\n\n      dataset = dataset.prefetch(batch_size)\n\n      dataset = dataset.apply(\n          tf.contrib.data.batch_and_drop_remainder(batch_size))\n\n      dataset = dataset.prefetch(2)  # Prefetch overlaps in-feed with training\n\n      images, labels = dataset.make_one_shot_iterator().get_next()\n      images.set_shape([batch_size, FLAGS.height, FLAGS.width, 3])\n    else:\n      images = tf.random_uniform(\n          [batch_size, FLAGS.height, FLAGS.width, 3], minval=-1, maxval=1)\n      labels = tf.random_uniform(\n          [batch_size], minval=0, maxval=999, dtype=tf.int32)\n\n    images = tensor_transform_fn(images, params[\'output_perm\'])\n    return images, labels\n\n\ndef tensor_transform_fn(data, perm):\n  """"""Transpose function.\n\n  This function is used to transpose an image tensor on the host and then\n  perform an inverse transpose on the TPU. The transpose on the TPU gets\n  effectively elided thus voiding any associated computational cost.\n\n  NOTE: Eventually the compiler will be able to detect when this kind of\n  operation may prove beneficial and perform these types of transformations\n  implicitly, voiding the need for user intervention\n\n  Args:\n    data: Tensor to be transposed\n    perm: Permutation of the dimensions of a\n\n  Returns:\n    Transposed tensor\n  """"""\n  if FLAGS.transpose_enabled:\n    return tf.transpose(data, perm)\n  return data\n\n\ndef model_fn(features, labels, mode, params):\n  """"""Mobilenet v1 model using Estimator API.""""""\n  num_classes = FLAGS.num_classes\n  training_active = (mode == tf.estimator.ModeKeys.TRAIN)\n  eval_active = (mode == tf.estimator.ModeKeys.EVAL)\n\n  features = tensor_transform_fn(features, params[\'input_perm\'])\n\n  if FLAGS.clear_update_collections:\n    # updates_collections must be set to None in order to use fused batchnorm\n    with arg_scope(mobilenet_v1.mobilenet_v1_arg_scope()):\n      logits, end_points = mobilenet_v1.mobilenet_v1(\n          features,\n          num_classes,\n          is_training=training_active,\n          depth_multiplier=FLAGS.depth_multiplier)\n  else:\n    with arg_scope(mobilenet_v1.mobilenet_v1_arg_scope()):\n      logits, end_points = mobilenet_v1.mobilenet_v1(\n          features,\n          num_classes,\n          is_training=training_active,\n          depth_multiplier=FLAGS.depth_multiplier)\n\n  predictions = {\n      \'classes\': tf.argmax(input=logits, axis=1),\n      \'probabilities\': tf.nn.softmax(logits, name=\'softmax_tensor\')\n  }\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n  if mode == tf.estimator.ModeKeys.EVAL and FLAGS.display_tensors and (\n      not FLAGS.use_tpu):\n    with tf.control_dependencies([\n        tf.Print(\n            predictions[\'classes\'], [predictions[\'classes\']],\n            summarize=FLAGS.eval_batch_size,\n            message=\'prediction: \')\n    ]):\n      labels = tf.Print(\n          labels, [labels], summarize=FLAGS.eval_batch_size, message=\'label: \')\n\n  one_hot_labels = tf.one_hot(labels, FLAGS.num_classes, dtype=tf.int32)\n\n  tf.losses.softmax_cross_entropy(\n      onehot_labels=one_hot_labels,\n      logits=logits,\n      weights=1.0,\n      label_smoothing=0.1)\n  loss = tf.losses.get_total_loss(add_regularization_losses=True)\n\n  initial_learning_rate = FLAGS.learning_rate * FLAGS.train_batch_size / 256\n  final_learning_rate = 0.0001 * initial_learning_rate\n\n  train_op = None\n  if training_active:\n    batches_per_epoch = _NUM_TRAIN_IMAGES // FLAGS.train_batch_size\n    global_step = tf.train.get_or_create_global_step()\n\n    learning_rate = tf.train.exponential_decay(\n        learning_rate=initial_learning_rate,\n        global_step=global_step,\n        decay_steps=FLAGS.learning_rate_decay_epochs * batches_per_epoch,\n        decay_rate=FLAGS.learning_rate_decay,\n        staircase=True)\n\n    # Set a minimum boundary for the learning rate.\n    learning_rate = tf.maximum(\n        learning_rate, final_learning_rate, name=\'learning_rate\')\n\n    if FLAGS.optimizer == \'sgd\':\n      tf.logging.info(\'Using SGD optimizer\')\n      optimizer = tf.train.GradientDescentOptimizer(\n          learning_rate=learning_rate)\n    elif FLAGS.optimizer == \'momentum\':\n      tf.logging.info(\'Using Momentum optimizer\')\n      optimizer = tf.train.MomentumOptimizer(\n          learning_rate=learning_rate, momentum=0.9)\n    elif FLAGS.optimizer == \'RMS\':\n      tf.logging.info(\'Using RMS optimizer\')\n      optimizer = tf.train.RMSPropOptimizer(\n          learning_rate,\n          RMSPROP_DECAY,\n          momentum=RMSPROP_MOMENTUM,\n          epsilon=RMSPROP_EPSILON)\n    else:\n      tf.logging.fatal(\'Unknown optimizer:\', FLAGS.optimizer)\n\n    if FLAGS.use_tpu:\n      optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.minimize(loss, global_step=global_step)\n    if FLAGS.moving_average:\n      ema = tf.train.ExponentialMovingAverage(\n          decay=MOVING_AVERAGE_DECAY, num_updates=global_step)\n      variables_to_average = (tf.trainable_variables() +\n                              tf.moving_average_variables())\n      with tf.control_dependencies([train_op]), tf.name_scope(\'moving_average\'):\n        train_op = ema.apply(variables_to_average)\n\n  eval_metrics = None\n  if eval_active:\n    def metric_fn(labels, predictions):\n      accuracy = tf.metrics.accuracy(labels, tf.argmax(\n          input=predictions, axis=1))\n      return {\'accuracy\': accuracy}\n\n    if FLAGS.use_logits:\n      eval_predictions = logits\n    else:\n      eval_predictions = end_points[\'Predictions\']\n\n    eval_metrics = (metric_fn, [labels, eval_predictions])\n\n  return tf.contrib.tpu.TPUEstimatorSpec(\n      mode=mode, loss=loss, train_op=train_op, eval_metrics=eval_metrics)\n\n\nclass LoadEMAHook(tf.train.SessionRunHook):\n  """"""Hook to load EMA into their corresponding variables.""""""\n\n  def __init__(self, model_dir):\n    super(LoadEMAHook, self).__init__()\n    self._model_dir = model_dir\n\n  def begin(self):\n    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n    variables_to_restore = ema.variables_to_restore()\n    self._load_ema = tf.contrib.framework.assign_from_checkpoint_fn(\n        tf.train.latest_checkpoint(self._model_dir), variables_to_restore)\n\n  def after_create_session(self, sess, coord):\n    tf.logging.info(\'Reloading EMA...\')\n    self._load_ema(sess)\n\n\ndef main(unused_argv):\n  del unused_argv  # Unused\n\n  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n            FLAGS.tpu,\n            zone=FLAGS.tpu_zone,\n            project=FLAGS.gcp_project)\n\n  batch_size_per_shard = FLAGS.train_batch_size // FLAGS.num_shards\n  params = {\n      \'input_perm\': [0, 1, 2, 3],\n      \'output_perm\': [0, 1, 2, 3],\n  }\n\n  batch_axis = 0\n  if FLAGS.transpose_enabled:\n    if batch_size_per_shard >= 64:\n      params[\'input_perm\'] = [3, 0, 1, 2]\n      params[\'output_perm\'] = [1, 2, 3, 0]\n      batch_axis = 3\n    else:\n      params[\'input_perm\'] = [2, 0, 1, 3]\n      params[\'output_perm\'] = [1, 2, 0, 3]\n      batch_axis = 2\n\n  if FLAGS.eval_total_size > 0:\n    eval_size = FLAGS.eval_total_size\n  else:\n    eval_size = _NUM_EVAL_IMAGES\n  eval_steps = eval_size // FLAGS.eval_batch_size\n\n  iterations = (eval_steps if FLAGS.mode == \'eval\' else\n                FLAGS.iterations)\n\n  eval_batch_size = (None if FLAGS.mode == \'train\' else\n                     FLAGS.eval_batch_size)\n\n  per_host_input_for_training = (FLAGS.num_shards <= 8 if\n                                 FLAGS.mode == \'train\' else True)\n\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_secs=FLAGS.save_checkpoints_secs,\n      save_summary_steps=FLAGS.save_summary_steps,\n      session_config=tf.ConfigProto(\n          allow_soft_placement=True,\n          log_device_placement=FLAGS.log_device_placement),\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=iterations,\n          num_shards=FLAGS.num_shards,\n          per_host_input_for_training=per_host_input_for_training))\n\n  inception_classifier = tf.contrib.tpu.TPUEstimator(\n      model_fn=model_fn,\n      use_tpu=FLAGS.use_tpu,\n      config=run_config,\n      params=params,\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=eval_batch_size,\n      batch_axis=(batch_axis, 0))\n\n  # Input pipelines are slightly different (with regards to shuffling and\n  # preprocessing) between training and evaluation.\n  imagenet_train = InputPipeline(\n      is_training=True,\n      data_dir=FLAGS.data_dir)\n  imagenet_eval = InputPipeline(\n      is_training=False,\n      data_dir=FLAGS.data_dir)\n\n  if FLAGS.moving_average:\n    eval_hooks = [LoadEMAHook(FLAGS.model_dir)]\n  else:\n    eval_hooks = []\n\n  if FLAGS.mode == \'eval\':\n    def terminate_eval():\n      tf.logging.info(\'%d seconds without new checkpoints have elapsed \'\n                      \'... terminating eval\' % FLAGS.eval_timeout)\n      return True\n\n    def get_next_checkpoint():\n      return evaluation.checkpoints_iterator(\n          FLAGS.model_dir,\n          min_interval_secs=FLAGS.min_eval_interval,\n          timeout=FLAGS.eval_timeout,\n          timeout_fn=terminate_eval)\n\n    for checkpoint in get_next_checkpoint():\n      tf.logging.info(\'Starting to evaluate.\')\n      try:\n        eval_results = inception_classifier.evaluate(\n            input_fn=imagenet_eval.input_fn,\n            steps=eval_steps,\n            hooks=eval_hooks,\n            checkpoint_path=checkpoint)\n        tf.logging.info(\'Evaluation results: %s\' % eval_results)\n      except tf.errors.NotFoundError:\n        # skip checkpoint if it gets deleted prior to evaluation\n        tf.logging.info(\'Checkpoint %s no longer exists ... skipping\')\n\n  elif FLAGS.mode == \'train_and_eval\':\n    for cycle in range(FLAGS.train_steps // FLAGS.train_steps_per_eval):\n      tf.logging.info(\'Starting training cycle %d.\' % cycle)\n      inception_classifier.train(\n          input_fn=imagenet_train.input_fn, steps=FLAGS.train_steps_per_eval)\n\n      tf.logging.info(\'Starting evaluation cycle %d .\' % cycle)\n      eval_results = inception_classifier.evaluate(\n          input_fn=imagenet_eval.input_fn, steps=eval_steps, hooks=eval_hooks)\n      tf.logging.info(\'Evaluation results: %s\' % eval_results)\n\n  else:\n    tf.logging.info(\'Starting training ...\')\n    inception_classifier.train(\n        input_fn=imagenet_train.input_fn, steps=FLAGS.train_steps)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/models/official/mobilenet/mobilenet_model.py,10,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\n# This file is a copy of the model available via the tensorflow/models repo:\n#\n# https://raw.githubusercontent.com/tensorflow/models/master/research/slim/nets/mobilenet_v1.py\n\n""""""MobileNet v1.\n\nMobileNet is a general architecture and can be used for multiple use cases.\nDepending on the use case, it can use different input layer size and different\nhead (for example: embeddings, localization and classification).\n\nAs described in https://arxiv.org/abs/1704.04861.\n\n  MobileNets: Efficient Convolutional Neural Networks for\n    Mobile Vision Applications\n  Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,\n    Tobias Weyand, Marco Andreetto, Hartwig Adam\n\n100% Mobilenet V1 (base) with input size 224x224:\n\nSee mobilenet_v1()\n\nLayer                                                     params           macs\n--------------------------------------------------------------------------------\nMobilenetV1/Conv2d_0/Conv2D:                                 864      10,838,016\nMobilenetV1/Conv2d_1_depthwise/depthwise:                    288       3,612,672\nMobilenetV1/Conv2d_1_pointwise/Conv2D:                     2,048      25,690,112\nMobilenetV1/Conv2d_2_depthwise/depthwise:                    576       1,806,336\nMobilenetV1/Conv2d_2_pointwise/Conv2D:                     8,192      25,690,112\nMobilenetV1/Conv2d_3_depthwise/depthwise:                  1,152       3,612,672\nMobilenetV1/Conv2d_3_pointwise/Conv2D:                    16,384      51,380,224\nMobilenetV1/Conv2d_4_depthwise/depthwise:                  1,152         903,168\nMobilenetV1/Conv2d_4_pointwise/Conv2D:                    32,768      25,690,112\nMobilenetV1/Conv2d_5_depthwise/depthwise:                  2,304       1,806,336\nMobilenetV1/Conv2d_5_pointwise/Conv2D:                    65,536      51,380,224\nMobilenetV1/Conv2d_6_depthwise/depthwise:                  2,304         451,584\nMobilenetV1/Conv2d_6_pointwise/Conv2D:                   131,072      25,690,112\nMobilenetV1/Conv2d_7_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_7_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_8_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_8_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_9_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_9_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_10_depthwise/depthwise:                 4,608         903,168\nMobilenetV1/Conv2d_10_pointwise/Conv2D:                  262,144      51,380,224\nMobilenetV1/Conv2d_11_depthwise/depthwise:                 4,608         903,168\nMobilenetV1/Conv2d_11_pointwise/Conv2D:                  262,144      51,380,224\nMobilenetV1/Conv2d_12_depthwise/depthwise:                 4,608         225,792\nMobilenetV1/Conv2d_12_pointwise/Conv2D:                  524,288      25,690,112\nMobilenetV1/Conv2d_13_depthwise/depthwise:                 9,216         451,584\nMobilenetV1/Conv2d_13_pointwise/Conv2D:                1,048,576      51,380,224\n--------------------------------------------------------------------------------\nTotal:                                                 3,185,088     567,716,352\n\n\n75% Mobilenet V1 (base) with input size 128x128:\n\nSee mobilenet_v1_075()\n\nLayer                                                     params           macs\n--------------------------------------------------------------------------------\nMobilenetV1/Conv2d_0/Conv2D:                                 648       2,654,208\nMobilenetV1/Conv2d_1_depthwise/depthwise:                    216         884,736\nMobilenetV1/Conv2d_1_pointwise/Conv2D:                     1,152       4,718,592\nMobilenetV1/Conv2d_2_depthwise/depthwise:                    432         442,368\nMobilenetV1/Conv2d_2_pointwise/Conv2D:                     4,608       4,718,592\nMobilenetV1/Conv2d_3_depthwise/depthwise:                    864         884,736\nMobilenetV1/Conv2d_3_pointwise/Conv2D:                     9,216       9,437,184\nMobilenetV1/Conv2d_4_depthwise/depthwise:                    864         221,184\nMobilenetV1/Conv2d_4_pointwise/Conv2D:                    18,432       4,718,592\nMobilenetV1/Conv2d_5_depthwise/depthwise:                  1,728         442,368\nMobilenetV1/Conv2d_5_pointwise/Conv2D:                    36,864       9,437,184\nMobilenetV1/Conv2d_6_depthwise/depthwise:                  1,728         110,592\nMobilenetV1/Conv2d_6_pointwise/Conv2D:                    73,728       4,718,592\nMobilenetV1/Conv2d_7_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_7_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_8_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_8_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_9_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_9_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_10_depthwise/depthwise:                 3,456         221,184\nMobilenetV1/Conv2d_10_pointwise/Conv2D:                  147,456       9,437,184\nMobilenetV1/Conv2d_11_depthwise/depthwise:                 3,456         221,184\nMobilenetV1/Conv2d_11_pointwise/Conv2D:                  147,456       9,437,184\nMobilenetV1/Conv2d_12_depthwise/depthwise:                 3,456          55,296\nMobilenetV1/Conv2d_12_pointwise/Conv2D:                  294,912       4,718,592\nMobilenetV1/Conv2d_13_depthwise/depthwise:                 6,912         110,592\nMobilenetV1/Conv2d_13_pointwise/Conv2D:                  589,824       9,437,184\n--------------------------------------------------------------------------------\nTotal:                                                 1,800,144     106,002,432\n\n""""""\n\n# Tensorflow mandates these.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import namedtuple\nimport functools\n\n# Standard Imports\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n# Conv and DepthSepConv namedtuple define layers of the MobileNet architecture\n# Conv defines 3x3 convolution layers\n# DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.\n# stride is the stride of the convolution\n# depth is the number of channels or filters in a layer\nConv = namedtuple(\'Conv\', [\'kernel\', \'stride\', \'depth\'])\nDepthSepConv = namedtuple(\'DepthSepConv\', [\'kernel\', \'stride\', \'depth\'])\n\n# _CONV_DEFS specifies the MobileNet body\n_CONV_DEFS = [\n    Conv(kernel=[3, 3], stride=2, depth=32),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=64),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=128),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=128),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=256),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=256),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=1024),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=1024)\n]\n\n\ndef mobilenet_v1_base(inputs,\n                      final_endpoint=\'Conv2d_13_pointwise\',\n                      min_depth=8,\n                      depth_multiplier=1.0,\n                      conv_defs=None,\n                      output_stride=None,\n                      scope=None):\n  """"""Mobilenet v1.\n\n  Constructs a Mobilenet v1 network from inputs to the given final endpoint.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_0\', \'Conv2d_1_pointwise\', \'Conv2d_2_pointwise\',\n      \'Conv2d_3_pointwise\', \'Conv2d_4_pointwise\', \'Conv2d_5\'_pointwise,\n      \'Conv2d_6_pointwise\', \'Conv2d_7_pointwise\', \'Conv2d_8_pointwise\',\n      \'Conv2d_9_pointwise\', \'Conv2d_10_pointwise\', \'Conv2d_11_pointwise\',\n      \'Conv2d_12_pointwise\', \'Conv2d_13_pointwise\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. Allowed values are 8 (accurate fully convolutional\n      mode), 16 (fast fully convolutional mode), 32 (classification mode).\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0, or the target output_stride is not\n                allowed.\n  """"""\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  if conv_defs is None:\n    conv_defs = _CONV_DEFS\n\n  if output_stride is not None and output_stride not in [8, 16, 32]:\n    raise ValueError(\'Only allowed output_stride values are 8, 16, 32.\')\n\n  with tf.variable_scope(scope, \'MobilenetV1\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d], padding=\'SAME\'):\n      # The current_stride variable keeps track of the output stride of the\n      # activations, i.e., the running product of convolution strides up to the\n      # current network layer. This allows us to invoke atrous convolution\n      # whenever applying the next convolution would result in the activations\n      # having output stride larger than the target output_stride.\n      current_stride = 1\n\n      # The atrous convolution rate parameter.\n      rate = 1\n\n      net = inputs\n      for i, conv_def in enumerate(conv_defs):\n        end_point_base = \'Conv2d_%d\' % i\n\n        if output_stride is not None and current_stride == output_stride:\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          layer_stride = 1\n          layer_rate = rate\n          rate *= conv_def.stride\n        else:\n          layer_stride = conv_def.stride\n          layer_rate = 1\n          current_stride *= conv_def.stride\n\n        if isinstance(conv_def, Conv):\n          end_point = end_point_base\n          net = slim.conv2d(net, depth(conv_def.depth), conv_def.kernel,\n                            stride=conv_def.stride,\n                            normalizer_fn=slim.batch_norm,\n                            scope=end_point)\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n\n        elif isinstance(conv_def, DepthSepConv):\n          end_point = end_point_base + \'_depthwise\'\n\n          # By passing filters=None\n          # separable_conv2d produces only a depthwise convolution layer\n          net = slim.separable_conv2d(net, None, conv_def.kernel,\n                                      depth_multiplier=1,\n                                      stride=layer_stride,\n                                      rate=layer_rate,\n                                      normalizer_fn=slim.batch_norm,\n                                      scope=end_point)\n\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n\n          end_point = end_point_base + \'_pointwise\'\n\n          net = slim.conv2d(net, depth(conv_def.depth), [1, 1],\n                            stride=1,\n                            normalizer_fn=slim.batch_norm,\n                            scope=end_point)\n\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n        else:\n          raise ValueError(\'Unknown convolution type %s for layer %d\'\n                           % (conv_def.ltype, i))\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef mobilenet_v1(inputs,\n                 num_classes=1000,\n                 dropout_keep_prob=0.999,\n                 is_training=True,\n                 min_depth=8,\n                 depth_multiplier=1.0,\n                 conv_defs=None,\n                 prediction_fn=tf.contrib.layers.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'MobilenetV1\',\n                 global_pool=False):\n  """"""Mobilenet v1 model for classification.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    is_training: whether is training or not.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    global_pool: Optional boolean flag to control the avgpooling before the\n      logits layer. If false or unset, pooling is done with a fixed window\n      that reduces default-sized inputs to 1x1, while larger inputs lead to\n      larger outputs. If true, any input size is pooled down to 1x1.\n\n  Returns:\n    net: a 2D Tensor with the logits (pre-softmax activations) if num_classes\n      is a non-zero integer, or the non-dropped-out input to the logits layer\n      if num_classes is 0 or None.\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Invalid input tensor rank, expected 4, was: %d\' %\n                     len(input_shape))\n\n  with tf.variable_scope(scope, \'MobilenetV1\', [inputs], reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = mobilenet_v1_base(inputs, scope=scope,\n                                          min_depth=min_depth,\n                                          depth_multiplier=depth_multiplier,\n                                          conv_defs=conv_defs)\n      with tf.variable_scope(\'Logits\'):\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name=\'global_pool\')\n          end_points[\'global_pool\'] = net\n        else:\n          # Pooling with a fixed kernel size.\n          kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n          net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                                scope=\'AvgPool_1a\')\n          end_points[\'AvgPool_1a\'] = net\n        if not num_classes:\n          return net, end_points\n        # 1 x 1 x 1024\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      if prediction_fn:\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\n\nmobilenet_v1.default_image_size = 224\n\n\ndef wrapped_partial(func, *args, **kwargs):\n  partial_func = functools.partial(func, *args, **kwargs)\n  functools.update_wrapper(partial_func, func)\n  return partial_func\n\n\nmobilenet_v1_075 = wrapped_partial(mobilenet_v1, depth_multiplier=0.75)\nmobilenet_v1_050 = wrapped_partial(mobilenet_v1, depth_multiplier=0.50)\nmobilenet_v1_025 = wrapped_partial(mobilenet_v1, depth_multiplier=0.25)\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ndef mobilenet_v1_arg_scope(is_training=True,\n                           weight_decay=0.00004,\n                           stddev=0.09,\n                           regularize_depthwise=False):\n  """"""Defines the default MobilenetV1 arg scope.\n\n  Args:\n    is_training: Whether or not we\'re training the model.\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: The standard deviation of the trunctated normal weight initializer.\n    regularize_depthwise: Whether or not apply regularization on depthwise.\n\n  Returns:\n    An `arg_scope` to use for the mobilenet v1 model.\n  """"""\n  batch_norm_params = {\n      \'is_training\': is_training,\n      \'center\': True,\n      \'scale\': True,\n      \'decay\': 0.9997,\n      \'epsilon\': 0.001,\n  }\n\n  # Set weight_decay for weights in Conv and DepthSepConv layers.\n  weights_init = tf.truncated_normal_initializer(stddev=stddev)\n  regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n  if regularize_depthwise:\n    depthwise_regularizer = regularizer\n  else:\n    depthwise_regularizer = None\n  with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                      weights_initializer=weights_init,\n                      activation_fn=tf.nn.relu6, normalizer_fn=slim.batch_norm):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      with slim.arg_scope([slim.conv2d], weights_regularizer=regularizer):\n        with slim.arg_scope([slim.separable_conv2d],\n                            weights_regularizer=depthwise_regularizer) as sc:\n          return sc\n'"
tpu/models/official/mobilenet/vgg_preprocessing.py,60,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nThe preprocessing steps for VGG were introduced in the following technical\nreport:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n  """"""Crops the given image using the provided offsets and sizes.\n\n  Note that the method doesn\'t assume we know the input image size but it does\n  assume we know the input image rank.\n\n  Args:\n    image: an image of shape [height, width, channels].\n    offset_height: a scalar tensor indicating the height offset.\n    offset_width: a scalar tensor indicating the width offset.\n    crop_height: the height of the cropped image.\n    crop_width: the width of the cropped image.\n\n  Returns:\n    the cropped (and resized) image.\n\n  Raises:\n    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n      less than the crop size.\n  """"""\n  original_shape = tf.shape(image)\n\n  rank_assertion = tf.Assert(\n      tf.equal(tf.rank(image), 3),\n      [\'Rank of image must be equal to 3.\'])\n  with tf.control_dependencies([rank_assertion]):\n    cropped_shape = tf.stack([crop_height, crop_width, original_shape[2]])\n\n  size_assertion = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(original_shape[0], crop_height),\n          tf.greater_equal(original_shape[1], crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))\n\n  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n  # define the crop size.\n  with tf.control_dependencies([size_assertion]):\n    image = tf.slice(image, offsets, cropped_shape)\n  return tf.reshape(image, cropped_shape)\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n  """"""Crops the given list of images.\n\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n\n  Returns:\n    the image_list with cropped images.\n\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  """"""\n  if not image_list:\n    raise ValueError(\'Empty image_list.\')\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n        tf.equal(image_rank, 3),\n        [\'Wrong rank for tensor  %s [expected] [actual]\',\n         image_list[i].name, 3, image_rank])\n    rank_assertions.append(rank_assert)\n\n  with tf.control_dependencies([rank_assertions[0]]):\n    image_shape = tf.shape(image_list[0])\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(image_height, crop_height),\n          tf.greater_equal(image_width, crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    with tf.control_dependencies([rank_assertions[i]]):\n      shape = tf.shape(image)\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n        tf.equal(height, image_height),\n        [\'Wrong height for tensor %s [expected][actual]\',\n         image.name, height, image_height])\n    width_assert = tf.Assert(\n        tf.equal(width, image_width),\n        [\'Wrong width for tensor %s [expected][actual]\',\n         image.name, width, image_width])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  with tf.control_dependencies(asserts):\n    max_offset_height = tf.reshape(image_height - crop_height + 1, [])\n  with tf.control_dependencies(asserts):\n    max_offset_width = tf.reshape(image_width - crop_width + 1, [])\n  offset_height = tf.random_uniform(\n      [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random_uniform(\n      [], maxval=max_offset_width, dtype=tf.int32)\n\n  return [_crop(image, offset_height, offset_width,\n                crop_height, crop_width) for image in image_list]\n\n\ndef _central_crop(image_list, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    the list of cropped images.\n  """"""\n  outputs = []\n  for image in image_list:\n    image_height = tf.shape(image)[0]\n    image_width = tf.shape(image)[1]\n\n    offset_height = (image_height - crop_height) / 2\n    offset_width = (image_width - crop_width) / 2\n\n    outputs.append(_crop(image, offset_height, offset_width,\n                         crop_height, crop_width))\n  return outputs\n\n\ndef _mean_image_subtraction(image, means):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n  num_channels = image.get_shape().as_list()[-1]\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] -= means[i]\n  return tf.concat(axis=2, values=channels)\n\n\ndef _smallest_size_at_least(height, width, smallest_side):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: and int32 scalar tensor indicating the new width.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  height = tf.to_float(height)\n  width = tf.to_float(width)\n  smallest_side = tf.to_float(smallest_side)\n\n  scale = tf.cond(tf.greater(height, width),\n                  lambda: smallest_side / width,\n                  lambda: smallest_side / height)\n  new_height = tf.to_int32(height * scale)\n  new_width = tf.to_int32(width * scale)\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, smallest_side):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  shape = tf.shape(image)\n  height = shape[0]\n  width = shape[1]\n  new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n  image = tf.expand_dims(image, 0)\n  resized_image = tf.image.resize_bilinear(image, [new_height, new_width],\n                                           align_corners=False)\n  resized_image = tf.squeeze(resized_image)\n  resized_image.set_shape([None, None, 3])\n  return resized_image\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         resize_side_min=_RESIZE_SIDE_MIN,\n                         resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  resize_side = tf.random_uniform(\n      [], minval=resize_side_min, maxval=resize_side_max+1, dtype=tf.int32)\n\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _random_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  image = tf.image.random_flip_left_right(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_for_eval(image, output_height, output_width, resize_side):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side: The smallest side of the image for aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _central_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_image(image, output_height, output_width, is_training=False,\n                     resize_side_min=_RESIZE_SIDE_MIN,\n                     resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, then this value\n      is used for rescaling.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, this value is\n      ignored. Otherwise, the resize side is sampled from\n        [resize_size_min, resize_size_max].\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    image = preprocess_for_train(image, output_height, output_width,\n                                 resize_side_min, resize_side_max)\n  else:\n    image = preprocess_for_eval(image, output_height, output_width,\n                                resize_side_min)\n  # Scale to (-1,1). TODO(currutia): check whether this is actually needed\n  image = tf.multiply(image, 1. / 128.)\n  return image\n'"
tpu/models/official/resnet/__init__.py,0,b''
tpu/models/official/resnet/imagenet_input.py,34,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Efficient ImageNet input pipeline using tf.data.Dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nfrom collections import namedtuple\nimport functools\nimport os\nimport tensorflow as tf\nfrom official.resnet import resnet_preprocessing\n\ndef image_serving_input_fn():\n  """"""Serving input fn for raw images.""""""\n\n  def _preprocess_image(image_bytes):\n    """"""Preprocess a single raw image.""""""\n    image = resnet_preprocessing.preprocess_image(\n        image_bytes=image_bytes, is_training=False)\n    return image\n\n  image_bytes_list = tf.placeholder(\n      shape=[None],\n      dtype=tf.string,\n  )\n  images = tf.map_fn(\n      _preprocess_image, image_bytes_list, back_prop=False, dtype=tf.float32)\n  return tf.estimator.export.ServingInputReceiver(\n      images, {\'image_bytes\': image_bytes_list})\n\n\nclass ImageNetTFExampleInput(object):\n  """"""Base class for ImageNet input_fn generator.\n\n  Args:\n    is_training: `bool` for whether the input is for training\n    use_bfloat16: If True, use bfloat16 precision; else use float32.\n    transpose_input: \'bool\' for whether to use the double transpose trick\n    num_cores: `int` for the number of TPU cores\n  """"""\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self,\n               is_training,\n               use_bfloat16,\n               num_cores=8,\n               image_size=224,\n               transpose_input=False):\n    self.image_preprocessing_fn = resnet_preprocessing.preprocess_image\n    self.is_training = is_training\n    self.use_bfloat16 = use_bfloat16\n    self.num_cores = num_cores\n    self.transpose_input = transpose_input\n    self.image_size = image_size\n\n  def set_shapes(self, batch_size, images, labels):\n    """"""Statically set the batch_size dimension.""""""\n    if self.transpose_input:\n      images.set_shape(images.get_shape().merge_with(\n          tf.TensorShape([None, None, None, batch_size])))\n      labels.set_shape(labels.get_shape().merge_with(\n          tf.TensorShape([batch_size])))\n    else:\n      images.set_shape(images.get_shape().merge_with(\n          tf.TensorShape([batch_size, None, None, None])))\n      labels.set_shape(labels.get_shape().merge_with(\n          tf.TensorShape([batch_size])))\n\n    return images, labels\n\n  def dataset_parser(self, value):\n    """"""Parses an image and its label from a serialized ResNet-50 TFExample.\n\n    Args:\n      value: serialized string containing an ImageNet TFExample.\n\n    Returns:\n      Returns a tuple of (image, label) from the TFExample.\n    """"""\n    keys_to_features = {\n        \'image/encoded\': tf.FixedLenFeature((), tf.string, \'\'),\n        \'image/format\': tf.FixedLenFeature((), tf.string, \'jpeg\'),\n        \'image/class/label\': tf.FixedLenFeature([], tf.int64, -1),\n    }\n\n    parsed = tf.parse_single_example(value, keys_to_features)\n    image_bytes = tf.reshape(parsed[\'image/encoded\'], shape=[])\n\n    image = self.image_preprocessing_fn(\n        image_bytes=image_bytes,\n        is_training=self.is_training,\n        image_size=self.image_size,\n        use_bfloat16=self.use_bfloat16)\n\n    label = tf.cast(\n        tf.reshape(parsed[\'image/class/label\'], shape=[]), dtype=tf.int32)\n\n    return image, label\n\n  @abc.abstractmethod\n  def make_source_dataset(self, index, num_hosts):\n    """"""Makes dataset of serialized TFExamples.\n\n    The returned dataset will contain `tf.string` tensors, but these strings are\n    serialized `TFExample` records that will be parsed by `dataset_parser`.\n\n    If self.is_training, the dataset should be infinite.\n\n    Args:\n      index: current host index.\n      num_hosts: total number of hosts.\n\n    Returns:\n      A `tf.data.Dataset` object.\n    """"""\n    return\n\n  def input_fn(self, params):\n    """"""Input function which provides a single batch for train or eval.\n\n    Args:\n      params: `dict` of parameters passed from the `TPUEstimator`.\n          `params[\'batch_size\']` is always provided and should be used as the\n          effective batch size.\n\n    Returns:\n      A `tf.data.Dataset` object.\n    """"""\n\n    # Retrieves the batch size for the current shard. The # of shards is\n    # computed according to the input pipeline deployment. See\n    # tf.contrib.tpu.RunConfig for details.\n    batch_size = params[\'batch_size\']\n\n    # TODO(dehao): Replace the following with params[\'context\'].current_host\n    current_host = params[""context""].current_input_fn_deployment()[1]\n    dataset = self.make_source_dataset(current_host,\n                                       params[\'context\'].num_hosts)\n\n    # Use the fused map-and-batch operation.\n    #\n    # For XLA, we must used fixed shapes. Because we repeat the source training\n    # dataset indefinitely, we can use `drop_remainder=True` to get fixed-size\n    # batches without dropping any training examples.\n    #\n    # When evaluating, `drop_remainder=True` prevents accidentally evaluating\n    # the same image twice by dropping the final batch if it is less than a full\n    # batch size. As long as this validation is done with consistent batch size,\n    # exactly the same images will be used.\n    dataset = dataset.apply(\n        tf.contrib.data.map_and_batch(\n            self.dataset_parser, batch_size=batch_size,\n            num_parallel_batches=self.num_cores, drop_remainder=True))\n\n    # Transpose for performance on TPU\n    if self.transpose_input:\n      dataset = dataset.map(\n          lambda images, labels: (tf.transpose(images, [1, 2, 3, 0]), labels),\n          num_parallel_calls=self.num_cores)\n\n    # Assign static batch size dimension\n    dataset = dataset.map(functools.partial(self.set_shapes, batch_size))\n\n    # Prefetch overlaps in-feed with training\n    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n    return dataset\n\n\nclass ImageNetInput(ImageNetTFExampleInput):\n  """"""Generates ImageNet input_fn from a series of TFRecord files.\n\n  The training data is assumed to be in TFRecord format with keys as specified\n  in the dataset_parser below, sharded across 1024 files, named sequentially:\n\n      train-00000-of-01024\n      train-00001-of-01024\n      ...\n      train-01023-of-01024\n\n  The validation data is in the same format but sharded in 128 files.\n\n  The format of the data required is created by the script at:\n      https://github.com/tensorflow/tpu/blob/master/tools/datasets/imagenet_to_gcs.py\n  """"""\n\n  def __init__(self,\n               is_training,\n               use_bfloat16,\n               transpose_input,\n               data_dir,\n               image_size=224,\n               num_parallel_calls=64,\n               cache=False):\n    """"""Create an input from TFRecord files.\n\n    Args:\n      is_training: `bool` for whether the input is for training\n      use_bfloat16: If True, use bfloat16 precision; else use float32.\n      transpose_input: \'bool\' for whether to use the double transpose trick\n      data_dir: `str` for the directory of the training and validation data;\n          if \'null\' (the literal string \'null\') or implicitly False\n          then construct a null pipeline, consisting of empty images\n          and blank labels.\n      num_parallel_calls: concurrency level to use when reading data from disk.\n      cache: if true, fill the dataset by repeating from its cache\n    """"""\n    super(ImageNetInput, self).__init__(\n        is_training=is_training,\n        image_size=image_size,\n        use_bfloat16=use_bfloat16,\n        transpose_input=transpose_input)\n    self.data_dir = data_dir\n    # TODO(b/112427086):  simplify the choice of input source\n    if self.data_dir == \'null\' or not self.data_dir:\n      self.data_dir = None\n    self.num_parallel_calls = num_parallel_calls\n    self.cache = cache\n\n  def _get_null_input(self, data):\n    """"""Returns a null image (all black pixels).\n\n    Args:\n      data: element of a dataset, ignored in this method, since it produces\n          the same null image regardless of the element.\n\n    Returns:\n      a tensor representing a null image.\n    """"""\n    del data  # Unused since output is constant regardless of input\n    return tf.zeros([self.image_size, self.image_size, 3], tf.bfloat16\n                    if self.use_bfloat16 else tf.float32)\n\n  def dataset_parser(self, value):\n    """"""See base class.""""""\n    if not self.data_dir:\n      return value, tf.constant(0, tf.int32)\n    return super(ImageNetInput, self).dataset_parser(value)\n\n  def make_source_dataset(self, index, num_hosts):\n    """"""See base class.""""""\n    if not self.data_dir:\n      tf.logging.info(\'Undefined data_dir implies null input\')\n      return tf.data.Dataset.range(1).repeat().map(self._get_null_input)\n\n    # Shuffle the filenames to ensure better randomization.\n    file_pattern = os.path.join(\n        self.data_dir, \'train-*\' if self.is_training else \'validation-*\')\n\n    # For multi-host training, we want each hosts to always process the same\n    # subset of files.  Each host only sees a subset of the entire dataset,\n    # allowing us to cache larger datasets in memory.\n    dataset = tf.data.Dataset.list_files(file_pattern, shuffle=False)\n    dataset = dataset.shard(num_hosts, index)\n\n    if self.is_training and not self.cache:\n      dataset = dataset.repeat()\n\n    def fetch_dataset(filename):\n      buffer_size = 8 * 1024 * 1024  # 8 MiB per file\n      dataset = tf.data.TFRecordDataset(filename, buffer_size=buffer_size)\n      return dataset\n\n    # Read the data from disk in parallel\n    dataset = dataset.apply(\n        tf.contrib.data.parallel_interleave(\n            fetch_dataset, cycle_length=self.num_parallel_calls, sloppy=True))\n\n    if self.cache:\n      dataset = dataset.cache().apply(\n          tf.contrib.data.shuffle_and_repeat(1024 * 16))\n    else:\n      dataset = dataset.shuffle(1024)\n    return dataset\n\n\n# Defines a selection of data from a Cloud Bigtable.\nBigtableSelection = namedtuple(\'BigtableSelection\',\n                               [\'project\',\n                                \'instance\',\n                                \'table\',\n                                \'prefix\',\n                                \'column_family\',\n                                \'column_qualifier\'])\n\n\nclass ImageNetBigtableInput(ImageNetTFExampleInput):\n  """"""Generates ImageNet input_fn from a Bigtable for training or evaluation.\n  """"""\n\n  def __init__(self, is_training, use_bfloat16, transpose_input, selection):\n    """"""Constructs an ImageNet input from a BigtableSelection.\n\n    Args:\n      is_training: `bool` for whether the input is for training\n      use_bfloat16: If True, use bfloat16 precision; else use float32.\n      transpose_input: \'bool\' for whether to use the double transpose trick\n      selection: a BigtableSelection specifying a part of a Bigtable.\n    """"""\n    super(ImageNetBigtableInput, self).__init__(\n        is_training=is_training,\n        use_bfloat16=use_bfloat16,\n        transpose_input=transpose_input)\n    self.selection = selection\n\n  def make_source_dataset(self, index, num_hosts):\n    """"""See base class.""""""\n    data = self.selection\n    client = tf.contrib.cloud.BigtableClient(data.project, data.instance)\n    table = client.table(data.table)\n    ds = table.parallel_scan_prefix(data.prefix,\n                                    columns=[(data.column_family,\n                                              data.column_qualifier)])\n    # The Bigtable datasets will have the shape (row_key, data)\n    ds_data = ds.map(lambda index, data: data)\n\n    if self.is_training:\n      ds_data = ds_data.repeat()\n\n    return ds_data\n'"
tpu/models/official/resnet/lars_util.py,6,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Enable Layer-wise Adaptive Rate Scaling optimizer in ResNet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport tensorflow as tf\n\nFLAGS = flags.FLAGS\n\n\ndef poly_rate_schedule(current_epoch,\n                       poly_rate=0.0):\n  """"""Handles linear scaling rule, gradual warmup, and LR decay.\n\n  The learning rate starts at 0, then it increases linearly per step.  After\n  FLAGS.poly_warmup_epochs, we reach the base learning rate (scaled to account\n  for batch size). The learning rate is then decayed using a polynomial rate\n  decay schedule with power 2.0.\n\n  Args:\n    current_epoch: `Tensor` for current epoch.\n    poly_rate: Polynomial decay rate.\n\n  Returns:\n    A scaled `Tensor` for current learning rate.\n  """"""\n\n  if FLAGS.train_batch_size <= 16384:\n    plr = 25.0\n    w_epochs = 5\n  elif FLAGS.train_batch_size == 32768:\n    plr = 34.0\n    w_epochs = 12\n  else:\n    plr = 41.0\n    w_epochs = 16\n\n  # Override default poly learning rate and warmup epochs\n  if poly_rate > 0.0:\n    plr = poly_rate\n\n  wrate = (plr * current_epoch / w_epochs)\n  w_steps = (w_epochs * FLAGS.num_train_images // FLAGS.train_batch_size)\n  min_step = tf.constant(1, dtype=tf.int64)\n  global_step = tf.train.get_or_create_global_step()\n  decay_steps = tf.maximum(min_step, tf.subtract(global_step, w_steps))\n  poly_rate = tf.train.polynomial_decay(\n      plr,\n      decay_steps,\n      FLAGS.train_steps - w_steps + 1,\n      power=2.0)\n  decay_rate = tf.where(current_epoch <= w_epochs, wrate, poly_rate)\n  return decay_rate\n\n\ndef init_lars_optimizer(current_epoch):\n  """"""Initialize the LARS Optimizer.""""""\n\n  learning_rate = poly_rate_schedule(current_epoch, FLAGS.poly_rate)\n  optimizer = tf.contrib.opt.LARSOptimizer(\n      learning_rate,\n      momentum=FLAGS.momentum,\n      weight_decay=FLAGS.weight_decay,\n      skip_list=[\'batch_normalization\', \'bias\'])\n  return optimizer\n'"
tpu/models/official/resnet/resnet_main.py,75,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Train a ResNet-50 model on ImageNet on TPU.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nimport numpy as np\n\nfrom absl import flags\nimport absl.logging as _logging  # pylint: disable=unused-import\nimport tensorflow as tf\nfrom official.resnet import imagenet_input\nfrom official.resnet import lars_util\nfrom official.resnet import resnet_model\nfrom tensorflow.contrib import summary\nfrom tensorflow.contrib.training.python.training import evaluation\nfrom tensorflow.python.estimator import estimator\n\nFLAGS = flags.FLAGS\n\nFAKE_DATA_DIR = \'gs://cloud-tpu-test-datasets/fake_imagenet\'\n\nflags.DEFINE_bool(\n    \'use_tpu\', default=True,\n    help=(\'Use TPU to execute the model for training and evaluation. If\'\n          \' --use_tpu=false, will use whatever devices are available to\'\n          \' TensorFlow by default (e.g. CPU and GPU)\'))\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'tpu\', default=None,\n    help=\'The Cloud TPU to use for training. This should be either the name \'\n    \'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\')\n\nflags.DEFINE_string(\n    \'gcp_project\', default=None,\n    help=\'Project name for the Cloud TPU-enabled project. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\n\nflags.DEFINE_string(\n    \'tpu_zone\', default=None,\n    help=\'GCE zone where the Cloud TPU is located in. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\n\n# Model specific flags\nflags.DEFINE_string(\n    \'data_dir\', default=FAKE_DATA_DIR,\n    help=(\'The directory where the ImageNet input data is stored. Please see\'\n          \' the README.md for the expected data format.\'))\n\nflags.DEFINE_string(\n    \'model_dir\', default=None,\n    help=(\'The directory where the model and training/evaluation summaries are\'\n          \' stored.\'))\n\nflags.DEFINE_integer(\n    \'resnet_depth\', default=50,\n    help=(\'Depth of ResNet model to use. Must be one of {18, 34, 50, 101, 152,\'\n          \' 200}. ResNet-18 and 34 use the pre-activation residual blocks\'\n          \' without bottleneck layers. The other models use pre-activation\'\n          \' bottleneck layers. Deeper models require more training time and\'\n          \' more memory and may require reducing --train_batch_size to prevent\'\n          \' running out of memory.\'))\n\nflags.DEFINE_string(\n    \'mode\', default=\'train_and_eval\',\n    help=\'One of {""train_and_eval"", ""train"", ""eval""}.\')\n\nflags.DEFINE_integer(\n    \'train_steps\', default=112603,\n    help=(\'The number of steps to use for training. Default is 112603 steps\'\n          \' which is approximately 90 epochs at batch size 1024. This flag\'\n          \' should be adjusted according to the --train_batch_size flag.\'))\n\nflags.DEFINE_integer(\n    \'train_batch_size\', default=1024, help=\'Batch size for training.\')\n\nflags.DEFINE_integer(\n    \'eval_batch_size\', default=1024, help=\'Batch size for evaluation.\')\n\nflags.DEFINE_integer(\n    \'num_train_images\', default=1281167, help=\'Size of training data set.\')\n\nflags.DEFINE_integer(\n    \'num_eval_images\', default=50000, help=\'Size of evaluation data set.\')\n\nflags.DEFINE_integer(\n    \'num_label_classes\', default=1000, help=\'Number of classes, at least 2\')\n\nflags.DEFINE_integer(\n    \'steps_per_eval\', default=1251,\n    help=(\'Controls how often evaluation is performed. Since evaluation is\'\n          \' fairly expensive, it is advised to evaluate as infrequently as\'\n          \' possible (i.e. up to --train_steps, which evaluates the model only\'\n          \' after finishing the entire training regime).\'))\n\nflags.DEFINE_integer(\n    \'eval_timeout\',\n    default=None,\n    help=\'Maximum seconds between checkpoints before evaluation terminates.\')\n\nflags.DEFINE_bool(\n    \'skip_host_call\', default=False,\n    help=(\'Skip the host_call which is executed every training step. This is\'\n          \' generally used for generating training summaries (train loss,\'\n          \' learning rate, etc...). When --skip_host_call=false, there could\'\n          \' be a performance drop if host_call function is slow and cannot\'\n          \' keep up with the TPU-side computation.\'))\n\nflags.DEFINE_integer(\n    \'iterations_per_loop\', default=1251,\n    help=(\'Number of steps to run on TPU before outfeeding metrics to the CPU.\'\n          \' If the number of iterations in the loop would exceed the number of\'\n          \' train steps, the loop will exit before reaching\'\n          \' --iterations_per_loop. The larger this value is, the higher the\'\n          \' utilization on the TPU.\'))\n\nflags.DEFINE_integer(\n    \'num_parallel_calls\', default=64,\n    help=(\'Number of parallel threads in CPU for the input pipeline\'))\n\nflags.DEFINE_integer(\n    \'num_cores\', default=8,\n    help=(\'Number of TPU cores. For a single TPU device, this is 8 because each\'\n          \' TPU has 4 chips each with 2 cores.\'))\n\nflags.DEFINE_string(\n    \'bigtable_project\', None,\n    \'The Cloud Bigtable project.  If None, --gcp_project will be used.\')\nflags.DEFINE_string(\n    \'bigtable_instance\', None,\n    \'The Cloud Bigtable instance to load data from.\')\nflags.DEFINE_string(\n    \'bigtable_table\', \'imagenet\',\n    \'The Cloud Bigtable table to load data from.\')\nflags.DEFINE_string(\n    \'bigtable_train_prefix\', \'train_\',\n    \'The prefix identifying training rows.\')\nflags.DEFINE_string(\n    \'bigtable_eval_prefix\', \'validation_\',\n    \'The prefix identifying evaluation rows.\')\nflags.DEFINE_string(\n    \'bigtable_column_family\', \'tfexample\',\n    \'The column family storing TFExamples.\')\nflags.DEFINE_string(\n    \'bigtable_column_qualifier\', \'example\',\n    \'The column name storing TFExamples.\')\n\nflags.DEFINE_string(\n    \'data_format\', default=\'channels_last\',\n    help=(\'A flag to override the data format used in the model. The value\'\n          \' is either channels_first or channels_last. To run the network on\'\n          \' CPU or TPU, channels_last should be used. For GPU, channels_first\'\n          \' will improve performance.\'))\n\n# TODO(chrisying): remove this flag once --transpose_tpu_infeed flag is enabled\n# by default for TPU\nflags.DEFINE_bool(\n    \'transpose_input\', default=True,\n    help=\'Use TPU double transpose optimization\')\n\nflags.DEFINE_string(\n    \'export_dir\',\n    default=None,\n    help=(\'The directory where the exported SavedModel will be stored.\'))\n\nflags.DEFINE_string(\n    \'precision\', default=\'bfloat16\',\n    help=(\'Precision to use; one of: {bfloat16, float32}\'))\n\nflags.DEFINE_float(\n    \'base_learning_rate\', default=0.1,\n    help=(\'Base learning rate when train batch size is 256.\'))\n\nflags.DEFINE_float(\n    \'momentum\', default=0.9,\n    help=(\'Momentum parameter used in the MomentumOptimizer.\'))\n\nflags.DEFINE_float(\n    \'weight_decay\', default=1e-4,\n    help=(\'Weight decay coefficiant for l2 regularization.\'))\n\nflags.DEFINE_integer(\'log_step_count_steps\', 64, \'The number of steps at \'\n                     \'which the global step information is logged.\')\n\nflags.DEFINE_bool(\'enable_lars\',\n                  default=False,\n                  help=(\'Enable LARS optimizer for large batch training.\'))\n\nflags.DEFINE_float(\'poly_rate\', default=0.0,\n                   help=(\'Set LARS/Poly learning rate.\'))\n\nflags.DEFINE_bool(\n    \'use_cache\', default=True, help=(\'Enable cache for training input.\'))\n\nflags.DEFINE_string(\n    \'img_num_per_cls_file\', default=None,\n    help=(\'File location for image number per class.\'))\n\n# Parameters for class balanced focal loss\nflags.DEFINE_float(\n    \'beta\', default=0.999,\n    help=(\'Beta for calculating number of effective data.\'))\n\nflags.DEFINE_float(\n    \'gamma\', default=1.0,\n    help=(\'Focal loss: gamma.\'))\n\n# Learning rate schedule\nLR_SCHEDULE = [    # (multiplier, epoch to start) tuples\n    (1.0, 5), (0.1, 30), (0.01, 60), (0.001, 80)\n]\n\n# The input tensor is in the range of [0, 255], we need to scale them to the\n# range of [0, 1]\n# # ImageNet\n# MEAN_RGB = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n# STDDEV_RGB = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n\n# iNat\nMEAN_RGB = [0.466 * 255, 0.471 * 255, 0.380 * 255]\t\nSTDDEV_RGB = [0.195 * 255, 0.194 * 255, 0.192 * 255]\n\n\ndef learning_rate_schedule(current_epoch):\n  """"""Handles linear scaling rule, gradual warmup, and LR decay.\n\n  The learning rate starts at 0, then it increases linearly per step.\n  After 5 epochs we reach the base learning rate (scaled to account\n    for batch size).\n  After 30, 60 and 80 epochs the learning rate is divided by 10.\n  After 90 epochs training stops and the LR is set to 0. This ensures\n    that we train for exactly 90 epochs for reproducibility.\n\n  Args:\n    current_epoch: `Tensor` for current epoch.\n\n  Returns:\n    A scaled `Tensor` for current learning rate.\n  """"""\n  scaled_lr = FLAGS.base_learning_rate * (FLAGS.train_batch_size / 256.0)\n\n  decay_rate = (scaled_lr * LR_SCHEDULE[0][0] *\n                current_epoch / LR_SCHEDULE[0][1])\n  for mult, start_epoch in LR_SCHEDULE:\n    decay_rate = tf.where(current_epoch < start_epoch,\n                          decay_rate, scaled_lr * mult)\n  return decay_rate\n\n\ndef focal_loss(labels, logits, alpha, gamma):\n  """"""Compute the focal loss between `logits` and the ground truth `labels`.\n\n  Focal loss = -alpha_t * (1-pt)^gamma * log(pt)\n  where pt is the probability of being classified to the true class.\n  pt = p (if true class), otherwise pt = 1 - p. p = sigmoid(logit).\n\n  Args:\n    labels: A float32 tensor of size [batch, num_classes].\n    logits: A float32 tensor of size [batch, num_classes].\n    alpha: A float32 tensor of size [batch_size]\n      specifying per-example weight for balanced cross entropy.\n    gamma: A float32 scalar modulating loss from hard and easy examples.\n  Returns:\n    focal_loss: A float32 scalar representing normalized total loss.\n  """"""\n  with tf.name_scope(\'focal_loss\'):\n    logits = tf.cast(logits, dtype=tf.float32)\n    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=labels, logits=logits)\n\n    # positive_label_mask = tf.equal(labels, 1.0)\n    # probs = tf.sigmoid(logits)\n    # probs_gt = tf.where(positive_label_mask, probs, 1.0 - probs)\n    # # With gamma < 1, the implementation could produce NaN during back prop.\n    # modulator = tf.pow(1.0 - probs_gt, gamma)\n\n    # A numerically stable implementation of modulator.\n    if gamma == 0.0:\n      modulator = 1.0\n    else:\n      modulator = tf.exp(-gamma * labels * logits - gamma * tf.log1p(\n          tf.exp(-1.0 * logits)))\n\n    loss = modulator * cross_entropy\n\n    weighted_loss = alpha * loss\n    focal_loss = tf.reduce_sum(weighted_loss)\n    # Normalize by the total number of positive samples.\n    focal_loss /= tf.reduce_sum(labels)\n  return focal_loss\n\n\ndef resnet_model_fn(features, labels, mode, params):\n  """"""The model_fn for ResNet to be used with TPUEstimator.\n\n  Args:\n    features: `Tensor` of batched images.\n    labels: `Tensor` of labels for the data samples\n    mode: one of `tf.estimator.ModeKeys.{TRAIN,EVAL,PREDICT}`\n    params: `dict` of parameters passed to the model from the TPUEstimator,\n        `params[\'batch_size\']` is always provided and should be used as the\n        effective batch size.\n\n  Returns:\n    A `TPUEstimatorSpec` for the model\n  """"""\n  if isinstance(features, dict):\n    features = features[\'feature\']\n\n  # In most cases, the default data format NCHW instead of NHWC should be\n  # used for a significant performance boost on GPU/TPU. NHWC should be used\n  # only if the network needs to be run on CPU since the pooling operations\n  # are only supported on NHWC.\n  if FLAGS.data_format == \'channels_first\':\n    assert not FLAGS.transpose_input    # channels_first only for GPU\n    features = tf.transpose(features, [0, 3, 1, 2])\n\n  if FLAGS.transpose_input and mode != tf.estimator.ModeKeys.PREDICT:\n    features = tf.transpose(features, [3, 0, 1, 2])  # HWCN to NHWC\n\n  # Normalize the image to zero mean and unit variance.\n  features -= tf.constant(MEAN_RGB, shape=[1, 1, 3], dtype=features.dtype)\n  features /= tf.constant(STDDEV_RGB, shape=[1, 1, 3], dtype=features.dtype)\n\n  # This nested function allows us to avoid duplicating the logic which\n  # builds the network, for different values of --precision.\n  def build_network():\n    network = resnet_model.resnet_v1(\n        resnet_depth=FLAGS.resnet_depth,\n        num_classes=FLAGS.num_label_classes,\n        data_format=FLAGS.data_format)\n    return network(\n        inputs=features, is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n\n  if FLAGS.precision == \'bfloat16\':\n    with tf.contrib.tpu.bfloat16_scope():\n      logits = build_network()\n    logits = tf.cast(logits, tf.float32)\n  elif FLAGS.precision == \'float32\':\n    logits = build_network()\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    predictions = {\n        \'classes\': tf.argmax(logits, axis=1),\n        \'probabilities\': tf.sigmoid(logits, name=\'sigmoid_tensor\')\n    }\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=predictions,\n        export_outputs={\n            \'classify\': tf.estimator.export.PredictOutput(predictions)\n        })\n\n  # If necessary, in the model_fn, use params[\'batch_size\'] instead the batch\n  # size flags (--train_batch_size or --eval_batch_size).\n  batch_size = params[\'batch_size\']   # pylint: disable=unused-variable\n\n  # Calculate loss, which includes classification loss and L2 regularization.\n  one_hot_labels = tf.one_hot(labels, FLAGS.num_label_classes)\n\n  # Normalized weights based on inverse number of effective data per class.\n  img_num_per_cls = [int(line.strip()) for line in open(\n        FLAGS.img_num_per_cls_file, \'r\')]\n  effective_num = 1.0 - np.power(FLAGS.beta, img_num_per_cls)\n  weights = (1.0 - FLAGS.beta) / np.array(effective_num)\n  weights = weights / np.sum(weights) * FLAGS.num_label_classes\n\n  weights = tf.cast(weights, dtype=tf.float32)\n  weights = tf.expand_dims(weights, 0)\n  weights = tf.tile(weights, [tf.shape(one_hot_labels)[0], 1]) * one_hot_labels\n  weights = tf.reduce_sum(weights, axis=1)\n  weights = tf.expand_dims(weights, 1)\n  weights = tf.tile(weights, [1, FLAGS.num_label_classes])\n\n  classification_loss = focal_loss(\n      one_hot_labels, logits, weights, FLAGS.gamma)\n\n  # Add weight decay to the loss for non-batch-normalization variables.\n  loss = classification_loss + FLAGS.weight_decay * tf.add_n(\n      [tf.nn.l2_loss(v) for v in tf.trainable_variables()\n       if \'batch_normalization\' not in v.name and \'dense/bias\' not in v.name])\n\n  host_call = None\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    # Compute the current epoch and associated learning rate from global_step.\n    global_step = tf.train.get_global_step()\n    steps_per_epoch = FLAGS.num_train_images / FLAGS.train_batch_size\n    current_epoch = (tf.cast(global_step, tf.float32) /\n                     steps_per_epoch)\n    # LARS is a large batch optimizer. LARS enables higher accuracy at batch 16K\n    # and larger batch sizes.\n    if FLAGS.train_batch_size >= 16384 and FLAGS.enable_lars:\n      learning_rate = 0.0\n      optimizer = lars_util.init_lars_optimizer(current_epoch)\n    else:\n      learning_rate = learning_rate_schedule(current_epoch)\n      optimizer = tf.train.MomentumOptimizer(\n          learning_rate=learning_rate,\n          momentum=FLAGS.momentum,\n          use_nesterov=True)\n    if FLAGS.use_tpu:\n      # When using TPU, wrap the optimizer with CrossShardOptimizer which\n      # handles synchronization details between different TPU cores. To the\n      # user, this should look like regular synchronous training.\n      optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n    # Batch normalization requires UPDATE_OPS to be added as a dependency to\n    # the train operation.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.minimize(loss, global_step)\n\n    if not FLAGS.skip_host_call:\n      def host_call_fn(gs, fl_loss, loss, lr, ce):\n        """"""Training host call. Creates scalar summaries for training metrics.\n\n        This function is executed on the CPU and should not directly reference\n        any Tensors in the rest of the `model_fn`. To pass Tensors from the\n        model to the `metric_fn`, provide as part of the `host_call`. See\n        https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n        for more information.\n\n        Arguments should match the list of `Tensor` objects passed as the second\n        element in the tuple passed to `host_call`.\n\n        Args:\n          gs: `Tensor with shape `[batch]` for the global_step.\n          fl_loss: `Tensor` with shape `[batch]` for the training focal loss.\n          loss: `Tensor` with shape `[batch]` for the training loss.\n          lr: `Tensor` with shape `[batch]` for the learning_rate.\n          ce: `Tensor` with shape `[batch]` for the current_epoch.\n\n        Returns:\n          List of summary ops to run on the CPU host.\n        """"""\n        gs = gs[0]\n        # Host call fns are executed FLAGS.iterations_per_loop times after one\n        # TPU loop is finished, setting max_queue value to the same as number of\n        # iterations will make the summary writer only flush the data to storage\n        # once per loop.\n        with summary.create_file_writer(\n            FLAGS.model_dir, max_queue=FLAGS.iterations_per_loop).as_default():\n          with summary.always_record_summaries():\n            summary.scalar(\'focal_loss\', fl_loss[0], step=gs)\n            summary.scalar(\'loss\', loss[0], step=gs)\n            summary.scalar(\'learning_rate\', lr[0], step=gs)\n            summary.scalar(\'current_epoch\', ce[0], step=gs)\n\n            return summary.all_summary_ops()\n\n      # To log the loss, current learning rate, and epoch for Tensorboard, the\n      # summary op needs to be run on the host CPU via host_call. host_call\n      # expects [batch_size, ...] Tensors, thus reshape to introduce a batch\n      # dimension. These Tensors are implicitly concatenated to\n      # [params[\'batch_size\']].\n      gs_t = tf.reshape(global_step, [1])\n      fl_loss_t = tf.reshape(classification_loss, [1])\n      loss_t = tf.reshape(loss, [1])\n      lr_t = tf.reshape(learning_rate, [1])\n      ce_t = tf.reshape(current_epoch, [1])\n\n      host_call = (host_call_fn, [gs_t, fl_loss_t, loss_t, lr_t, ce_t])\n\n  else:\n    train_op = None\n\n  eval_metrics = None\n  if mode == tf.estimator.ModeKeys.EVAL:\n    def metric_fn(labels, logits):\n      """"""Evaluation metric function. Evaluates accuracy.\n\n      This function is executed on the CPU and should not directly reference\n      any Tensors in the rest of the `model_fn`. To pass Tensors from the model\n      to the `metric_fn`, provide as part of the `eval_metrics`. See\n      https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n      for more information.\n\n      Arguments should match the list of `Tensor` objects passed as the second\n      element in the tuple passed to `eval_metrics`.\n\n      Args:\n        labels: `Tensor` with shape `[batch]`.\n        logits: `Tensor` with shape `[batch, num_classes]`.\n\n      Returns:\n        A dict of the metrics to return from evaluation.\n      """"""\n      predictions = tf.argmax(logits, axis=1)\n      top_1_accuracy = tf.metrics.accuracy(labels, predictions)\n      in_top_5 = tf.cast(tf.nn.in_top_k(logits, labels, 5), tf.float32)\n      top_5_accuracy = tf.metrics.mean(in_top_5)\n\n      return {\n          \'top_1_accuracy\': top_1_accuracy,\n          \'top_5_accuracy\': top_5_accuracy,\n      }\n\n    eval_metrics = (metric_fn, [labels, logits])\n\n  return tf.contrib.tpu.TPUEstimatorSpec(\n      mode=mode,\n      loss=loss,\n      train_op=train_op,\n      host_call=host_call,\n      eval_metrics=eval_metrics)\n\n\ndef _verify_non_empty_string(value, field_name):\n  """"""Ensures that a given proposed field value is a non-empty string.\n\n  Args:\n    value:  proposed value for the field.\n    field_name:  string name of the field, e.g. `project`.\n\n  Returns:\n    The given value, provided that it passed the checks.\n\n  Raises:\n    ValueError:  the value is not a string, or is a blank string.\n  """"""\n  if not isinstance(value, str):\n    raise ValueError(\n        \'Bigtable parameter ""%s"" must be a string.\' % field_name)\n  if not value:\n    raise ValueError(\n        \'Bigtable parameter ""%s"" must be non-empty.\' % field_name)\n  return value\n\n\ndef _select_tables_from_flags():\n  """"""Construct training and evaluation Bigtable selections from flags.\n\n  Returns:\n    [training_selection, evaluation_selection]\n  """"""\n  project = _verify_non_empty_string(\n      FLAGS.bigtable_project or FLAGS.gcp_project,\n      \'project\')\n  instance = _verify_non_empty_string(FLAGS.bigtable_instance, \'instance\')\n  table = _verify_non_empty_string(FLAGS.bigtable_table, \'table\')\n  train_prefix = _verify_non_empty_string(FLAGS.bigtable_train_prefix,\n                                          \'train_prefix\')\n  eval_prefix = _verify_non_empty_string(FLAGS.bigtable_eval_prefix,\n                                         \'eval_prefix\')\n  column_family = _verify_non_empty_string(FLAGS.bigtable_column_family,\n                                           \'column_family\')\n  column_qualifier = _verify_non_empty_string(FLAGS.bigtable_column_qualifier,\n                                              \'column_qualifier\')\n  return [\n      imagenet_input.BigtableSelection(\n          project=project,\n          instance=instance,\n          table=table,\n          prefix=p,\n          column_family=column_family,\n          column_qualifier=column_qualifier)\n      for p in (train_prefix, eval_prefix)\n  ]\n\ndef main(unused_argv):\n  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n      FLAGS.tpu if (FLAGS.tpu or FLAGS.use_tpu) else \'\',\n      zone=FLAGS.tpu_zone,\n      project=FLAGS.gcp_project)\n\n  config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_steps=max(600, FLAGS.iterations_per_loop),\n      log_step_count_steps=FLAGS.log_step_count_steps,\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=FLAGS.iterations_per_loop,\n          num_shards=FLAGS.num_cores,\n          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))  # pylint: disable=line-too-long\n\n  resnet_classifier = tf.contrib.tpu.TPUEstimator(\n      use_tpu=FLAGS.use_tpu,\n      model_fn=resnet_model_fn,\n      config=config,\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=FLAGS.eval_batch_size,\n      export_to_tpu=False)\n  assert FLAGS.precision == \'bfloat16\' or FLAGS.precision == \'float32\', (\n      \'Invalid value for --precision flag; must be bfloat16 or float32.\')\n  tf.logging.info(\'Precision: %s\', FLAGS.precision)\n  use_bfloat16 = FLAGS.precision == \'bfloat16\'\n\n  # Input pipelines are slightly different (with regards to shuffling and\n  # preprocessing) between training and evaluation.\n  if FLAGS.bigtable_instance:\n    tf.logging.info(\'Using Bigtable dataset, table %s\', FLAGS.bigtable_table)\n    select_train, select_eval = _select_tables_from_flags()\n    imagenet_train, imagenet_eval = [imagenet_input.ImageNetBigtableInput(\n        is_training=is_training,\n        use_bfloat16=use_bfloat16,\n        transpose_input=FLAGS.transpose_input,\n        selection=selection) for (is_training, selection) in\n                                     [(True, select_train),\n                                      (False, select_eval)]]\n  else:\n    if FLAGS.data_dir == FAKE_DATA_DIR:\n      tf.logging.info(\'Using fake dataset.\')\n    else:\n      tf.logging.info(\'Using dataset: %s\', FLAGS.data_dir)\n    imagenet_train, imagenet_eval = [\n        imagenet_input.ImageNetInput(\n            is_training=is_training,\n            data_dir=FLAGS.data_dir,\n            transpose_input=FLAGS.transpose_input,\n            cache=FLAGS.use_cache and is_training,\n            num_parallel_calls=FLAGS.num_parallel_calls,\n            use_bfloat16=use_bfloat16) for is_training in [True, False]\n    ]\n\n  steps_per_epoch = FLAGS.num_train_images // FLAGS.train_batch_size\n  eval_steps = FLAGS.num_eval_images // FLAGS.eval_batch_size\n\n  if FLAGS.mode == \'eval\':\n\n    # Run evaluation when there\'s a new checkpoint\n    for ckpt in evaluation.checkpoints_iterator(\n        FLAGS.model_dir, timeout=FLAGS.eval_timeout):\n      tf.logging.info(\'Starting to evaluate.\')\n      try:\n        start_timestamp = time.time()  # This time will include compilation time\n        eval_results = resnet_classifier.evaluate(\n            input_fn=imagenet_eval.input_fn,\n            steps=eval_steps,\n            checkpoint_path=ckpt)\n        elapsed_time = int(time.time() - start_timestamp)\n        tf.logging.info(\'Eval results: %s. Elapsed seconds: %d\',\n                        eval_results, elapsed_time)\n\n        # Terminate eval job when final checkpoint is reached\n        current_step = int(os.path.basename(ckpt).split(\'-\')[1])\n        if current_step >= FLAGS.train_steps:\n          tf.logging.info(\n              \'Evaluation finished after training step %d\', current_step)\n          break\n\n      except tf.errors.NotFoundError:\n        # Since the coordinator is on a different job than the TPU worker,\n        # sometimes the TPU worker does not finish initializing until long after\n        # the CPU job tells it to start evaluating. In this case, the checkpoint\n        # file could have been deleted already.\n        tf.logging.info(\n            \'Checkpoint %s no longer exists, skipping checkpoint\', ckpt)\n\n  else:   # FLAGS.mode == \'train\' or FLAGS.mode == \'train_and_eval\'\n    current_step = estimator._load_global_step_from_checkpoint_dir(FLAGS.model_dir)  # pylint: disable=protected-access,line-too-long\n    steps_per_epoch = FLAGS.num_train_images // FLAGS.train_batch_size\n\n    tf.logging.info(\'Training for %d steps (%.2f epochs in total). Current\'\n                    \' step %d.\',\n                    FLAGS.train_steps,\n                    FLAGS.train_steps / steps_per_epoch,\n                    current_step)\n\n    start_timestamp = time.time()  # This time will include compilation time\n\n    if FLAGS.mode == \'train\':\n      resnet_classifier.train(\n          input_fn=imagenet_train.input_fn, max_steps=FLAGS.train_steps)\n\n    else:\n      assert FLAGS.mode == \'train_and_eval\'\n      while current_step < FLAGS.train_steps:\n        # Train for up to steps_per_eval number of steps.\n        # At the end of training, a checkpoint will be written to --model_dir.\n        next_checkpoint = min(current_step + FLAGS.steps_per_eval,\n                              FLAGS.train_steps)\n        resnet_classifier.train(\n            input_fn=imagenet_train.input_fn, max_steps=next_checkpoint)\n        current_step = next_checkpoint\n\n        tf.logging.info(\'Finished training up to step %d. Elapsed seconds %d.\',\n                        next_checkpoint, int(time.time() - start_timestamp))\n\n        # Evaluate the model on the most recent model in --model_dir.\n        # Since evaluation happens in batches of --eval_batch_size, some images\n        # may be excluded modulo the batch size. As long as the batch size is\n        # consistent, the evaluated images are also consistent.\n        tf.logging.info(\'Starting to evaluate.\')\n        eval_results = resnet_classifier.evaluate(\n            input_fn=imagenet_eval.input_fn,\n            steps=FLAGS.num_eval_images // FLAGS.eval_batch_size)\n        tf.logging.info(\'Eval results at step %d: %s\',\n                        next_checkpoint, eval_results)\n\n      elapsed_time = int(time.time() - start_timestamp)\n      tf.logging.info(\'Finished training up to step %d. Elapsed seconds %d.\',\n                      FLAGS.train_steps, elapsed_time)\n\n    if FLAGS.export_dir is not None:\n      # The guide to serve a exported TensorFlow model is at:\n      #    https://www.tensorflow.org/serving/serving_basic\n      tf.logging.info(\'Starting to export model.\')\n      resnet_classifier.export_savedmodel(\n          export_dir_base=FLAGS.export_dir,\n          serving_input_receiver_fn=imagenet_input.image_serving_input_fn)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/models/official/resnet/resnet_model.py,22,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the post-activation form of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nBATCH_NORM_DECAY = 0.9\nBATCH_NORM_EPSILON = 1e-5\n\n\ndef batch_norm_relu(inputs, is_training, relu=True, init_zero=False,\n                    data_format=\'channels_first\'):\n  """"""Performs a batch normalization followed by a ReLU.\n\n  Args:\n    inputs: `Tensor` of shape `[batch, channels, ...]`.\n    is_training: `bool` for whether the model is training.\n    relu: `bool` if False, omits the ReLU operation.\n    init_zero: `bool` if True, initializes scale parameter of batch\n        normalization with 0 instead of 1 (default).\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    A normalized `Tensor` with the same `data_format`.\n  """"""\n  if init_zero:\n    gamma_initializer = tf.zeros_initializer()\n  else:\n    gamma_initializer = tf.ones_initializer()\n\n  if data_format == \'channels_first\':\n    axis = 1\n  else:\n    axis = 3\n\n  inputs = tf.layers.batch_normalization(\n      inputs=inputs,\n      axis=axis,\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      center=True,\n      scale=True,\n      training=is_training,\n      fused=True,\n      gamma_initializer=gamma_initializer)\n\n  if relu:\n    inputs = tf.nn.relu(inputs)\n  return inputs\n\n\ndef fixed_padding(inputs, kernel_size, data_format=\'channels_first\'):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Args:\n    inputs: `Tensor` of size `[batch, channels, height, width]` or\n        `[batch, height, width, channels]` depending on `data_format`.\n    kernel_size: `int` kernel size to be used for `conv2d` or max_pool2d`\n        operations. Should be a positive integer.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    A padded `Tensor` of the same `data_format` with size either intact\n    (if `kernel_size == 1`) or padded (if `kernel_size > 1`).\n  """"""\n  pad_total = kernel_size - 1\n  pad_beg = pad_total // 2\n  pad_end = pad_total - pad_beg\n  if data_format == \'channels_first\':\n    padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                    [pad_beg, pad_end], [pad_beg, pad_end]])\n  else:\n    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                    [pad_beg, pad_end], [0, 0]])\n\n  return padded_inputs\n\n\ndef conv2d_fixed_padding(inputs, filters, kernel_size, strides,\n                         data_format=\'channels_first\'):\n  """"""Strided 2-D convolution with explicit padding.\n\n  The padding is consistent and is based only on `kernel_size`, not on the\n  dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n\n  Args:\n    inputs: `Tensor` of size `[batch, channels, height_in, width_in]`.\n    filters: `int` number of filters in the convolution.\n    kernel_size: `int` size of the kernel to be used in the convolution.\n    strides: `int` strides of the convolution.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    A `Tensor` of shape `[batch, filters, height_out, width_out]`.\n  """"""\n  if strides > 1:\n    inputs = fixed_padding(inputs, kernel_size, data_format=data_format)\n\n  return tf.layers.conv2d(\n      inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n      padding=(\'SAME\' if strides == 1 else \'VALID\'), use_bias=False,\n      kernel_initializer=tf.variance_scaling_initializer(),\n      data_format=data_format)\n\n\ndef residual_block(inputs, filters, is_training, strides,\n                   use_projection=False, data_format=\'channels_first\'):\n  """"""Standard building block for residual networks with BN after convolutions.\n\n  Args:\n    inputs: `Tensor` of size `[batch, channels, height, width]`.\n    filters: `int` number of filters for the first two convolutions. Note that\n        the third and final convolution will use 4 times as many filters.\n    is_training: `bool` for whether the model is in training.\n    strides: `int` block stride. If greater than 1, this block will ultimately\n        downsample the input.\n    use_projection: `bool` for whether this block should use a projection\n        shortcut (versus the default identity shortcut). This is usually `True`\n        for the first block of a block group, which may change the number of\n        filters and the resolution.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    The output `Tensor` of the block.\n  """"""\n  shortcut = inputs\n  if use_projection:\n    # Projection shortcut in first layer to match filters and strides\n    shortcut = conv2d_fixed_padding(\n        inputs=inputs, filters=filters, kernel_size=1, strides=strides,\n        data_format=data_format)\n    shortcut = batch_norm_relu(shortcut, is_training, relu=False,\n                               data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n      data_format=data_format)\n  inputs = batch_norm_relu(inputs, is_training, data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=3, strides=1,\n      data_format=data_format)\n  inputs = batch_norm_relu(inputs, is_training, relu=False, init_zero=True,\n                           data_format=data_format)\n\n  return tf.nn.relu(inputs + shortcut)\n\n\ndef bottleneck_block(inputs, filters, is_training, strides,\n                     use_projection=False, data_format=\'channels_first\'):\n  """"""Bottleneck block variant for residual networks with BN after convolutions.\n\n  Args:\n    inputs: `Tensor` of size `[batch, channels, height, width]`.\n    filters: `int` number of filters for the first two convolutions. Note that\n        the third and final convolution will use 4 times as many filters.\n    is_training: `bool` for whether the model is in training.\n    strides: `int` block stride. If greater than 1, this block will ultimately\n        downsample the input.\n    use_projection: `bool` for whether this block should use a projection\n        shortcut (versus the default identity shortcut). This is usually `True`\n        for the first block of a block group, which may change the number of\n        filters and the resolution.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    The output `Tensor` of the block.\n  """"""\n  shortcut = inputs\n  if use_projection:\n    # Projection shortcut only in first block within a group. Bottleneck blocks\n    # end with 4 times the number of filters.\n    filters_out = 4 * filters\n    shortcut = conv2d_fixed_padding(\n        inputs=inputs, filters=filters_out, kernel_size=1, strides=strides,\n        data_format=data_format)\n    shortcut = batch_norm_relu(shortcut, is_training, relu=False,\n                               data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=1, strides=1,\n      data_format=data_format)\n  inputs = batch_norm_relu(inputs, is_training, data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n      data_format=data_format)\n  inputs = batch_norm_relu(inputs, is_training, data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=4 * filters, kernel_size=1, strides=1,\n      data_format=data_format)\n  inputs = batch_norm_relu(inputs, is_training, relu=False, init_zero=True,\n                           data_format=data_format)\n\n  return tf.nn.relu(inputs + shortcut)\n\n\ndef block_group(inputs, filters, block_fn, blocks, strides, is_training, name,\n                data_format=\'channels_first\'):\n  """"""Creates one group of blocks for the ResNet model.\n\n  Args:\n    inputs: `Tensor` of size `[batch, channels, height, width]`.\n    filters: `int` number of filters for the first convolution of the layer.\n    block_fn: `function` for the block to use within the model\n    blocks: `int` number of blocks contained in the layer.\n    strides: `int` stride to use for the first convolution of the layer. If\n        greater than 1, this layer will downsample the input.\n    is_training: `bool` for whether the model is training.\n    name: `str`name for the Tensor output of the block layer.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    The output `Tensor` of the block layer.\n  """"""\n  # Only the first block per block_group uses projection shortcut and strides.\n  inputs = block_fn(inputs, filters, is_training, strides,\n                    use_projection=True, data_format=data_format)\n\n  for _ in range(1, blocks):\n    inputs = block_fn(inputs, filters, is_training, 1,\n                      data_format=data_format)\n\n  return tf.identity(inputs, name)\n\n\ndef resnet_v1_generator(block_fn, layers, num_classes,\n                        data_format=\'channels_first\'):\n  """"""Generator for ResNet v1 models.\n\n  Args:\n    block_fn: `function` for the block to use within the model. Either\n        `residual_block` or `bottleneck_block`.\n    layers: list of 4 `int`s denoting the number of blocks to include in each\n      of the 4 block groups. Each group consists of blocks that take inputs of\n      the same resolution.\n    num_classes: `int` number of possible classes for image classification.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    Model `function` that takes in `inputs` and `is_training` and returns the\n    output `Tensor` of the ResNet model.\n  """"""\n  def model(inputs, is_training):\n    """"""Creation of the model graph.""""""\n    inputs = conv2d_fixed_padding(\n        inputs=inputs, filters=64, kernel_size=7, strides=2,\n        data_format=data_format)\n    inputs = tf.identity(inputs, \'initial_conv\')\n    inputs = batch_norm_relu(inputs, is_training, data_format=data_format)\n\n    inputs = tf.layers.max_pooling2d(\n        inputs=inputs, pool_size=3, strides=2, padding=\'SAME\',\n        data_format=data_format)\n    inputs = tf.identity(inputs, \'initial_max_pool\')\n\n    inputs = block_group(\n        inputs=inputs, filters=64, block_fn=block_fn, blocks=layers[0],\n        strides=1, is_training=is_training, name=\'block_group1\',\n        data_format=data_format)\n    inputs = block_group(\n        inputs=inputs, filters=128, block_fn=block_fn, blocks=layers[1],\n        strides=2, is_training=is_training, name=\'block_group2\',\n        data_format=data_format)\n    inputs = block_group(\n        inputs=inputs, filters=256, block_fn=block_fn, blocks=layers[2],\n        strides=2, is_training=is_training, name=\'block_group3\',\n        data_format=data_format)\n    inputs = block_group(\n        inputs=inputs, filters=512, block_fn=block_fn, blocks=layers[3],\n        strides=2, is_training=is_training, name=\'block_group4\',\n        data_format=data_format)\n\n    # Global average pooling based on the activation map size.\n    inputs = tf.layers.average_pooling2d(\n        inputs=inputs, pool_size=inputs.get_shape()[1:3], strides=1,\n        padding=\'VALID\', data_format=data_format)\n    inputs = tf.identity(inputs, \'final_avg_pool\')\n    inputs = tf.reshape(\n        inputs, [-1, 2048 if block_fn is bottleneck_block else 512])\n    inputs = tf.layers.dense(\n        inputs=inputs,\n        units=num_classes,\n        bias_initializer=tf.constant_initializer(-np.log(num_classes - 1)),\n        kernel_initializer=tf.random_normal_initializer(stddev=.01))\n    inputs = tf.identity(inputs, \'final_dense\')\n    return inputs\n\n  model.default_image_size = 224\n  return model\n\n\ndef resnet_v1(resnet_depth, num_classes, data_format=\'channels_first\'):\n  """"""Returns the ResNet model for a given size and number of output classes.""""""\n  model_params = {\n      18: {\'block\': residual_block, \'layers\': [2, 2, 2, 2]},\n      34: {\'block\': residual_block, \'layers\': [3, 4, 6, 3]},\n      50: {\'block\': bottleneck_block, \'layers\': [3, 4, 6, 3]},\n      101: {\'block\': bottleneck_block, \'layers\': [3, 4, 23, 3]},\n      152: {\'block\': bottleneck_block, \'layers\': [3, 8, 36, 3]},\n      200: {\'block\': bottleneck_block, \'layers\': [3, 24, 36, 3]}\n  }\n\n  if resnet_depth not in model_params:\n    raise ValueError(\'Not a valid resnet_depth:\', resnet_depth)\n\n  params = model_params[resnet_depth]\n  return resnet_v1_generator(\n      params[\'block\'], params[\'layers\'], num_classes, data_format)\n'"
tpu/models/official/resnet/resnet_preprocessing.py,30,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""ImageNet preprocessing for ResNet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nIMAGE_SIZE = 224\nCROP_PADDING = 32\n# IMAGE_SIZE = 320\t\n# CROP_PADDING = 46\t\n# IMAGE_SIZE = 448\t\n# CROP_PADDING = 64\n\n\ndef distorted_bounding_box_crop(image_bytes,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image_bytes: `Tensor` of binary image data.\n    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n        where each coordinate is [0, 1) and the coordinates are arranged\n        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n        image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n        area of the image must contain at least this fraction of any bounding\n        box supplied.\n    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n        image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `float`s. The cropped area of the image\n        must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n        region of the image of the specified constraints. After `max_attempts`\n        failures, return the entire image.\n    scope: Optional `str` for name scope.\n  Returns:\n    cropped image `Tensor`\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image_bytes, bbox]):\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        shape,\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n    target_height, target_width, _ = tf.unstack(bbox_size)\n    crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n\n    return image\n\n\ndef _at_least_x_are_equal(a, b, x):\n  """"""At least `x` of `a` and `b` `Tensors` are equal.""""""\n  match = tf.equal(a, b)\n  match = tf.cast(match, tf.int32)\n  return tf.greater_equal(tf.reduce_sum(match), x)\n\n\ndef _decode_and_random_crop(image_bytes, image_size):\n  """"""Make a random crop of image_size.""""""\n  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n  image = distorted_bounding_box_crop(\n      image_bytes,\n      bbox,\n      min_object_covered=0.1,\n      aspect_ratio_range=(3. / 4, 4. / 3.),\n      area_range=(0.08, 1.0),\n      max_attempts=10,\n      scope=None)\n  original_shape = tf.image.extract_jpeg_shape(image_bytes)\n  bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)\n\n  image = tf.cond(\n      bad,\n      lambda: _decode_and_center_crop(image_bytes, image_size),\n      lambda: tf.image.resize_bicubic([image],  # pylint: disable=g-long-lambda\n                                      [image_size, image_size])[0])\n\n  return image\n\n\ndef _decode_and_center_crop(image_bytes, image_size):\n  """"""Crops to center of image with padding then scales image_size.""""""\n  shape = tf.image.extract_jpeg_shape(image_bytes)\n  image_height = shape[0]\n  image_width = shape[1]\n\n  padded_center_crop_size = tf.cast(\n      ((image_size / (image_size + CROP_PADDING)) *\n       tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n      tf.int32)\n\n  offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n  offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n  crop_window = tf.stack([offset_height, offset_width,\n                          padded_center_crop_size, padded_center_crop_size])\n  image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n  image = tf.image.resize_bicubic([image], [image_size, image_size])[0]\n\n  return image\n\n\ndef _flip(image):\n  """"""Random horizontal image flip.""""""\n  image = tf.image.random_flip_left_right(image)\n  return image\n\n\ndef preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.\n    use_bfloat16: `bool` for whether to use bfloat16.\n    image_size: image size.\n\n  Returns:\n    A preprocessed image `Tensor`.\n  """"""\n  image = _decode_and_random_crop(image_bytes, image_size)\n  image = _flip(image)\n  image = tf.reshape(image, [image_size, image_size, 3])\n  image = tf.image.convert_image_dtype(\n      image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n  return image\n\n\ndef preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.\n    use_bfloat16: `bool` for whether to use bfloat16.\n    image_size: image size.\n\n  Returns:\n    A preprocessed image `Tensor`.\n  """"""\n  image = _decode_and_center_crop(image_bytes, image_size)\n  image = tf.reshape(image, [image_size, image_size, 3])\n  image = tf.image.convert_image_dtype(\n      image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n  return image\n\n\ndef preprocess_image(image_bytes, is_training=False, use_bfloat16=False,\n      image_size=IMAGE_SIZE):\n  """"""Preprocesses the given image.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.\n    is_training: `bool` for whether the preprocessing is for training.\n    use_bfloat16: `bool` for whether to use bfloat16.\n    image_size: image size.\n\n  Returns:\n    A preprocessed image `Tensor` with value range of [0, 255].\n  """"""\n  if is_training:\n    return preprocess_for_train(image_bytes, use_bfloat16, image_size)\n  else:\n    return preprocess_for_eval(image_bytes, use_bfloat16, image_size)\n'"
tpu/models/official/retinanet/anchors.py,9,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""RetinaNet anchor definition.\n\nThis module implements RetinaNet anchor described in:\n\nT.-Y. Lin, P. Goyal, R. Girshick, K. He,  and P. Dollar\nFocal Loss for Dense Object Detection. arXiv:1708.02002\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import OrderedDict\nimport numpy as np\nimport tensorflow as tf\nfrom object_detection import argmax_matcher\nfrom object_detection import box_list\nfrom object_detection import faster_rcnn_box_coder\nfrom object_detection import region_similarity_calculator\nfrom object_detection import target_assigner\n\n# The minimum score to consider a logit for identifying detections.\nMIN_CLASS_SCORE = -5.0\n\n# The score for a dummy detection\n_DUMMY_DETECTION_SCORE = -1e5\n\n# The maximum number of (anchor,class) pairs to keep for non-max suppression.\nMAX_DETECTION_POINTS = 5000\n\n# The maximum number of detections per image.\nMAX_DETECTIONS_PER_IMAGE = 100\n\n\ndef sigmoid(x):\n  """"""Sigmoid function for use with Numpy for CPU evaluation.""""""\n  return 1 / (1 + np.exp(-x))\n\n\ndef decode_box_outputs(rel_codes, anchors):\n  """"""Transforms relative regression coordinates to absolute positions.\n\n  Network predictions are normalized and relative to a given anchor; this\n  reverses the transformation and outputs absolute coordinates for the input\n  image.\n\n  Args:\n    rel_codes: box regression targets.\n    anchors: anchors on all feature levels.\n  Returns:\n    outputs: bounding boxes.\n\n  """"""\n  ycenter_a = (anchors[0] + anchors[2]) / 2\n  xcenter_a = (anchors[1] + anchors[3]) / 2\n  ha = anchors[2] - anchors[0]\n  wa = anchors[3] - anchors[1]\n  ty, tx, th, tw = rel_codes\n\n  w = np.exp(tw) * wa\n  h = np.exp(th) * ha\n  ycenter = ty * ha + ycenter_a\n  xcenter = tx * wa + xcenter_a\n  ymin = ycenter - h / 2.\n  xmin = xcenter - w / 2.\n  ymax = ycenter + h / 2.\n  xmax = xcenter + w / 2.\n  return np.column_stack([ymin, xmin, ymax, xmax])\n\n\ndef nms(dets, thresh):\n  """"""Non-maximum supression.""""""\n  x1 = dets[:, 0]\n  y1 = dets[:, 1]\n  x2 = dets[:, 2]\n  y2 = dets[:, 3]\n  scores = dets[:, 4]\n\n  areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n  order = scores.argsort()[::-1]\n\n  keep = []\n  while order.size > 0:\n    i = order[0]\n    keep.append(i)\n    xx1 = np.maximum(x1[i], x1[order[1:]])\n    yy1 = np.maximum(y1[i], y1[order[1:]])\n    xx2 = np.minimum(x2[i], x2[order[1:]])\n    yy2 = np.minimum(y2[i], y2[order[1:]])\n\n    w = np.maximum(0.0, xx2 - xx1 + 1)\n    h = np.maximum(0.0, yy2 - yy1 + 1)\n    intersection = w * h\n    overlap = intersection / (areas[i] + areas[order[1:]] - intersection)\n\n    inds = np.where(overlap <= thresh)[0]\n    order = order[inds + 1]\n  return keep\n\n\ndef _generate_anchor_configs(min_level, max_level, num_scales, aspect_ratios):\n  """"""Generates mapping from output level to a list of anchor configurations.\n\n  A configuration is a tuple of (num_anchors, scale, aspect_ratio).\n\n  Args:\n      min_level: integer number of minimum level of the output feature pyramid.\n      max_level: integer number of maximum level of the output feature pyramid.\n      num_scales: integer number representing intermediate scales added\n        on each level. For instances, num_scales=2 adds two additional\n        anchor scales [2^0, 2^0.5] on each level.\n      aspect_ratios: list of tuples representing the aspect raito anchors added\n        on each level. For instances, aspect_ratios =\n        [(1, 1), (1.4, 0.7), (0.7, 1.4)] adds three anchors on each level.\n  Returns:\n    anchor_configs: a dictionary with keys as the levels of anchors and\n      values as a list of anchor configuration.\n  """"""\n  anchor_configs = {}\n  for level in range(min_level, max_level + 1):\n    anchor_configs[level] = []\n    for scale_octave in range(num_scales):\n      for aspect in aspect_ratios:\n        anchor_configs[level].append(\n            (2**level, scale_octave / float(num_scales), aspect))\n  return anchor_configs\n\n\ndef _generate_anchor_boxes(image_size, anchor_scale, anchor_configs):\n  """"""Generates multiscale anchor boxes.\n\n  Args:\n    image_size: integer number of input image size. The input image has the\n      same dimension for width and height. The image_size should be divided by\n      the largest feature stride 2^max_level.\n    anchor_scale: float number representing the scale of size of the base\n      anchor to the feature stride 2^level.\n    anchor_configs: a dictionary with keys as the levels of anchors and\n      values as a list of anchor configuration.\n  Returns:\n    anchor_boxes: a numpy array with shape [N, 4], which stacks anchors on all\n      feature levels.\n  Raises:\n    ValueError: input size must be the multiple of largest feature stride.\n  """"""\n  boxes_all = []\n  for _, configs in anchor_configs.items():\n    boxes_level = []\n    for config in configs:\n      stride, octave_scale, aspect = config\n      if image_size % stride != 0:\n        raise ValueError(""input size must be divided by the stride."")\n      base_anchor_size = anchor_scale * stride * 2**octave_scale\n      anchor_size_x_2 = base_anchor_size * aspect[0] / 2.0\n      anchor_size_y_2 = base_anchor_size * aspect[1] / 2.0\n\n      x = np.arange(stride / 2, image_size, stride)\n      y = np.arange(stride / 2, image_size, stride)\n      xv, yv = np.meshgrid(x, y)\n      xv = xv.reshape(-1)\n      yv = yv.reshape(-1)\n\n      boxes = np.vstack((yv - anchor_size_y_2, xv - anchor_size_x_2,\n                         yv + anchor_size_y_2, xv + anchor_size_x_2))\n      boxes = np.swapaxes(boxes, 0, 1)\n      boxes_level.append(np.expand_dims(boxes, axis=1))\n    # concat anchors on the same level to the reshape NxAx4\n    boxes_level = np.concatenate(boxes_level, axis=1)\n    boxes_all.append(boxes_level.reshape([-1, 4]))\n\n  anchor_boxes = np.vstack(boxes_all)\n  return anchor_boxes\n\n\ndef _generate_detections(cls_outputs, box_outputs, anchor_boxes, indices,\n                         classes, image_id, image_scale, num_classes):\n  """"""Generates detections with RetinaNet model outputs and anchors.\n\n  Args:\n    cls_outputs: a numpy array with shape [N, 1], which has the highest class\n      scores on all feature levels. The N is the number of selected\n      top-K total anchors on all levels.  (k being MAX_DETECTION_POINTS)\n    box_outputs: a numpy array with shape [N, 4], which stacks box regression\n      outputs on all feature levels. The N is the number of selected top-k\n      total anchors on all levels. (k being MAX_DETECTION_POINTS)\n    anchor_boxes: a numpy array with shape [N, 4], which stacks anchors on all\n      feature levels. The N is the number of selected top-k total anchors on\n      all levels.\n    indices: a numpy array with shape [N], which is the indices from top-k\n      selection.\n    classes: a numpy array with shape [N], which represents the class\n      prediction on all selected anchors from top-k selection.\n    image_id: an integer number to specify the image id.\n    image_scale: a float tensor representing the scale between original image\n      and input image for the detector. It is used to rescale detections for\n      evaluating with the original groundtruth annotations.\n    num_classes: a integer that indicates the number of classes.\n  Returns:\n    detections: detection results in a tensor with each row representing\n      [image_id, x, y, width, height, score, class]\n  """"""\n  anchor_boxes = anchor_boxes[indices, :]\n  scores = sigmoid(cls_outputs)\n  # apply bounding box regression to anchors\n  boxes = decode_box_outputs(\n      box_outputs.swapaxes(0, 1), anchor_boxes.swapaxes(0, 1))\n  boxes = boxes[:, [1, 0, 3, 2]]\n  # run class-wise nms\n  detections = []\n  for c in range(num_classes):\n    indices = np.where(classes == c)[0]\n    if indices.shape[0] == 0:\n      continue\n    boxes_cls = boxes[indices, :]\n    scores_cls = scores[indices]\n    # Select top-scoring boxes in each class and apply non-maximum suppression\n    # (nms) for boxes in the same class. The selected boxes from each class are\n    # then concatenated for the final detection outputs.\n    all_detections_cls = np.column_stack((boxes_cls, scores_cls))\n    top_detection_idx = nms(all_detections_cls, 0.5)\n    top_detections_cls = all_detections_cls[top_detection_idx]\n    top_detections_cls[:, 2] -= top_detections_cls[:, 0]\n    top_detections_cls[:, 3] -= top_detections_cls[:, 1]\n    top_detections_cls = np.column_stack(\n        (np.repeat(image_id, len(top_detection_idx)),\n         top_detections_cls,\n         np.repeat(c + 1, len(top_detection_idx)))\n    )\n    detections.append(top_detections_cls)\n\n  def _generate_dummy_detections(number):\n    detections_dummy = np.zeros((number, 7), dtype=np.float32)\n    detections_dummy[:, 0] = image_id[0]\n    detections_dummy[:, 5] = _DUMMY_DETECTION_SCORE\n    return detections_dummy\n\n  if detections:\n    detections = np.vstack(detections)\n    # take final 100 detections\n    indices = np.argsort(-detections[:, -2])\n    detections = np.array(\n        detections[indices[0:MAX_DETECTIONS_PER_IMAGE]], dtype=np.float32)\n    # Add dummy detections to fill up to 100 detections\n    n = max(MAX_DETECTIONS_PER_IMAGE - len(detections), 0)\n    detections_dummy = _generate_dummy_detections(n)\n    detections = np.vstack([detections, detections_dummy])\n    detections[:, 1:5] *= image_scale\n  else:\n    detections = _generate_dummy_detections(MAX_DETECTIONS_PER_IMAGE)\n    detections[:, 1:5] *= image_scale\n\n  return detections\n\n\nclass Anchors(object):\n  """"""RetinaNet Anchors class.""""""\n\n  def __init__(self, min_level, max_level, num_scales, aspect_ratios,\n               anchor_scale, image_size):\n    """"""Constructs multiscale RetinaNet anchors.\n\n    Args:\n      min_level: integer number of minimum level of the output feature pyramid.\n      max_level: integer number of maximum level of the output feature pyramid.\n      num_scales: integer number representing intermediate scales added\n        on each level. For instances, num_scales=2 adds two additional\n        anchor scales [2^0, 2^0.5] on each level.\n      aspect_ratios: list of tuples representing the aspect raito anchors added\n        on each level. For instances, aspect_ratios =\n        [(1, 1), (1.4, 0.7), (0.7, 1.4)] adds three anchors on each level.\n      anchor_scale: float number representing the scale of size of the base\n        anchor to the feature stride 2^level.\n      image_size: integer number of input image size. The input image has the\n        same dimension for width and height. The image_size should be divided by\n        the largest feature stride 2^max_level.\n    """"""\n    self.min_level = min_level\n    self.max_level = max_level\n    self.num_scales = num_scales\n    self.aspect_ratios = aspect_ratios\n    self.anchor_scale = anchor_scale\n    self.image_size = image_size\n    self.config = self._generate_configs()\n    self.boxes = self._generate_boxes()\n\n  def _generate_configs(self):\n    """"""Generate configurations of anchor boxes.""""""\n    return _generate_anchor_configs(self.min_level, self.max_level,\n                                    self.num_scales, self.aspect_ratios)\n\n  def _generate_boxes(self):\n    """"""Generates multiscale anchor boxes.""""""\n    boxes = _generate_anchor_boxes(self.image_size, self.anchor_scale,\n                                   self.config)\n    boxes = tf.convert_to_tensor(boxes, dtype=tf.float32)\n    return boxes\n\n  def get_anchors_per_location(self):\n    return self.num_scales * len(self.aspect_ratios)\n\n\nclass AnchorLabeler(object):\n  """"""Labeler for multiscale anchor boxes.""""""\n\n  def __init__(self, anchors, num_classes, match_threshold=0.5):\n    """"""Constructs anchor labeler to assign labels to anchors.\n\n    Args:\n      anchors: an instance of class Anchors.\n      num_classes: integer number representing number of classes in the dataset.\n      match_threshold: float number between 0 and 1 representing the threshold\n        to assign positive labels for anchors.\n    """"""\n    similarity_calc = region_similarity_calculator.IouSimilarity()\n    matcher = argmax_matcher.ArgMaxMatcher(\n        match_threshold,\n        unmatched_threshold=match_threshold,\n        negatives_lower_than_unmatched=True,\n        force_match_for_each_row=True)\n    box_coder = faster_rcnn_box_coder.FasterRcnnBoxCoder()\n\n    self._target_assigner = target_assigner.TargetAssigner(\n        similarity_calc, matcher, box_coder)\n    self._anchors = anchors\n    self._match_threshold = match_threshold\n    self._num_classes = num_classes\n\n  def _unpack_labels(self, labels):\n    """"""Unpacks an array of labels into multiscales labels.""""""\n    labels_unpacked = OrderedDict()\n    anchors = self._anchors\n    count = 0\n    for level in range(anchors.min_level, anchors.max_level + 1):\n      feat_size = int(anchors.image_size / 2**level)\n      steps = feat_size**2 * anchors.get_anchors_per_location()\n      indices = tf.range(count, count + steps)\n      count += steps\n      labels_unpacked[level] = tf.reshape(\n          tf.gather(labels, indices), [feat_size, feat_size, -1])\n    return labels_unpacked\n\n  def label_anchors(self, gt_boxes, gt_labels):\n    """"""Labels anchors with ground truth inputs.\n\n    Args:\n      gt_boxes: A float tensor with shape [N, 4] representing groundtruth boxes.\n        For each row, it stores [y0, x0, y1, x1] for four corners of a box.\n      gt_labels: A integer tensor with shape [N, 1] representing groundtruth\n        classes.\n    Returns:\n      cls_targets_dict: ordered dictionary with keys\n        [min_level, min_level+1, ..., max_level]. The values are tensor with\n        shape [height_l, width_l, num_anchors]. The height_l and width_l\n        represent the dimension of class logits at l-th level.\n      box_targets_dict: ordered dictionary with keys\n        [min_level, min_level+1, ..., max_level]. The values are tensor with\n        shape [height_l, width_l, num_anchors * 4]. The height_l and\n        width_l represent the dimension of bounding box regression output at\n        l-th level.\n      num_positives: scalar tensor storing number of positives in an image.\n    """"""\n    gt_box_list = box_list.BoxList(gt_boxes)\n    anchor_box_list = box_list.BoxList(self._anchors.boxes)\n\n    # cls_weights, box_weights are not used\n    cls_targets, _, box_targets, _, matches = self._target_assigner.assign(\n        anchor_box_list, gt_box_list, gt_labels)\n\n    # class labels start from 1 and the background class = -1\n    cls_targets -= 1\n    cls_targets = tf.cast(cls_targets, tf.int32)\n\n    # Unpack labels.\n    cls_targets_dict = self._unpack_labels(cls_targets)\n    box_targets_dict = self._unpack_labels(box_targets)\n    num_positives = tf.reduce_sum(\n        tf.cast(tf.not_equal(matches.match_results, -1), tf.float32))\n\n    return cls_targets_dict, box_targets_dict, num_positives\n\n  def generate_detections(self, cls_outputs, box_outputs, indices, classes,\n                          image_id, image_scale):\n    return tf.py_func(_generate_detections, [\n        cls_outputs, box_outputs, self._anchors.boxes, indices, classes,\n        image_id, image_scale, self._num_classes\n    ], tf.float32)\n'"
tpu/models/official/retinanet/coco_metric.py,5,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""COCO-style evaluation metrics.\n\nImplements the interface of COCO API and metric_fn in tf.TPUEstimator.\n\nCOCO API: github.com/cocodataset/cocoapi/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\n\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nimport tensorflow as tf\n\nFLAGS = flags.FLAGS\n\n\nclass EvaluationMetric(object):\n  """"""COCO evaluation metric class.""""""\n\n  def __init__(self, filename):\n    """"""Constructs COCO evaluation class.\n\n    The class provides the interface to metrics_fn in TPUEstimator. The\n    _update_op() takes detections from each image and push them to\n    self.detections. The _evaluate() loads a JSON file in COCO annotation format\n    as the groundtruths and runs COCO evaluation.\n\n    Args:\n      filename: Ground truth JSON file name. If filename is None, use\n        groundtruth data passed from the dataloader for evaluation.\n    """"""\n    if filename:\n      self.coco_gt = COCO(filename)\n    self.filename = filename\n    self.metric_names = [\'AP\', \'AP50\', \'AP75\', \'APs\', \'APm\', \'APl\', \'ARmax1\',\n                         \'ARmax10\', \'ARmax100\', \'ARs\', \'ARm\', \'ARl\']\n    self._reset()\n\n  def _reset(self):\n    """"""Reset COCO API object.""""""\n    if self.filename is None:\n      self.coco_gt = COCO()\n    self.detections = []\n    self.dataset = {\n        \'images\': [],\n        \'annotations\': [],\n        \'categories\': []\n    }\n    self.image_id = 1\n    self.annotation_id = 1\n    self.category_ids = []\n\n  def estimator_metric_fn(self, detections, groundtruth_data):\n    """"""Constructs the metric function for tf.TPUEstimator.\n\n    For each metric, we return the evaluation op and an update op; the update op\n    is shared across all metrics and simply appends the set of detections to the\n    `self.detections` list. The metric op is invoked after all examples have\n    been seen and computes the aggregate COCO metrics. Please find details API\n    in: https://www.tensorflow.org/api_docs/python/tf/contrib/learn/MetricSpec\n    Args:\n      detections: Detection results in a tensor with each row representing\n        [image_id, x, y, width, height, score, class]\n      groundtruth_data: Groundtruth annotations in a tensor with each row\n        representing [y1, x1, y2, x2, is_crowd, area, class].\n    Returns:\n      metrics_dict: A dictionary mapping from evaluation name to a tuple of\n        operations (`metric_op`, `update_op`). `update_op` appends the\n        detections for the metric to the `self.detections` list.\n    """"""\n\n    def _evaluate():\n      """"""Evaluates with detections from all images with COCO API.\n\n      Returns:\n        coco_metric: float numpy array with shape [12] representing the\n          coco-style evaluation metrics.\n      """"""\n      if self.filename is None:\n        self.coco_gt.dataset = self.dataset\n        self.coco_gt.createIndex()\n\n      detections = np.array(self.detections)\n      image_ids = list(set(detections[:, 0]))\n      coco_dt = self.coco_gt.loadRes(detections)\n      coco_eval = COCOeval(self.coco_gt, coco_dt, iouType=\'bbox\')\n      coco_eval.params.imgIds = image_ids\n      coco_eval.evaluate()\n      coco_eval.accumulate()\n      coco_eval.summarize()\n      coco_metrics = coco_eval.stats\n      # clean self.detections after evaluation is done.\n      # this makes sure the next evaluation will start with an empty list of\n      # self.detections.\n      self._reset()\n      return np.array(coco_metrics, dtype=np.float32)\n\n    def _update_op(detections, groundtruth_data):\n      """"""Update detection results and groundtruth data.\n\n      Append detection results to self.detections to aggregate results from\n      all validation set. The groundtruth_data is parsed and added into a\n      dictinoary with the same format as COCO dataset, which can be used for\n      evaluation.\n\n      Args:\n       detections: Detection results in a tensor with each row representing\n         [image_id, x, y, width, height, score, class].\n       groundtruth_data: Groundtruth annotations in a tensor with each row\n         representing [y1, x1, y2, x2, is_crowd, area, class].\n      """"""\n      for i in range(len(detections)):\n        if detections[i].shape[0] == 0:\n          continue\n        self.detections.extend(detections[i])\n        # Append groundtruth annotaitons to create COCO dataset object.\n        # Add images.\n        image_id = detections[i][0, 0]\n        if image_id == -1:\n          image_id = self.image_id\n        self.dataset[\'images\'].append({\n            \'id\': int(image_id),\n        })\n        detections[i][:, 0] = image_id\n        # Add annotations.\n        indices = np.where(groundtruth_data[i, :, -1] > -1)[0]\n        for data in groundtruth_data[i, indices]:\n          box = data[0:4]\n          is_crowd = data[4]\n          area = data[5]\n          category_id = data[6]\n          if category_id < 0:\n            break\n          if area == -1:\n            area = (box[3] - box[1]) * (box[2] - box[0])\n          self.dataset[\'annotations\'].append({\n              \'id\': int(self.annotation_id),\n              \'image_id\': int(image_id),\n              \'category_id\': int(category_id),\n              \'bbox\': [box[1], box[0], box[3] - box[1], box[2] - box[0]],\n              \'area\': area,\n              \'iscrowd\': int(is_crowd)\n          })\n          self.annotation_id += 1\n          self.category_ids.append(category_id)\n        self.image_id += 1\n      self.category_ids = list(set(self.category_ids))\n      self.dataset[\'categories\'] = [\n          {\'id\': int(category_id)} for category_id in self.category_ids\n      ]\n\n    with tf.name_scope(\'coco_metric\'):\n      update_op = tf.py_func(_update_op, [detections, groundtruth_data], [])\n      metrics = tf.py_func(_evaluate, [], tf.float32)\n      metrics_dict = {}\n      for i, name in enumerate(self.metric_names):\n        metrics_dict[name] = (metrics[i], update_op)\n      return metrics_dict\n'"
tpu/models/official/retinanet/dataloader.py,80,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Data loader and processing.\n\nDefines input_fn of RetinaNet for TF Estimator. The input_fn includes training\ndata for category classification, bounding box regression, and number of\npositive examples to normalize the loss during training.\n\nT.-Y. Lin, P. Goyal, R. Girshick, K. He,  and P. Dollar\nFocal Loss for Dense Object Detection. arXiv:1708.02002\n""""""\n\nimport tensorflow as tf\n\nimport anchors\nfrom object_detection import preprocessor\nfrom object_detection import tf_example_decoder\n\nMAX_NUM_INSTANCES = 100\n\n\nclass InputProcessor(object):\n  """"""Base class of Input processor.""""""\n\n  def __init__(self, image, output_size):\n    """"""Initializes a new `InputProcessor`.\n\n    Args:\n      image: The input image before processing.\n      output_size: The output image size after calling resize_and_crop_image\n        function.\n    """"""\n    self._image = image\n    self._output_size = output_size\n    # Parameters to control rescaling and shifting during preprocessing.\n    # Image scale defines scale from original image to scaled image.\n    self._image_scale = tf.constant(1.0)\n    # The integer height and width of scaled image.\n    self._scaled_height = tf.shape(image)[0]\n    self._scaled_width = tf.shape(image)[1]\n    # The x and y translation offset to crop scaled image to the output size.\n    self._crop_offset_y = tf.constant(0)\n    self._crop_offset_x = tf.constant(0)\n\n  def normalize_image(self):\n    """"""Normalize the image to zero mean and unit variance.""""""\n    # The image normalization is identical to Cloud TPU ResNet.\n    self._image = tf.image.convert_image_dtype(self._image, dtype=tf.float32)\n    offset = tf.constant([0.485, 0.456, 0.406])\n    offset = tf.expand_dims(offset, axis=0)\n    offset = tf.expand_dims(offset, axis=0)\n    self._image -= offset\n\n    scale = tf.constant([0.229, 0.224, 0.225])\n    scale = tf.expand_dims(scale, axis=0)\n    scale = tf.expand_dims(scale, axis=0)\n    self._image /= scale\n\n  def set_training_random_scale_factors(self, scale_min, scale_max):\n    """"""Set the parameters for multiscale training.""""""\n    # Select a random scale factor.\n    random_scale_factor = tf.random_uniform([], scale_min, scale_max)\n    scaled_size = tf.to_int32(random_scale_factor * self._output_size)\n\n    # Recompute the accurate scale_factor using rounded scaled image size.\n    height = tf.shape(self._image)[0]\n    width = tf.shape(self._image)[1]\n    max_image_size = tf.to_float(tf.maximum(height, width))\n    image_scale = tf.to_float(scaled_size) / max_image_size\n\n    # Select non-zero random offset (x, y) if scaled image is larger than\n    # self._output_size.\n    scaled_height = tf.to_int32(tf.to_float(height) * image_scale)\n    scaled_width = tf.to_int32(tf.to_float(width) * image_scale)\n    offset_y = tf.to_float(scaled_height - self._output_size)\n    offset_x = tf.to_float(scaled_width - self._output_size)\n    offset_y = tf.maximum(0.0, offset_y) * tf.random_uniform([], 0, 1)\n    offset_x = tf.maximum(0.0, offset_x) * tf.random_uniform([], 0, 1)\n    offset_y = tf.to_int32(offset_y)\n    offset_x = tf.to_int32(offset_x)\n    self._image_scale = image_scale\n    self._scaled_height = scaled_height\n    self._scaled_width = scaled_width\n    self._crop_offset_x = offset_x\n    self._crop_offset_y = offset_y\n\n  def set_scale_factors_to_output_size(self):\n    """"""Set the parameters to resize input image to self._output_size.""""""\n    # Compute the scale_factor using rounded scaled image size.\n    height = tf.shape(self._image)[0]\n    width = tf.shape(self._image)[1]\n    max_image_size = tf.to_float(tf.maximum(height, width))\n    image_scale = tf.to_float(self._output_size) / max_image_size\n    scaled_height = tf.to_int32(tf.to_float(height) * image_scale)\n    scaled_width = tf.to_int32(tf.to_float(width) * image_scale)\n    self._image_scale = image_scale\n    self._scaled_height = scaled_height\n    self._scaled_width = scaled_width\n\n  def resize_and_crop_image(self, method=tf.image.ResizeMethod.BILINEAR):\n    """"""Resize input image and crop it to the self._output dimension.""""""\n    scaled_image = tf.image.resize_images(\n        self._image, [self._scaled_height, self._scaled_width], method=method)\n    scaled_image = scaled_image[\n        self._crop_offset_y:self._crop_offset_y + self._output_size,\n        self._crop_offset_x:self._crop_offset_x + self._output_size, :]\n    output_image = tf.image.pad_to_bounding_box(\n        scaled_image, 0, 0, self._output_size, self._output_size)\n    return output_image\n\n\nclass DetectionInputProcessor(InputProcessor):\n  """"""Input processor for object detection.""""""\n\n  def __init__(self, image, output_size, boxes=None, classes=None):\n    InputProcessor.__init__(self, image, output_size)\n    self._boxes = boxes\n    self._classes = classes\n\n  def random_horizontal_flip(self):\n    """"""Randomly flip input image and bounding boxes.""""""\n    self._image, self._boxes = preprocessor.random_horizontal_flip(\n        self._image, boxes=self._boxes)\n\n  def clip_boxes(self, boxes):\n    """"""Clip boxes to fit in an image.""""""\n    boxes = tf.where(tf.less(boxes, 0), tf.zeros_like(boxes), boxes)\n    boxes = tf.where(tf.greater(boxes, self._output_size - 1),\n                     (self._output_size - 1) * tf.ones_like(boxes), boxes)\n    return boxes\n\n  def resize_and_crop_boxes(self):\n    """"""Resize boxes and crop it to the self._output dimension.""""""\n    boxlist = preprocessor.box_list.BoxList(self._boxes)\n    boxes = preprocessor.box_list_scale(\n        boxlist, self._scaled_height, self._scaled_width).get()\n    # Adjust box coordinates based on the offset.\n    box_offset = tf.stack([self._crop_offset_y, self._crop_offset_x,\n                           self._crop_offset_y, self._crop_offset_x,])\n    boxes -= tf.to_float(tf.reshape(box_offset, [1, 4]))\n    # Clip the boxes.\n    boxes = self.clip_boxes(boxes)\n    # Filter out ground truth boxes that are all zeros.\n    indices = tf.where(tf.not_equal(tf.reduce_sum(boxes, axis=1), 0))\n    boxes = tf.gather_nd(boxes, indices)\n    classes = tf.gather_nd(self._classes, indices)\n    return boxes, classes\n\n  @property\n  def image_scale(self):\n    # Return image scale from original image to scaled image.\n    return self._image_scale\n\n  @property\n  def image_scale_to_original(self):\n    # Return image scale from scaled image to original image.\n    return 1.0 / self._image_scale\n\n  @property\n  def offset_x(self):\n    return self._crop_offset_x\n\n  @property\n  def offset_y(self):\n    return self._crop_offset_y\n\n\nclass SegmentationInputProcessor(InputProcessor):\n  """"""Input processor for semantic segmentation.""""""\n\n  def __init__(self, image, output_size, label):\n    InputProcessor.__init__(self, image, output_size)\n    self._label = label\n\n  def random_horizontal_flip(self):\n    """"""Randomly flip input image and segmentation label.""""""\n    self._label = tf.expand_dims(self._label, 0)\n    self._image, self._label = preprocessor.random_horizontal_flip(\n        self._image, masks=self._label)\n    self._label = self._label[0, :, :]\n\n  def resize_and_crop_label(self, padding_label,\n                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR):\n    """"""Resize label and crop it to the self._output dimension.""""""\n    scaled_label = tf.image.resize_images(\n        self._label, [self._scaled_height, self._scaled_width], method=method)\n    scaled_label = scaled_label[\n        self._crop_offset_y:self._crop_offset_y + self._output_size,\n        self._crop_offset_x:self._crop_offset_x + self._output_size]\n    scaled_label -= padding_label\n    scaled_label = tf.image.pad_to_bounding_box(\n        scaled_label, 0, 0, self._output_size, self._output_size)\n    scaled_label += padding_label\n    return scaled_label\n\n\ndef pad_to_fixed_size(data, pad_value, output_shape):\n  """"""Pad data to a fixed length at the first dimension.\n\n  Args:\n    data: Tensor to be padded to output_shape.\n    pad_value: A constant value assigned to the paddings.\n    output_shape: The output shape of a 2D tensor.\n\n  Returns:\n    The Padded tensor with output_shape [max_num_instances, dimension].\n  """"""\n  max_num_instances = output_shape[0]\n  dimension = output_shape[1]\n  data = tf.reshape(data, [-1, dimension])\n  num_instances = tf.shape(data)[0]\n  assert_length = tf.Assert(\n      tf.less_equal(num_instances, max_num_instances), [num_instances])\n  with tf.control_dependencies([assert_length]):\n    pad_length = max_num_instances - num_instances\n  paddings = pad_value * tf.ones([pad_length, dimension])\n  padded_data = tf.concat([data, paddings], axis=0)\n  padded_data = tf.reshape(padded_data, output_shape)\n  return padded_data\n\n\nclass InputReader(object):\n  """"""Input reader for dataset.""""""\n\n  def __init__(self, file_pattern, is_training):\n    self._file_pattern = file_pattern\n    self._is_training = is_training\n    self._max_num_instances = MAX_NUM_INSTANCES\n\n  def __call__(self, params):\n    input_anchors = anchors.Anchors(params[\'min_level\'], params[\'max_level\'],\n                                    params[\'num_scales\'],\n                                    params[\'aspect_ratios\'],\n                                    params[\'anchor_scale\'],\n                                    params[\'image_size\'])\n    anchor_labeler = anchors.AnchorLabeler(input_anchors, params[\'num_classes\'])\n    example_decoder = tf_example_decoder.TfExampleDecoder()\n\n    def _dataset_parser(value):\n      """"""Parse data to a fixed dimension input image and learning targets.\n\n      Args:\n        value: A dictionary contains an image and groundtruth annotations.\n\n      Returns:\n        image: Image tensor that is preproessed to have normalized value and\n          fixed dimension [image_size, image_size, 3]\n        cls_targets_dict: ordered dictionary with keys\n          [min_level, min_level+1, ..., max_level]. The values are tensor with\n          shape [height_l, width_l, num_anchors]. The height_l and width_l\n          represent the dimension of class logits at l-th level.\n        box_targets_dict: ordered dictionary with keys\n          [min_level, min_level+1, ..., max_level]. The values are tensor with\n          shape [height_l, width_l, num_anchors * 4]. The height_l and\n          width_l represent the dimension of bounding box regression output at\n          l-th level.\n        num_positives: Number of positive anchors in the image.\n        source_id: Source image id. Default value -1 if the source id is empty\n          in the groundtruth annotation.\n        image_scale: Scale of the proccessed image to the original image.\n        boxes: Groundtruth bounding box annotations. The box is represented in\n          [y1, x1, y2, x2] format. The tennsor is padded with -1 to the fixed\n          dimension [self._max_num_instances, 4].\n        is_crowds: Groundtruth annotations to indicate if an annotation\n          represents a group of instances by value {0, 1}. The tennsor is\n          padded with 0 to the fixed dimension [self._max_num_instances].\n        areas: Groundtruth areas annotations. The tennsor is padded with -1\n          to the fixed dimension [self._max_num_instances].\n        classes: Groundtruth classes annotations. The tennsor is padded with -1\n          to the fixed dimension [self._max_num_instances].\n      """"""\n      with tf.name_scope(\'parser\'):\n        data = example_decoder.decode(value)\n        source_id = data[\'source_id\']\n        image = data[\'image\']\n        boxes = data[\'groundtruth_boxes\']\n        classes = data[\'groundtruth_classes\']\n        classes = tf.reshape(tf.cast(classes, dtype=tf.float32), [-1, 1])\n        areas = data[\'groundtruth_area\']\n        is_crowds = data[\'groundtruth_is_crowd\']\n        classes = tf.reshape(tf.cast(classes, dtype=tf.float32), [-1, 1])\n\n        if params[\'skip_crowd_during_training\'] and self._is_training:\n          indices = tf.where(tf.logical_not(data[\'groundtruth_is_crowd\']))\n          classes = tf.gather_nd(classes, indices)\n          boxes = tf.gather_nd(boxes, indices)\n\n        input_processor = DetectionInputProcessor(\n            image, params[\'image_size\'], boxes, classes)\n        input_processor.normalize_image()\n        if self._is_training and params[\'input_rand_hflip\']:\n          input_processor.random_horizontal_flip()\n        if self._is_training:\n          input_processor.set_training_random_scale_factors(\n              params[\'train_scale_min\'], params[\'train_scale_max\'])\n        else:\n          input_processor.set_scale_factors_to_output_size()\n        image = input_processor.resize_and_crop_image()\n        boxes, classes = input_processor.resize_and_crop_boxes()\n\n        # Assign anchors.\n        (cls_targets, box_targets,\n         num_positives) = anchor_labeler.label_anchors(boxes, classes)\n\n        source_id = tf.where(tf.equal(source_id, tf.constant(\'\')), \'-1\',\n                             source_id)\n        source_id = tf.string_to_number(source_id)\n\n        # Pad groundtruth data for evaluation.\n        image_scale = input_processor.image_scale_to_original\n        boxes *= image_scale\n        is_crowds = tf.cast(is_crowds, dtype=tf.float32)\n        boxes = pad_to_fixed_size(boxes, -1, [self._max_num_instances, 4])\n        is_crowds = pad_to_fixed_size(is_crowds, 0,\n                                      [self._max_num_instances, 1])\n        areas = pad_to_fixed_size(areas, -1, [self._max_num_instances, 1])\n        classes = pad_to_fixed_size(classes, -1, [self._max_num_instances, 1])\n        if params[\'use_bfloat16\']:\n          image = tf.cast(image, dtype=tf.bfloat16)\n        return (image, cls_targets, box_targets, num_positives, source_id,\n                image_scale, boxes, is_crowds, areas, classes)\n\n    batch_size = params[\'batch_size\']\n    dataset = tf.data.Dataset.list_files(\n        self._file_pattern, shuffle=self._is_training)\n    if self._is_training:\n      dataset = dataset.repeat()\n\n    # Prefetch data from files.\n    def _prefetch_dataset(filename):\n      dataset = tf.data.TFRecordDataset(filename).prefetch(1)\n      return dataset\n\n    dataset = dataset.apply(\n        tf.contrib.data.parallel_interleave(\n            _prefetch_dataset, cycle_length=32, sloppy=self._is_training))\n    if self._is_training:\n      dataset = dataset.shuffle(64)\n\n    # Parse the fetched records to input tensors for model function.\n    dataset = dataset.map(_dataset_parser, num_parallel_calls=64)\n    dataset = dataset.prefetch(batch_size)\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n\n    def _process_example(images, cls_targets, box_targets, num_positives,\n                         source_ids, image_scales, boxes, is_crowds, areas,\n                         classes):\n      """"""Processes one batch of data.""""""\n      labels = {}\n      # Count num_positives in a batch.\n      num_positives_batch = tf.reduce_mean(num_positives)\n      labels[\'mean_num_positives\'] = tf.reshape(\n          tf.tile(tf.expand_dims(num_positives_batch, 0), [\n              batch_size,\n          ]), [batch_size, 1])\n\n      for level in range(params[\'min_level\'], params[\'max_level\'] + 1):\n        labels[\'cls_targets_%d\' % level] = cls_targets[level]\n        labels[\'box_targets_%d\' % level] = box_targets[level]\n      # Concatenate groundtruth annotations to a tensor.\n      groundtruth_data = tf.concat([boxes, is_crowds, areas, classes], axis=2)\n      labels[\'source_ids\'] = source_ids\n      labels[\'groundtruth_data\'] = groundtruth_data\n      labels[\'image_scales\'] = image_scales\n      return images, labels\n\n    dataset = dataset.map(_process_example)\n    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n    return dataset\n\n\nclass SegmentationInputReader(object):\n  """"""Input reader for dataset.""""""\n\n  def __init__(self, file_pattern, is_training):\n    self._file_pattern = file_pattern\n    self._is_training = is_training\n\n  def __call__(self, params):\n    example_decoder = tf_example_decoder.TfExampleSegmentationDecoder()\n    def _dataset_parser(value):\n      """"""Parse data to a fixed dimension input image and learning targets.\n\n      Args:\n        value: A dictionary contains an image and groundtruth annotations.\n\n      Returns:\n        A list of the following elements in order:\n        image: Image tensor that is preproessed to have normalized value and\n          fixed dimension [image_size, image_size, 3]\n        label: label tensor of the same spatial dimension as the image.\n      """"""\n      with tf.name_scope(\'parser\'):\n        data = example_decoder.decode(value)\n        image = data[\'image\']\n        label = data[\'labels_class\']\n        label = tf.to_int32(label)\n        input_processor = SegmentationInputProcessor(image,\n                                                     params[\'image_size\'],\n                                                     label)\n        # The image normalization is identical to Cloud TPU ResNet.\n        input_processor.normalize_image()\n        if self._is_training and params[\'input_rand_hflip\']:\n          input_processor.random_horizontal_flip()\n        if self._is_training:\n          input_processor.set_training_random_scale_factors(\n              params[\'train_scale_min\'], params[\'train_scale_max\'])\n        image = input_processor.resize_and_crop_image()\n\n        # Set padding to background (class=0) during training.\n        if self._is_training:\n          label = input_processor.resize_and_crop_label(0)\n        else:\n          label = input_processor.resize_and_crop_label(params[\'ignore_label\'])\n        if params[\'use_bfloat16\']:\n          image = tf.cast(image, dtype=tf.bfloat16)\n        return image, label\n\n    batch_size = params[\'batch_size\']\n\n    dataset = tf.data.Dataset.list_files(\n        self._file_pattern, shuffle=self._is_training)\n    if self._is_training:\n      dataset = dataset.repeat()\n\n    def _prefetch_dataset(filename):\n      dataset = tf.data.TFRecordDataset(filename).prefetch(1)\n      return dataset\n\n    dataset = dataset.apply(\n        tf.contrib.data.parallel_interleave(\n            _prefetch_dataset, cycle_length=32, sloppy=self._is_training))\n    if self._is_training:\n      dataset = dataset.shuffle(64)\n\n    dataset = dataset.map(_dataset_parser, num_parallel_calls=64)\n    dataset = dataset.prefetch(batch_size)\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n    return dataset\n'"
tpu/models/official/retinanet/retinanet_architecture.py,54,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""RetinaNet (via ResNet) model definition.\n\nDefines the RetinaNet model and loss functions from this paper:\n\nhttps://arxiv.org/pdf/1708.02002\n\nUses the ResNet model as a basis.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n_WEIGHT_DECAY = 1e-4\n_BATCH_NORM_DECAY = 0.997\n_BATCH_NORM_EPSILON = 1e-4\n_RESNET_MAX_LEVEL = 5\n\n\ndef batch_norm_relu(inputs,\n                    is_training_bn,\n                    relu=True,\n                    init_zero=False,\n                    data_format=\'channels_last\',\n                    name=None):\n  """"""Performs a batch normalization followed by a ReLU.\n\n  Args:\n    inputs: `Tensor` of shape `[batch, channels, ...]`.\n    is_training_bn: `bool` for whether the model is training.\n    relu: `bool` if False, omits the ReLU operation.\n    init_zero: `bool` if True, initializes scale parameter of batch\n        normalization with 0 instead of 1 (default).\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n    name: the name of the batch normalization layer\n\n  Returns:\n    A normalized `Tensor` with the same `data_format`.\n  """"""\n  if init_zero:\n    gamma_initializer = tf.zeros_initializer()\n  else:\n    gamma_initializer = tf.ones_initializer()\n\n  if data_format == \'channels_first\':\n    axis = 1\n  else:\n    axis = 3\n\n  inputs = tf.layers.batch_normalization(\n      inputs=inputs,\n      axis=axis,\n      momentum=_BATCH_NORM_DECAY,\n      epsilon=_BATCH_NORM_EPSILON,\n      center=True,\n      scale=True,\n      training=is_training_bn,\n      fused=True,\n      gamma_initializer=gamma_initializer,\n      name=name)\n\n  if relu:\n    inputs = tf.nn.relu(inputs)\n  return inputs\n\n\ndef fixed_padding(inputs, kernel_size, data_format=\'channels_last\'):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Args:\n    inputs: `Tensor` of size `[batch, channels, height, width]` or\n        `[batch, height, width, channels]` depending on `data_format`.\n    kernel_size: `int` kernel size to be used for `conv2d` or max_pool2d`\n        operations. Should be a positive integer.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    A padded `Tensor` of the same `data_format` with size either intact\n    (if `kernel_size == 1`) or padded (if `kernel_size > 1`).\n  """"""\n  pad_total = kernel_size - 1\n  pad_beg = pad_total // 2\n  pad_end = pad_total - pad_beg\n  if data_format == \'channels_first\':\n    padded_inputs = tf.pad(\n        inputs, [[0, 0], [0, 0], [pad_beg, pad_end], [pad_beg, pad_end]])\n  else:\n    padded_inputs = tf.pad(\n        inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n\n  return padded_inputs\n\n\ndef conv2d_fixed_padding(inputs,\n                         filters,\n                         kernel_size,\n                         strides,\n                         data_format=\'channels_last\'):\n  """"""Strided 2-D convolution with explicit padding.\n\n  The padding is consistent and is based only on `kernel_size`, not on the\n  dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n\n  Args:\n    inputs: `Tensor` of size `[batch, channels, height_in, width_in]`.\n    filters: `int` number of filters in the convolution.\n    kernel_size: `int` size of the kernel to be used in the convolution.\n    strides: `int` strides of the convolution.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    A `Tensor` of shape `[batch, filters, height_out, width_out]`.\n  """"""\n  if strides > 1:\n    inputs = fixed_padding(inputs, kernel_size, data_format=data_format)\n\n  return tf.layers.conv2d(\n      inputs=inputs,\n      filters=filters,\n      kernel_size=kernel_size,\n      strides=strides,\n      padding=(\'SAME\' if strides == 1 else \'VALID\'),\n      use_bias=False,\n      kernel_initializer=tf.variance_scaling_initializer(),\n      data_format=data_format)\n\n\ndef residual_block(inputs,\n                   filters,\n                   is_training_bn,\n                   strides,\n                   use_projection=False,\n                   data_format=\'channels_last\'):\n  """"""Standard building block for residual networks with BN after convolutions.\n\n  Args:\n    inputs: `Tensor` of size `[batch, channels, height, width]`.\n    filters: `int` number of filters for the first two convolutions. Note that\n        the third and final convolution will use 4 times as many filters.\n    is_training_bn: `bool` for whether the model is in training.\n    strides: `int` block stride. If greater than 1, this block will ultimately\n        downsample the input.\n    use_projection: `bool` for whether this block should use a projection\n        shortcut (versus the default identity shortcut). This is usually `True`\n        for the first block of a block group, which may change the number of\n        filters and the resolution.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    The output `Tensor` of the block.\n  """"""\n  shortcut = inputs\n  if use_projection:\n    # Projection shortcut in first layer to match filters and strides\n    shortcut = conv2d_fixed_padding(\n        inputs=inputs,\n        filters=filters,\n        kernel_size=1,\n        strides=strides,\n        data_format=data_format)\n    shortcut = batch_norm_relu(\n        shortcut, is_training_bn, relu=False, data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs,\n      filters=filters,\n      kernel_size=3,\n      strides=strides,\n      data_format=data_format)\n  inputs = batch_norm_relu(inputs, is_training_bn, data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs,\n      filters=filters,\n      kernel_size=3,\n      strides=1,\n      data_format=data_format)\n  inputs = batch_norm_relu(\n      inputs,\n      is_training_bn,\n      relu=False,\n      init_zero=True,\n      data_format=data_format)\n\n  return tf.nn.relu(inputs + shortcut)\n\n\ndef bottleneck_block(inputs,\n                     filters,\n                     is_training_bn,\n                     strides,\n                     use_projection=False,\n                     data_format=\'channels_last\'):\n  """"""Bottleneck block variant for residual networks with BN after convolutions.\n\n  Args:\n    inputs: `Tensor` of size `[batch, channels, height, width]`.\n    filters: `int` number of filters for the first two convolutions. Note that\n        the third and final convolution will use 4 times as many filters.\n    is_training_bn: `bool` for whether the model is in training.\n    strides: `int` block stride. If greater than 1, this block will ultimately\n        downsample the input.\n    use_projection: `bool` for whether this block should use a projection\n        shortcut (versus the default identity shortcut). This is usually `True`\n        for the first block of a block group, which may change the number of\n        filters and the resolution.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    The output `Tensor` of the block.\n  """"""\n  shortcut = inputs\n  if use_projection:\n    # Projection shortcut only in first block within a group. Bottleneck blocks\n    # end with 4 times the number of filters.\n    filters_out = 4 * filters\n    shortcut = conv2d_fixed_padding(\n        inputs=inputs,\n        filters=filters_out,\n        kernel_size=1,\n        strides=strides,\n        data_format=data_format)\n    shortcut = batch_norm_relu(\n        shortcut, is_training_bn, relu=False, data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs,\n      filters=filters,\n      kernel_size=1,\n      strides=1,\n      data_format=data_format)\n  inputs = batch_norm_relu(inputs, is_training_bn, data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs,\n      filters=filters,\n      kernel_size=3,\n      strides=strides,\n      data_format=data_format)\n  inputs = batch_norm_relu(inputs, is_training_bn, data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs,\n      filters=4 * filters,\n      kernel_size=1,\n      strides=1,\n      data_format=data_format)\n  inputs = batch_norm_relu(\n      inputs,\n      is_training_bn,\n      relu=False,\n      init_zero=True,\n      data_format=data_format)\n\n  return tf.nn.relu(inputs + shortcut)\n\n\ndef block_group(inputs,\n                filters,\n                block_fn,\n                blocks,\n                strides,\n                is_training_bn,\n                name,\n                data_format=\'channels_last\'):\n  """"""Creates one group of blocks for the ResNet model.\n\n  Args:\n    inputs: `Tensor` of size `[batch, channels, height, width]`.\n    filters: `int` number of filters for the first convolution of the layer.\n    block_fn: `function` for the block to use within the model\n    blocks: `int` number of blocks contained in the layer.\n    strides: `int` stride to use for the first convolution of the layer. If\n        greater than 1, this layer will downsample the input.\n    is_training_bn: `bool` for whether the model is training.\n    name: `str`name for the Tensor output of the block layer.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    The output `Tensor` of the block layer.\n  """"""\n  # Only the first block per block_group uses projection shortcut and strides.\n  inputs = block_fn(\n      inputs,\n      filters,\n      is_training_bn,\n      strides,\n      use_projection=True,\n      data_format=data_format)\n\n  for _ in range(1, blocks):\n    inputs = block_fn(\n        inputs, filters, is_training_bn, 1, data_format=data_format)\n\n  return tf.identity(inputs, name)\n\n\ndef resnet_v1_generator(block_fn, layers, data_format=\'channels_last\'):\n  """"""Generator of ResNet v1 model with classification layers removed.\n\n    Our actual ResNet network.  We return the output of c2, c3,c4,c5\n    N.B. batch norm is always run with trained parameters, as we use very small\n    batches when training the object layers.\n\n  Args:\n    block_fn: `function` for the block to use within the model. Either\n        `residual_block` or `bottleneck_block`.\n    layers: list of 4 `int`s denoting the number of blocks to include in each\n      of the 4 block groups. Each group consists of blocks that take inputs of\n      the same resolution.\n    data_format: `str` either ""channels_first"" for `[batch, channels, height,\n        width]` or ""channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    Model `function` that takes in `inputs` and `is_training` and returns the\n    output `Tensor` of the ResNet model.\n  """"""\n  def model(inputs, is_training_bn=False):\n    """"""Creation of the model graph.""""""\n    inputs = conv2d_fixed_padding(\n        inputs=inputs,\n        filters=64,\n        kernel_size=7,\n        strides=2,\n        data_format=data_format)\n    inputs = tf.identity(inputs, \'initial_conv\')\n    inputs = batch_norm_relu(inputs, is_training_bn, data_format=data_format)\n\n    inputs = tf.layers.max_pooling2d(\n        inputs=inputs,\n        pool_size=3,\n        strides=2,\n        padding=\'SAME\',\n        data_format=data_format)\n    inputs = tf.identity(inputs, \'initial_max_pool\')\n\n    c2 = block_group(\n        inputs=inputs,\n        filters=64,\n        blocks=layers[0],\n        strides=1,\n        block_fn=block_fn,\n        is_training_bn=is_training_bn,\n        name=\'block_group1\',\n        data_format=data_format)\n    c3 = block_group(\n        inputs=c2,\n        filters=128,\n        blocks=layers[1],\n        strides=2,\n        block_fn=block_fn,\n        is_training_bn=is_training_bn,\n        name=\'block_group2\',\n        data_format=data_format)\n    c4 = block_group(\n        inputs=c3,\n        filters=256,\n        blocks=layers[2],\n        strides=2,\n        block_fn=block_fn,\n        is_training_bn=is_training_bn,\n        name=\'block_group3\',\n        data_format=data_format)\n    c5 = block_group(\n        inputs=c4,\n        filters=512,\n        blocks=layers[3],\n        strides=2,\n        block_fn=block_fn,\n        is_training_bn=is_training_bn,\n        name=\'block_group4\',\n        data_format=data_format)\n    return c2, c3, c4, c5\n\n  return model\n\n\ndef resnet_v1(resnet_depth, data_format=\'channels_last\'):\n  """"""Returns the ResNet model for a given size and number of output classes.""""""\n  model_params = {\n      18: {\'block\': residual_block, \'layers\': [2, 2, 2, 2]},\n      34: {\'block\': residual_block, \'layers\': [3, 4, 6, 3]},\n      50: {\'block\': bottleneck_block, \'layers\': [3, 4, 6, 3]},\n      101: {\'block\': bottleneck_block, \'layers\': [3, 4, 23, 3]},\n      152: {\'block\': bottleneck_block, \'layers\': [3, 8, 36, 3]},\n      200: {\'block\': bottleneck_block, \'layers\': [3, 24, 36, 3]}\n  }\n\n  if resnet_depth not in model_params:\n    raise ValueError(\'Not a valid resnet_depth:\', resnet_depth)\n\n  params = model_params[resnet_depth]\n  return resnet_v1_generator(\n      params[\'block\'], params[\'layers\'], data_format)\n\n\ndef nearest_upsampling(data, scale):\n  """"""Nearest neighbor upsampling implementation.\n\n  Args:\n    data: A float32 tensor of size [batch, height_in, width_in, channels].\n    scale: An integer multiple to scale resolution of input data.\n  Returns:\n    data_up: A float32 tensor of size\n      [batch, height_in*scale, width_in*scale, channels].\n  """"""\n  with tf.name_scope(\'nearest_upsampling\'):\n    bs, h, w, c = data.get_shape().as_list()\n    bs = -1 if bs is None else bs\n    # Use reshape to quickly upsample the input.  The nearest pixel is selected\n    # implicitly via broadcasting.\n    data = tf.reshape(data, [bs, h, 1, w, 1, c]) * tf.ones(\n        [1, 1, scale, 1, scale, 1], dtype=data.dtype)\n    return tf.reshape(data, [bs, h * scale, w * scale, c])\n\n\n# TODO(b/111271774): Removes this wrapper once b/111271774 is resolved.\ndef resize_bilinear(images, size, output_type):\n  """"""Returns resized images as output_type.\n\n  Args:\n    images: A tensor of size [batch, height_in, width_in, channels].\n    size: A 1-D int32 Tensor of 2 elements: new_height, new_width. The new size\n      for the images.\n    output_type: The destination type.\n  Returns:\n    A tensor of size [batch, height_out, width_out, channels] as a dtype of\n      output_type.\n  """"""\n  images = tf.image.resize_bilinear(images, size, align_corners=True)\n  return tf.cast(images, output_type)\n\n\n## RetinaNet specific layers\ndef class_net(images, level, num_classes, num_anchors=6, is_training_bn=False):\n  """"""Class prediction network for RetinaNet.""""""\n  for i in range(4):\n    images = tf.layers.conv2d(\n        images,\n        256,\n        kernel_size=(3, 3),\n        bias_initializer=tf.zeros_initializer(),\n        kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n        activation=None,\n        padding=\'same\',\n        name=\'class-%d\' % i)\n    # The convolution layers in the class net are shared among all levels, but\n    # each level has its batch normlization to capture the statistical\n    # difference among different levels.\n    images = batch_norm_relu(images, is_training_bn, relu=True, init_zero=False,\n                             name=\'class-%d-bn-%d\' % (i, level))\n\n  classes = tf.layers.conv2d(\n      images,\n      num_classes * num_anchors,\n      kernel_size=(3, 3),\n      bias_initializer=tf.constant_initializer(-np.log((1 - 0.01) / 0.01)),\n      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n      padding=\'same\',\n      name=\'class-predict\')\n\n  return classes\n\n\ndef box_net(images, level, num_anchors=6, is_training_bn=False):\n  """"""Box regression network for RetinaNet.""""""\n  for i in range(4):\n    images = tf.layers.conv2d(\n        images,\n        256,\n        kernel_size=(3, 3),\n        activation=None,\n        bias_initializer=tf.zeros_initializer(),\n        kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n        padding=\'same\',\n        name=\'box-%d\' % i)\n    # The convolution layers in the box net are shared among all levels, but\n    # each level has its batch normlization to capture the statistical\n    # difference among different levels.\n    images = batch_norm_relu(images, is_training_bn, relu=True, init_zero=False,\n                             name=\'box-%d-bn-%d\' % (i, level))\n\n  boxes = tf.layers.conv2d(\n      images,\n      4 * num_anchors,\n      kernel_size=(3, 3),\n      bias_initializer=tf.zeros_initializer(),\n      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n      padding=\'same\',\n      name=\'box-predict\')\n\n  return boxes\n\n\ndef resnet_fpn(features,\n               min_level=3,\n               max_level=7,\n               resnet_depth=50,\n               is_training_bn=False,\n               use_nearest_upsampling=True):\n  """"""ResNet feature pyramid networks.""""""\n  # upward layers\n  with tf.variable_scope(\'resnet%s\' % resnet_depth):\n    resnet_fn = resnet_v1(resnet_depth)\n    u2, u3, u4, u5 = resnet_fn(features, is_training_bn)\n\n  feats_bottom_up = {\n      2: u2,\n      3: u3,\n      4: u4,\n      5: u5,\n  }\n\n  with tf.variable_scope(\'resnet_fpn\'):\n    # lateral connections\n    feats_lateral = {}\n    for level in range(min_level, _RESNET_MAX_LEVEL + 1):\n      feats_lateral[level] = tf.layers.conv2d(\n          feats_bottom_up[level],\n          filters=256,\n          kernel_size=(1, 1),\n          padding=\'same\',\n          name=\'l%d\' % level)\n\n    # add top-down path\n    feats = {_RESNET_MAX_LEVEL: feats_lateral[_RESNET_MAX_LEVEL]}\n    for level in range(_RESNET_MAX_LEVEL - 1, min_level - 1, -1):\n      if use_nearest_upsampling:\n        feats[level] = nearest_upsampling(feats[level + 1],\n                                          2) + feats_lateral[level]\n      else:\n        feats[level] = resize_bilinear(\n            feats[level + 1], tf.shape(feats_lateral[level])[1:3],\n            feats[level + 1].dtype) + feats_lateral[level]\n\n    # add post-hoc 3x3 convolution kernel\n    for level in range(min_level, _RESNET_MAX_LEVEL + 1):\n      feats[level] = tf.layers.conv2d(\n          feats[level],\n          filters=256,\n          strides=(1, 1),\n          kernel_size=(3, 3),\n          padding=\'same\',\n          name=\'post_hoc_d%d\' % level)\n\n    # coarser FPN levels introduced for RetinaNet\n    for level in range(_RESNET_MAX_LEVEL + 1, max_level + 1):\n      feats_in = feats[level - 1]\n      if level > _RESNET_MAX_LEVEL + 1:\n        feats_in = tf.nn.relu(feats_in)\n      feats[level] = tf.layers.conv2d(\n          feats_in,\n          filters=256,\n          strides=(2, 2),\n          kernel_size=(3, 3),\n          padding=\'same\',\n          name=\'p%d\' % level)\n    # add batchnorm\n    for level in range(min_level, max_level + 1):\n      feats[level] = tf.layers.batch_normalization(\n          inputs=feats[level],\n          momentum=_BATCH_NORM_DECAY,\n          epsilon=_BATCH_NORM_EPSILON,\n          center=True,\n          scale=True,\n          training=is_training_bn,\n          fused=True,\n          name=\'p%d-bn\' % level)\n\n  return feats\n\n\ndef retinanet(features,\n              min_level=3,\n              max_level=7,\n              num_classes=90,\n              num_anchors=6,\n              resnet_depth=50,\n              use_nearest_upsampling=True,\n              is_training_bn=False):\n  """"""RetinaNet classification and regression model.""""""\n  # create feature pyramid networks\n  feats = resnet_fpn(features, min_level, max_level, resnet_depth,\n                     is_training_bn, use_nearest_upsampling)\n  # add class net and box net in RetinaNet. The class net and the box net are\n  # shared among all the levels.\n  with tf.variable_scope(\'retinanet\'):\n    class_outputs = {}\n    box_outputs = {}\n    with tf.variable_scope(\'class_net\', reuse=tf.AUTO_REUSE):\n      for level in range(min_level, max_level + 1):\n        class_outputs[level] = class_net(feats[level], level, num_classes,\n                                         num_anchors, is_training_bn)\n    with tf.variable_scope(\'box_net\', reuse=tf.AUTO_REUSE):\n      for level in range(min_level, max_level + 1):\n        box_outputs[level] = box_net(feats[level], level,\n                                     num_anchors, is_training_bn)\n\n  return class_outputs, box_outputs\n\n\ndef remove_variables(variables, resnet_depth=50):\n  """"""Removes low-level variables from the input.\n\n  Removing low-level parameters (e.g., initial convolution layer) from training\n  usually leads to higher training speed and slightly better testing accuracy.\n  The intuition is that the low-level architecture (e.g., ResNet-50) is able to\n  capture low-level features such as edges; therefore, it does not need to be\n  fine-tuned for the detection task.\n\n  Args:\n    variables: all the variables in training\n    resnet_depth: the depth of ResNet model\n\n  Returns:\n    var_list: a list containing variables for training\n\n  """"""\n  var_list = [v for v in variables\n              if v.name.find(\'resnet%s/conv2d/\' % resnet_depth) == -1]\n  return var_list\n\n\ndef segmentation_class_net(images,\n                           level,\n                           num_channels=256,\n                           is_training_bn=False):\n  """"""Segmentation Feature Extraction Module.\n\n  Args:\n    images: A tensor of size [batch, height_in, width_in, channels_in].\n    level: The level of features at FPN output_size /= 2^level.\n    num_channels: The number of channels in convolution layers\n    is_training_bn: Whether batch_norm layers are in training mode.\n  Returns:\n    images: A feature tensor of size [batch, output_size, output_size,\n      channel_number]\n  """"""\n\n  for i in range(3):\n    images = tf.layers.conv2d(\n        images,\n        num_channels,\n        kernel_size=(3, 3),\n        bias_initializer=tf.zeros_initializer(),\n        kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n        activation=None,\n        padding=\'same\',\n        name=\'class-%d\' % i)\n    images = batch_norm_relu(images, is_training_bn, relu=True, init_zero=False,\n                             name=\'class-%d-bn-%d\' % (i, level))\n  images = tf.layers.conv2d(\n      images,\n      num_channels,\n      kernel_size=(3, 3),\n      bias_initializer=tf.zeros_initializer(),\n      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n      activation=None,\n      padding=\'same\',\n      name=\'class-final\')\n  return images\n\n\ndef retinanet_segmentation(features,\n                           min_level=3,\n                           max_level=5,\n                           num_classes=21,\n                           resnet_depth=50,\n                           use_nearest_upsampling=False,\n                           is_training_bn=False):\n  """"""RetinaNet extension for semantic segmentation.\n\n  Args:\n    features: A tensor of size [batch, height_in, width_in, channels].\n    min_level: The minimum output feature pyramid level. This input defines the\n      smallest nominal feature stride = 2^min_level.\n    max_level: The maximum output feature pyramid level. This input defines the\n      largest nominal feature stride = 2^max_level.\n    num_classes: Number of object classes.\n    resnet_depth: The depth of ResNet backbone model.\n    use_nearest_upsampling: Whether use nearest upsampling for FPN network.\n      Alternatively, use bilinear upsampling.\n    is_training_bn: Whether batch_norm layers are in training mode.\n  Returns:\n    A tensor of size [batch, height_l, width_l, num_classes]\n      representing pixel-wise predictions before Softmax function.\n  """"""\n  feats = resnet_fpn(features, min_level, max_level, resnet_depth,\n                     is_training_bn, use_nearest_upsampling)\n\n  with tf.variable_scope(\'class_net\', reuse=tf.AUTO_REUSE):\n    for level in range(min_level, max_level + 1):\n      feats[level] = segmentation_class_net(\n          feats[level], level, is_training_bn=is_training_bn)\n      if level == min_level:\n        fused_feature = feats[level]\n      else:\n        if use_nearest_upsampling:\n          scale = level / min_level\n          feats[level] = nearest_upsampling(feats[level], scale)\n        else:\n          feats[level] = resize_bilinear(\n              feats[level], tf.shape(feats[min_level])[1:3], feats[level].dtype)\n        fused_feature += feats[level]\n  fused_feature = batch_norm_relu(\n      fused_feature, is_training_bn, relu=True, init_zero=False)\n  classes = tf.layers.conv2d(\n      fused_feature,\n      num_classes,\n      kernel_size=(3, 3),\n      bias_initializer=tf.zeros_initializer(),\n      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n      padding=\'same\',\n      name=\'class-predict\')\n\n  return classes\n'"
tpu/models/official/retinanet/retinanet_main.py,27,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Training script for RetinaNet.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import flags\nimport numpy as np\nimport tensorflow as tf\n\nimport dataloader\nimport retinanet_model\n\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'tpu\', default=None,\n    help=\'The Cloud TPU to use for training. This should be either the name \'\n    \'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \'\n    \'url.\')\nflags.DEFINE_string(\n    \'gcp_project\', default=None,\n    help=\'Project name for the Cloud TPU-enabled project. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\nflags.DEFINE_string(\n    \'tpu_zone\', default=None,\n    help=\'GCE zone where the Cloud TPU is located in. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\n\n# Model specific paramenters\nflags.DEFINE_string(\n    \'eval_master\', default=\'\',\n    help=\'GRPC URL of the eval master. Set to an appropiate value when running \'\n    \'on CPU/GPU\')\nflags.DEFINE_bool(\'use_tpu\', True, \'Use TPUs rather than CPUs\')\nflags.DEFINE_bool(\n    \'use_xla\', False,\n    \'Use XLA even if use_tpu is false.  If use_tpu is true, we always use XLA, \'\n    \'and this flag has no effect.\')\nflags.DEFINE_string(\'model_dir\', None, \'Location of model_dir\')\nflags.DEFINE_string(\'resnet_checkpoint\', \'\',\n                    \'Location of the ResNet50 checkpoint to use for model \'\n                    \'initialization.\')\nflags.DEFINE_string(\'hparams\', \'\',\n                    \'Comma separated k=v pairs of hyperparameters.\')\nflags.DEFINE_integer(\n    \'num_cores\', default=8, help=\'Number of TPU cores for training\')\nflags.DEFINE_bool(\'use_spatial_partition\', False, \'Use spatial partition.\')\nflags.DEFINE_integer(\n    \'num_cores_per_replica\', default=8, help=\'Number of TPU cores per\'\n    \'replica when using spatial partition.\')\nflags.DEFINE_multi_integer(\n    \'input_partition_dims\', [1, 4, 2, 1],\n    \'A list that describes the partition dims for all the tensors.\')\nflags.DEFINE_integer(\'train_batch_size\', 64, \'training batch size\')\nflags.DEFINE_integer(\'eval_batch_size\', 1, \'evaluation batch size\')\nflags.DEFINE_integer(\'eval_samples\', 5000, \'The number of samples for \'\n                     \'evaluation.\')\nflags.DEFINE_integer(\n    \'iterations_per_loop\', 100, \'Number of iterations per TPU training loop\')\nflags.DEFINE_string(\n    \'training_file_pattern\', None,\n    \'Glob for training data files (e.g., COCO train - minival set)\')\nflags.DEFINE_string(\n    \'validation_file_pattern\', None,\n    \'Glob for evaluation tfrecords (e.g., COCO val2017 set)\')\nflags.DEFINE_string(\n    \'val_json_file\',\n    None,\n    \'COCO validation JSON containing golden bounding boxes.\')\nflags.DEFINE_integer(\'num_examples_per_epoch\', 120000,\n                     \'Number of examples in one epoch\')\nflags.DEFINE_integer(\'num_epochs\', 15, \'Number of epochs for training\')\nflags.DEFINE_string(\'mode\', \'train\',\n                    \'Mode to run: train or eval (default: train)\')\nflags.DEFINE_bool(\'eval_after_training\', False, \'Run one eval after the \'\n                  \'training finishes.\')\n\n# For Eval mode\nflags.DEFINE_integer(\'min_eval_interval\', 180,\n                     \'Minimum seconds between evaluations.\')\nflags.DEFINE_integer(\n    \'eval_timeout\', None,\n    \'Maximum seconds between checkpoints before evaluation terminates.\')\n\nFLAGS = flags.FLAGS\n\n\ndef main(argv):\n  del argv  # Unused.\n\n  if FLAGS.use_tpu:\n    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu,\n        zone=FLAGS.tpu_zone,\n        project=FLAGS.gcp_project)\n    tpu_grpc_url = tpu_cluster_resolver.get_master()\n    tf.Session.reset(tpu_grpc_url)\n  else:\n    tpu_cluster_resolver = None\n\n  # Check data path\n  if FLAGS.mode in (\'train\',\n                    \'train_and_eval\') and FLAGS.training_file_pattern is None:\n    raise RuntimeError(\'You must specify --training_file_pattern for training.\')\n  if FLAGS.mode in (\'eval\', \'train_and_eval\'):\n    if FLAGS.validation_file_pattern is None:\n      raise RuntimeError(\'You must specify --validation_file_pattern \'\n                         \'for evaluation.\')\n    if FLAGS.val_json_file is None:\n      raise RuntimeError(\'You must specify --val_json_file for evaluation.\')\n\n  # Parse hparams\n  hparams = retinanet_model.default_hparams()\n  hparams.parse(FLAGS.hparams)\n\n  # The following is for spatial partitioning. `features` has one tensor while\n  # `labels` had 4 + (`max_level` - `min_level` + 1) * 2 tensors. The input\n  # partition is performed on `features` and all partitionable tensors of\n  # `labels`, see the partition logic below.\n  # In the TPUEstimator context, the meaning of `shard` and `replica` is the\n  # same; follwing the API, here has mixed use of both.\n  if FLAGS.use_spatial_partition:\n    # Checks input_partition_dims agrees with num_cores_per_replica.\n    if FLAGS.num_cores_per_replica != np.prod(FLAGS.input_partition_dims):\n      raise RuntimeError(\'--num_cores_per_replica must be a product of array\'\n                         \'elements in --input_partition_dims.\')\n\n    labels_partition_dims = {\n        \'mean_num_positives\': None,\n        \'source_ids\': None,\n        \'groundtruth_data\': None,\n        \'image_scales\': None,\n    }\n    # The Input Partition Logic: We partition only the partition-able tensors.\n    # Spatial partition requires that the to-be-partitioned tensors must have a\n    # dimension that is a multiple of `partition_dims`. Depending on the\n    # `partition_dims` and the `image_size` and the `max_level` in hparams, some\n    # high-level anchor labels (i.e., `cls_targets` and `box_targets`) cannot\n    # be partitioned. For example, when `partition_dims` is [1, 4, 2, 1], image\n    # size is 1536, `max_level` is 9, `cls_targets_8` has a shape of\n    # [batch_size, 6, 6, 9], which cannot be partitioned (6 % 4 != 0). In this\n    # case, the level-8 and level-9 target tensors are not partition-able, and\n    # the highest partition-able level is 7.\n    image_size = hparams.get(\'image_size\')\n    for level in range(hparams.get(\'min_level\'), hparams.get(\'max_level\') + 1):\n\n      def _can_partition(spatial_dim):\n        partitionable_index = np.where(\n            spatial_dim % np.array(FLAGS.input_partition_dims) == 0)\n        return len(partitionable_index[0]) == len(FLAGS.input_partition_dims)\n\n      spatial_dim = image_size // (2 ** level)\n      if _can_partition(spatial_dim):\n        labels_partition_dims[\n            \'box_targets_%d\' % level] = FLAGS.input_partition_dims\n        labels_partition_dims[\n            \'cls_targets_%d\' % level] = FLAGS.input_partition_dims\n      else:\n        labels_partition_dims[\'box_targets_%d\' % level] = None\n        labels_partition_dims[\'cls_targets_%d\' % level] = None\n\n    num_cores_per_replica = FLAGS.num_cores_per_replica\n    input_partition_dims = [\n        FLAGS.input_partition_dims, labels_partition_dims]\n    num_shards = FLAGS.num_cores // num_cores_per_replica\n  else:\n    num_cores_per_replica = None\n    input_partition_dims = None\n    num_shards = FLAGS.num_cores\n\n  params = dict(\n      hparams.values(),\n      num_shards=num_shards,\n      num_examples_per_epoch=FLAGS.num_examples_per_epoch,\n      use_tpu=FLAGS.use_tpu,\n      resnet_checkpoint=FLAGS.resnet_checkpoint,\n      val_json_file=FLAGS.val_json_file,\n      mode=FLAGS.mode,\n  )\n  config_proto = tf.ConfigProto(\n      allow_soft_placement=True, log_device_placement=False)\n  if FLAGS.use_xla and not FLAGS.use_tpu:\n    config_proto.graph_options.optimizer_options.global_jit_level = (\n        tf.OptimizerOptions.ON_1)\n\n  tpu_config = tf.contrib.tpu.TPUConfig(\n      FLAGS.iterations_per_loop,\n\n      num_shards=num_shards,\n      num_cores_per_replica=num_cores_per_replica,\n      input_partition_dims=input_partition_dims,\n      per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n  )\n\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      evaluation_master=FLAGS.eval_master,\n      model_dir=FLAGS.model_dir,\n      log_step_count_steps=FLAGS.iterations_per_loop,\n      session_config=config_proto,\n      tpu_config=tpu_config,\n  )\n\n  # TPU Estimator\n  if FLAGS.mode == \'train\':\n    tf.logging.info(params)\n    train_estimator = tf.contrib.tpu.TPUEstimator(\n        model_fn=retinanet_model.retinanet_model_fn,\n        use_tpu=FLAGS.use_tpu,\n        train_batch_size=FLAGS.train_batch_size,\n        config=run_config,\n        params=params)\n    train_estimator.train(\n        input_fn=dataloader.InputReader(FLAGS.training_file_pattern,\n                                        is_training=True),\n        max_steps=int((FLAGS.num_epochs * FLAGS.num_examples_per_epoch) /\n                      FLAGS.train_batch_size))\n\n    if FLAGS.eval_after_training:\n      # Run evaluation after training finishes.\n      eval_params = dict(\n          params,\n          use_tpu=False,\n          input_rand_hflip=False,\n          resnet_checkpoint=None,\n          is_training_bn=False,\n          use_bfloat16=False,\n      )\n      eval_estimator = tf.contrib.tpu.TPUEstimator(\n          model_fn=retinanet_model.retinanet_model_fn,\n          use_tpu=False,\n          train_batch_size=FLAGS.train_batch_size,\n          eval_batch_size=FLAGS.eval_batch_size,\n          config=run_config,\n          params=eval_params)\n      eval_results = eval_estimator.evaluate(\n          input_fn=dataloader.InputReader(FLAGS.validation_file_pattern,\n                                          is_training=False),\n          steps=FLAGS.eval_samples//FLAGS.eval_batch_size)\n      tf.logging.info(\'Eval results: %s\' % eval_results)\n\n  elif FLAGS.mode == \'eval\':\n    # Eval only runs on CPU or GPU host with batch_size = 1.\n    # Override the default options: disable randomization in the input pipeline\n    # and don\'t run on the TPU.\n    # Also, disable use_bfloat16 for eval on CPU/GPU.\n    eval_params = dict(\n        params,\n        use_tpu=False,\n        input_rand_hflip=False,\n        resnet_checkpoint=None,\n        is_training_bn=False,\n        use_bfloat16=False,\n    )\n\n    eval_estimator = tf.contrib.tpu.TPUEstimator(\n        model_fn=retinanet_model.retinanet_model_fn,\n        use_tpu=False,\n        train_batch_size=FLAGS.train_batch_size,\n        eval_batch_size=FLAGS.eval_batch_size,\n        config=run_config,\n        params=eval_params)\n\n    def terminate_eval():\n      tf.logging.info(\'Terminating eval after %d seconds of no checkpoints\' %\n                      FLAGS.eval_timeout)\n      return True\n\n    # Run evaluation when there\'s a new checkpoint\n    for ckpt in tf.contrib.training.checkpoints_iterator(\n        FLAGS.model_dir,\n        min_interval_secs=FLAGS.min_eval_interval,\n        timeout=FLAGS.eval_timeout,\n        timeout_fn=terminate_eval):\n\n      tf.logging.info(\'Starting to evaluate.\')\n      try:\n        eval_results = eval_estimator.evaluate(\n            input_fn=dataloader.InputReader(FLAGS.validation_file_pattern,\n                                            is_training=False),\n            steps=FLAGS.eval_samples//FLAGS.eval_batch_size)\n        tf.logging.info(\'Eval results: %s\' % eval_results)\n\n        # Terminate eval job when final checkpoint is reached\n        current_step = int(os.path.basename(ckpt).split(\'-\')[1])\n        total_step = int((FLAGS.num_epochs * FLAGS.num_examples_per_epoch) /\n                         FLAGS.train_batch_size)\n        if current_step >= total_step:\n          tf.logging.info(\'Evaluation finished after training step %d\' %\n                          current_step)\n          break\n\n      except tf.errors.NotFoundError:\n        # Since the coordinator is on a different job than the TPU worker,\n        # sometimes the TPU worker does not finish initializing until long after\n        # the CPU job tells it to start evaluating. In this case, the checkpoint\n        # file could have been deleted already.\n        tf.logging.info(\'Checkpoint %s no longer exists, skipping checkpoint\' %\n                        ckpt)\n\n  elif FLAGS.mode == \'train_and_eval\':\n    for cycle in range(FLAGS.num_epochs):\n      tf.logging.info(\'Starting training cycle, epoch: %d.\' % cycle)\n      train_estimator = tf.contrib.tpu.TPUEstimator(\n          model_fn=retinanet_model.retinanet_model_fn,\n          use_tpu=FLAGS.use_tpu,\n          train_batch_size=FLAGS.train_batch_size,\n          config=run_config,\n          params=params)\n      train_estimator.train(\n          input_fn=dataloader.InputReader(FLAGS.training_file_pattern,\n                                          is_training=True),\n          steps=int(FLAGS.num_examples_per_epoch / FLAGS.train_batch_size))\n\n      tf.logging.info(\'Starting evaluation cycle, epoch: %d.\' % cycle)\n      # Run evaluation after every epoch.\n      eval_params = dict(\n          params,\n          use_tpu=False,\n          input_rand_hflip=False,\n          resnet_checkpoint=None,\n          is_training_bn=False,\n      )\n\n      eval_estimator = tf.contrib.tpu.TPUEstimator(\n          model_fn=retinanet_model.retinanet_model_fn,\n          use_tpu=False,\n          train_batch_size=FLAGS.train_batch_size,\n          eval_batch_size=FLAGS.eval_batch_size,\n          config=run_config,\n          params=eval_params)\n      eval_results = eval_estimator.evaluate(\n          input_fn=dataloader.InputReader(FLAGS.validation_file_pattern,\n                                          is_training=False),\n          steps=FLAGS.eval_samples//FLAGS.eval_batch_size)\n      tf.logging.info(\'Evaluation results: %s\' % eval_results)\n\n  else:\n    tf.logging.info(\'Mode not found.\')\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
tpu/models/official/retinanet/retinanet_model.py,63,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Model defination for the RetinaNet Model.\n\nDefines model_fn of RetinaNet for TF Estimator. The model_fn includes RetinaNet\nmodel architecture, loss function, learning rate schedule, and evaluation\nprocedure.\n\nT.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar\nFocal Loss for Dense Object Detection. arXiv:1708.02002\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport anchors\nimport coco_metric\nimport retinanet_architecture\n\n_DEFAULT_BATCH_SIZE = 64\n_WEIGHT_DECAY = 1e-4\n\n\ndef update_learning_rate_schedule_parameters(params):\n  """"""Updates params that are related to the learning rate schedule.\n\n  This function adjusts the learning schedule based on the given batch size and\n  other LR-schedule-related parameters. The default values specified in the\n  default_hparams() are for training with a batch size of 64 and COCO dataset.\n\n  For other batch sizes that train with the same schedule w.r.t. the number of\n  epochs, this function handles the learning rate schedule.\n\n    For batch size=64, the default values are listed below:\n      learning_rate=0.08,\n      lr_warmup_epoch=1.0,\n      first_lr_drop_epoch=8.0,\n      second_lr_drop_epoch=11.0;\n    The values are converted to a LR schedule listed below:\n      adjusted_learning_rate=0.08,\n      lr_warmup_step=1875,\n      first_lr_drop_step=15000,\n      second_lr_drop_step=20625;\n    For batch size=8, the default values will have the following LR shedule:\n      adjusted_learning_rate=0.01,\n      lr_warmup_step=15000,\n      first_lr_drop_step=120000,\n      second_lr_drop_step=165000;\n    For batch size=256 the default values will have the following LR shedule:\n      adjusted_learning_rate=0.32,\n      lr_warmup_step=468,\n      first_lr_drop_step=3750,\n      second_lr_drop_step=5157.\n\n  For training with different schedules, such as extended schedule with double\n  number of epochs, adjust the values in default_hparams(). Note that the\n  values are w.r.t. a batch size of 64.\n\n    For batch size=64, 1x schedule (default values),\n      learning_rate=0.08,\n      lr_warmup_step=1875,\n      first_lr_drop_step=15000,\n      second_lr_drop_step=20625;\n    For batch size=64, 2x schedule, *lr_drop_epoch are doubled.\n      first_lr_drop_epoch=16.0,\n      second_lr_drop_epoch=22.0;\n    The values are converted to a LR schedule listed below:\n      adjusted_learning_rate=0.08,\n      lr_warmup_step=1875,\n      first_lr_drop_step=30000,\n      second_lr_drop_step=41250.\n\n  Args:\n    params: a parameter dictionary that includes learning_rate,\n      lr_warmup_epoch, first_lr_drop_epoch, and second_lr_drop_epoch.\n  """"""\n  # params[\'batch_size\'] is per-shard within model_fn if use_tpu=true.\n  batch_size = (params[\'batch_size\'] * params[\'num_shards\'] if params[\'use_tpu\']\n                else params[\'batch_size\'])\n  # Learning rate is proportional to the batch size\n  params[\'adjusted_learning_rate\'] = (params[\'learning_rate\'] * batch_size /\n                                      _DEFAULT_BATCH_SIZE)\n  steps_per_epoch = params[\'num_examples_per_epoch\'] / batch_size\n  params[\'lr_warmup_step\'] = int(params[\'lr_warmup_epoch\'] * steps_per_epoch)\n  params[\'first_lr_drop_step\'] = int(params[\'first_lr_drop_epoch\'] *\n                                     steps_per_epoch)\n  params[\'second_lr_drop_step\'] = int(params[\'second_lr_drop_epoch\'] *\n                                      steps_per_epoch)\n\n\ndef learning_rate_schedule(adjusted_learning_rate, lr_warmup_init,\n                           lr_warmup_step, first_lr_drop_step,\n                           second_lr_drop_step, global_step):\n  """"""Handles linear scaling rule, gradual warmup, and LR decay.""""""\n  # lr_warmup_init is the starting learning rate; the learning rate is linearly\n  # scaled up to the full learning rate after `lr_warmup_steps` before decaying.\n  linear_warmup = (lr_warmup_init +\n                   (tf.cast(global_step, dtype=tf.float32) / lr_warmup_step *\n                    (adjusted_learning_rate - lr_warmup_init)))\n  learning_rate = tf.where(global_step < lr_warmup_step,\n                           linear_warmup, adjusted_learning_rate)\n  lr_schedule = [[1.0, lr_warmup_step],\n                 [0.1, first_lr_drop_step],\n                 [0.01, second_lr_drop_step]]\n  for mult, start_global_step in lr_schedule:\n    learning_rate = tf.where(global_step < start_global_step, learning_rate,\n                             adjusted_learning_rate * mult)\n  return learning_rate\n\n\ndef focal_loss(logits, targets, alpha, gamma, normalizer):\n  """"""Compute the focal loss between `logits` and the golden `target` values.\n\n  Focal loss = -(1-pt)^gamma * log(pt)\n  where pt is the probability of being classified to the true class.\n\n  Args:\n    logits: A float32 tensor of size\n      [batch, height_in, width_in, num_predictions].\n    targets: A float32 tensor of size\n      [batch, height_in, width_in, num_predictions].\n    alpha: A float32 scalar multiplying alpha to the loss from positive examples\n      and (1-alpha) to the loss from negative examples.\n    gamma: A float32 scalar modulating loss from hard and easy examples.\n    normalizer: A float32 scalar normalizes the total loss from all examples.\n  Returns:\n    loss: A float32 scalar representing normalized total loss.\n  """"""\n  with tf.name_scope(\'focal_loss\'):\n    positive_label_mask = tf.equal(targets, 1.0)\n    cross_entropy = (\n        tf.nn.sigmoid_cross_entropy_with_logits(labels=targets, logits=logits))\n    probs = tf.sigmoid(logits)\n    probs_gt = tf.where(positive_label_mask, probs, 1.0 - probs)\n    # With small gamma, the implementation could produce NaN during back prop.\n    modulator = tf.pow(1.0 - probs_gt, gamma)\n    loss = modulator * cross_entropy\n    weighted_loss = tf.where(positive_label_mask, alpha * loss,\n                             (1.0 - alpha) * loss)\n    total_loss = tf.reduce_sum(weighted_loss)\n    total_loss /= normalizer\n  return total_loss\n\n\ndef _classification_loss(cls_outputs,\n                         cls_targets,\n                         num_positives,\n                         alpha=0.25,\n                         gamma=2.0):\n  """"""Computes classification loss.""""""\n  normalizer = num_positives\n  classification_loss = focal_loss(cls_outputs, cls_targets, alpha, gamma,\n                                   normalizer)\n  return classification_loss\n\n\ndef _box_loss(box_outputs, box_targets, num_positives, delta=0.1):\n  """"""Computes box regression loss.""""""\n  # delta is typically around the mean value of regression target.\n  # for instances, the regression targets of 512x512 input with 6 anchors on\n  # P3-P7 pyramid is about [0.1, 0.1, 0.2, 0.2].\n  normalizer = num_positives * 4.0\n  mask = tf.not_equal(box_targets, 0.0)\n  box_loss = tf.losses.huber_loss(\n      box_targets,\n      box_outputs,\n      weights=mask,\n      delta=delta,\n      reduction=tf.losses.Reduction.SUM)\n  box_loss /= normalizer\n  return box_loss\n\n\ndef detection_loss(cls_outputs, box_outputs, labels, params):\n  """"""Computes total detection loss.\n\n  Computes total detection loss including box and class loss from all levels.\n  Args:\n    cls_outputs: an OrderDict with keys representing levels and values\n      representing logits in [batch_size, height, width, num_anchors].\n    box_outputs: an OrderDict with keys representing levels and values\n      representing box regression targets in\n      [batch_size, height, width, num_anchors * 4].\n    labels: the dictionary that returned from dataloader that includes\n      groundturth targets.\n    params: the dictionary including training parameters specified in\n      default_haprams function in this file.\n  Returns:\n    total_loss: an integar tensor representing total loss reducing from\n      class and box losses from all levels.\n    cls_loss: an integar tensor representing total class loss.\n    box_loss: an integar tensor representing total box regression loss.\n  """"""\n  # Sum all positives in a batch for normalization and avoid zero\n  # num_positives_sum, which would lead to inf loss during training\n  num_positives_sum = tf.reduce_sum(labels[\'mean_num_positives\']) + 1.0\n  levels = cls_outputs.keys()\n\n  cls_losses = []\n  box_losses = []\n  for level in levels:\n    # Onehot encoding for classification labels.\n    cls_targets_at_level = tf.one_hot(\n        labels[\'cls_targets_%d\' % level],\n        params[\'num_classes\'])\n    bs, width, height, _, _ = cls_targets_at_level.get_shape().as_list()\n    cls_targets_at_level = tf.reshape(cls_targets_at_level,\n                                      [bs, width, height, -1])\n    box_targets_at_level = labels[\'box_targets_%d\' % level]\n    cls_losses.append(\n        _classification_loss(\n            cls_outputs[level],\n            cls_targets_at_level,\n            num_positives_sum,\n            alpha=params[\'alpha\'],\n            gamma=params[\'gamma\']))\n    box_losses.append(\n        _box_loss(\n            box_outputs[level],\n            box_targets_at_level,\n            num_positives_sum,\n            delta=params[\'delta\']))\n\n  # Sum per level losses to total loss.\n  cls_loss = tf.add_n(cls_losses)\n  box_loss = tf.add_n(box_losses)\n  total_loss = cls_loss + params[\'box_loss_weight\'] * box_loss\n  return total_loss, cls_loss, box_loss\n\n\ndef add_metric_fn_inputs(params, cls_outputs, box_outputs, metric_fn_inputs):\n  """"""Selects top-k predictions and adds the selected to metric_fn_inputs.\n\n  Args:\n    params: a parameter dictionary that includes `min_level`, `max_level`,\n      `batch_size`, and `num_classes`.\n    cls_outputs: an OrderDict with keys representing levels and values\n      representing logits in [batch_size, height, width, num_anchors].\n    box_outputs: an OrderDict with keys representing levels and values\n      representing box regression targets in\n      [batch_size, height, width, num_anchors * 4].\n    metric_fn_inputs: a dictionary that will hold the top-k selections.\n  """"""\n  cls_outputs_all = []\n  box_outputs_all = []\n  # Concatenates class and box of all levels into one tensor.\n  for level in range(params[\'min_level\'], params[\'max_level\'] + 1):\n    cls_outputs_all.append(tf.reshape(\n        cls_outputs[level],\n        [params[\'batch_size\'], -1, params[\'num_classes\']]))\n    box_outputs_all.append(tf.reshape(\n        box_outputs[level], [params[\'batch_size\'], -1, 4]))\n  cls_outputs_all = tf.concat(cls_outputs_all, 1)\n  box_outputs_all = tf.concat(box_outputs_all, 1)\n\n  # cls_outputs_all has a shape of [batch_size, N, num_classes] and\n  # box_outputs_all has a shape of [batch_size, N, 4]. The batch_size here\n  # is per-shard batch size. Recently, top-k on TPU supports batch\n  # dimension (b/67110441), but the following function performs top-k on\n  # each sample.\n  cls_outputs_all_after_topk = []\n  box_outputs_all_after_topk = []\n  indices_all = []\n  classes_all = []\n  for index in range(params[\'batch_size\']):\n    cls_outputs_per_sample = cls_outputs_all[index]\n    box_outputs_per_sample = box_outputs_all[index]\n    cls_outputs_per_sample_reshape = tf.reshape(cls_outputs_per_sample,\n                                                [-1])\n    _, cls_topk_indices = tf.nn.top_k(\n        cls_outputs_per_sample_reshape, k=anchors.MAX_DETECTION_POINTS)\n    # Gets top-k class and box scores.\n    indices = tf.div(cls_topk_indices, params[\'num_classes\'])\n    classes = tf.mod(cls_topk_indices, params[\'num_classes\'])\n    cls_indices = tf.stack([indices, classes], axis=1)\n    cls_outputs_after_topk = tf.gather_nd(cls_outputs_per_sample,\n                                          cls_indices)\n    cls_outputs_all_after_topk.append(cls_outputs_after_topk)\n    box_outputs_after_topk = tf.gather_nd(\n        box_outputs_per_sample, tf.expand_dims(indices, 1))\n    box_outputs_all_after_topk.append(box_outputs_after_topk)\n\n    indices_all.append(indices)\n    classes_all.append(classes)\n  # Concatenates via the batch dimension.\n  cls_outputs_all_after_topk = tf.stack(cls_outputs_all_after_topk, axis=0)\n  box_outputs_all_after_topk = tf.stack(box_outputs_all_after_topk, axis=0)\n  indices_all = tf.stack(indices_all, axis=0)\n  classes_all = tf.stack(classes_all, axis=0)\n  metric_fn_inputs[\'cls_outputs_all\'] = cls_outputs_all_after_topk\n  metric_fn_inputs[\'box_outputs_all\'] = box_outputs_all_after_topk\n  metric_fn_inputs[\'indices_all\'] = indices_all\n  metric_fn_inputs[\'classes_all\'] = classes_all\n\n\ndef coco_metric_fn(batch_size, anchor_labeler, filename=None, **kwargs):\n  """"""Evaluation metric fn. Performed on CPU, do not reference TPU ops.""""""\n  # add metrics to output\n  detections_bs = []\n  for index in range(batch_size):\n    cls_outputs_per_sample = kwargs[\'cls_outputs_all\'][index]\n    box_outputs_per_sample = kwargs[\'box_outputs_all\'][index]\n    indices_per_sample = kwargs[\'indices_all\'][index]\n    classes_per_sample = kwargs[\'classes_all\'][index]\n    detections = anchor_labeler.generate_detections(\n        cls_outputs_per_sample, box_outputs_per_sample, indices_per_sample,\n        classes_per_sample, tf.slice(kwargs[\'source_ids\'], [index], [1]),\n        tf.slice(kwargs[\'image_scales\'], [index], [1])\n    )\n    detections_bs.append(detections)\n  eval_metric = coco_metric.EvaluationMetric(filename=filename)\n  coco_metrics = eval_metric.estimator_metric_fn(detections_bs,\n                                                 kwargs[\'groundtruth_data\'])\n  return coco_metrics\n\n\ndef _model_fn(features, labels, mode, params, model, variable_filter_fn=None):\n  """"""Model defination for the RetinaNet model based on ResNet.\n\n  Args:\n    features: the input image tensor with shape [batch_size, height, width, 3].\n      The height and width are fixed and equal.\n    labels: the input labels in a dictionary. The labels include class targets\n      and box targets which are dense label maps. The labels are generated from\n      get_input_fn function in data/dataloader.py\n    mode: the mode of TPUEstimator including TRAIN, EVAL, and PREDICT.\n    params: the dictionary defines hyperparameters of model. The default\n      settings are in default_hparams function in this file.\n    model: the RetinaNet model outputs class logits and box regression outputs.\n    variable_filter_fn: the filter function that takes trainable_variables and\n      returns the variable list after applying the filter rule.\n\n  Returns:\n    tpu_spec: the TPUEstimatorSpec to run training, evaluation, or prediction.\n  """"""\n  def _model_outputs():\n    return model(\n        features,\n        min_level=params[\'min_level\'],\n        max_level=params[\'max_level\'],\n        num_classes=params[\'num_classes\'],\n        num_anchors=len(params[\'aspect_ratios\'] * params[\'num_scales\']),\n        resnet_depth=params[\'resnet_depth\'],\n        is_training_bn=params[\'is_training_bn\'])\n\n  if params[\'use_bfloat16\']:\n    with tf.contrib.tpu.bfloat16_scope():\n      cls_outputs, box_outputs = _model_outputs()\n      levels = cls_outputs.keys()\n      for level in levels:\n        cls_outputs[level] = tf.cast(cls_outputs[level], tf.float32)\n        box_outputs[level] = tf.cast(box_outputs[level], tf.float32)\n  else:\n    cls_outputs, box_outputs = _model_outputs()\n    levels = cls_outputs.keys()\n\n  # First check if it is in PREDICT mode.\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    predictions = {\n        \'image\': features,\n    }\n    for level in levels:\n      predictions[\'cls_outputs_%d\' % level] = cls_outputs[level]\n      predictions[\'box_outputs_%d\' % level] = box_outputs[level]\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n  # Load pretrained model from checkpoint.\n  if params[\'resnet_checkpoint\'] and mode == tf.estimator.ModeKeys.TRAIN:\n\n    def scaffold_fn():\n      """"""Loads pretrained model through scaffold function.""""""\n      tf.train.init_from_checkpoint(params[\'resnet_checkpoint\'], {\n          \'/\': \'resnet%s/\' % params[\'resnet_depth\'],\n      })\n      return tf.train.Scaffold()\n  else:\n    scaffold_fn = None\n\n  # Set up training loss and learning rate.\n  update_learning_rate_schedule_parameters(params)\n  global_step = tf.train.get_global_step()\n  learning_rate = learning_rate_schedule(\n      params[\'adjusted_learning_rate\'], params[\'lr_warmup_init\'],\n      params[\'lr_warmup_step\'], params[\'first_lr_drop_step\'],\n      params[\'second_lr_drop_step\'], global_step)\n  # cls_loss and box_loss are for logging. only total_loss is optimized.\n  total_loss, cls_loss, box_loss = detection_loss(cls_outputs, box_outputs,\n                                                  labels, params)\n  total_loss += _WEIGHT_DECAY * tf.add_n(\n      [tf.nn.l2_loss(v) for v in tf.trainable_variables()\n       if \'batch_normalization\' not in v.name])\n\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate, momentum=params[\'momentum\'])\n    if params[\'use_tpu\']:\n      optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n    # Batch norm requires update_ops to be added as a train_op dependency.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    var_list = variable_filter_fn(\n        tf.trainable_variables(),\n        params[\'resnet_depth\']) if variable_filter_fn else None\n\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.minimize(total_loss, global_step, var_list=var_list)\n\n  else:\n    train_op = None\n\n  eval_metrics = None\n  if mode == tf.estimator.ModeKeys.EVAL:\n    def metric_fn(**kwargs):\n      """"""Returns a dictionary that has the evaluation metrics.""""""\n      batch_size = params[\'batch_size\']\n      eval_anchors = anchors.Anchors(params[\'min_level\'],\n                                     params[\'max_level\'],\n                                     params[\'num_scales\'],\n                                     params[\'aspect_ratios\'],\n                                     params[\'anchor_scale\'],\n                                     params[\'image_size\'])\n      anchor_labeler = anchors.AnchorLabeler(eval_anchors,\n                                             params[\'num_classes\'])\n      cls_loss = tf.metrics.mean(kwargs[\'cls_loss_repeat\'])\n      box_loss = tf.metrics.mean(kwargs[\'box_loss_repeat\'])\n      coco_metrics = coco_metric_fn(batch_size, anchor_labeler,\n                                    params[\'val_json_file\'], **kwargs)\n\n      # Add metrics to output.\n      output_metrics = {\n          \'cls_loss\': cls_loss,\n          \'box_loss\': box_loss,\n      }\n      output_metrics.update(coco_metrics)\n      return output_metrics\n\n    cls_loss_repeat = tf.reshape(\n        tf.tile(tf.expand_dims(cls_loss, 0), [params[\'batch_size\'],]),\n        [params[\'batch_size\'], 1])\n    box_loss_repeat = tf.reshape(\n        tf.tile(tf.expand_dims(box_loss, 0), [params[\'batch_size\'],]),\n        [params[\'batch_size\'], 1])\n    metric_fn_inputs = {\n        \'cls_loss_repeat\': cls_loss_repeat,\n        \'box_loss_repeat\': box_loss_repeat,\n        \'source_ids\': labels[\'source_ids\'],\n        \'groundtruth_data\': labels[\'groundtruth_data\'],\n        \'image_scales\': labels[\'image_scales\'],\n    }\n    add_metric_fn_inputs(params, cls_outputs, box_outputs, metric_fn_inputs)\n    eval_metrics = (metric_fn, metric_fn_inputs)\n\n  return tf.contrib.tpu.TPUEstimatorSpec(\n      mode=mode,\n      loss=total_loss,\n      train_op=train_op,\n      eval_metrics=eval_metrics,\n      scaffold_fn=scaffold_fn)\n\n\ndef retinanet_model_fn(features, labels, mode, params):\n  """"""RetinaNet model.""""""\n  return _model_fn(\n      features,\n      labels,\n      mode,\n      params,\n      model=retinanet_architecture.retinanet,\n      variable_filter_fn=retinanet_architecture.remove_variables)\n\n\ndef default_hparams():\n  return tf.contrib.training.HParams(\n      # input preprocessing parameters\n      image_size=640,\n      input_rand_hflip=True,\n      train_scale_min=1.0,\n      train_scale_max=1.0,\n      # dataset specific parameters\n      num_classes=90,\n      skip_crowd_during_training=True,\n      # model architecture\n      min_level=3,\n      max_level=7,\n      num_scales=3,\n      aspect_ratios=[(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)],\n      anchor_scale=4.0,\n      resnet_depth=50,\n      # is batchnorm training mode\n      is_training_bn=True,\n      # optimization\n      momentum=0.9,\n      learning_rate=0.08,\n      lr_warmup_init=0.008,\n      lr_warmup_epoch=1.0,\n      first_lr_drop_epoch=8.0,\n      second_lr_drop_epoch=11.0,\n      # classification loss\n      alpha=0.25,\n      gamma=1.5,\n      # localization loss\n      delta=0.1,\n      box_loss_weight=50.0,\n      # enable bfloat\n      use_bfloat16=True,\n  )\n'"
tpu/models/official/retinanet/retinanet_segmentation_main.py,25,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Training script for RetinaNet segmentation model.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import flags\nimport absl.logging as _logging  # pylint: disable=unused-import\nimport tensorflow as tf\n\nimport dataloader\nimport retinanet_segmentation_model\n\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'tpu\', default=None,\n    help=\'The Cloud TPU to use for training. This should be either the name \'\n    \'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \'\n    \'url.\')\nflags.DEFINE_string(\n    \'gcp_project\', default=None,\n    help=\'Project name for the Cloud TPU-enabled project. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\nflags.DEFINE_string(\n    \'tpu_zone\', default=None,\n    help=\'GCE zone where the Cloud TPU is located in. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\n\n# Model specific paramenters\nflags.DEFINE_bool(\'use_tpu\', True, \'Use TPUs rather than CPUs\')\nflags.DEFINE_string(\'model_dir\', None, \'Location of model_dir\')\nflags.DEFINE_string(\'resnet_checkpoint\', None,\n                    \'Location of the ResNet50 checkpoint to use for model \'\n                    \'initialization.\')\nflags.DEFINE_string(\'hparams\', \'\',\n                    \'Comma separated k=v pairs of hyperparameters.\')\nflags.DEFINE_integer(\n    \'num_shards\', default=8, help=\'Number of shards (TPU cores)\')\nflags.DEFINE_integer(\'train_batch_size\', 64, \'training batch size\')\nflags.DEFINE_integer(\'eval_batch_size\', 8, \'evaluation batch size\')\nflags.DEFINE_integer(\'eval_samples\', 1449, \'The number of samples for \'\n                     \'evaluation.\')\nflags.DEFINE_integer(\n    \'iterations_per_loop\', 100, \'Number of iterations per TPU training loop\')\nflags.DEFINE_string(\n    \'training_file_pattern\', None,\n    \'Glob for training data files (e.g., Pascal VOC train set)\')\nflags.DEFINE_string(\n    \'validation_file_pattern\', None,\n    \'Glob for evaluation tfrecords (e.g., Pascal VOC validation set)\')\nflags.DEFINE_integer(\'num_examples_per_epoch\', 10582,\n                     \'Number of examples in one epoch\')\nflags.DEFINE_integer(\'num_epochs\', 45, \'Number of epochs for training\')\nflags.DEFINE_string(\'mode\', \'train_and_eval\',\n                    \'Mode to run: train or eval (default: train)\')\nflags.DEFINE_bool(\'eval_after_training\', False, \'Run one eval after the \'\n                  \'training finishes.\')\n\n# For Eval mode\nflags.DEFINE_integer(\'min_eval_interval\', 180,\n                     \'Minimum seconds between evaluations.\')\nflags.DEFINE_integer(\n    \'eval_timeout\', None,\n    \'Maximum seconds between checkpoints before evaluation terminates.\')\n\nFLAGS = flags.FLAGS\n\n\ndef main(argv):\n  del argv  # Unused.\n\n  if FLAGS.use_tpu:\n    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu,\n        zone=FLAGS.tpu_zone,\n        project=FLAGS.gcp_project)\n    tpu_grpc_url = tpu_cluster_resolver.get_master()\n    tf.Session.reset(tpu_grpc_url)\n\n  if FLAGS.mode in (\'train\',\n                    \'train_and_eval\') and FLAGS.training_file_pattern is None:\n    raise RuntimeError(\'You must specify --training_file_pattern for training.\')\n  if FLAGS.mode in (\'eval\', \'train_and_eval\'):\n    if FLAGS.validation_file_pattern is None:\n      raise RuntimeError(\'You must specify\'\n                         \'--validation_file_pattern for evaluation.\')\n\n  # Parse hparams\n  hparams = retinanet_segmentation_model.default_hparams()\n  hparams.parse(FLAGS.hparams)\n\n  params = dict(\n      hparams.values(),\n      num_shards=FLAGS.num_shards,\n      num_examples_per_epoch=FLAGS.num_examples_per_epoch,\n      use_tpu=FLAGS.use_tpu,\n      resnet_checkpoint=FLAGS.resnet_checkpoint,\n      mode=FLAGS.mode,\n  )\n\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      evaluation_master=\'\',\n      model_dir=FLAGS.model_dir,\n      keep_checkpoint_max=3,\n      log_step_count_steps=FLAGS.iterations_per_loop,\n      session_config=tf.ConfigProto(\n          allow_soft_placement=True, log_device_placement=False),\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          FLAGS.iterations_per_loop,\n          FLAGS.num_shards,\n          per_host_input_for_training=(\n              tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2)\n      ))\n\n  model_fn = retinanet_segmentation_model.segmentation_model_fn\n\n  # TPU Estimator\n  eval_params = dict(\n      params,\n      use_tpu=FLAGS.use_tpu,\n      input_rand_hflip=False,\n      resnet_checkpoint=None,\n      is_training_bn=False,\n  )\n  if FLAGS.mode == \'train\':\n    train_estimator = tf.contrib.tpu.TPUEstimator(\n        model_fn=model_fn,\n        use_tpu=FLAGS.use_tpu,\n        train_batch_size=FLAGS.train_batch_size,\n        config=run_config,\n        params=params)\n\n    train_estimator.train(\n        input_fn=dataloader.SegmentationInputReader(\n            FLAGS.training_file_pattern, is_training=True),\n        max_steps=int((FLAGS.num_epochs * FLAGS.num_examples_per_epoch) /\n                      FLAGS.train_batch_size),\n        )\n\n    if FLAGS.eval_after_training:\n      # Run evaluation on CPU after training finishes.\n\n      eval_estimator = tf.contrib.tpu.TPUEstimator(\n          model_fn=retinanet_segmentation_model.segmentation_model_fn,\n          use_tpu=FLAGS.use_tpu,\n          train_batch_size=FLAGS.train_batch_size,\n          eval_batch_size=FLAGS.eval_batch_size,\n          config=run_config,\n          params=eval_params)\n      eval_results = eval_estimator.evaluate(\n          input_fn=dataloader.SegmentationInputReader(\n              FLAGS.validation_file_pattern, is_training=False),\n          steps=FLAGS.eval_samples//FLAGS.eval_batch_size)\n      tf.logging.info(\'Eval results: %s\' % eval_results)\n\n  elif FLAGS.mode == \'eval\':\n\n    eval_estimator = tf.contrib.tpu.TPUEstimator(\n        model_fn=retinanet_segmentation_model.segmentation_model_fn,\n        use_tpu=FLAGS.use_tpu,\n        train_batch_size=FLAGS.train_batch_size,\n        eval_batch_size=FLAGS.eval_batch_size,\n        config=run_config,\n        params=eval_params)\n\n    def terminate_eval():\n      tf.logging.info(\'Terminating eval after %d seconds of no checkpoints\' %\n                      FLAGS.eval_timeout)\n      return True\n\n    # Run evaluation when there\'s a new checkpoint\n    for ckpt in tf.contrib.training.checkpoints_iterator(\n        FLAGS.model_dir,\n        min_interval_secs=FLAGS.min_eval_interval,\n        timeout=FLAGS.eval_timeout,\n        timeout_fn=terminate_eval):\n\n      tf.logging.info(\'Starting to evaluate.\')\n      try:\n        # Note that if the eval_samples size is not fully divided by the\n        # eval_batch_size. The remainder will be dropped and result in\n        # differet evaluation performance than validating on the full set.\n        eval_results = eval_estimator.evaluate(\n            input_fn=dataloader.SegmentationInputReader(\n                FLAGS.validation_file_pattern, is_training=False),\n            steps=FLAGS.eval_samples//FLAGS.eval_batch_size)\n        tf.logging.info(\'Eval results: %s\' % eval_results)\n\n        # Terminate eval job when final checkpoint is reached\n        current_step = int(os.path.basename(ckpt).split(\'-\')[1])\n        total_step = int((FLAGS.num_epochs * FLAGS.num_examples_per_epoch) /\n                         FLAGS.train_batch_size)\n        if current_step >= total_step:\n          tf.logging.info(\'Evaluation finished after training step %d\' %\n                          current_step)\n          break\n\n      except tf.errors.NotFoundError:\n        # Since the coordinator is on a different job than the TPU worker,\n        # sometimes the TPU worker does not finish initializing until long after\n        # the CPU job tells it to start evaluating. In this case, the checkpoint\n        # file could have been deleted already.\n        tf.logging.info(\'Checkpoint %s no longer exists, skipping checkpoint\' %\n                        ckpt)\n\n  elif FLAGS.mode == \'train_and_eval\':\n    train_estimator = tf.contrib.tpu.TPUEstimator(\n        model_fn=retinanet_segmentation_model.segmentation_model_fn,\n        use_tpu=FLAGS.use_tpu,\n        train_batch_size=FLAGS.train_batch_size,\n        config=run_config,\n        params=params)\n\n    eval_estimator = tf.contrib.tpu.TPUEstimator(\n        model_fn=retinanet_segmentation_model.segmentation_model_fn,\n        use_tpu=FLAGS.use_tpu,\n        train_batch_size=FLAGS.train_batch_size,\n        eval_batch_size=FLAGS.eval_batch_size,\n        config=run_config,\n        params=eval_params)\n    for cycle in range(0, FLAGS.num_epochs):\n      tf.logging.info(\'Starting training cycle, epoch: %d.\' % cycle)\n      train_estimator.train(\n          input_fn=dataloader.SegmentationInputReader(\n              FLAGS.training_file_pattern, is_training=True),\n          steps=int(FLAGS.num_examples_per_epoch / FLAGS.train_batch_size))\n\n      tf.logging.info(\'Starting evaluation cycle, epoch: {:d}.\'.format(\n          cycle + 1))\n      # Run evaluation after training finishes.\n      eval_results = eval_estimator.evaluate(\n          input_fn=dataloader.SegmentationInputReader(\n              FLAGS.validation_file_pattern, is_training=False),\n          steps=FLAGS.eval_samples//FLAGS.eval_batch_size)\n      tf.logging.info(\'Evaluation results: %s\' % eval_results)\n\n  else:\n    tf.logging.info(\'Mode not found.\')\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
tpu/models/official/retinanet/retinanet_segmentation_model.py,42,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Model defination for the RetinaNet Model for segmentation.\n\nDefines model_fn of RetinaNet for TF Estimator. The model_fn includes RetinaNet\nmodel architecture, loss function, learning rate schedule, and evaluation\nprocedure.\n\nT.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar\nFocal Loss for Dense Object Detection. arXiv:1708.02002\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport retinanet_architecture\nimport retinanet_model\n\n\ndef _segmentation_loss(logits, labels, params):\n  """"""Compute segmentation loss. So far it\'s only for single scale.\n\n  Args:\n    logits: A tensor specifies the logits as returned from model function.\n      The tensor size is [batch_size, height_l, width_l, num_classes].\n      The height_l and width_l depends on the min_level feature resolution.\n    labels: A tensor specifies the groundtruth targets ""cls_targets"",\n      as returned from dataloader. The tensor has the same spatial resolution\n      as input image with size [batch_size, height, width, 1].\n    params: Dictionary including training parameters specified in\n      default_hparams function in this file.\n  Returns:\n    A float tensor representing total classification loss. The loss is\n      normalized by the total non-ignored pixels.\n  """"""\n  # Downsample labels by the min_level feature stride.\n  stride = 2**params[\'min_level\']\n  scaled_labels = labels[:, 0::stride, 0::stride]\n\n  scaled_labels = tf.cast(scaled_labels, tf.int32)\n  scaled_labels = scaled_labels[:, :, :, 0]\n  bit_mask = tf.not_equal(scaled_labels, params[\'ignore_label\'])\n  # Assign ignore label to background to avoid error when computing\n  # Cross entropy loss.\n  scaled_labels = tf.where(bit_mask, scaled_labels,\n                           tf.zeros_like(scaled_labels))\n\n  normalizer = tf.reduce_sum(tf.to_float(bit_mask))\n  cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      labels=scaled_labels, logits=logits)\n  cross_entropy_loss *= tf.to_float(bit_mask)\n  loss = tf.reduce_sum(cross_entropy_loss) / normalizer\n  return loss\n\n\ndef _model_fn(features, labels, mode, params, model, variable_filter_fn=None):\n  """"""Model defination for the RetinaNet model based on ResNet-50.\n\n  Args:\n    features: The input images tensor with shape [batch_size, height, width, 3].\n      The height and width are fixed and equal.\n    labels: The input labels in a tensor with the same shape as input images.\n    mode: The mode of TPUEstimator including TRAIN, EVAL, and PREDICT.\n    params: The dictionary defines hyperparameters of model. The default\n      settings are in default_hparams function in this file.\n    model: The FPN segmentation model outputs class logits.\n    variable_filter_fn: the filter function that takes trainable_variables and\n      returns the variable list after applying the filter rule.\n\n  Returns:\n    tpu_spec: the TPUEstimatorSpec to run training, evaluation, or prediction.\n  """"""\n  def _model_outputs():\n    return model(\n        features,\n        min_level=params[\'min_level\'],\n        max_level=params[\'max_level\'],\n        num_classes=params[\'num_classes\'],\n        resnet_depth=params[\'resnet_depth\'],\n        is_training_bn=params[\'is_training_bn\'])\n\n  if params[\'use_bfloat16\']:\n    with tf.contrib.tpu.bfloat16_scope():\n      cls_outputs = _model_outputs()\n      cls_outputs = tf.cast(cls_outputs, tf.float32)\n  else:\n    cls_outputs = _model_outputs()\n\n  # First check if it is in PREDICT mode.\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    predictions = {\n        \'image\': features,\n        \'cls_outputs\': cls_outputs\n    }\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n  # Load pretrained model from checkpoint.\n  if params[\'resnet_checkpoint\'] and mode == tf.estimator.ModeKeys.TRAIN:\n\n    def scaffold_fn():\n      """"""Loads pretrained model through scaffold function.""""""\n      tf.train.init_from_checkpoint(params[\'resnet_checkpoint\'], {\n          \'/\': \'resnet%s/\' % params[\'resnet_depth\'],\n      })\n      return tf.train.Scaffold()\n  else:\n    scaffold_fn = None\n\n  # Set up training loss and learning rate.\n  retinanet_model.update_learning_rate_schedule_parameters(params)\n  global_step = tf.train.get_global_step()\n  learning_rate = retinanet_model.learning_rate_schedule(\n      params[\'adjusted_learning_rate\'], params[\'lr_warmup_init\'],\n      params[\'lr_warmup_step\'], params[\'first_lr_drop_step\'],\n      params[\'second_lr_drop_step\'], global_step)\n\n  cls_loss = _segmentation_loss(cls_outputs, labels, params)\n  weight_decay_loss = params[\'weight_decay\'] * tf.add_n(\n      [tf.nn.l2_loss(v) for v in tf.trainable_variables()\n       if \'batch_normalization\' not in v.name])\n  # Add L2 regularization loss\n  total_loss = cls_loss + weight_decay_loss\n\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate, momentum=params[\'momentum\'])\n    if params[\'use_tpu\']:\n      optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n    # Batch norm requires update_ops to be added as a train_op dependency.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    var_list = variable_filter_fn(\n        tf.trainable_variables(),\n        params[\'resnet_depth\']) if variable_filter_fn else None\n\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.minimize(total_loss, global_step,\n                                    var_list=var_list)\n  else:\n    train_op = None\n\n  # Evaluation only works on GPU/CPU host and batch_size=1\n  eval_metrics = None\n  if mode == tf.estimator.ModeKeys.EVAL:\n    batch_size = params[\'batch_size\']\n\n    def metric_fn(**kwargs):\n      """"""Creates metric_fn for TPUEstimatorSpec.""""""\n      cls_loss = tf.metrics.mean(kwargs[\'cls_loss_repeat\'])\n      total_loss = tf.metrics.mean(kwargs[\'total_loss_repeat\'])\n      logits = tf.image.resize_bilinear(kwargs[\'prediction\'],\n                                        tf.shape(kwargs[\'labels\'])[1:3],\n                                        align_corners=True)\n      predictions_with_shape = tf.argmax(logits, 3, output_type=tf.int32)\n      predictions = tf.reshape(predictions_with_shape, shape=[-1])\n\n      labels = tf.reshape(kwargs[\'labels\'], shape=[-1])\n      # Background class is considered as a class. Not ignored.\n      weights = tf.to_float(tf.not_equal(labels, params[\'ignore_label\']))\n\n      # Set ignore_label regions to label 0, because metrics.mean_iou requires\n      # range of labels = [0, dataset.num_classes).\n      # Note the ignore_lable regions are not evaluated since the corresponding\n      # regions contain weights = 0.\n      labels = tf.where(tf.equal(labels,\n                                 params[\'ignore_label\']),\n                        tf.zeros_like(labels),\n                        labels)\n\n      return {\n          \'total_loss\': total_loss,\n          \'cls_loss\': cls_loss,\n          \'miou\':\n              tf.metrics.mean_iou(\n                  predictions, labels, params[\'num_classes\'], weights=weights),\n      }\n\n    cls_loss_repeat = tf.reshape(\n        tf.tile(tf.expand_dims(cls_loss, 0), [\n            batch_size,\n        ]), [batch_size, 1])\n\n    total_loss_repeat = tf.reshape(\n        tf.tile(tf.expand_dims(total_loss, 0), [\n            batch_size,\n        ]), [batch_size, 1])\n\n    metric_fn_inputs = {\n        \'cls_loss_repeat\': cls_loss_repeat,\n        \'total_loss_repeat\': total_loss_repeat,\n        \'prediction\': cls_outputs,\n        \'labels\': labels,\n    }\n\n    eval_metrics = (metric_fn, metric_fn_inputs)\n\n  return tf.contrib.tpu.TPUEstimatorSpec(\n      mode=mode,\n      loss=total_loss,\n      train_op=train_op,\n      eval_metrics=eval_metrics,\n      scaffold_fn=scaffold_fn,\n      )\n\n\ndef segmentation_model_fn(features, labels, mode, params):\n  """"""RetinaNet model.""""""\n  return _model_fn(features, labels, mode, params,\n                   model=retinanet_architecture.retinanet_segmentation,\n                   variable_filter_fn=retinanet_architecture.remove_variables)\n\n\ndef default_hparams():\n  return tf.contrib.training.HParams(\n      image_size=513,\n      input_rand_hflip=True,\n      # dataset specific parameters\n      num_classes=21,\n      # model architecture\n      min_level=3,\n      max_level=5,\n      resnet_depth=101,\n      # is batchnorm training mode\n      is_training_bn=True,\n      # optimization\n      momentum=0.9,\n      learning_rate=0.02,\n      lr_warmup_init=0.002,\n      lr_warmup_epoch=1.0,\n      first_lr_drop_epoch=25.,\n      second_lr_drop_epoch=35.,\n      weight_decay=0.00001,\n      # classification loss\n      ignore_label=255,\n      loss_weight=1.0,\n      # resnet checkpoint\n      resnet_checkpoint=None,\n      train_scale_min=0.75,\n      train_scale_max=1.5,\n      # enable mixed-precision training (using bfloat16 on TPU)\n      use_bfloat16=True,\n  )\n'"
tpu/models/official/squeezenet/data_pipeline.py,82,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Imagenet loading and preprocessing pipeline.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\n_FILE_PATTERN = ""%s-*""\n_NUM_CLASSES = 1001\n_NUM_CHANNELS = 3\n\n_SPLITS = set([""train"", ""validation""])\n\n_R_MEAN = 123.68 / 255\n_G_MEAN = 116.78 / 255\n_B_MEAN = 103.94 / 255\n\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n  """"""Crops the given image using the provided offsets and sizes.\n\n  Note that the method doesn""t assume we know the input image size but it does\n  assume we know the input image rank.\n\n  Args:\n    image: an image of shape [height, width, channels].\n    offset_height: a scalar tensor indicating the height offset.\n    offset_width: a scalar tensor indicating the width offset.\n    crop_height: the height of the cropped image.\n    crop_width: the width of the cropped image.\n\n  Returns:\n    the cropped (and resized) image.\n\n  Raises:\n    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n      less than the crop size.\n  """"""\n  original_shape = tf.shape(image)\n\n  rank_assertion = tf.Assert(\n      tf.equal(tf.rank(image), 3), [""Rank of image must be equal to 3.""])\n  with tf.control_dependencies([rank_assertion]):\n    cropped_shape = tf.stack([crop_height, crop_width, original_shape[2]])\n\n  size_assertion = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(original_shape[0], crop_height),\n          tf.greater_equal(original_shape[1], crop_width)),\n      [""Crop size greater than the image size.""])\n\n  offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))\n\n  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n  # define the crop size.\n  with tf.control_dependencies([size_assertion]):\n    image = tf.slice(image, offsets, cropped_shape)\n  return tf.reshape(image, cropped_shape)\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n  """"""Crops the given list of images.\n\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n\n  Returns:\n    the image_list with cropped images.\n\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  """"""\n  if not image_list:\n    raise ValueError(""Empty image_list."")\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n        tf.equal(image_rank, 3), [\n            ""Wrong rank for tensor  %s [expected] [actual]"", image_list[i].name,\n            3, image_rank\n        ])\n    rank_assertions.append(rank_assert)\n\n  with tf.control_dependencies([rank_assertions[0]]):\n    image_shape = tf.shape(image_list[0])\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(image_height, crop_height),\n          tf.greater_equal(image_width, crop_width)),\n      [""Crop size greater than the image size.""])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    with tf.control_dependencies([rank_assertions[i]]):\n      shape = tf.shape(image)\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n        tf.equal(height, image_height), [\n            ""Wrong height for tensor %s [expected][actual]"", image.name, height,\n            image_height\n        ])\n    width_assert = tf.Assert(\n        tf.equal(width, image_width), [\n            ""Wrong width for tensor %s [expected][actual]"", image.name, width,\n            image_width\n        ])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  with tf.control_dependencies(asserts):\n    max_offset_height = tf.reshape(image_height - crop_height + 1, [])\n  with tf.control_dependencies(asserts):\n    max_offset_width = tf.reshape(image_width - crop_width + 1, [])\n  offset_height = tf.random_uniform(\n      [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random_uniform([], maxval=max_offset_width, dtype=tf.int32)\n\n  return [\n      _crop(image, offset_height, offset_width, crop_height, crop_width)\n      for image in image_list\n  ]\n\n\ndef _central_crop(image_list, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    the list of cropped images.\n  """"""\n  outputs = []\n  for image in image_list:\n    image_height = tf.shape(image)[0]\n    image_width = tf.shape(image)[1]\n\n    offset_height = (image_height - crop_height) / 2\n    offset_width = (image_width - crop_width) / 2\n\n    outputs.append(\n        _crop(image, offset_height, offset_width, crop_height, crop_width))\n  return outputs\n\n\ndef _mean_image_subtraction(image, means):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn""t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(""Input must be of size [height, width, C>0]"")\n  num_channels = image.get_shape().as_list()[-1]\n  if len(means) != num_channels:\n    raise ValueError(""len(means) must match the number of channels"")\n\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] -= means[i]\n  return tf.concat(axis=2, values=channels)\n\n\ndef _smallest_size_at_least(height, width, smallest_side):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: and int32 scalar tensor indicating the new width.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  height = tf.to_float(height)\n  width = tf.to_float(width)\n  smallest_side = tf.to_float(smallest_side)\n\n  scale = tf.cond(\n      tf.greater(height, width), lambda: smallest_side / width,\n      lambda: smallest_side / height)\n  new_height = tf.to_int32(height * scale)\n  new_width = tf.to_int32(width * scale)\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, smallest_side):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  shape = tf.shape(image)\n  height = shape[0]\n  width = shape[1]\n  new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n  image = tf.expand_dims(image, 0)\n  resized_image = tf.image.resize_bilinear(\n      image, [new_height, new_width], align_corners=False)\n  resized_image = tf.squeeze(resized_image)\n  resized_image.set_shape([None, None, 3])\n  return resized_image\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         resize_side_min=_RESIZE_SIDE_MIN,\n                         resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing.\n  Returns:\n    A preprocessed image.\n  """"""\n  resize_side = tf.random_uniform(\n      [], minval=resize_side_min, maxval=resize_side_max + 1, dtype=tf.int32)\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _random_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  image = tf.image.random_flip_left_right(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_for_eval(image, output_height, output_width, resize_side):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side: The smallest side of the image for aspect-preserving resizing.\n  Returns:\n    A preprocessed image.\n  """"""\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _central_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_image(image,\n                     output_height,\n                     output_width,\n                     is_training=False,\n                     resize_side_min=_RESIZE_SIDE_MIN,\n                     resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, then this value\n      is used for rescaling.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, this value is\n      ignored. Otherwise, the resize side is sampled from\n        [resize_size_min, resize_size_max].\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    image = preprocess_for_train(image, output_height, output_width,\n                                 resize_side_min, resize_side_max)\n  else:\n    image = preprocess_for_eval(image, output_height, output_width,\n                                resize_side_min)\n  image = tf.subtract(image, 0.5)\n  image = tf.multiply(image, 2.0)\n  return image\n\n\ndef get_split(split_name, dataset_dir):\n  """"""Gets a dataset tuple with instructions for reading ImageNet.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in _SPLITS:\n    raise ValueError(""split name %s was not recognized."" % split_name)\n\n  file_pattern = os.path.join(dataset_dir, _FILE_PATTERN % split_name)\n  return tf.data.Dataset.list_files(file_pattern, shuffle=False)\n\n\nclass InputReader(object):\n  """"""Provides TFEstimator input function for imagenet, with preprocessing.""""""\n\n  def __init__(self, data_dir, is_training, image_width=224, image_height=224):\n    self._is_training = is_training\n    self._data_dir = data_dir\n    self._image_width = image_width\n    self._image_height = image_height\n\n  def _parse_record(self, record):\n    """"""Parse an Imagenet record from a tf.Example.""""""\n    keys_to_features = {\n        ""image/encoded"": tf.FixedLenFeature((), tf.string, """"),\n        ""image/format"": tf.FixedLenFeature((), tf.string, ""jpeg""),\n        ""image/class/label"": tf.FixedLenFeature([], tf.int64, -1),\n        ""image/class/text"": tf.FixedLenFeature([], tf.string, """"),\n        ""image/object/bbox/xmin"": tf.VarLenFeature(dtype=tf.float32),\n        ""image/object/bbox/ymin"": tf.VarLenFeature(dtype=tf.float32),\n        ""image/object/bbox/xmax"": tf.VarLenFeature(dtype=tf.float32),\n        ""image/object/bbox/ymax"": tf.VarLenFeature(dtype=tf.float32),\n        ""image/object/class/label"": tf.VarLenFeature(dtype=tf.int64),\n    }\n\n    parsed = tf.parse_single_example(record, keys_to_features)\n\n    image = tf.image.decode_image(\n        tf.reshape(parsed[""image/encoded""], shape=[]), _NUM_CHANNELS)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    image = preprocess_image(\n        image=image,\n        output_height=self._image_width,\n        output_width=self._image_height,\n        is_training=self._is_training)\n\n    label = tf.cast(\n        tf.reshape(parsed[""image/class/label""], shape=[]), dtype=tf.int32)\n\n    return image, label\n\n  def __call__(self, params):\n    bs = params[""batch_size""]\n\n    dataset = get_split(\n        ""train"" if self._is_training else ""validation"",\n        dataset_dir=self._data_dir)\n\n    if self._is_training:\n      dataset = dataset.shuffle(buffer_size=1024)\n      dataset = dataset.repeat()\n\n    def _load_records(filename):\n      return tf.data.TFRecordDataset(filename, buffer_size=32 * 1000 * 1000)\n\n    dataset = dataset.apply(\n        tf.contrib.data.parallel_interleave(\n            _load_records, sloppy=True, cycle_length=64))\n\n    dataset = dataset.prefetch(bs * 4)\n    dataset = dataset.map(self._parse_record, num_parallel_calls=32)\n    dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(bs))\n    dataset = dataset.prefetch(4)\n\n    features, labels = dataset.make_one_shot_iterator().get_next()\n    labels = tf.cast(labels, tf.int32)\n    features.set_shape([bs, 224, 224, 3])\n    labels.set_shape([bs])\n    return features, labels\n'"
tpu/models/official/squeezenet/squeezenet_main.py,9,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""SqueezeNet implementation with TPU support.\n\nTraining loop and input pipeline.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport absl.logging as _logging  # pylint: disable=unused-import\nimport tensorflow as tf\n\nimport data_pipeline\nimport squeezenet_model\n\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'tpu\', default=None,\n    help=\'The Cloud TPU to use for training. This should be either the name \'\n    \'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\')\nflags.DEFINE_string(\n    ""gcp_project"", default=None,\n    help=""Project name for the Cloud TPU-enabled project. If not specified, we ""\n    ""will attempt to automatically detect the GCE project from metadata."")\nflags.DEFINE_string(\n    ""tpu_zone"", default=None,\n    help=""GCE zone where the Cloud TPU is located in. If not specified, we ""\n    ""will attempt to automatically detect the GCE project from metadata."")\n\n# Model specific paramenters\nflags.DEFINE_string(""data_dir"", """", ""Location of training files."")\nflags.DEFINE_string(""model_dir"", """", ""Where to store model checkpoints."")\nflags.DEFINE_integer(""save_checkpoints_secs"", 3600,\n                     ""Interval between saving model checkpoints."")\nflags.DEFINE_integer(""num_shards"", 8, ""Number of TPU shards."")\nflags.DEFINE_integer(""batch_size"", 1024, ""Batch size for training and eval."")\nflags.DEFINE_boolean(""use_tpu"", True, ""If true, use TPU device."")\n\nflags.DEFINE_string(""optimizer"", ""momentum"", ""Optimizer: momentum|adam|rmsprop"")\nflags.DEFINE_float(""momentum"", 0.9, ""Momentum parameter for SGD optimizer."")\nflags.DEFINE_integer(""num_epochs"", 150,\n                     ""Number of epochs of the training set to process."")\nflags.DEFINE_integer(""num_evals"", 10,\n                     ""How many times to run an evaluation during training."")\nflags.DEFINE_float(""learning_rate"", 0.03, ""Learning rate."")\n\nFLAGS = flags.FLAGS\n\n\ndef main(argv):\n  del argv\n\n  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n      FLAGS.tpu,\n      zone=FLAGS.tpu_zone,\n      project=FLAGS.gcp_project)\n\n  training_examples = 1300 * 1000 * FLAGS.num_epochs\n  eval_examples = 50 * 1000\n\n  params = {\n      ""num_classes"": 1001,\n      ""lr"": FLAGS.learning_rate,\n      ""min_lr"": 0.005,\n      ""momentum"": FLAGS.momentum,\n      ""optimizer"": FLAGS.optimizer,\n      ""num_eval_examples"": eval_examples,\n      ""num_shards"": FLAGS.num_shards,\n      ""num_epochs"": FLAGS.num_epochs,\n  }\n\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_secs=FLAGS.save_checkpoints_secs,\n      session_config=tf.ConfigProto(\n          allow_soft_placement=True, log_device_placement=False),\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=100,\n          num_shards=FLAGS.num_shards,\n      ),\n  )\n\n  estimator = tf.contrib.tpu.TPUEstimator(\n      model_fn=squeezenet_model.model_fn,\n      use_tpu=FLAGS.use_tpu,\n      config=run_config,\n      train_batch_size=FLAGS.batch_size,\n      eval_batch_size=FLAGS.batch_size,\n      params=dict(params, use_tpu=FLAGS.use_tpu),\n  )\n\n  num_evals = max(FLAGS.num_evals, 1)\n  examples_per_eval = training_examples // num_evals\n  for _ in range(num_evals):\n    estimator.train(\n        input_fn=data_pipeline.InputReader(FLAGS.data_dir, is_training=True),\n        steps=examples_per_eval // FLAGS.batch_size)\n\n    tf.logging.info(""Running evaluation"")\n    tf.logging.info(""%s"",\n                    estimator.evaluate(\n                        input_fn=data_pipeline.InputReader(\n                            FLAGS.data_dir, is_training=False),\n                        steps=eval_examples // FLAGS.batch_size,\n                    ))\n\n\nif __name__ == ""__main__"":\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
tpu/models/official/squeezenet/squeezenet_model.py,34,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""SqueezeNet implementation with TPU support.\n\nThis version does not contain the model compression components (\nsparsification and quantization).\n\nOriginal paper: (https://arxiv.org/pdf/1602.07360.pdf)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\n\ndef conv2d(inputs,\n           filters,\n           kernel_size,\n           strides=(1, 1),\n           kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n           bias_initializer=tf.zeros_initializer(),\n           kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0002),\n           name=None):\n  return tf.layers.conv2d(\n      inputs,\n      filters,\n      kernel_size,\n      strides,\n      kernel_initializer=kernel_initializer,\n      bias_initializer=bias_initializer,\n      kernel_regularizer=kernel_regularizer,\n      activation=tf.nn.relu,\n      name=name,\n      padding=""same"")\n\n\ndef fire_module(inputs, squeeze_depth, expand_depth, name):\n  """"""Fire module: squeeze input filters, then apply spatial convolutions.""""""\n  with tf.variable_scope(name, ""fire"", [inputs]):\n    squeezed = conv2d(inputs, squeeze_depth, [1, 1], name=""squeeze"")\n    e1x1 = conv2d(squeezed, expand_depth, [1, 1], name=""e1x1"")\n    e3x3 = conv2d(squeezed, expand_depth, [3, 3], name=""e3x3"")\n    return tf.concat([e1x1, e3x3], axis=3)\n\n\ndef squeezenet(images, is_training=True, num_classes=1001):\n  """"""Squeezenet 1.0 model.""""""\n  net = conv2d(images, 96, [7, 7], strides=(2, 2), name=""conv1"")\n  net = tf.layers.max_pooling2d(net, [3, 3], strides=(2, 2), name=""maxpool1"")\n  net = fire_module(net, 16, 64, name=""fire2"")\n  net = fire_module(net, 16, 64, name=""fire3"")\n  net = fire_module(net, 32, 128, name=""fire4"")\n  net = tf.layers.max_pooling2d(net, [3, 3], strides=(2, 2), name=""maxpool4"")\n  net = fire_module(net, 32, 128, name=""fire5"")\n  net = fire_module(net, 48, 192, name=""fire6"")\n  net = fire_module(net, 48, 192, name=""fire7"")\n  net = fire_module(net, 64, 256, name=""fire8"")\n  net = tf.layers.max_pooling2d(net, [3, 3], strides=(2, 2), name=""maxpool8"")\n  net = fire_module(net, 64, 256, name=""fire9"")\n  net = tf.layers.dropout(net, rate=0.5 if is_training else 0.0, name=""drop9"")\n  net = conv2d(net, num_classes, [1, 1], strides=(1, 1), name=""conv10"")\n  net = tf.layers.average_pooling2d(net, pool_size=(13, 13), strides=(1, 1))\n  logits = tf.layers.flatten(net)\n  return logits\n\n\ndef metric_fn(labels, logits, learning_rate):\n  predictions = tf.cast(tf.argmax(logits, 1), tf.int32)\n  labels = tf.cast(labels, tf.int64)\n  return {\n      ""accuracy"": tf.metrics.accuracy(labels, predictions),\n      ""recall_at_5"": tf.metrics.recall_at_k(labels, logits, 5),\n      ""recall_at_1"": tf.metrics.recall_at_k(labels, logits, 1),\n      ""learning_rate"": tf.metrics.mean(learning_rate),\n  }\n\n\ndef model_fn(features, labels, mode, params):\n  """"""TPUEstimatorSpec for the Squeezenet model.""""""\n  is_training = mode == tf.estimator.ModeKeys.TRAIN\n  logits = squeezenet(\n      features, is_training=is_training, num_classes=params[""num_classes""])\n\n  loss = tf.reduce_mean(\n      tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels))\n\n  global_batch_size = params[""num_shards""] * params[""batch_size""]\n  decay_steps = 1300 * 1000 * params[""num_epochs""] // global_batch_size\n  learning_rate = tf.train.polynomial_decay(\n      params[""lr""],\n      global_step=tf.train.get_or_create_global_step(),\n      end_learning_rate=params[""min_lr""],\n      decay_steps=decay_steps,\n      power=1.0,\n      cycle=False)\n\n  # TODO(power): Hack copied from resnet: remove when summaries are working.\n  lr_repeat = tf.reshape(\n      tf.tile(tf.expand_dims(learning_rate, 0), [params[""batch_size""],]),\n      [params[""batch_size""], 1])\n\n  if params[""optimizer""] == ""adam"":\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n  elif params[""optimizer""] == ""rmsprop"":\n    optimizer = tf.train.RMSPropOptimizer(\n        learning_rate=learning_rate,\n        momentum=params[""momentum""],\n        epsilon=1.0\n    )\n  else:\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate=learning_rate,\n        momentum=params[""momentum""],\n        use_nesterov=True)\n\n  if params[""use_tpu""]:\n    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n  train_op = optimizer.minimize(loss, tf.train.get_global_step())\n\n  return tf.contrib.tpu.TPUEstimatorSpec(\n      mode=mode,\n      loss=loss,\n      train_op=train_op,\n      eval_metrics=(metric_fn, [labels, logits, lr_repeat]),\n      predictions={\n          ""classes"": tf.argmax(input=logits, axis=1),\n          ""probabilities"": tf.nn.softmax(logits, name=""softmax_tensor"")\n      },\n  )\n'"
tpu/models/experimental/keras/colab/shakespeare_lstm.py,18,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Colab Example for Shakespeare LSTM example.\n\nTo test on TPU:\n    python shapespear_lstm.py --use_tpu=True [--tpu=$TPU_NAME]\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nfrom absl import flags\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nflags.DEFINE_bool(\'use_tpu\', True, \'Use TPU model instead of CPU.\')\nflags.DEFINE_string(\'tpu\', None, \'Name of the TPU to use.\')\n\nFLAGS = flags.FLAGS\n\n\n# The data can be obtained from http://www.gutenberg.org/files/100/100-0.txt\nSHAKESPEARE_TXT = \'gs://cloud-tpu-artifacts/shakespeare/shakespeare.txt\'\n\nWEIGHTS_TXT = \'/tmp/bard.h5\'\n\nEMBEDDING_DIM = 512\n\n\ndef transform(txt, pad_to=None):\n  """"""Transforms the input `txt` to model sequence data.""""""\n  # drop any non-ascii characters\n  output = np.asarray([ord(c) for c in txt if ord(c) < 255],\n                      dtype=np.int32)\n  if pad_to is not None:\n    output = output[:pad_to]\n    output = np.concatenate([\n        np.zeros([pad_to - len(txt)], dtype=np.int32),\n        output,\n    ])\n  return output\n\n\ndef training_generator(data, seq_len=100, batch_size=1024):\n  """"""A generator yields (seq, target) arrays for training.""""""\n  while True:\n    offsets = np.random.randint(0, len(data) - seq_len, batch_size)\n\n    # Our model uses sparse crossentropy loss, but Keras requires labels\n    # to have the same rank as the input logits.  We add an empty final\n    # dimension to account for this.\n    yield (\n        np.stack([data[idx:idx + seq_len] for idx in offsets]),\n        np.expand_dims(\n            np.stack([data[idx + 1:idx + seq_len + 1] for idx in offsets]),\n            -1),\n    )\n\n\ndef lstm_model(seq_len=100, batch_size=None, stateful=True):\n  """"""Language model: predict the next char given the current sequence.""""""\n  source = tf.keras.Input(\n      name=\'seed\', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n\n  embedding = tf.keras.layers.Embedding(\n      input_dim=256, output_dim=EMBEDDING_DIM)(source)\n  lstm_1 = tf.keras.layers.LSTM(\n      EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n  lstm_2 = tf.keras.layers.LSTM(\n      EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n  predicted_char = tf.keras.layers.TimeDistributed(\n      tf.keras.layers.Dense(256, activation=\'softmax\'))(lstm_2)\n\n  model = tf.keras.Model(\n      inputs=[source], outputs=[predicted_char],\n  )\n  model.compile(\n      optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01),\n      loss=\'sparse_categorical_crossentropy\',\n      metrics=[\'sparse_categorical_accuracy\'])\n  return model\n\n\ndef main(unused_dev):\n  with tf.gfile.GFile(SHAKESPEARE_TXT, \'r\') as f:\n    txt = f.read()\n\n  print(\'Input text [{}]: {}\'.format(len(txt), txt[:50]))\n  data = transform(txt)\n\n  seq_len = 10\n  x, y = six.next(training_generator(data, seq_len=seq_len, batch_size=1))\n  print(\'Random sample of the data (seq_len={}):\'.format(seq_len))\n  print(\'  x:\', x)\n  print(\'  y:\', y)\n\n  seq_len = 100\n  training_model = lstm_model(seq_len=seq_len, batch_size=None, stateful=False)\n\n  print()\n  print(\'Model Summary\')\n  training_model.summary()\n\n  if FLAGS.use_tpu:\n    strategy = tf.contrib.tpu.TPUDistributionStrategy(\n        tf.contrib.cluster_resolver.TPUClusterResolver(tpu=flags.FLAGS.tpu)\n    )\n    training_model = tf.contrib.tpu.keras_to_tpu_model(\n        training_model, strategy=strategy)\n\n  print(\'Training on\', \'TPU\' if FLAGS.use_tpu else \'CPU\')\n  training_model.fit_generator(\n      training_generator(data, seq_len=seq_len, batch_size=1024),\n      steps_per_epoch=100,\n      epochs=10,\n  )\n  training_model.save_weights(WEIGHTS_TXT, overwrite=True)\n\n  print(\'Running inference on the CPU.\')\n  batch_size = 5\n  predict_len = 500\n\n  # We seed the model with our initial string, copied batch_size times\n  seed_txt = \'Looks it not like the king?  Verily, we must go! \'\n  print(\'Seed:\', seed_txt)\n\n  seed = transform(seed_txt)\n  seed = np.repeat(np.expand_dims(seed, 0), batch_size, axis=0)\n\n  # Keras requires the batch size be specified ahead of time for stateful\n  # models.  We use a sequence length of 1, as we will be feeding in one\n  # character at a time and predicting the next character.\n  prediction_model = lstm_model(seq_len=1, batch_size=batch_size, stateful=True)\n  prediction_model.load_weights(WEIGHTS_TXT)\n  if FLAGS.use_tpu:\n    strategy = tf.contrib.tpu.TPUDistributionStrategy(\n        tf.contrib.cluster_resolver.TPUClusterResolver(tpu=flags.FLAGS.tpu))\n    prediction_model = tf.contrib.tpu.keras_to_tpu_model(\n        prediction_model, strategy=strategy)\n\n  # First, run the seed forward to prime the state of the model.\n  prediction_model.reset_states()\n  for i in range(len(seed_txt) - 1):\n    prediction_model.predict(seed[:, i:i + 1])\n\n  # Now we can accumulate predictions!\n  predictions = [seed[:, -1:]]\n  for i in range(predict_len):\n    last_word = predictions[-1]\n    next_probits = prediction_model.predict(last_word)[:, 0, :]\n\n    # sample from our output distribution\n    next_idx = [\n        np.random.choice(256, p=next_probits[i])\n        for i in range(batch_size)\n    ]\n    predictions.append(np.asarray(next_idx, dtype=np.int32))\n\n  for i in range(batch_size):\n    print(\'\\nPREDICTION %d\\n\\n\' % i)\n    p = [predictions[j][i] for j in range(predict_len)]\n    print(\'\'.join([chr(c) for c in p]))\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/models/official/resnet/benchmark/__init__.py,0,b''
tpu/models/official/resnet/benchmark/read_training_time.py,5,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Reads Training Start/End Time from Events File.""""""\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\n\nfrom absl import flags\n\nimport tensorflow as tf\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    \'model_dir\', default=None,\n    help=(\'The directory where the model and training/evaluation summaries are\'\n          \' stored.\'))\n\nflags.DEFINE_string(\n    \'event_name\', default=\'loss\',\n    help=(\'Name of event to track.\'))\n\nflags.DEFINE_integer(\n    \'warmup_steps\', default=None, help=\'Number of warmup steps taken.\')\n\nflags.DEFINE_integer(\n    \'end_step\',\n    default=None,\n    help=\'If set stops counting inclusive of end_step, else to the end.\')\n\nflags.DEFINE_boolean(\n    \'tpu\', default=False, help=\'Read TPU event file.\')\n\n\ndef main(unused_argv):\n  if not FLAGS.model_dir:\n    raise ValueError(\'--model_dir must be specified.\')\n\n  if not FLAGS.warmup_steps:\n    raise ValueError(\'--warmup_steps must be non-zero.\')\n\n  target_step = FLAGS.warmup_steps\n  current_step = 0\n  start_time = 0\n  max_wall_time = 0.0\n\n  if FLAGS.tpu:\n    event_file = tf.gfile.Glob(FLAGS.model_dir + \'events.out.tfevents.*.n-*\')[0]\n  else:\n    event_file = tf.gfile.Glob(FLAGS.model_dir + \'events.out.tfevents.*\')[0]\n\n  for e in tf.train.summary_iterator(event_file):\n    current_step = e.step\n    for v in e.summary.value:\n      if v.tag == FLAGS.event_name:\n        if current_step == target_step:\n          start_time = e.wall_time\n          print(\'training start (step %d): %s\' %\n                (current_step, datetime.datetime.fromtimestamp(\n                    e.wall_time).strftime(\'%Y-%m-%d %H:%M:%S.%f\')))\n        max_wall_time = max(e.wall_time, max_wall_time)\n    if FLAGS.end_step and e.step >= FLAGS.end_step:\n      break\n\n  if not start_time:\n    raise Exception(\'Error: Starting event not found. Check arg event_name and \'\n                    \'warmup_steps. Possible no events were found.\')\n\n  if FLAGS.end_step and current_step < FLAGS.end_step:\n    raise Exception(\'Error: Final step was less than the requested end_step.\')\n\n  elapse_time = max_wall_time - start_time\n  print(\'training end (step %d): %s\' %\n        (current_step, datetime.datetime.fromtimestamp(\n            max_wall_time).strftime(\'%Y-%m-%d %H:%M:%S.%f\')))\n  print(\'elapsed time:{}m\'.format(elapse_time / 60))\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/models/official/resnet/benchmark/resnet_benchmark.py,19,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Train a ResNet-50 model on ImageNet on TPU.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport csv\nimport os\nimport re\nimport sys\nimport time\n\nfrom absl import flags\n\nimport tensorflow as tf\n\n# For Cloud environment, add parent directory for imports\nsys.path.append(os.path.dirname(os.path.abspath(sys.path[0])))\n\nfrom official.resnet import imagenet_input    # pylint: disable=g-import-not-at-top\nfrom official.resnet import resnet_main\nfrom tensorflow.python.estimator import estimator\n\n\nFLAGS = tf.flags.FLAGS\n\nCKPT_PATTERN = r\'model\\.ckpt-(?P<gs>[0-9]+)\\.data\'\n\nflags.DEFINE_string(\n    \'data_dir_small\', default=None,\n    help=(\'The directory where the resized (160x160) ImageNet input data is \'\n          \'stored. This is only to be used in conjunction with the \'\n          \'resnet_benchmark.py script.\'))\n\nflags.DEFINE_bool(\n    \'use_fast_lr\', default=False,\n    help=(\'Enabling this uses a faster learning rate schedule along with \'\n          \'different image sizes in the input pipeline. This is only to be \'\n          \'used in conjunction with the resnet_benchmark.py script.\'))\n\n\n# Number of training and evaluation images in the standard ImageNet dataset\nNUM_TRAIN_IMAGES = 1281167\nNUM_EVAL_IMAGES = 50000\n\n\ndef main(unused_argv):\n  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n      FLAGS.tpu,\n      zone=FLAGS.tpu_zone,\n      project=FLAGS.gcp_project)\n\n  config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_steps=FLAGS.iterations_per_loop,\n      keep_checkpoint_max=None,\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=FLAGS.iterations_per_loop,\n          num_shards=FLAGS.num_cores,\n          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))  # pylint: disable=line-too-long\n\n  # Input pipelines are slightly different (with regards to shuffling and\n  # preprocessing) between training and evaluation.\n  imagenet_train = imagenet_input.ImageNetInput(\n      is_training=True,\n      data_dir=FLAGS.data_dir,\n      use_bfloat16=True,\n      transpose_input=FLAGS.transpose_input)\n  imagenet_eval = imagenet_input.ImageNetInput(\n      is_training=False,\n      data_dir=FLAGS.data_dir,\n      use_bfloat16=True,\n      transpose_input=FLAGS.transpose_input)\n\n  if FLAGS.use_fast_lr:\n    resnet_main.LR_SCHEDULE = [    # (multiplier, epoch to start) tuples\n        (1.0, 4), (0.1, 21), (0.01, 35), (0.001, 43)\n    ]\n    imagenet_train_small = imagenet_input.ImageNetInput(\n        is_training=True,\n        image_size=128,\n        data_dir=FLAGS.data_dir_small,\n        num_parallel_calls=FLAGS.num_parallel_calls,\n        use_bfloat16=True,\n        transpose_input=FLAGS.transpose_input,\n        cache=True)\n    imagenet_eval_small = imagenet_input.ImageNetInput(\n        is_training=False,\n        image_size=128,\n        data_dir=FLAGS.data_dir_small,\n        num_parallel_calls=FLAGS.num_parallel_calls,\n        use_bfloat16=True,\n        transpose_input=FLAGS.transpose_input,\n        cache=True)\n    imagenet_train_large = imagenet_input.ImageNetInput(\n        is_training=True,\n        image_size=288,\n        data_dir=FLAGS.data_dir,\n        num_parallel_calls=FLAGS.num_parallel_calls,\n        use_bfloat16=True,\n        transpose_input=FLAGS.transpose_input)\n    imagenet_eval_large = imagenet_input.ImageNetInput(\n        is_training=False,\n        image_size=288,\n        data_dir=FLAGS.data_dir,\n        num_parallel_calls=FLAGS.num_parallel_calls,\n        use_bfloat16=True,\n        transpose_input=FLAGS.transpose_input)\n\n  resnet_classifier = tf.contrib.tpu.TPUEstimator(\n      use_tpu=FLAGS.use_tpu,\n      model_fn=resnet_main.resnet_model_fn,\n      config=config,\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=FLAGS.eval_batch_size)\n\n  if FLAGS.mode == \'train\':\n    current_step = estimator._load_global_step_from_checkpoint_dir(FLAGS.model_dir)  # pylint: disable=protected-access,line-too-long\n    batches_per_epoch = NUM_TRAIN_IMAGES / FLAGS.train_batch_size\n    tf.logging.info(\'Training for %d steps (%.2f epochs in total). Current\'\n                    \' step %d.\' % (FLAGS.train_steps,\n                                   FLAGS.train_steps / batches_per_epoch,\n                                   current_step))\n\n    start_timestamp = time.time()  # This time will include compilation time\n\n    # Write a dummy file at the start of training so that we can measure the\n    # runtime at each checkpoint from the file write time.\n    tf.gfile.MkDir(FLAGS.model_dir)\n    if not tf.gfile.Exists(os.path.join(FLAGS.model_dir, \'START\')):\n      with tf.gfile.GFile(os.path.join(FLAGS.model_dir, \'START\'), \'w\') as f:\n        f.write(str(start_timestamp))\n\n    if FLAGS.use_fast_lr:\n      small_steps = int(18 * NUM_TRAIN_IMAGES / FLAGS.train_batch_size)\n      normal_steps = int(41 * NUM_TRAIN_IMAGES / FLAGS.train_batch_size)\n      large_steps = int(min(50 * NUM_TRAIN_IMAGES / FLAGS.train_batch_size,\n                            FLAGS.train_steps))\n\n      resnet_classifier.train(\n          input_fn=imagenet_train_small.input_fn, max_steps=small_steps)\n      resnet_classifier.train(\n          input_fn=imagenet_train.input_fn, max_steps=normal_steps)\n      resnet_classifier.train(\n          input_fn=imagenet_train_large.input_fn,\n          max_steps=large_steps)\n    else:\n      resnet_classifier.train(\n          input_fn=imagenet_train.input_fn, max_steps=FLAGS.train_steps)\n\n  else:\n    assert FLAGS.mode == \'eval\'\n\n    start_timestamp = tf.gfile.Stat(\n        os.path.join(FLAGS.model_dir, \'START\')).mtime_nsec\n    results = []\n    eval_steps = NUM_EVAL_IMAGES // FLAGS.eval_batch_size\n\n    ckpt_steps = set()\n    all_files = tf.gfile.ListDirectory(FLAGS.model_dir)\n    for f in all_files:\n      mat = re.match(CKPT_PATTERN, f)\n      if mat is not None:\n        ckpt_steps.add(int(mat.group(\'gs\')))\n    ckpt_steps = sorted(list(ckpt_steps))\n    tf.logging.info(\'Steps to be evaluated: %s\' % str(ckpt_steps))\n\n    for step in ckpt_steps:\n      ckpt = os.path.join(FLAGS.model_dir, \'model.ckpt-%d\' % step)\n\n      batches_per_epoch = NUM_TRAIN_IMAGES // FLAGS.train_batch_size\n      current_epoch = step // batches_per_epoch\n\n      if FLAGS.use_fast_lr:\n        if current_epoch < 18:\n          eval_input_fn = imagenet_eval_small.input_fn\n        if current_epoch >= 18 and current_epoch < 41:\n          eval_input_fn = imagenet_eval.input_fn\n        if current_epoch >= 41:  # 41:\n          eval_input_fn = imagenet_eval_large.input_fn\n      else:\n        eval_input_fn = imagenet_eval.input_fn\n\n      end_timestamp = tf.gfile.Stat(ckpt + \'.index\').mtime_nsec\n      elapsed_hours = (end_timestamp - start_timestamp) / (1e9 * 3600.0)\n\n      tf.logging.info(\'Starting to evaluate.\')\n      eval_start = time.time()  # This time will include compilation time\n      eval_results = resnet_classifier.evaluate(\n          input_fn=eval_input_fn,\n          steps=eval_steps,\n          checkpoint_path=ckpt)\n      eval_time = int(time.time() - eval_start)\n      tf.logging.info(\'Eval results: %s. Elapsed seconds: %d\' %\n                      (eval_results, eval_time))\n      results.append([\n          current_epoch,\n          elapsed_hours,\n          \'%.2f\' % (eval_results[\'top_1_accuracy\'] * 100),\n          \'%.2f\' % (eval_results[\'top_5_accuracy\'] * 100),\n      ])\n\n      time.sleep(60)\n\n    with tf.gfile.GFile(os.path.join(FLAGS.model_dir, \'results.tsv\'), \'wb\') as tsv_file:   # pylint: disable=line-too-long\n      writer = csv.writer(tsv_file, delimiter=\'\\t\')\n      writer.writerow([\'epoch\', \'hours\', \'top1Accuracy\', \'top5Accuracy\'])\n      writer.writerows(results)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tpu/models/official/retinanet/object_detection/__init__.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tpu/models/official/retinanet/object_detection/argmax_matcher.py,20,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Argmax matcher implementation.\n\nThis class takes a similarity matrix and matches columns to rows based on the\nmaximum value per column. One can specify matched_thresholds and\nto prevent columns from matching to rows (generally resulting in a negative\ntraining example) and unmatched_theshold to ignore the match (generally\nresulting in neither a positive or negative training example).\n\nThis matcher is used in Fast(er)-RCNN.\n\nNote: matchers are used in TargetAssigners. There is a create_target_assigner\nfactory function for popular implementations.\n""""""\nimport tensorflow as tf\n\nfrom object_detection import matcher\nfrom object_detection import shape_utils\n\n\nclass ArgMaxMatcher(matcher.Matcher):\n  """"""Matcher based on highest value.\n\n  This class computes matches from a similarity matrix. Each column is matched\n  to a single row.\n\n  To support object detection target assignment this class enables setting both\n  matched_threshold (upper threshold) and unmatched_threshold (lower thresholds)\n  defining three categories of similarity which define whether examples are\n  positive, negative, or ignored:\n  (1) similarity >= matched_threshold: Highest similarity. Matched/Positive!\n  (2) matched_threshold > similarity >= unmatched_threshold: Medium similarity.\n          Depending on negatives_lower_than_unmatched, this is either\n          Unmatched/Negative OR Ignore.\n  (3) unmatched_threshold > similarity: Lowest similarity. Depending on flag\n          negatives_lower_than_unmatched, either Unmatched/Negative OR Ignore.\n  For ignored matches this class sets the values in the Match object to -2.\n  """"""\n\n  def __init__(self,\n               matched_threshold,\n               unmatched_threshold=None,\n               negatives_lower_than_unmatched=True,\n               force_match_for_each_row=False):\n    """"""Construct ArgMaxMatcher.\n\n    Args:\n      matched_threshold: Threshold for positive matches. Positive if\n        sim >= matched_threshold, where sim is the maximum value of the\n        similarity matrix for a given column. Set to None for no threshold.\n      unmatched_threshold: Threshold for negative matches. Negative if\n        sim < unmatched_threshold. Defaults to matched_threshold\n        when set to None.\n      negatives_lower_than_unmatched: Boolean which defaults to True. If True\n        then negative matches are the ones below the unmatched_threshold,\n        whereas ignored matches are in between the matched and umatched\n        threshold. If False, then negative matches are in between the matched\n        and unmatched threshold, and everything lower than unmatched is ignored.\n      force_match_for_each_row: If True, ensures that each row is matched to\n        at least one column (which is not guaranteed otherwise if the\n        matched_threshold is high). Defaults to False. See\n        argmax_matcher_test.testMatcherForceMatch() for an example.\n\n    Raises:\n      ValueError: if unmatched_threshold is set but matched_threshold is not set\n        or if unmatched_threshold > matched_threshold.\n    """"""\n    if (matched_threshold is None) and (unmatched_threshold is not None):\n      raise ValueError(\'Need to also define matched_threshold when\'\n                       \'unmatched_threshold is defined\')\n    self._matched_threshold = matched_threshold\n    if unmatched_threshold is None:\n      self._unmatched_threshold = matched_threshold\n    else:\n      if unmatched_threshold > matched_threshold:\n        raise ValueError(\'unmatched_threshold needs to be smaller or equal\'\n                         \'to matched_threshold\')\n      self._unmatched_threshold = unmatched_threshold\n    if not negatives_lower_than_unmatched:\n      if self._unmatched_threshold == self._matched_threshold:\n        raise ValueError(\'When negatives are in between matched and \'\n                         \'unmatched thresholds, these cannot be of equal \'\n                         \'value. matched: %s, unmatched: %s\',\n                         self._matched_threshold, self._unmatched_threshold)\n    self._force_match_for_each_row = force_match_for_each_row\n    self._negatives_lower_than_unmatched = negatives_lower_than_unmatched\n\n  def _match(self, similarity_matrix):\n    """"""Tries to match each column of the similarity matrix to a row.\n\n    Args:\n      similarity_matrix: tensor of shape [N, M] representing any similarity\n        metric.\n\n    Returns:\n      Match object with corresponding matches for each of M columns.\n    """"""\n\n    def _match_when_rows_are_empty():\n      """"""Performs matching when the rows of similarity matrix are empty.\n\n      When the rows are empty, all detections are false positives. So we return\n      a tensor of -1\'s to indicate that the columns do not match to any rows.\n\n      Returns:\n        matches:  int32 tensor indicating the row each column matches to.\n      """"""\n      similarity_matrix_shape = shape_utils.combined_static_and_dynamic_shape(\n          similarity_matrix)\n      return -1 * tf.ones([similarity_matrix_shape[1]], dtype=tf.int32)\n\n    def _match_when_rows_are_non_empty():\n      """"""Performs matching when the rows of similarity matrix are non empty.\n\n      Returns:\n        matches:  int32 tensor indicating the row each column matches to.\n      """"""\n      # Matches for each column\n      matches = tf.argmax(similarity_matrix, 0, output_type=tf.int32)\n\n      # Deal with matched and unmatched threshold\n      if self._matched_threshold is not None:\n        # Get logical indices of ignored and unmatched columns as tf.int64\n        matched_vals = tf.reduce_max(similarity_matrix, 0)\n        below_unmatched_threshold = tf.greater(self._unmatched_threshold,\n                                               matched_vals)\n        between_thresholds = tf.logical_and(\n            tf.greater_equal(matched_vals, self._unmatched_threshold),\n            tf.greater(self._matched_threshold, matched_vals))\n\n        if self._negatives_lower_than_unmatched:\n          matches = self._set_values_using_indicator(matches,\n                                                     below_unmatched_threshold,\n                                                     -1)\n          matches = self._set_values_using_indicator(matches,\n                                                     between_thresholds,\n                                                     -2)\n        else:\n          matches = self._set_values_using_indicator(matches,\n                                                     below_unmatched_threshold,\n                                                     -2)\n          matches = self._set_values_using_indicator(matches,\n                                                     between_thresholds,\n                                                     -1)\n\n      if self._force_match_for_each_row:\n        similarity_matrix_shape = shape_utils.combined_static_and_dynamic_shape(\n            similarity_matrix)\n        force_match_column_ids = tf.argmax(similarity_matrix, 1,\n                                           output_type=tf.int32)\n        force_match_column_indicators = tf.one_hot(\n            force_match_column_ids, depth=similarity_matrix_shape[1])\n        force_match_row_ids = tf.argmax(force_match_column_indicators, 0,\n                                        output_type=tf.int32)\n        force_match_column_mask = tf.cast(\n            tf.reduce_max(force_match_column_indicators, 0), tf.bool)\n        final_matches = tf.where(force_match_column_mask,\n                                 force_match_row_ids, matches)\n        return final_matches\n      else:\n        return matches\n\n    if similarity_matrix.shape.is_fully_defined():\n      if similarity_matrix.shape[0].value == 0:\n        return _match_when_rows_are_empty()\n      else:\n        return _match_when_rows_are_non_empty()\n    else:\n      return tf.cond(\n          tf.greater(tf.shape(similarity_matrix)[0], 0),\n          _match_when_rows_are_non_empty, _match_when_rows_are_empty)\n\n  def _set_values_using_indicator(self, x, indicator, val):\n    """"""Set the indicated fields of x to val.\n\n    Args:\n      x: tensor.\n      indicator: boolean with same shape as x.\n      val: scalar with value to set.\n\n    Returns:\n      modified tensor.\n    """"""\n    indicator = tf.cast(indicator, x.dtype)\n    return tf.add(tf.multiply(x, 1 - indicator), val * indicator)\n'"
tpu/models/official/retinanet/object_detection/box_coder.py,4,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Base box coder.\n\nBox coders convert between coordinate frames, namely image-centric\n(with (0,0) on the top left of image) and anchor-centric (with (0,0) being\ndefined by a specific anchor).\n\nUsers of a BoxCoder can call two methods:\n encode: which encodes a box with respect to a given anchor\n  (or rather, a tensor of boxes wrt a corresponding tensor of anchors) and\n decode: which inverts this encoding with a decode operation.\nIn both cases, the arguments are assumed to be in 1-1 correspondence already;\nit is not the job of a BoxCoder to perform matching.\n""""""\nfrom abc import ABCMeta\nfrom abc import abstractmethod\nfrom abc import abstractproperty\n\nimport tensorflow as tf\n\n\n# Box coder types.\nFASTER_RCNN = \'faster_rcnn\'\nKEYPOINT = \'keypoint\'\nMEAN_STDDEV = \'mean_stddev\'\nSQUARE = \'square\'\n\n\nclass BoxCoder(object):\n  """"""Abstract base class for box coder.""""""\n  __metaclass__ = ABCMeta\n\n  @abstractproperty\n  def code_size(self):\n    """"""Return the size of each code.\n\n    This number is a constant and should agree with the output of the `encode`\n    op (e.g. if rel_codes is the output of self.encode(...), then it should have\n    shape [N, code_size()]).  This abstractproperty should be overridden by\n    implementations.\n\n    Returns:\n      an integer constant\n    """"""\n    pass\n\n  def encode(self, boxes, anchors):\n    """"""Encode a box list relative to an anchor collection.\n\n    Args:\n      boxes: BoxList holding N boxes to be encoded\n      anchors: BoxList of N anchors\n\n    Returns:\n      a tensor representing N relative-encoded boxes\n    """"""\n    with tf.name_scope(\'Encode\'):\n      return self._encode(boxes, anchors)\n\n  def decode(self, rel_codes, anchors):\n    """"""Decode boxes that are encoded relative to an anchor collection.\n\n    Args:\n      rel_codes: a tensor representing N relative-encoded boxes\n      anchors: BoxList of anchors\n\n    Returns:\n      boxlist: BoxList holding N boxes encoded in the ordinary way (i.e.,\n        with corners y_min, x_min, y_max, x_max)\n    """"""\n    with tf.name_scope(\'Decode\'):\n      return self._decode(rel_codes, anchors)\n\n  @abstractmethod\n  def _encode(self, boxes, anchors):\n    """"""Method to be overriden by implementations.\n\n    Args:\n      boxes: BoxList holding N boxes to be encoded\n      anchors: BoxList of N anchors\n\n    Returns:\n      a tensor representing N relative-encoded boxes\n    """"""\n    pass\n\n  @abstractmethod\n  def _decode(self, rel_codes, anchors):\n    """"""Method to be overriden by implementations.\n\n    Args:\n      rel_codes: a tensor representing N relative-encoded boxes\n      anchors: BoxList of anchors\n\n    Returns:\n      boxlist: BoxList holding N boxes encoded in the ordinary way (i.e.,\n        with corners y_min, x_min, y_max, x_max)\n    """"""\n    pass\n\n\ndef batch_decode(encoded_boxes, box_coder, anchors):\n  """"""Decode a batch of encoded boxes.\n\n  This op takes a batch of encoded bounding boxes and transforms\n  them to a batch of bounding boxes specified by their corners in\n  the order of [y_min, x_min, y_max, x_max].\n\n  Args:\n    encoded_boxes: a float32 tensor of shape [batch_size, num_anchors,\n      code_size] representing the location of the objects.\n    box_coder: a BoxCoder object.\n    anchors: a BoxList of anchors used to encode `encoded_boxes`.\n\n  Returns:\n    decoded_boxes: a float32 tensor of shape [batch_size, num_anchors,\n      coder_size] representing the corners of the objects in the order\n      of [y_min, x_min, y_max, x_max].\n\n  Raises:\n    ValueError: if batch sizes of the inputs are inconsistent, or if\n    the number of anchors inferred from encoded_boxes and anchors are\n    inconsistent.\n  """"""\n  encoded_boxes.get_shape().assert_has_rank(3)\n  if encoded_boxes.get_shape()[1].value != anchors.num_boxes_static():\n    raise ValueError(\'The number of anchors inferred from encoded_boxes\'\n                     \' and anchors are inconsistent: shape[1] of encoded_boxes\'\n                     \' %s should be equal to the number of anchors: %s.\' %\n                     (encoded_boxes.get_shape()[1].value,\n                      anchors.num_boxes_static()))\n\n  decoded_boxes = tf.stack([\n      box_coder.decode(boxes, anchors).get()\n      for boxes in tf.unstack(encoded_boxes)\n  ])\n  return decoded_boxes\n'"
tpu/models/official/retinanet/object_detection/box_list.py,8,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Bounding Box List definition.\n\nBoxList represents a list of bounding boxes as tensorflow\ntensors, where each bounding box is represented as a row of 4 numbers,\n[y_min, x_min, y_max, x_max].  It is assumed that all bounding boxes\nwithin a given list correspond to a single image.  See also\nbox_list_ops.py for common box related operations (such as area, iou, etc).\n\nOptionally, users can add additional related fields (such as weights).\nWe assume the following things to be true about fields:\n* they correspond to boxes in the box_list along the 0th dimension\n* they have inferrable rank at graph construction time\n* all dimensions except for possibly the 0th can be inferred\n  (i.e., not None) at graph construction time.\n\nSome other notes:\n  * Following tensorflow conventions, we use height, width ordering,\n  and correspondingly, y,x (or ymin, xmin, ymax, xmax) ordering\n  * Tensors are always provided as (flat) [N, 4] tensors.\n""""""\n\nimport tensorflow as tf\n\n\nclass BoxList(object):\n  """"""Box collection.""""""\n\n  def __init__(self, boxes):\n    """"""Constructs box collection.\n\n    Args:\n      boxes: a tensor of shape [N, 4] representing box corners\n\n    Raises:\n      ValueError: if invalid dimensions for bbox data or if bbox data is not in\n          float32 format.\n    """"""\n    if len(boxes.get_shape()) != 2 or boxes.get_shape()[-1] != 4:\n      raise ValueError(\'Invalid dimensions for box data.\')\n    if boxes.dtype != tf.float32:\n      raise ValueError(\'Invalid tensor type: should be tf.float32\')\n    self.data = {\'boxes\': boxes}\n\n  def num_boxes(self):\n    """"""Returns number of boxes held in collection.\n\n    Returns:\n      a tensor representing the number of boxes held in the collection.\n    """"""\n    return tf.shape(self.data[\'boxes\'])[0]\n\n  def num_boxes_static(self):\n    """"""Returns number of boxes held in collection.\n\n    This number is inferred at graph construction time rather than run-time.\n\n    Returns:\n      Number of boxes held in collection (integer) or None if this is not\n        inferrable at graph construction time.\n    """"""\n    return self.data[\'boxes\'].get_shape()[0].value\n\n  def get_all_fields(self):\n    """"""Returns all fields.""""""\n    return self.data.keys()\n\n  def get_extra_fields(self):\n    """"""Returns all non-box fields (i.e., everything not named \'boxes\').""""""\n    return [k for k in self.data.keys() if k != \'boxes\']\n\n  def add_field(self, field, field_data):\n    """"""Add field to box list.\n\n    This method can be used to add related box data such as\n    weights/labels, etc.\n\n    Args:\n      field: a string key to access the data via `get`\n      field_data: a tensor containing the data to store in the BoxList\n    """"""\n    self.data[field] = field_data\n\n  def has_field(self, field):\n    return field in self.data\n\n  def get(self):\n    """"""Convenience function for accessing box coordinates.\n\n    Returns:\n      a tensor with shape [N, 4] representing box coordinates.\n    """"""\n    return self.get_field(\'boxes\')\n\n  def set(self, boxes):\n    """"""Convenience function for setting box coordinates.\n\n    Args:\n      boxes: a tensor of shape [N, 4] representing box corners\n\n    Raises:\n      ValueError: if invalid dimensions for bbox data\n    """"""\n    if len(boxes.get_shape()) != 2 or boxes.get_shape()[-1] != 4:\n      raise ValueError(\'Invalid dimensions for box data.\')\n    self.data[\'boxes\'] = boxes\n\n  def get_field(self, field):\n    """"""Accesses a box collection and associated fields.\n\n    This function returns specified field with object; if no field is specified,\n    it returns the box coordinates.\n\n    Args:\n      field: this optional string parameter can be used to specify\n        a related field to be accessed.\n\n    Returns:\n      a tensor representing the box collection or an associated field.\n\n    Raises:\n      ValueError: if invalid field\n    """"""\n    if not self.has_field(field):\n      raise ValueError(\'field \' + str(field) + \' does not exist\')\n    return self.data[field]\n\n  def set_field(self, field, value):\n    """"""Sets the value of a field.\n\n    Updates the field of a box_list with a given value.\n\n    Args:\n      field: (string) name of the field to set value.\n      value: the value to assign to the field.\n\n    Raises:\n      ValueError: if the box_list does not have specified field.\n    """"""\n    if not self.has_field(field):\n      raise ValueError(\'field %s does not exist\' % field)\n    self.data[field] = value\n\n  def get_center_coordinates_and_sizes(self, scope=None):\n    """"""Computes the center coordinates, height and width of the boxes.\n\n    Args:\n      scope: name scope of the function.\n\n    Returns:\n      a list of 4 1-D tensors [ycenter, xcenter, height, width].\n    """"""\n    with tf.name_scope(scope, \'get_center_coordinates_and_sizes\'):\n      box_corners = self.get()\n      ymin, xmin, ymax, xmax = tf.unstack(tf.transpose(box_corners))\n      width = xmax - xmin\n      height = ymax - ymin\n      ycenter = ymin + height / 2.\n      xcenter = xmin + width / 2.\n      return [ycenter, xcenter, height, width]\n\n  def transpose_coordinates(self, scope=None):\n    """"""Transpose the coordinate representation in a boxlist.\n\n    Args:\n      scope: name scope of the function.\n    """"""\n    with tf.name_scope(scope, \'transpose_coordinates\'):\n      y_min, x_min, y_max, x_max = tf.split(\n          value=self.get(), num_or_size_splits=4, axis=1)\n      self.set(tf.concat([x_min, y_min, x_max, y_max], 1))\n\n  def as_tensor_dict(self, fields=None):\n    """"""Retrieves specified fields as a dictionary of tensors.\n\n    Args:\n      fields: (optional) list of fields to return in the dictionary.\n        If None (default), all fields are returned.\n\n    Returns:\n      tensor_dict: A dictionary of tensors specified by fields.\n\n    Raises:\n      ValueError: if specified field is not contained in boxlist.\n    """"""\n    tensor_dict = {}\n    if fields is None:\n      fields = self.get_all_fields()\n    for field in fields:\n      if not self.has_field(field):\n        raise ValueError(\'boxlist must contain all specified fields\')\n      tensor_dict[field] = self.get_field(field)\n    return tensor_dict\n'"
tpu/models/official/retinanet/object_detection/faster_rcnn_box_coder.py,7,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Faster RCNN box coder.\n\nFaster RCNN box coder follows the coding schema described below:\n  ty = (y - ya) / ha\n  tx = (x - xa) / wa\n  th = log(h / ha)\n  tw = log(w / wa)\n  where x, y, w, h denote the box\'s center coordinates, width and height\n  respectively. Similarly, xa, ya, wa, ha denote the anchor\'s center\n  coordinates, width and height. tx, ty, tw and th denote the anchor-encoded\n  center, width and height respectively.\n\n  See http://arxiv.org/abs/1506.01497 for details.\n""""""\n\nimport tensorflow as tf\n\nfrom object_detection import box_coder\nfrom object_detection import box_list\n\nEPSILON = 1e-8\n\n\nclass FasterRcnnBoxCoder(box_coder.BoxCoder):\n  """"""Faster RCNN box coder.""""""\n\n  def __init__(self, scale_factors=None):\n    """"""Constructor for FasterRcnnBoxCoder.\n\n    Args:\n      scale_factors: List of 4 positive scalars to scale ty, tx, th and tw.\n        If set to None, does not perform scaling. For Faster RCNN,\n        the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0].\n    """"""\n    if scale_factors:\n      assert len(scale_factors) == 4\n      for scalar in scale_factors:\n        assert scalar > 0\n    self._scale_factors = scale_factors\n\n  @property\n  def code_size(self):\n    return 4\n\n  def _encode(self, boxes, anchors):\n    """"""Encode a box collection with respect to anchor collection.\n\n    Args:\n      boxes: BoxList holding N boxes to be encoded.\n      anchors: BoxList of anchors.\n\n    Returns:\n      a tensor representing N anchor-encoded boxes of the format\n      [ty, tx, th, tw].\n    """"""\n    # Convert anchors to the center coordinate representation.\n    ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()\n    ycenter, xcenter, h, w = boxes.get_center_coordinates_and_sizes()\n    # Avoid NaN in division and log below.\n    ha += EPSILON\n    wa += EPSILON\n    h += EPSILON\n    w += EPSILON\n\n    tx = (xcenter - xcenter_a) / wa\n    ty = (ycenter - ycenter_a) / ha\n    tw = tf.log(w / wa)\n    th = tf.log(h / ha)\n    # Scales location targets as used in paper for joint training.\n    if self._scale_factors:\n      ty *= self._scale_factors[0]\n      tx *= self._scale_factors[1]\n      th *= self._scale_factors[2]\n      tw *= self._scale_factors[3]\n    return tf.transpose(tf.stack([ty, tx, th, tw]))\n\n  def _decode(self, rel_codes, anchors):\n    """"""Decode relative codes to boxes.\n\n    Args:\n      rel_codes: a tensor representing N anchor-encoded boxes.\n      anchors: BoxList of anchors.\n\n    Returns:\n      boxes: BoxList holding N bounding boxes.\n    """"""\n    ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()\n\n    ty, tx, th, tw = tf.unstack(tf.transpose(rel_codes))\n    if self._scale_factors:\n      ty /= self._scale_factors[0]\n      tx /= self._scale_factors[1]\n      th /= self._scale_factors[2]\n      tw /= self._scale_factors[3]\n    w = tf.exp(tw) * wa\n    h = tf.exp(th) * ha\n    ycenter = ty * ha + ycenter_a\n    xcenter = tx * wa + xcenter_a\n    ymin = ycenter - h / 2.\n    xmin = xcenter - w / 2.\n    ymax = ycenter + h / 2.\n    xmax = xcenter + w / 2.\n    return box_list.BoxList(tf.transpose(tf.stack([ymin, xmin, ymax, xmax])))\n'"
tpu/models/official/retinanet/object_detection/matcher.py,17,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Matcher interface and Match class.\n\nThis module defines the Matcher interface and the Match object. The job of the\nmatcher is to match row and column indices based on the similarity matrix and\nother optional parameters. Each column is matched to at most one row. There\nare three possibilities for the matching:\n\n1) match: A column matches a row.\n2) no_match: A column does not match any row.\n3) ignore: A column that is neither \'match\' nor no_match.\n\nThe ignore case is regularly encountered in object detection: when an anchor has\na relatively small overlap with a ground-truth box, one neither wants to\nconsider this box a positive example (match) nor a negative example (no match).\n\nThe Match class is used to store the match results and it provides simple apis\nto query the results.\n""""""\nfrom abc import ABCMeta\nfrom abc import abstractmethod\n\nimport tensorflow as tf\n\n\nclass Match(object):\n  """"""Class to store results from the matcher.\n\n  This class is used to store the results from the matcher. It provides\n  convenient methods to query the matching results.\n  """"""\n\n  def __init__(self, match_results):\n    """"""Constructs a Match object.\n\n    Args:\n      match_results: Integer tensor of shape [N] with (1) match_results[i]>=0,\n        meaning that column i is matched with row match_results[i].\n        (2) match_results[i]=-1, meaning that column i is not matched.\n        (3) match_results[i]=-2, meaning that column i is ignored.\n\n    Raises:\n      ValueError: if match_results does not have rank 1 or is not an\n        integer int32 scalar tensor\n    """"""\n    if match_results.shape.ndims != 1:\n      raise ValueError(\'match_results should have rank 1\')\n    if match_results.dtype != tf.int32:\n      raise ValueError(\'match_results should be an int32 or int64 scalar \'\n                       \'tensor\')\n    self._match_results = match_results\n\n  @property\n  def match_results(self):\n    """"""The accessor for match results.\n\n    Returns:\n      the tensor which encodes the match results.\n    """"""\n    return self._match_results\n\n  def matched_column_indices(self):\n    """"""Returns column indices that match to some row.\n\n    The indices returned by this op are always sorted in increasing order.\n\n    Returns:\n      column_indices: int32 tensor of shape [K] with column indices.\n    """"""\n    return self._reshape_and_cast(tf.where(tf.greater(self._match_results, -1)))\n\n  def matched_column_indicator(self):\n    """"""Returns column indices that are matched.\n\n    Returns:\n      column_indices: int32 tensor of shape [K] with column indices.\n    """"""\n    return tf.greater_equal(self._match_results, 0)\n\n  def num_matched_columns(self):\n    """"""Returns number (int32 scalar tensor) of matched columns.""""""\n    return tf.size(self.matched_column_indices())\n\n  def unmatched_column_indices(self):\n    """"""Returns column indices that do not match any row.\n\n    The indices returned by this op are always sorted in increasing order.\n\n    Returns:\n      column_indices: int32 tensor of shape [K] with column indices.\n    """"""\n    return self._reshape_and_cast(tf.where(tf.equal(self._match_results, -1)))\n\n  def unmatched_column_indicator(self):\n    """"""Returns column indices that are unmatched.\n\n    Returns:\n      column_indices: int32 tensor of shape [K] with column indices.\n    """"""\n    return tf.equal(self._match_results, -1)\n\n  def num_unmatched_columns(self):\n    """"""Returns number (int32 scalar tensor) of unmatched columns.""""""\n    return tf.size(self.unmatched_column_indices())\n\n  def ignored_column_indices(self):\n    """"""Returns column indices that are ignored (neither Matched nor Unmatched).\n\n    The indices returned by this op are always sorted in increasing order.\n\n    Returns:\n      column_indices: int32 tensor of shape [K] with column indices.\n    """"""\n    return self._reshape_and_cast(tf.where(self.ignored_column_indicator()))\n\n  def ignored_column_indicator(self):\n    """"""Returns boolean column indicator where True means the colum is ignored.\n\n    Returns:\n      column_indicator: boolean vector which is True for all ignored column\n      indices.\n    """"""\n    return tf.equal(self._match_results, -2)\n\n  def num_ignored_columns(self):\n    """"""Returns number (int32 scalar tensor) of matched columns.""""""\n    return tf.size(self.ignored_column_indices())\n\n  def unmatched_or_ignored_column_indices(self):\n    """"""Returns column indices that are unmatched or ignored.\n\n    The indices returned by this op are always sorted in increasing order.\n\n    Returns:\n      column_indices: int32 tensor of shape [K] with column indices.\n    """"""\n    return self._reshape_and_cast(tf.where(tf.greater(0, self._match_results)))\n\n  def matched_row_indices(self):\n    """"""Returns row indices that match some column.\n\n    The indices returned by this op are ordered so as to be in correspondence\n    with the output of matched_column_indicator().  For example if\n    self.matched_column_indicator() is [0,2], and self.matched_row_indices() is\n    [7, 3], then we know that column 0 was matched to row 7 and column 2 was\n    matched to row 3.\n\n    Returns:\n      row_indices: int32 tensor of shape [K] with row indices.\n    """"""\n    return self._reshape_and_cast(\n        tf.gather(self._match_results, self.matched_column_indices()))\n\n  def _reshape_and_cast(self, t):\n    return tf.cast(tf.reshape(t, [-1]), tf.int32)\n\n  def gather_based_on_match(self, input_tensor, unmatched_value,\n                            ignored_value):\n    """"""Gathers elements from `input_tensor` based on match results.\n\n    For columns that are matched to a row, gathered_tensor[col] is set to\n    input_tensor[match_results[col]]. For columns that are unmatched,\n    gathered_tensor[col] is set to unmatched_value. Finally, for columns that\n    are ignored gathered_tensor[col] is set to ignored_value.\n\n    Note that the input_tensor.shape[1:] must match with unmatched_value.shape\n    and ignored_value.shape\n\n    Args:\n      input_tensor: Tensor to gather values from.\n      unmatched_value: Constant tensor value for unmatched columns.\n      ignored_value: Constant tensor value for ignored columns.\n\n    Returns:\n      gathered_tensor: A tensor containing values gathered from input_tensor.\n        The shape of the gathered tensor is [match_results.shape[0]] +\n        input_tensor.shape[1:].\n    """"""\n    input_tensor = tf.concat([tf.stack([ignored_value, unmatched_value]),\n                              input_tensor], axis=0)\n    gather_indices = tf.maximum(self.match_results + 2, 0)\n    gathered_tensor = tf.gather(input_tensor, gather_indices)\n    return gathered_tensor\n\n\nclass Matcher(object):\n  """"""Abstract base class for matcher.\n  """"""\n  __metaclass__ = ABCMeta\n\n  def match(self, similarity_matrix, scope=None, **params):\n    """"""Computes matches among row and column indices and returns the result.\n\n    Computes matches among the row and column indices based on the similarity\n    matrix and optional arguments.\n\n    Args:\n      similarity_matrix: Float tensor of shape [N, M] with pairwise similarity\n        where higher value means more similar.\n      scope: Op scope name. Defaults to \'Match\' if None.\n      **params: Additional keyword arguments for specific implementations of\n        the Matcher.\n\n    Returns:\n      A Match object with the results of matching.\n    """"""\n    with tf.name_scope(scope, \'Match\', [similarity_matrix, params]) as scope:\n      return Match(self._match(similarity_matrix, **params))\n\n  @abstractmethod\n  def _match(self, similarity_matrix, **params):\n    """"""Method to be overridden by implementations.\n\n    Args:\n      similarity_matrix: Float tensor of shape [N, M] with pairwise similarity\n        where higher value means more similar.\n      **params: Additional keyword arguments for specific implementations of\n        the Matcher.\n\n    Returns:\n      match_results: Integer tensor of shape [M]: match_results[i]>=0 means\n        that column i is matched to row match_results[i], match_results[i]=-1\n        means that the column is not matched. match_results[i]=-2 means that\n        the column is ignored (usually this happens when there is a very weak\n        match which one neither wants as positive nor negative example).\n    """"""\n    pass\n'"
tpu/models/official/retinanet/object_detection/preprocessor.py,54,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Preprocess images and bounding boxes for detection.\n\nWe perform two sets of operations in preprocessing stage:\n(a) operations that are applied to both training and testing data,\n(b) operations that are applied only to training data for the purpose of\n    data augmentation.\n\nA preprocessing function receives a set of inputs,\ne.g. an image and bounding boxes,\nperforms an operation on them, and returns them.\nSome examples are: randomly cropping the image, randomly mirroring the image,\n                   randomly changing the brightness, contrast, hue and\n                   randomly jittering the bounding boxes.\n\nThe image is a rank 4 tensor: [1, height, width, channels] with\ndtype=tf.float32. The groundtruth_boxes is a rank 2 tensor: [N, 4] where\nin each row there is a box with [ymin xmin ymax xmax].\nBoxes are in normalized coordinates meaning\ntheir coordinate values range in [0, 1]\n\nImportant Note: In tensor_dict, images is a rank 4 tensor, but preprocessing\nfunctions receive a rank 3 tensor for processing the image. Thus, inside the\npreprocess function we squeeze the image to become a rank 3 tensor and then\nwe pass it to the functions. At the end of the preprocess we expand the image\nback to rank 4.\n""""""\n\nimport tensorflow as tf\n\nfrom object_detection import box_list\n\n\ndef _flip_boxes_left_right(boxes):\n  """"""Left-right flip the boxes.\n\n  Args:\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n\n  Returns:\n    Flipped boxes.\n  """"""\n  ymin, xmin, ymax, xmax = tf.split(value=boxes, num_or_size_splits=4, axis=1)\n  flipped_xmin = tf.subtract(1.0, xmax)\n  flipped_xmax = tf.subtract(1.0, xmin)\n  flipped_boxes = tf.concat([ymin, flipped_xmin, ymax, flipped_xmax], 1)\n  return flipped_boxes\n\n\ndef _flip_masks_left_right(masks):\n  """"""Left-right flip masks.\n\n  Args:\n    masks: rank 3 float32 tensor with shape\n      [num_instances, height, width] representing instance masks.\n\n  Returns:\n    flipped masks: rank 3 float32 tensor with shape\n      [num_instances, height, width] representing instance masks.\n  """"""\n  return masks[:, :, ::-1]\n\n\ndef keypoint_flip_horizontal(keypoints, flip_point, flip_permutation,\n                             scope=None):\n  """"""Flips the keypoints horizontally around the flip_point.\n\n  This operation flips the x coordinate for each keypoint around the flip_point\n  and also permutes the keypoints in a manner specified by flip_permutation.\n\n  Args:\n    keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n    flip_point:  (float) scalar tensor representing the x coordinate to flip the\n      keypoints around.\n    flip_permutation: rank 1 int32 tensor containing the keypoint flip\n      permutation. This specifies the mapping from original keypoint indices\n      to the flipped keypoint indices. This is used primarily for keypoints\n      that are not reflection invariant. E.g. Suppose there are 3 keypoints\n      representing [\'head\', \'right_eye\', \'left_eye\'], then a logical choice for\n      flip_permutation might be [0, 2, 1] since we want to swap the \'left_eye\'\n      and \'right_eye\' after a horizontal flip.\n    scope: name scope.\n\n  Returns:\n    new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n  """"""\n  with tf.name_scope(scope, \'FlipHorizontal\'):\n    keypoints = tf.transpose(keypoints, [1, 0, 2])\n    keypoints = tf.gather(keypoints, flip_permutation)\n    v, u = tf.split(value=keypoints, num_or_size_splits=2, axis=2)\n    u = flip_point * 2.0 - u\n    new_keypoints = tf.concat([v, u], 2)\n    new_keypoints = tf.transpose(new_keypoints, [1, 0, 2])\n    return new_keypoints\n\n\ndef random_horizontal_flip(image,\n                           boxes=None,\n                           masks=None,\n                           keypoints=None,\n                           keypoint_flip_permutation=None,\n                           seed=None):\n  """"""Randomly flips the image and detections horizontally.\n\n  The probability of flipping the image is 50%.\n\n  Args:\n    image: rank 3 float32 tensor with shape [height, width, channels].\n    boxes: (optional) rank 2 float32 tensor with shape [N, 4]\n           containing the bounding boxes.\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n    masks: (optional) rank 3 float32 tensor with shape\n           [num_instances, height, width] containing instance masks. The masks\n           are of the same height, width as the input `image`.\n    keypoints: (optional) rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]. The keypoints are in y-x\n               normalized coordinates.\n    keypoint_flip_permutation: rank 1 int32 tensor containing the keypoint flip\n                               permutation.\n    seed: random seed\n\n  Returns:\n    image: image which is the same shape as input image.\n\n    If boxes, masks, keypoints, and keypoint_flip_permutation are not None,\n    the function also returns the following tensors.\n\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n    masks: rank 3 float32 tensor with shape [num_instances, height, width]\n           containing instance masks.\n    keypoints: rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]\n\n  Raises:\n    ValueError: if keypoints are provided but keypoint_flip_permutation is not.\n  """"""\n\n  def _flip_image(image):\n    # flip image\n    image_flipped = tf.image.flip_left_right(image)\n    return image_flipped\n\n  if keypoints is not None and keypoint_flip_permutation is None:\n    raise ValueError(\n        \'keypoints are provided but keypoints_flip_permutation is not provided\')\n\n  with tf.name_scope(\'RandomHorizontalFlip\', values=[image, boxes]):\n    result = []\n    # random variable defining whether to do flip or not\n    do_a_flip_random = tf.greater(tf.random_uniform([], seed=seed), 0.5)\n\n    # flip image\n    image = tf.cond(do_a_flip_random, lambda: _flip_image(image), lambda: image)\n    result.append(image)\n\n    # flip boxes\n    if boxes is not None:\n      boxes = tf.cond(do_a_flip_random, lambda: _flip_boxes_left_right(boxes),\n                      lambda: boxes)\n      result.append(boxes)\n\n    # flip masks\n    if masks is not None:\n      masks = tf.cond(do_a_flip_random, lambda: _flip_masks_left_right(masks),\n                      lambda: masks)\n      result.append(masks)\n\n    # flip keypoints\n    if keypoints is not None and keypoint_flip_permutation is not None:\n      permutation = keypoint_flip_permutation\n      keypoints = tf.cond(\n          do_a_flip_random,\n          lambda: keypoint_flip_horizontal(keypoints, 0.5, permutation),\n          lambda: keypoints)\n      result.append(keypoints)\n\n    return tuple(result)\n\n\ndef _compute_new_static_size(image, min_dimension, max_dimension):\n  """"""Compute new static shape for resize_to_range method.""""""\n  image_shape = image.get_shape().as_list()\n  orig_height = image_shape[0]\n  orig_width = image_shape[1]\n  num_channels = image_shape[2]\n  orig_min_dim = min(orig_height, orig_width)\n  # Calculates the larger of the possible sizes\n  large_scale_factor = min_dimension / float(orig_min_dim)\n  # Scaling orig_(height|width) by large_scale_factor will make the smaller\n  # dimension equal to min_dimension, save for floating point rounding errors.\n  # For reasonably-sized images, taking the nearest integer will reliably\n  # eliminate this error.\n  large_height = int(round(orig_height * large_scale_factor))\n  large_width = int(round(orig_width * large_scale_factor))\n  large_size = [large_height, large_width]\n  if max_dimension:\n    # Calculates the smaller of the possible sizes, use that if the larger\n    # is too big.\n    orig_max_dim = max(orig_height, orig_width)\n    small_scale_factor = max_dimension / float(orig_max_dim)\n    # Scaling orig_(height|width) by small_scale_factor will make the larger\n    # dimension equal to max_dimension, save for floating point rounding\n    # errors. For reasonably-sized images, taking the nearest integer will\n    # reliably eliminate this error.\n    small_height = int(round(orig_height * small_scale_factor))\n    small_width = int(round(orig_width * small_scale_factor))\n    small_size = [small_height, small_width]\n    new_size = large_size\n    if max(large_size) > max_dimension:\n      new_size = small_size\n  else:\n    new_size = large_size\n  return tf.constant(new_size + [num_channels])\n\n\ndef _compute_new_dynamic_size(image, min_dimension, max_dimension):\n  """"""Compute new dynamic shape for resize_to_range method.""""""\n  image_shape = tf.shape(image)\n  orig_height = tf.to_float(image_shape[0])\n  orig_width = tf.to_float(image_shape[1])\n  num_channels = image_shape[2]\n  orig_min_dim = tf.minimum(orig_height, orig_width)\n  # Calculates the larger of the possible sizes\n  min_dimension = tf.constant(min_dimension, dtype=tf.float32)\n  large_scale_factor = min_dimension / orig_min_dim\n  # Scaling orig_(height|width) by large_scale_factor will make the smaller\n  # dimension equal to min_dimension, save for floating point rounding errors.\n  # For reasonably-sized images, taking the nearest integer will reliably\n  # eliminate this error.\n  large_height = tf.to_int32(tf.round(orig_height * large_scale_factor))\n  large_width = tf.to_int32(tf.round(orig_width * large_scale_factor))\n  large_size = tf.stack([large_height, large_width])\n  if max_dimension:\n    # Calculates the smaller of the possible sizes, use that if the larger\n    # is too big.\n    orig_max_dim = tf.maximum(orig_height, orig_width)\n    max_dimension = tf.constant(max_dimension, dtype=tf.float32)\n    small_scale_factor = max_dimension / orig_max_dim\n    # Scaling orig_(height|width) by small_scale_factor will make the larger\n    # dimension equal to max_dimension, save for floating point rounding\n    # errors. For reasonably-sized images, taking the nearest integer will\n    # reliably eliminate this error.\n    small_height = tf.to_int32(tf.round(orig_height * small_scale_factor))\n    small_width = tf.to_int32(tf.round(orig_width * small_scale_factor))\n    small_size = tf.stack([small_height, small_width])\n    new_size = tf.cond(\n        tf.to_float(tf.reduce_max(large_size)) > max_dimension,\n        lambda: small_size, lambda: large_size)\n  else:\n    new_size = large_size\n  return tf.stack(tf.unstack(new_size) + [num_channels])\n\n\ndef resize_to_range(image,\n                    masks=None,\n                    min_dimension=None,\n                    max_dimension=None,\n                    method=tf.image.ResizeMethod.BILINEAR,\n                    align_corners=False,\n                    pad_to_max_dimension=False):\n  """"""Resizes an image so its dimensions are within the provided value.\n\n  The output size can be described by two cases:\n  1. If the image can be rescaled so its minimum dimension is equal to the\n     provided value without the other dimension exceeding max_dimension,\n     then do so.\n  2. Otherwise, resize so the largest dimension is equal to max_dimension.\n\n  Args:\n    image: A 3D tensor of shape [height, width, channels]\n    masks: (optional) rank 3 float32 tensor with shape\n           [num_instances, height, width] containing instance masks.\n    min_dimension: (optional) (scalar) desired size of the smaller image\n                   dimension.\n    max_dimension: (optional) (scalar) maximum allowed size\n                   of the larger image dimension.\n    method: (optional) interpolation method used in resizing. Defaults to\n            BILINEAR.\n    align_corners: bool. If true, exactly align all 4 corners of the input\n                   and output. Defaults to False.\n    pad_to_max_dimension: Whether to resize the image and pad it with zeros\n      so the resulting image is of the spatial size\n      [max_dimension, max_dimension]. If masks are included they are padded\n      similarly.\n\n  Returns:\n    Note that the position of the resized_image_shape changes based on whether\n    masks are present.\n    resized_image: A 3D tensor of shape [new_height, new_width, channels],\n      where the image has been resized (with bilinear interpolation) so that\n      min(new_height, new_width) == min_dimension or\n      max(new_height, new_width) == max_dimension.\n    resized_masks: If masks is not None, also outputs masks. A 3D tensor of\n      shape [num_instances, new_height, new_width].\n    resized_image_shape: A 1D tensor of shape [3] containing shape of the\n      resized image.\n\n  Raises:\n    ValueError: if the image is not a 3D tensor.\n  """"""\n  if len(image.get_shape()) != 3:\n    raise ValueError(\'Image should be 3D tensor\')\n\n  with tf.name_scope(\'ResizeToRange\', values=[image, min_dimension]):\n    if image.get_shape().is_fully_defined():\n      new_size = _compute_new_static_size(image, min_dimension, max_dimension)\n    else:\n      new_size = _compute_new_dynamic_size(image, min_dimension, max_dimension)\n    new_image = tf.image.resize_images(\n        image, new_size[:-1], method=method, align_corners=align_corners)\n\n    if pad_to_max_dimension:\n      new_image = tf.image.pad_to_bounding_box(\n          new_image, 0, 0, max_dimension, max_dimension)\n\n    result = [new_image]\n    if masks is not None:\n      new_masks = tf.expand_dims(masks, 3)\n      new_masks = tf.image.resize_images(\n          new_masks,\n          new_size[:-1],\n          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n          align_corners=align_corners)\n      new_masks = tf.squeeze(new_masks, 3)\n      if pad_to_max_dimension:\n        new_masks = tf.image.pad_to_bounding_box(\n            new_masks, 0, 0, max_dimension, max_dimension)\n      result.append(new_masks)\n\n    result.append(new_size)\n    return result\n\n\ndef _copy_extra_fields(boxlist_to_copy_to, boxlist_to_copy_from):\n  """"""Copies the extra fields of boxlist_to_copy_from to boxlist_to_copy_to.\n\n  Args:\n    boxlist_to_copy_to: BoxList to which extra fields are copied.\n    boxlist_to_copy_from: BoxList from which fields are copied.\n\n  Returns:\n    boxlist_to_copy_to with extra fields.\n  """"""\n  for field in boxlist_to_copy_from.get_extra_fields():\n    boxlist_to_copy_to.add_field(field, boxlist_to_copy_from.get_field(field))\n  return boxlist_to_copy_to\n\n\ndef box_list_scale(boxlist, y_scale, x_scale, scope=None):\n  """"""scale box coordinates in x and y dimensions.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    y_scale: (float) scalar tensor\n    x_scale: (float) scalar tensor\n    scope: name scope.\n\n  Returns:\n    boxlist: BoxList holding N boxes\n  """"""\n  with tf.name_scope(scope, \'Scale\'):\n    y_scale = tf.cast(y_scale, tf.float32)\n    x_scale = tf.cast(x_scale, tf.float32)\n    y_min, x_min, y_max, x_max = tf.split(\n        value=boxlist.get(), num_or_size_splits=4, axis=1)\n    y_min = y_scale * y_min\n    y_max = y_scale * y_max\n    x_min = x_scale * x_min\n    x_max = x_scale * x_max\n    scaled_boxlist = box_list.BoxList(\n        tf.concat([y_min, x_min, y_max, x_max], 1))\n    return _copy_extra_fields(scaled_boxlist, boxlist)\n\n\ndef keypoint_scale(keypoints, y_scale, x_scale, scope=None):\n  """"""Scales keypoint coordinates in x and y dimensions.\n\n  Args:\n    keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n    y_scale: (float) scalar tensor\n    x_scale: (float) scalar tensor\n    scope: name scope.\n\n  Returns:\n    new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n  """"""\n  with tf.name_scope(scope, \'Scale\'):\n    y_scale = tf.cast(y_scale, tf.float32)\n    x_scale = tf.cast(x_scale, tf.float32)\n    new_keypoints = keypoints * [[[y_scale, x_scale]]]\n    return new_keypoints\n\n\ndef scale_boxes_to_pixel_coordinates(image, boxes, keypoints=None):\n  """"""Scales boxes from normalized to pixel coordinates.\n\n  Args:\n    image: A 3D float32 tensor of shape [height, width, channels].\n    boxes: A 2D float32 tensor of shape [num_boxes, 4] containing the bounding\n      boxes in normalized coordinates. Each row is of the form\n      [ymin, xmin, ymax, xmax].\n    keypoints: (optional) rank 3 float32 tensor with shape\n      [num_instances, num_keypoints, 2]. The keypoints are in y-x normalized\n      coordinates.\n\n  Returns:\n    image: unchanged input image.\n    scaled_boxes: a 2D float32 tensor of shape [num_boxes, 4] containing the\n      bounding boxes in pixel coordinates.\n    scaled_keypoints: a 3D float32 tensor with shape\n      [num_instances, num_keypoints, 2] containing the keypoints in pixel\n      coordinates.\n  """"""\n  boxlist = box_list.BoxList(boxes)\n  image_height = tf.shape(image)[0]\n  image_width = tf.shape(image)[1]\n  scaled_boxes = box_list_scale(boxlist, image_height, image_width).get()\n  result = [image, scaled_boxes]\n  if keypoints is not None:\n    scaled_keypoints = keypoint_scale(keypoints, image_height, image_width)\n    result.append(scaled_keypoints)\n  return tuple(result)\n'"
tpu/models/official/retinanet/object_detection/region_similarity_calculator.py,18,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Region Similarity Calculators for BoxLists.\n\nRegion Similarity Calculators compare a pairwise measure of similarity\nbetween the boxes in two BoxLists.\n""""""\nfrom abc import ABCMeta\nfrom abc import abstractmethod\n\nimport tensorflow as tf\n\n\ndef area(boxlist, scope=None):\n  """"""Computes area of boxes.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N] representing box areas.\n  """"""\n  with tf.name_scope(scope, \'Area\'):\n    y_min, x_min, y_max, x_max = tf.split(\n        value=boxlist.get(), num_or_size_splits=4, axis=1)\n    return tf.squeeze((y_max - y_min) * (x_max - x_min), [1])\n\n\ndef intersection(boxlist1, boxlist2, scope=None):\n  """"""Compute pairwise intersection areas between boxes.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N, M] representing pairwise intersections\n  """"""\n  with tf.name_scope(scope, \'Intersection\'):\n    y_min1, x_min1, y_max1, x_max1 = tf.split(\n        value=boxlist1.get(), num_or_size_splits=4, axis=1)\n    y_min2, x_min2, y_max2, x_max2 = tf.split(\n        value=boxlist2.get(), num_or_size_splits=4, axis=1)\n    all_pairs_min_ymax = tf.minimum(y_max1, tf.transpose(y_max2))\n    all_pairs_max_ymin = tf.maximum(y_min1, tf.transpose(y_min2))\n    intersect_heights = tf.maximum(0.0, all_pairs_min_ymax - all_pairs_max_ymin)\n    all_pairs_min_xmax = tf.minimum(x_max1, tf.transpose(x_max2))\n    all_pairs_max_xmin = tf.maximum(x_min1, tf.transpose(x_min2))\n    intersect_widths = tf.maximum(0.0, all_pairs_min_xmax - all_pairs_max_xmin)\n    return intersect_heights * intersect_widths\n\n\ndef iou(boxlist1, boxlist2, scope=None):\n  """"""Computes pairwise intersection-over-union between box collections.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N, M] representing pairwise iou scores.\n  """"""\n  with tf.name_scope(scope, \'IOU\'):\n    intersections = intersection(boxlist1, boxlist2)\n    areas1 = area(boxlist1)\n    areas2 = area(boxlist2)\n    unions = (\n        tf.expand_dims(areas1, 1) + tf.expand_dims(areas2, 0) - intersections)\n    return tf.where(\n        tf.equal(intersections, 0.0),\n        tf.zeros_like(intersections), tf.truediv(intersections, unions))\n\n\nclass RegionSimilarityCalculator(object):\n  """"""Abstract base class for region similarity calculator.""""""\n  __metaclass__ = ABCMeta\n\n  def compare(self, boxlist1, boxlist2, scope=None):\n    """"""Computes matrix of pairwise similarity between BoxLists.\n\n    This op (to be overriden) computes a measure of pairwise similarity between\n    the boxes in the given BoxLists. Higher values indicate more similarity.\n\n    Note that this method simply measures similarity and does not explicitly\n    perform a matching.\n\n    Args:\n      boxlist1: BoxList holding N boxes.\n      boxlist2: BoxList holding M boxes.\n      scope: Op scope name. Defaults to \'Compare\' if None.\n\n    Returns:\n      a (float32) tensor of shape [N, M] with pairwise similarity score.\n    """"""\n    with tf.name_scope(scope, \'Compare\', [boxlist1, boxlist2]) as scope:\n      return self._compare(boxlist1, boxlist2)\n\n  @abstractmethod\n  def _compare(self, boxlist1, boxlist2):\n    pass\n\n\nclass IouSimilarity(RegionSimilarityCalculator):\n  """"""Class to compute similarity based on Intersection over Union (IOU) metric.\n\n  This class computes pairwise similarity between two BoxLists based on IOU.\n  """"""\n\n  def _compare(self, boxlist1, boxlist2):\n    """"""Compute pairwise IOU similarity between the two BoxLists.\n\n    Args:\n      boxlist1: BoxList holding N boxes.\n      boxlist2: BoxList holding M boxes.\n\n    Returns:\n      A tensor with shape [N, M] representing pairwise iou scores.\n    """"""\n    return iou(boxlist1, boxlist2)\n'"
tpu/models/official/retinanet/object_detection/shape_utils.py,4,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utils used to manipulate tensor shapes.""""""\n\nimport tensorflow as tf\n\n\ndef assert_shape_equal(shape_a, shape_b):\n  """"""Asserts that shape_a and shape_b are equal.\n\n  If the shapes are static, raises a ValueError when the shapes\n  mismatch.\n\n  If the shapes are dynamic, raises a tf InvalidArgumentError when the shapes\n  mismatch.\n\n  Args:\n    shape_a: a list containing shape of the first tensor.\n    shape_b: a list containing shape of the second tensor.\n\n  Returns:\n    Either a tf.no_op() when shapes are all static and a tf.assert_equal() op\n    when the shapes are dynamic.\n\n  Raises:\n    ValueError: When shapes are both static and unequal.\n  """"""\n  if (all(isinstance(dim, int) for dim in shape_a) and\n      all(isinstance(dim, int) for dim in shape_b)):\n    if shape_a != shape_b:\n      raise ValueError(\'Unequal shapes {}, {}\'.format(shape_a, shape_b))\n    else: return tf.no_op()\n  else:\n    return tf.assert_equal(shape_a, shape_b)\n\n\ndef combined_static_and_dynamic_shape(tensor):\n  """"""Returns a list containing static and dynamic values for the dimensions.\n\n  Returns a list of static and dynamic values for shape dimensions. This is\n  useful to preserve static shapes when available in reshape operation.\n\n  Args:\n    tensor: A tensor of any type.\n\n  Returns:\n    A list of size tensor.shape.ndims containing integers or a scalar tensor.\n  """"""\n  static_tensor_shape = tensor.shape.as_list()\n  dynamic_tensor_shape = tf.shape(tensor)\n  combined_shape = []\n  for index, dim in enumerate(static_tensor_shape):\n    if dim is not None:\n      combined_shape.append(dim)\n    else:\n      combined_shape.append(dynamic_tensor_shape[index])\n  return combined_shape\n'"
tpu/models/official/retinanet/object_detection/target_assigner.py,12,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Base target assigner module.\n\nThe job of a TargetAssigner is, for a given set of anchors (bounding boxes) and\ngroundtruth detections (bounding boxes), to assign classification and regression\ntargets to each anchor as well as weights to each anchor (specifying, e.g.,\nwhich anchors should not contribute to training loss).\n\nIt assigns classification/regression targets by performing the following steps:\n1) Computing pairwise similarity between anchors and groundtruth boxes using a\n  provided RegionSimilarity Calculator\n2) Computing a matching based on the similarity matrix using a provided Matcher\n3) Assigning regression targets based on the matching and a provided BoxCoder\n4) Assigning classification targets based on the matching and groundtruth labels\n\nNote that TargetAssigners only operate on detections from a single\nimage at a time, so any logic for applying a TargetAssigner to multiple\nimages must be handled externally.\n""""""\nimport tensorflow as tf\n\nfrom object_detection import box_list\nfrom object_detection import shape_utils\n\n\nKEYPOINTS_FIELD_NAME = \'keypoints\'\n\n\nclass TargetAssigner(object):\n  """"""Target assigner to compute classification and regression targets.""""""\n\n  def __init__(self, similarity_calc, matcher, box_coder,\n               negative_class_weight=1.0, unmatched_cls_target=None):\n    """"""Construct Object Detection Target Assigner.\n\n    Args:\n      similarity_calc: a RegionSimilarityCalculator\n      matcher: Matcher used to match groundtruth to anchors.\n      box_coder: BoxCoder used to encode matching groundtruth boxes with\n        respect to anchors.\n      negative_class_weight: classification weight to be associated to negative\n        anchors (default: 1.0). The weight must be in [0., 1.].\n      unmatched_cls_target: a float32 tensor with shape [d_1, d_2, ..., d_k]\n        which is consistent with the classification target for each\n        anchor (and can be empty for scalar targets).  This shape must thus be\n        compatible with the groundtruth labels that are passed to the ""assign""\n        function (which have shape [num_gt_boxes, d_1, d_2, ..., d_k]).\n        If set to None, unmatched_cls_target is set to be [0] for each anchor.\n\n    Raises:\n      ValueError: if similarity_calc is not a RegionSimilarityCalculator or\n        if matcher is not a Matcher or if box_coder is not a BoxCoder\n    """"""\n    self._similarity_calc = similarity_calc\n    self._matcher = matcher\n    self._box_coder = box_coder\n    self._negative_class_weight = negative_class_weight\n    if unmatched_cls_target is None:\n      self._unmatched_cls_target = tf.constant([0], tf.float32)\n    else:\n      self._unmatched_cls_target = unmatched_cls_target\n\n  @property\n  def box_coder(self):\n    return self._box_coder\n\n  def assign(self, anchors, groundtruth_boxes, groundtruth_labels=None,\n             groundtruth_weights=None, **params):\n    """"""Assign classification and regression targets to each anchor.\n\n    For a given set of anchors and groundtruth detections, match anchors\n    to groundtruth_boxes and assign classification and regression targets to\n    each anchor as well as weights based on the resulting match (specifying,\n    e.g., which anchors should not contribute to training loss).\n\n    Anchors that are not matched to anything are given a classification target\n    of self._unmatched_cls_target which can be specified via the constructor.\n\n    Args:\n      anchors: a BoxList representing N anchors\n      groundtruth_boxes: a BoxList representing M groundtruth boxes\n      groundtruth_labels:  a tensor of shape [M, d_1, ... d_k]\n        with labels for each of the ground_truth boxes. The subshape\n        [d_1, ... d_k] can be empty (corresponding to scalar inputs).  When set\n        to None, groundtruth_labels assumes a binary problem where all\n        ground_truth boxes get a positive label (of 1).\n      groundtruth_weights: a float tensor of shape [M] indicating the weight to\n        assign to all anchors match to a particular groundtruth box. The weights\n        must be in [0., 1.]. If None, all weights are set to 1.\n      **params: Additional keyword arguments for specific implementations of\n              the Matcher.\n\n    Returns:\n      cls_targets: a float32 tensor with shape [num_anchors, d_1, d_2 ... d_k],\n        where the subshape [d_1, ..., d_k] is compatible with groundtruth_labels\n        which has shape [num_gt_boxes, d_1, d_2, ... d_k].\n      cls_weights: a float32 tensor with shape [num_anchors]\n      reg_targets: a float32 tensor with shape [num_anchors, box_code_dimension]\n      reg_weights: a float32 tensor with shape [num_anchors]\n      match: a matcher.Match object encoding the match between anchors and\n        groundtruth boxes, with rows corresponding to groundtruth boxes\n        and columns corresponding to anchors.\n\n    Raises:\n      ValueError: if anchors or groundtruth_boxes are not of type\n        box_list.BoxList\n    """"""\n    if not isinstance(anchors, box_list.BoxList):\n      raise ValueError(\'anchors must be an BoxList\')\n    if not isinstance(groundtruth_boxes, box_list.BoxList):\n      raise ValueError(\'groundtruth_boxes must be an BoxList\')\n\n    if groundtruth_labels is None:\n      groundtruth_labels = tf.ones(tf.expand_dims(groundtruth_boxes.num_boxes(),\n                                                  0))\n      groundtruth_labels = tf.expand_dims(groundtruth_labels, -1)\n    unmatched_shape_assert = shape_utils.assert_shape_equal(\n        shape_utils.combined_static_and_dynamic_shape(groundtruth_labels)[1:],\n        shape_utils.combined_static_and_dynamic_shape(\n            self._unmatched_cls_target))\n    labels_and_box_shapes_assert = shape_utils.assert_shape_equal(\n        shape_utils.combined_static_and_dynamic_shape(\n            groundtruth_labels)[:1],\n        shape_utils.combined_static_and_dynamic_shape(\n            groundtruth_boxes.get())[:1])\n\n    if groundtruth_weights is None:\n      num_gt_boxes = groundtruth_boxes.num_boxes_static()\n      if not num_gt_boxes:\n        num_gt_boxes = groundtruth_boxes.num_boxes()\n      groundtruth_weights = tf.ones([num_gt_boxes], dtype=tf.float32)\n    with tf.control_dependencies(\n        [unmatched_shape_assert, labels_and_box_shapes_assert]):\n      match_quality_matrix = self._similarity_calc.compare(groundtruth_boxes,\n                                                           anchors)\n      match = self._matcher.match(match_quality_matrix, **params)\n      reg_targets = self._create_regression_targets(anchors,\n                                                    groundtruth_boxes,\n                                                    match)\n      cls_targets = self._create_classification_targets(groundtruth_labels,\n                                                        match)\n      reg_weights = self._create_regression_weights(match, groundtruth_weights)\n      cls_weights = self._create_classification_weights(match,\n                                                        groundtruth_weights)\n\n    num_anchors = anchors.num_boxes_static()\n    if num_anchors is not None:\n      reg_targets = self._reset_target_shape(reg_targets, num_anchors)\n      cls_targets = self._reset_target_shape(cls_targets, num_anchors)\n      reg_weights = self._reset_target_shape(reg_weights, num_anchors)\n      cls_weights = self._reset_target_shape(cls_weights, num_anchors)\n\n    return cls_targets, cls_weights, reg_targets, reg_weights, match\n\n  def _reset_target_shape(self, target, num_anchors):\n    """"""Sets the static shape of the target.\n\n    Args:\n      target: the target tensor. Its first dimension will be overwritten.\n      num_anchors: the number of anchors, which is used to override the target\'s\n        first dimension.\n\n    Returns:\n      A tensor with the shape info filled in.\n    """"""\n    target_shape = target.get_shape().as_list()\n    target_shape[0] = num_anchors\n    target.set_shape(target_shape)\n    return target\n\n  def _create_regression_targets(self, anchors, groundtruth_boxes, match):\n    """"""Returns a regression target for each anchor.\n\n    Args:\n      anchors: a BoxList representing N anchors\n      groundtruth_boxes: a BoxList representing M groundtruth_boxes\n      match: a matcher.Match object\n\n    Returns:\n      reg_targets: a float32 tensor with shape [N, box_code_dimension]\n    """"""\n    matched_gt_boxes = match.gather_based_on_match(\n        groundtruth_boxes.get(),\n        unmatched_value=tf.zeros(4),\n        ignored_value=tf.zeros(4))\n    matched_gt_boxlist = box_list.BoxList(matched_gt_boxes)\n    if groundtruth_boxes.has_field(KEYPOINTS_FIELD_NAME):\n      groundtruth_keypoints = groundtruth_boxes.get_field(KEYPOINTS_FIELD_NAME)\n      matched_keypoints = match.gather_based_on_match(\n          groundtruth_keypoints,\n          unmatched_value=tf.zeros(groundtruth_keypoints.get_shape()[1:]),\n          ignored_value=tf.zeros(groundtruth_keypoints.get_shape()[1:]))\n      matched_gt_boxlist.add_field(KEYPOINTS_FIELD_NAME, matched_keypoints)\n    matched_reg_targets = self._box_coder.encode(matched_gt_boxlist, anchors)\n    match_results_shape = shape_utils.combined_static_and_dynamic_shape(\n        match.match_results)\n\n    # Zero out the unmatched and ignored regression targets.\n    unmatched_ignored_reg_targets = tf.tile(\n        self._default_regression_target(), [match_results_shape[0], 1])\n    matched_anchors_mask = match.matched_column_indicator()\n    reg_targets = tf.where(matched_anchors_mask,\n                           matched_reg_targets,\n                           unmatched_ignored_reg_targets)\n    return reg_targets\n\n  def _default_regression_target(self):\n    """"""Returns the default target for anchors to regress to.\n\n    Default regression targets are set to zero (though in\n    this implementation what these targets are set to should\n    not matter as the regression weight of any box set to\n    regress to the default target is zero).\n\n    Returns:\n      default_target: a float32 tensor with shape [1, box_code_dimension]\n    """"""\n    return tf.constant([self._box_coder.code_size*[0]], tf.float32)\n\n  def _create_classification_targets(self, groundtruth_labels, match):\n    """"""Create classification targets for each anchor.\n\n    Assign a classification target of for each anchor to the matching\n    groundtruth label that is provided by match.  Anchors that are not matched\n    to anything are given the target self._unmatched_cls_target\n\n    Args:\n      groundtruth_labels:  a tensor of shape [num_gt_boxes, d_1, ... d_k]\n        with labels for each of the ground_truth boxes. The subshape\n        [d_1, ... d_k] can be empty (corresponding to scalar labels).\n      match: a matcher.Match object that provides a matching between anchors\n        and groundtruth boxes.\n\n    Returns:\n      a float32 tensor with shape [num_anchors, d_1, d_2 ... d_k], where the\n      subshape [d_1, ..., d_k] is compatible with groundtruth_labels which has\n      shape [num_gt_boxes, d_1, d_2, ... d_k].\n    """"""\n    return match.gather_based_on_match(\n        groundtruth_labels,\n        unmatched_value=self._unmatched_cls_target,\n        ignored_value=self._unmatched_cls_target)\n\n  def _create_regression_weights(self, match, groundtruth_weights):\n    """"""Set regression weight for each anchor.\n\n    Only positive anchors are set to contribute to the regression loss, so this\n    method returns a weight of 1 for every positive anchor and 0 for every\n    negative anchor.\n\n    Args:\n      match: a matcher.Match object that provides a matching between anchors\n        and groundtruth boxes.\n      groundtruth_weights: a float tensor of shape [M] indicating the weight to\n        assign to all anchors match to a particular groundtruth box.\n\n    Returns:\n      a float32 tensor with shape [num_anchors] representing regression weights.\n    """"""\n    return match.gather_based_on_match(\n        groundtruth_weights, ignored_value=0., unmatched_value=0.)\n\n  def _create_classification_weights(self,\n                                     match,\n                                     groundtruth_weights):\n    """"""Create classification weights for each anchor.\n\n    Positive (matched) anchors are associated with a weight of\n    positive_class_weight and negative (unmatched) anchors are associated with\n    a weight of negative_class_weight. When anchors are ignored, weights are set\n    to zero. By default, both positive/negative weights are set to 1.0,\n    but they can be adjusted to handle class imbalance (which is almost always\n    the case in object detection).\n\n    Args:\n      match: a matcher.Match object that provides a matching between anchors\n        and groundtruth boxes.\n      groundtruth_weights: a float tensor of shape [M] indicating the weight to\n        assign to all anchors match to a particular groundtruth box.\n\n    Returns:\n      a float32 tensor with shape [num_anchors] representing classification\n      weights.\n    """"""\n    return match.gather_based_on_match(\n        groundtruth_weights,\n        ignored_value=0.,\n        unmatched_value=self._negative_class_weight)\n\n  def get_box_coder(self):\n    """"""Get BoxCoder of this TargetAssigner.\n\n    Returns:\n      BoxCoder object.\n    """"""\n    return self._box_coder\n'"
tpu/models/official/retinanet/object_detection/tf_example_decoder.py,35,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tensorflow Example proto decoder for object detection.\n\nA decoder to decode string tensors containing serialized tensorflow.Example\nprotos for object detection.\n""""""\nimport tensorflow as tf\n\n\nslim_example_decoder = tf.contrib.slim.tfexample_decoder\n\n\nclass TfExampleDecoder(object):\n  """"""Tensorflow Example proto decoder.""""""\n\n  def __init__(self):\n    """"""Constructor sets keys_to_features and items_to_handlers.""""""\n    self.keys_to_features = {\n        \'image/encoded\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\':\n            tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/filename\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/key/sha256\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/source_id\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/height\':\n            tf.FixedLenFeature((), tf.int64, 1),\n        \'image/width\':\n            tf.FixedLenFeature((), tf.int64, 1),\n        # Object boxes and classes.\n        \'image/object/bbox/xmin\':\n            tf.VarLenFeature(tf.float32),\n        \'image/object/bbox/xmax\':\n            tf.VarLenFeature(tf.float32),\n        \'image/object/bbox/ymin\':\n            tf.VarLenFeature(tf.float32),\n        \'image/object/bbox/ymax\':\n            tf.VarLenFeature(tf.float32),\n        \'image/object/class/label\':\n            tf.VarLenFeature(tf.int64),\n        \'image/object/class/text\':\n            tf.VarLenFeature(tf.string),\n        \'image/object/area\':\n            tf.VarLenFeature(tf.float32),\n        \'image/object/is_crowd\':\n            tf.VarLenFeature(tf.int64),\n        \'image/object/difficult\':\n            tf.VarLenFeature(tf.int64),\n        \'image/object/group_of\':\n            tf.VarLenFeature(tf.int64),\n        \'image/object/weight\':\n            tf.VarLenFeature(tf.float32),\n    }\n    self.items_to_handlers = {\n        \'image\': slim_example_decoder.Image(\n            image_key=\'image/encoded\', format_key=\'image/format\', channels=3),\n        \'source_id\': (\n            slim_example_decoder.Tensor(\'image/source_id\')),\n        \'key\': (\n            slim_example_decoder.Tensor(\'image/key/sha256\')),\n        \'filename\': (\n            slim_example_decoder.Tensor(\'image/filename\')),\n        # Object boxes and classes.\n        \'groundtruth_boxes\': (\n            slim_example_decoder.BoundingBox(\n                [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\')),\n        \'groundtruth_area\': slim_example_decoder.Tensor(\n            \'image/object/area\'),\n        \'groundtruth_is_crowd\': (\n            slim_example_decoder.Tensor(\'image/object/is_crowd\')),\n        \'groundtruth_difficult\': (\n            slim_example_decoder.Tensor(\'image/object/difficult\')),\n        \'groundtruth_group_of\': (\n            slim_example_decoder.Tensor(\'image/object/group_of\')),\n        \'groundtruth_weights\': (\n            slim_example_decoder.Tensor(\'image/object/weight\')),\n    }\n    label_handler = slim_example_decoder.Tensor(\'image/object/class/label\')\n    self.items_to_handlers[\'groundtruth_classes\'] = label_handler\n\n  def decode(self, tf_example_string_tensor):\n    """"""Decodes serialized tensorflow example and returns a tensor dictionary.\n\n    Args:\n      tf_example_string_tensor: a string tensor holding a serialized tensorflow\n        example proto.\n\n    Returns:\n      A dictionary of the following tensors.\n      image - 3D uint8 tensor of shape [None, None, 3]\n        containing image.\n      source_id - string tensor containing original\n        image id.\n      key - string tensor with unique sha256 hash key.\n      filename - string tensor with original dataset\n        filename.\n      groundtruth_boxes - 2D float32 tensor of shape\n        [None, 4] containing box corners.\n      groundtruth_classes - 1D int64 tensor of shape\n      groundtruth_weights - 1D float32 tensor of\n        shape [None] indicating the weights of groundtruth boxes.\n        [None] containing classes for the boxes.\n      groundtruth_area - 1D float32 tensor of shape\n        [None] containing containing object mask area in pixel squared.\n      groundtruth_is_crowd - 1D bool tensor of shape\n        [None] indicating if the boxes enclose a crowd.\n\n    Optional:\n      groundtruth_difficult - 1D bool tensor of shape\n        [None] indicating if the boxes represent `difficult` instances.\n      groundtruth_group_of - 1D bool tensor of shape\n        [None] indicating if the boxes represent `group_of` instances.\n      groundtruth_instance_masks - 3D float32 tensor of\n        shape [None, None, None] containing instance masks.\n    """"""\n    serialized_example = tf.reshape(tf_example_string_tensor, shape=[])\n    decoder = slim_example_decoder.TFExampleDecoder(self.keys_to_features,\n                                                    self.items_to_handlers)\n    keys = sorted(decoder.list_items())\n\n    tensors = decoder.decode(serialized_example, items=keys)\n    tensor_dict = dict(zip(keys, tensors))\n    is_crowd = \'groundtruth_is_crowd\'\n    tensor_dict[is_crowd] = tf.cast(tensor_dict[is_crowd], dtype=tf.bool)\n    tensor_dict[\'image\'].set_shape([None, None, 3])\n\n    def default_groundtruth_weights():\n      return tf.ones(\n          tf.shape(tensor_dict[\'groundtruth_boxes\'])[0],\n          dtype=tf.float32)\n\n    tensor_dict[\'groundtruth_weights\'] = tf.cond(\n        tf.greater(\n            tf.shape(\n                tensor_dict[\'groundtruth_weights\'])[0],\n            0), lambda: tensor_dict[\'groundtruth_weights\'],\n        default_groundtruth_weights)\n    return tensor_dict\n\n\nclass TfExampleSegmentationDecoder(object):\n  """"""Tensorflow Example proto decoder.""""""\n\n  def __init__(self):\n    """"""Constructor sets keys_to_features and items_to_handlers.""""""\n    self.keys_to_features = {\n        \'image/encoded\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/filename\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\':\n            tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/height\':\n            tf.FixedLenFeature((), tf.int64, default_value=0),\n        \'image/width\':\n            tf.FixedLenFeature((), tf.int64, default_value=0),\n        \'image/segmentation/class/encoded\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/segmentation/class/format\':\n            tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n    }\n    self.items_to_handlers = {\n        \'image\': slim_example_decoder.Image(\n            image_key=\'image/encoded\', format_key=\'image/format\', channels=3),\n        \'labels_class\': slim_example_decoder.Image(\n            image_key=\'image/segmentation/class/encoded\',\n            format_key=\'image/segmentation/class/format\',\n            channels=1)\n    }\n\n  def decode(self, tf_example_string_tensor):\n    """"""Decodes serialized tensorflow example and returns a tensor dictionary.\n\n    Args:\n      tf_example_string_tensor: a string tensor holding a serialized tensorflow\n        example proto.\n\n    Returns:\n      A dictionary of the following tensors.\n      image - 3D uint8 tensor of shape [None, None, 3] containing image.\n      labels_class - 2D unit8 tensor of shape [None, None] containing\n        pixel-wise class labels.\n    """"""\n    serialized_example = tf.reshape(tf_example_string_tensor, shape=[])\n    decoder = slim_example_decoder.TFExampleDecoder(self.keys_to_features,\n                                                    self.items_to_handlers)\n    keys = sorted(decoder.list_items())\n    keys = [\'image\', \'labels_class\']\n\n    tensors = decoder.decode(serialized_example, items=keys)\n    tensor_dict = dict(zip(keys, tensors))\n    tensor_dict[\'image\'].set_shape([None, None, 3])\n    return tensor_dict\n'"
