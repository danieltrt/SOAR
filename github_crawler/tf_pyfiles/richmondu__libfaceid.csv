file_path,api_count,code
agegenderemotion_webcam.py,0,"b'import sys\nimport argparse\nimport cv2\nimport datetime\nfrom libfaceid.detector import FaceDetectorModels, FaceDetector\nfrom libfaceid.encoder  import FaceEncoderModels, FaceEncoder\nfrom libfaceid.pose import FacePoseEstimatorModels, FacePoseEstimator\nfrom libfaceid.age import FaceAgeEstimatorModels, FaceAgeEstimator\nfrom libfaceid.gender import FaceGenderEstimatorModels, FaceGenderEstimator\nfrom libfaceid.emotion import FaceEmotionEstimatorModels, FaceEmotionEstimator\n\n\n\n# Set the window name\nWINDOW_NAME = ""Facial_Recognition""\n\n# Set the input directories\nINPUT_DIR_DATASET               = ""datasets""\nINPUT_DIR_MODEL_DETECTION       = ""models/detection/""\nINPUT_DIR_MODEL_ENCODING        = ""models/encoding/""\nINPUT_DIR_MODEL_TRAINING        = ""models/training/""\nINPUT_DIR_MODEL_ESTIMATION      = ""models/estimation/""\n\n# Set width and height\nRESOLUTION_QVGA   = (320, 240)\nRESOLUTION_VGA    = (640, 480)\nRESOLUTION_HD     = (1280, 720)\nRESOLUTION_FULLHD = (1920, 1080)\n\n\n\ndef cam_init(cam_index, width, height): \n    cap = cv2.VideoCapture(cam_index)\n    if sys.version_info < (3, 0):\n        cap.set(cv2.cv.CV_CAP_PROP_FPS, 30)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT, height)\n    else:\n        cap.set(cv2.CAP_PROP_FPS, 30)\n        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n    return cap\n\n\ndef label_face(frame, face_rect, face_id, confidence):\n    (x, y, w, h) = face_rect\n    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 255), 1)\n    if face_id is not None:\n        cv2.putText(frame, ""{} {:.2f}%"".format(face_id, confidence), \n            (x+5,y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n\ndef process_facedetection(model_detector, model_poseestimator, model_ageestimator, model_genderestimator, model_emotionestimator, cam_resolution, cam_index):\n\n    # Initialize the camera\n    camera = cam_init(cam_index, cam_resolution[0], cam_resolution[1])\n\n    try:\n        # Initialize face detection\n        face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION, minfacesize=120)\n        # Initialize face pose/age/gender estimation\n        face_pose_estimator = FacePoseEstimator(model=model_poseestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n        face_age_estimator = FaceAgeEstimator(model=model_ageestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n        face_gender_estimator = FaceGenderEstimator(model=model_genderestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n        face_emotion_estimator = FaceEmotionEstimator(model=model_emotionestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n    except:\n        print(""Warning, check if models and trained dataset models exists!"")\n    (age, gender, emotion) = (None, None, None)\n\n\n    while (True):\n\n        # Capture frame from webcam\n        ret, frame = camera.read()\n        if frame is None:\n            print(""Error, check if camera is connected!"")\n            break\n\n\n        # Detect and identify faces in the frame\n        faces = face_detector.detect(frame)\n        for (index, face) in enumerate(faces):\n            (x, y, w, h) = face\n\n            # Detect age, gender, emotion\n            face_image = frame[y:y+h, h:h+w]\n            age = face_age_estimator.estimate(frame, face_image)\n            gender = face_gender_estimator.estimate(frame, face_image)\n            emotion = face_emotion_estimator.estimate(frame, face_image)\n\n            # Detect and draw face pose locations\n            shape = face_pose_estimator.detect(frame, face)\n            face_pose_estimator.add_overlay(frame, shape)\n\n            # Display age, gender, emotion\n            if True: # Added condition to easily disable text\n                cv2.putText(frame, ""Age: {}"".format(age), \n                    (x, y-45), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n                cv2.putText(frame, ""Gender: {}"".format(gender), \n                    (x, y-30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n                cv2.putText(frame, ""Emotion: {}"".format(emotion), \n                    (x, y-15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n\n        # Display the resulting frame\n        cv2.imshow(WINDOW_NAME, frame)\n\n        # Check for user actions\n        keyPressed = cv2.waitKey(1) & 0xFF\n        if keyPressed == 27: # ESC\n            break\n        elif keyPressed == 13: # Enter\n            cv2.imwrite(WINDOW_NAME + ""_"" + datetime.datetime.now().strftime(""%Y%m%d_%H%M%S"") + "".jpg"", frame);\n\n\n    # Release the camera\n    camera.release()\n    cv2.destroyAllWindows()\n\n\ndef run(cam_index, cam_resolution):\n\n#    detector=FaceDetectorModels.HAARCASCADE\n#    detector=FaceDetectorModels.DLIBHOG\n#    detector=FaceDetectorModels.DLIBCNN\n#    detector=FaceDetectorModels.SSDRESNET\n    detector=FaceDetectorModels.MTCNN\n#    detector=FaceDetectorModels.FACENET\n\n    encoder=FaceEncoderModels.LBPH\n#    encoder=FaceEncoderModels.OPENFACE\n#    encoder=FaceEncoderModels.DLIBRESNET\n#    encoder=FaceEncoderModels.FACENET\n\n    poseestimator    = FacePoseEstimatorModels.DLIB68\n    ageestimator     = FaceAgeEstimatorModels.CV2CAFFE\n    genderestimator  = FaceGenderEstimatorModels.CV2CAFFE\n    emotionestimator = FaceEmotionEstimatorModels.KERAS\n\n    process_facedetection(\n        detector, \n        poseestimator, \n        ageestimator, \n        genderestimator,\n        emotionestimator,\n        cam_resolution, \n        cam_index)\n\n\ndef main(args):\n    if sys.version_info < (3, 0):\n        print(""Error: Python2 is slow. Use Python3 for max performance."")\n        return\n\n    cam_index = int(args.webcam)\n    resolutions = [ RESOLUTION_QVGA, RESOLUTION_VGA, RESOLUTION_HD, RESOLUTION_FULLHD ]\n    try:\n        cam_resolution = resolutions[int(args.resolution)]\n    except:\n        cam_resolution = RESOLUTION_QVGA\n\n    if args.detector:\n        try:\n            detector = FaceDetectorModels(int(args.detector))\n            print( ""Parameters: {}"".format(detector) )\n            process_facedetection(\n                detector, \n                FacePoseEstimatorModels.DEFAULT, \n                FaceAgeEstimatorModels.DEFAULT, \n                FaceGenderEstimatorModels.DEFAULT,\n                FaceEmotionEstimatorModels.DEFAULT,\n                cam_resolution, \n                cam_index)\n        except:\n            print( ""Invalid parameter"" )\n        return\n    run(cam_index, cam_resolution)\n\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--detector\', required=False,\n        help=\'Detector model to use. Options: 0-HAARCASCADE, 1-DLIBHOG, 2-DLIBCNN, 3-SSDRESNET, 4-MTCNN, 5-FACENET\')\n    parser.add_argument(\'--webcam\', required=False, default=0, \n        help=\'Camera index to use. Default is 0. Assume only 1 camera connected.)\')\n    parser.add_argument(\'--resolution\', required=False, default=0,\n        help=\'Camera resolution to use. Default is 0. Options: 0-QVGA, 1-VGA, 2-HD, 3-FULLHD\')\n    return parser.parse_args(argv)\n\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
agegenderemotion_webcam_flask.py,0,"b'import sys\nimport argparse\nimport cv2\nfrom libfaceid.detector import FaceDetectorModels, FaceDetector\nfrom libfaceid.encoder  import FaceEncoderModels, FaceEncoder\nfrom libfaceid.pose import FacePoseEstimatorModels, FacePoseEstimator\nfrom libfaceid.age import FaceAgeEstimatorModels, FaceAgeEstimator\nfrom libfaceid.gender import FaceGenderEstimatorModels, FaceGenderEstimator\nfrom libfaceid.emotion import FaceEmotionEstimatorModels, FaceEmotionEstimator\n\n\n# Use flask for web app\nfrom flask import Flask, render_template, Response\napp = Flask(__name__)\n\n\n# Set the input directories\nINPUT_DIR_DATASET               = ""datasets""\nINPUT_DIR_MODEL_DETECTION       = ""models/detection/""\nINPUT_DIR_MODEL_ENCODING        = ""models/encoding/""\nINPUT_DIR_MODEL_TRAINING        = ""models/training/""\nINPUT_DIR_MODEL_ESTIMATION      = ""models/estimation/""\n\n# Set width and height\nRESOLUTION_QVGA   = (320, 240)\nRESOLUTION_VGA    = (640, 480)\nRESOLUTION_HD     = (1280, 720)\nRESOLUTION_FULLHD = (1920, 1080)\n\n\n\ndef cam_init(cam_index, width, height): \n    cap = cv2.VideoCapture(cam_index)\n    if sys.version_info < (3, 0):\n        cap.set(cv2.cv.CV_CAP_PROP_FPS, 30)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT, height)\n    else:\n        cap.set(cv2.CAP_PROP_FPS, 30)\n        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n    return cap\n\n\ndef label_face(frame, face_rect, face_id, confidence):\n    (x, y, w, h) = face_rect\n    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 255), 1)\n    if face_id is not None:\n        cv2.putText(frame, ""{} {:.2f}%"".format(face_id, confidence), \n            (x+5,y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n\ndef process_facedetection():\n\n    cam_index = 0\n    cam_resolution = RESOLUTION_QVGA\n    model_detector=FaceDetectorModels.HAARCASCADE\n#    model_detector=FaceDetectorModels.DLIBHOG\n#    model_detector=FaceDetectorModels.DLIBCNN\n#    model_detector=FaceDetectorModels.SSDRESNET\n#    model_detector=FaceDetectorModels.MTCNN\n#    model_detector=FaceDetectorModels.FACENET\n\n    model_poseestimator=FacePoseEstimatorModels.DEFAULT\n    model_ageestimator=FaceAgeEstimatorModels.DEFAULT\n    model_genderestimator=FaceGenderEstimatorModels.DEFAULT\n    model_emotionestimator=FaceEmotionEstimatorModels.DEFAULT\n\n\n    # Initialize the camera\n    camera = cam_init(cam_index, cam_resolution[0], cam_resolution[1])\n\n    try:\n        # Initialize face detection\n        face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION)#, optimize=True)\n        # Initialize face pose/age/gender estimation\n        face_pose_estimator = FacePoseEstimator(model=model_poseestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n        face_age_estimator = FaceAgeEstimator(model=model_ageestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n        face_gender_estimator = FaceGenderEstimator(model=model_genderestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n        face_emotion_estimator = FaceEmotionEstimator(model=model_emotionestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n    except:\n        print(""Warning, check if models and trained dataset models exists!"")\n    (age, gender, emotion) = (None, None, None)\n\n\n    while (True):\n\n        # Capture frame from webcam\n        ret, frame = camera.read()\n        if frame is None:\n            print(""Error, check if camera is connected!"")\n            break\n\n\n        # Detect and identify faces in the frame\n        faces = face_detector.detect(frame)\n        for (index, face) in enumerate(faces):\n            (x, y, w, h) = face\n\n            # Detect age, gender, emotion\n            face_image = frame[y:y+h, h:h+w]\n            age = face_age_estimator.estimate(frame, face_image)\n            gender = face_gender_estimator.estimate(frame, face_image)\n            emotion = face_emotion_estimator.estimate(frame, face_image)\n\n            # Detect and draw face pose locations\n            shape = face_pose_estimator.detect(frame, face)\n            face_pose_estimator.add_overlay(frame, shape)\n\n            # Display age, gender, emotion\n            cv2.putText(frame, ""Age: {}"".format(age), \n                (x, y-45), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n            cv2.putText(frame, ""Gender: {}"".format(gender), \n                (x, y-30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n            cv2.putText(frame, ""Emotion: {}"".format(emotion), \n                (x, y-15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n\n        # Display updated frame to web app\n        yield (b\'--frame\\r\\nContent-Type: image/jpeg\\r\\n\\r\\n\' + cv2.imencode(\'.jpg\', frame)[1].tobytes() + b\'\\r\\n\\r\\n\')\n\n    # Release the camera\n    camera.release()\n    cv2.destroyAllWindows()\n\n\n# Initialize for web app\n@app.route(\'/\')\ndef index():\n    return render_template(\'web_app_flask.html\')\n\n# Entry point for web app\n@app.route(\'/video_viewer\')\ndef video_viewer():\n    return Response(process_facedetection(), mimetype=\'multipart/x-mixed-replace; boundary=frame\')\n\n\nif __name__ == \'__main__\':\n    print(""\\n\\nNote: Open browser and type http://127.0.0.1:5000/ or http://ip_address:5000/ \\n\\n"")\n    # Run flask for web app\n    app.run(host=\'0.0.0.0\', threaded=True, debug=True)\n'"
enrollment.py,0,"b'import sys\nimport argparse\nimport cv2\nimport numpy as np\nimport os\nimport datetime\nimport math\nimport imutils\nfrom libfaceid.detector import FaceDetectorModels, FaceDetector\nfrom libfaceid.encoder  import FaceEncoderModels, FaceEncoder\nfrom libfaceid.classifier import FaceClassifierModels\n\n\n\n# Set the window name\nWINDOW_NAME = ""Facial_Recognition""\n\n# Set the input directories\nINPUT_DIR_DATASET               = ""datasets""\nINPUT_DIR_MODEL_DETECTION       = ""models/detection/""\nINPUT_DIR_MODEL_ENCODING        = ""models/encoding/""\nINPUT_DIR_MODEL_TRAINING        = ""models/training/""\nINPUT_DIR_MODEL_ESTIMATION      = ""models/estimation/""\n\n# Set width and height\nRESOLUTION_QVGA   = (320, 240)\nRESOLUTION_VGA    = (640, 480)\nRESOLUTION_HD     = (1280, 720)\nRESOLUTION_FULLHD = (1920, 1080)\n\n\n\ndef cam_init(cam_index, width, height): \n    cap = cv2.VideoCapture(cam_index)\n    if sys.version_info < (3, 0):\n        cap.set(cv2.cv.CV_CAP_PROP_FPS, 30)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT, height)\n    else:\n        cap.set(cv2.CAP_PROP_FPS, 30)\n        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n    return cap\n\n\ndef ensure_directory(file_path):\n    directory = os.path.dirname(""./"" + file_path)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n\ndef label_face(frame, face_rect, face_id=None, confidence=0):\n    (x, y, w, h) = face_rect\n    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 255), 1)\n    if face_id is not None:\n        cv2.putText(frame, ""{} {:.2f}%"".format(face_id, confidence), \n            (x+5,y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n\ndef save_video(saveVideo, out, resolution, filename):\n    if saveVideo == True:\n        print(""video recording ended!"")\n        out.release()\n        out = None\n        saveVideo = False\n    else:\n        print(""video recording started..."")\n        print(""Press space key to stop recording!"")\n        fourcc = cv2.VideoWriter_fourcc(\'M\', \'J\', \'P\', \'G\')\n        (h, w) = resolution\n        out = cv2.VideoWriter(filename, fourcc, 12, (w, h))\n        saveVideo = True\n    return saveVideo, out\n\n# inner\ns1={}\nfor i in range(120):\n    for j in range(2):\n        if (j%2==0):\n            s1[i,j]=int(160+105*math.cos(3.0*i*3.14/180))\n        else:\n            s1[i,j]=int(120+105*math.sin(3.0*i*3.14/180))\n        #print(s1[i, j]) \n# outer\ns2={}\nfor i in range(120):\n    for j in range(2):\n        if (j%2==0):\n            s2[i,j]=int(160+120*math.cos(3.0*i*3.14/180))\n        else:\n            s2[i,j]=int(120+120*math.sin(3.0*i*3.14/180))\n        #print(s2[i, j]) \n\ndef process_faceenrollment(model_detector, cam_index, cam_resolution):\n\n    # Initialize the camera\n    camera = cam_init(cam_index, cam_resolution[0], cam_resolution[1])\n\n    try:\n        # Initialize face detection\n        face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION)\n    except:\n        print(""Warning, check if models and trained dataset models exists!"")\n\n    print("""")\n    print(""Press SPACEBAR to record video or ENTER to capture picture!"")\n    print(""Make sure that your face is inside the circular region!"")\n    print("""")\n\n    saveVideo = False\n    out = None\n    color_recording = (255,255,255)\n    is_windows = (os.name == \'nt\')\n    \n\n    while (True):\n\n        # Capture frame from webcam\n        ret, frame = camera.read()\n        if frame is None:\n            print(""Error, check if camera is connected!"")\n            break\n\n        # Detect and identify faces in the frame\n        faces = face_detector.detect(frame)\n        for (index, face) in enumerate(faces):\n            (x, y, w, h) = face\n            #print(""{} {} {} {}"".format(x,y,w,h))\n\n            if saveVideo and len(faces) == 1:\n                out.write(frame)\n\n            # Set text and bounding box on face\n            label_face(frame, (x, y, w, h))\n\n            # Process 1 face only\n            break\n\n\n        mask = np.full((frame.shape[0], frame.shape[1]), 0, dtype=np.uint8)  # mask is only\n        cv2.circle(mask, (int(cam_resolution[0]/2),int(cam_resolution[1]/2)), 110, (255,255,255), -1, cv2.LINE_AA)\n        fg = cv2.bitwise_or(frame, frame, mask=mask)\n        cv2.circle(fg, (int(cam_resolution[0]/2),int(cam_resolution[1]/2)), 110, color_recording, 15, cv2.LINE_AA)\n        for i in range(120):\n            cv2.line(fg, (s1[i,0], s1[i,1]), (s2[i,0], s2[i,1]), (0, 0, 0), 2, cv2.LINE_AA)\n\n        # Display updated frame\n        if is_windows:\n            fg = imutils.resize(fg, height=480)\n        cv2.imshow(WINDOW_NAME, fg)\n\n        # Check for user actions\n        keyPressed = cv2.waitKey(1) & 0xFF\n        if keyPressed == 27: # ESC to exit\n            break\n        elif keyPressed == 32: # Space to save video\n            saveVideo, out = save_video(saveVideo, out, frame.shape[:2], WINDOW_NAME + "".avi"")\n            if out is not None:\n                color_recording = (0, 255, 0)\n            else:\n                color_recording = (0, 0, 0)\n                break\n        elif keyPressed == 13: # Enter to capture picture\n            cv2.imwrite(WINDOW_NAME + ""_"" + datetime.datetime.now().strftime(""%Y%m%d_%H%M%S"") + "".jpg"", frame);\n\n\n    # Release the camera\n    camera.release()\n    cv2.destroyAllWindows()\n\n\ndef video_to_images(model_detector, dir, name, one_image_only=False):\n\n    ensure_directory(dir + ""/"" + name + ""/"")\n\n    try:\n        video = cv2.VideoCapture(WINDOW_NAME + "".avi"")\n        if video is None:\n            return\n    except:\n        return\n\n    try:\n        # Initialize face detection\n        face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION)\n    except:\n        print(""Warning, check if models and trained dataset models exists!"")\n\n    i = 1\n    while (True):\n\n        ret, frame = video.read()\n        if frame is None:\n            break\n\n        faces = face_detector.detect(frame)\n        if len(faces) == 1:\n            cv2.imwrite(""{}/{}/{}.jpg"".format(dir, name, i), frame);\n            i += 1\n            if one_image_only:\n                break\n            \n        #cv2.imshow(WINDOW_NAME, frame)\n        #cv2.waitKey(1)\n\n    video.release()\n    cv2.destroyAllWindows()\n\n\n\ndef run(cam_index, cam_resolution, name):\n#    detector=FaceDetectorModels.HAARCASCADE\n#    detector=FaceDetectorModels.DLIBHOG\n#    detector=FaceDetectorModels.DLIBCNN\n#    detector=FaceDetectorModels.SSDRESNET\n    detector=FaceDetectorModels.MTCNN\n#    detector=FaceDetectorModels.FACENET\n\n    process_faceenrollment(detector, cam_index, cam_resolution)\n\n    print("""")\n    print(""Processing of video recording started..."")\n#    video_to_images(detector, ""x"" + INPUT_DIR_DATASET, name)\n#    video_to_images(detector, INPUT_DIR_DATASET, name, one_image_only=True)\n    video_to_images(detector, INPUT_DIR_DATASET, name)\n    print(""Processing of video recording completed!"")\n    print(""Make sure to train the new datasets before testing!"")\n    print("""")\n\n\ndef main(args):\n    if sys.version_info < (3, 0):\n        print(""Error: Python2 is slow. Use Python3 for max performance."")\n        return\n\n    cam_index = int(args.webcam)\n    resolutions = [ RESOLUTION_QVGA, RESOLUTION_VGA, RESOLUTION_HD, RESOLUTION_FULLHD ]\n    try:\n        cam_resolution = resolutions[int(args.resolution)]\n    except:\n        cam_resolution = RESOLUTION_VGA\n\n    if args.detector and args.name:\n        try:\n            detector = FaceDetectorModels(int(args.detector))\n            name = str(args.name)\n            print( ""Parameters: {}"".format(detector))\n\n            process_faceenrollment(detector, cam_index, cam_resolution)\n\n            print("""")\n            print(""Processing of video recording started..."")\n            #video_to_images(detector, ""x"" + INPUT_DIR_DATASET, name)\n            #video_to_images(detector, INPUT_DIR_DATASET, name, one_image_only=True)\n            video_to_images(detector, INPUT_DIR_DATASET, name)\n            print(""Processing of video recording completed!"")\n            print(""Make sure to train the new datasets before testing!"")\n            print("""")\n        except:\n            print( ""Invalid parameter"" )\n        return\n    run(cam_index, cam_resolution, str(args.name))\n\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--detector\', required=False, default=0,\n        help=\'Detector model to use. Options: 0-HAARCASCADE, 1-DLIBHOG, 2-DLIBCNN, 3-SSDRESNET, 4-MTCNN, 5-FACENET\')\n    parser.add_argument(\'--webcam\', required=False, default=0, \n        help=\'Camera index to use. Default is 0. Assume only 1 camera connected.)\')\n    parser.add_argument(\'--resolution\', required=False, default=0,\n        help=\'Camera resolution to use. Default is 0. Options: 0-QVGA, 1-VGA, 2-HD, 3-FULLHD\')\n    parser.add_argument(\'--name\', required=False, default=""Unknown"",\n        help=\'Name of person to enroll\')\n    return parser.parse_args(argv)\n\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
facial_recognition.py,0,"b'import os\nimport sys\nfrom time import time\nimport datetime\nimport argparse\nimport numpy as np\nimport cv2\nfrom libfaceid.detector import FaceDetectorModels, FaceDetector\nfrom libfaceid.encoder import FaceEncoderModels, FaceEncoder\nfrom libfaceid.classifier import FaceClassifierModels\n\n\n\n# Set the window name\nWINDOW_NAME = ""Facial Recognition""\n\n# Set the input directories\nINPUT_DIR_DATASET               = ""datasets""\nINPUT_DIR_MODEL_DETECTION       = ""models/detection/""\nINPUT_DIR_MODEL_ENCODING        = ""models/encoding/""\nINPUT_DIR_MODEL_TRAINING        = ""models/training/""\nINPUT_DIR_MODEL_ESTIMATION      = ""models/estimation/""\n\n# Set width and height\nRESOLUTION_QVGA   = (320, 240)\nRESOLUTION_VGA    = (640, 480)\nRESOLUTION_HD     = (1280, 720)\nRESOLUTION_FULLHD = (1920, 1080)\n\n\n\ndef cam_init(width, height): \n    cap = cv2.VideoCapture(0)\n    if sys.version_info < (3, 0):\n        cap.set(cv2.cv.CV_CAP_PROP_FPS, 30)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT, height)\n    else:\n        cap.set(cv2.CAP_PROP_FPS, 30)\n        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n    return cap\n\ndef cam_release(cap):\n    cap.release()\n    cv2.destroyAllWindows()\n\ndef ensure_directory(file_path):\n    directory = os.path.dirname(""./"" + file_path)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\ndef process_webcam(cam_resolution, out_resolution, framecount):\n\n    # Initialize the camera\n    cap = cam_init(cam_resolution[0], cam_resolution[1])\n\n    # Initialize fps counter\n    fps_frames = 0\n    fps_start = time()\n\n    while (True):\n       \n        # Capture frame-by-frame\n        ret, frame = cap.read()\n        if ret == 0:\n            break\n        \n        # Resize to QVGA so that RPI we can have acceptable fps\n        if out_resolution is not None:\n            frame = cv2.resize(frame, out_resolution);\n        \n        # Display the resulting frame\n        cv2.imshow(WINDOW_NAME, frame)\n\n        # Update frame count\n        fps_frames += 1\n        if (framecount!=0 and fps_frames >= framecount):\n            break\n\n        # Check for user actions\n        keyPressed = cv2.waitKey(1) & 0xFF\n        if keyPressed == 27:\n            break\n\n    # Set the fps\n    fps = fps_frames / (time() - fps_start)\n\n    # Release the camera\n    cam_release(cap)\n    \n    return fps\n\ndef process_facedetection(cam_resolution, out_resolution, framecount, model_detector=0):\n\n    from libfaceid.pose    import FacePoseEstimatorModels,    FacePoseEstimator\n    from libfaceid.age     import FaceAgeEstimatorModels,     FaceAgeEstimator\n    from libfaceid.gender  import FaceGenderEstimatorModels,  FaceGenderEstimator\n    from libfaceid.emotion import FaceEmotionEstimatorModels, FaceEmotionEstimator\n    model_poseestimator    = FacePoseEstimatorModels.DEFAULT\n    model_ageestimator     = FaceAgeEstimatorModels.DEFAULT\n    model_genderestimator  = FaceGenderEstimatorModels.DEFAULT\n    model_emotionestimator = FaceEmotionEstimatorModels.DEFAULT\n\n\n    # Initialize the camera\n    cap = cam_init(cam_resolution[0], cam_resolution[1])\n\n    ###############################################################################\n    # FACE DETECTION\n    ###############################################################################\n    # Initialize face detection\n    face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION)#, optimize=True)\n\n    ###############################################################################\n    # FACE POSE/AGE/GENDER/EMOTION ESTIMATION\n    ###############################################################################\n    # Initialize face pose/age/gender/emotion estimation\n    if model_poseestimator is not None:\n        face_pose_estimator = FacePoseEstimator(model=model_poseestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n    if model_ageestimator is not None:\n        face_age_estimator = FaceAgeEstimator(model=model_ageestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n    if model_genderestimator is not None:\n        face_gender_estimator = FaceGenderEstimator(model=model_genderestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n    if model_emotionestimator is not None:\n        face_emotion_estimator = FaceEmotionEstimator(model=model_emotionestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n    (age, gender, emotion) = (None, None, None)\n\n\n    # Initialize fps counter\n    fps_frames = 0\n    fps_start = time()\n\n    while (True):\n       \n        # Capture frame-by-frame\n        ret, frame = cap.read()\n        if ret == 0:\n            break\n        \n        # Resize to QVGA so that RPI we can have acceptable fps\n        if out_resolution is not None:\n            frame = cv2.resize(frame, out_resolution);\n\n        ###############################################################################\n        # FACE DETECTION\n        ###############################################################################\n        # Detect faces and set bounding boxes\n        faces = face_detector.detect(frame)\n        for (index, face) in enumerate(faces):\n            (x, y, w, h) = face\n\n            ###############################################################################\n            # FACE AGE/GENDER/EMOTION ESTIMATION\n            ###############################################################################\n            face_image = frame[y:y+h, h:h+w]\n            if model_ageestimator is not None:\n                age = face_age_estimator.estimate(frame, face_image)\n            if model_genderestimator is not None:\n                gender = face_gender_estimator.estimate(frame, face_image)\n            if model_emotionestimator is not None:\n                emotion = face_emotion_estimator.estimate(frame, face_image)\n\n            ###############################################################################\n            # FACE POSE ESTIMATION\n            ###############################################################################\n            # Detect and draw face pose locations\n            if model_poseestimator is not None:\n                shape = face_pose_estimator.detect(frame, face)\n                face_pose_estimator.add_overlay(frame, shape)\n            else:\n                cv2.rectangle(frame, (x,y), (x+w,y+h), (255,255,255), 1)\n\n            # Display age, gender, emotion\n            if age is not None and gender is not None and emotion is not None:\n                cv2.putText(frame, ""Age: {}"".format(age), \n                    (x, y-45), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n                cv2.putText(frame, ""Gender: {}"".format(gender), \n                    (x, y-30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n                cv2.putText(frame, ""Emotion: {}"".format(emotion), \n                    (x, y-15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n\n        # Display the resulting frame\n        cv2.imshow(WINDOW_NAME, frame)\n\n        # Update frame count\n        fps_frames += 1\n        if (framecount!=0 and fps_frames >= framecount):\n            break\n\n        # Check for user actions\n        keyPressed = cv2.waitKey(1) & 0xFF\n        if keyPressed == 27:\n            break\n\n    # Set the fps\n    fps = fps_frames / (time() - fps_start)\n\n    # Release the camera\n    cam_release(cap)\n    \n    return fps\n\ndef save_video(saveVideo, out, resolution, filename):\n    if saveVideo == True:\n        print(""video recording ended!"")\n        out.release()\n        out = None\n        saveVideo = False\n    else:\n        print(""video recording started..."")\n        fourcc = cv2.VideoWriter_fourcc(\'M\', \'J\', \'P\', \'G\')\n        (h, w) = resolution\n        out = cv2.VideoWriter(filename, fourcc, 12, (w, h))\n        saveVideo = True\n    return saveVideo, out\n\ndef save_photo(frame, filename):\n    print(""photo capture started..."")\n    cv2.imwrite(filename, frame);\n    print(""photo capture ended!"")\n\ndef label_face(frame, face_rect, face_id, confidence, draw_box=True):\n    (x, y, w, h) = face_rect\n    if draw_box == True:\n        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 255), 1)\n    if face_id is not None:\n        cv2.putText(frame, ""{} {:.2f}%"".format(face_id, confidence), \n            (x+5,y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\ndef process_facerecognition(cam_resolution, out_resolution, framecount, image=None, model_detector=0, model_recognizer=0):\n\n    # Initialize the camera\n    if image is not None:\n        cap = cv2.VideoCapture(image)\n    else:\n        cap = cam_init(cam_resolution[0], cam_resolution[1])\n\n\n    ###############################################################################\n    # FACE DETECTION\n    ###############################################################################\n    # Initialize face detection\n    face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION, optimize=True)\n\n    ###############################################################################\n    # FACE RECOGNITION\n    ###############################################################################\n    # Initialize face recognizer\n    face_encoder = FaceEncoder(model=model_recognizer, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\n    face_id, confidence = (""Unknown"", 0)\n\n\n    # Initialize fps counter\n    fps_frames = 0\n    fps_start = time()\n    fps = 0\n    saveVideo = False\n    out = None\n\n    # Optimization\n    skip_frames = True\n    skip_frames_count = 0\n    skip_frames_set = 2\n\n    while (True):\n       \n        # Capture frame-by-frame\n        ret, frame = cap.read()\n        if ret == 0:\n            print(""Unexpected error! "" + image)\n            break\n        \n        ###############################################################################\n        # FACE DETECTION and FACE RECOGNITION\n        ###############################################################################\n        # Detect and recognize each face in the images\n\n        # Resize to QVGA so that RPI we can have acceptable fps\n        if out_resolution is not None:\n            #frame = imutils.resize(frame, width=out_resolution[0])\n            (h, w) = image.shape[:2]\n            frame = cv2.resize(frame, (out_resolution[0], int(h * out_resolution[0] / float(w) )));\n\n\n        ###############################################################################\n        # FACE DETECTION\n        ###############################################################################\n        faces = face_detector.detect(frame)\n        for (index, face) in enumerate(faces):\n            (x, y, w, h) = face\n\n            ###############################################################################\n            # FACE RECOGNITION\n            ###############################################################################\n            face_id, confidence = face_encoder.identify(frame, (x, y, w, h))\n\n            # Set bounding box and text\n            label_face(frame, (x, y, w, h), face_id, confidence)\n\n\n        # Update frame count\n        fps_frames += 1\n        if (framecount!=0 and fps_frames >= framecount):\n            break\n        if (fps_frames % 30 == 29):\n            fps = fps_frames / (time() - fps_start)\n            fps_frames = 0\n            fps_start = time()\n        cv2.putText(frame, ""FPS {:.2f}"".format(fps), \n            (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n        # Save the frame to a video\n        if saveVideo:\n            out.write(frame)\n\n        # Display the resulting frame\n        cv2.imshow(WINDOW_NAME, frame)\n\n        # Check for user actions\n        keyPressed = cv2.waitKey(1) & 0xFF\n        if keyPressed == 27: # ESC\n            break\n        elif keyPressed == 32: # Space\n            saveVideo, out = save_video(saveVideo, out, frame.shape[:2], ""facial_recognition_rpi3.avi"")\n\n\n    # Set the fps\n    time_diff = time() - fps_start\n    if time_diff:\n        fps = fps_frames / time_diff\n\n    if image is not None:\n        cv2.waitKey(3000)\n\n    if saveVideo == True:\n        out.release()\n    \n    # Release the camera\n    cam_release(cap)\n    \n    return fps\n\n\ndef process_facerecognition_livenessdetection_poseagegenderemotion(cam_resolution, out_resolution, framecount, image=None, model_detector=0, model_recognizer=0):\n\n    from libfaceid.liveness import FaceLivenessModels, FaceLiveness\n    from libfaceid.pose    import FacePoseEstimatorModels,    FacePoseEstimator\n    from libfaceid.age     import FaceAgeEstimatorModels,     FaceAgeEstimator\n    from libfaceid.gender  import FaceGenderEstimatorModels,  FaceGenderEstimator\n    from libfaceid.emotion import FaceEmotionEstimatorModels, FaceEmotionEstimator\n    model_poseestimator    = FacePoseEstimatorModels.DEFAULT\n    model_ageestimator     = FaceAgeEstimatorModels.DEFAULT\n    model_genderestimator  = FaceGenderEstimatorModels.DEFAULT\n    model_emotionestimator = FaceEmotionEstimatorModels.DEFAULT\n\n\n    # Initialize the camera\n    if image is not None:\n        cap = cv2.VideoCapture(image)\n    else:\n        cap = cam_init(cam_resolution[0], cam_resolution[1])\n\n\n    ###############################################################################\n    # FACE DETECTION\n    ###############################################################################\n    # Initialize face detection\n    face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION)#, optimize=True)\n\n    ###############################################################################\n    # FACE RECOGNITION\n    ###############################################################################\n    # Initialize face recognizer\n    face_encoder = FaceEncoder(model=model_recognizer, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\n\n\n    ###############################################################################\n    # EYE BLINKING DETECTOR\n    ###############################################################################\n    # Initialize detector for blinking eyes\n    face_liveness = FaceLiveness(model=FaceLivenessModels.EYEBLINKING, path=INPUT_DIR_MODEL_ESTIMATION)\n    face_liveness.initialize()\n    (eye_counter, total_eye_blinks) = (0, 0)\n\n    ###############################################################################\n    # FACE POSE/AGE/GENDER/EMOTION ESTIMATION\n    ###############################################################################\n    # Initialize pose/age/gender/emotion estimation\n    if model_poseestimator is not None:\n        face_pose_estimator = FacePoseEstimator(model=model_poseestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n    if model_ageestimator is not None:\n        face_age_estimator = FaceAgeEstimator(model=model_ageestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n    if model_genderestimator is not None:\n        face_gender_estimator = FaceGenderEstimator(model=model_genderestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n    if model_emotionestimator is not None:\n        face_emotion_estimator = FaceEmotionEstimator(model=model_emotionestimator, path=INPUT_DIR_MODEL_ESTIMATION)\n    (age, gender, emotion) = (None, None, None)\n\n\n    # Initialize fps counter\n    fps_frames = 0\n    fps_start = time()\n    fps = 0\n    saveVideo = False\n    out = None\n    \n    while (True):\n       \n        # Capture frame-by-frame\n        ret, frame = cap.read()\n        if ret == 0:\n            print(""Unexpected error! "" + image)\n            break\n        \n        ###############################################################################\n        # FACE DETECTION and FACE RECOGNITION\n        ###############################################################################\n        # Detect and recognize each face in the images\n        \n        # Resize to QVGA so that RPI we can have acceptable fps\n        if out_resolution is not None:\n            #frame = imutils.resize(frame, width=out_resolution[0])\n            (h, w) = image.shape[:2]\n            frame = cv2.resize(frame, (out_resolution[0], int(h * out_resolution[0] / float(w) )));\n\n        ###############################################################################\n        # FACE DETECTION\n        ###############################################################################\n        faces = face_detector.detect(frame)\n        for (index, face) in enumerate(faces):\n            (x, y, w, h) = face\n\n            ###############################################################################\n            # FACE AGE/GENDER/EMOTION ESTIMATION\n            ###############################################################################\n            face_image = frame[y:y+h, h:h+w]\n            if model_ageestimator is not None:\n                age = face_age_estimator.estimate(frame, face_image)\n            if model_genderestimator is not None:\n                gender = face_gender_estimator.estimate(frame, face_image)\n            if model_emotionestimator is not None:\n                emotion = face_emotion_estimator.estimate(frame, face_image)\n\n            ###############################################################################\n            # FACE RECOGNITION\n            ###############################################################################\n            face_id, confidence = face_encoder.identify(frame, (x, y, w, h))\n\n            ###############################################################################\n            # EYE BLINKING DETECTION\n            ###############################################################################\n            total_eye_blinks, eye_counter = face_liveness.detect(frame, (x, y, w, h), total_eye_blinks, eye_counter) \n\n            ###############################################################################\n            # FACE POSE ESTIMATION\n            ###############################################################################\n            # Detect and draw face pose locations\n            if model_poseestimator is not None:\n                shape = face_pose_estimator.detect(frame, face)\n                face_pose_estimator.add_overlay(frame, shape)\n\n            # Display name, age, gender, emotion\n            cv2.putText(frame, ""Age: {}"".format(age), \n                (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n            cv2.putText(frame, ""Gender: {}"".format(gender), \n                (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n            cv2.putText(frame, ""Emotion: {}"".format(emotion), \n                (20, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n            cv2.putText(frame, ""Name: {} [{:.2f}%]"".format(face_id, confidence), \n                (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n        ###############################################################################\n        # EYE BLINKING DETECTION\n        ###############################################################################\n        cv2.putText(frame, ""Blinks: {}"".format(total_eye_blinks), (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n\n        # Update frame count\n        fps_frames += 1\n        if (framecount!=0 and fps_frames >= framecount):\n            break\n        if (fps_frames % 30 == 29):\n            fps = fps_frames / (time() - fps_start)\n            fps_frames = 0\n            fps_start = time()\n        cv2.putText(frame, ""FPS {:.2f}"".format(fps), (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n        # Save the frame to a video\n        if saveVideo:\n            out.write(frame)\n\n        # Display the resulting frame\n        cv2.imshow(WINDOW_NAME, frame)\n\n        # Check for user actions\n        keyPressed = cv2.waitKey(1) & 0xFF\n        if keyPressed == 27: # ESC\n            break\n        elif keyPressed == 32: # Space\n            saveVideo, out = save_video(saveVideo, out, frame.shape[:2], WINDOW_NAME + "".avi"")\n        elif keyPressed == 13: # Enter\n            save_photo(frame, WINDOW_NAME + ""_"" + datetime.datetime.now().strftime(""%Y%m%d_%H%M%S"") + "".jpg"")\n\n\n    # Set the fps\n    time_diff = time() - fps_start\n    if time_diff:\n        fps = fps_frames / time_diff\n\n    if image is not None:\n        cv2.waitKey(3000)\n\n    if saveVideo == True:\n        out.release()\n    \n    # Release the camera\n    cam_release(cap)\n    \n    return fps\n\n\n\ndef test_resolution_fps():\n    \n    resolutions = [ RESOLUTION_QVGA, RESOLUTION_VGA, RESOLUTION_HD, RESOLUTION_FULLHD ] #3.5-4FPS 7.5-8.25FPS, 22-23FPS\n    frame_count = 100\n  \n    for resolution in resolutions:\n        fps = process_webcam( resolution, None, frame_count )\n        print( ""resolution = {}x{}\\tfps = {:.2f}"".format(resolution[0], resolution[1], fps) )\n\ndef test_detection_fps():\n    \n    frame_count = 100\n    for i in range(len(FaceDetectorModels)):\n        fps = process_facedetection( RESOLUTION_QVGA, None, frame_count, model_detector = i )\n        print( ""MODEL = {}\\tfps = {:.2f}"".format(i, fps) )\n\ndef test_recognition_fps():\n    \n    frame_count = 100\n    for i in range(len(FaceDetectorModels)):\n        for j in range(len(FaceEncoderModels)):\n            fps = process_facerecognition( RESOLUTION_QVGA, None, 0, model_detector=i, model_recognizer=j)\n            print( ""MODEL = {}x{}\\tfps = {:.2f}"".format(i, j, fps) )\n\ndef test():\n\n    # check webcam speed\n    test_resolution_fps()\n\n    # check face detection\n    test_detection_fps()\n\n    # check face recognition\n    test_recognition_fps()\n\ndef train_recognition(model_detector, model_encoder, model_classifier, verify):\n\n    ensure_directory(INPUT_DIR_DATASET)\n    ensure_directory(INPUT_DIR_MODEL_TRAINING)\n    face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION)\n    face_encoder = FaceEncoder(model=model_encoder, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=True)\n    face_encoder.train(face_detector, path_dataset=INPUT_DIR_DATASET, verify=verify, classifier=model_classifier)\n\n\ndef run():\n    # set models to use\n    detector=FaceDetectorModels.HAARCASCADE\n#    detector=FaceDetectorModels.DLIBHOG\n#    detector=FaceDetectorModels.DLIBCNN\n#    detector=FaceDetectorModels.SSDRESNET\n#    detector=FaceDetectorModels.MTCNN\n#    detector=FaceDetectorModels.FACENET\n\n    encoder=FaceEncoderModels.LBPH\n#    encoder=FaceEncoderModels.OPENFACE\n#    encoder=FaceEncoderModels.DLIBRESNET\n#    encoder=FaceEncoderModels.FACENET\n\n    classifier=FaceClassifierModels.NAIVE_BAYES\n#    classifier=FaceClassifierModels.LINEAR_SVM\n#    classifier=FaceClassifierModels.RBF_SVM\n#    classifier=FaceClassifierModels.NEAREST_NEIGHBORS\n#    classifier=FaceClassifierModels.DECISION_TREE\n#    classifier=FaceClassifierModels.RANDOM_FOREST\n#    classifier=FaceClassifierModels.NEURAL_NET\n#    classifier=FaceClassifierModels.ADABOOST\n#    classifier=FaceClassifierModels.QDA\n\n    # check face detection with pose estimation and age/gender classification\n    #fps = process_facedetection( RESOLUTION_QVGA, None, 0, model_detector=detector)\n\n    # check face recognition\n    train_recognition(detector, encoder, classifier, True)\n    #fps = process_facerecognition( RESOLUTION_QVGA, None, 0, model_detector=detector, model_recognizer=encoder)\n    fps = process_facerecognition_livenessdetection_poseagegenderemotion( RESOLUTION_QVGA, None, 0, model_detector=detector, model_recognizer=encoder)\n    print( ""resolution = {}x{}\\tfps = {:.2f}"".format(RESOLUTION_QVGA[0], RESOLUTION_QVGA[1], fps) )\n\n\ndef main(args):\n    if sys.version_info < (3, 0):\n        print(""Error: Python2 is slow. Use Python3 for max performance."")\n        return\n    if args.detector and args.encoder:\n        try:\n            detector = FaceDetectorModels(int(args.detector))\n            encoder = FaceEncoderModels(int(args.encoder))\n            classifier = FaceEncoderModels(int(args.classifier))\n            print( ""Parameters: {} {} {}"".format(detector, encoder, classifier) )\n            train_recognition(detector, encoder, classifier, True)\n            fps = process_facerecognition( RESOLUTION_QVGA, None, 0, model_detector=detector, model_recognizer=encoder)\n            print( ""Result: {}x{} {:.2f} fps"".format(RESOLUTION_QVGA[0], RESOLUTION_QVGA[1], fps) )\n        except:\n            print( ""Invalid parameter"" )\n        return\n    run()\n\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--detector\', required=False,\n        help=\'Detector model to use.\\nOptions: 0-HAARCASCADE, 1-DLIBHOG, 2-DLIBCNN, 3-SSDRESNET, 4-MTCNN, 5-FACENET\')\n    parser.add_argument(\'--encoder\', required=False,\n        help=\'Encoder model to use.\\nOptions: 0-LBPH, 1-OPENFACE, 2-DLIBRESNET, 3-FACENET\')\n    parser.add_argument(\'--classifier\', required=False,\n        help=\'Classifier algorithm to use. Options: 0-NAIVE_BAYES, 1-LINEAR_SVM, 2-RBF_SVM, 3-NEAREST_NEIGHBORS, 4-DECISION_TREE, 5-RANDOM_FOREST, 6-NEURAL_NET, 7-ADABOOST, 8-QDA.\')\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
testing_image.py,0,"b'import sys\nimport argparse\nimport cv2\nimport os\nfrom libfaceid.detector import FaceDetectorModels, FaceDetector\nfrom libfaceid.encoder  import FaceEncoderModels, FaceEncoder\n\n\n\n# Set the window name\nWINDOW_NAME = ""Facial_Recognition""\n\n# Set the input directories\nINPUT_DIR_DATASET         = ""datasets""\nINPUT_DIR_MODEL_DETECTION = ""models/detection/""\nINPUT_DIR_MODEL_ENCODING  = ""models/encoding/""\nINPUT_DIR_MODEL_TRAINING  = ""models/training/""\nINPUT_DIR_MODEL           = ""models/""\n\n\n\ndef label_face(frame, face_rect, face_id, confidence):\n    (x, y, w, h) = face_rect\n    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 255), 1)\n    if face_id is not None:\n        cv2.putText(frame, ""{} {:.2f}%"".format(face_id, confidence), \n            (x+5,y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n\ndef process_facerecognition(model_detector, model_recognizer, image):\n\n    # Initialize the camera\n    image = cv2.VideoCapture(image)\n\n    # Initialize face detection\n    face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION)\n\n    # Initialize face recognizer\n    try:\n        face_encoder = FaceEncoder(model=model_recognizer, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\n    except:\n        face_encoder = None\n        print(""Warning, check if models and trained dataset models exists!"")\n    face_id, confidence = (None, 0)\n\n\n    # Capture frame-by-frame\n    ret, frame = image.read()\n    if ret == 0:\n        print(""Unexpected error! "" + image)\n        return\n\n\n    # Detect faces in the image\n    faces = face_detector.detect(frame)\n    for (index, face) in enumerate(faces):\n        (x, y, w, h) = face\n        # Indentify face based on trained dataset (note: should run facial_recognition_training.py)\n        if face_encoder is not None:\n            face_id, confidence = face_encoder.identify(frame, (x, y, w, h))\n            print(face_id)\n        # Set text and bounding box on face\n        label_face(frame, (x, y, w, h), face_id, confidence)\n\n\n    # Display the resulting frame\n    cv2.imshow(WINDOW_NAME, frame)\n    cv2.waitKey(1)\n\n    # Release the image\n    image.release()\n    cv2.destroyAllWindows()\n\n\ndef run(image):\n    detector=FaceDetectorModels.HAARCASCADE\n#    detector=FaceDetectorModels.DLIBHOG\n#    detector=FaceDetectorModels.DLIBCNN\n#    detector=FaceDetectorModels.SSDRESNET\n#    detector=FaceDetectorModels.MTCNN\n#    detector=FaceDetectorModels.MTCNN\n\n    encoder=FaceEncoderModels.LBPH\n#    encoder=FaceEncoderModels.OPENFACE\n#    encoder=FaceEncoderModels.DLIBRESNET\n#    encoder=FaceEncoderModels.FACENET\n\n    # check face recognition\n    if not image.endswith("".jpg"") and not image.endswith("".png""):\n        # test all files inside the provided directory\t\n        for (_d, _n, files) in os.walk(image):\n            print(type(files))\n            for file in files:\n                file_image = image + ""/"" + file\n                print(file_image)\n                process_facerecognition(detector, encoder, file_image)\n    else:\n        process_facerecognition(detector, encoder, image)\n\n\ndef main(args):\n    if sys.version_info < (3, 0):\n        print(""Error: Python2 is slow. Use Python3 for max performance."")\n        return\n    if args.detector and args.encoder:\n        try:\n            detector = FaceDetectorModels(int(args.detector))\n            encoder = FaceEncoderModels(int(args.encoder))\n            print( ""Parameters: {} {}"".format(detector, encoder) )\n            process_facerecognition(detector, encoder, args.image)\n        except:\n            print( ""Invalid parameter"" )\n        return\n    run(args.image)\n\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--detector\', required=False,\n        help=\'Detector model to use. Options: 0-HAARCASCADE, 1-DLIBHOG, 2-DLIBCNN, 3-SSDRESNET, 4-MTCNN, 5-FACENET\')\n    parser.add_argument(\'--encoder\', required=False,\n        help=\'Encoder model to use. Options: 0-LBPH, 1-OPENFACE, 2-DLIBRESNET, 3-FACENET\')\n    parser.add_argument(\'--image\', required=True, \n        help=\'Image to process.\')\n    return parser.parse_args(argv)\n\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
testing_webcam.py,0,"b'import sys\nimport argparse\nimport cv2\nfrom libfaceid.detector    import FaceDetectorModels, FaceDetector\nfrom libfaceid.encoder     import FaceEncoderModels, FaceEncoder\n\n\n\n# Set the window name\nWINDOW_NAME = ""Facial_Recognition""\n\n# Set the input directories\nINPUT_DIR_DATASET               = ""datasets""\nINPUT_DIR_MODEL_DETECTION       = ""models/detection/""\nINPUT_DIR_MODEL_ENCODING        = ""models/encoding/""\nINPUT_DIR_MODEL_TRAINING        = ""models/training/""\nINPUT_DIR_MODEL_ESTIMATION      = ""models/estimation/""\n\n# Set width and height\nRESOLUTION_QVGA   = (320, 240)\nRESOLUTION_VGA    = (640, 480)\nRESOLUTION_HD     = (1280, 720)\nRESOLUTION_FULLHD = (1920, 1080)\n\n\n\ndef cam_init(cam_index, width, height): \n    cap = cv2.VideoCapture(cam_index)\n    if sys.version_info < (3, 0):\n        cap.set(cv2.cv.CV_CAP_PROP_FPS, 30)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT, height)\n    else:\n        cap.set(cv2.CAP_PROP_FPS, 30)\n        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n    return cap\n\n\ndef label_face(frame, face_rect, face_id, confidence):\n    (x, y, w, h) = face_rect\n    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 255), 1)\n    if face_id is not None:\n        cv2.putText(frame, ""{} {:.2f}%"".format(face_id, confidence), \n            (x+5,y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n\ndef process_facerecognition(model_detector, model_recognizer, cam_index, cam_resolution):\n\n    # Initialize the camera\n    camera = cam_init(cam_index, cam_resolution[0], cam_resolution[1])\n\n    try:\n        # Initialize face detection\n        face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION)\n\n        # Initialize face recognizer\n        face_encoder = FaceEncoder(model=model_recognizer, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\n    except:\n        face_encoder = None\n        print(""Warning, check if models and trained dataset models exists!"")\n    face_id, confidence = (None, 0)\n\n\n    while (True):\n\n        # Capture frame from webcam\n        ret, frame = camera.read()\n        if frame is None:\n            print(""Error, check if camera is connected!"")\n            break\n\n\n        # Detect and identify faces in the frame\n        faces = face_detector.detect(frame)\n        for (index, face) in enumerate(faces):\n            (x, y, w, h) = face\n            # Indentify face based on trained dataset (note: should run facial_recognition_training.py)\n            if face_encoder is not None:\n                face_id, confidence = face_encoder.identify(frame, (x, y, w, h))\n            # Set text and bounding box on face\n            label_face(frame, (x, y, w, h), face_id, confidence)\n\n            # Process 1 face only\n            break\n\n\n        # Display updated frame\n        cv2.imshow(WINDOW_NAME, frame)\n\n        # Check for user actions\n        if cv2.waitKey(1) & 0xFF == 27: # ESC\n            break\n\n    # Release the camera\n    camera.release()\n    cv2.destroyAllWindows()\n\n\ndef run(cam_index, cam_resolution):\n    detector=FaceDetectorModels.HAARCASCADE\n#    detector=FaceDetectorModels.DLIBHOG\n#    detector=FaceDetectorModels.DLIBCNN\n#    detector=FaceDetectorModels.SSDRESNET\n#    detector=FaceDetectorModels.MTCNN\n#    detector=FaceDetectorModels.FACENET\n\n    encoder=FaceEncoderModels.LBPH\n#    encoder=FaceEncoderModels.OPENFACE\n#    encoder=FaceEncoderModels.DLIBRESNET\n#    encoder=FaceEncoderModels.FACENET\n\n    process_facerecognition(detector, encoder, cam_index, cam_resolution)\n\n\ndef main(args):\n    if sys.version_info < (3, 0):\n        print(""Error: Python2 is slow. Use Python3 for max performance."")\n        return\n\n    cam_index = int(args.webcam)\n    resolutions = [ RESOLUTION_QVGA, RESOLUTION_VGA, RESOLUTION_HD, RESOLUTION_FULLHD ]\n    try:\n        cam_resolution = resolutions[int(args.resolution)]\n    except:\n        cam_resolution = RESOLUTION_QVGA\n\n    if args.detector and args.encoder:\n        try:\n            detector = FaceDetectorModels(int(args.detector))\n            encoder = FaceEncoderModels(int(args.encoder))\n            print( ""Parameters: {} {}"".format(detector, encoder) )\n            process_facerecognition(detector, encoder, cam_index, cam_resolution)\n        except:\n            print( ""Invalid parameter"" )\n        return\n    run(cam_index, cam_resolution)\n\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--detector\', required=False, default=0, \n        help=\'Detector model to use. Options: 0-HAARCASCADE, 1-DLIBHOG, 2-DLIBCNN, 3-SSDRESNET, 4-MTCNN, 5-FACENET\')\n    parser.add_argument(\'--encoder\', required=False, default=0, \n        help=\'Encoder model to use. Options: 0-LBPH, 1-OPENFACE, 2-DLIBRESNET, 3-FACENET\')\n    parser.add_argument(\'--webcam\', required=False, default=0, \n        help=\'Camera index to use. Default is 0. Assume only 1 camera connected.)\')\n    parser.add_argument(\'--resolution\', required=False, default=0,\n        help=\'Camera resolution to use. Default is 0. Options: 0-QVGA, 1-VGA, 2-HD, 3-FULLHD\')\n    return parser.parse_args(argv)\n\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
testing_webcam_flask.py,0,"b'import sys\nimport argparse\nimport cv2\nfrom libfaceid.detector import FaceDetectorModels, FaceDetector\nfrom libfaceid.encoder  import FaceEncoderModels, FaceEncoder\n\n\n# Use flask for web app\nfrom flask import Flask, render_template, Response\napp = Flask(__name__)\n\n\n# Set the input directories\nINPUT_DIR_DATASET               = ""datasets""\nINPUT_DIR_MODEL_DETECTION       = ""models/detection/""\nINPUT_DIR_MODEL_ENCODING        = ""models/encoding/""\nINPUT_DIR_MODEL_TRAINING        = ""models/training/""\nINPUT_DIR_MODEL_ESTIMATION      = ""models/estimation/""\n\n# Set width and height\nRESOLUTION_QVGA   = (320, 240)\nRESOLUTION_VGA    = (640, 480)\nRESOLUTION_HD     = (1280, 720)\nRESOLUTION_FULLHD = (1920, 1080)\n\n\n\ndef cam_init(cam_index, width, height): \n    cap = cv2.VideoCapture(cam_index)\n    if sys.version_info < (3, 0):\n        cap.set(cv2.cv.CV_CAP_PROP_FPS, 30)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT, height)\n    else:\n        cap.set(cv2.CAP_PROP_FPS, 30)\n        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n    return cap\n\n\ndef label_face(frame, face_rect, face_id, confidence):\n    (x, y, w, h) = face_rect\n    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 255), 1)\n    if face_id is not None:\n        cv2.putText(frame, ""{} {:.2f}%"".format(face_id, confidence), \n            (x+5,y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n\ndef process_facerecognition():\n\n    cam_index = 0\n    cam_resolution = RESOLUTION_QVGA\n    model_detector=FaceDetectorModels.HAARCASCADE\n#    model_detector=FaceDetectorModels.DLIBHOG\n#    model_detector=FaceDetectorModels.DLIBCNN\n#    model_detector=FaceDetectorModels.SSDRESNET\n#    model_detector=FaceDetectorModels.MTCNN\n#    model_detector=FaceDetectorModels.FACENET\n\n    model_recognizer=FaceEncoderModels.LBPH\n#    model_recognizer=FaceEncoderModels.OPENFACE\n#    model_recognizer=FaceEncoderModels.DLIBRESNET\n#    model_recognizer=FaceEncoderModels.FACENET\n\n\n    # Initialize the camera\n    camera = cam_init(cam_index, cam_resolution[0], cam_resolution[1])\n\n    try:\n        # Initialize face detection\n        face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION)\n        # Initialize face recognizer\n        face_encoder = FaceEncoder(model=model_recognizer, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\n    except:\n        face_encoder = None\n        print(""Warning, check if models and trained dataset models exists!"")\n    face_id, confidence = (None, 0)\n\n\n    while (True):\n\n        # Capture frame from webcam\n        ret, frame = camera.read()\n        if frame is None:\n            print(""Error, check if camera is connected!"")\n            break\n\n\n        # Detect and identify faces in the frame\n        faces = face_detector.detect(frame)\n        for (index, face) in enumerate(faces):\n            (x, y, w, h) = face\n            # Indentify face based on trained dataset (note: should run facial_recognition_training.py)\n            if face_encoder is not None:\n                face_id, confidence = face_encoder.identify(frame, (x, y, w, h))\n            # Set text and bounding box on face\n            label_face(frame, (x, y, w, h), face_id, confidence)\n\n            # Process 1 face only\n            break\n\n\n        # Display updated frame to web app\n        yield (b\'--frame\\r\\nContent-Type: image/jpeg\\r\\n\\r\\n\' + cv2.imencode(\'.jpg\', frame)[1].tobytes() + b\'\\r\\n\\r\\n\')\n\n    # Release the camera\n    camera.release()\n    cv2.destroyAllWindows()\n\n\n# Initialize for web app\n@app.route(\'/\')\ndef index():\n    return render_template(\'web_app_flask.html\')\n\n# Entry point for web app\n@app.route(\'/video_viewer\')\ndef video_viewer():\n    return Response(process_facerecognition(), mimetype=\'multipart/x-mixed-replace; boundary=frame\')\n\n\nif __name__ == \'__main__\':\n    print(""\\n\\nNote: Open browser and type http://127.0.0.1:5000/ or http://ip_address:5000/ \\n\\n"")\n    # Run flask for web app\n    app.run(host=\'0.0.0.0\', threaded=True, debug=True)\n'"
testing_webcam_livenessdetection.py,0,"b'import sys\nimport argparse\nimport cv2\nfrom time import time\nfrom libfaceid.detector    import FaceDetectorModels, FaceDetector\nfrom libfaceid.encoder     import FaceEncoderModels, FaceEncoder\nfrom libfaceid.liveness    import FaceLivenessModels, FaceLiveness\n\n\n\n# Set the window name\nWINDOW_NAME = ""Facial_Recognition""\n\n# Set the input directories\nINPUT_DIR_DATASET               = ""datasets""\nINPUT_DIR_MODEL_DETECTION       = ""models/detection/""\nINPUT_DIR_MODEL_ENCODING        = ""models/encoding/""\nINPUT_DIR_MODEL_TRAINING        = ""models/training/""\nINPUT_DIR_MODEL_ESTIMATION      = ""models/estimation/""\nINPUT_DIR_MODEL_LIVENESS        = ""models/liveness/""\n\n# Set width and height\nRESOLUTION_QVGA   = (320, 240)\nRESOLUTION_VGA    = (640, 480)\nRESOLUTION_HD     = (1280, 720)\nRESOLUTION_FULLHD = (1920, 1080)\n\n\n\ndef cam_init(cam_index, width, height): \n    cap = cv2.VideoCapture(cam_index)\n    if sys.version_info < (3, 0):\n        cap.set(cv2.cv.CV_CAP_PROP_FPS, 30)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT, height)\n    else:\n        cap.set(cv2.CAP_PROP_FPS, 30)\n        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n    return cap\n\n\ndef label_face(frame, face_rect, face_id, confidence):\n    (x, y, w, h) = face_rect\n    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 255), 1)\n    if face_id is not None:\n        if confidence is not None:\n            text = ""{} {:.2f}%"".format(face_id, confidence)\n        else:\n            text = ""{}"".format(face_id)\n        cv2.putText(frame, text, (x+5,y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n\ndef monitor_eye_blinking(eyes_close, eyes_ratio, total_eye_blinks, eye_counter, eye_continuous_close):\n    if eyes_close:\n        #print(""eye less than threshold {:.2f}"".format(eyes_ratio))\n        eye_counter += 1\n    else:\n        #print(""eye:{:.2f} blinks:{}"".format(eyes_ratio, total_eye_blinks))\n        if eye_counter >= eye_continuous_close:\n            total_eye_blinks += 1\n        eye_counter = 0\n    return total_eye_blinks, eye_counter\n\n\ndef monitor_mouth_opening(mouth_open, mouth_ratio, total_mouth_opens, mouth_counter, mouth_continuous_open):\n    if mouth_open:\n        #print(""mouth more than threshold {:.2f}"".format(mouth_ratio))\n        mouth_counter += 1\n    else:\n        #print(""mouth:{:.2f} opens:{}"".format(mouth_ratio, total_mouth_opens))\n        if mouth_counter >= mouth_continuous_open:\n            total_mouth_opens += 1\n        mouth_counter = 0\n    return total_mouth_opens, mouth_counter\n\n\n# process_livenessdetection is supposed to run before process_facerecognition\ndef process_livenessdetection(model_detector, model_recognizer, model_liveness, cam_index, cam_resolution):\n\n    # Initialize the camera\n    camera = cam_init(cam_index, cam_resolution[0], cam_resolution[1])\n\n    try:\n        # Initialize face detection\n        face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION)\n\n        # Initialize face recognizer\n        face_encoder = FaceEncoder(model=model_recognizer, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\n\n        # Initialize face liveness detection\n        face_liveness  = FaceLiveness(model=FaceLivenessModels.EYESBLINK_MOUTHOPEN, path=INPUT_DIR_MODEL_LIVENESS)\n        face_liveness2 = FaceLiveness(model=FaceLivenessModels.COLORSPACE_YCRCBLUV, path=INPUT_DIR_MODEL_LIVENESS)\n\n    except:\n        print(""Error, check if models and trained dataset models exists!"")\n        return\n\n    face_id, confidence = (None, 0)\n\n    eyes_close, eyes_ratio = (False, 0)\n    total_eye_blinks, eye_counter, eye_continuous_close = (0, 0, 1) # eye_continuous_close should depend on frame rate\n    mouth_open, mouth_ratio = (False, 0)\n    total_mouth_opens, mouth_counter, mouth_continuous_open = (0, 0, 1) # eye_continuous_close should depend on frame rate\n\n    time_start = time()\n    time_elapsed = 0\n    frame_count = 0\n    identified_unique_faces = {} # dictionary\n    runtime = 10 # monitor for 10 seconds only\n    is_fake_count_print = 0\n\n\n    print(""Note: this will run for {} seconds only"".format(runtime))\n    while (time_elapsed < runtime): \n\n        # Capture frame from webcam\n        ret, frame = camera.read()\n        if frame is None:\n            print(""Error, check if camera is connected!"")\n            break\n\n\n        # Detect and identify faces in the frame\n        # Indentify face based on trained dataset (note: should run facial_recognition_training.py)\n        faces = face_detector.detect(frame)\n        for (index, face) in enumerate(faces):\n\n            # Check if eyes are close and if mouth is open\n            eyes_close, eyes_ratio = face_liveness.is_eyes_close(frame, face)\n            mouth_open, mouth_ratio = face_liveness.is_mouth_open(frame, face)\n            print(""eyes_close={}, eyes_ratio ={:.2f}"".format(mouth_open, mouth_ratio))\n            print(""mouth_open={}, mouth_ratio={:.2f}"".format(mouth_open, mouth_ratio))\n\n            # Detect if frame is a print attack or replay attack based on colorspace\n            is_fake_print  = face_liveness2.is_fake(frame, face)\n            #is_fake_replay = face_liveness2.is_fake(frame, face, flag=1)\n\n            # Identify face only if it is not fake and eyes are open and mouth is close\n            if is_fake_print:\n                is_fake_count_print += 1\n                face_id, confidence = (""Fake"", None)\n            elif not eyes_close and not mouth_open:\n                face_id, confidence = face_encoder.identify(frame, face)\n                if face_id not in identified_unique_faces:\n                    identified_unique_faces[face_id] = 1\n                else:\n                    identified_unique_faces[face_id] += 1\n\n            label_face(frame, face, face_id, confidence) # Set text and bounding box on face\n            break # Process 1 face only\n\n\n        # Monitor eye blinking and mouth opening for liveness detection\n        total_eye_blinks, eye_counter = monitor_eye_blinking(eyes_close, eyes_ratio, total_eye_blinks, eye_counter, eye_continuous_close)\n        total_mouth_opens, mouth_counter = monitor_mouth_opening(mouth_open, mouth_ratio, total_mouth_opens, mouth_counter, mouth_continuous_open)\n\n\n        # Update frame count\n        frame_count += 1\n        time_elapsed = time()-time_start\n\n        # Display updated frame\n        cv2.imshow(WINDOW_NAME, frame)\n\n        # Check for user actions\n        if cv2.waitKey(1) & 0xFF == 27: # ESC\n            break\n\n\n    print(""Note: this will run for {} seconds only"".format(runtime))\n\n    # Determining if face is alive can depend on the following factors and more:\n    time_elapsed = int(time()-time_start)\n    print(""\\n"")\n    print(""Face Liveness Data:"")\n    print(""time_elapsed            = {}"".format(time_elapsed))            # recognition will run for specific time (ex. 3 seconds)\n    print(""frame_count             = {}"".format(frame_count))             # can be used for averaging\n    print(""total_eye_blinks        = {}"".format(total_eye_blinks))        # fake face if 0\n    print(""total_mouth_opens       = {}"".format(total_mouth_opens))       # fake face if 0\n    print(""is_fake_count_print     = {}"".format(is_fake_count_print))     # fake face if not 0\n    print(""identified_unique_faces = {}"".format(identified_unique_faces)) # fake face if recognized more than 1 face\n    print(""Todo: determine if face is alive using this data."")\n    print(""\\n"")\n\n    # Release the camera\n    camera.release()\n    cv2.destroyAllWindows()\n\n\ndef run(cam_index, cam_resolution):\n    detector=FaceDetectorModels.HAARCASCADE\n#    detector=FaceDetectorModels.DLIBHOG\n#    detector=FaceDetectorModels.DLIBCNN\n#    detector=FaceDetectorModels.SSDRESNET\n#    detector=FaceDetectorModels.MTCNN\n#    detector=FaceDetectorModels.FACENET\n\n    encoder=FaceEncoderModels.LBPH\n#    encoder=FaceEncoderModels.OPENFACE\n#    encoder=FaceEncoderModels.DLIBRESNET\n#    encoder=FaceEncoderModels.FACENET\n\n    liveness=FaceLivenessModels.EYESBLINK_MOUTHOPEN\n#    liveness=FaceLivenessModels.COLORSPACE_YCRCBLUV\n\n    process_livenessdetection(detector, encoder, liveness, cam_index, cam_resolution)\n\n\ndef main(args):\n    if sys.version_info < (3, 0):\n        print(""Error: Python2 is slow. Use Python3 for max performance."")\n        return\n\n    cam_index = int(args.webcam)\n    resolutions = [ RESOLUTION_QVGA, RESOLUTION_VGA, RESOLUTION_HD, RESOLUTION_FULLHD ]\n    try:\n        cam_resolution = resolutions[int(args.resolution)]\n    except:\n        cam_resolution = RESOLUTION_QVGA\n\n    if args.detector and args.encoder and args.liveness:\n        try:\n            detector = FaceDetectorModels(int(args.detector))\n            encoder  = FaceEncoderModels(int(args.encoder))\n            liveness = FaceLivenessModels(int(args.liveness))\n            print( ""Parameters: {} {} {}"".format(detector, encoder, liveness) )\n            process_livenessdetection(detector, encoder, liveness, cam_index, cam_resolution)\n        except:\n            print( ""Invalid parameter"" )\n        return\n    run(cam_index, cam_resolution)\n\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--detector\', required=False, default=0, \n        help=\'Detector model to use. Options: 0-HAARCASCADE, 1-DLIBHOG, 2-DLIBCNN, 3-SSDRESNET, 4-MTCNN, 5-FACENET\')\n    parser.add_argument(\'--encoder\', required=False, default=0, \n        help=\'Encoder model to use. Options: 0-LBPH, 1-OPENFACE, 2-DLIBRESNET, 3-FACENET\')\n    parser.add_argument(\'--liveness\', required=False, default=0, \n        help=\'Liveness detection model to use. Options: 0-EYESBLINK_MOUTHOPEN, 1-COLORSPACE_YCRCBLUV\')\n    parser.add_argument(\'--webcam\', required=False, default=0, \n        help=\'Camera index to use. Default is 0. Assume only 1 camera connected.)\')\n    parser.add_argument(\'--resolution\', required=False, default=0,\n        help=\'Camera resolution to use. Default is 0. Options: 0-QVGA, 1-VGA, 2-HD, 3-FULLHD\')\n    return parser.parse_args(argv)\n\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
testing_webcam_voiceenabled.py,0,"b'import sys\nimport argparse\nimport cv2\nfrom libfaceid.detector import FaceDetectorModels, FaceDetector\nfrom libfaceid.encoder  import FaceEncoderModels, FaceEncoder\nfrom libfaceid.speech_synthesizer import SpeechSynthesizerModels, SpeechSynthesizer\n\n\n\n# Set the window name\nWINDOW_NAME = ""Facial_Recognition""\n\n# Set the input directories\nINPUT_DIR_DATASET               = ""datasets""\nINPUT_DIR_MODEL_DETECTION       = ""models/detection/""\nINPUT_DIR_MODEL_ENCODING        = ""models/encoding/""\nINPUT_DIR_MODEL_TRAINING        = ""models/training/""\nINPUT_DIR_MODEL_ESTIMATION      = ""models/estimation/""\nINPUT_DIR_AUDIOSET              = ""audiosets""\n\n# Set width and height\nRESOLUTION_QVGA   = (320, 240)\nRESOLUTION_VGA    = (640, 480)\nRESOLUTION_HD     = (1280, 720)\nRESOLUTION_FULLHD = (1920, 1080)\n\n\n\ndef cam_init(cam_index, width, height): \n    cap = cv2.VideoCapture(cam_index)\n    if sys.version_info < (3, 0):\n        cap.set(cv2.cv.CV_CAP_PROP_FPS, 30)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT, height)\n    else:\n        cap.set(cv2.CAP_PROP_FPS, 30)\n        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n    return cap\n\n\ndef label_face(frame, face_rect, face_id, confidence):\n    (x, y, w, h) = face_rect\n    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 255), 1)\n    if face_id is not None:\n        cv2.putText(frame, ""{} {:.2f}%"".format(face_id, confidence), \n            (x+5,y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n\ndef process_facerecognition(model_detector, model_recognizer, model_speech_synthesizer, cam_index, cam_resolution):\n\n    # Initialize the camera\n    camera = cam_init(cam_index, cam_resolution[0], cam_resolution[1])\n\n    try:\n        # Initialize face detection\n        face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION)\n\n        # Initialize face recognizer\n        face_encoder = FaceEncoder(model=model_recognizer, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\n\n        # Initialize text-to-speech synthesizer\n        speech_synthesizer = SpeechSynthesizer(model=model_speech_synthesizer, path=None, path_output=None, training=False)\n    except:\n        #face_encoder = None\n        print(""Warning, check if models and trained dataset models exists!"")\n    face_id, confidence = (None, 0)\n\n\n    frame_count = 0\n    while (True):\n\n        # Capture frame from webcam\n        ret, frame = camera.read()\n        if frame is None:\n            print(""Error, check if camera is connected!"")\n            break\n\n\n        # Detect and identify faces in the frame\n        faces = face_detector.detect(frame)\n        for (index, face) in enumerate(faces):\n            (x, y, w, h) = face\n            # Indentify face based on trained dataset (note: should run facial_recognition_training.py)\n            if face_encoder is not None:\n                face_id, confidence = face_encoder.identify(frame, (x, y, w, h))\n            # Set text and bounding box on face\n            label_face(frame, (x, y, w, h), face_id, confidence)\n\n            # Play audio file corresponding to the recognized name \n            if (frame_count % 30 == 0):\n                if len(faces) == 1 and (face_id is not None) and (face_id != ""Unknown""):\n                    speech_synthesizer.playaudio(INPUT_DIR_AUDIOSET, face_id, block=False)\n\n            # Process 1 face only\n            break\n\n\n        # Display updated frame\n        cv2.imshow(WINDOW_NAME, frame)\n\n        # Check for user actions\n        if cv2.waitKey(1) & 0xFF == 27: # ESC\n            break\n\n        frame_count += 1\n\n    # Release the camera\n    camera.release()\n    cv2.destroyAllWindows()\n\n\ndef run(cam_index, cam_resolution):\n    detector=FaceDetectorModels.HAARCASCADE\n#    detector=FaceDetectorModels.DLIBHOG\n#    detector=FaceDetectorModels.DLIBCNN\n#    detector=FaceDetectorModels.SSDRESNET\n#    detector=FaceDetectorModels.MTCNN\n#    detector=FaceDetectorModels.FACENET\n\n    encoder=FaceEncoderModels.LBPH\n#    encoder=FaceEncoderModels.OPENFACE\n#    encoder=FaceEncoderModels.DLIBRESNET\n#    encoder=FaceEncoderModels.FACENET\n\n    speech_synthesizer=SpeechSynthesizerModels.TTSX3\n#    speech_synthesizer=SpeechSynthesizerModels.TACOTRON\n#    speech_synthesizer=SpeechSynthesizerModels.GOOGLECLOUD\n\n    process_facerecognition(detector, encoder, speech_synthesizer, cam_index, cam_resolution)\n\n\ndef main(args):\n    if sys.version_info < (3, 0):\n        print(""Error: Python2 is slow. Use Python3 for max performance."")\n        return\n\n    cam_index = int(args.webcam)\n    resolutions = [ RESOLUTION_QVGA, RESOLUTION_VGA, RESOLUTION_HD, RESOLUTION_FULLHD ]\n    try:\n        cam_resolution = resolutions[int(args.resolution)]\n    except:\n        cam_resolution = RESOLUTION_QVGA\n\n    if args.detector and args.encoder and args.speech_synthesizer:\n        try:\n            detector = FaceDetectorModels(int(args.detector))\n            encoder = FaceEncoderModels(int(args.encoder))\n            speech_synthesizer = SpeechSynthesizerModels(int(args.speech_synthesizer))\n            print( ""Parameters: {} {} {}"".format(detector, encoder, speech_synthesizer) )\n            process_facerecognition(detector, encoder, speech_synthesizer, cam_index, cam_resolution)\n        except:\n            print( ""Invalid parameter"" )\n        return\n    run(cam_index, cam_resolution)\n\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--detector\', required=False, default=0, \n        help=\'Detector model to use. Options: 0-HAARCASCADE, 1-DLIBHOG, 2-DLIBCNN, 3-SSDRESNET, 4-MTCNN, 5-FACENET\')\n    parser.add_argument(\'--encoder\', required=False, default=0, \n        help=\'Encoder model to use. Options: 0-LBPH, 1-OPENFACE, 2-DLIBRESNET, 3-FACENET\')\n    parser.add_argument(\'--speech_synthesizer\', required=False, default=0, \n        help=\'Speech synthesizer model to use. Options: 0-TTSX3, 1-TACOTRON, 2-GOOGLECLOUD\')\n    parser.add_argument(\'--webcam\', required=False, default=0, \n        help=\'Camera index to use. Default is 0. Assume only 1 camera connected.)\')\n    parser.add_argument(\'--resolution\', required=False, default=0,\n        help=\'Camera resolution to use. Default is 0. Options: 0-QVGA, 1-VGA, 2-HD, 3-FULLHD\')\n    return parser.parse_args(argv)\n\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
testing_webcam_voiceenabled_voiceactivated.py,0,"b'import sys\nimport argparse\nimport cv2\nimport time\nfrom libfaceid.detector import FaceDetectorModels, FaceDetector\nfrom libfaceid.encoder  import FaceEncoderModels, FaceEncoder\nfrom libfaceid.speech_synthesizer import SpeechSynthesizerModels, SpeechSynthesizer\nfrom libfaceid.speech_recognizer  import SpeechRecognizerModels,  SpeechRecognizer\n\n\n\n# Set the window name\nWINDOW_NAME = ""Facial_Recognition""\n\n# Set the input directories\nINPUT_DIR_DATASET               = ""datasets""\nINPUT_DIR_MODEL_DETECTION       = ""models/detection/""\nINPUT_DIR_MODEL_ENCODING        = ""models/encoding/""\nINPUT_DIR_MODEL_TRAINING        = ""models/training/""\nINPUT_DIR_MODEL_ESTIMATION      = ""models/estimation/""\nINPUT_DIR_AUDIOSET              = ""audiosets""\n\n# Set width and height\nRESOLUTION_QVGA   = (320, 240)\nRESOLUTION_VGA    = (640, 480)\nRESOLUTION_HD     = (1280, 720)\nRESOLUTION_FULLHD = (1920, 1080)\n\n# Set the trigger words\nTRIGGER_WORDS = [""Hey Google"", ""Alexa"", ""Activate"", ""Open Sesame"", ""Panel""]\n\n\n\ndef cam_init(cam_index, width, height): \n    cap = cv2.VideoCapture(cam_index)\n    if sys.version_info < (3, 0):\n        cap.set(cv2.cv.CV_CAP_PROP_FPS, 30)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT, height)\n    else:\n        cap.set(cv2.CAP_PROP_FPS, 30)\n        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  width)\n        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n    return cap\n\n\ndef label_face(frame, face_rect, face_id, confidence):\n    (x, y, w, h) = face_rect\n    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 255), 1)\n    if face_id is not None:\n        cv2.putText(frame, ""{} {:.2f}%"".format(face_id, confidence), \n            (x+5,y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n\ntrigger_word_detected = False\ndef speech_recognizer_callback(word):\n    print(""Trigger word detected! \'{}\'"".format(word))\n    global trigger_word_detected\n    trigger_word_detected = True\n\n\ndef process_facerecognition(model_detector, model_recognizer, model_speech_synthesizer, model_speech_recognizer, cam_index, cam_resolution):\n    \n    # Initialize speech-to-text (speech recognizer) for voice-activated capability (wake-word/hot-word/trigger-word detection)\n    # Then wait for trigger word before starting face recognition\n    if True:\n        speech_recognizer = SpeechRecognizer(model=model_speech_recognizer, path=None)\n        print(""\\nWaiting for a trigger word: {}"".format(TRIGGER_WORDS))\n        speech_recognizer.start(TRIGGER_WORDS, speech_recognizer_callback)\n        global trigger_word_detected\n        try:\n            while (trigger_word_detected == False):\n                time.sleep(1)\n        except:\n            pass\n        speech_recognizer.stop()\n        speech_recognizer = None\n\n    \n    # Initialize the camera\n    camera = cam_init(cam_index, cam_resolution[0], cam_resolution[1])\n\n    try:\n        # Initialize face detection\n        face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION)\n\n        # Initialize face recognizer\n        face_encoder = FaceEncoder(model=model_recognizer, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\n\n        # Initialize text-to-speech (speech synthesizer) for voice-enabled capability\n        speech_synthesizer = SpeechSynthesizer(model=model_speech_synthesizer, path=None, path_output=None, training=False)\n\n    except:\n        face_encoder = None\n        print(""Warning, check if models and trained dataset models exists!"")\n    face_id, confidence = (None, 0)\n\n\n    # Start face recognition\n    frame_count = 0\n    while (True):\n\n        # Capture frame from webcam\n        ret, frame = camera.read()\n        if frame is None:\n            print(""Error, check if camera is connected!"")\n            break\n\n\n        # Detect and identify faces in the frame\n        faces = face_detector.detect(frame)\n        for (index, face) in enumerate(faces):\n            (x, y, w, h) = face\n            # Indentify face based on trained dataset (note: should run facial_recognition_training.py)\n            if face_encoder is not None:\n                face_id, confidence = face_encoder.identify(frame, (x, y, w, h))\n            # Set text and bounding box on face\n            label_face(frame, (x, y, w, h), face_id, confidence)\n\n            # Play audio file corresponding to the recognized name \n            if (frame_count % 30 == 0):\n                if len(faces) == 1 and (face_id is not None) and (face_id != ""Unknown""):\n                    speech_synthesizer.playaudio(INPUT_DIR_AUDIOSET, face_id, block=False)\n\n            # Process 1 face only\n            break\n\n\n        # Display updated frame\n        cv2.imshow(WINDOW_NAME, frame)\n\n        # Check for user actions\n        if cv2.waitKey(1) & 0xFF == 27: # ESC\n            break\n\n        frame_count += 1\n\n    # Release the camera\n    camera.release()\n    cv2.destroyAllWindows()\n\n\ndef run(cam_index, cam_resolution):\n    detector=FaceDetectorModels.HAARCASCADE\n#    detector=FaceDetectorModels.DLIBHOG\n#    detector=FaceDetectorModels.DLIBCNN\n#    detector=FaceDetectorModels.SSDRESNET\n#    detector=FaceDetectorModels.MTCNN\n#    detector=FaceDetectorModels.FACENET\n\n    encoder=FaceEncoderModels.LBPH\n#    encoder=FaceEncoderModels.OPENFACE\n#    encoder=FaceEncoderModels.DLIBRESNET\n#    encoder=FaceEncoderModels.FACENET\n\n    speech_synthesizer=SpeechSynthesizerModels.TTSX3\n#    speech_synthesizer=SpeechSynthesizerModels.TACOTRON\n#    speech_synthesizer=SpeechSynthesizerModels.GOOGLECLOUD\n\n    speech_recognizer=SpeechRecognizerModels.GOOGLECLOUD\n#    speech_recognizer=SpeechRecognizerModels.WITAI\n#    speech_recognizer=SpeechRecognizerModels.HOUNDIFY\n\n    process_facerecognition(detector, encoder, speech_synthesizer, speech_recognizer, cam_index, cam_resolution)\n\n\ndef main(args):\n    if sys.version_info < (3, 0):\n        print(""Error: Python2 is slow. Use Python3 for max performance."")\n        return\n\n    cam_index = int(args.webcam)\n    resolutions = [ RESOLUTION_QVGA, RESOLUTION_VGA, RESOLUTION_HD, RESOLUTION_FULLHD ]\n    try:\n        cam_resolution = resolutions[int(args.resolution)]\n    except:\n        cam_resolution = RESOLUTION_QVGA\n\n    if args.detector and args.encoder and args.speech_synthesizer and args.speech_recognizer:\n        try:\n            detector = FaceDetectorModels(int(args.detector))\n            encoder = FaceEncoderModels(int(args.encoder))\n            speech_synthesizer = SpeechSynthesizerModels(int(args.speech_synthesizer))\n            speech_recognizer = SpeechRecognizerModels(int(args.speech_recognizer))\n\n            print( ""Parameters: {} {} {} {}"".format(detector, encoder, speech_synthesizer, speech_recognizer) )\n            process_facerecognition(detector, encoder, speech_synthesizer, speech_recognizer, cam_index, cam_resolution)\n        except:\n            print( ""Invalid parameter"" )\n        return\n    run(cam_index, cam_resolution)\n\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--detector\', required=False, default=0, \n        help=\'Detector model to use. Options: 0-HAARCASCADE, 1-DLIBHOG, 2-DLIBCNN, 3-SSDRESNET, 4-MTCNN, 5-FACENET\')\n    parser.add_argument(\'--encoder\', required=False, default=0, \n        help=\'Encoder model to use. Options: 0-LBPH, 1-OPENFACE, 2-DLIBRESNET, 3-FACENET\')\n    parser.add_argument(\'--speech_synthesizer\', required=False, default=0, \n        help=\'Speech synthesizer model to use. Options: 0-TTSX3, 1-TACOTRON, 2-GOOGLECLOUD\')\n    parser.add_argument(\'--speech_recognizer\', required=False, default=0, \n        help=\'Speech recognizer model to use. Options: 0-GOOGLECLOUD, 1-WITAI, 2-HOUNDIFY\')\n    parser.add_argument(\'--webcam\', required=False, default=0, \n        help=\'Camera index to use. Default is 0. Assume only 1 camera connected.)\')\n    parser.add_argument(\'--resolution\', required=False, default=0,\n        help=\'Camera resolution to use. Default is 0. Options: 0-QVGA, 1-VGA, 2-HD, 3-FULLHD\')\n    return parser.parse_args(argv)\n\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
training.py,0,"b'import os\nimport sys\nimport argparse\nfrom libfaceid.detector import FaceDetectorModels, FaceDetector\nfrom libfaceid.encoder  import FaceEncoderModels, FaceEncoder\nfrom libfaceid.classifier  import FaceClassifierModels\n\n\n\nINPUT_DIR_DATASET         = ""datasets""\nINPUT_DIR_MODEL_DETECTION = ""models/detection/""\nINPUT_DIR_MODEL_ENCODING  = ""models/encoding/""\nINPUT_DIR_MODEL_TRAINING  = ""models/training/""\nOUTPUT_DIR_AUDIOSET       = ""audiosets/""\nINPUT_DIR_MODEL_SYNTHESIS = ""models/synthesis/""\n\n\ndef ensure_directory(file_path):\n    directory = os.path.dirname(""./"" + file_path)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\ndef get_dataset_names(file_path):\n    for (_d, names, _f) in os.walk(file_path):\n        return names\n    return None\n\ndef train_recognition(model_detector, model_encoder, model_classifier, verify):\n\n    ensure_directory(INPUT_DIR_DATASET)\n\n    print("""")\n    names = get_dataset_names(INPUT_DIR_DATASET)\n    if names is not None:\n        print(""Names "" + str(names))\n        for name in names:\n            for (_d, _n, files) in os.walk(INPUT_DIR_DATASET + ""/"" + name):\n                print(name + "": "" + str(files))\n    print("""")\n\n    ensure_directory(INPUT_DIR_MODEL_TRAINING)\n    face_detector = FaceDetector(model=model_detector, path=INPUT_DIR_MODEL_DETECTION)\n    face_encoder = FaceEncoder(model=model_encoder, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=True)\n    face_encoder.train(face_detector, path_dataset=INPUT_DIR_DATASET, verify=verify, classifier=model_classifier)\n    #print(""train_recognition completed"")\n    \n# generate audio samples for image datasets using text to speech synthesizer\ndef train_audiosets(model_speech_synthesizer):\n\n    ensure_directory(OUTPUT_DIR_AUDIOSET)\n    from libfaceid.speech_synthesizer import SpeechSynthesizer # lazy loading\n    speech_synthesizer = SpeechSynthesizer(model=model_speech_synthesizer, path=INPUT_DIR_MODEL_SYNTHESIS, path_output=OUTPUT_DIR_AUDIOSET)\n    speech_synthesizer.synthesize_datasets(INPUT_DIR_DATASET)\n    #speech_synthesizer.synthesize_name(""libfaceid"")\n    #speech_synthesizer.synthesize(""Hello World"", ""World.wav"")\n\n\ndef run():\n#    detector=FaceDetectorModels.HAARCASCADE\n#    detector=FaceDetectorModels.DLIBHOG\n#    detector=FaceDetectorModels.DLIBCNN\n#    detector=FaceDetectorModels.SSDRESNET\n    detector=FaceDetectorModels.MTCNN\n#    detector=FaceDetectorModels.FACENET\n\n    encoder=FaceEncoderModels.LBPH\n#    encoder=FaceEncoderModels.OPENFACE\n#    encoder=FaceEncoderModels.DLIBRESNET\n#    encoder=FaceEncoderModels.FACENET\n\n    classifier=FaceClassifierModels.NAIVE_BAYES\n#    classifier=FaceClassifierModels.LINEAR_SVM\n#    classifier=FaceClassifierModels.RBF_SVM\n#    classifier=FaceClassifierModels.NEAREST_NEIGHBORS\n#    classifier=FaceClassifierModels.DECISION_TREE\n#    classifier=FaceClassifierModels.RANDOM_FOREST\n#    classifier=FaceClassifierModels.NEURAL_NET\n#    classifier=FaceClassifierModels.ADABOOST\n#    classifier=FaceClassifierModels.QDA\n\n    train_recognition(detector, encoder, classifier, True)\n    print( ""\\nImage dataset training completed!"" )\n\n    # generate audio samples for image datasets using text to speech synthesizer\n    if True: # Set true to enable generation of audio for each person in datasets folder \n        from libfaceid.speech_synthesizer import SpeechSynthesizerModels # lazy loading\n        speech_synthesizer = SpeechSynthesizerModels.TTSX3\n        #speech_synthesizer = SpeechSynthesizerModels.TACOTRON\n        #speech_synthesizer = SpeechSynthesizerModels.GOOGLECLOUD\n        train_audiosets(speech_synthesizer)\n        print( ""Audio samples created!"" )\n\n\ndef main(args):\n    if args.detector and args.encoder:\n        try:\n            detector   = FaceDetectorModels(int(args.detector))\n            encoder    = FaceEncoderModels(int(args.encoder))\n            classifier = FaceClassifierModels(int(args.classifier))\n            print( ""Parameters: {} {} {}"".format(detector, encoder, classifier) )\n\n            train_recognition(detector, encoder, classifier, True)\n            print( ""\\nImage dataset training completed!"" )\n\n            # generate audio samples for image datasets using text to speech synthesizer\n            if args.set_speech_synthesizer:\n                from libfaceid.speech_synthesizer import SpeechSynthesizerModels # lazy loading\n                speech_synthesizer= SpeechSynthesizerModels(int(args.speech_synthesizer))\n                #print( ""Parameters: {}"".format(speech_synthesizer) )\n                train_audiosets(speech_synthesizer)\n                print( ""Audio samples created!"" )\n        except:\n            print( ""Invalid parameter"" )\n        return\n    run()\n\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--detector\', required=False,\n        help=\'Detector model to use. Options: 0-HAARCASCADE, 1-DLIBHOG, 2-DLIBCNN, 3-SSDRESNET, 4-MTCNN, 5-FACENET\')\n    parser.add_argument(\'--encoder\', required=False,\n        help=\'Encoder model to use. Options: 0-LBPH, 1-OPENFACE, 2-DLIBRESNET, 3-FACENET\')\n    parser.add_argument(\'--classifier\', required=False, default=0,\n        help=\'Classifier algorithm to use. Options: 0-NAIVE_BAYES, 1-LINEAR_SVM, 2-RBF_SVM, 3-NEAREST_NEIGHBORS, 4-DECISION_TREE, 5-RANDOM_FOREST, 6-NEURAL_NET, 7-ADABOOST, 8-QDA.\')\n    parser.add_argument(\'--set_speech_synthesizer\', required=False, default=False,\n        help=\'Use text to speech synthesizier.\')\n    parser.add_argument(\'--speech_synthesizer\', required=False, default=0,\n        help=\'Speech synthesizier algorithm to use. Options: 0-TTSX3, 1-TACOTRON, 2-GOOGLECLOUD\')\n    return parser.parse_args(argv)\n\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
libfaceid/__init__.py,0,b''
libfaceid/age.py,0,"b""from enum import Enum\nimport cv2\n\n\n\n\n\nclass FaceAgeEstimatorModels(Enum):\n\n    CV2CAFFE = 0\n    DEFAULT = CV2CAFFE\n\n\nclass FaceAgeEstimator:\n\n    def __init__(self, model=FaceAgeEstimatorModels.DEFAULT, path=None):\n        self._base = None\n        if model == FaceAgeEstimatorModels.CV2CAFFE:\n            self._base = FaceAgeEstimator_CV2CAFFE(path)\n\n    def estimate(self, frame, face_image):\n        return self._base.estimate(frame, face_image)\n\n\nclass FaceAgeEstimator_CV2CAFFE:\n\n    def __init__(self, path):\n        self._mean_values = (78.4263377603, 87.7689143744, 114.895847746)\n        self._classifier = cv2.dnn.readNetFromCaffe(path + 'age_deploy.prototxt', path + 'age_net.caffemodel')\n        self._selection = ['(0, 2)', '(4, 6)', '(8, 12)', '(15, 20)', '(25, 32)', '(38, 43)', '(48, 53)', '(60, 100)']\n\n    def estimate(self, frame, face_image):\n        blob = cv2.dnn.blobFromImage(face_image, 1, (227, 227), self._mean_values, swapRB=False)\n        self._classifier.setInput(blob)\n        prediction = self._classifier.forward()\n        return self._selection[prediction[0].argmax()]\n\n"""
libfaceid/classifier.py,0,"b'from enum import Enum\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.naive_bayes import GaussianNB\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\r\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\r\nfrom sklearn.neural_network import MLPClassifier\r\n\r\n\r\n\r\n\r\n\r\nclass FaceClassifierModels(Enum):\r\n\r\n    NAIVE_BAYES         = 0\r\n    LINEAR_SVM          = 1\r\n    RBF_SVM             = 2\r\n    NEAREST_NEIGHBORS   = 3\r\n    DECISION_TREE       = 4\r\n    RANDOM_FOREST       = 5\r\n    NEURAL_NET          = 6\r\n    ADABOOST            = 7\r\n    QDA                 = 8\r\n    DEFAULT = LINEAR_SVM\r\n\r\n\r\nclass FaceClassifier():\r\n\r\n    def __init__(self, classifier=FaceClassifierModels.DEFAULT):\r\n        self._clf = None\r\n        if classifier == FaceClassifierModels.LINEAR_SVM:\r\n            self._clf = SVC(C=1.0, kernel=""linear"", probability=True)\r\n        elif classifier == FaceClassifierModels.NAIVE_BAYES:\r\n            self._clf = GaussianNB()\r\n        elif classifier == FaceClassifierModels.RBF_SVM:\r\n            self._clf = SVC(C=1, kernel=\'rbf\', probability=True, gamma=2)\r\n        elif classifier == FaceClassifierModels.NEAREST_NEIGHBORS:\r\n            self._clf = KNeighborsClassifier(1)\r\n        elif classifier == FaceClassifierModels.DECISION_TREE:\r\n            self._clf = DecisionTreeClassifier(max_depth=5)\r\n        elif classifier == FaceClassifierModels.RANDOM_FOREST:\r\n            self._clf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\r\n        elif classifier == FaceClassifierModels.NEURAL_NET:\r\n            self._clf = MLPClassifier(alpha=1)\r\n        elif classifier == FaceClassifierModels.ADABOOST:\r\n            self._clf = AdaBoostClassifier()\r\n        elif classifier == FaceClassifierModels.QDA:\r\n            self._clf = QuadraticDiscriminantAnalysis()\r\n        print(""classifier={}"".format(FaceClassifierModels(classifier)))\r\n\r\n    def fit(self, embeddings, labels):\r\n        self._clf.fit(embeddings, labels)\r\n\r\n    def predict(self, vec):\r\n        return self._clf.predict_proba(vec)\r\n\r\n\r\n'"
libfaceid/detector.py,3,"b'import numpy as np\r\nfrom enum import Enum\r\nimport cv2\r\n\r\n\r\n\r\n\r\n\r\nclass FaceDetectorModels(Enum):\r\n\r\n    HAARCASCADE         = 0    # [ML] OpenCV Haar Cascade Classifier\r\n    DLIBHOG             = 1    # [ML] DLIB HOG - Histogram of Oriented Gradients\r\n    DLIBCNN             = 2    # [DL] DLIB CNN // Slow without GPU.\r\n    SSDRESNET           = 3    # [DL] OpenCV SSD with ResNet-10\r\n    MTCNN               = 4    # [DL] Tensorflow Multi-task Cascaded CNN (MTCNN)\r\n    FACENET             = 5    # [DL] Tensorflow FaceNet\'s Multi-task Cascaded CNN (MTCNN)\r\n    DEFAULT = HAARCASCADE\r\n\r\n\r\nclass FaceDetector:\r\n\r\n    def __init__(self, model=FaceDetectorModels.DEFAULT, path=None, optimize=False, minfacesize=20):\r\n        if optimize:\r\n            minfacesize = max(180, minfacesize)\r\n        else:\r\n            minfacesize = minfacesize\r\n        self._base = None\r\n\r\n        if model == FaceDetectorModels.HAARCASCADE:\r\n            self._base = FaceDetector_HAARCASCADE(path, optimize, minfacesize)\r\n        elif model == FaceDetectorModels.DLIBHOG:\r\n            self._base = FaceDetector_DLIBHOG(path, optimize, minfacesize)\r\n        elif model == FaceDetectorModels.DLIBCNN:\r\n            self._base = FaceDetector_DLIBCNN(path, optimize, minfacesize)\r\n        elif model == FaceDetectorModels.SSDRESNET:\r\n            self._base = FaceDetector_SSDRESNET(path, optimize, minfacesize)\r\n        elif model == FaceDetectorModels.MTCNN:\r\n            self._base = FaceDetector_MTCNN(path, optimize, minfacesize)\r\n        elif model == FaceDetectorModels.FACENET:\r\n            self._base = FaceDetector_FACENET(path, optimize, minfacesize)\r\n\r\n    def detect(self, frame):\r\n        return self._base.detect(frame)\r\n\r\n\r\nclass FaceDetector_HAARCASCADE:\r\n\r\n    def __init__(self, path, optimize, minfacesize):\r\n        self._optimize = optimize\r\n        self._minfacesize = minfacesize\r\n        self._detector = cv2.CascadeClassifier(path + \'haarcascade_frontalface_default.xml\')\r\n\r\n    def detect(self, frame):\r\n        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n        frame_gray = cv2.equalizeHist(frame_gray)\r\n        return self._detector.detectMultiScale(frame_gray, 1.1, 5, minSize=(self._minfacesize, self._minfacesize))\r\n\r\n\r\nclass FaceDetector_DLIBHOG:\r\n\r\n    def __init__(self, path, optimize, minfacesize):\r\n        import dlib # lazy loading\r\n        self._optimize = optimize\r\n        self._minfacesize = minfacesize\r\n        self._detector = dlib.get_frontal_face_detector()\r\n\r\n    def detect(self, frame):\r\n        frame_rgb = frame[:, :, ::-1]\r\n        faces = self._detector(frame_rgb, 0)\r\n        faces_updated = []\r\n        for face in faces:\r\n            (x, y, w, h) = (face.left(), face.top(), face.right()-face.left(), face.bottom()-face.top())\r\n            faces_updated.append((x, y, w, h))\r\n        return faces_updated\r\n\r\n\r\nclass FaceDetector_DLIBCNN:\r\n\r\n    def __init__(self, path, optimize, minfacesize):\r\n        import dlib # lazy loading\r\n        self._optimize = optimize\r\n        self._minfacesize = minfacesize\r\n        self._detector = dlib.cnn_face_detection_model_v1(path + \'mmod_human_face_detector.dat\')\r\n\r\n    def detect(self, frame):\r\n        frame_rgb = frame[:, :, ::-1]\r\n        faces = self._detector(frame_rgb, 0)\r\n        faces_updated = []\r\n        for face in faces:\r\n            (x, y, w, h) = (face.rect.left(), face.rect.top(), face.rect.right()-face.rect.left(), face.rect.bottom()-face.rect.top())\r\n            faces_updated.append((x, y, w, h))\r\n        return faces_updated\r\n\r\n\r\nclass FaceDetector_SSDRESNET:\r\n\r\n    def __init__(self, path, optimize, minfacesize):\r\n        self._optimize = optimize\r\n        self._minfacesize = minfacesize\r\n        self._detector = cv2.dnn.readNetFromCaffe(path + \'deploy.prototxt\', path + \'res10_300x300_ssd_iter_140000.caffemodel\')\r\n\r\n    def detect(self, frame):\r\n        if self._optimize:\r\n            imageBlob = cv2.dnn.blobFromImage(cv2.resize(frame, (150,150)), 1.0, (50, 50), (104.0, 177.0, 123.0), swapRB=False, crop=False)\r\n        else:\r\n            imageBlob = cv2.dnn.blobFromImage(cv2.resize(frame, (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0), swapRB=False, crop=False)\r\n        self._detector.setInput(imageBlob)\r\n        faces = self._detector.forward()\r\n        faces_filtered = []\r\n        for index in range(faces.shape[2]):\r\n            confidence = faces[0, 0, index, 2]\r\n            if confidence > 0.5:\r\n                box = faces[0, 0, index, 3:7] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])\r\n                (x, y, x2, y2) = box.astype(""int"")\r\n                (x, y, w, h) = (x, y, x2-x, y2-y)\r\n                faces_filtered.append((x, y, w, h))\r\n        return faces_filtered\r\n\r\n\r\nclass FaceDetector_MTCNN:\r\n\r\n    def __init__(self, path, optimize, minfacesize):\r\n        from mtcnn.mtcnn import MTCNN # lazy loading\r\n        self._optimize = optimize\r\n        self._minfacesize = minfacesize\r\n        self._detector = MTCNN(min_face_size = minfacesize)\r\n\r\n    def detect(self, frame):\r\n        faces = self._detector.detect_faces(frame)\r\n        faces_updated = []\r\n        for face in faces:\r\n            boxd = face[\'box\']\r\n            (x, y, w, h) = (boxd[0], boxd[1], boxd[2], boxd[3])\r\n            faces_updated.append((x, y, w, h))\r\n        return faces_updated\r\n\r\n\r\nclass FaceDetector_FACENET:\r\n\r\n    # TODO: Add border and alignment\r\n    _threshold = [0.6, 0.7, 0.7]  # three steps\'s threshold\r\n    _factor = 0.709  # scale factor\r\n\r\n    def __init__(self, path, optimize, minfacesize):\r\n        import tensorflow as tf                         # lazy loading\r\n        import facenet.src.align.detect_face as facenet # lazy loading\r\n        self._optimize = optimize\r\n        self._minfacesize = minfacesize\r\n        with tf.Graph().as_default():\r\n            gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\r\n            sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\r\n            with sess.as_default():\r\n                self._pnet, self._rnet, self._onet = facenet.create_mtcnn(sess, None)\r\n\r\n    def detect(self, frame):\r\n        import facenet.src.align.detect_face as facenet # lazy loading\r\n        faces, _ = facenet.detect_face(frame, self._minfacesize, \r\n            self._pnet, self._rnet, self._onet, self._threshold, self._factor)\r\n        faces_updated = []\r\n        for face in faces:\r\n            face = face.astype(""int"")\r\n            (x, y, w, h) = (max(face[0], 0), max(face[1],0), \r\n                min(face[2],frame.shape[1])-max(face[0],0), \r\n                min(face[3],frame.shape[0])-max(face[1],0) )\r\n            faces_updated.append((x, y, w, h))\r\n        return faces_updated\r\n'"
libfaceid/emotion.py,0,"b'from enum import Enum\nimport cv2\nimport numpy as np\nfrom keras.models import model_from_json\nfrom keras.preprocessing import image\n\n\n\n\n\nclass FaceEmotionEstimatorModels(Enum):\n\n    KERAS = 0\n    DEFAULT = KERAS\n\n\nclass FaceEmotionEstimator:\n\n    def __init__(self, model=FaceEmotionEstimatorModels.DEFAULT, path=None):\n        self._base = None\n        if model == FaceEmotionEstimatorModels.KERAS:\n            self._base = FaceEmotionEstimator_KERAS(path)\n\n    def estimate(self, frame, face_image):\n        return self._base.estimate(frame, face_image)\n\n\nclass FaceEmotionEstimator_KERAS:\n\n    def __init__(self, path):\n        self._classifier = model_from_json(open(path + \'emotion_deploy.json\', ""r"").read())\n        self._classifier.load_weights(path + \'emotion_net.h5\')\n        self._selection = [\'Angry\', \'Disgust\', \'Fear\', \'Happy\', \'Sad\', \'Surprise\', \'Neutral\']\n\n    def estimate(self, frame, face_image):\n        face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n        face_image = cv2.resize(face_image, (48, 48))\n        img_pixels = np.expand_dims(image.img_to_array(face_image), axis = 0) / 255\n        predictions = self._classifier.predict(img_pixels)\n        max_index = np.argmax(predictions[0])\n        return self._selection[max_index]\n\n'"
libfaceid/encoder.py,4,"b'import os\r\nimport numpy as np\r\nfrom enum import Enum\r\nimport cv2                     # for FaceEncoderModels.LBPH, FaceEncoderModels.OPENFACE\r\nimport pickle                  # for FaceEncoderModels.OPENFACE and FaceEncoderModels.DLIBRESNET\r\nfrom imutils import paths      # for FaceEncoderModels.LBPH\r\nfrom sklearn.preprocessing import LabelEncoder # for FaceEncoderModels\r\nfrom libfaceid.classifier import FaceClassifierModels, FaceClassifier\r\nfrom scipy import misc         # for FaceDetector_FACENET\r\n\r\n\r\n\r\n\r\n\r\nOUTPUT_LBPH_CLASSIFIER       = \'lbph.yml\'\r\nOUTPUT_LBPH_LABELER          = \'lbph_le.pickle\'\r\n\r\nINPUT_OPENFACE_MODEL         = \'openface_nn4.small2.v1.t7\'\r\nOUTPUT_OPENFACE_CLASSIFIER   = \'openface_re.pickle\'\r\nOUTPUT_OPENFACE_LABELER      = \'openface_le.pickle\'\r\n\r\nINPUT_DLIBRESNET_MODEL       = \'dlib_face_recognition_resnet_model_v1.dat\'\r\nINPUT_DLIBRESNET_MODEL2      = \'shape_predictor_5_face_landmarks.dat\'\r\nOUTPUT_DLIBRESNET_CLASSIFIER = \'dlib_re.pickle\'\r\nOUTPUT_DLIBRESNET_LABELER    = \'dlib_le.pickle\'\r\n\r\nINPUT_FACENET_MODEL          = \'facenet_20180402-114759.pb\'\r\nOUTPUT_FACENET_CLASSIFIER    = \'facenet_re.pickle\'\r\nOUTPUT_FACENET_LABELER       = \'facenet_le.pickle\'\r\n\r\n\r\nclass FaceEncoderModels(Enum):\r\n\r\n    LBPH                = 0    # [ML] LBPH Local Binary Patterns Histograms\r\n    OPENFACE            = 1    # [DL] OpenCV OpenFace\r\n    DLIBRESNET          = 2    # [DL] DLIB ResNet\r\n    FACENET             = 3    # [DL] FaceNet implementation by David Sandberg\r\n    # VGGFACE1_VGG16    = 4    # Refer to models\\others\\vggface_recognition\r\n    # VGGFACE2_RESNET50 = 5    # Refer to models\\others\\vggface_recognition\r\n    DEFAULT = LBPH\r\n\r\n\r\nclass FaceEncoder():\r\n\r\n    def __init__(self, model=FaceEncoderModels.DEFAULT, path=None, path_training=None, training=False):\r\n        self._base = None\r\n        if model == FaceEncoderModels.LBPH:\r\n            self._base = FaceEncoder_LBPH(path, path_training, training)\r\n        elif model == FaceEncoderModels.OPENFACE:\r\n            self._base = FaceEncoder_OPENFACE(path, path_training, training)\r\n        elif model == FaceEncoderModels.DLIBRESNET:\r\n            self._base = FaceEncoder_DLIBRESNET(path, path_training, training)\r\n        elif model == FaceEncoderModels.FACENET:\r\n            self._base = FaceEncoder_FACENET(path, path_training, training)\r\n\r\n    def identify(self, frame, face_rect):\r\n        try:\r\n            return self._base.identify(frame, face_rect)\r\n        except:\r\n            return ""Unknown"", 0\r\n\r\n    def train(self, face_detector, path_dataset, verify=False, classifier=FaceClassifierModels.LINEAR_SVM):\r\n        try:\r\n            self._base.train(face_detector, path_dataset, verify, classifier)\r\n            print(""Note: Make sure you use the same models for training and testing"")\r\n        except:\r\n            return ""Training failed! an exception occurred!""\r\n\r\n\r\nclass FaceEncoder_Utils():\r\n\r\n    def save_training(self, classifier, knownNames, knownEmbeddings, output_clf, output_le):\r\n        #print(len(knownNames))\r\n        #print(len(knownEmbeddings))\r\n        #print(""[INFO] Number of classes = {}"".format(knownNames))\r\n        le = LabelEncoder()\r\n        labels = le.fit_transform(knownNames)\r\n        #print(le.classes_)\r\n        #print(labels)\r\n        clf = FaceClassifier(classifier)\r\n        clf.fit(knownEmbeddings, labels)\r\n        f = open(output_clf, ""wb"")\r\n        f.write(pickle.dumps(clf))\r\n        f.close()\r\n        f = open(output_le, ""wb"")\r\n        f.write(pickle.dumps(le))\r\n        f.close() \r\n\r\n\r\n\r\nclass FaceEncoder_LBPH():\r\n\r\n    def __init__(self, path=None, path_training=None, training=False):\r\n        self._path_training = path_training\r\n        self._label_encoder = None\r\n        self._clf = cv2.face.LBPHFaceRecognizer_create()\r\n        if training == False:\r\n            self._clf.read(self._path_training + OUTPUT_LBPH_CLASSIFIER)\r\n            self._label_encoder = pickle.loads(open(self._path_training + OUTPUT_LBPH_LABELER, ""rb"").read())\r\n            #print(self._label_encoder.classes_)\r\n\r\n    def identify(self, frame, face_rect):\r\n        face_id = ""Unknown""\r\n        confidence = 99.99\r\n        (x, y, w, h) = face_rect\r\n        frame_gray = frame[y:y+h, x:x+w]\r\n        face = cv2.cvtColor(frame_gray, cv2.COLOR_BGR2GRAY)\r\n        id, confidence = self._clf.predict(face)\r\n        if confidence > 99.99: \r\n            confidence = 99.99\r\n        face_id = self._label_encoder.classes_[id]\r\n        return face_id, confidence\r\n\r\n    def train(self, face_detector, path_dataset, verify, classifier):\r\n        imagePaths = sorted(list(paths.list_images(path_dataset)))\r\n        faceSamples=[]\r\n        ids = []\r\n        knownNames = []\r\n\r\n        id = -1\r\n        for (i, imagePath) in enumerate(imagePaths):\r\n            frame = cv2.imread(imagePath, cv2.IMREAD_COLOR)\r\n            name = imagePath.split(os.path.sep)[-2]\r\n            try:\r\n                id = knownNames.index(name)\r\n            except:\r\n                id = id + 1\r\n            #print(""name=%s id=%d"" % (name, id))\r\n\r\n            frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n            faces = face_detector.detect(frame)\r\n            for (index, face) in enumerate(faces):\r\n                (x, y, w, h) = face\r\n                faceSamples.append(frame_gray[y:y+h,x:x+w])\r\n                knownNames.append(name)\r\n                ids.append(id)\r\n                #cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 255), 1)\r\n                break\r\n\r\n            if verify and len(faces) != 1:\r\n                print(""[WARNING] Image {} has {} faces "".format(imagePath, len(faces)))\r\n                cv2.imshow(\'frame\', frame)\r\n                cv2.waitKey(10)\r\n\r\n        #print(ids)\r\n        #print(knownNames)\r\n\r\n        if verify:\r\n            cv2.destroyAllWindows()\r\n\r\n        self._clf.train(faceSamples, np.array(ids))\r\n        self._clf.write(self._path_training + OUTPUT_LBPH_CLASSIFIER)\r\n\r\n        le = LabelEncoder()\r\n        labels = le.fit_transform(knownNames)\r\n        #print(le.classes_)\r\n        #print(labels)\r\n        \r\n        f = open(self._path_training + OUTPUT_LBPH_LABELER, ""wb"")\r\n        f.write(pickle.dumps(le))\r\n        f.close()\r\n\r\n\r\nclass FaceEncoder_OPENFACE():\r\n\r\n    def __init__(self, path=None, path_training=None, training=False):\r\n        self._path_training = path_training\r\n        self._clf = None\r\n        self._label_encoder = None\r\n        self._embedder = cv2.dnn.readNetFromTorch(path + INPUT_OPENFACE_MODEL)\r\n        if training == False:\r\n            self._clf = pickle.loads(open(self._path_training + OUTPUT_OPENFACE_CLASSIFIER, ""rb"").read())\r\n            self._label_encoder = pickle.loads(open(self._path_training + OUTPUT_OPENFACE_LABELER, ""rb"").read())\r\n            #print(self._label_encoder.classes_)\r\n\r\n    def identify(self, frame, face_rect):\r\n        face_id = ""Unknown""\r\n        confidence = 99.99\r\n        vec = self.encode(frame, face_rect)\r\n        predictions_face = self._clf.predict(vec)[0]\r\n        id = np.argmax(predictions_face)\r\n        confidence = predictions_face[id] * 100\r\n        face_id = self._label_encoder.classes_[id]\r\n        return face_id, confidence\r\n\r\n    def encode(self, frame, face_rect):\r\n        (x, y, w, h) = face_rect\r\n        face = frame[y:y+h, x:x+w]\r\n        faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96), (0, 0, 0), swapRB=True, crop=False)\r\n        self._embedder.setInput(faceBlob)\r\n        return self._embedder.forward()\r\n\r\n    def train(self, face_detector, path_dataset, verify, classifier):\r\n        knownEmbeddings = []\r\n        knownNames = []\r\n        imagePaths = sorted(list(paths.list_images(path_dataset)))\r\n        for (j, imagePath) in enumerate(imagePaths):\r\n            name = imagePath.split(os.path.sep)[-2]\r\n            frame = cv2.imread(imagePath, cv2.IMREAD_COLOR)\r\n            faces = face_detector.detect(frame)\r\n            for face in faces:\r\n                vec = self.encode(frame, face)\r\n                knownNames.append(name)\r\n                knownEmbeddings.append(vec.flatten())\r\n        FaceEncoder_Utils().save_training(classifier, knownNames, knownEmbeddings, \r\n            self._path_training + OUTPUT_OPENFACE_CLASSIFIER, \r\n            self._path_training + OUTPUT_OPENFACE_LABELER)\r\n\r\n\r\nclass FaceEncoder_DLIBRESNET():\r\n\r\n    def __init__(self, path=None, path_training=None, training=False):\r\n        import dlib # lazy loading\r\n        self._path_training = path_training\r\n        self._clf = None\r\n        self._label_encoder = None\r\n        self._embedder = dlib.face_recognition_model_v1(path + INPUT_DLIBRESNET_MODEL)\r\n        self._shaper = dlib.shape_predictor(path + INPUT_DLIBRESNET_MODEL2)\r\n        if training == False:\r\n            self._clf = pickle.loads(open(self._path_training + OUTPUT_DLIBRESNET_CLASSIFIER, ""rb"").read())\r\n            self._label_encoder = pickle.loads(open(self._path_training + OUTPUT_DLIBRESNET_LABELER, ""rb"").read())\r\n            #print(self._label_encoder.classes_)\r\n\r\n    def identify(self, frame, face_rect):\r\n        face_id = ""Unknown""\r\n        confidence = 99.99\r\n        vec = self.encode(frame, face_rect)\r\n        predictions_face = self._clf.predict(vec)[0]\r\n        #print(predictions_face)\r\n        id = np.argmax(predictions_face)\r\n        confidence = predictions_face[id] * 100\r\n        face_id = self._label_encoder.classes_[id]\r\n        return face_id, confidence\r\n\r\n    def encode(self, frame, face_rect):\r\n        import dlib # lazy loading\r\n        (x, y, w, h) = face_rect\r\n        rect = dlib.rectangle(x, y, x+w, y+h)\r\n        frame_rgb = frame[:, :, ::-1]\r\n        shape = self._shaper(frame_rgb, rect)\r\n        vec = self._embedder.compute_face_descriptor(frame_rgb, shape)\r\n        return np.array([vec])\r\n\r\n    def train(self, face_detector, path_dataset, verify, classifier):\r\n        knownEmbeddings = []\r\n        knownNames = []\r\n        imagePaths = sorted(list(paths.list_images(path_dataset)))\r\n        for (j, imagePath) in enumerate(imagePaths):\r\n            name = imagePath.split(os.path.sep)[-2]\r\n            frame = cv2.imread(imagePath, cv2.IMREAD_COLOR)\r\n            faces = face_detector.detect(frame)\r\n            for face in faces:\r\n                vec = self.encode(frame, face)\r\n                knownNames.append(name)\r\n                knownEmbeddings.append(vec.flatten())\r\n        FaceEncoder_Utils().save_training(classifier, knownNames, knownEmbeddings, \r\n            self._path_training + OUTPUT_DLIBRESNET_CLASSIFIER, \r\n            self._path_training + OUTPUT_DLIBRESNET_LABELER)\r\n\r\n\r\nclass FaceEncoder_FACENET():\r\n\r\n    _face_crop_size=160\r\n    _face_crop_margin=0\r\n\r\n    def __init__(self, path=None, path_training=None, training=False):\r\n        import tensorflow as tf               # lazy loading\r\n        import facenet.src.facenet as facenet # lazy loading\r\n        self._path_training = path_training\r\n        self._sess = tf.Session()\r\n        with self._sess.as_default():\r\n            facenet.load_model(path + INPUT_FACENET_MODEL)\r\n        if training == False:\r\n            self._clf = pickle.loads(open(self._path_training + OUTPUT_FACENET_CLASSIFIER, ""rb"").read())\r\n            self._label_encoder = pickle.loads(open(self._path_training + OUTPUT_FACENET_LABELER, ""rb"").read())\r\n\r\n    def identify(self, frame, face_rect):\r\n        vec = self.encode(frame, face_rect)\r\n        predictions_face = self._clf.predict([vec])[0]\r\n        id = np.argmax(predictions_face)\r\n        confidence = predictions_face[id] * 100\r\n        face_id = self._label_encoder.classes_[id]\r\n        return face_id, confidence\r\n\r\n    def set_face_crop(self, crop_size, crop_margin):\r\n        self._face_crop_size = crop_size\r\n        self._face_crop_margin = crop_margin\r\n\r\n    def encode(self, frame, face_rect):\r\n        import tensorflow as tf               # lazy loading\r\n        import facenet.src.facenet as facenet # lazy loading\r\n        (x, y, w, h) = face_rect\r\n        if self._face_crop_margin:\r\n            (x, y, w, h) = (\r\n                max(x - int(self._face_crop_margin/2), 0), \r\n                max(y - int(self._face_crop_margin/2), 0), \r\n                min(x+w + int(self._face_crop_margin/2), frame.shape[1]) - x, \r\n                min(y+h + int(self._face_crop_margin/2), frame.shape[0]) - y)\r\n        face = misc.imresize(frame[y:y+h, x:x+w, :], (self._face_crop_size, self._face_crop_size), interp=\'bilinear\')\r\n        images_placeholder = tf.get_default_graph().get_tensor_by_name(""input:0"")\r\n        embeddings = tf.get_default_graph().get_tensor_by_name(""embeddings:0"")\r\n        phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(""phase_train:0"")\r\n        prewhiten_face = facenet.prewhiten(face)\r\n        feed_dict = {images_placeholder: [prewhiten_face], phase_train_placeholder: False}\r\n        return self._sess.run(embeddings, feed_dict=feed_dict)[0]\r\n\r\n    def train(self, face_detector, path_dataset, verify, classifier):\r\n        knownEmbeddings = []\r\n        knownNames = []\r\n        imagePaths = sorted(list(paths.list_images(path_dataset)))\r\n        for (j, imagePath) in enumerate(imagePaths):\r\n            name = imagePath.split(os.path.sep)[-2]\r\n            frame = cv2.imread(imagePath, cv2.IMREAD_COLOR)\r\n            faces = face_detector.detect(frame)\r\n            for face in faces:\r\n                vec = self.encode(frame, face)\r\n                knownNames.append(name)\r\n                knownEmbeddings.append(vec.flatten())\r\n        FaceEncoder_Utils().save_training(classifier, knownNames, knownEmbeddings, \r\n            self._path_training + OUTPUT_FACENET_CLASSIFIER, \r\n            self._path_training + OUTPUT_FACENET_LABELER)\r\n\r\n'"
libfaceid/gender.py,0,"b""from enum import Enum\nimport cv2\n\n\n\n\n\nclass FaceGenderEstimatorModels(Enum):\n\n    CV2CAFFE = 0\n    DEFAULT = CV2CAFFE\n\n\nclass FaceGenderEstimator:\n\n    def __init__(self, model=FaceGenderEstimatorModels.DEFAULT, path=None):\n        self._base = None\n        if model == FaceGenderEstimatorModels.CV2CAFFE:\n            self._base = FaceGenderEstimator_CV2CAFFE(path)\n\n    def estimate(self, frame, face_image):\n        return self._base.estimate(frame, face_image)\n\n\nclass FaceGenderEstimator_CV2CAFFE:\n\n    def __init__(self, path):\n        self._mean_values = (78.4263377603, 87.7689143744, 114.895847746)\n        self._classifier = cv2.dnn.readNetFromCaffe(path + 'gender_deploy.prototxt', path + 'gender_net.caffemodel')\n        self._selection = ['Male', 'Female']\n\n    def estimate(self, frame, face_image):\n        blob = cv2.dnn.blobFromImage(face_image, 1, (227, 227), self._mean_values, swapRB=False)\n        self._classifier.setInput(blob)\n        prediction = self._classifier.forward()\n        return self._selection[prediction[0].argmax()]\n\n"""
libfaceid/liveness.py,0,"b'import numpy as np\r\nfrom enum import Enum\r\nimport cv2\r\nfrom imutils import face_utils\r\nfrom sklearn.externals import joblib\r\n\r\n\r\n\r\n\r\n\r\nclass FaceLivenessModels(Enum):\r\n\r\n    EYESBLINK_MOUTHOPEN = 0 # depends on multiple frames\r\n    COLORSPACE_YCRCBLUV = 1 # depends on single frame only\r\n    DEFAULT             = EYESBLINK_MOUTHOPEN\r\n\r\n\r\nclass FaceLiveness:\r\n\r\n    def __init__(self, model=FaceLivenessModels.DEFAULT, path=None):\r\n        if model == FaceLivenessModels.EYESBLINK_MOUTHOPEN:\r\n            self._base = FaceLiveness_EYESBLINK_MOUTHOPEN(path)\r\n        elif model == FaceLivenessModels.COLORSPACE_YCRCBLUV:\r\n            self._base = FaceLiveness_COLORSPACE_YCRCBLUV(path)\r\n\r\n    def is_fake(self, frame, face, flag=0):\r\n        res = self._base.is_fake(frame, face, flag)\r\n        return res\r\n\r\n    def is_eyes_close(self, frame, face):\r\n        return self._base.is_eyes_close(frame, face)\r\n\r\n    def is_mouth_open(self, frame, face):\r\n        return self._base.is_mouth_open(frame, face)\r\n\r\n    def set_eye_threshold(self, threshold):\r\n        self._base.set_eye_threshold(threshold)\r\n\r\n    def get_eye_threshold(self):\r\n        return self._base.get_eye_threshold()\r\n\r\n    def set_mouth_threshold(self, threshold):\r\n        self._base.set_mouth_threshold(threshold)\r\n\r\n    def get_mouth_threshold(self):\r\n        return self._base.get_mouth_threshold()\r\n\r\n\r\nclass FaceLiveness_EYESBLINK_MOUTHOPEN:\r\n\r\n    _ear_threshold = 0.3 # eye aspect ratio (ear); less than this value, means eyes is close\r\n    _mar_threshold = 0.3 # mouth aspect ratio (mar); more than this value, means mouth is open\r\n    _ear_consecutive_frames = 1\r\n\r\n\r\n    def __init__(self, path):\r\n        import dlib # lazy loading\r\n        # use dlib 68-point facial landmark\r\n        self._detector = dlib.shape_predictor(path + \'shape_predictor_68_face_landmarks.dat\')\r\n        (self._leye_start, self._leye_end) = face_utils.FACIAL_LANDMARKS_IDXS[""left_eye""]\r\n        (self._reye_start, self._reye_end) = face_utils.FACIAL_LANDMARKS_IDXS[""right_eye""]\r\n        try:\r\n            (self._mouth_start, self._mouth_end) = face_utils.FACIAL_LANDMARKS_IDXS[""inner_mouth""]\r\n        except:\r\n            (self._mouth_start, self._mouth_end) = (60, 68)\r\n\r\n\r\n    def is_eyes_close(self, frame, face):\r\n        shape = self.get_shape(frame, face)\r\n        # get the average eye aspect ratio (ear) of left eye and right eye\r\n        average_ear = (self.eye_aspect_ratio(shape[self._leye_start:self._leye_end]) + self.eye_aspect_ratio(shape[self._reye_start:self._reye_end])) / 2.0\r\n        return (average_ear < self._ear_threshold), average_ear\r\n\r\n    def set_eye_threshold(self, threshold):\r\n        self._ear_threshold = threshold\r\n\r\n    def get_eye_threshold(self):\r\n        return self._ear_threshold\r\n\r\n\r\n    def is_mouth_open(self, frame, face):\r\n        shape = self.get_shape(frame, face)\r\n        # get the mouth aspect ratio (mar) of inner mouth\r\n        mar = self.mouth_aspect_ratio(shape[self._mouth_start:self._mouth_end])\r\n        return (mar > self._mar_threshold), mar\r\n\r\n    def set_mouth_threshold(self, threshold):\r\n        self._mar_threshold = threshold\r\n\r\n    def get_mouth_threshold(self):\r\n        return self._mar_threshold\r\n\r\n\r\n    # private function\r\n    def mouth_aspect_ratio(self, mouth):\r\n        # (|m1-m7|+|m2-m6|+|m3-m5|) / (2|m0-m4|)\r\n        # np.linalg.norm is faster than dist.euclidean\r\n        return (np.linalg.norm(mouth[1]-mouth[7]) + np.linalg.norm(mouth[2]-mouth[6]) + np.linalg.norm(mouth[3]-mouth[5])) / (2.0 * np.linalg.norm(mouth[0]-mouth[4]))\r\n        #return (dist.euclidean(mouth[1],mouth[7]) + dist.euclidean(mouth[2],mouth[6]) + dist.euclidean(mouth[3],mouth[5])) / (2.0 * dist.euclidean(mouth[0],mouth[4]))\r\n\r\n    # private function\r\n    def eye_aspect_ratio(self, eye):\r\n        # https://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf\r\n        # (|e1-e5|+|e2-e4|) / (2|e0-e3|)\r\n        # np.linalg.norm is faster than dist.euclidean\r\n        return (np.linalg.norm(eye[1]-eye[5]) + np.linalg.norm(eye[2]-eye[4])) / (2.0 * np.linalg.norm(eye[0]-eye[3]))\r\n        #return (dist.euclidean(eye[1],eye[5]) + dist.euclidean(eye[2],eye[4])) / (2.0 * dist.euclidean(eye[0],eye[3]))\r\n\r\n    # private function\r\n    def get_shape(self, frame, face):\r\n        import dlib # lazy loading\r\n        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n        (x, y, w, h) = face\r\n        rect = dlib.rectangle(int(x), int(y), int(x+w), int(y+h))\r\n        shape = self._detector(frame_gray, rect)\r\n        coords = np.zeros((shape.num_parts, 2), dtype=""int"")\r\n        for i in range(0, shape.num_parts):\r\n            coords[i] = (shape.part(i).x, shape.part(i).y)\r\n        return coords\r\n\r\n\r\nclass FaceLiveness_COLORSPACE_YCRCBLUV:\r\n\r\n    _threshold_print = 0.35\r\n    _threshold_replay = 0.93\r\n\r\n    def __init__(self, path):\r\n        from sklearn.externals import joblib\r\n        # https://github.com/ee09115/spoofing_detection\r\n        # https://www.idiap.ch/dataset/printattack\r\n        # https://www.idiap.ch/dataset/replayattack\r\n        try:\r\n            self._clf_print = joblib.load(path + ""colorspace_ycrcbluv_print.pkl"")\r\n        except Exception as e:\r\n            print(""FaceLiveness_COLORSPACE_YCRCBLUV joblib exception {}"".format(e))\r\n        try:\r\n            self._clf_replay = joblib.load(path + ""colorspace_ycrcbluv_replay.pkl"")\r\n        except:\r\n            print(""FaceLiveness_COLORSPACE_YCRCBLUV joblib2 exception {}"".format(e))\r\n\r\n    def is_fake(self, frame, face, flag=0):\r\n        feature_vector = self.get_embeddings(frame, face)\r\n        if flag == 0:\r\n            prediction = self._clf_print.predict_proba(feature_vector)\r\n            #print(""print ={:.2f}"".format(np.mean(prediction[0][1])))\r\n            if np.mean(prediction[0][1]) >= self._threshold_print:\r\n                return True\r\n            return False\r\n        else:\r\n            prediction = self._clf_replay.predict_proba(feature_vector)\r\n            #print(""replay={:.2f}"".format(np.mean(prediction[0][1])))\r\n            if np.mean(prediction[0][1]) >= self._threshold_replay:\r\n                return True\r\n            return False\r\n        return False\r\n\r\n\r\n    # private function\r\n    def get_embeddings(self, frame, face):\r\n        (x, y, w, h) = face\r\n        img = frame[y:y+h, x:x+w]\r\n        img_ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)\r\n        img_luv = cv2.cvtColor(img, cv2.COLOR_BGR2LUV)\r\n        hist_ycrcb = self.calc_hist(img_ycrcb)\r\n        hist_luv = self.calc_hist(img_luv)\r\n        feature_vector = np.append(hist_ycrcb.ravel(), hist_luv.ravel())\r\n        return feature_vector.reshape(1, len(feature_vector))\r\n\r\n    # private function\r\n    def calc_hist(self, img):\r\n        histogram = [0] * 3\r\n        for j in range(3):\r\n            histr = cv2.calcHist([img], [j], None, [256], [0, 256])\r\n            histr *= 255.0 / histr.max()\r\n            histogram[j] = histr\r\n        return np.array(histogram)\r\n\r\n'"
libfaceid/pose.py,0,"b'from enum import Enum\nimport cv2\nimport numpy as np\nimport dlib # for FacePoseEstimatorModels.DLIB68\n\n\n\n\n\ncolor_green  = (0,255,0)\ncolor_blue   = (255,0,0)\ncolor_red    = (0,0,255)\ncolor_yellow = (0,255,255)\ncolor_pink   = (255,0,255)\ncolor_white  = (255,255,255)\ncolor_black  = (0,0,0)\n\n\n\nclass FacePoseEstimatorModels(Enum):\n\n    DLIB68 = 0\n    DEFAULT = DLIB68\n\n\nclass FacePoseEstimatorColor(Enum):\n\n    GREEN  = 0\n    BLUE   = 1\n    RED    = 2\n    YELLOW = 3\n    PINK   = 4\n    WHITE  = 5\n    BLACK  = 6\n    DEFAULT = GREEN\n\n\nclass FacePoseEstimatorOverlay(Enum):\n\n    ORIG   = 0\n    OZ     = 1\n    INT    = 2\n    INTOZ  = 3\n    DEFAULT = ORIG\n\n\nclass FacePoseEstimator():\n\n    def __init__(self, model=FacePoseEstimatorModels.DEFAULT, path=None, overlay=FacePoseEstimatorOverlay.DEFAULT, color=FacePoseEstimatorColor.DEFAULT):\n        self._base = None\n        colors = [color_green, color_blue, color_red, color_yellow, color_pink, color_white, color_black]\n        if model == FacePoseEstimatorModels.DLIB68:\n            self._base = FacePoseEstimator_DLIB68(path, overlay, colors[color.value])\n\n    def detect(self, frame, face):\n        return self._base.detect(frame, face)\n\n    def add_overlay(self, frame, shape):\n        return self._base.add_overlay(frame, shape)\n\n\nclass FacePoseEstimator_DLIB68():\n\n    def __init__(self, path, overlay, color):\n        self._overlay = overlay\n        self._color = color\n        self._detector = dlib.shape_predictor(path + \'shape_predictor_68_face_landmarks.dat\')\n        if self._overlay == FacePoseEstimatorOverlay.OZ:\n            self._connection = {(0,1), (1,2), (2,3), (3,4), (4,5), (5,6), (6,7), (7,8), (8,9), (9,10), (10,11), (11,12), (12,13), (13,14), (14,15), (15,16), \n                (16,26), (26,25), (25,24), (24,23), (23,22), \n                (22,21), (21,20), (20,19), (19,18), (18,17), (17,0),\n                (36,37), (37,38), (38,39), (39,40), (40,41), (41,36), # left eye\n                (42,43), (43,44), (44,45), (45,46), (46,47), (47,42), # right eye\n                (48,49), (49,50), (50,51), (51,52), (52,53), (53,54), (54,55), (55,56), (56,57), (57,58), (58,59), (59,48), # outer lip\n                (60,61), (61,62), (62,63), (63,64), (64,65), (65,66), (66,67), (67,60), # inner lip\n                (31,32), (32,33), (33,34), (34,35), (27,28), (28,29), (29,30), (30,33), (27,31), (27,35), # nose\n            }\n        elif self._overlay == FacePoseEstimatorOverlay.ORIG:\n            self._connection = {(0,1), (1,2), (2,3), (3,4), (4,5), (5,6), (6,7), (7,8), (8,9), (9,10), (10,11), (11,12), (12,13), (13,14), (14,15), (15,16), \n                (26,25), (25,24), (24,23), (23,22), \n                (21,20), (20,19), (19,18), (18,17),\n                (36,37), (37,38), (38,39), (39,40), (40,41), (41,36), # left eye\n                (42,43), (43,44), (44,45), (45,46), (46,47), (47,42), # right eye\n                (48,49), (49,50), (50,51), (51,52), (52,53), (53,54), (54,55), (55,56), (56,57), (57,58), (58,59), (59,48), # outer lip\n                (60,61), (61,62), (62,63), (63,64), (64,65), (65,66), (66,67), (67,60), # inner lip\n                (31,32), (32,33), (33,34), (34,35), (27,28), (28,29), (29,30), (30,31), (30,35) # nose\n            }\n        elif self._overlay == FacePoseEstimatorOverlay.INT or self._overlay == FacePoseEstimatorOverlay.INTOZ:\n            self._connection = {(1,4), (4,7), (7,9), (9,12), (12, 15), (1, 36), (15,45), (1, 31), (15,45), (15,35), (39, 42),\n                (36,37), (37,38), (38,39), (39,40), (40,41), (41,36), # left eye\n                (42,43), (43,44), (44,45), (45,46), (46,47), (47,42), # right eye\n                (48,51), (51,54), (54,57), (57,48), (48, 4), (54,12), # outer lip\n                (30,31), (31,35), (35,30), (30,39), (30,42), (31,48), (35,54), # nose\n                (48, 7), (7, 57), (57, 9), (9,54),  (31, 39), (35, 42), (31, 36), (35, 45), (31, 51), (51, 35), (48, 1), (54,15),\n            }\n\n    def detect(self, frame, face):\n        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        (x, y, w, h) = face\n        rect = dlib.rectangle(int(x), int(y), int(x+w), int(y+h))\n        shape = self.shape_to_np(self._detector(frame_gray, rect))\n        return shape\n\n    def add_overlay(self, frame, shape):\n        if self._overlay == FacePoseEstimatorOverlay.OZ:\n            j = 0\n            for (x, y) in shape:\n                if j >= 36 and j <= 47:\n                    cv2.circle(frame, (x, y), 3, color_green, -1)\n                elif j >= 48 and j <= 67:\n                    cv2.circle(frame, (x, y), 3, color_pink, -1)\n                elif j >= 27 and j <= 35:\n                    cv2.circle(frame, (x, y), 3, color_red, -1)\n                else:\n                    cv2.circle(frame, (x, y), 3, color_yellow, -1)\n                j += 1\n            for conn in self._connection:\n                cv2.line(frame, (shape[conn[0]][0], shape[conn[0]][1]), (shape[conn[1]][0], shape[conn[1]][1]), color_yellow, 1)\n        elif self._overlay == FacePoseEstimatorOverlay.ORIG:\n            for conn in self._connection:\n                cv2.line(frame, (shape[conn[0]][0], shape[conn[0]][1]), (shape[conn[1]][0], shape[conn[1]][1]), self._color, 1)\n        elif self._overlay == FacePoseEstimatorOverlay.INT:\n#            cv2.polylines(frame, [np.array(self._connection.values, np.int32)], True, color_white, thickness=3)\n            for conn in self._connection:\n                cv2.circle(frame, (shape[conn[0]][0], shape[conn[0]][1]), 2, self._color, -1, cv2.LINE_AA)\n                cv2.line(frame, (shape[conn[0]][0], shape[conn[0]][1]), (shape[conn[1]][0], shape[conn[1]][1]), self._color, 1, cv2.LINE_AA)\n        elif self._overlay == FacePoseEstimatorOverlay.INTOZ:\n            for conn in self._connection:\n                cv2.line(frame, (shape[conn[0]][0], shape[conn[0]][1]), (shape[conn[1]][0], shape[conn[1]][1]), color_yellow, 1)\n\n    # private function\n    def shape_to_np(self, shape, dtype=""int""):\n        coords = np.zeros((shape.num_parts, 2), dtype=dtype)\n        for i in range(0, shape.num_parts):\n            coords[i] = (shape.part(i).x, shape.part(i).y)\n        return coords\n'"
libfaceid/speech_recognizer.py,0,"b'from enum import Enum\n\n\n\n#registered_mic = \'C922 Pro Stream Webcam: USB Audio (hw:1,0)\'\nregistered_mic= \'Microphone (C922 Pro Stream Web\'\n\n\n# Added these accounts for testing purposes only\nCLIENT_KEY_WITAI    = ""KCSDQHNAJ74ORMOG2PFOO4NRZBJVAIDT""\nCLIENT_ID_HOUNDIFY  = ""n33EnhuJaA6DZpYqRwiYBg==""\nCLIENT_KEY_HOUNDIFY = ""C06_n6A37PS4WDRb3Zi7mejyZ9BbLABya-n6nAsskyt7ZEos5PHZKsanRJlXuYhg5yPtDC6k9Xb--B1iVumNVA==""\n\n\nclass SpeechRecognizerModels(Enum):\n\n    GOOGLECLOUD = 0 # requires internet access\n    WITAI       = 1 # requires internet access, requires key from https://wit.ai\n    HOUNDIFY    = 2 # requires internet access, requires key and id from https://www.houndify.com\n    DEFAULT = GOOGLECLOUD\n\n\nclass SpeechRecognizer:\n\n    def __init__(self, model=SpeechRecognizerModels.DEFAULT, path=None):\n        if model == SpeechRecognizerModels.GOOGLECLOUD:\n            self._base = SpeechRecognizer_Common(model)\n        elif model == SpeechRecognizerModels.WITAI:\n            self._base = SpeechRecognizer_Common(model)\n        elif model == SpeechRecognizerModels.HOUNDIFY:\n            self._base = SpeechRecognizer_Common(model)\n\n    def start(self, words, callback):\n        self._base.start(words, callback)\n\n    def stop(self):\n        self._base.stop()\n\n\nclass SpeechRecognizer_Common:\n\n    def __init__(self, model):\n        import speech_recognition # lazy loading\n        import pyaudio\n        device_count = pyaudio.PyAudio().get_device_count() - 1\n        print(""\\ndevice_count: "" + str(device_count))\n        self._r = speech_recognition.Recognizer()\n        try:\n            mics = speech_recognition.Microphone.list_microphone_names()\n            print(""mics: "" + str(mics))\n            index = mics.index(registered_mic)\n        except:\n            index = -1\n        print(""index: "" + str(index))\n        try:\n            if index == -1:\n                self._m = speech_recognition.Microphone()\n            else:\n                self._m = speech_recognition.Microphone(device_index=index)                \n        except:\n            self._m = None\n            print(""SpeechRecognizer_Common, no mic detected!"")\n        self._model = model\n\n    def start(self, words, user_callback):\n        self._user_callback = user_callback\n        self._trigger_words = [word.lower() for word in words]\n        #print(self._trigger_words)\n        with self._m as source:\n            self._r.adjust_for_ambient_noise(source)\n        self._listener = self._r.listen_in_background(self._m, self.callback)\n\n    def stop(self):\n        self._listener(wait_for_stop=True)\n\n    def callback(self, recognizer, audio):\n        text = None\n        # recognize input from microphone\n        try:\n            if self._model == SpeechRecognizerModels.GOOGLECLOUD:\n                print(""test"")\n                text = recognizer.recognize_google(audio)\n                #print(""test2"")\n            elif self._model == SpeechRecognizerModels.WITAI:\n                text = recognizer.recognize_wit(audio, key=CLIENT_KEY_WITAI)\n                text = text.lower()\n            elif self._model == SpeechRecognizerModels.HOUNDIFY:\n                text = recognizer.recognize_houndify(audio, client_id=CLIENT_ID_HOUNDIFY, client_key=CLIENT_KEY_HOUNDIFY)\n        except Exception as e:\n            #print(""callback exception "" + str(e))\n            if self._model != SpeechRecognizerModels.GOOGLECLOUD:\n                text = recognizer.recognize_google(audio)\n        \n        if text is not None:\n            text = text.lower()\n\n        # check if detected text is in the list of words to recognize\n        try:\n            if text is not None and text in self._trigger_words:\n                if self._user_callback is not None:\n                    self._user_callback(text)\n            else:\n                print(text + "" - unknown"")\n        except:\n            pass\n\n'"
libfaceid/speech_synthesizer.py,0,"b'from enum import Enum\nimport os\n\n\n\n\n\nINPUT_TACOTRON_MODEL = ""tacotron-20180906/model.ckpt""\n\n\nclass SpeechSynthesizerModels(Enum):\n\n    TTSX3               = 0 # real-time, no .wav file generated\n    TACOTRON            = 1 # generates .wav file during training\n    GOOGLECLOUD         = 2 # generates .mp3 file during training, requires internet access\n    DEFAULT = TTSX3\n\n\nclass SpeechSynthesizer:\n\n    def __init__(self, model=SpeechSynthesizerModels.DEFAULT, path=None, path_output=None, training=True):\n        self._base = None\n        if model == SpeechSynthesizerModels.TTSX3:\n            self._base = SpeechSynthesizer_TTSX3(path, path_output, training)\n        elif model == SpeechSynthesizerModels.TACOTRON:\n            self._base = SpeechSynthesizer_TACOTRON(path, path_output, training)\n        elif model == SpeechSynthesizerModels.GOOGLECLOUD:\n            self._base = SpeechSynthesizer_GOOGLECLOUD(path, path_output, training)\n        #print(""Synthesizer loaded!"")\n\n    def synthesize(self, text, outputfile):\n        self._base.synthesize(text, outputfile)\n        #print(""Synthesized text={} output={}"".format(text, outputfile))\n\n    def synthesize_name(self, name):\n        text = ""Hello "" + name\n        outputfile = name\n        self.synthesize(text, outputfile)\n\n    def synthesize_datasets(self, path_datasets):\n        for (_d, names, _f) in os.walk(path_datasets):\n            #print(""names "" + str(names))\n            for name in names:\n                self.synthesize_name(name)\n            break\n\n    def playaudio(self, path, name, block=True):\n        try:\n            if (name is not None) and (name != ""Unknown"") and (name != ""Fake""):\n                self._base.playaudio(path, name, block)\n        except:\n            print(""SpeechSynthesizer playaudio EXCEPTION"")\n            pass\n\n\nclass SpeechSynthesizer_TTSX3:\n\n    def __init__(self, path, path_output, training):\n        self._training = training\n        if not self._training:\n            import pyttsx3 # lazy loading\n            try:\n                self._synthesizer = pyttsx3.init()\n            except Exception as e:\n                print(""pyttsx3 exception "" + str(e))\n            self._synthesizer.setProperty(\'voice\', \'english\')\n            \n    def synthesize(self, text, outputfile):\n        pass\n\n    def playaudio(self, path, name, block):\n        #print(""SpeechSynthesizer_TTSX3"")\n        if not self._training:\n            text = ""Hello "" + name\n            if block:\n                self._synthesizer.say(text)\n                self._synthesizer.runAndWait()\n            else:\n                from threading import Thread # lazy loading\n                self._thread =  None\n                self._thread = Thread(target=self.thread_play, kwargs=dict(text=text))\n                self._thread.setDaemon(True)\n                self._thread.start()\n\n    def thread_play(self, text=""None""):\n        self._synthesizer.say(text)\n        self._synthesizer.runAndWait()\n        self._thread = None\n        #import time\n        #time.sleep(1)\n\n\nclass SpeechSynthesizer_TACOTRON:\n\n    _file_extension = "".wav""\n\n    def __init__(self, path, path_output, training):\n        self._training = training\n        if self._training:\n            from libfaceid.tacotron.synthesizer import Synthesizer # lazy loading\n            self._path_output = path_output\n            self._synthesizer = Synthesizer()\n            self._synthesizer.load(path + INPUT_TACOTRON_MODEL)\n\n    def synthesize(self, text, outputfile):\n        if self._training:\n            with open(self._path_output + outputfile + self._file_extension, \'wb\') as file:\n                file.write(self._synthesizer.synthesize(text))\n\n    def playaudio(self, path, name, block):\n        #print(""SpeechSynthesizer_TACOTRON"")\n        filename = os.path.abspath(path + ""/"" + name + self._file_extension)\n        SpeechSynthesizer_Utils().playaudio(filename, block)\n\n\nclass SpeechSynthesizer_GOOGLECLOUD:\n\n    _file_extension = "".mp3""\n\n    def __init__(self, path, path_output, training):\n        self._training = training\n        self._path_output = path_output\n\n    def synthesize(self, text, outputfile):\n        if self._training:\n            from gtts import gTTS # lazy loading\n            tts = gTTS(text)\n            tts.save(self._path_output + outputfile + self._file_extension)\n\n    def playaudio(self, path, name, block):\n        #print(""SpeechSynthesizer_GOOGLECLOUD"")\n        filename = os.path.abspath(path + ""/"" + name + self._file_extension)\n        SpeechSynthesizer_Utils().playaudio(filename, block)\n\n\nclass SpeechSynthesizer_Utils:\n    def __init__(self):\n        self._option = 0\n        \n    def playaudio(self, filename, block):\n        if self._option == 0:\n            from playsound import playsound # lazy loading\n            #print(""playsound"")\n            playsound(filename, block)\n        else:\n            pass\n            \n'"
libfaceid/tacotron/__init__.py,0,b''
libfaceid/tacotron/demo_server.py,0,"b'import argparse\nimport falcon\nfrom hparams import hparams, hparams_debug_string\nimport os\nfrom synthesizer import Synthesizer\n\n\nhtml_body = \'\'\'<html><title>Demo</title>\n<style>\nbody {padding: 16px; font-family: sans-serif; font-size: 14px; color: #444}\ninput {font-size: 14px; padding: 8px 12px; outline: none; border: 1px solid #ddd}\ninput:focus {box-shadow: 0 1px 2px rgba(0,0,0,.15)}\np {padding: 12px}\nbutton {background: #28d; padding: 9px 14px; margin-left: 8px; border: none; outline: none;\n        color: #fff; font-size: 14px; border-radius: 4px; cursor: pointer;}\nbutton:hover {box-shadow: 0 1px 2px rgba(0,0,0,.15); opacity: 0.9;}\nbutton:active {background: #29f;}\nbutton[disabled] {opacity: 0.4; cursor: default}\n</style>\n<body>\n<form>\n  <input id=""text"" type=""text"" size=""40"" placeholder=""Enter Text"">\n  <button id=""button"" name=""synthesize"">Speak</button>\n</form>\n<p id=""message""></p>\n<audio id=""audio"" controls autoplay hidden></audio>\n<script>\nfunction q(selector) {return document.querySelector(selector)}\nq(\'#text\').focus()\nq(\'#button\').addEventListener(\'click\', function(e) {\n  text = q(\'#text\').value.trim()\n  if (text) {\n    q(\'#message\').textContent = \'Synthesizing...\'\n    q(\'#button\').disabled = true\n    q(\'#audio\').hidden = true\n    synthesize(text)\n  }\n  e.preventDefault()\n  return false\n})\nfunction synthesize(text) {\n  fetch(\'/synthesize?text=\' + encodeURIComponent(text), {cache: \'no-cache\'})\n    .then(function(res) {\n      if (!res.ok) throw Error(res.statusText)\n      return res.blob()\n    }).then(function(blob) {\n      q(\'#message\').textContent = \'\'\n      q(\'#button\').disabled = false\n      q(\'#audio\').src = URL.createObjectURL(blob)\n      q(\'#audio\').hidden = false\n    }).catch(function(err) {\n      q(\'#message\').textContent = \'Error: \' + err.message\n      q(\'#button\').disabled = false\n    })\n}\n</script></body></html>\n\'\'\'\n\n\nclass UIResource:\n  def on_get(self, req, res):\n    res.content_type = \'text/html\'\n    res.body = html_body\n\n\nclass SynthesisResource:\n  def on_get(self, req, res):\n    if not req.params.get(\'text\'):\n      raise falcon.HTTPBadRequest()\n    res.data = synthesizer.synthesize(req.params.get(\'text\'))\n    res.content_type = \'audio/wav\'\n\n\nsynthesizer = Synthesizer()\napi = falcon.API()\napi.add_route(\'/synthesize\', SynthesisResource())\napi.add_route(\'/\', UIResource())\n\n\nif __name__ == \'__main__\':\n  from wsgiref import simple_server\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--checkpoint\', required=True, help=\'Full path to model checkpoint\')\n  parser.add_argument(\'--port\', type=int, default=9000)\n  parser.add_argument(\'--hparams\', default=\'\',\n    help=\'Hyperparameter overrides as a comma-separated list of name=value pairs\')\n  args = parser.parse_args()\n  os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n  hparams.parse(args.hparams)\n  print(hparams_debug_string())\n  synthesizer.load(args.checkpoint)\n  print(\'Serving on port %d\' % args.port)\n  simple_server.make_server(\'0.0.0.0\', args.port, api).serve_forever()\nelse:\n  synthesizer.load(os.environ[\'CHECKPOINT\'])\n'"
libfaceid/tacotron/eval.py,0,"b""import argparse\nimport os\nimport re\nfrom hparams import hparams, hparams_debug_string\nfrom synthesizer import Synthesizer\n\n\nsentences = [\n  # From July 8, 2017 New York Times:\n  'Scientists at the CERN laboratory say they have discovered a new particle.',\n  'There\xe2\x80\x99s a way to measure the acute emotional intelligence that has never gone out of style.',\n  'President Trump met with other leaders at the Group of 20 conference.',\n  'The Senate\\'s bill to repeal and replace the Affordable Care Act is now imperiled.',\n  # From Google's Tacotron example page:\n  'Generative adversarial network or variational auto-encoder.',\n  'The buses aren\\'t the problem, they actually provide a solution.',\n  'Does the quick brown fox jump over the lazy dog?',\n  'Talib Kweli confirmed to AllHipHop that he will be releasing an album in the next year.',\n]\n\n\ndef get_output_base_path(checkpoint_path):\n  base_dir = os.path.dirname(checkpoint_path)\n  m = re.compile(r'.*?\\.ckpt\\-([0-9]+)').match(checkpoint_path)\n  name = 'eval-%d' % int(m.group(1)) if m else 'eval'\n  return os.path.join(base_dir, name)\n\n\ndef run_eval(args):\n  print(hparams_debug_string())\n  synth = Synthesizer()\n  synth.load(args.checkpoint)\n  base_path = get_output_base_path(args.checkpoint)\n  for i, text in enumerate(sentences):\n    path = '%s-%d.wav' % (base_path, i)\n    print('Synthesizing: %s' % path)\n    with open(path, 'wb') as f:\n      f.write(synth.synthesize(text))\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--checkpoint', required=True, help='Path to model checkpoint')\n  parser.add_argument('--hparams', default='',\n    help='Hyperparameter overrides as a comma-separated list of name=value pairs')\n  args = parser.parse_args()\n  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n  hparams.parse(args.hparams)\n  run_eval(args)\n\n\nif __name__ == '__main__':\n  main()\n"""
libfaceid/tacotron/hparams.py,1,"b'import tensorflow as tf\n\n\n# Default hyperparameters:\nhparams = tf.contrib.training.HParams(\n  # Comma-separated list of cleaners to run on text prior to training and eval. For non-English\n  # text, you may want to use ""basic_cleaners"" or ""transliteration_cleaners"" See TRAINING_DATA.md.\n  cleaners=\'english_cleaners\',\n\n  # Audio:\n  num_mels=80,\n  num_freq=1025,\n  sample_rate=20000,\n  frame_length_ms=50,\n  frame_shift_ms=12.5,\n  preemphasis=0.97,\n  min_level_db=-100,\n  ref_level_db=20,\n\n  # Model:\n  outputs_per_step=5,\n  embed_depth=256,\n  prenet_depths=[256, 128],\n  encoder_depth=256,\n  postnet_depth=256,\n  attention_depth=256,\n  decoder_depth=256,\n\n  # Training:\n  batch_size=32,\n  adam_beta1=0.9,\n  adam_beta2=0.999,\n  initial_learning_rate=0.002,\n  decay_learning_rate=True,\n  use_cmudict=False,  # Use CMUDict during training to learn pronunciation of ARPAbet phonemes\n\n  # Eval:\n  max_iters=200,\n  griffin_lim_iters=60,\n  power=1.5,              # Power to raise magnitudes to prior to Griffin-Lim\n)\n\n\ndef hparams_debug_string():\n  values = hparams.values()\n  hp = [\'  %s: %s\' % (name, values[name]) for name in sorted(values)]\n  return \'Hyperparameters:\\n\' + \'\\n\'.join(hp)\n'"
libfaceid/tacotron/preprocess.py,0,"b'import argparse\nimport os\nfrom multiprocessing import cpu_count\nfrom tqdm import tqdm\nfrom datasets import blizzard, ljspeech\nfrom hparams import hparams\n\n\ndef preprocess_blizzard(args):\n  in_dir = os.path.join(args.base_dir, \'Blizzard2012\')\n  out_dir = os.path.join(args.base_dir, args.output)\n  os.makedirs(out_dir, exist_ok=True)\n  metadata = blizzard.build_from_path(in_dir, out_dir, args.num_workers, tqdm=tqdm)\n  write_metadata(metadata, out_dir)\n\n\ndef preprocess_ljspeech(args):\n  in_dir = os.path.join(args.base_dir, \'LJSpeech-1.1\')\n  out_dir = os.path.join(args.base_dir, args.output)\n  os.makedirs(out_dir, exist_ok=True)\n  metadata = ljspeech.build_from_path(in_dir, out_dir, args.num_workers, tqdm=tqdm)\n  write_metadata(metadata, out_dir)\n\n\ndef write_metadata(metadata, out_dir):\n  with open(os.path.join(out_dir, \'train.txt\'), \'w\', encoding=\'utf-8\') as f:\n    for m in metadata:\n      f.write(\'|\'.join([str(x) for x in m]) + \'\\n\')\n  frames = sum([m[2] for m in metadata])\n  hours = frames * hparams.frame_shift_ms / (3600 * 1000)\n  print(\'Wrote %d utterances, %d frames (%.2f hours)\' % (len(metadata), frames, hours))\n  print(\'Max input length:  %d\' % max(len(m[3]) for m in metadata))\n  print(\'Max output length: %d\' % max(m[2] for m in metadata))\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--base_dir\', default=os.path.expanduser(\'~/tacotron\'))\n  parser.add_argument(\'--output\', default=\'training\')\n  parser.add_argument(\'--dataset\', required=True, choices=[\'blizzard\', \'ljspeech\'])\n  parser.add_argument(\'--num_workers\', type=int, default=cpu_count())\n  args = parser.parse_args()\n  if args.dataset == \'blizzard\':\n    preprocess_blizzard(args)\n  elif args.dataset == \'ljspeech\':\n    preprocess_ljspeech(args)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
libfaceid/tacotron/synthesizer.py,6,"b""import io\nimport numpy as np\nimport tensorflow as tf\nfrom libfaceid.tacotron.hparams import hparams\nfrom librosa import effects\nfrom libfaceid.tacotron.models import create_model\nfrom libfaceid.tacotron.text import text_to_sequence\nfrom libfaceid.tacotron.util import audio\n\n\nclass Synthesizer:\n  def load(self, checkpoint_path, model_name='tacotron'):\n    #print('Constructing model: %s' % model_name)\n    inputs = tf.placeholder(tf.int32, [1, None], 'inputs')\n    input_lengths = tf.placeholder(tf.int32, [1], 'input_lengths')\n    with tf.variable_scope('model') as scope:\n      self.model = create_model(model_name, hparams)\n      self.model.initialize(inputs, input_lengths)\n      self.wav_output = audio.inv_spectrogram_tensorflow(self.model.linear_outputs[0])\n\n    #print('Loading checkpoint: %s' % checkpoint_path)\n    self.session = tf.Session()\n    self.session.run(tf.global_variables_initializer())\n    saver = tf.train.Saver()\n    saver.restore(self.session, checkpoint_path)\n\n\n  def synthesize(self, text):\n    cleaner_names = [x.strip() for x in hparams.cleaners.split(',')]\n    seq = text_to_sequence(text, cleaner_names)\n    feed_dict = {\n      self.model.inputs: [np.asarray(seq, dtype=np.int32)],\n      self.model.input_lengths: np.asarray([len(seq)], dtype=np.int32)\n    }\n    wav = self.session.run(self.wav_output, feed_dict=feed_dict)\n    wav = audio.inv_preemphasis(wav)\n    wav = wav[:audio.find_endpoint(wav)]\n    out = io.BytesIO()\n    audio.save_wav(wav, out)\n    return out.getvalue()\n"""
libfaceid/tacotron/train.py,21,"b""import argparse\nfrom datetime import datetime\nimport math\nimport os\nimport subprocess\nimport time\nimport tensorflow as tf\nimport traceback\n\nfrom datasets.datafeeder import DataFeeder\nfrom hparams import hparams, hparams_debug_string\nfrom models import create_model\nfrom text import sequence_to_text\nfrom util import audio, infolog, plot, ValueWindow\nlog = infolog.log\n\n\ndef get_git_commit():\n  subprocess.check_output(['git', 'diff-index', '--quiet', 'HEAD'])   # Verify client is clean\n  commit = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip()[:10]\n  log('Git commit: %s' % commit)\n  return commit\n\n\ndef add_stats(model):\n  with tf.variable_scope('stats') as scope:\n    tf.summary.histogram('linear_outputs', model.linear_outputs)\n    tf.summary.histogram('linear_targets', model.linear_targets)\n    tf.summary.histogram('mel_outputs', model.mel_outputs)\n    tf.summary.histogram('mel_targets', model.mel_targets)\n    tf.summary.scalar('loss_mel', model.mel_loss)\n    tf.summary.scalar('loss_linear', model.linear_loss)\n    tf.summary.scalar('learning_rate', model.learning_rate)\n    tf.summary.scalar('loss', model.loss)\n    gradient_norms = [tf.norm(grad) for grad in model.gradients]\n    tf.summary.histogram('gradient_norm', gradient_norms)\n    tf.summary.scalar('max_gradient_norm', tf.reduce_max(gradient_norms))\n    return tf.summary.merge_all()\n\n\ndef time_string():\n  return datetime.now().strftime('%Y-%m-%d %H:%M')\n\n\ndef train(log_dir, args):\n  commit = get_git_commit() if args.git else 'None'\n  checkpoint_path = os.path.join(log_dir, 'model.ckpt')\n  input_path = os.path.join(args.base_dir, args.input)\n  log('Checkpoint path: %s' % checkpoint_path)\n  log('Loading training data from: %s' % input_path)\n  log('Using model: %s' % args.model)\n  log(hparams_debug_string())\n\n  # Set up DataFeeder:\n  coord = tf.train.Coordinator()\n  with tf.variable_scope('datafeeder') as scope:\n    feeder = DataFeeder(coord, input_path, hparams)\n\n  # Set up model:\n  global_step = tf.Variable(0, name='global_step', trainable=False)\n  with tf.variable_scope('model') as scope:\n    model = create_model(args.model, hparams)\n    model.initialize(feeder.inputs, feeder.input_lengths, feeder.mel_targets, feeder.linear_targets)\n    model.add_loss()\n    model.add_optimizer(global_step)\n    stats = add_stats(model)\n\n  # Bookkeeping:\n  step = 0\n  time_window = ValueWindow(100)\n  loss_window = ValueWindow(100)\n  saver = tf.train.Saver(max_to_keep=5, keep_checkpoint_every_n_hours=2)\n\n  # Train!\n  with tf.Session() as sess:\n    try:\n      summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n      sess.run(tf.global_variables_initializer())\n\n      if args.restore_step:\n        # Restore from a checkpoint if the user requested it.\n        restore_path = '%s-%d' % (checkpoint_path, args.restore_step)\n        saver.restore(sess, restore_path)\n        log('Resuming from checkpoint: %s at commit: %s' % (restore_path, commit), slack=True)\n      else:\n        log('Starting new training run at commit: %s' % commit, slack=True)\n\n      feeder.start_in_session(sess)\n\n      while not coord.should_stop():\n        start_time = time.time()\n        step, loss, opt = sess.run([global_step, model.loss, model.optimize])\n        time_window.append(time.time() - start_time)\n        loss_window.append(loss)\n        message = 'Step %-7d [%.03f sec/step, loss=%.05f, avg_loss=%.05f]' % (\n          step, time_window.average, loss, loss_window.average)\n        log(message, slack=(step % args.checkpoint_interval == 0))\n\n        if loss > 100 or math.isnan(loss):\n          log('Loss exploded to %.05f at step %d!' % (loss, step), slack=True)\n          raise Exception('Loss Exploded')\n\n        if step % args.summary_interval == 0:\n          log('Writing summary at step: %d' % step)\n          summary_writer.add_summary(sess.run(stats), step)\n\n        if step % args.checkpoint_interval == 0:\n          log('Saving checkpoint to: %s-%d' % (checkpoint_path, step))\n          saver.save(sess, checkpoint_path, global_step=step)\n          log('Saving audio and alignment...')\n          input_seq, spectrogram, alignment = sess.run([\n            model.inputs[0], model.linear_outputs[0], model.alignments[0]])\n          waveform = audio.inv_spectrogram(spectrogram.T)\n          audio.save_wav(waveform, os.path.join(log_dir, 'step-%d-audio.wav' % step))\n          plot.plot_alignment(alignment, os.path.join(log_dir, 'step-%d-align.png' % step),\n            info='%s, %s, %s, step=%d, loss=%.5f' % (args.model, commit, time_string(), step, loss))\n          log('Input: %s' % sequence_to_text(input_seq))\n\n    except Exception as e:\n      log('Exiting due to exception: %s' % e, slack=True)\n      traceback.print_exc()\n      coord.request_stop(e)\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--base_dir', default=os.path.expanduser('~/tacotron'))\n  parser.add_argument('--input', default='training/train.txt')\n  parser.add_argument('--model', default='tacotron')\n  parser.add_argument('--name', help='Name of the run. Used for logging. Defaults to model name.')\n  parser.add_argument('--hparams', default='',\n    help='Hyperparameter overrides as a comma-separated list of name=value pairs')\n  parser.add_argument('--restore_step', type=int, help='Global step to restore from checkpoint.')\n  parser.add_argument('--summary_interval', type=int, default=100,\n    help='Steps between running summary ops.')\n  parser.add_argument('--checkpoint_interval', type=int, default=1000,\n    help='Steps between writing checkpoints.')\n  parser.add_argument('--slack_url', help='Slack webhook URL to get periodic reports.')\n  parser.add_argument('--tf_log_level', type=int, default=1, help='Tensorflow C++ log level.')\n  parser.add_argument('--git', action='store_true', help='If set, verify that the client is clean.')\n  args = parser.parse_args()\n  os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(args.tf_log_level)\n  run_name = args.name or args.model\n  log_dir = os.path.join(args.base_dir, 'logs-%s' % run_name)\n  os.makedirs(log_dir, exist_ok=True)\n  infolog.init(os.path.join(log_dir, 'train.log'), run_name, args.slack_url)\n  hparams.parse(args.hparams)\n  train(log_dir, args)\n\n\nif __name__ == '__main__':\n  main()\n"""
libfaceid/tacotron/datasets/__init__.py,0,b''
libfaceid/tacotron/datasets/blizzard.py,0,"b'from concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nimport numpy as np\nimport os\nfrom hparams import hparams\nfrom util import audio\n\n\n_max_out_length = 700\n_end_buffer = 0.05\n_min_confidence = 90\n\n# Note: ""A Tramp Abroad"" & ""The Man That Corrupted Hadleyburg"" are higher quality than the others.\nbooks = [\n  \'ATrampAbroad\',\n  \'TheManThatCorruptedHadleyburg\',\n  # \'LifeOnTheMississippi\',\n  # \'TheAdventuresOfTomSawyer\',\n]\n\ndef build_from_path(in_dir, out_dir, num_workers=1, tqdm=lambda x: x):\n  executor = ProcessPoolExecutor(max_workers=num_workers)\n  futures = []\n  index = 1\n  for book in books:\n    with open(os.path.join(in_dir, book, \'sentence_index.txt\')) as f:\n      for line in f:\n        parts = line.strip().split(\'\\t\')\n        if line[0] is not \'#\' and len(parts) == 8 and float(parts[3]) > _min_confidence:\n          wav_path = os.path.join(in_dir, book, \'wav\', \'%s.wav\' % parts[0])\n          labels_path = os.path.join(in_dir, book, \'lab\', \'%s.lab\' % parts[0])\n          text = parts[5]\n          task = partial(_process_utterance, out_dir, index, wav_path, labels_path, text)\n          futures.append(executor.submit(task))\n          index += 1\n  results = [future.result() for future in tqdm(futures)]\n  return [r for r in results if r is not None]\n\n\ndef _process_utterance(out_dir, index, wav_path, labels_path, text):\n  # Load the wav file and trim silence from the ends:\n  wav = audio.load_wav(wav_path)\n  start_offset, end_offset = _parse_labels(labels_path)\n  start = int(start_offset * hparams.sample_rate)\n  end = int(end_offset * hparams.sample_rate) if end_offset is not None else -1\n  wav = wav[start:end]\n  max_samples = _max_out_length * hparams.frame_shift_ms / 1000 * hparams.sample_rate\n  if len(wav) > max_samples:\n    return None\n  spectrogram = audio.spectrogram(wav).astype(np.float32)\n  n_frames = spectrogram.shape[1]\n  mel_spectrogram = audio.melspectrogram(wav).astype(np.float32)\n  spectrogram_filename = \'blizzard-spec-%05d.npy\' % index\n  mel_filename = \'blizzard-mel-%05d.npy\' % index\n  np.save(os.path.join(out_dir, spectrogram_filename), spectrogram.T, allow_pickle=False)\n  np.save(os.path.join(out_dir, mel_filename), mel_spectrogram.T, allow_pickle=False)\n  return (spectrogram_filename, mel_filename, n_frames, text)\n\n\ndef _parse_labels(path):\n  labels = []\n  with open(os.path.join(path)) as f:\n    for line in f:\n      parts = line.strip().split(\' \')\n      if len(parts) >= 3:\n        labels.append((float(parts[0]), \' \'.join(parts[2:])))\n  start = 0\n  end = None\n  if labels[0][1] == \'sil\':\n    start = labels[0][0]\n  if labels[-1][1] == \'sil\':\n    end = labels[-2][0] + _end_buffer\n  return (start, end)\n'"
libfaceid/tacotron/datasets/datafeeder.py,5,"b""import numpy as np\nimport os\nimport random\nimport tensorflow as tf\nimport threading\nimport time\nimport traceback\nfrom text import cmudict, text_to_sequence\nfrom util.infolog import log\n\n\n_batches_per_group = 32\n_p_cmudict = 0.5\n_pad = 0\n\n\nclass DataFeeder(threading.Thread):\n  '''Feeds batches of data into a queue on a background thread.'''\n\n  def __init__(self, coordinator, metadata_filename, hparams):\n    super(DataFeeder, self).__init__()\n    self._coord = coordinator\n    self._hparams = hparams\n    self._cleaner_names = [x.strip() for x in hparams.cleaners.split(',')]\n    self._offset = 0\n\n    # Load metadata:\n    self._datadir = os.path.dirname(metadata_filename)\n    with open(metadata_filename, encoding='utf-8') as f:\n      self._metadata = [line.strip().split('|') for line in f]\n      hours = sum((int(x[2]) for x in self._metadata)) * hparams.frame_shift_ms / (3600 * 1000)\n      log('Loaded metadata for %d examples (%.2f hours)' % (len(self._metadata), hours))\n\n    # Create placeholders for inputs and targets. Don't specify batch size because we want to\n    # be able to feed different sized batches at eval time.\n    self._placeholders = [\n      tf.placeholder(tf.int32, [None, None], 'inputs'),\n      tf.placeholder(tf.int32, [None], 'input_lengths'),\n      tf.placeholder(tf.float32, [None, None, hparams.num_mels], 'mel_targets'),\n      tf.placeholder(tf.float32, [None, None, hparams.num_freq], 'linear_targets')\n    ]\n\n    # Create queue for buffering data:\n    queue = tf.FIFOQueue(8, [tf.int32, tf.int32, tf.float32, tf.float32], name='input_queue')\n    self._enqueue_op = queue.enqueue(self._placeholders)\n    self.inputs, self.input_lengths, self.mel_targets, self.linear_targets = queue.dequeue()\n    self.inputs.set_shape(self._placeholders[0].shape)\n    self.input_lengths.set_shape(self._placeholders[1].shape)\n    self.mel_targets.set_shape(self._placeholders[2].shape)\n    self.linear_targets.set_shape(self._placeholders[3].shape)\n\n    # Load CMUDict: If enabled, this will randomly substitute some words in the training data with\n    # their ARPABet equivalents, which will allow you to also pass ARPABet to the model for\n    # synthesis (useful for proper nouns, etc.)\n    if hparams.use_cmudict:\n      cmudict_path = os.path.join(self._datadir, 'cmudict-0.7b')\n      if not os.path.isfile(cmudict_path):\n        raise Exception('If use_cmudict=True, you must download ' +\n          'http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b to %s'  % cmudict_path)\n      self._cmudict = cmudict.CMUDict(cmudict_path, keep_ambiguous=False)\n      log('Loaded CMUDict with %d unambiguous entries' % len(self._cmudict))\n    else:\n      self._cmudict = None\n\n\n  def start_in_session(self, session):\n    self._session = session\n    self.start()\n\n\n  def run(self):\n    try:\n      while not self._coord.should_stop():\n        self._enqueue_next_group()\n    except Exception as e:\n      traceback.print_exc()\n      self._coord.request_stop(e)\n\n\n  def _enqueue_next_group(self):\n    start = time.time()\n\n    # Read a group of examples:\n    n = self._hparams.batch_size\n    r = self._hparams.outputs_per_step\n    examples = [self._get_next_example() for i in range(n * _batches_per_group)]\n\n    # Bucket examples based on similar output sequence length for efficiency:\n    examples.sort(key=lambda x: x[-1])\n    batches = [examples[i:i+n] for i in range(0, len(examples), n)]\n    random.shuffle(batches)\n\n    log('Generated %d batches of size %d in %.03f sec' % (len(batches), n, time.time() - start))\n    for batch in batches:\n      feed_dict = dict(zip(self._placeholders, _prepare_batch(batch, r)))\n      self._session.run(self._enqueue_op, feed_dict=feed_dict)\n\n\n  def _get_next_example(self):\n    '''Loads a single example (input, mel_target, linear_target, cost) from disk'''\n    if self._offset >= len(self._metadata):\n      self._offset = 0\n      random.shuffle(self._metadata)\n    meta = self._metadata[self._offset]\n    self._offset += 1\n\n    text = meta[3]\n    if self._cmudict and random.random() < _p_cmudict:\n      text = ' '.join([self._maybe_get_arpabet(word) for word in text.split(' ')])\n\n    input_data = np.asarray(text_to_sequence(text, self._cleaner_names), dtype=np.int32)\n    linear_target = np.load(os.path.join(self._datadir, meta[0]))\n    mel_target = np.load(os.path.join(self._datadir, meta[1]))\n    return (input_data, mel_target, linear_target, len(linear_target))\n\n\n  def _maybe_get_arpabet(self, word):\n    arpabet = self._cmudict.lookup(word)\n    return '{%s}' % arpabet[0] if arpabet is not None and random.random() < 0.5 else word\n\n\ndef _prepare_batch(batch, outputs_per_step):\n  random.shuffle(batch)\n  inputs = _prepare_inputs([x[0] for x in batch])\n  input_lengths = np.asarray([len(x[0]) for x in batch], dtype=np.int32)\n  mel_targets = _prepare_targets([x[1] for x in batch], outputs_per_step)\n  linear_targets = _prepare_targets([x[2] for x in batch], outputs_per_step)\n  return (inputs, input_lengths, mel_targets, linear_targets)\n\n\ndef _prepare_inputs(inputs):\n  max_len = max((len(x) for x in inputs))\n  return np.stack([_pad_input(x, max_len) for x in inputs])\n\n\ndef _prepare_targets(targets, alignment):\n  max_len = max((len(t) for t in targets)) + 1\n  return np.stack([_pad_target(t, _round_up(max_len, alignment)) for t in targets])\n\n\ndef _pad_input(x, length):\n  return np.pad(x, (0, length - x.shape[0]), mode='constant', constant_values=_pad)\n\n\ndef _pad_target(t, length):\n  return np.pad(t, [(0, length - t.shape[0]), (0,0)], mode='constant', constant_values=_pad)\n\n\ndef _round_up(x, multiple):\n  remainder = x % multiple\n  return x if remainder == 0 else x + multiple - remainder\n"""
libfaceid/tacotron/datasets/ljspeech.py,0,"b""from concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nimport numpy as np\nimport os\nfrom util import audio\n\n\ndef build_from_path(in_dir, out_dir, num_workers=1, tqdm=lambda x: x):\n  '''Preprocesses the LJ Speech dataset from a given input path into a given output directory.\n\n    Args:\n      in_dir: The directory where you have downloaded the LJ Speech dataset\n      out_dir: The directory to write the output into\n      num_workers: Optional number of worker processes to parallelize across\n      tqdm: You can optionally pass tqdm to get a nice progress bar\n\n    Returns:\n      A list of tuples describing the training examples. This should be written to train.txt\n  '''\n\n  # We use ProcessPoolExecutor to parallelize across processes. This is just an optimization and you\n  # can omit it and just call _process_utterance on each input if you want.\n  executor = ProcessPoolExecutor(max_workers=num_workers)\n  futures = []\n  index = 1\n  with open(os.path.join(in_dir, 'metadata.csv'), encoding='utf-8') as f:\n    for line in f:\n      parts = line.strip().split('|')\n      wav_path = os.path.join(in_dir, 'wavs', '%s.wav' % parts[0])\n      text = parts[2]\n      futures.append(executor.submit(partial(_process_utterance, out_dir, index, wav_path, text)))\n      index += 1\n  return [future.result() for future in tqdm(futures)]\n\n\ndef _process_utterance(out_dir, index, wav_path, text):\n  '''Preprocesses a single utterance audio/text pair.\n\n  This writes the mel and linear scale spectrograms to disk and returns a tuple to write\n  to the train.txt file.\n\n  Args:\n    out_dir: The directory to write the spectrograms into\n    index: The numeric index to use in the spectrogram filenames.\n    wav_path: Path to the audio file containing the speech input\n    text: The text spoken in the input audio file\n\n  Returns:\n    A (spectrogram_filename, mel_filename, n_frames, text) tuple to write to train.txt\n  '''\n\n  # Load the audio to a numpy array:\n  wav = audio.load_wav(wav_path)\n\n  # Compute the linear-scale spectrogram from the wav:\n  spectrogram = audio.spectrogram(wav).astype(np.float32)\n  n_frames = spectrogram.shape[1]\n\n  # Compute a mel-scale spectrogram from the wav:\n  mel_spectrogram = audio.melspectrogram(wav).astype(np.float32)\n\n  # Write the spectrograms to disk:\n  spectrogram_filename = 'ljspeech-spec-%05d.npy' % index\n  mel_filename = 'ljspeech-mel-%05d.npy' % index\n  np.save(os.path.join(out_dir, spectrogram_filename), spectrogram.T, allow_pickle=False)\n  np.save(os.path.join(out_dir, mel_filename), mel_spectrogram.T, allow_pickle=False)\n\n  # Return a tuple describing this training example:\n  return (spectrogram_filename, mel_filename, n_frames, text)\n"""
libfaceid/tacotron/models/__init__.py,0,"b""from .tacotron import Tacotron\n\n\ndef create_model(name, hparams):\n  if name == 'tacotron':\n    return Tacotron(hparams)\n  else:\n    raise Exception('Unknown model: ' + name)\n"""
libfaceid/tacotron/models/helpers.py,17,"b""import numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.seq2seq import Helper\n\n\n# Adapted from tf.contrib.seq2seq.GreedyEmbeddingHelper\nclass TacoTestHelper(Helper):\n  def __init__(self, batch_size, output_dim, r):\n    with tf.name_scope('TacoTestHelper'):\n      self._batch_size = batch_size\n      self._output_dim = output_dim\n      self._end_token = tf.tile([0.0], [output_dim * r])\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  @property\n  def sample_ids_shape(self):\n    return tf.TensorShape([])\n\n  @property\n  def sample_ids_dtype(self):\n    return np.int32\n\n  def initialize(self, name=None):\n    return (tf.tile([False], [self._batch_size]), _go_frames(self._batch_size, self._output_dim))\n\n  def sample(self, time, outputs, state, name=None):\n    return tf.tile([0], [self._batch_size])  # Return all 0; we ignore them\n\n  def next_inputs(self, time, outputs, state, sample_ids, name=None):\n    '''Stop on EOS. Otherwise, pass the last output as the next input and pass through state.'''\n    with tf.name_scope('TacoTestHelper'):\n      finished = tf.reduce_all(tf.equal(outputs, self._end_token), axis=1)\n      # Feed last output frame as next input. outputs is [N, output_dim * r]\n      next_inputs = outputs[:, -self._output_dim:]\n      return (finished, next_inputs, state)\n\n\nclass TacoTrainingHelper(Helper):\n  def __init__(self, inputs, targets, output_dim, r):\n    # inputs is [N, T_in], targets is [N, T_out, D]\n    with tf.name_scope('TacoTrainingHelper'):\n      self._batch_size = tf.shape(inputs)[0]\n      self._output_dim = output_dim\n\n      # Feed every r-th target frame as input\n      self._targets = targets[:, r-1::r, :]\n\n      # Use full length for every target because we don't want to mask the padding frames\n      num_steps = tf.shape(self._targets)[1]\n      self._lengths = tf.tile([num_steps], [self._batch_size])\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  @property\n  def sample_ids_shape(self):\n    return tf.TensorShape([])\n\n  @property\n  def sample_ids_dtype(self):\n    return np.int32\n\n  def initialize(self, name=None):\n    return (tf.tile([False], [self._batch_size]), _go_frames(self._batch_size, self._output_dim))\n\n  def sample(self, time, outputs, state, name=None):\n    return tf.tile([0], [self._batch_size])  # Return all 0; we ignore them\n\n  def next_inputs(self, time, outputs, state, sample_ids, name=None):\n    with tf.name_scope(name or 'TacoTrainingHelper'):\n      finished = (time + 1 >= self._lengths)\n      next_inputs = self._targets[:, time, :]\n      return (finished, next_inputs, state)\n\n\ndef _go_frames(batch_size, output_dim):\n  '''Returns all-zero <GO> frames for a given batch size and output dimension'''\n  return tf.tile([[0.0]], [batch_size, output_dim])\n\n"""
libfaceid/tacotron/models/modules.py,22,"b""import tensorflow as tf\nfrom tensorflow.contrib.rnn import GRUCell\n\n\ndef prenet(inputs, is_training, layer_sizes, scope=None):\n  x = inputs\n  drop_rate = 0.5 if is_training else 0.0\n  with tf.variable_scope(scope or 'prenet'):\n    for i, size in enumerate(layer_sizes):\n      dense = tf.layers.dense(x, units=size, activation=tf.nn.relu, name='dense_%d' % (i+1))\n      x = tf.layers.dropout(dense, rate=drop_rate, training=is_training, name='dropout_%d' % (i+1))\n  return x\n\n\ndef encoder_cbhg(inputs, input_lengths, is_training, depth):\n  input_channels = inputs.get_shape()[2]\n  return cbhg(\n    inputs,\n    input_lengths,\n    is_training,\n    scope='encoder_cbhg',\n    K=16,\n    projections=[128, input_channels],\n    depth=depth)\n\n\ndef post_cbhg(inputs, input_dim, is_training, depth):\n  return cbhg(\n    inputs,\n    None,\n    is_training,\n    scope='post_cbhg',\n    K=8,\n    projections=[256, input_dim],\n    depth=depth)\n\n\ndef cbhg(inputs, input_lengths, is_training, scope, K, projections, depth):\n  with tf.variable_scope(scope):\n    with tf.variable_scope('conv_bank'):\n      # Convolution bank: concatenate on the last axis to stack channels from all convolutions\n      conv_outputs = tf.concat(\n        [conv1d(inputs, k, 128, tf.nn.relu, is_training, 'conv1d_%d' % k) for k in range(1, K+1)],\n        axis=-1\n      )\n\n    # Maxpooling:\n    maxpool_output = tf.layers.max_pooling1d(\n      conv_outputs,\n      pool_size=2,\n      strides=1,\n      padding='same')\n\n    # Two projection layers:\n    proj1_output = conv1d(maxpool_output, 3, projections[0], tf.nn.relu, is_training, 'proj_1')\n    proj2_output = conv1d(proj1_output, 3, projections[1], None, is_training, 'proj_2')\n\n    # Residual connection:\n    highway_input = proj2_output + inputs\n\n    half_depth = depth // 2\n    assert half_depth*2 == depth, 'encoder and postnet depths must be even.'\n\n    # Handle dimensionality mismatch:\n    if highway_input.shape[2] != half_depth:\n      highway_input = tf.layers.dense(highway_input, half_depth)\n\n    # 4-layer HighwayNet:\n    for i in range(4):\n      highway_input = highwaynet(highway_input, 'highway_%d' % (i+1), half_depth)\n    rnn_input = highway_input\n\n    # Bidirectional RNN\n    outputs, states = tf.nn.bidirectional_dynamic_rnn(\n      GRUCell(half_depth),\n      GRUCell(half_depth),\n      rnn_input,\n      sequence_length=input_lengths,\n      dtype=tf.float32)\n    return tf.concat(outputs, axis=2)  # Concat forward and backward\n\n\ndef highwaynet(inputs, scope, depth):\n  with tf.variable_scope(scope):\n    H = tf.layers.dense(\n      inputs,\n      units=depth,\n      activation=tf.nn.relu,\n      name='H')\n    T = tf.layers.dense(\n      inputs,\n      units=depth,\n      activation=tf.nn.sigmoid,\n      name='T',\n      bias_initializer=tf.constant_initializer(-1.0))\n    return H * T + inputs * (1.0 - T)\n\n\ndef conv1d(inputs, kernel_size, channels, activation, is_training, scope):\n  with tf.variable_scope(scope):\n    conv1d_output = tf.layers.conv1d(\n      inputs,\n      filters=channels,\n      kernel_size=kernel_size,\n      activation=activation,\n      padding='same')\n    return tf.layers.batch_normalization(conv1d_output, training=is_training)\n"""
libfaceid/tacotron/models/rnn_wrappers.py,1,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import RNNCell\nfrom .modules import prenet\n\n\nclass DecoderPrenetWrapper(RNNCell):\n  \'\'\'Runs RNN inputs through a prenet before sending them to the cell.\'\'\'\n  def __init__(self, cell, is_training, layer_sizes):\n    super(DecoderPrenetWrapper, self).__init__()\n    self._cell = cell\n    self._is_training = is_training\n    self._layer_sizes = layer_sizes\n\n  @property\n  def state_size(self):\n    return self._cell.state_size\n\n  @property\n  def output_size(self):\n    return self._cell.output_size\n\n  def call(self, inputs, state):\n    prenet_out = prenet(inputs, self._is_training, self._layer_sizes, scope=\'decoder_prenet\')\n    return self._cell(prenet_out, state)\n\n  def zero_state(self, batch_size, dtype):\n    return self._cell.zero_state(batch_size, dtype)\n\n\n\nclass ConcatOutputAndAttentionWrapper(RNNCell):\n  \'\'\'Concatenates RNN cell output with the attention context vector.\n\n  This is expected to wrap a cell wrapped with an AttentionWrapper constructed with\n  attention_layer_size=None and output_attention=False. Such a cell\'s state will include an\n  ""attention"" field that is the context vector.\n  \'\'\'\n  def __init__(self, cell):\n    super(ConcatOutputAndAttentionWrapper, self).__init__()\n    self._cell = cell\n\n  @property\n  def state_size(self):\n    return self._cell.state_size\n\n  @property\n  def output_size(self):\n    return self._cell.output_size + self._cell.state_size.attention\n\n  def call(self, inputs, state):\n    output, res_state = self._cell(inputs, state)\n    return tf.concat([output, res_state.attention], axis=-1), res_state\n\n  def zero_state(self, batch_size, dtype):\n    return self._cell.zero_state(batch_size, dtype)\n'"
libfaceid/tacotron/models/tacotron.py,22,"b'import tensorflow as tf\nfrom tensorflow.contrib.rnn import GRUCell, MultiRNNCell, OutputProjectionWrapper, ResidualWrapper\nfrom tensorflow.contrib.seq2seq import BasicDecoder, BahdanauAttention, AttentionWrapper\nfrom libfaceid.tacotron.text.symbols import symbols\n#from util.infolog import log\nfrom .helpers import TacoTestHelper, TacoTrainingHelper\nfrom .modules import encoder_cbhg, post_cbhg, prenet\nfrom .rnn_wrappers import DecoderPrenetWrapper, ConcatOutputAndAttentionWrapper\n\n\n\nclass Tacotron():\n  def __init__(self, hparams):\n    self._hparams = hparams\n\n\n  def initialize(self, inputs, input_lengths, mel_targets=None, linear_targets=None):\n    \'\'\'Initializes the model for inference.\n\n    Sets ""mel_outputs"", ""linear_outputs"", and ""alignments"" fields.\n\n    Args:\n      inputs: int32 Tensor with shape [N, T_in] where N is batch size, T_in is number of\n        steps in the input time series, and values are character IDs\n      input_lengths: int32 Tensor with shape [N] where N is batch size and values are the lengths\n        of each sequence in inputs.\n      mel_targets: float32 Tensor with shape [N, T_out, M] where N is batch size, T_out is number\n        of steps in the output time series, M is num_mels, and values are entries in the mel\n        spectrogram. Only needed for training.\n      linear_targets: float32 Tensor with shape [N, T_out, F] where N is batch_size, T_out is number\n        of steps in the output time series, F is num_freq, and values are entries in the linear\n        spectrogram. Only needed for training.\n    \'\'\'\n    with tf.variable_scope(\'inference\') as scope:\n      is_training = linear_targets is not None\n      batch_size = tf.shape(inputs)[0]\n      hp = self._hparams\n\n      # Embeddings\n      embedding_table = tf.get_variable(\n        \'embedding\', [len(symbols), hp.embed_depth], dtype=tf.float32,\n        initializer=tf.truncated_normal_initializer(stddev=0.5))\n      embedded_inputs = tf.nn.embedding_lookup(embedding_table, inputs)          # [N, T_in, embed_depth=256]\n\n      # Encoder\n      prenet_outputs = prenet(embedded_inputs, is_training, hp.prenet_depths)    # [N, T_in, prenet_depths[-1]=128]\n      encoder_outputs = encoder_cbhg(prenet_outputs, input_lengths, is_training, # [N, T_in, encoder_depth=256]\n                                     hp.encoder_depth)\n\n      # Attention\n      attention_cell = AttentionWrapper(\n        GRUCell(hp.attention_depth),\n        BahdanauAttention(hp.attention_depth, encoder_outputs),\n        alignment_history=True,\n        output_attention=False)                                                  # [N, T_in, attention_depth=256]\n      \n      # Apply prenet before concatenation in AttentionWrapper.\n      attention_cell = DecoderPrenetWrapper(attention_cell, is_training, hp.prenet_depths)\n\n      # Concatenate attention context vector and RNN cell output into a 2*attention_depth=512D vector.\n      concat_cell = ConcatOutputAndAttentionWrapper(attention_cell)              # [N, T_in, 2*attention_depth=512]\n\n      # Decoder (layers specified bottom to top):\n      decoder_cell = MultiRNNCell([\n          OutputProjectionWrapper(concat_cell, hp.decoder_depth),\n          ResidualWrapper(GRUCell(hp.decoder_depth)),\n          ResidualWrapper(GRUCell(hp.decoder_depth))\n        ], state_is_tuple=True)                                                  # [N, T_in, decoder_depth=256]\n\n      # Project onto r mel spectrograms (predict r outputs at each RNN step):\n      output_cell = OutputProjectionWrapper(decoder_cell, hp.num_mels * hp.outputs_per_step)\n      decoder_init_state = output_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n\n      if is_training:\n        helper = TacoTrainingHelper(inputs, mel_targets, hp.num_mels, hp.outputs_per_step)\n      else:\n        helper = TacoTestHelper(batch_size, hp.num_mels, hp.outputs_per_step)\n\n      (decoder_outputs, _), final_decoder_state, _ = tf.contrib.seq2seq.dynamic_decode(\n        BasicDecoder(output_cell, helper, decoder_init_state),\n        maximum_iterations=hp.max_iters)                                         # [N, T_out/r, M*r]\n\n      # Reshape outputs to be one output per entry\n      mel_outputs = tf.reshape(decoder_outputs, [batch_size, -1, hp.num_mels])   # [N, T_out, M]\n\n      # Add post-processing CBHG:\n      post_outputs = post_cbhg(mel_outputs, hp.num_mels, is_training,            # [N, T_out, postnet_depth=256]\n                               hp.postnet_depth)\n      linear_outputs = tf.layers.dense(post_outputs, hp.num_freq)                # [N, T_out, F]\n\n      # Grab alignments from the final decoder state:\n      alignments = tf.transpose(final_decoder_state[0].alignment_history.stack(), [1, 2, 0])\n\n      self.inputs = inputs\n      self.input_lengths = input_lengths\n      self.mel_outputs = mel_outputs\n      self.linear_outputs = linear_outputs\n      self.alignments = alignments\n      self.mel_targets = mel_targets\n      self.linear_targets = linear_targets\n      #log(\'Initialized Tacotron model. Dimensions: \')\n      #log(\'  embedding:               %d\' % embedded_inputs.shape[-1])\n      #log(\'  prenet out:              %d\' % prenet_outputs.shape[-1])\n      #log(\'  encoder out:             %d\' % encoder_outputs.shape[-1])\n      #log(\'  attention out:           %d\' % attention_cell.output_size)\n      #log(\'  concat attn & out:       %d\' % concat_cell.output_size)\n      #log(\'  decoder cell out:        %d\' % decoder_cell.output_size)\n      #log(\'  decoder out (%d frames):  %d\' % (hp.outputs_per_step, decoder_outputs.shape[-1]))\n      #log(\'  decoder out (1 frame):   %d\' % mel_outputs.shape[-1])\n      #log(\'  postnet out:             %d\' % post_outputs.shape[-1])\n      #log(\'  linear out:              %d\' % linear_outputs.shape[-1])\n\n\n  def add_loss(self):\n    \'\'\'Adds loss to the model. Sets ""loss"" field. initialize must have been called.\'\'\'\n    with tf.variable_scope(\'loss\') as scope:\n      hp = self._hparams\n      self.mel_loss = tf.reduce_mean(tf.abs(self.mel_targets - self.mel_outputs))\n      l1 = tf.abs(self.linear_targets - self.linear_outputs)\n      # Prioritize loss for frequencies under 3000 Hz.\n      n_priority_freq = int(3000 / (hp.sample_rate * 0.5) * hp.num_freq)\n      self.linear_loss = 0.5 * tf.reduce_mean(l1) + 0.5 * tf.reduce_mean(l1[:,:,0:n_priority_freq])\n      self.loss = self.mel_loss + self.linear_loss\n\n\n  def add_optimizer(self, global_step):\n    \'\'\'Adds optimizer. Sets ""gradients"" and ""optimize"" fields. add_loss must have been called.\n\n    Args:\n      global_step: int32 scalar Tensor representing current global step in training\n    \'\'\'\n    with tf.variable_scope(\'optimizer\') as scope:\n      hp = self._hparams\n      if hp.decay_learning_rate:\n        self.learning_rate = _learning_rate_decay(hp.initial_learning_rate, global_step)\n      else:\n        self.learning_rate = tf.convert_to_tensor(hp.initial_learning_rate)\n      optimizer = tf.train.AdamOptimizer(self.learning_rate, hp.adam_beta1, hp.adam_beta2)\n      gradients, variables = zip(*optimizer.compute_gradients(self.loss))\n      self.gradients = gradients\n      clipped_gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n\n      # Add dependency on UPDATE_OPS; otherwise batchnorm won\'t work correctly. See:\n      # https://github.com/tensorflow/tensorflow/issues/1122\n      with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        self.optimize = optimizer.apply_gradients(zip(clipped_gradients, variables),\n          global_step=global_step)\n\n\ndef _learning_rate_decay(init_lr, global_step):\n  # Noam scheme from tensor2tensor:\n  warmup_steps = 4000.0\n  step = tf.cast(global_step + 1, dtype=tf.float32)\n  return init_lr * warmup_steps**0.5 * tf.minimum(step * warmup_steps**-1.5, step**-0.5)\n'"
libfaceid/tacotron/tests/__init__.py,0,b''
libfaceid/tacotron/tests/cmudict_test.py,0,"b'import io\nfrom text import cmudict\n\n\ntest_data = \'\'\'\n;;; # CMUdict  --  Major Version: 0.07\n)PAREN  P ER EH N\n\'TIS  T IH Z\nADVERSE  AE0 D V ER1 S\nADVERSE(1)  AE1 D V ER2 S\nADVERSE(2)  AE2 D V ER1 S\nADVERSELY  AE0 D V ER1 S L IY0\nADVERSITY  AE0 D V ER1 S IH0 T IY2\nBARBERSHOP  B AA1 R B ER0 SH AA2 P\nYOU\'LL  Y UW1 L\n\'\'\'\n\n\ndef test_cmudict():\n  c = cmudict.CMUDict(io.StringIO(test_data))\n  assert len(c) == 6\n  assert len(cmudict.valid_symbols) == 84\n  assert c.lookup(\'ADVERSITY\') == [\'AE0 D V ER1 S IH0 T IY2\']\n  assert c.lookup(\'BarberShop\') == [\'B AA1 R B ER0 SH AA2 P\']\n  assert c.lookup(""You\'ll"") == [\'Y UW1 L\']\n  assert c.lookup(""\'tis"") == [\'T IH Z\']\n  assert c.lookup(\'adverse\') == [\n    \'AE0 D V ER1 S\',\n    \'AE1 D V ER2 S\',\n    \'AE2 D V ER1 S\',\n  ]\n  assert c.lookup(\'\') == None\n  assert c.lookup(\'foo\') == None\n  assert c.lookup(\')paren\') == None\n\n\ndef test_cmudict_no_keep_ambiguous():\n  c = cmudict.CMUDict(io.StringIO(test_data), keep_ambiguous=False)\n  assert len(c) == 5\n  assert c.lookup(\'adversity\') == [\'AE0 D V ER1 S IH0 T IY2\']\n  assert c.lookup(\'adverse\') == None\n'"
libfaceid/tacotron/tests/numbers_test.py,0,"b""from text.numbers import normalize_numbers\n\n\ndef test_normalize_numbers():\n  assert normalize_numbers('1') == 'one'\n  assert normalize_numbers('15') == 'fifteen'\n  assert normalize_numbers('24') == 'twenty-four'\n  assert normalize_numbers('100') == 'one hundred'\n  assert normalize_numbers('101') == 'one hundred one'\n  assert normalize_numbers('456') == 'four hundred fifty-six'\n  assert normalize_numbers('1000') == 'one thousand'\n  assert normalize_numbers('1800') == 'eighteen hundred'\n  assert normalize_numbers('2,000') == 'two thousand'\n  assert normalize_numbers('3000') == 'three thousand'\n  assert normalize_numbers('18000') == 'eighteen thousand'\n  assert normalize_numbers('24,000') == 'twenty-four thousand'\n  assert normalize_numbers('124,001') == 'one hundred twenty-four thousand one'\n  assert normalize_numbers('6.4 sec') == 'six point four sec'\n\n\ndef test_normalize_ordinals():\n  assert normalize_numbers('1st') == 'first'\n  assert normalize_numbers('2nd') == 'second'\n  assert normalize_numbers('9th') == 'ninth'\n  assert normalize_numbers('243rd place') == 'two hundred and forty-third place'\n\n\ndef test_normalize_dates():\n  assert normalize_numbers('1400') == 'fourteen hundred'\n  assert normalize_numbers('1901') == 'nineteen oh one'\n  assert normalize_numbers('1999') == 'nineteen ninety-nine'\n  assert normalize_numbers('2000') == 'two thousand'\n  assert normalize_numbers('2004') == 'two thousand four'\n  assert normalize_numbers('2010') == 'twenty ten'\n  assert normalize_numbers('2012') == 'twenty twelve'\n  assert normalize_numbers('2025') == 'twenty twenty-five'\n  assert normalize_numbers('September 11, 2001') == 'September eleven, two thousand one'\n  assert normalize_numbers('July 26, 1984.') == 'July twenty-six, nineteen eighty-four.'\n\n\ndef test_normalize_money():\n  assert normalize_numbers('$0.00') == 'zero dollars'\n  assert normalize_numbers('$1') == 'one dollar'\n  assert normalize_numbers('$10') == 'ten dollars'\n  assert normalize_numbers('$.01') == 'one cent'\n  assert normalize_numbers('$0.25') == 'twenty-five cents'\n  assert normalize_numbers('$5.00') == 'five dollars'\n  assert normalize_numbers('$5.01') == 'five dollars, one cent'\n  assert normalize_numbers('$135.99.') == 'one hundred thirty-five dollars, ninety-nine cents.'\n  assert normalize_numbers('$40,000') == 'forty thousand dollars'\n  assert normalize_numbers('for \xc2\xa32500!') == 'for twenty-five hundred pounds!'\n"""
libfaceid/tacotron/tests/text_test.py,0,"b'from text import cleaners, symbols, text_to_sequence, sequence_to_text\nfrom unidecode import unidecode\n\n\ndef test_symbols():\n  assert len(symbols) >= 3\n  assert symbols[0] == \'_\'\n  assert symbols[1] == \'~\'\n\n\ndef test_text_to_sequence():\n  assert text_to_sequence(\'\', []) == [1]\n  assert text_to_sequence(\'Hi!\', []) == [9, 36, 54, 1]\n  assert text_to_sequence(\'""A""_B\', []) == [2, 3, 1]\n  assert text_to_sequence(\'A {AW1 S} B\', []) == [2, 64, 83, 132, 64, 3, 1]\n  assert text_to_sequence(\'Hi\', [\'lowercase\']) == [35, 36, 1]\n  assert text_to_sequence(\'A {AW1 S}  B\', [\'english_cleaners\']) == [28, 64, 83, 132, 64, 29, 1]\n\n\ndef test_sequence_to_text():\n  assert sequence_to_text([]) == \'\'\n  assert sequence_to_text([1]) == \'~\'\n  assert sequence_to_text([9, 36, 54, 1]) == \'Hi!~\'\n  assert sequence_to_text([2, 64, 83, 132, 64, 3]) == \'A {AW1 S} B\'\n\n\ndef test_collapse_whitespace():\n  assert cleaners.collapse_whitespace(\'\') == \'\'\n  assert cleaners.collapse_whitespace(\'  \') == \' \'\n  assert cleaners.collapse_whitespace(\'x\') == \'x\'\n  assert cleaners.collapse_whitespace(\' x.  y,  \\tz\') == \' x. y, z\'\n\n\ndef test_convert_to_ascii():\n  assert cleaners.convert_to_ascii(""raison d\'\xc3\xaatre"") == ""raison d\'etre""\n  assert cleaners.convert_to_ascii(\'gr\xc3\xbc\xc3\x9f gott\') == \'gruss gott\'\n  assert cleaners.convert_to_ascii(\'\xec\x95\x88\xeb\x85\x95\') == \'annyeong\'\n  assert cleaners.convert_to_ascii(\'\xd0\x97\xd0\xb4\xd1\x80\xd0\xb0\xd0\xb2\xd1\x81\xd1\x82\xd0\xb2\xd1\x83\xd0\xb9\xd1\x82\xd0\xb5\') == \'Zdravstvuite\'\n\n\ndef test_lowercase():\n  assert cleaners.lowercase(\'Happy Birthday!\') == \'happy birthday!\'\n  assert cleaners.lowercase(\'CAF\xc3\x89\') == \'caf\xc3\xa9\'\n\n\ndef test_expand_abbreviations():\n  assert cleaners.expand_abbreviations(\'mr. and mrs. smith\') == \'mister and misess smith\'\n\n\ndef test_expand_numbers():\n  assert cleaners.expand_numbers(\'3 apples and 44 pears\') == \'three apples and forty-four pears\'\n  assert cleaners.expand_numbers(\'$3.50 for gas.\') == \'three dollars, fifty cents for gas.\'\n\n\ndef test_cleaner_pipelines():\n  text = \'Mr. M\xc3\xbcller ate  2 Apples\'\n  assert cleaners.english_cleaners(text) == \'mister muller ate two apples\'\n  assert cleaners.transliteration_cleaners(text) == \'mr. muller ate 2 apples\'\n  assert cleaners.basic_cleaners(text) == \'mr. m\xc3\xbcller ate 2 apples\'\n\n'"
libfaceid/tacotron/text/__init__.py,0,"b'import re\nfrom libfaceid.tacotron.text import cleaners\nfrom libfaceid.tacotron.text.symbols import symbols\n\n\n# Mappings from symbol to numeric ID and vice versa:\n_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n\n# Regular expression matching text enclosed in curly braces:\n_curly_re = re.compile(r\'(.*?)\\{(.+?)\\}(.*)\')\n\n\ndef text_to_sequence(text, cleaner_names):\n  \'\'\'Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n\n    The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n    in it. For example, ""Turn left on {HH AW1 S S T AH0 N} Street.""\n\n    Args:\n      text: string to convert to a sequence\n      cleaner_names: names of the cleaner functions to run the text through\n\n    Returns:\n      List of integers corresponding to the symbols in the text\n  \'\'\'\n  sequence = []\n\n  # Check for curly braces and treat their contents as ARPAbet:\n  while len(text):\n    m = _curly_re.match(text)\n    if not m:\n      sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n      break\n    sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n    sequence += _arpabet_to_sequence(m.group(2))\n    text = m.group(3)\n\n  # Append EOS token\n  sequence.append(_symbol_to_id[\'~\'])\n  return sequence\n\n\ndef sequence_to_text(sequence):\n  \'\'\'Converts a sequence of IDs back to a string\'\'\'\n  result = \'\'\n  for symbol_id in sequence:\n    if symbol_id in _id_to_symbol:\n      s = _id_to_symbol[symbol_id]\n      # Enclose ARPAbet back in curly braces:\n      if len(s) > 1 and s[0] == \'@\':\n        s = \'{%s}\' % s[1:]\n      result += s\n  return result.replace(\'}{\', \' \')\n\n\ndef _clean_text(text, cleaner_names):\n  for name in cleaner_names:\n    cleaner = getattr(cleaners, name)\n    if not cleaner:\n      raise Exception(\'Unknown cleaner: %s\' % name)\n    text = cleaner(text)\n  return text\n\n\ndef _symbols_to_sequence(symbols):\n  return [_symbol_to_id[s] for s in symbols if _should_keep_symbol(s)]\n\n\ndef _arpabet_to_sequence(text):\n  return _symbols_to_sequence([\'@\' + s for s in text.split()])\n\n\ndef _should_keep_symbol(s):\n  return s in _symbol_to_id and s is not \'_\' and s is not \'~\'\n'"
libfaceid/tacotron/text/cleaners.py,0,"b'\'\'\'\nCleaners are transformations that run over the input text at both training and eval time.\n\nCleaners can be selected by passing a comma-delimited list of cleaner names as the ""cleaners""\nhyperparameter. Some cleaners are English-specific. You\'ll typically want to use:\n  1. ""english_cleaners"" for English text\n  2. ""transliteration_cleaners"" for non-English text that can be transliterated to ASCII using\n     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n  3. ""basic_cleaners"" if you do not want to transliterate (in this case, you should also update\n     the symbols in symbols.py to match your data).\n\'\'\'\n\nimport re\nfrom unidecode import unidecode\nfrom .numbers import normalize_numbers\n\n\n# Regular expression matching whitespace:\n_whitespace_re = re.compile(r\'\\s+\')\n\n# List of (regular expression, replacement) pairs for abbreviations:\n_abbreviations = [(re.compile(\'\\\\b%s\\\\.\' % x[0], re.IGNORECASE), x[1]) for x in [\n  (\'mrs\', \'misess\'),\n  (\'mr\', \'mister\'),\n  (\'dr\', \'doctor\'),\n  (\'st\', \'saint\'),\n  (\'co\', \'company\'),\n  (\'jr\', \'junior\'),\n  (\'maj\', \'major\'),\n  (\'gen\', \'general\'),\n  (\'drs\', \'doctors\'),\n  (\'rev\', \'reverend\'),\n  (\'lt\', \'lieutenant\'),\n  (\'hon\', \'honorable\'),\n  (\'sgt\', \'sergeant\'),\n  (\'capt\', \'captain\'),\n  (\'esq\', \'esquire\'),\n  (\'ltd\', \'limited\'),\n  (\'col\', \'colonel\'),\n  (\'ft\', \'fort\'),\n]]\n\n\ndef expand_abbreviations(text):\n  for regex, replacement in _abbreviations:\n    text = re.sub(regex, replacement, text)\n  return text\n\n\ndef expand_numbers(text):\n  return normalize_numbers(text)\n\n\ndef lowercase(text):\n  return text.lower()\n\n\ndef collapse_whitespace(text):\n  return re.sub(_whitespace_re, \' \', text)\n\n\ndef convert_to_ascii(text):\n  return unidecode(text)\n\n\ndef basic_cleaners(text):\n  \'\'\'Basic pipeline that lowercases and collapses whitespace without transliteration.\'\'\'\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef transliteration_cleaners(text):\n  \'\'\'Pipeline for non-English text that transliterates to ASCII.\'\'\'\n  text = convert_to_ascii(text)\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef english_cleaners(text):\n  \'\'\'Pipeline for English text, including number and abbreviation expansion.\'\'\'\n  text = convert_to_ascii(text)\n  text = lowercase(text)\n  text = expand_numbers(text)\n  text = expand_abbreviations(text)\n  text = collapse_whitespace(text)\n  return text\n'"
libfaceid/tacotron/text/cmudict.py,0,"b'import re\n\n\nvalid_symbols = [\n  \'AA\', \'AA0\', \'AA1\', \'AA2\', \'AE\', \'AE0\', \'AE1\', \'AE2\', \'AH\', \'AH0\', \'AH1\', \'AH2\',\n  \'AO\', \'AO0\', \'AO1\', \'AO2\', \'AW\', \'AW0\', \'AW1\', \'AW2\', \'AY\', \'AY0\', \'AY1\', \'AY2\',\n  \'B\', \'CH\', \'D\', \'DH\', \'EH\', \'EH0\', \'EH1\', \'EH2\', \'ER\', \'ER0\', \'ER1\', \'ER2\', \'EY\',\n  \'EY0\', \'EY1\', \'EY2\', \'F\', \'G\', \'HH\', \'IH\', \'IH0\', \'IH1\', \'IH2\', \'IY\', \'IY0\', \'IY1\',\n  \'IY2\', \'JH\', \'K\', \'L\', \'M\', \'N\', \'NG\', \'OW\', \'OW0\', \'OW1\', \'OW2\', \'OY\', \'OY0\',\n  \'OY1\', \'OY2\', \'P\', \'R\', \'S\', \'SH\', \'T\', \'TH\', \'UH\', \'UH0\', \'UH1\', \'UH2\', \'UW\',\n  \'UW0\', \'UW1\', \'UW2\', \'V\', \'W\', \'Y\', \'Z\', \'ZH\'\n]\n\n_valid_symbol_set = set(valid_symbols)\n\n\nclass CMUDict:\n  \'\'\'Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict\'\'\'\n  def __init__(self, file_or_path, keep_ambiguous=True):\n    if isinstance(file_or_path, str):\n      with open(file_or_path, encoding=\'latin-1\') as f:\n        entries = _parse_cmudict(f)\n    else:\n      entries = _parse_cmudict(file_or_path)\n    if not keep_ambiguous:\n      entries = {word: pron for word, pron in entries.items() if len(pron) == 1}\n    self._entries = entries\n\n\n  def __len__(self):\n    return len(self._entries)\n\n\n  def lookup(self, word):\n    \'\'\'Returns list of ARPAbet pronunciations of the given word.\'\'\'\n    return self._entries.get(word.upper())\n\n\n\n_alt_re = re.compile(r\'\\([0-9]+\\)\')\n\n\ndef _parse_cmudict(file):\n  cmudict = {}\n  for line in file:\n    if len(line) and (line[0] >= \'A\' and line[0] <= \'Z\' or line[0] == ""\'""):\n      parts = line.split(\'  \')\n      word = re.sub(_alt_re, \'\', parts[0])\n      pronunciation = _get_pronunciation(parts[1])\n      if pronunciation:\n        if word in cmudict:\n          cmudict[word].append(pronunciation)\n        else:\n          cmudict[word] = [pronunciation]\n  return cmudict\n\n\ndef _get_pronunciation(s):\n  parts = s.strip().split(\' \')\n  for part in parts:\n    if part not in _valid_symbol_set:\n      return None\n  return \' \'.join(parts)\n'"
libfaceid/tacotron/text/numbers.py,0,"b""import inflect\nimport re\n\n\n_inflect = inflect.engine()\n_comma_number_re = re.compile(r'([0-9][0-9\\,]+[0-9])')\n_decimal_number_re = re.compile(r'([0-9]+\\.[0-9]+)')\n_pounds_re = re.compile(r'\xc2\xa3([0-9\\,]*[0-9]+)')\n_dollars_re = re.compile(r'\\$([0-9\\.\\,]*[0-9]+)')\n_ordinal_re = re.compile(r'[0-9]+(st|nd|rd|th)')\n_number_re = re.compile(r'[0-9]+')\n\n\ndef _remove_commas(m):\n  return m.group(1).replace(',', '')\n\n\ndef _expand_decimal_point(m):\n  return m.group(1).replace('.', ' point ')\n\n\ndef _expand_dollars(m):\n  match = m.group(1)\n  parts = match.split('.')\n  if len(parts) > 2:\n    return match + ' dollars'  # Unexpected format\n  dollars = int(parts[0]) if parts[0] else 0\n  cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n  if dollars and cents:\n    dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n    cent_unit = 'cent' if cents == 1 else 'cents'\n    return '%s %s, %s %s' % (dollars, dollar_unit, cents, cent_unit)\n  elif dollars:\n    dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n    return '%s %s' % (dollars, dollar_unit)\n  elif cents:\n    cent_unit = 'cent' if cents == 1 else 'cents'\n    return '%s %s' % (cents, cent_unit)\n  else:\n    return 'zero dollars'\n\n\ndef _expand_ordinal(m):\n  return _inflect.number_to_words(m.group(0))\n\n\ndef _expand_number(m):\n  num = int(m.group(0))\n  if num > 1000 and num < 3000:\n    if num == 2000:\n      return 'two thousand'\n    elif num > 2000 and num < 2010:\n      return 'two thousand ' + _inflect.number_to_words(num % 100)\n    elif num % 100 == 0:\n      return _inflect.number_to_words(num // 100) + ' hundred'\n    else:\n      return _inflect.number_to_words(num, andword='', zero='oh', group=2).replace(', ', ' ')\n  else:\n    return _inflect.number_to_words(num, andword='')\n\n\ndef normalize_numbers(text):\n  text = re.sub(_comma_number_re, _remove_commas, text)\n  text = re.sub(_pounds_re, r'\\1 pounds', text)\n  text = re.sub(_dollars_re, _expand_dollars, text)\n  text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n  text = re.sub(_ordinal_re, _expand_ordinal, text)\n  text = re.sub(_number_re, _expand_number, text)\n  return text\n"""
libfaceid/tacotron/text/symbols.py,0,"b'\'\'\'\nDefines the set of symbols used in text input to the model.\n\nThe default is a set of ASCII characters that works well for English or text that has been run\nthrough Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details.\n\'\'\'\nfrom libfaceid.tacotron.text import cmudict\n\n_pad        = \'_\'\n_eos        = \'~\'\n_characters = \'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\\\'(),-.:;? \'\n\n# Prepend ""@"" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n_arpabet = [\'@\' + s for s in cmudict.valid_symbols]\n\n# Export all symbols:\nsymbols = [_pad, _eos] + list(_characters) + _arpabet\n'"
libfaceid/tacotron/util/__init__.py,0,"b'class ValueWindow():\n  def __init__(self, window_size=100):\n    self._window_size = window_size\n    self._values = []\n\n  def append(self, x):\n    self._values = self._values[-(self._window_size - 1):] + [x]\n\n  @property\n  def sum(self):\n    return sum(self._values)\n\n  @property\n  def count(self):\n    return len(self._values)\n\n  @property\n  def average(self):\n    return self.sum / max(1, self.count)\n\n  def reset(self):\n    self._values = []\n'"
libfaceid/tacotron/util/audio.py,10,"b""import librosa\nimport librosa.filters\nimport math\nimport numpy as np\nimport tensorflow as tf\nimport scipy\nfrom libfaceid.tacotron.hparams import hparams\n\n\ndef load_wav(path):\n  return librosa.core.load(path, sr=hparams.sample_rate)[0]\n\n\ndef save_wav(wav, path):\n  wav *= 32767 / max(0.01, np.max(np.abs(wav)))\n  scipy.io.wavfile.write(path, hparams.sample_rate, wav.astype(np.int16))\n\n\ndef preemphasis(x):\n  return scipy.signal.lfilter([1, -hparams.preemphasis], [1], x)\n\n\ndef inv_preemphasis(x):\n  return scipy.signal.lfilter([1], [1, -hparams.preemphasis], x)\n\n\ndef spectrogram(y):\n  D = _stft(preemphasis(y))\n  S = _amp_to_db(np.abs(D)) - hparams.ref_level_db\n  return _normalize(S)\n\n\ndef inv_spectrogram(spectrogram):\n  '''Converts spectrogram to waveform using librosa'''\n  S = _db_to_amp(_denormalize(spectrogram) + hparams.ref_level_db)  # Convert back to linear\n  return inv_preemphasis(_griffin_lim(S ** hparams.power))          # Reconstruct phase\n\n\ndef inv_spectrogram_tensorflow(spectrogram):\n  '''Builds computational graph to convert spectrogram to waveform using TensorFlow.\n\n  Unlike inv_spectrogram, this does NOT invert the preemphasis. The caller should call\n  inv_preemphasis on the output after running the graph.\n  '''\n  S = _db_to_amp_tensorflow(_denormalize_tensorflow(spectrogram) + hparams.ref_level_db)\n  return _griffin_lim_tensorflow(tf.pow(S, hparams.power))\n\n\ndef melspectrogram(y):\n  D = _stft(preemphasis(y))\n  S = _amp_to_db(_linear_to_mel(np.abs(D))) - hparams.ref_level_db\n  return _normalize(S)\n\n\ndef find_endpoint(wav, threshold_db=-40, min_silence_sec=0.8):\n  window_length = int(hparams.sample_rate * min_silence_sec)\n  hop_length = int(window_length / 4)\n  threshold = _db_to_amp(threshold_db)\n  for x in range(hop_length, len(wav) - window_length, hop_length):\n    if np.max(wav[x:x+window_length]) < threshold:\n      return x + hop_length\n  return len(wav)\n\n\ndef _griffin_lim(S):\n  '''librosa implementation of Griffin-Lim\n  Based on https://github.com/librosa/librosa/issues/434\n  '''\n  angles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n  S_complex = np.abs(S).astype(np.complex)\n  y = _istft(S_complex * angles)\n  for i in range(hparams.griffin_lim_iters):\n    angles = np.exp(1j * np.angle(_stft(y)))\n    y = _istft(S_complex * angles)\n  return y\n\n\ndef _griffin_lim_tensorflow(S):\n  '''TensorFlow implementation of Griffin-Lim\n  Based on https://github.com/Kyubyong/tensorflow-exercises/blob/master/Audio_Processing.ipynb\n  '''\n  with tf.variable_scope('griffinlim'):\n    # TensorFlow's stft and istft operate on a batch of spectrograms; create batch of size 1\n    S = tf.expand_dims(S, 0)\n    S_complex = tf.identity(tf.cast(S, dtype=tf.complex64))\n    y = _istft_tensorflow(S_complex)\n    for i in range(hparams.griffin_lim_iters):\n      est = _stft_tensorflow(y)\n      angles = est / tf.cast(tf.maximum(1e-8, tf.abs(est)), tf.complex64)\n      y = _istft_tensorflow(S_complex * angles)\n    return tf.squeeze(y, 0)\n\n\ndef _stft(y):\n  n_fft, hop_length, win_length = _stft_parameters()\n  return librosa.stft(y=y, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n\n\ndef _istft(y):\n  _, hop_length, win_length = _stft_parameters()\n  return librosa.istft(y, hop_length=hop_length, win_length=win_length)\n\n\ndef _stft_tensorflow(signals):\n  n_fft, hop_length, win_length = _stft_parameters()\n  return tf.contrib.signal.stft(signals, win_length, hop_length, n_fft, pad_end=False)\n\n\ndef _istft_tensorflow(stfts):\n  n_fft, hop_length, win_length = _stft_parameters()\n  return tf.contrib.signal.inverse_stft(stfts, win_length, hop_length, n_fft)\n\n\ndef _stft_parameters():\n  n_fft = (hparams.num_freq - 1) * 2\n  hop_length = int(hparams.frame_shift_ms / 1000 * hparams.sample_rate)\n  win_length = int(hparams.frame_length_ms / 1000 * hparams.sample_rate)\n  return n_fft, hop_length, win_length\n\n\n# Conversions:\n\n_mel_basis = None\n\ndef _linear_to_mel(spectrogram):\n  global _mel_basis\n  if _mel_basis is None:\n    _mel_basis = _build_mel_basis()\n  return np.dot(_mel_basis, spectrogram)\n\ndef _build_mel_basis():\n  n_fft = (hparams.num_freq - 1) * 2\n  return librosa.filters.mel(hparams.sample_rate, n_fft, n_mels=hparams.num_mels)\n\ndef _amp_to_db(x):\n  return 20 * np.log10(np.maximum(1e-5, x))\n\ndef _db_to_amp(x):\n  return np.power(10.0, x * 0.05)\n\ndef _db_to_amp_tensorflow(x):\n  return tf.pow(tf.ones(tf.shape(x)) * 10.0, x * 0.05)\n\ndef _normalize(S):\n  return np.clip((S - hparams.min_level_db) / -hparams.min_level_db, 0, 1)\n\ndef _denormalize(S):\n  return (np.clip(S, 0, 1) * -hparams.min_level_db) + hparams.min_level_db\n\ndef _denormalize_tensorflow(S):\n  return (tf.clip_by_value(S, 0, 1) * -hparams.min_level_db) + hparams.min_level_db\n"""
libfaceid/tacotron/util/infolog.py,0,"b'import atexit\nfrom datetime import datetime\nimport json\nfrom threading import Thread\nfrom urllib.request import Request, urlopen\n\n\n_format = \'%Y-%m-%d %H:%M:%S.%f\'\n_file = None\n_run_name = None\n_slack_url = None\n\n\ndef init(filename, run_name, slack_url=None):\n  global _file, _run_name, _slack_url\n  _close_logfile()\n  _file = open(filename, \'a\', encoding=""utf-8"")\n  _file.write(\'\\n-----------------------------------------------------------------\\n\')\n  _file.write(\'Starting new training run\\n\')\n  _file.write(\'-----------------------------------------------------------------\\n\')\n  _run_name = run_name\n  _slack_url = slack_url\n\n\ndef log(msg, slack=False):\n  print(msg)\n  if _file is not None:\n    _file.write(\'[%s]  %s\\n\' % (datetime.now().strftime(_format)[:-3], msg))\n  if slack and _slack_url is not None:\n    Thread(target=_send_slack, args=(msg,)).start()\n\n\ndef _close_logfile():\n  global _file\n  if _file is not None:\n    _file.close()\n    _file = None\n\n\ndef _send_slack(msg):\n  req = Request(_slack_url)\n  req.add_header(\'Content-Type\', \'application/json\')\n  urlopen(req, json.dumps({\n    \'username\': \'tacotron\',\n    \'icon_emoji\': \':taco:\',\n    \'text\': \'*%s*: %s\' % (_run_name, msg)\n  }).encode())\n\n\natexit.register(_close_logfile)\n'"
libfaceid/tacotron/util/plot.py,0,"b""import matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\n\ndef plot_alignment(alignment, path, info=None):\n  fig, ax = plt.subplots()\n  im = ax.imshow(\n    alignment,\n    aspect='auto',\n    origin='lower',\n    interpolation='none')\n  fig.colorbar(im, ax=ax)\n  xlabel = 'Decoder timestep'\n  if info is not None:\n    xlabel += '\\n\\n' + info\n  plt.xlabel(xlabel)\n  plt.ylabel('Encoder timestep')\n  plt.tight_layout()\n  plt.savefig(path, format='png')\n"""
