file_path,api_count,code
setup.py,0,"b""import os\n\n\nprint('Installing requirements...')\nos.system('pip install -r requirements.txt')\n\nimport requests\nimport zipfile\nfrom clint.textui import progress\n\n\ndef download_data(url, zipped_path, extract):\n  # Open the url and download the data with progress bars.\n  data_stream = requests.get(url, stream=True)\n\n  with open(zipped_path, 'wb') as file:\n    total_length = int(data_stream.headers.get('content-length'))\n    for chunk in progress.bar(data_stream.iter_content(chunk_size=1024),\n                              expected_size=total_length / 1024 + 1):\n      if chunk:\n        file.write(chunk)\n        file.flush()\n\n  # Extract file.\n  zip_file = zipfile.ZipFile(zipped_path, 'r')\n  zip_file.extractall(extract)\n  zip_file.close()\n\n\nprint('Do you want to download all datasets used in the paper (116 MB)? (y/n)')\nif input() == 'y':\n  if not os.path.exists('data'):\n    os.mkdir('data')\n  download_data('https://ricsinaruto.github.io/website/docs/Twitter.zip', 'data/Twitter.zip', 'data')\n  download_data('https://ricsinaruto.github.io/website/docs/Cornell.zip', 'data/Cornell.zip', 'data')\n  download_data('https://ricsinaruto.github.io/website/docs/DailyDialog.zip', 'data/DailyDialog.zip', 'data')\n\nprint('Do you want to download all generated responses on the test set by the different models (7 MB)? (y/n)')\nif input() == 'y':\n  download_data('https://ricsinaruto.github.io/website/docs/responses.zip', 'responses.zip', '')\n"""
t2t_csaky/__init__.py,0,"b""'''\nThis file is the main python file of the project importing all\n  problem,\n  model,\n  hparam registrations\n'''\n\nfrom t2t_csaky.problems import character_chatbot\nfrom t2t_csaky.problems import cornell_chatbots\nfrom t2t_csaky.problems import daily_dialog_chatbot\nfrom t2t_csaky.problems import persona_chat_chatbot\nfrom t2t_csaky.problems import opensubtitles_chatbot\n\nfrom t2t_csaky.models import gradient_checkpointed_seq2seq\n\nfrom t2t_csaky.hparams import transformer_hparams\nfrom t2t_csaky.hparams import seq2seq_hparams\n"""
t2t_csaky/config.py,0,"b""'''\nIn this file you can set all tensor2tensor flags, hparams and other settings\nfor the current run. This file will also be copied to the provided directory.\n'''\n\n\nFLAGS = {\n    't2t_usr_dir': 't2t_csaky',  # Tensor2tensor imports from this dir.\n    'data_dir': 'data_dir/DailyDialog/test_repo',\n    'train_dir': 'train_dir/DailyDialog/seq2seq_base-base_with_numbers',\n    'decode_dir': 'decode_dir/DailyDialog/trf_20_dropout-base',\n    'problem': 'daily_dialog_chatbot',\n    'model': 'transformer',\n    'hparams': '',  # This is empty if we use hparams defined in this file.\n                    # Otherwise you have to specify a registered hparams_set.\n    'profile_perform': 'True',\n\n    # Training related flags.\n    'train_mode': 'train_and_evaluate',\n    'memory_fraction': 0.95,    # Fraction of total GPU memory to use.\n    'keep_checkpoints': 3,      # How many checkpoints to keep on disk.\n    'train_steps': 1000000,\n    'save_every_n_hour': 0,     # Save checkpoints every n hours.\n    'save_every_n_secs': 1800,  # Every n seconds, overrides hour param.\n    'evaluation_steps': 1000,   # Number of evaluation steps at each cycle.\n    'evaluation_freq': 1000,    # Evaluation cycle is run every n train steps.\n\n    # Decoding related flags.\n    'batch_size': 32,\n    'output_file_name': 'inference_at_11k.txt',  # Save the inference outputs.\n    'input_file_name': 'NCM_examples.txt',  # Read inputs to be fed.\n    'decode_mode': 'file',  # Can be: interactive, file, dataset.\n    'beam_size': 10,\n    'return_beams': 'True'  # If False return only top beam, else beam_size.\n}\n\nPROBLEM_HPARAMS = {\n    'num_train_shards': 1,\n    'num_dev_shards': 1,\n    'vocabulary_size': 16384,\n    'dataset_size': 0,  # If zero, take the full dataset.\n    'dataset_split': {'train': 80, 'val': 10, 'test': 10},\n    'dataset_version': 2012,  # Only for opensubtitles.\n    'name_vocab_size': 3000   # Only for cornell names problem.\n}\n\n# These will be applied on top of the transformer_base hparams_set.\nTRANSFORMER_HPARAMS = {\n    # My hparams.\n    'roulette_wheel': 'Normal',  # Only works with roulette_transformer.\n    'roulette_beam_size': 100,  # Only works with roulette_transformer.\n\n    # Hparams_set override.\n    'batch_size': 4096,\n    'layer_dropout': 0.4,\n    'attention_dropout': 0.2,\n    'relu_dropout': 0.2,\n    'embed_num_shards': 16,  # Shard the embedding matrix into n matrices.\n    'summarize_vars': True   # Print out the model parameters at the start.\n}\n\n# These will be applied on top of the lstm_seq2seq hparams_set.\nSEQ2SEQ_HPARAMS = {\n    # My hparams.\n    'lstm_hidden_size': 3072,\n\n    # Hparams_set override.\n    'optimizer': 'Adafactor',\n    'fixed_batch_size': False,  # If True, batch size is number of sentences.\n                                # Otherwise it's number of tokens.\n    'summarize_vars': True,\n    'embed_num_shards': 10,  # Shard the embedding matrix into n matrices.\n    'embedding_size': 2048,\n    'num_layers': 2,\n    'batch_size': 512,\n    'max_sentence_len': 64,  # Sentences longer than this will be ignored.\n    'shared_embedding_and_softmax_weights': True  # If True, use 1 matrix for\n                                                  # softmax/embedding weights.\n}\n"""
t2t_csaky/main.py,0,"b""import argparse\n\nfrom utils import run\n\n\ndef main():\n  # Create an argument parser.\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--mode', type=str, help='Can be one of the following: {\\\n                                                train,\\\n                                                decode,\\\n                                                generate_data,\\\n                                                experiment}')\n  args = parser.parse_args()\n\n  # Different modes will call different functions.\n  run_mode = {\n      'train': run.training,\n      'decode': run.decoding,\n      'generate_data': run.data_generating,\n      'experiment': run.experiment\n  }\n\n  # Initialize a mode.\n  if args.mode in run_mode:\n    run_mode[args.mode]()\n  else:\n    print('Program exited, because no suitable mode was defined. \\\n           The mode flag has to be set to one of the following:')\n    print('  train')\n    print('  decode')\n    print('  generate_data')\n    print('  experiment')\n\n\nif __name__ == '__main__':\n  main()\n"""
t2t_csaky/hparams/seq2seq_hparams.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensor2tensor.models import lstm\nfrom tensor2tensor.utils import registry\n\nfrom t2t_csaky.config import SEQ2SEQ_HPARAMS\n\n\n# Change these in config.py.\n@registry.register_hparams\ndef general_gradient_checkpointed_seq2seq_hparams():\n  hparams = lstm.lstm_seq2seq()\n\n  hparams.clip_grad_norm = 0.0\n  hparams.shared_embedding_and_softmax_weights = SEQ2SEQ_HPARAMS[\n      'shared_embedding_and_softmax_weights']\n  hparams.optimizer = SEQ2SEQ_HPARAMS['optimizer']\n  hparams.use_fixed_batch_size = SEQ2SEQ_HPARAMS['fixed_batch_size']\n  hparams.summarize_vars = SEQ2SEQ_HPARAMS['summarize_vars']\n\n  hparams.symbol_modality_num_shards = SEQ2SEQ_HPARAMS['embed_num_shards']\n  hparams.hidden_size = SEQ2SEQ_HPARAMS['embedding_size']\n  hparams.num_hidden_layers = SEQ2SEQ_HPARAMS['num_layers']\n  hparams.batch_size = SEQ2SEQ_HPARAMS['batch_size']\n  hparams.max_length = SEQ2SEQ_HPARAMS['max_sentence_len']\n  return hparams\n\n\n@registry.register_hparams\ndef general_extracted_seq2seq_hparams():\n  return general_gradient_checkpointed_seq2seq_hparams()\n\n\n@registry.register_hparams\ndef general_lstm_seq2seq_hparams():\n  return general_gradient_checkpointed_seq2seq_hparams()\n\n\n# From this only the hidden_size is used for the lstm_seq2seq model.\ndef chatbot_lstm_hparams():\n  hparams = chatbot_lstm_batch_256()\n  hparams.hidden_size = SEQ2SEQ_HPARAMS['lstm_hidden_size']\n  return hparams\n\n\n# Different batch sizes.\n@registry.register_hparams\ndef chatbot_lstm_batch_8k():\n  hparams = lstm.lstm_seq2seq()\n\n  hparams.clip_grad_norm = 0.0\n  hparams.shared_embedding_and_softmax_weights = True\n  hparams.optimizer = 'Adafactor'\n  hparams.use_fixed_batch_size = False\n  hparams.summarize_vars = True\n\n  hparams.symbol_modality_num_shards = 10\n  hparams.hidden_size = 2048\n  hparams.num_hidden_layers = 2\n  hparams.batch_size = 8192\n  hparams.max_length = 64\n  return hparams\n\n\n@registry.register_hparams\ndef chatbot_lstm_batch_1():\n  hparams = chatbot_lstm_batch_8k()\n  hparams.batch_size = 1\n  return hparams\n\n\n@registry.register_hparams\ndef chatbot_lstm_batch_2048():\n  hparams = chatbot_lstm_batch_8k()\n  hparams.batch_size = 2048\n  return hparams\n\n\n@registry.register_hparams\ndef chatbot_lstm_batch_1024():\n  hparams = chatbot_lstm_batch_8k()\n  hparams.batch_size = 1024\n  return hparams\n\n\n@registry.register_hparams\ndef chatbot_lstm_batch_4():\n  hparams = chatbot_lstm_batch_8k()\n  hparams.batch_size = 4\n  return hparams\n\n\n@registry.register_hparams\ndef chatbot_lstm_batch_8():\n  hparams = chatbot_lstm_batch_8k()\n  hparams.batch_size = 8\n  return hparams\n\n\n@registry.register_hparams\ndef chatbot_lstm_batch_512():\n  hparams = chatbot_lstm_batch_8k()\n  hparams.batch_size = 512\n  return hparams\n\n\n@registry.register_hparams\ndef chatbot_lstm_batch_256():\n  hparams = chatbot_lstm_batch_8k()\n  hparams.batch_size = 256\n  return hparams\n\n\n@registry.register_hparams\ndef chatbot_lstm_batch_128():\n  hparams = chatbot_lstm_batch_8k()\n  hparams.batch_size = 128\n  return hparams\n\n\n@registry.register_hparams\ndef chatbot_lstm_batch_64():\n  hparams = chatbot_lstm_batch_8k()\n  hparams.batch_size = 64\n  return hparams\n\n\n@registry.register_hparams\ndef chatbot_lstm_batch_32():\n  hparams = chatbot_lstm_batch_8k()\n  hparams.batch_size = 32\n  return hparams\n\n\n@registry.register_hparams\ndef chatbot_lstm_batch_40():\n  hparams = chatbot_lstm_batch_8k()\n  hparams.batch_size = 40\n  return hparams\n"""
t2t_csaky/hparams/transformer_hparams.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensor2tensor.models import transformer\nfrom tensor2tensor.utils import registry\n\nfrom t2t_csaky.config import TRANSFORMER_HPARAMS\n\n\n# change these in config.py\n@registry.register_hparams\ndef general_transformer_hparams():\n  hparams = transformer.transformer_base()\n  hparams.add_hparam('roulette', TRANSFORMER_HPARAMS['roulette_wheel'])\n  hparams.add_hparam('roulette_beam_size',\n                     TRANSFORMER_HPARAMS['roulette_beam_size'])\n\n  hparams.batch_size = TRANSFORMER_HPARAMS['batch_size']\n  hparams.layer_prepostprocess_dropout = TRANSFORMER_HPARAMS['layer_dropout']\n  hparams.symbol_modality_num_shards = TRANSFORMER_HPARAMS['embed_num_shards']\n  hparams.attention_dropout = TRANSFORMER_HPARAMS['attention_dropout']\n  hparams.relu_dropout = TRANSFORMER_HPARAMS['relu_dropout']\n  hparams.summarize_vars = TRANSFORMER_HPARAMS['summarize_vars']\n\n  return hparams\n\n\n@registry.register_hparams\ndef general_roulette_transformer_hparams():\n  return general_transformer_hparams()\n\n\n@registry.register_hparams\ndef general_extracted_transformer_hparams():\n  return general_transformer_hparams()\n\n\n# Exactly replicates the base transformer model described in the paper.\n@registry.register_hparams\ndef chatbot_cornell_base():\n  hparams = transformer.transformer_base()\n  hparams.learning_rate_warmup_steps = 16000\n  return hparams\n\n\n# Different batch sizes.\n@registry.register_hparams\ndef chatbot_transformer_batch_32k():\n  hparams = chatbot_cornell_base()\n  hparams.batch_size = 32768\n  return hparams\n\n\n@registry.register_hparams\ndef chatbot_transformer_batch_16k():\n  hparams = chatbot_cornell_base()\n  hparams.batch_size = 16384\n  return hparams\n\n\n@registry.register_hparams\ndef chatbot_transformer_batch_8k():\n  hparams = chatbot_cornell_base()\n  hparams.batch_size = 8192\n  return hparams\n\n\n@registry.register_hparams\ndef chatbot_transformer_batch_4k():\n  hparams = chatbot_cornell_base()\n  hparams.batch_size = 4096\n  return hparams\n\n\n@registry.register_hparams\ndef chatbot_transformer_batch_2k():\n  hparams = chatbot_cornell_base()\n  hparams.batch_size = 2048\n  return hparams\n\n\n# Different dropout values.\n@registry.register_hparams\ndef base_trf_20_10_drop():\n  hparams = chatbot_transformer_batch_2k()\n  hparams.layer_prepostprocess_dropout = 0.2\n  hparams.attention_dropout = 0.1\n  hparams.relu_dropout = 0.1\n  return hparams\n\n\n@registry.register_hparams\ndef base_trf_40_20_drop():\n  hparams = chatbot_transformer_batch_2k()\n  hparams.layer_prepostprocess_dropout = 0.4\n  hparams.attention_dropout = 0.2\n  hparams.relu_dropout = 0.2\n  return hparams\n\n\n@registry.register_hparams\ndef base_trf_50_30_drop():\n  hparams = chatbot_transformer_batch_2k()\n  hparams.layer_prepostprocess_dropout = 0.5\n  hparams.attention_dropout = 0.3\n  hparams.relu_dropout = 0.3\n  return hparams\n\n\n@registry.register_hparams\ndef base_trf_70_50_drop():\n  hparams = chatbot_transformer_batch_2k()\n  hparams.layer_prepostprocess_dropout = 0.7\n  hparams.attention_dropout = 0.5\n  hparams.relu_dropout = 0.5\n  return hparams\n"""
t2t_csaky/models/gradient_checkpointed_seq2seq.py,23,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport math\n\nfrom tensor2tensor.layers import common_layers\nfrom tensor2tensor.utils import t2t_model\nfrom tensor2tensor.utils import optimize\n\nfrom t2t_csaky.hparams import seq2seq_hparams\nfrom t2t_csaky.utils import optimizer\n\n\ndef lstm(inputs, hparams, train, name, initial_state=None):\n  '''Run LSTM cell on inputs, assuming they are [batch x time x size].'''\n\n  def dropout_lstm_cell():\n    return tf.contrib.rnn.DropoutWrapper(\n        tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(hparams.hidden_size),\n        input_keep_prob=1.0 - hparams.dropout * tf.to_float(train))\n\n  layers = [dropout_lstm_cell() for _ in range(hparams.num_hidden_layers)]\n  with tf.variable_scope(name):\n    return tf.nn.dynamic_rnn(\n        tf.contrib.rnn.MultiRNNCell(layers),\n        inputs,\n        initial_state=initial_state,\n        dtype=tf.float32,\n        time_major=False,\n        swap_memory=True,\n        parallel_iterations=1)\n\n\ndef lstm_seq2seq_internal_dynamic(inputs, targets, hparams, train):\n  '''The basic LSTM seq2seq model, main step used for training.'''\n  with tf.variable_scope('lstm_seq2seq'):\n    if inputs is not None:\n      # Flatten inputs.\n      inputs = common_layers.flatten4d3d(inputs)\n      # LSTM encoder.\n      _, final_encoder_state = lstm(\n          tf.reverse(inputs, axis=[1]), hparams, train, 'encoder')\n\n    else:\n      final_encoder_state = None\n    # LSTM decoder.\n    shifted_targets = common_layers.shift_right(targets)\n    decoder_outputs, _ = lstm(\n        common_layers.flatten4d3d(shifted_targets),\n        hparams,\n        train,\n        'decoder',\n        initial_state=final_encoder_state)\n\n    # Project the outputs.\n    with tf.variable_scope('projection'):\n      projected_outputs = tf.layers.dense(decoder_outputs,\n                                          2048,\n                                          activation=None,\n                                          use_bias=False)\n    return tf.expand_dims(projected_outputs, axis=2), final_encoder_state[0]\n\n\ndef lstm_seq2seq_internal_static(inputs, targets, hparams, train):\n  '''The basic LSTM seq2seq model, main step used for training.'''\n  with tf.variable_scope('lstm_seq2seq'):\n    if inputs is not None:\n      # Flatten inputs.\n      inputs = tf.reverse(common_layers.flatten4d3d(inputs), axis=[1])\n\n      # Construct static rnn input list.\n      # TODO: the length should be a parameter.\n      input_list = [inputs[:, i, :] for i in range(21)]\n\n      # LSTM encoder.\n      _, final_encoder_state = lstm(input_list, hparams, train, 'encoder')\n    else:\n      final_encoder_state = None\n    input_list.clear()\n    # LSTM decoder.\n    # Get a list of tensors.\n    shifted_trg = common_layers.flatten4d3d(common_layers.shift_right(targets))\n    target_list = [shifted_trg[:, i, :] for i in range(21)]\n\n    decoder_outputs, _ = lstm(target_list,\n                              hparams,\n                              train,\n                              'decoder',\n                              initial_state=final_encoder_state)\n    target_list.clear()\n\n    # Convert decoder outputs to tensor.\n    tensors = tf.transpose(tf.convert_to_tensor(decoder_outputs),\n                           perm=[1, 0, 2])\n    decoder_outputs.clear()\n\n    # project the outputs\n    with tf.variable_scope('projection'):\n      projected_outputs = tf.layers.dense(tensors,\n                                          2048,\n                                          activation=None,\n                                          use_bias=False)\n    return tf.expand_dims(projected_outputs, axis=2)\n\n\n# TODO: rename this (causes compatibility issues).\nclass GradientCheckpointedSeq2seq(t2t_model.T2TModel):\n  '''\n  A class where I replaced the internal hparams with my own function call.\n  This way the hidden_size param of chatbot_lstm_hparams refers to the size\n    of the lstm cells, while the hidden_size specified by the hparam set\n    given during training refers to the word embedding size.\n\n  In this class the output of the LSTM layer is projected to 2048 linear units:\n  https://arxiv.org/pdf/1506.05869.pdf\n\n  Moreover, gradient checkpointing was implemented, but didn't work.\n  https://github.com/openai/gradient-checkpointing\n  '''\n  def body(self, features):\n    if self._hparams.initializer == 'orthogonal':\n      raise ValueError('LSTM models fail with orthogonal initializer.')\n    train = self._hparams.mode == tf.estimator.ModeKeys.TRAIN\n    return lstm_seq2seq_internal_dynamic(\n        features.get('inputs'),\n        features['targets'],\n        seq2seq_hparams.chatbot_lstm_hparams(),\n        train)[0]\n\n  # Change the optimizer to a new one, which uses gradient checkpointing.\n  def optimize(self, loss, num_async_replicas=1):\n    '''Return a training op minimizing loss.'''\n    tf.logging.info('Base learning rate: %f', self.hparams.learning_rate)\n    lr = self.hparams.learning_rate\n    decay_rate = optimize.learning_rate_schedule(self.hparams)\n    lr *= decay_rate\n    if self.hparams.learning_rate_minimum:\n      lr_min = float(self.hparams.learning_rate_minimum)\n      tf.logging.info('Applying learning rate minimum: %f', lr_min)\n      lr = tf.max(lr, tf.to_float(lr_min))\n    if num_async_replicas > 1:\n      tf.logging.info('Dividing learning rate by num_async_replicas: %d',\n                      num_async_replicas)\n    lr /= math.sqrt(float(num_async_replicas))\n    train_op = optimizer.optimize(loss, lr, self.hparams)\n    return train_op\n"""
t2t_csaky/problems/character_chatbot.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\n\nfrom tensor2tensor.data_generators import text_encoder\nfrom tensor2tensor.data_generators import translate\nfrom tensor2tensor.utils import registry\n\nfrom t2t_csaky.problems import word_chatbot\n\n# End-of-sentence marker\nEOS = text_encoder.EOS_ID\n\n\n@registry.register_problem\nclass CharacterChatbot(word_chatbot.WordChatbot):\n  '''\n  A base class for character based chatbot problems.\n  '''\n\n  @property\n  def is_character_level(self):\n    return True\n\n  @property\n  def targeted_vocab_size(self):\n    return 0\n\n  @property\n  def targeted_dataset_size(self):\n    # Character chatbot currently only supports to run on the whole dataset.\n    return 0\n\n  def generator(self, data_dir, tmp_dir, train):\n    '''\n    Generate the vocab and then build train and validation t2t-datagen files.\n    Four .txt files have to be present in the data_dir directory:\n      trainSource.txt\n      trainTarget.txt\n      devSource.txt\n      devTarget.txt\n\n    Params:\n      :train: Whether we are in train mode or not.\n    '''\n    character_vocab = text_encoder.ByteTextEncoder()\n    mode = 'train' if train else 'dev'\n    print('t2t_csaky_log: ' + mode + ' data generation activated.')\n\n    sourcePath = os.path.join(data_dir, mode + 'Source.txt')\n    targetPath = os.path.join(data_dir, mode + 'Target.txt')\n\n    # Try to find the txt files.\n    if os.path.isfile(sourcePath) and os.path.isfile(targetPath):\n      print('t2t_csaky_log: Generating ' + mode + ' files in ' + data_dir)\n      return translate.character_generator(sourcePath,\n                                           targetPath,\n                                           character_vocab,\n                                           EOS)\n    else:\n      print('t2t_csaky_log: ' + mode +\n            ' source or target file not found, please check ' +\n            'that the following files exist in your ' + data_dir +\n            ' directory and rerun this program:')\n      print('  trainSource.txt')\n      print('  trainTarget.txt')\n      print('  devSource.txt')\n      print('  devTarget.txt')\n"""
t2t_csaky/problems/cornell_chatbots.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\nimport re\nfrom collections import Counter\n\nfrom tensor2tensor.data_generators import text_encoder\nfrom tensor2tensor.utils import registry\n\nfrom t2t_csaky.problems import opensubtitles_chatbot\nfrom t2t_csaky.config import PROBLEM_HPARAMS\n\n\n# End-of-sentence marker.\nEOS = text_encoder.EOS_ID\n\n\n@registry.register_problem\nclass CornellChatbotBasic(opensubtitles_chatbot.OpensubtitlesChatbot):\n  \'\'\'\n  A class implementing the chatbot problem with Cornell Movie Dialog dataset.\n  \'\'\'\n\n  # Main function where the preprocessing of the data starts.\n  def preprocess_data(self, train_mode):\n    \'\'\'\n    Params:\n      :train_mode: Whether we are in train or dev mode.\n    \'\'\'\n\n    # Set the raw data directory and data.\n    self.raw_data_dir = os.path.join(\'/\'.join(self._data_dir.split(\'/\')[:-1]),\n                                     \'raw_data\')\n    self.raw_data = os.path.join(self._raw_data_dir,\n                                 \'cornell movie-dialogs corpus\')\n    self.zipped_data = os.path.join(self._raw_data_dir,\n                                    \'cornell_movie_dialogs_corpus.zip\')\n\n    # Create the download url.\n    self.url = (\'http://www.cs.cornell.edu/~cristian/data/\' +\n                \'cornell_movie_dialogs_corpus.zip\')\n\n    # Check at which part of the pipeline are we at.\n    self.data_pipeline_status(train_mode)\n\n  # Create the source, target and vocab files.\n  def create_data(self, train_mode):\n    \'\'\'\n    Params:\n      :train_mode: Whether we are in train or dev mode.\n    \'\'\'\n\n    # Open the 6 files.\n    trainSource, trainTarget, devSource, devTarget, testSource, testTarget = \\\n        self.open_6_files()\n\n    # Open the raw data.\n    movie_lines = open(\n        os.path.join(self._raw_data, \'movie_lines.txt\'), errors=\'ignore\')\n    dialog_list = self.extract_dialog_ids()\n\n    vocabulary = Counter()\n    line_dict = {}\n    number_of_lines = 0\n    # Iterate through file.\n    for line in movie_lines:\n      if number_of_lines % 10000 == 0:\n        print(\'t2t_csaky_log: Parsed \' + str(number_of_lines) + \' lines.\')\n\n      line = line.split(\' +++$+++ \')\n      dialog_id = line[0]\n      line = line[4].lower()\n\n      # Do some cleaning.\n      line = self.clean_line(line)\n      line_dict[dialog_id] = line\n\n      number_of_lines += 1\n      # Check if we reached the desired dataset size.\n      if (self.targeted_dataset_size != 0 and\n              self.targeted_dataset_size < number_of_lines):\n        break\n\n    counter = 0\n    dataset_split_counter = 0\n    # Save the actual dialogs.\n    for dialog in dialog_list:\n      if counter % 10000 == 0:\n        print(\'t2t_csaky_log: Saved \' +\n              str(counter) + \'/\' + str(len(dialog_list)) + \' dialogs.\')\n\n      dataset_split_counter += 1\n      i = 0\n      # Save one utterance.\n      for utterance in dialog:\n        if (utterance != dialog[-1] and\n            dialog[i + 1] != \'L211194\' and\n                dialog[i + 1] != \'L1045\'):\n          source_line = line_dict[utterance] + \'\\n\'\n          target_line = line_dict[dialog[i + 1]] + \'\\n\'\n\n          # Save to the files according to dataset split.\n          if dataset_split_counter <= self.dataset_split[\'train\']:\n            # Build vocabulary.\n            words = source_line.split()\n            for word in words:\n              if word in vocabulary:\n                vocabulary[word] += 1\n              else:\n                vocabulary[word] = 1\n\n            trainSource.write(source_line)\n            trainTarget.write(target_line)\n\n          elif dataset_split_counter <= (self.dataset_split[\'train\'] +\n                                         self.dataset_split[\'val\']):\n            devSource.write(source_line)\n            devTarget.write(target_line)\n          else:\n            testSource.write(source_line)\n            testTarget.write(target_line)\n        i += 1\n\n      # Reset the split counter if we reached 100%.\n      if dataset_split_counter == 100:\n        dataset_split_counter = 0\n      counter += 1\n\n    # Close the files.\n    self.close_n_files([trainSource,\n                       trainTarget,\n                       devSource,\n                       devTarget,\n                       testSource,\n                       testTarget])\n    movie_lines.close()\n\n    # Save the vocabulary.\n    self.save_vocab(vocabulary)\n\n  # Clean a line with some re rules.\n  def clean_line(self, line):\n    \'\'\'\n    Params:\n      :line: Line to be processed and returned.\n    \'\'\'\n\n    # 2 functions for more complex replacing.\n    def replace(matchobj):\n      return re.sub(""\'"", "" \'"", str(matchobj.group(0)))\n\n    def replace_null(matchobj):\n      return re.sub(""\'"", \'\', str(matchobj.group(0)))\n\n    # Keep some special tokens.\n    line = re.sub(""[^a-z .?!\'0-9]"", \'\', line)\n    line = re.sub(\'[.]\', \' . \', line)\n    line = re.sub(\'[?]\', \' ? \', line)\n    line = re.sub(\'[!]\', \' ! \', line)\n\n    # Take care of apostrophes.\n    line = re.sub(""[ ]\'[ ]"", \' \', line)\n    line = re.sub("" \'[a-z]"", replace_null, line)\n    line = re.sub(""n\'t"", "" n\'t"", line)\n    line = re.sub(""[^ n]\'[^ t]"", replace, line)\n\n    return line\n\n  # Extract the dialog ids from the dialog file.\n  def extract_dialog_ids(self):\n    dialogs = open(os.path.join(self._raw_data, \'movie_conversations.txt\'),\n                   errors=\'ignore\')\n\n    dialog_list = []\n    # Each line contains a dialog.\n    for line in dialogs:\n      line = line.split(\' +++$+++ \')\n      line = line[3].split(\',\')\n\n      i = 0\n      for item in line:\n        line[i] = re.sub(\'[^A-Z0-9]\', \'\', item)\n        i += 1\n      dialog_list.append(line)\n\n    dialogs.close()\n    return dialog_list\n\n\n@registry.register_problem\nclass CornellChatbotSeparateNames(CornellChatbotBasic):\n  \'\'\'\n  A class implementing the chatbot problem for the Cornell Movie Dialog dataset\n  with the names of the characters saying a line appended to that line.\n  \'\'\'\n\n  @property\n  def targeted_name_vocab_size(self):\n    return PROBLEM_HPARAMS[\'name_vocab_size\']\n\n  @property\n  def targeted_vocab_size(self):\n    return (PROBLEM_HPARAMS[\'vocabulary_size\'] +\n            PROBLEM_HPARAMS[\'name_vocab_size\'])\n\n  # Create the source, target and vocab files.\n  def create_data(self, train_mode):\n    \'\'\'\n    Params:\n      :train_mode: Whether we are in train or dev mode.\n    \'\'\'\n\n    # Open the 6 files.\n    trainSource, trainTarget, devSource, devTarget, testSource, testTarget = \\\n        self.open_6_files()\n\n    # Open the raw data.\n    movie_lines = open(\n        os.path.join(self._raw_data, \'movie_lines.txt\'), errors=\'ignore\')\n    dialog_list = self.extract_dialog_ids()\n\n    vocabulary = Counter()\n    name_vocab = Counter()\n    line_dict = {}\n    number_of_lines = 0\n    # Iterate through file.\n    for line in movie_lines:\n      if number_of_lines % 10000 == 0:\n        print(\'t2t_csaky_log: Parsed \' + str(number_of_lines) + \' lines.\')\n\n      line = line.split(\' +++$+++ \')\n\n      # Separate characters with same names but appearing in different movies.\n      name = re.sub(\' \', \'_\', line[3]) + \'_\' + line[2]\n      dialog_id = line[0]\n      line = line[4].lower()\n\n      # Build vocabulary for names:\n      # Currently we build it based on the whole dataset, because we can assume\n      # that the list of most frequent names is the same in the whole dataset,\n      # and in a random sample of it, however it would be more accurate to\n      # build the name vocab based solely on the training examples.\n      if name in name_vocab:\n        name_vocab[name] += 1\n      elif name != \'\':\n        name_vocab[name] = 1\n\n      # Do some cleaning.\n      line = self.clean_line(line)\n      line_dict[dialog_id] = name + \' \' + line\n\n      number_of_lines += 1\n      # Check if we reached the desired dataset size.\n      if (self.targeted_dataset_size != 0 and\n              self.targeted_dataset_size < number_of_lines):\n        break\n\n    # Replace infrequent names with unknown.\n    line_dict = self.replace_names(line_dict, name_vocab)\n\n    # Save the actual dialogs.\n    counter = 0\n    dataset_split_counter = 0\n    for dialog in dialog_list:\n      if counter % 10000 == 0:\n        print(\'t2t_csaky_log: Saved \' +\n              str(counter) + \'/\' + str(len(dialog_list)) + \' dialogs.\')\n\n      dataset_split_counter += 1\n      i = 0\n      # Save one utterance.\n      for utterance in dialog:\n        if (utterance != dialog[-1] and\n            dialog[i + 1] != \'L211194\' and\n                dialog[i + 1] != \'L1045\'):\n          # Prepare the name annotated data.\n          target_words = line_dict[dialog[i + 1]].split()\n          target_name = target_words[0]\n          target_line = \' \'.join(target_words[1:]) + \'\\n\'\n          source_line = line_dict[utterance] + \' \' + target_name + \'\\n\'\n\n          # Save to the files according to dataset split.\n          if dataset_split_counter <= self.dataset_split[\'train\']:\n            # Build vocabulary.\n            words = source_line.split()[1:-1]\n            for word in words:\n              if word in vocabulary:\n                vocabulary[word] += 1\n              else:\n                vocabulary[word] = 1\n\n            trainSource.write(source_line)\n            trainTarget.write(target_line)\n\n          elif dataset_split_counter <= (self.dataset_split[\'train\'] +\n                                         self.dataset_split[\'val\']):\n            devSource.write(source_line)\n            devTarget.write(target_line)\n          else:\n            testSource.write(source_line)\n            testTarget.write(target_line)\n        i += 1\n\n      # Reset the split counter if we reached 100%.\n      if dataset_split_counter == 100:\n        dataset_split_counter = 0\n      counter += 1\n\n    # Close the files.\n    self.close_n_files([trainSource,\n                       trainTarget,\n                       devSource,\n                       devTarget,\n                       testSource,\n                       testTarget])\n    movie_lines.close()\n\n    # Save the vocabulary.\n    self.save_vocab(vocabulary, name_vocab)\n\n  # Replace infrequent names with unknown.\n  def replace_names(self, line_dict, name_vocab):\n    \'\'\'\n    Params:\n      :line_dict:   Dictionary containing all the parsed lines.\n      :name_vocab:  The vocabulary of names.\n    \'\'\'\n\n    name_list = []\n    for name, _ in name_vocab.most_common(self.targeted_name_vocab_size - 1):\n      name_list.append(name)\n\n    for dialog_id in line_dict:\n      line = line_dict[dialog_id].split()\n\n      if line[0] not in name_list:\n        string = \' \' + line[0] + \' \'\n        line_dict[dialog_id] = re.sub(string,\n                                      \' <unk_name> \',\n                                      \' \' + line_dict[dialog_id] + \' \')\n    return line_dict\n\n  # Save the vocabulary to a file.\n  def save_vocab(self, vocab, name_vocab):\n    \'\'\'\n    Params:\n      :vocab:       Vocabulary list.\n      :name_vocab:  Name vocabulary.\n    \'\'\'\n    voc_file = open(os.path.join(self._data_dir, self.vocab_file), \'w\')\n\n    # put the reserved tokens in\n    voc_file.write(\'<pad>\\n\')\n    voc_file.write(\'<EOS>\\n\')\n\n    # basic words\n    for word, _ in vocab.most_common(self.targeted_vocab_size - 3):\n      voc_file.write(word + \'\\n\')\n    voc_file.write(\'<unk>\' + \'\\n\')\n\n    # name vocab\n    for name, _ in name_vocab.most_common(self.targeted_name_vocab_size - 1):\n      voc_file.write(name + \'\\n\')\n    voc_file.write(\'<unk_name>\')\n\n    voc_file.close()\n'"
t2t_csaky/problems/daily_dialog_chatbot.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\nfrom collections import Counter\n\nfrom tensor2tensor.data_generators import text_encoder\nfrom tensor2tensor.utils import registry\n\nfrom t2t_csaky.problems import cornell_chatbots\n\n\n# End-of-sentence marker.\nEOS = text_encoder.EOS_ID\n\n\n@registry.register_problem\nclass DailyDialogChatbot(cornell_chatbots.CornellChatbotBasic):\n  '''\n  A class implementing a simple chatbot problem for the DailyDialog dataset.\n  This version doesn't use any auxiliary information.\n  '''\n\n  # Main function where the preprocessing of the data starts.\n  def preprocess_data(self, train_mode):\n    '''\n    Params:\n      :train_mode: Whether we are in train or dev mode.\n    '''\n\n    # Set the raw data directory and data.\n    self.raw_data_dir = os.path.join('/'.join(self._data_dir.split('/')[:-1]),\n                                     'raw_data')\n    self.raw_data = os.path.join(self._raw_data_dir, 'ijcnlp_dailydialog')\n    self.zipped_data = os.path.join(self._raw_data_dir,\n                                    'ijcnlp_dailydialog.zip')\n\n    # Create the download url.\n    self.url = 'http://yanran.li/files/ijcnlp_dailydialog.zip'\n\n    # Check at which part of the pipeline are we at.\n    self.data_pipeline_status(train_mode)\n\n  # Create the source, target and vocab files.\n  def create_data(self, train_mode):\n    '''\n    Params:\n      :train_mode: Whether we are in train or dev mode.\n    '''\n\n    # Open the 6 files.\n    trainSource, trainTarget, devSource, devTarget, testSource, testTarget = \\\n        self.open_6_files()\n\n    # Open the raw data.\n    dialogs = open(\n        os.path.join(self._raw_data, 'dialogues_text.txt'), errors='ignore')\n\n    vocabulary = Counter()\n    number_of_dialogs = 0\n    line_counter = 0\n    dataset_split_counter = 0\n    # Iterate through the file.\n    for dialog in dialogs:\n      dataset_split_counter += 1\n      if number_of_dialogs % 1000 == 0:\n        print('t2t_csaky_log: Parsed ' + str(number_of_dialogs) + ' dialogs.')\n\n      # Utterances are separated by the __eou__ token.\n      utterances = dialog.split('__eou__')[:-1]\n\n      # Check which file we should write to.\n      if dataset_split_counter <= self.dataset_split['train']:\n        source_file = trainSource\n        target_file = trainTarget\n      elif dataset_split_counter <= (self.dataset_split['train'] +\n                                     self.dataset_split['val']):\n        source_file = devSource\n        target_file = devTarget\n      else:\n        source_file = testSource\n        target_file = testTarget\n\n      # Clean the utterances.\n      i = 0\n      for utterance in utterances:\n        line_counter += 1\n        utterance = self.clean_line(utterance.lower())\n        i += 1\n\n        # Build vocabulary.\n        if dataset_split_counter <= self.dataset_split['train']:\n          words = utterance.split()\n          for word in words:\n            if word in vocabulary:\n              vocabulary[word] += 1\n            else:\n              vocabulary[word] = 1\n\n        # Write to files.\n        if i != len(utterances):\n          source_file.write(utterance + '\\n')\n        if i != 1:\n          target_file.write(utterance + '\\n')\n\n      number_of_dialogs += 1\n      # Reset the split counter if we reached 100%.\n      if dataset_split_counter == 100:\n        dataset_split_counter = 0\n\n      # Check if we reached the desired dataset size.\n      if (self.targeted_dataset_size != 0 and\n              self.targeted_dataset_size < line_counter):\n        break\n\n    # Close the files.\n    self.close_n_files([trainSource,\n                       trainTarget,\n                       devSource,\n                       devTarget,\n                       testSource,\n                       testTarget])\n    dialogs.close()\n\n    # Save the vocabulary.\n    self.save_vocab(vocabulary)\n"""
t2t_csaky/problems/opensubtitles_chatbot.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\nimport requests\nimport tarfile\nimport re\nimport zipfile\nimport unicodedata\nimport nltk\nfrom collections import Counter\nfrom clint.textui import progress\n\nfrom tensor2tensor.data_generators import text_encoder\nfrom tensor2tensor.utils import registry\n\nfrom t2t_csaky.problems import word_chatbot\nfrom t2t_csaky.config import PROBLEM_HPARAMS\n\n\n# End-of-sentence marker.\nEOS = text_encoder.EOS_ID\n\n\n@registry.register_problem\nclass OpensubtitlesChatbot(word_chatbot.WordChatbot):\n  \'\'\'\n  A class implementing the chatbot problem for the OpenSubtitles dataset.\n  \'\'\'\n\n  @property\n  def dataset_version(self):\n    # Year of the opensubtitles dataset creation.\n    return PROBLEM_HPARAMS[\'dataset_version\']\n\n  # Main function where the preprocessing of the data starts.\n  def preprocess_data(self, train_mode):\n    \'\'\'\n    Params:\n      :train_mode: Whether we are in train or dev mode.\n    \'\'\'\n\n    year = \'\' if self.dataset_version == 2009 else str(self.dataset_version)\n    # Set the raw data directory and data.\n    self.raw_data_dir = os.path.join(\'/\'.join(self._data_dir.split(\'/\')[:-1]),\n                                     \'raw_data_\' + str(self.dataset_version))\n    self.raw_data = os.path.join(self._raw_data_dir, \'OpenSubtitles\' + year)\n    self.zipped_data = os.path.join(self._raw_data_dir, \'en.tar.gz\')\n\n    # Create the download url.\n    self.url = (\'http://opus.nlpl.eu/download.php?f=OpenSubtitles\' +\n                str(year) + \'/en.tar.gz\')\n\n    # Check at which part of the pipeline are we at.\n    self.data_pipeline_status(train_mode)\n\n  # Check at which part of the pipeline are we at.\n  def data_pipeline_status(self, train_mode):\n    \'\'\'\n    This function first check recursively at which point in the\n    data processing point are we (what files can be found on the disk),\n    and then proceeds from there.\n\n    Params:\n      :train_mode: Whether we are in train or dev mode.\n    \'\'\'\n\n    # Build the source and target paths.\n    sourcePath = os.path.join(self._data_dir, train_mode + \'Source.txt\')\n    targetPath = os.path.join(self._data_dir, train_mode + \'Target.txt\')\n\n    # If raw data dir doesn\'t exist, create it.\n    if not os.path.exists(self._raw_data_dir):\n      os.makedirs(self._raw_data_dir)\n\n    # Check whether sourcePath.txt exists.\n    if (os.path.isfile(sourcePath) and os.path.isfile(targetPath) and\n            os.path.isfile(os.path.join(self._data_dir, self.vocab_file))):\n      print(\'t2t_csaky_log: Source, target and vocab files exist in \' +\n            self._data_dir + \', proceeding with data generation. \' +\n            \'If you want to rebuild these files, delete them first.\')\n      return\n\n    # Check whether the raw data is extracted to the raw_data_dir folder.\n    elif os.path.exists(self._raw_data):\n      print(\'t2t_csaky_log: No source, target or vocab files found in \' +\n            self._data_dir + \'.\')\n      print(\'t2t_csaky_log: Extracted raw data is in \' + self._raw_data_dir +\n            \'. Proceeding with creating source, target and vocab files.\')\n      self.create_data(train_mode)\n\n    # Check whether the data is downloaded in the raw_data_dir_folder.\n    elif os.path.exists(self._zipped_data):\n      print(\'t2t_csaky_log: No source, target or vocab files found in \' +\n            self._data_dir + \'.\')\n      print(\'t2t_csaky_log: No extracted raw data found in \' +\n            self._raw_data_dir + \'.\')\n      print(\'t2t_csaky_log: Unextracted raw data is in \' + self._raw_data_dir +\n            \'. Extracting and creating source, target and vocab files.\')\n      self.extract_data(train_mode)\n\n    else:\n      print(\'t2t_csaky_log: No source, target or vocab files found in \' +\n            self._data_dir + \'.\')\n      print(\'t2t_csaky_log: No raw data found in \' + self._raw_data_dir +\n            \'. Proceeding with downloading the data, extracting it, \' +\n            \'and creating source, target and vocab files.\')\n      self.download_data(train_mode)\n\n  # Download data from official sources.\n  def download_data(self, train_mode):\n    \'\'\'\n    Params:\n      :train_mode:  Whether we are in train or dev mode.\n    \'\'\'\n\n    # Open the url and download the data with progress bars.\n    data_stream = requests.get(self._url, stream=True)\n    with open(self._zipped_data, \'wb\') as file:\n      total_length = int(data_stream.headers.get(\'content-length\'))\n      for chunk in progress.bar(data_stream.iter_content(chunk_size=1024),\n                                expected_size=total_length / 1024 + 1):\n        if chunk:\n          file.write(chunk)\n          file.flush()\n\n    # Next step is extracting the data.\n    print(\'t2t_csaky_log: Extracting data to \' + self._zipped_data + \'.\')\n    self.extract_data(train_mode)\n\n  # Extract data and go to the next step.\n  def extract_data(self, train_mode):\n    \'\'\'\n    Params:\n      :train_mode:  Whether we are in train or dev mode.\n    \'\'\'\n\n    if self._zipped_data[-2:] == \'gz\':\n      zip_file = tarfile.open(self._zipped_data, \'r:gz\')\n    elif self._zipped_data[-3:] == \'zip\':\n      zip_file = zipfile.ZipFile(self._zipped_data, \'r\')\n    else:\n      print(\'t2t_csaky_log: \' + self._zipped_data +\n            \' is not a .zip or .gz file, so I can\\\'t extract it.\')\n\n    zip_file.extractall(self._raw_data_dir)\n    zip_file.close()\n\n    # Next step is creating the source, target and vocab files.\n    print(\'t2t_csaky_log: Creating \' +\n          train_mode + \' files in \' + self._data_dir)\n    self.create_data(train_mode)\n\n  # Create the source, target and vocab files.\n  def create_data(self, train_mode):\n    \'\'\'\n    Params:\n      :train_mode: Whether we are in train or dev mode.\n    \'\'\'\n\n    # open the 6 files\n    trainSource, trainTarget, devSource, devTarget, testSource, testTarget = \\\n        self.open_6_files()\n\n    conv_id = 0\n    number_of_lines = 0\n    dataset_split_counter = 0\n    vocabulary = Counter()\n    # Dind all the files.\n    for root, subfolders, files in os.walk(self._raw_data_dir):\n      for file in files:\n        if conv_id % 100 == 0:\n          print(\'t2t_csaky_log: Parsed \' + str(conv_id) + \' files.\')\n\n        source_lines = \'\'\n        target_lines = \'\'\n        conv_id += 1\n        dataset_split_counter += 1\n\n        # Open one .gz file and parse it.\n        with open(os.path.join(root, file), \'r\', errors=\'ignore\') as txt_file:\n          words = \'\'\n          line_id = 1\n\n          # Parse one line.\n          for line in txt_file:\n            line = str(line)\n\n            # Check if it\'s a new sentence.\n            if line.find(\'<s id=""\') != -1:\n              # Do some cleaning.\n              words = self.clean_line(words)\n              if len(words) > 0:\n                # Build the vocabulary.\n                if dataset_split_counter <= self.dataset_split[\'train\']:\n                  word_list = words.split()\n                  for word in word_list:\n                    if word in vocabulary:\n                      vocabulary[word] += 1\n                    else:\n                      vocabulary[word] = 1\n\n                # Add the previous line.\n                source_lines += words + \'\\n\'\n                if line_id != 1:\n                  target_lines += words + \'\\n\'\n                line_id += 1\n              words = \'\'\n\n            else:\n              index = line.find(\'<w id=""\')\n              if index >= 0:\n                line = line[index:]\n                word = line[line.find(\'>\') + 1:line.find(\'</w\')]\n                words = words + \' \' + word.replace(\'\\t\', \' \')\n\n          # Delete the final source sentence, since it doesn\'t have a target.\n          source_lines = \'\\n\'.join(source_lines.split(\'\\n\')[:-2]) + \'\\n\'\n\n        # Save the dialog according to the dataset split.\n        if dataset_split_counter <= self.dataset_split[\'train\']:\n          trainSource.write(source_lines)\n          trainTarget.write(target_lines)\n        elif dataset_split_counter <= (self.dataset_split[\'train\'] +\n                                       self.dataset_split[\'val\']):\n          devSource.write(source_lines)\n          devTarget.write(target_lines)\n        else:\n          testSource.write(source_lines)\n          testTarget.write(target_lines)\n\n        # Reset the split counter if we reached 100%.\n        if dataset_split_counter == 100:\n          dataset_split_counter = 0\n\n        # Check if we reached the desired dataset size.\n        number_of_lines += line_id\n        if (self.targeted_dataset_size != 0 and\n                self.targeted_dataset_size < number_of_lines):\n          break\n      else:\n        continue\n      break\n\n    # Close the files.\n    self.close_n_files([trainSource,\n                        trainTarget,\n                        devSource,\n                        devTarget,\n                        testSource,\n                        testTarget])\n    # Save the vocabulary.\n    self.save_vocab(vocabulary)\n\n  # Clean a line with some re rules.\n  def clean_line(self, line):\n    \'\'\'\n    Params:\n      :line: Line to be processed and returned.\n    \'\'\'\n    line = line.lower()\n    line = re.sub(\' \\\' \', \'\\\'\', line)\n    line = unicodedata.normalize(\'NFKD\', line)\n\n    # Keep some special tokens.\n    line = re.sub(\'[^a-z .?!\\\'0-9]\', \'\', line)\n    line = re.sub(\'n \\\'t\', \'n\\\'t\', line)\n    line = re.sub(\'[.]\', \' . \', line)\n    line = re.sub(\'[?]\', \' ? \', line)\n    line = re.sub(\'[!]\', \' ! \', line)\n\n    words = nltk.word_tokenize(line)\n    line = \' \'.join(words)\n    return line + \' \'\n'"
t2t_csaky/problems/persona_chat_chatbot.py,1,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport tensorflow as tf\nimport os\nimport tarfile\nimport zipfile\nfrom collections import Counter\n\nfrom tensor2tensor.data_generators import text_encoder\nfrom tensor2tensor.utils import registry\n\nfrom t2t_csaky.problems import cornell_chatbots\n\nFLAGS = tf.flags.FLAGS\n\n# End-of-sentence marker.\nEOS = text_encoder.EOS_ID\n\n\n@registry.register_problem\nclass PersonaChatChatbot(cornell_chatbots.CornellChatbotBasic):\n  '''\n  A class implementing a simple chatbot for the Persona-chat dataset.\n  The personas are not used in this class, only the raw dialogs.\n  '''\n\n  # Main function where the preprocessing of the data starts.\n  def preprocess_data(self, train_mode):\n    '''\n    Params:\n      :train_mode: Whether we are in train or dev mode.\n    '''\n\n    # Set the raw data directory and data.\n    self.raw_data_dir = os.path.join('/'.join(self._data_dir.split('/')[:-1]),\n                                     'raw_data')\n    self.raw_data = os.path.join(self._raw_data_dir, 'ConvAI2')\n    self.zipped_data = os.path.join(self._raw_data_dir, 'convai2.tar.gz')\n\n    # Create the download url.\n    self.url = 'http://parl.ai/downloads/convai2/convai2_fix_723.tgz'\n\n    # Check at which part of the pipeline are we at.\n    self.data_pipeline_status(train_mode)\n\n  # Extract data and go to the next step.\n  def extract_data(self, train_mode):\n    '''\n    Params:\n      :train_mode: Whether we are in train or dev mode.\n    '''\n\n    if self._zipped_data[-2:] == 'gz':\n      zip_file = tarfile.open(self._zipped_data, 'r:gz')\n    elif self._zipped_data[-3:] == 'zip':\n      zip_file = zipfile.ZipFile(self._zipped_data, 'r')\n    else:\n      print('t2t_csaky_log: ' + self._zipped_data +\n            ' is not a .zip or .gz file, so I can\\'t extract it.')\n\n    zip_file.extractall(self._raw_data)\n    zip_file.close()\n\n    # Next step is creating the source, target and vocab files.\n    print('t2t_csaky_log: Creating ' +\n          train_mode + ' files in ' + self._data_dir + '.')\n    self.create_data(train_mode)\n\n  # Create the source, target and vocab files.\n  def create_data(self, train_mode):\n    '''\n    Params:\n      :train_mode: Whether we are in train or dev mode.\n    '''\n\n    # Open the 6 files.\n    trainSource, trainTarget, devSource, devTarget, testSource, testTarget = \\\n        self.open_6_files()\n\n    # Open the raw data.\n    train_dialogs = open(\n        os.path.join(self._raw_data, 'train_none_original_no_cands.txt'),\n        errors='ignore')\n    valid_dialogs = open(\n        os.path.join(self._raw_data, 'valid_none_original_no_cands.txt'),\n        errors='ignore')\n    filenames = [train_dialogs, valid_dialogs]\n\n    # Copy the data to a new file.\n    with open(os.path.join(self._raw_data,\n                           'full_none_original_no_cands.txt'), 'w') as outfile:\n      for fname in filenames:\n        with fname as infile:\n          outfile.write(infile.read())\n    train_dialogs.close()\n    valid_dialogs.close()\n\n    # Open the big file.\n    dialogs = open(\n        os.path.join(self._raw_data, 'full_none_original_no_cands.txt'),\n        errors='ignore')\n\n    number_of_lines = 0\n    current_dialog = ''\n    dialog_list = []\n    dialog_silenced = False\n    # Iterate through the file and build list of dialogs separated by __eou__.\n    for line in dialogs:\n      if number_of_lines % 10000 == 0:\n        print('t2t_csaky_log: Parsed ' + str(number_of_lines) + ' lines.')\n\n      dialog_id = line.split()[0]\n      # Check if this is a refurbished line.\n      if ('__SILENCE__' not in line and\n              ((dialog_silenced and dialog_id == '1') or not dialog_silenced)):\n        dialog_silenced = False\n        number_of_lines += 1\n\n        # Get the utterances.\n        source = ' '.join(line.split('\\t')[0].split()[1:])\n        target = line.split('\\t')[1].strip('\\n')\n        source = self.clean_line(source.lower())\n        target = self.clean_line(target.lower())\n\n        # Whether this is a new dialog.\n        if dialog_id == '1' and current_dialog != '':\n          dialog_list.append(current_dialog)\n          current_dialog = source + '__eou__' + target + '__eou__'\n        else:\n          current_dialog += source + '__eou__' + target + '__eou__'\n      else:\n        dialog_silenced = True\n\n      if (self.targeted_dataset_size != 0 and\n              self.targeted_dataset_size < number_of_lines):\n        break\n    dialogs.close()\n\n    vocabulary = Counter()\n    number_of_dialogs = 0\n    dataset_split_counter = 0\n    # Build the dataset.\n    for dialog in dialog_list:\n      if number_of_dialogs % 1000 == 0:\n        print('t2t_csaky_log: Parsed ' + str(number_of_dialogs) + ' dialogs.')\n\n      # Check which file we should write to.\n      if dataset_split_counter <= self.dataset_split['train']:\n        source_file = trainSource\n        target_file = trainTarget\n      elif dataset_split_counter <= (self.dataset_split['train'] +\n                                     self.dataset_split['val']):\n        source_file = devSource\n        target_file = devTarget\n      else:\n        source_file = testSource\n        target_file = testTarget\n\n      utterances = dialog.split('__eou__')[:-1]\n      i = 0\n      # Loop through the dialog.\n      for utterance in utterances:\n        i += 1\n        # Build vocabulary.\n        if dataset_split_counter <= self.dataset_split['train']:\n          words = utterance.split()\n          for word in words:\n            if word in vocabulary:\n              vocabulary[word] += 1\n            else:\n              vocabulary[word] = 1\n\n        # Write to files.\n        if i != len(utterances):\n          source_file.write(utterance + '\\n')\n        if i != 1:\n          target_file.write(utterance + '\\n')\n\n      dataset_split_counter += 1\n      number_of_dialogs += 1\n      # Reset the split counter if we reached 100%.\n      if dataset_split_counter == 100:\n        dataset_split_counter = 0\n\n    # Close the files.\n    self.close_n_files([trainSource,\n                        trainTarget,\n                        devSource,\n                        devTarget,\n                        testSource,\n                        testTarget])\n    # Save the vocabulary.\n    self.save_vocab(vocabulary)\n"""
t2t_csaky/problems/word_chatbot.py,2,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport tensorflow as tf\nimport os\n\nfrom tensor2tensor.data_generators import problem\nfrom tensor2tensor.data_generators import text_problems\nfrom tensor2tensor.data_generators import text_encoder\nfrom tensor2tensor.data_generators import generator_utils\nfrom tensor2tensor.data_generators.text_problems import VocabType\nfrom tensor2tensor.utils import metrics\nfrom tensor2tensor.layers import modalities\n\nfrom t2t_csaky.config import PROBLEM_HPARAMS\n\n# End-of-sentence marker.\nEOS = text_encoder.EOS_ID\n\n\nclass WordChatbot(text_problems.Text2TextProblem):\n  '''\n  An abstract base class for word based chatbot problems.\n  '''\n\n  @property\n  def vocab_type(self):\n    return text_problems.VocabType.TOKEN\n\n  @property\n  def is_generate_per_split(self):\n    return True\n\n  @property\n  def vocab_file(self):\n    return self.vocab_filename\n\n  @property\n  def vocab_filename(self):\n    return 'vocab.chatbot.' + str(self.targeted_vocab_size)\n\n  @property\n  def oov_token(self):\n    return '<unk>'\n\n  @property\n  def use_subword_tokenizer(self):\n    return False\n\n  @property\n  def input_space_id(self):\n    return problem.SpaceID.EN_TOK\n\n  @property\n  def target_space_id(self):\n    return problem.SpaceID.EN_TOK\n\n  @property\n  def targeted_vocab_size(self):\n    return PROBLEM_HPARAMS['vocabulary_size']\n\n  @property\n  def targeted_dataset_size(self):\n    # Number of utterance pairs in the full dataset.\n    # If it's 0, then the full size of the dataset is used.\n    return PROBLEM_HPARAMS['dataset_size']\n\n  @property\n  def dataset_split(self):\n    return PROBLEM_HPARAMS['dataset_split']\n\n  @property\n  def dataset_splits(self):\n    return [{\n        'split': problem.DatasetSplit.TRAIN,\n        'shards': PROBLEM_HPARAMS['num_train_shards'],\n    }, {\n        'split': problem.DatasetSplit.EVAL,\n        'shards': PROBLEM_HPARAMS['num_dev_shards'],\n    }, {\n        'split': problem.DatasetSplit.TEST,\n        'shards': PROBLEM_HPARAMS['num_dev_shards'],\n    }]\n\n  @property\n  def data_dir(self):\n    return ''\n\n  @property\n  def raw_data_dir(self):\n    return ''\n\n  @property\n  def raw_data(self):\n    return ''\n\n  @property\n  def zipped_data(self):\n    return ''\n\n  @property\n  def url(self):\n    return ''\n\n  ''' Setter methods for the string properties. '''\n  @data_dir.setter\n  def data_dir(self, value):\n    self._data_dir = value\n\n  @raw_data_dir.setter\n  def raw_data_dir(self, value):\n    self._raw_data_dir = value\n\n  @raw_data.setter\n  def raw_data(self, value):\n    self._raw_data = value\n\n  @zipped_data.setter\n  def zipped_data(self, value):\n    self._zipped_data = value\n\n  @url.setter\n  def url(self, value):\n    self._url = value\n\n  # Main function where the preprocessing of the data starts.\n  def preprocess_data(self, train_mode):\n    return NotImplementedError\n\n  # hparams for the problem.\n  def hparams(self, defaults, unused_model_hparams):\n    p = defaults\n    p.stop_at_eos = int(True)\n\n    p.modality = {'targets': modalities.ModalityType.SYMBOL}\n    if self.has_inputs:\n      p.modality['inputs'] = modalities.ModalityType.SYMBOL\n      p.vocab_size = {'inputs': self._encoders['inputs'].vocab_size}\n    p.vocab_size['targets'] = self._encoders['inputs'].vocab_size\n\n    if self.vocab_type == VocabType.CHARACTER:\n      p.loss_multiplier = 2.0\n\n    if self.packed_length:\n      if self.has_inputs:\n        p.modality['inputs_segmentation'] = modalities.ModalityType.IDENTITY\n        p.modality['inputs_position'] = modalities.ModalityType.IDENTITY\n        p.vocab_size['inputs_segmentation'] = None\n        p.vocab_size['inputs_position'] = None\n      p.modality['targets_segmentation'] = modalities.ModalityType.IDENTITY\n      p.modality['targets_position'] = modalities.ModalityType.IDENTITY\n      p.vocab_size['targets_segmentation'] = None\n      p.vocab_size['targets_position'] = None\n\n  # What evaluation metrics to use with this problem.\n  def eval_metrics(self):\n    return [metrics.Metrics.ACC, metrics.Metrics.ACC_TOP5,\n            metrics.Metrics.ACC_PER_SEQ,\n            metrics.Metrics.NEG_LOG_PERPLEXITY,\n            metrics.Metrics.APPROX_BLEU]\n\n  # Override this, to start with preprocessing.\n  def generate_data(self, data_dir, tmp_dir, task_id=-1):\n    self.data_dir = data_dir\n    # Determine whether we are in training or validation mode.\n    self.mode = {problem.DatasetSplit.TRAIN: 'train',\n                 problem.DatasetSplit.EVAL: 'dev',\n                 problem.DatasetSplit.TEST: 'test'}\n    filepath_fns = {problem.DatasetSplit.TRAIN: self.training_filepaths,\n                    problem.DatasetSplit.EVAL: self.dev_filepaths,\n                    problem.DatasetSplit.TEST: self.test_filepaths}\n\n    split_paths = [(split['split'], filepath_fns[split['split']](\n      data_dir, split['shards'], shuffled=self.already_shuffled))\n      for split in self.dataset_splits]\n    all_paths = []\n    for _, paths in split_paths:\n      all_paths.extend(paths)\n\n    if self.is_generate_per_split:\n      for split, paths in split_paths:\n        # Create the source and target txt files from the raw data.\n        self.preprocess_data(self.mode[split])\n        generator_utils.generate_files(\n            self.generate_encoded_samples(data_dir, tmp_dir, split), paths)\n    else:\n      self.preprocess_data(self.mode[problem.DatasetSplit.TRAIN])\n      generator_utils.generate_files(\n          self.generate_encoded_samples(\n              data_dir, tmp_dir, problem.DatasetSplit.TRAIN), all_paths)\n\n    generator_utils.shuffle_dataset(all_paths, extra_fn=self._pack_fn())\n\n  # This function generates train and validation pairs in t2t-datagen style.\n  def generate_samples(self, data_dir, tmp_dir, data_split):\n    '''\n    The function assumes that if you have data at one level of the pipeline,\n    you don't want to re-generate it, so for example if the 4 txt files exist,\n    the function continues by generating the t2t-datagen format files.\n    So if you want to re-download or re-generate data,\n    you have to delete it first from the appropriate directories.\n\n    Params:\n      :data_dir: Directory where the data will be generated\n                 The raw data has to be downloaded one directory level higher.\n      :data_split: Which data split to generate samples for.\n    '''\n    self.data_dir = data_dir\n    print('t2t_csaky_log: ' +\n          self.mode[data_split] + ' data generation activated.')\n\n    sPath = os.path.join(data_dir, self.mode[data_split] + 'Source.txt')\n    tPath = os.path.join(data_dir, self.mode[data_split] + 'Target.txt')\n\n    # Open the files and yield source-target lines.\n    with tf.gfile.GFile(sPath, mode='r') as source_file:\n      with tf.gfile.GFile(tPath, mode='r') as target_file:\n        source, target = source_file.readline(), target_file.readline()\n        while source and target:\n          yield {'inputs': source.strip(), 'targets': target.strip()}\n          source, target = source_file.readline(), target_file.readline()\n\n  # Save the vocabulary to a file.\n  def save_vocab(self, vocab):\n    '''\n    Params:\n      :vocab: Vocabulary list.\n    '''\n    voc_file = open(os.path.join(self._data_dir, self.vocab_file), 'w')\n\n    # Put the reserved tokens in.\n    voc_file.write('<pad>\\n')\n    voc_file.write('<EOS>\\n')\n    for word, _ in vocab.most_common(self.targeted_vocab_size - 3):\n      voc_file.write(word + '\\n')\n    voc_file.write('<unk>')\n\n    voc_file.close()\n\n  # Open the 6 files to write the processed data into.\n  def open_6_files(self):\n    trainSource = open(os.path.join(self._data_dir, 'trainSource.txt'), 'w')\n    trainTarget = open(os.path.join(self._data_dir, 'trainTarget.txt'), 'w')\n    devSource = open(os.path.join(self._data_dir, 'devSource.txt'), 'w')\n    devTarget = open(os.path.join(self._data_dir, 'devTarget.txt'), 'w')\n    testSource = open(os.path.join(self._data_dir, 'testSource.txt'), 'w')\n    testTarget = open(os.path.join(self._data_dir, 'testTarget.txt'), 'w')\n\n    return trainSource, trainTarget, devSource, \\\n        devTarget, testSource, testTarget\n\n  # Close the 6 files to write the processed data into.\n  def close_n_files(self, files):\n    for file in files:\n      file.close()\n"""
t2t_csaky/scripts/frequencies.py,0,"b""import argparse\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument('-n', '--ntokens', type=int,\n                      help='number of tokens to show', default=20)\n  parser.add_argument('-o', '--output', type=str,\n                      help='name of the output file', default='top_tokens.txt')\n  parser.add_argument('-i', '--input', type=str, help='name of the input file')\n\n  args = parser.parse_args()\n  tokens = {}\n  with open(args.input, 'r') as fin:\n    for line in fin:\n      for token in line.strip().split():\n        tokens[token] = tokens.get(token, 0) + 1\n\n  freqs = sorted(\n      tokens.items(), key=lambda x: x[1], reverse=True)[:args.ntokens]\n  with open(args.output, 'w') as fou:\n    for freq in freqs:\n      fou.write(freq[0] + '\\n')\n\n  print(freqs)\n\n\nif __name__ == '__main__':\n  main()\n"""
t2t_csaky/scripts/remove.py,0,"b""import argparse\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument('-t', '--tokens', type=str,\n                      help='file containing the tokens '\n                           'to be removed from the input file')\n  parser.add_argument('-o', '--output', type=str,\n                      help='name of the output file', default=None)\n  parser.add_argument('-i', '--input', type=str, help='name of the input file')\n  args = parser.parse_args()\n\n  out_file = args.output\n  if not out_file:\n    out_file = 'filtered.txt'\n\n  tokens = set()\n  with open(args.tokens, 'r') as ftok:\n    for token in ftok:\n      tokens.add(token.strip())\n\n  with open(args.input, 'r') as fin, open(out_file, 'w') as fou:\n    for line in fin:\n      fou.write(' '.join([word for word in line.strip().split()\n                          if word not in tokens]) + '\\n')\n\n\nif __name__ == '__main__':\n  main()\n"""
t2t_csaky/utils/optimizer.py,23,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensor2tensor.utils import yellowfin\nfrom tensor2tensor.utils import optimize as t2t_opt\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.training import optimizer\n\n\ndef optimize(loss, learning_rate, hparams):\n  '''Minimize loss.'''\n  loss = t2t_opt.weight_decay_and_noise(loss, hparams, learning_rate)\n  loss = tf.identity(loss, name='total_loss')\n  t2t_opt.log_variable_sizes(verbose=hparams.summarize_vars)\n  diet_vars = [\n      v for v in tf.global_variables() if v.dtype == dtypes.float16_ref\n  ]\n  t2t_opt.log_variable_sizes(\n      diet_vars, 'Diet Variables', verbose=hparams.summarize_vars)\n  opt = GradientCheckpointedOptimizer(hparams.optimizer,\n                                      learning_rate,\n                                      hparams)\n\n  tf.summary.scalar('learning_rate', learning_rate)\n  opt_summaries = ['loss', 'global_gradient_norm']\n  if hparams.summarize_grads:\n    tf.logging.info('Summarizing gradients')\n    opt_summaries.extend(['gradients', 'gradient_norm'])\n\n  if hparams.clip_grad_norm:\n    tf.logging.info('Clipping gradients, norm: %0.5f', hparams.clip_grad_norm)\n  if hparams.grad_noise_scale:\n    tf.logging.info('Adding noise to gradients, noise scale: %0.5f',\n                    hparams.grad_noise_scale)\n\n  train_op = tf.contrib.layers.optimize_loss(\n      name='training',\n      loss=loss,\n      global_step=tf.train.get_or_create_global_step(),\n      learning_rate=learning_rate,\n      clip_gradients=hparams.clip_grad_norm or None,\n      gradient_noise_scale=hparams.grad_noise_scale or None,\n      optimizer=opt,\n      summaries=opt_summaries,\n      colocate_gradients_with_ops=True)\n  return train_op\n\n\nclass GradientCheckpointedOptimizer(tf.train.Optimizer):\n  '''Conditional optimizer.'''\n\n  def __init__(self, optimizer_name, lr, hparams, use_tpu=False):\n    if optimizer_name == 'Adam' and use_tpu:\n      # LazyAdamOptimizer does not work on TPU\n      optimizer_name = 'TrueAdam'\n\n    tf.logging.info('Using optimizer %s', optimizer_name)\n\n    if optimizer_name == 'Adam':\n      # We change the default epsilon for Adam and re-scale lr.\n      # Using LazyAdam as it's much faster for large vocabulary embeddings.\n      self._opt = tf.contrib.opt.LazyAdamOptimizer(\n          lr / 500.0,\n          beta1=hparams.optimizer_adam_beta1,\n          beta2=hparams.optimizer_adam_beta2,\n          epsilon=hparams.optimizer_adam_epsilon)\n    elif optimizer_name == 'Momentum':\n      self._opt = tf.train.MomentumOptimizer(\n          lr,\n          momentum=hparams.optimizer_momentum_momentum,\n          use_nesterov=hparams.optimizer_momentum_nesterov)\n    elif optimizer_name == 'YellowFin':\n      self._opt = yellowfin.YellowFinOptimizer(\n          learning_rate=lr, momentum=hparams.optimizer_momentum_momentum)\n    elif optimizer_name == 'TrueAdam':\n      self._opt = tf.train.AdamOptimizer(\n          lr / 500.0,\n          beta1=hparams.optimizer_adam_beta1,\n          beta2=hparams.optimizer_adam_beta2,\n          epsilon=hparams.optimizer_adam_epsilon)\n    elif optimizer_name == 'Adafactor':\n      self._opt = t2t_opt.AdafactorOptimizer(lr / 500.0)\n    elif optimizer_name == 'Adagrad':\n      self._opt = tf.train.AdagradOptimizer(lr / 500.0)\n    else:\n      self._opt = tf.contrib.layers.OPTIMIZER_CLS_NAMES[optimizer_name](lr)\n\n  def compute_gradients(self, loss, var_list=None,\n                        gate_gradients=tf.train.Optimizer.GATE_OP,\n                        aggregation_method=None,\n                        colocate_gradients_with_ops=False,\n                        grad_loss=None):\n    '''Compute gradients of `loss` for the variables in `var_list`.\n    This is the first part of `minimize()`.  It returns a list\n    of (gradient, variable) pairs where 'gradient' is the gradient\n    for 'variable'.  Note that 'gradient' can be a `Tensor`, an\n    `IndexedSlices`, or `None` if there is no gradient for the\n    given variable.\n    Args:\n      loss: A Tensor containing the value to minimize.\n      var_list: Optional list or tuple of `tf.Variable` to update to minimize\n        `loss`.  Defaults to the list of variables collected in the graph\n        under the key `GraphKey.TRAINABLE_VARIABLES`.\n      gate_gradients: How to gate the computation of gradients.  Can be\n        `GATE_NONE`, `GATE_OP`, or `GATE_GRAPH`.\n      aggregation_method: Specifies the method used to combine gradient terms.\n        Valid values are defined in the class `AggregationMethod`.\n      colocate_gradients_with_ops: If True, try colocating gradients with\n        the corresponding op.\n      grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n    Returns:\n      A list of (gradient, variable) pairs. Variable is always present, but\n      gradient can be `None`.\n    Raises:\n      TypeError: If `var_list` contains anything else than `Variable` objects.\n      ValueError: If some arguments are invalid.\n    '''\n    if gate_gradients not in [tf.train.Optimizer.GATE_NONE,\n                              tf.train.Optimizer.GATE_OP,\n                              tf.train.Optimizer.GATE_GRAPH]:\n      raise ValueError('gate_gradients must be one of: Optimizer.GATE_NONE, '\n                       'Optimizer.GATE_OP, Optimizer.GATE_GRAPH.  Not %s' %\n                       gate_gradients)\n    self._assert_valid_dtypes([loss])\n    if grad_loss is not None:\n      self._assert_valid_dtypes([grad_loss])\n    if var_list is None:\n      var_list = (\n          variables.trainable_variables() +\n          ops.get_collection(ops.GraphKeys.TRAINABLE_RESOURCE_VARIABLES))\n    else:\n      var_list = nest.flatten(var_list)\n    # pylint: disable=protected-access\n    var_list += ops.get_collection(ops.GraphKeys._STREAMING_MODEL_PORTS)\n    # pylint: enable=protected-access\n    processors = [optimizer._get_processor(v) for v in var_list]\n    if not var_list:\n      raise ValueError('No variables to optimize.')\n    var_refs = [p.target() for p in processors]\n    # TODO: make the type of gradient checkpointing choosable.\n    grads = tf.gradients(\n        loss, var_refs, grad_ys=grad_loss,\n        gate_gradients=(gate_gradients == tf.train.Optimizer.GATE_OP),\n        aggregation_method=aggregation_method,\n        colocate_gradients_with_ops=colocate_gradients_with_ops)\n    if gate_gradients == tf.train.Optimizer.GATE_GRAPH:\n      grads = control_flow_ops.tuple(grads)\n    grads_and_vars = list(zip(grads, var_list))\n    self._assert_valid_dtypes(\n        [v for g, v in grads_and_vars\n         if g is not None and v.dtype != dtypes.resource])\n    return grads_and_vars\n\n  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n    return self._opt.apply_gradients(\n        grads_and_vars, global_step=global_step, name=name)\n"""
t2t_csaky/utils/run.py,0,"b'import os\nimport datetime\nimport sys\n\nsys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), \'..\'))\n\nfrom config import FLAGS\n\n\n# Save the config.py file for a specific run.\ndef save_config_file(directory):\n  # Make the data dir if it doesn\'t exist.\n  if not os.path.exists(directory):\n    os.makedirs(directory)\n\n  # This will be used in the names of saved files.\n  now = datetime.datetime.now()\n  time_string = (str(now.year) + \'.\' +\n                 str(now.month) + \'.\' +\n                 str(now.day) + \'.\' +\n                 str(now.hour) + \'.\' +\n                 str(now.minute) + \'.\' +\n                 str(now.second))\n\n  os.system(\'cp \' + FLAGS[\'t2t_usr_dir\'] + \'/config.py \' +\n            directory + \'/config.\' + time_string + \'.txt\')\n\n\n# Initialize a data generation problem.\ndef data_generating():\n  print(\'Program is running in data generation mode.\')\n  save_config_file(FLAGS[\'data_dir\'])\n  os.system(\'t2t-datagen \\\n               --t2t_usr_dir=\' + FLAGS[\'t2t_usr_dir\'] +\n            \' --data_dir=\' + FLAGS[\'data_dir\'] +\n            \' --problem=\' + FLAGS[\'problem\'])\n\n\n# initialize a training loop with the given flags.\ndef training():\n  print(\'Program is running in training mode.\')\n  save_config_file(FLAGS[\'train_dir\'])\n\n  # What hparams should we use.\n  if FLAGS[\'hparams\'] == \'\':\n    hparam_string = \'general_\' + FLAGS[\'model\'] + \'_hparams\'\n  else:\n    hparam_string = FLAGS[\'hparams\']\n\n  os.system(\'t2t-trainer \\\n               --generate_data=False \\\n               --t2t_usr_dir=\' + FLAGS[\'t2t_usr_dir\'] +\n            \' --data_dir=\' + FLAGS[\'data_dir\'] +\n            \' --problem=\' + FLAGS[\'problem\'] +\n            \' --output_dir=\' + FLAGS[\'train_dir\'] +\n            \' --model=\' + FLAGS[\'model\'] +\n            \' --hparams_set=\' + hparam_string +\n            \' --schedule=\' + FLAGS[\'train_mode\'] +\n            \' --worker_gpu_memory_fraction=\' + str(FLAGS[\'memory_fraction\']) +\n            \' --keep_checkpoint_max=\' + str(FLAGS[\'keep_checkpoints\']) +\n            \' --keep_checkpoint_every_n_hours=\' +\n            str(FLAGS[\'save_every_n_hour\']) +\n            \' --save_checkpoints_secs=\' + str(FLAGS[\'save_every_n_secs\']) +\n            \' --train_steps=\' + str(FLAGS[\'train_steps\']) +\n            \' --eval_steps=\' + str(FLAGS[\'evaluation_steps\']) +\n            \' --local_eval_frequency=\' + str(FLAGS[\'evaluation_freq\']))\n\n\n# Intialize an inference test with the given flags.\ndef decoding():\n  print(\'Program is running in inference/decoding mode.\')\n  save_config_file(FLAGS[\'decode_dir\'])\n\n  # What hparams should we use.\n  if FLAGS[\'hparams\'] == \'\':\n    hparam_string = \'general_\' + FLAGS[\'model\'] + \'_hparams\'\n  else:\n    hparam_string = FLAGS[\'hparams\']\n\n  decode_mode_string = \'\'\n  # Determine the decode mode flag.\n  if FLAGS[\'decode_mode\'] == \'interactive\':\n    decode_mode_string = \' --decode_interactive\'\n  elif FLAGS[\'decode_mode\'] == \'file\':\n    decode_mode_string = (\' --decode_from_file=\' +\n                          FLAGS[\'decode_dir\'] + \'/\' +\n                          FLAGS[\'input_file_name\'])\n\n  os.system(\'t2t-decoder \\\n               --generate_data=False \\\n               --t2t_usr_dir=\' + FLAGS[\'t2t_usr_dir\'] +\n            \' --data_dir=\' + FLAGS[\'data_dir\'] +\n            \' --problem=\' + FLAGS[\'problem\'] +\n            \' --output_dir=\' + FLAGS[\'train_dir\'] +\n            \' --model=\' + FLAGS[\'model\'] +\n            \' --worker_gpu_memory_fraction=\' + str(FLAGS[\'memory_fraction\']) +\n            \' --hparams_set=\' + hparam_string +\n            \' --decode_to_file=\' +\n            FLAGS[\'decode_dir\'] + \'/\' + FLAGS[\'output_file_name\'] +\n            \' --decode_hparams=\\\'beam_size=\' + str(FLAGS[\'beam_size\']) +\n            \',return_beams=\' + FLAGS[\'return_beams\'] +\n            \',batch_size=\' + str(FLAGS[\'batch_size\']) + \'\\\'\' +\n            decode_mode_string)\n\n\n# Run a longer experiment, with many calls to the above functions.\ndef experiment():\n  ckpt_list = [1, 1328, 2647, 3963, 5284, 6611, 7932, 9254, 10581, 11902, 13227, 14558, 15882, 17209]\n  dir_list = [""base_with_numbers"", ""base_both_identity_clustering"", ""base_source_based_identity_clustering_CORRECT"", ""base_target_based_identity_clustering"",\n              ""base_both_avg_embedding"", ""base_target_based_avg_embedding"", ""base_source_based_avg_embedding"",\n              ""base_both_sent_eval"", ""base_target_based_sent_eval"", ""base_source_based_sent_eval""]\n  for ckpt in ckpt_list:\n    #FLAGS[""data_dir""] = ""data_dir/DailyDialog/"" + folder\n    #FLAGS[""train_dir""] = ""train_dir/DailyDialog/trf_20_dropout-"" + folder\n    #FLAGS[""decode_dir""] = ""decode_dir/DailyDialog/trf_20_dropout-"" + folder\n    with open(FLAGS[""train_dir""] + ""/checkpoint"", ""w"") as ckpt_file:\n      ckpt_file.write(\'model_checkpoint_path: ""model.ckpt-\' + str(ckpt) + \'""\')\n    FLAGS[""output_file_name""] = ""test_set_"" + str(ckpt) + "".txt""\n    decoding()\n'"
