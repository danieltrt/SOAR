file_path,api_count,code
setup.py,0,"b""from distutils.core import setup\nimport setuptools\nfrom os import path\n\nthis_directory = path.abspath(path.dirname(__file__))\nwith open(path.join(this_directory, 'README.md')) as f:\n\tlong_description = f.read()\n\nsetup(\n\tname = 'gpt2_client',\n\tpackages = ['gpt2_client'],\n\tversion = '2.1.5',\n\tlicense='MIT',\n\tdescription = 'Easy-to-use Wrapper for the GPT-2 117M, 345M, 774M, and 1.5B Transformer Models',\n\tlong_description = long_description,\n\tlong_description_content_type = 'text/markdown',\n\tauthor = 'Rishabh Anand',\n\tauthor_email = 'mail.rishabh.anand@gmail.com',\n\turl = 'https://github.com/rish-16/gpt2client',\n\tdownload_url = 'https://github.com/rish-16/gpt2client/archive/2.1.tar.gz',\n\tkeywords = ['gpt-2', 'AI', 'ML', 'wrapper', 'transformer', 'machine learning', 'openai', 'text generation'],\n\tinstall_requires=[\n\t\t\t'numpy',\n\t\t\t'tensorflow',\n\t\t\t'regex',\n\t\t\t'tqdm',\n\t\t\t'requests',\n\t\t\t'termcolor',\n\t\t\t'gpt_2_simple'\n\t\t],\n\tclassifiers=[\n\t\t'Development Status :: 4 - Beta',\n\t\t'Intended Audience :: Developers',\n\t\t'Topic :: Software Development :: Build Tools',\n\t\t'License :: OSI Approved :: MIT License',\n\t\t'Programming Language :: Python :: 3',\n\t\t'Programming Language :: Python :: 3.4',\n\t\t'Programming Language :: Python :: 3.5',\n\t\t'Programming Language :: Python :: 3.6',\n\t],\n)"""
demos/batch_prompts.py,0,"b'""""""\nThe following snippet covers how to pass in prompts using a list. The client iterates through the prompts and returns\nand array containing the generated text pertaining to each prompt.\n""""""\n\nfrom gpt2_client import GPT2Client\n\ngpt2 = GPT2Client(\'117M\', save_dir=""models"")\n\n# optional -> if you already have the assets, you can comment this out\ngpt2.load_model(force_download=False)\n\nprompts = [\n    ""Today is a beautiful day"",\n    ""Today was a very bad day""\n]\n\ntext = gpt2.generate_batch_from_prompts(prompts)\nprint (text)'"
demos/controls.py,0,"b'""""""\nThe following snippet covers how to control the specifics of the generated text.\nWith GPT2Client, you can control the number of words generated (for now).\n""""""\n\nfrom gpt2_client import GPT2Client\n\ngpt2 = GPT2Client(\'117M\', save_dir=""models"")\n\n# optional -> if you already have the assets, you can comment this out\ngpt2.load_model(force_download=False)\n\n# returns a body of text 40 words in length\ntext = gpt2.generate(words=40)\nprint (text)'"
demos/finetuning.py,0,"b'""""""\nThe following snippet covers how to finetune models on custom text datasets (in any language!).\nThe client returns an array of the generated text.\n""""""\n\nfrom gpt2_client import GPT2Client\n\n# optional -> if you already have the assets, you can comment this out\ngpt2 = GPT2Client(\'117M\')\n\nmy_corpus = \'./data/shakespeare.txt\' # path to corpus\ncustom_text = gpt2.finetune(my_corpus, return_text=True)\nprint (custom_text)'"
demos/interactive_mode.py,0,"b'""""""\nThe following snippet covers how to use GPT2Client interactively. \nThe client asks you to enter a prompt `n_samples` times. \n\nIt will return an array containing the generated text for each prompt in order.\n""""""\n\nfrom gpt2_client import GPT2Client\n\ngpt2 = GPT2Client(\'117M\', save_dir=""models"")\n\n# optional -> if you already have the assets, you can comment this out\ngpt2.load_model(force_download=False)\n\ntext = gpt2.generate(interactive=True, n_samples=4, return_text=True)\nprint (text)'"
demos/random_sample.py,0,"b'""""""\nThe following snippet covers how to use GPT2Client to generate text randomly. \nIt will return an array containing the `n_samples` pieces of generated text.\n""""""\n\nfrom gpt2_client import GPT2Client\n\ngpt2 = GPT2Client(\'117M\', save_dir=""models"")\n\n# optional -> if you already have the assets, you can comment this out\ngpt2.load_model(force_download=False)\n\n# interative mode is False by default\n# This will return 4 pieces of generated text\ntext = gpt2.generate(n_samples=4, return_text=True)\nprint (text)'"
gpt2_client/__init__.py,0,b'from gpt2_client.gpt2_client import GPT2Client'
gpt2_client/encoder.py,0,"b'# coding=utf-8\n\n#!/usr/bin/env python3\n\n""""""Byte pair encoding utilities""""""\n\nimport os\nimport json\nimport regex as re\n\ntry:\n    from functools import lru_cache\nexcept ImportError:\n    from backports.functools_lru_cache import lru_cache\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you\'re at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""\n    bs = list(range(ord(""!""), ord(""~"")+1))+list(range(ord(""\xc2\xa1""), ord(""\xc2\xac"")+1))+list(range(ord(""\xc2\xae""), ord(""\xc3\xbf"")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\ndef get_pairs(word):\n    """"""Return set of symbol pairs in a word.\n\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\nclass Encoder:\n    def __init__(self, encoder, bpe_merges, errors=\'replace\'):\n        self.encoder = encoder\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        self.errors = errors # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n        self.pat = re.compile(r""""""\'s|\'t|\'re|\'ve|\'m|\'ll|\'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+"""""")\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        for token in re.findall(self.pat, text):\n            token = \'\'.join(self.byte_encoder[b] for b in token.encode(\'utf-8\'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\' \'))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \'\'.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\'utf-8\', errors=self.errors)\n        return text\n\ndef get_encoder(model_name, models_dir):\n    with open(os.path.join(models_dir, model_name, \'encoder.json\'), \'r\') as f:\n        encoder = json.load(f)\n    with open(os.path.join(models_dir, model_name, \'vocab.bpe\'), \'r\', encoding=""utf-8"") as f:\n        bpe_data = f.read()\n    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\'\\n\')[1:-1]]\n    return Encoder(\n        encoder=encoder,\n        bpe_merges=bpe_merges,\n    )\n'"
gpt2_client/gpt2_client.py,91,"b'#!/usr/bin/env python3\n\nimport os\nfrom termcolor import colored, cprint\nimport requests\nimport sys\nfrom tqdm import tqdm\nimport json\nimport json\nimport regex as re\n\nimport tensorflow as tf\nfrom tensorflow.contrib.training import HParams\nimport numpy as np\nimport gpt_2_simple as gpt2\n\ntry:\n    from functools import lru_cache\nexcept ImportError:\n    from backports.functools_lru_cache import lru_cache\n\nclass GPT2Client(object):\n    def __init__(self, model_name=\'117M\', save_dir=\'models\'):\n        """"""\n        Attributes\n        ----------\n        attr: model_name (string)\n        - default: \'117M\'\n        - desc: Downloads the \'117M\' GPT-2 model. Can alternatively be set to the \'345M\', \'774M\', or \'1558M\' model\n\n        attr: save_dir (string)\n        - default: \'models\'\n        - desc: Name of directory where the weights, checkpoints, and \n                hyper-parameters are downloaded and saved\n                \n        Methods\n        -------\n        download_helper(filename : string)\n        load_model(force_download : bool)\n        generate(interactive : bool, n_samples : int, words : int, display : bool, return_text: bool) -> list of string\n        generate_batch_from_prompts(prompts : list) -> list of string\n        fintune(corpus : object, return_text : bool) -> text\n        encode_seq(sequence : string) -> numpy array of integer\n        decode_seq(sequence : integers) -> list of string\n        """"""\n        \n        assert model_name in [\'117M\', \'345M\', \'774M\', \'1558M\'], \'Please choose from either 117M, 345M, 774M, or 1558M parameter models only. This library does support other model sizes.\'\n        assert save_dir != \'\', \'Please provide a save directory for the model weights and checkpoints. This cannot be empty.\'\n\n        self.model_name = model_name\n        self.save_dir = save_dir\n        \n    def download_helper(self, filename):\n        r = requests.get(\'https://storage.googleapis.com/gpt-2/models/\' + self.model_name + \'/\' + filename, stream=True)\n        \n        with open(""./{}/{}/{}"".format(self.save_dir, self.model_name, filename), \'wb\') as f:\n            file_size = int(r.headers[\'content-length\'])\n            chunk_size = 1000\n            with tqdm(ncols=100, desc=\'Downloading {}\'.format(colored(filename, \'cyan\', attrs=[\'bold\'])), total=file_size, unit_scale=True) as pbar:\n                for chunk in r.iter_content(chunk_size=chunk_size):\n                    f.write(chunk)\n                    pbar.update(chunk_size)\n\n    def load_model(self, force_download=False):\n        """""" Creates `models` directory and downloads model weights and checkpoints\n\n        Parameters\n        ----------\n        arg: force_download (bool)\n            - default: False\n            - desc: Ignore cached files and redownload model weights and checkpoints when set to `True`\n        """"""\n        \n        subdir = ""./{}/{}/"".format(self.save_dir, self.model_name)\n        if os.path.exists(subdir) == False:\n            os.makedirs(subdir)\n            print (\'Created `{}/{}` directory to save model weights and checkpoints.\'.format(self.save_dir, self.model_name))\n            \n        if force_download == True:\n            for filename in [\'checkpoint\', \'encoder.json\', \'hparams.json\', \'model.ckpt.data-00000-of-00001\', \'model.ckpt.index\', \'model.ckpt.meta\', \'vocab.bpe\']:\n                self.download_helper(filename)\n        else:\n            for filename in [\'checkpoint\', \'encoder.json\', \'hparams.json\', \'model.ckpt.data-00000-of-00001\', \'model.ckpt.index\', \'model.ckpt.meta\', \'vocab.bpe\']:\n                if os.path.exists(subdir + filename):\n                    print (\'{0:<60}{1:<20}\'.format(""Loading "" + colored(filename, \'cyan\', attrs=[\'bold\']), ""File already exists""))\n                else:\n                    self.download_helper(filename)\n\n    def generate(self, interactive=False, n_samples=1, words=None, display=True, return_text=False):\n        """""" Returns generated text sample\n        \n        Parameters\n        ----------\n        arg: interactive (bool)\n            - default: False\n            - desc: Toggles interactive mode which prompts user for input text\n\n        arg: n_samples (int)\n            - default: 0\n            - desc: Number of samples to be generated by GPT-2 Model. If 0, it generates indefinitely\n     \n        arg: words (int)\n            - default=None\n            - desc: Number of words generated by the client\n\n        arg: display (bool)\n            - default: True\n            - desc: Prints out text to console when set to True\n\n        arg: return_text (bool)\n            - default: False\n            - desc: Returns generated text when set to True\n\n        Returns:\n            An array of generated strings\n        """"""\n        \n        models_dir = models_dir = os.path.expanduser(os.path.expandvars(self.save_dir))\n        enc = get_encoder(self.model_name, self.save_dir)\n        hparams = default_hparams()\n\n        with open(os.path.join(self.save_dir, self.model_name, \'hparams.json\')) as f:\n            data = json.load(f)\n            hparams.override_from_dict(data)\n\n        length = hparams.n_ctx\n\n        with tf.Session(graph=tf.Graph()) as sess:\n            batch_size = 1\n            temperature = 1\n            top_k = 40\n\n            context = tf.placeholder(tf.int32, [batch_size, None])\n            np.random.seed(None)\n            tf.set_random_seed(None)\n\n            output = sample_sequence(\n                hparams=hparams,\n                length=length,\n                start_token=enc.encoder[\'<|endoftext|>\'],\n                batch_size=batch_size,\n                temperature=temperature, \n                top_k=top_k\n            )\n\n            saver = tf.train.Saver()\n            ckpt = tf.train.latest_checkpoint(os.path.join(self.save_dir, self.model_name))\n            saver.restore(sess, ckpt)\n\n            if not interactive:\n                # Generate random samples from scratch\n                print (colored(\'Generating sample...\', \'yellow\'))\n\n                while n_samples == 0 or generated < n_samples:\n                    out = sess.run(output)\n                    for i in range(batch_size):\n                        generated += batch_size\n                        text.append(enc.decode(out[i]))\n                        print (colored(\'---------------------SAMPLE---------------------\\n\', \'cyan\'))\n\n                        if display:\n                            print (text)\n\n                        if return_text:\n                            return text\n\n            else:\n                # Generate random samples from prompt\n                for _ in range(n_samples):\n                    prompt = input(colored(\'Enter a prompt got GPT-2 >> \', \'cyan\'))\n                    print (\'{}: {}\\n\'.format(colored(\'Prompt\', attrs=[\'bold\']), colored(prompt, \'green\')))\n                    print (colored(\'Generating sample...\', \'yellow\'))\n\n                    context_tokens = enc.encode(prompt)\n                    text_array = []\n                    text = \'\'\n                    generated = 0\n                    for _ in range(n_samples // batch_size):\n                        out = sess.run(output, feed_dict={\n                            context: [context_tokens for _ in range(batch_size)]\n                        })[:, len(context_tokens):]\n\n                        for i in range(batch_size):\n                            generated += 1\n                            text += enc.decode(out[i])\n                            text_array.append(enc.decode(out[i]))\n                            print (colored(\'---------------------SAMPLE---------------------\\n\', \'cyan\'))\n\n                            if display:\n                                print (text)\n\n                            if return_text:\n                                return text_array\n\n    def generate_batch_from_prompts(self, batch):\n        """""" Returns an array of generated text\n\n        Parameters\n        ----------\n        arg: batch (list)\n            - desc: An array of prompts given to the GPT2Client instance.\n                    The contents of the array are fed to the instance one by one\n\n        Returns:\n            An array of generated text for each prompt given in `batch`\n        """"""                \n        \n        final_generated_text = []\n        \n        models_dir = models_dir = os.path.expanduser(os.path.expandvars(self.save_dir))\n        enc = get_encoder(self.model_name, self.save_dir)\n        hparams = default_hparams()\n\n        with open(os.path.join(self.save_dir, self.model_name, \'hparams.json\')) as f:\n            data = json.load(f)\n            hparams.override_from_dict(data)\n\n        length = hparams.n_ctx\n\n        with tf.Session(graph=tf.Graph()) as sess:\n            batch_size = 1\n            temperature = 1\n            top_k = 40\n\n            context = tf.placeholder(tf.int32, [batch_size, None])\n            np.random.seed(None)\n            tf.set_random_seed(None)\n\n            output = sample_sequence(\n                hparams=hparams,\n                length=length,\n                start_token=enc.encoder[\'<|endoftext|>\'],\n                batch_size=batch_size,\n                temperature=temperature, \n                top_k=top_k\n            )\n\n            saver = tf.train.Saver()\n            ckpt = tf.train.latest_checkpoint(os.path.join(self.save_dir, self.model_name))\n            saver.restore(sess, ckpt)\n        \n            for i in batch:\n                print (\'Prompt: {}\'.format(colored(i, \'green\')))\n                context_tokens = enc.encode(i)\n                text_array = []\n                text = \'\'\n                generated = 0\n                for _ in range(len(batch) // batch_size):\n                    out = sess.run(output, feed_dict={\n                        context: [context_tokens for _ in range(batch_size)]\n                    })[:, len(context_tokens):]\n\n                    for i in range(batch_size):\n                        generated += 1\n                        text += enc.decode(out[i])\n                        \n                        final_generated_text.append(enc.decode(out[i]))\n                \n        return final_generated_text\n\n    def finetune(self, corpus, return_text=True):\n        """""" Returns generated text sample\n\n        Parameters\n        ----------\n        arg: corpus (object)\n            - desc: Custom dataset text file\n\n        arg: return_text (bool)\n            - default: True\n            - desc: Toggles whether to return custom-generated text in an array after fine-tuning\n\n        Returns:\n            Generated string in an array\n        """"""\n        sess = gpt2.start_tf_sess()\n        gpt2.finetune(sess,\n                corpus,\n                model_name=self.model_name,\n                steps=1000)     # steps is max number of training steps\n\n        if return_text:\n            text = gpt2.generate(sess, return_as_list=True)\n            return text\n        else:\n            gpt2.generate(sess)\t\n                 \n    def encode_seq(self, sequence):\n        models_dir = models_dir = os.path.expanduser(os.path.expandvars(self.save_dir))\n        enc = get_encoder(self.model_name, self.save_dir)\n        hparams = default_hparams()\n\n        with open(os.path.join(self.save_dir, self.model_name, \'hparams.json\')) as f:\n            data = json.load(f)\n            hparams.override_from_dict(data)\n        \n        length = hparams.n_ctx\n\n        with tf.Session(graph=tf.Graph()) as sess:\n            batch_size = 1\n            temperature = 1\n            top_k = 40\n\n            context = tf.placeholder(tf.int32, [batch_size, None])\n            np.random.seed(None)\n            tf.set_random_seed(None)\n\n            output = sample_sequence(\n                hparams=hparams,\n                length=length,\n                start_token=enc.encoder[\'<|endoftext|>\'],\n                batch_size=batch_size,\n                temperature=temperature, \n                top_k=top_k\n            )\n\n            saver = tf.train.Saver()\n            ckpt = tf.train.latest_checkpoint(os.path.join(self.save_dir, self.model_name))\n            saver.restore(sess, ckpt)\n            \n            context_tokens = enc.encode(sequence)\n            content_tokens = np.array(content_tokens)\n            \n            return context_tokens\n        \n    def decode_seq(self, encodings):\n        # converting numpy array to list\n        if type(encodings).__module__ == np.__name__:\n            encodings = encodings.tolist()\n\n        models_dir = models_dir = os.path.expanduser(os.path.expandvars(self.save_dir))\n        enc = get_encoder(self.model_name, self.save_dir)\n        hparams = default_hparams()\n\n        with open(os.path.join(self.save_dir, self.model_name, \'hparams.json\')) as f:\n            data = json.load(f)\n            hparams.override_from_dict(data)\n            \n        length = hparams.n_ctx\n\n        with tf.Session(graph=tf.Graph()) as sess:\n            batch_size = 1\n            temperature = 1\n            top_k = 40\n\n            context = tf.placeholder(tf.int32, [batch_size, None])\n            np.random.seed(None)\n            tf.set_random_seed(None)\n\n            output = sample_sequence(\n                hparams=hparams,\n                length=length,\n                start_token=enc.encoder[\'<|endoftext|>\'],\n                batch_size=batch_size,\n                temperature=temperature, \n                top_k=top_k\n            )\n\n            saver = tf.train.Saver()\n            ckpt = tf.train.latest_checkpoint(os.path.join(self.save_dir, self.model_name))\n            saver.restore(sess, ckpt)\n            \n            sequences = enc.decode(encodings)\n            \n            return sequences\n            \n@lru_cache()\ndef bytes_to_unicode():\n    """"""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you\'re at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""\n    bs = list(range(ord(""!""), ord(""~"")+1))+list(range(ord(""\xc2\xa1""), ord(""\xc2\xac"")+1))+list(range(ord(""\xc2\xae""), ord(""\xc3\xbf"")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\ndef get_pairs(word):\n    """"""Return set of symbol pairs in a word.\n\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\nclass Encoder:\n    def __init__(self, encoder, bpe_merges, errors=\'replace\'):\n        self.encoder = encoder\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        self.errors = errors # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n        self.pat = re.compile(r""""""\'s|\'t|\'re|\'ve|\'m|\'ll|\'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+"""""")\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        for token in re.findall(self.pat, text):\n            token = \'\'.join(self.byte_encoder[b] for b in token.encode(\'utf-8\'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\' \'))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \'\'.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\'utf-8\', errors=self.errors)\n        return text\n\ndef get_encoder(model_name, models_dir):\n    with open(""./{}/{}/"".format(models_dir, model_name) + \'encoder.json\', \'r\') as f:\n        encoder = json.load(f)\n    with open(""./{}/{}/"".format(models_dir, model_name) + \'vocab.bpe\', \'r\', encoding=""utf-8"") as f:\n        bpe_data = f.read()\n    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\'\\n\')[1:-1]]\n    return Encoder(\n        encoder=encoder,\n        bpe_merges=bpe_merges,\n    )\n\ndef top_k_logits(logits, k):\n    if k == 0:\n        # no truncation\n        return logits\n\n    def _top_k():\n        values, _ = tf.nn.top_k(logits, k=k)\n        min_values = values[:, -1, tf.newaxis]\n        return tf.where(\n            logits < min_values,\n            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n            logits,\n        )\n    return tf.cond(\n         tf.equal(k, 0),\n         lambda: logits,\n         lambda: _top_k(),\n    )\n\ndef sample_sequence(hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0):\n    if start_token is None:\n        assert context is not None, \'Specify exactly one of start_token and context!\'\n    else:\n        assert context is None, \'Specify exactly one of start_token and context!\'\n        context = tf.fill([batch_size, 1], start_token)\n\n    def step(hparams, tokens, past=None):\n        lm_output = model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n\n        logits = lm_output[\'logits\'][:, :, :hparams.n_vocab]\n        presents = lm_output[\'present\']\n        presents.set_shape(past_shape(hparams=hparams, batch_size=batch_size))\n        return {\n            \'logits\': logits,\n            \'presents\': presents,\n        }\n\n    with tf.name_scope(\'sample_sequence\'):\n        def body(past, prev, output):\n            next_outputs = step(hparams, prev, past=past)\n            logits = next_outputs[\'logits\'][:, -1, :]    / tf.to_float(temperature)\n            logits = top_k_logits(logits, k=top_k)\n            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)\n            return [\n                next_outputs[\'presents\'] if past is None else tf.concat([past, next_outputs[\'presents\']], axis=-2),\n                samples,\n                tf.concat([output, samples], axis=1)\n            ]\n\n        past, prev, output = body(None, context, context)\n\n        def cond(*args):\n            return True\n\n        _, _, tokens = tf.while_loop(\n            cond=cond, body=body,\n            maximum_iterations=length - 1,\n            loop_vars=[\n                past,\n                prev,\n                output\n            ],\n            shape_invariants=[\n                tf.TensorShape(past_shape(hparams=hparams, batch_size=batch_size)),\n                tf.TensorShape([batch_size, None]),\n                tf.TensorShape([batch_size, None]),\n            ],\n            back_prop=False,\n        )\n\n        return tokens\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n\ndef default_hparams():\n    return HParams(\n        n_vocab=0,\n        n_ctx=1024,\n        n_embd=768,\n        n_head=12,\n        n_layer=12,\n    )\n\ndef shape_list(x):\n    """"""Deal with dynamic shape in tensorflow cleanly.""""""\n    static = x.shape.as_list()\n    dynamic = tf.shape(x)\n    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n\ndef softmax(x, axis=-1):\n    x = x - tf.reduce_max(x, axis=axis, keepdims=True)\n    ex = tf.exp(x)\n    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)\n\ndef gelu(x):\n    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n\ndef norm(x, scope, axis=-1, epsilon=1e-5):\n    """"""Normalize to mean = 0, std = 1, then do a diagonal affine transform.""""""\n    with tf.variable_scope(scope):\n        n_state = x.shape[-1].value\n        g = tf.get_variable(\'g\', [n_state], initializer=tf.constant_initializer(1))\n        b = tf.get_variable(\'b\', [n_state], initializer=tf.constant_initializer(0))\n        u = tf.reduce_mean(x, axis=axis, keepdims=True)\n        s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n        x = (x - u) * tf.rsqrt(s + epsilon)\n        x = x*g + b\n        return x\n\ndef split_states(x, n):\n    """"""Reshape the last dimension of x into [n, x.shape[-1]/n].""""""\n    *start, m = shape_list(x)\n    return tf.reshape(x, start + [n, m//n])\n\ndef merge_states(x):\n    """"""Smash the last two dimensions of x into a single dimension.""""""\n    *start, a, b = shape_list(x)\n    return tf.reshape(x, start + [a*b])\n\ndef conv1d(x, scope, nf, w_init_stdev=0.02):\n    with tf.variable_scope(scope):\n        *start, nx = shape_list(x)\n        w = tf.get_variable(\'w\', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))\n        b = tf.get_variable(\'b\', [nf], initializer=tf.constant_initializer(0))\n        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n        return c\n\ndef attention_mask(nd, ns, dtype):\n    """"""1\'s in the lower triangle, counting from the lower right corner.\n\n    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn\'t produce garbage on TPUs.\n    """"""\n    i = tf.range(nd)[:,None]\n    j = tf.range(ns)\n    m = i >= j - ns + nd\n    return tf.cast(m, dtype)\n\ndef attn(x, scope, n_state, past, hparams):\n    assert x.shape.ndims == 3    # Should be [batch, sequence, features]\n    assert n_state % hparams.n_head == 0\n    if past is not None:\n        assert past.shape.ndims == 5    # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n\n    def split_heads(x):\n        # From [batch, sequence, features] to [batch, heads, sequence, features]\n        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])\n\n    def merge_heads(x):\n        # Reverse of split_heads\n        return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n\n    def mask_attn_weights(w):\n        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n        _, _, nd, ns = shape_list(w)\n        b = attention_mask(nd, ns, dtype=w.dtype)\n        b = tf.reshape(b, [1, 1, nd, ns])\n        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n        return w\n\n    def multihead_attn(q, k, v):\n        # q, k, v have shape [batch, heads, sequence, features]\n        w = tf.matmul(q, k, transpose_b=True)\n        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n\n        w = mask_attn_weights(w)\n        w = softmax(w)\n        a = tf.matmul(w, v)\n        return a\n\n    with tf.variable_scope(scope):\n        c = conv1d(x, \'c_attn\', n_state*3)\n        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n        present = tf.stack([k, v], axis=1)\n        if past is not None:\n            pk, pv = tf.unstack(past, axis=1)\n            k = tf.concat([pk, k], axis=-2)\n            v = tf.concat([pv, v], axis=-2)\n        a = multihead_attn(q, k, v)\n        a = merge_heads(a)\n        a = conv1d(a, \'c_proj\', n_state)\n        return a, present\n\ndef mlp(x, scope, n_state, hparams):\n    with tf.variable_scope(scope):\n        nx = x.shape[-1].value\n        h = gelu(conv1d(x, \'c_fc\', n_state))\n        h2 = conv1d(h, \'c_proj\', nx)\n        return h2\n\ndef block(x, scope, past, hparams):\n    with tf.variable_scope(scope):\n        nx = x.shape[-1].value\n        a, present = attn(norm(x, \'ln_1\'), \'attn\', nx, past=past, hparams=hparams)\n        x = x + a\n        m = mlp(norm(x, \'ln_2\'), \'mlp\', nx*4, hparams=hparams)\n        x = x + m\n        return x, present\n\ndef past_shape(hparams, batch_size=None, sequence=None):\n    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n\ndef expand_tile(value, size):\n    """"""Add a new axis of given size.""""""\n    value = tf.convert_to_tensor(value, name=\'value\')\n    ndims = value.shape.ndims\n    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n\ndef positions_for(tokens, past_length):\n    batch_size = tf.shape(tokens)[0]\n    nsteps = tf.shape(tokens)[1]\n    return expand_tile(past_length + tf.range(nsteps), batch_size)\n\ndef model(hparams, X, past=None, scope=\'model\', reuse=False):\n    with tf.variable_scope(scope, reuse=reuse):\n        results = {}\n        batch, sequence = shape_list(X)\n\n        wpe = tf.get_variable(\'wpe\', [hparams.n_ctx, hparams.n_embd],\n                             initializer=tf.random_normal_initializer(stddev=0.01))\n        wte = tf.get_variable(\'wte\', [hparams.n_vocab, hparams.n_embd],\n                             initializer=tf.random_normal_initializer(stddev=0.02))\n        past_length = 0 if past is None else tf.shape(past)[-2]\n        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n\n        # Transformer\n        presents = []\n        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n        assert len(pasts) == hparams.n_layer\n        for layer, past in enumerate(pasts):\n            h, present = block(h, \'h%d\' % layer, past=past, hparams=hparams)\n            presents.append(present)\n        results[\'present\'] = tf.stack(presents, axis=1)\n        h = norm(h, \'ln_f\')\n\n        # Language model loss. Do tokens <n predict token n?\n        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n        logits = tf.matmul(h_flat, wte, transpose_b=True)\n        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n        results[\'logits\'] = logits\n        return results'"
gpt2_client/model.py,53,"b'#!/usr/bin/env python3\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.training import HParams\n\ndef default_hparams():\n    return HParams(\n        n_vocab=0,\n        n_ctx=1024,\n        n_embd=768,\n        n_head=12,\n        n_layer=12,\n    )\n\ndef shape_list(x):\n    """"""Deal with dynamic shape in tensorflow cleanly.""""""\n    static = x.shape.as_list()\n    dynamic = tf.shape(x)\n    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n\ndef softmax(x, axis=-1):\n    x = x - tf.reduce_max(x, axis=axis, keepdims=True)\n    ex = tf.exp(x)\n    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)\n\ndef gelu(x):\n    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n\ndef norm(x, scope, axis=-1, epsilon=1e-5):\n    """"""Normalize to mean = 0, std = 1, then do a diagonal affine transform.""""""\n    with tf.variable_scope(scope):\n        n_state = x.shape[-1].value\n        g = tf.get_variable(\'g\', [n_state], initializer=tf.constant_initializer(1))\n        b = tf.get_variable(\'b\', [n_state], initializer=tf.constant_initializer(0))\n        u = tf.reduce_mean(x, axis=axis, keepdims=True)\n        s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n        x = (x - u) * tf.rsqrt(s + epsilon)\n        x = x*g + b\n        return x\n\ndef split_states(x, n):\n    """"""Reshape the last dimension of x into [n, x.shape[-1]/n].""""""\n    *start, m = shape_list(x)\n    return tf.reshape(x, start + [n, m//n])\n\ndef merge_states(x):\n    """"""Smash the last two dimensions of x into a single dimension.""""""\n    *start, a, b = shape_list(x)\n    return tf.reshape(x, start + [a*b])\n\ndef conv1d(x, scope, nf, w_init_stdev=0.02):\n    with tf.variable_scope(scope):\n        *start, nx = shape_list(x)\n        w = tf.get_variable(\'w\', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))\n        b = tf.get_variable(\'b\', [nf], initializer=tf.constant_initializer(0))\n        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n        return c\n\ndef attention_mask(nd, ns, dtype):\n    """"""1\'s in the lower triangle, counting from the lower right corner.\n\n    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn\'t produce garbage on TPUs.\n    """"""\n    i = tf.range(nd)[:,None]\n    j = tf.range(ns)\n    m = i >= j - ns + nd\n    return tf.cast(m, dtype)\n\n\ndef attn(x, scope, n_state, past, hparams):\n    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n    assert n_state % hparams.n_head == 0\n    if past is not None:\n        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n\n    def split_heads(x):\n        # From [batch, sequence, features] to [batch, heads, sequence, features]\n        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])\n\n    def merge_heads(x):\n        # Reverse of split_heads\n        return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n\n    def mask_attn_weights(w):\n        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n        _, _, nd, ns = shape_list(w)\n        b = attention_mask(nd, ns, dtype=w.dtype)\n        b = tf.reshape(b, [1, 1, nd, ns])\n        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n        return w\n\n    def multihead_attn(q, k, v):\n        # q, k, v have shape [batch, heads, sequence, features]\n        w = tf.matmul(q, k, transpose_b=True)\n        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n\n        w = mask_attn_weights(w)\n        w = softmax(w)\n        a = tf.matmul(w, v)\n        return a\n\n    with tf.variable_scope(scope):\n        c = conv1d(x, \'c_attn\', n_state*3)\n        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n        present = tf.stack([k, v], axis=1)\n        if past is not None:\n            pk, pv = tf.unstack(past, axis=1)\n            k = tf.concat([pk, k], axis=-2)\n            v = tf.concat([pv, v], axis=-2)\n        a = multihead_attn(q, k, v)\n        a = merge_heads(a)\n        a = conv1d(a, \'c_proj\', n_state)\n        return a, present\n\n\ndef mlp(x, scope, n_state, hparams):\n    with tf.variable_scope(scope):\n        nx = x.shape[-1].value\n        h = gelu(conv1d(x, \'c_fc\', n_state))\n        h2 = conv1d(h, \'c_proj\', nx)\n        return h2\n\n\ndef block(x, scope, past, hparams):\n    with tf.variable_scope(scope):\n        nx = x.shape[-1].value\n        a, present = attn(norm(x, \'ln_1\'), \'attn\', nx, past=past, hparams=hparams)\n        x = x + a\n        m = mlp(norm(x, \'ln_2\'), \'mlp\', nx*4, hparams=hparams)\n        x = x + m\n        return x, present\n\ndef past_shape(hparams, batch_size=None, sequence=None):\n    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n\ndef expand_tile(value, size):\n    """"""Add a new axis of given size.""""""\n    value = tf.convert_to_tensor(value, name=\'value\')\n    ndims = value.shape.ndims\n    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n\ndef positions_for(tokens, past_length):\n    batch_size = tf.shape(tokens)[0]\n    nsteps = tf.shape(tokens)[1]\n    return expand_tile(past_length + tf.range(nsteps), batch_size)\n\n\ndef model(hparams, X, past=None, scope=\'model\', reuse=False):\n    with tf.variable_scope(scope, reuse=reuse):\n        results = {}\n        batch, sequence = shape_list(X)\n\n        wpe = tf.get_variable(\'wpe\', [hparams.n_ctx, hparams.n_embd],\n                             initializer=tf.random_normal_initializer(stddev=0.01))\n        wte = tf.get_variable(\'wte\', [hparams.n_vocab, hparams.n_embd],\n                             initializer=tf.random_normal_initializer(stddev=0.02))\n        past_length = 0 if past is None else tf.shape(past)[-2]\n        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n\n        # Transformer\n        presents = []\n        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n        assert len(pasts) == hparams.n_layer\n        for layer, past in enumerate(pasts):\n            h, present = block(h, \'h%d\' % layer, past=past, hparams=hparams)\n            presents.append(present)\n        results[\'present\'] = tf.stack(presents, axis=1)\n        h = norm(h, \'ln_f\')\n\n        # Language model loss.  Do tokens <n predict token n?\n        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n        logits = tf.matmul(h_flat, wte, transpose_b=True)\n        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n        results[\'logits\'] = logits\n        return results'"
gpt2_client/sample.py,17,"b""import tensorflow as tf\nimport model\n\ndef top_k_logits(logits, k):\n    if k == 0:\n        # no truncation\n        return logits\n\n    def _top_k():\n        values, _ = tf.nn.top_k(logits, k=k)\n        min_values = values[:, -1, tf.newaxis]\n        return tf.where(\n            logits < min_values,\n            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n            logits,\n        )\n    return tf.cond(\n       tf.equal(k, 0),\n       lambda: logits,\n       lambda: _top_k(),\n    )\n\n\ndef sample_sequence(hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0):\n    if start_token is None:\n        assert context is not None, 'Specify exactly one of start_token and context!'\n    else:\n        assert context is None, 'Specify exactly one of start_token and context!'\n        context = tf.fill([batch_size, 1], start_token)\n\n    def step(hparams, tokens, past=None):\n        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n\n        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n        presents = lm_output['present']\n        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))\n        return {\n            'logits': logits,\n            'presents': presents,\n        }\n\n    with tf.name_scope('sample_sequence'):\n        def body(past, prev, output):\n            next_outputs = step(hparams, prev, past=past)\n            logits = next_outputs['logits'][:, -1, :]  / tf.to_float(temperature)\n            logits = top_k_logits(logits, k=top_k)\n            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)\n            return [\n                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n                samples,\n                tf.concat([output, samples], axis=1)\n            ]\n\n        past, prev, output = body(None, context, context)\n\n        def cond(*args):\n            return True\n\n        _, _, tokens = tf.while_loop(\n            cond=cond, body=body,\n            maximum_iterations=length - 1,\n            loop_vars=[\n                past,\n                prev,\n                output\n            ],\n            shape_invariants=[\n                tf.TensorShape(model.past_shape(hparams=hparams, batch_size=batch_size)),\n                tf.TensorShape([batch_size, None]),\n                tf.TensorShape([batch_size, None]),\n            ],\n            back_prop=False,\n        )\n\n        return tokens"""
build/lib/gpt2_client/__init__.py,0,b'from gpt2_client.gpt2_client import GPT2Client'
build/lib/gpt2_client/encoder.py,0,"b'# coding=utf-8\n\n#!/usr/bin/env python3\n\n""""""Byte pair encoding utilities""""""\n\nimport os\nimport json\nimport regex as re\n\ntry:\n    from functools import lru_cache\nexcept ImportError:\n    from backports.functools_lru_cache import lru_cache\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you\'re at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""\n    bs = list(range(ord(""!""), ord(""~"")+1))+list(range(ord(""\xc2\xa1""), ord(""\xc2\xac"")+1))+list(range(ord(""\xc2\xae""), ord(""\xc3\xbf"")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\ndef get_pairs(word):\n    """"""Return set of symbol pairs in a word.\n\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\nclass Encoder:\n    def __init__(self, encoder, bpe_merges, errors=\'replace\'):\n        self.encoder = encoder\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        self.errors = errors # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n        self.pat = re.compile(r""""""\'s|\'t|\'re|\'ve|\'m|\'ll|\'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+"""""")\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        for token in re.findall(self.pat, text):\n            token = \'\'.join(self.byte_encoder[b] for b in token.encode(\'utf-8\'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\' \'))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \'\'.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\'utf-8\', errors=self.errors)\n        return text\n\ndef get_encoder(model_name, models_dir):\n    with open(os.path.join(models_dir, model_name, \'encoder.json\'), \'r\') as f:\n        encoder = json.load(f)\n    with open(os.path.join(models_dir, model_name, \'vocab.bpe\'), \'r\', encoding=""utf-8"") as f:\n        bpe_data = f.read()\n    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\'\\n\')[1:-1]]\n    return Encoder(\n        encoder=encoder,\n        bpe_merges=bpe_merges,\n    )\n'"
build/lib/gpt2_client/gpt2_client.py,81,"b'#!/usr/bin/env python3\n\nimport os\nfrom termcolor import colored, cprint\nimport requests\nimport sys\nfrom tqdm import tqdm\nimport json\nimport json\nimport regex as re\n\nimport tensorflow as tf\nfrom tensorflow.contrib.training import HParams\nimport numpy as np\nimport gpt_2_simple as gpt2\n\ntry:\n    from functools import lru_cache\nexcept ImportError:\n    from backports.functools_lru_cache import lru_cache\n\nclass GPT2Client(object):\n    def __init__(self, model_name=\'117M\', save_dir=\'models\'):\n        """"""\n        Attributes\n        ----------\n        attr: model_name (string)\n        - default: \'117M\'\n        - desc: Downloads the \'117M\' GPT-2 model. Can be set to \'345M\' model \n\n        attr: save_dir (string)\n        - default: \'models\'\n        - desc: Name of directory where the weights, checkpoints, and \n                hyper-parameters are downloaded and saved\n                \n        Methods\n        -------\n        download_helper(filename : string)\n        load_model(force_download : bool)\n        generate(interactive : bool, n_samples : int, words : int, display : bool, return_text: bool) -> list\n        generate_batch_from_prompts(prompts : list) -> list\n        fintune(corpus : object, return_text : bool) -> text\n        """"""\n        \n        assert model_name in [\'117M\', \'345M\', \'774M\'], \'Please choose from either 117M, 345M, or 774M parameter models only. This library does support other model sizes.\'\n        assert save_dir != \'\', \'Please provide a save directory for the model weights and checkpoints. This cannot be empty.\'\n\n        self.model_name = model_name\n        self.save_dir = save_dir\n        \n    def download_helper(self, filename):\n        r = requests.get(\'https://storage.googleapis.com/gpt-2/models/\' + self.model_name + \'/\' + filename, stream=True)\n        \n        with open(""./{}/{}/{}"".format(self.save_dir, self.model_name, filename), \'wb\') as f:\n            file_size = int(r.headers[\'content-length\'])\n            chunk_size = 1000\n            with tqdm(ncols=100, desc=\'Downloading {}\'.format(colored(filename, \'cyan\', attrs=[\'bold\'])), total=file_size, unit_scale=True) as pbar:\n                for chunk in r.iter_content(chunk_size=chunk_size):\n                    f.write(chunk)\n                    pbar.update(chunk_size)\n\n    def load_model(self, force_download=False):\n        """""" Creates `models` directory and downloads model weights and checkpoints\n\n        Parameters\n        ----------\n        arg: force_download (bool)\n            - default: False\n            - desc: Ignore cached files and redownload model weights and checkpoints when set to `True`\n        """"""\n        \n        subdir = ""./{}/{}/"".format(self.save_dir, self.model_name)\n        if os.path.exists(subdir) == False:\n            os.makedirs(subdir)\n            print (\'Created `{}/{}` directory to save model weights and checkpoints.\'.format(self.save_dir, self.model_name))\n            \n        if force_download == True:\n            for filename in [\'checkpoint\', \'encoder.json\', \'hparams.json\', \'model.ckpt.data-00000-of-00001\', \'model.ckpt.index\', \'model.ckpt.meta\', \'vocab.bpe\']:\n                self.download_helper(filename)\n        else:\n            for filename in [\'checkpoint\', \'encoder.json\', \'hparams.json\', \'model.ckpt.data-00000-of-00001\', \'model.ckpt.index\', \'model.ckpt.meta\', \'vocab.bpe\']:\n                if os.path.exists(subdir + filename):\n                    print (\'{0:<60}{1:<20}\'.format(""Loading "" + colored(filename, \'cyan\', attrs=[\'bold\']), ""File already exists""))\n                else:\n                    self.download_helper(filename)\n\n    def generate(self, interactive=False, n_samples=1, words=None, display=True, return_text=False):\n        """""" Returns generated text sample\n        \n        Parameters\n        ----------\n        arg: interactive (bool)\n            - default: False\n            - desc: Toggles interactive mode which prompts user for input text\n\n        arg: n_samples (int)\n            - default: 0\n            - desc: Number of samples to be generated by GPT-2 Model. If 0, it generates indefinitely\n     \n        arg: words (int)\n            - default=None\n            - desc: Number of words generated by the client\n\n        arg: display (bool)\n            - default: True\n            - desc: Prints out text to console when set to True\n\n        arg: return_text (bool)\n            - default: False\n            - desc: Returns generated text when set to True\n\n        Returns:\n            An array of generated strings\n        """"""\n        \n        models_dir = models_dir = os.path.expanduser(os.path.expandvars(self.save_dir))\n        enc = get_encoder(self.model_name, self.save_dir)\n        hparams = default_hparams()\n\n        with open(os.path.join(self.save_dir, self.model_name, \'hparams.json\')) as f:\n            data = json.load(f)\n            hparams.override_from_dict(data)\n\n        length = hparams.n_ctx\n\n        with tf.Session(graph=tf.Graph()) as sess:\n            batch_size = 1\n            temperature = 1\n            top_k = 40\n\n            context = tf.placeholder(tf.int32, [batch_size, None])\n            np.random.seed(None)\n            tf.set_random_seed(None)\n\n            output = sample_sequence(\n                hparams=hparams,\n                length=length,\n                start_token=enc.encoder[\'<|endoftext|>\'],\n                batch_size=batch_size,\n                temperature=temperature, \n                top_k=top_k\n            )\n\n            saver = tf.train.Saver()\n            ckpt = tf.train.latest_checkpoint(os.path.join(self.save_dir, self.model_name))\n            saver.restore(sess, ckpt)\n\n            if not interactive:\n                # Generate random samples from scratch\n                print (colored(\'Generating sample...\', \'yellow\'))\n\n                while n_samples == 0 or generated < n_samples:\n                    out = sess.run(output)\n                    for i in range(batch_size):\n                        generated += batch_size\n                        text.append(enc.decode(out[i]))\n                        print (colored(\'---------------------SAMPLE---------------------\\n\', \'cyan\'))\n\n                        if display:\n                            print (text)\n\n                        if return_text:\n                            return text\n\n            else:\n                # Generate random samples from prompt\n                for _ in range(n_samples):\n                    prompt = input(colored(\'Enter a prompt got GPT-2 >> \', \'cyan\'))\n                    print (\'{}: {}\\n\'.format(colored(\'Prompt\', attrs=[\'bold\']), colored(prompt, \'green\')))\n                    print (colored(\'Generating sample...\', \'yellow\'))\n\n                    context_tokens = enc.encode(prompt)\n                    text_array = []\n                    text = \'\'\n                    generated = 0\n                    for _ in range(n_samples // batch_size):\n                        out = sess.run(output, feed_dict={\n                            context: [context_tokens for _ in range(batch_size)]\n                        })[:, len(context_tokens):]\n\n                        for i in range(batch_size):\n                            generated += 1\n                            text += enc.decode(out[i])\n                            text_array.append(enc.decode(out[i]))\n                            print (colored(\'---------------------SAMPLE---------------------\\n\', \'cyan\'))\n\n                            if display:\n                                print (text)\n\n                            if return_text:\n                                return text_array\n\n    def generate_batch_from_prompts(self, batch):\n        """""" Returns an array of generated text\n\n        Parameters\n        ----------\n        arg: batch (list)\n            - desc: An array of prompts given to the GPT2Client instance.\n                    The contents of the array are fed to the instance one by one\n\n        Returns:\n            An array of generated text for each prompt given in `batch`\n        """"""                \n        \n        final_generated_text = []\n        \n        models_dir = models_dir = os.path.expanduser(os.path.expandvars(self.save_dir))\n        enc = get_encoder(self.model_name, self.save_dir)\n        hparams = default_hparams()\n\n        with open(os.path.join(self.save_dir, self.model_name, \'hparams.json\')) as f:\n            data = json.load(f)\n            hparams.override_from_dict(data)\n\n        length = hparams.n_ctx\n\n        with tf.Session(graph=tf.Graph()) as sess:\n            batch_size = 1\n            temperature = 1\n            top_k = 40\n\n            context = tf.placeholder(tf.int32, [batch_size, None])\n            np.random.seed(None)\n            tf.set_random_seed(None)\n\n            output = sample_sequence(\n                hparams=hparams,\n                length=length,\n                start_token=enc.encoder[\'<|endoftext|>\'],\n                batch_size=batch_size,\n                temperature=temperature, \n                top_k=top_k\n            )\n\n            saver = tf.train.Saver()\n            ckpt = tf.train.latest_checkpoint(os.path.join(self.save_dir, self.model_name))\n            saver.restore(sess, ckpt)\n        \n            for i in batch:\n                print (\'Prompt: {}\'.format(colored(i, \'green\')))\n                context_tokens = enc.encode(i)\n                text_array = []\n                text = \'\'\n                generated = 0\n                for _ in range(len(batch) // batch_size):\n                    out = sess.run(output, feed_dict={\n                        context: [context_tokens for _ in range(batch_size)]\n                    })[:, len(context_tokens):]\n\n                    for i in range(batch_size):\n                        generated += 1\n                        text += enc.decode(out[i])\n                        \n                        final_generated_text.append(enc.decode(out[i]))\n                \n        return final_generated_text\n\n    def finetune(self, corpus, return_text=True):\n        """""" Returns generated text sample\n\n        Parameters\n        ----------\n        arg: corpus (object)\n            - desc: Custom dataset text file\n\n        arg: return_text (bool)\n            - default: True\n            - desc: Toggles whether to return custom-generated text in an array after fine-tuning\n\n        Returns:\n            Generated string in an array\n        """"""\n        sess = gpt2.start_tf_sess()\n        gpt2.finetune(sess,\n                corpus,\n                model_name=self.model_name,\n                steps=1000)     # steps is max number of training steps\n\n        if return_text:\n            text = gpt2.generate(sess, return_as_list=True)\n            return text\n        else:\n            gpt2.generate(sess)\t\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you\'re at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""\n    bs = list(range(ord(""!""), ord(""~"")+1))+list(range(ord(""\xc2\xa1""), ord(""\xc2\xac"")+1))+list(range(ord(""\xc2\xae""), ord(""\xc3\xbf"")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\ndef get_pairs(word):\n    """"""Return set of symbol pairs in a word.\n\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\nclass Encoder:\n    def __init__(self, encoder, bpe_merges, errors=\'replace\'):\n        self.encoder = encoder\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        self.errors = errors # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n        self.pat = re.compile(r""""""\'s|\'t|\'re|\'ve|\'m|\'ll|\'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+"""""")\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        for token in re.findall(self.pat, text):\n            token = \'\'.join(self.byte_encoder[b] for b in token.encode(\'utf-8\'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\' \'))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \'\'.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\'utf-8\', errors=self.errors)\n        return text\n\ndef get_encoder(model_name, models_dir):\n    with open(""./{}/{}/"".format(models_dir, model_name) + \'encoder.json\', \'r\') as f:\n        encoder = json.load(f)\n    with open(""./{}/{}/"".format(models_dir, model_name) + \'vocab.bpe\', \'r\', encoding=""utf-8"") as f:\n        bpe_data = f.read()\n    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\'\\n\')[1:-1]]\n    return Encoder(\n        encoder=encoder,\n        bpe_merges=bpe_merges,\n    )\n\ndef top_k_logits(logits, k):\n    if k == 0:\n        # no truncation\n        return logits\n\n    def _top_k():\n        values, _ = tf.nn.top_k(logits, k=k)\n        min_values = values[:, -1, tf.newaxis]\n        return tf.where(\n            logits < min_values,\n            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n            logits,\n        )\n    return tf.cond(\n         tf.equal(k, 0),\n         lambda: logits,\n         lambda: _top_k(),\n    )\n\ndef sample_sequence(hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0):\n    if start_token is None:\n        assert context is not None, \'Specify exactly one of start_token and context!\'\n    else:\n        assert context is None, \'Specify exactly one of start_token and context!\'\n        context = tf.fill([batch_size, 1], start_token)\n\n    def step(hparams, tokens, past=None):\n        lm_output = model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n\n        logits = lm_output[\'logits\'][:, :, :hparams.n_vocab]\n        presents = lm_output[\'present\']\n        presents.set_shape(past_shape(hparams=hparams, batch_size=batch_size))\n        return {\n            \'logits\': logits,\n            \'presents\': presents,\n        }\n\n    with tf.name_scope(\'sample_sequence\'):\n        def body(past, prev, output):\n            next_outputs = step(hparams, prev, past=past)\n            logits = next_outputs[\'logits\'][:, -1, :]    / tf.to_float(temperature)\n            logits = top_k_logits(logits, k=top_k)\n            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)\n            return [\n                next_outputs[\'presents\'] if past is None else tf.concat([past, next_outputs[\'presents\']], axis=-2),\n                samples,\n                tf.concat([output, samples], axis=1)\n            ]\n\n        past, prev, output = body(None, context, context)\n\n        def cond(*args):\n            return True\n\n        _, _, tokens = tf.while_loop(\n            cond=cond, body=body,\n            maximum_iterations=length - 1,\n            loop_vars=[\n                past,\n                prev,\n                output\n            ],\n            shape_invariants=[\n                tf.TensorShape(past_shape(hparams=hparams, batch_size=batch_size)),\n                tf.TensorShape([batch_size, None]),\n                tf.TensorShape([batch_size, None]),\n            ],\n            back_prop=False,\n        )\n\n        return tokens\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n\ndef default_hparams():\n    return HParams(\n        n_vocab=0,\n        n_ctx=1024,\n        n_embd=768,\n        n_head=12,\n        n_layer=12,\n    )\n\ndef shape_list(x):\n    """"""Deal with dynamic shape in tensorflow cleanly.""""""\n    static = x.shape.as_list()\n    dynamic = tf.shape(x)\n    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n\ndef softmax(x, axis=-1):\n    x = x - tf.reduce_max(x, axis=axis, keepdims=True)\n    ex = tf.exp(x)\n    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)\n\ndef gelu(x):\n    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n\ndef norm(x, scope, axis=-1, epsilon=1e-5):\n    """"""Normalize to mean = 0, std = 1, then do a diagonal affine transform.""""""\n    with tf.variable_scope(scope):\n        n_state = x.shape[-1].value\n        g = tf.get_variable(\'g\', [n_state], initializer=tf.constant_initializer(1))\n        b = tf.get_variable(\'b\', [n_state], initializer=tf.constant_initializer(0))\n        u = tf.reduce_mean(x, axis=axis, keepdims=True)\n        s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n        x = (x - u) * tf.rsqrt(s + epsilon)\n        x = x*g + b\n        return x\n\ndef split_states(x, n):\n    """"""Reshape the last dimension of x into [n, x.shape[-1]/n].""""""\n    *start, m = shape_list(x)\n    return tf.reshape(x, start + [n, m//n])\n\ndef merge_states(x):\n    """"""Smash the last two dimensions of x into a single dimension.""""""\n    *start, a, b = shape_list(x)\n    return tf.reshape(x, start + [a*b])\n\ndef conv1d(x, scope, nf, w_init_stdev=0.02):\n    with tf.variable_scope(scope):\n        *start, nx = shape_list(x)\n        w = tf.get_variable(\'w\', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))\n        b = tf.get_variable(\'b\', [nf], initializer=tf.constant_initializer(0))\n        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n        return c\n\ndef attention_mask(nd, ns, dtype):\n    """"""1\'s in the lower triangle, counting from the lower right corner.\n\n    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn\'t produce garbage on TPUs.\n    """"""\n    i = tf.range(nd)[:,None]\n    j = tf.range(ns)\n    m = i >= j - ns + nd\n    return tf.cast(m, dtype)\n\ndef attn(x, scope, n_state, past, hparams):\n    assert x.shape.ndims == 3    # Should be [batch, sequence, features]\n    assert n_state % hparams.n_head == 0\n    if past is not None:\n        assert past.shape.ndims == 5    # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n\n    def split_heads(x):\n        # From [batch, sequence, features] to [batch, heads, sequence, features]\n        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])\n\n    def merge_heads(x):\n        # Reverse of split_heads\n        return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n\n    def mask_attn_weights(w):\n        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n        _, _, nd, ns = shape_list(w)\n        b = attention_mask(nd, ns, dtype=w.dtype)\n        b = tf.reshape(b, [1, 1, nd, ns])\n        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n        return w\n\n    def multihead_attn(q, k, v):\n        # q, k, v have shape [batch, heads, sequence, features]\n        w = tf.matmul(q, k, transpose_b=True)\n        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n\n        w = mask_attn_weights(w)\n        w = softmax(w)\n        a = tf.matmul(w, v)\n        return a\n\n    with tf.variable_scope(scope):\n        c = conv1d(x, \'c_attn\', n_state*3)\n        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n        present = tf.stack([k, v], axis=1)\n        if past is not None:\n            pk, pv = tf.unstack(past, axis=1)\n            k = tf.concat([pk, k], axis=-2)\n            v = tf.concat([pv, v], axis=-2)\n        a = multihead_attn(q, k, v)\n        a = merge_heads(a)\n        a = conv1d(a, \'c_proj\', n_state)\n        return a, present\n\ndef mlp(x, scope, n_state, hparams):\n    with tf.variable_scope(scope):\n        nx = x.shape[-1].value\n        h = gelu(conv1d(x, \'c_fc\', n_state))\n        h2 = conv1d(h, \'c_proj\', nx)\n        return h2\n\ndef block(x, scope, past, hparams):\n    with tf.variable_scope(scope):\n        nx = x.shape[-1].value\n        a, present = attn(norm(x, \'ln_1\'), \'attn\', nx, past=past, hparams=hparams)\n        x = x + a\n        m = mlp(norm(x, \'ln_2\'), \'mlp\', nx*4, hparams=hparams)\n        x = x + m\n        return x, present\n\ndef past_shape(hparams, batch_size=None, sequence=None):\n    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n\ndef expand_tile(value, size):\n    """"""Add a new axis of given size.""""""\n    value = tf.convert_to_tensor(value, name=\'value\')\n    ndims = value.shape.ndims\n    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n\ndef positions_for(tokens, past_length):\n    batch_size = tf.shape(tokens)[0]\n    nsteps = tf.shape(tokens)[1]\n    return expand_tile(past_length + tf.range(nsteps), batch_size)\n\ndef model(hparams, X, past=None, scope=\'model\', reuse=False):\n    with tf.variable_scope(scope, reuse=reuse):\n        results = {}\n        batch, sequence = shape_list(X)\n\n        wpe = tf.get_variable(\'wpe\', [hparams.n_ctx, hparams.n_embd],\n                             initializer=tf.random_normal_initializer(stddev=0.01))\n        wte = tf.get_variable(\'wte\', [hparams.n_vocab, hparams.n_embd],\n                             initializer=tf.random_normal_initializer(stddev=0.02))\n        past_length = 0 if past is None else tf.shape(past)[-2]\n        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n\n        # Transformer\n        presents = []\n        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n        assert len(pasts) == hparams.n_layer\n        for layer, past in enumerate(pasts):\n            h, present = block(h, \'h%d\' % layer, past=past, hparams=hparams)\n            presents.append(present)\n        results[\'present\'] = tf.stack(presents, axis=1)\n        h = norm(h, \'ln_f\')\n\n        # Language model loss. Do tokens <n predict token n?\n        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n        logits = tf.matmul(h_flat, wte, transpose_b=True)\n        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n        results[\'logits\'] = logits\n        return results'"
build/lib/gpt2_client/model.py,53,"b'#!/usr/bin/env python3\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.training import HParams\n\ndef default_hparams():\n    return HParams(\n        n_vocab=0,\n        n_ctx=1024,\n        n_embd=768,\n        n_head=12,\n        n_layer=12,\n    )\n\ndef shape_list(x):\n    """"""Deal with dynamic shape in tensorflow cleanly.""""""\n    static = x.shape.as_list()\n    dynamic = tf.shape(x)\n    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n\ndef softmax(x, axis=-1):\n    x = x - tf.reduce_max(x, axis=axis, keepdims=True)\n    ex = tf.exp(x)\n    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)\n\ndef gelu(x):\n    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n\ndef norm(x, scope, axis=-1, epsilon=1e-5):\n    """"""Normalize to mean = 0, std = 1, then do a diagonal affine transform.""""""\n    with tf.variable_scope(scope):\n        n_state = x.shape[-1].value\n        g = tf.get_variable(\'g\', [n_state], initializer=tf.constant_initializer(1))\n        b = tf.get_variable(\'b\', [n_state], initializer=tf.constant_initializer(0))\n        u = tf.reduce_mean(x, axis=axis, keepdims=True)\n        s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n        x = (x - u) * tf.rsqrt(s + epsilon)\n        x = x*g + b\n        return x\n\ndef split_states(x, n):\n    """"""Reshape the last dimension of x into [n, x.shape[-1]/n].""""""\n    *start, m = shape_list(x)\n    return tf.reshape(x, start + [n, m//n])\n\ndef merge_states(x):\n    """"""Smash the last two dimensions of x into a single dimension.""""""\n    *start, a, b = shape_list(x)\n    return tf.reshape(x, start + [a*b])\n\ndef conv1d(x, scope, nf, w_init_stdev=0.02):\n    with tf.variable_scope(scope):\n        *start, nx = shape_list(x)\n        w = tf.get_variable(\'w\', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))\n        b = tf.get_variable(\'b\', [nf], initializer=tf.constant_initializer(0))\n        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n        return c\n\ndef attention_mask(nd, ns, dtype):\n    """"""1\'s in the lower triangle, counting from the lower right corner.\n\n    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn\'t produce garbage on TPUs.\n    """"""\n    i = tf.range(nd)[:,None]\n    j = tf.range(ns)\n    m = i >= j - ns + nd\n    return tf.cast(m, dtype)\n\n\ndef attn(x, scope, n_state, past, hparams):\n    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n    assert n_state % hparams.n_head == 0\n    if past is not None:\n        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n\n    def split_heads(x):\n        # From [batch, sequence, features] to [batch, heads, sequence, features]\n        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])\n\n    def merge_heads(x):\n        # Reverse of split_heads\n        return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n\n    def mask_attn_weights(w):\n        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n        _, _, nd, ns = shape_list(w)\n        b = attention_mask(nd, ns, dtype=w.dtype)\n        b = tf.reshape(b, [1, 1, nd, ns])\n        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n        return w\n\n    def multihead_attn(q, k, v):\n        # q, k, v have shape [batch, heads, sequence, features]\n        w = tf.matmul(q, k, transpose_b=True)\n        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n\n        w = mask_attn_weights(w)\n        w = softmax(w)\n        a = tf.matmul(w, v)\n        return a\n\n    with tf.variable_scope(scope):\n        c = conv1d(x, \'c_attn\', n_state*3)\n        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n        present = tf.stack([k, v], axis=1)\n        if past is not None:\n            pk, pv = tf.unstack(past, axis=1)\n            k = tf.concat([pk, k], axis=-2)\n            v = tf.concat([pv, v], axis=-2)\n        a = multihead_attn(q, k, v)\n        a = merge_heads(a)\n        a = conv1d(a, \'c_proj\', n_state)\n        return a, present\n\n\ndef mlp(x, scope, n_state, hparams):\n    with tf.variable_scope(scope):\n        nx = x.shape[-1].value\n        h = gelu(conv1d(x, \'c_fc\', n_state))\n        h2 = conv1d(h, \'c_proj\', nx)\n        return h2\n\n\ndef block(x, scope, past, hparams):\n    with tf.variable_scope(scope):\n        nx = x.shape[-1].value\n        a, present = attn(norm(x, \'ln_1\'), \'attn\', nx, past=past, hparams=hparams)\n        x = x + a\n        m = mlp(norm(x, \'ln_2\'), \'mlp\', nx*4, hparams=hparams)\n        x = x + m\n        return x, present\n\ndef past_shape(hparams, batch_size=None, sequence=None):\n    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n\ndef expand_tile(value, size):\n    """"""Add a new axis of given size.""""""\n    value = tf.convert_to_tensor(value, name=\'value\')\n    ndims = value.shape.ndims\n    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n\ndef positions_for(tokens, past_length):\n    batch_size = tf.shape(tokens)[0]\n    nsteps = tf.shape(tokens)[1]\n    return expand_tile(past_length + tf.range(nsteps), batch_size)\n\n\ndef model(hparams, X, past=None, scope=\'model\', reuse=False):\n    with tf.variable_scope(scope, reuse=reuse):\n        results = {}\n        batch, sequence = shape_list(X)\n\n        wpe = tf.get_variable(\'wpe\', [hparams.n_ctx, hparams.n_embd],\n                             initializer=tf.random_normal_initializer(stddev=0.01))\n        wte = tf.get_variable(\'wte\', [hparams.n_vocab, hparams.n_embd],\n                             initializer=tf.random_normal_initializer(stddev=0.02))\n        past_length = 0 if past is None else tf.shape(past)[-2]\n        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n\n        # Transformer\n        presents = []\n        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n        assert len(pasts) == hparams.n_layer\n        for layer, past in enumerate(pasts):\n            h, present = block(h, \'h%d\' % layer, past=past, hparams=hparams)\n            presents.append(present)\n        results[\'present\'] = tf.stack(presents, axis=1)\n        h = norm(h, \'ln_f\')\n\n        # Language model loss.  Do tokens <n predict token n?\n        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n        logits = tf.matmul(h_flat, wte, transpose_b=True)\n        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n        results[\'logits\'] = logits\n        return results'"
build/lib/gpt2_client/sample.py,17,"b""import tensorflow as tf\nimport model\n\ndef top_k_logits(logits, k):\n    if k == 0:\n        # no truncation\n        return logits\n\n    def _top_k():\n        values, _ = tf.nn.top_k(logits, k=k)\n        min_values = values[:, -1, tf.newaxis]\n        return tf.where(\n            logits < min_values,\n            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n            logits,\n        )\n    return tf.cond(\n       tf.equal(k, 0),\n       lambda: logits,\n       lambda: _top_k(),\n    )\n\n\ndef sample_sequence(hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0):\n    if start_token is None:\n        assert context is not None, 'Specify exactly one of start_token and context!'\n    else:\n        assert context is None, 'Specify exactly one of start_token and context!'\n        context = tf.fill([batch_size, 1], start_token)\n\n    def step(hparams, tokens, past=None):\n        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n\n        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n        presents = lm_output['present']\n        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))\n        return {\n            'logits': logits,\n            'presents': presents,\n        }\n\n    with tf.name_scope('sample_sequence'):\n        def body(past, prev, output):\n            next_outputs = step(hparams, prev, past=past)\n            logits = next_outputs['logits'][:, -1, :]  / tf.to_float(temperature)\n            logits = top_k_logits(logits, k=top_k)\n            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)\n            return [\n                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n                samples,\n                tf.concat([output, samples], axis=1)\n            ]\n\n        past, prev, output = body(None, context, context)\n\n        def cond(*args):\n            return True\n\n        _, _, tokens = tf.while_loop(\n            cond=cond, body=body,\n            maximum_iterations=length - 1,\n            loop_vars=[\n                past,\n                prev,\n                output\n            ],\n            shape_invariants=[\n                tf.TensorShape(model.past_shape(hparams=hparams, batch_size=batch_size)),\n                tf.TensorShape([batch_size, None]),\n                tf.TensorShape([batch_size, None]),\n            ],\n            back_prop=False,\n        )\n\n        return tokens"""
