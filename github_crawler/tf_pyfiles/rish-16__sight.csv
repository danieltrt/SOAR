file_path,api_count,code
main.py,0,"b'from sightseer import Sightseer\nfrom sightseer.zoo import YOLOv3Client\n\nyolo = YOLOv3Client()\nyolo.load_model()\n\nss = Sightseer()\nframes = ss.load_vidsource(""./test_data/img/london.mp4"")\nprint (frames.shape)\n\npreds, det_frames = yolo.framewise_predict(frames, stride=10, verbose=False)\nss.render_footage(det_frames)'"
setup.py,0,"b'from distutils.core import setup\nimport setuptools\nfrom os import path\n\nthis_directory = path.abspath(path.dirname(__file__))\nwith open(path.join(this_directory, \'README.md\')) as f:\n\tlong_description = f.read()\n\nsetup(\n  name=""sightseer"",\n  version=""1.1.3"",\n  description=""State-of-the-art Computer Vision and Object Detection for TensorFlow."",\n  long_description=long_description,\n  long_description_content_type=""text/markdown"",\n  url=\'https://github.com/rish-16/sight\',\n  download_url=""https://github.com/rish-16/sight/archive/1.0.0.tar.gz"",\n  author=""Rishabh Anand"",\n  author_email=""mail.rishabh.anand@gmail.com"",\n  license=""ASF"",\n  packages=[""sightseer""],\n  zip_safe=False\n)'"
sightseer/__init__.py,0,"b'# __init__.py\n__version__ = ""1.1.3""\n\nfrom sightseer.sightseer import Sightseer\nimport sightseer.zoo\nimport sightseer.proc'"
sightseer/blocks.py,0,"b'import struct\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, ZeroPadding2D, BatchNormalization, LeakyReLU, add\t\n\nclass BoundingBox(object):\n    def __init__(self, xmin, ymin, xmax, ymax, objectness=None, classes=None):\n        self.xmin = xmin\n        self.ymin = ymin\n        self.xmax = xmax\n        self.ymax = ymax\n\n        self.objectness = objectness\n        self.classes = classes\n\n        self.label = -1\n        self.confidence = -1\n\n    def get_label(self):\n        if self.label == -1:\n            self.label = np.argmax(self.classes)\n        return self.label\n\n    def get_confidence(self):\n        if self.confidence == -1:\n            self.confidence = self.classes[self.get_label()]\n        return self.confidence\t\t\n\nclass SightLoader():\n    def __init__(self, weights_path):\n        """"""\n        Weights loading framework for all Sight models\n        """"""\n        with open(weights_path, \'rb\') as wf:\n            major, = struct.unpack(\'i\', wf.read(4))\n            minor, = struct.unpack(\'i\', wf.read(4))\n            revision, = struct.unpack(\'i\', wf.read(4))\n\n            if (major*10+ minor) >= 2 and major < 1000 and minor < 1000:\n                wf.read(8)\n            else:\n                wf.read(4)\n\n            transpose = (major > 1000) or (minor > 1000)\n\n            binary = wf.read()\n\n        self.offset = 0\n        self.all_weights = np.frombuffer(binary, dtype=""float32"")\n\n    def read_bytes(self, chunk_size):\n        self.offset = self.offset + chunk_size\n        return self.all_weights[self.offset - chunk_size:self.offset]\n\n    def load_weights(self, model, verbose=True):\n        for i in range(106): # standard darknet layer count\n            try:\n                conv_layer = model.get_layer(""conv_"" + str(i))\n                \n                if verbose:\n                    print (""Loading Convolution #{}"".format(i))\n\n                if i not in [81, 93, 105]:\n                    norm_layer = model.get_layer(""bnorm_"" + str(i))\n\n                    size = np.prod(norm_layer.get_weights()[0].shape)\n\n                    beta = self.read_bytes(size)\n                    gamma = self.read_bytes(size)\n                    mean = self.read_bytes(size)\n                    var = self.read_bytes(size)\n\n                    weights = norm_layer.set_weights([gamma, beta, mean, var])\n\n                if len(conv_layer.get_weights()) > 1:\n                    bias = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n\n                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n                    kernel = kernel.transpose([2, 3, 1, 0])\n                    conv_layer.set_weights([kernel, bias])\n                else:\n                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n                    kernel = kernel.transpose([2, 3, 1, 0])\n                    conv_layer.set_weights([kernel])\n\n            except ValueError:\n                if verbose:\n                    print (""No Convolution #{}"".format(i))\n                else:\n                    pass\n\n        if verbose:\n            print (""Finished loading weights into model. Predicting on input data..."")\n\n    def reset_offset(self):\n        self.offset = 0\t\t\n\nclass Layer():\n\tdef get_conv_block(inp, convs, skip=True):\n\t\tx = inp\n\t\tcount = 0\n\n\t\tfor conv in convs:\n\t\t\tif count == (len(convs) - 2) and skip:\n\t\t\t\tskip_conn = x\n\t\t\tcount += 1\n\n\t\t\tif conv[\'stride\'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x)\n\t\t\t\n\t\t\tx = Conv2D(conv[\'filter\'], \n\t\t\t\t\tconv[\'kernel\'], \n\t\t\t\t\tstrides=conv[\'stride\'], \n\t\t\t\t\tpadding=""valid"" if conv[\'stride\']>1 else ""same"", \n\t\t\t\t\tname=""conv_""+str(conv[\'layer_idx\']), \n\t\t\t\t\tuse_bias=False if conv[\'bnorm\'] else True)(x)\n\t\t\t\n\t\t\tif conv[\'bnorm\']: x = BatchNormalization(epsilon=0.001, name=""bnorm_""+str(conv[\'layer_idx\']))(x)\n\t\t\t\n\t\t\tif conv[\'leaky\']: x = LeakyReLU(alpha=0.1, name=""leaky_""+str(conv[\'layer_idx\']))(x)\n\n\t\treturn add([skip_conn, x]) if skip else x'"
sightseer/proc.py,2,"b""import io\nimport json\nimport glob\nimport os\nfrom PIL import Image\nimport xml.etree.ElementTree as ET\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport pandas as pd\n\nclass DataAnnotator(object):\n\tdef __init__(self, classes):\n\t\tself.classes = classes # array of class labels\n\t\t\n\tdef list_to_csv(self, annotations, outfile):\n\t\tcolumns = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n\t\txml_df = pd.DataFrame(annotations, columns=columns)\n\t\txml_df.to_csv(outfile, index=None)\n\n\tdef class_to_int(self, class_label):\n\t\tfor i in range(len(self.classes)):\n\t\t\tif self.classes[i] == class_label:\n\t\t\t\treturn i + 1\n\t\t\telse:\n\t\t\t\treturn None\n\n\tdef xml_to_csv(self, xml_path, csv_path):\n\t\tannotations = []\n\t\tfor xml_file in glob.glob(xml_path + '*.xml'):\n\t\t\ttree = ET.parse(xml_file)\n\t\t\troot = tree.getroot()\n\t\t\tfor member in root.findall('object'):\n\t\t\t\tvalue = (root.find('filename').text,\n\t\t\t\t\t\t int(root.find('size')[0].text), int(root.find('size')[1].text), \n\t\t\t\t\t\t member[0].text,\n\t\t\t\t\t\t int(member[4][0].text), int(member[4][1].text),\n\t\t\t\t\t\t int(member[4][2].text), int(member[4][3].text))\n\t\t\t\tannotations.append(value)\n\n\t\tself.list_to_csv(annotations, csv_path)\n\n\tdef json_to_csv(self, jsonpath, csvpath):\n\t\twith open(jsonpath) as f:\n\t\t\timages = json.load(f)\n\n\t\tannotations = []\n\n\t\tfor entry in images:\n\t\t\tfilename = images[entry]['filename']\n\t\t\tfor region in images[entry]['regions']:\n\t\t\t\tc = region['region_attributes']['class']\n\t\t\t\txmin = region['shape_attributes']['x']\n\t\t\t\tymin = region['shape_attributes']['y']\n\t\t\t\txmax = xmin + region['shape_attributes']['width']\n\t\t\t\tymax = ymin + region['shape_attributes']['height']\n\t\t\t\twidth = 0\n\t\t\t\theight = 0\n\n\t\t\t\tvalue = (filename, width, height, c, xmin, ymin, xmax, ymax)\n\t\t\t\tannotations.append(value)\n\n\t\tself.list_to_csv(annotations, csvpath)\n\n\tdef generate_tfexample(self, group, path):\n\t\twith tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n\t\t\tencoded_jpg = fid.read()\n\t\tencoded_jpg_io = io.BytesIO(encoded_jpg)\n\t\timage = Image.open(encoded_jpg_io)\n\t\twidth, height = image.size\n\n\t\tfilename = group.filename.encode('utf8')\n\t\timage_format = b'jpg'\n\t\txmins = []\n\t\txmaxs = []\n\t\tymins = []\n\t\tymaxs = []\n\t\tclasses_text = []\n\t\tclasses = []\n\n\t\tfor index, row in group.object.iterrows():\n\t\t\txmins.append(row['xmin'] / width)\n\t\t\txmaxs.append(row['xmax'] / width)\n\t\t\tymins.append(row['ymin'] / height)\n\t\t\tymaxs.append(row['ymax'] / height)\n\t\t\tclasses_text.append(row['class'].encode('utf8'))\n\t\t\tclasses.append(self.class_to_int(row['class']))\n\n\t\ttf_example = tf.train.Example(features=tf.train.Features(feature={\n\t\t\t'image/height': dataset_util.int64_feature(height),\n\t\t\t'image/width': dataset_util.int64_feature(width),\n\t\t\t'image/filename': dataset_util.bytes_feature(filename),\n\t\t\t'image/source_id': dataset_util.bytes_feature(filename),\n\t\t\t'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n\t\t\t'image/format': dataset_util.bytes_feature(image_format),\n\t\t\t'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n\t\t\t'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n\t\t\t'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n\t\t\t'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n\t\t\t'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n\t\t\t'image/object/class/label': dataset_util.int64_list_feature(classes),\n\t\t}))\n\t\t\n\t\treturn tf_example\t\t\n\n\tdef csv_to_tfrecord(self, csvpath, filename, tfrpath):\n\t\tcsv = pd.read_csv(csvpath).values\n"""
sightseer/sightseer.py,0,"b'import cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\n# need to find Linux-friendly alternative\n# from PIL import ImageGrab\n\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport matplotlib.animation as animation\n\nclass Sightseer(object):\n\tdef __init__(self):\n\t\tself.filepath = None\n\n\tdef render_grayscale(self, frame):\n\t\tgray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\t\treturn gray_frame\n\n\t# Experimental\n\tdef load_webcam(self, return_data=True, set_gray=True, kill_key=""q"", width=160, height=120):\n\n\t\tcap = cv2.VideoCapture(0)\n\t\tcap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n\t\tcap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n\n\t\tframes = []\n\n\t\twhile True:\n\t\t\tret, frame = cap.read()\n\t\t\tprint (frame.shape)\n\n\t\t\tif set_gray:\n\t\t\t\tframe = self.render_grayscale(frame)\n\t\t\t\n\t\t\tframe = cv2.flip(frame, 1) # prevent lateral inversion\n\t\t\tcv2.imshow(\'frame\', frame)\n\t\t\tframes.append(frame)\n\n\t\t\tif cv2.waitKey(1) & 0xFF == ord(kill_key):\n\t\t\t\tbreak\n\n\t\tcap.release()\n\t\tcv2.destroyAllWindows()\t\n\n\t\tif return_data:\n\t\t\tframes = np.array(frames)\n\t\t\treturn frames\n\t\t\n\t# Experimental\n\t# def screen_grab(self, set_gray=True, write_data=True, return_data=True, kill_key=""q"", filename=\'output.avi\', width=400, height=400):\n\t# \tfourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n\t# \tout = cv2.VideoWriter(filename, fourcc, 20.0, (640, 480))\n\n\t# \tframes = []\n\n\t# \twhile True:\n\t# \t\timg = np.array(ImageGrab.grab(bbox=(0, 0, width, height)))\n\t# \t\tframe = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n\n\t# \t\tif write_data:\n\t# \t\t\tout.write(imcv)\n\n\t# \t\tif set_gray:\n\t# \t\t\tframe = self.render_grayscale(img)\n\n\t# \t\tcv2.imshow(\'frame\', frame)\n\t# \t\tframes.append(frame)\n\n\t# \t\tif cv2.waitKey(1) & 0xFF == ord(kill_key):\n\t# \t\t\tbreak\n\n\t# \tout.release()\n\t# \tcv2.destroyAllWindows()\n\n\t# \tif return_data:\n\t# \t\tframes = np.array(frames)\n\t# \t\treturn frames\n\n\tdef load_vidsource(self, filepath, stride=5, return_data=True, set_gray=False):\n\t\tself.filepath = filepath\n\t\tvidcap = cv2.VideoCapture(filepath)\n\t\t\n\t\tframes = []\n\t\tcur_frame = 0\n\n\t\tprint (""Extracting frames from video..."")\n\n\t\twhile vidcap.isOpened():\n\t\t\tframe_exists, frame = vidcap.read()\n\n\t\t\tif frame_exists == False:\n\t\t\t\tbreak\n\n\t\t\tif set_gray:\n\t\t\t\tframe = self.render_grayscale(frame)\n\n\t\t\tframes.append(frame)\n\n\t\t\tcur_frame += 1\n\n\t\tprint (\'Number of frames: {}\'.format(len(frames)))\n\t\t\n\t\tvidcap.release()\n\t\tcv2.destroyAllWindows()\n\t\t\n\t\tif return_data:\n\t\t\tframes = np.array(frames)\n\t\t\treturn frames\n\n\tdef load_image(self, filepath):\n\t\tself.filepath = filepath\n\t\ttry:\n\t\t\timg = cv2.imread(filepath)\n\t\t\treturn img\n\t\texcept:\n\t\t\traise FileExistsError (""File does not exist. You may want to check the filepath again."")\n\n\tdef get_final_filepath(self, image_path):\n\t\timage_path = image_path.split(\'/\')\n\t\timg_name = image_path[-1]\n\t\timg_name = img_name.split(\'.\')\n\t\timg_name = img_name[0] + ""_preds."" + img_name[-1]\n\t\timage_path = ""/"".join(image_path[:-1]) + ""/"" + img_name\n\n\t\treturn image_path\t\n\t\n\tdef render_image(self, image, save_image=False):\n\t\tplt.imshow(image)\n\t\tplt.show()\n\n\t\tif save_image:\n\t\t\tnew_filepath = self.get_final_filepath(self.filepath)\n\t\t\tplt.savefig(new_filepath)\n\t\n\tdef render_footage(self, frames, save_footage=True):\n\t\tfig = plt.figure()\n\t\tfinal_frames = []\n\n\t\tfor i in range(len(frames)):\n\t\t\tfinal_frames.append([plt.imshow(frames[i], animated=True)])\n\t\t\n\t\tani = animation.ArtistAnimation(fig, final_frames, interval=50, blit=True, repeat_delay=1000)\n\n\t\tif save_footage:\n\t\t\tfinal_filename = self.get_final_filepath(self.filepath)\n\t\t\tani.save(final_filename)\n\n\t\tplt.show()'"
sightseer/zoo.py,3,"b'import os\nimport wget\nimport struct\nimport shutil\nimport logging\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, UpSampling2D, concatenate\nfrom tensorflow.keras.models import Model, load_model\n\nfrom .blocks import Layer, BoundingBox, SightLoader\n\n# disabling warnings and logging\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\ntf.autograph.set_verbosity(tf.compat.v1.logging.ERROR)\nlogging.disable(logging.WARNING)\n\nclass YOLOv3Client(object):\n\tdef __init__(self, nms_threshold=0.45, obj_threshold=0.5, net_h=416, net_w=416, anchors=[[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]):\n\t\tself.nms_threshold = nms_threshold\n\t\tself.obj_threshold = obj_threshold\n\t\tself.net_h, self.net_w = net_h, net_w\n\t\tself.anchors = anchors\n\t\tself.yolo_model = None # initialised after weights are loaded into model\n\t\tself.weights_url = ""https://pjreddie.com/media/files/yolov3.weights""\n\t\tself.all_labels = [""person"", ""bicycle"", ""car"", ""motorbike"", ""aeroplane"", ""bus"", ""train"", ""truck"",\n\t\t\t\t\t\t""boat"", ""traffic light"", ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"",\n\t\t\t\t\t\t""bird"", ""cat"", ""dog"", ""horse"", ""sheep"", ""cow"", ""elephant"", ""bear"", ""zebra"", ""giraffe"",\n\t\t\t\t\t\t""backpack"", ""umbrella"", ""handbag"", ""tie"", ""suitcase"", ""frisbee"", ""skis"", ""snowboard"",\n\t\t\t\t\t\t""sports ball"", ""kite"", ""baseball bat"", ""baseball glove"", ""skateboard"", ""surfboard"",\n\t\t\t\t\t\t""tennis racket"", ""bottle"", ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"", ""bowl"", ""banana"",\n\t\t\t\t\t\t""apple"", ""sandwich"", ""orange"", ""broccoli"", ""carrot"", ""hot dog"", ""pizza"", ""donut"", ""cake"",\n\t\t\t\t\t\t""chair"", ""sofa"", ""pottedplant"", ""bed"", ""diningtable"", ""toilet"", ""tvmonitor"", ""laptop"", ""mouse"",\n\t\t\t\t\t\t""remote"", ""keyboard"", ""cell phone"", ""microwave"", ""oven"", ""toaster"", ""sink"", ""refrigerator"",\n\t\t\t\t\t\t""book"", ""clock"", ""vase"", ""scissors"", ""teddy bear"", ""hair drier"", ""toothbrush""]\n\t\t\t\t\t\t\n\tdef download_weights(self):\n\t\t""""""\n\t\tDownloads the weights from online and saves them locally\n\t\t""""""\n\n\t\tif os.path.exists(""./bin/yolov3.weights""):\n\t\t\tprint (""Weights already exist. Proceeding to load YOLOv3Client..."")\n\t\telse:\n\t\t\tprint (""Downloading weights. This may take a moment..."")\n\t\n\t\t\twget.download(self.weights_url, os.getcwd() + ""/yolov3.weights"")\n\n\t\t\tos.mkdir(""./bin"", 0o755) # configuring admin rights\n\t\t\tshutil.move(""./yolov3.weights"", ""./bin/yolov3.weights"")\n\n\t\t\tprint (""\\n\\nWeights downloaded successfully!"")\n\n\tdef load_architecture(self):\n\t\t""""""\n\t\tReturns a tf.keras.models.Model instance\n\t\t""""""\n\t\tinp_image = Input(shape=[None, None, 3])\n\n\t\tx = Layer.get_conv_block(inp_image, [{\'filter\': 32, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 0},\n\t\t\t\t\t\t\t\t\t\t{\'filter\': 64, \'kernel\': 3, \'stride\': 2, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 1},\n\t\t\t\t\t\t\t\t\t\t{\'filter\': 32, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 2},\n\t\t\t\t\t\t\t\t\t\t{\'filter\': 64, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 3}])\n\n\t\tx = Layer.get_conv_block(x, [{\'filter\': 128, \'kernel\': 3, \'stride\': 2, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 5},\n\t\t\t\t\t\t\t{\'filter\':  64, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 6},\n\t\t\t\t\t\t\t{\'filter\': 128, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 7}])\n\n\t\tx = Layer.get_conv_block(x, [{\'filter\':  64, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 9},\n\t\t\t\t\t\t\t{\'filter\': 128, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 10}])\n\n\t\tx = Layer.get_conv_block(x, [{\'filter\': 256, \'kernel\': 3, \'stride\': 2, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 12},\n\t\t\t\t\t\t\t{\'filter\': 128, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 13},\n\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 14}])\n\n\t\tfor i in range(7):\n\t\t\tx = Layer.get_conv_block(x, [{\'filter\': 128, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 16+i*3},\n\t\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 17+i*3}])\n\t\t\t\n\t\tskip_36 = x\n\t\t\t\n\t\tx = Layer.get_conv_block(x, [{\'filter\': 512, \'kernel\': 3, \'stride\': 2, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 37},\n\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 38},\n\t\t\t\t\t\t\t{\'filter\': 512, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 39}])\n\n\t\tfor i in range(7):\n\t\t\tx = Layer.get_conv_block(x, [{\'filter\': 256, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 41+i*3},\n\t\t\t\t\t\t\t\t{\'filter\': 512, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 42+i*3}])\n\t\t\t\n\t\tskip_61 = x\n\t\t\t\n\t\tx = Layer.get_conv_block(x, [{\'filter\': 1024, \'kernel\': 3, \'stride\': 2, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 62},\n\t\t\t\t\t\t\t{\'filter\':  512, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 63},\n\t\t\t\t\t\t\t{\'filter\': 1024, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 64}])\n\n\t\tfor i in range(3):\n\t\t\tx = Layer.get_conv_block(x, [{\'filter\':  512, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 66+i*3},\n\t\t\t\t\t\t\t\t{\'filter\': 1024, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 67+i*3}])\n\t\t\t\n\t\tx = Layer.get_conv_block(x, [{\'filter\':  512, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 75},\n\t\t\t\t\t\t\t{\'filter\': 1024, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 76},\n\t\t\t\t\t\t\t{\'filter\':  512, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 77},\n\t\t\t\t\t\t\t{\'filter\': 1024, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 78},\n\t\t\t\t\t\t\t{\'filter\':  512, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 79}], skip=False)\n\n\t\tyolo_82 = Layer.get_conv_block(x, [{\'filter\': 1024, \'kernel\': 3, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 80},\n\t\t\t\t\t\t\t\t\t{\'filter\':  255, \'kernel\': 1, \'stride\': 1, \'bnorm\': False, \'leaky\': False, \'layer_idx\': 81}], skip=False)\n\n\t\tx = Layer.get_conv_block(x, [{\'filter\': 256, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 84}], skip=False)\n\t\tx = UpSampling2D(2)(x)\n\t\tx = concatenate([x, skip_61])\n\n\t\tx = Layer.get_conv_block(x, [{\'filter\': 256, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 87},\n\t\t\t\t\t\t\t{\'filter\': 512, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 88},\n\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 89},\n\t\t\t\t\t\t\t{\'filter\': 512, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 90},\n\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 91}], skip=False)\n\n\t\tyolo_94 = Layer.get_conv_block(x, [{\'filter\': 512, \'kernel\': 3, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 92},\n\t\t\t\t\t\t\t\t\t{\'filter\': 255, \'kernel\': 1, \'stride\': 1, \'bnorm\': False, \'leaky\': False, \'layer_idx\': 93}], skip=False)\n\n\t\tx = Layer.get_conv_block(x, [{\'filter\': 128, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True,   \'layer_idx\': 96}], skip=False)\n\t\tx = UpSampling2D(2)(x)\n\t\tx = concatenate([x, skip_36])\n\n\t\tyolo_106 = Layer.get_conv_block(x, [{\'filter\': 128, \'kernel\': 1, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 99},\n\t\t\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 3, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 100},\n\t\t\t\t\t\t\t\t\t{\'filter\': 128, \'kernel\': 1, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 101},\n\t\t\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 3, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 102},\n\t\t\t\t\t\t\t\t\t{\'filter\': 128, \'kernel\': 1, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 103},\n\t\t\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 3, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 104},\n\t\t\t\t\t\t\t\t\t{\'filter\': 255, \'kernel\': 1, \'stride\': 1, \'bnorm\': False, \'leaky\': False, \'layer_idx\': 105}], skip=False)\n\n\t\tmodel = Model(inp_image, [yolo_82, yolo_94, yolo_106])    \n\t\treturn model\n\n\tdef sigmoid(self, z):\n\t\treturn 1 / (1 + np.exp(-z))\n\n\tdef preprocess(self, image):\n\t\t""""""\n\t\tResizes image to appropriate dimensions for YOLOv3\n\t\t""""""\n\t\tnew_h, new_w = image.shape[:2]\n\n\t\tif (float(self.net_w)/new_w) < (float(self.net_h)/new_h):\n\t\t\tnew_h = (new_h * self.net_w)//new_w\n\t\t\tnew_w = self.net_w\n\t\telse:\n\t\t\tnew_w = (new_w * self.net_h)//new_h\n\t\t\tnew_h = self.net_h        \n\n\t\t# resize the image to the new size\n\t\tresized = cv2.resize(image[:, :, ::-1]/255., (int(new_w), int(new_h)))\n\n\t\t# embed the image into the standard template\n\t\tnew_img = np.ones((self.net_h, self.net_w, 3)) * 0.5\n\t\tnew_img[int((self.net_h-new_h)//2):int((self.net_h+new_h)//2), int((self.net_w-new_w)//2):int((self.net_w+new_w)//2), :] = resized\n\t\tnew_img = np.expand_dims(new_img, 0)\n\n\t\treturn new_img\n\n\tdef interval_overlap(self, int_a, int_b):\n\t\tx1, x2 = int_a\n\t\tx3, x4 = int_b\n\n\t\tif x3 < x1:\n\t\t\tif x4 < x1:\n\t\t\t\treturn 0\n\t\t\telse:\n\t\t\t\treturn min(x2, x4) - x1\n\t\telse:\n\t\t\tif x2 < x3:\n\t\t\t\treturn 0\n\t\t\telse:\n\t\t\t\treturn min(x2, x4) - x3\t\n\n\tdef bbox_iou(self, box1, box2):\n\t\t""""""\n\t\tFinds IOU between all bounding boxes before non maximum suppression process\n\t\t""""""\n\t\tint_w = self.interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n\t\tint_h = self.interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n\n\t\tintersect = int_w * int_h\n\n\t\tw1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin\n\t\tw2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin\n\n\t\tunion = w1*h1 + w2*h2 - intersect\n\n\t\treturn float(intersect) / union\t\t\n\n\tdef non_maximum_suppression(self, boxes):\n\t\tif len(boxes) > 0:\n\t\t\tnb_class = len(boxes[0].classes)\n\t\telse:\n\t\t\treturn\n\t\t\t\n\t\tfor c in range(nb_class):\n\t\t\tsorted_indices = np.argsort([-box.classes[c] for box in boxes])\n\n\t\t\tfor i in range(len(sorted_indices)):\n\t\t\t\tindex_i = sorted_indices[i]\n\n\t\t\t\tif boxes[index_i].classes[c] == 0: continue\n\n\t\t\t\tfor j in range(i+1, len(sorted_indices)):\n\t\t\t\t\tindex_j = sorted_indices[j]\n\n\t\t\t\t\tif self.bbox_iou(boxes[index_i], boxes[index_j]) >= self.nms_threshold:\n\t\t\t\t\t\tboxes[index_j].classes[c] = 0\n\n\t\treturn boxes\n\n\tdef decode_preds(self, preds, anchors):\n\t\tgridh, gridw = preds.shape[:2]\n\t\tnb_box = 3\n\t\tpreds = preds.reshape([gridh, gridw, nb_box, -1])\n\t\tnb_class = preds.shape[-1] - 5\n\n\t\tboxes = []\n\t\t\n\t\tpreds[..., :2]  = self.sigmoid(preds[..., :2])\n\t\tpreds[..., 4:]  = self.sigmoid(preds[..., 4:])\n\t\tpreds[..., 5:]  = preds[..., 4][..., np.newaxis] * preds[..., 5:]\n\t\tpreds[..., 5:] *= preds[..., 5:] > self.obj_threshold\n\n\t\tfor i in range(gridh * gridw):\n\t\t\trow = i / gridw\n\t\t\tcol = i % gridw\n\n\t\t\tfor b in range(nb_box):\n\t\t\t\tobjectness = preds[int(row)][int(col)][b][4]\n\n\t\t\t\tif (objectness.all() <= self.obj_threshold): continue\n\n\t\t\t\tx, y, w, h = preds[int(row)][int(col)][b][:4]\n\n\t\t\t\tx = (col + x) / gridw \n\t\t\t\ty = (row + y) / gridh \n\t\t\t\tw = anchors[2 * b + 0] * np.exp(w) / self.net_w\n\t\t\t\th = anchors[2 * b + 1] * np.exp(h) / self.net_h\n\n\t\t\t\tclasses = preds[int(row)][col][b][5:]\n\n\t\t\t\tbox = BoundingBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n\n\t\t\t\tboxes.append(box)\n\n\t\treturn boxes\n\n\tdef rectify_boxes(self, boxes, image_h, image_w):\n\t\tif (float(self.net_w)/image_w) < (float(self.net_h)/image_h):\n\t\t\tnew_w = self.net_w\n\t\t\tnew_h = (image_h * self.net_w)/ image_w\n\t\telse:\n\t\t\tnew_h = self.net_w\n\t\t\tnew_w = (image_w * self.net_h) / image_h\n\t\t\t\n\t\tfor i in range(len(boxes)):\n\t\t\tx_offset, x_scale = (self.net_w - new_w)/2./self.net_w, float(new_w)/self.net_w\n\t\t\ty_offset, y_scale = (self.net_h - new_h)/2./self.net_h, float(new_h)/self.net_h\n\t\t\t\n\t\t\tboxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n\t\t\tboxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n\t\t\tboxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n\t\t\tboxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n\n\t\treturn boxes\n\n\tdef get_boxes(self, image, boxes, verbose=True, random_coloring=True):\n\t\tfinal_boxes = []\n\n\t\tfor box in boxes:\n\t\t\tfinal_label = """"\n\t\t\tlabel = -1\n\n\t\t\tfor i in range(len(self.all_labels)):\n\t\t\t\tif box.classes[i] > self.obj_threshold:\n\t\t\t\t\tfinal_label += self.all_labels[i]\n\t\t\t\t\tlabel = i\n\n\t\t\t\t\tif verbose:\n\t\t\t\t\t\tprint (""{}: {:.3f}%"".format(self.all_labels[i], box.classes[i]*100))\n\n\t\t\t\t\tfinal_boxes.append([final_label,\n\t\t\t\t\t\t\t\t\t\tbox.classes[i] * 100,\n\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\t\'xmin\': box.xmin,\n\t\t\t\t\t\t\t\t\t\t\t\'ymin\': box.ymin,\n\t\t\t\t\t\t\t\t\t\t\t\'xmax\': box.xmax,\n\t\t\t\t\t\t\t\t\t\t\t\'ymax\': box.ymax\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t])\n\n\t\t\tif label >= 0:\n\t\t\t\tif random_coloring:\n\t\t\t\t\tr, g, b = np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255)\n\t\t\t\telse:\n\t\t\t\t\tr, g, b = 0, 255, 0\n\n\t\t\t\tcv2.rectangle(image, (box.xmin, box.ymin), (box.xmax, box.ymax), (r, g, b), 2)\n\t\t\t\tcv2.putText(image, \'{} {:.3f}\'.format(final_label, box.get_confidence()), (box.xmax, box.ymin - 13), cv2.FONT_HERSHEY_SIMPLEX, 1e-3 * image.shape[0], (r, g, b), 2)\n\n\t\treturn final_boxes, image\n\n\tdef load_model(self, default_path=""./bin/yolov3.weights"", cache=True, verbose=True):\n\t\t""""""\n\t\tDownloads weights and config, loads checkpoints into architecture\n\t\t""""""\n\t\tif os.path.exists(""./bin/yolov3.h5""):\n\t\t\tprint (""Weights already exist. Proceeding to load YOLOv3Client..."")\n\t\t\tself.yolo_model = load_model(""./bin/yolov3.h5"")\n\t\telse:\n\t\t\tself.download_weights() # downloading weights from online\n\t\t\tloader = SightLoader(default_path)\n\t\t\t\n\t\t\tself.yolo_model = self.load_architecture() # loading weights into model\n\t\t\tloader.load_weights(self.yolo_model, verbose)\n\n\t\t\tif cache:\n\t\t\t\tself.yolo_model.save(""./bin/yolov3.h5"") # saves .h5 weights file\n\t\t\t\tos.remove(""./bin/yolov3.weights"") # removes original .weights file\n\n\tdef predict(self, original_image, return_img=False, verbose=True):\n\t\t""""""\n\t\tReturns a list of BoundingBox metadata (class label, confidence score, coordinates)\n\t\tand the edited image with bounding boxes and their corresponding text labels/confidence scores\n\t\t""""""\n\t\timage_h, image_w = original_image.shape[:2]\n\n\t\tif self.yolo_model == None:\n\t\t\traise ValueError (""YOLOv3 weights needs to be downloaded and configured into the model before use. You can use the `load_model()` method to do so."")\n\n\t\tproc_image = self.preprocess(original_image)\n\t\tpreds = self.yolo_model.predict(proc_image)\n\t\tboxes = []\n\n\t\tfor i in range(len(preds)):\n\t\t\tboxes += self.decode_preds(preds[i][0], self.anchors[i])\n\n\t\tboxes = self.rectify_boxes(boxes, image_h, image_w)\n\t\tboxes = self.non_maximum_suppression(boxes)\n\n\t\tbox_list, box_image = self.get_boxes(original_image, boxes, verbose)\n\n\t\tif return_img:\n\t\t\tbox_image = box_image.squeeze()\n\t\t\treturn box_list, box_image\n\t\telse:\n\t\t\treturn box_list\n\n\tdef framewise_predict(self, frames, stride=1, verbose=True):\n\t\tfinal_preds = []\n\t\tfinal_frames = []\n\n\t\tfor i in range(len(frames)):\n\t\t\tif i % stride == 0:\n\t\t\t\tif verbose:\n\t\t\t\t\tprint (""Frame {}"".format(i))\n\n\t\t\t\tcur_preds, edited_frame = self.predict(frames[i], return_img=True, verbose=False)\n\n\t\t\t\tfinal_preds.append(cur_preds)\n\t\t\t\tfinal_frames.append(edited_frame)\n\n\t\treturn final_preds, final_frames\n\nclass TinyYOLOClient(object):\n\tdef __init__(self):\n\t\tself.tiny_yolo_model = None\n\t\tself.weights_url = ""https://pjreddie.com/media/files/yolov3-tiny.weights""\n\n\tdef download_weights(self):\n\t\t""""""\n\t\tDownloads the weights from online and saves them locally\n\t\t""""""\n\n\t\tif os.path.exists(""./bin/tiny_yolo.h5""):\n\t\t\tprint (""Weights already exist. Proceeding to load TinyYOLOClient..."")\n\t\telse:\n\t\t\tprint (""Downloading weights. This may take a moment..."")\n\t\n\t\t\twget.download(self.weights_url, os.getcwd() + ""/tiny_yolo.weights"")\n\n\t\t\tif not os.path.exists(""./bin""):\n\t\t\t\tos.mkdir(""./bin"", 0o755) # configuring admin rights\n\t\t\t\n\t\t\tshutil.move(""./tiny_yolo.weights"", ""./bin/tiny_yolo.weights"")\n\n\t\t\tprint (""\\n\\nWeights downloaded successfully!"")\n\n\tdef load_architecture(self):\n\t\t""""""\n\t\tReturns a tf.keras.models.Model instance\n\t\t""""""\n\n\t\tinp_image = Image(shape=[None, None, 3])\n\n\tdef load_model(self):\n\t\tif os.path.exists(""./bin/tiny_yolo.h5""):\n\t\t\tprint (""Weights already exist. Proceeding to load TinyYOLOClient..."")\n\t\t\tself.tiny_yolo_model = load_model(""./bin/tiny_yolo.h5"")\n\t\telse:\n\t\t\tself.download_weights()\t'"
build/lib/sightseer/__init__.py,0,"b'# __init__.py\n__version__ = ""1.1.3""\n\nfrom sightseer.sightseer import Sightseer\nimport sightseer.zoo\nimport sightseer.proc'"
build/lib/sightseer/blocks.py,0,"b'import struct\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, ZeroPadding2D, BatchNormalization, LeakyReLU, add\t\n\nclass BoundingBox(object):\n    def __init__(self, xmin, ymin, xmax, ymax, objectness=None, classes=None):\n        self.xmin = xmin\n        self.ymin = ymin\n        self.xmax = xmax\n        self.ymax = ymax\n\n        self.objectness = objectness\n        self.classes = classes\n\n        self.label = -1\n        self.confidence = -1\n\n    def get_label(self):\n        if self.label == -1:\n            self.label = np.argmax(self.classes)\n        return self.label\n\n    def get_confidence(self):\n        if self.confidence == -1:\n            self.confidence = self.classes[self.get_label()]\n        return self.confidence\t\t\n\nclass SightLoader():\n    def __init__(self, weights_path):\n        """"""\n        Weights loading framework for all Sight models\n        """"""\n        with open(weights_path, \'rb\') as wf:\n            major, = struct.unpack(\'i\', wf.read(4))\n            minor, = struct.unpack(\'i\', wf.read(4))\n            revision, = struct.unpack(\'i\', wf.read(4))\n\n            if (major*10+ minor) >= 2 and major < 1000 and minor < 1000:\n                wf.read(8)\n            else:\n                wf.read(4)\n\n            transpose = (major > 1000) or (minor > 1000)\n\n            binary = wf.read()\n\n        self.offset = 0\n        self.all_weights = np.frombuffer(binary, dtype=""float32"")\n\n    def read_bytes(self, chunk_size):\n        self.offset = self.offset + chunk_size\n        return self.all_weights[self.offset - chunk_size:self.offset]\n\n    def load_weights(self, model, verbose=True):\n        for i in range(106): # standard darknet layer count\n            try:\n                conv_layer = model.get_layer(""conv_"" + str(i))\n                \n                if verbose:\n                    print (""Loading Convolution #{}"".format(i))\n\n                if i not in [81, 93, 105]:\n                    norm_layer = model.get_layer(""bnorm_"" + str(i))\n\n                    size = np.prod(norm_layer.get_weights()[0].shape)\n\n                    beta = self.read_bytes(size)\n                    gamma = self.read_bytes(size)\n                    mean = self.read_bytes(size)\n                    var = self.read_bytes(size)\n\n                    weights = norm_layer.set_weights([gamma, beta, mean, var])\n\n                if len(conv_layer.get_weights()) > 1:\n                    bias = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n\n                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n                    kernel = kernel.transpose([2, 3, 1, 0])\n                    conv_layer.set_weights([kernel, bias])\n                else:\n                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n                    kernel = kernel.transpose([2, 3, 1, 0])\n                    conv_layer.set_weights([kernel])\n\n            except ValueError:\n                if verbose:\n                    print (""No Convolution #{}"".format(i))\n                else:\n                    pass\n\n        if verbose:\n            print (""Finished loading weights into model. Predicting on input data..."")\n\n    def reset_offset(self):\n        self.offset = 0\t\t\n\nclass ConvBlock():\n\tdef get_conv_block(inp, convs, skip=True):\n\t\tx = inp\n\t\tcount = 0\n\n\t\tfor conv in convs:\n\t\t\tif count == (len(convs) - 2) and skip:\n\t\t\t\tskip_conn = x\n\t\t\tcount += 1\n\n\t\t\tif conv[\'stride\'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x)\n\t\t\t\n\t\t\tx = Conv2D(conv[\'filter\'], \n\t\t\t\t\tconv[\'kernel\'], \n\t\t\t\t\tstrides=conv[\'stride\'], \n\t\t\t\t\tpadding=""valid"" if conv[\'stride\']>1 else ""same"", \n\t\t\t\t\tname=""conv_""+str(conv[\'layer_idx\']), \n\t\t\t\t\tuse_bias=False if conv[\'bnorm\'] else True)(x)\n\t\t\t\n\t\t\tif conv[\'bnorm\']: x = BatchNormalization(epsilon=0.001, name=""bnorm_""+str(conv[\'layer_idx\']))(x)\n\t\t\t\n\t\t\tif conv[\'leaky\']: x = LeakyReLU(alpha=0.1, name=""leaky_""+str(conv[\'layer_idx\']))(x)\n\n\t\treturn add([skip_conn, x]) if skip else x'"
build/lib/sightseer/proc.py,2,"b""import io\nimport json\nimport glob\nimport os\nfrom PIL import Image\nimport xml.etree.ElementTree as ET\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport pandas as pd\n\nclass DataAnnotator(object):\n\tdef __init__(self, classes):\n\t\tself.classes = classes # array of class labels\n\t\t\n\tdef list_to_csv(self, annotations, outfile):\n\t\tcolumns = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n\t\txml_df = pd.DataFrame(annotations, columns=columns)\n\t\txml_df.to_csv(outfile, index=None)\n\n\tdef class_to_int(self, class_label):\n\t\tfor i in range(len(self.classes)):\n\t\t\tif self.classes[i] == class_label:\n\t\t\t\treturn i + 1\n\t\t\telse:\n\t\t\t\treturn None\n\n\tdef xml_to_csv(self, xml_path, csv_path):\n\t\tannotations = []\n\t\tfor xml_file in glob.glob(xml_path + '*.xml'):\n\t\t\ttree = ET.parse(xml_file)\n\t\t\troot = tree.getroot()\n\t\t\tfor member in root.findall('object'):\n\t\t\t\tvalue = (root.find('filename').text,\n\t\t\t\t\t\t int(root.find('size')[0].text), int(root.find('size')[1].text), \n\t\t\t\t\t\t member[0].text,\n\t\t\t\t\t\t int(member[4][0].text), int(member[4][1].text),\n\t\t\t\t\t\t int(member[4][2].text), int(member[4][3].text))\n\t\t\t\tannotations.append(value)\n\n\t\tself.list_to_csv(annotations, csv_path)\n\n\tdef json_to_csv(self, jsonpath, csvpath):\n\t\twith open(jsonpath) as f:\n\t\t\timages = json.load(f)\n\n\t\tannotations = []\n\n\t\tfor entry in images:\n\t\t\tfilename = images[entry]['filename']\n\t\t\tfor region in images[entry]['regions']:\n\t\t\t\tc = region['region_attributes']['class']\n\t\t\t\txmin = region['shape_attributes']['x']\n\t\t\t\tymin = region['shape_attributes']['y']\n\t\t\t\txmax = xmin + region['shape_attributes']['width']\n\t\t\t\tymax = ymin + region['shape_attributes']['height']\n\t\t\t\twidth = 0\n\t\t\t\theight = 0\n\n\t\t\t\tvalue = (filename, width, height, c, xmin, ymin, xmax, ymax)\n\t\t\t\tannotations.append(value)\n\n\t\tself.list_to_csv(annotations, csvpath)\n\n\tdef generate_tfexample(self, group, path):\n\t\twith tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n\t\t\tencoded_jpg = fid.read()\n\t\tencoded_jpg_io = io.BytesIO(encoded_jpg)\n\t\timage = Image.open(encoded_jpg_io)\n\t\twidth, height = image.size\n\n\t\tfilename = group.filename.encode('utf8')\n\t\timage_format = b'jpg'\n\t\txmins = []\n\t\txmaxs = []\n\t\tymins = []\n\t\tymaxs = []\n\t\tclasses_text = []\n\t\tclasses = []\n\n\t\tfor index, row in group.object.iterrows():\n\t\t\txmins.append(row['xmin'] / width)\n\t\t\txmaxs.append(row['xmax'] / width)\n\t\t\tymins.append(row['ymin'] / height)\n\t\t\tymaxs.append(row['ymax'] / height)\n\t\t\tclasses_text.append(row['class'].encode('utf8'))\n\t\t\tclasses.append(self.class_to_int(row['class']))\n\n\t\ttf_example = tf.train.Example(features=tf.train.Features(feature={\n\t\t\t'image/height': dataset_util.int64_feature(height),\n\t\t\t'image/width': dataset_util.int64_feature(width),\n\t\t\t'image/filename': dataset_util.bytes_feature(filename),\n\t\t\t'image/source_id': dataset_util.bytes_feature(filename),\n\t\t\t'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n\t\t\t'image/format': dataset_util.bytes_feature(image_format),\n\t\t\t'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n\t\t\t'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n\t\t\t'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n\t\t\t'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n\t\t\t'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n\t\t\t'image/object/class/label': dataset_util.int64_list_feature(classes),\n\t\t}))\n\t\t\n\t\treturn tf_example\t\t\n\n\tdef csv_to_tfrecord(self, csvpath, filename, tfrpath):\n\t\tcsv = pd.read_csv(csvpath).values\n"""
build/lib/sightseer/sightseer.py,0,"b'import cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\n# need to find Linux-friendly alternative\n# from PIL import ImageGrab\n\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport matplotlib.animation as animation\n\nclass Sightseer(object):\n\tdef __init__(self):\n\t\tself.filepath = None\n\n\tdef render_grayscale(self, frame):\n\t\tgray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\t\treturn gray_frame\n\n\t# Experimental\n\tdef load_webcam(self, return_data=True, set_gray=True, kill_key=""q"", width=160, height=120):\n\n\t\tcap = cv2.VideoCapture(0)\n\t\tcap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n\t\tcap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n\n\t\tframes = []\n\n\t\twhile True:\n\t\t\tret, frame = cap.read()\n\t\t\tprint (frame.shape)\n\n\t\t\tif set_gray:\n\t\t\t\tframe = self.render_grayscale(frame)\n\t\t\t\n\t\t\tframe = cv2.flip(frame, 1) # prevent lateral inversion\n\t\t\tcv2.imshow(\'frame\', frame)\n\t\t\tframes.append(frame)\n\n\t\t\tif cv2.waitKey(1) & 0xFF == ord(kill_key):\n\t\t\t\tbreak\n\n\t\tcap.release()\n\t\tcv2.destroyAllWindows()\t\n\n\t\tif return_data:\n\t\t\tframes = np.array(frames)\n\t\t\treturn frames\n\t\t\n\t# Experimental\n\t# def screen_grab(self, set_gray=True, write_data=True, return_data=True, kill_key=""q"", filename=\'output.avi\', width=400, height=400):\n\t# \tfourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n\t# \tout = cv2.VideoWriter(filename, fourcc, 20.0, (640, 480))\n\n\t# \tframes = []\n\n\t# \twhile True:\n\t# \t\timg = np.array(ImageGrab.grab(bbox=(0, 0, width, height)))\n\t# \t\tframe = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n\n\t# \t\tif write_data:\n\t# \t\t\tout.write(imcv)\n\n\t# \t\tif set_gray:\n\t# \t\t\tframe = self.render_grayscale(img)\n\n\t# \t\tcv2.imshow(\'frame\', frame)\n\t# \t\tframes.append(frame)\n\n\t# \t\tif cv2.waitKey(1) & 0xFF == ord(kill_key):\n\t# \t\t\tbreak\n\n\t# \tout.release()\n\t# \tcv2.destroyAllWindows()\n\n\t# \tif return_data:\n\t# \t\tframes = np.array(frames)\n\t# \t\treturn frames\n\n\tdef load_vidsource(self, filepath, return_data=True, set_gray=False):\n\t\tself.filepath = filepath\n\t\tvidcap = cv2.VideoCapture(filepath)\n\n\t\tprint (""Extracting frames from video..."")\n\t\t\n\t\tframes = []\n\n\t\twhile vidcap.isOpened():\n\t\t\tframe_exists, frame = vidcap.read()\n\n\t\t\tif frame_exists == False:\n\t\t\t\tbreak\n\n\t\t\tif set_gray:\n\t\t\t\tframe = self.render_grayscale(frame)\n\n\t\t\tframes.append(frame)\n\t\t\n\t\tvidcap.release()\n\t\tcv2.destroyAllWindows()\n\t\t\n\t\tif return_data:\n\t\t\tframes = np.array(frames)\n\t\t\treturn frames\n\n\tdef load_image(self, filepath):\n\t\tself.filepath = filepath\n\t\ttry:\n\t\t\timg = cv2.imread(filepath)\n\t\t\treturn img\n\t\texcept:\n\t\t\traise FileExistsError (""File does not exist. You may want to check the filepath again."")\n\n\tdef get_final_filepath(self, image_path):\n\t\timage_path = image_path.split(\'/\')\n\t\timg_name = image_path[-1]\n\t\timg_name = img_name.split(\'.\')\n\t\timg_name = img_name[0] + ""_preds."" + img_name[-1]\n\t\timage_path = ""/"".join(image_path[:-1]) + ""/"" + img_name\n\n\t\treturn image_path\t\n\t\n\tdef render_image(self, image, save_image=False):\n\t\tplt.imshow(image)\n\t\tplt.show()\n\n\t\tif save_image:\n\t\t\tnew_filepath = self.get_final_filepath(self.filepath)\n\t\t\tplt.savefig(new_filepath)\n\t\n\tdef render_footage(self, frames):\n\t\tfig = plt.figure()\n\t\tfinal_frames = []\n\n\t\tfor i in range(len(frames)):\n\t\t\tfinal_frames.append([plt.imshow(frames[i], animated=True)])\n\t\t\n\t\tani = animation.ArtistAnimation(fig, final_frames, interval=50, blit=True, repeat_delay=1000)\n\t\tfinal_filename = self.get_final_filepath(self.filepath)\n\t\tani.save(final_filename)\n\n\t\tplt.show()'"
build/lib/sightseer/zoo.py,2,"b'import os\nimport wget\nimport struct\nimport shutil\nimport logging\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, UpSampling2D, concatenate\nfrom tensorflow.keras.models import Model, load_model\n\nfrom .blocks import ConvBlock, BoundingBox, SightLoader\n\n# disabling warnings and logging\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\ntf.autograph.set_verbosity(tf.compat.v1.logging.ERROR)\nlogging.disable(logging.WARNING)\n\nclass YOLOv3Client(object):\n\tdef __init__(self, nms_threshold=0.45, obj_threshold=0.5, net_h=416, net_w=416, anchors=[[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]):\n\t\tself.nms_threshold = nms_threshold\n\t\tself.obj_threshold = obj_threshold\n\t\tself.net_h, self.net_w = net_h, net_w\n\t\tself.anchors = anchors\n\t\tself.yolo_model = None # initialised after weights are loaded into model\n\t\tself.weights_url = ""https://pjreddie.com/media/files/yolov3.weights""\n\t\tself.all_labels = [""person"", ""bicycle"", ""car"", ""motorbike"", ""aeroplane"", ""bus"", ""train"", ""truck"",\n\t\t\t\t\t\t""boat"", ""traffic light"", ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"",\n\t\t\t\t\t\t""bird"", ""cat"", ""dog"", ""horse"", ""sheep"", ""cow"", ""elephant"", ""bear"", ""zebra"", ""giraffe"",\n\t\t\t\t\t\t""backpack"", ""umbrella"", ""handbag"", ""tie"", ""suitcase"", ""frisbee"", ""skis"", ""snowboard"",\n\t\t\t\t\t\t""sports ball"", ""kite"", ""baseball bat"", ""baseball glove"", ""skateboard"", ""surfboard"",\n\t\t\t\t\t\t""tennis racket"", ""bottle"", ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"", ""bowl"", ""banana"",\n\t\t\t\t\t\t""apple"", ""sandwich"", ""orange"", ""broccoli"", ""carrot"", ""hot dog"", ""pizza"", ""donut"", ""cake"",\n\t\t\t\t\t\t""chair"", ""sofa"", ""pottedplant"", ""bed"", ""diningtable"", ""toilet"", ""tvmonitor"", ""laptop"", ""mouse"",\n\t\t\t\t\t\t""remote"", ""keyboard"", ""cell phone"", ""microwave"", ""oven"", ""toaster"", ""sink"", ""refrigerator"",\n\t\t\t\t\t\t""book"", ""clock"", ""vase"", ""scissors"", ""teddy bear"", ""hair drier"", ""toothbrush""]\n\t\t\t\t\t\t\n\tdef download_weights(self):\n\t\t""""""\n\t\tDownloads the weights from online and saves them locally\n\t\t""""""\n\n\t\tif os.path.exists(""./bin/yolov3.weights""):\n\t\t\tprint (""Weights already exist. Proceeding to load YOLOv3Client..."")\n\t\telse:\n\t\t\tprint (""Downloading weights. This may may take a moment..."")\n\t\n\t\t\twget.download(self.weights_url, os.getcwd() + ""/yolov3.weights"")\n\n\t\t\tos.mkdir(""./bin"", 0o755) # configuring admin rights\n\t\t\tshutil.move(""./yolov3.weights"", ""./bin/yolov3.weights"")\n\n\t\t\tprint (""\\n\\nWeights downloaded successfully!"")\n\n\tdef load_architecture(self):\n\t\t""""""\n\t\tReturns tf.keras.models.Model instance\n\t\t""""""\n\t\tinp_image = Input(shape=[None, None, 3])\n\n\t\tx = ConvBlock.get_conv_block(inp_image, [{\'filter\': 32, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 0},\n\t\t\t\t\t\t\t\t\t\t{\'filter\': 64, \'kernel\': 3, \'stride\': 2, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 1},\n\t\t\t\t\t\t\t\t\t\t{\'filter\': 32, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 2},\n\t\t\t\t\t\t\t\t\t\t{\'filter\': 64, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 3}])\n\n\t\tx = ConvBlock.get_conv_block(x, [{\'filter\': 128, \'kernel\': 3, \'stride\': 2, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 5},\n\t\t\t\t\t\t\t{\'filter\':  64, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 6},\n\t\t\t\t\t\t\t{\'filter\': 128, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 7}])\n\n\t\tx = ConvBlock.get_conv_block(x, [{\'filter\':  64, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 9},\n\t\t\t\t\t\t\t{\'filter\': 128, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 10}])\n\n\t\tx = ConvBlock.get_conv_block(x, [{\'filter\': 256, \'kernel\': 3, \'stride\': 2, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 12},\n\t\t\t\t\t\t\t{\'filter\': 128, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 13},\n\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 14}])\n\n\t\tfor i in range(7):\n\t\t\tx = ConvBlock.get_conv_block(x, [{\'filter\': 128, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 16+i*3},\n\t\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 17+i*3}])\n\t\t\t\n\t\tskip_36 = x\n\t\t\t\n\t\tx = ConvBlock.get_conv_block(x, [{\'filter\': 512, \'kernel\': 3, \'stride\': 2, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 37},\n\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 38},\n\t\t\t\t\t\t\t{\'filter\': 512, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 39}])\n\n\t\tfor i in range(7):\n\t\t\tx = ConvBlock.get_conv_block(x, [{\'filter\': 256, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 41+i*3},\n\t\t\t\t\t\t\t\t{\'filter\': 512, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 42+i*3}])\n\t\t\t\n\t\tskip_61 = x\n\t\t\t\n\t\tx = ConvBlock.get_conv_block(x, [{\'filter\': 1024, \'kernel\': 3, \'stride\': 2, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 62},\n\t\t\t\t\t\t\t{\'filter\':  512, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 63},\n\t\t\t\t\t\t\t{\'filter\': 1024, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 64}])\n\n\t\tfor i in range(3):\n\t\t\tx = ConvBlock.get_conv_block(x, [{\'filter\':  512, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 66+i*3},\n\t\t\t\t\t\t\t\t{\'filter\': 1024, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 67+i*3}])\n\t\t\t\n\t\tx = ConvBlock.get_conv_block(x, [{\'filter\':  512, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 75},\n\t\t\t\t\t\t\t{\'filter\': 1024, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 76},\n\t\t\t\t\t\t\t{\'filter\':  512, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 77},\n\t\t\t\t\t\t\t{\'filter\': 1024, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 78},\n\t\t\t\t\t\t\t{\'filter\':  512, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 79}], skip=False)\n\n\t\tyolo_82 = ConvBlock.get_conv_block(x, [{\'filter\': 1024, \'kernel\': 3, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 80},\n\t\t\t\t\t\t\t\t\t{\'filter\':  255, \'kernel\': 1, \'stride\': 1, \'bnorm\': False, \'leaky\': False, \'layer_idx\': 81}], skip=False)\n\n\t\tx = ConvBlock.get_conv_block(x, [{\'filter\': 256, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 84}], skip=False)\n\t\tx = UpSampling2D(2)(x)\n\t\tx = concatenate([x, skip_61])\n\n\t\tx = ConvBlock.get_conv_block(x, [{\'filter\': 256, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 87},\n\t\t\t\t\t\t\t{\'filter\': 512, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 88},\n\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 89},\n\t\t\t\t\t\t\t{\'filter\': 512, \'kernel\': 3, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 90},\n\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True, \'layer_idx\': 91}], skip=False)\n\n\t\tyolo_94 = ConvBlock.get_conv_block(x, [{\'filter\': 512, \'kernel\': 3, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 92},\n\t\t\t\t\t\t\t\t\t{\'filter\': 255, \'kernel\': 1, \'stride\': 1, \'bnorm\': False, \'leaky\': False, \'layer_idx\': 93}], skip=False)\n\n\t\tx = ConvBlock.get_conv_block(x, [{\'filter\': 128, \'kernel\': 1, \'stride\': 1, \'bnorm\': True, \'leaky\': True,   \'layer_idx\': 96}], skip=False)\n\t\tx = UpSampling2D(2)(x)\n\t\tx = concatenate([x, skip_36])\n\n\t\tyolo_106 = ConvBlock.get_conv_block(x, [{\'filter\': 128, \'kernel\': 1, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 99},\n\t\t\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 3, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 100},\n\t\t\t\t\t\t\t\t\t{\'filter\': 128, \'kernel\': 1, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 101},\n\t\t\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 3, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 102},\n\t\t\t\t\t\t\t\t\t{\'filter\': 128, \'kernel\': 1, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 103},\n\t\t\t\t\t\t\t\t\t{\'filter\': 256, \'kernel\': 3, \'stride\': 1, \'bnorm\': True,  \'leaky\': True,  \'layer_idx\': 104},\n\t\t\t\t\t\t\t\t\t{\'filter\': 255, \'kernel\': 1, \'stride\': 1, \'bnorm\': False, \'leaky\': False, \'layer_idx\': 105}], skip=False)\n\n\t\tmodel = Model(inp_image, [yolo_82, yolo_94, yolo_106])    \n\t\treturn model\n\n\tdef sigmoid(self, z):\n\t\treturn 1 / (1 + np.exp(-z))\n\n\tdef preprocess(self, image):\n\t\t""""""\n\t\tResizes image to appropriate dimensions for YOLOv3\n\t\t""""""\n\t\tnew_h, new_w = image.shape[:2]\n\n\t\tif (float(self.net_w)/new_w) < (float(self.net_h)/new_h):\n\t\t\tnew_h = (new_h * self.net_w)//new_w\n\t\t\tnew_w = self.net_w\n\t\telse:\n\t\t\tnew_w = (new_w * self.net_h)//new_h\n\t\t\tnew_h = self.net_h        \n\n\t\t# resize the image to the new size\n\t\tresized = cv2.resize(image[:, :, ::-1]/255., (int(new_w), int(new_h)))\n\n\t\t# embed the image into the standard letter box\n\t\tnew_img = np.ones((self.net_h, self.net_w, 3)) * 0.5\n\t\tnew_img[int((self.net_h-new_h)//2):int((self.net_h+new_h)//2), int((self.net_w-new_w)//2):int((self.net_w+new_w)//2), :] = resized\n\t\tnew_img = np.expand_dims(new_img, 0)\n\n\t\treturn new_img\n\n\tdef interval_overlap(self, int_a, int_b):\n\t\tx1, x2 = int_a\n\t\tx3, x4 = int_b\n\n\t\tif x3 < x1:\n\t\t\tif x4 < x1:\n\t\t\t\treturn 0\n\t\t\telse:\n\t\t\t\treturn min(x2, x4) - x1\n\t\telse:\n\t\t\tif x2 < x3:\n\t\t\t\treturn 0\n\t\t\telse:\n\t\t\t\treturn min(x2, x4) - x3\t\n\n\tdef bbox_iou(self, box1, box2):\n\t\t""""""\n\t\tFinds IOU between all bounding boxes before non maximum suppression process\n\t\t""""""\n\t\tint_w = self.interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n\t\tint_h = self.interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n\n\t\tintersect = int_w * int_h\n\n\t\tw1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin\n\t\tw2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin\n\n\t\tunion = w1*h1 + w2*h2 - intersect\n\n\t\treturn float(intersect) / union\t\t\n\n\tdef non_maximum_suppression(self, boxes):\n\t\tif len(boxes) > 0:\n\t\t\tnb_class = len(boxes[0].classes)\n\t\telse:\n\t\t\treturn\n\t\t\t\n\t\tfor c in range(nb_class):\n\t\t\tsorted_indices = np.argsort([-box.classes[c] for box in boxes])\n\n\t\t\tfor i in range(len(sorted_indices)):\n\t\t\t\tindex_i = sorted_indices[i]\n\n\t\t\t\tif boxes[index_i].classes[c] == 0: continue\n\n\t\t\t\tfor j in range(i+1, len(sorted_indices)):\n\t\t\t\t\tindex_j = sorted_indices[j]\n\n\t\t\t\t\tif self.bbox_iou(boxes[index_i], boxes[index_j]) >= self.nms_threshold:\n\t\t\t\t\t\tboxes[index_j].classes[c] = 0\n\n\t\treturn boxes\n\n\tdef decode_preds(self, preds, anchors):\n\t\tgridh, gridw = preds.shape[:2]\n\t\tnb_box = 3\n\t\tpreds = preds.reshape([gridh, gridw, nb_box, -1])\n\t\tnb_class = preds.shape[-1] - 5\n\n\t\tboxes = []\n\t\t\n\t\tpreds[..., :2]  = self.sigmoid(preds[..., :2])\n\t\tpreds[..., 4:]  = self.sigmoid(preds[..., 4:])\n\t\tpreds[..., 5:]  = preds[..., 4][..., np.newaxis] * preds[..., 5:]\n\t\tpreds[..., 5:] *= preds[..., 5:] > self.obj_threshold\n\n\t\tfor i in range(gridh * gridw):\n\t\t\trow = i / gridw\n\t\t\tcol = i % gridw\n\n\t\t\tfor b in range(nb_box):\n\t\t\t\tobjectness = preds[int(row)][int(col)][b][4]\n\n\t\t\t\tif (objectness.all() <= self.obj_threshold): continue\n\n\t\t\t\tx, y, w, h = preds[int(row)][int(col)][b][:4]\n\n\t\t\t\tx = (col + x) / gridw \n\t\t\t\ty = (row + y) / gridh \n\t\t\t\tw = anchors[2 * b + 0] * np.exp(w) / self.net_w\n\t\t\t\th = anchors[2 * b + 1] * np.exp(h) / self.net_h\n\n\t\t\t\tclasses = preds[int(row)][col][b][5:]\n\n\t\t\t\tbox = BoundingBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n\n\t\t\t\tboxes.append(box)\n\n\t\treturn boxes\n\n\tdef rectify_boxes(self, boxes, image_h, image_w):\n\t\tif (float(self.net_w)/image_w) < (float(self.net_h)/image_h):\n\t\t\tnew_w = self.net_w\n\t\t\tnew_h = (image_h * self.net_w)/ image_w\n\t\telse:\n\t\t\tnew_h = self.net_w\n\t\t\tnew_w = (image_w * self.net_h) / image_h\n\t\t\t\n\t\tfor i in range(len(boxes)):\n\t\t\tx_offset, x_scale = (self.net_w - new_w)/2./self.net_w, float(new_w)/self.net_w\n\t\t\ty_offset, y_scale = (self.net_h - new_h)/2./self.net_h, float(new_h)/self.net_h\n\t\t\t\n\t\t\tboxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n\t\t\tboxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n\t\t\tboxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n\t\t\tboxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n\n\t\treturn boxes\n\n\tdef get_boxes(self, image, boxes, verbose=True, random_coloring=True):\n\t\tfinal_boxes = []\n\n\t\tfor box in boxes:\n\t\t\tfinal_label = """"\n\t\t\tlabel = -1\n\n\t\t\tfor i in range(len(self.all_labels)):\n\t\t\t\tif box.classes[i] > self.obj_threshold:\n\t\t\t\t\tfinal_label += self.all_labels[i]\n\t\t\t\t\tlabel = i\n\n\t\t\t\t\tif verbose:\n\t\t\t\t\t\tprint (""{}: {:.3f}%"".format(self.all_labels[i], box.classes[i]*100))\n\n\t\t\t\t\tfinal_boxes.append([final_label,\n\t\t\t\t\t\t\t\t\t\tbox.classes[i] * 100,\n\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\t\'xmin\': box.xmin,\n\t\t\t\t\t\t\t\t\t\t\t\'ymin\': box.ymin,\n\t\t\t\t\t\t\t\t\t\t\t\'xmax\': box.xmax,\n\t\t\t\t\t\t\t\t\t\t\t\'ymax\': box.ymax\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t])\n\n\t\t\tif label >= 0:\n\t\t\t\tif random_coloring:\n\t\t\t\t\tr, g, b = np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255)\n\t\t\t\telse:\n\t\t\t\t\tr, g, b = 0, 255, 0\n\n\t\t\t\tcv2.rectangle(image, (box.xmin, box.ymin), (box.xmax, box.ymax), (r, g, b), 2)\n\t\t\t\tcv2.putText(image, \'{} {:.3f}\'.format(final_label, box.get_confidence()), (box.xmax, box.ymin - 13), cv2.FONT_HERSHEY_SIMPLEX, 1e-3 * image.shape[0], (r, g, b), 2)\n\n\t\treturn final_boxes, image\n\n\tdef load_model(self, default_path=""./bin/yolov3.weights"", cache=True, verbose=True):\n\t\t""""""\n\t\tDownloads weights and config, loads checkpoints into architecture\n\t\t""""""\n\t\tif os.path.exists(""./bin/yolov3.h5""):\n\t\t\tprint (""Weights already exist. Proceeding to load YOLOv3Client..."")\n\t\t\tself.yolo_model = load_model(""./bin/yolov3.h5"")\n\t\telse:\n\t\t\tself.download_weights() # downloading weights from online\n\t\t\tloader = SightLoader(default_path)\n\t\t\t\n\t\t\tself.yolo_model = self.load_architecture() # loading weights into model\n\t\t\tloader.load_weights(self.yolo_model, verbose)\n\n\t\t\tself.yolo_model.save(""./bin/yolov3.h5"") # saves .h5 weights file\n\t\t\tos.remove(""./bin/yolov3.weights"") # removes original .weights file\n\n\tdef predict(self, original_image, return_img=False, verbose=True):\n\t\t""""""\n\t\tReturns a list of BoundingBox metadata (class label, confidence score, coordinates)\n\t\tand the edited image with bounding boxes and their corresponding text labels/confidence scores\n\t\t""""""\n\t\timage_h, image_w = original_image.shape[:2]\n\n\t\tif self.yolo_model == None:\n\t\t\traise ValueError (""YOLOv3 weights needs to be downloaded and configured into the model before use. You can use the `load_model()` method to do so."")\n\n\t\tproc_image = self.preprocess(original_image)\n\t\tpreds = self.yolo_model.predict(proc_image)\n\t\tboxes = []\n\n\t\tfor i in range(len(preds)):\n\t\t\tboxes += self.decode_preds(preds[i][0], self.anchors[i])\n\n\t\tboxes = self.rectify_boxes(boxes, image_h, image_w)\n\t\tboxes = self.non_maximum_suppression(boxes)\n\n\t\tbox_list, box_image = self.get_boxes(original_image, boxes, verbose)\n\n\t\tif return_img:\n\t\t\tbox_image = box_image.squeeze()\n\t\t\treturn box_list, box_image\n\t\telse:\n\t\t\treturn box_list\n\n\tdef framewise_predict(self, frames, verbose=True):\n\t\tfinal_preds = []\n\t\tfinal_frames = []\n\n\t\tfor i in range(len(frames)):\n\t\t\tprint (""Frame {}"".format(i))\n\n\t\t\tcur_preds, edited_frame = self.predict(frames[i], return_img=True, verbose=False)\n\n\t\t\tfinal_preds.append(cur_preds)\n\t\t\tfinal_frames.append(edited_frame)\n\n\t\treturn final_preds, final_frames\n\nclass MaskRCNNClient(object):\n\tdef __init__(self):\n\t\tpass'"
