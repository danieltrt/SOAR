file_path,api_count,code
create_pascal_tf_record.py,13,"b'""""""Converts PASCAL dataset to TFRecords file format.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport io\nimport os\nimport sys\n\nimport PIL.Image\nimport tensorflow as tf\n\nfrom utils import dataset_util\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'--data_dir\', type=str, default=\'./dataset/VOCdevkit/VOC2012\',\n                    help=\'Path to the directory containing the PASCAL VOC data.\')\n\nparser.add_argument(\'--output_path\', type=str, default=\'./dataset\',\n                    help=\'Path to the directory to create TFRecords outputs.\')\n\nparser.add_argument(\'--train_data_list\', type=str, default=\'./dataset/train.txt\',\n                    help=\'Path to the file listing the training data.\')\n\nparser.add_argument(\'--valid_data_list\', type=str, default=\'./dataset/val.txt\',\n                    help=\'Path to the file listing the validation data.\')\n\nparser.add_argument(\'--image_data_dir\', type=str, default=\'JPEGImages\',\n                    help=\'The directory containing the image data.\')\n\nparser.add_argument(\'--label_data_dir\', type=str, default=\'SegmentationClassAug\',\n                    help=\'The directory containing the augmented label data.\')\n\n\ndef dict_to_tf_example(image_path,\n                       label_path):\n  """"""Convert image and label to tf.Example proto.\n\n  Args:\n    image_path: Path to a single PASCAL image.\n    label_path: Path to its corresponding label.\n\n  Returns:\n    example: The converted tf.Example.\n\n  Raises:\n    ValueError: if the image pointed to by image_path is not a valid JPEG or\n                if the label pointed to by label_path is not a valid PNG or\n                if the size of image does not match with that of label.\n  """"""\n  with tf.gfile.GFile(image_path, \'rb\') as fid:\n    encoded_jpg = fid.read()\n  encoded_jpg_io = io.BytesIO(encoded_jpg)\n  image = PIL.Image.open(encoded_jpg_io)\n  if image.format != \'JPEG\':\n    raise ValueError(\'Image format not JPEG\')\n\n  with tf.gfile.GFile(label_path, \'rb\') as fid:\n    encoded_label = fid.read()\n  encoded_label_io = io.BytesIO(encoded_label)\n  label = PIL.Image.open(encoded_label_io)\n  if label.format != \'PNG\':\n    raise ValueError(\'Label format not PNG\')\n\n  if image.size != label.size:\n    raise ValueError(\'The size of image does not match with that of label.\')\n\n  width, height = image.size\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n    \'image/height\': dataset_util.int64_feature(height),\n    \'image/width\': dataset_util.int64_feature(width),\n    \'image/encoded\': dataset_util.bytes_feature(encoded_jpg),\n    \'image/format\': dataset_util.bytes_feature(\'jpeg\'.encode(\'utf8\')),\n    \'label/encoded\': dataset_util.bytes_feature(encoded_label),\n    \'label/format\': dataset_util.bytes_feature(\'png\'.encode(\'utf8\')),\n  }))\n  return example\n\n\ndef create_tf_record(output_filename,\n                     image_dir,\n                     label_dir,\n                     examples):\n  """"""Creates a TFRecord file from examples.\n\n  Args:\n    output_filename: Path to where output file is saved.\n    image_dir: Directory where image files are stored.\n    label_dir: Directory where label files are stored.\n    examples: Examples to parse and save to tf record.\n  """"""\n  writer = tf.python_io.TFRecordWriter(output_filename)\n  for idx, example in enumerate(examples):\n    if idx % 500 == 0:\n      tf.logging.info(\'On image %d of %d\', idx, len(examples))\n    image_path = os.path.join(image_dir, example + \'.jpg\')\n    label_path = os.path.join(label_dir, example + \'.png\')\n\n    if not os.path.exists(image_path):\n      tf.logging.warning(\'Could not find %s, ignoring example.\', image_path)\n      continue\n    elif not os.path.exists(label_path):\n      tf.logging.warning(\'Could not find %s, ignoring example.\', label_path)\n      continue\n\n    try:\n      tf_example = dict_to_tf_example(image_path, label_path)\n      writer.write(tf_example.SerializeToString())\n    except ValueError:\n      tf.logging.warning(\'Invalid example: %s, ignoring.\', example)\n\n  writer.close()\n\n\ndef main(unused_argv):\n  if not os.path.exists(FLAGS.output_path):\n    os.makedirs(FLAGS.output_path)\n\n  tf.logging.info(""Reading from VOC dataset"")\n  image_dir = os.path.join(FLAGS.data_dir, FLAGS.image_data_dir)\n  label_dir = os.path.join(FLAGS.data_dir, FLAGS.label_data_dir)\n\n  if not os.path.isdir(label_dir):\n    raise ValueError(""Missing Augmentation label directory. ""\n                     ""You may download the augmented labels from the link (Thanks to DrSleep): ""\n                     ""https://www.dropbox.com/s/oeu149j8qtbs1x0/SegmentationClassAug.zip"")\n  train_examples = dataset_util.read_examples_list(FLAGS.train_data_list)\n  val_examples = dataset_util.read_examples_list(FLAGS.valid_data_list)\n\n  train_output_path = os.path.join(FLAGS.output_path, \'voc_train.record\')\n  val_output_path = os.path.join(FLAGS.output_path, \'voc_val.record\')\n\n  create_tf_record(train_output_path, image_dir, label_dir, train_examples)\n  create_tf_record(val_output_path, image_dir, label_dir, val_examples)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
deeplab_model.py,81,"b'""""""DeepLab v3 models based on slim library.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib.slim.nets import resnet_v2\nfrom tensorflow.contrib import layers as layers_lib\nfrom tensorflow.contrib.framework.python.ops import arg_scope\nfrom tensorflow.contrib.layers.python.layers import layers\nfrom tensorflow.contrib.slim.python.slim.nets import resnet_utils\n\nfrom utils import preprocessing\n\n_BATCH_NORM_DECAY = 0.9997\n_WEIGHT_DECAY = 5e-4\n\n\ndef atrous_spatial_pyramid_pooling(inputs, output_stride, batch_norm_decay, is_training, depth=256):\n  """"""Atrous Spatial Pyramid Pooling.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    output_stride: The ResNet unit\'s stride. Determines the rates for atrous convolution.\n      the rates are (6, 12, 18) when the stride is 16, and doubled when 8.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    is_training: A boolean denoting whether the input is for training.\n    depth: The depth of the ResNet unit output.\n\n  Returns:\n    The atrous spatial pyramid pooling output.\n  """"""\n  with tf.variable_scope(""aspp""):\n    if output_stride not in [8, 16]:\n      raise ValueError(\'output_stride must be either 8 or 16.\')\n\n    atrous_rates = [6, 12, 18]\n    if output_stride == 8:\n      atrous_rates = [2*rate for rate in atrous_rates]\n\n    with tf.contrib.slim.arg_scope(resnet_v2.resnet_arg_scope(batch_norm_decay=batch_norm_decay)):\n      with arg_scope([layers.batch_norm], is_training=is_training):\n        inputs_size = tf.shape(inputs)[1:3]\n        # (a) one 1x1 convolution and three 3x3 convolutions with rates = (6, 12, 18) when output stride = 16.\n        # the rates are doubled when output stride = 8.\n        conv_1x1 = layers_lib.conv2d(inputs, depth, [1, 1], stride=1, scope=""conv_1x1"")\n        conv_3x3_1 = resnet_utils.conv2d_same(inputs, depth, 3, stride=1, rate=atrous_rates[0], scope=\'conv_3x3_1\')\n        conv_3x3_2 = resnet_utils.conv2d_same(inputs, depth, 3, stride=1, rate=atrous_rates[1], scope=\'conv_3x3_2\')\n        conv_3x3_3 = resnet_utils.conv2d_same(inputs, depth, 3, stride=1, rate=atrous_rates[2], scope=\'conv_3x3_3\')\n\n        # (b) the image-level features\n        with tf.variable_scope(""image_level_features""):\n          # global average pooling\n          image_level_features = tf.reduce_mean(inputs, [1, 2], name=\'global_average_pooling\', keepdims=True)\n          # 1x1 convolution with 256 filters( and batch normalization)\n          image_level_features = layers_lib.conv2d(image_level_features, depth, [1, 1], stride=1, scope=\'conv_1x1\')\n          # bilinearly upsample features\n          image_level_features = tf.image.resize_bilinear(image_level_features, inputs_size, name=\'upsample\')\n\n        net = tf.concat([conv_1x1, conv_3x3_1, conv_3x3_2, conv_3x3_3, image_level_features], axis=3, name=\'concat\')\n        net = layers_lib.conv2d(net, depth, [1, 1], stride=1, scope=\'conv_1x1_concat\')\n\n        return net\n\n\ndef deeplab_v3_generator(num_classes,\n                         output_stride,\n                         base_architecture,\n                         pre_trained_model,\n                         batch_norm_decay,\n                         data_format=\'channels_last\'):\n  """"""Generator for DeepLab v3 models.\n\n  Args:\n    num_classes: The number of possible classes for image classification.\n    output_stride: The ResNet unit\'s stride. Determines the rates for atrous convolution.\n      the rates are (6, 12, 18) when the stride is 16, and doubled when 8.\n    base_architecture: The architecture of base Resnet building block.\n    pre_trained_model: The path to the directory that contains pre-trained models.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    data_format: The input format (\'channels_last\', \'channels_first\', or None).\n      If set to None, the format is dependent on whether a GPU is available.\n      Only \'channels_last\' is supported currently.\n\n  Returns:\n    The model function that takes in `inputs` and `is_training` and\n    returns the output tensor of the DeepLab v3 model.\n  """"""\n  if data_format is None:\n    # data_format = (\n    #     \'channels_first\' if tf.test.is_built_with_cuda() else \'channels_last\')\n    pass\n\n  if batch_norm_decay is None:\n    batch_norm_decay = _BATCH_NORM_DECAY\n\n  if base_architecture not in [\'resnet_v2_50\', \'resnet_v2_101\']:\n    raise ValueError(""\'base_architrecture\' must be either \'resnet_v2_50\' or \'resnet_v2_101\'."")\n\n  if base_architecture == \'resnet_v2_50\':\n    base_model = resnet_v2.resnet_v2_50\n  else:\n    base_model = resnet_v2.resnet_v2_101\n\n  def model(inputs, is_training):\n    """"""Constructs the ResNet model given the inputs.""""""\n    if data_format == \'channels_first\':\n      # Convert the inputs from channels_last (NHWC) to channels_first (NCHW).\n      # This provides a large performance boost on GPU. See\n      # https://www.tensorflow.org/performance/performance_guide#data_formats\n      inputs = tf.transpose(inputs, [0, 3, 1, 2])\n\n    # tf.logging.info(\'net shape: {}\'.format(inputs.shape))\n\n    with tf.contrib.slim.arg_scope(resnet_v2.resnet_arg_scope(batch_norm_decay=batch_norm_decay)):\n      logits, end_points = base_model(inputs,\n                                      num_classes=None,\n                                      is_training=is_training,\n                                      global_pool=False,\n                                      output_stride=output_stride)\n\n    if is_training:\n      exclude = [base_architecture + \'/logits\', \'global_step\']\n      variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=exclude)\n      tf.train.init_from_checkpoint(pre_trained_model,\n                                    {v.name.split(\':\')[0]: v for v in variables_to_restore})\n\n    inputs_size = tf.shape(inputs)[1:3]\n    net = end_points[base_architecture + \'/block4\']\n    net = atrous_spatial_pyramid_pooling(net, output_stride, batch_norm_decay, is_training)\n    with tf.variable_scope(""upsampling_logits""):\n      net = layers_lib.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, scope=\'conv_1x1\')\n      logits = tf.image.resize_bilinear(net, inputs_size, name=\'upsample\')\n\n    return logits\n\n  return model\n\n\ndef deeplabv3_model_fn(features, labels, mode, params):\n  """"""Model function for PASCAL VOC.""""""\n  if isinstance(features, dict):\n    features = features[\'feature\']\n\n  images = tf.cast(\n      tf.map_fn(preprocessing.mean_image_addition, features),\n      tf.uint8)\n\n  network = deeplab_v3_generator(params[\'num_classes\'],\n                                 params[\'output_stride\'],\n                                 params[\'base_architecture\'],\n                                 params[\'pre_trained_model\'],\n                                 params[\'batch_norm_decay\'])\n\n  logits = network(features, mode == tf.estimator.ModeKeys.TRAIN)\n\n  pred_classes = tf.expand_dims(tf.argmax(logits, axis=3, output_type=tf.int32), axis=3)\n\n  pred_decoded_labels = tf.py_func(preprocessing.decode_labels,\n                                   [pred_classes, params[\'batch_size\'], params[\'num_classes\']],\n                                   tf.uint8)\n\n  predictions = {\n      \'classes\': pred_classes,\n      \'probabilities\': tf.nn.softmax(logits, name=\'softmax_tensor\'),\n      \'decoded_labels\': pred_decoded_labels\n  }\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    # Delete \'decoded_labels\' from predictions because custom functions produce error when used with saved_model\n    predictions_without_decoded_labels = predictions.copy()\n    del predictions_without_decoded_labels[\'decoded_labels\']\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=predictions,\n        export_outputs={\n            \'preds\': tf.estimator.export.PredictOutput(\n                predictions_without_decoded_labels)\n        })\n\n  gt_decoded_labels = tf.py_func(preprocessing.decode_labels,\n                                 [labels, params[\'batch_size\'], params[\'num_classes\']], tf.uint8)\n\n  labels = tf.squeeze(labels, axis=3)  # reduce the channel dimension.\n\n  logits_by_num_classes = tf.reshape(logits, [-1, params[\'num_classes\']])\n  labels_flat = tf.reshape(labels, [-1, ])\n\n  valid_indices = tf.to_int32(labels_flat <= params[\'num_classes\'] - 1)\n  valid_logits = tf.dynamic_partition(logits_by_num_classes, valid_indices, num_partitions=2)[1]\n  valid_labels = tf.dynamic_partition(labels_flat, valid_indices, num_partitions=2)[1]\n\n  preds_flat = tf.reshape(pred_classes, [-1, ])\n  valid_preds = tf.dynamic_partition(preds_flat, valid_indices, num_partitions=2)[1]\n  confusion_matrix = tf.confusion_matrix(valid_labels, valid_preds, num_classes=params[\'num_classes\'])\n\n  predictions[\'valid_preds\'] = valid_preds\n  predictions[\'valid_labels\'] = valid_labels\n  predictions[\'confusion_matrix\'] = confusion_matrix\n\n  cross_entropy = tf.losses.sparse_softmax_cross_entropy(\n      logits=valid_logits, labels=valid_labels)\n\n  # Create a tensor named cross_entropy for logging purposes.\n  tf.identity(cross_entropy, name=\'cross_entropy\')\n  tf.summary.scalar(\'cross_entropy\', cross_entropy)\n\n  if not params[\'freeze_batch_norm\']:\n    train_var_list = [v for v in tf.trainable_variables()]\n  else:\n    train_var_list = [v for v in tf.trainable_variables()\n                      if \'beta\' not in v.name and \'gamma\' not in v.name]\n\n  # Add weight decay to the loss.\n  with tf.variable_scope(""total_loss""):\n    loss = cross_entropy + params.get(\'weight_decay\', _WEIGHT_DECAY) * tf.add_n(\n        [tf.nn.l2_loss(v) for v in train_var_list])\n  # loss = tf.losses.get_total_loss()  # obtain the regularization losses as well\n\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    tf.summary.image(\'images\',\n                     tf.concat(axis=2, values=[images, gt_decoded_labels, pred_decoded_labels]),\n                     max_outputs=params[\'tensorboard_images_max_outputs\'])  # Concatenate row-wise.\n\n    global_step = tf.train.get_or_create_global_step()\n\n    if params[\'learning_rate_policy\'] == \'piecewise\':\n      # Scale the learning rate linearly with the batch size. When the batch size\n      # is 128, the learning rate should be 0.1.\n      initial_learning_rate = 0.1 * params[\'batch_size\'] / 128\n      batches_per_epoch = params[\'num_train\'] / params[\'batch_size\']\n      # Multiply the learning rate by 0.1 at 100, 150, and 200 epochs.\n      boundaries = [int(batches_per_epoch * epoch) for epoch in [100, 150, 200]]\n      values = [initial_learning_rate * decay for decay in [1, 0.1, 0.01, 0.001]]\n      learning_rate = tf.train.piecewise_constant(\n          tf.cast(global_step, tf.int32), boundaries, values)\n    elif params[\'learning_rate_policy\'] == \'poly\':\n      learning_rate = tf.train.polynomial_decay(\n          params[\'initial_learning_rate\'],\n          tf.cast(global_step, tf.int32) - params[\'initial_global_step\'],\n          params[\'max_iter\'], params[\'end_learning_rate\'], power=params[\'power\'])\n    else:\n      raise ValueError(\'Learning rate policy must be ""piecewise"" or ""poly""\')\n\n    # Create a tensor named learning_rate for logging purposes\n    tf.identity(learning_rate, name=\'learning_rate\')\n    tf.summary.scalar(\'learning_rate\', learning_rate)\n\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate=learning_rate,\n        momentum=params[\'momentum\'])\n\n    # Batch norm requires update ops to be added as a dependency to the train_op\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.minimize(loss, global_step, var_list=train_var_list)\n  else:\n    train_op = None\n\n  accuracy = tf.metrics.accuracy(\n      valid_labels, valid_preds)\n  mean_iou = tf.metrics.mean_iou(valid_labels, valid_preds, params[\'num_classes\'])\n  metrics = {\'px_accuracy\': accuracy, \'mean_iou\': mean_iou}\n\n  # Create a tensor named train_accuracy for logging purposes\n  tf.identity(accuracy[1], name=\'train_px_accuracy\')\n  tf.summary.scalar(\'train_px_accuracy\', accuracy[1])\n\n  def compute_mean_iou(total_cm, name=\'mean_iou\'):\n    """"""Compute the mean intersection-over-union via the confusion matrix.""""""\n    sum_over_row = tf.to_float(tf.reduce_sum(total_cm, 0))\n    sum_over_col = tf.to_float(tf.reduce_sum(total_cm, 1))\n    cm_diag = tf.to_float(tf.diag_part(total_cm))\n    denominator = sum_over_row + sum_over_col - cm_diag\n\n    # The mean is only computed over classes that appear in the\n    # label or prediction tensor. If the denominator is 0, we need to\n    # ignore the class.\n    num_valid_entries = tf.reduce_sum(tf.cast(\n        tf.not_equal(denominator, 0), dtype=tf.float32))\n\n    # If the value of the denominator is 0, set it to 1 to avoid\n    # zero division.\n    denominator = tf.where(\n        tf.greater(denominator, 0),\n        denominator,\n        tf.ones_like(denominator))\n    iou = tf.div(cm_diag, denominator)\n\n    for i in range(params[\'num_classes\']):\n      tf.identity(iou[i], name=\'train_iou_class{}\'.format(i))\n      tf.summary.scalar(\'train_iou_class{}\'.format(i), iou[i])\n\n    # If the number of valid entries is 0 (no classes) we return 0.\n    result = tf.where(\n        tf.greater(num_valid_entries, 0),\n        tf.reduce_sum(iou, name=name) / num_valid_entries,\n        0)\n    return result\n\n  train_mean_iou = compute_mean_iou(mean_iou[1])\n\n  tf.identity(train_mean_iou, name=\'train_mean_iou\')\n  tf.summary.scalar(\'train_mean_iou\', train_mean_iou)\n\n  return tf.estimator.EstimatorSpec(\n      mode=mode,\n      predictions=predictions,\n      loss=loss,\n      train_op=train_op,\n      eval_metric_ops=metrics)\n'"
evaluate.py,8,"b'""""""Evaluate a DeepLab v3 model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\n\nimport tensorflow as tf\n\nimport deeplab_model\nfrom utils import preprocessing\nfrom utils import dataset_util\n\nimport numpy as np\nimport timeit\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'--image_data_dir\', type=str, default=\'dataset/VOCdevkit/VOC2012/JPEGImages\',\n                    help=\'The directory containing the image data.\')\n\nparser.add_argument(\'--label_data_dir\', type=str, default=\'dataset/VOCdevkit/VOC2012/SegmentationClassAug\',\n                    help=\'The directory containing the ground truth label data.\')\n\nparser.add_argument(\'--evaluation_data_list\', type=str, default=\'./dataset/val.txt\',\n                    help=\'Path to the file listing the evaluation images.\')\n\nparser.add_argument(\'--model_dir\', type=str, default=\'./model\',\n                    help=""Base directory for the model. ""\n                         ""Make sure \'model_checkpoint_path\' given in \'checkpoint\' file matches ""\n                         ""with checkpoint name."")\n\nparser.add_argument(\'--base_architecture\', type=str, default=\'resnet_v2_101\',\n                    choices=[\'resnet_v2_50\', \'resnet_v2_101\'],\n                    help=\'The architecture of base Resnet building block.\')\n\nparser.add_argument(\'--output_stride\', type=int, default=16,\n                    choices=[8, 16],\n                    help=\'Output stride for DeepLab v3. Currently 8 or 16 is supported.\')\n\n_NUM_CLASSES = 21\n\n\ndef main(unused_argv):\n  # Using the Winograd non-fused algorithms provides a small performance boost.\n  os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n  examples = dataset_util.read_examples_list(FLAGS.evaluation_data_list)\n  image_files = [os.path.join(FLAGS.image_data_dir, filename) + \'.jpg\' for filename in examples]\n  label_files = [os.path.join(FLAGS.label_data_dir, filename) + \'.png\' for filename in examples]\n\n  features, labels = preprocessing.eval_input_fn(image_files, label_files)\n\n  predictions = deeplab_model.deeplabv3_model_fn(\n      features,\n      labels,\n      tf.estimator.ModeKeys.EVAL,\n      params={\n          \'output_stride\': FLAGS.output_stride,\n          \'batch_size\': 1,  # Batch size must be 1 because the images\' size may differ\n          \'base_architecture\': FLAGS.base_architecture,\n          \'pre_trained_model\': None,\n          \'batch_norm_decay\': None,\n          \'num_classes\': _NUM_CLASSES,\n          \'freeze_batch_norm\': True\n      }).predictions\n\n  # Manually load the latest checkpoint\n  saver = tf.train.Saver()\n  with tf.Session() as sess:\n    ckpt = tf.train.get_checkpoint_state(FLAGS.model_dir)\n    saver.restore(sess, ckpt.model_checkpoint_path)\n\n    # Loop through the batches and store predictions and labels\n    step = 1\n    sum_cm = np.zeros((_NUM_CLASSES, _NUM_CLASSES), dtype=np.int32)\n    start = timeit.default_timer()\n    while True:\n      try:\n        preds = sess.run(predictions)\n        sum_cm += preds[\'confusion_matrix\']\n        if not step % 100:\n          stop = timeit.default_timer()\n          tf.logging.info(""current step = {} ({:.3f} sec)"".format(step, stop-start))\n          start = timeit.default_timer()\n        step += 1\n      except tf.errors.OutOfRangeError:\n        break\n\n    def compute_mean_iou(total_cm):\n      """"""Compute the mean intersection-over-union via the confusion matrix.""""""\n      sum_over_row = np.sum(total_cm, axis=0).astype(float)\n      sum_over_col = np.sum(total_cm, axis=1).astype(float)\n      cm_diag = np.diagonal(total_cm).astype(float)\n      denominator = sum_over_row + sum_over_col - cm_diag\n\n      # The mean is only computed over classes that appear in the\n      # label or prediction tensor. If the denominator is 0, we need to\n      # ignore the class.\n      num_valid_entries = np.sum((denominator != 0).astype(float))\n\n      # If the value of the denominator is 0, set it to 1 to avoid\n      # zero division.\n      denominator = np.where(\n          denominator > 0,\n          denominator,\n          np.ones_like(denominator))\n\n      ious = cm_diag / denominator\n\n      print(\'Intersection over Union for each class:\')\n      for i, iou in enumerate(ious):\n        print(\'    class {}: {:.4f}\'.format(i, iou))\n\n      # If the number of valid entries is 0 (no classes) we return 0.\n      m_iou = np.where(\n          num_valid_entries > 0,\n          np.sum(ious) / num_valid_entries,\n          0)\n      m_iou = float(m_iou)\n      print(\'mean Intersection over Union: {:.4f}\'.format(float(m_iou)))\n\n    def compute_accuracy(total_cm):\n      """"""Compute the accuracy via the confusion matrix.""""""\n      denominator = total_cm.sum().astype(float)\n      cm_diag_sum = np.diagonal(total_cm).sum().astype(float)\n\n      # If the number of valid entries is 0 (no classes) we return 0.\n      accuracy = np.where(\n          denominator > 0,\n          cm_diag_sum / denominator,\n          0)\n      accuracy = float(accuracy)\n      print(\'Pixel Accuracy: {:.4f}\'.format(float(accuracy)))\n\n    compute_mean_iou(sum_cm)\n    compute_accuracy(sum_cm)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
export_inference_graph.py,6,"b'""""""Export inference graph.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\n\nimport tensorflow as tf\n\nimport deeplab_model\nfrom utils import preprocessing\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'--model_dir\', type=str, default=\'./model\',\n                    help=""Base directory for the model. ""\n                         ""Make sure \'model_checkpoint_path\' given in \'checkpoint\' file matches ""\n                         ""with checkpoint name."")\n\nparser.add_argument(\'--export_dir\', type=str, default=\'dataset/export_output\',\n                    help=\'The directory where the exported SavedModel will be stored.\')\n\nparser.add_argument(\'--base_architecture\', type=str, default=\'resnet_v2_101\',\n                    choices=[\'resnet_v2_50\', \'resnet_v2_101\'],\n                    help=\'The architecture of base Resnet building block.\')\n\nparser.add_argument(\'--output_stride\', type=int, default=16,\n                    choices=[8, 16],\n                    help=\'Output stride for DeepLab v3. Currently 8 or 16 is supported.\')\n\n\n_NUM_CLASSES = 21\n\n\ndef main(unused_argv):\n  # Using the Winograd non-fused algorithms provides a small performance boost.\n  os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n  model = tf.estimator.Estimator(\n      model_fn=deeplab_model.deeplabv3_model_fn,\n      model_dir=FLAGS.model_dir,\n      params={\n          \'output_stride\': FLAGS.output_stride,\n          \'batch_size\': 1,  # Batch size must be 1 because the images\' size may differ\n          \'base_architecture\': FLAGS.base_architecture,\n          \'pre_trained_model\': None,\n          \'batch_norm_decay\': None,\n          \'num_classes\': _NUM_CLASSES,\n      })\n\n  # Export the model\n  def serving_input_receiver_fn():\n    image = tf.placeholder(tf.float32, [None, None, None, 3], name=\'image_tensor\')\n    receiver_tensors = {\'image\': image}\n    features = tf.map_fn(preprocessing.mean_image_subtraction, image)\n    return tf.estimator.export.ServingInputReceiver(\n        features=features,\n        receiver_tensors=receiver_tensors)\n\n  model.export_savedmodel(FLAGS.export_dir, serving_input_receiver_fn)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
inference.py,4,"b'""""""Run inference a DeepLab v3 model using tf.estimator API.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\n\nimport tensorflow as tf\n\nimport deeplab_model\nfrom utils import preprocessing\nfrom utils import dataset_util\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.python import debug as tf_debug\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'--data_dir\', type=str, default=\'dataset/VOCdevkit/VOC2012/JPEGImages\',\n                    help=\'The directory containing the image data.\')\n\nparser.add_argument(\'--output_dir\', type=str, default=\'./dataset/inference_output\',\n                    help=\'Path to the directory to generate the inference results\')\n\nparser.add_argument(\'--infer_data_list\', type=str, default=\'./dataset/sample_images_list.txt\',\n                    help=\'Path to the file listing the inferring images.\')\n\nparser.add_argument(\'--model_dir\', type=str, default=\'./model\',\n                    help=""Base directory for the model. ""\n                         ""Make sure \'model_checkpoint_path\' given in \'checkpoint\' file matches ""\n                         ""with checkpoint name."")\n\nparser.add_argument(\'--base_architecture\', type=str, default=\'resnet_v2_101\',\n                    choices=[\'resnet_v2_50\', \'resnet_v2_101\'],\n                    help=\'The architecture of base Resnet building block.\')\n\nparser.add_argument(\'--output_stride\', type=int, default=16,\n                    choices=[8, 16],\n                    help=\'Output stride for DeepLab v3. Currently 8 or 16 is supported.\')\n\nparser.add_argument(\'--debug\', action=\'store_true\',\n                    help=\'Whether to use debugger to track down bad values during training.\')\n\n_NUM_CLASSES = 21\n\n\ndef main(unused_argv):\n  # Using the Winograd non-fused algorithms provides a small performance boost.\n  os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n  pred_hooks = None\n  if FLAGS.debug:\n    debug_hook = tf_debug.LocalCLIDebugHook()\n    pred_hooks = [debug_hook]\n\n  model = tf.estimator.Estimator(\n      model_fn=deeplab_model.deeplabv3_model_fn,\n      model_dir=FLAGS.model_dir,\n      params={\n          \'output_stride\': FLAGS.output_stride,\n          \'batch_size\': 1,  # Batch size must be 1 because the images\' size may differ\n          \'base_architecture\': FLAGS.base_architecture,\n          \'pre_trained_model\': None,\n          \'batch_norm_decay\': None,\n          \'num_classes\': _NUM_CLASSES,\n      })\n\n  examples = dataset_util.read_examples_list(FLAGS.infer_data_list)\n  image_files = [os.path.join(FLAGS.data_dir, filename) for filename in examples]\n\n  predictions = model.predict(\n        input_fn=lambda: preprocessing.eval_input_fn(image_files),\n        hooks=pred_hooks)\n\n  output_dir = FLAGS.output_dir\n  if not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n  for pred_dict, image_path in zip(predictions, image_files):\n    image_basename = os.path.splitext(os.path.basename(image_path))[0]\n    output_filename = image_basename + \'_mask.png\'\n    path_to_output = os.path.join(output_dir, output_filename)\n\n    print(""generating:"", path_to_output)\n    mask = pred_dict[\'decoded_labels\']\n    mask = Image.fromarray(mask)\n    plt.axis(\'off\')\n    plt.imshow(mask)\n    plt.savefig(path_to_output, bbox_inches=\'tight\')\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
train.py,26,"b'""""""Train a DeepLab v3 model using tf.estimator API.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\n\nimport tensorflow as tf\nimport deeplab_model\nfrom utils import preprocessing\nfrom tensorflow.python import debug as tf_debug\n\nimport shutil\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'--model_dir\', type=str, default=\'./model\',\n                    help=\'Base directory for the model.\')\n\nparser.add_argument(\'--clean_model_dir\', action=\'store_true\',\n                    help=\'Whether to clean up the model directory if present.\')\n\nparser.add_argument(\'--train_epochs\', type=int, default=26,\n                    help=\'Number of training epochs: \'\n                         \'For 30K iteration with batch size 6, train_epoch = 17.01 (= 30K * 6 / 10,582). \'\n                         \'For 30K iteration with batch size 8, train_epoch = 22.68 (= 30K * 8 / 10,582). \'\n                         \'For 30K iteration with batch size 10, train_epoch = 25.52 (= 30K * 10 / 10,582). \'\n                         \'For 30K iteration with batch size 11, train_epoch = 31.19 (= 30K * 11 / 10,582). \'\n                         \'For 30K iteration with batch size 15, train_epoch = 42.53 (= 30K * 15 / 10,582). \'\n                         \'For 30K iteration with batch size 16, train_epoch = 45.36 (= 30K * 16 / 10,582).\')\n\nparser.add_argument(\'--epochs_per_eval\', type=int, default=1,\n                    help=\'The number of training epochs to run between evaluations.\')\n\nparser.add_argument(\'--tensorboard_images_max_outputs\', type=int, default=6,\n                    help=\'Max number of batch elements to generate for Tensorboard.\')\n\nparser.add_argument(\'--batch_size\', type=int, default=10,\n                    help=\'Number of examples per batch.\')\n\nparser.add_argument(\'--learning_rate_policy\', type=str, default=\'poly\',\n                    choices=[\'poly\', \'piecewise\'],\n                    help=\'Learning rate policy to optimize loss.\')\n\nparser.add_argument(\'--max_iter\', type=int, default=30000,\n                    help=\'Number of maximum iteration used for ""poly"" learning rate policy.\')\n\nparser.add_argument(\'--data_dir\', type=str, default=\'./dataset/\',\n                    help=\'Path to the directory containing the PASCAL VOC data tf record.\')\n\nparser.add_argument(\'--base_architecture\', type=str, default=\'resnet_v2_101\',\n                    choices=[\'resnet_v2_50\', \'resnet_v2_101\'],\n                    help=\'The architecture of base Resnet building block.\')\n\nparser.add_argument(\'--pre_trained_model\', type=str, default=\'./ini_checkpoints/resnet_v2_101/resnet_v2_101.ckpt\',\n                    help=\'Path to the pre-trained model checkpoint.\')\n\nparser.add_argument(\'--output_stride\', type=int, default=16,\n                    choices=[8, 16],\n                    help=\'Output stride for DeepLab v3. Currently 8 or 16 is supported.\')\n\nparser.add_argument(\'--freeze_batch_norm\', action=\'store_true\',\n                    help=\'Freeze batch normalization parameters during the training.\')\n\nparser.add_argument(\'--initial_learning_rate\', type=float, default=7e-3,\n                    help=\'Initial learning rate for the optimizer.\')\n\nparser.add_argument(\'--end_learning_rate\', type=float, default=1e-6,\n                    help=\'End learning rate for the optimizer.\')\n\nparser.add_argument(\'--initial_global_step\', type=int, default=0,\n                    help=\'Initial global step for controlling learning rate when fine-tuning model.\')\n\nparser.add_argument(\'--weight_decay\', type=float, default=2e-4,\n                    help=\'The weight decay to use for regularizing the model.\')\n\nparser.add_argument(\'--debug\', action=\'store_true\',\n                    help=\'Whether to use debugger to track down bad values during training.\')\n\n_NUM_CLASSES = 21\n_HEIGHT = 513\n_WIDTH = 513\n_DEPTH = 3\n_MIN_SCALE = 0.5\n_MAX_SCALE = 2.0\n_IGNORE_LABEL = 255\n\n_POWER = 0.9\n_MOMENTUM = 0.9\n\n_BATCH_NORM_DECAY = 0.9997\n\n_NUM_IMAGES = {\n    \'train\': 10582,\n    \'validation\': 1449,\n}\n\n\ndef get_filenames(is_training, data_dir):\n  """"""Return a list of filenames.\n\n  Args:\n    is_training: A boolean denoting whether the input is for training.\n    data_dir: path to the the directory containing the input data.\n\n  Returns:\n    A list of file names.\n  """"""\n  if is_training:\n    return [os.path.join(data_dir, \'voc_train.record\')]\n  else:\n    return [os.path.join(data_dir, \'voc_val.record\')]\n\n\ndef parse_record(raw_record):\n  """"""Parse PASCAL image and label from a tf record.""""""\n  keys_to_features = {\n      \'image/height\':\n      tf.FixedLenFeature((), tf.int64),\n      \'image/width\':\n      tf.FixedLenFeature((), tf.int64),\n      \'image/encoded\':\n      tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\':\n      tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n      \'label/encoded\':\n      tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'label/format\':\n      tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n  }\n\n  parsed = tf.parse_single_example(raw_record, keys_to_features)\n\n  # height = tf.cast(parsed[\'image/height\'], tf.int32)\n  # width = tf.cast(parsed[\'image/width\'], tf.int32)\n\n  image = tf.image.decode_image(\n      tf.reshape(parsed[\'image/encoded\'], shape=[]), _DEPTH)\n  image = tf.to_float(tf.image.convert_image_dtype(image, dtype=tf.uint8))\n  image.set_shape([None, None, 3])\n\n  label = tf.image.decode_image(\n      tf.reshape(parsed[\'label/encoded\'], shape=[]), 1)\n  label = tf.to_int32(tf.image.convert_image_dtype(label, dtype=tf.uint8))\n  label.set_shape([None, None, 1])\n\n  return image, label\n\n\ndef preprocess_image(image, label, is_training):\n  """"""Preprocess a single image of layout [height, width, depth].""""""\n  if is_training:\n    # Randomly scale the image and label.\n    image, label = preprocessing.random_rescale_image_and_label(\n        image, label, _MIN_SCALE, _MAX_SCALE)\n\n    # Randomly crop or pad a [_HEIGHT, _WIDTH] section of the image and label.\n    image, label = preprocessing.random_crop_or_pad_image_and_label(\n        image, label, _HEIGHT, _WIDTH, _IGNORE_LABEL)\n\n    # Randomly flip the image and label horizontally.\n    image, label = preprocessing.random_flip_left_right_image_and_label(\n        image, label)\n\n    image.set_shape([_HEIGHT, _WIDTH, 3])\n    label.set_shape([_HEIGHT, _WIDTH, 1])\n\n  image = preprocessing.mean_image_subtraction(image)\n\n  return image, label\n\n\ndef input_fn(is_training, data_dir, batch_size, num_epochs=1):\n  """"""Input_fn using the tf.data input pipeline for CIFAR-10 dataset.\n\n  Args:\n    is_training: A boolean denoting whether the input is for training.\n    data_dir: The directory containing the input data.\n    batch_size: The number of samples per batch.\n    num_epochs: The number of epochs to repeat the dataset.\n\n  Returns:\n    A tuple of images and labels.\n  """"""\n  dataset = tf.data.Dataset.from_tensor_slices(get_filenames(is_training, data_dir))\n  dataset = dataset.flat_map(tf.data.TFRecordDataset)\n\n  if is_training:\n    # When choosing shuffle buffer sizes, larger sizes result in better\n    # randomness, while smaller sizes have better performance.\n    # is a relatively small dataset, we choose to shuffle the full epoch.\n    dataset = dataset.shuffle(buffer_size=_NUM_IMAGES[\'train\'])\n\n  dataset = dataset.map(parse_record)\n  dataset = dataset.map(\n      lambda image, label: preprocess_image(image, label, is_training))\n  dataset = dataset.prefetch(batch_size)\n\n  # We call repeat after shuffling, rather than before, to prevent separate\n  # epochs from blending together.\n  dataset = dataset.repeat(num_epochs)\n  dataset = dataset.batch(batch_size)\n\n  iterator = dataset.make_one_shot_iterator()\n  images, labels = iterator.get_next()\n\n  return images, labels\n\n\ndef main(unused_argv):\n  # Using the Winograd non-fused algorithms provides a small performance boost.\n  os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n  if FLAGS.clean_model_dir:\n    shutil.rmtree(FLAGS.model_dir, ignore_errors=True)\n\n  # Set up a RunConfig to only save checkpoints once per training cycle.\n  run_config = tf.estimator.RunConfig().replace(save_checkpoints_secs=1e9)\n  model = tf.estimator.Estimator(\n      model_fn=deeplab_model.deeplabv3_model_fn,\n      model_dir=FLAGS.model_dir,\n      config=run_config,\n      params={\n          \'output_stride\': FLAGS.output_stride,\n          \'batch_size\': FLAGS.batch_size,\n          \'base_architecture\': FLAGS.base_architecture,\n          \'pre_trained_model\': FLAGS.pre_trained_model,\n          \'batch_norm_decay\': _BATCH_NORM_DECAY,\n          \'num_classes\': _NUM_CLASSES,\n          \'tensorboard_images_max_outputs\': FLAGS.tensorboard_images_max_outputs,\n          \'weight_decay\': FLAGS.weight_decay,\n          \'learning_rate_policy\': FLAGS.learning_rate_policy,\n          \'num_train\': _NUM_IMAGES[\'train\'],\n          \'initial_learning_rate\': FLAGS.initial_learning_rate,\n          \'max_iter\': FLAGS.max_iter,\n          \'end_learning_rate\': FLAGS.end_learning_rate,\n          \'power\': _POWER,\n          \'momentum\': _MOMENTUM,\n          \'freeze_batch_norm\': FLAGS.freeze_batch_norm,\n          \'initial_global_step\': FLAGS.initial_global_step\n      })\n\n  for _ in range(FLAGS.train_epochs // FLAGS.epochs_per_eval):\n    tensors_to_log = {\n      \'learning_rate\': \'learning_rate\',\n      \'cross_entropy\': \'cross_entropy\',\n      \'train_px_accuracy\': \'train_px_accuracy\',\n      \'train_mean_iou\': \'train_mean_iou\',\n    }\n\n    logging_hook = tf.train.LoggingTensorHook(\n        tensors=tensors_to_log, every_n_iter=10)\n    train_hooks = [logging_hook]\n    eval_hooks = None\n\n    if FLAGS.debug:\n      debug_hook = tf_debug.LocalCLIDebugHook()\n      train_hooks.append(debug_hook)\n      eval_hooks = [debug_hook]\n\n    tf.logging.info(""Start training."")\n    model.train(\n        input_fn=lambda: input_fn(True, FLAGS.data_dir, FLAGS.batch_size, FLAGS.epochs_per_eval),\n        hooks=train_hooks,\n        # steps=1  # For debug\n    )\n\n    tf.logging.info(""Start evaluation."")\n    # Evaluate the model and print results\n    eval_results = model.evaluate(\n        # Batch size must be 1 for testing because the images\' size differs\n        input_fn=lambda: input_fn(False, FLAGS.data_dir, 1),\n        hooks=eval_hooks,\n        # steps=1  # For debug\n    )\n    print(eval_results)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
utils/__init__.py,0,b''
utils/dataset_util.py,16,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utility functions for creating TFRecord data sets.\nsource: https://github.com/tensorflow/models/blob/master/research/object_detection/utils/dataset_util.py\n""""""\n\nimport tensorflow as tf\n\n\ndef int64_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef int64_list_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef bytes_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef bytes_list_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\n\ndef float_list_feature(value):\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef read_examples_list(path):\n  """"""Read list of training or validation examples.\n\n  The file is assumed to contain a single example per line where the first\n  token in the line is an identifier that allows us to find the image and\n  annotation xml for that example.\n\n  For example, the line:\n  xyz 3\n  would allow us to find files xyz.jpg and xyz.xml (the 3 would be ignored).\n\n  Args:\n    path: absolute path to examples list file.\n\n  Returns:\n    list of example identifiers (strings).\n  """"""\n  with tf.gfile.GFile(path) as fid:\n    lines = fid.readlines()\n  return [line.strip().split(\' \')[0] for line in lines]\n\n\ndef recursive_parse_xml_to_dict(xml):\n  """"""Recursively parses XML contents to python dict.\n\n  We assume that `object` tags are the only ones that can appear\n  multiple times at the same level of a tree.\n\n  Args:\n    xml: xml tree obtained by parsing XML file contents using lxml.etree\n\n  Returns:\n    Python dictionary holding XML contents.\n  """"""\n  if not xml:\n    return {xml.tag: xml.text}\n  result = {}\n  for child in xml:\n    child_result = recursive_parse_xml_to_dict(child)\n    if child.tag != \'object\':\n      result[child.tag] = child_result[child.tag]\n    else:\n      if child.tag not in result:\n        result[child.tag] = []\n      result[child.tag].append(child_result[child.tag])\n  return {xml.tag: result}\n\n\ndef make_initializable_iterator(dataset):\n  """"""Creates an iterator, and initializes tables.\n\n  This is useful in cases where make_one_shot_iterator wouldn\'t work because\n  the graph contains a hash table that needs to be initialized.\n\n  Args:\n    dataset: A `tf.data.Dataset` object.\n\n  Returns:\n    A `tf.data.Iterator`.\n  """"""\n  iterator = dataset.make_initializable_iterator()\n  tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)\n  return iterator\n\n\ndef read_dataset(\n    file_read_func, decode_func, input_files, config, num_workers=1,\n    worker_index=0):\n  """"""Reads a dataset, and handles repetition and shuffling.\n\n  Args:\n    file_read_func: Function to use in tf.data.Dataset.interleave, to read\n      every individual file into a tf.data.Dataset.\n    decode_func: Function to apply to all records.\n    input_files: A list of file paths to read.\n    config: A input_reader_builder.InputReader object.\n    num_workers: Number of workers / shards.\n    worker_index: Id for the current worker.\n\n  Returns:\n    A tf.data.Dataset based on config.\n  """"""\n  # Shard, shuffle, and read files.\n  filenames = tf.concat([tf.matching_files(pattern) for pattern in input_files],\n                        0)\n  dataset = tf.data.Dataset.from_tensor_slices(filenames)\n  dataset = dataset.shard(num_workers, worker_index)\n  dataset = dataset.repeat(config.num_epochs or None)\n  if config.shuffle:\n    dataset = dataset.shuffle(config.filenames_shuffle_buffer_size,\n                              reshuffle_each_iteration=True)\n\n  # Read file records and shuffle them.\n  # If cycle_length is larger than the number of files, more than one reader\n  # will be assigned to the same file, leading to repetition.\n  cycle_length = tf.cast(\n      tf.minimum(config.num_readers, tf.size(filenames)), tf.int64)\n  # TODO: find the optimal block_length.\n  dataset = dataset.interleave(\n      file_read_func, cycle_length=cycle_length, block_length=1)\n\n  if config.shuffle:\n    dataset = dataset.shuffle(config.shuffle_buffer_size,\n                              reshuffle_each_iteration=True)\n\n  dataset = dataset.map(decode_func, num_parallel_calls=config.num_readers)\n  return dataset.prefetch(config.prefetch_buffer_size)\n'"
utils/preprocessing.py,35,"b'""""""Utility functions for preprocessing data sets.""""""\n\nfrom PIL import Image\nimport numpy as np\nimport tensorflow as tf\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n\n# colour map\nlabel_colours = [(0, 0, 0),  # 0=background\n                 # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n                 (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),\n                 # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n                 (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0),\n                 # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person\n                 (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128),\n                 # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n                 (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)]\n\n\ndef decode_labels(mask, num_images=1, num_classes=21):\n  """"""Decode batch of segmentation masks.\n\n  Args:\n    mask: result of inference after taking argmax.\n    num_images: number of images to decode from the batch.\n    num_classes: number of classes to predict (including background).\n\n  Returns:\n    A batch with num_images RGB images of the same size as the input.\n  """"""\n  n, h, w, c = mask.shape\n  assert (n >= num_images), \'Batch size %d should be greater or equal than number of images to save %d.\' \\\n                            % (n, num_images)\n  outputs = np.zeros((num_images, h, w, 3), dtype=np.uint8)\n  for i in range(num_images):\n    img = Image.new(\'RGB\', (len(mask[i, 0]), len(mask[i])))\n    pixels = img.load()\n    for j_, j in enumerate(mask[i, :, :, 0]):\n      for k_, k in enumerate(j):\n        if k < num_classes:\n          pixels[k_, j_] = label_colours[k]\n    outputs[i] = np.array(img)\n  return outputs\n\n\ndef mean_image_addition(image, means=(_R_MEAN, _G_MEAN, _B_MEAN)):\n  """"""Adds the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n  num_channels = image.get_shape().as_list()[-1]\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] += means[i]\n  return tf.concat(axis=2, values=channels)\n\n\ndef mean_image_subtraction(image, means=(_R_MEAN, _G_MEAN, _B_MEAN)):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n  num_channels = image.get_shape().as_list()[-1]\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] -= means[i]\n  return tf.concat(axis=2, values=channels)\n\n\ndef random_rescale_image_and_label(image, label, min_scale, max_scale):\n  """"""Rescale an image and label with in target scale.\n\n  Rescales an image and label within the range of target scale.\n\n  Args:\n    image: 3-D Tensor of shape `[height, width, channels]`.\n    label: 3-D Tensor of shape `[height, width, 1]`.\n    min_scale: Min target scale.\n    max_scale: Max target scale.\n\n  Returns:\n    Cropped and/or padded image.\n    If `images` was 3-D, a 3-D float Tensor of shape\n    `[new_height, new_width, channels]`.\n    If `labels` was 3-D, a 3-D float Tensor of shape\n    `[new_height, new_width, 1]`.\n  """"""\n  if min_scale <= 0:\n    raise ValueError(\'\\\'min_scale\\\' must be greater than 0.\')\n  elif max_scale <= 0:\n    raise ValueError(\'\\\'max_scale\\\' must be greater than 0.\')\n  elif min_scale >= max_scale:\n    raise ValueError(\'\\\'max_scale\\\' must be greater than \\\'min_scale\\\'.\')\n\n  shape = tf.shape(image)\n  height = tf.to_float(shape[0])\n  width = tf.to_float(shape[1])\n  scale = tf.random_uniform(\n      [], minval=min_scale, maxval=max_scale, dtype=tf.float32)\n  new_height = tf.to_int32(height * scale)\n  new_width = tf.to_int32(width * scale)\n  image = tf.image.resize_images(image, [new_height, new_width],\n                                 method=tf.image.ResizeMethod.BILINEAR)\n  # Since label classes are integers, nearest neighbor need to be used.\n  label = tf.image.resize_images(label, [new_height, new_width],\n                                 method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n  return image, label\n\n\ndef random_crop_or_pad_image_and_label(image, label, crop_height, crop_width, ignore_label):\n  """"""Crops and/or pads an image to a target width and height.\n\n  Resizes an image to a target width and height by rondomly\n  cropping the image or padding it evenly with zeros.\n\n  Args:\n    image: 3-D Tensor of shape `[height, width, channels]`.\n    label: 3-D Tensor of shape `[height, width, 1]`.\n    crop_height: The new height.\n    crop_width: The new width.\n    ignore_label: Label class to be ignored.\n\n  Returns:\n    Cropped and/or padded image.\n    If `images` was 3-D, a 3-D float Tensor of shape\n    `[new_height, new_width, channels]`.\n  """"""\n  label = label - ignore_label  # Subtract due to 0 padding.\n  label = tf.to_float(label)\n  image_height = tf.shape(image)[0]\n  image_width = tf.shape(image)[1]\n  image_and_label = tf.concat([image, label], axis=2)\n  image_and_label_pad = tf.image.pad_to_bounding_box(\n      image_and_label, 0, 0,\n      tf.maximum(crop_height, image_height),\n      tf.maximum(crop_width, image_width))\n  image_and_label_crop = tf.random_crop(\n      image_and_label_pad, [crop_height, crop_width, 4])\n\n  image_crop = image_and_label_crop[:, :, :3]\n  label_crop = image_and_label_crop[:, :, 3:]\n  label_crop += ignore_label\n  label_crop = tf.to_int32(label_crop)\n\n  return image_crop, label_crop\n\n\ndef random_flip_left_right_image_and_label(image, label):\n  """"""Randomly flip an image and label horizontally (left to right).\n\n  Args:\n    image: A 3-D tensor of shape `[height, width, channels].`\n    label: A 3-D tensor of shape `[height, width, 1].`\n\n  Returns:\n    A 3-D tensor of the same type and shape as `image`.\n    A 3-D tensor of the same type and shape as `label`.\n  """"""\n  uniform_random = tf.random_uniform([], 0, 1.0)\n  mirror_cond = tf.less(uniform_random, .5)\n  image = tf.cond(mirror_cond, lambda: tf.reverse(image, [1]), lambda: image)\n  label = tf.cond(mirror_cond, lambda: tf.reverse(label, [1]), lambda: label)\n\n  return image, label\n\n\ndef eval_input_fn(image_filenames, label_filenames=None, batch_size=1):\n  """"""An input function for evaluation and inference.\n\n  Args:\n    image_filenames: The file names for the inferred images.\n    label_filenames: The file names for the grand truth labels.\n    batch_size: The number of samples per batch. Need to be 1\n        for the images of different sizes.\n\n  Returns:\n    A tuple of images and labels.\n  """"""\n  # Reads an image from a file, decodes it into a dense tensor\n  def _parse_function(filename, is_label):\n    if not is_label:\n      image_filename, label_filename = filename, None\n    else:\n      image_filename, label_filename = filename\n\n    image_string = tf.read_file(image_filename)\n    image = tf.image.decode_image(image_string)\n    image = tf.to_float(tf.image.convert_image_dtype(image, dtype=tf.uint8))\n    image.set_shape([None, None, 3])\n\n    image = mean_image_subtraction(image)\n\n    if not is_label:\n      return image\n    else:\n      label_string = tf.read_file(label_filename)\n      label = tf.image.decode_image(label_string)\n      label = tf.to_int32(tf.image.convert_image_dtype(label, dtype=tf.uint8))\n      label.set_shape([None, None, 1])\n\n      return image, label\n\n  if label_filenames is None:\n    input_filenames = image_filenames\n  else:\n    input_filenames = (image_filenames, label_filenames)\n\n  dataset = tf.data.Dataset.from_tensor_slices(input_filenames)\n  if label_filenames is None:\n    dataset = dataset.map(lambda x: _parse_function(x, False))\n  else:\n    dataset = dataset.map(lambda x, y: _parse_function((x, y), True))\n  dataset = dataset.prefetch(batch_size)\n  dataset = dataset.batch(batch_size)\n  iterator = dataset.make_one_shot_iterator()\n\n  if label_filenames is None:\n    images = iterator.get_next()\n    labels = None\n  else:\n    images, labels = iterator.get_next()\n\n  return images, labels\n'"
