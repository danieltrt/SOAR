file_path,api_count,code
attention.py,20,"b'from PIL import Image\nimport tensorflow as tf\nimport tflib\nimport tflib.ops\nimport tflib.network\nfrom tqdm import tqdm\nimport numpy as np\nimport data_loaders\nimport time\nimport os\n\nBATCH_SIZE      = 20\nEMB_DIM         = 80\nENC_DIM         = 256\nDEC_DIM         = ENC_DIM*2\nNUM_FEATS_START = 64\nD               = NUM_FEATS_START*8\nV               = 502\nNB_EPOCHS       = 50\nH               = 20\nW               = 50\n\n# with tf.device(""/cpu:0""):\n#     custom_runner = data_loaders.CustomRunner()\n#     X,seqs,mask,reset = custom_runner.get_inputs()\n#\n# print X,seqs\nX = tf.placeholder(shape=(None,None,None,None),dtype=tf.float32)\nmask = tf.placeholder(shape=(None,None),dtype=tf.int32)\nseqs = tf.placeholder(shape=(None,None),dtype=tf.int32)\nlearn_rate = tf.placeholder(tf.float32)\ninput_seqs = seqs[:,:-1]\ntarget_seqs = seqs[:,1:]\nemb_seqs = tflib.ops.Embedding(\'Embedding\',V,EMB_DIM,input_seqs)\n\nctx = tflib.network.im2latex_cnn(X,NUM_FEATS_START,True)\nout,state = tflib.ops.FreeRunIm2LatexAttention(\'AttLSTM\',emb_seqs,ctx,EMB_DIM,ENC_DIM,DEC_DIM,D,H,W)\nlogits = tflib.ops.Linear(\'MLP.1\',out,DEC_DIM,V)\npredictions = tf.argmax(tf.nn.softmax(logits[:,-1]),axis=1)\n\n\nloss = tf.reshape(tf.nn.sparse_softmax_cross_entropy_with_logits(\n    logits=tf.reshape(logits,[-1,V]),\n    labels=tf.reshape(seqs[:,1:],[-1])\n    ), [tf.shape(X)[0], -1])\n\nmask_mult = tf.to_float(mask[:,1:])\nloss = tf.reduce_sum(loss*mask_mult)/tf.reduce_sum(mask_mult)\n\n#train_step = tf.train.AdamOptimizer(1e-2).minimize(loss)\noptimizer = tf.train.GradientDescentOptimizer(learn_rate)\ngvs = optimizer.compute_gradients(loss)\ncapped_gvs = [(tf.clip_by_norm(grad, 5.), var) for grad, var in gvs]\ntrain_step = optimizer.apply_gradients(capped_gvs)\n\ndef predict(set=\'test\',batch_size=1,visualize=True):\n    if visualize:\n        assert (batch_size==1), ""Batch size should be 1 for visualize mode""\n    import random\n    # f = np.load(\'train_list_buckets.npy\').tolist()\n    f = np.load(set+\'_buckets.npy\').tolist()\n    random_key = random.choice(f.keys())\n    #random_key = (160,40)\n    f = f[random_key]\n    imgs = []\n    print ""Image shape: "",random_key\n    while len(imgs)!=batch_size:\n        start = np.random.randint(0,len(f),1)[0]\n        if os.path.exists(\'./images_processed/\'+f[start][0]):\n            imgs.append(np.asarray(Image.open(\'./images_processed/\'+f[start][0]).convert(\'YCbCr\'))[:,:,0][:,:,None])\n\n    imgs = np.asarray(imgs,dtype=np.float32).transpose(0,3,1,2)\n    inp_seqs = np.zeros((batch_size,160)).astype(\'int32\')\n    print imgs.shape\n    inp_seqs[:,0] = np.load(\'properties.npy\').tolist()[\'char_to_idx\'][\'#START\']\n    tflib.ops.ctx_vector = []\n\n    l_size = random_key[0]*2\n    r_size = random_key[1]*2\n    inp_image = Image.fromarray(imgs[0][0]).resize((l_size,r_size))\n    l = int(np.ceil(random_key[1]/8.))\n    r = int(np.ceil(random_key[0]/8.))\n    properties = np.load(\'properties.npy\').tolist()\n    idx_to_chars = lambda Y: \' \'.join(map(lambda x: properties[\'idx_to_char\'][x],Y))\n\n    for i in xrange(1,160):\n        inp_seqs[:,i] = sess.run(predictions,feed_dict={X:imgs,input_seqs:inp_seqs[:,:i]})\n        #print i,inp_seqs[:,i]\n        if visualize==True:\n            att = sorted(list(enumerate(tflib.ops.ctx_vector[-1].flatten())),key=lambda tup:tup[1],reverse=True)\n            idxs,att = zip(*att)\n            j=1\n            while sum(att[:j])<0.9:\n                j+=1\n            positions = idxs[:j]\n            print ""Attention weights: "",att[:j]\n            positions = [(pos/r,pos%r) for pos in positions]\n            outarray = np.ones((l,r))*255.\n            for loc in positions:\n                outarray[loc] = 0.\n            out_image = Image.fromarray(outarray).resize((l_size,r_size),Image.NEAREST)\n            print ""Latex sequence: "",idx_to_chars(inp_seqs[0,:i])\n            outp = Image.blend(inp_image.convert(\'RGBA\'),out_image.convert(\'RGBA\'),0.5)\n            outp.show(title=properties[\'idx_to_char\'][inp_seqs[0,i]])\n            # raw_input()\n            time.sleep(3)\n            os.system(\'pkill display\')\n\n    np.save(\'pred_imgs\',imgs)\n    np.save(\'pred_latex\',inp_seqs)\n    print ""Saved npy files! Use Predict.ipynb to view results""\n    return inp_seqs\n\ndef score(set=\'valid\',batch_size=32):\n    score_itr = data_loaders.data_iterator(set,batch_size)\n    losses = []\n    start = time.time()\n    for score_imgs,score_seqs,score_mask in score_itr:\n        _loss = sess.run(loss,feed_dict={X:score_imgs,seqs:score_seqs,mask:score_mask})\n        losses.append(_loss)\n        print _loss\n\n    set_loss = np.mean(losses)\n    perp = np.mean(map(lambda x: np.power(np.e,x), losses))\n    print ""\\tMean %s Loss: "", set_loss\n    print ""\\tTotal %s Time: "", time.time()-start\n    print ""\\tMean %s Perplexity: "", perp\n    return set_loss, perp\n\nsess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=8))\ninit = tf.global_variables_initializer()\n# init = tf.initialize_all_variables()\nsess.run(init)\nsaver = tf.train.Saver()\n# saver.restore(sess,\'./weights_best.ckpt\')\n## start the tensorflow QueueRunner\'s\n# tf.train.start_queue_runners(sess=sess)\n## start our custom queue runner\'s threads\n# custom_runner.start_threads(sess)\n\nlosses = []\ntimes = []\nprint ""Compiled Train function!""\n## Test is train func runs\n# train_fn(np.random.randn(32,1,128,256).astype(\'float32\'),np.random.randint(0,107,(32,50)).astype(\'int32\'),np.random.randint(0,2,(32,50)).astype(\'int32\'), np.zeros((32,1024)).astype(\'float32\'))\ni=0\nlr = 0.1\nbest_perp = np.finfo(np.float32).max\nfor i in xrange(i,NB_EPOCHS):\n    iter=0\n    costs=[]\n    times=[]\n    itr = data_loaders.data_iterator(\'train\', BATCH_SIZE)\n    for train_img,train_seq,train_mask in itr:\n        iter += 1\n        start = time.time()\n        _ , _loss = sess.run([train_step,loss],feed_dict={X:train_img,seqs:train_seq,mask:train_mask,learn_rate:lr})\n        # _ , _loss = sess.run([train_step,loss],feed_dict={X:train_img,seqs:train_seq,mask:train_mask})\n\n        times.append(time.time()-start)\n        costs.append(_loss)\n        if iter%100==0:\n            print ""Iter: %d (Epoch %d)""%(iter,i+1)\n            print ""\\tMean cost: "",np.mean(costs)\n            print ""\\tMean time: "",np.mean(times)\n\n    print ""\\n\\nEpoch %d Completed!""%(i+1)\n    print ""\\tMean train cost: "",np.mean(costs)\n    print ""\\tMean train perplexity: "",np.mean(map(lambda x: np.power(np.e,x), costs))\n    print ""\\tMean time: "",np.mean(times)\n    val_loss, val_perp = score(\'valid\',BATCH_SIZE)\n    if val_perp < best_perp:\n        best_perp = val_perp\n        saver.save(sess,""weights_best.ckpt"")\n        print ""\\tBest Perplexity Till Now! Saving state!""\n    else:\n        lr = lr * 0.5\n    print ""\\n\\n""\n\n#sess.run([train_step,loss],feed_dict={X:np.random.randn(32,1,256,512),seqs:np.random.randint(0,107,(32,40)),mask:np.random.randint(0,2,(32,40))})\n'"
data_loaders.py,4,"b'import tensorflow as tf\nimport time\nimport threading\nimport numpy as np\nimport re\nimport glob\nfrom PIL import Image\n\ndef data_iterator(set=\'train\',batch_size = 32):\n    \'\'\'\n    Python data generator to facilitate mini-batch training\n    Arguments:\n        set - \'train\',\'valid\',\'test\' sets\n        batch_size - integer (Usually 32,64,128, etc.)\n    \'\'\'\n    train_dict = np.load(set+\'_buckets.npy\').tolist()\n    print ""Length of %s data: ""%set,np.sum([len(train_dict[x]) for x in train_dict.keys()])\n\n    for keys in train_dict.keys():\n        train_list = train_dict[keys]\n        N_FILES = (len(train_list)//batch_size)*batch_size\n        for batch_idx in xrange(0,N_FILES,batch_size):\n            train_sublist = train_list[batch_idx:batch_idx+batch_size]\n            imgs = []\n            batch_forms = []\n            for x,y in train_sublist:\n                imgs.append(np.asarray(Image.open(\'./images_processed/\'+x).convert(\'YCbCr\'))[:,:,0][:,:,None])\n                batch_forms.append(y)\n            imgs = np.asarray(imgs,dtype=np.float32).transpose(0,3,1,2)\n            lens = [len(x) for x in batch_forms]\n\n            mask = np.zeros((batch_size,max(lens)),dtype=np.int32)\n            Y = np.zeros((batch_size,max(lens)),dtype=np.int32)\n            for i,form in enumerate(batch_forms):\n                mask[i,:len(form)] = 1\n                Y[i,:len(form)] = form\n            yield imgs, Y, mask\n\n## Deprecated!! Queue Runners cannot be used as image is of variable size\nclass CustomRunner(object):\n    """"""\n    This class manages the the background threads needed to fill\n        a queue full of data.\n    """"""\n    def __init__(self,batch_size=32, SEQ_LEN=50):\n        self.dataX = tf.placeholder(dtype=tf.float32, shape=[None, 1, 128, 256])\n        self.dataY = tf.placeholder(dtype=tf.int32, shape=[None, None])\n        self.dataMask = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        # The actual queue of data. The queue contains a vector for\n        # the mnist features, and a scalar label.\n        self.queue = tf.RandomShuffleQueue(dtypes=[tf.float32, tf.int32, tf.int32],\n                                           capacity=2000,\n                                           min_after_dequeue=1000)\n\n        self.SEQ_LEN = SEQ_LEN\n        self.batch_size = batch_size\n\n        # The symbolic operation to add data to the queue\n        # we could do some preprocessing here or do it in numpy. In this example\n        # we do the scaling in numpy\n        self.enqueue_op = self.queue.enqueue_many([self.dataX, self.dataY, self.dataMask])\n\n    def get_inputs(self):\n        """"""\n        Return\'s tensors containing a batch of images and labels\n        """"""\n        images_batch, labels_batch, mask_batch = self.queue.dequeue_many(self.batch_size)\n        return images_batch, labels_batch, mask_batch\n\n    def thread_main(self, sess):\n        """"""\n        Function run on alternate thread. Basically, keep adding data to the queue.\n        """"""\n        for dataX, dataY, dataMask in data_iterator(self.batch_size,self.SEQ_LEN):\n            sess.run(self.enqueue_op, feed_dict={self.dataX:dataX, self.dataY:dataY, self.dataMask:dataMask})\n\n    def start_threads(self, sess, n_threads=1):\n        """""" Start background threads to feed queue """"""\n        threads = []\n        for n in range(n_threads):\n            t = threading.Thread(target=self.thread_main, args=(sess,))\n            t.daemon = True # thread will close when parent quits\n            t.start()\n            threads.append(t)\n        return threads\n'"
predict.py,8,"b'from PIL import Image\nimport tensorflow as tf\nimport tflib\nimport tflib.ops\nimport tflib.network\nfrom tqdm import tqdm\nimport numpy as np\nimport data_loaders\nimport time\nimport os\nimport json\nimport sys\nimport pyperclip\nsys.path.append(\'./im2markup/scripts/utils\')\nfrom image_utils import *\nimport subprocess\nimport glob\n\nBATCH_SIZE      = 2\nEMB_DIM         = 80\nENC_DIM         = 256\nDEC_DIM         = ENC_DIM*2\nNUM_FEATS_START = 64\nD               = NUM_FEATS_START*8\nV               = 502\nNB_EPOCHS       = 50\nH               = 20\nW               = 50\n\nX = tf.placeholder(shape=(None,None,None,None),dtype=tf.float32)\nmask = tf.placeholder(shape=(None,None),dtype=tf.int32)\nseqs = tf.placeholder(shape=(None,None),dtype=tf.int32)\nlearn_rate = tf.placeholder(tf.float32)\n\nctx = tflib.network.im2latex_cnn(X,NUM_FEATS_START,True)\nout,state = tflib.ops.FreeRunIm2LatexAttention(\'AttLSTM\',ctx,EMB_DIM,V,ENC_DIM,DEC_DIM,D,H,W)\npredictions = tf.argmax(out[:,:,-V:],axis=2)\n\nsess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=8))\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nweights = np.load(\'weights_best_numpy.npy\').tolist()\nvar_dict = {x.name: x for x in tf.get_collection(\'variables\')}\nfor key in var_dict.keys():\n    if key not in weights.keys():\n        print key,"" not found!!""\n    else:\n        sess.run(var_dict[key].assign(weights[key]))\n        print ""Initialized "",key\nsess.run(var_dict[\'RNN/while/Embedding/Embedding:0\'].assign(weights[\'Embedding/Embedding:0\']))\nsess.run(var_dict[\'RNN/while/MLP.1/MLP.1.W:0\'].assign(weights[\'MLP.1/MLP.1.W:0\']))\nsess.run(var_dict[\'RNN/while/MLP.1/MLP.1.b:0\'].assign(weights[\'MLP.1/MLP.1.b:0\']))\nproperties = np.load(\'properties.npy\').tolist()\ndef show():\n    batch_size=1\n    imgs = np.asarray(Image.open(\'tmp3.png\').convert(\'YCbCr\'))[:,:,0][None,None,:]\n    inp_seqs = np.zeros((batch_size,160)).astype(\'int32\')\n    inp_seqs[:,0] = properties[\'char_to_idx\'][\'#START\']\n    tflib.ops.ctx_vector = []\n\n    idx_to_chars = lambda Y: \' \'.join(map(lambda x: properties[\'idx_to_char\'][x],Y))\n    visualize=False\n    inp_seqs = sess.run(predictions,feed_dict={X:imgs})\n\n    str = idx_to_chars(inp_seqs.flatten().tolist()).split(\'#END\')[0].replace(\'\\left\',\'\').replace(\'\\\\right\',\'\').replace(\'&\',\'\')\n    print ""Latex sequence: "",str\n    pyperclip.copy(\'$\'+str+\'$\')\n\ndef run_demo(filename=None,scale=2):\n    if filename:\n        file = \'/home/rithesh/Downloads/%s.pdf\'%(filename)\n    else:\n        file = np.random.choice(glob.glob(\'Papers_PDF/*\'))\n    os.system(\'xdg-open \' + file)\n    num = str(input(""Enter page number: "")-1)\n    os.system(\'convert -density 200 -quality 100 %s tmp.png\'%(file+\'[%s]\'%num))\n    Image.open(\'tmp.png\').show()\n    time.sleep(3)\n    os.system(\'import screenshot.png\')\n    Image.open(\'screenshot.png\').show()\n    while raw_input(""Is the crop correct? (y/n) : "").lower() not in [\'y\',\'yes\']:\n        os.system(\'pkill display\')\n        Image.open(\'tmp.png\').show()\n        time.sleep(3)\n        os.system(\'import screenshot.png\')\n        os.system(\'pkill display\')\n        Image.open(\'screenshot.png\').show()\n    os.system(\'pkill display\')\n\n    status = crop_image(\'screenshot.png\', \'./tmp1.png\', (600,60))\n    buckets = json.loads(\'[[240,100], [320,80], [400,80],[400,100], [480,80], [480,100], [560,80], [560,100], [640,80],[640,100], [720,80], [720,100], [720,120], [720, 200], [800,100],[800,320], [1000,200]]\')\n    buckets_2 = json.loads(\'[[120,50], [160,40], [200,40],[200,50], [240,40], [240,50], [280,40], [280,50], [320,40],[320,50], [360,40], [360,50], [360,60], [360, 100], [400,50],[400,160], [500,100]]\')\n    status = pad_group_image(\'./tmp1.png\', \'./tmp2.png\', [8,8,8,8], buckets)\n    status = downsample_image(\'./tmp2.png\', \'./tmp3.png\', scale)\n    show()\n\nrun_demo()\n'"
tflib/__init__.py,3,"b'import numpy as np\nimport tensorflow as tf\n\nimport locale\n\nlocale.setlocale(locale.LC_ALL, \'\')\n\n_params = {}\ndef param(name, *args, **kwargs):\n    """"""\n    A wrapper for `tf.Variable` which enables parameter sharing in models.\n\n    Creates and returns theano shared variables similarly to `tf.Variable`,\n    except if you try to create a param with the same name as a\n    previously-created one, `param(...)` will just return the old one instead of\n    making a new one.\n    This constructor also adds a `param` attribute to the shared variables it\n    creates, so that you can easily search a graph for all params.\n    """"""\n\n    if name not in _params:\n        kwargs[\'name\'] = name\n        param = tf.Variable(*args, **kwargs)\n        param.param = True\n        _params[name] = param\n    return _params[name]\n\ndef params_with_name(name):\n    return [p for n,p in _params.items() if name in n]\n\ndef delete_all_params():\n    _params.clear()\n\ndef print_model_settings(locals_):\n    print ""Model settings:""\n    all_vars = [(k,v) for (k,v) in locals_.items() if (k.isupper() and k!=\'T\')]\n    all_vars = sorted(all_vars, key=lambda x: x[0])\n    for var_name, var_value in all_vars:\n        print ""\\t{}: {}"".format(var_name, var_value)\n'"
tflib/network.py,33,"b""import tflib\nimport tflib.ops\nimport tensorflow as tf\nimport numpy as np\n\ndef alex_net(inp,DIM=512):\n    X = tf.nn.relu(tflib.ops.conv2d('conv1', inp, 11, 4, 1, 96, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tflib.ops.max_pool('pool1', X, k=3, s=2)\n    X = tflib.ops.norm('norm1', X, lsize=5)\n\n    X = tf.nn.relu(tflib.ops.conv2d('conv2', X, 5, 1, 96, 256, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tflib.ops.max_pool('pool2', X, k=3, s=2)\n    X = tflib.ops.norm('norm2', X, lsize=5)\n\n    X = tf.nn.relu(tflib.ops.conv2d('conv3', X, 3, 1, 256, 384, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tf.nn.relu(tflib.ops.conv2d('conv4', X, 3, 1, 384, 384, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tf.nn.relu(tflib.ops.conv2d('conv5', X, 3, 1, 384, 256, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tflib.ops.max_pool('pool5', X, k=3, s=2)\n    X = tflib.ops.norm('norm5', X, lsize=5)\n\n    X = tf.nn.relu(tflib.ops.Linear('fc6',tf.reshape(X,[tf.shape(X)[0],-1]),32768,4096))\n    X = tf.nn.dropout(X,0.5)\n\n    X = tf.nn.relu(tflib.ops.Linear('fc7',X,4096,4096))\n    X = tf.nn.dropout(X,0.5)\n\n    X = tflib.ops.Linear('fc8',X,4096,DIM)\n\n    return X\n\ndef alex_net_att(inp):\n    X = tf.nn.relu(tflib.ops.conv2d('conv1', inp, 11, 4, 1, 96, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tflib.ops.max_pool('pool1', X, k=3, s=2)\n    X = tflib.ops.norm('norm1', X, lsize=5)\n\n    X = tf.nn.relu(tflib.ops.conv2d('conv2', X, 5, 1, 96, 256, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tflib.ops.max_pool('pool2', X, k=3, s=2)\n    X = tflib.ops.norm('norm2', X, lsize=5)\n\n    X = tf.nn.relu(tflib.ops.conv2d('conv3', X, 3, 1, 256, 384, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tf.nn.relu(tflib.ops.conv2d('conv4', X, 3, 1, 384, 384, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tf.nn.relu(tflib.ops.conv2d('conv5', X, 3, 1, 384, 256, bias=True, batchnorm=False, pad = 'SAME'))\n\n    return X\n\n\ndef vgg16(X,num_feats=64):\n    X = tf.nn.relu(tflib.ops.conv2d('conv1_1', X, 3, 1, 1, num_feats, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tf.nn.relu(tflib.ops.conv2d('conv1_2', X, 3, 1, num_feats, num_feats, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tflib.ops.max_pool('pool1', X, k=2, s=2)\n\n    X = tf.nn.relu(tflib.ops.conv2d('conv2_1', X, 3, 1, num_feats, 2*num_feats, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tf.nn.relu(tflib.ops.conv2d('conv2_2', X, 3, 1, 2*num_feats, 2*num_feats, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tflib.ops.max_pool('pool2', X, k=2, s=2)\n\n    X = tf.nn.relu(tflib.ops.conv2d('conv3_1', X, 3, 1, 2*num_feats, 4*num_feats, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tf.nn.relu(tflib.ops.conv2d('conv3_2', X, 3, 1, 4*num_feats, 4*num_feats, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tf.nn.relu(tflib.ops.conv2d('conv3_3', X, 3, 1, 4*num_feats, 4*num_feats, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tflib.ops.max_pool('pool3', X, k=2, s=2)\n\n    X = tf.nn.relu(tflib.ops.conv2d('conv4_1', X, 3, 1, 4*num_feats, 8*num_feats, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tf.nn.relu(tflib.ops.conv2d('conv4_2', X, 3, 1, 8*num_feats, 8*num_feats, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tf.nn.relu(tflib.ops.conv2d('conv4_3', X, 3, 1, 8*num_feats, 8*num_feats, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tflib.ops.max_pool('pool4', X, k=2, s=2)\n\n    X = tf.nn.relu(tflib.ops.conv2d('conv5_1', X, 3, 1, 8*num_feats, 8*num_feats, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tf.nn.relu(tflib.ops.conv2d('conv5_2', X, 3, 1, 8*num_feats, 8*num_feats, bias=True, batchnorm=False, pad = 'SAME'))\n    X = tf.nn.relu(tflib.ops.conv2d('conv5_3', X, 3, 1, 8*num_feats, 8*num_feats, bias=True, batchnorm=False, pad = 'SAME'))\n\n    return X\n\ndef im2latex_cnn(X, num_feats, bn, train_mode=True):\n    X = X-128.\n    X = X/128.\n\n    X = tf.nn.relu(tflib.ops.conv2d('conv1', X, 3, 1, 1, num_feats, pad = 'SAME', bias=False)) \n    X = tflib.ops.max_pool('pool1', X, k=2, s=2)\n\n    X = tf.nn.relu(tflib.ops.conv2d('conv2', X, 3, 1, num_feats, num_feats*2, pad = 'SAME', bias=False))\n    X = tflib.ops.max_pool('pool2', X, k=2, s=2)\n\n    X = tf.nn.relu(tflib.ops.conv2d('conv3', X, 3, 1, num_feats*2, num_feats*4,  batchnorm=bn, is_training=train_mode, pad = 'SAME', bias=False))\n\n    X = tf.nn.relu(tflib.ops.conv2d('conv4', X, 3, 1, num_feats*4, num_feats*4, pad = 'SAME', bias=False))\n    X = tflib.ops.max_pool('pool4', X, k=(1,2), s=(1,2))\n\n    X = tf.nn.relu(tflib.ops.conv2d('conv5', X, 3, 1, num_feats*4, num_feats*8, batchnorm=bn, is_training=train_mode, pad = 'SAME', bias=False))\n    X = tflib.ops.max_pool('pool5', X, k=(2,1), s=(2,1))\n\n    X = tf.nn.relu(tflib.ops.conv2d('conv6', X, 3, 1, num_feats*8, num_feats*8, batchnorm=bn, is_training=train_mode, pad = 'SAME', bias=False))\n\n    return X\n"""
tflib/ops.py,100,"b'import tensorflow as tf\nimport time\nimport numpy as np\nimport theano\ntheano.config.floatX=\'float32\'\nimport lasagne\nimport tflib\nimport numpy\nfrom tensorflow.python.ops import array_ops\n\ndef initializer(\n    name,\n    shape,\n    val=0,\n    gain=\'linear\',\n    std=0.01,\n    mean=0.0,\n    range=0.01,\n    alpha=0.01\n    ):\n    """"""\n    Wrapper function to perform weight initialization using standard techniques\n\n    :parameters:\n        name: Name of initialization technique. Follows same names as lasagne.init module\n        shape: list or tuple containing shape of weights\n        val: Fill value in case of constant initialization\n        gain: one of \'linear\',\'sigmoid\',\'tanh\', \'relu\' or \'leakyrelu\'\n        std: standard deviation used for normal / uniform initialization\n        mean: mean value used for normal / uniform initialization\n        alpha: used when gain = \'leakyrelu\'\n    """"""\n\n    if gain in [\'linear\',\'sigmoid\',\'tanh\']:\n        gain = 1.0\n    elif gain==\'leakyrelu\':\n        gain = np.sqrt(2/(1+alpha**2))\n    elif gain==\'relu\':\n        gain = np.sqrt(2)\n    else:\n        raise NotImplementedError\n\n    if name==\'Constant\':\n        return lasagne.init.Constant(val).sample(shape)\n    elif name==\'Normal\':\n        return lasagne.init.Normal(std,mean).sample(shape)\n    elif name==\'Uniform\':\n        return lasagne.init.Uniform(range=range,std=std,mean=mean).sample(shape)\n    elif name==\'GlorotNormal\':\n        return lasagne.init.GlorotNormal(gain=gain).sample(shape)\n    elif name==\'GlorotUniform\':\n        return lasagne.init.GlorotUniform(gain=gain).sample(shape)\n    elif name==\'HeNormal\':\n        return lasagne.init.HeNormal(gain=gain).sample(shape)\n    elif name==\'HeUniform\':\n        return lasagne.init.HeUniform(gain=gain).sample(shape)\n    elif name==\'Orthogonal\':\n        return lasagne.init.Orthogonal(gain=gain).sample(shape)\n    else:\n        return lasagne.init.GlorotUniform(gain=gain).sample(shape)\n\ndef Embedding(\n    name,\n    n_symbols,\n    output_dim,\n    indices\n    ):\n    """"""\n    Creates an embedding matrix of dimensions n_symbols x output_dim upon first use.\n    Looks up embedding vector for each input symbol\n\n    :parameters:\n        name: name of embedding matrix tensor variable\n        n_symbols: No. of input symbols\n        output_dim: Embedding dimension\n        indices: input symbols tensor\n    """"""\n\n    with tf.name_scope(name) as scope:\n        emb = tflib.param(\n            name,\n            initializer(\'Normal\',[n_symbols, output_dim],std=1.0/np.sqrt(n_symbols))\n            )\n\n        return tf.nn.embedding_lookup(emb,indices)\n\ndef Linear(\n    name,\n    inputs,\n    input_dim,\n    output_dim,\n    activation=\'linear\',\n    bias=True,\n    init=None,\n    weightnorm=False,\n    **kwargs\n    ):\n    """"""\n    Compute a linear transform of one or more inputs, optionally with a bias.\n    Supports more than 2 dimensions. (in which case last axis is considered the dimension to be transformed)\n\n    :parameters:\n        input_dim: tuple of ints, or int; dimensionality of the input\n        output_dim: int; dimensionality of output\n        activation: \'linear\',\'sigmoid\', etc. ; used as gain parameter for weight initialization ;\n                     DOES NOT APPLY THE ACTIVATION MENTIONED IN THIS PARAMETER\n        bias: flag that denotes whether bias should be applied\n        init: name of weight initializer to be used\n        weightnorm: flag that denotes whether weight normalization should be applied\n    """"""\n\n    with tf.name_scope(name) as scope:\n        weight_values = initializer(init,(input_dim,output_dim),gain=activation, **kwargs)\n\n        weight = tflib.param(\n            name + \'.W\',\n            weight_values\n        )\n\n        batch_size = None\n\n        if weightnorm:\n            norm_values = np.sqrt(np.sum(np.square(weight_values), axis=0))\n            # nort.m_values = np.linalg.norm(weight_values, axis=0)\n\n            target_norms = tflib.param(\n                name + \'.g\',\n                norm_values\n            )\n\n            with tf.name_scope(\'weightnorm\') as scope:\n                norms = tf.sqrt(tf.reduce_sum(tf.square(weight), reduction_indices=[0]))\n                weight = weight * (target_norms / norms)\n\n        if inputs.get_shape().ndims == 2:\n            result = tf.matmul(inputs, weight)\n        else:\n            reshaped_inputs = tf.reshape(inputs, [-1, input_dim])\n            result = tf.matmul(reshaped_inputs, weight)\n            result = tf.reshape(result, tf.stack(tf.unstack(tf.shape(inputs))[:-1] + [output_dim]))\n\n        if bias:\n            b = tflib.param(\n                name + \'.b\',\n                numpy.zeros((output_dim,), dtype=\'float32\')\n            )\n\n            result = tf.nn.bias_add(result,b)\n\n        return result\n\ndef conv2d(\n    name,\n    input,\n    kernel,\n    stride,\n    depth,\n    num_filters,\n    init = \'GlorotUniform\',\n    pad = \'SAME\',\n    bias = True,\n    weightnorm = False,\n    batchnorm = False,\n    is_training=True,\n    **kwargs\n    ):\n    """"""\n    Performs 2D convolution on input in NCHW data format\n\n    :parameters:\n        input - input to be convolved\n        kernel - int; size of convolutional kernel\n        stride - int; horizontal / vertical stride to be used\n        depth - int; no. of channels of input\n        num_filters - int; no. of output channels required\n        batchnorm - flag that denotes whether batch normalization should be applied\n        is_training - flag that denotes batch normalization mode\n    """"""\n    with tf.name_scope(name) as scope:\n        filter_values = initializer(init,(kernel,kernel,depth,num_filters),gain=\'relu\',**kwargs)\n        filters = tflib.param(\n            name+\'.W\',\n            filter_values\n        )\n\n        if weightnorm:\n            norm_values = np.sqrt(np.sum(np.square(filter_values), axis=(0,1,2)))\n            target_norms = tflib.param(\n                name + \'.g\',\n                norm_values\n            )\n            with tf.name_scope(\'weightnorm\') as scope:\n                norms = tf.sqrt(tf.reduce_sum(tf.square(filters), reduction_indices=[0,1,2]))\n                filters = filters * (target_norms / norms)\n\n        out = tf.nn.conv2d(input, filters, strides=[1, 1, stride, stride], padding=pad, data_format=\'NCHW\')\n\n        if bias:\n            b = tflib.param(\n                name+\'.b\',\n                np.zeros(num_filters,dtype=np.float32)\n            )\n\n            out = tf.nn.bias_add(out,b,data_format=\'NCHW\')\n\n        if batchnorm:\n            out = tf.contrib.layers.batch_norm(inputs=out,scope=scope,is_training=is_training,data_format=\'NCHW\')\n\n        return out\n\ndef max_pool(\n    name,\n    l_input,\n    k,\n    s\n    ):\n    """"""\n    Max pooling operation with kernel size k and stride s on input with NCHW data format\n\n    :parameters:\n        l_input: input in NCHW data format\n        k: tuple of int, or int ; kernel size\n        s: tuple of int, or int ; stride value\n    """"""\n\n    if type(k)==int:\n        k1=k\n        k2=k\n    else:\n        k1 = k[0]\n        k2 = k[1]\n    if type(s)==int:\n        s1=s\n        s2=s\n    else:\n        s1 = s[0]\n        s2 = s[1]\n    return tf.nn.max_pool(l_input, ksize=[1, 1, k1, k2], strides=[1, 1, s1, s2],\n                          padding=\'SAME\', name=name, data_format=\'NCHW\')\n\ndef norm(\n    name,\n    l_input,\n    lsize=4\n    ):\n    """"""\n    Wrapper function to perform local response normalization (ref. Alexnet)\n    """"""\n    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)\n\nclass GRUCell(tf.nn.rnn_cell.RNNCell):\n    def __init__(self, name, n_in, n_hid):\n        self._n_in = n_in\n        self._n_hid = n_hid\n        self._name = name\n\n    @property\n    def state_size(self):\n        return self._n_hid\n\n    @property\n    def output_size(self):\n        return self._n_hid\n\n    def __call__(self, inputs, state, scope=None):\n        gates = tf.nn.sigmoid(\n            tflib.ops.Linear(\n                self._name+\'.Gates\',\n                tf.concat(axis=1, values=[inputs, state]),\n                self._n_in + self._n_hid,\n                2 * self._n_hid\n            )\n        )\n\n        update, reset = tf.split(axis=1, num_or_size_splits=2, value=gates)\n        scaled_state = reset * state\n\n        candidate = tf.tanh(\n            tflib.ops.Linear(\n                self._name+\'.Candidate\',\n                tf.concat(axis=1, values=[inputs, scaled_state]),\n                self._n_in + self._n_hid,\n                self._n_hid\n            )\n        )\n\n        output = (update * candidate) + ((1 - update) * state)\n\n        return output, output\n\ndef GRU(\n    name,\n    inputs,\n    n_in,\n    n_hid\n    ):\n    """"""\n    Compute recurrent memory states using Gated Recurrent Units\n\n    :parameters:\n        n_in : int ; Dimensionality of input\n        n_hid : int ; Dimensionality of hidden state / memory state\n    """"""\n    h0 = tflib.param(name+\'.h0\', np.zeros(n_hid, dtype=\'float32\'))\n    batch_size = tf.shape(inputs)[0]\n    h0 = tf.reshape(tf.tile(h0, tf.stack([batch_size])), tf.stack([batch_size, n_hid]))\n    return tf.nn.dynamic_rnn(GRUCell(name, n_in, n_hid), inputs, initial_state=h0, swap_memory=True)[0]\n\nclass LSTMCell(tf.nn.rnn_cell.RNNCell):\n    def __init__(self, name, n_in, n_hid, forget_bias=1.0):\n        self._n_in = n_in\n        self._n_hid = n_hid\n        self._name = name\n        self._forget_bias = forget_bias\n\n    @property\n    def state_size(self):\n        return self._n_hid\n\n    @property\n    def output_size(self):\n        return self._n_hid\n\n    def __call__(self, inputs, state, scope=None):\n        c_tm1, h_tm1 = tf.split(axis=1,num_or_size_splits=2,value=state)\n        gates = tflib.ops.Linear(\n                self._name+\'.Gates\',\n                tf.concat(axis=1, values=[inputs, h_tm1]),\n                self._n_in + self._n_hid,\n                4 * self._n_hid,\n                activation=\'sigmoid\'\n                )\n\n        i_t,f_t,o_t,g_t = tf.split(axis=1, num_or_size_splits=4, value=gates)\n\n        c_t = tf.nn.sigmoid(f_t+self._forget_bias)*c_tm1 + tf.nn.sigmoid(i_t)*tf.tanh(g_t)\n        h_t = tf.nn.sigmoid(o_t)*tf.tanh(c_t)\n\n        new_state = tf.concat(axis=1, values=[c_t,h_t])\n\n        return h_t,new_state\n\ndef LSTM(\n    name,\n    inputs,\n    n_in,\n    n_hid,\n    h0\n    ):\n    """"""\n    Compute recurrent memory states using Long Short-Term Memory units\n\n    :parameters:\n        n_in : int ; Dimensionality of input\n        n_hid : int ; Dimensionality of hidden state / memory state\n    """"""\n    batch_size = tf.shape(inputs)[0]\n    if h0 is None:\n        h0 = tflib.param(name+\'.init.h0\', np.zeros(2*n_hid, dtype=\'float32\'))\n        h0 = tf.reshape(tf.tile(h0_1, tf.stack([batch_size])), tf.stack([batch_size, 2*n_hid]))\n\n    return tf.nn.dynamic_rnn(LSTMCell(name, n_in, n_hid), inputs, initial_state=h0, swap_memory=True)\n\ndef BiLSTM(\n    name,\n    inputs,\n    n_in,\n    n_hid,\n    h0_1=None,\n    h0_2=None\n    ):\n    """"""\n    Compute recurrent memory states using Bidirectional Long Short-Term Memory units\n\n    :parameters:\n        n_in : int ; Dimensionality of input\n        n_hid : int ; Dimensionality of hidden state / memory state\n        h0_1: vector ; Initial hidden state of forward LSTM\n        h0_2: vector ; Initial hidden state of backward LSTM\n    """"""\n\n    batch_size = tf.shape(inputs)[0]\n    if h0_1 is None:\n        h0_1 = tflib.param(name+\'.init.h0_1\', np.zeros(2*n_hid, dtype=\'float32\'))\n        h0_1 = tf.reshape(tf.tile(h0_1, tf.stack([batch_size])), tf.stack([batch_size, 2*n_hid]))\n\n    if h0_2 is None:\n        h0_2 = tflib.param(name+\'.init.h0_2\', np.zeros(2*n_hid, dtype=\'float32\'))\n        h0_2 = tf.reshape(tf.tile(h0_2, tf.stack([batch_size])), tf.stack([batch_size, 2*n_hid]))\n\n\n    cell1 = LSTMCell(name+\'_fw\', n_in, n_hid)\n    cell2 = LSTMCell(name+\'_bw\', n_in, n_hid)\n\n    seq_len = tf.tile(tf.expand_dims(tf.shape(inputs)[1],0),[batch_size])\n    outputs = tf.nn.bidirectional_dynamic_rnn(cell1, cell2, inputs, sequence_length=seq_len, initial_state_fw=h0_1, initial_state_bw=h0_2, swap_memory=True)\n    return tf.concat(axis=2,values=[outputs[0][0],outputs[0][1]])\n\n\'\'\'\nAttentional Decoder as proposed in HarvardNLp paper (https://arxiv.org/pdf/1609.04938v1.pdf)\n\'\'\'\nctx_vector = []\nclass im2latexAttentionCell(tf.nn.rnn_cell.RNNCell):\n    def __init__(self, name, n_in, n_hid, L, D, ctx, forget_bias=1.0):\n        self._n_in = n_in\n        self._n_hid = n_hid\n        self._name = name\n        self._forget_bias = forget_bias\n        self._ctx = ctx\n        self._L = L\n        self._D = D\n\n    @property\n    def state_size(self):\n        return self._n_hid\n\n    @property\n    def output_size(self):\n        return self._n_hid\n\n    def __call__(self, _input, state, scope=None):\n\n        h_tm1, c_tm1, output_tm1 = tf.split(axis=1,num_or_size_splits=3,value=state)\n\n        gates = tflib.ops.Linear(\n                self._name+\'.Gates\',\n                tf.concat(axis=1, values=[_input, output_tm1]),\n                self._n_in + self._n_hid,\n                4 * self._n_hid,\n                activation=\'sigmoid\'\n            )\n\n        i_t,f_t,o_t,g_t = tf.split(axis=1, num_or_size_splits=4, value=gates)\n\n        ## removing forget_bias\n        c_t = tf.nn.sigmoid(f_t)*c_tm1 + tf.nn.sigmoid(i_t)*tf.tanh(g_t)\n        h_t = tf.nn.sigmoid(o_t)*tf.tanh(c_t)\n\n\n        target_t = tf.expand_dims(tflib.ops.Linear(self._name+\'.target_t\',h_t,self._n_hid,self._n_hid,bias=False),2)\n        # target_t = tf.expand_dims(h_t,2) # (B, HID, 1)\n        a_t = tf.nn.softmax(tf.matmul(self._ctx,target_t)[:,:,0],name=\'a_t\') # (B, H*W, D) * (B, D, 1)\n        print a_t.name\n\n        def _debug_bkpt(val):\n            global ctx_vector\n            ctx_vector = []\n            ctx_vector += [val]\n            return False\n\n        debug_print_op = tf.py_func(_debug_bkpt,[a_t],[tf.bool])\n        with tf.control_dependencies(debug_print_op):\n            a_t = tf.identity(a_t,name=\'a_t_debug\')\n\n        a_t = tf.expand_dims(a_t,1) # (B, 1, H*W)\n        z_t = tf.matmul(a_t,self._ctx)[:,0]\n        # a_t = tf.expand_dims(a_t,2)\n        # z_t = tf.reduce_sum(a_t*self._ctx,1)\n\n        output_t = tf.tanh(tflib.ops.Linear(\n            self._name+\'.output_t\',\n            tf.concat(axis=1,values=[h_t,z_t]),\n            self._D+self._n_hid,\n            self._n_hid,\n            bias=False,\n            activation=\'tanh\'\n            ))\n\n        new_state = tf.concat(axis=1,values=[h_t,c_t,output_t])\n\n        return output_t,new_state\n\ndef im2latexAttention(\n    name,\n    inputs,\n    ctx,\n    input_dim,\n    ENC_DIM,\n    DEC_DIM,\n    D,\n    H,\n    W\n    ):\n    """"""\n    Function that encodes the feature grid extracted from CNN using BiLSTM encoder\n    and decodes target sequences using an attentional decoder mechanism\n\n    PS: Feature grid can be of variable size (as long as size is within \'H\' and \'W\')\n\n    :parameters:\n        ctx - (N,C,H,W) format ; feature grid extracted from CNN\n        input_dim - int ; Dimensionality of input sequences (Usually, Embedding Dimension)\n        ENC_DIM - int; Dimensionality of BiLSTM Encoder\n        DEC_DIM - int; Dimensionality of Attentional Decoder\n        D - int; No. of channels in feature grid\n        H - int; Maximum height of feature grid\n        W - int; Maximum width of feature grid\n    """"""\n\n    V = tf.transpose(ctx,[0,2,3,1]) # (B, H, W, D)\n    V_cap = []\n    batch_size = tf.shape(ctx)[0]\n    count=0\n\n    h0_i_1 = tf.tile(tflib.param(\n        name+\'.Enc_.init.h0_1\',\n        np.zeros((1,H,2*ENC_DIM)).astype(\'float32\')\n    ),[batch_size,1,1])\n\n    h0_i_2 = tf.tile(tflib.param(\n        name+\'.Enc_init.h0_2\',\n        np.zeros((1,H,2*ENC_DIM)).astype(\'float32\')\n    ),[batch_size,1,1])\n\n\n    def fn(prev_out,i):\n    # for i in xrange(H):\n        return tflib.ops.BiLSTM(name+\'.BiLSTMEncoder\',V[:,i],D,ENC_DIM,h0_i_1[:,i],h0_i_2[:,i])\n\n    V_cap = tf.scan(fn,tf.range(tf.shape(V)[1]), initializer=tf.placeholder(shape=(None,None,2*ENC_DIM),dtype=tf.float32))\n\n    V_t = tf.reshape(tf.transpose(V_cap,[1,0,2,3]),[tf.shape(inputs)[0],-1,ENC_DIM*2]) # (B, L, ENC_DIM)\n\n    h0_dec = tf.tile(tflib.param(\n        name+\'.Decoder.init.h0\',\n        np.zeros((1,3*DEC_DIM)).astype(\'float32\')\n    ),[batch_size,1])\n\n    cell = tflib.ops.im2latexAttentionCell(name+\'.AttentionCell\',input_dim,DEC_DIM,H*W,2*ENC_DIM,V_t)\n    seq_len = tf.tile(tf.expand_dims(tf.shape(inputs)[1],0),[batch_size])\n    out = tf.nn.dynamic_rnn(cell, inputs, initial_state=h0_dec, sequence_length=seq_len, swap_memory=True)\n\n    return out\n\n\nclass FreeRunIm2LatexAttentionCell(tf.nn.rnn_cell.RNNCell):\n    def __init__(self, name, n_in, n_out, n_hid, L, D, ctx, forget_bias=1.0):\n        self._n_in = n_in\n        self._n_hid = n_hid\n        self._name = name\n        self._forget_bias = forget_bias\n        self._ctx = ctx\n        self._L = L\n        self._D = D\n        self._n_out = n_out\n\n    @property\n    def state_size(self):\n        return self._n_hid\n\n    @property\n    def output_size(self):\n        return self._n_out\n\n    def __call__(self, _input, state, scope=None):\n        h_tm1, c_tm1, output_tm1 = tf.split(axis=1,num_or_size_splits=3,value=state[:,:3*self._n_hid])\n        _input = tf.argmax(state[:,3*self._n_hid:],axis=1)\n        _input = tflib.ops.Embedding(\'Embedding\',self._n_out,self._n_in,_input)\n\n\n        gates = tflib.ops.Linear(\n                self._name+\'.Gates\',\n                tf.concat(axis=1, values=[_input, output_tm1]),\n                self._n_in + self._n_hid,\n                4 * self._n_hid,\n                activation=\'sigmoid\'\n            )\n\n        i_t,f_t,o_t,g_t = tf.split(axis=1, num_or_size_splits=4, value=gates)\n\n        ## removing forget_bias\n        c_t = tf.nn.sigmoid(f_t)*c_tm1 + tf.nn.sigmoid(i_t)*tf.tanh(g_t)\n        h_t = tf.nn.sigmoid(o_t)*tf.tanh(c_t)\n\n\n        target_t = tf.expand_dims(tflib.ops.Linear(self._name+\'.target_t\',h_t,self._n_hid,self._n_hid,bias=False),2)\n        # target_t = tf.expand_dims(h_t,2) # (B, HID, 1)\n        a_t = tf.nn.softmax(tf.matmul(self._ctx,target_t)[:,:,0],name=\'a_t\') # (B, H*W, D) * (B, D, 1)\n        a_t = tf.expand_dims(a_t,1) # (B, 1, H*W)\n        z_t = tf.matmul(a_t,self._ctx)[:,0]\n        # a_t = tf.expand_dims(a_t,2)\n        # z_t = tf.reduce_sum(a_t*self._ctx,1)\n\n        output_t = tf.tanh(tflib.ops.Linear(\n            self._name+\'.output_t\',\n            tf.concat(axis=1,values=[h_t,z_t]),\n            self._D+self._n_hid,\n            self._n_hid,\n            bias=False,\n            activation=\'tanh\'\n            ))\n\n        logits = tf.nn.softmax(tflib.ops.Linear(\'MLP.1\',output_t,self._n_hid,self._n_out))\n        new_state = tf.concat(axis=1,values=[h_t,c_t,output_t,logits])\n\n        return logits,new_state\n\ndef FreeRunIm2LatexAttention(\n    name,\n    ctx,\n    input_dim,\n    output_dim,\n    ENC_DIM,\n    DEC_DIM,\n    D,\n    H,\n    W\n    ):\n    """"""\n    Function that encodes the feature grid extracted from CNN using BiLSTM encoder\n    and decodes target sequences using an attentional decoder mechanism\n\n    PS: Feature grid can be of variable size (as long as size is within \'H\' and \'W\')\n\n    :parameters:\n        ctx - (N,C,H,W) format ; feature grid extracted from CNN\n        input_dim - int ; Dimensionality of input sequences (Usually, Embedding Dimension)\n        ENC_DIM - int; Dimensionality of BiLSTM Encoder\n        DEC_DIM - int; Dimensionality of Attentional Decoder\n        D - int; No. of channels in feature grid\n        H - int; Maximum height of feature grid\n        W - int; Maximum width of feature grid\n    """"""\n\n    V = tf.transpose(ctx,[0,2,3,1]) # (B, H, W, D)\n    V_cap = []\n    batch_size = tf.shape(ctx)[0]\n    count=0\n\n    h0_i_1 = tf.tile(tflib.param(\n        name+\'.Enc_.init.h0_1\',\n        np.zeros((1,H,2*ENC_DIM)).astype(\'float32\')\n    ),[batch_size,1,1])\n\n    h0_i_2 = tf.tile(tflib.param(\n        name+\'.Enc_init.h0_2\',\n        np.zeros((1,H,2*ENC_DIM)).astype(\'float32\')\n    ),[batch_size,1,1])\n\n\n    def fn(prev_out,i):\n    # for i in xrange(H):\n        return tflib.ops.BiLSTM(name+\'.BiLSTMEncoder\',V[:,i],D,ENC_DIM,h0_i_1[:,i],h0_i_2[:,i])\n\n    V_cap = tf.scan(fn,tf.range(tf.shape(V)[1]), initializer=tf.placeholder(shape=(None,None,2*ENC_DIM),dtype=tf.float32))\n\n    V_t = tf.reshape(tf.transpose(V_cap,[1,0,2,3]),[batch_size,-1,ENC_DIM*2]) # (B, L, ENC_DIM)\n\n    h0_dec = tf.concat(axis=1,values=[tf.tile(tflib.param(\n        name+\'.Decoder.init.h0\',\n        np.zeros((1,3*DEC_DIM)).astype(\'float32\')\n    ),[batch_size,1]),tf.reshape(tf.one_hot(500,output_dim),(batch_size,output_dim))])\n\n    inputs = tf.zeros((batch_size,160,100))\n\n    cell = tflib.ops.FreeRunIm2LatexAttentionCell(name+\'.AttentionCell\',input_dim,output_dim,DEC_DIM,H*W,2*ENC_DIM,V_t)\n    seq_len = tf.tile(tf.expand_dims(160,0),[batch_size])\n    out = tf.nn.dynamic_rnn(cell, inputs, initial_state=h0_dec, sequence_length=seq_len, swap_memory=True)\n    return out\n'"
im2markup/scripts/evaluation/LevSeq.py,0,"b'from Levenshtein import *\nfrom warnings import warn\n\nclass StringMatcher:\n    """"""A SequenceMatcher-like class built on the top of Levenshtein""""""\n\n    def _reset_cache(self):\n        self._ratio = self._distance = None\n        self._opcodes = self._editops = self._matching_blocks = None\n\n    def __init__(self, isjunk=None, seq1=\'\', seq2=\'\'):\n        if isjunk:\n            warn(""isjunk not NOT implemented, it will be ignored"")\n        self._str1, self._str2 = seq1, seq2\n        self._reset_cache()\n\n    def set_seqs(self, seq1, seq2):\n        self._str1, self._str2 = seq1, seq2\n        self._reset_cache()\n\n    def set_seq1(self, seq1):\n        self._str1 = seq1\n        self._reset_cache()\n\n    def set_seq2(self, seq2):\n        self._str2 = seq2\n        self._reset_cache()\n\n    def get_opcodes(self):\n        if not self._opcodes:\n            if self._editops:\n                self._opcodes = opcodes(self._editops, self._str1, self._str2)\n            else:\n                self._opcodes = opcodes(self._str1, self._str2)\n        return self._opcodes\n\n    def get_editops(self):\n        if not self._editops:\n            if self._opcodes:\n                self._editops = editops(self._opcodes, self._str1, self._str2)\n            else:\n                self._editops = editops(self._str1, self._str2)\n        return self._editops\n\n    def get_matching_blocks(self):\n        if not self._matching_blocks:\n            self._matching_blocks = matching_blocks(self.get_opcodes(),\n                                                    self._str1, self._str2)\n        return self._matching_blocks\n\n    def ratio(self):\n        if not self._ratio:\n            self._ratio = ratio(self._str1, self._str2)\n        return self._ratio\n\n    def quick_ratio(self):\n        # This is usually quick enough :o)\n        if not self._ratio:\n            self._ratio = ratio(self._str1, self._str2)\n        return self._ratio\n\n    def real_quick_ratio(self):\n        len1, len2 = len(self._str1), len(self._str2)\n        return 2.0 * min(len1, len2) / (len1 + len2)\n\n    def distance(self):\n        if not self._distance:\n            self._distance = distance(self._str1, self._str2)\n        return self._distance\n'"
im2markup/scripts/evaluation/evaluate_bleu.py,0,"b'import os, sys, copy, argparse, shutil, pickle, subprocess, logging\n\ndef process_args(args):\n    parser = argparse.ArgumentParser(description=\'Evaluate BLEU score\')\n    parser.add_argument(\'--result-path\', dest=\'result_path\',\n                        type=str, required=True,\n                        help=(\'Result file containing <img_path> <label_gold> <label_pred> <score_pred> <score_gold> per line. This should be set to the output file of the model.\'\n                        ))\n    parser.add_argument(\'--data-path\', dest=\'data_path\',\n                        type=str, required=True,\n                        help=(\'Input file which contains the samples to be evaluated. The format is <img_path> <label_idx> per line.\'\n                        ))\n    parser.add_argument(\'--label-path\', dest=\'label_path\',\n                        type=str, required=True,\n                        help=(\'Gold label file which contains a tokenized formula per line.\'\n                        ))\n    parser.add_argument(\'--log-path\', dest=""log_path"",\n                        type=str, default=\'log.txt\',\n                        help=(\'Log file path, default=log.txt\' \n                        ))\n    parameters = parser.parse_args(args)\n    return parameters\n\ndef main(args):\n    script_path = os.path.realpath(__file__)\n    script_dir = os.path.dirname(script_path)\n    app_dir = os.path.join(script_dir, \'../..\')\n\n    parameters = process_args(args)\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\',\n        filename=parameters.log_path)\n\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\')\n    console.setFormatter(formatter)\n    logging.getLogger(\'\').addHandler(console)\n\n    logging.info(\'Script being executed: %s\'%__file__)\n   \n    label_path = parameters.label_path\n    data_path = parameters.data_path\n    result_path = parameters.result_path\n    assert os.path.exists(label_path), \'Label file %s not found\'%label_path\n    assert os.path.exists(data_path), \'Data file %s not found\'%data_path\n    assert os.path.exists(result_path), \'Result file %s not found\'%result_path\n\n    labels_tmp = {}\n    labels = {}\n    with open(label_path) as flabel:\n        with open(data_path) as fdata:\n            line_idx = 0\n            for line in flabel:\n                labels_tmp[line_idx] = line.strip()\n                line_idx += 1\n            for line in fdata:\n                img_path, idx = line.strip().split()\n                labels[img_path] = labels_tmp[int(idx)]\n\n    results = {}\n    with open(result_path) as fin:\n        for line_idx,line in enumerate(fin):\n            if line_idx % 1000 == 0:\n                print (line_idx)\n            items = line.strip().split(\'\\t\')\n            if len(items) == 5:\n                img_path, label_gold, label_pred, score_pred, score_gold = items\n                if not img_path in labels:\n                    logging.warning(\'%s in result file while not in the gold file!\'%img_path)\n                results[img_path] = label_pred+\'\\n\'\n\n    fpred = open(\'.tmp.pred.txt\', \'w\')\n    fgold = open(\'.tmp.gold.txt\', \'w\')\n    for img_path in labels:\n        fpred.write(results.setdefault(img_path, \'\\n\'))\n        fgold.write(labels[img_path]+\'\\n\')\n    fpred.close()\n    fgold.close()\n    metric = subprocess.check_output(\'perl third_party/multi-bleu.perl %s < %s\'%(\'.tmp.gold.txt\', \'.tmp.pred.txt\'), shell=True)\n    #os.remove(\'.tmp.pred.txt\')\n    #os.remove(\'.tmp.gold.txt\')\n    logging.info(metric)\n\nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n    logging.info(\'Jobs finished\')\n'"
im2markup/scripts/evaluation/evaluate_image.py,0,"b'import sys, os, argparse, logging, glob\nimport numpy as np\nfrom PIL import Image\nimport distance\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nimport difflib\nfrom LevSeq import StringMatcher\n\ndef process_args(args):\n    parser = argparse.ArgumentParser(description=\'Evaluate image related metrics.\')\n\n    parser.add_argument(\'--images-dir\', dest=\'images_dir\',\n                        type=str, required=True,\n                        help=(\'Images directory containing the rendered images. A subfolder with name ""images_gold"" for the rendered gold images, and a subfolder ""images_pred"" must be created beforehand by using scripts/evaluation/render_latex.py.\'\n                        ))\n\n    parser.add_argument(\'--log-path\', dest=""log_path"",\n                        type=str, default=\'log.txt\',\n                        help=(\'Log file path, default=log.txt\' \n                        ))\n    parameters = parser.parse_args(args)\n    return parameters\n\ndef main(args):\n    parameters = process_args(args)\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\',\n        filename=parameters.log_path)\n\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\')\n    console.setFormatter(formatter)\n    logging.getLogger(\'\').addHandler(console)\n\n    logging.info(\'Script being executed: %s\'%__file__)\n   \n    images_dir = parameters.images_dir\n    gold_dir = os.path.join(images_dir, \'images_gold\')\n    pred_dir = os.path.join(images_dir, \'images_pred\')\n    assert os.path.exists(gold_dir), gold_dir \n    assert os.path.exists(pred_dir), pred_dir \n    total_edit_distance = 0\n    total_ref = 0\n    total_num = 0\n    total_correct = 0\n    total_correct_eliminate = 0\n    filenames = glob.glob(os.path.join(gold_dir, \'*\'))\n    for filename in filenames:\n        filename2 = os.path.join(pred_dir, os.path.basename(filename))\n        edit_distance, ref, match1, match2 = img_edit_distance_file(filename, filename2)\n        total_edit_distance += edit_distance\n        total_ref += ref\n        total_num += 1\n        if match1:\n            total_correct += 1\n        if match2:\n            total_correct_eliminate += 1\n        if total_num % 100 == 0:\n            logging.info (\'Total Num: %d\'%total_num)\n            logging.info (\'Accuracy (w spaces): %f\'%(float(total_correct)/total_num))\n            logging.info (\'Accuracy (w/o spaces): %f\'%(float(total_correct_eliminate)/total_num))\n            logging.info (\'Edit Dist (w spaces): %f\'%(1.-float(total_edit_distance)/total_ref))\n            logging.info (\'Total Correct (w spaces): %d\'%total_correct)\n            logging.info (\'Total Correct (w/o spaces): %d\'%total_correct_eliminate)\n            logging.info (\'Total Edit Dist (w spaces): %d\'%total_edit_distance)\n            logging.info (\'Total Ref (w spaces): %d\'%total_ref)\n            logging.info (\'\')\n\n    logging.info (\'------------------------------------\')\n    logging.info (\'Final\')\n    logging.info (\'Total Num: %d\'%total_num)\n    logging.info (\'Accuracy (w spaces): %f\'%(float(total_correct)/total_num))\n    logging.info (\'Accuracy (w/o spaces): %f\'%(float(total_correct_eliminate)/total_num))\n    logging.info (\'Edit Dist (w spaces): %f\'%(1.-float(total_edit_distance)/total_ref))\n    logging.info (\'Total Correct (w spaces): %d\'%total_correct)\n    logging.info (\'Total Correct (w/o spaces): %d\'%total_correct_eliminate)\n    logging.info (\'Total Edit Dist (w spaces): %d\'%total_edit_distance)\n    logging.info (\'Total Ref (w spaces): %d\'%total_ref)\n\n# return (edit_distance, ref, match, match w/o)\ndef img_edit_distance(im1, im2, out_path=None):\n    img_data1 = np.asarray(im1, dtype=np.uint8) # height, width\n    img_data1 = np.transpose(img_data1)\n    h1 = img_data1.shape[1]\n    w1 = img_data1.shape[0]\n    img_data1 = (img_data1<=128).astype(np.uint8)\n    if im2:\n        img_data2 = np.asarray(im2, dtype=np.uint8) # height, width\n        img_data2 = np.transpose(img_data2)\n        h2 = img_data2.shape[1]\n        w2 = img_data2.shape[0]\n        img_data2 = (img_data2<=128).astype(np.uint8)\n    else:\n        img_data2 = []\n        h2 = h1\n    if h1 == h2:\n        seq1 = [\'\'.join([str(i) for i in item]) for item in img_data1]\n        seq2 = [\'\'.join([str(i) for i in item]) for item in img_data2]\n    elif h1 > h2:# pad h2\n        seq1 = [\'\'.join([str(i) for i in item]) for item in img_data1]\n        seq2 = [\'\'.join([str(i) for i in item])+\'\'.join([\'0\']*(h1-h2)) for item in img_data2]\n    else:\n        seq1 = [\'\'.join([str(i) for i in item])+\'\'.join([\'0\']*(h2-h1)) for item in img_data1]\n        seq2 = [\'\'.join([str(i) for i in item]) for item in img_data2]\n\n    seq1_int = [int(item,2) for item in seq1]\n    seq2_int = [int(item,2) for item in seq2]\n    big = int(\'\'.join([\'0\' for i in range(max(h1,h2))]),2)\n    seq1_eliminate = []\n    seq2_eliminate = []\n    seq1_new = []\n    seq2_new = []\n    for idx,items in enumerate(seq1_int):\n        if items>big:\n            seq1_eliminate.append(items)\n            seq1_new.append(seq1[idx])\n    for idx,items in enumerate(seq2_int):\n        if items>big:\n            seq2_eliminate.append(items)\n            seq2_new.append(seq2[idx])\n    if len(seq2) == 0:\n        return (len(seq1), len(seq1), False, False)\n\n    def make_strs(int_ls, int_ls2):\n        d = {}\n        seen = []\n        def build(ls):\n            for l in ls:\n                if int(l, 2) in d: continue\n                found = False\n                l_arr = np.array(map(int, l))\n            \n                for l2,l2_arr in seen:\n                    if np.abs(l_arr -l2_arr).sum() < 5:\n                        d[int(l, 2)] = d[int(l2, 2)]\n                        found = True\n                        break\n                if not found:\n                    d[int(l, 2)] = unichr(len(seen))\n                    seen.append((l, np.array(map(int, l))))\n                    \n        build(int_ls)\n        build(int_ls2)\n        return """".join([d[int(l, 2)] for l in int_ls]), """".join([d[int(l, 2)] for l in int_ls2])\n    #if out_path:\n    seq1_t, seq2_t = make_strs(seq1, seq2)\n\n    edit_distance = distance.levenshtein(seq1_int, seq2_int)\n    match = True\n    if edit_distance>0:\n        matcher = StringMatcher(None, seq1_t, seq2_t)\n\n        ls = []\n        for op in matcher.get_opcodes():\n            if op[0] == ""equal"" or (op[2]-op[1] < 5):\n                ls += [[int(r) for r in l]\n                       for l in seq1[op[1]:op[2]]\n                       ] \n            elif op[0] == ""replace"":\n                a = seq1[op[1]:op[2]]\n                b = seq2[op[3]:op[4]]\n                ls += [[int(r1)*3 + int(r2)*2\n                        if int(r1) != int(r2) else int(r1)\n                        for r1, r2 in zip(a[i] if i < len(a) else [0]*1000,\n                                          b[i] if i < len(b) else [0]*1000)]\n                       for i in range(max(len(a), len(b)))]\n                match = False\n            elif op[0] == ""insert"":\n\n                ls += [[int(r)*3 for r in l]\n                       for l in seq2[op[3]:op[4]]]\n                match = False\n            elif op[0] == ""delete"":\n                match = False\n                ls += [[int(r)*2 for r in l] for l in seq1[op[1]:op[2]]]\n\n        #vmax = 3\n        #plt.imshow(np.array(ls).transpose(), vmax=vmax)\n\n        #cmap = LinearSegmentedColormap.from_list(\'mycmap\', [(0. /vmax, \'white\'),\n        #                                                    (1. /vmax, \'grey\'),\n        #                                                    (2. /vmax, \'blue\'),\n        #                                                    (3. /vmax, \'red\')])\n\n        #plt.set_cmap(cmap)\n        #plt.axis(\'off\')\n        #plt.savefig(out_path, bbox_inches=""tight"")\n\n    match1 = match\n    seq1_t, seq2_t = make_strs(seq1_new, seq2_new)\n\n    if len(seq2_new) == 0 or len(seq1_new) == 0:\n        if len(seq2_new) == len(seq1_new):\n            return (edit_distance, max(len(seq1_int),len(seq2_int)), match1, True)# all blank\n        return (edit_distance, max(len(seq1_int),len(seq2_int)), match1, False)\n    match = True\n    matcher = StringMatcher(None, seq1_t, seq2_t)\n\n    ls = []\n    for op in matcher.get_opcodes():\n        if op[0] == ""equal"" or (op[2]-op[1] < 5):\n            ls += [[int(r) for r in l]\n                   for l in seq1[op[1]:op[2]]\n                   ] \n        elif op[0] == ""replace"":\n            a = seq1[op[1]:op[2]]\n            b = seq2[op[3]:op[4]]\n            ls += [[int(r1)*3 + int(r2)*2\n                    if int(r1) != int(r2) else int(r1)\n                    for r1, r2 in zip(a[i] if i < len(a) else [0]*1000,\n                                      b[i] if i < len(b) else [0]*1000)]\n                   for i in range(max(len(a), len(b)))]\n            match = False\n        elif op[0] == ""insert"":\n\n            ls += [[int(r)*3 for r in l]\n                   for l in seq2[op[3]:op[4]]]\n            match = False\n        elif op[0] == ""delete"":\n            match = False\n            ls += [[int(r)*2 for r in l] for l in seq1[op[1]:op[2]]]\n\n    match2 = match\n\n    return (edit_distance, max(len(seq1_int),len(seq2_int)), match1, match2)\n\ndef img_edit_distance_file(file1, file2, output_path=None):\n    img1 = Image.open(file1).convert(\'L\')\n    if os.path.exists(file2):\n        img2 = Image.open(file2).convert(\'L\')\n    else:\n        img2 = None\n    return img_edit_distance(img1, img2, output_path)\n\nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n    logging.info(\'Jobs finished\')\n'"
im2markup/scripts/evaluation/evaluate_text_edit_distance.py,0,"b'import os, sys, argparse, logging\nimport distance\n\n\ndef process_args(args):\n    parser = argparse.ArgumentParser(description=\'Evaluate text edit distance.\')\n\n    parser.add_argument(\'--result-path\', dest=\'result_path\',\n                        type=str, required=True,\n                        help=(\'Result file containing <img_path> <label_gold> <label_pred> <score_pred> <score_gold> per line. This should be set to the output file of the model.\'\n                        ))\n\n    parser.add_argument(\'--log-path\', dest=""log_path"",\n                        type=str, default=\'log.txt\',\n                        help=(\'Log file path, default=log.txt\' \n                        ))\n    parameters = parser.parse_args(args)\n    return parameters\n\ndef main(args):\n    parameters = process_args(args)\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\',\n        filename=parameters.log_path)\n\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\')\n    console.setFormatter(formatter)\n    logging.getLogger(\'\').addHandler(console)\n\n    logging.info(\'Script being executed: %s\'%__file__)\n\n    result_file = parameters.result_path\n    total_ref = 0\n    total_edit_distance = 0\n    with open(result_file) as fin:\n        for idx,line in enumerate(fin):\n            if idx % 100 == 0:\n                print (idx)\n            items = line.strip().split(\'\\t\')\n            if len(items) == 5:\n                img_path, label_gold, label_pred, score_pred, score_gold = items\n                l_pred = label_pred.strip()\n                l_gold = label_gold.strip()\n                tokens_pred = l_pred.split(\' \')\n                tokens_gold = l_gold.split(\' \')\n                ref = max(len(tokens_gold), len(tokens_pred))\n                edit_distance = distance.levenshtein(tokens_gold, tokens_pred)\n                total_ref += ref\n                total_edit_distance += edit_distance\n    logging.info(\'Edit Distance Accuracy: %f\'%(1.-float(total_edit_distance)/total_ref))\n   \nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n    logging.info(\'Jobs finished\')\n'"
im2markup/scripts/evaluation/render_html.py,0,"b'import sys, os, re, shutil, argparse, logging\nsys.path.insert(0, \'%s\'%os.path.join(os.path.dirname(__file__), \'../utils/\'))\nfrom image_utils import *\nfrom multiprocessing import Pool\nfrom multiprocessing.dummy import Pool as ThreadPool \n\nW=100\nH=100\n\ndef process_args(args):\n    parser = argparse.ArgumentParser(description=\'Render HTML files for comparison. Note that we render both the predicted results, and the original HTMLs.\')\n\n    parser.add_argument(\'--result-path\', dest=\'result_path\',\n                        type=str, required=True,\n                        help=(\'Result file containing <img_path> <label_gold> <label_pred> <score_pred> <score_gold> per line. This should be set to the output file of the model.\'\n                        ))\n    parser.add_argument(\'--output-dir\', dest=\'output_dir\',\n                        type=str, required=True,\n                        help=(\'Output directory to put the rendered images. A subfolder with name ""images_gold"" will be created for the rendered gold images, and a subfolder with name ""images_pred"" will be created for the rendered predictions.\'\n                        ))\n\n    parser.add_argument(\'--replace\', dest=\'replace\', action=\'store_true\',\n                        help=(\'Replace flag, if set to false, will ignore the already existing images.\'\n                        ))\n    parser.add_argument(\'--no-replace\', dest=\'replace\', action=\'store_false\')\n    parser.set_defaults(replace=False)\n    parser.add_argument(\'--num-threads\', dest=\'num_threads\',\n                        type=int, default=4,\n                        help=(\'Number of threads, default=4.\'\n                        ))\n    parser.add_argument(\'--log-path\', dest=""log_path"",\n                        type=str, default=\'log.txt\',\n                        help=(\'Log file path, default=log.txt\' \n                        ))\n    parameters = parser.parse_args(args)\n    return parameters\n\ndef main(args):\n    parameters = process_args(args)\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\',\n        filename=parameters.log_path)\n\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\')\n    console.setFormatter(formatter)\n    logging.getLogger(\'\').addHandler(console)\n\n    logging.info(\'Script being executed: %s\'%__file__)\n   \n    result_path = parameters.result_path\n    output_dir = parameters.output_dir\n    assert os.path.exists(result_path), result_path\n\n    pred_dir = os.path.join(output_dir, \'images_pred\')\n    gold_dir = os.path.join(output_dir, \'images_gold\')\n    for dirname in [pred_dir, gold_dir]:\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n    lines = []\n    with open(result_path) as fin:\n        for idx,line in enumerate(fin):\n            items = line.strip().split(\'\\t\')\n            if len(items) == 5:\n                img_path, label_gold, label_pred, score_pred, score_gold = items\n                img_idx = img_path[:-9]\n                lines.append((label_pred, img_idx, pred_dir, parameters.replace))\n                lines.append((label_gold, img_idx, gold_dir, parameters.replace))\n    \n    logging.info(\'Creating pool with %d threads\'%parameters.num_threads)\n    pool = ThreadPool(parameters.num_threads)\n    logging.info(\'Jobs running...\')\n    results = pool.map(main_parallel, lines)\n    pool.close() \n    pool.join() \n\n\ndef main_parallel(l):\n    label, img_idx, dirname, replace = l\n    if replace or (not os.path.exists(\'%s/%s-full.png\'%(dirname, img_idx))):\n        html_name = \'%s_%s.html\'%(dirname, img_idx)\n        with open(html_name, \'w\') as fout:\n            fout.write(label)\n        os.system(\'webkit2png --clipwidth=1 --clipheight=1 -Fs 1 -W %d -H %d %s -o %s/%s\'%(W,H,html_name,dirname,img_idx))\n        os.remove(html_name)\n\nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n    logging.info(\'Jobs finished\')\n'"
im2markup/scripts/evaluation/render_latex.py,0,"b'import sys, os, re, shutil, argparse, logging\nsys.path.insert(0, \'%s\'%os.path.join(os.path.dirname(__file__), \'../utils/\'))\nfrom utils import run\nfrom image_utils import *\nfrom multiprocessing import Pool\nfrom multiprocessing.dummy import Pool as ThreadPool \n\n\nTIMEOUT = 10\n\n# replace \\pmatrix with \\begin{pmatrix}\\end{pmatrix}\n# replace \\matrix with \\begin{matrix}\\end{matrix}\ntemplate = r""""""\n\\documentclass[12pt]{article}\n\\pagestyle{empty}\n\\usepackage{amsmath}\n\\newcommand{\\mymatrix}[1]{\\begin{matrix}#1\\end{matrix}}\n\\newcommand{\\mypmatrix}[1]{\\begin{pmatrix}#1\\end{pmatrix}}\n\\begin{document}\n\\begin{displaymath}\n%s\n\\end{displaymath}\n\\end{document}\n""""""\n\n\ndef process_args(args):\n    parser = argparse.ArgumentParser(description=\'Render latex formulas for comparison. Note that we need to render both the predicted results, and the original formulas, since we need to make sure the same environment of rendering is used.\')\n\n    parser.add_argument(\'--result-path\', dest=\'result_path\',\n                        type=str, required=True,\n                        help=(\'Result file containing <img_path> <label_gold> <label_pred> <score_pred> <score_gold> per line. This should be set to the output file of the model.\'\n                        ))\n    parser.add_argument(\'--data-path\', dest=\'data_path\',\n                        type=str, required=True,\n                        help=(\'Input file which contains the samples to be evaluated. The format is <img_path> <label_idx> per line.\'\n                        ))\n    parser.add_argument(\'--label-path\', dest=\'label_path\',\n                        type=str, required=True,\n                        help=(\'Gold label file which contains a formula per line. Note that this does not necessarily need to be tokenized, and for comparing against the gold standard, the original (un-preprocessed) label file shall be used.\'\n                        ))\n    parser.add_argument(\'--output-dir\', dest=\'output_dir\',\n                        type=str, required=True,\n                        help=(\'Output directory to put the rendered images. A subfolder with name ""images_gold"" will be created for the rendered gold images, and a subfolder with name ""images_pred"" will be created for the rendered predictions.\'\n                        ))\n\n    parser.add_argument(\'--replace\', dest=\'replace\', action=\'store_true\',\n                        help=(\'Replace flag, if set to false, will ignore the already existing images.\'\n                        ))\n    parser.add_argument(\'--no-replace\', dest=\'replace\', action=\'store_false\')\n    parser.set_defaults(replace=False)\n    parser.add_argument(\'--num-threads\', dest=\'num_threads\',\n                        type=int, default=4,\n                        help=(\'Number of threads, default=4.\'\n                        ))\n    parser.add_argument(\'--log-path\', dest=""log_path"",\n                        type=str, default=\'log.txt\',\n                        help=(\'Log file path, default=log.txt\' \n                        ))\n    parameters = parser.parse_args(args)\n    return parameters\n\ndef main(args):\n    parameters = process_args(args)\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\',\n        filename=parameters.log_path)\n\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\')\n    console.setFormatter(formatter)\n    logging.getLogger(\'\').addHandler(console)\n\n    logging.info(\'Script being executed: %s\'%__file__)\n   \n    result_path = parameters.result_path\n    data_path = parameters.data_path\n    label_path = parameters.label_path\n    output_dir = parameters.output_dir\n    assert os.path.exists(label_path), label_path\n    assert os.path.exists(result_path), result_path\n    assert os.path.exists(data_path), data_path\n\n    pred_dir = os.path.join(output_dir, \'images_pred\')\n    gold_dir = os.path.join(output_dir, \'images_gold\')\n    for dirname in [pred_dir, gold_dir]:\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n\n    formulas = open(label_path).readlines()\n    lines = []\n    with open(data_path) as fin:\n        for line in fin:\n            img_path, line_idx = line.strip().split()\n            lines.append((img_path, formulas[int(line_idx)], os.path.join(gold_dir, img_path), parameters.replace))\n    with open(result_path) as fin:\n        for line in fin:\n            img_path, label_gold, label_pred, _, _ = line.strip().split(\'\\t\')\n            lines.append((img_path, label_pred, os.path.join(pred_dir, img_path), parameters.replace))\n    logging.info(\'Creating pool with %d threads\'%parameters.num_threads)\n    pool = ThreadPool(parameters.num_threads)\n    logging.info(\'Jobs running...\')\n    results = pool.map(main_parallel, lines)\n    pool.close() \n    pool.join() \n\ndef output_err(output_path, i, reason, img):\n    logging.info(\'ERROR: %s %s\\n\'%(img,reason))\n\ndef main_parallel(line):\n    img_path, l, output_path, replace = line\n    pre_name = output_path.replace(\'/\', \'_\').replace(\'.\',\'_\')\n    l = l.strip()\n    l = l.replace(r\'\\pmatrix\', r\'\\mypmatrix\')\n    l = l.replace(r\'\\matrix\', r\'\\mymatrix\')\n    # remove leading comments\n    l = l.strip(\'%\')\n    if len(l) == 0:\n        l = \'\\\\hspace{1cm}\'\n    # \\hspace {1 . 5 cm} -> \\hspace {1.5cm}\n    for space in [""hspace"", ""vspace""]:\n        match = re.finditer(space + "" {(.*?)}"", l)\n        if match:\n            new_l = """"\n            last = 0\n            for m in match:\n                new_l = new_l + l[last:m.start(1)] + m.group(1).replace("" "", """")\n                last = m.end(1)\n            new_l = new_l + l[last:]\n            l = new_l    \n    if replace or (not os.path.exists(output_path)):\n        tex_filename = pre_name+\'.tex\'\n        log_filename = pre_name+\'.log\'\n        aux_filename = pre_name+\'.aux\'\n        with open(tex_filename, ""w"") as w: \n            print >> w, (template%l)\n        run(""pdflatex -interaction=nonstopmode %s  >/dev/null""%tex_filename, TIMEOUT)\n        os.remove(tex_filename)\n        os.remove(log_filename)\n        os.remove(aux_filename)\n        pdf_filename = tex_filename[:-4]+\'.pdf\'\n        png_filename = tex_filename[:-4]+\'.png\'\n        if not os.path.exists(pdf_filename):\n            output_err(output_path, 0, \'cannot compile\', img_path)\n        else:\n            os.system(""convert -density 200 -quality 100 %s %s""%(pdf_filename, png_filename))\n            os.remove(pdf_filename)\n            if os.path.exists(png_filename):\n                crop_image(png_filename, output_path)\n                os.remove(png_filename)\n\n        \nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n    logging.info(\'Jobs finished\')\n'"
im2markup/scripts/preprocessing/generate_latex_vocab.py,0,"b'import sys, logging, argparse, os\n\ndef process_args(args):\n    parser = argparse.ArgumentParser(description=\'Generate vocabulary file.\')\n\n    parser.add_argument(\'--data-path\', dest=\'data_path\',\n                        type=str, required=True,\n                        help=(\'Input file containing <img_name> <line_idx> per line. This should be the file used for training.\'\n                        ))\n    parser.add_argument(\'--label-path\', dest=\'label_path\',\n                        type=str, required=True,\n                        help=(\'Input file containing a tokenized formula per line.\'\n                        ))\n    parser.add_argument(\'--output-file\', dest=\'output_file\',\n                        type=str, required=True,\n                        help=(\'Output file for putting vocabulary.\'\n                        ))\n    parser.add_argument(\'--unk-threshold\', dest=\'unk_threshold\',\n                        type=int, default=1,\n                        help=(\'If the number of occurences of a token is less than (including) the threshold, then it will be excluded from the generated vocabulary.\'\n                        ))\n    parser.add_argument(\'--log-path\', dest=""log_path"",\n                        type=str, default=\'log.txt\',\n                        help=(\'Log file path, default=log.txt\' \n                        ))\n    parameters = parser.parse_args(args)\n    return parameters\n\ndef main(args):\n    parameters = process_args(args)\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\',\n        filename=parameters.log_path)\n\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\')\n    console.setFormatter(formatter)\n    logging.getLogger(\'\').addHandler(console)\n\n    logging.info(\'Script being executed: %s\'%__file__)\n\n    label_path = parameters.label_path\n    assert os.path.exists(label_path), label_path\n    data_path = parameters.data_path\n    assert os.path.exists(data_path), data_path\n\n    formulas = open(label_path).readlines()\n    vocab = {}\n    max_len = 0\n    with open(data_path) as fin:\n        for line in fin:\n            _, line_idx = line.strip().split()\n            line_strip = formulas[int(line_idx)].strip()\n            tokens = line_strip.split()\n            tokens_out = []\n            for token in tokens:\n                tokens_out.append(token)\n                if token not in vocab:\n                    vocab[token] = 0\n                vocab[token] += 1\n\n    vocab_sort = sorted(list(vocab.keys()))\n    vocab_out = []\n    num_unknown = 0\n    for word in vocab_sort:\n        if vocab[word] > parameters.unk_threshold:\n            vocab_out.append(word)\n        else:\n            num_unknown += 1\n    #vocab = [""\'""+word.replace(\'\\\\\',\'\\\\\\\\\').replace(\'\\\'\', \'\\\\\\\'\')+""\'"" for word in vocab_out]\n    vocab = [word for word in vocab_out]\n\n    with open(parameters.output_file, \'w\') as fout:\n        fout.write(\'\\n\'.join(vocab))\n    logging.info(\'#UNK\\\'s: %d\'%num_unknown)\n\nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n    logging.info(\'Jobs finished\')\n'"
im2markup/scripts/preprocessing/preprocess_filter.py,0,"b'#!/usr/bin/env python\nimport sys, os, argparse, logging\nimport numpy as np\nimport PIL\nfrom PIL import Image\n\ndef process_args(args):\n    parser = argparse.ArgumentParser(description=\'Process im2latex-100k train, test, development files (<label_idx> <img_path> <mode>) for formatting files such that can be used for training. (<img_path> <label_idx>>). Additionaly, if <filter> flag is set, large images, too long formulas and formulas that cannot be parsed will be discarded.\')\n\n    parser.add_argument(\'--image-dir\', dest=\'image_dir\',\n                        type=str, default=\'\',\n                        help=(\'Directory containing processed images.\'\n                        ))\n    parser.add_argument(\'--data-path\', dest=\'data_path\',\n                        type=str, required=True,\n                        help=(\'Input file path containing <label_idx> <img_path> <mode> per line. Note that <img_path> does not contain postfix.\'\n                        ))\n    parser.add_argument(\'--output-path\', dest=\'output_path\',\n                        type=str, required=True,\n                        help=(\'Output file path containing <img_path> <label_idx> per line. Note that <img_path> does contain postfix. If filter flag is set, then the output file may have less lines than original file.\'\n                        ))\n\n    parser.add_argument(\'--label-path\', dest=\'label_path\',\n                        type=str, default=\'\',\n                        help=(\'Input label path containing <formula> per line. This is required if filter flag is set, and data point with blank formulas will be discarded.\'\n                        ))\n    parser.add_argument(\'--filter\', dest=\'filter\', action=\'store_true\',\n                        help=(\'Filter flag, if set, then too large images, formulas that cannot be parsed or have too many tokens will be discarded.\'\n                        ))\n    parser.add_argument(\'--no-filter\', dest=\'filter\', action=\'store_false\')\n    parser.set_defaults(filter=False)\n    parser.add_argument(\'--max-width\', dest=\'max_width\',\n                        type=int, default=500,\n                        help=(\'If filter flag is set, images with width than max-width will be discarded in the output file.\'\n                        ))\n    parser.add_argument(\'--max-height\', dest=\'max_height\',\n                        type=int, default=160,\n                        help=(\'If filter flag is set, images with larger height than max-width will be discarded in the output file.\'\n                        ))\n    parser.add_argument(\'--max-tokens\', dest=\'max_tokens\',\n                        type=int, default=150,\n                        help=(\'If filter flag is set, formulas with more than max-tokens tokens will be discarded in the output file.\'\n                        ))\n    parser.add_argument(\'--log-path\', dest=""log_path"",\n                        type=str, default=\'log.txt\',\n                        help=(\'Log file path, default=log.txt\' \n                        ))\n    parser.add_argument(\'--postfix\', dest=\'postfix\',\n                        type=str, default=\'.png\',\n                        help=(\'The format of images, default="".png"".\'\n                        ))\n    parameters = parser.parse_args(args)\n    return parameters\n\ndef main(args):\n    parameters = process_args(args)\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\',\n        filename=parameters.log_path)\n\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\')\n    console.setFormatter(formatter)\n    logging.getLogger(\'\').addHandler(console)\n\n    logging.info(\'Script being executed: %s\'%__file__)\n    data_path = parameters.data_path\n    output_path = parameters.output_path\n    image_dir = parameters.image_dir\n\n    num_discard = 0\n    num_nonexist = 0\n\n    if parameters.filter:\n        assert os.path.isfile(parameters.label_path), parameters.label_path\n        labels = open(parameters.label_path).readlines()\n    with open(output_path, \'w\') as fout:\n        with open(data_path, \'r\') as fdata:\n            for line in fdata:\n                line_strip = line.strip()\n                if len(line_strip) > 0:\n                    line_idx, img_path, mod = line_strip.split()\n                    img_path = os.path.join(image_dir, img_path) + parameters.postfix\n                    if parameters.filter:\n                        if not os.path.exists(img_path):\n                            logging.warning(\'%s does not exist!\'%os.path.basename(img_path))\n                            num_nonexist += 1\n                            continue\n                        old_im = Image.open(img_path)\n                        old_size = old_im.size\n                        w = old_size[0]\n                        h = old_size[1]\n                    else:\n                        w = 0\n                        h = 0\n                    if (not parameters.filter) or (w <= parameters.max_width and h <= parameters.max_height):\n                        if parameters.filter:\n                            label = labels[int(line_idx)]\n                            if len(label.strip()) == 0:\n                                logging.info(\'%s discarded due to cannot-be-parsed formula!\'%os.path.basename(img_path))\n                                continue\n                            if len(label.strip().split()) > parameters.max_tokens:\n                                logging.info(\'%s discarded due to too many tokens!\'%os.path.basename(img_path))\n                                continue\n                        fout.write(\'%s %s\\n\'%(os.path.basename(img_path),line_idx))\n                    else:\n                        logging.info(\'%s discarded due to large image size!\'%os.path.basename(img_path))\n                        num_discard += 1\n    logging.info(\'%d discarded. %d not found in %s.\'%(num_discard, num_nonexist, image_dir))\n\n\nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n    logging.info(\'Jobs finished\')\n'"
im2markup/scripts/preprocessing/preprocess_formulas.py,0,"b'#!/usr/bin/env python\n# tokenize latex formulas\nimport sys, os, argparse, logging, subprocess, shutil\n\ndef is_ascii(str):\n    try:\n        str.decode(\'ascii\')\n        return True\n    except UnicodeError:\n        return False\n\ndef process_args(args):\n    parser = argparse.ArgumentParser(description=\'Preprocess (tokenize or normalize) latex formulas\')\n\n    parser.add_argument(\'--mode\', dest=\'mode\',\n                        choices=[\'tokenize\', \'normalize\'], required=True,\n                        help=(\'Tokenize (split to tokens seperated by space) or normalize (further translate to an equivalent standard form).\'\n                        ))\n    parser.add_argument(\'--input-file\', dest=\'input_file\',\n                        type=str, required=True,\n                        help=(\'Input file containing latex formulas. One formula per line.\'\n                        ))\n    parser.add_argument(\'--output-file\', dest=\'output_file\',\n                        type=str, required=True,\n                        help=(\'Output file.\'\n                        ))\n    parser.add_argument(\'--num-threads\', dest=\'num_threads\',\n                        type=int, default=4,\n                        help=(\'Number of threads, default=4.\'\n                        ))\n    parser.add_argument(\'--log-path\', dest=""log_path"",\n                        type=str, default=\'log.txt\',\n                        help=(\'Log file path, default=log.txt\' \n                        ))\n    parameters = parser.parse_args(args)\n    return parameters\n\ndef main(args):\n    parameters = process_args(args)\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\',\n        filename=parameters.log_path)\n\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\')\n    console.setFormatter(formatter)\n    logging.getLogger(\'\').addHandler(console)\n\n    logging.info(\'Script being executed: %s\'%__file__)\n\n    input_file = parameters.input_file\n    output_file = parameters.output_file\n\n    assert os.path.exists(input_file), input_file\n    cmd = ""perl -pe \'s|hskip(.*?)(cm\\\\|in\\\\|pt\\\\|mm\\\\|em)|hspace{\\\\1\\\\2}|g\' %s > %s""%(input_file, output_file)\n    ret = subprocess.call(cmd, shell=True)\n    if ret != 0:\n        logging.error(\'FAILED: %s\'%cmd)\n\n    temp_file = output_file + \'.tmp\'\n    with open(temp_file, \'w\') as fout:\n        fout.write(open(output_file).read().replace(\'\\r\', \' \')) # delete \\r\n    #shutil.copy(output_file, temp_file)\n\n    cmd = ""cat %s | node scripts/preprocessing/preprocess_latex.js %s > %s ""%(temp_file, parameters.mode, output_file)\n    ret = subprocess.call(cmd, shell=True)\n    os.remove(temp_file)\n    if ret != 0:\n        logging.error(\'FAILED: %s\'%cmd)\n    temp_file = output_file + \'.tmp\'\n    shutil.move(output_file, temp_file)\n    with open(temp_file) as fin:\n        with open(output_file, \'w\') as fout:\n            for line in fin:\n                tokens = line.strip().split()\n                tokens_out = []\n                for token in tokens:\n                    if is_ascii(token):\n                        tokens_out.append(token)\n                fout.write(\' \'.join(tokens_out)+\'\\n\')\n    os.remove(temp_file)\n\nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n    logging.info(\'Jobs finished\')\n'"
im2markup/scripts/preprocessing/preprocess_images.py,0,"b'#!/usr/bin/env python\n# Preprocess images for ease of training\nimport sys, os, argparse, json, glob, logging\nimport numpy as np\nfrom PIL import Image\nsys.path.insert(0, \'%s\'%os.path.join(os.path.dirname(__file__), \'../utils/\'))\nfrom image_utils import *\nfrom multiprocessing import Pool\nfrom multiprocessing.dummy import Pool as ThreadPool\n\ndef process_args(args):\n    parser = argparse.ArgumentParser(description=\'Process images for ease of training. Crop images to get rid of the background. For a cropped image of size (w,h), we pad it with PAD_TOP, PAD_BOTTOM, PAD_LEFT, PAD_RIGHT, and the result is of size (w+PAD_LEFT+PAD_RIGHT, h+PAD_TOP+PAD_BOTTOM. Then we see which bucket it falls into and pad them with whitespace to match the smallest bucket that can hold it. Finally, downsample images.\')\n\n    parser.add_argument(\'--input-dir\', dest=\'input_dir\',\n                        type=str, required=True,\n                        help=(\'Input directory containing orginal images.\'\n                        ))\n    parser.add_argument(\'--output-dir\', dest=\'output_dir\',\n                        type=str, required=True,\n                        help=(\'Output directory to put processed images.\'\n                        ))\n    parser.add_argument(\'--num-threads\', dest=\'num_threads\',\n                        type=int, default=4,\n                        help=(\'Number of threads, default=4.\'\n                        ))\n    parser.add_argument(\'--crop-blank-default-size\', dest=\'crop_blank_default_size\',\n                        type=str, default=\'[600,60]\',\n                        help=(\'If an image is blank, this is the size of the cropped image, should be a Json string. Default=(600,60).\'\n                        ))\n    parser.add_argument(\'--pad-size\', dest=\'pad_size\',\n                        type=str, default=\'[8,8,8,8]\',\n                        help=(\'We pad the cropped image to the top, left, bottom, right with whitespace of size PAD_TOP, PAD_LEFT, PAD_BOTTOM, PAD_RIGHT, should be a Json string. Default=(8,8,8,8).\'\n                        ))\n    parser.add_argument(\'--buckets\', dest=\'buckets\',\n                        type=str, default=\'[[240,100], [320,80], [400,80],[400,100], [480,80], [480,100], [560,80], [560,100], [640,80],[640,100], [720,80], [720,100], [720,120], [720, 200], [800,100],[800,320], [1000,200]]\',\n                        help=(\'Bucket sizes used for grouping. Should be a Json string. Note that this denotes the bucket size after padding and before downsampling.\'\n                        ))\n    parser.add_argument(\'--downsample-ratio\', dest=\'downsample_ratio\',\n                        type=float, default=2.,\n                        help=(\'The ratio of downsampling, default=2.0.\'\n                        ))\n    parser.add_argument(\'--log-path\', dest=""log_path"",\n                        type=str, default=\'log.txt\',\n                        help=(\'Log file path, default=log.txt\'\n                        ))\n    parser.add_argument(\'--postfix\', dest=\'postfix\',\n                        type=str, default=\'.png\',\n                        help=(\'The format of images, default="".png"".\'\n                        ))\n    parameters = parser.parse_args(args)\n    return parameters\n\ndef main_parallel(l):\n    filename, postfix, output_filename, crop_blank_default_size, pad_size, buckets, downsample_ratio = l\n    postfix_length = len(postfix)\n    status = crop_image(filename, output_filename, crop_blank_default_size)\n    if not status:\n        logging.info(\'%s is blank, crop a white image of default size!\'%filename)\n    status = pad_group_image(output_filename, output_filename, pad_size, buckets)\n    if not status:\n        logging.info(\'%s (after cropping and padding) is larger than the largest provided bucket size, left unchanged!\'%filename)\n\tos.remove(output_filename)\n\treturn\n    status = downsample_image(output_filename, output_filename, downsample_ratio)\n\ndef main(args):\n    parameters = process_args(args)\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\',\n        filename=parameters.log_path)\n\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\')\n    console.setFormatter(formatter)\n    logging.getLogger(\'\').addHandler(console)\n\n    logging.info(\'Script being executed: %s\'%__file__)\n\n    output_dir = parameters.output_dir\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    input_dir = parameters.input_dir\n    postfix = parameters.postfix\n    crop_blank_default_size = json.loads(parameters.crop_blank_default_size)\n    pad_size = json.loads(parameters.pad_size)\n    buckets = json.loads(parameters.buckets)\n    print buckets[0]\n    raw_input()\n    downsample_ratio = parameters.downsample_ratio\n\n    filenames = glob.glob(os.path.join(input_dir, \'*\'+postfix))\n    logging.info(\'Creating pool with %d threads\'%parameters.num_threads)\n    pool = ThreadPool(parameters.num_threads)\n    logging.info(\'Jobs running...\')\n    results = pool.map(main_parallel, [(filename, postfix, os.path.join(output_dir, os.path.basename(filename)), crop_blank_default_size, pad_size, buckets, downsample_ratio) for filename in filenames])\n    pool.close()\n    pool.join()\n\nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n    logging.info(\'Jobs finished\')\n'"
im2markup/scripts/utils/image_utils.py,0,"b'import PIL\nfrom PIL import Image\nimport numpy as np\n\ndef crop_image(img, output_path, default_size=None):\n    old_im = Image.open(img).convert(\'L\')\n    img_data = np.asarray(old_im, dtype=np.uint8) # height, width\n    nnz_inds = np.where(img_data!=255)\n    if len(nnz_inds[0]) == 0:\n        if not default_size:\n            old_im.save(output_path)\n            return False\n        else:\n            assert len(default_size) == 2, default_size\n            x_min,y_min,x_max,y_max = 0,0,default_size[0],default_size[1]\n            old_im = old_im.crop((x_min, y_min, x_max+1, y_max+1))\n            old_im.save(output_path)\n            return False\n    y_min = np.min(nnz_inds[0])\n    y_max = np.max(nnz_inds[0])\n    x_min = np.min(nnz_inds[1])\n    x_max = np.max(nnz_inds[1])\n    old_im = old_im.crop((x_min, y_min, x_max+1, y_max+1))\n    old_im.save(output_path)\n    return True\n\ndef pad_group_image(img, output_path, pad_size, buckets):\n    PAD_TOP, PAD_LEFT, PAD_BOTTOM, PAD_RIGHT = pad_size\n    old_im = Image.open(img)\n    old_size = (old_im.size[0]+PAD_LEFT+PAD_RIGHT, old_im.size[1]+PAD_TOP+PAD_BOTTOM)\n    j = -1\n    for i in range(len(buckets)):\n        if old_size[0]<=buckets[i][0] and old_size[1]<=buckets[i][1]:\n            j = i\n            break\n    if j < 0:\n        new_size = old_size\n        new_im = Image.new(""RGB"", new_size, (255,255,255))\n        new_im.paste(old_im, (PAD_LEFT,PAD_TOP))\n        new_im.save(output_path)\n        return False\n    new_size = buckets[j]\n    print new_size\n    new_im = Image.new(""RGB"", new_size, (255,255,255))\n    new_im.paste(old_im, (PAD_LEFT,PAD_TOP))\n    new_im.save(output_path)\n    return True\n\ndef pad_image(img, output_path, pad_size, buckets):\n    PAD_TOP, PAD_LEFT, PAD_BOTTOM, PAD_RIGHT = pad_size\n    old_im = Image.open(img)\n    old_size = (old_im.size[0]+PAD_LEFT+PAD_RIGHT, old_im.size[1]+PAD_TOP+PAD_BOTTOM)\n    new_size = old_size\n    new_im = Image.new(""RGB"", new_size, (255,255,255))\n    new_im.paste(old_im, (PAD_LEFT,PAD_TOP))\n    new_im.save(output_path)\n    return True\n\ndef downsample_image(img, output_path, ratio):\n    assert ratio>=1, ratio\n    if ratio == 1:\n        return True\n    old_im = Image.open(img)\n    old_size = old_im.size\n    new_size = (int(old_size[0]/ratio), int(old_size[1]/ratio))\n\n    new_im = old_im.resize(new_size, PIL.Image.LANCZOS)\n    new_im.save(output_path)\n    return True\n'"
im2markup/scripts/utils/utils.py,0,"b'import subprocess, shlex\nfrom threading import Timer\n\ndef run(cmd, timeout_sec):\n    proc = subprocess.Popen(cmd, shell=True)\n    kill_proc = lambda p: p.kill()\n    timer = Timer(timeout_sec, kill_proc, [proc])\n    try:\n        timer.start()\n        stdout,stderr = proc.communicate()\n    finally:\n        timer.cancel()\n'"
