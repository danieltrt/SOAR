file_path,api_count,code
setup.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nfrom setuptools import setup, find_packages\n\n# Read __version__ avoiding imports that might be in install_requires.\nversion_vars = {}\nwith open(os.path.join(os.path.dirname(__file__), \'rlgraph\', \'version.py\')) as fp:\n    exec(fp.read(), version_vars)\n\ninstall_requires = [\n    \'absl-py\',\n    \'numpy\',\n    \'opencv-python\',\n    \'packaging\',  # Needed for version string comparisons.\n    \'pyyaml\',\n    \'pytest\',\n    \'requests\',\n    \'scipy\',\n    \'six\'\n]\n\nsetup_requires = []\n\nextras_require = {\n    # DL-backend specific extra-dependencies.\n    \'tf\': [\'tensorflow\', \'tensorflow_probability\'],\n    \'tf-gpu\': [\'tensorflow-gpu\', \'tensorflow_probability\'],\n    \'pytorch\': [\'torch\', \'torchvision\'],  # TODO platform dependent.\n    \'horovod\': \'horovod\',\n    \'ray\': [\'ray\', \'lz4\', \'pyarrow\'],\n    # Environment related extra dependencies.\n    \'gym\': [\'gym\', \'atari-py\'],  # To use openAI Gym Envs (e.g. Atari).\n    \'mlagents_env\': [\'mlagents\'],  # To use MLAgents Envs (Unity3D).\n    \'pygame\': [\'pygame\'],  # To use GridWorld Envs with visualization tools (not required).\n    \'graphviz\': [\'graphviz\']  # To use GraphViz\n}\n\nsetup(\n    name=\'rlgraph\',\n    version=version_vars[\'__version__\'],\n    description=\'A Framework for Modular Deep Reinforcement Learning\',\n    long_description=""""""\nRLgraph is a framework to quickly prototype, define and execute reinforcement learning\nalgorithms both in research and practice. RLgraph supports both\nTensorFlow (or static graphs in general) and Pytorch (eager/define-by run execution) through\na single component based interface. An introductory blogpost can be found here: \nhttps://rlgraph.github.io/rlgraph/2019/01/04/introducing-rlgraph.html\n"""""",\n    url=\'https://rlgraph.org\',\n    author=\'The RLgraph development team\',\n    author_email=\'rlgraph@rlgraph.org\',\n    license=\'Apache 2.0\',\n    packages=[package for package in find_packages() if package.startswith(\'rlgraph\')],\n    install_requires=install_requires,\n    setup_requires=setup_requires,\n    extras_require=extras_require,\n    zip_safe=False\n)\n'"
docs/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n'"
docs/conf.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nimport re\n\nsys.path.insert(0, os.path.abspath(\'.\'))\nsys.path.insert(1, os.path.abspath(\'../\'))\n# readthedocs path\nsys.path.insert(2, \'/home/docs/checkouts/readthedocs.org/user_builds/rlgraph/checkouts/latest/rlgraph\')\n\nfrom rlgraph.version import __version__\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'rlgraph\'\ncopyright = \'2018, RLgraph authors\'\nauthor = \'RLgraph authors\'\n\n# The short X.Y version\nversion = re.sub(r\'\\.\\d+$\', """", __version__)\n# The full version, including alpha/beta/rc tags\nrelease = __version__\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinxdoc\'\nhtml_style = \'rlgraph_sphinx.css\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'rlgraphdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'rlgraph.tex\', \'rlgraph Documentation\',\n     \'RLgraph authors\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'rlgraph\', \'rlgraph Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'rlgraph\', \'rlgraph Documentation\',\n     author, \'rlgraph\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for intersphinx extension ---------------------------------------\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\'https://docs.python.org/\': None}\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n'"
examples/actor_critic_cartpole.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nExample script for training an actor-critic policy gradient agent on an OpenAI gym environment.\n\nUsage:\n\npython actor_critic_cartpole.py [--config configs/actor_critic_cartpole.json] [--env CartPole-v0]\n\n```\n# Run script\npython actor_critic_cartpole.py\n```\n""""""\n\nimport json\nimport os\nimport sys\n\nimport numpy as np\nfrom absl import flags\n\nfrom rlgraph.agents import Agent\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.execution import SingleThreadedWorker\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'config\', \'./configs/actor_critic_cartpole.json\', \'Agent config file.\')\nflags.DEFINE_string(\'env\', \'CartPole-v0\', \'openAI Gym environment ID.\')\nflags.DEFINE_bool(\'visualize\', False, \'Whether to display the env during learning.\')\n\n\ndef main(argv):\n    try:\n        FLAGS(argv)\n    except flags.Error as e:\n        print(\'%s\\\\nUsage: %s ARGS\\\\n%s\' % (e, sys.argv[0], FLAGS))\n\n    agent_config_path = os.path.join(os.getcwd(), FLAGS.config)\n    with open(agent_config_path, \'rt\') as fp:\n        agent_config = json.load(fp)\n\n    env = OpenAIGymEnv.from_spec({\n        ""type"": ""openai"",\n        ""gym_env"": FLAGS.env,\n        ""visualize"": FLAGS.visualize\n    })\n\n    agent = Agent.from_spec(\n        agent_config,\n        state_space=env.state_space,\n        action_space=env.action_space\n    )\n\n    episode_returns = []\n\n    def episode_finished_callback(episode_return, duration, timesteps, **kwargs):\n        episode_returns.append(episode_return)\n        if len(episode_returns) % 10 == 0:\n            print(""Episode {} finished: reward={:.2f}, average reward={:.2f}."".format(\n                len(episode_returns), episode_return, np.mean(episode_returns[-10:])\n            ))\n\n    worker = SingleThreadedWorker(env_spec=lambda: env, agent=agent, render=False, worker_executes_preprocessing=False,\n                                  episode_finish_callback=episode_finished_callback)\n    print(""Starting workload, this will take some time for the agents to build."")\n\n    # Use exploration is true for training, false for evaluation.\n    worker.execute_timesteps(20000, use_exploration=True)\n\n    # Note: A basic actor critic is very sensitive to hyper-parameters and might collapse after reaching the maximum\n    # reward. In practice, it would be recommended to stop training when a reward threshold is reached.\n    print(""Mean reward: {:.2f} / over the last 10 episodes: {:.2f}"".format(\n        np.mean(episode_returns), np.mean(episode_returns[-10:])\n    ))\n\n\nif __name__ == \'__main__\':\n    main(sys.argv)\n'"
examples/apex_pong.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nExample script for training an APEX agent on the Arcade Learning Environment (ALE). This agent can be used both\nfor local and distributed training. You\'ll need to start Ray yourself using `ray start --head --redis-port 6379`.\nIf you want to use distributed training, just join the Ray cluster with `ray start --redis-address=master.host:6379`.\n\nUsage:\n\npython apex_ale.py [--config configs/apex_pong.json] [--gpu/--nogpu] [--env PongNoFrameSkip-v4] [--output results.csv]\n\nPlease make sure that Ray is configured as the distributed backend for RLgraph, e.g. by running this command:\n\n```bash\necho \'{""BACKEND"":""tf"",""DISTRIBUTED_BACKEND"":""ray""}\' > $HOME/.rlgraph/rlgraph.json\n```\n\nThen you can start up the Ape-X agent:\n\n```bash\n# Start ray on the head machine\nray start --head --redis-port 6379\n# Optionally join to this cluster from other machines with ray start --redis-address=...\n\n# Run script\npython apex_pong.py\n```\n""""""\n\nimport csv\nimport json\nimport os\nimport sys\n\nfrom absl import flags\n\nfrom rlgraph.execution.ray import ApexExecutor\n\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'config\', \'./configs/apex_pong.json\', \'Agent config file.\')\nflags.DEFINE_boolean(\'gpu\', True, \'Use GPU for training.\')\nflags.DEFINE_string(\'env\', \'PongNoFrameskip-v4\', \'gym environment ID.\')\nflags.DEFINE_string(\'output\', \'results.csv\', \'Output rewards file.\')\n\n\ndef main(argv):\n    try:\n        FLAGS(argv)\n    except flags.Error as e:\n        print(\'%s\\\\nUsage: %s ARGS\\\\n%s\' % (e, sys.argv[0], FLAGS))\n\n    agent_config_path = os.path.join(os.getcwd(), FLAGS.config)\n    with open(agent_config_path, \'rt\') as fp:\n        agent_config = json.load(fp)\n\n    # GPU usage is enabled in the config per default, so we disable it if --nogpu was passed.\n    if not FLAGS.gpu:\n        agent_config[\'execution_spec\'][\'gpu_spec\'][\'gpus_enabled\'] = False\n\n    env_spec = {\n        ""type"": ""openai"",\n        ""gym_env"": FLAGS.env,\n        ""frameskip"": 4,  # When using a NoFrameskip-environment, this setting will enable frameskipping within RLgraph\n        ""max_num_noops"": 30,\n        ""episodic_life"": False,\n        ""fire_reset"": True\n    }\n\n    executor = ApexExecutor(\n        environment_spec=env_spec,\n        agent_config=agent_config,\n    )\n\n    print(""Starting workload, this will take some time for the agents to build."")\n    results = executor.execute_workload(workload=dict(num_timesteps=2000000, report_interval=50000,\n                                                      report_interval_min_seconds=30))\n\n    # Now we will save a CSV with the rewards timeseries for all workers and environments.\n    print(""Fetching worker learning results"")\n    agent_config_path = os.path.join(os.getcwd(), FLAGS.output)\n    # First, we fetch all worker results as a large dict.\n    all_results = executor.get_all_worker_results()\n    with open(agent_config_path, \'wt\') as fp:\n        csvwriter = csv.writer(fp)\n        csvwriter.writerow([""worker_num"", ""env_num"", ""total_times"", ""sample_times"", ""steps"", ""rewards""])\n        # Then we iterate over all workers.\n        for worker_num, result_dict in enumerate(all_results):\n            # Workers step through multiple environments, so now we loop through those for each worker.\n            for env_num, (step_list, reward_list, total_times_list, sample_times_list) in enumerate(zip(\n                    result_dict[\'episode_timesteps\'],\n                    result_dict[\'episode_rewards\'],\n                    result_dict[\'episode_total_times\'],\n                    result_dict[\'episode_sample_times\'])):\n\n                # Lastly, we loop through the environment reward timeseries and save the results in our CSV.\n                for steps, rewards, total_times, sample_times in zip(step_list, reward_list,\n                                                                     total_times_list, sample_times_list):\n                    row = [worker_num, env_num, total_times, sample_times, steps, rewards]\n                    csvwriter.writerow(row)\n\n\nif __name__ == \'__main__\':\n    main(sys.argv)\n'"
examples/distributed_ppo_pendulum.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nExample script for training a Proximal policy optimization agent on an OpenAI gym environment.\n\nUsage:\n\npython distributed_ppo_pendulum.py [--config configs/ppo_cartpole.json] [--env Pendulum-v0]\n\n```\n# Run script\npython distributed_ppo_pendulum.py\n```\n""""""\n\nimport json\nimport os\nimport sys\n\nfrom absl import flags\nfrom rlgraph.execution.ray import SyncBatchExecutor\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'config\', \'./configs/distributed_ppo_pendulum.json\', \'Agent config file.\')\nflags.DEFINE_string(\'env\', \'Pendulum-v0\', \'gym environment ID.\')\n\n\ndef main(argv):\n    try:\n        FLAGS(argv)\n    except flags.Error as e:\n        print(\'%s\\\\nUsage: %s ARGS\\\\n%s\' % (e, sys.argv[0], FLAGS))\n\n    agent_config_path = os.path.join(os.getcwd(), FLAGS.config)\n    with open(agent_config_path, \'rt\') as fp:\n        agent_config = json.load(fp)\n\n    env_spec = {\n        ""type"": ""openai"",\n        ""gym_env"": FLAGS.env\n    }\n\n    # Distributed synchronous optimisation on ray.\n    executor = SyncBatchExecutor(\n        environment_spec=env_spec,\n        agent_config=agent_config,\n    )\n    results = executor.execute_workload(workload=dict(num_timesteps=500000, report_interval=50000,\n                                                      report_interval_min_seconds=30))\n    print(results)\n\n\nif __name__ == \'__main__\':\n    main(sys.argv)\n'"
examples/dqn_cartpole_with_tf_summaries.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nExample script for training a DQN agent on an OpenAI gym environment.\n\nUsage:\n\npython dqn_cartpole_with_tf_summaries.py [--config configs/dqn_cartpole.json] [--env CartPole-v0]\n\n```\n# Run script\npython dqn_cartpole_with_tf_summaries.py\n```\n""""""\n\nimport sys\n\nimport numpy as np\nfrom absl import flags\n\nfrom rlgraph.agents import Agent\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.execution import SingleThreadedWorker\nfrom rlgraph.utils.config_util import read_config_file\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'config\', \'./configs/dqn_cartpole.json\', \'Agent config file.\')\nflags.DEFINE_string(\'env\', \'CartPole-v0\', \'gym environment ID.\')\nflags.DEFINE_boolean(\'render\', False, \'Render the environment.\')\nflags.DEFINE_string(\'summary_regexp\', \'neural-network\', \'RegExp for component scopes to be included in tf-summaries.\')\n\n\ndef main(argv):\n    try:\n        FLAGS(argv)\n    except flags.Error as e:\n        print(\'%s\\\\nUsage: %s ARGS\\\\n%s\' % (e, sys.argv[0], FLAGS))\n\n    agent_config = read_config_file(FLAGS.config)\n\n    env = OpenAIGymEnv.from_spec({\n        ""type"": ""openai"",\n        ""gym_env"": FLAGS.env\n    })\n\n    agent = Agent.from_spec(\n        agent_config,\n        summary_spec=dict(\n            summary_regexp=FLAGS.summary_regexp\n        ),\n        state_space=env.state_space,\n        action_space=env.action_space\n    )\n\n    episode_returns = []\n\n    def episode_finished_callback(episode_return, duration, timesteps, **kwargs):\n        episode_returns.append(episode_return)\n        if len(episode_returns) % 10 == 0:\n            print(""Episode {} finished: reward={:.2f}, average reward={:.2f}."".format(\n                len(episode_returns), episode_return, np.mean(episode_returns[-10:])\n            ))\n\n    worker = SingleThreadedWorker(\n        env_spec=lambda: env, agent=agent, render=FLAGS.render, worker_executes_preprocessing=False,\n        episode_finish_callback=episode_finished_callback\n    )\n    print(""Starting workload, this will take some time for the agents to build."")\n    results = worker.execute_episodes(200, use_exploration=True)\n\n    print(""Mean reward: {:.2f} / over the last 10 episodes: {:.2f}"".format(\n        np.mean(episode_returns), np.mean(episode_returns[-10:])\n    ))\n\n\nif __name__ == \'__main__\':\n    main(sys.argv)\n'"
examples/impala_cartpole.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nExample script for training a single-node IMPALA [1] agent on an OpenAI gym environment.\nA single-node agent uses multi-threading (via tf\'s queue runners) to collect experiences (using\nthe ""mu""-policy) and a learner (main) thread to update the model (the ""pi""-policy).\n\nTODO: More examples for non-single node setup.\n\nUsage:\n\npython impala_cartpole.py [--config configs/impala_cartpole.json] [--env CartPole-v0]?\n\n[1] IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures - Espeholt, Soyer,\n    Munos et al. - 2018 (https://arxiv.org/abs/1802.01561)\n""""""\n\nimport json\nimport os\nimport sys\n\nfrom absl import flags\nimport numpy as np\nimport time\n\nfrom rlgraph.agents import Agent\nfrom rlgraph.environments import OpenAIGymEnv\n\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'config\', \'./configs/impala_cartpole.json\', \'Agent config file.\')\nflags.DEFINE_string(\'env\', None, \'openAI gym environment ID.\')\nflags.DEFINE_bool(\'visualize\', False, \'Show worker episodes.\')\n\n\ndef main(argv):\n    try:\n        FLAGS(argv)\n    except flags.Error as e:\n        print(\'%s\\\\nUsage: %s ARGS\\\\n%s\' % (e, sys.argv[0], FLAGS))\n\n    agent_config_path = os.path.join(os.getcwd(), FLAGS.config)\n    with open(agent_config_path, \'rt\') as fp:\n        agent_config = json.load(fp)\n\n    if FLAGS.env is None:\n        env_spec = agent_config[""environment_spec""]\n    else:\n        env_spec = dict(gym_env=FLAGS.env)\n    if FLAGS.visualize is not None:\n        env_spec[""visualize""] = FLAGS.visualize\n    dummy_env = OpenAIGymEnv.from_spec(env_spec)\n    agent = Agent.from_spec(\n        agent_config,\n        state_space=dummy_env.state_space,\n        action_space=dummy_env.action_space\n    )\n    dummy_env.terminate()\n\n    learn_updates = 100\n    mean_returns = []\n    for i in range(learn_updates):\n        ret = agent.update()\n        mean_return = _calc_mean_return(ret)\n        mean_returns.append(mean_return)\n        print(""Iteration={} Loss={:.4f} Avg-reward={:.2f}"".format(i, float(ret[1]), mean_return))\n\n    print(""Mean return: {:.2f} / over the last 10 episodes: {:.2f}"".format(\n        np.nanmean(mean_returns), np.nanmean(mean_returns[-10:])\n    ))\n\n    time.sleep(1)\n    agent.terminate()\n    time.sleep(1)\n\n\ndef _calc_mean_return(records):\n    size = records[3][""rewards""].size\n    rewards = records[3][""rewards""].reshape((size,))\n    terminals = records[3][""terminals""].reshape((size,))\n    returns = list()\n    return_ = 0.0\n    for r, t in zip(rewards, terminals):\n        return_ += r\n        if t:\n            returns.append(return_)\n            return_ = 0.0\n\n    return np.nanmean(returns)\n\n\nif __name__ == \'__main__\':\n    main(sys.argv)\n'"
examples/impala_distributed_dmlab.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nExample script for training a distributed IMPALA [1] agent on an OpenAI gym environment.\nA single-node agent uses multi-threading (via tf\'s queue runners) to collect experiences (using\nthe ""mu""-policy) and a learner (main) thread to update the model (the ""pi""-policy).\n\nUsage:\n\npython impala_distributed_dmlab.py [--config configs/impala_cartpole.json] [--env CartPole-v0]\n\n```\n# Run script\npython impala_cartpole.py\n```\n\n[1] IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures - Espeholt, Soyer,\n    Munos et al. - 2018 (https://arxiv.org/abs/1802.01561)\n""""""\n\nimport json\nimport os\nimport sys\n\nfrom absl import flags\nimport numpy as np\nimport time\n\nfrom rlgraph.agents import IMPALAAgent\nfrom rlgraph.environments import Environment\n\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'config\', \'./configs/impala_distributed_dmlab.json\', \'Agent config file.\')\nflags.DEFINE_string(\'level\', \'seekavoid_arena_01\', \'Deepmind lab level name.\')\n\nflags.DEFINE_string(\'cluster_spec\', \'./configs/impala_distributed_clusterspec.json\', \'Cluster spec file.\')\n\nflags.DEFINE_boolean(\'learner\', False, \'Start learner.\')\nflags.DEFINE_boolean(\'actor\', False, \'Start actor.\')\nflags.DEFINE_integer(\'task\', 0, \'Task index.\')\n\nflags.DEFINE_integer(\'updates\', 50, \'Number of updates.\')\n\n\ndef main(argv):\n    try:\n        FLAGS(argv)\n    except flags.Error as e:\n        print(\'%s\\\\nUsage: %s ARGS\\\\n%s\' % (e, sys.argv[0], FLAGS))\n\n    agent_config_path = os.path.join(os.getcwd(), FLAGS.config)\n    with open(agent_config_path, \'rt\') as fp:\n        agent_config = json.load(fp)\n\n    cluster_spec_config_path = os.path.join(os.getcwd(), FLAGS.cluster_spec)\n    with open(cluster_spec_config_path, \'rt\') as fp:\n        cluster_spec = json.load(fp)\n\n    # Environment options\n    env_spec = {\n        ""type"": ""deepmind-lab"",\n        ""level_id"": FLAGS.level,\n        ""frameskip"": 4,\n        ""observations"": [""RGB_INTERLEAVED"", ""INSTR""]\n    }\n\n    # Verbose usage errors\n    if FLAGS.actor and FLAGS.learner:\n        print(""Please only use either --actor or --learner, not both."")\n        sys.exit(1)\n\n    # We dynamically update the distributed spec according to the job and task index\n    if FLAGS.actor:\n        agent_type = \'actor\'\n        distributed_spec = dict(\n            job=\'actor\',\n            task_index=FLAGS.task,\n            cluster_spec=cluster_spec\n        )\n        # Actors should only act on CPUs\n        agent_config[\'execution_spec\'][\'gpu_spec\'].update({\n            ""gpus_enabled"": False,\n            ""max_usable_gpus"": 0,\n            ""num_gpus"": 0\n        })\n    elif FLAGS.learner:\n        agent_type = \'learner\'\n        distributed_spec = dict(\n            job=\'learner\',\n            task_index=FLAGS.task,\n            cluster_spec=cluster_spec\n        )\n    else:\n        print(""Please pass either --learner or --actor (or look at the CartPole example for single trainig mode)."")\n        sys.exit(1)\n\n    # Set the sample size for the workers\n    worker_sample_size = 100\n\n    # Since we dynamically update the cluster spec according to the job and task index, we need to\n    # manually update the execution spec as well.\n    execution_spec = agent_config[\'execution_spec\']\n    execution_spec.update(dict(\n        mode=""distributed"",\n        distributed_spec=distributed_spec\n    ))\n\n    # Now, create the environment\n    env = Environment.from_spec(env_spec)\n\n    agent_spec = dict(\n        type=agent_type,\n        architecture=""large"",\n        environment_spec=env_spec,\n        worker_sample_size=worker_sample_size,\n        state_space=env.state_space,\n        action_space=env.action_space,\n        # TODO: automate this (by lookup from NN).\n        internal_states_space=IMPALAAgent.default_internal_states_space,\n        execution_spec=execution_spec,\n        # update_spec=dict(batch_size=2),\n        # Summarize time-steps to have an overview of the env-stepping speed.\n        summary_spec=dict(summary_regexp=""time-step"", directory=""/tmp/impala_{}_{}/"".format(agent_type, FLAGS.task))\n    )\n\n    agent_config.update(agent_spec)\n\n    agent = IMPALAAgent(\n        **agent_config\n    )\n\n    if FLAGS.learner:\n        print(""Starting learner for {} updates."".format(FLAGS.updates))\n        for _ in range(FLAGS.updates):\n            start_time = time.perf_counter()\n            results = agent.update()\n    else:\n        # Actor just acts\n        print(""Starting actor. Terminate with SIGINT (Ctrl+C)."")\n        while True:\n            agent.call_api_method(""perform_n_steps_and_insert_into_fifo"")  #.monitored_session.run([agent.enqueue_op])\n\n    learn_updates = 100\n    mean_returns = []\n    for i in range(learn_updates):\n        ret = agent.update()\n        mean_return = _calc_mean_return(ret)\n        mean_returns.append(mean_return)\n        print(""Iteration={} Loss={:.4f} Avg-reward={:.2f}"".format(i, float(ret[1]), mean_return))\n\n    print(""Mean return: {:.2f} / over the last 10 episodes: {:.2f}"".format(\n        np.nanmean(mean_returns), np.nanmean(mean_returns[-10:])\n    ))\n\n    time.sleep(1)\n    agent.terminate()\n    time.sleep(1)\n\n\ndef _calc_mean_return(records):\n    size = records[3][""rewards""].size\n    rewards = records[3][""rewards""].reshape((size,))\n    terminals = records[3][""terminals""].reshape((size,))\n    returns = list()\n    return_ = 0.0\n    for r, t in zip(rewards, terminals):\n        return_ += r\n        if t:\n            returns.append(return_)\n            return_ = 0.0\n\n    return np.nanmean(returns)\n\n\nif __name__ == \'__main__\':\n    main(sys.argv)\n'"
examples/impala_openai_gym_with_lstm.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nExample script for training a single-node IMPALA [1] agent on an OpenAI gym environment.\nA single-node agent uses multi-threading (via tf\'s queue runners) to collect experiences (using\nthe ""mu""-policy) and a learner (main) thread to update the model (the ""pi""-policy).\n\nUsage:\n\npython impala_openai_gym_with_lstm.py [--config configs/impala_openai_gym_with_lstm.json] [--env LunarLander-v2]?\n\n[1] IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures - Espeholt, Soyer,\n    Munos et al. - 2018 (https://arxiv.org/abs/1802.01561)\n""""""\n\nimport json\nimport os\nimport sys\n\nfrom absl import flags\nimport numpy as np\nimport time\n\nfrom rlgraph.agents import Agent\nfrom rlgraph.environments import OpenAIGymEnv\n\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'config\', \'./configs/impala_openai_gym_with_lstm.json\', \'Agent config file.\')\nflags.DEFINE_string(\'env\', None, \'openAI gym environment ID.\')\nflags.DEFINE_integer(\'visualize\', -1, \'Show training for n worker(s).\')\n\n\ndef main(argv):\n    try:\n        FLAGS(argv)\n    except flags.Error as e:\n        print(\'%s\\\\nUsage: %s ARGS\\\\n%s\' % (e, sys.argv[0], FLAGS))\n\n    agent_config_path = os.path.join(os.getcwd(), FLAGS.config)\n    with open(agent_config_path, \'rt\') as fp:\n        agent_config = json.load(fp)\n\n    # Override openAI gym env per command line.\n    if FLAGS.env is None:\n        env_spec = agent_config[""environment_spec""]\n    else:\n        env_spec = dict(type=""openai-gym"", gym_env=FLAGS.env)\n    # Override number of visualized envs per command line.\n    if FLAGS.visualize != -1:\n        env_spec[""visualize""] = FLAGS.visualize\n\n    dummy_env = OpenAIGymEnv.from_spec(env_spec)\n    agent = Agent.from_spec(\n        agent_config,\n        state_space=dummy_env.state_space,\n        action_space=dummy_env.action_space\n    )\n    dummy_env.terminate()\n\n    learn_updates = 6000\n    mean_returns = []\n    for i in range(learn_updates):\n        ret = agent.update()\n        mean_return = _calc_mean_return(ret)\n        mean_returns.append(mean_return)\n        print(""Iteration={} Loss={:.4f} Avg-reward={:.2f}"".format(i, float(ret[1]), mean_return))\n\n    print(""Mean return: {:.2f} / over the last 10 episodes: {:.2f}"".format(\n        np.nanmean(mean_returns), np.nanmean(mean_returns[-10:])\n    ))\n\n    time.sleep(1)\n    agent.terminate()\n    time.sleep(3)\n\n\ndef _calc_mean_return(records):\n    size = records[3][""rewards""].size\n    rewards = records[3][""rewards""].reshape((size,))\n    terminals = records[3][""terminals""].reshape((size,))\n    returns = list()\n    return_ = 0.0\n    for r, t in zip(rewards, terminals):\n        return_ += r\n        if t:\n            returns.append(return_)\n            return_ = 0.0\n\n    return np.nanmean(returns)\n\n\nif __name__ == \'__main__\':\n    main(sys.argv)\n'"
examples/ppo_cartpole.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nExample script for training a Proximal policy optimization agent on an OpenAI gym environment.\n\nUsage:\n\npython ppo_cartpole.py [--config configs/ppo_cartpole.json] [--env CartPole-v0]\n\n```\n# Run script\npython ppo_cartpole.py\n```\n""""""\n\nimport json\nimport os\nimport sys\nimport time\n\nimport numpy as np\nfrom absl import flags\n\nfrom rlgraph.agents import Agent\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.execution import SingleThreadedWorker\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'config\', \'./configs/ppo_cartpole.json\', \'Agent config file.\')\nflags.DEFINE_string(\'env\', \'CartPole-v0\', \'gym environment ID.\')\n\n\ndef main(argv):\n    try:\n        FLAGS(argv)\n    except flags.Error as e:\n        print(\'%s\\\\nUsage: %s ARGS\\\\n%s\' % (e, sys.argv[0], FLAGS))\n\n    agent_config_path = os.path.join(os.getcwd(), FLAGS.config)\n    with open(agent_config_path, \'rt\') as fp:\n        agent_config = json.load(fp)\n\n    if ""summary_spec"" in agent_config and ""directory"" not in agent_config[""summary_spec""]:\n        agent_config[""summary_spec""][""directory""] = os.path.expanduser(""~/rlgraph_summaries/"" + str(int(time.time())))\n\n    env = OpenAIGymEnv.from_spec({\n        ""type"": ""openai"",\n        ""gym_env"": FLAGS.env\n    })\n\n    agent = Agent.from_spec(\n        agent_config,\n        state_space=env.state_space,\n        action_space=env.action_space\n    )\n    episode_returns = []\n\n    def episode_finished_callback(episode_return, duration, timesteps, *args, **kwargs):\n        episode_returns.append(episode_return)\n        if len(episode_returns) % 100 == 0:\n            print(""Episode {} finished: reward={:.2f}, average reward={:.2f}."".format(\n                len(episode_returns), episode_return, np.mean(episode_returns[-100:])\n            ))\n\n    worker = SingleThreadedWorker(env_spec=lambda: env, agent=agent, render=False, worker_executes_preprocessing=False,\n                                  episode_finish_callback=episode_finished_callback)\n    print(""Starting workload, this will take some time for the agents to build."")\n\n    # Use exploration is true for training, false for evaluation.\n    worker.execute_timesteps(100000, use_exploration=True)\n\n    print(""Mean reward: {:.2f} / over the last 10 episodes: {:.2f}"".format(\n        np.mean(episode_returns), np.mean(episode_returns[-10:])\n    ))\n\n\nif __name__ == \'__main__\':\n    main(sys.argv)\n'"
examples/ppo_or_sac_on_mlagents.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nExample script for training a Proximal policy optimization agent on an ML-Agents Env running locally in Unity3D.\n\nUsage:\n\npython ppo_or_sac_on_mlagents.py [--config ./configs/ppo_mlagents_[3dball_hard|banana_collector].json]\n\n""""""\n\nimport json\nimport os\nimport sys\n\nimport numpy as np\nfrom absl import flags\nfrom rlgraph.agents import Agent\nfrom rlgraph.environments import MLAgentsEnv\nfrom rlgraph.execution import SingleThreadedWorker\n\nFLAGS = flags.FLAGS\n\n# Use different configs here for either PPO or SAC algos.\n# - ./configs/ppo_mlagents_banana_collector.json learns the BananaCollector Env and reaches the benchmark (10)\n#   after about 1500 episodes using PPO with container actions.\n# - ./configs/ppo_mlagents_3dball_hard.json learns the 3DBall (hard version) Env using PPO.\n# - ./configs/sac_mlagents_3dball_hard.json learns the 3DBall (hard version) Env using SAC.\nflags.DEFINE_string(\'config\', \'./configs/ppo_mlagents_banana_collector.json\', \'Agent config file.\')\n\n\ndef main(argv):\n    try:\n        FLAGS(argv)\n    except flags.Error as e:\n        print(\'%s\\\\nUsage: %s ARGS\\\\n%s\' % (e, sys.argv[0], FLAGS))\n\n    agent_config_path = os.path.join(os.getcwd(), FLAGS.config)\n    with open(agent_config_path, \'rt\') as fp:\n        agent_config = json.load(fp)\n\n    env = MLAgentsEnv()\n\n    agent = Agent.from_spec(\n        agent_config,\n        state_space=env.state_space,\n        action_space=env.action_space\n    )\n    episode_returns = []\n\n    def episode_finished_callback(episode_return, duration, timesteps, **kwargs):\n        episode_returns.append(episode_return)\n        finished_episodes = len(episode_returns)\n        if finished_episodes % 4 == 0:\n            print(\n                ""Episode {} finished in {:d}sec: total avg. reward={:.2f}; last 10 episodes={:.2f}; last ""\n                ""100 episodes={:.2f}"".format(\n                    finished_episodes, int(duration), np.mean(episode_returns),\n                    np.mean(episode_returns[-min(finished_episodes, 10):]),\n                    np.mean(episode_returns[-min(finished_episodes, 100):])\n                )\n            )\n\n    worker = SingleThreadedWorker(\n        env_spec=env, agent=agent, render=False, worker_executes_preprocessing=False,\n        episode_finish_callback=episode_finished_callback\n    )\n    print(""Starting workload, this will take some time for the agents to build."")\n\n    # Use exploration is true for training, false for evaluation.\n    worker.execute_timesteps(500000, use_exploration=True)\n\n    print(""Mean reward: {:.2f} / over the last 10 episodes: {:.2f}"".format(\n        np.mean(episode_returns), np.mean(episode_returns[-10:])\n    ))\n\n\nif __name__ == \'__main__\':\n    main(sys.argv)\n'"
examples/sac_pendulum.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nExample script for training a Proximal policy optimization agent on an OpenAI gym environment.\n\nUsage:\n\npython ppo_cartpole.py [--config configs/ppo_cartpole.json] [--env CartPole-v0]\n\n```\n# Run script\npython ppo_cartpole.py\n```\n""""""\n\nimport json\nimport os\nimport sys\n\nimport numpy as np\nfrom absl import flags\n\nfrom rlgraph.agents import Agent\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.execution import SingleThreadedWorker\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'config\', \'./configs/sac_pendulum.json\', \'Agent config file.\')\nflags.DEFINE_string(\'env\', \'Pendulum-v0\', \'gym environment ID.\')\nflags.DEFINE_boolean(\'render\', False, \'Render the environment.\')\n\n\ndef main(argv):\n    try:\n        FLAGS(argv)\n    except flags.Error as e:\n        print(\'%s\\\\nUsage: %s ARGS\\\\n%s\' % (e, sys.argv[0], FLAGS))\n\n    agent_config_path = os.path.join(os.getcwd(), FLAGS.config)\n    with open(agent_config_path, \'rt\') as fp:\n        agent_config = json.load(fp)\n\n    env = OpenAIGymEnv.from_spec({\n        ""type"": ""openai"",\n        ""gym_env"": FLAGS.env\n    })\n\n    agent = Agent.from_spec(\n        agent_config,\n        state_space=env.state_space,\n        action_space=env.action_space\n    )\n    episode_returns = []\n\n    def episode_finished_callback(episode_return, duration, timesteps, **kwargs):\n        episode_returns.append(episode_return)\n        if len(episode_returns) % 10 == 0:\n            print(""Episode {} finished: reward={:.2f}, average reward={:.2f}."".format(\n                len(episode_returns), episode_return, np.mean(episode_returns[-10:])\n            ))\n\n    worker = SingleThreadedWorker(\n        env_spec=lambda: env, agent=agent, render=FLAGS.render, worker_executes_preprocessing=False,\n        episode_finish_callback=episode_finished_callback\n    )\n    print(""Starting workload, this will take some time for the agents to build."")\n\n    # Use exploration is true for training, false for evaluation.\n    worker.execute_timesteps(10000, use_exploration=True)\n\n    print(""Mean reward: {:.2f} / over the last 10 episodes: {:.2f}"".format(\n        np.mean(episode_returns), np.mean(episode_returns[-10:])\n    ))\n\n\nif __name__ == \'__main__\':\n    main(sys.argv)\n'"
examples/train_agent_openai.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nExample script for training agent on an OpenAI gym environment.\n\nUsage:\n\npython train_agent_openai.py [--config configs/sac_pendulum.json] [--env Pendulum-v0]\n""""""\n\nimport json\nimport os\nimport sys\n\nimport numpy as np\nfrom absl import flags\n\nfrom rlgraph.agents import Agent\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.execution import SingleThreadedWorker\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'config\', \'./configs/sac_pendulum.json\', \'Agent config file.\')\nflags.DEFINE_string(\'env\', \'Pendulum-v0\', \'gym environment ID.\')\nflags.DEFINE_boolean(\'render\', False, \'Render the environment.\')\n\n\ndef main(argv):\n    try:\n        FLAGS(argv)\n    except flags.Error as e:\n        print(\'%s\\\\nUsage: %s ARGS\\\\n%s\' % (e, sys.argv[0], FLAGS))\n\n    agent_config_path = os.path.join(os.getcwd(), FLAGS.config)\n    with open(agent_config_path, \'rt\') as fp:\n        agent_config = json.load(fp)\n\n    env = OpenAIGymEnv.from_spec({\n        ""type"": ""openai"",\n        ""gym_env"": FLAGS.env\n    })\n    print(env.state_space)\n\n    agent = Agent.from_spec(\n        agent_config,\n        state_space=env.state_space,\n        action_space=env.action_space\n    )\n\n    episode_returns = []\n\n    def episode_finished_callback(episode_return, duration, timesteps, **kwargs):\n        episode_returns.append(episode_return)\n        if len(episode_returns) % 5 == 0:\n            print(""Episode {} finished: reward={:.2f}, average reward={:.2f}."".format(\n                len(episode_returns), episode_return, np.mean(episode_returns[-5:])\n            ))\n\n    worker = SingleThreadedWorker(env_spec=lambda: env, agent=agent, render=FLAGS.render, worker_executes_preprocessing=False,\n                                  episode_finish_callback=episode_finished_callback)\n    print(""Starting workload, this will take some time for the agents to build."")\n\n    worker.execute_episodes(100, use_exploration=True)\n\n    # Use exploration is true for training, false for evaluation.\n\n    #worker.execute_episodes(100, use_exploration=False)\n\n    print(""Mean reward: {:.2f} / over the last 10 episodes: {:.2f}"".format(\n        np.mean(episode_returns), np.mean(episode_returns[-10:])\n    ))\n\n\nif __name__ == \'__main__\':\n    main(sys.argv)\n'"
rlgraph/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport logging\nimport os\n\nfrom rlgraph.version import __version__\n\n# Libraries should add NullHandler() by default, as its the application code\'s\n# responsibility to configure log handlers.\n# https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library\ntry:\n    from logging import NullHandler\nexcept ImportError:\n    class NullHandler(logging.Handler):\n        def emit(self, record):\n            pass\n\nlogging.getLogger(__name__).addHandler(NullHandler())\n\nif ""RLGRAPH_HOME"" in os.environ:\n    rlgraph_dir = os.environ.get(""RLGRAPH_HOME"")\nelse:\n    rlgraph_dir = os.path.expanduser(\'~\')\n    rlgraph_dir = os.path.join(rlgraph_dir, "".rlgraph"")\n\n\n# RLgraph config (contents of .rlgraph file).\nCONFIG = dict()\n\n# TODO ""tensorflow"" for tensorflow?\n# Default backend (\'tf\' for tensorflow or \'pytorch\' for PyTorch)\nBACKEND = ""tf""\n\n# Default distributed backend is distributed ray.\nDISTRIBUTED_BACKEND = ""distributed_tf""\n\ndistributed_compatible_backends = dict(\n    tf=[""distributed_tf"", ""ray"", ""horovod""],\n    pytorch=[""ray"", ""horovod""]\n)\n\n\nconfig_file = os.path.expanduser(os.path.join(rlgraph_dir, \'rlgraph.json\'))\nif os.path.exists(config_file):\n    try:\n        with open(config_file) as f:\n            CONFIG = json.load(f)\n    except ValueError:\n        pass\n\n    # Read from config or leave defaults.\n    backend = CONFIG.get(""BACKEND"", None)\n    if backend is not None:\n        BACKEND = backend\n    distributed_backend = CONFIG.get(""DISTRIBUTED_BACKEND"", None)\n    if distributed_backend is not None:\n        DISTRIBUTED_BACKEND = distributed_backend\n\n# Create dir if necessary:\nif not os.path.exists(rlgraph_dir):\n    try:\n        os.makedirs(rlgraph_dir)\n    except OSError:\n        pass\n\n\n# Write to file if there was none:\nif not os.path.exists(config_file):\n    CONFIG = {\n        ""BACKEND"": BACKEND,\n        ""DISTRIBUTED_BACKEND"": DISTRIBUTED_BACKEND,\n        ""GRAPHVIZ_RENDER_BUILD_ERRORS"": True\n    }\n    try:\n        with open(config_file, \'w\') as f:\n            f.write(json.dumps(CONFIG, indent=4))\n    except IOError:\n        # Except permission denied.\n        pass\n\n# Overwrite backend if set in ENV.\nif \'RLGRAPH_BACKEND\' in os.environ:\n    backend = os.environ.get(\'RLGRAPH_BACKEND\', None)\n    if backend is not None:\n        logging.info(""Setting BACKEND to \'{}\' per environment variable \'RLGRAPH_BACKEND\'."".format(backend))\n        BACKEND = backend\n\n# Overwrite distributed-backend if set in ENV.\nif \'RLGRAPH_DISTRIBUTED_BACKEND\' in os.environ:\n    distributed_backend = os.environ.get(\'RLGRAPH_DISTRIBUTED_BACKEND\', None)\n    if distributed_backend is not None:\n        logging.info(\n            ""Setting DISTRIBUTED_BACKEND to \'{}\' per environment variable ""\n            ""\'RLGRAPH_DISTRIBUTED_BACKEND\'."".format(distributed_backend)\n        )\n        DISTRIBUTED_BACKEND = distributed_backend\n\n\n# Test compatible backend.\nif DISTRIBUTED_BACKEND not in distributed_compatible_backends[BACKEND]:\n    raise ValueError(""Distributed backend {} not compatible with backend {}. Compatible backends""\n                     ""are: {}"".format(DISTRIBUTED_BACKEND, BACKEND, distributed_compatible_backends[BACKEND]))\n\n\n# Test imports.\nif DISTRIBUTED_BACKEND == \'distributed_tf\':\n    assert BACKEND == ""tf""\n    try:\n        import tensorflow\n    except ImportError as e:\n        raise ImportError(\n            ""INIT ERROR: Cannot run distributed_tf without backend (tensorflow)! Please install tensorflow first ""\n            ""via `pip install tensorflow` or `pip install tensorflow-gpu`.""\n        )\nelif DISTRIBUTED_BACKEND == ""horovod"":\n    try:\n        import horovod\n    except ImportError as e:\n        raise ValueError(""INIT ERROR: Cannot run RLGraph with distributed backend Horovod."")\nelif DISTRIBUTED_BACKEND == ""ray"":\n    try:\n        import ray\n    except ImportError as e:\n        raise ValueError(""INIT ERROR: Cannot run RLGraph with distributed backend Ray."")\nelse:\n    raise ValueError(""Distributed backend {} not supported"".format(DISTRIBUTED_BACKEND))\n\n\ndef get_backend():\n    return BACKEND\n\n\ndef get_distributed_backend():\n    return DISTRIBUTED_BACKEND\n\n\ndef get_config():\n    return CONFIG\n\n\n# Import useful packages, such that ""import rlgraph as rl"" will enable one to do e.g. ""agent = rl.agents.PPOAgent""\nimport rlgraph.agents\nimport rlgraph.components\nimport rlgraph.environments\nimport rlgraph.spaces\nimport rlgraph.utils\n\n\n__all__ = [\n    ""__version__"",  ""get_backend"", ""get_distributed_backend"", ""rlgraph_dir"",\n    ""get_config""\n]\n'"
rlgraph/version.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n__version__ = ""0.5.5""\n'"
rlgraph/agents/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.agents.agent import Agent\nfrom rlgraph.agents.dqn_agent import DQNAgent\nfrom rlgraph.agents.dqfd_agent import DQFDAgent\nfrom rlgraph.agents.apex_agent import ApexAgent\nfrom rlgraph.agents.impala_agents import IMPALAAgent, SingleIMPALAAgent\nfrom rlgraph.agents.ppo_agent import PPOAgent\nfrom rlgraph.agents.actor_critic_agent import ActorCriticAgent\nfrom rlgraph.agents.random_agent import RandomAgent\nfrom rlgraph.agents.sac_agent import SACAgent\n\n\nAgent.__lookup_classes__ = dict(\n    apex=ApexAgent,\n    apexagent=ApexAgent,\n    actorcritic=ActorCriticAgent,\n    dqn=DQNAgent,\n    dqnagent=DQNAgent,\n    dqfd=DQFDAgent,\n    dqfdagent=DQFDAgent,\n    impala=IMPALAAgent,  # TODO: Split non-single agents into Actor and Learner\n    singleimpala=SingleIMPALAAgent,\n    singleimpalaagent=SingleIMPALAAgent,\n    ppo=PPOAgent,\n    ppoagent=PPOAgent,\n    random=RandomAgent,\n    randomagent=RandomAgent,\n    sac=SACAgent,\n    sacagent=SACAgent\n)\n\n__all__ = [""Agent""] + \\\n          list(set(map(lambda x: x.__name__, Agent.__lookup_classes__.values())))\n\n'"
rlgraph/agents/actor_critic_agent.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\n\nfrom rlgraph.agents import Agent\nfrom rlgraph.components import Memory, ContainerMerger, ContainerSplitter, RingBuffer\nfrom rlgraph.components.helpers import GeneralizedAdvantageEstimation\nfrom rlgraph.components.loss_functions.actor_critic_loss_function import ActorCriticLossFunction\nfrom rlgraph.spaces import FloatBox, BoolBox\nfrom rlgraph.utils import util\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.util import strip_list\n\n\nclass ActorCriticAgent(Agent):\n    """"""\n    Basic actor-critic policy gradient architecture with generalized advantage estimation,\n    and entropy regularization. Suitable for execution with A2C, A3C.\n    """"""\n\n    def __init__(\n        self,\n        state_space,\n        action_space,\n        discount=0.98,\n        preprocessing_spec=None,\n        network_spec=None,\n        internal_states_space=None,\n        policy_spec=None,\n        value_function_spec=None,\n        execution_spec=None,\n        optimizer_spec=None,\n        value_function_optimizer_spec=None,\n        observe_spec=None,\n        update_spec=None,\n        summary_spec=None,\n        saver_spec=None,\n        auto_build=True,\n        name=""actor-critic-agent"",\n        gae_lambda=1.0,\n        clip_rewards=0.0,\n        sample_episodes=False,\n        weight_pg=None,\n        weight_vf=None,\n        weight_entropy=None,\n        memory_spec=None\n     ):\n        """"""\n        Args:\n            state_space (Union[dict,Space]): Spec dict for the state Space or a direct Space object.\n            action_space (Union[dict,Space]): Spec dict for the action Space or a direct Space object.\n            preprocessing_spec (Optional[list,PreprocessorStack]): The spec list for the different necessary states\n                preprocessing steps or a PreprocessorStack object itself.\n            discount (float): The discount factor (gamma).\n            network_spec (Optional[list,NeuralNetwork]): Spec list for a NeuralNetwork Component or the NeuralNetwork\n                object itself.\n            internal_states_space (Optional[Union[dict,Space]]): Spec dict for the internal-states Space or a direct\n                Space object for the Space(s) of the internal (RNN) states.\n            policy_spec (Optional[dict]): An optional dict for further kwargs passing into the Policy c\'tor.\n            value_function_spec (list, dict, ValueFunction): Neural network specification for baseline or instance\n                of ValueFunction.\n            execution_spec (Optional[dict,Execution]): The spec-dict specifying execution settings.\n            optimizer_spec (Optional[dict,Optimizer]): The spec-dict to create the Optimizer for this Agent.\n            value_function_optimizer_spec (dict): Optimizer config for value function optimizer. If None, the optimizer\n                spec for the policy is used (same learning rate and optimizer type).\n            observe_spec (Optional[dict]): Spec-dict to specify `Agent.observe()` settings.\n            update_spec (Optional[dict]): Spec-dict to specify `Agent.update()` settings.\n            summary_spec (Optional[dict]): Spec-dict to specify summary settings.\n            saver_spec (Optional[dict]): Spec-dict to specify saver settings.\n            auto_build (Optional[bool]): If True (default), immediately builds the graph using the agent\'s\n                graph builder. If false, users must separately call agent.build(). Useful for debugging or analyzing\n                components before building.\n            name (str): Some name for this Agent object.\n            gae_lambda (float): Lambda for generalized advantage estimation.\n            clip_rewards (float): Reward clip value. If not 0, rewards will be clipped into this range.\n            sample_episodes (bool): If true, the update method interprets the batch_size as the number of\n                episodes to fetch from the memory. If false, batch_size will refer to the number of time-steps. This\n                is especially relevant for environments where episode lengths may vastly differ throughout training. For\n                example, in CartPole, a losing episode is typically 10 steps, and a winning episode 200 steps.\n            weight_pg (float): The coefficient used for the policy gradient loss term (L[PG]).\n            weight_vf (float): The coefficient used for the state value function loss term (L[V]).\n            weight_entropy (float): The coefficient used for the entropy regularization term (L[E]).\n            memory_spec (Optional[dict,Memory]): The spec for the Memory to use. Should typically be\n            a ring-buffer.\n        """"""\n        # Set policy to stochastic.\n        if policy_spec is not None:\n            policy_spec[""deterministic""] = False\n        else:\n            policy_spec = dict(deterministic=False)\n        super(ActorCriticAgent, self).__init__(\n            state_space=state_space,\n            action_space=action_space,\n            discount=discount,\n            preprocessing_spec=preprocessing_spec,\n            network_spec=network_spec,\n            internal_states_space=internal_states_space,\n            policy_spec=policy_spec,\n            value_function_spec=value_function_spec,\n            execution_spec=execution_spec,\n            optimizer_spec=optimizer_spec,\n            value_function_optimizer_spec=value_function_optimizer_spec,\n            observe_spec=observe_spec,\n            update_spec=update_spec,\n            summary_spec=summary_spec,\n            saver_spec=saver_spec,\n            name=name,\n            auto_build=auto_build\n        )\n        self.sample_episodes = sample_episodes\n\n        # Extend input Space definitions to this Agent\'s specific API-methods.\n        preprocessed_state_space = self.preprocessed_state_space.with_batch_rank()\n        reward_space = FloatBox(add_batch_rank=True)\n        terminal_space = BoolBox(add_batch_rank=True)\n\n        self.input_spaces.update(dict(\n            actions=self.action_space.with_batch_rank(),\n            policy_weights=""variables:{}"".format(self.policy.scope),\n            deterministic=bool,\n            preprocessed_states=preprocessed_state_space,\n            rewards=reward_space,\n            terminals=terminal_space,\n            sequence_indices=BoolBox(add_batch_rank=True)\n        ))\n\n        # The merger to merge inputs into one record Dict going into the memory.\n        self.merger = ContainerMerger(""states"", ""actions"", ""rewards"", ""terminals"")\n        self.memory = Memory.from_spec(memory_spec)\n        assert isinstance(self.memory, RingBuffer), \\\n            ""ERROR: Actor-critic memory must be ring-buffer for episode-handling.""\n        # The splitter for splitting up the records coming from the memory.\n        self.splitter = ContainerSplitter(""states"", ""actions"", ""rewards"", ""terminals"")\n\n        self.gae_function = GeneralizedAdvantageEstimation(gae_lambda=gae_lambda, discount=self.discount,\n                                                           clip_rewards=clip_rewards)\n        self.loss_function = ActorCriticLossFunction(\n            weight_pg=weight_pg, weight_vf=weight_vf, weight_entropy=weight_entropy\n        )\n\n        # Add all our sub-components to the core.\n        sub_components = [self.preprocessor, self.merger, self.memory, self.splitter, self.policy,\n                          self.loss_function, self.optimizer, self.value_function, self.value_function_optimizer,\n                          self.gae_function]\n        self.root_component.add_components(*sub_components)\n\n        # Define the Agent\'s (root-Component\'s) API.\n        self.define_graph_api()\n        self.build_options = dict(vf_optimizer=self.value_function_optimizer)\n\n        if self.auto_build:\n            self._build_graph([self.root_component], self.input_spaces, optimizer=self.optimizer,\n                              batch_size=self.update_spec[""batch_size""],\n                              build_options=self.build_options)\n\n            self.graph_built = True\n\n    def define_graph_api(self):\n        super(ActorCriticAgent, self).define_graph_api()\n\n        agent = self\n        sample_episodes = self.sample_episodes\n\n        # Reset operation (resets preprocessor).\n        if self.preprocessing_required:\n            @rlgraph_api(component=self.root_component)\n            def reset_preprocessor(root):\n                reset_op = agent.preprocessor.reset()\n                return reset_op\n\n        # Act from preprocessed states.\n        @rlgraph_api(component=self.root_component)\n        def action_from_preprocessed_state(root, preprocessed_states, deterministic=False):\n            out = agent.policy.get_action(preprocessed_states, deterministic=deterministic)\n            return out[""action""], preprocessed_states\n\n        # State (from environment) to action with preprocessing.\n        @rlgraph_api(component=self.root_component)\n        def get_preprocessed_state_and_action(root, states, deterministic=False):\n            preprocessed_states = agent.preprocessor.preprocess(states)\n            return root.action_from_preprocessed_state(preprocessed_states, deterministic)\n\n        # Insert into memory.\n        @rlgraph_api(component=self.root_component)\n        def insert_records(root, preprocessed_states, actions, rewards, terminals):\n            records = agent.merger.merge(preprocessed_states, actions, rewards, terminals)\n            return agent.memory.insert_records(records)\n\n        @rlgraph_api(component=self.root_component)\n        def post_process(root, preprocessed_states, rewards, terminals, sequence_indices):\n            baseline_values = agent.value_function.value_output(preprocessed_states)\n            pg_advantages = agent.gae_function.calc_gae_values(baseline_values, rewards, terminals, sequence_indices)\n            return pg_advantages\n\n        # Learn from memory.\n        @rlgraph_api(component=self.root_component)\n        def update_from_memory(root, time_percentage=None):\n            if sample_episodes:\n                records = agent.memory.get_episodes(agent.update_spec[""batch_size""])\n            else:\n                records = agent.memory.get_records(agent.update_spec[""batch_size""])\n            preprocessed_s, actions, rewards, terminals = agent.splitter.call(records)\n            sequence_indices = terminals\n            return root.post_process_and_update(\n                preprocessed_s, actions, rewards, terminals, sequence_indices, time_percentage\n            )\n\n        # First post-process, then update (so we can separately update already post-processed data).\n        @rlgraph_api(component=self.root_component)\n        def post_process_and_update(root, preprocessed_states, actions, rewards, terminals, sequence_indices,\n                                    time_percentage):\n            rewards = root.post_process(preprocessed_states, rewards, terminals, sequence_indices)\n            return root.update_from_external_batch(preprocessed_states, actions, rewards, terminals, time_percentage)\n\n        # Learn from an external batch.\n        @rlgraph_api(component=self.root_component)\n        def update_from_external_batch(root, preprocessed_states, actions, rewards, terminals, time_percentage):\n\n            baseline_values = agent.value_function.value_output(preprocessed_states)\n            log_probs = agent.policy.get_log_likelihood(preprocessed_states, actions)[""log_likelihood""]\n            entropy = agent.policy.get_entropy(preprocessed_states)[""entropy""]\n            loss, loss_per_item, vf_loss, vf_loss_per_item = agent.loss_function.loss(\n                log_probs, baseline_values, rewards, entropy, time_percentage\n            )\n\n            # Args are passed in again because some device strategies may want to split them to different devices.\n            policy_vars = agent.policy.variables()\n            vf_vars = agent.value_function.variables()\n\n            step_op = agent.optimizer.step(policy_vars, loss, loss_per_item, time_percentage)\n            vf_step_op = agent.value_function_optimizer.step(vf_vars, vf_loss, vf_loss_per_item, time_percentage)\n\n            return step_op, loss, loss_per_item, vf_step_op, vf_loss, vf_loss_per_item\n\n    def get_action(self, states, internals=None, use_exploration=True, apply_preprocessing=True, extra_returns=None,\n                   time_percentage=None):\n        """"""\n        Args:\n            extra_returns (Optional[Set[str],str]): Optional string or set of strings for additional return\n                values (besides the actions). Possible values are:\n                - \'preprocessed_states\': The preprocessed states after passing the given states through the\n                preprocessor stack.\n                - \'internal_states\': The internal states returned by the RNNs in the NN pipeline.\n                - \'used_exploration\': Whether epsilon- or noise-based exploration was used or not.\n\n        Returns:\n            tuple or single value depending on `extra_returns`:\n                - action\n                - the preprocessed states\n        """"""\n        extra_returns = {extra_returns} if isinstance(extra_returns, str) else (extra_returns or set())\n        # States come in without preprocessing -> use state space.\n        if apply_preprocessing:\n            call_method = ""get_preprocessed_state_and_action""\n            batched_states, remove_batch_rank = self.state_space.force_batch(states)\n        else:\n            call_method = ""action_from_preprocessed_state""\n            batched_states = states\n            remove_batch_rank = False\n        #remove_batch_rank = batched_states.ndim == np.asarray(states).ndim + 1\n\n        # Increase timesteps by the batch size (number of states in batch).\n        batch_size = len(batched_states)\n        self.timesteps += batch_size\n\n        # Control, which return value to ""pull"" (depending on `additional_returns`).\n        return_ops = [0, 1] if ""preprocessed_states"" in extra_returns else [0]  # 1=preprocessed_states, 0=action\n        ret = self.graph_executor.execute((\n            call_method,\n            [batched_states, not use_exploration],  # deterministic = not use_exploration\n            return_ops\n        ))\n        if remove_batch_rank:\n            return strip_list(ret)\n        else:\n            return ret\n\n    # TODO make next states optional in observe API.\n    def _observe_graph(self, preprocessed_states, actions, internals, rewards, next_states, terminals):\n        self.graph_executor.execute((""insert_records"", [preprocessed_states, actions, rewards, terminals]))\n\n    def update(self, batch=None, sequence_indices=None, apply_postprocessing=True):\n        """"""\n        Args:\n            batch (dict): Update batch.\n            sequence_indices (Optional[np.ndarray, list]): Sequence indices are used in multi-env batches where\n                partial episode fragments may be concatenated within the trajectory. For a single env, these are equal\n                to terminals. If None are given, terminals will be used as sequence indices. A sequence index is True\n                where an episode fragment ends and False otherwise. The reason separate indices are necessary is so that\n                e.g. in GAE discounting, correct boot-strapping is applied depending on whether a true terminal state\n                was reached, or a partial episode fragment of an environment ended.\n\n                Example: If env_1 has terminals [0 0 0] for an episode fragment and env_2 terminals = [0 0 1],\n                    we may pass them in as one combined array [0 0 0 0 0 1] with sequence indices showing where each\n                    episode ends: [0 0 1 0 0 1].\n            apply_postprocessing (Optional[(bool]): If True, apply post-processing such as generalised\n                advantage estimation to collected batch in-graph. If False, update assumed post-processing has already\n                been applied. The purpose of internal versus external post-processing is to be able to off-load\n                post-processing in large scale distributed scenarios.\n        """"""\n        # [0] step_op, [1] loss, [2] loss_per_item, [3] vf_step_op, [4]vf_loss, [5]vf_loss_per_item\n        return_ops = [0, 1, 2, 3, 4, 5]\n        if batch is None:\n            ret = self.graph_executor.execute((""update_from_memory"", None, return_ops))\n\n            # Remove unnecessary return dicts (e.g. sync-op).\n            if isinstance(ret, dict):\n                ret = ret[""update_from_memory""]\n        else:\n            # No sequence indices means terminals are used in place.\n            if sequence_indices is None:\n                sequence_indices = batch[""terminals""]\n\n            pps_dtype = self.preprocessed_state_space.dtype\n            batch[""states""] = np.asarray(batch[""states""], dtype=util.convert_dtype(dtype=pps_dtype, to=\'np\'))\n            batch_input = [batch[""states""], batch[""actions""], batch[""rewards""], batch[""terminals""], sequence_indices]\n\n            # Execute post-processing or already post-processed by workers?\n            if apply_postprocessing:\n                ret = self.graph_executor.execute((""post_process_and_update"", batch_input, return_ops))\n                # Remove unnecessary return dicts (e.g. sync-op).\n                if isinstance(ret, dict):\n                    ret = ret[""post_process_and_update""]\n            else:\n                ret = self.graph_executor.execute((""update_from_external_batch"", batch_input, return_ops))\n                # Remove unnecessary return dicts (e.g. sync-op).\n                if isinstance(ret, dict):\n                    ret = ret[""update_from_external_batch""]\n\n        # [1] loss, [2] loss per item\n        return ret[1], ret[2]\n\n    def reset(self):\n        """"""\n        Resets our preprocessor, but only if it contains stateful PreprocessLayer Components (meaning\n        the PreprocessorStack has at least one variable defined).\n        """"""\n        if self.preprocessing_required and len(self.preprocessor.variable_registry) > 0:\n            self.graph_executor.execute(""reset_preprocessor"")\n\n    def __repr__(self):\n        return ""ActorCriticAgent""\n'"
rlgraph/agents/agent.py,11,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom collections import defaultdict\nfrom functools import partial\nimport logging\n\nimport numpy as np\nfrom rlgraph import get_backend\nfrom rlgraph.components import Component, Exploration, PreprocessorStack, Synchronizable, Policy, Optimizer, \\\n    ContainerMerger, ContainerSplitter\nfrom rlgraph.graphs.graph_builder import GraphBuilder\nfrom rlgraph.graphs.graph_executor import GraphExecutor\nfrom rlgraph.spaces import Space, ContainerSpace\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.input_parsing import parse_execution_spec, parse_observe_spec, parse_update_spec, \\\n    parse_value_function_spec\nfrom rlgraph.utils.ops import flatten_op\nfrom rlgraph.utils.specifiable import Specifiable\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass Agent(Specifiable):\n    """"""\n    Generic agent defining RLGraph-API operations and parses and sanitizes configuration specs.\n    """"""\n\n    def __init__(self, state_space, action_space, discount=0.98,\n                 preprocessing_spec=None, network_spec=None, internal_states_space=None,\n                 policy_spec=None, value_function_spec=None,\n                 exploration_spec=None, execution_spec=None, optimizer_spec=None, value_function_optimizer_spec=None,\n                 observe_spec=None, update_spec=None,\n                 summary_spec=None, saver_spec=None, auto_build=True, name=""agent""):\n        """"""\n        Args:\n            state_space (Union[dict,Space]): Spec dict for the state Space or a direct Space object.\n            action_space (Union[dict,Space]): Spec dict for the action Space or a direct Space object.\n\n            preprocessing_spec (Optional[list,PreprocessorStack]): The spec list for the different necessary states\n                preprocessing steps or a PreprocessorStack object itself.\n\n            discount (float): The discount factor (gamma).\n\n            network_spec (Optional[list,NeuralNetwork]): Spec list for a NeuralNetwork Component or the NeuralNetwork\n                object itself.\n\n            internal_states_space (Optional[Union[dict,Space]]): Spec dict for the internal-states Space or a direct\n                Space object for the Space(s) of the internal (RNN) states.\n\n            policy_spec (Optional[dict]): An optional dict for further kwargs passing into the Policy c\'tor.\n            value_function_spec (list, dict, ValueFunction): Neural network specification for baseline or instance\n                of ValueFunction.\n\n            exploration_spec (Optional[dict]): The spec-dict to create the Exploration Component.\n            execution_spec (Optional[dict,Execution]): The spec-dict specifying execution settings.\n            optimizer_spec (Optional[dict,Optimizer]): The spec-dict to create the Optimizer for this Agent.\n\n            value_function_optimizer_spec (dict): Optimizer config for value function optimizer. If None, the optimizer\n                spec for the policy is used (same learning rate and optimizer type).\n\n            observe_spec (Optional[dict]): Spec-dict to specify `Agent.observe()` settings.\n            update_spec (Optional[dict]): Spec-dict to specify `Agent.update()` settings.\n            summary_spec (Optional[dict]): Spec-dict to specify summary settings.\n            saver_spec (Optional[dict]): Spec-dict to specify saver settings.\n\n            auto_build (Optional[bool]): If True (default), immediately builds the graph using the agent\'s\n                graph builder. If false, users must separately call agent.build(). Useful for debugging or analyzing\n                components before building.\n\n            name (str): Some name for this Agent object.\n        """"""\n        super(Agent, self).__init__()\n        self.name = name\n        self.auto_build = auto_build\n        self.graph_built = False\n        self.logger = logging.getLogger(__name__)\n\n        self.state_space = Space.from_spec(state_space).with_batch_rank(False)\n        self.flat_state_space = self.state_space.flatten(scope_separator_at_start=False)\\\n            if isinstance(self.state_space, ContainerSpace) else None\n        self.logger.info(""Parsed state space definition: {}"".format(self.state_space))\n        self.action_space = Space.from_spec(action_space).with_batch_rank(False)\n        self.flat_action_space = self.action_space.flatten() if isinstance(self.action_space, ContainerSpace) else None\n        self.logger.info(""Parsed action space definition: {}"".format(self.action_space))\n\n        self.discount = discount\n        self.build_options = {}\n\n        # The agent\'s root-Component.\n        self.root_component = Component(name=self.name, nesting_level=0)\n\n        # Define the input-Spaces:\n        # Tag the input-Space to `self.set_weights` as equal to whatever the variables-Space will be for\n        # the Agent\'s policy Component.\n        self.input_spaces = dict(\n            states=self.state_space.with_batch_rank(),\n            time_percentage=float,\n            increment=int,\n            episode_reward=float\n        )\n\n        # Construct the Preprocessor.\n        self.preprocessor = PreprocessorStack.from_spec(preprocessing_spec)\n        self.preprocessed_state_space = self.preprocessor.get_preprocessed_space(self.state_space)\n        self.preprocessing_required = preprocessing_spec is not None and len(preprocessing_spec) > 0\n        if self.preprocessing_required:\n            self.logger.info(""Preprocessing required."")\n            self.logger.info(""Parsed preprocessed-state space definition: {}"".format(self.preprocessed_state_space))\n        else:\n            self.logger.info(""No preprocessing required."")\n\n        # Construct the Policy network.\n        policy_spec = policy_spec or {}\n        if ""network_spec"" not in policy_spec:\n            policy_spec[""network_spec""] = network_spec\n        if ""action_space"" not in policy_spec:\n            policy_spec[""action_space""] = self.action_space\n        self.policy_spec = policy_spec\n        # The behavioral policy of the algorithm. Also the one that gets updated.\n        self.policy = Policy.from_spec(self.policy_spec)\n        # Done by default.\n        self.policy.add_components(Synchronizable(), expose_apis=""sync"")\n\n        # Create non-shared baseline network.\n        self.value_function = parse_value_function_spec(value_function_spec)\n        # TODO move this to specific agents.\n        if self.value_function is not None:\n            self.vars_merger = ContainerMerger(""policy"", ""vf"", scope=""variable-dict-merger"")\n            self.vars_splitter = ContainerSplitter(""policy"", ""vf"", scope=""variable-container-splitter"")\n        else:\n            self.vars_merger = ContainerMerger(""policy"", scope=""variable-dict-merger"")\n            self.vars_splitter = ContainerSplitter(""policy"", scope=""variable-container-splitter"")\n        self.internal_states_space = Space.from_spec(internal_states_space)\n\n        # An object implementing the loss function interface is only strictly needed\n        # if automatic device strategies like multi-gpu are enabled. This is because\n        # the device strategy needs to know the name of the loss function to infer the appropriate\n        # operations.\n        self.loss_function = None\n\n        self.exploration = Exploration.from_spec(exploration_spec)  # TODO: Move this to DQN/DQFN. PG\'s don\'t use it.\n        self.execution_spec = parse_execution_spec(execution_spec)\n\n        # Python-side experience buffer for better performance (may be disabled).\n        self.default_env = ""env_0""\n\n        def factory_(i):\n            if i == 0:\n                return []\n            return tuple([[] for _ in range(i)])\n\n        self.states_buffer = defaultdict(partial(factory_, len(self.flat_state_space or [])))\n        self.actions_buffer = defaultdict(partial(factory_, len(self.flat_action_space or [])))\n        self.internals_buffer = defaultdict(list)\n        self.rewards_buffer = defaultdict(list)\n        self.next_states_buffer = defaultdict(partial(factory_, len(self.flat_state_space or [])))\n        self.terminals_buffer = defaultdict(list)\n\n        self.observe_spec = parse_observe_spec(observe_spec)\n\n        # Global time step counter.\n        self.timesteps = 0\n\n        # Create the Agent\'s optimizer based on optimizer_spec and execution strategy.\n        self.optimizer = None\n        if optimizer_spec is not None:\n            # Save spec in case agent needs to create more optimizers e.g. for baseline.\n            self.optimizer_spec = optimizer_spec\n            self.optimizer = Optimizer.from_spec(optimizer_spec)\n\n        self.value_function_optimizer = None\n        if self.value_function is not None:\n            if value_function_optimizer_spec is None:\n                vf_optimizer_spec = self.optimizer_spec\n            else:\n                vf_optimizer_spec = value_function_optimizer_spec\n            vf_optimizer_spec[""scope""] = ""value-function-optimizer""\n            self.value_function_optimizer = Optimizer.from_spec(vf_optimizer_spec)\n\n        # Update-spec dict tells the Agent how to update (e.g. memory batch size).\n        self.update_spec = parse_update_spec(update_spec)\n\n        # Create our GraphBuilder and -Executor.\n        self.graph_builder = GraphBuilder(action_space=self.action_space, summary_spec=summary_spec)\n        self.graph_executor = GraphExecutor.from_spec(\n            get_backend(),\n            graph_builder=self.graph_builder,\n            execution_spec=self.execution_spec,\n            saver_spec=saver_spec\n        )  # type: GraphExecutor\n\n    def reset_env_buffers(self, env_id=None):\n        """"""\n        Resets an environment buffer for buffered `observe` calls.\n\n        Args:\n            env_id (Optional[str]): Environment id to reset. Defaults to a default environment if None provided.\n        """"""\n        if env_id is None:\n            env_id = self.default_env\n        del self.states_buffer[env_id]  # = ([] for _ in range(len(self.flat_state_space)))\n        del self.actions_buffer[env_id]  # = ([] for _ in range(len(self.flat_action_space)))\n        del self.internals_buffer[env_id]  # = []\n        del self.rewards_buffer[env_id]  # = []\n        del self.next_states_buffer[env_id]  # = ([] for _ in range(len(self.flat_state_space)))\n        del self.terminals_buffer[env_id]  # = []\n\n    def define_graph_api(self, *args, **kwargs):\n        """"""\n        Can be used to specify and then `self.define_api_method` the Agent\'s CoreComponent\'s API methods.\n        Each agent implements this to build its algorithm logic.\n        """"""\n        agent = self\n\n        if self.value_function is not None:\n            # This avoids variable-incompleteness for the value-function component in a multi-GPU setup, where the root\n            # value-function never performs any forward pass (only used as variable storage).\n            @rlgraph_api(component=self.root_component)\n            def get_state_values(root, preprocessed_states):\n                vf = root.get_sub_component_by_name(agent.value_function.scope)\n                return vf.value_output(preprocessed_states)\n\n        # Variable that stores episode rewards.\n        self.root_component.episode_reward = None\n\n        # Assign `create_variables` method to root.\n        def _create_variables(self, input_spaces, action_space):\n            # Skip this for multi-GPU towers.\n            if not hasattr(self, ""is_multi_gpu_tower"") or self.is_multi_gpu_tower is False:\n                agent.root_component.episode_reward = agent.root_component.get_variable(\n                    ""episode-reward"", shape=(), initializer=0.0\n                )\n        # Bind `_create_variables` method (as ""create_variables"") to this Component object.\n        setattr(self.root_component, ""create_variables"", _create_variables.__get__(self.root_component, None))\n\n        # Add API methods for syncing.\n        @rlgraph_api(component=self.root_component)\n        def get_weights(root):\n            policy = root.get_sub_component_by_name(agent.policy.scope)\n            policy_weights = policy.variables()\n            value_function_weights = None\n            if agent.value_function is not None:\n                value_func = root.get_sub_component_by_name(agent.value_function.scope)\n                value_function_weights = value_func.variables()\n            return dict(policy_weights=policy_weights, value_function_weights=value_function_weights)\n\n        @rlgraph_api(component=self.root_component, must_be_complete=False)\n        def set_weights(root, policy_weights, value_function_weights=None):\n            policy = root.get_sub_component_by_name(agent.policy.scope)\n            policy_sync_op = policy.sync(policy_weights)\n            if value_function_weights is not None:\n                assert agent.value_function is not None\n                vf = root.get_sub_component_by_name(agent.value_function.scope)\n                vf_sync_op = vf.sync(value_function_weights)\n                return root._graph_fn_group(policy_sync_op, vf_sync_op)\n            else:\n                return policy_sync_op\n\n        # TODO: Replace this with future on-the-fly-API-components.\n        @graph_fn(component=self.root_component)\n        def _graph_fn_group(root, *ops):\n            if get_backend() == ""tf"":\n                return tf.group(*ops)\n            return ops[0]\n\n        # To pre-process external data if needed.\n        @rlgraph_api(component=self.root_component)\n        def preprocess_states(root, states):\n            preprocessor_stack = root.get_sub_component_by_name(agent.preprocessor.scope)\n            return preprocessor_stack.preprocess(states)\n\n        @graph_fn(component=self.root_component)\n        def _graph_fn_training_step(root, other_step_op=None):\n            """"""\n            Increases the global training timestep by 1. Should be called by all training API-methods to\n            timestamp each training/update step.\n\n            Args:\n                other_step_op (Optional[DataOp]): Another DataOp (e.g. a step_op) which should be\n                    executed before the increase takes place.\n\n            Returns:\n                DataOp: no_op() or identity(other_step_op) in tf, None in pytorch.\n            """"""\n            if get_backend() == ""tf"":\n                add_op = tf.assign_add(self.graph_executor.global_training_timestep, 1)\n                op_list = [add_op] + [other_step_op] if other_step_op is not None else []\n                with tf.control_dependencies(op_list):\n                    if other_step_op is None or hasattr(other_step_op, ""type"") and other_step_op.type == ""NoOp"":\n                        return tf.no_op()\n                    else:\n                        return tf.identity(other_step_op)\n            elif get_backend == ""pytorch"":\n                self.graph_executor.global_training_timestep += 1\n                return None\n\n        @rlgraph_api(component=self.root_component)\n        def get_global_timestep(root):\n            return root.read_variable(self.graph_executor.global_timestep)\n\n        @rlgraph_api(component=self.root_component)\n        def _graph_fn_update_global_timestep(root, increment):\n            if get_backend() == ""tf"":\n                add_op = tf.assign_add(self.graph_executor.global_timestep, increment)\n                return add_op\n            elif get_backend() == ""pytorch"":\n                # If we are in build phase, don\'t do anything here.\n                if self.graph_executor.global_timestep is not None:\n                    self.graph_executor.global_timestep += increment\n                return None\n\n        @rlgraph_api(component=self.root_component)\n        def _graph_fn_get_episode_reward(root):\n            return root.episode_reward\n\n        @rlgraph_api(component=self.root_component)\n        def _graph_fn_set_episode_reward(root, episode_reward):\n            if get_backend() == ""tf"":\n                return tf.assign(root.episode_reward, episode_reward)\n            elif get_backend() == ""pytorch"":\n                root.episode_reward = episode_reward\n                return None\n\n    def _build_graph(self, root_components, input_spaces, **kwargs):\n        """"""\n        Builds the internal graph from the RLGraph meta-graph via the graph executor..\n        """"""\n        return self.graph_executor.build(root_components, input_spaces, **kwargs)\n\n    def build(self, build_options=None):\n        """"""\n        Builds this agent. This method call only be called if the agent parameter ""auto_build""\n        was set to False.\n\n        Args:\n            build_options (Optional[dict]): Optional build options, see build doc.\n        """"""\n        if build_options is not None:\n            self.build_options.update(build_options)\n        assert not self.graph_built, \\\n            ""ERROR: Attempting to build agent which has already been built. Ensure auto_build parameter is set to "" \\\n            ""False (was {}), and method has not been called twice"".format(self.auto_build)\n\n        # TODO let agent have a list of root-components\n        return self._build_graph(\n            [self.root_component], self.input_spaces, optimizer=self.optimizer,\n            build_options=self.build_options, batch_size=self.update_spec[""batch_size""]\n        )\n\n    def preprocess_states(self, states):\n        """"""\n        Applies the agent\'s preprocessor to one or more states, e.g. to preprocess external data\n        before inserting to memory without acting. Returns identity if no preprocessor defined.\n\n        Args:\n            states (np.array): State(s) to preprocess.\n\n        Returns:\n            np.array: Preprocessed states.\n        """"""\n        if self.preprocessing_required:\n            return self.call_api_method(""preprocess_states"", states)\n        else:\n            # Return identity.\n            return states\n\n    def get_action(self, states, internals=None, use_exploration=True, apply_preprocessing=True, extra_returns=None,\n                   time_percentage=None):\n        """"""\n        Returns action(s) for the passed state(s). If `states` is a single state, returns a single action, otherwise,\n        returns a batch of actions, where batch-size = number of states passed in.\n\n        Args:\n            states (Union[dict,np.ndarray]): States dict/tuple or numpy array.\n            internals (Union[dict,np.ndarray]): Internal states dict/tuple or numpy array.\n\n            use_exploration (bool): If False, no exploration or sampling may be applied\n                when retrieving an action.\n\n            apply_preprocessing (bool): If True, apply any state preprocessors configured to the action. Set to\n                false if all pre-processing is handled externally both for acting and updating.\n\n            extra_returns (Optional[Set[str]]): Optional set of Agent-specific strings for additional return\n                values (besides the actions). All Agents must support ""preprocessed_states"".\n\n        Returns:\n            any: Action(s) as dict/tuple/np.ndarray (depending on `self.action_space`).\n                Optional: The preprocessed states as a 2nd return value.\n        """"""\n        raise NotImplementedError\n\n    def observe(self, preprocessed_states, actions, internals, rewards, next_states, terminals, env_id=None,\n                batched=False):\n        """"""\n        Observes an experience tuple or a batch of experience tuples. Note: If configured,\n        first uses buffers and then internally calls _observe_graph() to actually run the computation graph.\n        If buffering is disabled, this just routes the call to the respective `_observe_graph()` method of the\n        child Agent.\n\n        Args:\n            preprocessed_states (Union[dict,ndarray]): Preprocessed states dict or array.\n            actions (Union[dict,ndarray]): Actions dict or array containing actions performed for the given state(s).\n\n            internals (Optional[list]): Internal state(s) returned by agent for the given states.Must be\n                empty list if no internals available.\n\n            rewards (Union[float,List[float]]): Scalar reward(s) observed.\n            terminals (Union[bool,List[bool]]): Boolean indicating terminal.\n            next_states (Union[dict,ndarray]): Preprocessed next states dict or array.\n\n            env_id (Optional[str]): Environment id to observe for. When using vectorized execution and\n                buffering, using environment ids is necessary to ensure correct trajectories are inserted.\n                See `SingleThreadedWorker` for example usage.\n\n            batched (bool): Whether given data (states, actions, etc..) is already batched or not.\n        """"""\n        # Check for illegal internals.\n        if internals is None:\n            internals = []\n\n        if self.observe_spec[""buffer_enabled""] is True:\n            if env_id is None:\n                env_id = self.default_env\n\n            # If data is already batched, just have to extend our buffer lists.\n            if batched:\n                if self.flat_state_space is not None:\n                    for i, flat_key in enumerate(self.flat_state_space.keys()):\n                        self.states_buffer[env_id][i].extend(preprocessed_states[flat_key])\n                        self.next_states_buffer[env_id][i].extend(next_states[flat_key])\n                else:\n                    self.states_buffer[env_id].extend(preprocessed_states)\n                    self.next_states_buffer[env_id].extend(next_states)\n                if self.flat_action_space is not None:\n                    flat_action = flatten_op(actions)\n                    for i, flat_key in enumerate(self.flat_action_space.keys()):\n                        self.actions_buffer[env_id][i].append(flat_action[flat_key])\n                else:\n                    self.actions_buffer[env_id].extend(actions)\n                self.internals_buffer[env_id].extend(internals)\n                self.rewards_buffer[env_id].extend(rewards)\n                self.terminals_buffer[env_id].extend(terminals)\n            # Data is not batched, append single items (without creating new lists first!) to buffer lists.\n            else:\n                if self.flat_state_space is not None:\n                    for i, flat_key in enumerate(self.flat_state_space.keys()):\n                        self.states_buffer[env_id][i].append(preprocessed_states[flat_key])\n                        self.next_states_buffer[env_id][i].append(next_states[flat_key])\n                else:\n                    self.states_buffer[env_id].append(preprocessed_states)\n                    self.next_states_buffer[env_id].append(next_states)\n                if self.flat_action_space is not None:\n                    flat_action = flatten_op(actions)\n                    for i, flat_key in enumerate(self.flat_action_space.keys()):\n                        self.actions_buffer[env_id][i].append(flat_action[flat_key])\n                else:\n                    self.actions_buffer[env_id].append(actions)\n                self.internals_buffer[env_id].append(internals)\n                self.rewards_buffer[env_id].append(rewards)\n                self.terminals_buffer[env_id].append(terminals)\n\n            buffer_is_full = len(self.rewards_buffer[env_id]) >= self.observe_spec[""buffer_size""]\n\n            # If the buffer (per environment) is full OR the episode was aborted:\n            # Change terminal of last record artificially to True (also give warning ""buffer too small""),\n            # insert and flush the buffer.\n            if buffer_is_full or self.terminals_buffer[env_id][-1]:\n                # Warn if full and last terminal is False.\n                if buffer_is_full and not self.terminals_buffer[env_id][-1]:\n                    self.logger.warning(\n                        ""Buffer of size {} of Agent \'{}\' may be too small! Had to add artificial terminal=True ""\n                        ""to end."".format(self.observe_spec[""buffer_size""], self)\n                    )\n                    self.terminals_buffer[env_id][-1] = True\n\n                # TODO: Apply n-step post-processing if necessary.\n                if self.flat_state_space is not None:\n                    states_ = {}\n                    next_states_ = {}\n                    for i, key in enumerate(self.flat_state_space.keys()):\n                        states_[key] = np.asarray(self.states_buffer[env_id][i])\n                        next_states_[key] = np.asarray(self.next_states_buffer[env_id][i])\n                        # Squeeze, but do not squeeze (1,) to ().\n                        if len(states_[key]) > 1:\n                            states_[key] = np.squeeze(states_[key])\n                            next_states_[key] = np.squeeze(next_states_[key])\n                        #else:\n                        #    states_[key] = np.reshape(states_[key], (1,))\n                        #    next_states_[key] = np.reshape(next_states_[key], (1,))\n                else:\n                    states_ = np.asarray(self.states_buffer[env_id])\n                    next_states_ = np.asarray(self.next_states_buffer[env_id])\n\n                if self.flat_action_space is not None:\n                    actions_ = {}\n                    for i, key in enumerate(self.flat_action_space.keys()):\n                        actions_[key] = np.asarray(self.actions_buffer[env_id][i])\n                        # Squeeze, but do not squeeze (1,) to ().\n                        if len(actions_[key]) > 1:\n                            actions_[key] = np.squeeze(actions_[key])\n                        else:\n                            actions_[key] = np.reshape(actions_[key], (1,))\n                else:\n                    actions_ = np.asarray(self.actions_buffer[env_id])\n\n                self._observe_graph(\n                    preprocessed_states=states_,\n                    actions=actions_,\n                    internals=np.asarray(self.internals_buffer[env_id]),\n                    rewards=np.asarray(self.rewards_buffer[env_id]),\n                    next_states=next_states_,\n                    terminals=np.asarray(self.terminals_buffer[env_id])\n                )\n                self.reset_env_buffers(env_id)\n        else:\n            if not batched:\n                preprocessed_states, _ = self.preprocessed_state_space.force_batch(preprocessed_states)\n                next_states, _ = self.preprocessed_state_space.force_batch(next_states)\n                actions, _ = self.action_space.force_batch(actions)\n                rewards = [rewards]\n                terminals = [terminals]\n\n            self._observe_graph(preprocessed_states, actions, internals, rewards, next_states, terminals)\n\n    def _observe_graph(self, preprocessed_states, actions, internals, rewards, next_states, terminals):\n        """"""\n        This methods defines the actual call to the computational graph by executing\n        the respective graph op via the graph executor. Since this may use varied underlying\n        components and api_methods, every agent defines which ops it may want to call. The buffered observer\n        calls this method to move data into the graph.\n\n        Args:\n            preprocessed_states (Union[dict,ndarray]): Preprocessed states dict or array.\n            actions (Union[dict,ndarray]): Actions dict or array containing actions performed for the given state(s).\n            internals (Union[list]): Internal state(s) returned by agent for the given states. Must be an empty list\n                if no internals available.\n            rewards (Union[ndarray,list,float]): Scalar reward(s) observed.\n            next_states (Union[dict, ndarray]): Preprocessed next states dict or array.\n            terminals (Union[list,bool]): Boolean indicating terminal.\n        """"""\n        raise NotImplementedError\n\n    # def _write_rewards_summary(self, rewards, terminals, env_id):\n    #     """"""\n    #     Writes summary for the observed rewards.\n    #\n    #     Args:\n    #         rewards (float): The observed rewards.\n    #         terminals (bool): The observed episode terminal states.\n    #         env_id (str): The id of the environment.\n    #     """"""\n    #     # TODO: This is a hack to avoid failure in RandomAgent.\n    #     if ""get_episode_reward"" in self.graph_builder.api and ""get_global_timestep"" in self.graph_builder.api:\n    #         if get_backend() == ""tf"":\n    #             # TODO: 1. handle env_id\n    #             # TODO: 2. control the level of verbosity\n    #             # TODO: 3. can we reduce the graph interactions?\n    #             ret = self.graph_executor.execute(\n    #                 ""get_episode_reward"", ""get_global_timestep""\n    #             )\n    #             episode_reward = ret[""get_episode_reward""]\n    #             timestep = ret[""get_global_timestep""]\n    #             for i in range(len(rewards)):\n    #                 summary = tf.Summary(value=[\n    #                     tf.Summary.Value(tag=""reward/raw_reward"", simple_value=rewards[i])])\n    #                 self.graph_executor.summary_writer.add_summary(summary, timestep)\n    #                 episode_reward += rewards[i]\n    #                 if terminals[i]:\n    #                     summary = tf.Summary(value=[\n    #                         tf.Summary.Value(tag=""reward/episode_reward"", simple_value=episode_reward)])\n    #                     self.graph_executor.summary_writer.add_summary(summary, timestep)\n    #                     episode_reward = 0\n    #             timestep += 1\n    #             self.graph_executor.execute(\n    #                 (""set_episode_reward"", [episode_reward]),\n    #                 (""update_global_timestep"", [len(rewards)])\n    #             )\n\n    def update(self, batch=None, time_percentage=None, **kwargs):\n        """"""\n        Performs an update on the computation graph either via externally experience or\n        by sampling from an internal memory.\n\n        Args:\n            batch (Optional[dict]): Optional external data batch to use for update. If None, the\n                agent should be configured to sample internally.\n\n            time_percentage (Optional[float]): A percentage value (between 0.0 and 1.0) of the time already passed until\n                some max timesteps have been reached. This can be used by the algorithm to decay certain parameters\n                (e.g. learning rate) over time.\n\n        Returns:\n            Union(list, tuple, float): The loss value calculated in this update.\n        """"""\n        raise NotImplementedError\n\n    def import_observations(self, observations):\n        """"""\n        Bulk imports observations, potentially using device pre-fetching. Can be optionally\n        implemented by agents requiring pre-training.\n\n        Args:\n            observations (dict): Dict or list of observation data.\n        """"""\n        pass\n\n    def reset(self):\n        """"""\n        Must be implemented to define some reset behavior (before starting a new episode).\n        This could include resetting the preprocessor and other Components.\n        """"""\n        pass  # optional\n\n    def terminate(self):\n        """"""\n        Terminates the Agent, so it will no longer be usable.\n        Things that need to be cleaned up should be placed into this function, e.g. closing sessions\n        and other open connections.\n        """"""\n        self.graph_executor.terminate()\n\n    def call_api_method(self, op, inputs=None, return_ops=None):\n        """"""\n        Utility method to call any desired api method on the graph, identified via output socket.\n        Delegate this call to the RLGraph graph executor.\n\n        Args:\n            op (str): Name of the api method.\n\n            inputs (Optional[dict,np.array]): Dict specifying the provided api_methods for (key=input space name,\n                values=the values that should go into this space (e.g. numpy arrays)).\n        Returns:\n            any: Result of the op call.\n        """"""\n        return self.graph_executor.execute((op, inputs, return_ops))\n\n    def export_graph(self, filename=None):\n        """"""\n        Any algorithm defined as a full-graph, as opposed to mixed (mixed Python and graph control flow)\n        should be able to export its graph for deployment.\n\n        Args:\n            filename (str): Export path. Depending on the backend, different filetypes may be required.\n        """"""\n        self.graph_executor.export_graph_definition(filename)\n\n    def store_model(self, path=None, add_timestep=True):\n        """"""\n        Store model using the backend\'s check-pointing mechanism.\n\n        Args:\n            path (str): Path to model directory.\n\n            add_timestep (bool): Indicates if current training step should be appended to exported model.\n                If false, may override previous checkpoints.\n        """"""\n        self.graph_executor.store_model(path=path, add_timestep=add_timestep)\n\n    def load_model(self, checkpoint_directory=None, checkpoint_path=None):\n        """"""\n        Loads model from specified path location using the following semantics:\n\n        If checkpoint directory and checkpoint path are given, attempts to find `checkpoint_path` as relative path from\n        `checkpoint_directory`.\n\n        If a checkpoint directory is given but no path (e.g. because timestep of checkpoint is not known in advance),\n        attempts to fetch latest check-point.\n\n        If no directory is given, attempts to fetch checkpoint from the full absolute path `checkpoint_path\'.\n\n        Args:\n            checkpoint_directory (str): Optional path to directory containing checkpoint(s).\n            checkpoint_path (str): Path to specific model checkpoint.\n        """"""\n        self.graph_executor.load_model(checkpoint_directory=checkpoint_directory, checkpoint_path=checkpoint_path)\n\n    def get_weights(self):\n        """"""\n        Returns all weights relevant for the agent\'s policy for syncing purposes.\n\n        Returns:\n            any: Weights and optionally weight meta data for this model.\n        """"""\n        return self.graph_executor.execute(""get_weights"")\n\n    def set_weights(self, policy_weights, value_function_weights=None):\n        """"""\n        Sets policy weights of this agent, e.g. for external syncing purposes.\n\n        Args:\n            policy_weights (any): Weights and optionally meta data to update depending on the backend.\n            value_function_weights (Optional[any]): Optional value function weights.\n\n        Raises:\n            ValueError if weights do not match graph weights in shapes and types.\n        """"""\n        # TODO generic *args here and specific names in specific agents?\n        if value_function_weights is not None:\n            return self.graph_executor.execute((""set_weights"", [policy_weights, value_function_weights]))\n        else:\n            return self.graph_executor.execute((""set_weights"", policy_weights))\n\n    def post_process(self, batch):\n        """"""\n        Optional method to post-processes a batch if post-processing is off-loaded to workers instead of\n        executed by a central learner before computing the loss.\n\n        The post-processing function must be able to post-process batches of multiple environments\n        and episodes with non-terminated fragments via sequence-indices.\n\n        This enables efficient processing of multi-environment batches.\n\n        Args:\n            batch (dict): Batch to process. Must contain key \'sequence-indices\' to describe where\n                environment fragments end (even if the corresponding episode has not terminated.\n\n        Returns:\n            any: Post-processed batch.\n        """"""\n        pass\n\n    def __repr__(self):\n        """"""\n        Returns:\n            str: A short, but informative description for this Agent.\n        """"""\n        raise NotImplementedError\n'"
rlgraph/agents/apex_agent.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph.agents import DQNAgent\nfrom rlgraph.utils import util\n\n\nclass ApexAgent(DQNAgent):\n    """"""\n    Ape-X is a DQN variant designed for large scale distributed execution where many workers\n    share a distributed prioritized experience replay.\n\n    Paper: https://arxiv.org/abs/1803.00933\n\n    The distinction to standard DQN is mainly that Ape-X needs to provide additional operations\n    to enable external updates of priorities. Ape-X also enables per default dueling and double\n    DQN.\n    """"""\n    def __init__(\n        self,\n        state_space,\n        action_space,\n        discount=0.98,\n        preprocessing_spec=None,\n        network_spec=None,\n        internal_states_space=None,\n        policy_spec=None,\n        exploration_spec=None,\n        execution_spec=None,\n        optimizer_spec=None,\n        observe_spec=None,\n        update_spec=None,\n        summary_spec=None,\n        saver_spec=None,\n        auto_build=True,\n        name=""apex-agent"",\n        double_q=True,\n        dueling_q=True,\n        huber_loss=True,\n        n_step=1,\n        shared_container_action_target=True,\n        memory_spec=None\n    ):\n        """"""\n        Args:\n            state_space (Union[dict,Space]): Spec dict for the state Space or a direct Space object.\n            action_space (Union[dict,Space]): Spec dict for the action Space or a direct Space object.\n            preprocessing_spec (Optional[list,PreprocessorStack]): The spec list for the different necessary states\n                preprocessing steps or a PreprocessorStack object itself.\n            discount (float): The discount factor (gamma).\n            network_spec (Optional[list,NeuralNetwork]): Spec list for a NeuralNetwork Component or the NeuralNetwork\n                object itself.\n            internal_states_space (Optional[Union[dict,Space]]): Spec dict for the internal-states Space or a direct\n                Space object for the Space(s) of the internal (RNN) states.\n            policy_spec (Optional[dict]): An optional dict for further kwargs passing into the Policy c\'tor.\n            exploration_spec (Optional[dict]): The spec-dict to create the Exploration Component.\n            execution_spec (Optional[dict,Execution]): The spec-dict specifying execution settings.\n            optimizer_spec (Optional[dict,Optimizer]): The spec-dict to create the Optimizer for this Agent.\n            observe_spec (Optional[dict]): Spec-dict to specify `Agent.observe()` settings.\n            update_spec (Optional[dict]): Spec-dict to specify `Agent.update()` settings.\n            summary_spec (Optional[dict]): Spec-dict to specify summary settings.\n            saver_spec (Optional[dict]): Spec-dict to specify saver settings.\n            auto_build (Optional[bool]): If True (default), immediately builds the graph using the agent\'s\n                graph builder. If false, users must separately call agent.build(). Useful for debugging or analyzing\n                components before building.\n            name (str): Some name for this Agent object.\n            double_q (bool): Whether to use the double DQN loss function (see [2]).\n            dueling_q (bool): Whether to use a dueling layer in the ActionAdapter  (see [3]).\n            huber_loss (bool) : Whether to apply a Huber loss. (see [4]).\n            n_step (Optional[int]): n-step adjustment to discounting.\n            memory_spec (Optional[dict,Memory]): The spec for the Memory to use for the DQN algorithm.\n        """"""\n        assert memory_spec[""type""] == ""prioritized_replay"" or memory_spec[""type""] == ""mem_prioritized_replay""\n        super(ApexAgent, self).__init__(\n            state_space=state_space,\n            action_space=action_space,\n            discount=discount,\n            preprocessing_spec=preprocessing_spec,\n            network_spec=network_spec,\n            internal_states_space=internal_states_space,\n            policy_spec=policy_spec,\n            exploration_spec=exploration_spec,\n            execution_spec=execution_spec,\n            optimizer_spec=optimizer_spec,\n            observe_spec=observe_spec,\n            update_spec=update_spec,\n            summary_spec=summary_spec,\n            saver_spec=saver_spec,\n            auto_build=auto_build,\n            name=name,\n            double_q=double_q,\n            dueling_q=dueling_q,\n            huber_loss=huber_loss,\n            n_step=n_step,\n            shared_container_action_target=shared_container_action_target,\n            memory_spec=memory_spec\n        )\n        self.num_updates = 0\n\n    def update(self, batch=None, time_percentage=None, **kwargs):\n        # In apex, syncing is based on num steps trained, not steps sampled.\n        sync_call = None\n        # Apex uses train time steps for syncing.\n        self.steps_since_target_net_sync += len(batch[""terminals""])\n        if self.steps_since_target_net_sync >= self.update_spec[""sync_interval""]:\n            sync_call = ""sync_target_qnet""\n            self.steps_since_target_net_sync = 0\n        return_ops = [0, 1]\n        self.num_updates += 1\n        if batch is None:\n            # Add some additional return-ops to pull (left out normally for performance reasons).\n            ret = self.graph_executor.execute((""update_from_memory"", None, return_ops), sync_call)\n\n            # Remove unnecessary return dicts (e.g. sync-op).\n            if isinstance(ret, dict):\n                ret = ret[""update_from_memory""]\n\n            return ret[1]\n        else:\n            # Add some additional return-ops to pull (left out normally for performance reasons).\n            pps_dtype = self.preprocessed_state_space.dtype\n            batch_input = [np.asarray(batch[""states""], dtype=util.convert_dtype(dtype=pps_dtype, to=\'np\')),\n                           batch[""actions""],\n                           batch[""rewards""], batch[""terminals""],\n                           np.asarray(batch[""next_states""], dtype=util.convert_dtype(dtype=pps_dtype, to=\'np\')),\n                           batch[""importance_weights""],\n                           True]\n            ret = self.graph_executor.execute((""update_from_external_batch"", batch_input), sync_call)\n            # Remove unnecessary return dicts (e.g. sync-op).\n            if isinstance(ret, dict):\n                ret = ret[""update_from_external_batch""]\n\n            # Return [1]=total loss, [2]=loss-per-item (skip [0]=update noop).\n            return ret[1], ret[2]\n\n    def __repr__(self):\n        return ""ApexAgent""\n'"
rlgraph/agents/dqfd_agent.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph.agents import Agent\nfrom rlgraph.components import Memory, PrioritizedReplay, ContainerMerger, ContainerSplitter, DQFDLossFunction\nfrom rlgraph.spaces import FloatBox, BoolBox\nfrom rlgraph.utils import RLGraphError\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.util import strip_list\n\n\nclass DQFDAgent(Agent):\n    """"""\n    Deep-Q-learning from demonstration is an extension compatible with Double/Dueling DQN which\n    uses a supervised large-margin loss to pretrain an agent from expert demonstrations. Paper:\n\n    https://arxiv.org/abs/1704.03732\n    """"""\n    def __init__(\n        self,\n        state_space,\n        action_space,\n        discount=0.98,\n        preprocessing_spec=None,\n        network_spec=None,\n        internal_states_space=None,\n        policy_spec=None,\n        exploration_spec=None,\n        execution_spec=None,\n        optimizer_spec=None,\n        observe_spec=None,\n        update_spec=None,\n        summary_spec=None,\n        saver_spec=None,\n        auto_build=True,\n        name=""dqfd-agent"",\n        expert_margin=0.5,\n        supervised_weight=1.0,\n        double_q=True,\n        dueling_q=True,\n        huber_loss=False,\n        n_step=1,\n        shared_container_action_target=False,\n        memory_spec=None,\n        demo_memory_spec=None,\n        demo_sample_ratio=0.2,\n    ):\n\n        """"""\n        Args:\n            state_space (Union[dict,Space]): Spec dict for the state Space or a direct Space object.\n            action_space (Union[dict,Space]): Spec dict for the action Space or a direct Space object.\n            preprocessing_spec (Optional[list,PreprocessorStack]): The spec list for the different necessary states\n                preprocessing steps or a PreprocessorStack object itself.\n            discount (float): The discount factor (gamma).\n            network_spec (Optional[list,NeuralNetwork]): Spec list for a NeuralNetwork Component or the NeuralNetwork\n                object itself.\n            internal_states_space (Optional[Union[dict,Space]]): Spec dict for the internal-states Space or a direct\n                Space object for the Space(s) of the internal (RNN) states.\n            policy_spec (Optional[dict]): An optional dict for further kwargs passing into the Policy c\'tor.\n            exploration_spec (Optional[dict]): The spec-dict to create the Exploration Component.\n            execution_spec (Optional[dict,Execution]): The spec-dict specifying execution settings.\n            optimizer_spec (Optional[dict,Optimizer]): The spec-dict to create the Optimizer for this Agent.\n            observe_spec (Optional[dict]): Spec-dict to specify `Agent.observe()` settings.\n            update_spec (Optional[dict]): Spec-dict to specify `Agent.update()` settings.\n            summary_spec (Optional[dict]): Spec-dict to specify summary settings.\n            saver_spec (Optional[dict]): Spec-dict to specify saver settings.\n            auto_build (Optional[bool]): If True (default), immediately builds the graph using the agent\'s\n                graph builder. If false, users must separately call agent.build(). Useful for debugging or analyzing\n                components before building.\n            name (str): Some name for this Agent object.\n            expert_margin (float): The expert margin enforces a distance in Q-values between expert action and\n                all other actions.\n            supervised_weight (float): Indicates weight of the expert loss.\n            double_q (bool): Whether to use the double DQN loss function (see [2]).\n            dueling_q (bool): Whether to use a dueling layer in the ActionAdapter  (see [3]).\n            huber_loss (bool) : Whether to apply a Huber loss. (see [4]).\n            n_step (Optional[int]): n-step adjustment to discounting.\n            memory_spec (Optional[dict,Memory]): The spec for the Memory to use.\n            demo_memory_spec (Optional[dict,Memory]): The spec for the Demo-Memory to use.\n        """"""\n        # Fix action-adapter before passing it to the super constructor.\n        # Use a DuelingPolicy (instead of a basic Policy) if option is set.\n        if dueling_q is True:\n            if policy_spec is None:\n                policy_spec = {}\n            policy_spec[""type""] = ""dueling-policy""\n            # Give us some default state-value nodes.\n            if ""units_state_value_stream"" not in policy_spec:\n                policy_spec[""units_state_value_stream""] = 128\n        super(DQFDAgent, self).__init__(\n            state_space=state_space,\n            action_space=action_space,\n            discount=discount,\n            preprocessing_spec=preprocessing_spec,\n            network_spec=network_spec,\n            internal_states_space=internal_states_space,\n            policy_spec=policy_spec,\n            exploration_spec=exploration_spec,\n            execution_spec=execution_spec,\n            optimizer_spec=optimizer_spec,\n            observe_spec=observe_spec,\n            update_spec=update_spec,\n            summary_spec=summary_spec,\n            saver_spec=saver_spec,\n            auto_build=auto_build,\n            name=name\n        )\n        # Assert that the synch interval is a multiple of the update_interval.\n        if self.update_spec[""sync_interval""] / self.update_spec[""update_interval""] != \\\n                self.update_spec[""sync_interval""] // self.update_spec[""update_interval""]:\n            raise RLGraphError(\n                ""ERROR: sync_interval ({}) must be multiple of update_interval ""\n                ""({})!"".format(self.update_spec[""sync_interval""], self.update_spec[""update_interval""])\n            )\n\n        self.double_q = double_q\n        self.dueling_q = dueling_q\n        self.huber_loss = huber_loss\n        self.expert_margin = expert_margin\n\n        self.batch_size = self.update_spec[""batch_size""]\n        self.default_margins = np.asarray([self.expert_margin] * self.batch_size)\n\n        self.demo_batch_size = int(demo_sample_ratio * self.update_spec[""batch_size""] / (1.0 - demo_sample_ratio))\n        self.demo_margins = np.asarray([self.expert_margin] * self.demo_batch_size)\n        self.shared_container_action_target = shared_container_action_target\n\n        # Extend input Space definitions to this Agent\'s specific API-methods.\n        preprocessed_state_space = self.preprocessed_state_space.with_batch_rank()\n        reward_space = FloatBox(add_batch_rank=True)\n        terminal_space = BoolBox(add_batch_rank=True)\n        weight_space = FloatBox(add_batch_rank=True)\n\n        self.input_spaces.update(dict(\n            actions=self.action_space.with_batch_rank(),\n            policy_weights=""variables:{}"".format(self.policy.scope),\n            time_step=int,\n            use_exploration=bool,\n            demo_batch_size=int,\n            apply_demo_loss=bool,\n            preprocessed_states=preprocessed_state_space,\n            rewards=reward_space,\n            terminals=terminal_space,\n            expert_margins=FloatBox(add_batch_rank=True),\n            next_states=preprocessed_state_space,\n            preprocessed_next_states=preprocessed_state_space,\n            importance_weights=weight_space\n        ))\n\n        # The merger to merge inputs into one record Dict going into the memory.\n        self.merger = ContainerMerger(""states"", ""actions"", ""rewards"", ""next_states"", ""terminals"")\n\n        # The replay memory.\n        self.memory = Memory.from_spec(memory_spec)\n        # Cannot have same default name.\n        demo_memory_spec[""scope""] = ""demo-memory""\n        self.demo_memory = Memory.from_spec(demo_memory_spec)\n\n        # The splitter for splitting up the records from the memories.\n        self.splitter = ContainerSplitter(""states"", ""actions"", ""rewards"", ""terminals"", ""next_states"")\n\n        # Copy our Policy (target-net), make target-net synchronizable.\n        self.target_policy = self.policy.copy(scope=""target-policy"", trainable=False)\n        # Number of steps since the last target-net synching from the main policy.\n        self.steps_since_target_net_sync = 0\n\n        self.use_importance_weights = isinstance(self.memory, PrioritizedReplay)\n        self.loss_function = DQFDLossFunction(\n            supervised_weight=supervised_weight,\n            discount=self.discount, double_q=self.double_q, huber_loss=self.huber_loss,\n            shared_container_action_target=shared_container_action_target,\n            importance_weights=self.use_importance_weights, n_step=n_step\n        )\n\n        # Add all our sub-components to the core.\n        self.root_component.add_components(\n            self.preprocessor, self.merger, self.memory, self.demo_memory, self.splitter, self.policy,\n            self.target_policy, self.exploration, self.loss_function, self.optimizer\n        )\n\n        # Define the Agent\'s (root-Component\'s) API.\n        self.define_graph_api()\n\n        if self.auto_build:\n            self._build_graph([self.root_component], self.input_spaces, optimizer=self.optimizer,\n                              batch_size=self.update_spec[""batch_size""])\n            self.graph_built = True\n\n    def define_graph_api(self):\n        super(DQFDAgent, self).define_graph_api()\n\n        agent = self\n\n        # Reset operation (resets preprocessor).\n        if self.preprocessing_required:\n            @rlgraph_api(component=self.root_component)\n            def reset_preprocessor(root):\n                reset_op = agent.preprocessor.reset()\n                return reset_op\n\n        # Act from preprocessed states.\n        @rlgraph_api(component=self.root_component)\n        def action_from_preprocessed_state(root, preprocessed_states, time_step=0, use_exploration=True):\n            sample_deterministic = agent.policy.get_deterministic_action(preprocessed_states)\n            actions = agent.exploration.get_action(sample_deterministic[""action""], time_step, use_exploration)\n            return actions, preprocessed_states\n\n        # State (from environment) to action with preprocessing.\n        @rlgraph_api(component=self.root_component)\n        def get_preprocessed_state_and_action(root, states, time_step=0, use_exploration=True):\n            preprocessed_states = agent.preprocessor.preprocess(states)\n            return root.action_from_preprocessed_state(preprocessed_states, time_step, use_exploration)\n\n        # Insert into memory.\n        @rlgraph_api(component=self.root_component)\n        def insert_records(root, preprocessed_states, actions, rewards, next_states, terminals):\n            records = agent.merger.merge(preprocessed_states, actions, rewards, next_states, terminals)\n            return agent.memory.insert_records(records)\n\n        # Insert into demo memory.\n        @rlgraph_api(component=self.root_component)\n        def insert_demos(root, preprocessed_states, actions, rewards, next_states, terminals):\n            records = agent.merger.merge(preprocessed_states, actions, rewards, next_states, terminals)\n            return agent.demo_memory.insert_records(records)\n\n        # Syncing target-net.\n        @rlgraph_api(component=self.root_component)\n        def sync_target_qnet(root):\n            # If we are a multi-GPU root:\n            # Simply feeds everything into the multi-GPU sync optimizer\'s method and return.\n            if ""multi-gpu-synchronizer"" in root.sub_components:\n                multi_gpu_syncer = root.sub_components[""multi-gpu-synchronizer""]\n                return multi_gpu_syncer.sync_target_qnets()\n            # We could be the main root or a multi-GPU tower.\n            else:\n                policy_vars = root.get_sub_component_by_name(agent.policy.scope).variables()\n                return root.get_sub_component_by_name(agent.target_policy.scope).sync(policy_vars)\n\n        # Learn from online memory AND demo memory.\n        @rlgraph_api(component=self.root_component)\n        def update_from_memory(root, apply_demo_loss, expert_margins, time_percentage=None):\n            # Sample from online memory.\n            records, sample_indices, importance_weights = agent.memory.get_records(self.batch_size)\n            preprocessed_s, actions, rewards, terminals, preprocessed_s_prime = agent.splitter.call(records)\n\n            # Do not apply demo loss to online experience.\n            online_step_op, loss, loss_per_item = root.update_from_external_batch(\n                preprocessed_s, actions, rewards, terminals, preprocessed_s_prime, importance_weights, apply_demo_loss,\n                expert_margins, time_percentage\n            )\n\n            if isinstance(agent.memory, PrioritizedReplay):\n                update_pr_step_op = agent.memory.update_records(sample_indices, loss_per_item)\n                return online_step_op, loss, loss_per_item, update_pr_step_op\n            else:\n                return online_step_op, loss, loss_per_item\n\n        # Learn from demo-data.\n        @rlgraph_api(component=self.root_component)\n        def update_from_demos(root, demo_batch_size, apply_demo_loss, expert_margins, time_percentage=None):\n            # Sample from demo memory.\n            records, sample_indices, importance_weights = agent.demo_memory.get_records(demo_batch_size)\n            preprocessed_s, actions, rewards, terminals, preprocessed_s_prime = agent.splitter.call(records)\n            step_op, loss, loss_per_item = root.update_from_external_batch(\n                preprocessed_s, actions, rewards, terminals, preprocessed_s_prime, importance_weights, apply_demo_loss,\n                expert_margins, time_percentage\n            )\n            return step_op, loss, loss_per_item\n\n        # Learn from an external batch - note the flag to apply demo loss.\n        @rlgraph_api(component=self.root_component)\n        def update_from_external_batch(\n                root, preprocessed_states, actions, rewards, terminals, preprocessed_next_states, importance_weights,\n                apply_demo_loss, expert_margins, time_percentage=None\n        ):\n            # If we are a multi-GPU root:\n            # Simply feeds everything into the multi-GPU sync optimizer\'s method and return.\n            if ""multi-gpu-synchronizer"" in root.sub_components:\n                main_policy_vars = agent.policy.variables()\n                # TODO: This may be called differently in other agents (replace by root-policy).\n                grads_and_vars, loss, loss_per_item, q_values_s = \\\n                    root.sub_components[""multi-gpu-synchronizer""].calculate_update_from_external_batch(\n                        dict(policy=main_policy_vars), preprocessed_states, actions, rewards, terminals,\n                        preprocessed_next_states, importance_weights, apply_demo_loss, expert_margins, time_percentage\n                    )\n                step_op = agent.optimizer.apply_gradients(grads_and_vars)\n                # Increase the global training step counter.\n                step_op = root._graph_fn_training_step(step_op)\n                step_and_sync_op = root.sub_components[""multi-gpu-synchronizer""].sync_variables_to_towers(\n                    step_op, main_policy_vars\n                )\n                return step_and_sync_op, loss, loss_per_item\n\n            # Get sub-components relative to the root (could be multi-GPU setup where root=some-tower).\n            policy = root.get_sub_component_by_name(agent.policy.scope)\n            target_policy = root.get_sub_component_by_name(agent.target_policy.scope)\n            loss_function = root.get_sub_component_by_name(agent.loss_function.scope)\n            optimizer = root.get_sub_component_by_name(agent.optimizer.scope)\n\n            # Get the different Q-values.\n            q_values_s = policy.get_adapter_outputs(preprocessed_states)[""adapter_outputs""]\n            qt_values_sp = target_policy.get_adapter_outputs(preprocessed_next_states)[""adapter_outputs""]\n\n            q_values_sp = None\n            if self.double_q:\n                q_values_sp = policy.get_adapter_outputs(preprocessed_next_states)[""adapter_outputs""]\n\n            loss, loss_per_item = loss_function.loss(\n                q_values_s, actions, rewards, terminals, qt_values_sp, expert_margins, q_values_sp,\n                importance_weights, apply_demo_loss\n            )\n\n            # Args are passed in again because some device strategies may want to split them to different devices.\n            policy_vars = policy.variables()\n            if hasattr(root, ""is_multi_gpu_tower"") and root.is_multi_gpu_tower is True:\n                grads_and_vars = optimizer.calculate_gradients(policy_vars, loss, time_percentage)\n                return grads_and_vars, loss, loss_per_item\n            else:\n                step_op = optimizer.step(policy_vars, loss, loss_per_item, time_percentage)\n                # Increase the global training step counter.\n                step_op = root._graph_fn_training_step(step_op)\n                return step_op, loss, loss_per_item\n\n        @rlgraph_api(component=self.root_component)\n        def get_td_loss(root, preprocessed_states, actions, rewards,\n                        terminals, preprocessed_next_states, importance_weights, apply_demo_loss, expert_margins,\n                        time_percentage=None):\n\n            policy = root.get_sub_component_by_name(agent.policy.scope)\n            target_policy = root.get_sub_component_by_name(agent.target_policy.scope)\n            loss_function = root.get_sub_component_by_name(agent.loss_function.scope)\n\n            # Get the different Q-values.\n            q_values_s = policy.get_adapter_outputs(preprocessed_states)[""adapter_outputs""]\n            qt_values_sp = target_policy.get_adapter_outputs(preprocessed_next_states)[""adapter_outputs""]\n\n            q_values_sp = None\n            if self.double_q:\n                q_values_sp = policy.get_adapter_outputs(preprocessed_next_states)[""adapter_outputs""]\n\n            loss, loss_per_item = loss_function.loss(\n                q_values_s, actions, rewards, terminals, qt_values_sp, expert_margins,\n                q_values_sp, importance_weights, apply_demo_loss\n            )\n            return loss, loss_per_item\n\n    def get_action(self, states, internals=None, use_exploration=True, apply_preprocessing=True, extra_returns=None,\n                   time_percentage=None):\n        """"""\n        Args:\n            extra_returns (Optional[Set[str],str]): Optional string or set of strings for additional return\n                values (besides the actions). Possible values are:\n                - \'preprocessed_states\': The preprocessed states after passing the given states through the\n                preprocessor stack.\n                - \'internal_states\': The internal states returned by the RNNs in the NN pipeline.\n                - \'used_exploration\': Whether epsilon- or noise-based exploration was used or not.\n\n        Returns:\n            tuple or single value depending on `extra_returns`:\n                - action\n                - the preprocessed states\n        """"""\n        # TODO: Move update_spec to Worker. Agent should not hold these execution details.\n        if time_percentage is None:\n            time_percentage = self.timesteps / self.update_spec.get(""max_timesteps"", 1e6)\n\n        extra_returns = {extra_returns} if isinstance(extra_returns, str) else (extra_returns or set())\n        # States come in without preprocessing -> use state space.\n        if apply_preprocessing:\n            call_method = ""get_preprocessed_state_and_action""\n            batched_states, remove_batch_rank = self.state_space.force_batch(states)\n        else:\n            call_method = ""action_from_preprocessed_state""\n            batched_states = states\n            remove_batch_rank = False\n\n        # Increase timesteps by the batch size (number of states in batch).\n        batch_size = len(batched_states)\n        self.timesteps += batch_size\n\n        # Control, which return value to ""pull"" (depending on `additional_returns`).\n        return_ops = [0, 1] if ""preprocessed_states"" in extra_returns else [0]  # 1=preprocessed_states, 0=action\n        ret = self.graph_executor.execute((\n            call_method,\n            [batched_states, time_percentage, use_exploration],\n            return_ops\n        ))\n        if remove_batch_rank:\n            return strip_list(ret)\n        else:\n            return ret\n\n    def _observe_graph(self, preprocessed_states, actions, internals, rewards, next_states, terminals):\n        self.graph_executor.execute((""insert_records"", [preprocessed_states, actions, rewards, next_states, terminals]))\n\n    def update(self, batch=None, time_percentage=None, update_from_demos=False, expert_margins=None,\n               apply_demo_loss_to_batch=False):\n        """"""\n        Updates from external batch or replay memory.\n        Args:\n            batch (Optional[dict]): Optional dict to use for updating. If false, samples from replay memory\n            update_from_demos (bool): If true, also updates from demo memory by sampling demonstrations from\n                demo memory. Default false (only updating from main replay memory).\n            expert_margins (SingleDataOp): The expert margin enforces a distance in Q-values between expert action and\n                all other actions.\n            apply_demo_loss_to_batch (bool): If true and an external batch is given, demo loss is applied to this\n                batch. Can be combined with external per-sample expert margins. The purpose of this is to update\n                from external demonstration data where each sample has a different margin. If false and an\n                external batch is given, the agent updates this with the normal Double/Dueling-q loss. Default\n                false. Only valid if batch is not None.\n        Returns:\n            tuple: Loss and loss per item.\n        """"""\n        # TODO: Move update_spec to Worker. Agent should not hold these execution details.\n        if time_percentage is None:\n            time_percentage = self.timesteps / self.update_spec.get(""max_timesteps"", 1e6)\n\n        # Should we sync the target net?\n        self.steps_since_target_net_sync += self.update_spec[""update_interval""]\n        if self.steps_since_target_net_sync >= self.update_spec[""sync_interval""]:\n            sync_call = ""sync_target_qnet""\n            self.steps_since_target_net_sync = 0\n        else:\n            sync_call = None\n\n        # Update from replay memory.  Potentially also update from demo memory with default margins.\n        if batch is None:\n            # Combine: Update from memory (apply_demo_loss=False), update_from_demo (apply=True).\n            # Otherwise only update from online memory.\n            if update_from_demos:\n                # Use default margins whe sampling from memory.\n                ret = self.graph_executor.execute(\n                    (""update_from_memory"", [False, self.default_margins, time_percentage]),\n                    (""update_from_demos"", [self.demo_batch_size, True, self.demo_margins, time_percentage]),\n                    sync_call\n                )\n            else:\n                ret = self.graph_executor.execute(\n                    (""update_from_memory"",  [False, self.default_margins, time_percentage]),\n                    sync_call\n                )\n\n            # Remove unnecessary return dicts (e.g. sync-op).\n            if isinstance(ret, dict):\n                ret = ret[""update_from_memory""]\n        else:\n            # Update from external batch, optionally applying demo loss. Also optionally\n            # sample demos from separate demo memory.\n            if expert_margins is None:\n                # Default margins with correct len.\n                expert_margins = np.asarray([self.expert_margin] * len(batch[""terminals""]))\n\n            # Apply demo loss to external flag depending on batch.\n            # Expert margins only have effect if True.\n            batch_input = [\n                batch[""states""], batch[""actions""], batch[""rewards""], batch[""terminals""], batch[""next_states""],\n                batch[""importance_weights""], apply_demo_loss_to_batch, expert_margins, time_percentage\n            ]\n\n            if update_from_demos:\n                ret = self.graph_executor.execute(\n                    (""update_from_external_batch"", batch_input),\n                    (""update_from_demos"", [self.demo_batch_size, True, self.demo_margins, time_percentage]),\n                    sync_call\n                )\n            else:\n                # Only update from external batch (which may use demo loss depending on flag.\n                ret = self.graph_executor.execute(\n                    (""update_from_external_batch"", batch_input),\n                    sync_call\n                )\n\n            # Remove unnecessary return dicts (e.g. sync-op).\n            if isinstance(ret, dict):\n                ret = ret[""update_from_external_batch""]\n\n        # [1]=the loss (0=update noop)\n        # [2]=loss per item for external update, records for update from memory\n        return ret[1], ret[2]\n\n    def reset(self):\n        """"""\n        Resets our preprocessor, but only if it contains stateful PreprocessLayer Components (meaning\n        the PreprocessorStack has at least one variable defined).\n        """"""\n        if self.preprocessing_required and len(self.preprocessor.variable_registry) > 0:\n            self.graph_executor.execute(""reset_preprocessor"")\n\n    def update_from_demos(self, batch_size=None, time_percentage=None, num_updates=1):\n        """"""\n        Executes a number of updates by sampling from the expert memory.\n\n        Args:\n            num_updates (int): The number of samples to execute.\n            batch_size (Optional[int]): Sampling batch size to use. If None, uses the demo\n                batch size computed via the sampling ratio.\n        """"""\n        # TODO: Move update_spec to Worker. Agent should not hold these execution details.\n        if time_percentage is None:\n            time_percentage = self.timesteps / self.update_spec.get(""max_timesteps"", 1e6)\n\n        if batch_size is None:\n            batch_size = self.demo_batch_size\n            margins = self.demo_margins\n        else:\n            margins = np.asarray([self.expert_margin] * batch_size)\n        for _ in range(num_updates):\n            self.graph_executor.execute((""update_from_demos"", [batch_size, True, margins, time_percentage]))\n\n    def observe_demos(self, preprocessed_states, actions, rewards, next_states, terminals):\n        """"""\n        Inserts observations into the demonstration memory.\n        """"""\n        self.graph_executor.execute((""insert_demos"", [preprocessed_states, actions, rewards, next_states, terminals]))\n\n    def __repr__(self):\n        return ""DQFDAgent(doubleQ={} duelingQ={}, expert margin={})"".format(\n            self.double_q, self.dueling_q, self.expert_margin\n        )\n'"
rlgraph/agents/dqn_agent.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph.agents import Agent\nfrom rlgraph.components import Memory, PrioritizedReplay, DQNLossFunction, ContainerSplitter\nfrom rlgraph.spaces import FloatBox, BoolBox\nfrom rlgraph.utils import RLGraphError\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.util import strip_list\n\n\nclass DQNAgent(Agent):\n    """"""\n    A collection of DQN algorithms published in the following papers:\n\n    [1] Human-level control through deep reinforcement learning. Mnih, Kavukcuoglu, Silver et al. - 2015\n    [2] Deep Reinforcement Learning with Double Q-learning. v. Hasselt, Guez, Silver - 2015\n    [3] Dueling Network Architectures for Deep Reinforcement Learning, Wang et al. - 2016\n    [4] https://en.wikipedia.org/wiki/Huber_loss\n    """"""\n    def __init__(\n        self,\n        state_space,\n        action_space,\n        discount=0.98,\n        preprocessing_spec=None,\n        network_spec=None,\n        internal_states_space=None,\n        policy_spec=None,\n        exploration_spec=None,\n        execution_spec=None,\n        optimizer_spec=None,\n        observe_spec=None,\n        update_spec=None,\n        summary_spec=None,\n        saver_spec=None,\n        auto_build=True,\n        name=""dqn-agent"",\n        double_q=True,\n        dueling_q=True,\n        huber_loss=False,\n        n_step=1,\n        shared_container_action_target=True,\n        memory_spec=None,\n    ):\n        """"""\n        Args:\n            state_space (Union[dict,Space]): Spec dict for the state Space or a direct Space object.\n            action_space (Union[dict,Space]): Spec dict for the action Space or a direct Space object.\n            preprocessing_spec (Optional[list,PreprocessorStack]): The spec list for the different necessary states\n                preprocessing steps or a PreprocessorStack object itself.\n            discount (float): The discount factor (gamma).\n            network_spec (Optional[list,NeuralNetwork]): Spec list for a NeuralNetwork Component or the NeuralNetwork\n                object itself.\n            internal_states_space (Optional[Union[dict,Space]]): Spec dict for the internal-states Space or a direct\n                Space object for the Space(s) of the internal (RNN) states.\n            policy_spec (Optional[dict]): An optional dict for further kwargs passing into the Policy c\'tor.\n            exploration_spec (Optional[dict]): The spec-dict to create the Exploration Component.\n            execution_spec (Optional[dict,Execution]): The spec-dict specifying execution settings.\n            optimizer_spec (Optional[dict,Optimizer]): The spec-dict to create the Optimizer for this Agent.\n            observe_spec (Optional[dict]): Spec-dict to specify `Agent.observe()` settings.\n            update_spec (Optional[dict]): Spec-dict to specify `Agent.update()` settings.\n            summary_spec (Optional[dict]): Spec-dict to specify summary settings.\n            saver_spec (Optional[dict]): Spec-dict to specify saver settings.\n            auto_build (Optional[bool]): If True (default), immediately builds the graph using the agent\'s\n                graph builder. If false, users must separately call agent.build(). Useful for debugging or analyzing\n                components before building.\n            name (str): Some name for this Agent object.\n            double_q (bool): Whether to use the double DQN loss function (see [2]).\n            dueling_q (bool): Whether to use a dueling layer in the ActionAdapter  (see [3]).\n            huber_loss (bool) : Whether to apply a Huber loss. (see [4]).\n            n_step (Optional[int]): n-step adjustment to discounting.\n            memory_spec (Optional[dict,Memory]): The spec for the Memory to use for the DQN algorithm.\n        """"""\n        # Fix action-adapter before passing it to the super constructor.\n        # Use a DuelingPolicy (instead of a basic Policy) if option is set.\n        if dueling_q is True:\n            policy_spec[""type""] = ""dueling-policy""\n            # Give us some default state-value nodes.\n            if ""units_state_value_stream"" not in policy_spec:\n                policy_spec[""units_state_value_stream""] = 128\n\n        super(DQNAgent, self).__init__(\n            state_space=state_space,\n            action_space=action_space,\n            discount=discount,\n            preprocessing_spec=preprocessing_spec,\n            network_spec=network_spec,\n            internal_states_space=internal_states_space,\n            policy_spec=policy_spec,\n            exploration_spec=exploration_spec,\n            execution_spec=execution_spec,\n            optimizer_spec=optimizer_spec,\n            observe_spec=observe_spec,\n            update_spec=update_spec,\n            summary_spec=summary_spec,\n            saver_spec=saver_spec,\n            auto_build=auto_build,\n            name=name\n        )\n\n        # TODO: Have to manually set it here for multi-GPU synchronizer to know its number\n        # TODO: of return values when calling _graph_fn_calculate_update_from_external_batch.\n        # self.root_component.graph_fn_num_outputs[""_graph_fn_update_from_external_batch""] = 4\n\n        # Assert that the synch interval is a multiple of the update_interval.\n        if self.update_spec[""sync_interval""] / self.update_spec[""update_interval""] != \\\n                self.update_spec[""sync_interval""] // self.update_spec[""update_interval""]:\n            raise RLGraphError(\n                ""ERROR: sync_interval ({}) must be multiple of update_interval ""\n                ""({})!"".format(self.update_spec[""sync_interval""], self.update_spec[""update_interval""])\n            )\n\n        self.double_q = double_q\n        self.dueling_q = dueling_q\n        self.huber_loss = huber_loss\n        self.shared_container_action_target = shared_container_action_target\n\n        # Extend input Space definitions to this Agent\'s specific API-methods.\n        preprocessed_state_space = self.preprocessed_state_space.with_batch_rank()\n        reward_space = FloatBox(add_batch_rank=True)\n        terminal_space = BoolBox(add_batch_rank=True)\n        weight_space = FloatBox(add_batch_rank=True)\n\n        self.input_spaces.update(dict(\n            actions=self.action_space.with_batch_rank(),\n            # Weights will have a Space derived from the vars of policy.\n            policy_weights=""variables:{}"".format(self.policy.scope),\n            use_exploration=bool,\n            preprocessed_states=preprocessed_state_space,\n            rewards=reward_space,\n            terminals=terminal_space,\n            next_states=preprocessed_state_space,\n            preprocessed_next_states=preprocessed_state_space,\n            importance_weights=weight_space,\n            apply_postprocessing=bool\n        ))\n        if self.value_function is not None:\n            self.input_spaces[""value_function_weights""] = ""variables:{}"".format(self.value_function.scope),\n\n        # The replay memory.\n        self.memory = Memory.from_spec(memory_spec)\n        # The splitter for splitting up the records coming from the memory.\n        self.splitter = ContainerSplitter(""states"", ""actions"", ""rewards"", ""terminals"", ""next_states"")\n\n        # Make sure the python buffer is not larger than our memory capacity.\n        assert self.observe_spec[""buffer_size""] <= self.memory.capacity,\\\n            ""ERROR: Buffer\'s size ({}) in `observe_spec` must be smaller or equal to the memory\'s capacity ({})!"".\\\n            format(self.observe_spec[""buffer_size""], self.memory.capacity)\n\n        # Copy our Policy (target-net), make target-net synchronizable.\n        self.target_policy = self.policy.copy(scope=""target-policy"", trainable=False)\n        # Number of steps since the last target-net synching from the main policy.\n        self.steps_since_target_net_sync = 0\n\n        use_importance_weights = isinstance(self.memory, PrioritizedReplay)\n        self.loss_function = DQNLossFunction(\n            discount=self.discount, double_q=self.double_q, huber_loss=self.huber_loss,\n            shared_container_action_target=shared_container_action_target,\n            importance_weights=use_importance_weights, n_step=n_step\n        )\n\n        self.root_component.add_components(\n            self.preprocessor, self.memory, self.splitter, self.policy, self.target_policy,\n            self.value_function, self.value_function_optimizer,  # <- should both be None for DQN\n            self.exploration, self.loss_function, self.optimizer, self.vars_merger, self.vars_splitter\n        )\n\n        # Define the Agent\'s (root-Component\'s) API.\n        self.define_graph_api()\n\n        # markup = get_graph_markup(self.graph_builder.root_component)\n        # print(markup)\n        if self.auto_build:\n            self._build_graph([self.root_component], self.input_spaces, optimizer=self.optimizer,\n                              batch_size=self.update_spec[""batch_size""])\n            self.graph_built = True\n\n    def define_graph_api(self, *args, **kwargs):\n        super(DQNAgent, self).define_graph_api()\n\n        agent = self\n\n        # Reset operation (resets preprocessor).\n        if self.preprocessing_required:\n            @rlgraph_api(component=self.root_component)\n            def reset_preprocessor(root):\n                reset_op = agent.preprocessor.reset()\n                return reset_op\n\n        # Act from preprocessed states.\n        @rlgraph_api(component=self.root_component)\n        def action_from_preprocessed_state(root, preprocessed_states, time_percentage=None, use_exploration=True):\n            sample_deterministic = agent.policy.get_deterministic_action(preprocessed_states)\n            actions = agent.exploration.get_action(sample_deterministic[""action""], time_percentage, use_exploration)\n            return actions, preprocessed_states\n\n        # State (from environment) to action with preprocessing.\n        @rlgraph_api(component=self.root_component)\n        def get_preprocessed_state_and_action(root, states, time_percentage=None, use_exploration=True):\n            preprocessed_states = agent.preprocessor.preprocess(states)\n            return root.action_from_preprocessed_state(preprocessed_states, time_percentage, use_exploration)\n\n        # Insert into memory.\n        @rlgraph_api(component=self.root_component)\n        def insert_records(root, preprocessed_states, actions, rewards, next_states, terminals):\n            records = dict(\n                states=preprocessed_states, actions=actions, rewards=rewards, next_states=next_states, terminals=terminals\n            )\n            return agent.memory.insert_records(records)\n\n        # Syncing target-net.\n        @rlgraph_api(component=self.root_component)\n        def sync_target_qnet(root):\n            # If we are a multi-GPU root:\n            # Simply feeds everything into the multi-GPU sync optimizer\'s method and return.\n            if ""multi-gpu-synchronizer"" in root.sub_components:\n                multi_gpu_syncer = root.sub_components[""multi-gpu-synchronizer""]\n                return multi_gpu_syncer.sync_target_qnets()\n            # We could be the main root or a multi-GPU tower.\n            else:\n                policy_vars = root.get_sub_component_by_name(agent.policy.scope).variables()\n                return root.get_sub_component_by_name(agent.target_policy.scope).sync(policy_vars)\n\n        # Learn from memory.\n        @rlgraph_api(component=self.root_component)\n        def update_from_memory(root, apply_postprocessing, time_percentage=None):\n            # Non prioritized memory will just return weight 1.0 for all samples.\n            records, sample_indices, importance_weights = agent.memory.get_records(agent.update_spec[""batch_size""])\n            preprocessed_s, actions, rewards, terminals, preprocessed_s_prime = agent.splitter.call(records)\n\n            step_op, loss, loss_per_item = root.update_from_external_batch(\n                preprocessed_s, actions, rewards, terminals, preprocessed_s_prime, importance_weights,\n                apply_postprocessing, time_percentage\n            )\n\n            # TODO this is really annoying. Will be solved once we have dict returns.\n            if isinstance(agent.memory, PrioritizedReplay):\n                update_pr_step_op = agent.memory.update_records(sample_indices, loss_per_item)\n                return step_op, loss, loss_per_item, update_pr_step_op\n            else:\n                return step_op, loss, loss_per_item\n\n        # Learn from an external batch.\n        @rlgraph_api(component=self.root_component)\n        def update_from_external_batch(\n                root, preprocessed_states, actions, rewards, terminals, preprocessed_next_states,\n                importance_weights, apply_postprocessing, time_percentage=None\n        ):\n            # If we are a multi-GPU root:\n            # Simply feeds everything into the multi-GPU sync optimizer\'s method and return.\n            if ""multi-gpu-synchronizer"" in root.sub_components:\n                main_policy_vars = agent.policy.variables()\n                all_vars = agent.vars_merger.merge(main_policy_vars)\n                out = root.sub_components[""multi-gpu-synchronizer""].calculate_update_from_external_batch(\n                    all_vars, preprocessed_states, actions, rewards, terminals,\n                    preprocessed_next_states, importance_weights, apply_postprocessing=apply_postprocessing,\n                    time_percentage=time_percentage\n                )\n                avg_grads_and_vars = agent.vars_splitter.call(out[""avg_grads_and_vars_by_component""])\n                step_op = agent.optimizer.apply_gradients(avg_grads_and_vars)\n                # Increase the global training step counter.\n                step_op = root._graph_fn_training_step(step_op)\n                step_and_sync_op = root.sub_components[""multi-gpu-synchronizer""].sync_variables_to_towers(\n                    step_op, all_vars\n                )\n                return step_and_sync_op, out[""loss""], out[""loss_per_item""]\n\n            # Get sub-components relative to the root (could be multi-GPU setup where root=some-tower).\n            policy = root.get_sub_component_by_name(agent.policy.scope)\n            target_policy = root.get_sub_component_by_name(agent.target_policy.scope)\n            loss_function = root.get_sub_component_by_name(agent.loss_function.scope)\n            optimizer = root.get_sub_component_by_name(agent.optimizer.scope)\n            vars_merger = root.get_sub_component_by_name(agent.vars_merger.scope)\n\n            # Get the different Q-values.\n            q_values_s = policy.get_adapter_outputs_and_parameters(preprocessed_states)[""adapter_outputs""]\n            qt_values_sp = target_policy.get_adapter_outputs_and_parameters(preprocessed_next_states)[""adapter_outputs""]\n\n            q_values_sp = None\n            if self.double_q:\n                q_values_sp = policy.get_adapter_outputs_and_parameters(preprocessed_next_states)[""adapter_outputs""]\n\n            loss, loss_per_item = loss_function.loss(\n                q_values_s, actions, rewards, terminals, qt_values_sp, q_values_sp, importance_weights\n            )\n\n            # Args are passed in again because some device strategies may want to split them to different devices.\n            policy_vars = policy.variables()\n\n            # TODO: for a fully automated multi-GPU strategy, we would have to make sure that:\n            # TODO: - every agent (root_component) has an update_from_external_batch method\n            # TODO: - this if check is somehow automated and not necessary anymore (local optimizer must be called with different API-method, not step)\n            if hasattr(root, ""is_multi_gpu_tower"") and root.is_multi_gpu_tower is True:\n                grads_and_vars = optimizer.calculate_gradients(policy_vars, loss, time_percentage)\n                grads_and_vars_by_component = vars_merger.merge(grads_and_vars)\n                return grads_and_vars_by_component, loss, loss_per_item\n            else:\n                step_op = optimizer.step(policy_vars, loss, loss_per_item, time_percentage)\n                # Increase the global training step counter.\n                step_op = root._graph_fn_training_step(step_op)\n                return step_op, loss, loss_per_item\n\n        @rlgraph_api(component=self.root_component)\n        def get_td_loss(root, preprocessed_states, actions, rewards,\n                        terminals, preprocessed_next_states, importance_weights):\n\n            policy = root.get_sub_component_by_name(agent.policy.scope)\n            target_policy = root.get_sub_component_by_name(agent.target_policy.scope)\n            loss_function = root.get_sub_component_by_name(agent.loss_function.scope)\n\n            # Get the different Q-values.\n            q_values_s = policy.get_adapter_outputs_and_parameters(preprocessed_states)[""adapter_outputs""]\n            qt_values_sp = target_policy.get_adapter_outputs_and_parameters(preprocessed_next_states)[""adapter_outputs""]\n\n            q_values_sp = None\n            if self.double_q:\n                q_values_sp = policy.get_adapter_outputs_and_parameters(preprocessed_next_states)[""adapter_outputs""]\n\n            loss, loss_per_item = loss_function.loss(\n                q_values_s, actions, rewards, terminals, qt_values_sp, q_values_sp, importance_weights\n            )\n            return loss, loss_per_item\n\n        @rlgraph_api(component=self.root_component)\n        def get_q_values(root, preprocessed_states):\n            q_values = root.get_sub_component_by_name(agent.policy.scope).get_adapter_outputs_and_parameters(\n                preprocessed_states)[""adapter_outputs""]\n            return q_values\n\n    def get_action(self, states, internals=None, use_exploration=True, apply_preprocessing=True, extra_returns=None,\n                   time_percentage=None):\n        """"""\n        Args:\n            extra_returns (Optional[Set[str],str]): Optional string or set of strings for additional return\n                values (besides the actions). Possible values are:\n                - \'preprocessed_states\': The preprocessed states after passing the given states through the\n                preprocessor stack.\n                - \'internal_states\': The internal states returned by the RNNs in the NN pipeline.\n                - \'used_exploration\': Whether epsilon- or noise-based exploration was used or not.\n\n        Returns:\n            tuple or single value depending on `extra_returns`:\n                - action\n                - the preprocessed states\n        """"""\n        # TODO: Move update_spec to Worker. Agent should not hold these execution details.\n        if time_percentage is None:\n            time_percentage = self.timesteps / self.update_spec.get(""max_timesteps"", 1e6)\n\n        extra_returns = {extra_returns} if isinstance(extra_returns, str) else (extra_returns or set())\n        # States come in without preprocessing -> use state space.\n        if apply_preprocessing:\n            call_method = ""get_preprocessed_state_and_action""\n            batched_states, remove_batch_rank = self.state_space.force_batch(states)\n        else:\n            call_method = ""action_from_preprocessed_state""\n            batched_states = states\n            remove_batch_rank = False  #batched_states.ndim == np.asarray(states).ndim + 1\n\n        # Increase timesteps by the batch size (number of states in batch).\n        batch_size = len(batched_states)\n        self.timesteps += batch_size\n\n        # Control, which return value to ""pull"" (depending on `additional_returns`).\n        return_ops = [0, 1] if ""preprocessed_states"" in extra_returns else [0]  # 1=preprocessed_states, 0=action\n        ret = self.graph_executor.execute((\n            call_method,\n            [batched_states, time_percentage, use_exploration],\n            return_ops\n        ))\n        if remove_batch_rank:\n            return strip_list(ret)\n        else:\n            return ret\n\n    def _observe_graph(self, preprocessed_states, actions, internals, rewards, next_states, terminals):\n        self.graph_executor.execute((""insert_records"", [preprocessed_states, actions, rewards, next_states, terminals]))\n\n    def update(self, batch=None, time_percentage=None, **kwargs):\n        # TODO: Move update_spec to Worker. Agent should not hold these execution details.\n        if time_percentage is None:\n            time_percentage = self.timesteps / self.update_spec.get(""max_timesteps"", 1e6)\n\n        # Should we sync the target net?\n        self.steps_since_target_net_sync += self.update_spec[""update_interval""]\n        if self.steps_since_target_net_sync >= self.update_spec[""sync_interval""]:\n            sync_call = ""sync_target_qnet""\n            self.steps_since_target_net_sync = 0\n        else:\n            sync_call = None\n\n        if batch is None:\n            ret = self.graph_executor.execute((""update_from_memory"", [True, time_percentage]))\n        else:\n            # TODO apply postprocessing always true atm.\n            input_ = [batch[""states""], batch[""actions""], batch[""rewards""], batch[""terminals""],\n                           batch[""next_states""], batch[""importance_weights""], True, time_percentage]\n            ret = self.graph_executor.execute((""update_from_external_batch"", input_))\n\n        # Do the target net synching after the update (for better clarity: after a sync, we would expect for both\n        # networks to be the exact same).\n        if sync_call:\n            self.graph_executor.execute(sync_call)\n\n        # 1=the loss\n        # 2=loss per item for external update, records for update from memory\n        return ret[1], ret[2]\n\n    def reset(self):\n        """"""\n        Resets our preprocessor, but only if it contains stateful PreprocessLayer Components (meaning\n        the PreprocessorStack has at least one variable defined).\n        """"""\n        if self.preprocessing_required and len(self.preprocessor.variable_registry) > 0:\n            self.graph_executor.execute(""reset_preprocessor"")\n\n    def post_process(self, batch):\n        batch_input = [batch[""states""], batch[""actions""], batch[""rewards""], batch[""terminals""],\n                       batch[""next_states""], batch[""importance_weights""]]\n        ret = self.graph_executor.execute((""get_td_loss"", batch_input))\n\n        # Remove unnecessary return dicts.\n        if isinstance(ret, dict):\n            ret = ret[""get_td_loss""]\n\n        # Return [0]=total loss, [1]=loss-per-item\n        return ret[0], ret[1]\n\n    def __repr__(self):\n        return ""DQNAgent(doubleQ={} duelingQ={})"".format(self.double_q, self.dueling_q)\n'"
rlgraph/agents/impala_agents.py,3,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\n\nfrom rlgraph import get_backend\nfrom rlgraph.agents.agent import Agent\nfrom rlgraph.components.common.container_merger import ContainerMerger\nfrom rlgraph.components.common.environment_stepper import EnvironmentStepper\nfrom rlgraph.components.common.slice import Slice\nfrom rlgraph.components.common.staging_area import StagingArea\nfrom rlgraph.components.layers.preprocessing.container_splitter import ContainerSplitter\nfrom rlgraph.components.layers.preprocessing.reshape import ReShape\nfrom rlgraph.components.layers.preprocessing.transpose import Transpose\nfrom rlgraph.components.loss_functions.impala_loss_function import IMPALALossFunction\nfrom rlgraph.components.memories.fifo_queue import FIFOQueue\nfrom rlgraph.components.memories.queue_runner import QueueRunner\nfrom rlgraph.components.neural_networks.actor_component import ActorComponent\nfrom rlgraph.components.policies.dynamic_batching_policy import DynamicBatchingPolicy\nfrom rlgraph.spaces import FloatBox, Dict, Tuple\nfrom rlgraph.utils import RLGraphError\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.util import default_dict\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass IMPALAAgent(Agent):\n    """"""\n    An Agent implementing the IMPALA algorithm described in [1]. The Agent contains both learner and actor\n    API-methods, which will be put into the graph depending on the type ().\n\n    [1] IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures - Espeholt, Soyer,\n        Munos et al. - 2018 (https://arxiv.org/abs/1802.01561)\n    """"""\n\n    default_internal_states_space = Tuple(FloatBox(shape=(256,)), FloatBox(shape=(256,)), add_batch_rank=False)\n    default_environment_spec = dict(\n        type=""deepmind_lab"", level_id=""seekavoid_arena_01"", observations=[""RGB_INTERLEAVED"", ""INSTR""],\n        frameskip=4\n    )\n\n    def __init__(self, discount=0.99, fifo_queue_spec=None, architecture=""large"", environment_spec=None,\n                 feed_previous_action_through_nn=True, feed_previous_reward_through_nn=True,\n                 weight_pg=None, weight_baseline=None, weight_entropy=None, worker_sample_size=100,\n                 **kwargs):\n        """"""\n        Args:\n            discount (float): The discount factor gamma.\n            architecture (str): Which IMPALA architecture to use. One of ""small"" or ""large"". Will be ignored if\n                `network_spec` is given explicitly in kwargs. Default: ""large"".\n            fifo_queue_spec (Optional[dict,FIFOQueue]): The spec for the FIFOQueue to use for the IMPALA algorithm.\n            environment_spec (dict): The spec for constructing an Environment object for an actor-type IMPALA agent.\n            feed_previous_action_through_nn (bool): Whether to add the previous action as another input channel to the\n                ActionComponent\'s (NN\'s) input at each step. This is only possible if the state space is already a Dict.\n                It will be added under the key ""previous_action"". Default: True.\n            feed_previous_reward_through_nn (bool): Whether to add the previous reward as another input channel to the\n                ActionComponent\'s (NN\'s) input at each step. This is only possible if the state space is already a Dict.\n                It will be added under the key ""previous_reward"". Default: True.\n            weight_pg (float): See IMPALALossFunction Component.\n            weight_baseline (float): See IMPALALossFunction Component.\n            weight_entropy (float): See IMPALALossFunction Component.\n            worker_sample_size (int): How many steps the actor will perform in the environment each sample-run.\n\n        Keyword Args:\n            type (str): One of ""single"", ""actor"" or ""learner"". Default: ""single"".\n        """"""\n        type_ = kwargs.pop(""type"", ""single"")\n        assert type_ in [""single"", ""actor"", ""learner""]\n        self.type = type_\n        self.worker_sample_size = worker_sample_size\n\n        # Network-spec by default is a ""large architecture"" IMPALA network.\n        self.network_spec = kwargs.pop(\n            ""network_spec"",\n            dict(type=""rlgraph.components.neural_networks.impala.impala_networks.{}IMPALANetwork"".\n                 format(""Large"" if architecture == ""large"" else ""Small""))\n        )\n        if isinstance(self.network_spec, dict) and ""type"" in self.network_spec and \\\n                ""IMPALANetwork"" in self.network_spec[""type""]:\n            self.network_spec = default_dict(\n                self.network_spec,\n                dict(worker_sample_size=1 if self.type == ""actor"" else self.worker_sample_size + 1)\n            )\n\n        # Depending on the job-type, remove the pieces from the Agent-spec/graph we won\'t need.\n        self.exploration_spec = kwargs.pop(""exploration_spec"", None)\n        optimizer_spec = kwargs.pop(""optimizer_spec"", None)\n        observe_spec = kwargs.pop(""observe_spec"", None)\n\n        self.feed_previous_action_through_nn = feed_previous_action_through_nn\n        self.feed_previous_reward_through_nn = feed_previous_reward_through_nn\n\n        # Run everything in a single process.\n        if self.type == ""single"":\n            environment_spec = environment_spec or self.default_environment_spec\n            update_spec = kwargs.pop(""update_spec"", None)\n        # Actors won\'t need to learn (no optimizer needed in graph).\n        elif self.type == ""actor"":\n            optimizer_spec = None\n            update_spec = kwargs.pop(""update_spec"", dict(do_updates=False))\n            environment_spec = environment_spec or self.default_environment_spec\n        # Learners won\'t need to explore (act) or observe (insert into Queue).\n        else:\n            observe_spec = None\n            update_spec = kwargs.pop(""update_spec"", None)\n            environment_spec = None\n\n        # Add previous-action/reward preprocessors to env-specific preprocessor spec.\n        # TODO: remove this empty hard-coded preprocessor.\n        self.preprocessing_spec = kwargs.pop(\n            ""preprocessing_spec"", dict(type=""dict-preprocessor-stack"", preprocessors=dict(\n                # Flatten actions.\n                previous_action=[\n                    dict(type=""reshape"", flatten=True, flatten_categories=kwargs.get(""action_space"").num_categories)\n                ],\n                # Bump reward and convert to float32, so that it can be concatenated by the Concat layer.\n                previous_reward=[\n                    dict(type=""reshape"", new_shape=(1,))\n                ]\n            ))\n        )\n\n        # Limit communication in distributed mode between each actor and the learner (never between actors).\n        execution_spec = kwargs.pop(""execution_spec"", None)\n        if execution_spec is not None and execution_spec.get(""mode"") == ""distributed"":\n            default_dict(execution_spec[""session_config""], dict(\n                type=""monitored-training-session"",\n                allow_soft_placement=True,\n                device_filters=[""/job:learner/task:0""] + (\n                    [""/job:actor/task:{}"".format(execution_spec[""distributed_spec""][""task_index""])] if\n                    self.type == ""actor"" else [""/job:learner/task:0""]\n                )\n            ))\n            # If Actor, make non-chief in either case (even if task idx == 0).\n            if self.type == ""actor"":\n                execution_spec[""distributed_spec""][""is_chief""] = False\n                # Hard-set device to the CPU for actors.\n                execution_spec[""device_strategy""] = ""custom""\n                execution_spec[""default_device""] = ""/job:{}/task:{}/cpu"".format(self.type, execution_spec[""distributed_spec""][""task_index""])\n\n        self.policy_spec = kwargs.pop(""policy_spec"", dict())\n        # TODO: Create some auto-setting based on LSTM inside the NN.\n        default_dict(self.policy_spec, dict(\n            type=""shared-value-function-policy"",\n            deterministic=False,\n            reuse_variable_scope=""shared-policy"",\n            action_space=kwargs.get(""action_space"")\n        ))\n\n        # Now that we fixed the Agent\'s spec, call the super constructor.\n        super(IMPALAAgent, self).__init__(\n            discount=discount,\n            preprocessing_spec=self.preprocessing_spec,\n            network_spec=self.network_spec,\n            policy_spec=self.policy_spec,\n            exploration_spec=self.exploration_spec,\n            optimizer_spec=optimizer_spec,\n            observe_spec=observe_spec,\n            update_spec=update_spec,\n            execution_spec=execution_spec,\n            name=kwargs.pop(""name"", ""impala-{}-agent"".format(self.type)),\n            **kwargs\n        )\n        # Always use 1st learner as the parameter server for all policy variables.\n        if self.execution_spec[""mode""] == ""distributed"" and self.execution_spec[""distributed_spec""][""cluster_spec""]:\n            self.policy.propagate_sub_component_properties(dict(device=dict(variables=""/job:learner/task:0/cpu"")))\n\n        # Check whether we have an RNN.\n        self.has_rnn = self.policy.neural_network.has_rnn()\n        # Check, whether we are running with GPU.\n        self.has_gpu = self.execution_spec[""gpu_spec""][""gpus_enabled""] is True and \\\n            self.execution_spec[""gpu_spec""][""num_gpus""] > 0\n\n        # Some FIFO-queue specs.\n        self.fifo_queue_keys = [""terminals"", ""states""] + \\\n                               ([""actions""] if not self.feed_previous_action_through_nn else []) + \\\n                               ([""rewards""] if not self.feed_previous_reward_through_nn else []) + \\\n                               [""action_probs""] + \\\n                               ([""initial_internal_states""] if self.has_rnn else [])\n        # Define FIFO record space.\n        # Note that only states and internal_states (RNN) contain num-steps+1 items, all other sub-records only contain\n        # num-steps items.\n        self.fifo_record_space = Dict(\n            {\n                ""terminals"": bool,\n                ""action_probs"": FloatBox(shape=(self.action_space.num_categories,)),\n            }, add_batch_rank=False, add_time_rank=self.worker_sample_size\n        )\n        self.fifo_record_space[""states""] = self.state_space.with_time_rank(self.worker_sample_size + 1)\n        # Add action and rewards to state or do they have an extra channel?\n        if self.feed_previous_action_through_nn:\n            self.fifo_record_space[""states""][""previous_action""] = \\\n                self.action_space.with_time_rank(self.worker_sample_size + 1)\n        else:\n            self.fifo_record_space[""actions""] = self.action_space.with_time_rank(self.worker_sample_size)\n        if self.feed_previous_action_through_nn:\n            self.fifo_record_space[""states""][""previous_reward""] = FloatBox(add_time_rank=self.worker_sample_size + 1)\n        else:\n            self.fifo_record_space[""rewards""] = FloatBox(add_time_rank=self.worker_sample_size)\n\n        if self.has_rnn:\n            self.fifo_record_space[""initial_internal_states""] = self.internal_states_space.with_time_rank(False)\n\n        # Create our FIFOQueue (actors will enqueue, learner(s) will dequeue).\n        self.fifo_queue = FIFOQueue.from_spec(\n            fifo_queue_spec or dict(capacity=1),\n            reuse_variable_scope=""shared-fifo-queue"",\n            only_insert_single_records=True,\n            record_space=self.fifo_record_space,\n            device=""/job:learner/task:0/cpu"" if self.execution_spec[""mode""] == ""distributed"" and\n            self.execution_spec[""distributed_spec""][""cluster_spec""] else None\n        )\n\n        # Remove `states` key from input_spaces: not needed.\n        del self.input_spaces[""states""]\n\n        # TODO: implement the reward summary\n        del self.input_spaces[""increment""]\n        del self.input_spaces[""episode_reward""]\n\n        # Add all our sub-components to the core.\n        if self.type == ""single"":\n            pass\n\n        elif self.type == ""actor"":\n            # No learning, no loss function.\n            self.loss_function = None\n            # A Dict Splitter to split things from the EnvStepper.\n            self.env_output_splitter = ContainerSplitter(tuple_length=4, scope=""env-output-splitter"")\n\n            self.states_dict_splitter = None\n\n            # Slice some data from the EnvStepper (e.g only first internal states are needed).\n            self.internal_states_slicer = Slice(scope=""internal-states-slicer"", squeeze=True)\n            # Merge back to insert into FIFO.\n            self.fifo_input_merger = ContainerMerger(*self.fifo_queue_keys)\n\n            # Dummy Flattener to calculate action-probs space.\n            dummy_flattener = ReShape(flatten=True, flatten_categories=self.action_space.num_categories)\n            self.environment_stepper = EnvironmentStepper(\n                environment_spec=environment_spec,\n                actor_component_spec=ActorComponent(self.preprocessor, self.policy, self.exploration),\n                state_space=self.state_space.with_batch_rank(),\n                reward_space=float,  # TODO <- float64 for deepmind? may not work for other envs\n                internal_states_space=self.internal_states_space,\n                num_steps=self.worker_sample_size,\n                add_previous_action_to_state=True,\n                add_previous_reward_to_state=True,\n                add_action_probs=True,\n                action_probs_space=dummy_flattener.get_preprocessed_space(self.action_space)\n            )\n            sub_components = [\n                self.environment_stepper, self.env_output_splitter,\n                self.internal_states_slicer, self.fifo_input_merger,\n                self.fifo_queue\n            ]\n        # Learner.\n        else:\n            self.environment_stepper = None\n\n            # A Dict splitter to split up items from the queue.\n            self.fifo_input_merger = None\n            self.fifo_output_splitter = ContainerSplitter(*self.fifo_queue_keys, scope=""fifo-output-splitter"")\n            self.states_dict_splitter = ContainerSplitter(\n                *list(self.fifo_record_space[""states""].keys()), scope=""states-dict-splitter""\n            )\n            self.internal_states_slicer = None\n\n            self.transposer = Transpose(\n                scope=""transposer"",\n                device=dict(ops=""/job:learner/task:0/cpu"")\n            )\n            self.staging_area = StagingArea(num_data=len(self.fifo_queue_keys))\n\n            # Create an IMPALALossFunction with some parameters.\n            self.loss_function = IMPALALossFunction(\n                discount=self.discount, weight_pg=weight_pg, weight_baseline=weight_baseline,\n                weight_entropy=weight_entropy,\n                slice_actions=self.feed_previous_action_through_nn,\n                slice_rewards=self.feed_previous_reward_through_nn,\n                device=""/job:learner/task:0/gpu""\n            )\n\n            self.policy.propagate_sub_component_properties(\n                dict(device=dict(variables=""/job:learner/task:0/cpu"", ops=""/job:learner/task:0/gpu""))\n            )\n            for component in [self.staging_area, self.preprocessor, self.optimizer]:\n                component.propagate_sub_component_properties(\n                    dict(device=""/job:learner/task:0/gpu"")\n                )\n\n            sub_components = [\n                self.fifo_output_splitter, self.fifo_queue, self.states_dict_splitter,\n                self.transposer,\n                self.staging_area, self.preprocessor, self.policy,\n                self.loss_function, self.optimizer\n            ]\n\n        if self.type != ""single"":\n            # Add all the agent\'s sub-components to the root.\n            self.root_component.add_components(*sub_components)\n\n            # Define the Agent\'s (root Component\'s) API.\n            self.define_graph_api(*sub_components)\n\n        if self.type != ""single"" and self.auto_build:\n            if self.type == ""learner"":\n                build_options = dict(\n                    build_device_context=""/job:learner/task:0/cpu"",\n                    pin_global_variable_device=""/job:learner/task:0/cpu""\n                )\n                self._build_graph([self.root_component], self.input_spaces, optimizer=self.optimizer,\n                                  build_options=build_options)\n            else:\n                self._build_graph([self.root_component], self.input_spaces, optimizer=self.optimizer,\n                                  build_options=None)\n\n            self.graph_built = True\n\n            if self.has_gpu:\n                # Get 1st return op of API-method `stage` of sub-component `staging-area` (which is the stage-op).\n                self.stage_op = self.root_component.sub_components[""staging-area""].api_methods[""stage""]. \\\n                    out_op_columns[0].op_records[0].op\n                # Initialize the stage.\n                self.graph_executor.monitored_session.run_step_fn(\n                    lambda step_context: step_context.session.run(self.stage_op)\n                )\n\n                # TODO remove after full refactor.\n                self.dequeue_op = self.root_component.sub_components[""fifo-queue""].api_methods[""get_records""]. \\\n                    out_op_columns[0].op_records[0].op\n            if self.type == ""actor"":\n                self.enqueue_op = self.root_component.sub_components[""fifo-queue""].api_methods[""insert_records""]. \\\n                    out_op_columns[0].op_records[0].op\n\n    def define_graph_api(self, *sub_components):\n        # TODO: Unify agents with/w/o synchronizable policy.\n        # TODO: Unify Agents with/w/o get_action method (w/ env-stepper vs w/o).\n        #global_scope_base = ""environment-stepper/actor-component/"" if self.type == ""actor"" else """"\n        #super(IMPALAAgent, self).define_graph_api(\n        #    global_scope_base+""policy"",\n        #    global_scope_base+""dict-preprocessor-stack""\n        #)\n\n        # Assemble the specific agent.\n        if self.type == ""single"":\n            pass\n        elif self.type == ""actor"":\n            self.define_graph_api_actor(*sub_components)\n        else:\n            self.define_graph_api_learner(*sub_components)\n\n    def define_graph_api_actor(self, env_stepper, env_output_splitter, internal_states_slicer, merger, fifo_queue):\n        """"""\n        Defines the API-methods used by an IMPALA actor. Actors only step through an environment (n-steps at\n        a time), collect the results and push them into the FIFO queue. Results include: The actions actually\n        taken, the discounted accumulated returns for each action, the probability of each taken action according to\n        the behavior policy.\n\n        Args:\n            env_stepper (EnvironmentStepper): The EnvironmentStepper Component to setp through the Env n steps\n                in a single op call.\n\n            fifo_queue (FIFOQueue): The FIFOQueue Component used to enqueue env sample runs (n-step).\n        """"""\n        # Perform n-steps in the env and insert the results into our FIFO-queue.\n        @rlgraph_api(component=self.root_component)\n        def perform_n_steps_and_insert_into_fifo(root):\n            # Take n steps in the environment.\n            step_results = env_stepper.step()\n\n            split_output = env_output_splitter.call(step_results)\n            # Slice off the initial internal state (so the learner can re-feed-forward from that internal-state).\n            initial_internal_states = internal_states_slicer.slice(split_output[-1], 0)  # -1=internal states\n            to_merge = split_output[:-1] + (initial_internal_states,)\n            record = merger.merge(*to_merge)\n\n            # Insert results into the FIFOQueue.\n            insert_op = fifo_queue.insert_records(record)\n\n            return insert_op, split_output[0]  # 0=terminals\n\n    def define_graph_api_learner(\n            self, fifo_output_splitter, fifo_queue, states_dict_splitter,\n            transposer, staging_area, preprocessor, policy, loss_function, optimizer\n    ):\n        """"""\n        Defines the API-methods used by an IMPALA learner. Its job is basically: Pull a batch from the\n        FIFOQueue, split it up into its components and pass these through the loss function and into the optimizer for\n        a learning update.\n\n        Args:\n            fifo_output_splitter (ContainerSplitter): The ContainerSplitter Component to split up a batch from the queue\n                along its items.\n\n            fifo_queue (FIFOQueue): The FIFOQueue Component used to enqueue env sample runs (n-step).\n\n            states_dict_splitter (ContainerSplitter): The ContainerSplitter Component to split the state components\n                into its single parts.\n\n            transposer (Transpose): A space-agnostic Transpose to flip batch- and time ranks of all state-components.\n            staging_area (StagingArea): A possible GPU stating area component.\n\n            preprocessor (PreprocessorStack): A preprocessing Component for the states (may be a DictPreprocessorStack\n                as well).\n\n            policy (Policy): The Policy Component, which to update.\n            loss_function (IMPALALossFunction): The IMPALALossFunction Component.\n            optimizer (Optimizer): The optimizer that we use to calculate an update and apply it.\n        """"""\n        @rlgraph_api(component=self.root_component)\n        def get_queue_size(root):\n            return fifo_queue.get_size()\n\n        @rlgraph_api(component=self.root_component)\n        def update_from_memory(root, time_percentage=None):\n            # Pull n records from the queue.\n            # Note that everything will come out as batch-major and must be transposed before the main-LSTM.\n            # This is done by the network itself for all network inputs:\n            # - preprocessed_s\n            # - preprocessed_last_s_prime\n            # But must still be done for actions, rewards, terminals here in this API-method via separate ReShapers.\n            records = fifo_queue.get_records(self.update_spec[""batch_size""])\n\n            split_record = fifo_output_splitter.call(records)\n            actions = None\n            rewards = None\n            if self.feed_previous_action_through_nn and self.feed_previous_reward_through_nn:\n                terminals, states, action_probs_mu, initial_internal_states = split_record\n            else:\n                terminals, states, actions, rewards, action_probs_mu, initial_internal_states = split_record\n\n            # Flip everything to time-major.\n            # TODO: Create components that are less input-space sensitive (those that have no variables should\n            # TODO: be reused for any kind of processing)\n            states = transposer.call(states)\n            terminals = transposer.call(terminals)\n            action_probs_mu = transposer.call(action_probs_mu)\n            if self.feed_previous_action_through_nn is False:\n                actions = transposer.call(actions)\n            if self.feed_previous_reward_through_nn is False:\n                rewards = transposer.call(rewards)\n\n            # If we use a GPU: Put everything on staging area (adds 1 time step policy lag, but makes copying\n            # data into GPU more efficient).\n            if self.has_gpu:\n                stage_op = staging_area.stage(states, terminals, action_probs_mu, initial_internal_states)\n                # Get data from stage again and continue.\n                states, terminals, action_probs_mu, initial_internal_states = staging_area.unstage()\n            else:\n                # TODO: No-op component?\n                stage_op = None\n\n            # Preprocess actions and rewards inside the state (actions: flatten one-hot, rewards: expand).\n            preprocessed_states = preprocessor.preprocess(states)\n\n            # Only retrieve logits and do faster sparse softmax in loss.\n            out = policy.get_state_values_adapter_outputs_and_parameters(preprocessed_states, initial_internal_states)\n            state_values_pi = out[""state_values""]\n            logits = out[""adapter_outputs""]\n            #current_internal_states = out[""last_internal_states""]\n\n            # Isolate actions and rewards from states.\n            if self.feed_previous_action_through_nn or self.feed_previous_reward_through_nn:\n                states_split = states_dict_splitter.call(states)\n                actions = states_split[-2]\n                rewards = states_split[-1]\n\n            # Calculate the loss.\n            loss, loss_per_item = loss_function.loss(\n                logits, action_probs_mu, state_values_pi, actions, rewards, terminals\n            )\n            policy_vars = policy.variables()\n\n            # Pass vars and loss values into optimizer.\n            step_op = optimizer.step(policy_vars, loss, loss_per_item, time_percentage)\n            # Increase the global training step counter.\n            step_op = root._graph_fn_training_step(step_op)\n\n            # Return optimizer op and all loss values.\n            # TODO: Make it possible to return None from API-method without messing with the meta-graph.\n            return step_op, (stage_op if stage_op else step_op), loss, loss_per_item, records\n\n    # TODO: Unify with other Agents.\n    def get_action(self, states, internals=None, use_exploration=True, apply_preprocessing=True, extra_returns=None,\n                   time_percentage=None):\n        pass\n\n    def _observe_graph(self, preprocessed_states, actions, internals, rewards, terminals):\n        self.graph_executor.execute((""insert_records"", [preprocessed_states, actions, rewards, terminals]))\n\n    def update(self, batch=None, time_percentage=None):\n        if batch is None:\n            # Include stage_op or not?\n            if self.has_gpu:\n                return self.graph_executor.execute(""update_from_memory"")\n            else:\n                return self.graph_executor.execute((""update_from_memory"", None, ([0, 2, 3, 4])))\n        else:\n            raise RLGraphError(""Cannot call update-from-batch on an IMPALA Agent."")\n\n    def __repr__(self):\n        return ""IMPALAAgent(type={})"".format(self.type)\n\n\nclass SingleIMPALAAgent(IMPALAAgent):\n    """"""\n    An single IMPALAAgent, performing both experience collection and learning updates via multi-threading\n    (queue runners).\n    """"""\n    def __init__(self, discount=0.99, fifo_queue_spec=None, architecture=""large"", environment_spec=None,\n                 feed_previous_action_through_nn=True, feed_previous_reward_through_nn=True,\n                 weight_pg=None, weight_baseline=None, weight_entropy=None,\n                 num_workers=1, worker_sample_size=100,\n                 dynamic_batching=False, visualize=False, **kwargs):\n        """"""\n        Args:\n            discount (float): The discount factor gamma.\n            architecture (str): Which IMPALA architecture to use. One of ""small"" or ""large"". Will be ignored if\n                `network_spec` is given explicitly in kwargs. Default: ""large"".\n            fifo_queue_spec (Optional[dict,FIFOQueue]): The spec for the FIFOQueue to use for the IMPALA algorithm.\n            environment_spec (dict): The spec for constructing an Environment object for an actor-type IMPALA agent.\n            feed_previous_action_through_nn (bool): Whether to add the previous action as another input channel to the\n                ActionComponent\'s (NN\'s) input at each step. This is only possible if the state space is already a Dict.\n                It will be added under the key ""previous_action"". Default: True.\n            feed_previous_reward_through_nn (bool): Whether to add the previous reward as another input channel to the\n                ActionComponent\'s (NN\'s) input at each step. This is only possible if the state space is already a Dict.\n                It will be added under the key ""previous_reward"". Default: True.\n            weight_pg (float): See IMPALALossFunction Component.\n            weight_baseline (float): See IMPALALossFunction Component.\n            weight_entropy (float): See IMPALALossFunction Component.\n            num_workers (int): How many actors (workers) should be run in separate threads.\n            worker_sample_size (int): How many steps the actor will perform in the environment each sample-run.\n            dynamic_batching (bool): Whether to use the deepmind\'s custom dynamic batching op for wrapping the\n                optimizer\'s step call. The batcher.so file must be compiled for this to work (see Docker file).\n                Default: False.\n            visualize (Union[int,bool]): Whether and how many workers to visualize.\n                Default: False (no visualization).\n        """"""\n        # Now that we fixed the Agent\'s spec, call the super constructor.\n        super(SingleIMPALAAgent, self).__init__(\n            type=""single"",\n            discount=discount,\n            architecture=architecture,\n            fifo_queue_spec=fifo_queue_spec,\n            environment_spec=environment_spec,\n            feed_previous_action_through_nn=feed_previous_action_through_nn,\n            feed_previous_reward_through_nn=feed_previous_reward_through_nn,\n            weight_pg=weight_pg,\n            weight_baseline=weight_baseline,\n            weight_entropy=weight_entropy,\n            worker_sample_size=worker_sample_size,\n            name=kwargs.pop(""name"", ""impala-single-agent""),\n            **kwargs\n        )\n        self.dynamic_batching = dynamic_batching\n        self.num_workers = num_workers\n        self.visualize = visualize\n\n        # If we use dynamic batching, wrap the dynamic batcher around the policy\'s graph_fn that we\n        # actually call below during our build.\n        if self.dynamic_batching:\n            self.policy = DynamicBatchingPolicy(policy_spec=self.policy, scope="""")\n\n        self.env_output_splitter = ContainerSplitter(\n            tuple_length=3 if self.has_rnn is False else 4, scope=""env-output-splitter""\n        )\n        self.fifo_output_splitter = ContainerSplitter(*self.fifo_queue_keys, scope=""fifo-output-splitter"")\n        self.states_dict_splitter = ContainerSplitter(\n            *list(self.fifo_record_space[""states""].keys() if isinstance(self.state_space, Dict) else ""dummy""),\n            scope=""states-dict-splitter""\n        )\n\n        self.staging_area = StagingArea(num_data=len(self.fifo_queue_keys))\n\n        # Slice some data from the EnvStepper (e.g only first internal states are needed).\n        if self.has_rnn:\n            internal_states_slicer = Slice(scope=""internal-states-slicer"", squeeze=True)\n        else:\n            internal_states_slicer = None\n\n        self.transposer = Transpose(scope=""transposer"")\n\n        # Create an IMPALALossFunction with some parameters.\n        self.loss_function = IMPALALossFunction(\n            discount=self.discount, weight_pg=weight_pg, weight_baseline=weight_baseline,\n            weight_entropy=weight_entropy, slice_actions=self.feed_previous_action_through_nn,\n            slice_rewards=self.feed_previous_reward_through_nn\n        )\n\n        # Merge back to insert into FIFO.\n        self.fifo_input_merger = ContainerMerger(*self.fifo_queue_keys)\n\n        # Dummy Flattener to calculate action-probs space.\n        dummy_flattener = ReShape(flatten=True, flatten_categories=self.action_space.num_categories)\n\n        self.environment_steppers = list()\n        for i in range(self.num_workers):\n            environment_spec_ = copy.deepcopy(environment_spec)\n            if self.visualize is True or (isinstance(self.visualize, int) and i+1 <= self.visualize):\n                environment_spec_[""visualize""] = True\n\n            # Force worker_sample_size for IMPALA NNs (LSTM) in env-stepper to be 1.\n            policy_spec = copy.deepcopy(self.policy_spec)\n            if isinstance(policy_spec, dict) and isinstance(policy_spec[""network_spec""], dict) and \\\n                    ""type"" in policy_spec[""network_spec""] and ""IMPALANetwork"" in policy_spec[""network_spec""][""type""]:\n                policy_spec[""network_spec""][""worker_sample_size""] = 1\n\n            env_stepper = EnvironmentStepper(\n                environment_spec=environment_spec_,\n                actor_component_spec=ActorComponent(\n                    preprocessor_spec=self.preprocessing_spec,\n                    policy_spec=policy_spec,\n                    exploration_spec=self.exploration_spec\n                ),\n                state_space=self.state_space.with_batch_rank(),\n                action_space=self.action_space.with_batch_rank(),\n                reward_space=float,\n                internal_states_space=self.internal_states_space,\n                num_steps=self.worker_sample_size,\n                add_action=not self.feed_previous_action_through_nn,\n                add_reward=not self.feed_previous_reward_through_nn,\n                add_previous_action_to_state=self.feed_previous_action_through_nn,\n                add_previous_reward_to_state=self.feed_previous_reward_through_nn,\n                add_action_probs=True,\n                action_probs_space=dummy_flattener.get_preprocessed_space(self.action_space),\n                scope=""env-stepper-{}"".format(i)\n            )\n            if self.dynamic_batching:\n                env_stepper.actor_component.policy.parent_component = None\n                env_stepper.actor_component.policy = DynamicBatchingPolicy(\n                    policy_spec=env_stepper.actor_component.policy, scope="""")\n                env_stepper.actor_component.add_components(env_stepper.actor_component.policy)\n\n            self.environment_steppers.append(env_stepper)\n\n        # Create the QueueRunners (one for each env-stepper).\n        self.queue_runner = QueueRunner(\n            self.fifo_queue, ""step"", -1,  # -1: Take entire return value of API-method `step` as record to insert.\n            self.env_output_splitter,\n            self.fifo_input_merger,\n            internal_states_slicer,\n            *self.environment_steppers\n        )\n\n        sub_components = [\n            self.fifo_output_splitter, self.fifo_queue, self.queue_runner,\n            self.transposer,\n            self.staging_area, self.preprocessor, self.states_dict_splitter,\n            self.policy, self.loss_function, self.optimizer\n        ]\n\n        # Add all the agent\'s sub-components to the root.\n        self.root_component.add_components(*sub_components)\n\n        # Define the Agent\'s (root Component\'s) API.\n        self.define_graph_api()\n\n        if self.auto_build:\n            self._build_graph([self.root_component], self.input_spaces, optimizer=self.optimizer,\n                              build_options=None)\n            self.graph_built = True\n\n            if self.has_gpu:\n                # Get 1st return op of API-method `stage` of sub-component `staging-area` (which is the stage-op).\n                self.stage_op = self.root_component.sub_components[""staging-area""].api_methods[""stage""]. \\\n                    out_op_columns[0].op_records[0].op\n                # Initialize the stage.\n                self.graph_executor.monitored_session.run_step_fn(\n                    lambda step_context: step_context.session.run(self.stage_op)\n                )\n                # TODO remove after full refactor.\n                self.dequeue_op = self.root_component.sub_components[""fifo-queue""].api_methods[""get_records""]. \\\n                    out_op_columns[0].op_records[0].op\n\n    def define_graph_api(self):\n        agent = self\n\n        @rlgraph_api(component=self.root_component)\n        def setup_queue_runner(root):\n            return agent.queue_runner.setup()\n\n        @rlgraph_api(component=self.root_component)\n        def get_queue_size(root):\n            return agent.fifo_queue.get_size()\n\n        @rlgraph_api(component=self.root_component)\n        def update_from_memory(root, time_percentage=None):\n            # Pull n records from the queue.\n            # Note that everything will come out as batch-major and must be transposed before the main-LSTM.\n            # This is done by the network itself for all network inputs:\n            # - preprocessed_s\n            # - preprocessed_last_s_prime\n            # But must still be done for actions, rewards, terminals here in this API-method via separate ReShapers.\n            records = agent.fifo_queue.get_records(self.update_spec[""batch_size""])\n\n            out = agent.fifo_output_splitter.split_into_dict(records)\n            terminals = out[""terminals""]\n            states = out[""states""]\n            action_probs_mu = out[""action_probs""]\n            initial_internal_states = None\n            if self.has_rnn:\n                initial_internal_states = out[""initial_internal_states""]\n\n            # Flip everything to time-major.\n            # TODO: Create components that are less input-space sensitive (those that have no variables should\n            # TODO: be reused for any kind of processing: already done, use space_agnostic feature. See ReShape)\n            states = agent.transposer.call(states)\n            terminals = agent.transposer.call(terminals)\n            action_probs_mu = agent.transposer.call(action_probs_mu)\n            actions = None\n            if not self.feed_previous_action_through_nn:\n                actions = agent.transposer.call(out[""actions""])\n            rewards = None\n            if not self.feed_previous_reward_through_nn:\n                rewards = agent.transposer.call(out[""rewards""])\n\n            # If we use a GPU: Put everything on staging area (adds 1 time step policy lag, but makes copying\n            # data into GPU more efficient).\n            if self.has_gpu:\n                if self.has_rnn:\n                    stage_op = agent.staging_area.stage(states, terminals, action_probs_mu, initial_internal_states)\n                    states, terminals, action_probs_mu, initial_internal_states = agent.staging_area.unstage()\n                else:\n                    stage_op = agent.staging_area.stage(states, terminals, action_probs_mu)\n                    states, terminals, action_probs_mu = agent.staging_area.unstage()\n            else:\n                # TODO: No-op component?\n                stage_op = None\n\n            # Preprocess actions and rewards inside the state (actions: flatten one-hot, rewards: expand).\n            if agent.preprocessing_required:\n                states = agent.preprocessor.preprocess(states)\n\n            # Get the pi-action probs AND the values for all our states.\n            out = agent.policy.get_state_values_adapter_outputs_and_parameters(states, initial_internal_states)\n            state_values_pi = out[""state_values""]\n            logits_pi = out[""adapter_outputs""]\n\n            # Isolate actions and rewards from states.\n            # TODO: What if only one of actions or rewards is fed through NN, but the other not?\n            if self.feed_previous_reward_through_nn and self.feed_previous_action_through_nn:\n                out = agent.states_dict_splitter.call(states)\n                actions = out[-2]  # TODO: Are these always the correct slots for ""previous_action"" and ""previous_reward""?\n                rewards = out[-1]\n\n            # Calculate the loss.\n            loss, loss_per_item = agent.loss_function.loss(\n                logits_pi, action_probs_mu, state_values_pi, actions, rewards, terminals\n            )\n            if self.dynamic_batching:\n                policy_vars = agent.queue_runner.data_producing_components[0].actor_component.policy.variables()\n            else:\n                policy_vars = agent.policy.variables()\n\n            # Pass vars and loss values into optimizer.\n            step_op = agent.optimizer.step(policy_vars, loss, loss_per_item, time_percentage)\n            # Increase the global training step counter.\n            step_op = root._graph_fn_training_step(step_op)\n\n            # Return optimizer op and all loss values.\n            # TODO: Make it possible to return None from API-method without messing with the meta-graph.\n            return step_op, (stage_op if stage_op else step_op), loss, loss_per_item, records\n\n        # TODO: Move this into generic AgentRootComponent.\n        @graph_fn(component=self.root_component)\n        def _graph_fn_training_step(root, other_step_op=None):\n            add_op = tf.assign_add(self.graph_executor.global_training_timestep, 1)\n            op_list = [add_op] + [other_step_op] if other_step_op is not None else []\n            with tf.control_dependencies(op_list):\n                return tf.no_op() if other_step_op is None else other_step_op\n\n    def __repr__(self):\n        return ""SingleIMPALAAgent()""\n'"
rlgraph/agents/ppo_agent.py,25,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.agents import Agent\nfrom rlgraph.components import Memory, RingBuffer, PPOLossFunction\nfrom rlgraph.components.helpers import GeneralizedAdvantageEstimation\nfrom rlgraph.spaces import BoolBox, FloatBox\nfrom rlgraph.utils import util\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.define_by_run_ops import define_by_run_flatten\nfrom rlgraph.utils.ops import flatten_op, DataOpDict, DataOp\nfrom rlgraph.utils.util import strip_list\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n    setattr(tf.Tensor, ""map"", DataOp.map)\nif get_backend() == ""pytorch"":\n    import torch\n\n\nclass PPOAgent(Agent):\n    """"""\n    Proximal policy optimization is a variant of policy optimization in which\n    the likelihood ratio between updated and prior policy is constrained by clipping, and\n    where updates are performed via repeated sub-sampling of the input batch.\n\n    Paper: https://arxiv.org/abs/1707.06347\n    """"""\n\n    def __init__(\n        self,\n        state_space,\n        action_space,\n        discount=0.98,\n        preprocessing_spec=None,\n        network_spec=None,\n        internal_states_space=None,\n        policy_spec=None,\n        value_function_spec=None,\n        execution_spec=None,\n        optimizer_spec=None,\n        value_function_optimizer_spec=None,\n        observe_spec=None,\n        update_spec=None,\n        summary_spec=None,\n        saver_spec=None,\n        auto_build=True,\n        name=""ppo-agent"",\n        clip_ratio=0.2,\n        gae_lambda=1.0,\n        clip_rewards=0.0,\n        value_function_clipping=None,\n        standardize_advantages=False,\n        sample_episodes=True,\n        weight_entropy=None,\n        memory_spec=None\n    ):\n        """"""\n        Args:\n            state_space (Union[dict,Space]): Spec dict for the state Space or a direct Space object.\n            action_space (Union[dict,Space]): Spec dict for the action Space or a direct Space object.\n\n            preprocessing_spec (Optional[list,PreprocessorStack]): The spec list for the different necessary states\n                preprocessing steps or a PreprocessorStack object itself.\n\n            discount (float): The discount factor (gamma).\n\n            network_spec (Optional[list,NeuralNetwork]): Spec list for a NeuralNetwork Component or the NeuralNetwork\n                object itself.\n\n            internal_states_space (Optional[Union[dict,Space]]): Spec dict for the internal-states Space or a direct\n                Space object for the Space(s) of the internal (RNN) states.\n\n            policy_spec (Optional[dict]): An optional dict for further kwargs passing into the Policy c\'tor.\n\n            value_function_spec (list, dict, ValueFunction): Neural network specification for baseline or instance\n                of ValueFunction.\n\n            execution_spec (Optional[dict,Execution]): The spec-dict specifying execution settings.\n            optimizer_spec (Optional[dict,Optimizer]): The spec-dict to create the Optimizer for this Agent.\n\n            value_function_optimizer_spec (dict): Optimizer config for value function optimizer. If None, the optimizer\n                spec for the policy is used (same learning rate and optimizer type).\n\n            observe_spec (Optional[dict]): Spec-dict to specify `Agent.observe()` settings.\n            update_spec (Optional[dict]): Spec-dict to specify `Agent.update()` settings.\n            summary_spec (Optional[dict]): Spec-dict to specify summary settings.\n            saver_spec (Optional[dict]): Spec-dict to specify saver settings.\n\n            auto_build (Optional[bool]): If True (default), immediately builds the graph using the agent\'s\n                graph builder. If false, users must separately call agent.build(). Useful for debugging or analyzing\n                components before building.\n\n            name (str): Some name for this Agent object.\n            clip_ratio (float): Clipping parameter for importance sampling (IS) likelihood ratio.\n            gae_lambda (float): Lambda for generalized advantage estimation.\n\n            clip_rewards (float): Reward clipping value. If not 0, rewards will be clipped within a +/- `clip_rewards`\n                range.\n\n            value_function_clipping (Optional[float]): If not None, uses clipped value function objective. If None,\n                uses simple value function objective.\n\n            standardize_advantages (bool): If true, standardize advantage values in update.\n\n            sample_episodes (bool): If True, the update method interprets the batch_size as the number of\n                episodes to fetch from the memory. If False, batch_size will refer to the number of time-steps. This\n                is especially relevant for environments where episode lengths may vastly differ throughout training. For\n                example, in CartPole, a losing episode is typically 10 steps, and a winning episode 200 steps.\n\n            weight_entropy (float): The coefficient used for the entropy regularization term (L[E]).\n\n            memory_spec (Optional[dict,Memory]): The spec for the Memory to use. Should typically be\n                a ring-buffer.\n        """"""\n        if policy_spec is not None:\n            policy_spec[""deterministic""] = False\n        else:\n            policy_spec = dict(deterministic=False)\n        super(PPOAgent, self).__init__(\n            state_space=state_space,\n            action_space=action_space,\n            discount=discount,\n            preprocessing_spec=preprocessing_spec,\n            network_spec=network_spec,\n            internal_states_space=internal_states_space,\n            policy_spec=policy_spec,\n            value_function_spec=value_function_spec,\n            execution_spec=execution_spec,\n            optimizer_spec=optimizer_spec,\n            value_function_optimizer_spec=value_function_optimizer_spec,\n            observe_spec=observe_spec,\n            update_spec=update_spec,\n            summary_spec=summary_spec,\n            saver_spec=saver_spec,\n            name=name,\n            auto_build=auto_build\n        )\n        self.sample_episodes = sample_episodes\n\n        # TODO: Have to manually set it here for multi-GPU synchronizer to know its number\n        # TODO: of return values when calling _graph_fn_calculate_update_from_external_batch.\n        # self.root_component.graph_fn_num_outputs[""_graph_fn_update_from_external_batch""] = 4\n\n        # Extend input Space definitions to this Agent\'s specific API-methods.\n        preprocessed_state_space = self.preprocessed_state_space.with_batch_rank()\n        reward_space = FloatBox(add_batch_rank=True)\n        terminal_space = BoolBox(add_batch_rank=True)\n\n        self.input_spaces.update(dict(\n            actions=self.action_space.with_batch_rank(),\n            policy_weights=""variables:policy"",\n            value_function_weights=""variables:value-function"",\n            deterministic=bool,\n            preprocessed_states=preprocessed_state_space,\n            rewards=reward_space,\n            terminals=terminal_space,\n            sequence_indices=BoolBox(add_batch_rank=True),\n            apply_postprocessing=bool,\n            num_records=int\n        ))\n\n        self.memory = Memory.from_spec(memory_spec)\n        assert isinstance(self.memory, RingBuffer), ""ERROR: PPO memory must be ring-buffer for episode-handling!""\n\n        # Make sure the python buffer is not larger than our memory capacity.\n        assert self.observe_spec[""buffer_size""] <= self.memory.capacity, \\\n            ""ERROR: Buffer\'s size ({}) in `observe_spec` must be smaller or equal to the memory\'s capacity ({})!"". \\\n            format(self.observe_spec[""buffer_size""], self.memory.capacity)\n\n        # The splitter for splitting up the records coming from the memory.\n        self.standardize_advantages = standardize_advantages\n        self.gae_function = GeneralizedAdvantageEstimation(\n            gae_lambda=gae_lambda, discount=self.discount, clip_rewards=clip_rewards\n        )\n        self.loss_function = PPOLossFunction(\n            clip_ratio=clip_ratio, value_function_clipping=value_function_clipping, weight_entropy=weight_entropy\n        )\n\n        self.iterations = self.update_spec[""num_iterations""]\n        self.sample_size = self.update_spec[""sample_size""]\n        self.batch_size = self.update_spec[""batch_size""]\n\n        # Add all our sub-components to the core.\n        self.root_component.add_components(\n            self.preprocessor, self.memory,\n            self.policy, self.exploration,\n            self.loss_function, self.optimizer, self.value_function, self.value_function_optimizer, self.vars_merger,\n            self.vars_splitter, self.gae_function\n        )\n        # Define the Agent\'s (root-Component\'s) API.\n        self.define_graph_api()\n        self.build_options = dict(vf_optimizer=self.value_function_optimizer)\n\n        if self.auto_build:\n            self._build_graph(\n                [self.root_component], self.input_spaces, optimizer=self.optimizer,\n                # Important: Use sample-size, not batch-size as the sub-samples (from a batch) are the ones that get\n                # multi-gpu-split.\n                batch_size=self.update_spec[""sample_size""],\n                build_options=self.build_options\n            )\n            self.graph_built = True\n\n    def define_graph_api(self):\n        super(PPOAgent, self).define_graph_api()\n\n        agent = self\n\n        # Reset operation (resets preprocessor).\n        if self.preprocessing_required:\n            @rlgraph_api(component=self.root_component)\n            def reset_preprocessor(root):\n                reset_op = agent.preprocessor.reset()\n                return reset_op\n\n        # Act from preprocessed states.\n        @rlgraph_api(component=self.root_component)\n        def action_from_preprocessed_state(root, preprocessed_states, deterministic=False):\n            out = agent.policy.get_action(preprocessed_states, deterministic=deterministic)\n            return out[""action""], preprocessed_states  # , out[""nn_outputs""], out[""adapter_outputs""], out[""parameters""], out[""action_probabilities""], out[""log_probs""]\n\n        # State (from environment) to action with preprocessing.\n        @rlgraph_api(component=self.root_component)\n        def get_preprocessed_state_and_action(root, states, deterministic=False):\n            preprocessed_states = agent.preprocessor.preprocess(states)\n            return root.action_from_preprocessed_state(preprocessed_states, deterministic)\n\n        # Insert into memory.\n        @rlgraph_api(component=self.root_component)\n        def insert_records(root, preprocessed_states, actions, rewards, terminals):\n            # Standardize rewards before pushing them into the memory.\n            #if root.standardize_rewards is not None:\n            #    rewards = self.standardize_rewards.call(rewards)\n            records = dict(states=preprocessed_states, actions=actions, rewards=rewards, terminals=terminals)\n            return agent.memory.insert_records(records)\n\n        @rlgraph_api(component=self.root_component)\n        def post_process(root, preprocessed_states, rewards, terminals, sequence_indices):\n            baseline_values = agent.value_function.value_output(preprocessed_states)\n            pg_advantages = agent.gae_function.calc_gae_values(baseline_values, rewards, terminals, sequence_indices)\n            return pg_advantages\n\n        # Learn from memory.\n        @rlgraph_api(component=self.root_component)\n        def update_from_memory(root, apply_postprocessing=True, time_percentage=None):\n            if agent.sample_episodes:\n                records = agent.memory.get_episodes(self.update_spec[""batch_size""])\n            else:\n                records = agent.memory.get_records(self.update_spec[""batch_size""])\n\n            # Route to post process and update method.\n            sequence_indices = records[""terminals""]  # TODO: return correct seq-indices automatically from mem\n            return root.update_from_external_batch(\n                records[""states""], records[""actions""], records[""rewards""], records[""terminals""],\n                sequence_indices, apply_postprocessing, time_percentage\n            )\n\n        @rlgraph_api(component=self.root_component)\n        def get_records(root, num_records=1):\n            return agent.memory.get_records(num_records)\n\n        # N.b. this is here because the iterative_optimization would need policy/losses as sub-components, but\n        # multiple parents are not allowed currently.\n        @rlgraph_api(component=self.root_component)\n        def _graph_fn_update_from_external_batch(\n                root, preprocessed_states, actions, rewards, terminals, sequence_indices, apply_postprocessing=True,\n                time_percentage=None\n        ):\n            """"""\n            Calls iterative optimization by repeatedly sub-sampling.\n            """"""\n            multi_gpu_sync_optimizer = root.sub_components.get(""multi-gpu-synchronizer"")\n\n            # Return values.\n            loss, loss_per_item, vf_loss, vf_loss_per_item = None, None, None, None\n\n            policy = root.get_sub_component_by_name(agent.policy.scope)\n            value_function = root.get_sub_component_by_name(agent.value_function.scope)\n            optimizer = root.get_sub_component_by_name(agent.optimizer.scope)\n            loss_function = root.get_sub_component_by_name(agent.loss_function.scope)\n            value_function_optimizer = root.get_sub_component_by_name(agent.value_function_optimizer.scope)\n            vars_merger = root.get_sub_component_by_name(agent.vars_merger.scope)\n            gae_function = root.get_sub_component_by_name(agent.gae_function.scope)\n\n            prev_log_probs = policy.get_log_likelihood(preprocessed_states, actions)[""log_likelihood""]\n            prev_state_values = value_function.value_output(preprocessed_states)\n\n            if get_backend() == ""tf"":\n                batch_size = tf.shape(list(flatten_op(preprocessed_states).values())[0])[0]\n\n                # Log probs before update (stop-gradient as these are used in target term).\n                prev_log_probs = tf.stop_gradient(prev_log_probs)\n                # State values before update (stop-gradient as these are used in target term).\n                prev_state_values = tf.stop_gradient(prev_state_values)\n\n                # Advantages are based on previous state values.\n                advantages = tf.cond(\n                    pred=apply_postprocessing,\n                    true_fn=lambda: gae_function.calc_gae_values(\n                        prev_state_values, rewards, terminals, sequence_indices\n                    ),\n                    false_fn=lambda: rewards\n                )\n                if self.standardize_advantages:\n                    mean, std = tf.nn.moments(x=advantages, axes=[0])\n                    advantages = (advantages - mean) / std\n\n                def opt_body(index_, loss_, loss_per_item_, vf_loss_, vf_loss_per_item_):\n                    start = tf.random_uniform(shape=(), minval=0, maxval=batch_size, dtype=tf.int32)\n                    indices = tf.range(start=start, limit=start + agent.sample_size) % batch_size\n\n                    # Use `map` here in case we have container states/actions.\n                    sample_states = preprocessed_states.map(lambda k, v: tf.gather(v, indices))\n                    sample_actions = actions.map(lambda k, v: tf.gather(v, indices))\n                    sample_prev_log_probs = tf.gather(params=prev_log_probs, indices=indices)\n                    sample_rewards = tf.gather(params=rewards, indices=indices)\n                    sample_terminals = tf.gather(params=terminals, indices=indices)\n                    sample_sequence_indices = tf.gather(params=sequence_indices, indices=indices)\n                    sample_advantages = tf.gather(params=advantages, indices=indices)\n                    sample_advantages.set_shape((agent.sample_size,))\n\n                    sample_state_values = value_function.value_output(sample_states)\n                    sample_prev_state_values = tf.gather(params=prev_state_values, indices=indices)\n\n                    # If we are a multi-GPU root:\n                    # Simply feeds everything into the multi-GPU sync optimizer\'s method and return.\n                    if multi_gpu_sync_optimizer is not None:\n                        main_policy_vars = agent.policy.variables()\n                        main_vf_vars = agent.value_function.variables()\n                        all_vars = agent.vars_merger.merge(main_policy_vars, main_vf_vars)\n                        # grads_and_vars, loss, loss_per_item, vf_loss, vf_loss_per_item = \\\n                        out = multi_gpu_sync_optimizer.calculate_update_from_external_batch(\n                            all_vars,\n                            sample_states, sample_actions, sample_rewards, sample_terminals, sample_sequence_indices,\n                            apply_postprocessing=apply_postprocessing\n                        )\n                        avg_grads_and_vars_policy, avg_grads_and_vars_vf = agent.vars_splitter.call(\n                            out[""avg_grads_and_vars_by_component""]\n                        )\n                        policy_step_op = agent.optimizer.apply_gradients(avg_grads_and_vars_policy)\n                        vf_step_op = agent.value_function_optimizer.apply_gradients(avg_grads_and_vars_vf)\n                        step_op = root._graph_fn_group(policy_step_op, vf_step_op)\n                        step_and_sync_op = multi_gpu_sync_optimizer.sync_variables_to_towers(\n                            step_op, all_vars\n                        )\n                        loss_vf, loss_per_item_vf = out[""additional_return_0""], out[""additional_return_1""]\n\n                        # Have to set all shapes here due to strict loop-var shape requirements.\n                        out[""loss""].set_shape(())\n                        loss_vf.set_shape(())\n                        loss_per_item_vf.set_shape((agent.sample_size,))\n                        out[""loss_per_item""].set_shape((agent.sample_size,))\n\n                        with tf.control_dependencies([step_and_sync_op]):\n                            if index_ == 0:\n                                # Increase the global training step counter.\n                                out[""loss""] = root._graph_fn_training_step(out[""loss""])\n                            return index_ + 1, out[""loss""], out[""loss_per_item""], loss_vf, loss_per_item_vf\n\n                    sample_log_probs = policy.get_log_likelihood(sample_states, sample_actions)[""log_likelihood""]\n                    entropy = policy.get_entropy(sample_states)[""entropy""]\n\n                    loss, loss_per_item, vf_loss, vf_loss_per_item = \\\n                        loss_function.loss(\n                            sample_log_probs, sample_prev_log_probs,\n                            sample_state_values, sample_prev_state_values, sample_advantages, entropy, time_percentage\n                        )\n\n                    if hasattr(root, ""is_multi_gpu_tower"") and root.is_multi_gpu_tower is True:\n                        policy_grads_and_vars = optimizer.calculate_gradients(policy.variables(), loss, time_percentage)\n                        vf_grads_and_vars = value_function_optimizer.calculate_gradients(\n                            value_function.variables(), vf_loss, time_percentage\n                        )\n                        grads_and_vars_by_component = vars_merger.merge(policy_grads_and_vars, vf_grads_and_vars)\n                        return grads_and_vars_by_component, loss, loss_per_item, vf_loss, vf_loss_per_item\n                    else:\n                        step_op = optimizer.step(policy.variables(), loss, loss_per_item, time_percentage)\n                        loss.set_shape(())\n                        loss_per_item.set_shape((agent.sample_size,))\n\n                        vf_step_op = value_function_optimizer.step(\n                            value_function.variables(), vf_loss, vf_loss_per_item, time_percentage\n                        )\n                        vf_loss.set_shape(())\n                        vf_loss_per_item.set_shape((agent.sample_size,))\n\n                        with tf.control_dependencies([step_op, vf_step_op]):\n                            return index_ + 1, loss, loss_per_item, vf_loss, vf_loss_per_item\n\n                def cond(index_, loss_, loss_per_item_, v_loss_, v_loss_per_item_):\n                    return index_ < agent.iterations\n\n                init_loop_vars = [\n                    0,\n                    tf.zeros(shape=(), dtype=tf.float32),\n                    tf.zeros(shape=(agent.sample_size,)),\n                    tf.zeros(shape=(), dtype=tf.float32),\n                    tf.zeros(shape=(agent.sample_size,))\n                ]\n\n                if hasattr(root, ""is_multi_gpu_tower"") and root.is_multi_gpu_tower is True:\n                    return opt_body(*init_loop_vars)\n                else:\n                    index, loss, loss_per_item, vf_loss, vf_loss_per_item = tf.while_loop(\n                        cond=cond,\n                        body=opt_body,\n                        loop_vars=init_loop_vars,\n                        parallel_iterations=1\n                    )\n                    root.register_summary_op(tf.summary.scalar(""losses/policy_loss"", loss))\n                    root.register_summary_op(tf.summary.scalar(""losses/vf_loss"", vf_loss))\n                    # Increase the global training step counter.\n                    loss = root._graph_fn_training_step(loss)\n                    return loss, loss_per_item, vf_loss, vf_loss_per_item\n\n            elif get_backend() == ""pytorch"":\n                batch_size = list(flatten_op(preprocessed_states).values())[0].shape[0]\n                sample_size = min(batch_size, agent.sample_size)\n\n                if isinstance(prev_log_probs, dict):\n                    for name in actions.keys():\n                        prev_log_probs[name] = prev_log_probs[name].detach()\n                else:\n                    prev_log_probs = prev_log_probs.detach()\n                prev_state_values = value_function.value_output(preprocessed_states).detach()\n                if apply_postprocessing:\n                    advantages = gae_function.calc_gae_values(prev_state_values, rewards, terminals, sequence_indices)\n                else:\n                    advantages = rewards\n                if self.standardize_advantages:\n                    std = torch.std(advantages)\n                    if not np.isnan(std):\n                        advantages = (advantages - torch.mean(advantages)) / std\n\n                for _ in range(agent.iterations):\n                    start = int(torch.rand(1) * (batch_size - 1))\n                    indices = torch.arange(start=start, end=start + sample_size, dtype=torch.long) % batch_size\n                    sample_states = torch.index_select(preprocessed_states, 0, indices)\n\n                    if isinstance(actions, dict):\n                        sample_actions = DataOpDict()\n                        sample_prev_log_probs = DataOpDict()\n                        for name, action in define_by_run_flatten(actions, scope_separator_at_start=False).items():\n                            sample_actions[name] = torch.index_select(action, 0, indices)\n                            sample_prev_log_probs[name] = torch.index_select(prev_log_probs[name], 0, indices)\n                    else:\n                        sample_actions = torch.index_select(actions, 0, indices)\n                        sample_prev_log_probs = torch.index_select(prev_log_probs, 0, indices)\n\n                    sample_advantages = torch.index_select(advantages, 0, indices)\n                    sample_prev_state_values = torch.index_select(prev_state_values, 0, indices)\n\n                    sample_log_probs = policy.get_log_likelihood(sample_states, sample_actions)[""log_likelihood""]\n                    sample_state_values = value_function.value_output(sample_states)\n\n                    entropy = policy.get_entropy(sample_states)[""entropy""]\n                    loss, loss_per_item, vf_loss, vf_loss_per_item = loss_function.loss(\n                        sample_log_probs, sample_prev_log_probs,\n                        sample_state_values, sample_prev_state_values, sample_advantages, entropy, time_percentage\n                    )\n\n                    # Do not need step op.\n                    optimizer.step(policy.variables(), loss, loss_per_item, time_percentage)\n                    value_function_optimizer.step(value_function.variables(), vf_loss, vf_loss_per_item, time_percentage)\n                return loss, loss_per_item, vf_loss, vf_loss_per_item\n\n    def get_action(self, states, internals=None, use_exploration=True, apply_preprocessing=True, extra_returns=None,\n                   time_percentage=None):\n        """"""\n        Args:\n            extra_returns (Optional[Set[str],str]): Optional string or set of strings for additional return\n                values (besides the actions). Possible values are:\n                - \'preprocessed_states\': The preprocessed states after passing the given states through the\n                preprocessor stack.\n                - \'internal_states\': The internal states returned by the RNNs in the NN pipeline.\n                - \'used_exploration\': Whether epsilon- or noise-based exploration was used or not.\n\n        Returns:\n            tuple or single value depending on `extra_returns`:\n                - action\n                - the preprocessed states\n        """"""\n        extra_returns = {extra_returns} if isinstance(extra_returns, str) else (extra_returns or set())\n        # States come in without preprocessing -> use state space.\n        if apply_preprocessing:\n            call_method = ""get_preprocessed_state_and_action""\n            batched_states, remove_batch_rank = self.state_space.force_batch(states, horizontal=False)\n        # States are already pre-processed (and therefore also batched).\n        else:\n            call_method = ""action_from_preprocessed_state""\n            batched_states = states\n            remove_batch_rank = False\n\n        # Increase timesteps by the batch size (number of states in batch).\n        if not isinstance(batched_states, (dict, tuple)):\n            batch_size = len(batched_states)\n        elif isinstance(batched_states, dict):\n            batch_size = len(batched_states[next(iter(batched_states))])\n        else:\n            batch_size = len(next(iter(batched_states)))\n        self.timesteps += batch_size\n\n        # Control, which return value to ""pull"" (depending on `additional_returns`).\n        #return_ops = [0, 1, 2, 3, 4, 5, 6] if ""preprocessed_states"" in extra_returns else [0, 2, 3, 4, 5,\n        # 6]  # 1=preprocessed_states, 0=action\n        return_ops = [0, 1] if ""preprocessed_states"" in extra_returns else [0]  # 1=preprocessed_states, 0=action\n        ret = self.graph_executor.execute((\n            call_method,\n            [batched_states, not use_exploration],  # deterministic = not use_exploration\n            # 0=preprocessed_states, 1=action\n            return_ops\n        ))\n\n        # If unbatched data came in, return unbatched data.\n        if remove_batch_rank:\n            return strip_list(ret)\n        # Return batched data.\n        else:\n            return ret\n\n    # TODO make next states optional in observe API.\n    def _observe_graph(self, preprocessed_states, actions, internals, rewards, next_states, terminals):\n        self.graph_executor.execute((""insert_records"", [preprocessed_states, actions, rewards, terminals]))\n\n    def update(self, batch=None, time_percentage=None, sequence_indices=None, apply_postprocessing=True):\n        """"""\n        Args:\n            sequence_indices (Optional[np.ndarray, list]): Sequence indices are used in multi-env batches where\n                partial episode fragments may be concatenated within the trajectory. For a single env, these are equal\n                to terminals. If None are given, terminals will be used as sequence indices. A sequence index is True\n                where an episode fragment ends and False otherwise. The reason separate indices are necessary is so that\n                e.g. in GAE discounting, correct boot-strapping is applied depending on whether a true terminal state\n                was reached, or a partial episode fragment of an environment ended.\n\n                Example: If env_1 has terminals [0 0 0] for an episode fragment and env_2 terminals = [0 0 1],\n                    we may pass them in as one combined array [0 0 0 0 0 1] with sequence indices showing where each\n                    episode ends: [0 0 1 0 0 1].\n            apply_postprocessing (Optional[(bool]): If True, apply post-processing such as generalised\n                advantage estimation to collected batch in-graph. If False, update assumed post-processing has already\n                been applied. The purpose of internal versus external post-processing is to be able to off-load\n                post-processing in large scale distributed scenarios.\n        """"""\n        # TODO: Move update_spec to Worker. Agent should not hold these execution details.\n        if time_percentage is None:\n            time_percentage = self.timesteps / self.update_spec.get(""max_timesteps"", 1e6)\n\n        # [0] = the loss; [1] = loss-per-item, [2] = vf-loss, [3] = vf-loss- per item\n        return_ops = [0, 1, 2, 3]\n        if batch is None:\n            ret = self.graph_executor.execute((""update_from_memory"", [True, time_percentage], return_ops))\n\n            # Remove unnecessary return dicts (e.g. sync-op).\n            if isinstance(ret, dict):\n                ret = ret[""update_from_memory""]\n        else:\n            # No sequence indices means terminals are used in place.\n            if sequence_indices is None:\n                sequence_indices = batch[""terminals""]\n\n            pps_dtype = self.preprocessed_state_space.dtype\n            batch[""states""] = np.asarray(batch[""states""], dtype=util.convert_dtype(dtype=pps_dtype, to=\'np\'))\n\n            ret = self.graph_executor.execute(\n                (""update_from_external_batch"", [\n                    batch[""states""], batch[""actions""], batch[""rewards""], batch[""terminals""], sequence_indices,\n                    apply_postprocessing, time_percentage\n                ], return_ops)\n            )\n            # Remove unnecessary return dicts (e.g. sync-op).\n            if isinstance(ret, dict):\n                ret = ret[""update_from_external_batch""]\n\n        # [0] loss, [1] loss per item\n        return ret[0], ret[1]\n\n    def get_records(self, num_records=1):\n        return self.graph_executor.execute((""get_records"", num_records))\n\n    def reset(self):\n        """"""\n        Resets our preprocessor, but only if it contains stateful PreprocessLayer Components (meaning\n        the PreprocessorStack has at least one variable defined).\n        """"""\n        if self.preprocessing_required and len(self.preprocessor.variable_registry) > 0:\n            self.graph_executor.execute(""reset_preprocessor"")\n\n    def post_process(self, batch):\n        batch_input = [batch[""states""], batch[""rewards""], batch[""terminals""], batch[""sequence_indices""]]\n        ret = self.graph_executor.execute((""post_process"", batch_input))\n        return ret\n\n    def __repr__(self):\n        return ""PPOAgent()""\n'"
rlgraph/agents/random_agent.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph.agents import Agent\n\n\nclass RandomAgent(Agent):\n    """"""\n    An Agent that picks random actions from the action Space.\n    """"""\n    def __init__(self, state_space, action_space, name=""random-agent"", **kwargs):\n        super(RandomAgent, self).__init__(\n            update_spec=dict(do_updates=False), state_space=state_space, action_space=action_space, name=name, **kwargs\n        )\n        self.action_space_batched = self.action_space.with_batch_rank()\n\n    def get_action(self, states, internals=None, use_exploration=False, apply_preprocessing=True, extra_returns=None,\n                   time_percentage=None):\n        a = self.action_space_batched.sample(size=len(states[0]))\n        if extra_returns is not None and ""preprocessed_states"" in extra_returns:\n            return a, states\n        else:\n            return a\n\n    def update(self, batch=None, time_percentage=None, **kwargs):\n        # Return fake loss and loss-per-item.\n        return 0.0, 0.0\n\n    def _observe_graph(self, preprocessed_states, actions, internals, rewards, next_states, terminals):\n        pass\n\n    # Override these with pass so we can use them when testing distributed strategies.\n    def set_weights(self, policy_weights, value_function_weights=None):\n        pass\n\n    def get_weights(self):\n        pass\n\n    def call_api_method(self, op, inputs=None, return_ops=None):\n        pass\n\n    def __repr__(self):\n        return ""RandomAgent()""\n'"
rlgraph/agents/sac_agent.py,25,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.agents import Agent\nfrom rlgraph.components import Component, Synchronizable, Memory, ValueFunction, ContainerMerger, PrioritizedReplay\nfrom rlgraph.components.loss_functions.sac_loss_function import SACLossFunction\nfrom rlgraph.spaces import FloatBox, BoolBox, IntBox, ContainerSpace\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils import RLGraphError\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.ops import flatten_op, DataOpTuple\nfrom rlgraph.utils.util import strip_list, force_list\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass SyncSpecification(object):\n    """"""Describes a synchronization schedule, used to update the target value weights. The target values are gradually\n    updates using exponential moving average as suggested by the paper.""""""\n    def __init__(self, sync_interval=None, sync_tau=None):\n        """"""\n        Arguments:\n            sync_interval: How often to update the target.\n            sync_tau: The smoothing constant to use in the averaging. Setting to 1 replaces the values each iteration.\n        """"""\n        self.sync_interval = sync_interval\n        self.sync_tau = sync_tau\n\n\nclass SACAgentComponent(Component):\n\n    def __init__(self, agent, policy, q_function, preprocessor, memory, discount, initial_alpha, target_entropy,\n                 optimizer, vf_optimizer, alpha_optimizer, q_sync_spec, num_q_functions=2):\n        super(SACAgentComponent, self).__init__(nesting_level=0)\n        self.agent = agent\n        self._policy = policy\n        self._preprocessor = preprocessor\n        self._memory = memory\n        self._q_functions = [q_function]\n        self._q_functions += [q_function.copy(scope=""{}-{}"".format(q_function.scope, i + 1), trainable=True)\n                              for i in range(num_q_functions - 1)]\n\n        # Set number of return values for get_q_values graph_fn.\n        self.graph_fn_num_outputs[""_graph_fn_get_q_values""] = num_q_functions\n\n        for q in self._q_functions:\n            # TODO: is there a better way to do this?\n            if ""synchronizable"" not in q.sub_components:\n                q.add_components(Synchronizable(), expose_apis=""sync"")\n        self._target_q_functions = [q.copy(scope=""target-"" + q.scope, trainable=True) for q in self._q_functions]\n        for target_q in self._target_q_functions:\n            # TODO: is there a better way to do this?\n            if ""synchronizable"" not in target_q.sub_components:\n                target_q.add_components(Synchronizable(), expose_apis=""sync"")\n        self._optimizer = optimizer\n        self.vf_optimizer = vf_optimizer\n        self.alpha_optimizer = alpha_optimizer\n        self.initial_alpha = initial_alpha\n        self.log_alpha = None\n        self.target_entropy = target_entropy\n        self.loss_function = SACLossFunction(target_entropy=target_entropy, discount=discount,\n                                             num_q_functions=num_q_functions)\n\n        memory_items = [""states"", ""actions"", ""rewards"", ""next_states"", ""terminals""]\n        self._merger = ContainerMerger(*memory_items)\n\n        q_names = [""q_{}"".format(i) for i in range(len(self._q_functions))]\n        self._q_vars_merger = ContainerMerger(*q_names, scope=""q_vars_merger"")\n\n        self.add_components(policy, preprocessor, memory, self._merger, self.loss_function,\n                            optimizer, vf_optimizer, self._q_vars_merger)  # , self._q_vars_splitter)\n        self.add_components(*self._q_functions)\n        self.add_components(*self._target_q_functions)\n        if self.alpha_optimizer is not None:\n            self.add_components(self.alpha_optimizer)\n\n        self.steps_since_last_sync = None\n        self.q_sync_spec = q_sync_spec\n        self.env_action_space = None\n        self.episode_reward = None\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        for s in [""states"", ""actions"", ""env_actions"", ""preprocessed_states"", ""rewards"", ""terminals""]:\n            sanity_check_space(input_spaces[s], must_have_batch_rank=True)\n\n        self.env_action_space = input_spaces[""env_actions""].flatten()\n\n    def create_variables(self, input_spaces, action_space=None):\n        self.steps_since_last_sync = self.get_variable(""steps_since_last_sync"", dtype=""int"", initializer=0)\n        self.log_alpha = self.get_variable(""log_alpha"", dtype=""float"", initializer=np.log(self.initial_alpha))\n        self.episode_reward = self.get_variable(""episode_reward"", shape=(), initializer=0.0)\n\n    @rlgraph_api\n    def get_policy_weights(self):\n        return self._policy.variables()\n\n    @rlgraph_api\n    def get_q_weights(self):\n        merged_weights = self._q_vars_merger.merge(*[q.variables() for q in self._q_functions])\n        return merged_weights\n\n    @rlgraph_api(must_be_complete=False)\n    def set_policy_weights(self, weights):\n        return self._policy.sync(weights)\n\n    """""" TODO: need to define the input space\n    @rlgraph_api(must_be_complete=False)\n    def set_q_weights(self, q_weights):\n        split_weights = self._q_vars_splitter.call(q_weights)\n        assert len(split_weights) == len(self._q_functions)\n        update_ops = [q.sync(q_weights) for q_weights, q in zip(split_weights, self._q_functions)]\n        update_ops.extend([q.sync(q_weights) for q_weights, q in zip(split_weights, self._target_q_functions)])\n        return tuple(update_ops)\n    """"""\n\n    @rlgraph_api\n    def preprocess_states(self, states):\n        return self._preprocessor.preprocess(states)\n\n    @rlgraph_api\n    def insert_records(self, preprocessed_states, env_actions, rewards, next_states, terminals):\n        records = self._merger.merge(preprocessed_states, env_actions, rewards, next_states, terminals)\n        return self._memory.insert_records(records)\n\n    @rlgraph_api\n    def update_from_memory(self, batch_size=64, time_percentage=None):\n        records, sample_indices, importance_weights = self._memory.get_records(batch_size)\n        result = self.update_from_external_batch(\n            records[""states""], records[""actions""], records[""rewards""], records[""terminals""],\n            records[""next_states""], importance_weights, time_percentage\n        )\n\n        if isinstance(self._memory, PrioritizedReplay):\n            update_pr_step_op = self._memory.update_records(sample_indices, result[""critic_loss_per_item""])\n            result[""update_pr_step_op""] = update_pr_step_op\n\n        return result\n\n    @rlgraph_api\n    def update_from_external_batch(\n        self, preprocessed_states, env_actions, rewards, terminals, next_states, importance_weights,\n            time_percentage=None\n    ):\n        actions = self._graph_fn_one_hot(env_actions)\n        actor_loss, actor_loss_per_item, critic_loss, critic_loss_per_item, alpha_loss, alpha_loss_per_item = \\\n            self.get_losses(preprocessed_states, actions, rewards, terminals, next_states, importance_weights)\n\n        policy_vars = self._policy.variables()\n        q_vars = [q_func.variables() for q_func in self._q_functions]\n        merged_q_vars = self._q_vars_merger.merge(*q_vars)\n        critic_step_op = self.vf_optimizer.step(merged_q_vars, critic_loss, critic_loss_per_item, time_percentage)\n        actor_step_op = self._optimizer.step(policy_vars, actor_loss, actor_loss_per_item, time_percentage)\n\n        if self.target_entropy is not None:\n            alpha_step_op = self._graph_fn_update_alpha(alpha_loss, alpha_loss_per_item, time_percentage)\n        else:\n            alpha_step_op = self._graph_fn_no_op()\n        # TODO: optimizer for alpha\n\n        sync_op = self.sync_targets()\n\n        # Increase the global training step counter.\n        alpha_step_op = self._graph_fn_training_step(alpha_step_op)\n\n        return dict(\n            actor_step_op=actor_step_op,\n            critic_step_op=critic_step_op,\n            sync_op=sync_op,\n            alpha_step_op=alpha_step_op,\n            actor_loss=actor_loss,\n            actor_loss_per_item=actor_loss_per_item,\n            critic_loss=critic_loss,\n            critic_loss_per_item=critic_loss_per_item,\n            alpha_loss=alpha_loss,\n            alpha_loss_per_item=alpha_loss_per_item\n        )\n\n    @graph_fn(flatten_ops=True, split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_one_hot(self, key, env_actions):\n        if isinstance(self.env_action_space[key], IntBox):\n            env_actions = tf.one_hot(env_actions, depth=self.env_action_space[key].num_categories, axis=-1)\n        return env_actions\n\n    @graph_fn(requires_variable_completeness=True)\n    def _graph_fn_update_alpha(self, alpha_loss, alpha_loss_per_item, time_percentage=None):\n        alpha_step_op = self.alpha_optimizer.step(\n            DataOpTuple([self.log_alpha]), alpha_loss, alpha_loss_per_item, time_percentage\n        )\n        return alpha_step_op\n\n    @rlgraph_api  # `returns` are determined in ctor\n    def _graph_fn_get_q_values(self, preprocessed_states, actions, target=False):\n        backend = get_backend()\n\n        flat_actions = flatten_op(actions)\n        actions = []\n        for flat_key, action_component in self._policy.action_space.flatten().items():\n            actions.append(flat_actions[flat_key])\n\n        if backend == ""tf"":\n            actions = tf.concat(actions, axis=-1)\n        elif backend == ""pytorch"":\n            actions = torch.cat(actions, dim=-1)\n\n        q_funcs = self._q_functions if target is False else self._target_q_functions\n\n        # We do not concat states yet because we might pass states through a conv stack before merging it\n        # with actions.\n        return tuple(q.state_action_value(preprocessed_states, actions) for q in q_funcs)\n\n    @rlgraph_api\n    def get_losses(self, preprocessed_states, actions, rewards, terminals, next_states, importance_weights):\n        # TODO: internal states\n        samples_next = self._policy.get_action_and_log_likelihood(next_states, deterministic=False)\n        next_sampled_actions = samples_next[""action""]\n        log_probs_next_sampled = samples_next[""log_likelihood""]\n\n        q_values_next_sampled = self.get_q_values(\n            next_states, next_sampled_actions, target=True\n        )\n        q_values = self.get_q_values(preprocessed_states, actions)\n        samples = self._policy.get_action_and_log_likelihood(preprocessed_states, deterministic=False)\n        sampled_actions = samples[""action""]\n        log_probs_sampled = samples[""log_likelihood""]\n        q_values_sampled = self.get_q_values(preprocessed_states, sampled_actions)\n\n        alpha = self._graph_fn_compute_alpha()\n\n        return self.loss_function.loss(\n            alpha,\n            log_probs_next_sampled,\n            q_values_next_sampled,\n            q_values,\n            log_probs_sampled,\n            q_values_sampled,\n            rewards,\n            terminals\n        )\n\n    @rlgraph_api\n    def get_preprocessed_state_and_action(self, states, deterministic=False):\n        preprocessed_states = self._preprocessor.preprocess(states)\n        return self.action_from_preprocessed_state(preprocessed_states, deterministic)\n\n    @rlgraph_api\n    def action_from_preprocessed_state(self, preprocessed_states, deterministic=False):\n        out = self._policy.get_action(preprocessed_states, deterministic=deterministic)\n        return out[""action""], preprocessed_states\n\n    @rlgraph_api(requires_variable_completeness=True)\n    def reset_targets(self):\n        ops = (target_q.sync(q.variables()) for q, target_q in zip(self._q_functions, self._target_q_functions))\n        return tuple(ops)\n\n    @rlgraph_api(requires_variable_completeness=True)\n    def sync_targets(self):\n        should_sync = self._graph_fn_get_should_sync()\n        return self._graph_fn_sync(should_sync)\n\n    @rlgraph_api\n    def get_memory_size(self):\n        return self._memory.get_size()\n\n    @graph_fn\n    def _graph_fn_compute_alpha(self):\n        backend = get_backend()\n        if backend == ""tf"":\n            return tf.exp(self.log_alpha)\n        elif backend == ""pytorch"":\n            return torch.exp(self.log_alpha)\n\n    # TODO: Move this into generic AgentRootComponent.\n    @graph_fn\n    def _graph_fn_training_step(self, other_step_op=None):\n        if self.agent is not None:\n            add_op = tf.assign_add(self.agent.graph_executor.global_training_timestep, 1)\n            op_list = [add_op] + [other_step_op] if other_step_op is not None else []\n            with tf.control_dependencies(op_list):\n                return tf.no_op() if other_step_op is None else other_step_op\n        else:\n            return tf.no_op() if other_step_op is None else other_step_op\n\n    @graph_fn(returns=1, requires_variable_completeness=True)\n    def _graph_fn_get_should_sync(self):\n        if get_backend() == ""tf"":\n            inc_op = tf.assign_add(self.steps_since_last_sync, 1)\n            should_sync = inc_op >= self.q_sync_spec.sync_interval\n\n            def reset_op():\n                op = tf.assign(self.steps_since_last_sync, 0)\n                with tf.control_dependencies([op]):\n                    return tf.no_op()\n\n            sync_op = tf.cond(\n                pred=inc_op >= self.q_sync_spec.sync_interval,\n                true_fn=reset_op,\n                false_fn=tf.no_op\n            )\n            with tf.control_dependencies([sync_op]):\n                return tf.identity(should_sync)\n        else:\n            raise NotImplementedError(""TODO"")\n\n    @graph_fn(returns=1, requires_variable_completeness=True)\n    def _graph_fn_sync(self, should_sync):\n        assign_ops = []\n        tau = self.q_sync_spec.sync_tau\n        if tau != 1.0:\n            all_source_vars = [source.get_variables(collections=None, custom_scope_separator=""-"") for source in self._q_functions]\n            all_dest_vars = [destination.get_variables(collections=None, custom_scope_separator=""-"") for destination in self._target_q_functions]\n            for source_vars, dest_vars in zip(all_source_vars, all_dest_vars):\n                for (source_key, source_var), (dest_key, dest_var) in zip(sorted(source_vars.items()), sorted(dest_vars.items())):\n                    assign_ops.append(tf.assign(dest_var, tau * source_var + (1.0 - tau) * dest_var))\n        else:\n            all_source_vars = [source.variables() for source in self._q_functions]\n            for source_vars, destination in zip(all_source_vars, self._target_q_functions):\n                assign_ops.append(destination.sync(source_vars))\n        assert len(assign_ops) > 0\n        grouped_op = tf.group(assign_ops)\n\n        def assign_op():\n            # Make sure we are returning no_op as opposed to reference\n            with tf.control_dependencies([grouped_op]):\n                return tf.no_op()\n\n        cond_assign_op = tf.cond(should_sync, true_fn=assign_op, false_fn=tf.no_op)\n        with tf.control_dependencies([cond_assign_op]):\n            return tf.no_op()\n\n    @graph_fn\n    def _graph_fn_no_op(self):\n        return tf.no_op()\n\n    @rlgraph_api\n    def get_global_timestep(self):\n        return self.read_variable(self.agent.graph_executor.global_timestep)\n\n    @rlgraph_api\n    def _graph_fn_update_global_timestep(self, increment):\n        if get_backend() == ""tf"":\n            add_op = tf.assign_add(self.agent.graph_executor.global_timestep, increment)\n            return add_op\n        elif get_backend == ""pytorch"":\n            self.agent.graph_executor.global_timestep += increment\n            return self.agent.graph_executor.global_timestep\n\n    @rlgraph_api\n    def _graph_fn_get_episode_reward(self):\n        return self.episode_reward\n\n    @rlgraph_api\n    def _graph_fn_set_episode_reward(self, episode_reward):\n        return tf.assign(self.episode_reward, episode_reward)\n\n\nclass SACAgent(Agent):\n    def __init__(\n        self,\n        state_space,\n        action_space,\n        discount=0.98,\n        preprocessing_spec=None,\n        network_spec=None,\n        internal_states_space=None,\n        policy_spec=None,\n        value_function_spec=None,\n        execution_spec=None,\n        optimizer_spec=None,\n        value_function_optimizer_spec=None,\n        observe_spec=None,\n        update_spec=None,\n        summary_spec=None,\n        saver_spec=None,\n        auto_build=True,\n        name=""sac-agent"",\n        double_q=True,\n        initial_alpha=1.0,\n        gumbel_softmax_temperature=1.0,\n        target_entropy=None,\n        memory_spec=None,\n        value_function_sync_spec=None\n    ):\n        """"""\n        This is an implementation of the Soft-Actor Critic algorithm.\n\n        Paper: http://arxiv.org/abs/1801.01290\n\n        Args:\n            state_space (Union[dict,Space]): Spec dict for the state Space or a direct Space object.\n            action_space (Union[dict,Space]): Spec dict for the action Space or a direct Space object.\n            preprocessing_spec (Optional[list,PreprocessorStack]): The spec list for the different necessary states\n                preprocessing steps or a PreprocessorStack object itself.\n            discount (float): The discount factor (gamma).\n            network_spec (Optional[list,NeuralNetwork]): Spec list for a NeuralNetwork Component or the NeuralNetwork\n                object itself.\n            internal_states_space (Optional[Union[dict,Space]]): Spec dict for the internal-states Space or a direct\n                Space object for the Space(s) of the internal (RNN) states.\n            policy_spec (Optional[dict]): An optional dict for further kwargs passing into the Policy c\'tor.\n            value_function_spec (list, dict, ValueFunction): Neural network specification for baseline or instance\n                of ValueFunction.\n            execution_spec (Optional[dict,Execution]): The spec-dict specifying execution settings.\n            optimizer_spec (Optional[dict,Optimizer]): The spec-dict to create the Optimizer for this Agent.\n            value_function_optimizer_spec (dict): Optimizer config for value function optimizer. If None, the optimizer\n                spec for the policy is used (same learning rate and optimizer type).\n            observe_spec (Optional[dict]): Spec-dict to specify `Agent.observe()` settings.\n            update_spec (Optional[dict]): Spec-dict to specify `Agent.update()` settings.\n            summary_spec (Optional[dict]): Spec-dict to specify summary settings.\n            saver_spec (Optional[dict]): Spec-dict to specify saver settings.\n            auto_build (Optional[bool]): If True (default), immediately builds the graph using the agent\'s\n                graph builder. If false, users must separately call agent.build(). Useful for debugging or analyzing\n                components before building.\n            name (str): Some name for this Agent object.\n            double_q (bool): Whether to train two q networks independently.\n            initial_alpha (float): ""The temperature parameter \xce\xb1 determines the\n                relative importance of the entropy term against the reward"".\n            gumbel_softmax_temperature (float): Temperature parameter for the Gumbel-Softmax distribution used\n                for discrete actions.\n            memory_spec (Optional[dict,Memory]): The spec for the Memory to use for the DQN algorithm.\n            update_spec (dict): Here we can have sync_interval or sync_tau (for the value network update).\n        """"""\n        # If VF spec is a network spec, wrap with SAC vf type. The VF must concatenate actions and states,\n        # which can require splitting the network in the case of e.g. conv-inputs.\n        if isinstance(value_function_spec, list):\n            value_function_spec = dict(type=""sac_value_function"", network_spec=value_function_spec)\n            self.logger.info(""Using default SAC value function."")\n        elif isinstance(value_function_spec, ValueFunction):\n            self.logger.info(""Using value function object {}"".format(ValueFunction))\n\n        if policy_spec is None:\n            # Continuous action space: Use squashed normal.\n            # Discrete: Gumbel-softmax.\n            policy_spec = dict(deterministic=False,\n                            distributions_spec=dict(\n                                bounded_distribution_type=""squashed"",\n                                discrete_distribution_type=""gumbel_softmax"",\n                                gumbel_softmax_temperature=gumbel_softmax_temperature\n                            ))\n\n        super(SACAgent, self).__init__(\n            state_space=state_space,\n            action_space=action_space,\n            discount=discount,\n            preprocessing_spec=preprocessing_spec,\n            network_spec=network_spec,\n            internal_states_space=internal_states_space,\n            policy_spec=policy_spec,\n            value_function_spec=value_function_spec,\n            execution_spec=execution_spec,\n            optimizer_spec=optimizer_spec,\n            value_function_optimizer_spec=value_function_optimizer_spec,\n            observe_spec=observe_spec,\n            update_spec=update_spec,\n            summary_spec=summary_spec,\n            saver_spec=saver_spec,\n            auto_build=auto_build,\n            name=name\n        )\n\n        self.double_q = double_q\n        self.target_entropy = target_entropy\n        self.initial_alpha = initial_alpha\n\n        # Assert that the synch interval is a multiple of the update_interval.\n        if ""sync_interval"" in self.update_spec:\n            if self.update_spec[""sync_interval""] / self.update_spec[""update_interval""] != \\\n                    self.update_spec[""sync_interval""] // self.update_spec[""update_interval""]:\n                raise RLGraphError(\n                    ""ERROR: sync_interval ({}) must be multiple of update_interval ""\n                    ""({})!"".format(self.update_spec[""sync_interval""], self.update_spec[""update_interval""])\n                )\n        elif ""sync_tau"" in self.update_spec:\n            if self.update_spec[""sync_tau""] <= 0 or self.update_spec[""sync_tau""] > 1.0:\n                raise RLGraphError(\n                    ""sync_tau ({}) must be in interval (0.0, 1.0]!"".format(self.update_spec[""sync_tau""])\n                )\n        else:\n            self.update_spec[""sync_tau""] = 0.005  # The value mentioned in the paper\n\n        # Extend input Space definitions to this Agent\'s specific API-methods.\n        preprocessed_state_space = self.preprocessed_state_space.with_batch_rank()\n        reward_space = FloatBox(add_batch_rank=True)\n        terminal_space = BoolBox(add_batch_rank=True)\n\n        #self.iterations = self.update_spec[""num_iterations""]\n        self.batch_size = self.update_spec[""batch_size""]\n\n        float_action_space = self.action_space.with_batch_rank().map(\n            mapping=lambda flat_key, space: space.as_one_hot_float_space() if isinstance(space, IntBox) else space\n        )\n\n        self.input_spaces.update(dict(\n            env_actions=self.action_space.with_batch_rank(),\n            actions=float_action_space,\n            preprocessed_states=preprocessed_state_space,\n            rewards=reward_space,\n            terminals=terminal_space,\n            next_states=preprocessed_state_space,\n            states=self.state_space.with_batch_rank(add_batch_rank=True),\n            batch_size=int,\n            importance_weights=FloatBox(add_batch_rank=True),\n            deterministic=bool,\n            weights=""variables:{}"".format(self.policy.scope)\n        ))\n\n        if value_function_sync_spec is None:\n            value_function_sync_spec = SyncSpecification(\n                sync_interval=self.update_spec[""sync_interval""] // self.update_spec[""update_interval""],\n                sync_tau=self.update_spec[""sync_tau""] if ""sync_tau"" in self.update_spec else 5e-3\n            )\n\n        self.memory = Memory.from_spec(memory_spec)\n        self.alpha_optimizer = self.optimizer.copy(scope=""alpha-"" + self.optimizer.scope) if self.target_entropy is not None else None\n\n        self.root_component = SACAgentComponent(\n            agent=self,\n            policy=self.policy,\n            q_function=self.value_function,\n            preprocessor=self.preprocessor,\n            memory=self.memory,\n            discount=self.discount,\n            initial_alpha=self.initial_alpha,\n            target_entropy=target_entropy,\n            optimizer=self.optimizer,\n            vf_optimizer=self.value_function_optimizer,\n            alpha_optimizer=self.alpha_optimizer,\n            q_sync_spec=value_function_sync_spec,\n            num_q_functions=2 if self.double_q is True else 1\n        )\n\n        extra_optimizers = [self.value_function_optimizer]\n        if self.alpha_optimizer is not None:\n            extra_optimizers.append(self.alpha_optimizer)\n        self.build_options = dict(optimizers=extra_optimizers)\n\n        if self.auto_build:\n            self._build_graph(\n                [self.root_component], self.input_spaces, optimizer=self.optimizer,\n                batch_size=self.update_spec[""batch_size""],\n                build_options=self.build_options\n            )\n            self.graph_built = True\n\n    def set_weights(self, policy_weights, value_function_weights=None):\n        # TODO: Overrides parent but should this be policy of value function?\n        return self.graph_executor.execute((self.root_component.set_policy_weights, policy_weights))\n\n    def get_weights(self):\n        return dict(policy_weights=self.graph_executor.execute(self.root_component.get_policy_weights))\n\n    def get_action(self, states, internals=None, use_exploration=True, apply_preprocessing=True, extra_returns=None,\n                   time_percentage=None):\n        # TODO: common pattern - move to Agent\n        """"""\n        Args:\n            extra_returns (Optional[Set[str],str]): Optional string or set of strings for additional return\n                values (besides the actions). Possible values are:\n                - \'preprocessed_states\': The preprocessed states after passing the given states through the\n                preprocessor stack.\n                - \'internal_states\': The internal states returned by the RNNs in the NN pipeline.\n                - \'used_exploration\': Whether epsilon- or noise-based exploration was used or not.\n\n        Returns:\n            tuple or single value depending on `extra_returns`:\n                - action\n                - the preprocessed states\n        """"""\n        extra_returns = {extra_returns} if isinstance(extra_returns, str) else (extra_returns or set())\n        # States come in without preprocessing -> use state space.\n        if apply_preprocessing:\n            call_method = self.root_component.get_preprocessed_state_and_action\n            batched_states, remove_batch_rank = self.state_space.force_batch(states)\n        else:\n            call_method = self.root_component.action_from_preprocessed_state\n            batched_states = states\n            remove_batch_rank = False\n        #remove_batch_rank = batched_states.ndim == np.asarray(states).ndim + 1\n\n        # Increase timesteps by the batch size (number of states in batch).\n        batch_size = len(batched_states)\n        self.timesteps += batch_size\n\n        # Control, which return value to ""pull"" (depending on `additional_returns`).\n        return_ops = [0, 1] if ""preprocessed_states"" in extra_returns else [0]\n        ret = force_list(self.graph_executor.execute((\n            call_method,\n            [batched_states, not use_exploration],  # deterministic = not use_exploration\n            # 0=preprocessed_states, 1=action\n            return_ops\n        )))\n        # Convert Gumble (relaxed one-hot) sample back into int type for all discrete composite actions.\n        if isinstance(self.action_space, ContainerSpace):\n            ret[0] = ret[0].map(\n                mapping=lambda key, action: np.argmax(action, axis=-1).astype(action.dtype)\n                if isinstance(self.flat_action_space[key], IntBox) else action\n            )\n        elif isinstance(self.action_space, IntBox):\n            ret[0] = np.argmax(ret[0], axis=-1).astype(self.action_space.dtype)\n\n        if remove_batch_rank:\n            ret[0] = strip_list(ret[0])\n\n        if ""preprocessed_states"" in extra_returns:\n            return ret[0], ret[1]\n        else:\n            return ret[0]\n\n    def _observe_graph(self, preprocessed_states, actions, internals, rewards, next_states, terminals):\n        self.graph_executor.execute((self.root_component.insert_records, [preprocessed_states, actions, rewards, next_states, terminals]))\n\n    def update(self, batch=None, time_percentage=None, **kwargs):\n        if batch is None:\n            size = self.graph_executor.execute(self.root_component.get_memory_size)\n            # TODO: is this necessary?\n            if size < self.batch_size:\n                return 0.0, 0.0, 0.0\n            ret = self.graph_executor.execute((self.root_component.update_from_memory, [self.batch_size, time_percentage]))\n        else:\n            ret = self.graph_executor.execute((self.root_component.update_from_external_batch, [\n                batch[""states""], batch[""actions""], batch[""rewards""], batch[""terminals""], batch[""next_states""],\n                batch[""importance_weights""], time_percentage\n            ]))\n\n        return ret[""actor_loss""], ret[""actor_loss_per_item""], ret[""critic_loss""], ret[""alpha_loss""]\n\n    def reset(self):\n        """"""\n        Resets our preprocessor, but only if it contains stateful PreprocessLayer Components (meaning\n        the PreprocessorStack has at least one variable defined).\n        """"""\n        if self.preprocessing_required and len(self.preprocessor.variables) > 0:\n            self.graph_executor.execute(""reset_preprocessor"")\n        self.graph_executor.execute(self.root_component.reset_targets)\n\n    def __repr__(self):\n        return ""SACAgent(double-q={}, initial-alpha={}, target-entropy={})"".format(\n            self.double_q, self.initial_alpha, self.target_entropy\n        )\n'"
rlgraph/components/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# Component child-classes.\nfrom rlgraph.components.common import *\n# Core.\nfrom rlgraph.components.component import Component\nfrom rlgraph.components.distributions import *\nfrom rlgraph.components.explorations import Exploration, EpsilonExploration\nfrom rlgraph.components.layers import *\nfrom rlgraph.components.loss_functions import *\nfrom rlgraph.components.memories import *\nfrom rlgraph.components.neural_networks import *\nfrom rlgraph.components.optimizers import *\nfrom rlgraph.components.policies import *\nfrom rlgraph.utils.util import default_dict\n\n# Create the lookup dict for Component.\nComponent.__lookup_classes__ = dict(\n    containermerger=ContainerMerger,\n    containersplitter=ContainerSplitter,\n    timedependentparameter=TimeDependentParameter\n)\n\n# Add all specific sub-classes to this one.\ndefault_dict(Component.__lookup_classes__, Distribution.__lookup_classes__)\ndefault_dict(Component.__lookup_classes__, Layer.__lookup_classes__)\ndefault_dict(Component.__lookup_classes__, Stack.__lookup_classes__)\ndefault_dict(Component.__lookup_classes__, LossFunction.__lookup_classes__)\ndefault_dict(Component.__lookup_classes__, Memory.__lookup_classes__)\ndefault_dict(Component.__lookup_classes__, NeuralNetwork.__lookup_classes__)\ndefault_dict(Component.__lookup_classes__, Optimizer.__lookup_classes__)\ndefault_dict(Component.__lookup_classes__, Policy.__lookup_classes__)\n\n\n__all__ = [""Component""] + \\\n          list(set(map(lambda x: x.__name__, Component.__lookup_classes__.values())))\n\n'"
rlgraph/components/component.py,21,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport copy\nimport inspect\nimport re\nimport uuid\nfrom collections import OrderedDict\n\nimport numpy as np\nfrom six.moves import xrange as range_\n\nfrom rlgraph import get_backend\nfrom rlgraph.utils import util\nfrom rlgraph.utils.decorators import rlgraph_api, component_api_registry, component_graph_fn_registry, \\\n    define_api_method, define_graph_fn\nfrom rlgraph.utils.ops import DataOpDict, FLAT_TUPLE_OPEN, FLAT_TUPLE_CLOSE, TraceContext\nfrom rlgraph.utils.rlgraph_errors import RLGraphError, RLGraphObsoletedError\nfrom rlgraph.utils.specifiable import Specifiable\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""tf-eager"":\n    import tensorflow as tf\n    import tensorflow.contrib.eager as eager\nelif get_backend() == ""pytorch"":\n    from rlgraph.utils import PyTorchVariable\n    import torch\n\n\nclass Component(Specifiable):\n    """"""\n    Base class for a graph component (such as a layer, an entire function approximator, a memory, an optimizers, etc..).\n\n    A component can contain other components and/or its own graph-logic (e.g. tf ops).\n    A component\'s sub-components are connected to each other via in- and out-Sockets (similar to LEGO blocks\n    and deepmind\'s sonnet).\n\n    This base class implements the interface to add sub-components, create connections between\n    different sub-components and between a sub-component and this one and between this component\n    and an external component.\n\n    A component also has a variable registry, the ability to save the component\'s structure and variable-values to disk,\n    and supports adding its graph_fns to the overall computation graph.\n    """"""\n    call_count = 0\n\n    # List of tuples (method_name, runtime)\n    call_times = []\n\n    def __init__(self, *sub_components, **kwargs):\n        """"""\n        Args:\n            sub_components (Component): Specification dicts for sub-Components to be added to this one.\n\n        Keyword Args:\n            name (str): The name of this Component. Names of sub-components within a containing component\n                must be unique. Names are used to label exposed Sockets of the containing component.\n                If name is empty, use scope as name (as last resort).\n            scope (str): The scope of this Component for naming variables in the Graph.\n            device (str): Device this component will be assigned to. If None, defaults to CPU.\n            trainable (Optional[bool]): Whether to make the variables of this Component always trainable or not.\n                Use None for no specific preference.\n\n            # TODO: remove when we have numpy-based Components (then we can do test calls to infer everything automatically)\n            graph_fn_num_outputs (dict): A dict specifying which graph_fns have how many return values.\n                This can be useful if graph_fns don\'t clearly have a fixed number of return values and the auto-inferral\n                utility function cannot determine the actual number of returned values.\n\n            switched_off_apis (Optional[Set[str]]): Set of API-method names that should NOT be build for this Component.\n            \n            backend (str): The custom backend that this Component obliges to. None to use the RLGraph global backend.\n                Default: None.\n            \n            space_agnostic (bool): Whether this component does not care about input spaces (e.g. if it does not\n                create any space-dependent variables). Default: False.\n\n            nesting_level (Optional[int]): The Component\'s nesting level. Root component has 0.\n                None for non-placed Components. Default: None.\n        """"""\n        super(Component, self).__init__()\n\n        # Scope if used to create scope hierarchies inside the Graph.\n        # self.logger = logging.getLogger(__name__)\n        self.scope = kwargs.pop(""scope"", """")\n\n        assert re.match(r\'^[\\w\\-.]*$\', self.scope), \\\n            ""ERROR: scope {} does not match scope-pattern! Needs to be \\\\w or \'-\'."".format(self.scope)\n        # The global scope string defining the exact nested position of this Component in the Graph.\n        # e.g. ""/core/component1/sub-component-a""\n        self.global_scope = self.scope\n\n        # Shared variable scope.\n        self.reuse_variable_scope = kwargs.pop(""reuse_variable_scope"", None)\n\n        # Names of sub-components that exist (parallelly) inside a containing component must be unique.\n        self.name = kwargs.pop(""name"", self.scope)  # if no name given, use scope\n        self.device = kwargs.pop(""device"", None)\n        self.trainable = kwargs.pop(""trainable"", None)\n        self.graph_fn_num_outputs = kwargs.pop(""graph_fn_num_outputs"", dict())\n        self.switched_off_apis = kwargs.pop(""switched_off_apis"", set())\n        self.backend = kwargs.pop(""backend"", None)\n        self.space_agnostic = kwargs.pop(""space_agnostic"", None)\n        self.nesting_level = kwargs.pop(""nesting_level"", None)\n\n        assert not kwargs, ""ERROR: kwargs ({}) still contains items!"".format(kwargs)\n\n        # Keep track of whether this Component has already been added to another Component and throw error\n        # if this is done twice. Each Component can only be added once to a parent Component.\n        self.parent_component = None  # type: Component\n\n        # Dict of sub-components that live inside this one (key=sub-component\'s scope).\n        self.sub_components = OrderedDict()\n\n        # Link to the GraphBuilder object.\n        self.graph_builder = None\n\n        # `self.api_methods`: Dict holding information about which op-record-tuples go via which API\n        # methods into this Component and come out of it.\n        # keys=API method name; values=APIMethodRecord\n        self.api_methods = {}\n\n        # Maps names to callable API functions for eager calls.\n        self.api_fn_by_name = {}\n        # Maps names of methods to synthetically defined methods.\n        self.synthetic_methods = set()\n\n        # How this component executes its \'call\' method.\n        self.execution_mode = ""static_graph"" if self.backend != ""python"" else ""define_by_run""\n\n        # `self.api_method_inputs`: Registry for all unique API-method input parameter names and their Spaces.\n        # Two API-methods may share the same input if their input parameters have the same names.\n        # keys=input parameter name; values=Space that goes into that parameter\n        self.api_method_inputs = {}\n        # Registry for graph_fn records (only populated at build time when the graph_fns are actually called).\n        self.graph_fns = {}\n        self.register_api_methods_and_graph_fns()\n\n        # Set of op-rec-columns going into a graph_fn of this Component and not having 0 op-records.\n        # Helps during the build procedure to call these right away after the Component is input-complete.\n        self.no_input_graph_fn_columns = set()\n        # Set of op-records that are constant and thus can be processed right away at the beginning of the build\n        # procedure.\n        self.constant_op_records = set()\n        # Whether we know already all our API-methods\' call args\' spaces.\n        self.input_complete = False\n        # Short-circuit-set to True, if no variables are generated by this Component anyway.\n        if re.search(r\'\\spass\\n$\', inspect.getsource(self.create_variables)) and \\\n                re.search(r\'\\spass\\n$\', inspect.getsource(self.check_input_spaces)):\n            self.input_complete = True\n\n        # Whether all our sub-Components are input-complete. Only after that point, we can run our _variables graph_fn.\n        self.variable_complete = False\n        # Has this component been built yet by the graph builder?\n        self.built = False\n\n        # All Variables that are held by this component (and its sub-components) by name.\n        # key=full-scope variable name (scope=component/sub-component scope)\n        # value=the actual variable\n        self.variable_registry = {}\n        # All summary ops that are held by this component (and its sub-components) by name.\n        # key=full-scope summary name (scope=component/sub-component scope)\n        # value=the actual summary op\n        self.summaries = {}\n        # The regexp that a summary\'s full-scope name has to match in order for it to be generated and registered.\n        # This will be set by the GraphBuilder at build time.\n        self.summary_regexp = None\n\n        # This is a stack of summaries to be filled by the graph_fn calls\n        self._summary_ops_buffer_stack = []\n\n        # Now add all sub-Components (also support all sub-Components being given in a single list).\n        sub_components = sub_components[0] if len(sub_components) == 1 and \\\n            isinstance(sub_components[0], (list, tuple)) else sub_components\n        self.add_components(*sub_components)\n\n    def register_api_methods_and_graph_fns(self):\n        """"""\n        Detects all methods of the Component that should be registered as API-methods for\n        this Component and complements `self.api_methods` and `self.api_method_inputs`.\n        Goes by the @api decorator before each API-method or graph_fn that should be\n        auto-thin-wrapped by an API-method.\n        """"""\n        # Goes through the class hierarchy of `self` and tries to lookup all registered functions\n        # (by name) that should be turned into API-methods.\n        class_hierarchy = inspect.getmro(type(self))\n        for class_ in class_hierarchy[:-2]:  # skip last two as its `Specifiable` and `object`\n            api_method_recs = component_api_registry.get(class_.__name__)\n            if api_method_recs is not None:\n                for api_method_rec in api_method_recs:\n                    if api_method_rec.name not in self.api_methods:\n                        define_api_method(self, api_method_rec)\n            graph_fn_recs = component_graph_fn_registry.get(class_.__name__)\n            if graph_fn_recs is not None:\n                for graph_fn_rec in graph_fn_recs:\n                    if graph_fn_rec.name not in self.graph_fns:\n                        define_graph_fn(self, graph_fn_rec)\n\n    def get_number_of_allowed_inputs(self, api_method_name):\n        """"""\n        Returns the number of allowed input args for a given API-method.\n\n        Args:\n            api_method_name (str): The API-method to analyze.\n\n        Returns:\n            Tuple[int,int]: A tuple with the range (lower/upper bound) of allowed input args for the given API-method.\n                An upper bound of None means that the API-method accepts any number of input args equal or larger\n                than the lower bound.\n        """"""\n        input_names = self.api_methods[api_method_name].input_names\n        num_allowed_inputs = [0, 0]\n        for in_name in input_names:\n            # Positional arg with default values (not required, only raise upper bound).\n            if self.api_method_inputs[in_name] == ""flex"":\n                num_allowed_inputs[1] += 1\n            # Var-positional (no upper bound anymore).\n            elif self.api_method_inputs[in_name] == ""*flex"":\n                num_allowed_inputs[1] = None\n            # Required arg (raise both lower and upper bound).\n            else:\n                num_allowed_inputs[0] += 1\n                num_allowed_inputs[1] += 1\n\n        return tuple(num_allowed_inputs)\n\n    def check_input_completeness(self):\n        """"""\n        Checks whether this Component is ""input-complete"" and stores the result in self.input_complete.\n        Input-completeness is reached (only once and then it stays that way) if all API-methods of this component\n        (whose `must_be_complete` field is not set to False) have all their input Spaces defined.\n\n        Also, if no variables are generated by this Component anyway, the component is input-complete.\n\n        Returns:\n            bool: Whether this Component is input-complete or not.\n        """"""\n        if self.input_complete is True:\n            return True\n\n        self.input_complete = True\n        # Loop through all API methods.\n        for method_name, api_method_rec in self.api_methods.items():\n            # This API method doesn\'t have to be completed, ignore and don\'t add it to space_dict.\n            if api_method_rec.must_be_complete is False:\n                continue\n\n            # Loop through all of this API-method\'s input parameter names and check, whether they\n            # all have a Space defined.\n            for input_name in api_method_rec.input_names:\n                assert input_name in self.api_method_inputs\n                # This one is not defined yet -> Component is not input-complete.\n                if self.api_method_inputs[input_name] is None:\n                    self.input_complete = False\n                    self.logger.debug(""Found incomplete input_name {} for method {}."".format(input_name, method_name))\n                    return False\n                # API-method has a var-positional parameter (*args): Check whether it has been called at\n                # least once (in which case we have Space information stored under ""args[0]"").\n                elif self.api_method_inputs[input_name] == ""*flex"":\n                    # Check all keys ""input_name[n]"" for any None. If one None found -> input incomplete.\n                    idx = 0\n                    while True:\n                        key = input_name+""[""+str(idx)+""]""\n                        if key not in self.api_method_inputs:\n                            # We require at least one param if the flex param is the only param. Otherwise, none are ok.\n                            if idx > (0 if len(api_method_rec.input_names) == 1 else -1):\n                                break\n                            # No input defined (has not been called) -> Not input complete.\n                            else:\n                                self.input_complete = False\n                                self.logger.debug(\n                                    ""Found incomplete flex key {} for method {}."".format(key, method_name))\n\n                                return False\n                        elif self.api_method_inputs[key] is None:\n                            self.input_complete = False\n                            self.logger.debug(\n                                ""Found incomplete flex key {} for method {}."".format(key, method_name))\n                            return False\n                        idx += 1\n                # API-method has **kwargs parameters: Check whether it has been called at\n                # least once (in which case we have Space information stored under ""kwargs[\'some key\']"").\n                elif self.api_method_inputs[input_name] == ""**flex"":\n                    keys = [k for k in self.api_method_inputs if re.match(r\'{}\\[\'.format(input_name), k)]\n                    # Check all keys ""input_name[n]"" for any None. If one None found -> input incomplete.\n                    for key in keys:\n                        if self.api_method_inputs[key] is None:\n                            self.logger.debug(\n                                ""Found incomplete kwargs key {} for method {}."".format(key, method_name))\n                            self.input_complete = False\n                            return False\n        return True\n\n    def check_variable_completeness(self):\n        """"""\n        Checks, whether this Component is input-complete AND all our sub-Components are input-complete.\n        At that point, all variables are defined and we can run the `variables` graph_fn.\n\n        Returns:\n            bool: Whether this Component is ""variables-complete"".\n        """"""\n        # We are already variable-complete -> shortcut return here.\n        if self.variable_complete is True:\n            return True\n        # We are not input-complete yet (our own variables have not been created) -> return False.\n        elif self.input_complete is False:\n            return False\n\n        # Simply check all direct sub-Components for variable-completeness.\n        for direct_child in self.sub_components.values():\n            if re.search(r\'^\\.helper-\', direct_child.scope):\n                continue\n            if not direct_child.check_variable_completeness():\n                return False\n        self.variable_complete = True\n        return self.variable_complete\n\n    def when_input_complete(self, input_spaces=None, action_space=None, device=None, summary_regexp=None):\n        """"""\n        Wrapper that calls both `self.check_input_spaces` and `self.create_variables` in sequence and passes\n        the dict with the input_spaces for each argument (key=arg name) and the action_space as parameter.\n\n        Args:\n            input_spaces (Optional[Dict[str,Space]]): A dict with Space/shape information.\n                keys=in-argument name (str); values=the associated Space.\n                Use None to take `self.api_method_inputs` instead.\n\n            action_space (Optional[Space]): The action Space of the Agent/GraphBuilder. Can be used to construct and connect\n                more Components (which rely on this information). This eliminates the need to pass the action Space\n                information into many Components\' constructors.\n\n            device (str): The device to use for the variables generated.\n\n            summary_regexp (Optional[str]): A regexp (str) that defines, which summaries should be generated\n                and registered.\n        """"""\n        # Store the summary_regexp to use.\n        self.summary_regexp = summary_regexp\n        # print(""Completing with input spaces api arg = "", input_spaces)\n        input_spaces = input_spaces or self.api_method_inputs\n        # print(""Completing with input spaces after lookup = "", input_spaces)\n\n        # Allow the Component to check its input Space and catch input-space errors.\n        self.check_input_spaces(input_spaces, action_space)\n\n        # Allow the Component to create all its variables.\n        if get_backend() == ""tf"":\n            # TODO: write custom scope generator for devices (in case None, etc..).\n            if device is not None:\n                with tf.device(device):\n                    if self.reuse_variable_scope:\n                        with tf.variable_scope(name_or_scope=self.reuse_variable_scope, reuse=tf.AUTO_REUSE):\n                            self.create_variables(input_spaces, action_space)\n                    else:\n                        with tf.variable_scope(self.global_scope):\n                            self.create_variables(input_spaces, action_space)\n            else:\n                if self.reuse_variable_scope:\n                    with tf.variable_scope(name_or_scope=self.reuse_variable_scope, reuse=tf.AUTO_REUSE):\n                        self.create_variables(input_spaces, action_space)\n                else:\n                    with tf.variable_scope(self.global_scope):\n                        self.create_variables(input_spaces, action_space)\n\n        elif get_backend() == ""pytorch"":\n            # No scoping/devices here, handled at tensor level.\n            self.create_variables(input_spaces, action_space)\n        # Add all created variables up the parent/container hierarchy.\n        self.propagate_variables()\n\n        self.built = True\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        """"""\n        Should check on the nature of all in-Sockets Spaces of this Component. This method is called automatically\n        by the Model when all these Spaces are know during the Model\'s build time.\n\n        Args:\n            input_spaces (Dict[str,Space]): A dict with Space/shape information.\n                keys=in-Socket name (str); values=the associated Space\n            action_space (Optional[Space]): The action Space of the Agent/GraphBuilder. Can be used to construct and\n                connect more Components (which rely on this information). This eliminates the need to pass the\n                action Space information into many Components\' constructors.\n        """"""\n        pass\n\n    def create_variables(self, input_spaces, action_space=None):\n        """"""\n        Should create all variables that are needed within this component,\n        unless a variable is only needed inside a single _graph_fn-method, in which case,\n        it should be created there.\n        Variables must be created via the backend-agnostic self.get_variable-method.\n\n        Note that for different scopes in which this component is being used, variables will not(!) be shared.\n\n        Args:\n            input_spaces (Dict[str,Space]): A dict with Space/shape information.\n                keys=in-Socket name (str); values=the associated Space\n            action_space (Optional[Space]): The action Space of the Agent/GraphBuilder. Can be used to construct and\n                connect more Components (which rely on this information). This eliminates the need to pass the action\n                Space information into many Components\' constructors.\n        """"""\n        pass\n\n    def register_variables(self, *variables):\n        """"""\n        Adds already created Variables to our registry. This could be useful if the variables are not created\n        by our own `self.get_variable` method, but by some backend-specific object (e.g. tf.layers).\n        Also auto-creates summaries (regulated by `self.summary_regexp`) for the given variables.\n\n        Args:\n            # TODO check if we warp PytorchVariable\n            variables (Union[PyTorchVariable, SingleDataOp]): The Variable objects to register.\n        """"""\n        for var in variables:\n            # Use our global_scope plus the var\'s name without anything in between.\n            # e.g. var.name = ""dense-layer/dense/kernel:0"" -> key = ""dense-layer/kernel""\n            # key = re.sub(r\'({}).*?([\\w\\-.]+):\\d+$\'.format(self.global_scope), r\'\\1/\\2\', var.name)\n            key = re.sub(r\':\\d+$\', """", var.name)\n            # Already registered: Must be the same (shared) variable.\n            if key in self.variable_registry:\n                assert self.variable_registry[key] is var,\\\n                    ""ERROR: Key \'{}\' in {}.variables already exists, but holds a different variable "" \\\n                    ""({} vs {})!"".format(key, self.global_scope, self.variable_registry[key], var)\n            # New variable: Register.\n            else:\n                self.variable_registry[key] = var\n                # Auto-create the summary for the variable.\n                scope_to_use = self.reuse_variable_scope or self.global_scope\n                summary_name = var.name[len(scope_to_use) + (1 if scope_to_use else 0):]\n                summary_name = re.sub(r\':\\d+$\', """", summary_name)\n                self.create_summary(summary_name, var)\n\n    def get_variable(self, name="""", shape=None, dtype=""float"", initializer=None, trainable=True,\n                     from_space=None, add_batch_rank=False, add_time_rank=False, time_major=False, flatten=False,\n                     local=False, use_resource=False):\n        """"""\n        Generates or returns a variable to use in the selected backend.\n        The generated variable is automatically registered in this component\'s (and all parent components\')\n        variable-registry under its global-scoped name.\n\n        Args:\n            name (str): The name under which the variable is registered in this component.\n\n            shape (Optional[tuple]): The shape of the variable. Default: empty tuple.\n\n            dtype (Union[str,type]): The dtype (as string) of this variable.\n\n            initializer (Optional[any]): Initializer for this variable.\n\n            trainable (bool): Whether this variable should be trainable. This will be overwritten, if the Component\n                has its own `trainable` property set to either True or False.\n\n            from_space (Optional[Space,str]): Whether to create this variable from a Space object\n                (shape and dtype are not needed then).\n\n            add_batch_rank (Optional[bool,int]): If True and `from_space` is given, will add a 0th (1st) rank (None) to\n                the created variable. If it is an int, will add that int instead of None.\n                Default: False.\n\n            add_time_rank (Optional[bool,int]): If True and `from_space` is given, will add a 1st (0th) rank (None) to\n                the created variable. If it is an int, will add that int instead of None.\n                Default: False.\n\n            time_major (bool): Only relevant if both `add_batch_rank` and `add_time_rank` are True.\n                Will make the time-rank the 0th rank and the batch-rank the 1st rank.\n                Otherwise, batch-rank will be 0th and time-rank will be 1st.\n                Default: False.\n\n            flatten (bool): Whether to produce a FlattenedDataOp with auto-keys.\n\n            local (bool): Whether the variable must not be shared across the network.\n                Default: False.\n\n            use_resource (bool): Whether to use the new tf resource-type variables.\n                Default: False.\n\n        Returns:\n            DataOp: The actual variable (dependent on the backend) or - if from\n                a ContainerSpace - a FlattenedDataOp or ContainerDataOp depending on the Space.\n        """"""\n\n        # Overwrite the given trainable parameter, iff self.trainable is actually defined as a bool.\n        trainable = self.trainable if self.trainable is not None else (trainable if trainable is not None else True)\n\n        # Called as getter.\n        if shape is None and initializer is None and from_space is None:\n            if name not in self.variable_registry:\n                raise KeyError(\n                    ""Variable with name \'{}\' not found in registry of Component \'{}\'!"".format(name, self.name)\n                )\n            # TODO: Maybe try both the pure name AND the name with global-scope in front.\n            return self.variable_registry[name]\n\n        # Called as setter.\n        var = None\n\n        # We are creating the variable using a Space as template.\n        if from_space is not None:\n            var = self._variable_from_space(\n                flatten, from_space, name, add_batch_rank, add_time_rank, time_major, trainable, initializer, local,\n                use_resource\n            )\n\n        # TODO: Revise possible arg combinations, move in utils.\n        elif self.backend == ""python"" or get_backend() == ""python"" or get_backend() == ""pytorch"":\n            if add_batch_rank is not False and isinstance(add_batch_rank, int):\n                if isinstance(add_time_rank, int):\n                    if time_major:\n                        var = [[initializer for _ in range_(add_batch_rank)] for _ in range_(add_time_rank)]\n                    else:\n                        var = [[initializer for _ in range_(add_time_rank)] for _ in range_(add_batch_rank)]\n                else:\n                    var = [initializer for _ in range_(add_batch_rank)]\n            elif add_time_rank is not False and isinstance(add_time_rank, int):\n                var = [initializer for _ in range_(add_time_rank)]\n            elif initializer is not None:\n                # Return\n                var = initializer\n            elif shape is not None:\n                # Use python list if possible:\n                if len(shape) == 1:\n                    if dtype == int:\n                        var = [0 for _ in range(shape[0])]\n                    elif dtype == float:\n                        var = [0.0 for _ in range(shape[0])]\n                else:\n                    if dtype == int:\n                        var = np.zeros(shape, dtype=np.int32)\n                    elif dtype == float:\n                        var = np.zeros(shape, dtype=np.float32)\n            else:\n                var = []\n\n        # Direct variable creation (using the backend).\n        elif get_backend() == ""tf"":\n            # Provide a shape, if initializer is not given or it is an actual Initializer object (rather than an array\n            # of fixed values, for which we then don\'t need a shape as it comes with one).\n            if initializer is None or isinstance(initializer, tf.keras.initializers.Initializer):\n                shape = tuple((() if add_batch_rank is False else\n                               (None,) if add_batch_rank is True else (add_batch_rank,)) + (shape or ()))\n            # Numpyize initializer and give it correct dtype.\n            else:\n                shape = None\n                initializer = np.asarray(initializer, dtype=util.convert_dtype(dtype, ""np""))\n\n            var = tf.get_variable(\n                name=name, shape=shape, dtype=util.convert_dtype(dtype), initializer=initializer, trainable=trainable,\n                collections=[tf.GraphKeys.GLOBAL_VARIABLES if local is False else tf.GraphKeys.LOCAL_VARIABLES],\n                use_resource=use_resource\n            )\n        elif get_backend() == ""tf-eager"":\n            shape = tuple(\n                (() if add_batch_rank is False else (None,) if add_batch_rank is True else (add_batch_rank,)) +\n                (shape or ())\n            )\n\n            var = eager.Variable(\n                name=name, shape=shape, dtype=util.convert_dtype(dtype), initializer=initializer, trainable=trainable,\n                collections=[tf.GraphKeys.GLOBAL_VARIABLES if local is False else tf.GraphKeys.LOCAL_VARIABLES]\n            )\n\n        # TODO: what about python variables?\n        # Registers the new variable with this Component.\n        key = ((self.reuse_variable_scope + ""/"") if self.reuse_variable_scope else\n               (self.global_scope + ""/"") if self.global_scope else """") + name\n        # Container-var: Save individual Variables.\n        # TODO: What about a var from Tuple space?\n        if isinstance(var, OrderedDict):\n            for sub_key, v in var.items():\n                self.variable_registry[key + sub_key] = v\n        else:\n            self.variable_registry[key] = var\n\n        return var\n\n    def _variable_from_space(self, flatten, from_space, name, add_batch_rank, add_time_rank, time_major, trainable,\n                             initializer, local=False, use_resource=False):\n        """"""\n        Private variable from space helper, see \'get_variable\' for API.\n        """"""\n        # Variables should be returned in a flattened OrderedDict.\n        # TODO can we hide this tf.variable_scope somewhere?\n        if get_backend() == ""tf"":\n            if self.reuse_variable_scope is not None:\n                with tf.variable_scope(name_or_scope=self.reuse_variable_scope, reuse=True):\n                    if flatten:\n                        return from_space.flatten(mapping=lambda key_, primitive: primitive.get_variable(\n                            name=name + key_, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank,\n                            time_major=time_major, trainable=trainable, initializer=initializer,\n                            is_python=(self.backend == ""python"" or get_backend() == ""python""),\n                            local=local, use_resource=use_resource\n                        ))\n                    # Normal, nested Variables from a Space (container or primitive).\n                    else:\n                        return from_space.get_variable(\n                            name=name, add_batch_rank=add_batch_rank, trainable=trainable, initializer=initializer,\n                            is_python=(self.backend == ""python"" or get_backend() == ""python""),\n                            local=local, use_resource=use_resource\n                        )\n            else:\n                if flatten:\n                    return from_space.flatten(mapping=lambda key_, primitive: primitive.get_variable(\n                        name=name + key_, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank,\n                        time_major=time_major, trainable=trainable, initializer=initializer,\n                        is_python=(self.backend == ""python"" or get_backend() == ""python""),\n                        local=local, use_resource=use_resource\n                    ))\n                # Normal, nested Variables from a Space (container or primitive).\n                else:\n                    return from_space.get_variable(\n                        name=name, add_batch_rank=add_batch_rank, trainable=trainable, initializer=initializer,\n                        is_python=(self.backend == ""python"" or get_backend() == ""python""),\n                        local=local, use_resource=use_resource\n                    )\n\n        elif get_backend() == ""pytorch"":\n            if flatten:\n                return from_space.flatten(mapping=lambda key_, primitive: primitive.get_variable(\n                    name=name + key_, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank,\n                    time_major=time_major, trainable=trainable, initializer=initializer,\n                    is_python=True, local=local, use_resource=use_resource\n                ))\n            # Normal, nested Variables from a Space (container or primitive).\n            else:\n                return from_space.get_variable(\n                    name=name, add_batch_rank=add_batch_rank, trainable=trainable, initializer=initializer,\n                    is_python=True, local=local, use_resource=use_resource\n                )\n\n    def get_variables(self, *names, **kwargs):\n        """"""\n        Utility method to get one or more component variable(s) by name(s).\n\n        Args:\n            names (List[str]): Lookup name strings for variables. None for all.\n\n        Keyword Args:\n            collections (set): A set of collections to which the variables have to belong in order to be returned here.\n                Default: tf.GraphKeys.TRAINABLE_VARIABLES\n            custom_scope_separator (str): The separator to use in the returned dict for scopes.\n                Default: \'/\'.\n            global_scope (bool): Whether to use keys in the returned dict that include the global-scopes of the\n                Variables. Default: False.\n            get_ref (bool): Whether to return the ref or the value when using PyTorch. Default: False (return\n                values).\n        Returns:\n            dict: A dict mapping variable names to their get_backend variables.\n        """"""\n        if get_backend() == ""tf"":\n            collections = kwargs.pop(""collections"", None) or tf.GraphKeys.GLOBAL_VARIABLES\n            custom_scope_separator = kwargs.pop(""custom_scope_separator"", ""/"")\n            global_scope = kwargs.pop(""global_scope"", True)\n            assert not kwargs, ""{}"".format(kwargs)\n\n            if len(names) == 1 and isinstance(names[0], list):\n                names = names[0]\n            names = util.force_list(names)\n            # Return all variables of this Component (for some collection).\n            if len(names) == 0:\n                collection_variables = tf.get_collection(collections)\n                ret = {}\n                for v in collection_variables:\n                    lookup = re.sub(r\':\\d+$\', """", v.name)\n                    if lookup in self.variable_registry:\n                        if global_scope:\n                            # Replace the scope separator with a custom one.\n                            ret[re.sub(r\'(/|{}|{})\'.format(FLAT_TUPLE_CLOSE, FLAT_TUPLE_OPEN),\n                                       custom_scope_separator, lookup)] = v\n                        else:\n                            ret[re.sub(r\'^.+/\', """", lookup)] = v\n                return ret\n            # Return only variables of this Component by name.\n            else:\n                return self.get_variables_by_name(\n                    *names, custom_scope_separator=custom_scope_separator, global_scope=global_scope\n                )\n        elif get_backend() == ""pytorch"":\n            # There are no collections - just return variables for this component if names are empty.\n            custom_scope_separator = kwargs.pop(""custom_scope_separator"", ""/"")\n            global_scope = kwargs.pop(""global_scope"", True)\n            get_ref = kwargs.pop(""get_ref"", False)\n\n            if len(names) == 0:\n                names = list(self.variable_registry.keys())\n            return self.get_variables_by_name(\n                *names, custom_scope_separator=custom_scope_separator, global_scope=global_scope,\n                get_ref=get_ref\n            )\n\n    def get_variables_by_name(self, *names, **kwargs):\n        """"""\n        Retrieves this components variables by name.\n\n        Args:\n            names (List[str]): List of names of Variable to return.\n\n        Keyword Args:\n            custom_scope_separator (str): The separator to use in the returned dict for scopes.\n                Default: \'/\'.\n            global_scope (bool): Whether to use keys in the returned dict that include the global-scopes of the\n                Variables. Default: False.\n            get_ref (bool): Whether to return the ref or the value when using PyTorch. Default: False (return\n                values).\n        Returns:\n            dict: Dict containing the requested names as keys and variables as values.\n        """"""\n        custom_scope_separator = kwargs.pop(""custom_scope_separator"", ""/"")\n        global_scope = kwargs.pop(""global_scope"", False)\n        get_ref = kwargs.pop(""get_ref"", False)\n        assert not kwargs\n\n        variables = {}\n        if get_backend() == ""tf"":\n            for name in names:\n                global_scope_name = ((self.global_scope + ""/"") if self.global_scope else """") + name\n                if name in self.variable_registry:\n                    variables[re.sub(r\'/\', custom_scope_separator, name)] = self.variable_registry[name]\n                elif global_scope_name in self.variable_registry:\n                    if global_scope:\n                        variables[re.sub(r\'/\', custom_scope_separator, global_scope_name)] = self.variable_registry[\n                            global_scope_name]\n                    else:\n                        variables[name] = self.variable_registry[global_scope_name]\n        elif get_backend() == ""pytorch"":\n            # Unpack tuple.\n            if isinstance(names, tuple) and len(names) == 1:\n                names = names[0]\n            # print(""names = "", names)\n            state = None\n            if TraceContext.DEFINE_BY_RUN_CONTEXT == ""execution"" and hasattr(self, ""get_state""):\n                state = self.get_state()\n            for name in names:\n                global_scope_name = ((self.global_scope + ""/"") if self.global_scope else """") + name\n                if state is not None:\n                    # Generally using underscores in attribute names, not scope separates\n                    lookup = name.replace(""-"", ""_"")\n                    if lookup in state:\n                        variables[name] = state[lookup]\n                    elif name in state:\n                        variables[name] = state[name]\n                elif name in self.variable_registry:\n                    if get_ref:\n                        variables[re.sub(r\'/\', custom_scope_separator, name)] = self.variable_registry[name]\n                    else:\n                        variables[re.sub(r\'/\', custom_scope_separator, name)] = self.read_variable(self.variable_registry[name])\n                elif global_scope_name in self.variable_registry:\n                    if global_scope:\n                        if get_ref:\n                            variables[re.sub(r\'/\', custom_scope_separator, global_scope_name)] = \\\n                                self.variable_registry[global_scope_name]\n                        else:\n                            variables[re.sub(r\'/\', custom_scope_separator, global_scope_name)] = \\\n                                self.read_variable(self.variable_registry[global_scope_name])\n                    else:\n                        if get_ref:\n                            variables[name] = self.variable_registry[global_scope_name]\n                        else:\n                            variables[name] = self.read_variable(self.variable_registry[global_scope_name])\n\n        return variables\n\n    def create_summary(self, name, values, summary_type=""histogram""):\n        """"""\n        Creates a summary op (and adds it to the graph).\n        Skips those, whose full name does not match `self.summary_regexp`.\n\n        Args:\n            name (str): The name for the summary. This has to match `self.summary_regexp`.\n                The name should not contain a ""summary""-prefix or any global scope information\n                (both will be added automatically by this method).\n\n            values (op): The op to summarize.\n\n            summary_type (str): The summary type to create. Currently supported are:\n                ""histogram"", ""scalar"" and ""text"".\n        """"""\n        # Prepend the ""summaries/""-prefix.\n        name = ""summaries/"" + name\n        # Get global name.\n        global_name = ((self.global_scope + ""/"") if self.global_scope else """") + name\n        # Skip non matching summaries (all if summary_regexp is None).\n        if self.summary_regexp is None or not re.search(self.summary_regexp, global_name):\n            return\n\n        summary = None\n        if get_backend() == ""tf"":\n            ctor = getattr(tf.summary, summary_type)\n            summary = ctor(name, values)\n\n        # Registers the new summary with this Component.\n        if global_name in self.summaries:\n            raise RLGraphError(""ERROR: Summary with name \'{}\' already exists in {}\'s summary ""\n                               ""registry!"".format(global_name, self.name))\n        self.summaries[global_name] = summary\n        self.propagate_summary(global_name)\n\n    def propagate_summary(self, summary_key):\n        """"""\n        Propagates a single summary op of this Component to its parents\' summaries registries.\n\n        Args:\n            summary_key (str): The lookup key for the summary to propagate.\n        """"""\n        # Return if there is no parent.\n        if self.parent_component is None:\n            return\n\n        # If already there -> Error.\n        if summary_key in self.parent_component.summaries:\n            raise RLGraphError(""ERROR: Summary registry of \'{}\' already has a summary under key \'{}\'!"".\n                               format(self.parent_component.name, summary_key))\n        self.parent_component.summaries[summary_key] = self.summaries[summary_key]\n\n        # Recurse up the container hierarchy.\n        self.parent_component.propagate_summary(summary_key)\n\n    def add_components(self, *components, expose_apis=None):\n        """"""\n        Adds sub-components to this one.\n\n        Args:\n            components (List[Component]): The list of Component objects to be added into this one.\n\n            expose_apis (Optional[Set[str],Dict[str,str]]): An optional set of strings with API-methods of the child\n                component that should be exposed as the parent\'s API via a simple wrapper API-method for the parent\n                (that calls the child\'s API-method).\n        """"""\n        if expose_apis is None:\n            expose_apis = {}\n        elif isinstance(expose_apis, str):\n            expose_apis = {expose_apis}\n\n        for component in components:\n            # Safety measure: Ignore Nones.\n            if component is None:\n                continue\n\n            # Try to create Component from spec.\n            if not isinstance(component, Component):\n                component = Component.from_spec(component)\n            # Make sure no two components with the same name are added to this one (own scope doesn\'t matter).\n            if component.name in self.sub_components:\n                raise RLGraphError(""ERROR: Sub-Component with name \'{}\' already exists in \'{}\'!"".\n                                   format(component.name, self.global_scope))\n            # Make sure each Component can only be added once to a parent/container Component.\n            elif component.parent_component is not None:\n                raise RLGraphError(\n                    ""ERROR: Sub-Component with name \'{}\' has already been added once to a containing Component! Each ""\n                    ""Component can only be added once to a parent."".format(component.name)\n                )\n            # Make sure we don\'t add to ourselves.\n            elif component is self:\n                raise RLGraphError(""ERROR: Cannot add a Component ({}) as a sub-Component to itself!"".format(self.name))\n            component.parent_component = self\n            component.nesting_level = (self.nesting_level or 0) + 1\n            component.graph_builder = self.graph_builder\n            self.sub_components[component.name] = component\n\n            # Fix the sub-component\'s (and sub-sub-component\'s etc..) scope(s).\n            self.propagate_scope(component)\n\n            # Execution modes must be coherent within one component subgraph.\n            self.propagate_sub_component_properties(\n                properties=dict(execution_mode=self.execution_mode, nesting_level=(self.nesting_level or 0) + 1),\n                component=component\n            )\n\n            # Should we expose some API-methods of the child?\n            # Only if parent does not have that method yet (otherwise, use parent method).\n            for api_method_name, api_method_rec in component.api_methods.items():\n                if api_method_name in expose_apis and api_method_name not in self.api_methods:\n                    exposed_api_method_name = api_method_name if isinstance(expose_apis, set) else \\\n                        expose_apis[api_method_name]\n                    # Build exposed method code per string, then eval it.\n                    code = ""@rlgraph_api(component=self, must_be_complete={}, ok_to_overwrite=False)\\n"".format(\n                        api_method_rec.must_be_complete\n                    )\n                    code += ""def {}(self, "".format(exposed_api_method_name)\n                    args_str = """"\n                    args_str_w_default = """"\n                    for i, ak in enumerate(api_method_rec.non_args_kwargs):\n                        args_str += ak + "", ""\n                        args_str_w_default += ak + (""=""+str(api_method_rec.default_values[api_method_rec.default_args.index(ak)]) if ak in api_method_rec.default_args else """") + "", ""\n                    args_str += (""*""+api_method_rec.args_name+"", "" if api_method_rec.args_name else """")\n                    args_str += (""**""+api_method_rec.kwargs_name+"", "" if api_method_rec.kwargs_name else """")\n                    args_str = args_str[:-2]  # -2=cut last \', \'\n                    code += args_str_w_default[:-2] + ""):\\n"" # -2=cut last \', \'\n                    code += ""\\treturn getattr(self.sub_components[\'{}\'], \'{}\')({})\\n"".format(component.name, api_method_name, args_str)\n                    #print(""Expose API {} from {} to {} code:\\n"".format(api_method_name, component.name, self.name) + code)\n                    exec(code, globals(), locals())\n\n        # Add own reusable scope to front of all sub-components\' reusable scope.\n        if self.reuse_variable_scope is not None:\n            # Propagate reuse_variable_scope down to the added Component\'s sub-components.\n            self.propagate_sub_component_properties(\n                properties=dict(reuse_variable_scope=self.reuse_variable_scope)\n            )\n\n    def get_all_sub_components(self, list_=None, level_=0, exclude_self=False):\n        """"""\n        Returns all sub-Components (including self, unless `exclude_self` is True) sorted by their nesting-level\n        (... grand-children before children before parents). If the nesting level is the same, sort alphabetically\n        by the scope (name) of the Components.\n\n        Args:\n            list_ (Optional[List[Component]])): A list of already collected components to append to.\n            level_ (int): The slot indicating the Component level depth in `list_` at which we are currently.\n            exclude_self (bool): Whether `self` should be returned as the last sub-Component in the list.\n                Default: True.\n\n        Returns:\n            List[Component]: A list with all the sub-components in `self` (and `self` itself if `exclude_self` is False).\n        """"""\n        return_ = False\n        # This is the final-return recursive call-level.\n        if list_ is None:\n            list_ = {}\n            return_ = True\n        if level_ not in list_:\n            list_[level_] = []\n        list_[level_].append(self)\n        level_ += 1\n        for sub_component in self.sub_components.values():\n            sub_component.get_all_sub_components(list_, level_)\n        if return_:\n            ret = []\n            for l in sorted(list_.keys(), reverse=True):\n                ret.extend(sorted(list_[l], key=lambda c: c.scope))\n            if exclude_self:\n                return ret[:-1]\n            return ret\n\n    def get_sub_component_by_global_scope(self, scope):\n        """"""\n        Returns a sub-Component (or None if not found) by scope. The sub-coponent\'s scope should be given\n        as global scope of the sub-component (not local scope with respect to this Component).\n\n        Args:\n            scope (str): The global scope of the sub-Component we are looking for.\n\n        Returns:\n            Component: The sub-Component with the given global scope if found, None if not found.\n        """"""\n        # TODO: make method more efficient.\n        components = self.get_all_sub_components(exclude_self=True)\n        for component in components:\n            if component.global_scope == scope:\n                return component\n        return None\n\n    def get_sub_component_by_name(self, name):\n        """"""\n        Returns a sub-Component (or None if not found) by its name (local scope). The sub-Component must be a direct\n        sub-Component of `self`.\n\n        Args:\n            name (str): The name (local scope) of the sub-Component we are looking for.\n\n        Returns:\n            Component: The sub-Component with the given name if found, None if not found.\n\n        Raises:\n            RLGraphError: If a sub-Component by that name could not be found.\n        """"""\n        sub_component = self.sub_components.get(name, None)\n        if sub_component is None:\n            raise RLGraphError(""ERROR: sub-Component with name \'{}\' not found in \'{}\'!"".format(name, self.global_scope))\n        return sub_component\n\n    def remove_sub_component_by_name(self, name):\n        """"""\n        Removes a sub-component from this one by its name. Thereby sets the `parent_component` property of the\n        removed Component to None.\n        Raises an error if the sub-component does not exist.\n\n        Args:\n            name (str): The name of the sub-component to be removed.\n\n        Returns:\n            Component: The removed component.\n        """"""\n        assert name in self.sub_components, ""ERROR: Component {} cannot be removed because it is not"" \\\n            ""a sub-component. Sub-components by name are: {}."".format(name, list(self.sub_components.keys()))\n        removed_component = self.sub_components.pop(name)\n        # Set parent of the removed component to None.\n        removed_component.parent_component = None\n        return removed_component\n\n    def get_parents(self):\n        """"""\n        Returns a list of parent and grand-parents of this component.\n\n        Returns:\n            List[Component]: A list (may be empty if this component has no parents) of all parent and grand-parents.\n        """"""\n        ret = []\n        component = self\n        while component.parent_component is not None:\n            ret.append(component.parent_component)\n            component = component.parent_component\n        return ret\n\n    def propagate_scope(self, sub_component):\n        """"""\n        Fixes all the sub-Component\'s (and its sub-Component\'s) global_scopes.\n\n        Args:\n            sub_component (Optional[Component]): The sub-Component object whose global_scope needs to be updated.\n                Use None for this Component itself.\n        """"""\n        # TODO this should be moved to use generic method below, but checking if global scope if set\n        # TODO does not work well within that.\n        if sub_component is None:\n            sub_component = self\n        elif self.global_scope:\n            sub_component.global_scope = self.global_scope + (\n                (""/"" + sub_component.scope) if sub_component.scope else """")\n\n        # Recurse.\n        for sc in sub_component.sub_components.values():\n            sub_component.propagate_scope(sc)\n\n    def propagate_sub_component_properties(self, properties, component=None, recursive_=False):\n        """"""\n        Recursively updates properties of component and its sub-components.\n\n        Args:\n            properties (dict): Dict with names of properties and their values to recursively update\n                sub-components with.\n\n            component (Optional([Component])): Component to recursively update. Uses self if None.\n\n            recursive_ (bool): Whether this is a recursive (sub-Component) call. Default: False.\n        """"""\n        if component is None:\n            component = self\n        properties_scoped = copy.deepcopy(properties)\n        for name, value in properties.items():\n            # Property is some scope (value is then that scope of the parent component).\n            # Have to align it with sub-component\'s local scope.\n            if value and (name == ""global_scope"" or name == ""reuse_variable_scope""):\n                # For the parent component, do not add its scope to the shared-scope.\n                if recursive_ is True:\n                    value += ((""/"" + component.scope) if component.scope else """")\n                properties_scoped[name] = value\n                setattr(component, name, value)\n            # Normal property: Set to static given value.\n            else:\n                setattr(component, name, value)\n                # Nesting_level: Increase by one the deeper we go.\n                if name == ""nesting_level"":\n                    properties_scoped[name] = value + 1\n        for sc in component.sub_components.values():\n            component.propagate_sub_component_properties(properties_scoped, sc, recursive_=True)\n\n    def propagate_variables(self, keys=None):\n        """"""\n        Propagates all variable from this Component to its parents\' variable registries.\n\n        Args:\n            keys (Optional[List[str]]): An optional list of variable names to propagate. Should only be used in\n                internal, recursive calls to this same method.\n        """"""\n        # Return if there is no parent.\n        if self.parent_component is None:\n            return\n\n        # Add all our variables to parent\'s variable registry.\n        keys = keys or self.variable_registry.keys()\n        for key in keys:\n            # If already there (bubbled up from some child component that was input complete before us)\n            # -> Make sure the variable object is identical.\n            if key in self.parent_component.variable_registry:\n                if self.variable_registry[key] is not self.parent_component.variable_registry[key]:\n                    raise RLGraphError(""ERROR: Variable registry of \'{}\' already has a variable under key \'{}\'!"". \\\n                          format(self.parent_component.name, key))\n            self.parent_component.variable_registry[key] = self.variable_registry[key]\n\n        # Recurse up the container hierarchy.\n        self.parent_component.propagate_variables(keys)\n\n    def copy(self, name=None, scope=None, device=None, trainable=None,\n             reuse_variable_scope=None, reuse_variable_scope_for_sub_components=None):\n        """"""\n        Copies this component and returns a new component with possibly another name and another scope.\n        The new component has its own variables (they are not shared with the variables of this component as they\n        will be created after this copy anyway, during the build phase).\n        and is initially not connected to any other component.\n\n        Args:\n            name (str): The name of the new Component. If None, use the value of scope.\n            scope (str): The scope of the new Component. If None, use the same scope as this component.\n            device (str): The device of the new Component. If None, use the same device as this one.\n\n            trainable (Optional[bool]): Whether to make all variables in this component trainable or not. Use None\n                for no specific preference.\n\n            reuse_variable_scope (Optional[str]): If not None, variables of the copy will be shared under this scope.\n\n            reuse_variable_scope_for_sub_components (Optional[str]): If not None, variables only of the sub-components\n                of the copy will be shared under this scope.\n\n        Returns:\n            Component: The copied component object.\n        """"""\n        # Make sure we are still in the assembly phase (should not copy actual ops).\n        assert self.built is False, ""ERROR: Cannot copy a Component (\'{}\') that has already been built!"". \\\n            format(self.name)\n\n        if scope is None:\n            scope = self.scope\n        if name is None:\n            name = scope\n        if device is None:\n            device = self.device\n        if trainable is None:\n            trainable = self.trainable\n\n        # Remove the parent ref (will be set to None for the copy anyway).\n        parent_ref = self.parent_component\n        self.parent_component = None\n\n        # Make sure, containing Agents are not copied either.\n        agent_ref = None\n        if hasattr(self, ""agent""):\n            agent_ref = self.agent\n            self.agent = None\n\n        # Simply deepcopy self and change name and scope.\n        new_component = copy.deepcopy(self)\n        new_component.name = name\n        new_component.scope = scope\n        # Change global_scope for the copy and all its sub-components.\n        new_component.global_scope = scope\n        new_component.propagate_scope(sub_component=None)\n\n        # Propagate reusable scope, device and other trainable.\n        new_component.propagate_sub_component_properties(\n            properties=dict(device=device, trainable=trainable)\n        )\n        if reuse_variable_scope:\n            new_component.propagate_sub_component_properties(dict(reuse_variable_scope=reuse_variable_scope))\n        # Gives us the chance to skip new_component\'s scope.\n        elif reuse_variable_scope_for_sub_components:\n            for sc in new_component.sub_components.values():\n                sc.propagate_sub_component_properties(dict(reuse_variable_scope=reuse_variable_scope_for_sub_components))\n\n        # Put back critical refs.\n        if agent_ref is not None:\n            self.agent = new_component.agent = agent_ref\n        # Leave the copy\'s parent_component at None.\n        self.parent_component = parent_ref\n\n        return new_component\n\n    @staticmethod\n    def scatter_update_variable(variable, indices, updates):\n        """"""\n        Updates a variable. Optionally returns the operation depending on the backend.\n\n        Args:\n            variable (any): Variable to update.\n            indices (array): Indices to update.\n            updates (any):  Update values.\n\n        Returns:\n            Optional[op]: The graph operation representing the update (or None).\n        """"""\n        if get_backend() == ""tf"":\n            return tf.scatter_update(ref=variable, indices=indices, updates=updates)\n\n    @staticmethod\n    def assign_variable(ref, value):\n        """"""\n        Assigns a variable to a value.\n\n        Args:\n            ref (any): The variable to assign to.\n            value (any): The value to use for the assignment.\n\n        Returns:\n            Optional[op]: None or the graph operation representing the assignment.\n        """"""\n        if get_backend() == ""tf"":\n            tensor_type = type(value).__name__\n            if tensor_type == ""Variable"" or tensor_type == ""RefVariable"":\n                return tf.assign(ref=ref, value=value.read_value())\n            else:\n                return tf.assign(ref=ref, value=value)\n        elif get_backend() == ""pytorch"":\n            ref.set_value(value)\n\n    @staticmethod\n    def read_variable(variable, indices=None, dtype=None, shape=None):\n        """"""\n        Reads a variable.\n\n        Args:\n            variable (DataOp): The variable whose value to read.\n            indices (Optional[np.ndarray,tf.Tensor]): Indices (if any) to fetch from the variable.\n            dtype (Optional[torch.dtype]): Optional dtype to convert read values to.\n            shape (Optional[tuple]): Optional default shape.\n\n        Returns:\n            any: Variable values.\n        """"""\n        if get_backend() == ""tf"":\n            if indices is not None:\n                # Could be redundant, question is if there may be special read operations\n                # in other backends, or read from remote variable requiring extra args.\n                return tf.gather(params=variable, indices=indices)\n            else:\n                return variable\n        elif get_backend() == ""pytorch"":\n            # PyTorchVariable is used to store torch parameters (e.g. layers).\n            if isinstance(variable, PyTorchVariable):\n                return variable.get_value()\n            # Lists or numpy arrays may be used to store mutable state that does not need\n            # tensor operations.\n            elif isinstance(variable, list) or isinstance(variable, np.ndarray):\n                if TraceContext.DEFINE_BY_RUN_CONTEXT == ""building"" \\\n                        and shape is not None and (indices is None or len(indices) == 0):\n                    return torch.zeros(shape, dtype=dtype)\n\n                if indices is not None:\n                    ret = []\n                    for i in indices:\n                        val = variable[i]\n                        # Type checking is necessary because torch.stack only works on same types.\n                        if isinstance(val, torch.Tensor):\n                            if dtype is None:\n                                ret.append(val)\n                            elif dtype == torch.float32:\n                                ret.append(val.float())\n                            elif dtype == torch.int32:\n                                ret.append(val.int())\n                            elif dtype == torch.uint8:\n                                ret.append(val.byte())\n                    # Stack list into one Tensor with a btach dim.\n                    return torch.stack(ret) if ret else ret\n                else:\n                    return variable\n            else:\n                # Catch all for raw types.\n                return variable\n\n    def sub_component_by_name(self, scope_name):\n        """"""\n        Returns a sub-component of this component by its name.\n\n        Args:\n            scope_name (str): Name of the component. This is typically its scope.\n\n        Returns:\n            Component: Sub-component if it exists.\n\n        Raises:\n            ValueError: Error if no sub-component with this name exists.\n        """"""\n        if scope_name not in self.sub_components:\n            raise RLGraphError(\n                ""Name {} is not a valid sub-component name for component {}. Sub-Components are: {}"".\n                format(scope_name, self.__str__(), self.sub_components.keys())\n            )\n        return self.sub_components[scope_name]\n\n    def _post_build(self, component):\n        component.post_define_by_run_build()\n        for sub_component in component.sub_components.values():\n            self._post_build(sub_component)\n\n    def post_define_by_run_build(self):\n        """"""\n        Optionally execute post-build calls.\n        """"""\n        # Try resetting state.\n        component_state = self.get_state()\n        if component_state is not None:\n            for name in component_state.keys():\n                if hasattr(self, name) and isinstance(getattr(self, name), (float, int)):\n                    self.__setattr__(name, 0)\n\n    def get_state(self):\n        """"""\n        Optionally provide define-by-run state as dict.\n\n        Returns:\n            dict: Names and values of variables.\n        """"""\n        pass\n\n    def get_helper_component(self, type_, *args, **kwargs):\n        """"""\n        Returns a helper component of the given type (only one helper component per type is allowed\n        and necessary). If a helper of the type does not exist yet in `self`, create a new one.\n\n        Args:\n            type_ (str): The type as a string of the helper Component to return.\n\n        Returns:\n            Component: The helper component.\n        """"""\n        name = "".helper-""+type_+""-{}"".format(uuid.uuid4())\n        helper = self.sub_components.get(name)\n        if helper is None:\n            kwargs.update(dict(type=type_, scope=name))\n            if len(args) > 0:\n                kwargs.update({""_args"": args})\n            helper = Component.from_spec(kwargs)\n            self.add_components(helper)\n        return helper\n\n    @rlgraph_api(returns=1, requires_variable_completeness=True)\n    def _graph_fn_variables(self):\n        """"""\n        Outputs all of this Component\'s variables in a DataOpDict (API-method ""variables"").\n\n        This can be used e.g. to sync this Component\'s variables into another Component, which owns\n        a Synchronizable() as a sub-component. The returns values of this graph_fn are then sent into\n        the other Component\'s API-method `sync` (parameter: ""values"") for syncing.\n\n        Returns:\n            DataOpDict: Dict with keys=variable names and values=variable (SingleDataOp).\n        """"""\n        # Must use custom_scope_separator here b/c RLGraph doesn\'t allow Dict with \'/\'-chars in the keys.\n        # \'/\' could collide with a FlattenedDataOp\'s keys and mess up the un-flatten process.\n        variables_dict = self.get_variables(custom_scope_separator=""-"")\n        return DataOpDict(variables_dict)\n\n    def register_summary_op(self, summary_op):\n        """"""\n        Registers a summary operation with the current graph function (position -1 on the stack).\n\n        Args:\n            summary_op (DataOp): The summary op to add to the last registry item in the stack.\n        """"""\n        self._summary_ops_buffer_stack[-1].append(summary_op)\n\n    def pop_summary_ops_buffer(self):\n        """"""\n        *Internal use only!* Pops the last frame of the summary ops buffer stack.\n\n        Returns:\n            The accumulated summary ops.\n        """"""\n        return self._summary_ops_buffer_stack.pop()\n\n    def start_summary_ops_buffer(self):\n        """"""\n        *Internal use only!* Starts a new frame in the summary ops buffer stack.\n        """"""\n        self._summary_ops_buffer_stack.append([])\n\n    def _variables(self):\n        """"""\n        OBSOLETED API method. New ""variables()"" should be used.\n        """"""\n        raise RLGraphObsoletedError(""API-method"", ""_variables()"", ""variables()"")\n\n    @staticmethod\n    def reset_profile():\n        """"""\n        Sets profiling values to 0.\n        """"""\n        Component.call_count = 0\n        Component.call_times = []\n\n    def __str__(self):\n        return ""{}(\'{}\' api={})"".format(type(self).__name__, self.name, str(list(self.api_methods.keys())))\n'"
rlgraph/environments/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.environments.environment import Environment\nfrom rlgraph.environments.deterministic_env import DeterministicEnv\nfrom rlgraph.environments.gaussian_density_as_reward_env import GaussianDensityAsRewardEnv\nfrom rlgraph.environments.grid_world import GridWorld\nfrom rlgraph.environments.openai_gym import OpenAIGymEnv\nfrom rlgraph.environments.random_env import RandomEnv\nfrom rlgraph.environments.vector_env import VectorEnv\nfrom rlgraph.environments.sequential_vector_env import SequentialVectorEnv\n\nEnvironment.__lookup_classes__ = dict(\n    deterministic=DeterministicEnv,\n    deterministicenv=DeterministicEnv,\n    gaussiandensity=GaussianDensityAsRewardEnv,\n    gaussiandensityasreward=GaussianDensityAsRewardEnv,\n    gaussiandensityasrewardenv=GaussianDensityAsRewardEnv,\n    gridworld=GridWorld,\n    gridworldenv=GridWorld,\n    openai=OpenAIGymEnv,\n    openaigym=OpenAIGymEnv,\n    openaigymenv=OpenAIGymEnv,\n    random=RandomEnv,\n    randomenv=RandomEnv,\n    sequentialvector=SequentialVectorEnv,\n    sequentialvectorenv=SequentialVectorEnv\n)\n\ntry:\n    import deepmind_lab\n\n    # If import works: Can import our Adapter.\n    from rlgraph.environments.deepmind_lab import DeepmindLabEnv\n\n    Environment.__lookup_classes__.update(dict(\n        deepmindlab=DeepmindLabEnv,\n        deepmindlabenv=DeepmindLabEnv,\n    ))\n    # TODO travis error on this, investigate.\nexcept Exception:\n    pass\n\n\ntry:\n    import mlagents\n\n    # If import works: Can import our Adapter.\n    from rlgraph.environments.mlagents_env import MLAgentsEnv\n\n    Environment.__lookup_classes__.update(dict(\n        mlagents=MLAgentsEnv,\n        mlagentsenv=MLAgentsEnv,\n    ))\n    # TODO travis error on this, investigate.\nexcept Exception:\n    pass\n\n\n__all__ = [""Environment""] + \\\n          list(set(map(lambda x: x.__name__, Environment.__lookup_classes__.values())))\n'"
rlgraph/environments/deepmind_lab.py,0,"b'# Copyright 2018/2019 The RLgraph authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport deepmind_lab\nimport numpy as np\nimport time\n\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\nfrom rlgraph.environments.environment import Environment\nfrom rlgraph.spaces import *\nfrom rlgraph.utils.util import force_list, default_dict\n\n\nclass DeepmindLabEnv(Environment):\n    """"""\n    Deepmind Lab Environment adapter for RLgraph:\n    https://arxiv.org/abs/1612.03801\n    https://github.com/deepmind/lab\n\n    Also note this paper, which uses the deepmind Lab as environment:\n    [1] IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures - Espeholt, Soyer,\n        Munos et al. - 2018 (https://arxiv.org/abs/1802.01561)\n    """"""\n    def __init__(self, level_id, observations=""RGB_INTERLEAVED"", actions=None, frameskip=4, config=None,\n                 renderer=""software"", seed=None, level_cache=None):\n        """"""\n        Args:\n            level_id (str): Specifier of the level to play, e.g. \'seekavoid_arena_01\'.\n            observations (Union[str,List[str]]): String specifier(s) for the observation(s) to be used with the\n                given level. Will be converted into either a (single) BoxSpace or a Tuple (of BoxSpaces).\n                See deepmind\'s documentation for all available observations.\n            actions (Optional[List[dict]]): The RLgraph action spec (currently, only IntBox (shape=()) RLgraph action\n                spaces are supported) that will be translated from and to the deepmind Lab actions.\n                List slots correspond to the single int-actions, list items are dicts with:\n                key=deepmind Lab partial action name e.g. LOOK_LEFT_RIGHT_PIXELS_PER_FRAME.\n                value=the value for that deepmind Lab partial action e.g. -100.\n            frameskip (Optional[Tuple[int,int],int]): How many frames should be skipped with (repeated action and\n                accumulated reward). E.g. (2,5) -> Uniformly pull from set [2,3,4].\n                Default: 4.\n            config (Optional[dict]): The `config` parameter to be passed into the Lab\'s constructor.\n                Supports \'width\', \'height\', \'fps\', and other useful parameters.\n                Values must be given as string values. e.g. dict(width=\'96\')\n            renderer (str): The `renderer` parameter to be passed into the Lab\'s constructor.\n            seed (Optional[int]): An optional seed to use to initialize a numpy random state object, which is then used\n                to seed all occurring resets in a deterministic fashion.\n            level_cache (Optional[object]): An optional custom level caching object to help increase performance\n                when playing many repeating levels. Will be passed as is into the Lab\'s constructor.\n        """"""\n        # Create the wrapped deepmind lab level object.\n        self.level_id = level_id\n        observations = force_list(observations)\n        config = default_dict(config, dict(width=\'96\', height=\'72\', fps=\'60\'))  # Default config.\n        self.level = deepmind_lab.Lab(\n            self.level_id, observations, config=config, renderer=renderer, level_cache=level_cache\n        )\n\n        # Dict mapping a discrete action (int) - we don\'t support continuous actions yet - into a\n        # deepmind Lab action vector.\n        self.action_list, action_space = self.define_actions(actions)\n        observation_space = self.define_observations(observations)\n        super(DeepmindLabEnv, self).__init__(observation_space, action_space)\n\n        self.frameskip = frameskip\n        self.random_state = np.random.RandomState(seed=seed or int(time.time()))\n        self.reset()\n\n    def terminate(self):\n        """"""\n        Shuts down the underlying Quake III Arena instance.\n        Invalidates `self.level` such that no other method calls are possible afterwards.\n        """"""\n        self.level.close()\n        self.level = None\n\n    def reset(self):\n        print(""\\n----------------------------\\nResetting DM Lab Env.\\n----------------------------\\n"")\n        self.level.reset(seed=self.random_state.randint(0, 2 ** 31 - 1))\n        state = self.level.observations()\n        return state\n\n    def reset_flow(self):\n        state = self.reset()\n        if isinstance(self.state_space, Dict):\n            return [state[key] for key in self.state_space]\n        else:\n            return state[next(iter(state))]\n\n    def step(self, actions):\n        # Do the actual step.\n        reward = self.level.step(action=self.action_list[actions], num_steps=self.frameskip)\n        terminal = not self.level.is_running()\n        # Quirk in DM Lab: A terminal state cannot be observed anymore\n        # (which is fine as its state-value (and action choices) is always 0 anyway).\n        state = self.level.observations() if terminal is False else self.state_space.zeros()\n\n        # Return state, reward, terminal, and None (info).\n        return state, np.array(reward, dtype=np.float32), terminal, None\n\n    def step_flow(self, actions):\n        # Do the actual step.\n        # TODO: Remove this impala hack again (just to see whether looking up action in graph is faster).\n        reward = self.level.step(action=self.action_list[actions], num_steps=self.frameskip)\n        #reward = self.level.step(action=actions, num_steps=self.frameskip)\n        terminal = np.array(not self.level.is_running())\n        # Flow Env logic.\n        if terminal:\n            state = self.reset()\n        else:\n            state = self.level.observations()\n\n        # Return state, reward, terminal.\n        if isinstance(self.state_space, Dict):\n            return [state[key] for key in self.state_space] + [np.array(reward, dtype=np.float32), terminal]\n        else:\n            return [state[next(iter(state))], np.array(reward, dtype=np.float32), terminal]\n\n    @staticmethod\n    def define_actions(actions_spec=None):\n        """"""\n        Translates and maps Rlgraph IntBox(shape=()) actions - provided by user - to the correct deepmind Lab\n        representation for the calls to `step`.\n\n        Args:\n            actions_spec (List[dict]): The discrete action definitions to be supported by this Environment.\n\n        Returns:\n            tuple:\n            - A lookup list of deepmind actions, where the slot is the RLgraph IntBox value\n            and the items are numpy arrays (with dtype=np.intc) that are understood by deepmind Lab.\n            - The RLgraph action Space (IntBox(shape=(), n)), where n is the number of discrete actions.\n        """"""\n        # Default actions: The ones used in the IMPALA paper (see [1]).\n        if actions_spec is None:\n            actions_spec = [\n                dict(MOVE_BACK_FORWARD=1),  # forward\n                dict(MOVE_BACK_FORWARD=-1),  # backward\n                dict(STRAFE_LEFT_RIGHT=1),  # strafe right\n                dict(STRAFE_LEFT_RIGHT=-1),  # strafe left\n                dict(LOOK_LEFT_RIGHT_PIXELS_PER_FRAME=-20),  # look left\n                dict(LOOK_LEFT_RIGHT_PIXELS_PER_FRAME=20),  # look right\n                dict(MOVE_BACK_FORWARD=1, LOOK_LEFT_RIGHT_PIXELS_PER_FRAME=-20),  # forward + look left\n                dict(MOVE_BACK_FORWARD=1, LOOK_LEFT_RIGHT_PIXELS_PER_FRAME=20),  # forward + look right\n                dict(FIRE=1),  # fire\n            ]\n\n        # Build the lookup dict mapping ints to deepmind-readable actions (numpy intc arrays).\n        lookup_list = list()\n        for action in actions_spec:\n            assert isinstance(action, dict), ""ERROR: Single action spec \'{}\' must be a dict!"".format(action)\n            lookup_list.append(np.array([0] * 7, dtype=np.intc))\n            for name, value in action.items():\n                # TODO: Sanity check values for deepmind lab bounds.\n                slot = 0 if name == ""LOOK_LEFT_RIGHT_PIXELS_PER_FRAME"" else 1 \\\n                    if name == ""LOOK_DOWN_UP_PIXELS_PER_FRAME"" else 2 \\\n                    if name == ""STRAFE_LEFT_RIGHT"" else 3 if name == ""MOVE_BACK_FORWARD"" \\\n                    else 4 if name == ""FIRE"" else 5 if name == ""JUMP"" else 6  # 6=CROUCH\n                lookup_list[-1][slot] = value\n\n        # Return the lookup_list and the RLgraph action Space.\n        return lookup_list, IntBox(len(actions_spec))\n\n    def define_observations(self, observation_spec):\n        """"""\n        Creates a RLgraph Space for the given deepmind Lab\'s observation specifier.\n\n        Args:\n            observation_spec (List[str]): A list with the wanted names from the deepmind Lab available observations.\n                Each available observation is a dict with the following keys: name, shape and dtype.\n\n        Returns:\n            Space: The RLgraph equivalent observation Space.\n        """"""\n        dict_space = dict()\n        space = None\n        available_observations = self.level.observation_spec()\n        for observation_name in observation_spec:\n            # Find the observation_item in the observation_spec of the Env.\n            observation_item = [o for o in available_observations if o[""name""] == observation_name][0]\n            if ""float"" in str(observation_item[""dtype""]):\n                space = FloatBox(shape=observation_item[""shape""], dtype=observation_item[""dtype""])\n            elif ""int"" in str(observation_item[""dtype""]):\n                space = IntBox(shape=observation_item[""shape""], dtype=observation_item[""dtype""])\n            elif ""str"" in str(observation_item[""dtype""]):\n                space = TextBox(shape=observation_item[""shape""])\n            else:\n                raise RLGraphError(""Unknown Deepmind Lab Space class for state_space!"")\n\n            dict_space[observation_name] = space\n\n        if len(dict_space) == 1:\n            return space\n        else:\n            return Dict(dict_space)\n\n    def __str__(self):\n        return ""DeepMindLab({})"".format(self.level_id)\n'"
rlgraph/environments/deterministic_env.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph.environments import Environment\nimport rlgraph.spaces as spaces\n\n\nclass DeterministicEnv(Environment):\n    """"""\n    An Env producing a simple float state starting from `state_start` after reset and counting upwards in steps of\n    1.0 (regardless of the actions). Same goes for the reward signal, which starts from `reward_start`.\n    The action space is IntBox(2). Episodes terminate after always `steps_to_terminal` steps.\n    """"""\n    def __init__(self, state_start=0.0, reward_start=-100.0, steps_to_terminal=10):\n        """"""\n        Args:\n            state_start (float): State to start with after reset.\n            reward_start (float): Reward to start with (after first action) after a reset.\n            steps_to_terminal (int): Number of steps after which a terminal signal is raised.\n        """"""\n        super(DeterministicEnv, self).__init__(state_space=spaces.FloatBox(), action_space=spaces.IntBox(2))\n\n        self.state_start = state_start\n        self.reward_start = reward_start\n        self.steps_to_terminal = steps_to_terminal\n\n        self.state = state_start\n        self.reward = reward_start\n        self.steps_into_episode = 0\n\n    def seed(self, seed=None):\n        return seed\n\n    def reset(self):\n        self.steps_into_episode = 0\n        self.state = self.state_start\n        self.reward = self.reward_start\n        return np.array([self.state], dtype=np.float32)\n\n    def reset_flow(self):\n        return self.reset()\n\n    def step(self, actions=None):\n        if actions is not None:\n            assert self.action_space.contains(actions), \\\n                ""ERROR: Given action ({}) in step is not part of action Space ({})!"".format(actions, self.action_space)\n\n        self.state += 1.0\n        reward = self.reward\n        self.reward += 1.0\n        self.steps_into_episode += 1\n        terminal = False\n        if self.steps_into_episode >= self.steps_to_terminal:\n            terminal = True\n        return np.array([self.state], dtype=np.float32), np.array(reward, dtype=np.float32), terminal, None\n\n    def step_flow(self, actions=None):\n        ret = self.step(actions)\n        state = ret[0]\n        if ret[2]:\n            state = self.reset()\n        return state, ret[1], ret[2]\n\n    def __str__(self):\n        return ""DeterministicEnv()""\n'"
rlgraph/environments/environment.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.utils.specifiable import Specifiable\nfrom rlgraph.spaces import Space\n\n\nclass Environment(Specifiable):\n    """"""\n    An Env class used to run experiment-based RL.\n    """"""\n    def __init__(self, state_space, action_space, seed=None):\n        """"""\n        Args:\n            state_space (Union[dict,Space]): The spec-dict for generating the state Space or the state Space object\n                itself.\n            action_space (Union[dict,Space]): The spec-dict for generating the action Space or the action Space object\n                itself.\n            #reward_clipping (Optionalp[Tuple[float,float],float]: An optional reward clipping setting used\n            #    to restrict all rewards produced by the Environment to be in a certain range.\n            #    None for no clipping. Single float for clipping between -`reward_clipping` and +`reward_clipping`.\n        """"""\n        super(Environment, self).__init__()\n\n        self.state_space = Space.from_spec(state_space)\n        self.action_space = Space.from_spec(action_space)\n        # self.reward_clipping = reward_clipping\n\n        # Add some seeding to the created Env.\n        if seed is not None:\n            self.seed(seed)\n\n    def seed(self, seed=None):\n        """"""\n        Sets the random seed of the environment to the given value.\n\n        Args:\n            seed (int): The seed to use (default: current epoch seconds).\n\n        Returns:\n            int: The seed actually used.\n        """"""\n        raise NotImplementedError\n\n    def reset(self):\n        """"""\n        Resets the state of the environment, returning an initial observation.\n\n        Returns:\n            The Env\'s state after the reset.\n        """"""\n        raise NotImplementedError\n\n    def reset_flow(self):\n        """"""\n        A special implementation of `reset` in which the state after the reset is returned as a tuple of flat\n        state-component iff a Dict state is given.\n\n        Returns:\n            The Env\'s state (flat components if Dict) after the reset.\n        """"""\n        pass  # optional\n\n    def step(self, actions, **kwargs):\n        """"""\n        Run one time step of the environment\'s dynamics. When the end of an episode is reached, reset() should be\n        called to reset the environment\'s internal state.\n\n        Args:\n            actions (any): The action(s) to be executed by the environment. Actions have to be members of this\n                Environment\'s action_space (a call to self.action_space.contains(action) must return True)\n\n        Returns:\n            tuple:\n                - The state s\' after(!) executing the given actions(s).\n                - The reward received after taking a in s.\n                - Whether s\' is a terminal state.\n                - Some Environment specific info.\n        """"""\n        raise NotImplementedError\n\n    def step_flow(self, **kwargs):\n        """"""\n        A special implementation of `step` in which `reset` is called automatically if a terminal is encountered, such\n        that only a sequence of `step_flow` is needed in any loop. Always returns the next state or - if terminal -\n        the first state after the reset (and then the last reward before the reset and True for terminal).\n        Also, if a Dict state is given, will flatten it into its single components.\n\n        Args:\n            kwargs (any): The action(s) to be executed by the environment. Actions have to be members of this\n                Environment\'s action_space (a call to self.action_space.contains(action) must return True)\n\n        Returns:\n            tuple:\n                - The state s\' after(!) executing the given actions(s) or after a reset if the action lead to a terminal\n                    state.\n                - The reward received after taking a in s.\n                - Whether s\' is a terminal state.\n        """"""\n        pass  # optional\n\n    def render(self):\n        """"""\n        Should render the Environment in its current state. May be implemented or not.\n        """"""\n        pass\n\n    def terminate(self):\n        """"""\n        Clean up operation. May be implemented or not.\n        """"""\n        pass\n\n    def __str__(self):\n        raise NotImplementedError\n\n\n'"
rlgraph/environments/gaussian_density_as_reward_env.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom scipy import stats\n\nfrom rlgraph.environments.environment import Environment\nfrom rlgraph.spaces import FloatBox\n\n\nclass GaussianDensityAsRewardEnv(Environment):\n    """"""\n    Environment where the reward is always the Gaussian probability density at the given\n    single-float action.\n    The state is a uniformly random value between -1 and 1 and determines the loc of the pdf.\n    The scale of the pdf is fixed in the ctor.\n\n    The optimal policy would be to pick actions that are closest to the current state.\n    """"""\n    def __init__(self, episode_length=5, scale=0.1):\n        super(GaussianDensityAsRewardEnv, self).__init__(\n            state_space=FloatBox(shape=(1,)), action_space=FloatBox(shape=(1,), low=-2.0, high=2.0)\n        )\n        self.episode_length = episode_length\n        self.episode_step = 0\n        self.loc = None\n        self.scale = scale\n\n    def seed(self, seed=None):\n        pass\n\n    def reset(self):\n        self.episode_step = 0\n        self.loc = np.random.uniform(size=(1, )) * 2 - 1\n        return self.loc\n\n    def step(self, actions, **kwargs):\n        reward = stats.norm.pdf(actions, loc=self.loc, scale=self.scale)[0]\n        self.episode_step += 1\n        self.loc = np.random.uniform(size=(1,)) * 2 - 1\n        return self.loc, reward, self.episode_step >= self.episode_length, None\n\n    def get_max_reward(self):\n        max_reward_per_step = stats.norm(loc=0.0, scale=self.scale).pdf(0.0)\n        return self.episode_length * max_reward_per_step\n\n    def __str__(self):\n        return self.__class__.__name__ + ""(episode-len={}, scale={})"".format(self.episode_length, self.scale)\n'"
rlgraph/environments/grid_world.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport random\nimport time\n\nimport math\nimport numpy as np\nfrom six.moves import xrange as range_\n\n# Init pygame?\npygame = None\n\n# Commented out to prevent pygame from loading with every program.\n# try:\n#     import pygame\n#     # Only use pygame if a display is available.\n#     pygame.display.init()\n#\n# except ImportError:\n#     print(""PyGame not installed. No human rendering possible."")\n#     pygame = None\n# except pygame.error:\n#     print(""No display for PyGame available. No human rendering possible."")\n#     pygame = None\n\nfrom rlgraph.spaces import IntBox, FloatBox, BoolBox, Dict\nfrom rlgraph.environments import Environment\n\n\nclass GridWorld(Environment):\n    """"""\n    A classic grid world.\n\n    Possible action spaces are:\n    - up, down, left, right\n    - forward/halt/backward + turn left/right/no-turn + jump (or not)\n\n    The state space is discrete.\n\n    Field types are:\n    \'S\' : starting point\n    \' \' : free space\n    \'W\' : wall (blocks, but can be jumped)\n    \'H\' : hole (terminates episode) (to be replaced by W in save-mode)\n    \'F\' : fire (usually causing negative reward, but can be jumped)\n    \'G\' : goal state (terminates episode)\n\n    TODO: Create an option to introduce a continuous action space.\n    """"""\n    # Some built-in maps.\n    MAPS = {\n        ""chain"": [\n            ""G    S  F G""\n        ],\n        ""long-chain"": [\n            ""                                 S                                 G""\n        ],\n        ""2x2"": [\n            ""SH"",\n            "" G""\n        ],\n        ""4x4"": [\n            ""S   "",\n            "" H H"",\n            ""   H"",\n            ""H  G""\n        ],\n        ""8x8"": [\n            ""S       "",\n            ""        "",\n            ""   H    "",\n            ""     H  "",\n            ""   H    "",\n            "" HH   H "",\n            "" H  H H "",\n            ""   H   G""\n        ],\n        ""8x16"": [\n            ""S      H        "",\n            ""   H       HH   "",\n            ""    FF   WWWWWWW"",\n            ""  H      W      "",\n            ""    FF   W  H   "",\n            ""         W      "",\n            ""    FF   W      "",\n            ""  H          H G""\n        ],\n        ""16x16"": [\n            ""S      H        "",\n            ""           HH   "",\n            ""    FF   W     W"",\n            ""         W      "",\n            ""WWW FF      H   "",\n            ""         W      "",\n            "" FFFF    W      "",\n            ""  H          H  "",\n            ""       H        "",\n            ""   H       HH   "",\n            ""WWWW     WWWWWWW"",\n            ""  H      W    W "",\n            ""    FF   W  H W "",\n            ""WWWW    WW    W "",\n            ""    FF   W      "",\n            ""  H          H G""\n        ],\n        ""4-room"": [  # 30=start state, 79=goal state\n            ""     W     "",\n            "" H   W     "",\n            ""        G  "",\n            ""     W     "",\n            ""     W     "",\n            ""W WWWW     "",\n            ""     WWW WW"",\n            ""     W F   "",\n            ""  S  W     "",\n            ""           "",\n            ""     W     ""\n        ]\n    }\n\n    # Some useful class vars.\n    grid_world_2x2_preprocessing_spec = [dict(type=""reshape"", flatten=True, flatten_categories=4)]\n    grid_world_4x4_preprocessing_spec = [dict(type=""reshape"", flatten=True, flatten_categories=16)]\n    grid_world_long_chain_preprocessing_spec = [dict(type=""reshape"", flatten=True, flatten_categories=68)]\n    # Preprocessed state spaces.\n    grid_world_2x2_flattened_state_space = FloatBox(shape=(4,), add_batch_rank=True)\n    grid_world_4x4_flattened_state_space = FloatBox(shape=(16,), add_batch_rank=True)\n    grid_world_long_chain_flattened_state_space = FloatBox(shape=(68,), add_batch_rank=True)\n\n    def __init__(self, world=""4x4"", save_mode=False, action_type=""udlr"",\n                 reward_function=""sparse"", state_representation=""discrete""):\n        """"""\n        Args:\n            world (Union[str,List[str]]): Either a string to map into `MAPS` or a list of strings describing the rows\n                of the world (e.g. [""S "", "" G""] for a two-row/two-column world with start and goal state).\n\n            save_mode (bool): Whether to replace holes (H) with walls (W). Default: False.\n\n            action_type (str): Which action space to use. Chose between ""udlr"" (up, down, left, right), which is a\n                discrete action space and ""ftj"" (forward + turn + jump), which is a container multi-discrete\n                action space. ""ftjb"" is the same as ""ftj"", except that sub-action ""jump"" is a boolean.\n\n            reward_function (str): One of\n                sparse: hole=-5, fire=-3, goal=1, all other steps=-0.1\n                rich: hole=-100, fire=-10, goal=50, all other steps=-0.1\n\n            state_representation (str):\n                - ""discrete"": An int representing the field on the grid, 0 meaning the upper left field, 1 the one\n                    below, etc..\n                - ""xy"": The x and y grid position tuple.\n                - ""xy+orientation"": The x and y grid position tuple plus the orientation (if any) as tuple of 2 values\n                    of the actor.\n                - ""camera"": A 3-channel image where each field in the grid-world is one pixel and the 3 channels are\n                    used to indicate different items in the scene (walls, holes, the actor, etc..).\n        """"""\n        # Build our map.\n        if isinstance(world, str):\n            self.description = world\n            world = self.MAPS[world]\n        else:\n            self.description = ""custom-map""\n\n        world = np.array(list(map(list, world)))\n        # Apply safety switch.\n        world[world == \'H\'] = (""H"" if not save_mode else ""F"")\n\n        # `world` is a list of lists that needs to be indexed using y/x pairs (first row, then column).\n        self.world = world\n        self.n_row, self.n_col = self.world.shape\n        (start_y,), (start_x,) = np.nonzero(self.world == ""S"")\n\n        # Init pygame (if installed) for visualizations.\n        if pygame is not None:\n            self.pygame_field_size = 30\n            pygame.init()\n            self.pygame_agent = pygame.image.load(\n                os.path.join(os.path.dirname(os.path.abspath(__file__)), ""images/agent.png"")\n            )\n            # Create basic grid Surface for reusage.\n            self.pygame_basic_surface = self.grid_to_surface()\n            self.pygame_display_set = False\n\n        # Figure out our state space.\n        assert state_representation in [""discrete"", ""xy"", ""xy+orientation"", ""camera""]\n        self.state_representation = state_representation\n        # Discrete states (single int from 0 to n).\n        if self.state_representation == ""discrete"":\n            state_space = IntBox(self.n_row * self.n_col)\n        # x/y position (2 ints).\n        elif self.state_representation == ""xy"":\n            state_space = IntBox(low=(0, 0), high=(self.n_col, self.n_row), shape=(2,))\n        # x/y position + orientation (3 ints).\n        elif self.state_representation == ""xy+orientation"":\n            state_space = IntBox(low=(0, 0, 0, 0), high=(self.n_col, self.n_row, 1, 1))\n        # Camera outputting a 2D color image of the world.\n        else:\n            state_space = IntBox(0, 255, shape=(self.n_row, self.n_col, 3))\n\n        self.default_start_pos = self.get_discrete_pos(start_x, start_y)\n        self.discrete_pos = self.default_start_pos\n\n        assert reward_function in [""sparse"", ""rich""]  # TODO: ""potential""-based reward\n        self.reward_function = reward_function\n\n        # Store the goal position for proximity calculations (for ""potential"" reward function).\n        (self.goal_y,), (self.goal_x,) = np.nonzero(self.world == ""G"")\n\n        # Specify the actual action spaces.\n        self.action_type = action_type\n        action_space = IntBox(4) if self.action_type == ""udlr"" else Dict(dict(\n            forward=IntBox(3), turn=IntBox(3), jump=(IntBox(2) if self.action_type == ""ftj"" else BoolBox())\n        ))\n\n        # Call the super\'s constructor.\n        super(GridWorld, self).__init__(state_space=state_space, action_space=action_space)\n\n        # Reset ourselves.\n        self.state = None\n        self.orientation = None  # int: 0, 90, 180, 270\n        self.camera_pixels = None  # only used, if state_representation==\'cam\'\n        self.reward = None\n        self.is_terminal = None\n        self.reset(randomize=False)\n\n    def seed(self, seed=None):\n        if seed is None:\n            seed = time.time()\n        np.random.seed(seed)\n        return seed\n\n    def reset(self, randomize=False):\n        """"""\n        Args:\n            randomize (bool): Whether to start the new episode in a random position (instead of ""S"").\n                This could be an empty space ("" ""), the default start (""S"") or a fire field (""F"").\n        """"""\n        if randomize is False:\n            self.discrete_pos = self.default_start_pos\n        else:\n            # Move to a random first position ("" "", ""S"", or ""F"" (ouch!) are all ok to start in).\n            while True:\n                self.discrete_pos = random.choice(range(self.n_row * self.n_col))\n                if self.world[self.y, self.x] in ["" "", ""S"", ""F""]:\n                    break\n\n        self.reward = 0.0\n        self.is_terminal = False\n        self.orientation = 0\n        self.refresh_state()\n        return self.state\n\n    def reset_flow(self, randomize=False):\n        return self.reset(randomize=randomize)\n\n    def step(self, actions, set_discrete_pos=None):\n        """"""\n        Action map:\n        0: up\n        1: right\n        2: down\n        3: left\n\n        Args:\n            actions (Optional[int,Dict[str,int]]):\n                For ""udlr"": An integer 0-3 that describes the next action.\n                For ""ftj"": A dict with keys: ""turn"" (0 (turn left), 1 (no turn), 2 (turn right)), ""forward""\n                    (0 (backward), 1(stay), 2 (forward)) and ""jump"" (0/False (no jump) and 1/True (jump)).\n\n            set_discrete_pos (Optional[int]): An integer to set the current discrete position to before acting.\n\n        Returns:\n            tuple: State Space (Space), reward (float), is_terminal (bool), info (usually None).\n        """"""\n        # Process possible manual setter instruction.\n        if set_discrete_pos is not None:\n            assert isinstance(set_discrete_pos, int) and 0 <= set_discrete_pos < self.state_space.flat_dim\n            self.discrete_pos = set_discrete_pos\n\n        # Forward, turn, jump container action.\n        move = None\n        # Up, down, left, right actions.\n        if self.action_type == ""udlr"":\n            move = actions\n        else:\n            actions = self._translate_action(actions)\n            # Turn around (0 (left turn), 1 (no turn), 2 (right turn)).\n            if ""turn"" in actions:\n                self.orientation += (actions[""turn""] - 1) * 90\n                self.orientation %= 360  # re-normalize orientation\n\n            # Forward (0=move back, 1=don\'t move, 2=move forward).\n            if ""forward"" in actions:\n                forward = actions[""forward""]\n                # Translate into classic grid world action (0=up, 1=right, 2=down, 3=left).\n                # We are actually moving in some direction.\n                if actions[""forward""] != 1:\n                    if self.orientation == 0 and forward == 2 or self.orientation == 180 and forward == 0:\n                        move = 0  # up\n                    elif self.orientation == 90 and forward == 2 or self.orientation == 270 and forward == 0:\n                        move = 1  # right\n                    elif self.orientation == 180 and forward == 2 or self.orientation == 0 and forward == 0:\n                        move = 2  # down\n                    else:\n                        move = 3  # left\n\n        if move is not None:\n            # determine the next state based on the transition function\n            next_positions = self.get_possible_next_positions(self.discrete_pos, move)\n            next_state_idx = np.random.choice(len(next_positions), p=[x[1] for x in next_positions])\n            # Update our pos.\n            self.discrete_pos = next_positions[next_state_idx][0]\n\n        # Jump? -> Move two fields forward (over walls/fires/holes w/o any damage).\n        if self.action_type == ""ftj"" and ""jump"" in actions:\n            assert actions[""jump""] == 0 or actions[""jump""] == 1 or actions[""jump""] is True or actions[""jump""] is False\n            if actions[""jump""]:  # 1 or True\n                # Translate into ""classic"" grid world action (0=up, ..., 3=left) and execute that action twice.\n                action = int(self.orientation / 90)\n                for i in range(2):\n                    # determine the next state based on the transition function\n                    next_positions = self.get_possible_next_positions(self.discrete_pos, action, in_air=(i==1))\n                    next_state_idx = np.random.choice(len(next_positions), p=[x[1] for x in next_positions])\n                    # Update our pos.\n                    self.discrete_pos = next_positions[next_state_idx][0]\n\n        next_x, next_y = self.get_x_y(self.discrete_pos)\n\n        # determine reward and done flag\n        next_state_type = self.world[next_y, next_x]\n        if next_state_type == ""H"":\n            self.is_terminal = True\n            self.reward = -5 if self.reward_function == ""sparse"" else -10\n        elif next_state_type == ""F"":\n            self.is_terminal = False\n            self.reward = -3 if self.reward_function == ""sparse"" else -10\n        elif next_state_type in ["" "", ""S""]:\n            self.is_terminal = False\n            self.reward = -0.1\n        elif next_state_type == ""G"":\n            self.is_terminal = True\n            self.reward = 1 if self.reward_function == ""sparse"" else 50\n        else:\n            raise NotImplementedError\n\n        self.refresh_state()\n\n        return self.state, np.array(self.reward, dtype=np.float32), np.array(self.is_terminal), None\n\n    def step_flow(self, actions):\n        state, reward, terminal, _ = self.step(actions)\n        # Flow Env logic.\n        if terminal:\n            state = self.reset()\n\n        return state, reward, terminal\n\n    def render(self, mode=""human""):\n        if mode == ""human"" and pygame is not None:\n            self.render_human()\n        else:\n            print(self.render_txt())\n\n    def render_human(self):\n        # Set pygame\'s display, if not already done.\n        if self.pygame_display_set is False:\n            pygame.display.set_mode((self.n_col * self.pygame_field_size, self.n_row * self.pygame_field_size))\n            self.pygame_display_set = True\n        surface = self.pygame_basic_surface.copy()\n        surface.blit(self.pygame_agent, (self.x * self.pygame_field_size + 1, self.y * self.pygame_field_size + 1))\n        pygame.display.get_surface().blit(surface, (0, 0))\n        pygame.display.flip()\n        pygame.event.get([])\n\n    def render_txt(self):\n        actor = ""X""\n        if self.action_type == ""ftj"":\n            actor = ""^"" if self.orientation == 0 else "">"" if self.orientation == 90 else ""v"" if \\\n                self.orientation == 180 else ""<""\n\n        # paints itself\n        txt = """"\n        for row in range_(len(self.world)):\n            for col, val in enumerate(self.world[row]):\n                if self.x == col and self.y == row:\n                    txt += actor\n                else:\n                    txt += val\n            txt += ""\\n""\n        txt += ""\\n""\n        return txt\n\n    def __str__(self):\n        return ""GridWorld({})"".format(self.description)\n\n    def refresh_state(self):\n        # Discrete state.\n        if self.state_representation == ""discrete"":\n            # TODO: If ftj-actions, maybe multiply discrete states with orientation (will lead to x4 state space size).\n            self.state = np.array(self.discrete_pos, dtype=np.int32)\n        # xy position.\n        elif self.state_representation == ""xy"":\n            self.state = np.array([self.x, self.y], dtype=np.int32)\n        # xy + orientation (only if `self.action_type` supports turns).\n        elif self.state_representation == ""xy+orientation"":\n            orient = [0, 1] if self.orientation == 0 else [1, 0] if self.orientation == 90 else [0, -1] \\\n                if self.orientation == 180 else [-1, 0]\n            self.state = np.array([self.x, self.y] + orient, dtype=np.int32)\n        # Camera.\n        else:\n            self.update_cam_pixels()\n            self.state = self.camera_pixels\n\n    def get_possible_next_positions(self, discrete_pos, action, in_air=False):\n        """"""\n        Given a discrete position value and an action, returns a list of possible next states and\n        their probabilities. Only next states with non-zero probabilities will be returned.\n        For now: Implemented as a deterministic MDP.\n\n        Args:\n            discrete_pos (int): The discrete position to return possible next states for.\n            action (int): The action choice.\n            in_air (bool): Whether we are actually in the air (jumping) right now (ignore if we come from ""H"" or ""W"").\n\n        Returns:\n            List[Tuple[int,float]]: A list of tuples (s\', p(s\'\\|s,a)). Where s\' is the next discrete position and\n                p(s\'|s,a) is the probability of ending up in that position when in state s and taking action a.\n        """"""\n        x, y = self.get_x_y(discrete_pos)\n        coords = np.array([x, y])\n\n        increments = np.array([[0, -1], [1, 0], [0, 1], [-1, 0]])\n        next_coords = np.clip(\n            coords + increments[action],\n            [0, 0],\n            [self.n_col - 1, self.n_row - 1]\n        )\n        next_pos = self.get_discrete_pos(next_coords[0], next_coords[1])\n        pos_type = self.world[y, x]\n        next_pos_type = self.world[next_coords[1], next_coords[0]]\n        # TODO: Allow stochasticity in this env. Right now, all probs are 1.0.\n        # Next field is a wall or we are already terminal. Stay where we are.\n        if next_pos_type == ""W"" or (in_air is False and pos_type in [""H"", ""G""]):\n            return [(discrete_pos, 1.)]\n        # Move to next field.\n        else:\n            return [(next_pos, 1.)]\n\n    def update_cam_pixels(self):\n        # Init camera?\n        if self.camera_pixels is None:\n            self.camera_pixels = np.zeros(shape=(self.n_row, self.n_col, 3), dtype=np.int32)\n        self.camera_pixels[:, :, :] = 0  # reset everything\n\n        # 1st channel -> Walls (127) and goal (255).\n        # 2nd channel -> Dangers (fire=127, holes=255)\n        # 3rd channel -> Actor position (255).\n        for row in range_(self.n_row):\n            for col in range_(self.n_col):\n                field = self.world[row, col]\n                if field == ""F"":\n                    self.camera_pixels[row, col, 0] = 127\n                elif field == ""H"":\n                    self.camera_pixels[row, col, 0] = 255\n                elif field == ""W"":\n                    self.camera_pixels[row, col, 1] = 127\n                elif field == ""G"":\n                    self.camera_pixels[row, col, 1] = 255  # will this work (goal==2x wall)?\n        # Overwrite player\'s position.\n        self.camera_pixels[self.y, self.x, 2] = 255\n\n    def get_dist_to_goal(self):\n        return math.sqrt((self.x - self.goal_x) ** 2 + (self.y - self.goal_y) ** 2)\n\n    def get_discrete_pos(self, x, y):\n        """"""\n        Returns a single, discrete int-value.\n        Calculated by walking down the rows of the grid first (starting in upper left corner),\n        then along the col-axis.\n\n        Args:\n            x (int): The x-coordinate.\n            y (int): The y-coordinate.\n\n        Returns:\n            int: The discrete pos value corresponding to the given x and y.\n        """"""\n        return x * self.n_row + y\n\n    def get_x_y(self, discrete_pos):\n        """"""\n        Returns an x/y tuple given a discrete position.\n\n        Args:\n            discrete_pos (int): An int describing the discrete position in the grid.\n\n        Returns:\n            Tuple[int,int]: x and y.\n        """"""\n        return discrete_pos // self.n_row, discrete_pos % self.n_row\n\n    @property\n    def x(self):\n        return self.get_x_y(self.discrete_pos)[0]\n\n    @property\n    def y(self):\n        return self.get_x_y(self.discrete_pos)[1]\n\n    def _translate_action(self, actions):\n        """"""\n        Maps a single integer action to dict actions. This allows us to compare how\n        container actions perform when instead using a large range on a single discrete action by enumerating\n        all combinations.\n\n        Args:\n            actions Union(int, dict): Maps single integer to different actions.\n\n        Returns:\n            dict: Actions dict.\n        """"""\n        # If already dict, do nothing.\n        if isinstance(actions, dict):\n            return actions\n        else:\n            # Unpack\n            if isinstance(actions, (np.ndarray, list)):\n                actions = actions[0]\n            # 3 x 3 x 2 = 18 actions\n            assert 18 > actions >= 0\n            # For ""ftj"": A dict with keys: ""turn"" (0 (turn left), 1 (no turn), 2 (turn right)), ""forward""\n            # (0 (backward), 1(stay), 2 (forward)) and ""jump"" (0 (no jump) and 1 (jump)).\n            converted_actions = {}\n\n            # Mapping:\n            # 0 = 0 0 0\n            # 1 = 0 0 1\n            # 2 = 0 1 0\n            # 3 = 0 1 1\n            # 4 = 0 2 0\n            # 5 = 0 2 1\n            # 6 = 1 0 0\n            # 7 = 1 0 1\n            # 8 = 1 1 0\n            # 9 = 1 1 1\n            # 10 = 1 2 0\n            # 11 = 1 2 1\n            # 12 = 2 0 0\n            # 13 = 2 0 1\n            # 14 = 2 1 0\n            # 15 = 2 1 1\n            # 16 = 2 2 0\n            # 17 = 2 2 1\n\n            # Set turn via range:\n            if 6 > actions >= 0:\n                converted_actions[""turn""] = 0\n            elif 12 > actions >= 6:\n                converted_actions[""turn""] = 1\n            elif 18 > actions >= 12:\n                converted_actions[""turn""] = 2\n\n            if actions in [0, 1, 6, 7, 12, 13]:\n                converted_actions[""forward""] = 0\n            elif actions in [2, 3, 8, 9, 14, 15]:\n                converted_actions[""forward""] = 1\n            elif actions in [4, 5, 10, 11, 16, 17]:\n                converted_actions[""forward""] = 2\n\n            # Bool or int as ""jump"".\n            if actions % 2 == 0:\n                converted_actions[""jump""] = 0 if self.action_type == ""ftj"" else False\n            else:\n                converted_actions[""jump""] = 1 if self.action_type == ""ftj"" else True\n            return converted_actions\n\n    # png Render helper methods.\n    def grid_to_surface(self):\n        """"""\n        Renders the grid-world as a png and returns the png as binary image.\n\n        Returns:\n\n        """"""\n        # Create the png surface.\n        surface = pygame.Surface((self.n_col * self.pygame_field_size, self.n_row * self.pygame_field_size), flags=pygame.SRCALPHA)\n        surface.fill(pygame.Color(""#ffffff""))\n        for col in range(self.n_col):\n            for row in range(self.n_row):\n                x = col * self.pygame_field_size\n                y = row * self.pygame_field_size\n                pygame.draw.rect(\n                    surface, pygame.Color(""#000000""), [x, y, self.pygame_field_size, self.pygame_field_size], 1\n                )\n                # Goal: G\n                if self.world[row][col] in [""G"", ""S""]:\n                    special_field = pygame.font.SysFont(""Arial"", 24, bold=True).render(\n                        self.world[row][col], False, pygame.Color(""#000000"")\n                    )\n                    surface.blit(special_field, (x + 7, y + 1))\n                # Wall: W (black rect)\n                elif self.world[row][col] in [""W""]:\n                    special_field = pygame.Surface((self.pygame_field_size, self.pygame_field_size))\n                    special_field.fill((0, 0, 0))\n                    surface.blit(special_field, (x, y))\n                # Hole: Hole image.\n                elif self.world[row][col] in [""H""]:\n                    special_field = pygame.image.load(\n                        os.path.join(os.path.dirname(os.path.abspath(__file__)), ""images/hole.png"")\n                    )\n                    surface.blit(special_field, (x, y))\n                # Fire: F (yellow rect)\n                elif self.world[row][col] in [""F""]:\n                    special_field = pygame.image.load(\n                        os.path.join(os.path.dirname(os.path.abspath(__file__)), ""images/fire.png"")\n                    )\n                    #special_field = pygame.Surface((field_size, field_size))\n                    #special_field.fill((255, 0, 0) if self.world[row][col] == ""H"" else (255, 255, 0))\n                    surface.blit(special_field, (x, y))\n        # Return a png.\n        return surface\n\n    def create_states_heatmap(self, states):\n        """"""\n        Generates a heatmap from a list of states.\n        """"""\n        state_counts = np.bincount(states)\n        alpha = int(255 / np.max(state_counts))\n        surface = self.pygame_basic_surface.copy()\n        for s in states:\n            x, y = self.get_x_y(s)\n            #pygame.draw.rect(surface, pygame.Color(0, 255, 0, alpha), [x * field_size, y * field_size, field_size, field_size])\n            rect = pygame.Surface((self.pygame_field_size - 2, self.pygame_field_size - 2))\n            rect.set_alpha(alpha)\n            rect.fill(pygame.Color(0, 255, 0))\n            surface.blit(rect, (x * self.pygame_field_size + 1, y * self.pygame_field_size + 1))\n        pygame.image.save(surface, ""test_states_heatmap.png"")\n\n    def create_states_trajectory(self, states):\n        """"""\n        Generates a trajectory from arrows between fields.\n        """"""\n        surface = self.pygame_basic_surface.copy()\n        for i, s in enumerate(states):\n            s_ = states[i + 1] if len(states) > i + 1 else None\n            if s_ is not None:\n                x, y = self.get_x_y(s)\n                x_, y_ = self.get_x_y(s_)\n                arrow = pygame.image.load(os.path.join(os.path.dirname(os.path.abspath(__file__)), ""images/arrow.png""))\n                self._add_field_connector(surface, x, x_, y, y_, arrow)\n        pygame.image.save(surface, ""test_trajectory.png"")\n\n    def create_rewards_trajectory(self, states, rewards):\n        """"""\n        Generates a trajectory of received rewards from arrows (green and red) between fields.\n        """"""\n        max_abs_r = max(abs(np.array(rewards)))\n        surface = self.pygame_basic_surface.copy()\n        for i, s in enumerate(states):\n            s_ = states[i + 1] if len(states) > i + 1 else None\n            if s_ is not None:\n                x, y = self.get_x_y(s)\n                x_, y_ = self.get_x_y(s_)\n                r = rewards[i]\n                arrow = pygame.image.load(os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                                       ""images/arrow_""+(""red"" if r < 0 else ""green"")+"".png""))\n                arrow_transparent = pygame.Surface((arrow.get_width(), arrow.get_height()), flags=pygame.SRCALPHA)\n                arrow_transparent.fill((255, 255, 255, int(255 * ((abs(r) / max_abs_r) / 2 + 0.5))))\n                #arrow_transparent.set_alpha(int(255 * abs(r) / max_abs_r))\n                #arrow_transparent = pygame.Surface.convert_alpha(arrow_transparent)\n                arrow.blit(arrow_transparent, (0, 0), special_flags=pygame.BLEND_RGBA_MULT)\n                self._add_field_connector(surface, x, x_, y, y_, arrow)\n        pygame.image.save(surface, ""test_rewards_trajectory.png"")\n\n    def _add_field_connector(self, surface, x, x_, y, y_, connector_surface):\n        # Rotate connector (assumed to be pointing right) according to the direction of the move.\n        if x_ == x - 1:  # left\n            connector_surface = pygame.transform.rotate(connector_surface, 180.0)\n            x = x * self.pygame_field_size - connector_surface.get_width() / 2\n            y = y * self.pygame_field_size + (self.pygame_field_size - connector_surface.get_height()) / 2\n        elif y_ == y - 1:  # up\n            connector_surface = pygame.transform.rotate(connector_surface, 90.0)\n            x = x * self.pygame_field_size + (self.pygame_field_size - connector_surface.get_width()) / 2\n            y = y * self.pygame_field_size - connector_surface.get_height() / 2\n        elif y_ == y + 1:  # down\n            connector_surface = pygame.transform.rotate(connector_surface, 270.0)\n            x = x * self.pygame_field_size + (self.pygame_field_size - connector_surface.get_width()) / 2\n            y = y * self.pygame_field_size + connector_surface.get_height() / 2\n        else:  # right\n            x = x * self.pygame_field_size + ((self.pygame_field_size * 2) - connector_surface.get_width()) / 2\n            y = y * self.pygame_field_size + (self.pygame_field_size - connector_surface.get_height()) / 2\n        surface.blit(connector_surface, (x, y))\n'"
rlgraph/environments/mlagents_env.py,0,"b'# Copyright 2018/2019 The Rlgraph Authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom mlagents.envs.environment import UnityEnvironment\n\nfrom rlgraph.environments.vector_env import VectorEnv\nfrom rlgraph.spaces import Dict, Tuple, IntBox\nfrom rlgraph.spaces.space_utils import get_space_from_op\n\n\nclass MLAgentsEnv(VectorEnv):\n    """"""\n    An Environment sitting behind a tcp connection and communicating through this adapter.\n    Note: Communication between Unity and Python takes place over an open socket without authentication.\n    Ensure that the network where training takes place is secure.\n    """"""\n    def __init__(self, file_name=None, worker_id=0, base_port=5005, seed=0, docker_training=False, no_graphics=False,\n                 timeout_wait=30, train_mode=True, **kwargs):\n        """"""\n        Args:\n            file_name (Optional[str]): Name of Unity environment binary.\n            base_port (int): Port number to connect to Unity environment. `worker_id` increments on top of this.\n            worker_id (int): Number to add to `base_port`. Used for asynchronous agent scenarios.\n            docker_training (bool): Informs this class, whether the process is being run within a container.\n                Default: False.\n            no_graphics (bool): Whether to run the Unity simulator in no-graphics mode. Default: False.\n            timeout_wait (int): Time (in seconds) to wait for connection from environment.\n            train_mode (bool): Whether to run in training mode, speeding up the simulation. Default: True.\n        """"""\n        # First create the UnityMLAgentsEnvironment to get state and action spaces, then create RLgraph Environment\n        # instance.\n        self.mlagents_env = UnityEnvironment(\n            file_name, worker_id, base_port, seed, docker_training, no_graphics\n        )\n        all_brain_info = self.mlagents_env.reset()\n        # Get all possible information from AllBrainInfo.\n        # TODO: Which scene do we pick?\n        self.scene_key = next(iter(all_brain_info))\n        first_brain_info = all_brain_info[self.scene_key]\n        num_environments = len(first_brain_info.agents)\n\n        state_space = {}\n        if len(first_brain_info.vector_observations[0]) > 0:\n            state_space[""vector""] = get_space_from_op(first_brain_info.vector_observations[0])\n            # TODO: This is a hack.\n            if state_space[""vector""].dtype == np.float64:\n                state_space[""vector""].dtype = np.float32\n        if len(first_brain_info.visual_observations) > 0:\n            state_space[""visual""] = get_space_from_op(first_brain_info.visual_observations[0])\n        if first_brain_info.text_observations[0]:\n            state_space[""text""] = get_space_from_op(first_brain_info.text_observations[0])\n\n        if len(state_space) == 1:\n            self.state_key = next(iter(state_space))\n            state_space = state_space[self.state_key]\n        else:\n            self.state_key = None\n            state_space = Dict(state_space)\n        brain_params = next(iter(self.mlagents_env.brains.values()))\n        if brain_params.vector_action_space_type == ""discrete"":\n            highs = brain_params.vector_action_space_size\n            # MultiDiscrete (Tuple(IntBox)).\n            if any(h != highs[0] for h in highs):\n                action_space = Tuple([IntBox(h) for h in highs])\n            # Normal IntBox:\n            else:\n                action_space = IntBox(\n                    low=np.zeros_like(highs, dtype=np.int32),\n                    high=np.array(highs, dtype=np.int32),\n                    shape=(len(highs),)\n                )\n        else:\n            action_space = get_space_from_op(first_brain_info.action_masks[0])\n        if action_space.dtype == np.float64:\n            action_space.dtype = np.float32\n\n        super(MLAgentsEnv, self).__init__(\n            num_environments=num_environments, state_space=state_space, action_space=action_space, **kwargs\n        )\n\n        # Caches the last observation we made (after stepping or resetting).\n        self.last_state = None\n\n    def get_env(self):\n        return self\n\n    def reset(self, index=0):\n        # Reset entire MLAgentsEnv iff global_done is True.\n        if self.mlagents_env.global_done is True or self.last_state is None:\n            self.reset_all()\n        return self.last_state[index]\n\n    def reset_all(self):\n        all_brain_info = self.mlagents_env.reset()\n        self.last_state = self._get_state_from_brain_info(all_brain_info)\n        return self.last_state\n\n    def step(self, actions, text_actions=None, **kwargs):\n        # MLAgents Envs don\'t like tuple-actions.\n        if isinstance(actions[0], tuple):\n            actions = [list(a) for a in actions]\n        all_brain_info = self.mlagents_env.step(\n            # TODO: Only support vector actions for now.\n            vector_action=actions, memory=None, text_action=text_actions, value=None\n        )\n        self.last_state = self._get_state_from_brain_info(all_brain_info)\n        r = self._get_reward_from_brain_info(all_brain_info)\n        t = self._get_terminal_from_brain_info(all_brain_info)\n        return self.last_state, r, t, None\n\n    def render(self):\n        # TODO: If no_graphics is True, maybe user can render through this method manually?\n        pass\n\n    def terminate(self):\n        self.mlagents_env.close()\n\n    def terminate_all(self):\n        return self.terminate()\n\n    def __str__(self):\n        return ""MLAgentsEnv(port={}{})"".format(\n            self.mlagents_env.port, "" [loaded]"" if self.mlagents_env._loaded else """"\n        )\n\n    def _get_state_from_brain_info(self, all_brain_info):\n        brain_info = all_brain_info[self.scene_key]\n        if self.state_key is None:\n            return {""vector"": list(brain_info.vector_observations), ""visual"": list(brain_info.visual_observations),\n                    ""text"": list(brain_info.text_observations)}\n        elif self.state_key == ""vector"":\n            return list(brain_info.vector_observations)\n        elif self.state_key == ""visual"":\n            return list(brain_info.visual_observations)\n        elif self.state_key == ""text"":\n            return list(brain_info.text_observations)\n\n    def _get_reward_from_brain_info(self, all_brain_info):\n        brain_info = all_brain_info[self.scene_key]\n        return [np.array(r_, dtype=np.float32) for r_ in brain_info.rewards]\n\n    def _get_terminal_from_brain_info(self, all_brain_info):\n        brain_info = all_brain_info[self.scene_key]\n        return brain_info.local_done\n'"
rlgraph/environments/openai_gym.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\nimport gym\nimport numpy as np\nfrom six.moves import xrange as range_\n\nfrom rlgraph.environments import Environment\nfrom rlgraph.spaces import *\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\n\n\nclass OpenAIGymEnv(Environment):\n    """"""\n    OpenAI Gym adapter for RLgraph: https://gym.openai.com/.\n    """"""\n\n    def __init__(\n            self, gym_env, frameskip=None, max_num_noops=0, noop_action=0, episodic_life=False, fire_reset=False,\n            monitor=None, monitor_safe=False, monitor_video=0, visualize=False,\n            force_float32=True, **kwargs\n    ):\n        """"""\n        Args:\n            gym_env (Union[str,gym.Env]): OpenAI Gym environment ID or actual gym.Env. See https://gym.openai.com/envs\n            frameskip (Optional[Tuple[int,int],int]): Number of game frames that should be skipped with each action\n                (repeats given action for this number of game frames and accumulates reward).\n                Default: (2,5) -> Uniformly pull from set [2,3,4].\n            max_num_noops (Optional[int]): How many no-ops to maximally perform when resetting\n                the environment before returning the reset state.\n            noop_action (any): The action representing no-op. 0 for Atari.\n            episodic_life (bool): If true, losing a life will lead to episode end from the perspective\n                of the agent. Internally, th environment will keep stepping the game and manage the true\n                termination (end of game).\n            fire_reset (Optional[bool]): If true, fire off environment after reset.\n            monitor: Output directory. Setting this to None disables monitoring.\n            monitor_safe: Setting this to True prevents existing log files to be overwritten. Default False.\n            monitor_video: Save a video every monitor_video steps. Setting this to 0 disables recording of videos.\n            visualize: If set True, the program will visualize the trainings of gym\'s environment. Note that such\n                visualization is probably going to slow down the training.\n            force_float32 (bool): Whether to convert all state signals (iff the state space is of dtype float64) into\n                float32. Note: This does not affect any int-type state spaces.\n                Default: True.\n        """"""\n        if isinstance(gym_env, str):\n            self.gym_env = gym.make(gym_env)  # Might raise gym.error.UnregisteredEnv or gym.error.DeprecatedEnv\n        else:\n            self.gym_env = gym_env\n\n        # Multi-goal environments states comes in a dict{observation: dtype, desired_goal: dtype, achieved_goal:dtype}\n        if hasattr(gym, ""GoalEnv"") and isinstance(self.gym_env.env, gym.GoalEnv):\n            self.gym_env = gym.wrappers.FlattenDictWrapper(self.gym_env, dict_keys=[\'observation\', \'desired_goal\'])\n            self.achieved_goal = self.translate_space(self.gym_env.env.observation_space.spaces[\'achieved_goal\'],\n                                                      force_float32=force_float32)\n        # Manually set the frameskip property.\n        self.frameskip = None\n        if frameskip is not None:\n            # Skip externally.\n            if ""NoFrameskip"" in gym_env:\n                self.state_buffer = np.zeros((2,) + self.gym_env.observation_space.shape, dtype=np.uint8)\n                self.frameskip = frameskip\n            else:\n                # Set gym property.\n                self.gym_env.env.frameskip = frameskip\n\n        # In Atari environments, 0 is no-op.\n        self.noop_action = noop_action\n        self.max_num_noops = max_num_noops\n\n        # Manage life as episodes.\n        self.episodic_life = episodic_life\n        self.true_terminal = True\n        self.lives = 0\n        self.fire_after_reset = fire_reset\n        self.force_float32 = False  # Set to False for now, later overwrite with a correct value.\n\n        if self.fire_after_reset:\n            assert self.gym_env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n            assert len(self.gym_env.unwrapped.get_action_meanings()) >= 3\n\n        self.visualize = visualize\n        if monitor:\n            if monitor_video == 0:\n                video_callable = False\n            else:\n                video_callable = (lambda x: x % monitor_video == 0)\n            self.gym_env = gym.wrappers.Monitor(self.gym_env, monitor, force=not monitor_safe,\n                                                video_callable=video_callable)\n\n        self.action_space = self.translate_space(self.gym_env.action_space)\n\n        # Don\'t trust gym\'s own information on dtype. Find out what the observation space really is.\n        # Gym_env.observation_space\'s low/high used to be float64 ndarrays, but the actual output was uint8.\n        self.state_space = self.translate_space(self.gym_env.observation_space, dtype=self.reset().dtype,\n                                                force_float32=force_float32)\n\n        super(OpenAIGymEnv, self).__init__(self.state_space, self.action_space, **kwargs)\n\n        # If state_space is not a FloatBox -> Set force_float32 to False.\n        if not isinstance(self.state_space, FloatBox):\n            force_float32 = False\n\n        self.force_float32 = force_float32\n\n    def seed(self, seed=None):\n        if seed is None:\n            seed = time.time()\n        self.gym_env.seed(seed)\n        return seed\n\n    def reset(self):\n        if self.fire_after_reset:\n            self.episodic_reset()\n            state, _, terminal, _ = self.step(1)\n            if terminal:\n                self.episodic_reset()\n            state, _, terminal, _ = self.step(2)\n            if terminal:\n                self.episodic_reset()\n            return state if self.force_float32 is False else np.array(state, dtype=np.float32)\n        else:\n            return self.episodic_reset()\n\n    def episodic_reset(self):\n        if self.episodic_life:\n            # If the last terminal was actually the end of the episode.\n            if self.true_terminal:\n                state = self.noop_reset()\n            else:\n                # If not, step.\n                state, _, _, _ = self._step_and_skip(self.noop_action)\n            # Update live property.\n            self.lives = self.gym_env.unwrapped.ale.lives()\n            return state if self.force_float32 is False else np.array(state, dtype=np.float32)\n        else:\n            return self.noop_reset()\n\n    def noop_reset(self):\n        """"""\n        Steps through reset and warm-start.\n        """"""\n        if isinstance(self.gym_env, gym.wrappers.Monitor):\n            self.gym_env.stats_recorder.done = True\n        state = self.gym_env.reset()\n        if self.max_num_noops > 0:\n            num_noops = np.random.randint(low=1, high=self.max_num_noops + 1)\n            # Do a number of noops to vary starting positions.\n            for _ in range_(num_noops):\n                state, reward, terminal, info = self.gym_env.step(self.noop_action)\n                if terminal:\n                    state = self.gym_env.reset()\n        return state if self.force_float32 is False else np.array(state, dtype=np.float32)\n\n    def reset_flow(self):\n        return self.reset()\n\n    def terminate(self):\n        self.gym_env.close()\n        self.gym_env = None\n\n    def _step_and_skip(self, actions):\n        # TODO - allow for goal reward substitution for multi-goal envs\n        if self.frameskip is None:\n            # Frames kipping is unset or set as env property.\n            return self.gym_env.step(actions)\n        else:\n            # Do frameskip loop in our wrapper class.\n            step_reward = 0.0\n            terminal = None\n            info = None\n            for i in range_(self.frameskip):\n                state, reward, terminal, info = self.gym_env.step(actions)\n                if i == self.frameskip - 2:\n                    self.state_buffer[0] = state\n                if i == self.frameskip - 1:\n                    self.state_buffer[1] = state\n                step_reward += reward\n                if terminal:\n                    break\n\n            max_frame = self.state_buffer.max(axis=0)\n\n            return max_frame, step_reward, terminal, info\n\n    def step(self, actions):\n        if self.visualize:\n            self.gym_env.render()\n        state, reward, terminal, info = self._step_and_skip(actions)\n\n        # Manage lives if necessary.\n        if self.episodic_life:\n            self.true_terminal = terminal\n            lives = self.gym_env.unwrapped.ale.lives()\n            # lives < self.lives -> lost a life so show terminal = true to learner.\n            if self.lives > lives > 0:\n                terminal = True\n            self.lives = lives\n\n        if self.force_float32 is True:\n            state = np.array(state, dtype=np.float32)\n\n        return state, np.asarray(reward, dtype=np.float32), terminal, info\n\n    def step_flow(self, actions):\n        state, reward, terminal, _ = self.step(actions)\n        if terminal:\n            state = self.reset_flow()\n        return state, reward, terminal\n\n    def render(self):\n        self.gym_env.render(""human"")\n\n    @staticmethod\n    def translate_space(space, dtype=None, force_float32=False):\n        """"""\n        Translates openAI spaces into RLGraph Space classes.\n\n        Args:\n            space (gym.spaces.Space): The openAI Space to be translated.\n\n        Returns:\n            Space: The translated rlgraph Space.\n        """"""\n        if isinstance(space, gym.spaces.Discrete):\n            return IntBox(space.n)\n        elif isinstance(space, gym.spaces.MultiBinary):\n            return BoolBox(shape=(space.n,))\n        elif isinstance(space, gym.spaces.MultiDiscrete):\n            return IntBox(low=np.zeros((space.nvec.ndim,), dtype(""uint8"", ""np"")), high=space.nvec)\n        elif isinstance(space, gym.spaces.Box):\n            # Decide by dtype:\n            box_dtype = str(dtype or space.low.dtype)\n            if ""int"" in box_dtype:\n                return IntBox(low=space.low, high=space.high, dtype=box_dtype)\n            elif ""float"" in box_dtype:\n                return FloatBox(\n                    low=space.low, high=space.high, dtype=""float32"" if force_float32 is True else box_dtype\n                )\n            elif ""bool"" in box_dtype:\n                return BoolBox(shape=space.shape)\n        elif isinstance(space, gym.spaces.Tuple):\n            return Tuple(*[OpenAIGymEnv.translate_space(s) for s in space.spaces])\n        elif isinstance(space, gym.spaces.Dict):\n            return Dict({key: OpenAIGymEnv.translate_space(value, dtype, force_float32)\n                         for key, value in space.spaces.items()})\n\n        raise RLGraphError(""Unknown openAI gym Space class ({}) for state_space!"".format(space))\n\n    def __str__(self):\n        return ""OpenAIGym({})"".format(self.gym_env)\n'"
rlgraph/environments/random_env.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport time\n\nfrom rlgraph.environments import Environment\nimport rlgraph.spaces as spaces\n\n\nclass RandomEnv(Environment):\n    """"""\n    An Env producing random states no matter what actions come in.\n    """"""\n    def __init__(self, state_space, action_space, reward_space=None, terminal_prob=0.1, deterministic=False):\n        """"""\n        Args:\n            reward_space (Union[dict,Space]): The reward Space from which to randomly sample for each step.\n            terminal_prob (Union[dict,Space]): The probability with which an episode ends for each step.\n            deterministic (bool): Convenience flag to seed the environment automatically upon construction.\n        """"""\n        super(RandomEnv, self).__init__(state_space=state_space, action_space=action_space)\n\n        self.reward_space = spaces.Space.from_spec(reward_space)\n        self.terminal_prob = terminal_prob\n\n        if deterministic is True:\n            np.random.seed(10)\n        self.last_state = np.random.get_state()\n\n    def seed(self, seed=None):\n        if seed is None:\n            seed = time.time()\n        np.random.seed(seed)\n        self.last_state = np.random.get_state()\n        return seed\n\n    def reset(self):\n        return self.step()[0]  # 0=state\n\n    def reset_flow(self):\n        return self.reset()\n\n    def step(self, actions=None):\n        if actions is not None:\n            assert self.action_space.contains(actions), \\\n                ""ERROR: Given action ({}) in step is not part of action Space ({})!"".format(actions, self.action_space)\n\n        # Set the seed to the last observed state for this instance.\n        np.random.set_state(self.last_state)\n        # Do the random sampling (using numpy).\n        state = self.state_space.sample()\n        reward = self.reward_space.sample()\n        terminal = np.random.choice([True, False], p=[self.terminal_prob, 1.0 - self.terminal_prob])\n        # Store the current state of the RNG.\n        self.last_state = np.random.get_state()\n        return state, reward, terminal, None\n\n    def step_flow(self, actions=None):\n        ret = self.step(actions)\n        return ret[0], ret[1], ret[2]\n\n    def __str__(self):\n        return ""RandomEnv()""\n'"
rlgraph/environments/sequential_vector_env.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom queue import Queue\nfrom threading import Thread\n\nfrom six.moves import xrange as range_\n\nfrom rlgraph.environments import VectorEnv, Environment\n\n\nclass SequentialVectorEnv(VectorEnv):\n    """"""\n    Sequential multi-environment class which iterates over a list of environments\n    to step them.\n    """"""\n    def __init__(self, num_environments, env_spec, num_background_envs=1, async_reset=False):\n        """"""\n            num_background_envs (Optional([int]): Number of environments asynchronously\n                reset in the background. Need to be calibrated depending on reset cost.\n            async_reset (Optional[bool]): If true, resets envs asynchronously in another thread.\n        """"""\n        self.environments = []\n\n        for _ in range_(num_environments):\n            if isinstance(env_spec, dict):\n                env = Environment.from_spec(env_spec)\n            elif hasattr(env_spec, \'__call__\'):\n                env = env_spec()\n            else:\n                raise ValueError(""Env_spec must be either a dict containing an environment spec or a callable""\n                                 ""returning a new environment object."")\n            self.environments.append(env)\n\n        super(SequentialVectorEnv, self).__init__(\n            num_environments=num_environments,\n            state_space=self.environments[0].state_space, action_space=self.environments[0].action_space\n        )\n\n        self.async_reset = async_reset\n        if self.async_reset:\n            self.resetter = ThreadedResetter(env_spec, num_background_envs)\n        else:\n            self.resetter = Resetter()\n\n    def seed(self, seed=None):\n        return [env.seed(seed) for env in self.environments]\n\n    def get_env(self, index=0):\n        return self.environments[index]\n\n    def reset(self, index=0):\n        state, env = self.resetter.swap(self.environments[index])\n        self.environments[index] = env\n        return state\n\n    def reset_all(self):\n        states = []\n        for i, env in enumerate(self.environments):\n            state, env = self.resetter.swap(self.environments[i])\n            states.append(state)\n            self.environments[i] = env\n        return states\n\n    def step(self, actions, **kwargs):\n        states, rewards, terminals, infos = [], [], [], []\n        for i in range_(self.num_environments):\n            state, reward, terminal, info = self.environments[i].step(actions[i])\n            states.append(state)\n            rewards.append(reward)\n            terminals.append(terminal)\n            infos.append(info)\n        return states, rewards, terminals, infos\n\n    def render(self, index=0):\n        self.environments[index].render()\n\n    def terminate(self, index=0):\n        self.environments[index].terminate()\n\n    def terminate_all(self):\n        for env in self.environments:\n            env.terminate()\n\n    def __str__(self):\n        return [str(env) for env in self.environments]\n\n\nclass Resetter(object):\n\n    def swap(self, env):\n        return env.reset(), env\n\n\nclass ThreadedResetter(Thread):\n    """"""\n    Keeps resetting environments in a queue,\n\n    n.b. mechanism originally seen ins RLlib, since removed.\n    """"""\n\n    def __init__(self, env_spec, num_environments):\n        super(ThreadedResetter, self).__init__()\n        self.daemon = True\n        self.in_need_reset = Queue()\n        self.out_ready = Queue()\n\n        # Create a set of environments ready to use.\n        for _ in range_(num_environments):\n            env = Environment.from_spec(env_spec)\n            state = env.reset()\n            self.out_ready.put((state, env))\n\n        self.start()\n\n    def swap(self, env):\n        """"""\n        Trade environment in need of reset for ready to use environment.\n        Args:\n            env (Environment): Environment object.\n\n        Returns:\n            any, Environment: State and ready to use environment.\n        """"""\n        self.in_need_reset.put(env)\n        state, ready_to_use_env = self.out_ready.get(timeout=30)\n        return state, ready_to_use_env\n\n    def run(self):\n        # Keeps resetting environments as they come in.\n        while True:\n            env = self.in_need_reset.get()\n            state = env.reset()\n            self.out_ready.put((state, env))\n'"
rlgraph/environments/vector_env.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.environments import Environment\n\n\nclass VectorEnv(Environment):\n    """"""\n    Abstract multi-environment class to support stepping through multiple environments at once.\n    """"""\n    def __init__(self, num_environments, **kwargs):\n        super(VectorEnv, self).__init__(**kwargs)\n        self.num_environments = num_environments\n\n    def get_env(self):\n        """"""\n        Returns an underlying sub-environment instance.\n\n        Returns:\n            Environment: Environment instance.\n        """"""\n        raise NotImplementedError\n\n    def reset(self, index=0):\n        """"""\n        Resets the given sub-environment.\n\n        Returns:\n            any: New state for sub-environment.\n        """"""\n        raise NotImplementedError\n\n    def reset_all(self):\n        """"""\n        Resets all environments.\n\n        Returns:\n            any: New states for environments.\n        """"""\n        raise NotImplementedError\n\n    def terminate_all(self):\n        raise NotImplementedError\n'"
rlgraph/environments/vizdoom.py,0,"b'# Copyright 2018/2019 The RLgraph authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport vizdoom\nimport numpy as np\nimport time\n\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\nfrom rlgraph.utils.util import convert_dtype as dtype_\nfrom rlgraph.environments import Environment\nfrom rlgraph.spaces import *\n\n\nclass VizDoomEnv(Environment):\n    """"""\n    VizDoom Integration: https://github.com/mwydmuch/ViZDoom.\n    """"""\n    def __init__(self, config_file, visible=False, mode=vizdoom.Mode.PLAYER, screen_format=vizdoom.ScreenFormat.GRAY8, screen_resolution=vizdoom.ScreenResolution.RES_640X480):\n        """"""\n        Args:\n            config_file (str): The config file to configure the DoomGame object.\n            visible (bool): \n            mode (vizdoom.Mode): The playing mode of the game.\n            screen_format (vizdoom.ScreenFormat): The screen (pixel) format of the game.\n            screen_resolution (vizdoom.ScreenResolution): The screen resolution (width x height) of the game.\n        """"""\n        # Some restrictions on the settings.\n        assert screen_format in [vizdoom.ScreenFormet.RGB24, vizdoom.ScreedFormat.GRAY8], ""ERROR: `screen_format` must be either GRAY8 or RGB24!""\n        assert screen_resolution in [vizdoom.ScreenResolution.RES_640X480], ""ERROR: `screen_resolution` must be 640x480!""\n\n        self.game = vizdoom.DoomGame()\n        self.game.load_config(config_file)\n        self.game.set_window_visible(False)\n        self.game.set_mode(mode)\n        self.game.set_screen_format(screen_format)\n        self.game.set_screen_resolution(screen_resolution)\n        self.game.init()\n \n        # Calculate action and state Spaces for Env c\'tor.\n        state_space = IntBox(255, shape=(480, 480, 1 if screen_format == vizdoom.ScreenFormat.GRAY8 else 3))  # image of size [resolution] with [screen-format] channels\n        action_space = IntBox(1, shape=(self.game.get_available_buttons_size(),))\n\n        super(VizDoomEnv, self).__init__(state_space=state_space, action_space=action_space)\n\n    def seed(self, seed=None):\n        if seed is None:\n            seed = time.time()\n        self.game.set_seed(seed)\n        return seed\n\n    def reset(self):\n        self.game.newEpisode()\n        return self.game.getState, 0.0, self.game.isEpisodeFinished(), dict(is_dead=self.game.isPlayerDead, )\n\n    def terminate(self):\n        #self.gym_env.close()\n        self.game = None\n\n    def step(self, actions=None):\n        if self.visualize:\n            self.gym_env.render()\n        state, reward, terminal, info = self.gym_env.step(actions)\n        return state, reward, terminal, info\n\n    @staticmethod\n    def translate_space(space):\n        """"""\n        Translates an openAI space into an RLGraph Space object.\n\n        Args:\n            space (gym.spaces.Space): The openAI Space to be translated.\n\n        Returns:\n            Space: The translated Rlgraph Space.\n        """"""\n        if isinstance(space, gym.spaces.Discrete):\n            if space.n == 2:\n                return BoolBox()\n            else:\n                return IntBox(space.n)\n        elif isinstance(space, gym.spaces.MultiBinary):\n            return BoolBox(shape=(space.n,))\n        elif isinstance(space, gym.spaces.MultiDiscrete):\n            return IntBox(low=np.zeros((space.nvec.ndim,), dtype_(""uint8"", ""np"")), high=space.nvec)\n        elif isinstance(space, gym.spaces.Box):\n            return FloatBox(low=space.low, high=space.high)\n        elif isinstance(space, gym.spaces.Tuple):\n            return Tuple(*[OpenAIGymEnv.translate_space(s) for s in space.spaces])\n        elif isinstance(space, gym.spaces.Dict):\n            return Dict({k: OpenAIGymEnv.translate_space(v) for k, v in space.spaces.items()})\n        else:\n            raise RLGraphError(""Unknown openAI gym Space class for state_space!"")\n\n    def __str__(self):\n        return ""OpenAIGym({})"".format(self.gym_env)\n\n'"
rlgraph/execution/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.execution.environment_sample import EnvironmentSample\nfrom rlgraph.execution.worker import Worker\nfrom rlgraph.execution.single_threaded_worker import SingleThreadedWorker\n\n__all__ = [""Worker"", ""SingleThreadedWorker"", ""EnvironmentSample""]\n\nWorker.__lookup_classes__ = dict(\n   single=SingleThreadedWorker,\n   singlethreadedworker=SingleThreadedWorker,\n   singlethreaded=SingleThreadedWorker\n)'"
rlgraph/execution/environment_sample.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nclass EnvironmentSample(object):\n    """"""\n    Represents a sampled trajectory from an environment.\n    """"""\n    def __init__(\n        self,\n        sample_batch,\n        batch_size=None,\n        metrics=None,\n        **kwargs\n    ):\n        """"""\n        Args:\n            sample_batch (dict): Dict containing sample trajectories.\n            **kwargs (dict): Any additional information relevant for processing the sample.\n        """"""\n        self.sample_batch = sample_batch\n        self.batch_size = batch_size\n        self.metrics = metrics\n        self.kwargs = kwargs\n\n    def get_batch(self):\n        """"""\n        Get experience sample in insert format.\n\n        Returns:\n            dict: Sample batch.\n        """"""\n        return self.sample_batch\n\n    def get_metrics(self):\n        return self.metrics\n'"
rlgraph/execution/single_threaded_worker.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport time\nfrom copy import deepcopy\n\nimport numpy as np\nfrom six.moves import xrange as range_\nfrom rlgraph.components import PreprocessorStack\nfrom rlgraph.execution.worker import Worker\nfrom rlgraph.spaces.space_utils import horizontalize_space_sample\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\nfrom rlgraph.utils.util import default_dict\n\n\nclass SingleThreadedWorker(Worker):\n\n    def __init__(self, preprocessing_spec=None, worker_executes_preprocessing=True, **kwargs):\n        super(SingleThreadedWorker, self).__init__(**kwargs)\n\n        self.logger.info(""Initialized single-threaded executor with {} environments \'{}\' and Agent \'{}\'"".format(\n            self.num_environments, self.vector_env.get_env(), self.agent\n        ))\n\n        # Switch off worker preprocessing if nothing to do anyway.\n        if preprocessing_spec is None or preprocessing_spec == []:\n            worker_executes_preprocessing = False\n\n        self.preprocessed_states_buffer = None\n        self.worker_executes_preprocessing = worker_executes_preprocessing\n        if self.worker_executes_preprocessing:\n            self.preprocessors = {}\n            self.state_is_preprocessed = {}\n            for env_id in self.env_ids:\n                self.preprocessors[env_id] = self.setup_preprocessor(\n                    preprocessing_spec, self.vector_env.state_space.with_batch_rank()\n                )\n                self.state_is_preprocessed[env_id] = False\n\n            self.preprocessed_states_buffer = [self.agent.preprocessed_state_space.map(\n                mapping=lambda flat_key, primitive_space: np.zeros(\n                    shape=primitive_space.shape,\n                    dtype=primitive_space.dtype\n                )\n            ) for _ in range(self.num_environments)]\n\n        # Global statistics.\n        self.env_frames = 0\n        self.finished_episode_returns = [[] for _ in range_(self.num_environments)]\n        self.finished_episode_durations = [[] for _ in range_(self.num_environments)]\n        self.finished_episode_timesteps = [[] for _ in range_(self.num_environments)]\n\n        # Accumulated return over the running episode.\n        self.episode_returns = [0 for _ in range_(self.num_environments)]\n        # The number of steps taken in the running episode.\n        self.episode_timesteps = [0 for _ in range_(self.num_environments)]\n        # Whether the running episode has terminated.\n        self.episode_terminals = [False for _ in range_(self.num_environments)]\n        # Wall time of the last start of the running episode.\n        self.episode_starts = [0 for _ in range_(self.num_environments)]\n        # The current state of the running episode.\n        self.env_states = [None for _ in range_(self.num_environments)]\n\n    @staticmethod\n    def setup_preprocessor(preprocessing_spec, in_space):\n        if preprocessing_spec is not None:\n            # TODO move ingraph for python component assembly.\n            preprocessing_spec = deepcopy(preprocessing_spec)\n            in_space = deepcopy(in_space)\n            # Store scopes (set if not given).\n            scopes = []\n            for i, preprocessor in enumerate(preprocessing_spec):\n                if ""scope"" not in preprocessor:\n                    preprocessor[""scope""] = ""preprocessor-{}"".format(i)\n                scopes.append(preprocessor[""scope""])\n                # Set backend to python.\n                preprocessor[""backend""] = ""python""\n\n            processor_stack = PreprocessorStack(*preprocessing_spec, backend=""python"")\n            build_space = in_space\n            for sub_comp_scope in scopes:\n                processor_stack.sub_components[sub_comp_scope].create_variables(input_spaces=dict(\n                    inputs=build_space\n                ), action_space=None)\n                build_space = processor_stack.sub_components[sub_comp_scope].get_preprocessed_space(build_space)\n            processor_stack.reset()\n            return processor_stack\n        else:\n            return None\n\n    def execute_timesteps(self, num_timesteps, max_timesteps_per_episode=0, update_spec=None, use_exploration=True,\n                          frameskip=None, reset=True):\n        return self._execute(\n            num_timesteps=num_timesteps,\n            max_timesteps_per_episode=max_timesteps_per_episode,\n            use_exploration=use_exploration,\n            update_spec=update_spec,\n            frameskip=frameskip,\n            reset=reset\n        )\n\n    def execute_episodes(self, num_episodes, max_timesteps_per_episode=0, update_spec=None, use_exploration=True,\n                         frameskip=None, reset=True):\n        return self._execute(\n            num_episodes=num_episodes,\n            max_timesteps_per_episode=max_timesteps_per_episode,\n            use_exploration=use_exploration,\n            update_spec=update_spec,\n            frameskip=frameskip,\n            reset=reset\n        )\n\n    def _execute(\n        self,\n        num_timesteps=None,\n        num_episodes=None,\n        max_timesteps_per_episode=None,\n        use_exploration=True,\n        update_spec=None,\n        frameskip=None,\n        reset=True\n    ):\n        """"""\n        Actual implementation underlying `execute_timesteps` and `execute_episodes`.\n\n        Args:\n            num_timesteps (Optional[int]): The maximum number of timesteps to run. At least one of `num_timesteps` or\n                `num_episodes` must be provided.\n            num_episodes (Optional[int]): The maximum number of episodes to run. At least one of `num_timesteps` or\n                `num_episodes` must be provided.\n            use_exploration (Optional[bool]): Indicates whether to utilize exploration (epsilon or noise based)\n                when picking actions. Default: True.\n            max_timesteps_per_episode (Optional[int]): Can be used to limit the number of timesteps per episode.\n                Use None or 0 for no limit. Default: None.\n            update_spec (Optional[dict]): Update parameters. If None, the worker only performs rollouts.\n                Matches the structure of an Agent\'s update_spec dict and will be ""defaulted"" by that dict.\n                See `input_parsing/parse_update_spec.py` for more details.\n            frameskip (Optional[int]): How often actions are repeated after retrieving them from the agent.\n                Rewards are accumulated over the number of skips. Use None for the Worker\'s default value.\n            reset (bool): Whether to reset the environment and all the Worker\'s internal counters.\n                Default: True.\n\n        Returns:\n            dict: Execution statistics.\n        """"""\n        assert num_timesteps is not None or num_episodes is not None,\\\n            ""ERROR: One of `num_timesteps` or `num_episodes` must be provided!""\n\n        # Determine `max_timesteps` for this execution run.\n        if self.max_timesteps is not None:\n            max_timesteps = self.max_timesteps\n        elif num_timesteps is not None:\n            max_timesteps = num_timesteps\n        elif max_timesteps_per_episode is not None and max_timesteps_per_episode > 0:\n            max_timesteps = num_episodes * max_timesteps_per_episode\n        else:\n            max_timesteps = 1e6\n\n        # Are we updating or just acting/observing?\n        update_spec = default_dict(update_spec, self.agent.update_spec)\n        self.set_update_schedule(update_spec)\n\n        num_timesteps = num_timesteps or 0\n        num_episodes = num_episodes or 0\n        max_timesteps_per_episode = [max_timesteps_per_episode or 0 for _ in range_(self.num_environments)]\n        frameskip = frameskip or self.frameskip\n\n        # Stats.\n        timesteps_executed = 0\n        episodes_executed = 0\n\n        start = time.perf_counter()\n        episode_terminals = self.episode_terminals\n        if reset is True:\n            self.env_frames = 0\n            self.episodes_since_update = 0\n            self.finished_episode_returns = [[] for _ in range_(self.num_environments)]\n            self.finished_episode_durations = [[] for _ in range_(self.num_environments)]\n            self.finished_episode_timesteps = [[] for _ in range_(self.num_environments)]\n\n            for i, env_id in enumerate(self.env_ids):\n                self.episode_returns[i] = 0\n                self.episode_timesteps[i] = 0\n                self.episode_terminals[i] = False\n                self.episode_starts[i] = time.perf_counter()\n                if self.worker_executes_preprocessing:\n                    self.state_is_preprocessed[env_id] = False\n\n            self.env_states = self.vector_env.reset_all()\n            self.agent.reset()\n        elif self.env_states[0] is None:\n            raise RLGraphError(""Runner must be reset at the very beginning. Environment is in invalid state."")\n\n        # Only run everything for at most num_timesteps (if defined).\n        env_states = self.env_states\n        while not (0 < num_timesteps <= timesteps_executed):\n            if self.render:\n                self.vector_env.render()\n\n            time_percentage = min(self.agent.timesteps / max_timesteps, 1.0)\n\n            if self.worker_executes_preprocessing:\n                for i, env_id in enumerate(self.env_ids):\n                    state, _ = self.agent.state_space.force_batch(env_states[i])\n                    if self.preprocessors[env_id] is not None:\n                        if self.state_is_preprocessed[env_id] is False:\n                            self.preprocessed_states_buffer[i] = self.preprocessors[env_id].preprocess(state)\n                            self.state_is_preprocessed[env_id] = True\n                    else:\n                        self.preprocessed_states_buffer[i] = env_states[i]\n                # TODO extra returns when worker is not applying preprocessing.\n                actions = self.agent.get_action(\n                    states=self.preprocessed_states_buffer, use_exploration=use_exploration,\n                    apply_preprocessing=not self.worker_executes_preprocessing, time_percentage=time_percentage\n                )\n                preprocessed_states = np.array(self.preprocessed_states_buffer)\n            else:\n                actions, preprocessed_states = self.agent.get_action(\n                    states=np.array(env_states), use_exploration=use_exploration,\n                    apply_preprocessing=True, extra_returns=""preprocessed_states"", time_percentage=time_percentage\n                )\n                preprocessed_states = horizontalize_space_sample(\n                    self.agent.preprocessed_state_space, preprocessed_states, self.num_environments\n                )\n\n            # Accumulate the reward over n env-steps (equals one action pick). n=self.frameskip.\n            env_rewards = [0 for _ in range_(self.num_environments)]\n            next_states = None\n\n            env_actions = horizontalize_space_sample(self.agent.action_space, actions, self.num_environments)\n\n            for _ in range_(frameskip):\n                next_states, step_rewards, episode_terminals, _ = self.vector_env.step(actions=env_actions)\n\n                self.env_frames += self.num_environments\n                for i, step_reward in enumerate(step_rewards):\n                    env_rewards[i] += step_reward\n                if np.any(episode_terminals):\n                    break\n\n            # Only render once per action.\n            #if self.render:\n            #    self.vector_env.environments[0].render()\n\n            for i, env_id in enumerate(self.env_ids):\n                self.episode_returns[i] += env_rewards[i]\n                self.episode_timesteps[i] += 1\n\n                if 0 < max_timesteps_per_episode[i] <= self.episode_timesteps[i]:\n                    episode_terminals[i] = True\n                if self.worker_executes_preprocessing:\n                    self.state_is_preprocessed[env_id] = False\n                # Do accounting for finished episodes.\n                if episode_terminals[i]:\n                    episodes_executed += 1\n                    self.episodes_since_update += 1\n                    episode_duration = time.perf_counter() - self.episode_starts[i]\n                    self.finished_episode_returns[i].append(self.episode_returns[i])\n                    self.finished_episode_durations[i].append(episode_duration)\n                    self.finished_episode_timesteps[i].append(self.episode_timesteps[i])\n\n                    self.log_finished_episode(\n                        episode_return=self.episode_returns[i],\n                        duration=episode_duration,\n                        timesteps=self.episode_timesteps[i],\n                        env_num=i\n                    )\n\n                    # Reset this environment and its preprocecssor stack.\n                    env_states[i] = self.vector_env.reset(i)\n                    if self.worker_executes_preprocessing and self.preprocessors[env_id] is not None:\n                        self.preprocessors[env_id].reset()\n                        # This re-fills the sequence with the reset state.\n                        state, _ = self.agent.state_space.force_batch(env_states[i])\n                        # Pre - process, add to buffer\n                        self.preprocessed_states_buffer[i] = np.array(self.preprocessors[env_id].preprocess(state))\n                        self.state_is_preprocessed[env_id] = True\n\n                    self.episode_returns[i] = 0\n                    self.episode_timesteps[i] = 0\n                    self.episode_starts[i] = time.perf_counter()\n                else:\n                    # Otherwise assign states to next states\n                    env_states[i] = next_states[i]\n\n                if self.worker_executes_preprocessing and self.preprocessors[env_id] is not None:\n                    #next_state, _ = self.agent.state_space.force_batch(env_states[i])\n                    next_states[i] = np.array(self.preprocessors[env_id].preprocess(env_states[i]))  # next_state\n                self._observe(\n                    self.env_ids[i], preprocessed_states[i], env_actions[i], env_rewards[i], next_states[i],\n                    episode_terminals[i]\n                )\n            self.update_if_necessary(time_percentage=time_percentage)\n            timesteps_executed += self.num_environments\n            num_timesteps_reached = (0 < num_timesteps <= timesteps_executed)\n\n            if 0 < num_episodes <= episodes_executed or num_timesteps_reached:\n                break\n\n        total_time = (time.perf_counter() - start) or 1e-10\n\n        # Return values for current episode(s) if None have been completed.\n        if episodes_executed == 0:\n            mean_episode_runtime = 0\n            mean_episode_reward = np.mean(self.episode_returns)\n            mean_episode_reward_last_10_episodes = mean_episode_reward\n            max_episode_reward = np.max(self.episode_returns)\n            final_episode_reward = self.episode_returns[0]\n        else:\n            all_finished_durations = []\n            all_finished_rewards = []\n            for i in range_(self.num_environments):\n                all_finished_rewards.extend(self.finished_episode_returns[i])\n                all_finished_durations.extend(self.finished_episode_durations[i])\n            mean_episode_runtime = np.mean(all_finished_durations)\n            mean_episode_reward = np.mean(all_finished_rewards)\n            mean_episode_reward_last_10_episodes = np.mean(all_finished_rewards[-10:])\n            max_episode_reward = np.max(all_finished_rewards)\n            final_episode_reward = all_finished_rewards[-1]\n\n        self.episode_terminals = episode_terminals\n        self.env_states = env_states\n        results = dict(\n            runtime=total_time,\n            # Agent act/observe throughput.\n            timesteps_executed=timesteps_executed,\n            ops_per_second=(timesteps_executed / total_time),\n            # Env frames including action repeats.\n            env_frames=self.env_frames,\n            env_frames_per_second=(self.env_frames / total_time),\n            episodes_executed=episodes_executed,\n            episodes_per_minute=(episodes_executed / (total_time / 60)),\n            mean_episode_runtime=mean_episode_runtime,\n            mean_episode_reward=mean_episode_reward,\n            mean_episode_reward_last_10_episodes=mean_episode_reward_last_10_episodes,\n            max_episode_reward=max_episode_reward,\n            final_episode_reward=final_episode_reward\n        )\n\n        # Total time of run.\n        self.logger.info(""Finished execution in {} s"".format(total_time))\n        # Total (RL) timesteps (actions) done (and timesteps/sec).\n        self.logger.info(""Time steps (actions) executed: {} ({} ops/s)"".\n                         format(results[\'timesteps_executed\'], results[""ops_per_second""]))\n        # Total env-timesteps done (including action repeats) (and env-timesteps/sec).\n        self.logger.info(""Env frames executed (incl. action repeats): {} ({} frames/s)"".\n                         format(results[\'env_frames\'], results[""env_frames_per_second""]))\n        # Total episodes done (and episodes/min).\n        self.logger.info(""Episodes finished: {} ({} episodes/min)"".\n                         format(results[\'episodes_executed\'], results[""episodes_per_minute""]))\n        self.logger.info(""Mean episode runtime: {}s"".format(results[""mean_episode_runtime""]))\n        self.logger.info(""Mean episode reward: {}"".format(results[""mean_episode_reward""]))\n        self.logger.info(""Mean episode reward (last 10 episodes): {}"".format(\n            results[""mean_episode_reward_last_10_episodes""])\n        )\n        self.logger.info(""Max. episode reward: {}"".format(results[""max_episode_reward""]))\n        self.logger.info(""Final episode reward: {}"".format(results[""final_episode_reward""]))\n\n        return results\n\n    def _observe(self, env_ids, states, actions, rewards, next_states, terminals):\n        #print(""states={} actions={} rewards={}"".format(states, actions, rewards))\n        # TODO: If worker does not execute preprocessing, next state is not preprocessed here.\n        # Observe per environment.\n        self.agent.observe(\n            preprocessed_states=states, actions=actions, internals=[],\n            rewards=rewards, next_states=next_states,\n            terminals=terminals, env_id=env_ids\n        )\n\n'"
rlgraph/execution/worker.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport logging\n\nimport numpy as np\nfrom six.moves import xrange as range_\nfrom rlgraph.environments import VectorEnv, SequentialVectorEnv\nfrom rlgraph.utils.specifiable import Specifiable\n\n\nclass Worker(Specifiable):\n    """"""\n    Generic worker to locally interact with simulator environments.\n    """"""\n    def __init__(self, agent, env_spec=None, num_environments=1, frameskip=1, render=False,\n                 worker_executes_exploration=True, exploration_epsilon=0.1, episode_finish_callback=None,\n                 max_timesteps=None):\n        """"""\n        Args:\n            agent (Agent): Agent to execute environment on.\n\n            env_spec Optional[Union[callable, dict]]): Either an environment spec or a callable returning a new\n                environment.\n\n            num_environments (int): How many single Environments should be run in parallel in a SequentialVectorEnv.\n\n            frameskip (int): How often actions are repeated after retrieving them from the agent.\n                This setting can be overwritten in the single calls to the different `execute_..` methods.\n\n            render (bool): Whether to render the environment after each action.\n                Default: False.\n\n            worker_executes_exploration (bool): If worker executes exploration by sampling.\n            exploration_epsilon (Optional[float]): Epsilon to use if worker executes exploration.\n\n            max_timesteps (Optional[int]): A max number on the time steps this Worker expects to perform.\n                This is not a forced limit, but serves to calculate the `time_percentage` value passed into\n                the Agent for time-dependent (decay) parameter calculations.\n                If None, Worker will try to infer this value automatically.\n        """"""\n        super(Worker, self).__init__()\n        self.num_environments = num_environments\n        self.logger = logging.getLogger(__name__)\n\n        # VectorEnv was passed in directly -> Use that one.\n        if isinstance(env_spec, VectorEnv):\n            self.vector_env = env_spec\n            self.num_environments = self.vector_env.num_environments\n            self.env_ids = [""env_{}"".format(i) for i in range_(self.num_environments)]\n        # `Env_spec` is for single envs inside a SequentialVectorEnv.\n        elif env_spec is not None:\n            self.vector_env = SequentialVectorEnv(env_spec=env_spec, num_environments=self.num_environments)\n            self.env_ids = [""env_{}"".format(i) for i in range_(self.num_environments)]\n        # No env_spec.\n        else:\n            self.vector_env = None\n            self.env_ids = []\n\n        self.agent = agent\n        self.frameskip = frameskip\n        self.render = render\n\n        # Update schedule if worker is performing updates.\n        self.updating = None\n        self.steps_before_update = None\n        self.update_interval = None\n        self.update_steps = None\n        self.sync_interval = None\n        self.episodes_since_update = 0\n\n        self.max_timesteps = max_timesteps\n\n        # Default val or None?\n        self.update_mode = ""time_steps""\n\n        self.worker_executes_exploration = worker_executes_exploration\n        self.exploration_epsilon = exploration_epsilon\n\n        self.episode_finish_callback = episode_finish_callback\n\n    def execute_timesteps(self, num_timesteps, max_timesteps_per_episode=0, update_spec=None, use_exploration=True,\n                          frameskip=1, reset=True):\n        """"""\n        Executes environment for a fixed number of timesteps.\n\n        Args:\n            num_timesteps (int): Number of time steps to execute.\n            max_timesteps_per_episode (Optional[int]): Can be used to limit the number of timesteps per episode.\n                Use None or 0 for no limit. Default: None.\n            update_spec (Optional[dict]): Update parameters. If None, the worker only peforms rollouts.\n                Expects keys \'update_interval\' to indicate how frequent update is called, \'num_updates\'\n                to indicate how many updates to perform every update interval, and \'steps_before_update\' to indicate\n                how many steps to perform before beginning to update.\n            use_exploration (Optional[bool]): Indicates whether to utilize exploration (epsilon or noise based)\n                when picking actions.\n            frameskip (int): How often actions are repeated after retrieving them from the agent.\n                Use None for the Worker\'s default value.\n            reset (bool): Whether to reset the environment and all the Worker\'s internal counters.\n                Default: True.\n\n        Returns:\n            dict: Execution statistics.\n        """"""\n        pass\n\n    def execute_and_get_timesteps(self, num_timesteps, max_timesteps_per_episode=0, use_exploration=True,\n                                  frameskip=None, reset=True):\n        """"""\n        Executes timesteps and returns experiences. Intended for distributed data collection\n        without performing updates.\n\n        Args:\n            num_timesteps (int): Number of time steps to execute.\n            max_timesteps_per_episode (Optional[int]): Can be used to limit the number of timesteps per episode.\n                Use None or 0 for no limit. Default: None.\n            use_exploration (Optional[bool]): Indicates whether to utilize exploration (epsilon or noise based)\n                when picking actions.\n            frameskip (int): How often actions are repeated after retrieving them from the agent.\n                Use None for the Worker\'s default value.\n            reset (bool): Whether to reset the environment and all the Worker\'s internal counters.\n                Default: True.\n\n        Returns:\n            EnvSample: EnvSample object holding the collected experiences.\n        """"""\n        pass\n\n    def execute_episodes(self, num_episodes, max_timesteps_per_episode=0, update_spec=None, use_exploration=True,\n                         frameskip=None, reset=True):\n        """"""\n        Executes environment for a fixed number of episodes.\n\n        Args:\n            num_episodes (int): Number of episodes to execute.\n            max_timesteps_per_episode (Optional[int]): Can be used to limit the number of timesteps per episode.\n                Use None or 0 for no limit. Default: None.\n            update_spec (Optional[dict]): Update parameters. If None, the worker only peforms rollouts.\n                Expects keys \'update_interval\' to indicate how frequent update is called, \'num_updates\'\n                to indicate how many updates to perform every update interval, and \'steps_before_update\' to indicate\n                how many steps to perform before beginning to update.\n            use_exploration (Optional[bool]): Indicates whether to utilize exploration (epsilon or noise based)\n                when picking actions.\n            frameskip (int): How often actions are repeated after retrieving them from the agent.\n                Use None for the Worker\'s default value.\n            reset (bool): Whether to reset the environment and all the Worker\'s internal counters.\n                Default: True.\n\n        Returns:\n            dict: Execution statistics.\n        """"""\n        pass\n\n    def execute_and_get_episodes(self, num_episodes, max_timesteps_per_episode=0, use_exploration=False,\n                                 frameskip=None, reset=True):\n        """"""\n        Executes episodes and returns experiences as separate episode sequences.\n        Intended for distributed data collection without performing updates.\n\n        Args:\n            num_episodes (int): Number of episodes to execute.\n            max_timesteps_per_episode (Optional[int]): Can be used to limit the number of timesteps per episode.\n                Use None or 0 for no limit. Default: None.\n            use_exploration (Optional[bool]): Indicates whether to utilize exploration (epsilon or noise based)\n                when picking actions.\n            frameskip (int): How often actions are repeated after retrieving them from the agent. Rewards are\n                accumulated over the skipped frames.\n            reset (bool): Whether to reset the environment and all the Worker\'s internal counters.\n                Default: True.\n\n        Returns:\n            EnvSample: EnvSample object holding the collected episodes.\n        """"""\n        pass\n\n    def update_if_necessary(self, time_percentage):\n        """"""\n        Calls update on the agent according to the update schedule set for this worker.\n\n        #Args:\n        #    timesteps_executed (int): Timesteps executed thus far.\n\n        Returns:\n            float: The summed up loss (over all self.update_steps).\n        """"""\n        if self.updating:\n            # Are we allowed to update?\n            if self.agent.timesteps > self.steps_before_update and \\\n                    (self.agent.observe_spec[""buffer_enabled""] is False or  # No update before some data in buffer\n                     int(self.agent.timesteps / self.num_environments) >= self.agent.observe_spec[""buffer_size""]):\n                # Updating according to one update mode:\n                if self.update_mode == ""time_steps"" and self.agent.timesteps % self.update_interval == 0:\n                    return self.execute_update(time_percentage)\n                elif self.update_mode == ""episodes"" and self.episodes_since_update == self.update_interval:\n                    # Do not do modulo here - this would be called every step in one episode otherwise.\n                    loss = self.execute_update(time_percentage)\n                    self.episodes_since_update = 0\n                    return loss\n        return None\n\n    def execute_update(self, time_percentage):\n        loss = 0\n        for _ in range_(self.update_steps):\n            ret = self.agent.update(time_percentage=time_percentage)\n            if isinstance(ret, tuple):\n                loss += ret[0]\n            else:\n                loss += ret\n        return loss\n\n    def set_update_schedule(self, update_schedule=None):\n        """"""\n        Sets this worker\'s update schedule. By default, a worker is not updating but only acting\n        and observing samples.\n\n        Args:\n            update_schedule (Optional[dict]): Update parameters. If None, the worker only performs rollouts.\n                Expects keys \'update_interval\' to indicate how frequent update is called, \'num_updates\'\n                to indicate how many updates to perform every update interval, and \'steps_before_update\' to indicate\n                how many steps to perform before beginning to update.\n        """"""\n        if update_schedule is not None:\n            self.updating = update_schedule[""do_updates""]\n            self.steps_before_update = update_schedule[""steps_before_update""]\n            self.update_interval = update_schedule[""update_interval""]\n            self.update_steps = update_schedule[""update_steps""]\n            self.sync_interval = update_schedule[""sync_interval""]\n\n            # Interpret update interval as n time-steps or n episodes.\n            self.update_mode = update_schedule.get(""update_mode"", ""time_steps"")\n        else:\n            self.updating = False\n\n    def get_action(self, states, use_exploration, apply_preprocessing, extra_returns):\n        if self.worker_executes_exploration:\n            # Only once for all actions otherwise we would have to call a session anyway.\n            if np.random.random() <= self.exploration_epsilon:\n                if self.num_environments == 1:\n                    # Sample returns without batch dim -> wrap.\n                    action = [self.agent.action_space.sample(size=self.num_environments)]\n                else:\n                    action = self.agent.action_space.sample(size=self.num_environments)\n            else:\n                if self.num_environments == 1:\n                    action = [self.agent.get_action(states=states, use_exploration=use_exploration,\n                              apply_preprocessing=apply_preprocessing, extra_returns=extra_returns)]\n                else:\n                    action = self.agent.get_action(states=states, use_exploration=use_exploration,\n                             apply_preprocessing=apply_preprocessing, extra_returns=extra_returns)\n            return action\n        else:\n            return self.agent.get_action(states=states, use_exploration=use_exploration,\n                                         apply_preprocessing=apply_preprocessing, extra_returns=extra_returns)\n\n    def log_finished_episode(self, episode_return, duration, timesteps, env_num=0):\n        self.logger.debug(""Finished episode: return={}, duration={}s, timesteps={}."".format(\n            episode_return, duration, timesteps))\n\n        if self.episode_finish_callback:\n            self.episode_finish_callback(\n                episode_return=episode_return, duration=duration, timesteps=timesteps, env_num=env_num\n            )\n'"
rlgraph/graphs/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.graphs.meta_graph import MetaGraph\nfrom rlgraph.graphs.meta_graph_builder import MetaGraphBuilder\nfrom rlgraph.graphs.graph_builder import GraphBuilder\nfrom rlgraph.graphs.graph_executor import GraphExecutor\nfrom rlgraph.graphs.pytorch_executor import PyTorchExecutor\nfrom rlgraph.graphs.tensorflow_executor import TensorFlowExecutor\n\n\ndef backend_executor():\n    """"""\n    Returns default class for backend.\n    Returns: Executioner for the specified backend.\n    """"""\n    if get_backend() == ""tf"":\n        return TensorFlowExecutor\n    elif get_backend() == ""pytorch"":\n        return PyTorchExecutor\n\n\nGraphExecutor.__lookup_classes__ = dict(\n    tf=TensorFlowExecutor,\n    tensorflow=TensorFlowExecutor,\n    pt=PyTorchExecutor,\n    pytorch=PyTorchExecutor\n)\n\n__all__ = [""MetaGraph"", ""MetaGraphBuilder"", ""GraphBuilder"",\n           ""GraphExecutor"", ""TensorFlowExecutor"", ""PyTorchExecutor"", ""backend_executor""]\n'"
rlgraph/graphs/graph_builder.py,10,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport inspect\nimport logging\nimport re\nimport time\n\nfrom rlgraph import get_backend, get_config\nfrom rlgraph.components.component import Component\nfrom rlgraph.spaces import Space, Dict\nfrom rlgraph.spaces.space_utils import get_space_from_op, check_space_equivalence\nfrom rlgraph.utils.define_by_run_ops import execute_define_by_run_graph_fn\nfrom rlgraph.utils.input_parsing import parse_summary_spec\nfrom rlgraph.utils.op_records import FlattenedDataOp, DataOpRecord, DataOpRecordColumnIntoGraphFn, \\\n    DataOpRecordColumnIntoAPIMethod, DataOpRecordColumnFromGraphFn, DataOpRecordColumnFromAPIMethod, get_call_param_name\nfrom rlgraph.utils.ops import is_constant, ContainerDataOp, DataOpDict, flatten_op, unflatten_op, TraceContext\nfrom rlgraph.utils.rlgraph_errors import RLGraphError, RLGraphBuildError, RLGraphSpaceError\nfrom rlgraph.utils.specifiable import Specifiable\nfrom rlgraph.utils.util import force_list, force_tuple, get_shape\nfrom rlgraph.utils.visualization_util import draw_sub_meta_graph_from_op_rec\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n    from rlgraph.utils.tf_util import pin_global_variables\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass GraphBuilder(Specifiable):\n    """"""\n    The graph builder assembles the RLGraph meta-graph by tracing through\n    components, sockets and connections and creating the underlying computation\n    graph.\n    """"""\n    def __init__(self, name=""model"", action_space=None, summary_spec=None, max_build_iterations=3000):\n        """"""\n        Args:\n            name (str): The name of this GraphBuilder and of the meta-graph\'s root-component.\n            action_space (Optional[Space]): The action Space information to be passed into calls to each Components\'\n                `when_input_complete` methods.\n            summary_spec (Optional[dict]): A specification dict that defines, which summaries we would like to\n                create in the graph and register with each Component.\n        """"""\n        super(GraphBuilder, self).__init__()\n\n        # The name of this model. Our root-Component gets this name.\n        self.logger = logging.getLogger(__name__)\n        self.name = name\n        self.action_space = None\n        if action_space is not None:\n            self.action_space = Space.from_spec(action_space)\n        self.summary_spec = parse_summary_spec(summary_spec)\n        self.max_build_iterations = max_build_iterations\n        # Set of components that have been analyzed on why they remain input-incomplete.\n        self.investigated_input_incomplete_components = set()\n\n        # All components assigned to each device, for debugging and analysis.\n        self.device_component_assignments = dict()\n        self.available_devices = None\n\n        # Device specifications.\n        self.device_strategy = None\n        self.default_device = None\n        self.device_map = None\n\n        # Build status/phase. Can take values\n        # None: Build has not started yet.\n        # ""building"": actual graph is being built from meta-graph.\n        self.phase = None\n\n        # Some build stats:\n        # Number of meta op-records.\n        self.num_meta_ops = 0\n        # Number of actual backend ops.\n        self.num_ops = 0\n        # Number of trainable variables (optimizable weights).\n        self.num_trainable_parameters = 0\n        self.graph_call_times = []\n        self.var_call_times = []\n\n        # Create an empty root-Component into which everything will be assembled by an Algo.\n        self.root_component = None\n\n        # Maps API method names to in- (placeholders) and out op columns (ops to pull).\n        self.api = {}\n\n        self.op_records_to_process = set()\n        self.op_recs_depending_on_variables = set()\n\n        # A register for all created placeholders by name.\n        self.placeholders = {}\n        # Our meta-graph from which to build the graph.\n        self.meta_graph = None\n\n    def build_graph_with_options(self, meta_graph, input_spaces, available_devices,\n                                 device_strategy=""default"", default_device=None,\n                                 device_map=None, build_options=None):\n        """"""\n        Builds graph with the given options. See build doc for build details.\n\n        Args:\n            meta_graph (MetaGraph): MetaGraph to build to backend graph.\n            input_spaces (dict): Input spaces to build for.\n            available_devices (list): Devices which can be used to assign parts of the graph\n                during graph assembly.\n            device_strategy (Optional[str]): Device strategy.\n            default_device (Optional[str]): Default device identifier.\n            device_map (Optional[Dict]): Dict of Component names mapped to device names to place the Component\'s ops.\n            build_options (Optional[Dict]): Dict of build options, e.g. default device handling for TF.\n        """"""\n        # No device context options.\n        if build_options is None or ""build_device_context"" not in build_options:\n            return self.build_graph(meta_graph, input_spaces, available_devices,\n                                    device_strategy, default_device, device_map)\n        else:\n            if get_backend() == ""tf"":\n                # Need to be fully specified to avoid errors, no defaults.\n                default_device_context = build_options[""build_device_context""]\n                pin_global = build_options[""pin_global_variable_device""]\n                if pin_global is not None:\n                    # Pin global variables for distributed TF.\n                    with tf.device(default_device_context), \\\n                         pin_global_variables(pin_global):\n                        return self.build_graph(meta_graph, input_spaces, available_devices,\n                                                device_strategy, default_device, device_map)\n                else:\n                    with tf.device(default_device_context):\n                        return self.build_graph(meta_graph, input_spaces, available_devices,\n                                                device_strategy, default_device, device_map)\n            else:\n                raise RLGraphError(""Build options are currently only available for TensorFlow."")\n\n    def build_graph(self, meta_graph, input_spaces, available_devices,\n                    device_strategy=""default"", default_device=None, device_map=None):\n        """"""\n        The actual iterative depth-first search algorithm to build our graph from the already existing\n        meta-Graph structure.\n        Starts from a set of DataOpRecords populated with the initial placeholders (from input\n        Spaces). Keeps pushing these ops through the meta-graph until a non-complete graph_fn\n        or a non-complete Component (with at least one non-complete API-method) is reached.\n        Replaces the ops in the set with the newly reached ones and re-iterates like this until all op-records\n        in the entire meta-graph have been filled with actual ops.\n\n        Args:\n            meta_graph (MetaGraph): MetaGraph to build to backend graph.\n            input_spaces (dict): Input spaces to build for.\n            available_devices (list): Devices which can be used to assign parts of the graph\n                during graph assembly.\n            device_strategy (Optional[str]): Device strategy.\n            default_device (Optional[str]): Default device identifier.\n            device_map (Optional[Dict]): Dict of Component names mapped to device names to place the Component\'s ops.\n        """"""\n        self.meta_graph = meta_graph\n\n        # Time the build procedure.\n        time_start = time.perf_counter()\n        assert self.meta_graph.build_status, ""ERROR: Meta graph must be built to build backend graph.""\n        self.root_component = self.meta_graph.root_component\n        self.graph_call_times = []\n        self.var_call_times = []\n        self.api = self.meta_graph.api\n        self.num_meta_ops = self.meta_graph.num_ops\n\n        # Set the build phase to `building`.\n        self.phase = ""building""\n\n        # Set devices usable for this graph.\n        self.available_devices = available_devices\n        self.device_strategy = device_strategy\n        self.default_device = default_device\n        self.device_map = device_map or {}\n\n        # Create the first actual ops based on the input-spaces.\n        # Some ops can only be created later when variable-based-Spaces are known (op_recs_depending_on_variables).\n        self.build_input_space_ops(input_spaces)\n\n        # Collect all components and add those op-recs to the set that are constant.\n        components = self.root_component.get_all_sub_components(exclude_self=False)\n        # Point to this GraphBuilder object.\n        # Add those op-recs to the set that are constant.\n        for component in components:\n            component.graph_builder = self\n        # Try to build if already input complete.\n        for component in components:\n            self.op_records_to_process.update(component.constant_op_records)\n            self.build_component_when_input_complete(component)\n\n        op_records_list = self._sort_op_recs(self.op_records_to_process)\n\n        # Re-iterate until our bag of op-recs to process is empty.\n        iterations = self._build(op_records_list)\n        time_build = time.perf_counter() - time_start\n        self.logger.info(""Computation-Graph build completed in {} s ({} iterations)."".format(time_build, iterations))\n\n        # Get some stats on the graph and report.\n        self.num_ops = self.count_ops()\n        self.logger.info(""Actual graph ops generated: {}"".format(self.num_ops))\n\n        self.num_trainable_parameters = self.count_trainable_parameters()\n        self.logger.info(""Number of trainable parameters: {}"".format(self.num_trainable_parameters))\n\n        # Sanity check the build.\n        self.sanity_check_build()\n\n        # The build here is the actual build overhead, so build time minus the tensorflow calls and variable\n        # creations which would have to happen either way.\n        build_overhead = time_build - sum(self.graph_call_times) - sum(self.var_call_times)\n\n        return dict(\n            build_overhead=build_overhead,\n            total_build_time=time_build,\n            op_creation=sum(self.graph_call_times),\n            var_creation=sum(self.var_call_times)\n        )\n\n    def build_input_space_ops(self, input_spaces):\n        """"""\n        Generates ops from Space information and stores these ops in the DataOpRecords of our API\n        methods.\n\n        Args:\n            input_spaces (dict): Dict with keys=api-method names; values=list of Space objects or specification dicts\n                to create the Spaces that go into the APIMethodRecords.\n\n        Returns:\n            Set[DataOpRecord]: A set of DataOpRecords with which we should start the building\n                process.\n        """"""\n        if input_spaces is None:\n            input_spaces = {}\n\n        for api_method_name, (in_op_records, _) in sorted(self.api.items()):\n            api_method_rec = self.root_component.api_methods[api_method_name]\n            spaces = []\n            for param_name in api_method_rec.input_names:\n                if self.root_component.api_method_inputs[param_name] == ""flex"":\n                    if param_name in input_spaces:\n                        spaces.append((param_name, input_spaces[param_name]))\n                        self.root_component.api_method_inputs[param_name] = Space.from_spec(input_spaces[param_name])\n                elif isinstance(self.root_component.api_method_inputs[param_name], Space):\n                    if param_name in input_spaces:\n                        spaces.append((param_name, self.root_component.api_method_inputs[param_name]))\n                else:\n                    if self.root_component.api_method_inputs[param_name] == ""*flex"":\n                        if param_name in input_spaces:\n                            for i, s in enumerate(force_list(input_spaces[param_name])):\n                                spaces.append((param_name+""-""+str(i), s))\n                                self.root_component.api_method_inputs[param_name+""[""+str(i)+""]""] = Space.from_spec(s)\n                    elif self.root_component.api_method_inputs[param_name] == ""**flex"":\n                        if param_name in input_spaces:\n                            for k in sorted(input_spaces[param_name].keys()):\n                                spaces.append((param_name+""-""+k, input_spaces[param_name][k]))\n                                self.root_component.api_method_inputs[param_name+""[""+k+""]""] = \\\n                                    Space.from_spec(input_spaces[param_name][k])\n                    else:\n                        assert param_name in input_spaces\n                        spaces.append((param_name, input_spaces[param_name]))\n                        self.root_component.api_method_inputs[param_name] = Space.from_spec(input_spaces[param_name])\n            assert len(spaces) == len(in_op_records)\n\n            # Create the placeholders and store them in the given DataOpRecords.\n            for i, (name, space) in enumerate(spaces):\n                # Space is dependent on the variables of some sub-component (wait with the construction of the\n                # placeholder until the component is variable-complete).\n                if isinstance(space, str) and re.match(r\'^variables:\', space):\n                    in_op_records[i].space = space\n                    self.op_recs_depending_on_variables.add(in_op_records[i])\n                    continue\n\n                # Construct Space from a spec.\n                elif not isinstance(space, Space):\n                    space = Space.from_spec(space)\n\n                in_op_records[i].space = space\n\n                in_op_records[i].op = self.get_placeholder(\n                    name=name, space=space,\n                    component=next(iter(in_op_records[0].next)).column.component\n                )\n                self.op_records_to_process.add(in_op_records[i])\n\n    def get_placeholder(self, name, space, component):\n        """"""\n        Generates one or more placeholders given a name, space and a component (for device inference).\n\n        Args:\n            name (str): The name of the placeholder to create.\n            space (spec(Space)): The Space object to generate the placeholder for.\n\n            component (Component): The Component into which the placeholder will go (needed  for automatic device\n                inference).\n\n        Returns:\n            DataOp: The generated placeholder(s) as a DataOp (e.g. DataOpTuple, SingleDataOp, etc..).\n        """"""\n        if name in self.placeholders:\n            return self.placeholders[name]\n\n        device = self.get_device(component)  #, variables=True)\n        placeholder = None\n        space = Space.from_spec(space)\n        if get_backend() == ""tf"":\n            with tf.device(device):\n                placeholder = space.get_variable(name=name, is_input_feed=True)\n        elif get_backend() == ""pytorch"":\n            placeholder = space.get_variable(name=name, is_input_feed=True, is_python=True)\n        self.placeholders[name] = placeholder\n        return placeholder\n\n    def build_component_when_input_complete(self, component, check_sub_components=True):\n        graph_fn_requiring_var_completeness = [gf.name for gf in component.graph_fns.values() if\n                                               gf.requires_variable_completeness is True]\n        # Not input complete yet -> Check now.\n        if component.input_complete is False or component.built is False:\n            component.check_input_completeness()\n            # Call `when_input_complete` once on that Component.\n            if component.input_complete is True:\n                self.logger.debug(""Component {} is input-complete; Spaces per API-method input parameter are: {}"".\n                                  format(component.name, component.api_method_inputs))\n                device = self.get_device(component, variables=True)\n                # This builds variables which would have to be done either way:\n                call_time = time.perf_counter()\n                # If the Component throws a Space checking error, catch it here and perform the proper debugging\n                # routine.\n                try:\n                    component.when_input_complete(\n                        input_spaces=None, action_space=self.action_space, device=device,\n                        summary_regexp=self.summary_spec[""summary_regexp""]\n                    )\n                except RLGraphSpaceError as e:\n                    # Should we plot the subgraph that lead to this error?\n                    if get_config().get(""GRAPHVIZ_RENDER_BUILD_ERRORS"", True) is True:\n                        op_rec = e.space.op_rec_ref\n                        assert op_rec is not None\n                        draw_sub_meta_graph_from_op_rec(op_rec, meta_graph=self.meta_graph)\n                    # Reraise e.\n                    raise e\n\n                self.var_call_times.append(time.perf_counter() - call_time)\n                # Call all no-input graph_fns of the new Component.\n                for no_in_col in component.no_input_graph_fn_columns:\n                    # Do not call _variables (only later, when Component is also variable-complete).\n                    if no_in_col.graph_fn.__name__ not in graph_fn_requiring_var_completeness:\n                        self.run_through_graph_fn_with_device_and_scope(no_in_col)\n                        # Keep working with the generated output ops.\n                        self.op_records_to_process.update(no_in_col.out_graph_fn_column.op_records)\n\n        if component.input_complete is True and component.check_variable_completeness():\n            # The graph_fn _variables has some in-op-columns that need to be run through the function.\n            for graph_fn_name in graph_fn_requiring_var_completeness:\n                graph_fn_rec = component.graph_fns[graph_fn_name]\n                # TODO: Think about only running through no-input-graph-fn once, no matter how many in-op-columns it has.\n                # TODO: Then link the first in-op-column (empty) to all out-op-columns.\n                for i, in_op_col in enumerate(graph_fn_rec.in_op_columns):\n                    if in_op_col.already_sent is False and in_op_col.is_complete():\n                        self.run_through_graph_fn_with_device_and_scope(in_op_col)\n                        # If graph_fn_rec doesn\'t know about the out-op-col yet, add it.\n                        if len(graph_fn_rec.out_op_columns) <= i:\n                            assert len(graph_fn_rec.out_op_columns) == i  # make sure, it\'s really just one col missing\n                            graph_fn_rec.out_op_columns.append(in_op_col.out_graph_fn_column)\n                        self.op_records_to_process.update(graph_fn_rec.out_op_columns[i].op_records)\n\n            if check_sub_components is True:\n                # Check variable-completeness and actually call the _variable graph_fn if not already done so.\n                # Collect sub-components and build them as well if they just became variable-complete.\n                sub_components = component.sub_components.values()\n                sub_components_not_var_complete = set()\n                for sub_component in sub_components:\n                    if sub_component.variable_complete is False:\n                        sub_components_not_var_complete.add(sub_component)\n\n                for sub_component in sub_components_not_var_complete:\n                    self.build_component_when_input_complete(sub_component)\n\n            # Now that the component is variable-complete, the parent may have become variable-complete as well.\n            if component.parent_component is not None and component.parent_component.variable_complete is False:\n                self.build_component_when_input_complete(component.parent_component, check_sub_components=False)\n\n    def run_through_graph_fn_with_device_and_scope(self, op_rec_column, create_new_out_column=None):\n        """"""\n        Runs through a graph_fn with the given ops and thereby assigns a device (Component\'s device or GraphBuilder\'s\n        default) to the ops generated by a graph_fn.\n\n        Args:\n            op_rec_column (DataOpRecordColumnIntoGraphFn): The column of DataOpRecords to be fed through the\n                graph_fn.\n            create_new_out_column (bool): Whether to produce the out op-record column (or use the one already in\n                the meta-graph). If True and the `op_rec_column` already links to an out op-rec column, raises\n                an error.\n                Default: False.\n        """"""\n        if op_rec_column.already_sent is not False:\n            raise RLGraphBuildError(\n                ""op_rec_column ID={} already sent through graph_fn \'{}\'! Cannot do so again."".format(\n                    op_rec_column.id, op_rec_column.graph_fn.__name__)\n            )\n\n        # Get the device for the ops generated in the graph_fn (None for custom device-definitions within the graph_fn).\n        device = self.get_device(op_rec_column.component, variables=False)\n\n        if get_backend() == ""tf"":\n            # TODO: Write custom scope generator for devices (in case None, etc..).\n            if device is not None:\n                # Assign proper device to all ops created in this context manager.\n                with tf.device(device):\n                    # Name ops correctly according to our Component hierarchy.\n                    with tf.name_scope(op_rec_column.component.global_scope +\n                                       (\'/\' if op_rec_column.component.global_scope else """")):\n                        self.logger.info(\n                            ""Assigning device \'{}\' to graph_fn \'{}\' (scope \'{}\')."".\n                            format(device, op_rec_column.graph_fn.__name__, op_rec_column.component.global_scope)\n                        )\n                        out_op_rec_column = self.run_through_graph_fn(\n                            op_rec_column, create_new_out_column=create_new_out_column\n                        )\n                        op_rec_column.out_graph_fn_column = out_op_rec_column\n            else:\n                # Name ops correctly according to our Component hierarchy.\n                with tf.name_scope(op_rec_column.component.global_scope +\n                                   (\'/\' if op_rec_column.component.global_scope else """")):\n                    out_op_rec_column = self.run_through_graph_fn(\n                        op_rec_column, create_new_out_column=create_new_out_column\n                    )\n                    op_rec_column.out_graph_fn_column = out_op_rec_column\n\n            # Store assigned names for debugging.\n            if device is not None:\n                if device not in self.device_component_assignments:\n                    self.device_component_assignments[device] = [str(op_rec_column.graph_fn.__name__)]\n                else:\n                    self.device_component_assignments[device].append(str(op_rec_column.graph_fn.__name__))\n\n        elif get_backend() == ""pytorch"":\n            # No device handling via decorators.\n            out_op_rec_column = self.run_through_graph_fn(\n                op_rec_column, create_new_out_column=create_new_out_column\n            )\n            op_rec_column.out_graph_fn_column = out_op_rec_column\n\n        # Tag column as already sent through graph_fn.\n        op_rec_column.already_sent = True\n        return op_rec_column.out_graph_fn_column\n\n    def get_device(self, component, variables=False):\n        """"""\n        Determines and returns a device based on a given Component (or `self.default_device`).\n        Also does some sanity checking against our `device_strategy`.\n\n        Args:\n            component (Component): The Component to check for a defined device.\n            variables (bool): Whether the device is for the variables of the Component (vs the ops).\n\n        Returns:\n            str: The device to use for the component (its ops or variables or both).\n        """"""\n        # Component specifies it\'s own device: Use that.\n        # Then follow our device_map.\n        # Last resort: Use `self.default_device` (may be None).\n        device = component.device\n        # Try a map lookup via global-scope of the component.\n        if device is None:\n            # Sort by scope-length (such that the most specific assignments have priority).\n            for key in sorted(self.device_map.keys(), key=len, reverse=True):\n                if re.search(r\'^{}\\b\'.format(key), component.global_scope):\n                    device = self.device_map[key]\n                    break\n            else:\n                device = self.default_device\n        # Device is specific to whether we are creating variables or ops.\n        if isinstance(device, dict):\n            device = device.get(""variables"", None) if variables is True else device.get(""ops"", None)\n\n        # If device is local, but not available, use the default device (or None).\n        # TODO rethink handling device maps\n        # if device is not None and not re.match(r\'^/job:\', device) and device not in self.available_devices:\n        #     device = self.device_map.get(component.name, self.default_device)\n        # Device is specific to whether we are creating variables or ops.\n        if isinstance(device, dict):\n            device = device.get(""variables"", None) if variables is True else device.get(""ops"", None)\n\n        return device\n\n    def _run_in_context(self, graph_fn, component, *args, **kwargs):\n        is_build_time = self.phase == ""building""\n        if is_build_time and TraceContext.ACTIVE_CALL_CONTEXT is False:\n            call_time = time.perf_counter()\n            TraceContext.ACTIVE_CALL_CONTEXT = True\n            TraceContext.CONTEXT_START = call_time\n\n        component.start_summary_ops_buffer()\n        ops = graph_fn(component, *args, **kwargs)\n        summary_ops = component.pop_summary_ops_buffer()\n        if is_build_time and TraceContext.ACTIVE_CALL_CONTEXT is True:\n            self.graph_call_times.append(time.perf_counter() - TraceContext.CONTEXT_START)\n            TraceContext.CONTEXT_START = None\n            TraceContext.ACTIVE_CALL_CONTEXT = False\n\n        return ops, summary_ops\n\n    def run_through_graph_fn(self, op_rec_column, create_new_out_column=None):\n        """"""\n        Pushes all ops in the column through the respective graph_fn (graph_fn-spec and call-options are part of\n        the column).\n        Call options include flattening ops, flattening+splitting ops and (when splitting) adding the auto-generated\n        flat key as first parameter to the different (split) calls of graph_fn.\n        After the call, the already existing output column is populated with the actual results.\n\n        Args:\n            op_rec_column (DataOpRecordColumnIntoGraphFn): The column of DataOpRecords to be fed through the\n                graph_fn.\n            create_new_out_column (Optional[bool]): If given, whether to produce the out op-record column\n                (or use the one already in the meta-graph). If True and the `op_rec_column` already links to an out\n                op-rec column, raises an error.\n                Default: None, meaning only create a new column if one does not exist.\n\n        Returns:\n            DataOpRecordColumnFromGraphFn: The op-record column coming out of the graph_fn. This column may have\n                already existed in the meta-graph before the graph_fn call or may have been generated during this\n                call (if `create_new_out_column` is True).\n        """"""\n        args = [r.op for r in op_rec_column.op_records if r.kwarg is None]\n        kwargs = {r.kwarg: r.op for r in op_rec_column.op_records if r.kwarg is not None}\n        assert all(op is not None for op in args)  # just make sure\n\n        #call_time = None\n        #is_build_time = self.phase == ""building""\n\n        # Build the ops from this input-combination.\n        # Flatten input items.\n        # print(""calling graph_fn = "", op_rec_column.graph_fn)\n        # print(""args = "", args)\n        # print(""kwargs = "", kwargs)\n        if op_rec_column.flatten_ops is not False:\n            flattened_args, flattened_kwargs = op_rec_column.flatten_input_ops(*args, **kwargs)\n            # Split into SingleDataOps?\n            if op_rec_column.split_ops:\n                split_args_and_kwargs = op_rec_column.split_flattened_input_ops(*flattened_args, **flattened_kwargs)\n                # There is some splitting to do. Call graph_fn many times (one for each split).\n                if isinstance(split_args_and_kwargs, FlattenedDataOp):\n                    ops = {}\n                    summary_ops = []\n                    num_return_values = -1\n                    for key, params in split_args_and_kwargs.items():\n                        params_args = params[0]\n                        params_kwargs = params[1]\n                        ret_ops, ret_summary_ops = self._run_in_context(\n                            op_rec_column.graph_fn, op_rec_column.component, *params_args, **params_kwargs\n                        )\n                        summary_ops.extend(ret_summary_ops)\n                        ops[key] = force_tuple(ret_ops)\n\n                        if num_return_values >= 0 and num_return_values != len(ops[key]):\n                            raise RLGraphError(\n                                ""Different split-runs through {} do not return the same number of values!"".\n                                format(op_rec_column.graph_fn.__name__)\n                            )\n                        num_return_values = len(ops[key])\n\n                    # Un-split the results dict into a tuple of `num_return_values` slots.\n                    un_split_ops = []\n                    for i in range(num_return_values):\n                        dict_with_singles = FlattenedDataOp()\n                        for key in split_args_and_kwargs.keys():\n                            dict_with_singles[key] = ops[key][i]\n                        un_split_ops.append(dict_with_singles)\n                    ops = tuple(un_split_ops)\n\n                # No splitting to do: Pass everything as-is.\n                else:\n                    split_args, split_kwargs = split_args_and_kwargs[0], split_args_and_kwargs[1]\n                    ops, summary_ops = self._run_in_context(\n                        op_rec_column.graph_fn, op_rec_column.component, *split_args, **split_kwargs\n                    )\n            else:\n                ops, summary_ops = self._run_in_context(\n                    op_rec_column.graph_fn, op_rec_column.component, *flattened_args, **flattened_kwargs\n                )\n        # Just pass in everything as-is.\n        else:\n            ops, summary_ops = self._run_in_context(\n                op_rec_column.graph_fn, op_rec_column.component, *args, **kwargs\n            )\n        # Make sure everything coming from a computation is always a tuple (for out-Socket indexing).\n        ops = force_tuple(ops)\n\n        # Always un-flatten all return values. Otherwise, we would allow Dict Spaces\n        # with \'/\' keys in them, which is not allowed.\n        ops = op_rec_column.unflatten_output_ops(*ops)\n\n        # Should we create a new out op-rec column?\n        if create_new_out_column is not False:\n            # Assert that we don\'t have an out column already (wouldn\'t make sense).\n            if create_new_out_column is True and op_rec_column.out_graph_fn_column is not None:\n                raise RLGraphError(\n                    ""New DataOpRecordColumnFromGraphFn requested, but one already exists in in-column ""\n                    ""{}!"".format(op_rec_column)\n                )\n            if op_rec_column.out_graph_fn_column is None:\n                out_graph_fn_column = DataOpRecordColumnFromGraphFn(\n                    len(ops), component=op_rec_column.component, graph_fn_name=op_rec_column.graph_fn.__name__,\n                    in_graph_fn_column=op_rec_column, summary_ops=summary_ops\n                )\n            else:\n                out_graph_fn_column = op_rec_column.out_graph_fn_column\n        else:\n            assert op_rec_column.out_graph_fn_column is not None,\\\n                ""ERROR: DataOpRecordColumnFromGraphFn for in-column {} is None!"".format(op_rec_column)\n            out_graph_fn_column = op_rec_column.out_graph_fn_column\n\n        # 0 return values or a single None as 1 return value?\n        if ops == ():\n            if len(out_graph_fn_column.op_records) == 1:\n                ops = (None,)\n\n        # Make sure the number of returned ops matches the number of op-records in the next column.\n        # TODO: instead of backend check, do a build mode check here.\n        # Define-by-run may return Nothing or None which is not an Op.\n        if get_backend() == ""tf"":\n            assert len(ops) == len(out_graph_fn_column.op_records), \\\n                ""ERROR: Number of returned values of graph_fn \'{}/{}\' ({}) does not match the number of op-records "" \\\n                ""({}) reserved for the return values of the method!"".format(\n                    op_rec_column.component.name, op_rec_column.graph_fn.__name__, len(ops),\n                    len(out_graph_fn_column.op_records)\n                )\n            # Replace graph_fn-returned Nones with no_op().\n            ops = tuple([tf.no_op() if o is None else o for o in ops])\n\n        # Determine the Spaces for each out op and then move it into the respective op and Space slot of the\n        # out_graph_fn_column.\n        for i, op in enumerate(ops):\n            space = get_space_from_op(op)\n            # Make sure the receiving op-record is still empty.\n            assert out_graph_fn_column.op_records[i].op is None\n            out_graph_fn_column.op_records[i].op = op\n            out_graph_fn_column.op_records[i].space = space\n            # Assign op-rec property in Space so we have a backref in case the Space causes an error during\n            # sanity checking.\n            if space:  # could be 0\n                space.op_rec_ref = out_graph_fn_column.op_records[i]\n        out_graph_fn_column.summary_ops = summary_ops\n\n        return out_graph_fn_column\n\n    @staticmethod\n    def count_trainable_parameters():\n        """"""\n        Counts the number of trainable parameters (e.g. tf.Variables) to get a rough idea of how complex\n        our Model is.\n\n        Returns:\n            int: The number of trainable parameters in the graph.\n        """"""\n        num_trainable_parameters = 0\n        if get_backend() == ""tf"":\n            for variable in tf.trainable_variables():\n                num_trainable_parameters += get_shape(variable, flat=True)\n\n        return num_trainable_parameters\n\n    @staticmethod\n    def count_ops():\n        """"""\n        Counts the number of all backend-specific ops present in the graph. This includes variables and placeholders.\n\n        Returns:\n            int: The number of backend-specific ops in the graph.\n        """"""\n        if get_backend() == ""tf"":\n            return len(tf.get_default_graph().as_graph_def().node)\n        return 0\n\n    def sanity_check_build(self, still_building=False):\n        """"""\n        Checks whether some of the root component\'s API-method output columns contain ops that are still None.\n        """"""\n        for api_method_rec in self.root_component.api_methods.values():\n            # Ignore `variables` API-method for now.\n            if api_method_rec.name == ""variables"":\n                continue\n            for out_op_column in api_method_rec.out_op_columns:\n                for op_rec in out_op_column.op_records:\n                    if op_rec.op is None:\n                        try:\n                            #draw_sub_meta_graph_from_op_rec(op_rec, self.meta_graph)\n                            self._analyze_none_op(op_rec)\n                        except RLGraphBuildError as e:\n                            if still_building:\n                                print(""Found problem in build process (causing a build-deadlock):"")\n                            raise e  # TODO: do something else with this error?\n\n    def _analyze_none_op(self, op_rec):\n        """"""\n        Args:\n            op_rec (DataOpRecord): The op-rec to analyze for errors (whose `op` property is None).\n\n        Raises:\n            RLGraphError: After the problem has been identified.\n        """"""\n        initial_op_rec = op_rec  # For debugging purposes.\n        # Step via `previous` through the graph backwards.\n        while True:\n            previous_op_rec = op_rec.previous\n            # Hit a graph_fn. Jump to incoming column.\n            if previous_op_rec is None:\n                # We have reached the beginning of the graph with a ""variables:..""-dependent Space, so no op expected\n                # yet.\n                if isinstance(op_rec.space, str) and re.match(r\'^variables:.+\', op_rec.space):\n                    assert False, ""  Needs error message here!""\n\n                else:\n                    assert isinstance(op_rec.column, DataOpRecordColumnFromGraphFn),\\\n                        ""ERROR: If previous op-rec is None, column must be of type `DataOpRecordColumnFromGraphFn` "" \\\n                        ""(but is type={})!"".format(type(op_rec.column).__name__)\n                    # All op-recs going into this graph_fn have actual ops.\n                    # -> We know now that this graph_fn is only not called because the Component is either\n                    # input-incomplete or variable-incomplete.\n                    if op_rec.column.in_graph_fn_column.is_complete():\n                        if op_rec.column.component.input_complete is False:\n                            self._analyze_input_incomplete_component(op_rec.column.component)\n                        else:\n                            assert op_rec.column.component.variable_complete is False and \\\n                                   op_rec.column.in_graph_fn_column.requires_variable_completeness is True, \\\n                                   ""ERROR: Component \'{}\' was expected to be either input-incomplete or "" \\\n                                   ""variable-incomplete!"".format(op_rec.column.component.global_scope)\n                            self._analyze_variable_incomplete_component(op_rec.column.component)\n                    else:\n                        # Take as an example None op the first incoming one that\'s None as well.\n                        empty_in_op_recs = list(or_ for or_ in op_rec.column.in_graph_fn_column.op_records if or_.op is None)\n                        if len(empty_in_op_recs) > 0:\n                            previous_op_rec = empty_in_op_recs[0]\n                        # All op-recs have actual ops -> .\n                        else:\n                            pass  # TODO: complete logic\n            # Continue with new op-record.\n            op_rec = previous_op_rec\n\n    def _analyze_input_incomplete_component(self, component):\n        """"""\n        Analyzes why a component is input-incomplete and what we can further track back from it\n        (e.g. maybe there is another one before that that is also input-incomplete).\n\n        Args:\n            component (Component): The defunct Component to analyze.\n\n        Raises:\n            RLGraphError: After the problem has been identified.\n        """"""\n        self.investigated_input_incomplete_components.add(component)\n        # Find any input-param that has no Space defined.\n        incomplete_input_args = list(name for name, space in component.api_method_inputs.items() if space is None)\n        assert len(incomplete_input_args) > 0,\\\n            ""ERROR: Expected at least one input-arg of \'{}\' to be without Space-definition!"".\\\n            format(component.global_scope)\n        incomplete_arg = incomplete_input_args[0]\n        ## Either we are blocked by ourselves (API-method calling another one of the same Component (internal deadlock)).\n        #internal_deadlock = False\n        # Now loop through all of the Component\'s API-methods and look for all calls using `incomplete_arg`.\n        # If they are all coming from the same Component -> internal deadlock.\n        # If at least one is coming from another component -> follow that one via this very method.\n        # TODO: What if there is external deadlock? Two components\n        calls_using_incomplete_arg = 0\n        components_making_calls_with_incomplete_arg = set()\n        for api_method_name, api_method_rec in component.api_methods.items():\n            # Found a call with `incomplete_arg`.\n            if incomplete_arg in api_method_rec.input_names and len(api_method_rec.in_op_columns) > 0:\n                calls_using_incomplete_arg += len(api_method_rec.in_op_columns)\n                for call_column in api_method_rec.in_op_columns:\n                    for op_rec in call_column.op_records:\n                        assert op_rec.previous is not None\n                        components_making_calls_with_incomplete_arg.add(op_rec.previous.column.component)\n\n        must_be_complete_suggestion = \\\n            ""If the space for this arg is not important in creating variables for this component, try flagging the "" \\\n            ""API-methods that use this arg via the `must_be_complete=False` flag.""\n\n        # What if incomplete_arg was never used in any calls?\n        # Then that\'s the reason, why this component is input-incomplete.\n        if calls_using_incomplete_arg == 0:\n            raise RLGraphBuildError(\n                ""The call argument \'{}\' in Component \'{}\' was never used in any calls to any API-method of this ""\n                ""component! Thus, the component remains input-incomplete. ""\n                ""{}"".format(incomplete_arg, component.global_scope, must_be_complete_suggestion)\n            )\n        # Only this very component uses this call arg -> ""Inner deadlock"".\n        elif len(components_making_calls_with_incomplete_arg) == 1 and \\\n                component in components_making_calls_with_incomplete_arg:\n            raise RLGraphBuildError(\n                ""Component \'{}\' has a circular dependency via API call arg \'{}\'! Only this component ever makes ""\n                ""calls using this arg, so it can never become input-complete. ""\n                ""{}"".format(component.global_scope, incomplete_arg, must_be_complete_suggestion)\n            )\n        # Some other component(s) use this call arg, but they might be input-incomplete themselves.\n        else:\n            for calling_component in components_making_calls_with_incomplete_arg:\n                if calling_component is component:\n                    continue\n                # Assume that the caller is input-incomplete itself.\n                assert calling_component.input_complete is False\n                # Continue investigating why this one is input incomplete.\n\n    def _analyze_variable_incomplete_component(self, component):\n        """"""\n        Analyzes why a component is variable-incomplete (one of its children is not input-complete) and keeps tracking\n        the root cause for this problem.\n\n        Args:\n            component (Component): The defunct Component to analyze.\n\n        Raises:\n            RLGraphError: After the problem has been identified.\n        """"""\n        # Find the sub-component that\'s input-incomplete and further track that one.\n        for sub_component in component.get_all_sub_components(exclude_self=True):\n            if sub_component.input_complete is False:\n                self._analyze_input_incomplete_component(sub_component)\n\n    def get_execution_inputs(self, *api_method_calls):\n        """"""\n        Creates a fetch-dict and a feed-dict for a graph session call.\n\n        Args:\n            api_method_calls (dict): See `rlgraph.graphs.graph_executor` for details.\n\n        Returns:\n            Tuple[list,dict]: Fetch-list, feed-dict with relevant args.\n        """"""\n        fetch_dict = {}\n        feed_dict = {}\n\n        for api_method_call in api_method_calls:\n            if api_method_call is None:\n                continue\n\n            api_method_name = api_method_call\n            params = []\n            return_ops = None\n\n            # Call is defined by a list/tuple of [method], [input params], [return_ops]?\n            if isinstance(api_method_call, (list, tuple)):\n                api_method_name = api_method_call[0] if not callable(api_method_call[0]) else \\\n                    api_method_call[0].__name__\n                # If input is one dict: Check first placeholder for being a dict as well and if so, do a normal 1:1\n                # mapping, otherwise, roll out the input dict as a list.\n                if isinstance(api_method_call[1], dict) and \\\n                        not isinstance(self.api[api_method_name][0][0].op, DataOpDict):\n                    params = [v for k, v in sorted(api_method_call[1].items())]\n                else:\n                    params = force_list(api_method_call[1])\n\n                return_ops = force_list(api_method_call[2]) if len(api_method_call) > 2 and \\\n                                                               api_method_call[2] is not None else None\n            # Allow passing the function directly\n            if callable(api_method_call):\n                api_method_name = api_method_call.__name__\n\n            if api_method_name not in self.api:\n                raise RLGraphError(""No API-method with name \'{}\' found!"".format(api_method_name))\n\n            # API returns a dict.\n            if len(self.api[api_method_name][1]) > 0 and self.api[api_method_name][1][0].kwarg is not None:\n                for op_rec in self.api[api_method_name][1]:\n                    if return_ops is None or op_rec.kwarg in return_ops:\n                        if api_method_name not in fetch_dict:\n                            fetch_dict[api_method_name] = {}\n                        flat_ops = flatten_op(op_rec.op, mapping=lambda o: o.op if isinstance(o, DataOpRecord) else o)\n                        fetch_dict[api_method_name][op_rec.kwarg] = unflatten_op(flat_ops)\n\n                if return_ops is not None:\n                    assert all(op in fetch_dict[api_method_name] for op in return_ops),\\\n                        ""ERROR: Not all wanted return_ops ({}) are returned by API-method `api_method_call`!"".format(\n                        return_ops)\n            # API returns a tuple.\n            else:\n                fetch_dict[api_method_name] = [op_rec.op for i, op_rec in enumerate(self.api[api_method_name][1]) if\n                                               return_ops is None or i in return_ops]\n                if return_ops is not None:\n                    assert len(fetch_dict[api_method_name]) == len(return_ops),\\\n                        ""ERROR: Not all wanted return_ops ({}) are returned by API-method `api_method_call`!"".format(\n                        return_ops)\n\n            for i, param in enumerate(params):\n                if param is None:\n                    assert len(self.api[api_method_name][0]) == i, \\\n                        ""ERROR: More input params given ({}) than expected ({}) for call to \'{}\'!"". \\\n                        format(len(params), len(self.api[api_method_name][0]), api_method_name)\n                    break\n\n                # TODO: What if len(params) < len(self.api[api_method][0])?\n                # Need to handle default API-method params also for the root-component (this one).\n                if len(self.api[api_method_name][0]) <= i:\n                    raise RLGraphError(\n                        ""API-method with name \'{}\' only has {} input parameters! You passed in ""\n                        ""{}."".format(api_method_name, len(self.api[api_method_name][0]), len(params))\n                    )\n\n                placeholder = self.api[api_method_name][0][i].op  # 0=input op-recs; i=ith input op-rec\n                if isinstance(placeholder, ContainerDataOp):\n                    flat_placeholders = flatten_op(placeholder)\n                    for flat_key, value in flatten_op(param).items():\n                        feed_dict[flat_placeholders[flat_key]] = value\n                # Special case: Get the default argument for this arg.\n                # TODO: Support API-method\'s kwargs here as well (mostly useful for test.test).\n                #elif param is None:\n                #    feed_dict[placeholder] = self.root_component.api_methods[api_method_call].default_values[i]\n                else:\n                    feed_dict[placeholder] = param\n\n        return fetch_dict, feed_dict\n\n    def execute_define_by_run_op(self, api_method, params=None):\n        """"""\n        Executes an API method by simply calling the respective function\n        directly with its parameters to trigger an eager call-chain through the graph.\n\n        Args:\n            api_method (str): Name of api-method.\n            params (Optional[list]): Optional arguments.\n\n        Returns:\n            any: Results of executing this api-method.\n        """"""\n        # Reset call profiler.\n        Component.reset_profile()\n        if api_method not in self.api:\n            raise RLGraphError(""No API-method with name \'{}\' found!"".format(api_method))\n\n        if params is not None:\n            if api_method in self.root_component.synthetic_methods:\n                return self.root_component.api_fn_by_name[api_method](self.root_component, *params)\n            else:\n                return self.root_component.api_fn_by_name[api_method](*params)\n        else:\n            if api_method in self.root_component.synthetic_methods:\n                return self.root_component.api_fn_by_name[api_method](self.root_component)\n            else:\n                return self.root_component.api_fn_by_name[api_method]()\n\n    def execute_define_by_run_graph_fn(self, component, graph_fn, options, *args, **kwargs):\n        """"""\n        Executes a graph_fn in define by run mode.\n\n        Args:\n            component (Component): Component this graph_fn is eecuted on.\n            graph_fn (callable): Graph function to execute.\n            options (dict): Execution options.\n        Returns:\n            any: Results of executing this graph-fn.\n        """"""\n        return execute_define_by_run_graph_fn(component, graph_fn, options, *args, **kwargs)\n\n    def build_define_by_run_graph(self, meta_graph, input_spaces, available_devices,\n                                  device_strategy=""default"", default_device=None, device_map=None):\n        """"""\n        Builds a graph for eager or define by run execution. This primarily consists of creating variables through\n        the component hierarchy by pushing the input spaces  through the graph.\n\n          Args:\n            meta_graph (MetaGraph): MetaGraph to build to backend graph.\n            input_spaces (dict): Input spaces to build for.\n            available_devices (list): Devices which can be used to assign parts of the graph\n                during the graph build.\n            device_strategy (Optional[str]): Device strategy.\n            default_device (Optional[str]): Default device identifier.\n            device_map (Optional[Dict]): Dict of Component names mapped to device names to place the Component\'s ops.\n        """"""\n        # Time the build procedure.\n        time_start = time.perf_counter()\n        assert meta_graph.build_status, ""ERROR: Meta graph must be built to build backend graph.""\n        self.root_component = meta_graph.root_component\n        self.graph_call_times = []\n        self.var_call_times = []\n        self.api = meta_graph.api\n        self.num_meta_ops = meta_graph.num_ops\n\n        # Set devices usable for this graph.\n        self.available_devices = available_devices\n        self.device_strategy = device_strategy\n        self.default_device = default_device\n        self.device_map = device_map or {}\n        self.phase = ""building""\n        TraceContext.DEFINE_BY_RUN_CONTEXT = ""building""\n\n        # TODO device strategy in pytorch?\n        # Build full registry of callable methods on root component.\n        for member in inspect.getmembers(self.root_component):\n            name, method = (member[0], member[1])\n\n            # N.b. this means _graph_fns are not directly callable here, just api functions.\n            if name not in self.root_component.api_fn_by_name and name in self.api:\n                self.root_component.api_fn_by_name[name] = method\n\n        # Create the first actual ops based on the input-spaces.\n        # Some ops can only be created later when variable-based-Spaces are known (op_recs_depending_on_variables).\n        self.build_input_space_ops(input_spaces)\n\n        # Collect all components and add those op-recs to the set that are constant.\n        components = self.root_component.get_all_sub_components(exclude_self=False)\n        for component in components:\n            component.graph_builder = self  # point to us.\n            self.op_records_to_process.update(component.constant_op_records)\n            # Check whether the Component is input-complete (and build already if it is).\n            self.build_component_when_input_complete(component)\n\n        op_records_list = self._sort_op_recs(self.op_records_to_process)\n        iterations = self._build(op_records_list)\n\n        # Set execution mode in components to change `call` behaviour to direct function evaluation.\n        self.root_component.propagate_sub_component_properties(properties=dict(execution_mode=""define_by_run""))\n\n        # Call post build logic.\n        self.root_component._post_build(self.root_component)\n\n        time_build = time.perf_counter() - time_start\n        self.logger.info(""Define-by-run computation-graph build completed in {} s ({} iterations)."".\n                         format(time_build, iterations))\n        build_overhead = time_build - sum(self.graph_call_times) - sum(self.var_call_times)\n        TraceContext.DEFINE_BY_RUN_CONTEXT = ""execution""\n        return dict(\n            build_overhead=build_overhead,\n            total_build_time=time_build,\n            op_creation=sum(self.graph_call_times),\n            var_creation=sum(self.var_call_times)\n        )\n\n    def _build(self, op_records_list):\n        """"""\n        Private implementation of the main build loop. For docs, see the respective build\n        methods.\n        """"""\n        loop_counter = 0\n        while len(op_records_list) > 0:\n            # In this iteration, do we still have API-method op-recs (which are part of columns that go into or come\n            # from API-methods).\n            have_api_method_recs = any(\n                isinstance(or_.column, (DataOpRecordColumnIntoAPIMethod, DataOpRecordColumnFromAPIMethod)) for or_ in\n                op_records_list\n            )\n            # Keep track of the highest nesting-level (depth of a component in the parent/child-component-tree) for\n            # a called graph_fn. Graph_fns with a lower nesting level will not be called in the same iteration.\n            # This allows for careful progress through the graph_fn-calls in case API methods of\n            # input/variable-incomplete components are called within these graph_fns (to be avoided at all costs\n            # as it will fail the build).\n            highest_nesting_of_called_graph_fn_column = -1\n\n            # Collect op-recs to process in the next iteration.\n            self.op_records_to_process = set()\n\n            # Set of Components that have been tried last to get input-complete. If build gets stuck, it\'ll be because\n            # of the Components in this set.\n            non_complete_components = set()\n            for op_rec in op_records_list:  # type: DataOpRecord\n                # There are next records:\n                if len(op_rec.next) > 0:\n                    # Push actual op and Space forward one op-rec at a time.\n                    for next_op_rec in self._sort_op_recs(op_rec.next):  # type: DataOpRecord\n                        # Assert that next-record\'s `previous` field points back to op_rec.\n                        assert next_op_rec.previous is op_rec, \\\n                            ""ERROR: Op-rec {} in meta-graph has {} as next, but {}\'s previous field points to {}!"". \\\n                            format(op_rec, next_op_rec, next_op_rec, next_op_rec.previous)\n                        # If not last op in this API-method -> continue.\n                        if next_op_rec.is_terminal_op is False:\n                            assert next_op_rec.op is None or is_constant(next_op_rec.op) or next_op_rec.op is op_rec.op\n                            self.op_records_to_process.add(next_op_rec)\n                        ## Push op and Space into next op-record.\n                        ## With op-instructions?\n                        #if ""key-lookup"" in next_op_rec.op_instructions:\n                        #    lookup_key = next_op_rec.op_instructions[""key-lookup""]\n                        #    if isinstance(lookup_key, str) and (not isinstance(op_rec.op, dict) or lookup_key\n                        #                                        not in op_rec.op):\n                        #        raise RLGraphError(\n                        #            ""op_rec.op ({}) is not a dict or does not contain the lookup key \'{}\'!"". \\\n                        #            format(op_rec.op, lookup_key)\n                        #        )\n                        #    elif isinstance(lookup_key, int) and (not isinstance(op_rec.op, (list, tuple)) or\n                        #                                          lookup_key >= len(op_rec.op)):\n                        #        raise RLGraphError(\n                        #            ""op_rec.op ({}) is not a list/tuple or contains not enough items for lookup ""\n                        #            ""index \'{}\'!"".format(op_rec.op, lookup_key)\n                        #        )\n                        #    next_op_rec.op = op_rec.op[lookup_key]\n                        #    next_op_rec.space = op_rec.space[lookup_key]\n                        ## No instructions -> simply pass on.\n                        #else:\n                        #    next_op_rec.op = op_rec.op\n                        #    next_op_rec.space = op_rec.space\n                        op_rec.connect_to(next_op_rec)\n\n                        # Also push Space into possible API-method record if slot\'s Space is still None.\n                        if isinstance(op_rec.column, DataOpRecordColumnIntoAPIMethod):\n                            param_name = get_call_param_name(op_rec)\n                            component = op_rec.column.api_method_rec.component\n\n                            # Place Space for this input-param name (valid for all input params of same name even of\n                            # different API-method of the same Component).\n                            if component.api_method_inputs[param_name] is None or \\\n                                    component.api_method_inputs[param_name] == ""flex"":\n                                component.api_method_inputs[param_name] = next_op_rec.space\n                            # For non-space agnostic Components: Sanity check, whether Spaces are equivalent.\n                            elif component.space_agnostic is False:\n                                generic_space = check_space_equivalence(\n                                    component.api_method_inputs[param_name], next_op_rec.space\n                                )\n                                # Spaces are not equivalent.\n                                if generic_space is False:\n                                    raise RLGraphError(\n                                        ""ERROR: op-rec \'{}\' going into API \'{}\' has Space \'{}\', but input-param ""\n                                        ""\'{}\' already has Space \'{}\'!"".format(\n                                            next_op_rec, op_rec.column.api_method_rec.api_method_name,\n                                            next_op_rec.space, param_name, component.api_method_inputs[param_name]\n                                        )\n                                    )\n                                # Overwrite both entries with the more generic Space.\n                                next_op_rec.space = component.api_method_inputs[param_name] = generic_space\n\n                        # Did we enter a new Component? If yes, check input-completeness and\n                        # - If op_rec.column is None -> We are at the very beginning of the graph (op_rec.op is a\n                        # placeholder).\n                        next_component = next_op_rec.column.component\n                        if op_rec.column is None or op_rec.column.component is not next_component:\n                            self.build_component_when_input_complete(next_component)\n                            if next_component.input_complete is False:\n                                non_complete_components.add(next_component.global_scope)\n\n                        # Update Space\'s op_rec ref (should always refer to the deepest-into-the-graph op-rec).\n                        if next_op_rec.space:  # space could be 0\n                            next_op_rec.space.op_rec_ref = next_op_rec\n\n                # No next records:\n                # - Op belongs to a column going into a graph_fn.\n                elif isinstance(op_rec.column, DataOpRecordColumnIntoGraphFn):\n                    # Only call the GraphFn iff:\n                    # There are no more DataOpRecordColumnIntoAPIMethod ops in our list: We would like to hold off\n                    # any graph fn calls for as long as possible.\n                    # We don\'t want to run through a graph_fn, then have to call an API-method from within that graph_fn\n                    # and the component of that API-method is not input-/variable-complete yet.\n                    if have_api_method_recs:\n                        # Recycle this op-rec.\n                        self.op_records_to_process.add(op_rec)\n                    # There are other graph_fn columns that have a higher Component nesting_level and are\n                    # actually callable -> Call those first.\n                    elif highest_nesting_of_called_graph_fn_column > op_rec.column.component.nesting_level:\n                        # Recycle this op-rec.\n                        self.op_records_to_process.add(op_rec)\n                    # GraphFn column must be complete AND has not been sent through the graph_fn yet.\n                    elif op_rec.column.is_complete() and op_rec.column.already_sent is False:\n                        do_call = False  # Do the actual graph_fn call?\n\n                        # Only call the graph_fn if the Component is already input-complete.\n                        if op_rec.column.component.variable_complete or \\\n                                (op_rec.column.requires_variable_completeness is False and\n                                 op_rec.column.component.input_complete):\n                            do_call = True\n                        # Component not input-/variable-complete yet. Recycle this op-rec.\n                        else:\n                            self.build_component_when_input_complete(op_rec.column.component)\n                            self.op_records_to_process.add(op_rec)\n                            if op_rec.column.component.input_complete is False:\n                                non_complete_components.add(op_rec.column.component.global_scope)\n                            # Call the graph_fn here right away iff component is ready now.\n                            elif op_rec.column.component.variable_complete or \\\n                                    op_rec.column.requires_variable_completeness is False:\n                                do_call = True\n\n                        if do_call:\n                            # Call the graph_fn with the given column and call-options.\n                            self.run_through_graph_fn_with_device_and_scope(op_rec.column)\n                            # Store all resulting op_recs (returned by the graph_fn) to be processed next.\n                            self.op_records_to_process.update(op_rec.column.out_graph_fn_column.op_records)\n                            highest_nesting_of_called_graph_fn_column = op_rec.column.component.nesting_level\n\n                    # - There are still into-API-method-op-recs that should be handled first.\n                    # - Op column is not complete yet: Discard this one (as others will keep coming in anyway).\n                    # - Op column has already been sent (sibling ops may have arrived in same iteration).\n                # else: - Op belongs to a column coming from a graph_fn or an API-method, but the op is no longer used.\n                # -> Ignore Op.\n\n            # If we are done with the build, check for API-methods\' ops that are dependent on variables\n            # generated during the build and build these now.\n            # TODO is this loop necessary for define by run?\n            if get_backend() == ""tf"":\n                if len(self.op_recs_depending_on_variables) > 0:\n                    op_records_list = list(self.op_recs_depending_on_variables)\n                    self.op_recs_depending_on_variables = set()\n\n                    # Loop through the op_records list and sanity check for ""variables""-dependent Spaces, then get these\n                    # Spaces (iff respective component is input-complete), create the placeholders and keep building.\n                    for op_rec in op_records_list:\n                        space_desc = op_rec.space  # type: str\n                        mo = re.search(r\'^variables:(.+)\', space_desc)\n                        assert mo\n                        component_path = mo.group(1).split(""/"")\n                        component = self.root_component\n                        for level in component_path:\n                            assert level in component.sub_components, \\\n                                ""ERROR: `component_path` (\'{}\') contains non-existent Components!"".format(\n                                    component_path)\n                            component = component.sub_components[level]\n                        if component.variable_complete is True:\n                            var_space = Dict({key: get_space_from_op(value) for key, value in sorted(\n                                component.get_variables(custom_scope_separator=""-"").items()\n                            )})\n                            op_rec.space = var_space\n                            var_space.op_rec_ref = op_rec  # store op-rec in Space for sanity-checking and debugging\n                            placeholder_name = next(iter(op_rec.next)).column.api_method_rec.input_names[op_rec.position]\n                            assert len(op_rec.next) == 1, \\\n                                ""ERROR: root_component API op-rec (\'{}\') expected to have only one `next` op-rec!"". \\\n                                format(placeholder_name)\n                            op_rec.op = self.get_placeholder(\n                                placeholder_name, space=var_space, component=self.root_component\n                            )\n                            self.op_records_to_process.add(op_rec)\n                        else:\n                            self.op_recs_depending_on_variables.add(op_rec)\n\n            # Sanity check, whether we are stuck.\n            new_op_records_list = self._sort_op_recs(self.op_records_to_process)\n            if op_records_list == new_op_records_list:\n                # Probably deadlocked. Do a premature sanity check to report possible problems.\n                if loop_counter > self.max_build_iterations:\n                    self.sanity_check_build(still_building=True)\n                    return\n\n            op_records_list = new_op_records_list\n\n            loop_counter += 1\n        return loop_counter\n\n    @staticmethod\n    def _sort_op_recs(recs):\n        """"""\n        Sorts op-recs according to:\n        - Give API-method calls priority over GraphFn calls (API-method call ops just have to be passed along without\n        worrying about input-/variable-completeness).\n        - Give deeper nested Components priority over shallower nested ones.\n        - Sort by op-rec ID to enforce determinism.\n\n        Note: We sort in reverse order, highest key-values first.\n\n        Args:\n            recs (Set[DataOpRecord]): The DataOpRecords to sort.\n\n        Returns:\n            list: The sorted op-recs.\n        """"""\n        def sorting_func(rec):\n            # Op-rec is a placeholder. Highest priority.\n            if rec.column is None:\n                return DataOpRecord.MAX_ID * 2 + rec.id\n            # API-methods have priority (over GraphFns).\n            elif isinstance(rec.column, DataOpRecordColumnIntoAPIMethod):\n                return DataOpRecord.MAX_ID + rec.id\n            # Deeper nested Components have priority. If same level, use op-rec\'s ID for determinism.\n            return rec.column.component.nesting_level + rec.id / DataOpRecord.MAX_ID\n\n        return sorted(recs, key=sorting_func, reverse=True)\n'"
rlgraph/graphs/graph_executor.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\n\nfrom rlgraph.graphs import MetaGraphBuilder\nfrom rlgraph.utils.input_parsing import parse_saver_spec, parse_execution_spec\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\nfrom rlgraph.utils.specifiable import Specifiable\n\n\nclass GraphExecutor(Specifiable):\n    """"""\n    A GraphExecutor manages local and distributed execution of graphs by encapsulating\n    session management, distributed optimization and communication.\n    """"""\n    def __init__(\n        self,\n        graph_builder,\n        saver_spec=None,\n        execution_spec=None,\n        load_from_file=None\n    ):\n        """"""\n        Abstract graph executor.\n        Args:\n            graph_builder (GraphBuilder): A graph builder which manages the RLGraph metagraph.\n            saver_spec (dict): The saver specification for saving this graph to disk.\n            execution_spec (dict): The specification dict for the execution types (local vs distributed, etc..) and\n                settings (cluster types, etc..).\n            load_from_file (Optional[bool,str]): If not None/False: Loads a previously stored checkpoint of the\n                graph from an existing file. Thereby, supported values are:\n                True: Use the latest checkpoint saved in `self.saver_spec[""directory""]`.\n                str: Use the given path/filename to load from.\n        """"""\n        super(GraphExecutor, self).__init__()\n\n        self.logger = logging.getLogger(__name__)\n\n        self.meta_graph_builder = MetaGraphBuilder()\n        self.graph_builder = graph_builder\n\n        self.saver_spec = parse_saver_spec(saver_spec)\n        self.summary_spec = self.graph_builder.summary_spec\n        self.execution_spec = parse_execution_spec(execution_spec)  # sanitize again (after Agent); one never knows\n\n        # A global training/update counter. Should be increased by 1 each update/learning step.\n        self.global_training_timestep = None\n\n        # Global timestep tracking the interactions with the environment.\n        self.global_timestep = None\n\n        self.logger.info(""Execution spec is: {}"".format(self.execution_spec))\n\n        self.load_from_file = load_from_file\n\n        self.seed = self.execution_spec.get(""seed"")\n\n        # Default single-process execution.\n        self.execution_mode = self.execution_spec.get(""mode"", ""single"")\n\n        # Warning: If this is set to True, no automatic checkpointing or summary writing will be\n        # performed because we will use a simple TensorFlow session instead of a monitored session.\n        self.disable_monitoring = self.execution_spec.get(""disable_monitoring"", False)\n\n        self.distributed_spec = self.execution_spec.get(""distributed_spec"")\n\n        # Number of available GPUs and their names.\n        self.gpus_enabled = None\n        # Whether to fake GPUs in case there are none available (in which case, we place everything on the CPU).\n        self.fake_gpus = None\n\n        self.gpu_names = None\n        self.used_devices = list()\n        self.max_usable_gpus = 0\n        self.num_gpus = 0\n\n        self.device_strategy = None\n        self.default_device = None\n        self.device_map = None\n\n    def build(self, root_components, input_spaces, **kwargs):\n        """"""\n        Sets up the computation graph by:\n        - Starting the Server, if necessary.\n        - Setting up the computation graph object.\n        - Assembling the computation graph defined inside our root-component.\n        - Setting up graph-savers, -summaries, and finalizing the graph.\n\n        Args:\n            root_components (list): List of root components where each root component corresponds to a\n                meta graph to be built.\n            input_spaces (dict): Dict with keys as core\'s API method names and values as tuples of Spaces that\n                should go into these API methods.\n        """"""\n        raise NotImplementedError\n\n    def execute(self, *api_method_calls):\n        """"""\n        Fetches one or more Socket outputs from the graph (given some api_methods) and returns their outputs.\n\n        Args:\n            api_method_calls (Union[str,list,tuple]): A specifier for an API-method call.\n                - str: Call the API-method that has the given name w/o any input args.\n                - tuple len=2: 0=the API-method name to call; 1=the input args to use for the call.\n                - tuple len=3: same as len=2, AND 2=list of returned op slots to pull (e.g. [0]: only pull\n                    the first op).\n\n        Returns:\n            any: The tuple of return values (or a single value) if only one API-method is called.\n                The dictionary of result tuples (or single values) if more than one API-method is called.\n        """"""\n        raise NotImplementedError\n\n    def read_variable_values(self, variables):\n        """"""\n        Read variable values from a graph, e.g. by calling the underlying graph\n        or just returning the variable in imperative modes.\n\n        Args:\n            variables (list): Variable objects to retrieve from the graph.\n\n        Returns:\n            list: Values of the variables provided.\n        """"""\n        pass\n\n    def init_execution(self):\n        """"""\n        Sets up backend-dependent execution, e.g. server for distributed TensorFlow\n        execution.\n        """"""\n        pass  # not mandatory\n\n    def finish_graph_setup(self):\n        """"""\n        Initializes any remaining backend-specific monitoring or session handling.\n        """"""\n        raise NotImplementedError\n\n    def get_available_devices(self):\n        """"""\n        Lists available devices for this model.\n\n        Returns:\n            list: Device identifiers visible to this model.\n        """"""\n        pass\n\n    def load_model(self, checkpoint_directory=None, checkpoint_path=None):\n        """"""\n        Loads model from specified path location using the following semantics:\n\n        If checkpoint directory and checkpoint path are given, attempts to find `checkpoint_path` as relative path from\n        `checkpoint_directory`.\n\n        If a checkpoint directory is given but no path (e.g. because timestep of checkpoint is not known in advance),\n        attempts to fetch latest check-point.\n\n        If no directory is given, attempts to fetch checkpoint from the full absolute path `checkpoint_path\'.\n\n        Args:\n            checkpoint_directory (str): Optional path to directory containing checkpoint(s).\n            checkpoint_path (str): Path to specific model checkpoint.\n        """"""\n        raise NotImplementedError\n\n    def store_model(self, path=None, add_timestep=True):\n        """"""\n        Saves the model to the given path (or to self.saver_directory). Optionally adds the current timestep\n        to the filename to prevent overwriting previous checkpoint files.\n\n        Args:\n            path (str): The directory in which to save (default: self.saver_directory).\n            add_timestep: Appends the current timestep to the checkpoint file if true.\n        """"""\n        raise NotImplementedError\n\n    def get_device_assignments(self, device_names=None):\n        """"""\n        Get assignments for device(s).\n\n        Args:\n            device_names Optional(list):  Device names to filter for. If None, all assignments\n                will be returned.\n\n        Returns:\n            dict: Dict mapping device identifiers (keys) to assigned components (list of component names).\n        """"""\n        pass\n\n    #def get_weights(self):\n    #    """"""\n    #    Returns all weights for computation graph of this graph executor.\n\n    #    Returns:\n    #        any: Weights for this graph..\n    #    """"""\n    #    return self.execute(""variables"")\n\n    #def set_weights(self, weights):\n    #    """"""\n    #    Sets weights of the underlying computation graph..\n\n    #    Args:\n    #        weights (any): Weights and optionally meta data to update depending on the backend.\n\n    #    Raises:\n    #        ValueError if weights do not match graph weights in shapes and types.\n    #    """"""\n    #    self.execute((""sync"", weights))\n\n    def terminate(self):\n        """"""\n        Terminates the GraphExecutor, so it will no longer be usable.\n        Things that need to be cleaned up should be placed into this function, e.g. closing sessions\n        and other open connections.\n        """"""\n        pass  # optional\n\n    def sanity_check_component_tree(self, root_component):\n        """"""\n        Checks the initial component nesting setup (parent and their child components).\n\n        Raises:\n              RLGraphError: If sanity of the init nesting setup could not be confirmed.\n        """"""\n        # Check whether every component (except root-component) has a parent.\n        components = root_component.get_all_sub_components(exclude_self=False)\n\n        self.logger.info(""Components created: {}"".format(len(components)))\n\n        core_found = False\n        for component in components:\n            if component.parent_component is None:\n                if component is not root_component:\n                    raise RLGraphError(\n                        ""ERROR: Component \'{}\' has no parent Component but is not the root-component! Only the ""\n                        ""root-component has a `parent_component` of None."".format(component)\n                    )\n                else:\n                    core_found = True\n            elif component.parent_component is not None and component is root_component:\n                raise RLGraphError(\n                    ""ERROR: Root-Component \'{}\' has a parent Component ({}), but is not allowed to!"".\n                    format(component, component.parent_component)\n                )\n        if core_found is False:\n            raise RLGraphError(""ERROR: Root-component \'{}\' was not found in meta-graph!"".format(root_component))\n'"
rlgraph/graphs/meta_graph.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass MetaGraph(object):\n    """"""\n    Represents a single RLgraph meta graph object.\n    """"""\n    def __init__(self, root_component, api, num_ops, build_status=False):\n        self.root_component = root_component\n        self.api = api\n        self.num_ops = num_ops\n        self._built = build_status\n\n    def set_to_built(self):\n        assert not self._built, ""ERROR: Cannot set graph to built if already built.""\n        self._built = True\n\n    @property\n    def build_status(self):\n        return self._built\n\n'"
rlgraph/graphs/meta_graph_builder.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport time\n\nfrom rlgraph.graphs import MetaGraph\nfrom rlgraph.spaces import Space\nfrom rlgraph.utils import force_list\nfrom rlgraph.utils.op_records import DataOpRecord\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\nfrom rlgraph.utils.specifiable import Specifiable\n\n\nclass MetaGraphBuilder(Specifiable):\n    """"""\n    A meta graph builder takes a connected component graph and generates its\n    API by building the meta graph.\n    """"""\n    def __init__(self):\n        super(MetaGraphBuilder, self).__init__()\n        self.logger = logging.getLogger(__name__)\n\n    def build(self, root_component, input_spaces=None):\n        """"""\n        Builds the meta-graph by constructing op-record columns going into and coming out of all API-methods\n        and graph_fns.\n\n        Args:\n            root_component (Component): Root component of the meta graph to build.\n            input_spaces (Optional[Space]): Input spaces for all (exposed) API methods of the root-component.\n        """"""\n\n        # Time the meta-graph build:\n        DataOpRecord.reset()\n        time_start = time.perf_counter()\n        api = {}\n\n        # Sanity check input_spaces dict.\n        if input_spaces is not None:\n            for input_param_name in input_spaces.keys():\n                if input_param_name not in root_component.api_method_inputs:\n                    raise RLGraphError(\n                        ""ERROR: `input_spaces` contains an input-parameter name (\'{}\') that\'s not defined in any of ""\n                        ""the root-component\'s (\'{}\') API-methods, whose args are \'{}\'!"".format(\n                            input_param_name, root_component.name, root_component.api_method_inputs\n                        )\n                    )\n        else:\n            input_spaces = {}\n\n        # Call all API methods of the core once and thereby, create empty in-op columns that serve as placeholders\n        # and bi-directional links between ops (for the build time).\n        for api_method_name, api_method_rec in root_component.api_methods.items():\n            self.logger.debug(""Building meta-graph of API-method \'{}\'."".format(api_method_name))\n\n            # Create the loose list of in-op-records depending on signature and input-spaces given.\n            # If an arg has a default value, its input-space does not have to be provided.\n            in_ops_records = []\n            use_named = False\n            for i, param_name in enumerate(api_method_rec.input_names):\n                # Arg has a default of None (flex). If in input_spaces, arg will be provided.\n                if root_component.api_method_inputs[param_name] == ""flex"":\n                    if param_name in input_spaces:\n                        in_ops_records.append(\n                            DataOpRecord(position=i, kwarg=param_name if use_named else None, placeholder=param_name)\n                        )\n                    else:\n                        use_named = True\n                # Already defined (per default arg value (e.g. bool)).\n                elif isinstance(root_component.api_method_inputs[param_name], Space):\n                    if param_name in input_spaces:\n                        in_ops_records.append(\n                            DataOpRecord(position=i, kwarg=param_name if use_named else None, placeholder=param_name)\n                        )\n                    else:\n                        use_named = True\n                # No default values -> Must be provided in `input_spaces`.\n                else:\n                    # A var-positional param.\n                    if root_component.api_method_inputs[param_name] == ""*flex"":\n                        assert use_named is False\n                        if param_name in input_spaces:\n                            for j in range(len(force_list(input_spaces[param_name]))):\n                                in_ops_records.append(\n                                    DataOpRecord(position=i + j, placeholder=param_name + ""[{}]"".format(j))\n                                )\n                    # A keyword param.\n                    elif root_component.api_method_inputs[param_name] == ""**flex"":\n                        if param_name in input_spaces:\n                            assert use_named is False\n                            for key in sorted(input_spaces[param_name].keys()):\n                                in_ops_records.append(\n                                    DataOpRecord(kwarg=key, placeholder=param_name + ""[{}]"".format(key))\n                                )\n                        use_named = True\n                    else:\n                        # TODO: If space not provided in input_spaces -> Try to call this API method later (maybe another API-method).\n                        assert param_name in input_spaces, \\\n                            ""ERROR: arg-name \'{}\' not defined in input_spaces for root component \'{}\'!"".format(\n                                param_name, root_component.global_scope\n                            )\n                        in_ops_records.append(DataOpRecord(\n                            position=i, kwarg=param_name if use_named else None, placeholder=param_name\n                        ))\n\n            # Do the actual core API-method call (thereby assembling the meta-graph).\n            args = [op_rec for op_rec in in_ops_records if op_rec.kwarg is None]\n            kwargs = {op_rec.kwarg: op_rec for op_rec in in_ops_records if op_rec.kwarg is not None}\n            getattr(api_method_rec.component, api_method_name)(*args, **kwargs)\n\n            # Register core\'s interface.\n            api[api_method_name] = (in_ops_records, api_method_rec.out_op_columns[-1].op_records)\n\n            # Tag very last out-op-records with is_terminal_op=True, so we know in the build process that we are done.\n            for op_rec in api_method_rec.out_op_columns[-1].op_records:\n                op_rec.is_terminal_op = True\n\n        time_build = time.perf_counter() - time_start\n        self.logger.info(""Meta-graph build completed in {} s."".format(time_build))\n\n        # Get some stats on the graph and report.\n        num_meta_ops = DataOpRecord._ID + 1\n        self.logger.info(""Meta-graph op-records generated: {}"".format(num_meta_ops))\n\n        return MetaGraph(root_component=root_component, api=api, num_ops=num_meta_ops, build_status=True)\n'"
rlgraph/graphs/pytorch_executor.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nimport numpy as np\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.graphs import GraphExecutor\nfrom rlgraph.utils import util\nfrom rlgraph.utils.define_by_run_ops import define_by_run_flatten, define_by_run_unflatten\nfrom rlgraph.utils.util import force_torch_tensors\n\nif get_backend() == ""pytorch"":\n    import torch\n\n\nclass PyTorchExecutor(GraphExecutor):\n    """"""\n    Manages execution for component graphs using define-by-run semantics.\n    """"""\n    def __init__(self, **kwargs):\n        super(PyTorchExecutor, self).__init__(**kwargs)\n\n        self.global_training_timestep = 0\n\n        self.cuda_enabled = torch.cuda.is_available()\n\n        # In PyTorch, tensors are default created on the CPU unless assigned to a visible CUDA device,\n        # e.g. via x = tensor([0, 0], device=""cuda:0"") for the first GPU.\n        self.available_devices = os.environ.get(""CUDA_VISIBLE_DEVICES"")\n        # TODO handle cuda tensors\n\n        self.default_torch_tensor_type = self.execution_spec.get(""dtype"", ""torch.FloatTensor"")\n        if self.default_torch_tensor_type is not None:\n            torch.set_default_tensor_type(self.default_torch_tensor_type)\n\n        self.torch_num_threads = self.execution_spec.get(""torch_num_threads"", 1)\n        self.omp_num_threads = self.execution_spec.get(""OMP_NUM_THREADS"", 1)\n\n        # Squeeze result dims, often necessary in tests.\n        self.remove_batch_dims = True\n\n    def build(self, root_components, input_spaces, **kwargs):\n        start = time.perf_counter()\n        self.init_execution()\n\n        meta_build_times = []\n        build_times = []\n        for component in root_components:\n            start = time.perf_counter()\n            meta_graph = self.meta_graph_builder.build(component, input_spaces)\n            meta_build_times.append(time.perf_counter() - start)\n\n            build_time = self.graph_builder.build_define_by_run_graph(\n                meta_graph=meta_graph, input_spaces=input_spaces, available_devices=self.available_devices\n            )\n            build_times.append(build_time)\n\n        return dict(\n            total_build_time=time.perf_counter() - start,\n            meta_graph_build_times=meta_build_times,\n            build_times=build_times,\n        )\n\n    def execute(self, *api_method_calls):\n        # Have to call each method separately.\n        ret = []\n        for api_method in api_method_calls:\n            if api_method is None:\n                continue\n            elif isinstance(api_method, (list, tuple)):\n                # Which ops are supposed to be returned?\n                op_or_indices_to_return = api_method[2] if len(api_method) > 2 else None\n                params = util.force_list(api_method[1])\n                api_method = api_method[0]\n                tensor_params = force_torch_tensors(params=params)\n\n                api_ret = self.graph_builder.execute_define_by_run_op(api_method, tensor_params)\n                is_dict_result = isinstance(api_ret, dict)\n                if not isinstance(api_ret, list) and not isinstance(api_ret, tuple):\n                    api_ret = [api_ret]\n                to_return = []\n                if op_or_indices_to_return is not None:\n                    # Op indices can be integers into a result list or strings into a result dict.\n                    if is_dict_result:\n                        if isinstance(op_or_indices_to_return, str):\n                            op_or_indices_to_return = [op_or_indices_to_return]\n                        result_dict = {}\n                        for key in op_or_indices_to_return:\n                                result_dict[key] = api_ret[0][key]\n                        to_return.append(result_dict)\n                    else:\n                        # Build return ops in correct order.\n                        # TODO clarify op indices order vs tensorflow.\n                        for i in sorted(op_or_indices_to_return):\n                            op_result = api_ret[i]\n                            if isinstance(op_result, torch.Tensor) and op_result.requires_grad is True:\n                                op_result = op_result.detach()\n                            to_return.append(op_result)\n\n                else:\n                    # Just return everything in the order it was returned by the API method.\n                    if api_ret is not None:\n                        for op_result in api_ret:\n                            if isinstance(op_result, torch.Tensor) and op_result.requires_grad is True:\n                                op_result = op_result.detach()\n                            to_return.append(op_result)\n\n                # Clean and return.\n                self.clean_results(ret, to_return)\n            else:\n                # Api method is string without args:\n                to_return = []\n                api_ret = self.graph_builder.execute_define_by_run_op(api_method)\n                if api_ret is None:\n                    continue\n                if not isinstance(api_ret, list) and not isinstance(api_ret, tuple):\n                    api_ret = [api_ret]\n                for op_result in api_ret:\n                    if isinstance(op_result, torch.Tensor) and op_result.requires_grad is True:\n                        op_result = op_result.detach()\n                    to_return.append(op_result)\n\n                # Clean and return.\n                self.clean_results(ret, to_return)\n\n        # Unwrap if len 1.\n        ret = ret[0] if len(ret) == 1 else ret\n        return ret\n\n    def clean_results(self, ret, to_return):\n        for result in to_return:\n            if isinstance(result, dict):\n                cleaned_dict = {k: v for k, v in result.items() if v is not None}\n                cleaned_dict = self.clean_dict(cleaned_dict)\n                ret.append(cleaned_dict)\n            elif self.remove_batch_dims and isinstance(result, np.ndarray):\n                ret.append(np.array(np.squeeze(result)))\n            elif hasattr(result, ""numpy""):\n                ret.append(np.array(result.numpy()))\n            else:\n                ret.append(result)\n\n    @staticmethod\n    def clean_dict(tensor_dict):\n        """"""\n        Detach tensor values in nested dict.\n        Args:\n            tensor_dict (dict): Dict containing torch tensor.\n\n        Returns:\n            dict: Dict containing numpy arrays.\n        """"""\n        # Un-nest.\n        param = define_by_run_flatten(tensor_dict)\n        ret = {}\n\n        # Detach tensor values.\n        for key, value in param.items():\n            if isinstance(value, torch.Tensor):\n                ret[key] = value.detach().numpy()\n\n        # Pack again.\n        return define_by_run_unflatten(ret)\n\n    def read_variable_values(self, variables):\n        # For test compatibility.\n        if isinstance(variables, dict):\n            ret = {}\n            for name, var in variables.items():\n                ret[name] = Component.read_variable(var)\n            return ret\n        elif isinstance(variables, list):\n            return [Component.read_variable(var) for var in variables]\n        else:\n            # Attempt to read as single var.\n            return Component.read_variable(variables)\n\n    def init_execution(self): \\\n        # TODO Import guards here are annoying but otherwise breaks if torch is not installed.\n        if get_backend() == ""torch"":\n            torch.set_num_threads(self.torch_num_threads)\n            os.environ[""OMP_NUM_THREADS""] = str(self.omp_num_threads)\n\n    def finish_graph_setup(self):\n        # Nothing to do here for PyTorch.\n        pass\n\n    def get_available_devices(self):\n        return self.available_devices\n\n    def load_model(self, path=None):\n        pass\n\n    def store_model(self, path=None, add_timestep=True):\n        pass\n\n    def get_device_assignments(self, device_names=None):\n        pass\n\n    def terminate(self):\n        pass\n'"
rlgraph/graphs/tensorflow_executor.py,45,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport time\n\nfrom rlgraph import get_backend, get_distributed_backend\nimport rlgraph.utils as util\nfrom rlgraph.components.common.multi_gpu_synchronizer import MultiGpuSynchronizer\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\nfrom rlgraph.graphs.graph_executor import GraphExecutor\nfrom rlgraph.utils.util import force_list\nfrom rlgraph.utils.op_records import gather_summaries\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n    from tensorflow.python.client import device_lib\n    from rlgraph.utils.specifiable_server import SpecifiableServer, SpecifiableServerHook\n    from tensorflow.python.client import timeline\n\n\nclass TensorFlowExecutor(GraphExecutor):\n    """"""\n    A TensorFlow executioner manages execution via TensorFlow sessions.\n\n    The following execution strategies are available:\n\n    - \'default\': Assign to CPU or provided default device if no assignment given,\n        otherwise consider user device assignments.\n    - \'custom\': Completely user defined device strategy, graph executor just executes calls\n    - \'multi_gpu_sync\': Parallelizes updates across multiple GPUs by averaging gradients.\n    """"""\n    def __init__(self, **kwargs):\n        super(TensorFlowExecutor, self).__init__(**kwargs)\n        self.session_config = self.execution_spec[""session_config""]\n\n        # The tf.Graph object to be run in a tf session.\n        self.graph = None\n        # Saver.\n        self.saver = None\n        self.saver_directory = None\n\n        # tf.Scaffold.\n        self.scaffold = None\n        # Ops used by the scaffold to initialize variables and check variables for initialization.\n        self.init_op = None\n        self.local_init_op = None\n        self.ready_op = None\n        self.ready_for_local_init_op = None\n\n        # # The tf.Server object (if any).\n        self.server = None\n\n        # Summary settings.\n        self.summary_writer = None\n\n        # Merged summaries per API method\n        self.summary_ops = dict()\n\n        # The session for the computation graph.\n        self.session = None\n        self.monitored_session = None\n\n        # The optimizer is a somewhat privileged graph component because it must manage\n        # devices depending on the device strategy and we hence keep an instance here to be able\n        # to request special device init ops.\n        self.optimizers = None\n\n        self.graph_default_context = None\n        self.local_device_protos = device_lib.list_local_devices()\n\n        # Just fetch CPUs. GPUs will be added when parsing the GPU configuration.\n        self.available_devices = [x.name for x in self.local_device_protos if x.device_type == \'CPU\']\n\n        # Local session config which needs to be updated with device options during setup.\n        self.tf_session_type = self.session_config.pop(""type"", ""monitored-training-session"")\n        self.tf_session_auto_start = self.session_config.pop(""auto_start"", True)\n        self.tf_session_config = tf.ConfigProto(**self.session_config)\n        self.tf_session_options = None\n\n        self.run_metadata = None\n\n        # Tf Profiler config.\n        self.profiling_enabled = self.execution_spec[""enable_profiler""]\n        if self.profiling_enabled is True:\n            self.profiler = None\n            self.profile_step = 0\n            self.profiling_frequency = self.execution_spec[""profiler_frequency""]\n            self.run_metadata = tf.RunMetadata()\n            if not self.disable_monitoring:\n                self.tf_session_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\n        self.timeline_enabled = self.execution_spec[""enable_timeline""]\n        if self.timeline_enabled is True:\n            if self.run_metadata is None:\n                self.run_metadata = tf.RunMetadata()\n            self.timeline_step = 0\n            self.timeline_frequency = self.execution_spec[""timeline_frequency""]\n            if not self.disable_monitoring:\n                self.tf_session_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\n        self.init_device_strategy()\n\n        # # Initialize distributed backend.\n        # distributed_backend_ = self.execution_spec.get(""distributed_backend"", ""distributed_tf"")\n        #\n        # self.logger.info(""Updating global distributed backend setting with backend {}"".format(distributed_backend_))\n        # set_distributed_backend(distributed_backend_)\n\n    def init_device_strategy(self):\n        """"""\n        Initializes default device and loads available devices.\n        """"""\n        self.device_strategy = self.execution_spec[""device_strategy""]\n        # Configures available GPUs.\n        self.init_gpus()\n\n        if self.device_strategy == ""default"":\n            if self.execution_spec[""device_map""] is not None:\n                self.logger.warning(\n                    ""`device_map` given for device-strategy=`default`. Map will be ignored. Use ""\n                    ""device-strategy=`custom` together with a `device_map`.""\n                )\n            self.logger.info(""Initializing graph executor with default device strategy. ""\n                             ""Backend will assign all visible devices."")\n            self.logger.info(""GPUs enabled: {}. Usable GPUs: {}"".format(self.gpus_enabled, self.gpu_names))\n        elif self.device_strategy == \'multi_gpu_sync\':\n            assert self.gpus_enabled, ""ERROR: device_strategy is \'multi_gpu_sync\' but GPUs are not enabled. Please"" \\\n                                      ""check your gpu_spec and set gpus_enabled to True.""\n            self.default_device = self.execution_spec.get(\n                ""default_device"", [x.name for x in self.local_device_protos if x.device_type == \'CPU\'][0]\n            )\n            self.logger.info(""Initializing graph executor with synchronized multi-gpu device strategy. ""\n                             ""Default device: {}. Available gpus are: {}."".format(self.default_device, self.gpu_names))\n        elif self.device_strategy == ""custom"":\n            # Default device is user provided device or first CPU.\n            default_device = self.execution_spec.get(""default_device"", None)\n            if default_device is None:\n                self.default_device = [x.name for x in self.local_device_protos if x.device_type == \'CPU\'][0]\n            else:\n                self.default_device = default_device\n                # Sanity check, whether given default device exists.\n                # if self.default_device not in self.available_devices:\n                #    raise RLGraphError(""Provided `default_device` (\'{}\') is not in `available_devices` ({})"".\n                #                       format(self.default_device, self.available_devices))\n            self.device_map = {}\n            # Clean up device map so it only contains devices that are actually available (otherwise,\n            # use the default device).\n            for component_name, device in self.execution_spec[""device_map""].items():\n                if device in self.available_devices:\n                    self.device_map[component_name] = device\n            self.logger.info(""Initializing graph executor with custom device strategy (default device: {})."".\n                             format(self.default_device))\n        else:\n            raise RLGraphError(""Invalid device_strategy (\'{}\') for TensorFlowExecutor!"".format(self.device_strategy))\n\n    def build(self, root_components, input_spaces, optimizer=None, build_options=None, batch_size=32):\n        # Use perf_counter for short tasks.\n        start = time.perf_counter()\n\n        # Check graph setup and construct the static graph object.\n        self.init_execution()\n        self.setup_graph()\n\n        # 1. Build phase: Meta graph construction -> All of the root_component\'s API methods are being called once,\n        # thereby calling other API-methods (of sub-Components). These API-method calls then build the meta-graph\n        # (generating empty op-record columns around API methods and graph_fns).\n        # TODO make compatible for multiple roots in graph builder.\n        meta_build_times = []\n        build_times = []\n\n        for component in root_components:\n            # Sanity-check the component tree (from root all the way down).\n            self.sanity_check_component_tree(root_component=component)\n\n            self._build_device_strategy(component, optimizer, batch_size=batch_size, extra_build_args=build_options)\n            start = time.perf_counter()\n            meta_graph = self.meta_graph_builder.build(component, input_spaces)\n            meta_build_times.append(time.perf_counter() - start)\n\n            # 2. Build phase: Backend compilation, build actual TensorFlow graph from meta graph.\n            # -> Inputs/Operations/variables\n            build_time = self.graph_builder.build_graph_with_options(\n                meta_graph=meta_graph, input_spaces=input_spaces, available_devices=self.available_devices,\n                device_strategy=self.device_strategy, default_device=self.default_device, device_map=self.device_map,\n                build_options=build_options\n            )\n\n            # Build time is a dict containing the cost of different parts of the build.\n            build_times.append(build_time)\n\n            # Check device assignments for inconsistencies or unused devices.\n            self._sanity_check_devices()\n\n            # Set up any remaining session or monitoring configurations.\n            self.finish_graph_setup()\n\n        return dict(\n            total_build_time=time.perf_counter() - start,\n            meta_graph_build_times=meta_build_times,\n            build_times=build_times,\n        )\n\n    def execute(self, *api_method_calls):\n        # Fetch inputs for the different API-methods.\n        fetch_dict, feed_dict = self.graph_builder.get_execution_inputs(*api_method_calls)\n        for api_name in fetch_dict.keys():\n            if api_name in self.summary_ops:\n                fetch_dict[api_name].append(self.summary_ops[api_name])\n\n        fetch_dict[""__GLOBAL_TRAINING_TIMESTEP""] = self.global_training_timestep\n        ret = self.monitored_session.run(\n            fetch_dict, feed_dict=feed_dict, options=self.tf_session_options, run_metadata=self.run_metadata\n        )\n        global_training_timestep_value = ret[""__GLOBAL_TRAINING_TIMESTEP""]\n        del ret[""__GLOBAL_TRAINING_TIMESTEP""]\n\n        for api_name in fetch_dict.keys():\n            if api_name in self.summary_ops:\n                assert len(ret[api_name]) > 1, ""Expected multiple values, but {} found"".format(len(fetch_dict[api_name]))\n                summary = ret[api_name].pop()\n                # Assuming that all API methods are on the training timesteps.\n                self.summary_writer.add_summary(summary, global_training_timestep_value)\n\n        if self.profiling_enabled:\n            self.update_profiler_if_necessary()\n\n        if self.timeline_enabled:\n            self.update_timeline_if_necessary()\n\n        # Return single values instead of lists of 1 item, but keep inner dicts as-are.\n        ret = {key: (value[0] if len(ret[key]) == 1 and not isinstance(ret[key], dict) else tuple(value)\n               if not isinstance(value, dict) else value) for key, value in ret.items()}\n\n        # If only one key in ret, remove it.\n        if len(api_method_calls) == 1:\n            ret = ret[next(iter(ret))]\n\n        return ret\n\n    def update_profiler_if_necessary(self):\n        """"""\n        Updates profiler according to specification.\n        """"""\n        if self.profile_step % self.profiling_frequency == 0:\n            self.profiler.add_step(self.profile_step, self.run_metadata)\n            self.profiler.profile_operations(\n                options=tf.profiler.ProfileOptionBuilder(\n                    options=tf.profiler.ProfileOptionBuilder.time_and_memory()).with_node_names().build()\n            )\n        self.profile_step += 1\n\n    def update_timeline_if_necessary(self):\n        """"""\n        Writes a timeline json file according to specification.\n        """"""\n        if self.timeline_step % self.timeline_frequency == 0:\n            fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n            chrome_trace = fetched_timeline.generate_chrome_trace_format()\n            with open(""timeline_{:02d}.json"".format(self.timeline_step), ""w"") as f:\n                f.write(chrome_trace)\n        self.timeline_step += 1\n\n    def read_variable_values(self, variables):\n        """"""\n        Fetches the given variables from the graph and returns their current values.\n        The returned structure corresponds to the data type and structure of `variables`\n        (e.g. if a dict with variables as values comes in, a dict with the same keys and current values as values\n        is returned).\n\n        Args:\n            variables (any): Any structure that contains variables.\n\n        Returns:\n            any: Values of the given variables in the exact same structure as `variables`.\n        """"""\n        self.logger.debug(\'Fetching values of variables {} from graph.\'.format(variables))\n        return self.monitored_session.run(variables, feed_dict=dict())\n\n    def init_execution(self):\n        """"""\n        Creates and sets up the distributed backend.\n        Also creates the global time step variable.\n        """"""\n        if self.execution_mode == ""distributed"":\n            if get_distributed_backend() == ""distributed_tf"":\n                self.setup_distributed_tf()\n            elif get_distributed_backend() == ""horovod"":\n                self.setup_horovod_execution()\n\n    def setup_distributed_tf(self):\n        """"""\n        Sets up distributed TensorFlow.\n        """"""\n        self.logger.info(""Setting up distributed TensorFlow execution mode."")\n        # Create a local server.\n        if self.distributed_spec[""cluster_spec""] is None:\n            self.server = tf.train.Server.create_local_server()\n        # Create an actual distributed Server.\n        else:\n            self.server = tf.train.Server(\n                server_or_cluster_def=self.distributed_spec[""cluster_spec""],\n                job_name=self.distributed_spec[""job""],\n                task_index=self.distributed_spec[""task_index""],\n                protocol=self.distributed_spec[""protocol""],\n                start=True\n            )\n\n            if self.distributed_spec[""job""] == ""ps"":\n                # Just join and be done.\n                self.logger.info(""Job is parameter server, joining and waiting."")\n                self.server.join()\n                quit()\n\n    def setup_horovod_execution(self):\n        """"""\n        Sets up Horovod.\n        """"""\n        # Check again to avoid import if unset which will crash if horovod is not installed.\n        if get_distributed_backend() == ""horovod"":\n            import horovod.tensorflow as hvd\n            self.logger.info(""Setting up Horovod execution."")\n            hvd.init()\n            config = tf.ConfigProto()\n            config.gpu_options.visible_device_list = str(hvd.local_rank())\n        \n    def get_available_devices(self):\n        return self.available_devices\n\n    def get_device_assignments(self, device_names=None):\n        if device_names is None:\n            return self.graph_builder.device_component_assignments\n        else:\n            assignments = dict()\n            for device in self.graph_builder.device_component_assignments:\n                if device in device_names:\n                    assignments[device] = self.graph_builder.device_component_assignments[device]\n            return assignments\n\n    def setup_graph(self):\n        """"""\n        Generates the tf-Graph object and enters its scope as default graph.\n        Also creates the global time step variable.\n        """"""\n        self.graph = tf.Graph()\n        self.graph_default_context = self.graph.as_default()\n        self.graph_default_context.__enter__()\n\n        # Create global training (update) timestep. Gets increased once per update.\n        # Do not include this in GLOBAL_STEP collection as only one variable (`global_timestep`) should be in there.\n        self.global_training_timestep = tf.get_variable(\n            name=""global-training-timestep"", dtype=util.convert_dtype(""int""), trainable=False, initializer=0,\n            collections=[""global-training-timestep""]\n        )\n        # Create global (env-stepping) timestep. Gets increased once per environment step.\n        # For vector-envs, gets increased each action by the number of parallel environments.\n        self.global_timestep = tf.get_variable(\n            name=""global-timestep"", dtype=util.convert_dtype(""int""), trainable=False, initializer=0,\n            collections=[""global-timestep"", tf.GraphKeys.GLOBAL_STEP]\n        )\n\n        # Set the random seed graph-wide.\n        if self.seed is not None:\n            self.logger.info(""Initializing TensorFlow graph with seed {}"".format(self.seed))\n            tf.set_random_seed(self.seed)\n\n    def finish_graph_setup(self):\n        # After the graph is built -> Setup saver, summaries, etc..\n        hooks = []  # Will be appended to in the following functions.\n        self.setup_saver(hooks)\n        self.setup_summaries(hooks)\n        self.setup_scaffold()\n        self.setup_specifiable_servers(hooks)\n\n        # Finalize our graph, create and enter the session.\n        if self.tf_session_auto_start is True:\n            self.setup_session(hooks)\n\n    def setup_saver(self, hooks):\n        """"""\n        Creates the tf.train.Saver object and stores it in self.saver.\n\n        Args:\n            hooks (list): List of hooks to use for Saver and Summarizer in Session. Should be appended to.\n        """"""\n        self.saver = tf.train.Saver(\n           var_list=list(self.graph_builder.root_component.variable_registry.values()),\n           reshape=False,\n           sharded=False,\n           max_to_keep=self.saver_spec.get(""max_checkpoints"", 1) if self.saver_spec else None,\n           keep_checkpoint_every_n_hours=10000.0,\n           name=None,\n           restore_sequentially=False,\n           saver_def=None,\n           builder=None,\n           defer_build=False,\n           allow_empty=True,\n           write_version=tf.train.SaverDef.V2,\n           pad_step_number=False,\n           save_relative_paths=True,\n           filename=None\n        )\n\n        # Add saver hook to session if saver spec was provided.\n        if self.saver_spec is not None and (self.execution_mode == ""single""\n                                           or self.distributed_spec[""task_index""] == 0):\n           self.saver_directory = self.saver_spec[""directory""]\n           saver_hook = tf.train.CheckpointSaverHook(\n               checkpoint_dir=self.saver_directory,\n               # Either save_secs or save_steps must be set.\n               save_secs=self.saver_spec[""save_secs""],  # TODO: open question: how to handle settings?\n               save_steps=self.saver_spec[""save_steps""],\n               saver=self.saver,\n               checkpoint_basename=self.saver_spec[""checkpoint_basename""],  # TODO: open question: how to handle settings?\n               scaffold=None,  # None since not created yet.\n               listeners=None\n           )\n           hooks.append(saver_hook)\n\n    def setup_summaries(self, hooks):\n        """"""\n        Sets up tf.summary ops generated during the build of the graph inside the different Components.\n\n        Args:\n            hooks (list): List of hooks to use for Saver and Summarizer in Session. Should be appended to.\n        """"""\n        self.summary_ops = dict()\n\n        for name, method in self.graph_builder.root_component.api_methods.items():\n            _, op_recs_to_fetch = self.graph_builder.api[name]\n            summaries = gather_summaries(op_recs_to_fetch)\n            if len(summaries) > 0:\n                self.logger.info(f""Summaries for {name}: {len(summaries)}"")\n                summary_op = tf.summary.merge(inputs=summaries)\n                self.summary_ops[name] = summary_op\n\n        # Create our tf summary writer object.\n        self.summary_writer = tf.summary.FileWriter(\n            logdir=self.summary_spec[""directory""],\n            graph=self.graph,\n            max_queue=10,\n            flush_secs=120,\n            filename_suffix=None\n        )\n\n        """"""\n        # Creates a single summary op to be used by the session to write the summary files.\n        summary_list = list(self.graph_builder.root_component.summaries.values())\n        if len(summary_list) > 0:\n            self.summary_op = tf.summary.merge(inputs=summary_list)\n            # Create an update saver hook for our summaries.\n            summary_saver_hook = tf.train.SummarySaverHook(\n                save_steps=self.summary_spec[""save_steps""],  # Either one or the other has to be set.\n                save_secs=self.summary_spec[""save_secs""],\n                output_dir=None,  # None since given via \'summary_writer\' argument.\n                summary_writer=self.summary_writer,\n                scaffold=None,  # None since summary_op given directly here.\n                summary_op=self.summary_op\n            )\n            # ... and append it to our list of hooks to use in the session.\n            hooks.append(summary_saver_hook)\n        """"""\n\n    def setup_scaffold(self):\n        """"""\n        Creates a tf.train.Scaffold object to be used by the session to initialize variables and to save models\n        and summaries.\n        Assigns the scaffold object to `self.scaffold`.\n        """"""\n        # Determine init_op and ready_op.\n        var_list = list(self.graph_builder.root_component.variable_registry.values())\n        var_list.append(self.global_training_timestep)\n        var_list.append(self.global_timestep)\n\n        # We can not fetch optimizer vars.\n        # TODO let graph builder do this\n        if self.optimizers is not None:\n            for optimizer in self.optimizers:\n                var_list.extend(optimizer.get_optimizer_variables())\n\n        if self.execution_mode == ""single"":\n            self.init_op = tf.variables_initializer(var_list=var_list)\n            self.ready_op = tf.report_uninitialized_variables(var_list=var_list)\n        else:\n            assert self.execution_mode == ""distributed"",\\\n                ""ERROR: execution_mode can only be \'single\' or \'distributed\'! Is \'{}\'."".format(self.execution_mode)\n            local_job_and_task = ""/job:{}/task:{}/"".format(self.execution_spec[""distributed_spec""][""job""],\n                                                          self.execution_spec[""distributed_spec""][""task_index""])\n            var_list_local = [var for var in var_list if not var.device or local_job_and_task in var.device]\n            var_list_remote = [var for var in var_list if var.device and local_job_and_task not in var.device]\n            self.init_op = tf.variables_initializer(var_list=var_list_remote)\n            self.ready_for_local_init_op = tf.report_uninitialized_variables(var_list=var_list_remote)\n            self.local_init_op = tf.variables_initializer(var_list=var_list_local)\n            self.ready_op = tf.report_uninitialized_variables(var_list=var_list)\n\n        def init_fn(scaffold, session):\n            # NOTE: `self.load_from_file` is either True or a string value.\n            # - No specific file given -> Use latest checkpoint.\n            saver_dir = self.saver_spec.get(""directory"", """") if self.saver_spec else """"\n            if self.load_from_file is True:\n                assert self.saver_spec is not None,\\\n                    ""ERROR: load_from_file is True but no saver_spec with \'directory\' provided""\n                file = tf.train.latest_checkpoint(\n                    checkpoint_dir=saver_dir,\n                    latest_filename=None\n                )\n            # - File given -> Look for it in cwd, then in our checkpoint directory.\n            else:\n                assert isinstance(self.load_from_file, str)\n                file = self.load_from_file\n                if not os.path.isfile(file):\n                    file = os.path.join(saver_dir, self.load_from_file)\n\n            if file is not None:\n                scaffold.saver.restore(sess=session, save_path=file)\n\n        # Create the tf.train.Scaffold object. Monitoring cannot be disabled for this.\n        if not self.disable_monitoring:\n            self.scaffold = tf.train.Scaffold(\n                init_op=self.init_op,\n                init_feed_dict=None,\n                init_fn=init_fn if self.load_from_file else None,\n                ready_op=self.ready_op,\n                ready_for_local_init_op=self.ready_for_local_init_op,\n                local_init_op=self.local_init_op,\n                #summary_op=self.summary_op,\n                saver=self.saver,\n                copy_from_scaffold=None\n            )\n\n    @staticmethod\n    def setup_specifiable_servers(hooks):\n        # Add the hook only if there have been SpecifiableServer objects created.\n        # TODO: Change this registry to a tf collections based one. Problem: EnvStepper is created before the Graph,\n        # TODO: So when the Graph gets entered, the registry (with the SpecifiableServer in it) is gone.\n        if len(SpecifiableServer.INSTANCES) > 0:\n            hooks.append(SpecifiableServerHook())\n\n    def setup_session(self, hooks):\n        """"""\n        Creates and then enters the session for this model. Also finalizes the graph.\n\n        Args:\n            hooks (list): A list of session hooks to use.\n        """"""\n        if self.execution_mode == ""distributed"":\n            self.logger.info(""Setting up distributed TensorFlow session."")\n            if self.server is None:\n                raise RLGraphError(\n                    ""TensorflowGraphExecutor\'s Server is None! It could be that your DISTRIBUTED_BACKEND (currently ""\n                    ""set to \'{}\') is not set to \'distributed_tf\'. You can do so via the RLGraph config file in your ""\n                    ""home directory or the ENV variable \'RLGRAPH_DISTRIBUTED_BACKEND=distributed_tf\'."".\n                    format(get_distributed_backend())\n                )\n            if self.tf_session_type == ""monitored-session"":\n                session_creator = tf.train.ChiefSessionCreator(\n                    scaffold=self.scaffold,\n                    master=self.server.target,\n                    config=self.tf_session_config,\n                    checkpoint_dir=None,\n                    checkpoint_filename_with_path=None\n                )\n                self.monitored_session = tf.train.MonitoredSession(\n                    #is_chief=self.execution_spec[""distributed_spec""][""task_index""] == 0,\n                    session_creator=session_creator,\n                    hooks=hooks,\n                    stop_grace_period_secs=120  # Default value.\n                )\n            else:\n                assert self.tf_session_type == ""monitored-training-session"",\\\n                    ""ERROR: Invalid session type: {}!"".format(self.tf_session_type)\n                is_chief = self.execution_spec[""distributed_spec""].get(\n                    ""is_chief"", self.execution_spec[""distributed_spec""][""task_index""] == 0\n                )\n                self.monitored_session = tf.train.MonitoredTrainingSession(\n                    master=self.server.target,\n                    is_chief=is_chief,\n                    checkpoint_dir=None,  # TODO: specify?\n                    save_checkpoint_secs=600,\n                    save_summaries_secs=30,\n                    log_step_count_steps=50000,\n                    # scaffold=self.scaffold,\n                    # Ignore other hooks\n                    hooks=[hooks[-1]] if hooks else None,\n                    config=self.tf_session_config,\n                    stop_grace_period_secs=120  # Default value.\n                )\n        else:\n            # If monitoring is disabled,\n            if self.disable_monitoring:\n                self.logger.info(""Setting up default session for non-distributed mode. Session config: {}"".format(\n                    self.tf_session_config))\n                self.monitored_session = tf.Session(config=self.tf_session_config)\n            else:\n                self.logger.info(""Setting up singular monitored session for non-distributed mode. Session config: {}"".\n                                 format(self.tf_session_config))\n                self.monitored_session = tf.train.SingularMonitoredSession(\n                    hooks=hooks,\n                    scaffold=self.scaffold,\n                    master=\'\',  # Default value.\n                    config=self.tf_session_config,\n                    checkpoint_dir=None\n                )\n\n        # Exit the graph-context and finalize the graph.\n        if self.graph_default_context is not None:\n            self.graph_default_context.__exit__(None, None, None)\n\n        # TODO back in\n        # self.graph.finalize()\n\n        if self.disable_monitoring:\n            # If no monitoring, both just end up being simple sessions.\n            self.session = self.monitored_session\n            self.session.run(self.init_op)\n        else:\n            # Enter the session to be ready for acting/learning.\n            self.monitored_session.__enter__()\n            self.session = self.monitored_session._tf_sess()\n\n        # Setup the tf Profiler.\n        if self.profiling_enabled and not self.disable_monitoring:\n            self.profiler = tf.profiler.Profiler(graph=self.session.graph)\n\n    def load_model(self, checkpoint_directory=None, checkpoint_path=None):\n        if checkpoint_directory is not None and checkpoint_path is not None:\n            checkpoint_file = os.path.join(checkpoint_directory, checkpoint_path)\n            self.logger.info(""Checkpoint directory and relative path given, fetching checkpoint from: {}""\n                             """".format(checkpoint_file))\n            self.saver.restore(self.session, checkpoint_file)\n        if checkpoint_directory is not None and checkpoint_path is None:\n            checkpoint_file = tf.train.latest_checkpoint(\n                checkpoint_dir=(self.saver_directory if checkpoint_directory is None else checkpoint_directory),\n            )\n            self.logger.info(""Checkpoint directory given without path, found latest checkpoint file: {}.""\n                             """".format(checkpoint_file))\n            self.saver.restore(self.session, checkpoint_file)\n        elif checkpoint_directory is None and checkpoint_path is not None:\n            self.logger.info(""No checkpoint directory given, fetching from absolute checkpoint_path: {}.""\n                             """".format(checkpoint_path))\n            self.saver.restore(self.session, checkpoint_path)\n        else:\n            raise ValueError(""Provide either a checkpoint directory or full checkpoint path to load a model."")\n\n    def store_model(self, path=None, add_timestep=True):\n        if self.summary_writer is not None:\n            self.summary_writer.flush()\n\n        self.saver.save(\n            sess=self.session,\n            save_path=(path or self.saver_directory),\n            global_step=(self.global_training_timestep if add_timestep is True else None),\n            latest_filename=None,\n            meta_graph_suffix=""meta"",\n            write_meta_graph=True,\n            write_state=True\n        )\n        self.logger.info(""Stored model to path: {}"".format(path))\n\n    def export_graph_definition(self, filename):\n        """"""\n        Exports TensorFlow meta graph to file.\n\n        Args:\n            filename (str): File to save meta graph. Should end in .meta\n        """"""\n        if not filename.endswith(\'.meta\'):\n            self.logger.warn(\'Filename for TensorFlow meta graph should end with .meta.\')\n        self.saver.export_meta_graph(filename=filename)\n\n    def terminate(self):\n        """"""\n        Terminates the GraphExecutor, so it will no longer be usable.\n        Things that need to be cleaned up should be placed into this function, e.g. closing sessions\n        and other open connections.\n        """"""\n        # Close the tf.Session.\n        if self.tf_session_auto_start is True:\n            self.monitored_session.close()\n\n    def _build_device_strategy(self, root_component, root_optimizer, batch_size, extra_build_args=None):\n        """"""\n        When using multiple GPUs or other special devices, additional graph components\n        may be required to split up incoming data, load it to device memories, and aggregate\n        results.\n\n        In RLGraph, graph building and execution are separated so different device strategies can be\n        plugged into single agent definitions. For example, one agent may use a single cpu or GPU,\n        a local multi-gpu strategy and combine this with distributed sample collection via distributed\n        TensorFlow or Ray.\n\n        This method expands the meta graph according to the given device strategy if necessary.\n\n        Args:\n            root_component (Component): The root Component (will be used to create towers via `Component.copy()`).\n            root_optimizer (Optimizer): The Optimizer object of the root Component.\n            batch_size (int): The batch size that needs to be split between the different GPUs.\n            extra_build_args (Optional[dict]): Extra build elements to pass.\n        """"""\n        self.optimizers = []\n        if root_optimizer is not None:\n            self.optimizers.append(root_optimizer)\n        # Save separate vf optimizer if necessary.\n        if extra_build_args is not None and ""vf_optimizer"" in extra_build_args:\n            self.optimizers.append(extra_build_args[""vf_optimizer""])\n        if extra_build_args is not None and ""optimizers"" in extra_build_args:\n            self.optimizers.extend(extra_build_args[""optimizers""])\n\n        if self.device_strategy == ""multi_gpu_sync"":\n            assert self.num_gpus > 1 or (self.fake_gpus is True and self.max_usable_gpus > 0), \\\n                ""ERROR: MultiGpuSync strategy needs more than one GPU available (or more than 0 `max_usable_gpus` if"" \\\n                ""GPUs are faked), but there are only {} GPUs visible and {} `max_usable_gpus`."". \\\n                format(self.num_gpus, self.max_usable_gpus)\n            self.logger.info(\n                ""Building MultiGpuSync strategy with {} GPUs (use fake GPUs={})."".format(self.num_gpus, self.fake_gpus)\n            )\n\n            # Support faked GPUs (will place all towers on the CPU in that case).\n            devices = self.gpu_names or [self.default_device for _ in range(self.max_usable_gpus)]\n            sub_graphs = []\n            for i, device in enumerate(devices):\n                # Copy and assign GPU to copy.\n                self.logger.info(""Creating device sub-graph for device: {}."".format(device))\n                # Only place the ops of the tower on the GPU (variables are shared with root).\n                sub_graph = root_component.copy(device=device, scope=""tower-{}"".format(i))\n                sub_graph.is_multi_gpu_tower = True\n\n                sub_graphs.append(sub_graph)\n                self.used_devices.append(device)\n\n            # Setup and add MultiGpuSynchronizer to root.\n            multi_gpu_optimizer = MultiGpuSynchronizer(batch_size=batch_size)\n            root_component.add_components(multi_gpu_optimizer)\n            #multi_gpu_optimizer.graph_fn_num_outputs[""_graph_fn_calculate_update_from_external_batch""] = \\\n            #    root_component.graph_fn_num_outputs[""_graph_fn_update_from_external_batch""]\n            multi_gpu_optimizer.setup_towers(sub_graphs, devices)\n\n    def _sanity_check_devices(self):\n        """"""\n        Checks device assignments to identify unused or conflicting assignments.\n        """"""\n        assignments = self.graph_builder.device_component_assignments\n        # Devices can be used here or within graph build assignments.\n        used_devices = list(assignments.keys()) + self.used_devices\n\n        # Warn if some devices have not been assigned.\n        self.logger.info(""Checking if all visible devices are in use for strategy: {}. Available devices are: {}."".\n                         format(self.device_strategy, self.available_devices))\n        for device in self.available_devices:\n            if device not in used_devices:\n                self.logger.warning(""Warning: Device {} is usable but has not been assigned."".format(\n                    device\n                ))\n\n    def init_gpus(self):\n        """"""\n        Parses GPU specs and initializes GPU devices by adjusting visible CUDA devices to\n        environment and setting memory allocation options.\n        """"""\n        gpu_spec = self.execution_spec.get(""gpu_spec"", None)\n\n        if gpu_spec is not None:\n            self.gpus_enabled = gpu_spec.get(""gpus_enabled"", False)\n            self.max_usable_gpus = gpu_spec.get(""max_usable_gpus"", 1)\n\n            if self.gpus_enabled:\n                assert self.max_usable_gpus > 0, ""ERROR: GPUs are enabled but max_usable_gpus are not >0 but {}"".\\\n                    format(self.max_usable_gpus)\n                gpu_names = sorted([x.name for x in self.local_device_protos if x.device_type == \'GPU\'])\n\n                # Set fake_gpus to True iff `fake_gpus_if_necessary` is True.\n                self.fake_gpus = gpu_spec.get(""fake_gpus_if_necessary"", False)\n\n                if self.fake_gpus is False:\n                    cuda_visible_devices = gpu_spec.get(""cuda_visible_devices"", None)\n                    if len(gpu_names) < self.max_usable_gpus:\n                        self.logger.warn(""WARNING: max_usable_gpus is {} but only {} gpus are locally visible. ""\n                                         ""Using all available GPUs."".format(self.max_usable_gpus, len(gpu_names)))\n\n                    # Indicate specific CUDA devices to be used.\n                    if cuda_visible_devices is not None:\n                        if not isinstance(cuda_visible_devices, (str, int, list)):\n                            raise ValueError(\n                                ""ERROR: \'cuda_visible_devices\' must be int/string or list of device index-values, e.g. ""\n                                ""[0,2] or \'0\' or 1, but is: {}"".format(type(cuda_visible_devices))\n                            )\n                        cuda_visible_devices = force_list(cuda_visible_devices)\n                        num_provided_cuda_devices = len(cuda_visible_devices)\n                        use_names = [gpu_names[int(device_id)] for device_id in cuda_visible_devices]\n                        cuda_visible_devices = "","".join(cuda_visible_devices)\n\n                        # Must match number of allowed GPUs.\n                        assert self.max_usable_gpus == num_provided_cuda_devices,\\\n                            ""ERROR: Provided CUDA {} devices: {}, but max_usable_gpus is {}. Must match!""\n\n                        # Expose these devices.\n                        self.logger.info(""GPU strategy: Exposing CUDA devices with ids: {}"".format(cuda_visible_devices))\n                        os.environ[""CUDA_VISIBLE_DEVICES""] = cuda_visible_devices\n                        self.gpu_names = use_names\n\n                    # Assign as many as specified.\n                    else:\n                        cuda_visible_devices = []\n                        use_names = []\n                        for i, name in enumerate(gpu_names):\n                            if len(use_names) < self.max_usable_gpus:\n                                use_names.append(name)\n                                cuda_visible_devices.append(str(i))\n                        os.environ[""CUDA_VISIBLE_DEVICES""] = "","".join(cuda_visible_devices)\n                        self.logger.info(""GPU strategy initialized with GPUs enabled: {}"".format(use_names))\n                        self.gpu_names = use_names\n\n                    self.num_gpus = len(self.gpu_names)\n                    self.available_devices.extend(self.gpu_names)\n                    per_process_gpu_memory_fraction = gpu_spec.get(""per_process_gpu_memory_fraction"", None)\n                    if per_process_gpu_memory_fraction is not None:\n                        self.tf_session_config.gpu_options.per_process_gpu_memory_fraction = per_process_gpu_memory_fraction\n\n                    self.tf_session_config.gpu_options.allow_growth = gpu_spec.get(""allow_memory_growth"", False)\n        else:\n            # Do not allow any GPUs to be used.\n            self.gpus_enabled = False\n            self.logger.info(""gpu_spec is None, disabling GPUs."")\n'"
rlgraph/spaces/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom functools import partial\n\nimport numpy as np\n\nfrom rlgraph.spaces.bool_box import BoolBox\nfrom rlgraph.spaces.box_space import BoxSpace\nfrom rlgraph.spaces.containers import ContainerSpace, Dict, Tuple\nfrom rlgraph.spaces.float_box import FloatBox\nfrom rlgraph.spaces.int_box import IntBox\nfrom rlgraph.spaces.space import Space\nfrom rlgraph.spaces.text_box import TextBox\nimport rlgraph.spaces.space_utils\n\nSpace.__lookup_classes__ = dict({\n    ""bool"": BoolBox,\n    ""boolbox"": BoolBox,\n    bool: BoolBox,\n    np.bool_: BoolBox,\n    ""int"": IntBox,\n    int: IntBox,\n    np.int32: IntBox,\n    ""intbox"": IntBox,\n    ""continuous"": FloatBox,\n    ""float"": FloatBox,\n    ""floatbox"": FloatBox,\n    ""float32"": FloatBox,\n    float: FloatBox,\n    np.float32: FloatBox,\n    np.float64: partial(FloatBox, dtype=np.float64),\n    ""float64"": partial(FloatBox, dtype=np.float64),\n    ""list"": Tuple,\n    ""tuple"": Tuple,\n    # ""sequence"" action type for nlp use cases and combinatorial optimisation.\n    ""sequence"": Tuple,\n    dict: Dict,\n    ""dict"": Dict,\n    str: TextBox,\n    ""str"": TextBox,\n    ""text"": TextBox,\n    ""textbox"": TextBox\n})\n\n# Default Space: A float from 0.0 to 1.0.\nSpace.__default_constructor__ = partial(FloatBox, 1.0)\n\n__all__ = [""Space"", ""BoxSpace"", ""FloatBox"", ""IntBox"", ""BoolBox"", ""TextBox"",\n           ""ContainerSpace"", ""Dict"", ""Tuple""]\n'"
rlgraph/spaces/bool_box.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph.utils.util import convert_dtype\nfrom rlgraph.spaces.box_space import BoxSpace\n\n\nclass BoolBox(BoxSpace):\n    def __init__(self, shape=None, **kwargs):\n        super(BoolBox, self).__init__(low=False, high=True, shape=shape, dtype=np.bool_, **kwargs)\n\n    def sample(self, size=None, fill_value=None):\n        shape = self._get_np_shape(num_samples=size)\n        if fill_value is None:\n            sample_ = np.random.choice(a=[False, True], size=shape)\n        else:\n            sample_ = np.full(shape=size, fill_value=fill_value)\n        return sample_\n\n    def contains(self, sample):\n        if self.shape == ():\n            return isinstance(sample, (bool, np.bool_))\n        else:\n            return convert_dtype(sample.dtype, ""np"") == np.bool_\n\n'"
rlgraph/spaces/box_space.py,3,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport re\n\nimport numpy as np\nfrom six.moves import xrange as range_\nfrom rlgraph import get_backend\nfrom rlgraph.spaces.space import Space\nfrom rlgraph.utils.initializer import Initializer\nfrom rlgraph.utils.util import convert_dtype\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nif get_backend() == ""pytorch"":\n    import torch\n\n\nclass BoxSpace(Space):\n    """"""\n    A box in R^n with a shape tuple of len n. Each dimension may be bounded.\n    """"""\n    def __init__(self, low, high, shape=None, add_batch_rank=False, add_time_rank=False, time_major=False,\n                 dtype=np.float32):\n        """"""\n        Args:\n            low (any): The lower bound (see Valid Inputs for more information).\n            high (any): The upper bound (see Valid Inputs for more information).\n            shape (tuple): The shape of this space.\n            dtype (np.type): The data type (as numpy type) for this Space.\n                Allowed are: np.int8,16,32,64, np.float16,32,64 and np.bool_.\n\n        Valid inputs:\n            BoxSpace(0.0, 1.0) # low and high are given as scalars and shape is assumed to be ()\n                -> single scalar between low and high.\n            BoxSpace(-1.0, 1.0, (3,4)) # low and high are scalars, and shape is provided -> nD array\n                where all(!) elements are between low and high.\n            BoxSpace(np.array([-1.0,-2.0]), np.array([2.0,4.0])) # low and high are arrays of the same shape\n                (no shape given!) -> nD array where each dimension has different bounds.\n        """"""\n        super(BoxSpace, self).__init__(add_batch_rank=add_batch_rank, add_time_rank=add_time_rank,\n                                       time_major=time_major)\n\n        self.dtype = dtype\n\n        # Determine the shape.\n        if shape is None:\n            if isinstance(low, (int, float, bool)):\n                self._shape = ()\n            else:\n                self._shape = np.shape(low)\n        else:\n            assert isinstance(shape, (tuple, list)), ""ERROR: `shape` must be None or a tuple/list.""\n            self._shape = tuple(shape)\n\n        # Determine the bounds.\n        # False if bounds are individualized (each dimension has its own lower and upper bounds and we can get\n        # the single values from self.low and self.high), or a tuple of the globally valid low/high values that apply\n        # to all values in all dimensions.\n        # 0D Space.\n        if self._shape == ():\n            if isinstance(low, np.ndarray):\n                assert low.shape == (), ""ERROR: If shape == (), `low` must be scalar!""\n                low = np.asscalar(low)\n            if isinstance(high, np.ndarray):\n                assert high.shape == (), ""ERROR: If shape == (), `high` must be scalar!""\n                high = np.asscalar(high)\n            self.global_bounds = (low, high)\n        # nD Space (n > 0). Bounds can be single number or individual bounds.\n        else:\n            # Low/high values are given individually per item.\n            if isinstance(low, (list, tuple, np.ndarray)):\n                self.global_bounds = False\n            # Only one low/high value. Use these as generic bounds for all values.\n            else:\n                assert np.isscalar(low) and np.isscalar(high)\n                self.global_bounds = (low, high)\n\n        self.low = np.array(low)\n        self.high = np.array(high)\n        assert self.low.shape == self.high.shape\n\n    def force_batch(self, samples, horizontal=None):\n        assert self.has_time_rank is False, ""ERROR: Cannot force a batch rank if Space `has_time_rank` is True!""\n        # 0D (means: certainly no batch rank) or no extra rank given (compared to this Space), add a batch rank.\n        if np.asarray(samples).ndim == 0 or \\\n                np.asarray(samples).ndim == len(self.get_shape(with_batch_rank=False, with_time_rank=False)):\n            return np.array([samples]), True  # batch size=1\n        # Samples is a list (whose len is interpreted as the batch size) -> return as np.array.\n        elif isinstance(samples, list):\n            return np.asarray(samples), False\n        # Samples is already assumed to be batched. Return as is.\n        return samples, False\n\n    def get_shape(self, with_batch_rank=False, with_time_rank=False, time_major=None, **kwargs):\n        batch_rank = ()\n        if with_batch_rank is not False:\n            # None shapes are typically only allowed in static graphs.\n            if get_backend() == ""tf"":\n                batch_rank = (((None,) if with_batch_rank is True else (with_batch_rank,))\n                              if self.has_batch_rank else ())\n            elif get_backend() == ""pytorch"":\n                batch_rank = (((1,) if with_batch_rank is True else (with_batch_rank,))\n                              if self.has_batch_rank else ())\n\n        time_rank = ()\n        if with_time_rank is not False:\n            time_rank = (((None,) if with_time_rank is True else (with_time_rank,))\n                          if self.has_time_rank else ())\n\n        time_major = self.time_major if time_major is None else time_major\n        if time_major is False:\n            return batch_rank + time_rank + self.shape\n        else:\n            return time_rank + batch_rank + self.shape\n\n    @property\n    def flat_dim(self):\n        return int(np.prod(self.shape))  # also works for shape=()\n\n    @property\n    def bounds(self):\n        return self.low, self.high\n\n    def tensor_backed_bounds(self):\n        if get_backend() == ""pytorch"":\n            return torch.tensor(self.low), torch.tensor(self.high)\n        else:\n            return self.low, self.high\n\n    def get_variable(self, name, is_input_feed=False, add_batch_rank=None, add_time_rank=None,\n                     time_major=None, is_python=False, local=False, **kwargs):\n        add_batch_rank = self.has_batch_rank if add_batch_rank is None else add_batch_rank\n        if add_batch_rank is False:\n            batch_rank = ()\n        elif add_batch_rank is True:\n            batch_rank = (None,) if get_backend() == ""tf"" else (1,)\n        else:\n            batch_rank = (add_batch_rank,)\n\n        add_time_rank = self.has_time_rank if add_time_rank is None else add_time_rank\n        if add_time_rank is False:\n            time_rank = ()\n        elif add_time_rank is True:\n            time_rank = (None,) if get_backend() == ""tf"" else (1,)\n        else:\n            time_rank = (add_time_rank,)\n        time_major = self.time_major if time_major is None else time_major\n\n        if time_major is False:\n            shape = batch_rank + time_rank + self.shape\n        else:\n            shape = time_rank + batch_rank + self.shape\n\n        if is_python is True or get_backend() == ""python"":\n            if isinstance(add_batch_rank, int):\n                if isinstance(add_time_rank, int) and add_time_rank > 0:\n                    if time_major:\n                        var = [[0 for _ in range_(add_batch_rank)] for _ in range_(add_time_rank)]\n                    else:\n                        print([0 for _ in range_(add_time_rank)])\n                        var = [[0 for _ in range_(add_time_rank)] for _ in range_(add_batch_rank)]\n                else:\n                    var = [0 for _ in range_(add_batch_rank)]\n            elif isinstance(add_time_rank, int) and add_time_rank > 0:\n                var = [0 for _ in range_(add_time_rank)]\n            else:\n                var = []\n\n            # Un-indent and just directly construct pytorch?\n            if get_backend() == ""pytorch"" and is_input_feed:\n                # Convert to PyTorch tensors as a faux placehodler.\n                return torch.zeros(shape, dtype=convert_dtype(dtype=self.dtype, to=""pytorch""))\n            else:\n                # TODO also convert?\n                return var\n\n        elif get_backend() == ""tf"":\n            # TODO: re-evaluate the cutting of a leading \'/_?\' (tf doesn\'t like it)\n            name = re.sub(r\'^/_?\', """", name)\n            if is_input_feed:\n                variable = tf.placeholder(dtype=convert_dtype(self.dtype), shape=shape, name=name)\n                if self.has_batch_rank:\n                    variable._batch_rank = self.has_batch_rank\n                if self.has_time_rank:\n                    variable._time_rank = self.has_time_rank\n            else:\n                init_spec = kwargs.pop(""initializer"", None)\n                # Bools should be initializable via 0 or not 0.\n                if self.dtype == np.bool_ and isinstance(init_spec, (int, float)):\n                    init_spec = (init_spec != 0)\n\n                if self.dtype == np.str_ and init_spec == 0:\n                    initializer = None\n                else:\n                    initializer = Initializer.from_spec(shape=shape, specification=init_spec).initializer\n\n                variable = tf.get_variable(\n                    name, shape=shape, dtype=convert_dtype(self.dtype), initializer=initializer,\n                    collections=[tf.GraphKeys.GLOBAL_VARIABLES if local is False else tf.GraphKeys.LOCAL_VARIABLES],\n                    **kwargs\n                )\n            # Add batch/time rank flags to the op.\n            if self.has_batch_rank:\n                variable._batch_rank = 0 if self.time_major is False else 1\n            if self.has_time_rank:\n                variable._time_rank = 1 if self.time_major is False else 0\n            return variable\n\n    def zeros(self, size=None):\n        return self.sample(size=size, fill_value=0)\n\n    def contains(self, sample):\n        sample_shape = sample.shape if not isinstance(sample, int) else ()\n        if sample_shape != self.shape:\n            return False\n        return (sample >= self.low).all() and (sample <= self.high).all()\n\n    def map(self, mapping):\n        return mapping("""", self)\n\n    def __repr__(self):\n        return ""{}({} {} {}{})"".format(\n            type(self).__name__.title(), self.shape, str(self.dtype), ""; +batch"" if self.has_batch_rank else\n            """", ""; +time"" if self.has_time_rank else """"\n        )\n\n    def __eq__(self, other):\n        return isinstance(other, self.__class__) and \\\n               self.shape == other.shape and self.dtype == other.dtype\n               # np.allclose(self.low, other.low) and np.allclose(self.high, other.high) and \\\n\n    def __hash__(self):\n        if self.shape == () or self.global_bounds is not False:\n            return hash((np.asscalar(self.low), np.asscalar(self.high)))\n        return hash((tuple(self.low), tuple(self.high)))\n'"
rlgraph/spaces/containers.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\n\nfrom rlgraph.spaces.space import Space\nfrom rlgraph.utils.ops import DataOpDict, DataOpTuple, FLAT_TUPLE_OPEN, FLAT_TUPLE_CLOSE, unflatten_op, flat_key_lookup\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\n\n\nclass ContainerSpace(Space):\n    """"""\n    A simple placeholder class for Spaces that contain other Spaces.\n    """"""\n    def sample(self, size=None, fill_value=None, horizontal=False):\n        """"""\n        Child classes must overwrite this one again with support for the `horizontal` parameter.\n\n        Args:\n            horizontal (bool): False: Within this container, sample each child-space `size` times.\n                True: Produce `size` single containers in an np.array of len `size`.\n        """"""\n        raise NotImplementedError\n\n    def flat_key_lookup(self, flat_key, custom_scope_separator=None):\n        return flat_key_lookup(self, flat_key, custom_scope_separator)\n\n\nclass Dict(ContainerSpace, dict):\n    """"""\n    A Dict space (an ordered and keyed combination of n other spaces).\n    Supports nesting of other Dict/Tuple spaces (or any other Space types) inside itself.\n    """"""\n    def __init__(self, spec=None, **kwargs):\n        add_batch_rank = kwargs.pop(""add_batch_rank"", False)\n        add_time_rank = kwargs.pop(""add_time_rank"", False)\n        time_major = kwargs.pop(""time_major"", False)\n\n        ContainerSpace.__init__(self, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank, time_major=time_major)\n\n        # Allow for any spec or already constructed Space to be passed in as values in the python-dict.\n        # Spec may be part of kwargs.\n        if spec is None:\n            spec = kwargs\n\n        space_dict = {}\n        for key in sorted(spec.keys()):\n            # Keys must be strings.\n            if not isinstance(key, str):\n                raise RLGraphError(""ERROR: No non-str keys allowed in a Dict-Space!"")\n            # Prohibit reserved characters (for flattened syntax).\n            #if re.search(r\'/|{}\\d+{}\'.format(FLAT_TUPLE_OPEN, FLAT_TUPLE_CLOSE), key):\n            #    raise RLGraphError(""ERROR: Key to Dict must not contain \'/\' or \'{}\\d+{}\'! Key=\'{}\'."".\n            #                       format(FLAT_TUPLE_OPEN, FLAT_TUPLE_CLOSE, key))\n            value = spec[key]\n            # Value is already a Space: Copy it (to not affect original Space) and maybe add/remove batch/time-ranks.\n            if isinstance(value, Space):\n                w_batch_w_time = value.with_extra_ranks(add_batch_rank, add_time_rank, time_major)\n                space_dict[key] = w_batch_w_time\n            # Value is a list/tuple -> treat as Tuple space.\n            elif isinstance(value, (list, tuple)):\n                space_dict[key] = Tuple(\n                    *value, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank, time_major=time_major\n                )\n            # Value is a spec (or a spec-dict with ""type"" field) -> produce via `from_spec`.\n            elif (isinstance(value, dict) and ""type"" in value) or not isinstance(value, dict):\n                space_dict[key] = Space.from_spec(\n                    value, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank, time_major=time_major\n                )\n            # Value is a simple dict -> recursively construct another Dict Space as a sub-space of this one.\n            else:\n                space_dict[key] = Dict(\n                    value, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank, time_major=time_major\n                )\n            # Set the parent of the added Space to `self`.\n            space_dict[key].parent = self\n\n        dict.__init__(self, space_dict)\n\n    def _add_batch_rank(self, add_batch_rank=False):\n        super(Dict, self)._add_batch_rank(add_batch_rank)\n        for v in self.values():\n            v._add_batch_rank(add_batch_rank)\n\n    def _add_time_rank(self, add_time_rank=False, time_major=False):\n        super(Dict, self)._add_time_rank(add_time_rank, time_major)\n        for v in self.values():\n            v._add_time_rank(add_time_rank, time_major)\n\n    def force_batch(self, samples, horizontal=False):\n        # Return a batch of dicts.\n        if horizontal is True:\n            # Input is already batched.\n            if isinstance(samples, (np.ndarray, list, tuple)):\n                return samples, False  # False=batch rank was not added\n            # Input is a single dict, return batch=1 sample.\n            else:\n                return np.array([samples]), True  # True=batch rank was added\n        # Return a dict of batched data.\n        else:\n            # `samples` is already a batched structure (list, tuple, ndarray).\n            if isinstance(samples, (np.ndarray, list, tuple)):\n                return dict({key: self[key].force_batch([s[key] for s in samples], horizontal=horizontal)[0]\n                             for key in sorted(self.keys())}), False\n            # `samples` is already a container (underlying data could be batched or not).\n            else:\n                # Figure out, whether underlying data is already batched.\n                first_key = next(iter(samples))\n                batch_was_added = self[first_key].force_batch(samples[first_key], horizontal=horizontal)[1]\n                return dict({key: self[key].force_batch(samples[key], horizontal=horizontal)[0]\n                             for key in sorted(self.keys())}), batch_was_added\n\n    @property\n    def shape(self):\n        return tuple([self[key].shape for key in sorted(self.keys())])\n\n    def get_shape(self, with_batch_rank=False, with_time_rank=False, time_major=None, with_category_rank=False):\n        return tuple([self[key].get_shape(\n            with_batch_rank=with_batch_rank, with_time_rank=with_time_rank, time_major=time_major,\n            with_category_rank=with_category_rank\n        ) for key in sorted(self.keys())])\n\n    @property\n    def rank(self):\n        return tuple([self[key].rank for key in sorted(self.keys())])\n\n    @property\n    def flat_dim(self):\n        return int(np.sum([c.flat_dim for c in self.values()]))\n\n    @property\n    def dtype(self):\n        return DataOpDict([(key, subspace.dtype) for key, subspace in self.items()])\n\n    def get_variable(self, name, is_input_feed=False, add_batch_rank=None, add_time_rank=None, time_major=None,\n                     **kwargs):\n        return DataOpDict(\n            [(key, subspace.get_variable(\n                name + ""/"" + key, is_input_feed=is_input_feed, add_batch_rank=add_batch_rank,\n                add_time_rank=add_time_rank, time_major=time_major, **kwargs\n            )) for key, subspace in self.items()]\n        )\n\n    def _flatten(self, mapping, custom_scope_separator, scope_separator_at_start, return_as_dict_space,\n                 scope_, list_):\n        # Iterate through this Dict.\n        scope_ += custom_scope_separator if len(scope_) > 0 or scope_separator_at_start else """"\n        for key in sorted(self.keys()):\n            self[key].flatten(\n                mapping, custom_scope_separator, scope_separator_at_start, return_as_dict_space, scope_ + key, list_\n            )\n\n    def sample(self, size=None, fill_value=None, horizontal=False):\n        if horizontal:\n            return np.array([{key: self[key].sample(fill_value=fill_value) for key in sorted(self.keys())}] *\n                            (size or 1))\n        else:\n            return {key: self[key].sample(size=size, fill_value=fill_value) for key in sorted(self.keys())}\n\n    def zeros(self, size=None):\n        return DataOpDict([(key, subspace.zeros(size=size)) for key, subspace in self.items()])\n\n    def contains(self, sample):\n        return isinstance(sample, dict) and all(self[key].contains(sample[key]) for key in self.keys())\n\n    def map(self, mapping):\n        flattened_self = self.flatten(mapping=mapping)\n        return Dict(\n            dict(unflatten_op(flattened_self)),\n            add_batch_rank=self.has_batch_rank, add_time_rank=self.has_time_rank, time_major=self.time_major\n        )\n\n    def __repr__(self):\n        return ""Dict({})"".format([(key, self[key].__repr__()) for key in self.keys()])\n\n    def __eq__(self, other):\n        if not isinstance(other, Dict):\n            return False\n        return dict(self) == dict(other)\n\n\nclass Tuple(ContainerSpace, tuple):\n    """"""\n    A Tuple space (an ordered sequence of n other spaces).\n    Supports nesting of other container (Dict/Tuple) spaces inside itself.\n    """"""\n    def __new__(cls, *components, **kwargs):\n        if isinstance(components[0], (list, tuple)) and not isinstance(components[0], Tuple):\n            assert len(components) == 1\n            components = components[0]\n\n        add_batch_rank = kwargs.get(""add_batch_rank"", False)\n        add_time_rank = kwargs.get(""add_time_rank"", False)\n        time_major = kwargs.get(""time_major"", False)\n\n        # Allow for any spec or already constructed Space to be passed in as values in the python-list/tuple.\n        list_ = list()\n        for value in components:\n            # Value is already a Space: Copy it (to not affect original Space) and maybe add/remove batch-rank.\n            if isinstance(value, Space):\n                list_.append(value.with_extra_ranks(add_batch_rank, add_time_rank, time_major))\n            # Value is a list/tuple -> treat as Tuple space.\n            elif isinstance(value, (list, tuple)):\n                list_.append(\n                    Tuple(*value, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank, time_major=time_major)\n                )\n            # Value is a spec (or a spec-dict with ""type"" field) -> produce via `from_spec`.\n            elif (isinstance(value, dict) and ""type"" in value) or not isinstance(value, dict):\n                list_.append(Space.from_spec(\n                    value, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank, time_major=time_major\n                ))\n            # Value is a simple dict -> recursively construct another Dict Space as a sub-space of this one.\n            else:\n                list_.append(Dict(\n                    value, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank, time_major=time_major\n                ))\n\n        return tuple.__new__(cls, list_)\n\n    def __init__(self, *components, **kwargs):\n        add_batch_rank = kwargs.get(""add_batch_rank"", False)\n        add_time_rank = kwargs.get(""add_time_rank"", False)\n        time_major = kwargs.get(""time_major"", False)\n        super(Tuple, self).__init__(add_batch_rank=add_batch_rank, add_time_rank=add_time_rank, time_major=time_major)\n\n        # Set the parent of the added Space to `self`.\n        for c in self:\n            c.parent = self\n\n    def _add_batch_rank(self, add_batch_rank=False):\n        super(Tuple, self)._add_batch_rank(add_batch_rank)\n        for v in self:\n            v._add_batch_rank(add_batch_rank)\n\n    def _add_time_rank(self, add_time_rank=False, time_major=False):\n        super(Tuple, self)._add_time_rank(add_time_rank, time_major)\n        for v in self:\n            v._add_time_rank(add_time_rank, time_major)\n\n    def force_batch(self, samples, horizontal=False):\n        return tuple([c.force_batch(samples[i])[0] for i, c in enumerate(self)])\n\n    @property\n    def shape(self):\n        return tuple([c.shape for c in self])\n\n    def get_shape(self, with_batch_rank=False, with_time_rank=False, time_major=None, with_category_rank=False):\n        return tuple([c.get_shape(\n            with_batch_rank=with_batch_rank, with_time_rank=with_time_rank, time_major=time_major,\n            with_category_rank=with_category_rank\n        ) for c in self])\n\n    @property\n    def rank(self):\n        return tuple([c.rank for c in self])\n\n    @property\n    def flat_dim(self):\n        return np.sum([c.flat_dim for c in self])\n\n    @property\n    def dtype(self):\n        return DataOpTuple([c.dtype for c in self])\n\n    def get_variable(self, name, is_input_feed=False, add_batch_rank=None, add_time_rank=None, time_major=None,\n                     **kwargs):\n        return DataOpTuple(\n            [subspace.get_variable(\n                name+""/""+str(i), is_input_feed=is_input_feed, add_batch_rank=add_batch_rank,\n                add_time_rank=add_time_rank, time_major=time_major, **kwargs\n            ) for i, subspace in enumerate(self)]\n        )\n\n    def _flatten(self, mapping, custom_scope_separator, scope_separator_at_start, return_as_dict_space, scope_, list_):\n        # Iterate through this Tuple.\n        scope_ += (custom_scope_separator if len(scope_) > 0 or scope_separator_at_start else """") + FLAT_TUPLE_OPEN\n        for i, component in enumerate(self):\n            component.flatten(\n                mapping, custom_scope_separator, scope_separator_at_start, return_as_dict_space,\n                scope_ + str(i) + FLAT_TUPLE_CLOSE, list_\n            )\n\n    def sample(self, size=None, fill_value=None, horizontal=False):\n        if horizontal:\n            return np.array([tuple(subspace.sample(fill_value=fill_value) for subspace in self)] * (size or 1))\n        else:\n            return tuple(x.sample(size=size, fill_value=fill_value) for x in self)\n\n    def zeros(self, size=None):\n        return tuple([c.zeros(size=size) for i, c in enumerate(self)])\n\n    def contains(self, sample):\n        return isinstance(sample, (tuple, list, np.ndarray)) and len(self) == len(sample) and \\\n               all(c.contains(xi) for c, xi in zip(self, sample))\n\n    def map(self, mapping):\n        flattened_self = self.flatten(mapping=mapping)\n        return Tuple(\n            unflatten_op(flattened_self),\n            add_batch_rank=self.has_batch_rank, add_time_rank=self.has_time_rank, time_major=self.time_major\n        )\n\n    def __repr__(self):\n        return ""Tuple({})"".format(tuple([cmp.__repr__() for cmp in self]))\n\n    def __eq__(self, other):\n        return tuple.__eq__(self, other)\n'"
rlgraph/spaces/float_box.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph.utils.util import convert_dtype as dtype_\nfrom rlgraph.spaces.box_space import BoxSpace\n\n\nclass FloatBox(BoxSpace):\n    def __init__(self, low=None, high=None, shape=None, dtype=""float32"", **kwargs):\n        if low is None:\n            assert high is None, ""ERROR: If `low` is None, `high` must be None as well!""\n            low = float(""-inf"")\n            high = float(""inf"")\n            self.unbounded = True\n        else:\n            self.unbounded = False\n            # support calls like (FloatBox(1.0) -> low=0.0, high=1.0)\n            if high is None:\n                high = low\n                low = 0.0\n\n        dtype = dtype_(dtype, ""np"")\n        assert dtype in [np.float16, np.float32, np.float64], ""ERROR: FloatBox does not allow dtype \'{}\'!"".format(dtype)\n\n        super(FloatBox, self).__init__(low=low, high=high, shape=shape, dtype=dtype, **kwargs)\n\n    def sample(self, size=None, fill_value=None):\n        shape = self._get_np_shape(num_samples=size)\n        if fill_value is not None:\n            sample_ = np.full(shape=shape, fill_value=fill_value)\n        else:\n            if self.unbounded:\n                sample_ = np.random.uniform(size=shape)\n            else:\n                sample_ = np.random.uniform(low=self.low, high=self.high, size=shape)\n\n        # Make sure return values have the right dtype (float64 is np.random\'s default).\n        return np.asarray(sample_, dtype=self.dtype)\n'"
rlgraph/spaces/int_box.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.spaces.box_space import BoxSpace\nfrom rlgraph.spaces.float_box import FloatBox\nfrom rlgraph.utils.util import convert_dtype as dtype_, LARGE_INTEGER\n\n\nclass IntBox(BoxSpace):\n    """"""\n    A box in Z^n (only integers; each coordinate is bounded)\n    e.g. an image (w x h x RGB) where each color channel pixel can be between 0 and 255.\n    """"""\n    def __init__(self, low=None, high=None, shape=None, dtype=""int32"", **kwargs):\n        """"""\n        Valid inputs:\n            IntBox(6)  # only high is given -> low assumed to be 0 (0D scalar).\n            IntBox(0, 2) # low and high are given as scalars and shape is assumed to be 0D scalar.\n            IntBox(-1, 1, (3,4)) # low and high are scalars, and shape is provided.\n            IntBox(np.array([-1,-2]), np.array([2,4])) # low and high are arrays of the same shape (no shape given!)\n\n        NOTE: The `high` value for IntBoxes is excluded. Valid values thus are from the interval: [low,high[\n        """"""\n        if low is None:\n            if high is not None:\n                low = 0\n            else:\n                low = -LARGE_INTEGER\n                high = LARGE_INTEGER\n        # support calls like (IntBox(5) -> low=0, high=5)\n        elif high is None:\n            high = low\n            low = 0\n\n        dtype = dtype_(dtype, ""np"")\n        assert dtype in [np.int16, np.int32, np.int64, np.uint8], \\\n            ""ERROR: IntBox does not allow dtype \'{}\'!"".format(dtype)\n\n        super(IntBox, self).__init__(low=low, high=high, shape=shape, dtype=dtype, **kwargs)\n\n        self.num_categories = None if self.global_bounds is False else self.global_bounds[1]\n\n    def get_shape(self, with_batch_rank=False, with_time_rank=False, **kwargs):\n        """"""\n        Keyword Args:\n            with_category_rank (bool): Whether to include a category rank for this IntBox (if all dims have equal\n                lower/upper bounds).\n        """"""\n        with_category_rank = kwargs.pop(""with_category_rank"", False)\n        shape = super(IntBox, self).get_shape(with_batch_rank=with_batch_rank, with_time_rank=with_time_rank, **kwargs)\n        if with_category_rank is not False:\n            return shape + ((self.num_categories,) if self.num_categories is not None else ())\n        return shape\n\n    def as_one_hot_float_space(self):\n        """"""\n        Returns a new FloatBox Space resulting from one-hot flattening out this space\n        along its number of categories.\n\n        Returns:\n            FloatBox: The resulting FloatBox Space (with the same batch and time-rank settings).\n        """"""\n        return FloatBox(\n            low=0.0, high=1.0, shape=self.get_shape(with_category_rank=True),\n            add_batch_rank=self.has_batch_rank, add_time_rank=self.has_time_rank\n        )\n\n    @property\n    def flat_dim_with_categories(self):\n        """"""\n        If we were to flatten this Space and also consider each single possible int value (assuming global bounds)\n        as one category, what would the dimension have to be to represent this Space?\n        """"""\n        if self.global_bounds is False:\n            return int(np.sum(self.high))  # TODO: this assumes that low is always 0.\n        return int(np.prod(self.shape) * self.global_bounds[1])\n\n    def sample(self, size=None, fill_value=None):\n        shape = self._get_np_shape(num_samples=size)\n        if fill_value is None:\n            sample_ = np.random.uniform(low=self.low, high=self.high, size=shape)\n        else:\n            sample_ = fill_value if shape == () or shape is None else np.full(shape=shape, fill_value=fill_value)\n\n        return np.asarray(sample_, dtype=self.dtype)\n\n    def get_variable(\n            self, name, is_input_feed=False, add_batch_rank=None, add_time_rank=None, time_major=None, is_python=False,\n            local=False, **kwargs\n    ):\n        variable = super(IntBox, self).get_variable(\n            name, is_input_feed, add_batch_rank, add_time_rank, time_major, is_python, local, **kwargs\n        )\n        # Add num_categories information to placeholder.\n        if get_backend() == ""tf"" and is_python is False:\n            variable._num_categories = self.num_categories\n        return variable\n\n    def contains(self, sample):\n        # If int: Check for int type in given sample.\n        if not np.equal(np.mod(sample, 1), 0).all():\n            return False\n        return super(IntBox, self).contains(sample)\n\n'"
rlgraph/spaces/space.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport copy\nimport re\nfrom collections import OrderedDict\n\nfrom rlgraph.utils.specifiable import Specifiable\n\n\nclass Space(Specifiable):\n    """"""\n    Space class (based on and compatible with openAI Spaces).\n    Provides a classification for state-, action-, reward- and other spaces.\n    """"""\n    # Global unique Space ID.\n    _ID = -1\n\n    def __init__(self, add_batch_rank=False, add_time_rank=False, time_major=False):\n        """"""\n        Args:\n            add_batch_rank (bool): Whether to always add a batch rank at the 0th (or 1st) position when creating\n                variables from this Space.\n            add_time_rank (bool): Whether to always add a time rank at the 1st (or 0th) position when creating\n                variables from this Space.\n            time_major (bool): Whether the time rank should come before the batch rank. Not important if one\n                of these ranks (or both) does not exist.\n        """"""\n        super(Space, self).__init__()\n\n        self.id = self.get_id()\n\n        self._shape = None\n\n        # Parent Space for usage in nested ContainerSpace structures.\n        self.parent = None\n\n        self.has_batch_rank = None\n        self.has_time_rank = None\n        self.time_major = None\n\n        # Back-reference to an op-record that has this Space.\n        self.op_rec_ref = None\n\n        self._add_batch_rank(add_batch_rank)\n        self._add_time_rank(add_time_rank, time_major)\n\n    def _add_batch_rank(self, add_batch_rank=False):\n        """"""\n        Changes the add_batch_rank property of this Space (and of all child Spaces in a ContainerSpace).\n\n        Args:\n            add_batch_rank (bool): Whether this Space (and all child Spaces in a ContainerSpace) should have a\n                batch rank.\n        """"""\n        self.has_batch_rank = add_batch_rank\n\n    def _add_time_rank(self, add_time_rank=False, time_major=False):\n        """"""\n        Changes the add_time_rank property of this Space (and of all child Spaces in a ContainerSpace).\n\n        Args:\n            add_time_rank (bool): Whether this Space (and all child Spaces in a ContainerSpace) should have a\n                time rank.\n            time_major (bool): Whether the time rank should come before the batch rank. Not important if no batch rank\n                exists.\n        """"""\n        self.has_time_rank = add_time_rank\n        self.time_major = time_major\n        # Auto-set time-major to True if there is only a time-rank.\n        if self.has_batch_rank is False and self.has_time_rank is True:\n            self.time_major = True\n\n    def with_extra_ranks(self, add_batch_rank=True, add_time_rank=True, time_major=False):\n        """"""\n        Returns a deepcopy of this Space, but with `has_batch_rank` and `has_time_rank`\n        set to the provided value. Use None to leave whatever value this Space has already.\n\n        Args:\n            add_batch_rank (Optional[bool]): If True or False, set the `has_batch_rank` property of the new Space\n                to this value. Use None to leave the property as is.\n            add_time_rank (Optional[bool]): If True or False, set the `has_time_rank` property of the new Space\n                to this value. Use None to leave the property as is.\n            time_major (Optional[bool]): Whether the time-rank should be the 0th rank (instead of the 1st by default).\n                Not important if either batch_rank or time_rank are not set. Use None to leave the property as is.\n\n        Returns:\n            Space: The deepcopy of this Space, but with `has_batch_rank` set to True.\n        """"""\n        # Spare deepcopying the op_rec_ref and parent Space (keeping them would lead to infinite copy cycles).\n        if hasattr(self, ""op_rec_ref""):\n            op_rec_ref_save = self.op_rec_ref\n            parent_safe = self.parent\n            # TODO: Remove logger from Specifyable. Only those children of Specifyable that really need logging (Workers and Agents)\n            # TODO: Should have their own logger object and then handle deepcopying properly.\n            #logger_safe = self.logger\n            self.parent = self.op_rec_ref = None\n\n        ret = copy.deepcopy(self)\n\n        if hasattr(self, ""op_rec_ref""):\n            self.op_rec_ref = op_rec_ref_save\n            self.parent = parent_safe\n            #self.logger = logger_safe\n\n        # Add the necessary special ranks.\n        if add_batch_rank is not None:\n            ret._add_batch_rank(add_batch_rank)\n        if add_time_rank is not None:\n            ret._add_time_rank(\n                add_time_rank, time_major if time_major is not None else self.time_major\n            )\n        return ret\n\n    def with_batch_rank(self, add_batch_rank=True):\n        """"""\n        Returns a deepcopy of this Space, but with `has_batch_rank` set to the provided value.\n\n        Args:\n            add_batch_rank (Union[bool,int]): The fixed size of the batch-rank or True or False.\n\n        Returns:\n            Space: The deepcopy of this Space, but with `has_batch_rank` set to True.\n        """"""\n        return self.with_extra_ranks(add_batch_rank=add_batch_rank, add_time_rank=None)\n\n    def with_time_rank(self, add_time_rank=True):\n        """"""\n        Returns a deepcopy of this Space, but with `has_time_rank` set to the provided value.\n\n        Args:\n            add_time_rank (Union[bool,int]): The fixed size of the time-rank or True or False.\n\n        Returns:\n            Space: The deepcopy of this Space, but with `has_time_rank` set to True.\n        """"""\n        return self.with_extra_ranks(add_batch_rank=None, add_time_rank=add_time_rank)\n\n    def force_batch(self, samples, horizontal=False):\n        """"""\n        Makes sure that `samples` is always returned with a batch rank no matter whether\n        it already has one or not (in which case this method returns a batch of 1) or\n        whether this Space has a batch rank or not. Optionally horizontalizes the given sample.\n\n        Args:\n            samples (any): The samples to be batched. If already batched, return as-is.\n\n            horizontal (Optional[bool]): For containers, whether the output should be a batch of\n                containers (horizontal=True) or a container of batched data (horizontal=False).\n                Default: None (for non-BoxSpaces) or False (for ContainerSpaces).\n\n        Returns:\n            tuple\n                - any: The batched sample.\n                - bool: True, if batch rank of 1 had to be added, False if `samples` was already batched.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def shape(self):\n        """"""\n        Returns:\n            tuple: The shape of this Space as a tuple. Without batch or time ranks.\n        """"""\n        return self._shape\n\n    def get_shape(self, with_batch_rank=False, with_time_rank=False, time_major=None, **kwargs):\n        """"""\n        Returns the shape of this Space as a tuple with certain additional ranks at the front (batch) or the back\n        (e.g. categories).\n\n        Args:\n            with_batch_rank (Union[bool,int]): Whether to include a possible batch-rank as `None` at 0th (or 1st)\n                position. If `with_batch_rank` is an int (e.g. -1), the possible batch-rank is returned as that number\n                (instead of None) at the 0th (or 1st if time_major is True) position.\n                Default: False.\n\n            with_time_rank (Union[bool,int]): Whether to include a possible time-rank as `None` at 1st (or 0th)\n                position. If `with_time_rank` is an int, the possible time-rank is returned as that number\n                (instead of None) at the 1st (or 0th if time_major is True) position.\n                Default: False.\n\n            time_major (bool): Overwrites `self.time_major` if not None. Default: None (use `self.time_major`).\n\n        Returns:\n            tuple: The shape of this Space as a tuple.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def rank(self):\n        """"""\n        Returns:\n            int: The rank of the Space not including batch- or time-ranks\n            (e.g. 3 for a space with shape=(10, 7, 5)).\n        """"""\n        return len(self.shape)\n\n    @property\n    def flat_dim(self):\n        """"""\n        Returns:\n            int: The length of a flattened vector derived from this Space.\n        """"""\n        raise NotImplementedError\n\n    def get_variable(self, name, is_input_feed=False, add_batch_rank=None, add_time_rank=None,\n                     time_major=False, is_python=False, local=False, **kwargs):\n        """"""\n        Returns a backend-specific variable/placeholder that matches the space\'s shape.\n\n        Args:\n            name (str): The name for the variable.\n\n            is_input_feed (bool): Whether the returned object should be an input placeholder,\n                instead of a full variable.\n\n            add_batch_rank (Optional[bool,int]): If True, will add a 0th (or 1st) rank (None) to\n                the created variable. If it is an int, will add that int (-1 means None).\n                If None, will use the Space\'s default value: `self.has_batch_rank`.\n                Default: None.\n\n            add_time_rank (Optional[bool,int]): If True, will add a 1st (or 0th) rank (None) to\n                the created variable. If it is an int, will add that int (-1 means None).\n                If None, will use the Space\'s default value: `self.has_time_rank`.\n                Default: None.\n\n            time_major (bool): Only relevant if both `add_batch_rank` and `add_time_rank` are True.\n                Will make the time-rank the 0th rank and the batch-rank the 1st rank.\n                Otherwise, batch-rank will be 0th and time-rank will be 1st.\n                Default: False.\n\n            is_python (bool): Whether to create a python-based variable (list) or a backend-specific one.\n\n            local (bool): Whether the variable must not be shared across the network.\n                Default: False.\n\n        Keyword Args:\n            To be passed on to backend-specific methods (e.g. trainable, initializer, etc..).\n\n        Returns:\n            any: A Tensor Variable/Placeholder.\n        """"""\n        raise NotImplementedError\n\n    def flatten(self, mapping=None, custom_scope_separator=\'/\', scope_separator_at_start=True,\n                return_as_dict_space=False,\n                scope_=None, list_=None):\n        """"""\n        A mapping function to flatten this Space into an OrderedDict whose only values are\n        primitive (non-container) Spaces. The keys are created automatically from Dict keys and\n        Tuple indexes.\n\n        Args:\n            mapping (Optional[callable]): A mapping function that takes a flattened auto-generated key and a primitive\n                Space and converts the primitive Space to something else. Default is pass through.\n\n            custom_scope_separator (str): The separator to use in the returned dict for scopes.\n                Default: \'/\'.\n\n            scope_separator_at_start (bool): Whether to add the scope-separator also at the beginning.\n                Default: False.\n\n            return_as_dict_space (bool): Whether to return a Dict space or as OrderedDict.\n                Default: False.\n\n            scope_ (Optional[str]): For recursive calls only. Used for automatic key generation.\n\n            list_ (Optional[list]): For recursive calls only. The list so far.\n\n        Returns:\n            OrderedDict: The OrderedDict using auto-generated keys and containing only primitive Spaces\n                (or whatever the mapping function maps the primitive Spaces to).\n        """"""\n        # default: no mapping\n        if mapping is None:\n            def mapping(key, x):\n                return x\n\n        # Are we in the non-recursive (first) call?\n        ret = False\n        if list_ is None:\n            list_ = []\n            ret = True\n            scope_ = """"\n\n        self._flatten(\n            mapping, custom_scope_separator=custom_scope_separator,\n            scope_separator_at_start=scope_separator_at_start, return_as_dict_space=return_as_dict_space,\n            scope_=scope_, list_=list_\n        )\n\n        # Non recursive (first) call -> Return the final flat OrderedDict or a Dict space.\n        if ret:\n            ordered_dict = OrderedDict(list_)\n            if return_as_dict_space:\n                ordered_dict[""type""] = dict\n                return Space.from_spec(ordered_dict)\n            else:\n                return ordered_dict\n\n    def _flatten(self, mapping, custom_scope_separator, scope_separator_at_start, return_as_dict_space, scope_, list_):\n        """"""\n        Base implementation. May be overridden by ContainerSpace classes.\n        Simply sends `self` through the mapping function.\n\n        Args:\n            mapping (callable): The mapping function to use on a primitive (non-container) Space.\n\n            custom_scope_separator (str): The separator to use in the returned dict for scopes.\n                Default: \'/\'.\n\n            scope_separator_at_start (bool): Whether to add the scope-separator also at the beginning.\n                Default: False.\n\n            scope_ (str): The flat-key to use to store the mapped result in list_.\n            list_ (list): The list to append the mapped results to (under key=`scope_`).\n        """"""\n        list_.append(tuple([scope_, mapping(scope_, self)]))\n\n    def map(self, mapping):\n        """"""\n        Maps this space via a given mapping function to another, corresponding Space.\n        Mostly useful for ContainerSpaces.\n\n        Args:\n            mapping (callable): The mapping function to use on each (container) sub-Space.\n\n        Returns:\n            Space: A copy of this Space, but all (container) sub-Spaces are mapped via the given mapping function.\n        """"""\n        raise NotImplementedError\n\n    def __repr__(self):\n        return ""Space(shape="" + str(self.shape) + "")""\n\n    def __eq__(self, other):\n        raise NotImplementedError\n\n    def sample(self, size=None, fill_value=None, **kwargs):\n        """"""\n        Uniformly randomly samples an element from this space. This is for testing purposes, e.g. to simulate\n        a random environment.\n\n        Args:\n            size (Optional[int]): The number of samples or batch size to sample.\n                If size is > 1: Returns a batch of size samples with the 0th rank being the batch rank\n                (even if `self.has_batch_rank` is False).\n                If size is None or (1 and self.has_batch_rank is False): Returns a single sample w/o batch rank.\n                If size is 1 and self.has_batch_rank is True: Returns a single sample w/ the batch rank.\n\n            fill_value (Optional[any]): The number or initializer specifier to fill the sample. Can be used to create\n                a (non-random) sample with a certain fill value in all elements.\n                TODO: support initializer spec-strings like \'normal\', \'truncated_normal\', etc..\n\n        Returns:\n            any: The sampled element(s).\n        """"""\n        raise NotImplementedError\n\n    def zeros(self, size=None):\n        """"""\n        Args:\n            size (Optional): Same as `Space.sample()`.\n\n        Returns:\n            np.ndarray: `size` zero samples where all values are zero and have the correct type.\n        """"""\n        raise NotImplementedError\n\n    def _get_np_shape(self, num_samples=None):\n        """"""\n        Helper to determine, which shape one should pass to the numpy random funcs for sampling from a Space.\n        Depends on `num_samples`, the `shape` of this Space and the `self.has_batch_rank/has_time_rank` settings.\n\n        Args:\n            num_samples (Optional[int,Tuple[int,int]]): Number of samples to pull. If None or 0, pull 1 sample, but\n                without batch/time rank (no matter what the value of `self.has_batch_rank` is).\n                If tuple given, use the given values as time/batch ranks.\n\n        Returns:\n            Tuple[int]: Shape to use for numpy random sampling.\n        """"""\n        # No extra batch/time rank.\n        if num_samples is None or (\n                num_samples == () or num_samples == 1 and not self.has_batch_rank and not self.has_time_rank\n        ):\n            if len(self.shape) == 0:\n                return None\n            else:\n                return self.shape\n        # With one extra rank.\n        elif isinstance(num_samples, int):\n            return (num_samples,) + self.shape\n        # With two extra ranks (given as list or tuple).\n        else:\n            assert isinstance(num_samples, (tuple, list)) and len(num_samples) == 2,\\\n                ""ERROR: num_samples must be int or tuple/list of two ints, but is \'{}\'!"".format(num_samples)\n            return tuple(num_samples) + self.shape\n\n    def contains(self, sample):\n        """"""\n        Checks whether this space contains the given sample. This is more for testing purposes.\n\n        Args:\n            sample: The element to check.\n\n        Returns:\n            bool: Whether sample is a valid member of this space.\n        """"""\n        raise NotImplementedError\n\n    @classmethod\n    def from_spec(cls, spec=None, **kwargs):\n        """"""\n        Handles special case that we are trying to construct a Space from a not-yet ready ""variables:.."" specification.\n        In this case, returns None, in all other cases, constructs the Space from_spec as usual.\n        """"""\n        if isinstance(spec, str) and re.search(r\'^variables:\', spec):\n            return None\n        return super(Space, cls).from_spec(spec, **kwargs)\n\n    # TODO: Same procedure as for DataOpRecords. Maybe unify somehow (common ancestor class: IDable).\n    @staticmethod\n    def get_id():\n        Space._ID += 1\n        return Space._ID\n\n    def get_top_level_container(self):\n        """"""\n        Returns:\n            Space: The top-most container containing this Space. This returned top-level container has no more\n                parents above it.\n        """"""\n        top_level = top_level_check = self\n        while top_level_check is not None:\n            top_level = top_level_check\n            top_level_check = top_level.parent\n        return top_level\n\n    def __hash__(self):\n        return hash(self.id)\n\n'"
rlgraph/spaces/space_utils.py,2,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport re\n\nimport numpy as np\nfrom six.moves import xrange as range_\nfrom rlgraph import get_backend\nfrom rlgraph.spaces.bool_box import BoolBox\nfrom rlgraph.spaces.box_space import BoxSpace\nfrom rlgraph.spaces.containers import ContainerSpace, Dict, Tuple\nfrom rlgraph.spaces.float_box import FloatBox\nfrom rlgraph.spaces.int_box import IntBox\nfrom rlgraph.spaces.text_box import TextBox\nfrom rlgraph.utils.rlgraph_errors import RLGraphError, RLGraphSpaceError\nfrom rlgraph.utils.util import convert_dtype, get_shape, LARGE_INTEGER, force_tuple\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\n# TODO: replace completely by `Component.get_variable` (python-backend)\ndef get_list_registry(from_space, capacity=None, initializer=0, flatten=True, add_batch_rank=False):\n    """"""\n    Creates a list storage for a space by providing an ordered dict mapping space names\n    to empty lists.\n\n    Args:\n        from_space: Space to create registry from.\n        capacity (Optional[int]): Optional capacity to initalize list.\n        initializer (Optional(any)): Optional initializer for list if capacity is not None.\n        flatten (bool): Whether to produce a FlattenedDataOp with auto-keys.\n\n        add_batch_rank (Optional[bool,int]): If from_space is given and is True, will add a 0th rank (None) to\n            the created variable. If it is an int, will add that int instead of None.\n            Default: False.\n\n    Returns:\n        dict: Container dict mapping spaces to empty lists.\n    """"""\n    if flatten:\n        if capacity is not None:\n            var = from_space.flatten(\n                custom_scope_separator=""-"", scope_separator_at_start=False,\n                mapping=lambda k, primitive: [initializer for _ in range_(capacity)]\n            )\n        else:\n            var = from_space.flatten(\n                custom_scope_separator=""-"", scope_separator_at_start=False,\n                mapping=lambda k, primitive: []\n            )\n    else:\n        if capacity is not None:\n            var = [initializer for _ in range_(capacity)]\n        else:\n            var = []\n    return var\n\n\ndef get_space_from_op(op, read_key_hints=False, dtype=None, low=None, high=None):\n    """"""\n    Tries to re-create a Space object given some DataOp (e.g. a tf op).\n    This is useful for shape inference on returned ops after having run through a graph_fn.\n\n    Args:\n        op (DataOp): The op to create a corresponding Space for.\n\n        read_key_hints (bool): If True, tries to read type- and low/high-hints from the pattern of the Dict keys (str).\n            - Preceding ""I_"": IntBox, ""F_"": FloatBox, ""B_"": BoolBox.\n            - Succeeding ""_low=0.0"": Low value.\n            - Succeeding ""_high=1.0"": High value.\n            E.g. Dict key ""F_somekey_low=0.0_high=2.0"" indicates a FloatBox with low=0.0 and high=2.0.\n                 Dict key ""I_somekey"" indicates an intbox with no limits.\n                 Dict key ""I_somekey_high=5"" indicates an intbox with high=5 (values 0-4).\n\n            Default: False.\n\n        dtype (Optional[str]): An optional indicator, what the `dtype` of a BoxSpace should be.\n        low (Optional[int,float]): An optional indicator, what the `low` property for a BoxSpace should be.\n        high (Optional[int,float]): An optional indicator, what the `high` property for a BoxSpace should be.\n\n    Returns:\n        Space: The inferred Space object.\n    """"""\n    # a Dict\n    if isinstance(op, dict):  # DataOpDict\n        spec = {}\n        add_batch_rank = False\n        add_time_rank = False\n        for key, value in op.items():\n            # Try to infer hints from the key.\n            if read_key_hints is True:\n                dtype, low, high = get_space_hints_from_dict_key(key)\n            spec[key] = get_space_from_op(value, dtype=dtype, low=low, high=high)\n            # Return\n            if spec[key] == 0:\n                return 0\n            if spec[key].has_batch_rank:\n                add_batch_rank = True\n            if spec[key].has_time_rank:\n                add_time_rank = True\n        return Dict(spec, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank)\n    # a Tuple\n    elif isinstance(op, tuple):  # DataOpTuple\n        spec = []\n        add_batch_rank = False\n        add_time_rank = False\n        for i in op:\n            space = get_space_from_op(i)\n            if space == 0:\n                return 0\n            spec.append(space)\n            if spec[-1].has_batch_rank:\n                add_batch_rank = True\n            if spec[-1].has_time_rank:\n                add_time_rank = True\n        return Tuple(spec, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank)\n\n    # primitive Space -> infer from op dtype and shape\n    else:\n        low_high = {}\n        if high is not None:\n            low_high[""high""] = high\n        if low is not None:\n            low_high[""low""] = low\n        # Op itself is a single value, simple python type.\n        if isinstance(op, (bool, int, float)):\n            return BoxSpace.from_spec(spec=(dtype or type(op)), shape=(), **low_high)\n        elif isinstance(op, str):\n            raise RLGraphError(""Cannot derive Space from non-allowed op ({})!"".format(op))\n        # A single numpy array.\n        elif isinstance(op, np.ndarray):\n            return BoxSpace.from_spec(spec=convert_dtype(str(op.dtype), ""np""), shape=op.shape, **low_high)\n        elif isinstance(op, list):\n            return try_space_inference_from_list(op, dtype=dtype, **low_high)\n        # No Space: e.g. the tf.no_op, a distribution (anything that\'s not a tensor).\n        # PyTorch Tensors do not have get_shape so must check backend.\n        elif hasattr(op, ""dtype"") is False or (get_backend() == ""tf"" and not hasattr(op, ""get_shape"")):\n            return 0\n        # Some tensor: can be converted into a BoxSpace.\n        else:\n            shape = get_shape(op)\n            # Unknown shape (e.g. a cond op).\n            if shape is None:\n                return 0\n            add_batch_rank = False\n            add_time_rank = False\n            time_major = False\n            new_shape = list(shape)\n\n            # New way: Detect via op._batch_rank and op._time_rank properties where these ranks are.\n            if hasattr(op, ""_batch_rank"") and isinstance(op._batch_rank, int):\n                add_batch_rank = True\n                new_shape[op._batch_rank] = -1\n\n            # elif get_backend() == ""pytorch"":\n            #     if isinstance(op, torch.Tensor):\n            #         if op.dim() > 1 and shape[0] == 1:\n            #             add_batch_rank = True\n            #             new_shape[0] = 1\n            if hasattr(op, ""_time_rank"") and isinstance(op._time_rank, int):\n                add_time_rank = True\n                if op._time_rank == 0:\n                    time_major = True\n                new_shape[op._time_rank] = -1\n            shape = tuple(n for n in new_shape if n != -1)\n\n            # Old way: Detect automatically whether the first rank(s) are batch and/or time rank.\n            if add_batch_rank is False and add_time_rank is False and shape != () and shape[0] is None:\n                if len(shape) > 1 and shape[1] is None:\n                    #raise RLGraphError(\n                    #    ""ERROR: Cannot determine time-major flag if both batch- and time-ranks are in an op w/o saying ""\n                    #    ""which rank goes to which position!""\n                    #)\n                    shape = shape[2:]\n                    add_time_rank = True\n                else:\n                    shape = shape[1:]\n                add_batch_rank = True\n\n            # TODO: If op._batch_rank and/or op._time_rank are not set, set them now.\n\n            base_dtype = op.dtype.base_dtype if hasattr(op.dtype, ""base_dtype"") else op.dtype\n            # PyTorch does not have a bool type\n            if get_backend() == ""pytorch"":\n                if op.dtype is torch.uint8:\n                    base_dtype = bool\n            base_dtype_str = str(base_dtype)\n\n            # FloatBox\n            if ""float"" in base_dtype_str:\n                return FloatBox(shape=shape, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank,\n                                time_major=time_major, dtype=convert_dtype(base_dtype, ""np""))\n            # IntBox\n            elif ""int"" in base_dtype_str:\n                high_ = high or getattr(op, ""_num_categories"", None)\n                return IntBox(high_, shape=shape, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank,\n                              time_major=time_major, dtype=convert_dtype(base_dtype, ""np""))\n            # a BoolBox\n            elif ""bool"" in base_dtype_str:\n                return BoolBox(shape=shape, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank,\n                               time_major=time_major)\n            # a TextBox\n            elif ""string"" in base_dtype_str:\n                return TextBox(shape=shape, add_batch_rank=add_batch_rank, add_time_rank=add_time_rank,\n                               time_major=time_major)\n\n    raise RLGraphError(""ERROR: Cannot derive Space from op \'{}\' (unknown type?)!"".format(op))\n\n\ndef get_space_hints_from_dict_key(key):\n    """"""\n    Args:\n        key (str): The Dict key to analyze for type- and low/high hints.\n\n    Returns:\n        dtype (str), low (numeric), high (numeric): Some of these may be None if no hints were found in the key.\n    """"""\n    # Look for dtype hint.\n    dtype = None\n    mo = re.search(r\'^([IFB])_\', key)\n    if mo is not None:\n        dtype = mo.group(1)\n        dtype = ""float"" if dtype == ""F"" else ""int"" if dtype == ""I"" else ""bool""\n    if dtype == ""bool"":\n        return dtype, None, None\n\n    # Look for low hint.\n    low = None\n    mo = re.search(r\'_low([\\-\\d\\.\\d]+)\', key)\n    if mo is not None:\n        low = mo.group(1)\n        low = float(low) if dtype == ""float"" else int(low)\n\n    # Look for high hint.\n    high = None\n    mo = re.search(r\'_high([\\-\\d\\.\\d]+)\', key)\n    if mo is not None:\n        high = mo.group(1)\n        high = float(high) if dtype == ""float"" else int(high)\n\n    return dtype, low, high\n\n\ndef sanity_check_space(\n        space, allowed_types=None, allowed_sub_types=None, non_allowed_types=None, non_allowed_sub_types=None,\n        must_have_batch_rank=None, must_have_time_rank=None, must_have_batch_or_time_rank=False,\n        must_have_categories=None, num_categories=None,\n        must_have_lower_limit=None, must_have_upper_limit=None,\n        rank=None, shape=None\n):\n    """"""\n    Sanity checks a given Space for certain criteria and raises exceptions if they are not met.\n\n    Args:\n        space (Space): The Space object to check.\n        allowed_types (Optional[List[type]]): A list of types that this Space must be an instance of.\n\n        allowed_sub_types (Optional[List[type]]): For container spaces, a list of sub-types that all\n            flattened sub-Spaces must be an instance of.\n\n        non_allowed_types (Optional[List[type]]): A list of type that this Space must not be an instance of.\n\n        non_allowed_sub_types (Optional[List[type]]): For container spaces, a list of sub-types that all\n            flattened sub-Spaces must not be an instance of.\n\n        must_have_batch_rank (Optional[bool]): Whether the Space must (True) or must not (False) have the\n            `has_batch_rank` property set to True. None, if it doesn\'t matter.\n\n        must_have_time_rank (Optional[bool]): Whether the Space must (True) or must not (False) have the\n            `has_time_rank` property set to True. None, if it doesn\'t matter.\n\n        must_have_batch_or_time_rank (Optional[bool]): Whether the Space must (True) or must not (False) have either\n            the `has_batch_rank` or the `has_time_rank` property set to True.\n\n        must_have_categories (Optional[bool]): For IntBoxes, whether the Space must (True) or must not (False) have\n            global bounds with `num_categories` > 0. None, if it doesn\'t matter.\n\n        num_categories (Optional[int,tuple]): An int or a tuple (min,max) range within which the Space\'s\n            `num_categories` rank must lie. Only valid for IntBoxes.\n            None if it doesn\'t matter.\n\n        must_have_lower_limit (Optional[bool]): If not None, whether this Space must have a lower limit.\n        must_have_upper_limit (Optional[bool]): If not None, whether this Space must have an upper limit.\n\n        rank (Optional[int,tuple]): An int or a tuple (min,max) range within which the Space\'s rank must lie.\n            None if it doesn\'t matter.\n\n        shape (Optional[tuple[int]]): A tuple of ints specifying the required shape. None if it doesn\'t matter.\n\n    Raises:\n        RLGraphSpaceError: If any of the conditions is not met.\n    """"""\n    flattened_space = space.flatten()\n\n    # Check the types.\n    if allowed_types is not None:\n        if not isinstance(space, force_tuple(allowed_types)):\n            raise RLGraphSpaceError(\n                space, ""ERROR: Space ({}) is not an instance of {}!"".format(space, allowed_types)\n            )\n\n    if allowed_sub_types is not None:\n        for flat_key, sub_space in flattened_space.items():\n            if not isinstance(sub_space, force_tuple(allowed_sub_types)):\n                raise RLGraphSpaceError(\n                    sub_space,\n                    ""ERROR: sub-Space \'{}\' ({}) is not an instance of {}!"".\n                    format(flat_key, sub_space, allowed_sub_types)\n                )\n\n    if non_allowed_types is not None:\n        if isinstance(space, force_tuple(non_allowed_types)):\n            raise RLGraphSpaceError(\n                space,\n                ""ERROR: Space ({}) must not be an instance of {}!"".format(space, non_allowed_types)\n            )\n\n    if non_allowed_sub_types is not None:\n        for flat_key, sub_space in flattened_space.items():\n            if isinstance(sub_space, force_tuple(non_allowed_sub_types)):\n                raise RLGraphSpaceError(\n                    sub_space,\n                    ""ERROR: sub-Space \'{}\' ({}) must not be an instance of {}!"".\n                    format(flat_key, sub_space, non_allowed_sub_types)\n                )\n\n    if must_have_batch_or_time_rank is True:\n        if space.has_batch_rank is False and space.has_time_rank is False:\n            raise RLGraphSpaceError(\n                space,\n                ""ERROR: Space ({}) does not have a batch- or a time-rank, but must have either one of ""\n                ""these!"".format(space)\n            )\n\n    if must_have_batch_rank is not None:\n        if (space.has_batch_rank is False and must_have_batch_rank is True) or \\\n                (space.has_batch_rank is not False and must_have_batch_rank is False):\n            # Last chance: Check for rank >= 2, that would be ok as well.\n            if must_have_batch_rank is True and len(space.get_shape(with_batch_rank=True)) >= 2:\n                pass\n            # Something is wrong.\n            elif space.has_batch_rank is not False:\n                raise RLGraphSpaceError(\n                    space,\n                    ""ERROR: Space ({}) has a batch rank, but is not allowed to!"".format(space)\n                )\n            else:\n                raise RLGraphSpaceError(\n                    space,\n                    ""ERROR: Space ({}) does not have a batch rank, but must have one!"".format(space)\n                )\n\n    if must_have_time_rank is not None:\n        if (space.has_time_rank is False and must_have_time_rank is True) or \\\n                (space.has_time_rank is not False and must_have_time_rank is False):\n            # Last chance: Check for rank >= 3, that would be ok as well.\n            if must_have_time_rank is True and len(space.get_shape(with_batch_rank=True, with_time_rank=True)) >= 2:\n                pass\n            # Something is wrong.\n            elif space.has_time_rank is not False:\n                raise RLGraphSpaceError(\n                    space,\n                    ""ERROR: Space ({}) has a time rank, but is not allowed to!"".format(space)\n                )\n            else:\n                raise RLGraphSpaceError(\n                    space,\n                    ""ERROR: Space ({}) does not have a time rank, but must have one!"".format(space)\n                )\n\n    if must_have_categories is not None:\n        for flat_key, sub_space in flattened_space.items():\n            if not isinstance(sub_space, IntBox):\n                raise RLGraphSpaceError(\n                    sub_space,\n                    ""ERROR: Space {}({}) is not an IntBox. Only IntBox Spaces can have categories!"".\n                    format("""" if flat_key == """" else ""\'{}\' "".format(flat_key), space)\n                )\n            elif sub_space.global_bounds is False:\n                raise RLGraphSpaceError(\n                    sub_space,\n                    ""ERROR: Space {}({}) must have categories (globally valid value bounds)!"".\n                    format("""" if flat_key == """" else ""\'{}\' "".format(flat_key), space)\n                )\n\n    if must_have_lower_limit is not None:\n        for flat_key, sub_space in flattened_space.items():\n            low = sub_space.low\n            if must_have_lower_limit is True and (low == -LARGE_INTEGER or low == float(""-inf"")):\n                raise RLGraphSpaceError(\n                    sub_space,\n                    ""ERROR: Space {}({}) must have a lower limit, but has none!"".\n                    format("""" if flat_key == """" else ""\'{}\' "".format(flat_key), space)\n                )\n            elif must_have_lower_limit is False and (low != -LARGE_INTEGER and low != float(""-inf"")):\n                raise RLGraphSpaceError(\n                    sub_space,\n                    ""ERROR: Space {}({}) must not have a lower limit, but has one ({})!"".\n                    format("""" if flat_key == """" else ""\'{}\' "".format(flat_key), space, low)\n                )\n\n    if must_have_upper_limit is not None:\n        for flat_key, sub_space in flattened_space.items():\n            high = sub_space.high\n            if must_have_upper_limit is True and (high != LARGE_INTEGER and high != float(""inf"")):\n                raise RLGraphSpaceError(\n                    sub_space,\n                    ""ERROR: Space {}({}) must have an upper limit, but has none!"".\n                    format("""" if flat_key == """" else ""\'{}\' "".format(flat_key), space)\n                )\n            elif must_have_upper_limit is False and (high == LARGE_INTEGER or high == float(""inf"")):\n                raise RLGraphSpaceError(\n                    sub_space,\n                    ""ERROR: Space {}({}) must not have a upper limit, but has one ({})!"".\n                    format("""" if flat_key == """" else ""\'{}\' "".format(flat_key), space, high)\n                )\n\n    if rank is not None:\n        if isinstance(rank, int):\n            for flat_key, sub_space in flattened_space.items():\n                if sub_space.rank != rank:\n                    raise RLGraphSpaceError(\n                        sub_space,\n                        ""ERROR: A Space (flat-key={}) of \'{}\' has rank {}, but must have rank ""\n                        ""{}!"".format(flat_key, space, sub_space.rank, rank)\n                    )\n        else:\n            for flat_key, sub_space in flattened_space.items():\n                if not ((rank[0] or 0) <= sub_space.rank <= (rank[1] or float(""inf""))):\n                    raise RLGraphSpaceError(\n\n                        sub_space,\n                        ""ERROR: A Space (flat-key={}) of \'{}\' has rank {}, but its rank must be between {} and ""\n                        ""{}!"".format(flat_key, space, sub_space.rank, rank[0], rank[1])\n                    )\n\n    if shape is not None:\n        for flat_key, sub_space in flattened_space.items():\n            if sub_space.shape != shape:\n                raise RLGraphSpaceError(\n                    sub_space,\n                    ""ERROR: A Space (flat-key={}) of \'{}\' has shape {}, but its shape must be ""\n                    ""{}!"".format(flat_key, space, sub_space.get_shape(), shape)\n                )\n\n    if num_categories is not None:\n        for flat_key, sub_space in flattened_space.items():\n            if not isinstance(sub_space, IntBox):\n                raise RLGraphSpaceError(\n                    sub_space,\n                    ""ERROR: A Space (flat-key={}) of \'{}\' is not an IntBox. Only IntBox Spaces can have ""\n                    ""categories!"".format(flat_key, space)\n                )\n            elif isinstance(num_categories, int):\n                if sub_space.num_categories != num_categories:\n                    raise RLGraphSpaceError(\n                        sub_space,\n                        ""ERROR: A Space (flat-key={}) of \'{}\' has `num_categories` {}, but must have {}!"".\n                        format(flat_key, space, sub_space.num_categories, num_categories)\n                    )\n            elif not ((num_categories[0] or 0) <= sub_space.num_categories <= (num_categories[1] or float(""inf""))):\n                raise RLGraphSpaceError(sub_space,\n                    ""ERROR: A Space (flat-key={}) of \'{}\' has `num_categories` {}, but this value must be between ""\n                    ""{} and {}!"".format(flat_key, space, sub_space.num_categories, num_categories[0], num_categories[1])\n                )\n\n\ndef check_space_equivalence(space1, space2):\n    """"""\n    Compares the two input Spaces for equivalence and returns the more generic Space of the two.\n    The more generic  Space  is the one that has the properties has_batch_rank and/or has _time_rank set (instead of\n    hard values in these ranks).\n    E.g.: FloatBox((64,)) is equivalent with FloatBox((), +batch-rank). The latter will be returned.\n\n    NOTE: FloatBox((2,)) and FloatBox((3,)) are NOT equivalent.\n\n    Args:\n        space1 (Space): The 1st Space to compare.\n        space2 (Space): The 2nd Space to compare.\n\n    Returns:\n        Union[Space,False]: False is the two spaces are not equivalent. The more generic Space of the two if they are\n            equivalent.\n    """"""\n    # Spaces are the same: Return one of them.\n    if space1 == space2:\n        return space1\n    # One has batch-rank, the other doesn\'t, but has one more rank.\n    elif space1.has_batch_rank and not space2.has_batch_rank and \\\n            (np.asarray(space1.rank) == np.asarray(space2.rank) - 1).all():\n        return space1\n    elif space2.has_batch_rank and not space1.has_batch_rank and \\\n            (np.asarray(space2.rank) == np.asarray(space1.rank) - 1).all():\n        return space2\n    elif get_backend() == ""pytorch"":\n        if not space1.has_batch_rank and not space2.has_batch_rank and \\\n                (np.asarray(space1.rank) == np.asarray(space2.rank)).all():\n            return space1\n\n        # TODO problem is that batch ranks are principally not handled correctly here ->\n        # e.g. (batch_size, 256), (256,) can both be valid between layers\n        if not space1.has_batch_rank and not space2.has_batch_rank and space1.rank > space2.rank:\n            return space2\n        if not space1.has_batch_rank and not space2.has_batch_rank and space1.rank < space2.rank:\n            return space1\n    # TODO: time rank?\n\n    return False\n\n\ndef try_space_inference_from_list(list_op, dtype=None, **low_high):\n    """"""\n    Attempts to infer shape space from a list op. A list op may be the result of fetching state from a Python\n    memory.\n\n    Args:\n        list_op (list): List with arbitrary sub-structure.\n\n    Returns:\n        Space: Inferred Space object represented by list.\n    """"""\n    shape = len(list_op)\n    if shape > 0:\n        # Try to infer more things by looking inside list.\n        elem = list_op[0]\n        if (get_backend() == ""pytorch"" and isinstance(elem, torch.Tensor)) or \\\n                get_backend() == ""tf"" and isinstance(elem, tf.Tensor):\n            list_type = dtype or elem.dtype\n            inner_shape = elem.shape\n            return BoxSpace.from_spec(spec=convert_dtype(list_type, ""np""), shape=(shape,) + inner_shape,\n                                      add_batch_rank=True, **low_high)\n        elif isinstance(elem, list):\n            inner_shape = len(elem)\n            return BoxSpace.from_spec(spec=convert_dtype(dtype or float, ""np""), shape=(shape, inner_shape),\n                                      add_batch_rank=True, **low_high)\n        # IntBox -> elem must be int and dtype hint must match (or None).\n        elif isinstance(elem, int) and (dtype is None or dtype == ""int""):\n            # In case of missing comma values, check all other items in list for float.\n            # If one float in there -> FloatBox, otherwise -> IntBox.\n            has_floats = any(isinstance(el, float) for el in list_op)\n            if has_floats is False:\n                return IntBox.from_spec(shape=(shape,), add_batch_rank=True, **low_high)\n            else:\n                return FloatBox.from_spec(shape=(shape,), add_batch_rank=True, **low_high)\n        # FloatBox -> elem must be float (or int) and dtype hint must match (or None).\n        elif isinstance(elem, (float, int)) and (dtype is None or dtype == ""float""):\n            return FloatBox.from_spec(shape=(shape,), add_batch_rank=True, **low_high)\n\n    # Most general guess is a Float box.\n    return FloatBox(shape=(shape,), **low_high)\n\n\ndef get_default_distribution_from_space(\n        space, bounded_distribution_type=""beta"", discrete_distribution_type=""categorical"",\n        gumbel_softmax_temperature=1.0\n):\n    """"""\n    Args:\n        space (Space): The primitive Space for which to derive a default distribution spec.\n        bounded_distribution_type (str): The lookup class string for a bounded FloatBox distribution.\n            Default: ""beta"".\n        discrete_distribution_type(str): The class of distributions to use for discrete action spaces. For options\n            check the components.distributions package. Default: categorical. Agents requiring reparameterization\n            may require a GumbelSoftmax distribution instead.\n        gumbel_softmax_temperature (float): Temperature parameter for the Gumbel-Softmax distribution used\n            for discrete actions.\n\n    Returns:\n        Dict: A Spec dict, from which a valid default distribution object can be created.\n    """"""\n    # IntBox: Categorical.\n    if isinstance(space, IntBox):\n        if discrete_distribution_type == ""gumbel_softmax"":\n            return dict(type=""gumbel-softmax"", temperature=gumbel_softmax_temperature)\n        else:\n            return dict(type=discrete_distribution_type)\n    # BoolBox: Bernoulli.\n    elif isinstance(space, BoolBox):\n        return dict(type=""bernoulli"")\n    # Continuous action space: Normal/Beta/etc. distribution.\n    elif isinstance(space, FloatBox):\n        # Unbounded -> Normal distribution.\n        if not is_bounded_space(space):\n            return dict(type=""normal"")\n        # Bounded -> according to the bounded_distribution parameter.\n        else:\n            return dict(type=bounded_distribution_type, low=space.low, high=space.high)\n    # Container Space.\n    elif isinstance(space, ContainerSpace):\n        return dict(type=""joint-cumulative"", distribution_specs=dict(\n            {k: get_default_distribution_from_space(s) for k, s in space.flatten().items()}\n        ))\n    else:\n        raise RLGraphError(""No distribution defined for space {}!"".format(space))\n\n\ndef is_bounded_space(box_space):\n    if not isinstance(box_space, FloatBox):\n        return False\n    # Unbounded.\n    if box_space.low == float(""-inf"") and box_space.high == float(""inf""):\n        return False\n    # Bounded.\n    elif box_space.low != float(""-inf"") and box_space.high != float(""inf""):\n        return True\n    # TODO: Semi-bounded -> Exponential distribution.\n    else:\n        raise RLGraphError(\n            ""Semi-bounded spaces for distribution-generation are not supported yet! You passed in low={} high={}."".\n            format(box_space.low, box_space.high)\n        )\n\n\ndef horizontalize_space_sample(space, sample, batch_size=1):\n    # For Dicts, we have to treat each key as an array with batch-rank at index 0.\n    # The dict is then translated into a list of dicts where each dict contains the original data\n    # but without the batch-rank.\n    # E.g. {\'A\': array([0, 1]), \'B\': array([2, 3])} -> [{\'A\': 0, \'B\': 2}, {\'A\': 1, \'B\': 3}]\n    if isinstance(space, Dict):\n        some_key = next(iter(sample))\n        assert isinstance(sample, dict) and isinstance(sample[some_key], np.ndarray), \\\n            ""ERROR: Cannot flip Dict batch with dict keys if returned value is not a dict OR "" \\\n            ""values of returned value are not np.ndarrays!""\n        # TODO: What if actions come as nested dicts (more than one level deep)?\n        # TODO: Use DataOpDict/Tuple\'s new `map` method.\n        if hasattr(sample[some_key], ""__len__""):\n            result = [{key: value[i] for key, value in sample.items()} for i in range(len(sample[some_key]))]\n        else:\n            # Action was not array type.\n            result = [{key: value for key, value in sample.items()}]\n    # Tuple:\n    # E.g. Tuple(array([0, 1]), array([2, 3])) -> [(0, 2), (1, 3)]\n    elif isinstance(space, Tuple):\n        assert isinstance(sample, tuple) and isinstance(sample[0], np.ndarray), \\\n            ""ERROR: Cannot flip tuple batch if returned value is not a tuple OR "" \\\n            ""values of returned value are not np.ndarrays!""\n        # TODO: Use DataOpDict/Tuple\'s new `map` method.\n        result = [tuple(value[i] for _, value in enumerate(sample)) for i in range(len(sample[0]))]\n    # No container batch-flipping necessary.\n    else:\n        result = sample\n        if batch_size == 1 and result.shape == ():\n            result = [result]\n\n    return result\n'"
rlgraph/spaces/text_box.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph.spaces.box_space import BoxSpace\n\n\n# import random\n# import string\n\n\nclass TextBox(BoxSpace):\n    """"""\n    A text box in TXT^n where the shape means the number of text chunks in each dimension.\n    A text chunk can consist of any number of words.\n    """"""\n    def __init__(self, shape=(), **kwargs):\n        """"""\n        Args:\n            shape (tuple): The shape of this space.\n        """"""\n        # Set both low/high to 0 (make no sense for text).\n        super(TextBox, self).__init__(low=0, high=0, **kwargs)\n\n        # Set dtype to numpy\'s unicode type.\n        self.dtype = np.unicode_\n\n        assert isinstance(shape, tuple), ""ERROR: `shape` must be a tuple.""\n        self._shape = shape\n\n    def sample(self, size=None, fill_value=None):\n        shape = self._get_np_shape(num_samples=size)\n\n        # TODO: Make it such that it doesn\'t only produce number strings (using `petname` module?).\n        sample_ = np.full(shape=shape, fill_value=fill_value, dtype=self.dtype)\n\n        return sample_.astype(self.dtype)\n\n    def contains(self, sample):\n        sample_shape = sample.shape if not isinstance(sample, str) else ()\n        return sample_shape == self.shape\n'"
rlgraph/tests/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.tests.agent_test import AgentTest\nfrom rlgraph.tests.component_test import ComponentTest\nfrom rlgraph.tests.test_util import recursive_assert_almost_equal, config_from_path\nfrom rlgraph.tests.dummy_components import *\nfrom rlgraph.tests.dummy_components_with_sub_components import *\n\n\n__all__ = [\n    ""recursive_assert_almost_equal"", ""config_from_path"",\n    ""AgentTest"", ""ComponentTest"",\n    # Simple dummies.\n    ""Dummy0To1"", ""Dummy1To1"", ""Dummy1To2"", ""Dummy2To1"", ""Dummy2GraphFns1To1"",\n    ""DummyWithVar"", ""SimpleDummyWithVar"", ""DummyWithOptimizer"",\n    ""FlattenSplitDummy"", ""NoFlattenNoSplitDummy"",  ""OnlyFlattenDummy"",\n    # Dummies with sub-components.\n    ""DummyWithSubComponents"", ""DummyCallingSubComponentsAPIFromWithinGraphFn"",\n    ""DummyNNWithDictInput"", ""DummyProducingInputIncompleteBuild""\n]\n'"
rlgraph/tests/agent_test.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.utils import root_logger\nfrom rlgraph.tests.test_util import recursive_assert_almost_equal\nfrom rlgraph.execution.worker import Worker\n\n\nclass AgentTest(object):\n    """"""\n    A simple (and limited) Agent-wrapper to test an Agent in an easy, straightforward way.\n    """"""\n    def __init__(self, worker, seed=10, logging_level=None, enable_profiler=False):\n        """"""\n        Args:\n            worker (Worker): The Worker (holding the Env and Agent) to use for stepping.\n            #seed (Optional[int]): The seed to use for random-seeding the Model object.\n            #    If None, do not seed the Graph (things may behave non-deterministically).\n            logging_level (Optional[int]): When provided, sets RLGraph\'s root_logger\'s logging level to this value.\n            enable_profiler (Optional(bool)): When enabled, activates backend profiling.\n        """"""\n        self.worker = worker\n        self.agent = self.worker.agent\n        self.env = self.worker.vector_env.get_env()\n        self.seed = seed\n        if logging_level is not None:\n            root_logger.setLevel(logging_level)\n\n        # Simply use the Agent\'s GraphExecutor.\n        self.graph_executor = self.agent.graph_executor\n\n    def step(self, num_timesteps=1, use_exploration=False, frameskip=None, reset=False):\n        """"""\n        Performs n steps in the environment, picking up from where the Agent/Environment was before (no reset).\n\n        Args:\n            num_timesteps (int): How many time steps to perform using the Worker.\n            use_exploration (Optional[bool]): Indicates whether to utilize exploration (epsilon or noise based)\n                when picking actions. Default: False (b/c we are testing).\n            frameskip (Union[int,None]): Number of repeated (same) actions in one ""step"".\n            reset (bool): Whether to reset the previous run(s) and start from scratch.\n                Default: False. Picks up from a previous `step` (even if in the middle of an episode).\n\n        Returns:\n            dict: The stats dict returned by the worker after num_timesteps have been taken.\n        """"""\n        # Perform n steps and return stats.\n        return self.worker.execute_timesteps(num_timesteps=num_timesteps, use_exploration=use_exploration,\n                                             frameskip=frameskip, reset=reset)\n\n    def check_env(self, prop, expected_value, decimals=7):\n        """"""\n        Checks a property of our environment for (almost) equality.\n\n        Args:\n            prop (str): The name of the Environment\'s property to check.\n            expected_value (any): The expected value of the given property.\n            decimals (Optional[int]): The number of digits after the floating point up to which to compare actual\n                and expected values.\n        """"""\n        is_value = getattr(self.env, prop, None)\n        recursive_assert_almost_equal(is_value, expected_value, decimals=decimals)\n\n    def check_agent(self, prop, expected_value, decimals=7, key_or_index=None):\n        """"""\n        Checks a property of our Agent for (almost) equality.\n\n        Args:\n            prop (str): The name of the Agent\'s property to check.\n            expected_value (any): The expected value of the given property.\n            decimals (Optional[int]): The number of digits after the floating point up to which to compare actual\n                and expected values.\n            key_or_index (Optional[int, str]): Optional key or index into the propery in case of nested data structure.\n        """"""\n        is_value = getattr(self.agent, prop, None)\n        if key_or_index is not None:\n            is_value = is_value[key_or_index]\n        recursive_assert_almost_equal(is_value, expected_value, decimals=decimals)\n\n    def check_var(self, variable, expected_value, decimals=7):\n        """"""\n        Checks a value of our an Agent\'s variable for (almost) equality against an expected one.\n\n        Args:\n            variable (str): The global scope (within Agent\'s root-component) of the variable to check.\n            expected_value (any): The expected value of the given variable.\n            decimals (Optional[int]): The number of digits after the floating point up to which to compare actual\n                and expected values.\n        """"""\n        variables_dict = self.agent.root_component.variable_registry\n        assert variable in variables_dict, ""ERROR: Variable \'{}\' not found in Agent \'{}\'!"".\\\n            format(variable, self.agent.name)\n        var = variables_dict[variable]\n        value = self.graph_executor.read_variable_values(var)\n        recursive_assert_almost_equal(value, expected_value, decimals=decimals)\n'"
rlgraph/tests/component_test.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\n\nimport numpy as np\nfrom rlgraph import get_backend\nfrom rlgraph.graphs import GraphBuilder\nfrom rlgraph.graphs.graph_executor import GraphExecutor\nfrom rlgraph.tests.test_util import recursive_assert_almost_equal\nfrom rlgraph.utils import root_logger, PyTorchVariable\nfrom rlgraph.utils.input_parsing import parse_execution_spec\n\n\nclass ComponentTest(object):\n    """"""\n    A simple (and limited) Graph-wrapper to test a single component in an easy, straightforward way.\n    """"""\n    def __init__(\n        self,\n        component,\n        input_spaces=None,\n        action_space=None,\n        seed=10,\n        logging_level=None,\n        execution_spec=None,\n        # TODO: Move all the below into execution_spec just like for Agent class.\n        enable_profiler=False,\n        disable_monitoring=False,\n        device_strategy=""default"",\n        device_map=None,\n        backend=None,\n        auto_build=True,\n        build_kwargs=None\n    ):\n        """"""\n        Args:\n            component (Component): The Component to be tested (may contain sub-components).\n            input_spaces (Optional[dict]): Dict with component\'s API input-parameter\' names as keys and Space objects\n                or Space specs as values. Describes the input Spaces for the component.\n                None, if the Component to be tested has no API methods with input parameters.\n            action_space (Optional[Space]): The action space to pass into the GraphBuilder.\n            seed (Optional[int]): The seed to use for random-seeding the Model object.\n                If None, do not seed the Graph (things may behave non-deterministically).\n            logging_level (Optional[int]): When provided, sets RLGraph\'s root_logger\'s logging level to this value.\n            execution_spec (Optional[dict]): Specification dict for execution settings.\n            enable_profiler (bool): When enabled, activates backend profiling. Default: False.\n            disable_monitoring (bool): When True, will not use a monitored session. Default: False.\n            device_strategy (str): Optional device-strategy to be passed into GraphExecutor.\n            device_map (Optional[Dict[str,str]]): Optional device-map to be passed into GraphExecutor.\n            backend (Optional[str]): Override global backend settings for a test by passing in a specific\n                backend, convenience method.\n            auto_build (Optional[bool]): If false, build has to be triggered manually to eval build stats.\n            build_kwargs (Optional[dict]): Dict to be passed as **kwargs to the call to `self.graph_executor.build`.\n        """"""\n        self.seed = seed\n        np.random.seed(seed)\n        random.seed(seed)\n\n        if logging_level is not None:\n            root_logger.setLevel(logging_level)\n\n        # Create a GraphBuilder.\n        self.graph_builder = GraphBuilder(action_space=action_space)\n        self.component = component\n        self.component.nesting_level = 0\n        self.input_spaces = input_spaces\n        self.build_kwargs = build_kwargs or dict()\n\n        # Build the model.\n        execution_spec = parse_execution_spec(execution_spec or dict(\n            seed=self.seed,\n            enable_profiler=enable_profiler,\n            profiler_frequency=1,\n            device_strategy=device_strategy,\n            disable_monitoring=disable_monitoring,\n            device_map=device_map\n        ))\n        use_backend = backend if backend is not None else get_backend()\n        self.graph_executor = GraphExecutor.from_spec(\n            use_backend,\n            graph_builder=self.graph_builder,\n            execution_spec=execution_spec\n        )\n        if auto_build:\n            self.build()\n        else:\n            print(""Auto-build false, did not build. Waiting for manual build."")\n\n    def build(self):\n        return self.graph_executor.build([self.component], self.input_spaces, **self.build_kwargs)\n\n    def test(self, *api_method_calls, **kwargs):\n        """"""\n        Does one test pass through the component to test.\n\n        Args:\n            api_method_calls (Union[str,list,tuple]): See rlgraph.graphs.graph_executor for details.\n            A specifier for an API-method call.\n                - str: Call the API-method that has the given name w/o any input args.\n                - tuple len=2: 0=the API-method name to call; 1=the input args to use for the call.\n                - tuple len=3: same as len=2, AND 2=list of returned op slots to pull (e.g. [0]: only pull\n                    the first op).\n\n        Keyword Args:\n            expected_outputs (Optional[any]): The expected return value(s) generated by the API-method.\n                If None, no checks will be done on the output.\n            decimals (Optional[int]): The number of digits after the floating point up to which to compare actual\n                outputs and expected values.\n            fn_test (Optional[callable]): Test function to call with (self, outs) as parameters.\n            print (bool): Whether to print out the actual results (before doing any checks on the results).\n\n        Returns:\n            any: The actual returned values when calling the API-method with the given parameters.\n        """"""\n        expected_outputs = kwargs.pop(""expected_outputs"", None)\n        decimals = kwargs.pop(""decimals"", 7)\n        fn_test = kwargs.pop(""fn_test"", None)\n        print_ = kwargs.pop(""print"", False)\n        assert not kwargs\n\n        # Get the outs ..\n        outs = self.graph_executor.execute(*api_method_calls)\n\n        if print_ is True:\n            print(""Results:\\n{}"".format(repr(outs)))\n\n        #  Optionally do test asserts here.\n        if expected_outputs is not None:\n            self.assert_equal(outs, expected_outputs, decimals=decimals)\n\n        if callable(fn_test):\n            fn_test(self, outs)\n\n        return outs\n\n    def variable_test(self, variables, expected_values):\n        """"""\n        Asserts that all given `variables` have the `expected_values`.\n        Variables can be given in an arbitrary structure including nested ones.\n\n        Args:\n            variables (any): Any structure that contains variables.\n            expected_values (any): Matching structure with the expected values for the given variables.\n        """"""\n        values = self.read_variable_values(variables)\n        self.assert_equal(values, expected_values)\n\n    def read_variable_values(self, *variables):\n        """"""\n        Executes a session to retrieve the values of the provided variables.\n\n        Args:\n            variables (Union[variable,List[variable]]): Variable objects whose values to retrieve from the graph.\n\n        Returns:\n            any: Values of the variables provided.\n        """"""\n        # No variables given: Read all variables of our component.\n        if len(variables) == 0:\n            variables = self.component.variable_registry\n        ret = self.graph_executor.read_variable_values(variables)\n        if len(variables) == 1:\n            return ret[0]\n        return ret\n\n    def get_variable_values(self, component, names):\n        """"""\n        Reads value of component state for given component and names.\n\n        Args:\n            component (Component):\n            *names (list): List of strings.\n\n        Returns:\n            Dict: Variable values.\n        """"""\n        variables = component.get_variables(names, global_scope=False)\n        if get_backend() == ""tf"":\n            return self.graph_executor.read_variable_values(variables)\n        else:\n            return variables\n\n    @staticmethod\n    def read_params(name, params, transpose_torch_params=True):\n        """"""\n        Tries to read name from params. Name may be either an actual key or a prefix of a key.\n\n        Args:\n            name (str): Key or prefix to key.\n            params (dict): Params to read.\n            transpose_torch_params (bool): If the parameter lookup yields a torch parameter and this argument is True,\n                transpose the result. Accounts for different internal data layouts.\n        Returns:\n            any: Param value for key.\n\n        Raises:\n            ValueError: If no key can be found.\n        """"""\n        param = ComponentTest.read_prefixed_params(name, params)\n        if isinstance(param, PyTorchVariable):\n            # Weights require gradients -> detach.\n            weight = param.get_value().detach().numpy()\n\n            # PyTorch and TF\n            if transpose_torch_params:\n                return weight.transpose()\n            else:\n                return weight\n        else:\n            return param\n\n    @staticmethod\n    def read_prefixed_params(name, params):\n        """"""\n        Tries to read name from params. Name may be either an actual key or a prefix of a key\n\n        Args:\n            name (str): Key or prefix to key.\n            params (dict): Params to read.\n\n        Returns:\n            any: Param value for key.\n\n        Raises:\n            ValueError: If no key can be found.\n        """"""\n        if name in params:\n            return params[name]\n\n        # Otherwise return first matching key.\n        for key in params.keys():\n            if key.startswith(name):\n                return params[key]\n            # Catch key ending with extra scope separator\n            elif name[-1] == ""/"" and key.startswith(name[:-1]):\n                return params[key]\n\n        raise ValueError(""No value found for key = {}. Keys are: {}"".format(name, params.keys()))\n\n    @staticmethod\n    def assert_equal(outs, expected_outputs, decimals=7):\n        """"""\n        Convenience wrapper: See implementation of `recursive_assert_almost_equal` for details.\n        """"""\n        recursive_assert_almost_equal(outs, expected_outputs, decimals=decimals)\n\n    def terminate(self):\n        """"""\n        Terminates this ComponentTest object (so it can no longer be used) allowing for cleanup\n        operations to be executed.\n        """"""\n        self.graph_executor.terminate()\n'"
rlgraph/tests/dummy_components.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.components.optimizers.local_optimizers import GradientDescentOptimizer\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.ops import FlattenedDataOp\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass Dummy1To1(Component):\n    def __init__(self, scope=""dummy-1-to-1"", constant_value=1.0, **kwargs):\n        """"""\n        Args:\n            constant_value (float): A constant to add to input in our graph_fn.\n        """"""\n        super(Dummy1To1, self).__init__(scope=scope, **kwargs)\n        self.constant_value = constant_value\n\n    @rlgraph_api(name=""run"", returns=1)\n    def _graph_fn_1to1(self, input_):\n        """"""\n        Returns:\n            `output` (SingleDataOp): Result of input_ + `self.constant_value`.\n        """"""\n        return input_ + self.constant_value\n\n\nclass Dummy2To1(Component):\n    """"""\n    API:\n        run(input1, input2): Result of input1 + input2.\n    """"""\n    def __init__(self, scope=""dummy-2-to-1""):\n        super(Dummy2To1, self).__init__(scope=scope)\n\n    @rlgraph_api(name=""run"", returns=1)\n    def _graph_fn_2to1(self, input1, input2):\n        return input1 + input2\n\n\nclass Dummy1To2(Component):\n    """"""\n    API:\n        run(input1): (input + `self.constant_value`, input * `self.constant_value`).\n    """"""\n    def __init__(self, scope=""dummy-1-to-2"", constant_value=1.0):\n        super(Dummy1To2, self).__init__(scope=scope)\n        self.constant_value = constant_value\n\n    @rlgraph_api(name=""run"", returns=2)\n    def _graph_fn_1to2(self, input_):\n        return input_ + self.constant_value, input_ * self.constant_value\n\n\nclass Dummy2To2(Component):\n    """"""\n    API:\n        run(input1, input2): (input1 + `self.constant_value`, input2 * `self.constant_value`).\n    """"""\n    def __init__(self, scope=""dummy-2-to-2"", constant_value=1.0):\n        super(Dummy2To2, self).__init__(scope=scope)\n        self.constant_value = constant_value\n\n    @rlgraph_api(name=""run"", returns=2)\n    def _graph_fn_2to2(self, input1, input2):\n        return input1 + self.constant_value, input2 * self.constant_value\n\n\nclass Dummy2To2WithDefaultValue(Component):\n    def __init__(self, scope=""dummy-2-to-2-w-default-value"", constant_value=1.0):\n        super(Dummy2To2WithDefaultValue, self).__init__(scope=scope)\n        self.constant_value = constant_value\n\n    @rlgraph_api(name=""run"", returns=2)\n    def _graph_fn_2to2(self, input1, input2=None):\n        return input1 + self.constant_value, (input2 or 5.0) * self.constant_value\n\n\nclass Dummy3To1WithDefaultValues(Component):\n    """"""\n    Tests forwarding a `None` arg in the middle of the API-call signature correctly to a graph_fn.\n    """"""\n    def __init__(self, scope=""dummy-3-to-1-w-default-value""):\n        super(Dummy3To1WithDefaultValues, self).__init__(scope=scope)\n\n    @rlgraph_api\n    def run(self, input1, input2=1.0, input3=None):\n        return self._graph_fn_run(input1, input2, input3)\n\n    @rlgraph_api\n    def run2(self, input1, input3=None, input4=1.0):\n        return self._graph_fn_run(input1, input4, input3)\n\n    @graph_fn\n    def _graph_fn_run(self, input1, input2, input3=None):\n        if input3 is None:\n            return input1 + input2\n        return input3\n\n\nclass Dummy0To1(Component):\n    """"""\n    A dummy component with one graph_fn without api_methods and one output.\n\n    API:\n        run() -> fixed value stored in a variable\n    """"""\n    def __init__(self, scope=""dummy-0-to-1"", var_value=1.0):\n        super(Dummy0To1, self).__init__(scope=scope)\n        self.var_value = var_value\n        self.var = None\n\n    def create_variables(self, input_spaces, action_space=None):\n        self.var = self.get_variable(name=""var"", initializer=self.var_value)\n\n    @rlgraph_api(name=""run"", returns=1)\n    def _graph_fn_0to1(self):\n        return self.var\n\n\nclass Dummy2GraphFns1To1(Component):\n    """"""\n    API:\n        run(input_): Result of input_ + `self.constant_value` - 2x`self.constant_value`\n    """"""\n    def __init__(self, scope=""dummy-2graph_fns-1to1"", constant_value=1.0):\n        """"""\n        Args:\n            constant_value (float): A constant to add to input in our graph_fn.\n        """"""\n        super(Dummy2GraphFns1To1, self).__init__(scope=scope)\n        self.constant_value = constant_value\n\n    @rlgraph_api\n    def run(self, input_):\n        # Explicit definition of an API-method using both our graph_fn.\n        result_1to1 = self._graph_fn_1to1(input_)\n        result_1to1_neg = self._graph_fn_1to1_neg(result_1to1)\n        return result_1to1_neg\n\n    @graph_fn\n    def _graph_fn_1to1(self, input_):\n        return input_ + self.constant_value\n\n    @graph_fn\n    def _graph_fn_1to1_neg(self, input_):\n        return input_ - self.constant_value * 2\n\n\nclass Dummy2NestedGraphFnCalls(Component):\n    """"""\n    API:\n        run(input_): Result of (input_ - `self.constant_value` * 2) + `self.constant_value`\n    """"""\n    def __init__(self, scope=""dummy-2nested-graph_fn-calls"", constant_value=1.0):\n        """"""\n        Args:\n            constant_value (float): A constant to add to input in our graph_fn.\n        """"""\n        super(Dummy2NestedGraphFnCalls, self).__init__(scope=scope)\n        self.constant_value = constant_value\n\n    @rlgraph_api\n    def run(self, input_):\n        result = self._graph_fn_outer(input_)\n        return result\n\n    @graph_fn\n    def _graph_fn_outer(self, input_):\n        intermediate_result = self._graph_fn_inner(input_)\n        return intermediate_result + self.constant_value\n\n    @graph_fn\n    def _graph_fn_inner(self, input_):\n        return input_ - self.constant_value * 2\n\n\nclass DummyThatDefinesCustomAPIMethod(Component):\n    def __init__(self, scope=""dummy-that-defines-custom-api-method"", **kwargs):\n        super(DummyThatDefinesCustomAPIMethod, self).__init__(scope=scope, **kwargs)\n\n        self.define_api_methods()\n\n    def define_api_methods(self):\n        @rlgraph_api(component=self)\n        def some_custom_api_method(self, input_):\n            return self._graph_fn_1to1(input_)\n\n    @graph_fn\n    def _graph_fn_1to1(self, input_):\n        return input_\n\n\nclass DummyThatDefinesCustomGraphFn(Component):\n    def __init__(self, scope=""dummy-that-defines-custom-graph-fn"", **kwargs):\n        super(DummyThatDefinesCustomGraphFn, self).__init__(scope=scope, **kwargs)\n\n        self.define_graph_fns()\n\n    @rlgraph_api\n    def run(self, input_):\n        return self._graph_fn_some_custom_graph_fn(input_)\n\n    def define_graph_fns(self):\n        @graph_fn(component=self)\n        def _graph_fn_some_custom_graph_fn(self, input_):\n            return input_\n\n\nclass DummyWithVar(Component):\n    """"""\n    A dummy component with a couple of sub-components that have their own API methods.\n\n    API:\n        run_plus(input_): input_ + `self.constant_variable`\n        run_minus(input_): input_ - `self.constant_value`\n    """"""\n    def __init__(self, scope=""dummy-with-var"", constant_value=2.0, **kwargs):\n        """"""\n        Args:\n            constant_value (float): A constant to add to input in our graph_fn.\n        """"""\n        super(DummyWithVar, self).__init__(scope=scope, **kwargs)\n        self.constant_value = constant_value\n        self.constant_variable = None\n\n    def create_variables(self, input_spaces, action_space=None):\n        self.constant_variable = self.get_variable(name=""constant-variable"", initializer=2.0)\n\n    @rlgraph_api\n    def run_plus(self, input_):\n        # Explicit definition of an API-method using one of our graph_fn.\n        result = self._graph_fn_1(input_)\n        return result\n\n    @graph_fn(returns=1)\n    def _graph_fn_1(self, input_):\n        return input_ + self.constant_value\n\n    @rlgraph_api(name=""run_minus"", returns=1)\n    def _graph_fn_2(self, input_):\n        return input_ - self.constant_variable\n\n\nclass SimpleDummyWithVar(Component):\n    """"""\n    A simpler dummy component with only one variable and one  graph_fn.\n\n    API:\n        run(input_): input_ + `self.variable`(3.0)\n    """"""\n    def __init__(self, variable_value=3.0, scope=""simple-dummy-with-var"", **kwargs):\n        """"""\n        Args:\n            variable_value (float): The initial value of our variable.\n        """"""\n        super(SimpleDummyWithVar, self).__init__(scope=scope, **kwargs)\n        self.variable = None\n        self.variable_value = variable_value\n\n    def create_variables(self, input_spaces, action_space=None):\n        self.variable = self.get_variable(name=""variable"", initializer=self.variable_value)\n\n    @rlgraph_api\n    def run(self, input_):\n        # Explicit definition of an API-method using one of our graph_fn.\n        result = self._graph_fn_1(input_)\n        return result\n\n    @graph_fn(returns=1)\n    def _graph_fn_1(self, input_):\n        return input_ + self.variable\n\n\nclass DummyWithOptimizer(SimpleDummyWithVar):\n    def __init__(self, variable_value=3.0, learning_rate=0.1, scope=""dummy-with-optimizer"", **kwargs):\n        super(DummyWithOptimizer, self).__init__(variable_value=variable_value, scope=scope, **kwargs)\n\n        assert isinstance(learning_rate, float), ""ERROR: Only float (constant) values allowed in `learning_rate`!""\n\n        self.optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n        self.add_components(self.optimizer)\n\n    @rlgraph_api\n    def calc_grads(self, time_percentage):\n        loss = self._graph_fn_simple_square_loss()\n        return self.optimizer.calculate_gradients(self.variables(), loss, time_percentage)\n\n    @rlgraph_api\n    def step(self, time_percentage):\n        loss = self._graph_fn_simple_square_loss()\n        return self.optimizer.step(self.variables(), loss, loss, time_percentage)\n\n    @graph_fn\n    def _graph_fn_simple_square_loss(self):\n        loss = None\n        if get_backend() == ""tf"":\n            loss = tf.square(x=tf.log(self.variable))\n        return loss\n\n\nclass FlattenSplitDummy(Component):\n    """"""\n    A dummy component with a 2-to-2 graph_fn mapping with flatten/split settings set to True.\n    """"""\n    def __init__(self, scope=""dummy-2-to-2-all-options"", constant_value=1.0, **kwargs):\n        super(FlattenSplitDummy, self).__init__(scope=scope, **kwargs)\n\n        self.constant_value = np.array(constant_value)\n\n    @rlgraph_api(name=""run"", returns=2, flatten_ops=True, split_ops=True)\n    def _graph_fn_2_to_2(self, input1, input2):\n        """"""\n        Returns:\n            Tuple:\n                - in1 + 1.0\n                - in1 + in2\n        """"""\n        return input1 + self.constant_value, input1 + input2\n\n\nclass NoFlattenNoSplitDummy(Component):\n    """"""\n    A dummy component with a 2-to-2 graph_fn mapping with flatten/split settings all set to False.\n    """"""\n    def __init__(self, scope=""dummy-2-to-2-no-options"", **kwargs):\n        super(NoFlattenNoSplitDummy, self).__init__(scope=scope, **kwargs)\n\n    @rlgraph_api(name=""run"", returns=2)\n    def _graph_fn_2_to_2(self, input1, input2):\n        """"""\n        Returns:\n            Tuple:\n                - in2\n                - in1\n        """"""\n        return input2, input1\n\n\nclass OnlyFlattenDummy(Component):\n    """"""\n    A dummy component with a 2-to-3 graph_fn mapping with only flatten_ops=True.\n    """"""\n    def __init__(self, scope=""dummy-2-to-2-all-options"", constant_value=1.0, **kwargs):\n        super(OnlyFlattenDummy, self).__init__(scope=scope, **kwargs)\n\n        self.constant_value = np.array(constant_value)\n\n    @rlgraph_api(name=""run"", returns=3, flatten_ops=True)\n    def _graph_fn_2_to_3(self, input1, input2):\n        """"""\n        NOTE: Both input1 and input2 are flattened dicts.\n\n        Returns:\n            Tuple:\n                - in1 + in2\n                - in1 - in2\n                - in2\n\n        """"""\n        ret = FlattenedDataOp()\n        ret2 = FlattenedDataOp()\n        for key, value in input1.items():\n            ret[key] = value + input2[""""]\n            ret2[key] = value - input2[""""]\n        return ret, ret2, input2\n\n\nclass DummyCallingOneAPIFromWithinOther(Component):\n    """"""\n    A Component with 2 API-methods (A and B) that calls B from within A.\n    Hence, if a parent component does not call B directly, this component will never be input-complete.\n    """"""\n    def __init__(self, scope=""dummy-calling-one-api-from-within-other"", **kwargs):\n        super(DummyCallingOneAPIFromWithinOther, self).__init__(scope=scope, **kwargs)\n        self.my_var = None\n\n    def create_variables(self, input_spaces, action_space=None):\n        in_space = input_spaces[""inner_input""]\n        self.my_var = self.get_variable(\n            name=""memory"", trainable=False, from_space=in_space, flatten=False, initializer=0\n        )\n\n    @rlgraph_api\n    def run(self, outer_input):\n        # Call another graph_fn here to force space coming out of it (space of tmp_out) to be unknown.\n        tmp_out = self._graph_fn_2(outer_input)\n        # At this point during the build, we have no idea what space tmp_out has and cannot complete this Component\n        # unless the parent itself calls `run_inner`.\n        return self.run_inner(tmp_out)\n\n    @rlgraph_api(name=""run_inner"")\n    def _graph_fn_1(self, inner_input):\n        return inner_input * self.my_var\n\n    @graph_fn\n    def _graph_fn_2(self, inner_input_2):\n        return inner_input_2 + 1.0\n'"
rlgraph/tests/dummy_components_with_sub_components.py,0,"b'# Copyright 2018/2019 The Rlgraph Authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.component import Component\nfrom rlgraph.components.layers.nn.concat_layer import ConcatLayer\nfrom rlgraph.components.layers.nn.dense_layer import DenseLayer\nfrom rlgraph.components.layers.preprocessing.container_splitter import ContainerSplitter\nfrom rlgraph.components.neural_networks.neural_network import NeuralNetwork\nfrom rlgraph.tests.dummy_components import DummyWithVar, SimpleDummyWithVar, DummyCallingOneAPIFromWithinOther\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\n\n\nclass DummyWithSubComponents(Component):\n    """"""\n    A dummy component with a couple of sub-components that have their own API methods.\n\n    API:\n        run(input_): Result of input_ + sub_comp.run(input_) + `self.constant_value`\n    """"""\n    def __init__(self, scope=""dummy-with-sub-components"", constant_value=1.0, **kwargs):\n        """"""\n        Args:\n            constant_value (float): A constant to add to input in our graph_fn.\n        """"""\n        super(DummyWithSubComponents, self).__init__(scope=scope, **kwargs)\n        self.constant_value = constant_value\n\n        # Create a sub-Component and add it.\n        self.sub_comp = DummyWithVar()\n        self.add_components(self.sub_comp)\n\n    @rlgraph_api\n    def run1(self, input_):\n        # Explicit definition of an API-method using one of our graph_fn and one of\n        # our child API-methods.\n        result = self.sub_comp.run_plus(input_)\n        result2 = self._graph_fn_call(result)\n        return result, result2\n\n    @rlgraph_api\n    def run2(self, input_):\n        result1 = self.sub_comp.run_minus(input_)\n        result3 = self._graph_fn_call(result1)\n        return result3\n\n    @graph_fn(returns=1)\n    def _graph_fn_call(self, input_):\n        return input_ + self.constant_value\n\n\nclass DummyNNWithDictInput(NeuralNetwork):\n    """"""\n    Dummy NN with dict input taking a dict with keys ""a"" and ""b"" passes them both through two different (parallel,\n    not connected in any way) dense layers and then concatenating the outputs to yield the final output.\n    """"""\n\n    def __init__(self, num_units_a=3, num_units_b=2, scope=""dummy-nn-with-dict-input"", **kwargs):\n        super(DummyNNWithDictInput, self).__init__(scope=scope, **kwargs)\n\n        self.num_units_a = num_units_a\n        self.num_units_b = num_units_b\n\n        # Splits the input into two streams.\n        self.splitter = ContainerSplitter(""a"", ""b"")\n        self.stack_a = DenseLayer(units=self.num_units_a, scope=""dense-a"")\n        self.stack_b = DenseLayer(units=self.num_units_b, scope=""dense-b"")\n        self.concat_layer = ConcatLayer()\n\n        # Add all sub-components to this one.\n        self.add_components(self.splitter, self.stack_a, self.stack_b, self.concat_layer)\n\n    @rlgraph_api\n    def call(self, input_dict):\n        # Split the input dict into two streams.\n        input_a, input_b = self.splitter.call(input_dict)\n\n        # Get the two stack outputs.\n        output_a = self.stack_a.call(input_a)\n        output_b = self.stack_b.call(input_b)\n\n        # Concat everything together, that\'s the output.\n        concatenated_data = self.concat_layer.call(output_a, output_b)\n\n        return concatenated_data\n\n\nclass DummyCallingSubComponentsAPIFromWithinGraphFn(Component):\n    """"""\n    A dummy component with one sub-component that has variables and an API-method.\n    This dummy calls the sub-componet\'s API-method from within its graph_fn.\n\n    API:\n        run(input_): Result of input_ + sub_comp.run(input_) + `self.constant_value`\n    """"""\n\n    def __init__(self, scope=""dummy-calling-sub-components-api-from-within-graph-fn"", constant_value=1.0, **kwargs):\n        """"""\n        Args:\n            constant_value (float): A constant to add to input in our graph_fn.\n        """"""\n        super(DummyCallingSubComponentsAPIFromWithinGraphFn, self).__init__(scope=scope, **kwargs)\n        self.constant_value = constant_value\n\n        # Create a sub-Component and add it.\n        self.sub_comp = SimpleDummyWithVar()\n        self.add_components(self.sub_comp)\n\n    @rlgraph_api\n    def run(self, input_):\n        # Returns 2*input_ + 10.0.\n        sub_comp_result = self.sub_comp.run(input_)  # input_ + 3.0\n        self_result = self._graph_fn_call(sub_comp_result)  # 2*(input_ + 3.0) + 4.0 = 2*input_ + 10.0\n        return self_result\n\n    @graph_fn(returns=1)\n    def _graph_fn_call(self, input_):\n        # Returns: input_ + [(input_ + 1.0) + 3.0] = 2*input_ + 4.0\n        intermediate_result = input_ + self.constant_value\n        after_api_call = self.sub_comp.run(intermediate_result)\n        return input_ + after_api_call\n\n\nclass DummyProducingInputIncompleteBuild(Component):\n    """"""\n    A dummy component which produces an input-incomplete build.\n    """"""\n    def __init__(self, scope=""dummy-calling-sub-components-api-from-within-graph-fn"", **kwargs):\n        """"""\n        Args:\n            constant_value (float): A constant to add to input in our graph_fn.\n        """"""\n        super(DummyProducingInputIncompleteBuild, self).__init__(scope=scope, **kwargs)\n\n        # Create the ""problematic"" sub-Component and add it.\n        self.sub_comp = DummyCallingOneAPIFromWithinOther()\n        self.add_components(self.sub_comp)\n\n    @rlgraph_api\n    def run(self, input_):\n        return self.sub_comp.run(input_)\n\n\n'"
rlgraph/tests/test_util.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\nimport re\n\nimport numpy as np\n\nfrom rlgraph import get_backend\n\nif get_backend() == ""pytorch"":\n    import torch\n\n\ndef config_from_path(path, root=None):\n    """"""\n    Generates an agent config from path relative to a specified directory (per\n    default the `tests` directory).\n\n    Args:\n        path (str): Path to config, e.g. json file.\n        root (str): Root directory. Per default it\'s the `tests` directory relativ\n            to this file.\n\n    Returns:\n        Union[dict,list]: Agent config dict or list.\n    """"""\n    if not root:\n        root = os.path.join(os.path.dirname(__file__))\n\n    path = os.path.join(root, path)\n    with open(path, \'rt\') as fp:\n        return json.load(fp)\n\n\ndef non_terminal_records(record_space, num_samples):\n    """"""\n    Samples a number of records and enforces all terminals to be 0,\n    which is needed for testing memories.\n\n    Args:\n        record_space (Space): Space to sample from.\n        num_samples (int): Number of samples to draw.\n\n    Returns:\n        Dict: Sampled records with all terminal values set to 0.\n    """"""\n    record_sample = record_space.sample(size=num_samples)\n    record_sample[\'terminals\'] = np.full(shape=(num_samples,), fill_value=np.bool_(False))\n\n    return record_sample\n\n\ndef terminal_records(record_space, num_samples):\n    """"""\n    Samples a number of records and enforces all terminals to be True,\n    which is needed for testing memories.\n\n    Args:\n        record_space (Space): Space to sample from.\n        num_samples (int): Number of samples to draw.\n\n    Returns:\n        Dict: Sampled records with all terminal values set to True.\n    """"""\n    record_sample = record_space.sample(size=num_samples)\n    record_sample[\'terminals\'] = np.full(shape=(num_samples,), fill_value=np.bool_(True))\n\n    return record_sample\n\n\ndef recursive_assert_almost_equal(x, y, decimals=7, atol=None, rtol=None):\n    """"""\n    Checks two structures (dict, DataOpDict, tuple, DataOpTuple, list, np.array, float, int, etc..) for (almost!)\n    numeric identity.\n    All numbers in the two structures have to match up to `decimal` digits after the floating point.\n    Uses assertions (not boolean return).\n\n    Args:\n        x (any): The first value to be compared (to `y`).\n        y (any): The second value to be compared (to `x`).\n        decimals (int): The number of digits after the floating point up to which all numeric values have to match.\n        atol (float): Absolute tolerance of the difference between x and y (overrides `decimals` if given).\n        rtol (float): Relative tolerance of the difference between x and y (overrides `decimals` if given).\n    """"""\n    # A dict type.\n    if isinstance(x, dict):\n        assert isinstance(y, dict), ""ERROR: If x is dict, y needs to be a dict as well!""\n        y_keys = set(x.keys())\n        for key, value in x.items():\n            assert key in y, ""ERROR: y does not have x\'s key=\'{}\'! y={}"".format(key, y)\n            recursive_assert_almost_equal(value, y[key], decimals=decimals, atol=atol, rtol=rtol)\n            y_keys.remove(key)\n        assert not y_keys, ""ERROR: y contains keys ({}) that are not in x! y={}"".format(list(y_keys), y)\n    # A tuple type.\n    elif isinstance(x, (tuple, list)):\n        assert isinstance(y, (tuple, list)), ""ERROR: If x is tuple, y needs to be a tuple as well!""\n        assert len(y) == len(x), ""ERROR: y does not have the same length as "" \\\n                                 ""x ({} vs {})!"".format(len(y), len(x))\n        for i, value in enumerate(x):\n            recursive_assert_almost_equal(value, y[i], decimals=decimals, atol=atol, rtol=rtol)\n    # Boolean comparison.\n    elif isinstance(x, (np.bool_, bool)):\n        assert bool(x) is bool(y), ""ERROR: x ({}) is not y ({})!"".format(x, y)\n    # Nones.\n    elif x is None or y is None:\n        assert x == y, ""ERROR: x ({}) is not the same as y ({})!"".format(x, y)\n    # String comparison.\n    elif hasattr(x, \'dtype\') and x.dtype == np.object:\n        np.testing.assert_array_equal(x, y)\n    # Everything else (assume numeric).\n    else:\n        if get_backend() == ""pytorch"":\n            if isinstance(x, torch.Tensor):\n                x = x.detach().numpy()\n            if isinstance(y, torch.Tensor):\n                y = y.detach().numpy()\n        # Using decimals.\n        if atol is None and rtol is None:\n            np.testing.assert_almost_equal(x, y, decimal=decimals)\n        # Using atol/rtol.\n        else:\n            # Provide defaults for either one of atol/rtol.\n            if atol is None:\n                atol = 0\n            if rtol is None:\n                rtol = 1e-7\n            np.testing.assert_allclose(x, y, atol=atol, rtol=rtol)\n\n\nclass regex_pattern(object):\n    def __init__(self, pattern):\n        self.pattern = pattern\n\n    def __eq__(self, other):\n        return re.match(self.pattern, other) is not None\n\n    def __ne__(self, other):\n        return not re.match(self.pattern, other)\n\n    def __str__(self):\n        return ""~ {}"".format(self.pattern)\n\n    def __repr__(self):\n        return ""~ r\\""{}\\"""".format(self.pattern)\n'"
rlgraph/utils/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph.utils.define_by_run_ops import print_call_chain\nfrom rlgraph.utils.initializer import Initializer\nfrom rlgraph.utils.numpy import sigmoid, softmax, relu, one_hot\nfrom rlgraph.utils.ops import DataOp, SingleDataOp, DataOpDict, DataOpTuple, ContainerDataOp, FlattenedDataOp\nfrom rlgraph.utils.pytorch_util import pytorch_one_hot, PyTorchVariable\nfrom rlgraph.utils.rlgraph_errors import RLGraphError, RLGraphAPICallParamError, RLGraphBuildError, \\\n    RLGraphInputIncompleteError, RLGraphVariableIncompleteError, RLGraphObsoletedError, RLGraphSpaceError\nfrom rlgraph.utils.specifiable import Specifiable\nfrom rlgraph.utils.util import convert_dtype, get_shape, get_rank, force_tuple, force_list, \\\n    LARGE_INTEGER, SMALL_NUMBER, MIN_LOG_STDDEV, MAX_LOG_STDDEV, \\\n    tf_logger, print_logging_handler, root_logger, logging_formatter, default_dict\n\n# from rlgraph.utils.specifiable_server import SpecifiableServer, SpecifiableServerHook\n#from rlgraph.utils.decorators import api\n\n\n__all__ = [\n    ""RLGraphError"", ""RLGraphAPICallParamError"", ""RLGraphBuildError"",\n    ""RLGraphInputIncompleteError"", ""RLGraphVariableIncompleteError"", ""RLGraphObsoletedError"", ""RLGraphSpaceError"",\n    ""Initializer"", ""Specifiable"", ""convert_dtype"", ""get_shape"", ""get_rank"", ""force_tuple"", ""force_list"",\n    ""logging_formatter"", ""root_logger"", ""tf_logger"", ""print_logging_handler"", ""sigmoid"", ""softmax"", ""relu"", ""one_hot"",\n    ""DataOp"", ""SingleDataOp"", ""DataOpDict"", ""DataOpTuple"", ""ContainerDataOp"", ""FlattenedDataOp"",\n    ""pytorch_one_hot"", ""PyTorchVariable"", ""LARGE_INTEGER"", ""SMALL_NUMBER"", ""MIN_LOG_STDDEV"", ""MAX_LOG_STDDEV""\n]\n'"
rlgraph/utils/config_util.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\n\n\ndef read_config_file(config_file):\n    config_path = os.path.join(os.getcwd(), config_file)\n    with open(config_path, \'rt\') as fp:\n        config = json.load(fp)\n    return config\n'"
rlgraph/utils/debug_util.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import defaultdict\nimport numpy as np\nimport pandas as pd\nimport time\nimport os\n\n\nclass PerformanceTimer(object):\n    def __init__(self, filename, columns):\n        self.start = 0\n        self.last = 0\n\n        self.data = defaultdict(lambda: np.nan)\n        self.subtimers = {}\n\n        self.filename = filename\n        self.columns = columns\n\n        self.complete_data = []\n\n    def __enter__(self):\n        self.start = time.time()\n        self.last = time.time()\n        return self\n\n    def __call__(self, column):\n        assert column in self.columns\n\n        delta = time.time() - self.last\n        self.data[column] = delta\n\n        self.last = time.time()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        end_delta = time.time() - self.start\n\n        all_columns = [\'start\']\n        data = dict(start=self.start)\n        for column in self.columns:\n            if column in self.subtimers:\n                subdata = self.subtimers[column].get_results(prefix=column)\n                all_columns += subdata.keys()\n                data.update(subdata)\n            else:\n                all_columns.append(column)\n                data[column] = self.data[column]\n\n        all_columns.append(\'total_time\')\n        data[\'total_time\'] = end_delta\n\n        self.complete_data.append(data)\n\n    def get_results(self, prefix=None):\n        df = pd.DataFrame(self.complete_data)\n        return_data = dict()\n        column_names = list()\n        for column in self.columns:\n            if not prefix:\n                column_name = column\n            else:\n                column_name = \'{}_{}\'.format(prefix, column)\n            column_data = np.asarray(df[column])\n            return_data[column_name] = np.mean(column_data)\n            column_names.append(column_name)\n\n        return return_data\n\n    def write(self):\n        df = pd.DataFrame(self.complete_data)\n\n        write_header = False\n        if not os.path.exists(self.filename):\n            write_header = True\n\n        df.to_csv(self.filename, mode=\'at\', index=write_header)\n\n    def sub(self, column, subcolumns):\n        if not column in self.subtimers:\n            subtimer = PerformanceTimer(filename=None, columns=subcolumns)\n            self.subtimers[column] = subtimer\n        else:\n            subtimer = self.subtimers[column]\n\n        return subtimer\n'"
rlgraph/utils/decorators.py,0,"b'# Copyright 2018/2019 The Rlgraph Authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport copy\nimport inspect\nimport re\nimport time\n\n# from rlgraph.components.common.container_merger import ContainerMerger\nfrom rlgraph.spaces.space_utils import get_space_from_op\nfrom rlgraph.utils import util\nfrom rlgraph.utils.define_by_run_ops import execute_define_by_run_graph_fn\nfrom rlgraph.utils.op_records import GraphFnRecord, APIMethodRecord, DataOpRecord, DataOpRecordColumnIntoAPIMethod, \\\n    DataOpRecordColumnFromAPIMethod, DataOpRecordColumnIntoGraphFn, DataOpRecordColumnFromGraphFn, gather_summaries\nfrom rlgraph.utils.ops import TraceContext\nfrom rlgraph.utils.rlgraph_errors import RLGraphError, RLGraphAPICallParamError, RLGraphVariableIncompleteError, \\\n    RLGraphInputIncompleteError\n\n# Global registries for Component classes\' API-methods and graph_fn.\ncomponent_api_registry = {}\ncomponent_graph_fn_registry = {}\n\n\ndef rlgraph_api(api_method=None, *, component=None, name=None, returns=None,\n                flatten_ops=False, split_ops=False, add_auto_key_as_first_param=False,\n                must_be_complete=True, ok_to_overwrite=False, requires_variable_completeness=False):\n    """"""\n    API-method decorator used to tag any Component\'s methods as API-methods.\n\n    Args:\n        api_method (callable): The actual function/method to tag as an API method.\n\n        component (Optional[Component]): The Component that the method should belong to. None if `api_method` is\n            decorated inside a Component class.\n\n        name (Optional[str]): The name under which the API-method should be registered. This is only necessary if\n            the API-method is automatically generated as a thin-wrapper around a graph_fn.\n\n        returns (Optional[int]): If the function is a graph_fn, we may specify, how many return values\n            it returns. If None, will try to get this number from looking at the source code or from the Component\'s\n            `num_graph_fn_return_values` property.\n\n        flatten_ops (Union[bool,Set[str]]): Whether to flatten all or some DataOps by creating\n            a FlattenedDataOp (with automatic key names).\n            Can also be a set of in-Socket names to flatten explicitly (True for all).\n            (default: True).\n\n        split_ops (Union[bool,Set[str]]): Whether to split all or some of the already flattened DataOps\n            and send the SingleDataOps one by one through the graph_fn.\n            Example: Spaces=A=Dict (container), B=int (primitive)\n            The graph_fn should then expect for each primitive Space in A:\n            _graph_fn(primitive-in-A (Space), B (int))\n            NOTE that B will be the same in all calls for all primitive-in-A\'s.\n            (default: True).\n\n        add_auto_key_as_first_param (bool): If `split_ops` is not False, whether to send the\n            automatically generated flat key as the very first parameter into each call of the graph_fn.\n            Example: Spaces=A=float (primitive), B=Tuple (container)\n            The graph_fn should then expect for each primitive Space in B:\n            _graph_fn(key, A (float), primitive-in-B (Space))\n            NOTE that A will be the same in all calls for all primitive-in-B\'s.\n            The key can now be used to index into variables equally structured as B.\n            Has no effect if `split_ops` is False.\n            (default: False).\n\n        must_be_complete (bool): Whether the exposed API methods must be input-complete or not.\n\n        ok_to_overwrite (bool): Set to True to indicate that this API-decorator will overwrite an already existing\n            API-method in the Component. Default: False.\n\n        requires_variable_completeness (bool): Whether the underlying graph_fn should only be called\n            after the Component is variable-complete. By default, only input-completeness is required.\n\n    Returns:\n        callable: The decorator function.\n    """"""\n    _sanity_check_decorator_options(flatten_ops, split_ops, add_auto_key_as_first_param)\n\n    def decorator_func(wrapped_func):\n\n        def api_method_wrapper(self, *args, **kwargs):\n            api_fn_name = name or re.sub(r\'^_graph_fn_\', """", wrapped_func.__name__)\n            # Direct evaluation of function.\n            if self.execution_mode == ""define_by_run"":\n                type(self).call_count += 1\n\n                start = time.perf_counter()\n                # Check with owner if extra args needed.\n                if \'_graph_fn_\' in wrapped_func.__name__:\n                    output = execute_define_by_run_graph_fn(\n                        self,\n                        wrapped_func,\n                        {""flatten_ops"": flatten_ops,\n                         ""add_auto_key_as_first_param"": add_auto_key_as_first_param,\n                         ""split_ops"": split_ops,\n                         },\n                        *args,\n                        **kwargs\n                    )\n                elif api_fn_name in self.api_methods and self.api_methods[api_fn_name].add_auto_key_as_first_param:\n                    output = wrapped_func(self, """", *args, **kwargs)\n                else:\n                    output = wrapped_func(self, *args, **kwargs)\n\n                # Store runtime for this method.\n                type(self).call_times.append(  # Component.call_times\n                    (self.name, wrapped_func.__name__, time.perf_counter() - start)\n                )\n                return output\n\n            api_method_rec = self.api_methods[api_fn_name]\n\n            # Create op-record column to call API method with. Ignore None input params. These should not be sent\n            # to the API-method.\n            in_op_column = DataOpRecordColumnIntoAPIMethod(\n                component=self, api_method_rec=api_method_rec, args=args, kwargs=kwargs\n            )\n            # Add the column to the API-method record.\n            api_method_rec.in_op_columns.append(in_op_column)\n\n            # Check minimum number of passed args.\n            minimum_num_call_params = len(in_op_column.api_method_rec.non_args_kwargs) - \\\n                len(in_op_column.api_method_rec.default_args)\n            if len(in_op_column.op_records) < minimum_num_call_params:\n                raise RLGraphAPICallParamError(\n                    ""Number of call params ({}) for call to API-method \'{}\' is too low. Needs to be at least {} ""\n                    ""params!"".format(len(in_op_column.op_records), api_method_rec.name, minimum_num_call_params)\n                )\n\n            # Link from incoming op_recs into the new column or populate new column with ops/Spaces (this happens\n            # if this call was made from within a graph_fn such that ops and Spaces are already known).\n            all_args = [(i, a) for i, a in enumerate(args) if a is not None] + \\\n                       [(k, v) for k, v in sorted(kwargs.items()) if v is not None]\n            flex = None\n            build_when_done = False\n            for i, (key, value) in enumerate(all_args):\n                # Named arg/kwarg -> get input_name from that and peel op_rec.\n                if isinstance(key, str):\n                    param_name = key\n                # Positional arg -> get input_name from input_names list.\n                else:\n                    slot = key if flex is None else flex\n                    if slot >= len(api_method_rec.input_names):\n                        raise RLGraphAPICallParamError(\n                            ""Too many input args given in call to AI-method \'{}\'! Expected={}, you passed in ""\n                            ""more than {}."".format(api_method_rec.name, len(api_method_rec.input_names), slot)\n                        )\n                    param_name = api_method_rec.input_names[slot]\n\n                # Var-positional arg, attach the actual position to input_name string.\n                if self.api_method_inputs.get(param_name, """") == ""*flex"":\n                    if flex is None:\n                        flex = i\n                    param_name += ""[{}]"".format(i - flex)\n                # Actual kwarg (not in list of api_method_inputs).\n                elif api_method_rec.kwargs_name is not None and param_name not in self.api_method_inputs:\n                    param_name = api_method_rec.kwargs_name + ""[{}]"".format(param_name)\n\n                # We are already in building phase (params may be coming from inside graph_fn).\n                if self.graph_builder is not None and self.graph_builder.phase == ""building"":\n                    # If Space not stored yet, determine it from op.\n                    assert in_op_column.op_records[i].op is not None\n                    if in_op_column.op_records[i].space is None:\n                        in_op_column.op_records[i].space = get_space_from_op(in_op_column.op_records[i].op)\n                    self.api_method_inputs[param_name] = in_op_column.op_records[i].space\n                    # Check input-completeness of Component (but not strict as we are only calling API, not a graph_fn).\n                    if self.input_complete is False:\n                        # Build right after this loop in case more Space information comes in through next args/kwargs.\n                        build_when_done = True\n\n                # A DataOpRecord from the meta-graph.\n                elif isinstance(value, DataOpRecord):\n                    # Create entry with unknown Space if it doesn\'t exist yet.\n                    if param_name not in self.api_method_inputs:\n                        self.api_method_inputs[param_name] = None\n\n                # Fixed value (instead of op-record): Store the fixed value directly in the op.\n                else:\n                    if self.api_method_inputs.get(param_name) is None:\n                        self.api_method_inputs[param_name] = in_op_column.op_records[i].space\n\n            if build_when_done:\n                # Check Spaces and create variables.\n                self.graph_builder.build_component_when_input_complete(self)\n\n            # Regular API-method: Call it here.\n            api_fn_args, api_fn_kwargs = in_op_column.get_args_and_kwargs()\n\n            if api_method_rec.is_graph_fn_wrapper is False:\n                return_values = wrapped_func(self, *api_fn_args, **api_fn_kwargs)\n            # Wrapped graph_fn: Call it through yet another wrapper.\n            else:\n                return_values = graph_fn_wrapper(\n                    self, wrapped_func, returns, dict(\n                        flatten_ops=flatten_ops, split_ops=split_ops,\n                        add_auto_key_as_first_param=add_auto_key_as_first_param,\n                        requires_variable_completeness=requires_variable_completeness\n                    ), *api_fn_args, **api_fn_kwargs\n                )\n\n            # Process the results (push into a column).\n            out_op_column = DataOpRecordColumnFromAPIMethod(\n                component=self,\n                api_method_name=api_fn_name,\n                args=util.force_tuple(return_values) if type(return_values) != dict else None,\n                kwargs=return_values if type(return_values) == dict else None\n            )\n\n            # If we already have actual op(s) and Space(s), push them already into the\n            # DataOpRecordColumnFromAPIMethod\'s records.\n            if self.graph_builder is not None and self.graph_builder.phase == ""building"":\n                # Link the returned ops to that new out-column.\n                for i, rec in enumerate(out_op_column.op_records):\n                    out_op_column.op_records[i].op = rec.op\n                    out_op_column.op_records[i].space = rec.space\n            # And append the new out-column to the api-method-rec.\n            api_method_rec.out_op_columns.append(out_op_column)\n\n            # Do we need to return the raw ops or the op-recs?\n            # Only need to check if False, otherwise, we return ops directly anyway.\n            return_ops = False\n            stack = inspect.stack()\n            f_locals = stack[1][0].f_locals\n            # We may be in a list comprehension, try next frame.\n            if f_locals.get("".0""):\n                f_locals = stack[2][0].f_locals\n            # Check whether the caller component is a parent of this one.\n            caller_component = f_locals.get(""root"", f_locals.get(""self_"", f_locals.get(""self"")))\n\n            # Potential call from a lambda.\n            if caller_component is None and ""fn"" in stack[2][0].f_locals:\n                # This is the component.\n                prev_caller_component = TraceContext.PREV_CALLER\n                lambda_obj = stack[2][0].f_locals[""fn""]\n                if ""lambda"" in inspect.getsource(lambda_obj):\n                    # Try to reconstruct caller by using parent of prior caller.\n                    caller_component = prev_caller_component.parent_component\n\n            if caller_component is None:\n                raise RLGraphError(\n                    ""API-method \'{}\' must have as 1st parameter (the component) either `root` or `self`. Other names ""\n                    ""are not allowed!"".format(api_method_rec.name)\n                )\n            # Not directly called by this method itself (auto-helper-component-API-call).\n            # AND call is coming from some caller Component, but that component is not this component\n            # OR a parent -> Error.\n            elif caller_component is not None and \\\n                    type(caller_component).__name__ != ""MetaGraphBuilder"" and \\\n                    caller_component not in [self] + self.get_parents():\n                if not (stack[1][3] == ""__init__"" and re.search(r\'op_records\\.py$\', stack[1][1])):\n                    raise RLGraphError(\n                        ""The component \'{}\' is not a child (or grand-child) of the caller ({})! Maybe you forgot to ""\n                        ""add it as a sub-component via `add_components()`."".\n                        format(self.global_scope, caller_component.global_scope)\n                    )\n\n            # Update trace context.\n            TraceContext.PREV_CALLER = caller_component\n\n            for stack_item in stack[1:]:  # skip current frame\n                # If we hit an API-method call -> return op-recs.\n                if stack_item[3] == ""api_method_wrapper"" and re.search(r\'decorators\\.py$\', stack_item[1]):\n                    break\n                # If we hit a graph_fn call -> return ops.\n                elif stack_item[3] == ""run_through_graph_fn"" and re.search(r\'graph_builder\\.py$\', stack_item[1]):\n                    return_ops = True\n                    break\n\n            if return_ops is True:\n                assert len(caller_component._summary_ops_buffer_stack) > 0,\\\n                    ""Called by other graph_fn, there should be summary_ops buffer started""\n                # Propagate the summaries to the parent now as this breaks the meta-graph chain.\n                summaries = gather_summaries(out_op_column.op_records)\n                for summary_op in summaries:\n                    caller_component.register_summary_op(summary_op)\n                if type(return_values) == dict:\n                    return {key: value.op for key, value in out_op_column.get_args_and_kwargs()[1].items()}\n                else:\n                    tuple_returns = tuple(map(lambda x: x.op, out_op_column.get_args_and_kwargs()[0]))\n                    return tuple_returns[0] if len(tuple_returns) == 1 else tuple_returns\n            # Parent caller is non-graph_fn: Return op-recs.\n            else:\n                if type(return_values) == dict:\n                    return return_values\n                else:\n                    tuple_returns = out_op_column.get_args_and_kwargs()[0]\n                    return tuple_returns[0] if len(tuple_returns) == 1 else tuple_returns\n\n        func_type = util.get_method_type(wrapped_func)\n        is_graph_fn_wrapper = (func_type == ""graph_fn"")\n        api_fn_name = name or (re.sub(r\'^_graph_fn_\', """", wrapped_func.__name__) if is_graph_fn_wrapper else\n                         wrapped_func.__name__)\n        api_method_rec = APIMethodRecord(\n            func=wrapped_func, wrapper_func=api_method_wrapper,\n            name=api_fn_name,\n            must_be_complete=must_be_complete, ok_to_overwrite=ok_to_overwrite,\n            is_graph_fn_wrapper=is_graph_fn_wrapper, is_class_method=(component is None),\n            flatten_ops=flatten_ops, split_ops=split_ops, add_auto_key_as_first_param=add_auto_key_as_first_param,\n            requires_variable_completeness=requires_variable_completeness\n        )\n\n        # Registers the given method with the Component (if not already done so).\n        if component is not None:\n            define_api_method(component, api_method_rec, copy_record=False)\n        # Registers the given function with the Component sub-class so we can define it for each\n        # constructed instance of that sub-class.\n        else:\n            cls = wrapped_func.__qualname__.split(\'.<locals>\', 1)[0].rsplit(\'.\', 1)[0]\n            if cls not in component_api_registry:\n                component_api_registry[cls] = []\n            component_api_registry[cls].append(api_method_rec)\n\n        return api_method_wrapper\n\n    if api_method is None:\n        return decorator_func\n    else:\n        return decorator_func(api_method)\n\n\ndef graph_fn(graph_fn=None, *, component=None, returns=None,\n             flatten_ops=False, split_ops=False, add_auto_key_as_first_param=False,\n             requires_variable_completeness=False):\n    """"""\n    Graph_fn decorator used to tag any Component\'s graph_fn (that is not directly wrapped by an API-method) as such.\n\n    Args:\n        graph_fn (callable): The actual graph_fn to tag.\n\n        component (Optional[Component]): The Component that the graph function should belong to. None if `graph_fn` is\n            decorated inside a Component class.\n\n        returns (Optional[int]): How many return values it returns. If None, will try to get this number from looking at the source code or from the Component\'s\n            `num_graph_fn_return_values` property.\n\n        flatten_ops (Union[bool,Set[str]]): Whether to flatten all or some DataOps by creating\n            a FlattenedDataOp (with automatic key names).\n            Can also be a set of in-Socket names to flatten explicitly (True for all).\n            Default: True.\n\n        split_ops (Union[bool,Set[str]]): Whether to split all or some of the already flattened DataOps\n            and send the SingleDataOps one by one through the graph_fn.\n            Example: Spaces=A=Dict (container), B=int (primitive)\n            The graph_fn should then expect for each primitive Space in A:\n            _graph_fn(primitive-in-A (Space), B (int))\n            NOTE that B will be the same in all calls for all primitive-in-A\'s.\n            Default: True.\n\n        add_auto_key_as_first_param (bool): If `split_ops` is not False, whether to send the\n            automatically generated flat key as the very first parameter into each call of the graph_fn.\n            Example: Spaces=A=float (primitive), B=Tuple (container)\n            The graph_fn should then expect for each primitive Space in B:\n            _graph_fn(key, A (float), primitive-in-B (Space))\n            NOTE that A will be the same in all calls for all primitive-in-B\'s.\n            The key can now be used to index into variables equally structured as B.\n            Has no effect if `split_ops` is False.\n            Default: False.\n\n        requires_variable_completeness (bool): Whether the underlying graph_fn should only be called\n            after the Component is variable-complete. By default, only input-completeness is required.\n\n    Returns:\n        callable: The decorator function.\n    """"""\n    _sanity_check_decorator_options(flatten_ops, split_ops, add_auto_key_as_first_param)\n\n    def decorator_func(wrapped_func):\n        def _graph_fn_wrapper(self, *args, **kwargs):\n            if self.execution_mode == ""define_by_run"":\n                # Direct execution.\n                return self.graph_builder.execute_define_by_run_graph_fn(self, wrapped_func,  dict(\n                        flatten_ops=flatten_ops, split_ops=split_ops,\n                        add_auto_key_as_first_param=add_auto_key_as_first_param\n                        ), *args, **kwargs)\n            else:\n                # Wrap construction of graph functions with op records.\n                return graph_fn_wrapper(\n                    self, wrapped_func, returns, dict(\n                        flatten_ops=flatten_ops, split_ops=split_ops,\n                        add_auto_key_as_first_param=add_auto_key_as_first_param,\n                        requires_variable_completeness=requires_variable_completeness\n                    ), *args, **kwargs\n                )\n\n        graph_fn_rec = GraphFnRecord(\n            func=wrapped_func, wrapper_func=_graph_fn_wrapper, is_class_method=(component is None),\n            flatten_ops=flatten_ops, split_ops=split_ops,\n            add_auto_key_as_first_param=add_auto_key_as_first_param,\n            requires_variable_completeness=requires_variable_completeness\n        )\n\n        # Registers the given method with the Component (if not already done so).\n        if component is not None:\n            define_graph_fn(component, graph_fn_rec, copy_record=False)\n        # Registers the given function with the Component sub-class so we can define it for each\n        # constructed instance of that sub-class.\n        else:\n            cls = wrapped_func.__qualname__.split(\'.<locals>\', 1)[0].rsplit(\'.\', 1)[0]\n            if cls not in component_graph_fn_registry:\n                component_graph_fn_registry[cls] = list()\n                component_graph_fn_registry[cls].append(graph_fn_rec)\n\n        return _graph_fn_wrapper\n\n    if graph_fn is None:\n        return decorator_func\n    else:\n        return decorator_func(graph_fn)\n\n\ndef define_api_method(component, api_method_record, copy_record=True):\n    """"""\n    Registers an API-method with a Component instance.\n\n    Args:\n        component (Component): The Component object to register the API method with.\n        api_method_record (APIMethodRecord): The APIMethodRecord describing the to-be-registered API-method.\n        copy_record (bool): Whether to deepcopy the APIMethodRecord prior to handing it to the Component for storing.\n    """"""\n    # Deep copy the record (in case this got registered the normal way with via decorating a class method).\n    if copy_record:\n        api_method_record = copy.deepcopy(api_method_record)\n    api_method_record.component = component\n\n    # Raise errors if `name` already taken in this Component.\n    if not api_method_record.ok_to_overwrite:\n        # There already is an API-method with that name.\n        if api_method_record.name in component.api_methods:\n            raise RLGraphError(""API-method with name \'{}\' already defined!"".format(api_method_record.name))\n        # There already is another object property with that name (avoid accidental overriding).\n        elif not api_method_record.is_class_method and getattr(component, api_method_record.name, None) is not None:\n            raise RLGraphError(\n                ""Component \'{}\' already has a property called \'{}\'. Cannot define an API-method with ""\n                ""the same name!"".format(component.name, api_method_record.name)\n            )\n\n    # Do not build this API as per ctor instructions.\n    if api_method_record.name in component.switched_off_apis:\n        return\n\n    component.synthetic_methods.add(api_method_record.name)\n    setattr(component, api_method_record.name, api_method_record.wrapper_func.__get__(component, component.__class__))\n    setattr(api_method_record.wrapper_func, ""__name__"", api_method_record.name)\n\n    component.api_methods[api_method_record.name] = api_method_record\n\n    # Direct callable for eager/define by run.\n    component.api_fn_by_name[api_method_record.name] = api_method_record.wrapper_func\n\n    # Update the api_method_inputs dict (with empty Spaces if not defined yet).\n    skip_args = 1  # self\n    skip_args += (api_method_record.is_graph_fn_wrapper and api_method_record.add_auto_key_as_first_param)\n    param_list = list(inspect.signature(api_method_record.func).parameters.values())[skip_args:]\n\n    for param in param_list:\n        component.api_methods[api_method_record.name].input_names.append(param.name)\n        if param.name not in component.api_method_inputs:\n            # This param has a default value.\n            if param.default != inspect.Parameter.empty:\n                # Default is None. Set to ""flex"" (to signal that this Space is not needed for input-completeness)\n                # and wait for first call using this parameter (only then set it to that Space).\n                if param.default is None:\n                    component.api_method_inputs[param.name] = ""flex""\n                # Default is some python value (e.g. a bool). Use that are the assigned Space.\n                else:\n                    space = get_space_from_op(param.default)\n                    component.api_method_inputs[param.name] = space\n            # This param is an *args param. Store as ""*flex"". Then with upcoming API calls, we determine the Spaces\n            # for the single items in *args and set them under ""param[0]"", ""param[1]"", etc..\n            elif param.kind == inspect.Parameter.VAR_POSITIONAL:\n                component.api_method_inputs[param.name] = ""*flex""\n            # This param is a **kwargs param. Store as ""**flex"". Then with upcoming API calls, we determine the Spaces\n            # for the single items in **kwargs and set them under ""param[some-key]"", ""param[some-other-key]"", etc..\n            elif param.kind == inspect.Parameter.VAR_KEYWORD:\n                component.api_method_inputs[param.name] = ""**flex""\n            # Normal POSITIONAL_ONLY parameter. Store as None (needed) for now.\n            else:\n                component.api_method_inputs[param.name] = None\n\n\ndef define_graph_fn(component, graph_fn_record, copy_record=True):\n    """"""\n    Registers a graph_fn with a Component instance.\n\n    Args:\n        component (Component): The Component object to register the graph function with.\n        graph_fn_record (GraphFnRecord): The GraphFnRecord describing the to-be-registered graph function.\n        copy_record (bool): Whether to deepcopy the GraphFnRecord prior to handing it to the Component for storing.\n    """"""\n    # Deep copy the record (in case this got registered the normal way with via decorating a class method).\n    if copy_record is True:\n        graph_fn_record = copy.deepcopy(graph_fn_record)\n\n    graph_fn_record.component = component\n\n    # Raise errors if `name` already taken in this Component.\n    # There already is a graph_fn with that name.\n    if graph_fn_record.name in component.graph_fns:\n        raise RLGraphError(""Graph-Fn with name \'{}\' already defined!"".format(graph_fn_record.name))\n    # There already is another object property with that name (avoid accidental overriding).\n    elif not graph_fn_record.is_class_method and getattr(component, graph_fn_record.name, None) is not None:\n        raise RLGraphError(\n            ""Component \'{}\' already has a property called \'{}\'. Cannot define a Graph-Fn with ""\n            ""the same name!"".format(component.name, graph_fn_record.name)\n        )\n\n    setattr(component, graph_fn_record.name, graph_fn_record.wrapper_func.__get__(component, component.__class__))\n    setattr(graph_fn_record.func, ""__self__"", component)\n\n    component.graph_fns[graph_fn_record.name] = graph_fn_record\n\n\ndef graph_fn_wrapper(component, wrapped_func, returns, options, *args, **kwargs):\n    """"""\n    Executes a dry run through a graph_fn (without calling it) just generating the empty\n    op-record-columns around the graph_fn (incoming and outgoing). Except if the GraphBuilder\n    is already in the ""building"" phase, in which case the graph_fn is actually called.\n\n    Args:\n        component (Component): The Component that this graph_fn belongs to.\n        wrapped_func (callable): The graph_fn to be called during the build process.\n        returns (Optional[int]): The number of return values of the graph_fn.\n\n        options (Dict): Dict with the following keys (optionally) set:\n            - flatten_ops (Union[bool,Set[str]]): Whether to flatten all or some DataOps by creating\n            a FlattenedDataOp (with automatic key names).\n            Can also be a set of in-Socket names to flatten explicitly (True for all).\n            Default: True.\n            - split_ops (Union[bool,Set[str]]): Whether to split all or some of the already flattened DataOps\n            and send the SingleDataOps one by one through the graph_fn.\n            Example: Spaces=A=Dict (container), B=int (primitive)\n            The graph_fn should then expect for each primitive Space in A:\n            _graph_fn(primitive-in-A (Space), B (int))\n            NOTE that B will be the same in all calls for all primitive-in-A\'s.\n            Default: True.\n            - add_auto_key_as_first_param (bool): If `split_ops` is not False, whether to send the\n            automatically generated flat key as the very first parameter into each call of the graph_fn.\n            Example: Spaces=A=float (primitive), B=Tuple (container)\n            The graph_fn should then expect for each primitive Space in B:\n            _graph_fn(key, A (float), primitive-in-B (Space))\n            NOTE that A will be the same in all calls for all primitive-in-B\'s.\n            The key can now be used to index into variables equally structured as B.\n            Has no effect if `split_ops` is False.\n            Default: False.\n\n        args (Union[DataOpRecord,np.array,numeric]): The DataOpRecords to be used for calling the method.\n    """"""\n    flatten_ops = options.pop(""flatten_ops"", False)\n    split_ops = options.pop(""split_ops"", False)\n    add_auto_key_as_first_param = options.pop(""add_auto_key_as_first_param"", False)\n    requires_variable_completeness = options.pop(""requires_variable_completeness"", False)\n\n    # Store a graph_fn record in this component for better in/out-op-record-column reference.\n    if wrapped_func.__name__ not in component.graph_fns:\n        component.graph_fns[wrapped_func.__name__] = GraphFnRecord(\n            func=wrapped_func, wrapper_func=graph_fn_wrapper, component=component,\n            requires_variable_completeness=requires_variable_completeness\n        )\n\n    # Generate in-going op-rec-column.\n    in_graph_fn_column = DataOpRecordColumnIntoGraphFn(\n        component=component, graph_fn=wrapped_func,\n        flatten_ops=flatten_ops, split_ops=split_ops,\n        add_auto_key_as_first_param=add_auto_key_as_first_param,\n        requires_variable_completeness=requires_variable_completeness,\n        args=args, kwargs=kwargs\n    )\n    # Add the column to the `graph_fns` record.\n    component.graph_fns[wrapped_func.__name__].in_op_columns.append(in_graph_fn_column)\n\n    # We are already building: Actually call the graph_fn after asserting that its Component is input-complete.\n    if component.graph_builder and component.graph_builder.phase == ""building"":\n        # Assert input-completeness of Component (if not already, then after this graph_fn/Space update).\n        # if self.input_complete is False:\n        # Check Spaces and create variables.\n        component.graph_builder.build_component_when_input_complete(component)\n        if component.input_complete is False:\n            raise RLGraphInputIncompleteError(component)\n        # If we are calling a variables-requiring graph_fn -> make sure we are also variable-complete.\n        if requires_variable_completeness is True and component.variable_complete is False:\n            raise RLGraphVariableIncompleteError(component)\n        # Call the graph_fn (only if not already done so by build above (e.g. `variables()`).\n        if in_graph_fn_column.out_graph_fn_column is None:\n            assert in_graph_fn_column.already_sent is False\n            out_graph_fn_column = component.graph_builder.run_through_graph_fn_with_device_and_scope(in_graph_fn_column)\n        else:\n            out_graph_fn_column = in_graph_fn_column.out_graph_fn_column\n        # Check again, in case we are now also variable-complete.\n        component.graph_builder.build_component_when_input_complete(component)\n\n    # We are still in the assembly phase: Don\'t actually call the graph_fn. Only generate op-rec-columns\n    # around it (in-coming and out-going).\n    else:\n        # Create 2 op-record columns, one going into the graph_fn and one getting out of there and link\n        # them together via the graph_fn (w/o calling it).\n        # TODO: remove when we have numpy-based Components (then we can do test calls to infer everything automatically)\n        if wrapped_func.__name__ in component.graph_fn_num_outputs:\n            num_graph_fn_return_values = component.graph_fn_num_outputs[wrapped_func.__name__]\n        elif returns is not None:\n            num_graph_fn_return_values = returns\n        else:\n            num_graph_fn_return_values = util.get_num_return_values(wrapped_func)\n            # Safety measure: If not explicitly declared, do not allow 0 return values.\n            assert num_graph_fn_return_values > 0,\\\n                ""ERROR: GraphFn \'{}/{}\' does not seem to be returning any values! If a simple None/no_op should be "" \\\n                ""returned from the function, it may help to explicitly write \'return None\' at the bottom of it."".\\\n                format(component.global_scope, wrapped_func.__name__)\n\n        component.logger.debug(""GraphFn has {} return values (inferred)."".format(\n            wrapped_func.__name__, num_graph_fn_return_values)\n        )\n        # If in-column is empty, add it to the ""empty in-column"" set.\n        if len(in_graph_fn_column.op_records) == 0:\n            component.no_input_graph_fn_columns.add(in_graph_fn_column)\n\n        # Generate the out-op-column from the number of return values (guessed during assembly phase or\n        # actually measured during build phase).\n        out_graph_fn_column = DataOpRecordColumnFromGraphFn(\n            num_op_records=num_graph_fn_return_values,\n            component=component, graph_fn_name=wrapped_func.__name__,\n            in_graph_fn_column=in_graph_fn_column,\n            summary_ops=[]\n        )\n\n        in_graph_fn_column.out_graph_fn_column = out_graph_fn_column\n\n    component.graph_fns[wrapped_func.__name__].out_op_columns.append(out_graph_fn_column)\n\n    return_ops = False\n    for stack_item in inspect.stack()[1:]:  # skip current frame\n        # If we hit an API-method call -> return op-recs.\n        if stack_item[3] == ""api_method_wrapper"" and re.search(r\'decorators\\.py$\', stack_item[1]):\n            break\n        # If we hit a graph_fn call -> return ops.\n        elif stack_item[3] == ""run_through_graph_fn"" and re.search(r\'graph_builder\\.py$\', stack_item[1]):\n            return_ops = True\n            break\n\n    if return_ops is True:  #re.match(r\'^_graph_fn_.+|<lambda>$\', stack[2][3]) and out_graph_fn_column.op_records[0].op is not None:\n        assert out_graph_fn_column.op_records[0].op is not None,\\\n            ""ERROR: Cannot return ops (instead of op-recs) if ops are still None!""\n        # By contract the graph functions are ""private"" to the component.\n        caller_component = out_graph_fn_column.component\n        assert len(caller_component._summary_ops_buffer_stack) > 0, \\\n            ""Called by other graph_fn, there should be summary_ops buffer started""\n        # Propagate the summaries to the parent now as this breaks the meta-graph chain.\n        for summary_op in out_graph_fn_column.summary_ops:\n            caller_component.register_summary_op(summary_op)\n        if len(out_graph_fn_column.op_records) == 1:\n            return out_graph_fn_column.op_records[0].op\n        else:\n            return tuple([op_rec.op for op_rec in out_graph_fn_column.op_records])\n    else:\n        if len(out_graph_fn_column.op_records) == 1:\n            return out_graph_fn_column.op_records[0]\n        else:\n            return tuple(out_graph_fn_column.op_records)\n\n\ndef _sanity_check_call_parameters(self, params, method, method_type, add_auto_key_as_first_param):\n    raw_signature_parameters = inspect.signature(method).parameters\n    actual_params = list(raw_signature_parameters.values())\n    if add_auto_key_as_first_param is True:\n        actual_params = actual_params[1:]\n    if len(params) != len(actual_params):\n        # Check whether the last arg is var_positional (e.g. *inputs; in that case it\'s ok if the number of params\n        # is larger than that of the actual graph_fn params or its one smaller).\n        if actual_params[-1].kind == inspect.Parameter.VAR_POSITIONAL and (len(params) > len(actual_params) > 0 or\n                                                                           len(params) == len(actual_params) - 1):\n            pass\n        # Some actual params have default values: Number of given params must be at least as large as the number\n        # of non-default actual params but maximally as large as the number of actual_parameters.\n        elif len(actual_params) >= len(params) >= sum(\n                [p.default is inspect.Parameter.empty for p in actual_params]):\n            pass\n        else:\n            raise RLGraphError(\n                ""ERROR: {} \'{}/{}\' has {} input-parameters, but {} ({}) were being provided in the ""\n                ""`Component.call` method!"".format(method_type, self.name, method.__name__,\n                                                  len(actual_params), len(params), params)\n            )\n\n\ndef _sanity_check_decorator_options(flatten_ops, split_ops, add_auto_key_as_first_param):\n    if split_ops:\n        assert flatten_ops,\\\n            ""ERROR in decorator options: `split_ops` cannot be True if `flatten_ops` is False!""\n\n    if add_auto_key_as_first_param:\n        assert split_ops,\\\n            ""ERROR in decorator options: `add_auto_key_as_first_param` cannot be True if `split_ops` is False!""\n'"
rlgraph/utils/define_by_run_ops.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nfrom collections import OrderedDict\n\nfrom rlgraph import get_backend\nfrom rlgraph.utils.ops import FLAT_TUPLE_OPEN, FLAT_TUPLE_CLOSE, deep_tuple, FlattenedDataOp, FLATTEN_SCOPE_PREFIX, \\\n    DataOpDict\n\nif get_backend() == ""pytorch"":\n    import torch\n\n\ndef print_call_chain(profile_data, sort=True, filter_threshold=None):\n    """"""\n    Prints a component call chain stdout. Useful to analyze define by run performance.\n\n    Args:\n        profile_data (list): Component file data.\n        sort (bool): If true, sorts call sorted by call duration.\n        filter_threshold (Optional[float]): Optionally specify an execution threshold in seconds (e.g. 0.01).\n            All call entries below the threshold be dropped from the printout.\n    """"""\n    original_length = len(profile_data)\n    if filter_threshold is not None:\n        assert isinstance(filter_threshold, float), ""ERROR: Filter threshold must be float but is {}."".format(\n            type(filter_threshold))\n        profile_data = [data for data in profile_data if data[2] > filter_threshold]\n    if sort:\n        res = sorted(profile_data, key=lambda v: v[2], reverse=True)\n        print(""Call chain sorted by runtime ({} calls, {} before filter):"".\n              format(len(profile_data), original_length))\n        for v in res:\n            print(""{}.{}: {} s"".format(v[0], v[1], v[2]))\n    else:\n        print(""Directed call chain ({} calls, {} before filter):"".format(len(profile_data), original_length))\n        for i in range(len(profile_data) - 1):\n            v = profile_data[i]\n            print(""({}.{}: {} s) ->"".format(v[0], v[1], v[2]))\n        v = profile_data[-1]\n        print(""({}.{}: {} s)"".format(v[0], v[1], v[2]))\n\n\ndef define_by_run_flatten(container, key_scope="""", tensor_tuple_list=None, scope_separator_at_start=True):\n    """"""\n    Flattens a native python dict/tuple into a flat dict with auto-key generation. Run-time equivalent\n    to build-time flatten operation.\n\n    Args:\n        container (Union[dict,tuple]): Container  to flatten.\n        key_scope (str): The recursive scope for auto-key generation.\n        tensor_tuple_list (list): The list of tuples (key, value) to be converted into the final results.\n        scope_separator_at_start (bool): If to prepend a scope separator before the first key in a\n            recursive structure. Default false.\n\n    Returns:\n        Dict: Flattened container.\n    """"""\n    ret = False\n\n    # Are we in the non-recursive (first) call?\n    if tensor_tuple_list is None:\n        tensor_tuple_list = []\n        if not isinstance(container, (dict, tuple)):\n            return DataOpDict([("""", container)])\n        ret = True\n\n    if isinstance(container, dict):\n        if scope_separator_at_start:\n            key_scope += FLATTEN_SCOPE_PREFIX\n        else:\n            key_scope = """"\n        for key in sorted(container.keys()):\n            # Make sure we have no double slashes from flattening an already FlattenedDataOp.\n            scope = (key_scope[:-1] if len(key) == 0 or key[0] == ""/"" else key_scope) + key\n            define_by_run_flatten(container[key], key_scope=scope, tensor_tuple_list=tensor_tuple_list, scope_separator_at_start=True)\n    elif isinstance(container, tuple):\n        if scope_separator_at_start:\n            key_scope += FLATTEN_SCOPE_PREFIX + FLAT_TUPLE_OPEN\n        else:\n            key_scope += """" + FLAT_TUPLE_OPEN\n        for i, c in enumerate(container):\n            define_by_run_flatten(c, key_scope=key_scope + str(i) + FLAT_TUPLE_CLOSE, tensor_tuple_list=tensor_tuple_list,\n                                  scope_separator_at_start=True)\n    else:\n        assert not isinstance(container, (dict, tuple))\n        tensor_tuple_list.append((key_scope, container))\n\n    # Non recursive (first) call -> Return the final dict.\n    if ret:\n        return DataOpDict(tensor_tuple_list)\n\n\ndef define_by_run_split_args(add_auto_key_as_first_param, *args, **kwargs):\n    """"""\n    Splits any container in *args and **kwargs and collects them to be evaluated\n    one by one by a graph_fn. If more than one container inputs exists in *args and **kwargs,\n    these must have the exact same keys.\n\n    Note that this does not perform any checks on aligned keys, these were performed at build time.\n\n    Args:\n        add_auto_key_as_first_param (bool): Add auto-key as very first parameter in each\n        returned parameter tuple.\n        *args (op): args to split.\n        **kwargs (op): kwargs to split.\n\n    Returns:\n        Union[OrderedDict,Tuple]]: The sorted parameter tuples (by flat-key) to use as inputs.\n    """"""\n\n    # Collect Dicts for checking their keys (must match).\n\n    flattened_args = []\n    # Use the loop below to unwrap single-key dicts with default keys to be used if no\n    # true splitting is happening.\n    unwrapped_args = [""""] if add_auto_key_as_first_param is True else []\n    lead_container_arg = None\n    if get_backend() == ""pytorch"":\n        for arg in args:\n            # Convert raw torch tensors: during flattening, we do not flatten single tensors\n            # to avoid flattening altogether if there are strictly raw tensors.\n            if isinstance(arg, torch.Tensor):\n                flattened_args.append({"""": arg})\n                unwrapped_args.append(arg)\n                # Append raw tensor.\n            elif isinstance(arg, dict):\n                if len(arg) > 1 or """" not in arg:\n                    flattened_args.append(arg)\n                    # Use first encountered container arg.\n                    if lead_container_arg is None:\n                        lead_container_arg = arg\n                else:\n                    unwrapped_args.append(arg[""""])\n\n    # One or more dicts: Split the calls.\n    if len(flattened_args) > 0 and lead_container_arg is not None:\n        # Re-create our iterators.\n        collected_call_params = OrderedDict()\n\n        for key in lead_container_arg.keys():\n            # Prep input params for a single call.\n            params = [key] if add_auto_key_as_first_param is True else []\n\n            for arg in flattened_args:\n                if isinstance(arg, dict):\n                    params.append(arg[key] if key in arg else arg[""""])\n                else:\n                    params.append(arg)\n\n            # Add kwarg_ops\n            kwargs = {}\n            for kwarg_key, kwarg_op in kwargs.items():\n                kwargs[key] = kwargs[kwarg_key][key] if key in kwargs[kwarg_key] else kwargs[kwarg_key][""""]\n\n            params = params[0] if len(params) == 1 else params\n            if kwargs:\n                collected_call_params[key] = (params, kwargs)\n            else:\n                collected_call_params[key] = params\n        return collected_call_params\n    # We don\'t have any containers: No splitting possible. Return args and kwargs as is.\n    else:\n        return unwrapped_args, {key: value[""""] for key, value in kwargs.items()}\n\n\ndef define_by_run_unflatten(result_dict):\n    """"""\n    Takes a dict with auto-generated keys and returns the corresponding\n    unflattened dict.\n    If the only key in the input dict is """", it returns the value under\n    that key.\n\n    Args:\n        result_dict (dict): The item to be unflattened (re-nested).\n\n    Returns:\n        Dict: The unflattened (re-nested) item.\n    """"""\n    # Special case: Dict with only 1 value (key="""")\n    if len(result_dict) == 1 and """" in result_dict:\n        return result_dict[""""]\n\n    # Normal case: OrderedDict that came from a ContainerItem.\n    base_structure = None\n\n    op_names = sorted(result_dict.keys())\n    for op_name in op_names:\n        op_val = result_dict[op_name]\n        parent_structure = None\n        parent_key = None\n        current_structure = None\n        op_type = None\n\n        if op_name.startswith(FLATTEN_SCOPE_PREFIX):\n            op_name = op_name[1:]\n        # N.b. removed this because we do not prepend / any more before first key.\n        op_key_list = op_name.split(FLATTEN_SCOPE_PREFIX)  # skip 1st char (/)\n        for sub_key in op_key_list:\n            mo = re.match(r\'^{}(\\d+){}$\'.format(FLAT_TUPLE_OPEN, FLAT_TUPLE_CLOSE), sub_key)\n            if mo:\n                op_type = list\n                idx = int(mo.group(1))\n            else:\n                op_type = OrderedDict\n                idx = sub_key\n\n            if current_structure is None:\n                if base_structure is None:\n                    base_structure = [None] if op_type == list else DataOpDict()\n                current_structure = base_structure\n            elif parent_key is not None:\n                if (isinstance(parent_structure, list) and (parent_structure[parent_key] is None)) or \\\n                        (isinstance(parent_structure, DataOpDict) and parent_key not in parent_structure):\n                    current_structure = [None] if op_type == list else DataOpDict()\n                    parent_structure[parent_key] = current_structure\n                else:\n                    current_structure = parent_structure[parent_key]\n                    if op_type == list and len(current_structure) == idx:\n                        current_structure.append(None)\n\n            parent_structure = current_structure\n            parent_key = idx\n            if isinstance(parent_structure, list) and len(parent_structure) == parent_key:\n                parent_structure.append(None)\n\n        if op_type == list and len(current_structure) == parent_key:\n            current_structure.append(None)\n        current_structure[parent_key] = op_val\n\n    # Deep conversion from list to tuple.\n    # TODO necessary in define by run?\n    return deep_tuple(base_structure)\n\n\ndef define_by_run_unpack(args):\n    """"""\n    Unpacks potential nested FlattenedDataOp wrapper args.\n    Args:\n        args (any):\n\n    Returns:\n        any: Unpacked args.\n    """"""\n    if isinstance(args, FlattenedDataOp) and len(args) == 1:\n        return next(iter(args.values()))\n    elif isinstance(args, (list, tuple)):\n        ret = []\n        # Check list elements one by one.\n        for value in args:\n            if isinstance(value, FlattenedDataOp) and len(value) == 1:\n                ret.append(next(iter(value.values())))\n            else:\n                ret.append(value)\n        return ret\n    else:\n        return args\n\n# TODO circular import issue, figure out where to put this fn.\nfrom rlgraph.spaces import Dict\n\n\ndef execute_define_by_run_graph_fn(component, graph_fn, options, *args, **kwargs):\n    """"""\n    Executes a graph_fn in define by run mode.\n\n    Args:\n        component (Component): Component this graph_fn is eecuted on.\n        graph_fn (callable): Graph function to execute.\n        options (dict): Execution options.\n    Returns:\n        any: Results of executing this graph-fn.\n    """"""\n    flatten_ops = options.pop(""flatten_ops"", False)\n    split_ops = options.pop(""split_ops"", False)\n    add_auto_key_as_first_param = options.pop(""add_auto_key_as_first_param"", False)\n\n    # No container arg handling.\n    if not flatten_ops:\n        return graph_fn(component, *args, **kwargs)\n    else:\n        # Flatten and identify containers for potential splits.\n        flattened_args = []\n\n        # Was there actually any flattening\n        args_actually_flattened = False\n        for arg in args:\n            if isinstance(arg, (Dict, dict, tuple)) or isinstance(arg, Dict) or isinstance(arg, tuple):\n                flattened_args.append(define_by_run_flatten(arg))\n                args_actually_flattened = True\n            else:\n                flattened_args.append(arg)\n\n        flattened_kwargs = {}\n        if len(kwargs) > 0:\n            for key, arg in kwargs.items():\n                if isinstance(arg, dict) or isinstance(arg, Dict) or isinstance(arg, tuple):\n                    flattened_kwargs[key] = define_by_run_flatten(arg)\n                    args_actually_flattened = True\n                else:\n                    flattened_kwargs[key] = arg\n\n        # If splitting args, split then iterate and merge. Only split if some args were actually flattened.\n        if args_actually_flattened:\n            split_args_and_kwargs = define_by_run_split_args(add_auto_key_as_first_param,\n                                                             *flattened_args, **flattened_kwargs)\n\n            # Idea: Unwrap light flattening by iterating over flattened args and reading out """" where possible\n            if split_ops and isinstance(split_args_and_kwargs, OrderedDict):\n                # Args were actually split.\n                ops = {}\n                num_return_values = -1\n                for key, params in split_args_and_kwargs.items():\n                    # Are there any kwargs?\n                    if isinstance(params, tuple):\n                        params_args = params[0]\n                        params_kwargs = params[1]\n                    else:\n                        params_args = params\n                        params_kwargs = {}\n                    ops[key] = graph_fn(component, *params_args, **params_kwargs)\n                    if hasattr(ops[key], ""shape""):\n                        num_return_values = 1\n                    else:\n                        num_return_values = len(ops[key])\n\n                # Un-split the results dict into a tuple of `num_return_values` slots.\n                un_split_ops = []\n                for i in range(num_return_values):\n                    dict_with_singles = OrderedDict()\n                    for key in split_args_and_kwargs.keys():\n                        # Use tensor as is.\n                        if hasattr(ops[key], ""shape""):\n                            dict_with_singles[key] = ops[key]\n                        else:\n                            dict_with_singles[key] = ops[key][i]\n                    un_split_ops.append(dict_with_singles)\n\n                flattened_ret = tuple(un_split_ops)\n            else:\n                if isinstance(split_args_and_kwargs, OrderedDict):\n                    flattened_ret = graph_fn(component, split_args_and_kwargs)\n                else:\n                    # Args and kwargs tuple.\n                    split_args = split_args_and_kwargs[0]\n                    split_kwargs = split_args_and_kwargs[1]\n                    # Args did not contain deep nested structure so\n                    flattened_ret = graph_fn(component, *split_args, **split_kwargs)\n\n            if flattened_ret is None:\n                return None\n            # If result is a raw tensor, return as is.\n            if get_backend() == ""pytorch"":\n                if isinstance(flattened_ret, torch.Tensor):\n                    return flattened_ret\n\n            unflattened_ret = []\n            for i, op in enumerate(flattened_ret):\n                # Try to re-nest ordered-dict it.\n                if isinstance(op, OrderedDict):\n                    unflattened_ret.append(define_by_run_unflatten(op))\n                # All others are left as-is.\n                else:\n                    unflattened_ret.append(op)\n\n            # Return unflattened results.\n            return unflattened_ret[0] if len(unflattened_ret) == 1 else unflattened_ret\n        else:\n            # Just pass in args and kwargs because not actually flattened, with or without default key.\n            if add_auto_key_as_first_param:\n                ret = graph_fn(component, """", *args, **kwargs)\n            else:\n                ret = graph_fn(component, *args, **kwargs)\n            if ret is not None:\n                return define_by_run_unpack(ret)\n            else:\n                return None'"
rlgraph/utils/initializer.py,2,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\nfrom rlgraph.utils.specifiable import Specifiable\nfrom rlgraph.utils.util import convert_dtype\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\n# TODO why is this here and not in e.g. layers?\nclass Initializer(Specifiable):\n    def __init__(self, shape, specification=None, **kwargs):\n        """"""\n        Args:\n            shape (tuple): The shape of the Variables to initialize.\n            specification (any): A spec that determines the nature of this initializer.\n\n        Raises:\n            RLGraphError: If a fixed shape in `specification` does not match `shape`.\n        """"""\n        super(Initializer, self).__init__()\n\n        # The shape of the variable to be initialized.\n        self.shape = shape\n        # The actual underlying initializer object.\n        self.initializer = None\n\n        # Truncated Normal.\n        if specification == ""truncated_normal"":\n            if get_backend() == ""tf"":\n                # Use the first dimension (num_rows or batch rank) to figure out the stddev.\n                stddev = 1 / math.sqrt(shape[0] if isinstance(shape, (tuple, list)) and len(shape) > 0 else 1.0)\n                self.initializer = tf.truncated_normal_initializer(stddev=stddev)\n            elif get_backend() == ""pytorch"":\n                stddev = 1 / math.sqrt(shape[0] if isinstance(shape, (tuple, list)) and len(shape) > 0 else 1.0)\n                self.initializer = lambda t: torch.nn.init.normal_(tensor=t, std=stddev)\n\n        # No spec -> Leave initializer as None for TF (will then use default;\n        #  e.g. for tf weights: Xavier uniform). For PyTorch, still have to set Xavier.\n        # TODO this is None or is False is very unclean because TF and PT have different defaults ->\n        # change to clean default values for weights and biases.\n        elif specification is None or specification is False:\n            if get_backend() == ""tf"":\n                pass\n            elif get_backend() == ""pytorch"":\n                self.initializer = torch.nn.init.xavier_uniform_\n\n        # Fixed values spec -> Use them, just do sanity checking.\n        else:\n            # Constant value across the variable.\n            if isinstance(specification, (float, int)):\n                pass\n            # A 1D initializer (e.g. for biases).\n            elif isinstance(specification, list):\n                array = np.asarray(specification, dtype=convert_dtype(""float32"", ""np""))\n                if array.shape != self.shape:\n                    raise RLGraphError(""ERROR: Number/shape of given items ({}) not identical with shape ({})!"".\n                                       format(array.shape, self.shape))\n            # A nD initializer (numpy-array).\n            elif isinstance(specification, np.ndarray):\n                if specification.shape != self.shape:\n                    raise RLGraphError(""ERROR: Shape of given items ({}) not identical with shape ({})!"".\n                                       format(specification.shape, self.shape))\n            # Unknown type.\n            else:\n                raise RLGraphError(""ERROR: Bad specification given ({}) for Initializer object!"".format(specification))\n\n            # Create the backend initializer object.\n            if get_backend() == ""tf"":\n                self.initializer = tf.constant_initializer(value=specification, dtype=convert_dtype(""float32""))\n            elif get_backend() == ""pytorch"":\n                self.initializer = lambda t: torch.nn.init.constant_(tensor=t, val=specification)'"
rlgraph/utils/input_parsing.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.common.synchronizable import Synchronizable\nfrom rlgraph.components.neural_networks.value_function import ValueFunction\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\nfrom rlgraph.utils.util import default_dict\n\n\ndef parse_saver_spec(saver_spec):\n    """"""\n    Parses the saver spec. Returns None if input None, otherwise\n    provides default parameters.\n\n    Args:\n        saver_spec (Union[None, dict]): Saver parameters.\n\n    Returns:\n        Union(dict, None): Saver spec or None.\n    """"""\n\n    if saver_spec is None:\n        return None\n    default_spec = dict(\n        # The directory in which to store model checkpoint files.\n        directory=os.path.expanduser(""~/rlgraph_checkpoints/""),  # default=home dir\n        # The base file name for a saved checkpoint.\n        checkpoint_basename=""model.ckpt"",\n        # How many files to maximally store for one graph.\n        max_checkpoints=5,\n        # Every how many seconds do we save? None if saving frequency should be step based.\n        save_secs=600,\n        # Every how many steps do we save? None if saving frequency should be time (seconds) based.\n        save_steps=None\n    )\n    return default_dict(saver_spec, default_spec)\n\n\ndef parse_value_function_spec(value_function_spec):\n    """"""\n    Creates value function from spec if needed.\n\n    Args:\n        value_function_spec (list, dict, ValueFunction): Neural network specification for baseline or instance\n            of ValueFunction.\n    Returns:\n        Optional(ValueFunction): None if Spec is None or instance of ValueFunction.\n    """"""\n    if value_function_spec is None:\n        return None\n    else:\n        vf_sync = Synchronizable()\n        # VF is already instantiated.\n        if isinstance(value_function_spec, ValueFunction):\n            value_function = value_function_spec\n            if vf_sync.name not in value_function.sub_components:\n                value_function.add_components(vf_sync, expose_apis=""sync"")\n        else:\n            if isinstance(value_function_spec, list):\n                # Use default type if given is layer-list.\n                value_function_spec = dict(type=""value_function"", network_spec=value_function_spec)\n            value_function = ValueFunction.from_spec(value_function_spec)\n            value_function.add_components(vf_sync, expose_apis=""sync"")\n        return value_function\n\n\ndef parse_summary_spec(summary_spec):\n    """"""\n    Expands summary spec with default values where necessary.\n\n    Args:\n        summary_spec (dict): Summary options.\n\n    Returns:\n        dict: Summary spec updated with default values.\n    """"""\n    default_spec = dict(\n        # The directory in which to store the summary files.\n        directory=os.path.expanduser(""~/rlgraph_summaries/""),  # default=home dir\n        # A regexp pattern that a summary op (including its global scope) has to match in order for it to\n        # be included in the graph\'s summaries.\n        summary_regexp=None,\n        # Every how many seconds do we save a summary? None if saving frequency should be step based.\n        save_secs=120,\n        # Every how many steps do we save a summary? None if saving frequency should be time (seconds) based.\n        save_steps=None\n    )\n    return default_dict(summary_spec, default_spec)\n\n\ndef parse_execution_spec(execution_spec):\n    """"""\n    Parses execution parameters and inserts default values where necessary.\n\n    Args:\n        execution_spec (Optional[dict]): Execution spec dict. Must specify an execution mode\n            ""single"" or ""distributed"". If mode ""distributed"", must specify a ""distributed_spec""\n            containing:\n             - a key cluster_spec mapping to a ClusterSpec object,\n             - a ""job"" for the job name,\n             - an integer ""task_index""\n\n    Returns:\n        dict: The sanitized execution_spec dict.\n    """"""\n    # TODO these are tensorflow specific\n    # If no spec given.\n    if get_backend() == ""tf"":\n        default_spec = dict(\n            mode=""single"",\n            distributed_spec=None,\n            # Using a monitored session enabling summaries and hooks per default.\n            disable_monitoring=False,\n            # Gpu settings.\n            gpu_spec=dict(\n                # Are GPUs allowed to be used if they are detected?\n                gpus_enabled=False,\n                # If yes, how many GPUs are to be used?\n                max_usable_gpus=0,\n                # If True, use `max_usable_gpus` fake-GPUs (CPU) iff no GPUs are available.\n                fake_gpus_if_necessary=False,\n                # Specify specific CUDA devices to be used, e.g. gpu 0 and 2 = [0, 2].\n                # If None, we use CUDA devices [0, max_usable_gpus - 1]\n                cuda_devices=None,\n                # Fraction of the overall amount of memory that each visible GPU should be allocated.\n                per_process_gpu_memory_fraction=None,\n                # If True, not all memory will be allocated which is relevant on shared resources.\n                allow_memory_growth=False\n            ),\n            # Device placement settings.\n            device_strategy=""default"",\n            default_device=None,\n            device_map={},\n\n            session_config=None,\n            # Random seed for the tf graph.\n            seed=None,\n            # Enabling the tf profiler?\n            enable_profiler=False,\n            # With which frequency do we print out profiler information?\n            profiler_frequency=1000,\n            # Enabling a timeline write?\n            enable_timeline=False,\n            # With which frequency do we write out a timeline file?\n            timeline_frequency=1,\n        )\n        execution_spec = default_dict(execution_spec, default_spec)\n\n        # Sub specifications:\n\n        # Distributed specifications.\n        if execution_spec.get(""mode"") == ""distributed"":\n            default_distributed = dict(\n                job=""ps"",\n                task_index=0,\n                cluster_spec=dict(\n                    ps=[""localhost:22222""],\n                    worker=[""localhost:22223""]\n                ),\n                protocol=None\n            )\n            execution_spec[""distributed_spec""] = default_dict(execution_spec.get(""distributed_spec""), default_distributed)\n\n        # Session config.\n        default_session_config = dict(\n            type=""monitored-training-session"",\n            allow_soft_placement=True,\n            log_device_placement=False\n        )\n        execution_spec[""session_config""] = default_dict(execution_spec.get(""session_config""), default_session_config)\n    elif get_backend() == ""pytorch"":\n        # No session configs, different GPU options.\n        default_spec = dict(\n            mode=""single"",\n            distributed_spec=None,\n            # Using a monitored session enabling summaries and hooks per default.\n            disable_monitoring=False,\n            # Gpu settings.\n            gpu_spec=dict(\n                # Are GPUs allowed to be used if they are detected?\n                gpus_enabled=False,\n                # If yes, how many GPUs are to be used?\n                max_usable_gpus=0,\n                # Specify specific CUDA devices to be used, e.g. gpu 0 and 2 = [0, 2].\n                # If None, we use CUDA devices [0, max_usable_gpus - 1]\n                cuda_devices=None\n            ),\n            # Device placement settings.\n            device_strategy=""default"",\n            default_device=None,\n            device_map={},\n            # TODO potentially set to nproc?\n            torch_num_threads=1,\n            OMP_NUM_THREADS=1\n        )\n        execution_spec = default_dict(execution_spec, default_spec)\n\n    return execution_spec\n\n\ndef parse_observe_spec(observe_spec):\n    """"""\n    Parses parameters for `Agent.observe()` calls and inserts default values where necessary.\n\n    Args:\n        observe_spec (Optional[dict]): Observe spec dict.\n\n    Returns:\n        dict: The sanitized observe_spec dict.\n    """"""\n    if observe_spec and ""buffer_enabled"" not in observe_spec:\n        # Set to true if a buffer size is given and > 0, but flag is not given.\n        if ""buffer_size"" in observe_spec and observe_spec[""buffer_size""] > 0:\n            observe_spec[""buffer_enabled""] = True\n\n    # If no spec given.\n    default_spec = dict(\n        # Do we buffer observations in python before sending them through the graph?\n        buffer_enabled=False,\n        # Fill buffer with n records before sending them through the graph.\n        buffer_size=100,  # only if buffer_enabled=True\n        # Set to > 1 if we want to post-process buffered values for n-step learning.\n        n_step=1,  # values > 1 are only allowed if buffer_enabled is True and buffer_size >> n.\n    )\n    observe_spec = default_dict(observe_spec, default_spec)\n\n    if observe_spec[""n_step""] > 1:\n        if observe_spec[""buffer_enabled""] is False:\n            raise RLGraphError(\n                ""Cannot setup observations with n-step (n={}), while buffering is switched ""\n                ""off"".format(observe_spec[""n_step""])\n            )\n        elif observe_spec[""buffer_size""] < 3 * observe_spec[""n_step""]:\n            raise RLGraphError(\n                ""Buffer must be at least 3x as large as n-step (n={}, min-buffer={})!"".format(\n                    observe_spec[""n_step""], 3 * observe_spec[""n_step""])\n            )\n\n    return observe_spec\n\n\ndef parse_update_spec(update_spec):\n    """"""\n    Parses update/learning parameters and inserts default values where necessary.\n\n    Args:\n        update_spec (Optional[dict]): Update/Learning spec dict.\n\n    Returns:\n        dict: The sanitized update_spec dict.\n    """"""\n    # If no spec given.\n    default_spec = dict(\n        # Whether to perform calls to `Agent.update()` at all.\n        do_updates=True,\n        # The unit in which we measure frequency: one of ""timesteps"", ""episodes"", ""sec"".\n        # unit=""timesteps"", # TODO: not supporting any other than timesteps\n        # The number of \'units\' to wait before we do any updating at all.\n        steps_before_update=0,\n        # The frequency with which we update (given in `unit`).\n        update_interval=4,\n        # The number of consecutive `Agent.update()` calls per update.\n        update_steps=1,\n        # The batch size with which to update (e.g. when pulling records from a memory).\n        batch_size=64,\n        sync_interval=128\n    )\n    update_spec = default_dict(update_spec, default_spec)\n\n    return update_spec\n'"
rlgraph/utils/model_util.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport logging\n\nfrom rlgraph.components import ValueFunction\n\n\ndef register_value_function(name, cls):\n    """"""\n    Registers a custom value function by name and type.\n\n    After registration, the custom network can be used by passing to agents\n    a value function spec with type=name.\n\n    Args:\n        name (str): Name of custom value function. Should usually correspond to the type name, e.g. if the class\n            is named MyCustomVF, the name should be my_custom_vf or similar.\n        cls (ValueFunction): Custom value function inheriting from ValueFuction.\n    """"""\n    if name in ValueFunction.__lookup_classes__[name]:\n        raise ValueError(""Name {} is already defined in ValueFunctions. All names are: {}"".format(\n            name, ValueFunction.__lookup_classes__.keys()\n        ))\n    else:\n        ValueFunction.__lookup_classes__[name] = cls\n        logging.info(""Registered custom value function {} under name: {}"".format(cls, name))'"
rlgraph/utils/numpy.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom rlgraph.utils.util import SMALL_NUMBER\n\n\ndef sigmoid(x, derivative=False):\n    """"""\n    Returns the sigmoid function applied to x.\n    Alternatively, can return the derivative or the sigmoid function.\n\n    Args:\n        x (np.ndarray): The input to the sigmoid function.\n        derivative (bool): Whether to return the derivative or not. Default: False.\n\n    Returns:\n        np.ndarray: The sigmoid function (or its derivative) applied to x.\n    """"""\n    if derivative:\n        return x * (1 - x)\n    else:\n        return 1 / (1 + np.exp(-x))\n\n\ndef softmax(x, axis=-1):\n    """"""\n    Returns the softmax values for x as:\n    S(xi) = e^xi / SUMj(e^xj), where j goes over all elements in x.\n\n    Args:\n        x (np.ndarray): The input to the softmax function.\n        axis (int): The axis along which to softmax.\n\n    Returns:\n        np.ndarray: The softmax over x.\n    """"""\n    # Below is more like what we do in RLgraph\'s tf implementations.\n    x_exp = np.exp(x)\n    return np.maximum(x_exp / np.sum(x_exp, axis, keepdims=True), SMALL_NUMBER)\n\n\ndef relu(x, alpha=0.0):\n    """"""\n    Implementation of the leaky ReLU function:\n    y = x * alpha if x < 0 else x\n\n    Args:\n        x (np.ndarray): The input values.\n        alpha (float): A scaling (""leak"") factor to use for negative x.\n\n    Returns:\n        np.ndarray: The leaky ReLU output for x.\n    """"""\n    return np.maximum(x, x*alpha, x)\n\n\ndef one_hot(x, depth=0, on_value=1, off_value=0):\n    """"""\n    One-hot utility function for numpy.\n    Thanks to qianyizhang:\n    https://gist.github.com/qianyizhang/07ee1c15cad08afb03f5de69349efc30.\n\n    Args:\n        x (np.ndarray): The input to be one-hot encoded.\n        depth (int): The max. number to be one-hot encoded (size of last rank).\n        on_value (float): The value to use for on. Default: 1.0.\n        off_value (float): The value to use for off. Default: 0.0.\n\n    Returns:\n        np.ndarray: The one-hot encoded equivalent of the input array.\n    """"""\n    # Handle bool arrays correctly.\n    if x.dtype == np.bool_:\n        x = x.astype(np.int)\n        depth = 2\n\n    if depth == 0:\n        depth = np.max(x) + 1\n    assert np.max(x) < depth, ""ERROR: The max. index of `x` ({}) is larger than depth ({})!"".format(np.max(x), depth)\n    shape = x.shape\n\n    # Python 2.7 compatibility, (*shape, depth) is not allowed.\n    shape_list = [dim for dim in shape]\n    shape_list.append(depth)\n    out = np.ones(shape_list) * off_value\n    indices = []\n    for i in range(x.ndim):\n        tiles = [1] * x.ndim\n        s = [1] * x.ndim\n        s[i] = -1\n        r = np.arange(shape[i]).reshape(s)\n        if i > 0:\n            tiles[i-1] = shape[i-1]\n            r = np.tile(r, tiles)\n        indices.append(r)\n    indices.append(x)\n    out[tuple(indices)] = on_value\n    return out\n\n\ndef dense_layer(x, weights, biases=None):\n    """"""\n    Calculates the outputs of a dense layer given weights/biases and an input.\n\n    Args:\n        x (np.ndarray): The input to the dense layer.\n        weights (np.ndarray): The weights matrix.\n        biases (Optional[np.ndarray]): The biases vector. All 0s if None.\n\n    Returns:\n        The dense layer\'s output.\n    """"""\n    return np.matmul(x, weights) + (0.0 if biases is None else biases)\n\n\ndef lstm_layer(x, weights, biases=None, initial_internal_states=None, time_major=False, forget_bias=1.0):\n    """"""\n    Calculates the outputs of an LSTM layer given weights/biases, internal_states, an input.\n\n    Args:\n        x (np.ndarray): The inputs to the LSTM layer including time-rank (oth if time-major, else 1st) and\n            the batch-rank (1st if time-major, else 0th).\n        weights (np.ndarray): The weights matrix.\n        biases (Optional[np.ndarray]): The biases vector. All 0s if None.\n        initial_internal_states (Optional[np.ndarray]): The initial internal states to pass into the layer.\n            All 0s if None.\n        time_major (bool): Whether to use time-major or not. Default: False.\n        forget_bias (float): Gets added to first sigmoid (forget gate) output.\n            Default: 1.0.\n\n    Returns:\n        Tuple:\n            - The LSTM layer\'s output.\n            - Tuple: Last (c-state, h-state).\n    """"""\n    sequence_length = x.shape[0 if time_major else 1]\n    batch_size = x.shape[1 if time_major else 0]\n    units = weights.shape[1] // 4  # 4 internal layers (3x sigmoid, 1x tanh)\n\n    if initial_internal_states is None:\n        c_states = np.zeros(shape=(batch_size, units))\n        h_states = np.zeros(shape=(batch_size, units))\n    else:\n        c_states = initial_internal_states[0]\n        h_states = initial_internal_states[1]\n\n    # Create a placeholder for all n-time step outputs.\n    if time_major:\n        unrolled_outputs = np.zeros(shape=(sequence_length, batch_size, units))\n    else:\n        unrolled_outputs = np.zeros(shape=(batch_size, sequence_length, units))\n\n    # Push the batch 4 times through the LSTM cell and capture the outputs plus the final h- and c-states.\n    for t in range(sequence_length):\n        input_matrix = x[t, :, :] if time_major else x[:, t, :]\n        input_matrix = np.concatenate((input_matrix, h_states), axis=1)\n        input_matmul_matrix = np.matmul(input_matrix, weights) + biases\n        # Forget gate (3rd slot in tf output matrix). Add static forget bias.\n        sigmoid_1 = sigmoid(input_matmul_matrix[:, units*2:units*3] + forget_bias)\n        c_states = np.multiply(c_states, sigmoid_1)\n        # Add gate (1st and 2nd slots in tf output matrix).\n        sigmoid_2 = sigmoid(input_matmul_matrix[:, 0:units])\n        tanh_3 = np.tanh(input_matmul_matrix[:, units:units*2])\n        c_states = np.add(c_states, np.multiply(sigmoid_2, tanh_3))\n        # Output gate (last slot in tf output matrix).\n        sigmoid_4 = sigmoid(input_matmul_matrix[:, units*3:units*4])\n        h_states = np.multiply(sigmoid_4, np.tanh(c_states))\n\n        # Store this output time-slice.\n        if time_major:\n            unrolled_outputs[t, :, :] = h_states\n        else:\n            unrolled_outputs[:, t, :] = h_states\n\n    return unrolled_outputs, (c_states, h_states)\n'"
rlgraph/utils/op_records.py,0,"b'# Copyright 2018/2019 The RLgraph Authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport inspect\n\nimport numpy as np\n\nfrom rlgraph.spaces.space_utils import get_space_from_op\nfrom rlgraph.utils import convert_dtype\nfrom rlgraph.utils.ops import FlattenedDataOp, flatten_op, unflatten_op, is_constant, DataOpDict\nfrom rlgraph.utils.rlgraph_errors import RLGraphError, RLGraphAPICallParamError\n\n\nclass DataOpRecord(object):\n    """"""\n    A simple wrapper class for a DataOp carrying the op itself and some additional information about it.\n    """"""\n    # The current ID value.\n    _ID = -1\n    # The highest allowed ID (can be changed to any higher value; used e.g. to compute sorting keys for\n    # build-prioritization).\n    MAX_ID = 1e6\n\n    def __init__(self, op=None, column=None, position=None, kwarg=None, space=None, previous=None, next_record=None,\n                 placeholder=None):\n        """"""\n        Args:\n            op (Optional[DataOp]): The optional DataOp to already store in this op-rec.\n            column (DataOpRecordColumn): The DataOpRecordColumn to which this op-rec belongs.\n            position (Optional[int]): An optional position (index) for this op inside `column`.\n\n            kwarg (Optional[str]): The keyword with which to call the API-method if this op-rec is not a positional\n                arg.\n\n            space (Optional[Space]): The Space of `op` if already known at construction time. Will be poulated\n                later (during build phase) if not.\n\n            next_ (Optional(Set[DataOpRecord],DataOpRecord)): The next op-record or set of op-records.\n            previous (Optional(DataOpRecord)): The previous op-record.\n\n            placeholder (Optional[str]): If this is a placeholder op-rec, what is the name of the placeholder arg\n                (root\'s API input-arg name).\n        """"""\n        self.id = self.get_id()\n        self.op = op\n        # Some instruction on how to derive the `op` property of this record (other than just: pass along).\n        # e.g. ""key-lookup: [some key]"" if previous op is a DataOpDict.\n        self.op_instructions = dict()\n        # Whether the op in this record is one of the last in the graph (a core API-method returned op).\n        self.is_terminal_op = False\n\n        self.column = column\n        self.position = position\n        self.kwarg = kwarg\n\n        # The inferred Space of this op.\n        self.space = space\n\n        # Set of (op-col ID, slot) tuples that are connected from this one.\n        self.next = next_record if isinstance(next_record, set) else ({next_record}\n                                                                      if next_record is not None else set())\n        # The previous op that lead to this one.\n        self.previous = previous\n\n        self.placeholder = placeholder\n\n    def connect_to(self, next_op_rec):\n        """"""\n        Connects this op-rec to a next one by passing on the `op` and `space` properties\n        and correctly setting the `next` and `previous` pointers in both op-recs.\n\n        Args:\n            next_op_rec (DataOpRecord): The next DataOpRecord to connect this one to.\n        """"""\n        # If already connected, make sure connection is the same as the already existing one.\n        if next_op_rec.previous is not None:\n            assert next_op_rec.previous is self\n        else:\n            # Set `previous` pointer.\n            next_op_rec.previous = self\n\n        # We do have an op -> Pass it (and its Space) on to the next op-rec.\n        if self.op is not None:\n            # Push op and Space into next op-record.\n            # With op-instructions?\n            #if ""key-lookup"" in next_op_rec.op_instructions:\n            if ""key-lookup"" in self.op_instructions:\n                lookup_key = self.op_instructions[""key-lookup""]\n                if isinstance(lookup_key, str):\n                    found_op = None\n                    found_space = None\n                    if isinstance(self.op, dict):\n                        assert isinstance(self.op, DataOpDict)\n                        if lookup_key in self.op:\n                            found_op = self.op[lookup_key]\n                            found_space = self.space[lookup_key]\n                        # Lookup-key could also be a flat-key. -> Try to find entry in nested (dict) op.\n                        else:\n                            found_op = self.op.flat_key_lookup(lookup_key)\n                            if found_op is not None:\n                                found_space = self.space.flat_key_lookup(lookup_key)\n\n                    # Did we find anything? If not, error for invalid key-lookup.\n                    if found_op is None or found_space is None:\n                        raise RLGraphError(\n                            ""Op ({}) is not a dict or does not contain the lookup key \'{}\'!"". \\\n                            format(self.op, lookup_key)\n                        )\n\n                    next_op_rec.op = found_op\n                    next_op_rec.space = found_space\n\n                elif isinstance(lookup_key, int) and \\\n                        (not isinstance(self.op, (list, tuple)) or lookup_key >= len(self.op)):\n                    raise RLGraphError(\n                        ""Op ({}) is not a list/tuple or contains not enough items for lookup ""\n                        ""index \'{}\'!"".format(self.op, lookup_key)\n                    )\n\n                else:\n                    next_op_rec.op = self.op[lookup_key]\n                    next_op_rec.space = self.space[lookup_key]\n            # No instructions -> simply pass on.\n            else:\n                next_op_rec.op = self.op\n                next_op_rec.space = self.space\n\n            assert next_op_rec.space is not None\n            #next_op_rec.space = get_space_from_op(self.op)\n\n        # Add `next` connection.\n        self.next.add(next_op_rec)\n\n    @staticmethod\n    def get_id():\n        DataOpRecord._ID += 1\n        if DataOpRecord._ID >= DataOpRecord.MAX_ID:\n            raise RLGraphError(""Maximum number of op-rec IDs reached! Simply hard-increase `DataOpRecord.MAX_ID`."")\n        return DataOpRecord._ID\n\n    @staticmethod\n    def reset():\n        DataOpRecord._ID = -1\n\n    def __getitem__(self, key):\n        """"""\n        Creates new DataOpRecordColumn with a single op-rec pointing via its `op_instruction` dict\n        back to the previous column\'s op-rec (this one). This can be used to instruct the building process to\n        do tuple/dict lookups during the build process for a more intuitive handling of DataOpRecords within Component\n        API methods.\n\n        Args:\n            key (str): The lookup key.\n\n        Returns:\n            A new DataOpRecord with the op_instructions set to do a tuple (idx) or dict (key) lookup at build time.\n        """"""\n        # TODO: This should be some specific type?\n        column = DataOpRecordColumn(\n            self.column.component, args=[self]\n        )\n        column.op_records[0].op_instructions[""key-lookup""] = key\n        return column.op_records[0]\n\n    def __str__(self):\n        return ""DataOpRec(id={} {}{})"".format(\n            self.id,""pos=""+str(self.position) if self.kwarg is None else ""kwarg=""+self.kwarg,\n            """" if self.column is None else "" in ""+str(self.column)\n        )\n\n    def __hash__(self):\n        return hash(self.id)\n\n\nclass DataOpRecordColumn(object):\n    """"""\n    A DataOpRecordColumn is a list of DataOpRecords that either go into (a call) or come from (return) a\n    Component\'s GraphFn or API method.\n    """"""\n    _ID = -1\n\n    def __init__(self, component, num_op_records=None, args=None, kwargs=None):\n        """"""\n        Args:\n            component (Component): The Component to which this column belongs.\n        """"""\n        self.id = self.get_id()\n\n        if num_op_records is None:\n            self.op_records = []\n            if args is not None:\n                args = list(args)\n                for i in range(len(args)):\n                    if args[i] is None:\n                        continue\n                    op_rec = DataOpRecord(op=None, column=self, position=i)\n\n                    # Dict instead of a DataOpRecord -> Translate on the fly into a DataOpRec held by a\n                    # ContainerMerger Component.\n                    if isinstance(args[i], dict):\n                        items = args[i].items()\n                        keys = [k for k, _ in items]\n                        values = [v for _, v in items]\n                        if isinstance(values[0], DataOpRecord):\n                            merger_component = values[0].column.component.get_helper_component(\n                                ""container-merger"", _args=list(keys)\n                            )\n                            args[i] = merger_component.merge(*list(values))\n                    # Tuple instead of a DataOpRecord -> Translate on the fly into a DataOpRec held by a\n                    # ContainerMerger Component.\n                    elif isinstance(args[i], tuple) and isinstance(args[i][0], DataOpRecord):\n                        merger_component = args[i][0].column.component.get_helper_component(\n                            ""container-merger"", _args=len(args[i])\n                        )\n                        args[i] = merger_component.merge(*args[i])\n\n                    # If incoming is an op-rec -> Link them.\n                    if isinstance(args[i], DataOpRecord):\n                        args[i].connect_to(op_rec)\n                    # Do constant value assignment here.\n                    elif args[i] is not None:\n                        op = args[i]\n                        if is_constant(op) and not isinstance(op, np.ndarray):\n                            op = np.array(op, dtype=convert_dtype(type(op), ""np""))\n                        op_rec.op = op\n                        op_rec.space = get_space_from_op(op)\n                        component.constant_op_records.add(op_rec)\n\n                    self.op_records.append(op_rec)\n\n            if kwargs is not None:\n                for key in sorted(kwargs.keys()):\n                    value = kwargs[key]\n                    if value is None:\n                        continue\n                    op_rec = DataOpRecord(op=None, column=self, kwarg=key)\n                    # If incoming is an op-rec -> Link them.\n                    if isinstance(value, DataOpRecord):\n                        op_rec.previous = value\n                        op_rec.op = value.op  # assign op if any\n                        value.next.add(op_rec)\n                    # Do constant value assignment here.\n                    elif value is not None:\n                        op = value\n                        if is_constant(op):\n                            op = np.array(op, dtype=convert_dtype(type(op), ""np""))\n                        op_rec.op = op\n                        op_rec.space = get_space_from_op(op)\n                        component.constant_op_records.add(op_rec)\n                    self.op_records.append(op_rec)\n        else:\n            self.op_records = [DataOpRecord(op=None, column=self, position=i) for i in range(num_op_records)]\n\n        # For __str__ purposes.\n        self.op_id_list = [o.id for o in self.op_records]\n\n        # The component this column belongs to.\n        self.component = component\n\n    def is_complete(self):\n        for op_rec in self.op_records:\n            if op_rec.op is None:\n                return False\n        return True\n\n    def get_args_and_kwargs(self):\n        args = []\n        kwargs = {}\n        for op_rec in self.op_records:\n            if op_rec.kwarg is None:\n                if op_rec.position > len(args):\n                    args += [None] * (op_rec.position - len(args))\n                args.append(op_rec)\n            else:\n                kwargs[op_rec.kwarg] = op_rec\n        return tuple(args), kwargs\n\n    def get_args_and_kwargs_as_list(self):\n        args, kwargs = self.get_args_and_kwargs()\n        return [(i, a) for i, a in enumerate(args)] + [(k, v) for k, v in sorted(kwargs.items())]\n\n    @staticmethod\n    def get_id():\n        DataOpRecordColumn._ID += 1\n        return DataOpRecordColumn._ID\n\n    def __hash__(self):\n        return hash(self.id)\n\n    def __str__(self):\n        raise NotImplementedError\n\n\nclass DataOpRecordColumnIntoGraphFn(DataOpRecordColumn):\n    """"""\n    An array of input parameters (DataOpRecord objects) that will go in a single call into a graph_fn.\n\n    GraphFns are called only at build-time. During assembly time, empty DataOpRecordColumns are created on both\n    side of the graph_fn (input=DataOpRecordColumnIntoGraphFn and return values=DataOpRecordColumnFromGraphFn).\n\n    Keeps a link to the graph_fn and also specifies options on how to call the graph_fn.\n    The call of the graph_fn will result in another column (return values) of DataOpRecords that this record points\n    to.\n    """"""\n    def __init__(self, component, graph_fn, flatten_ops=False,\n                 split_ops=False, add_auto_key_as_first_param=False,\n                 requires_variable_completeness=False,\n                 args=None, kwargs=None):\n        # TODO: need to pass in input-arg name information so we can switch to kwargs if some default arg not given.\n        super(DataOpRecordColumnIntoGraphFn, self).__init__(\n            component=component, args=args, kwargs=kwargs\n        )\n\n        # The graph_fn that our ops come from.\n        self.graph_fn = graph_fn\n\n        self.flatten_ops = flatten_ops\n        self.split_ops = split_ops\n        self.add_auto_key_as_first_param = add_auto_key_as_first_param\n        self.requires_variable_completeness = requires_variable_completeness\n\n        # The column after passing this one through the graph_fn.\n        self.out_graph_fn_column = None\n\n        # Whether this column has already been sent through the graph_fn.\n        self.already_sent = False\n\n    def flatten_input_ops(self, *ops, **kwarg_ops):\n        """"""\n        Flattens all DataOps in ops into FlattenedDataOp with auto-key generation.\n        Ops whose Sockets are not in self.flatten_ops (if its a set)\n        will be ignored.\n\n        Args:\n            *ops (op): The primitive ops to flatten.\n            **kwarg_ops (op): More primitive ops to flatten (but by named key).\n\n        Returns:\n            Tuple[DataOp]: A new tuple with all ops (or those specified by `flatten_ops` as FlattenedDataOp.\n        """"""\n        assert all(op is not None for op in ops)  # just make sure\n\n        flatten_alongside = None\n        if isinstance(self.flatten_ops, str):\n            flatten_alongside = self.component.__getattribute__(self.flatten_ops)\n\n        # The returned sequence of output ops.\n        ret = []\n        for i, op in enumerate(ops):\n            if self.flatten_ops is True or isinstance(self.flatten_ops, str) or \\\n                    (isinstance(self.flatten_ops, (set, dict)) and i in self.flatten_ops):\n                fa = flatten_alongside\n                if isinstance(self.flatten_ops, dict):\n                    fa = self.component.__getattribute__(self.flatten_ops[i])\n                if fa is not None:\n                    assert isinstance(fa, dict), \\\n                        ""ERROR: Given `flatten_alongside` property (\'{}\') is not a dict!"".format(fa)\n                ret.append(flatten_op(op, flatten_alongside=fa))\n            else:\n                ret.append(op)\n\n        # Process kwargs, if given.\n        kwarg_ret = {}\n        if len(kwarg_ops) > 0:\n            for key, op in kwarg_ops.items():\n                if self.flatten_ops is True or isinstance(self.flatten_ops, str) or \\\n                        (isinstance(self.flatten_ops, (set, dict)) and key in self.flatten_ops):\n                    fa = flatten_alongside\n                    if isinstance(self.flatten_ops, dict):\n                        fa = self.component.__getattribute__(self.flatten_ops[key])\n                    if fa is not None:\n                        assert isinstance(fa, dict), \\\n                            ""ERROR: Given `flatten_alongside` property (\'{}\') is not a dict!"".format(fa)\n                    kwarg_ret[key] = flatten_op(op, flatten_alongside=fa)\n                else:\n                    kwarg_ret[key] = op\n\n        # Always return a tuple for indexing into the return values.\n        return tuple(ret), kwarg_ret\n\n    def split_flattened_input_ops(self, *ops, **kwarg_ops):\n        """"""\n        Splits any FlattenedDataOp in *ops and **kwarg_ops into its SingleDataOps and collects them to be passed\n        one by one through some graph_fn. If more than one FlattenedDataOp exists in *ops and **kwarg_ops,\n        these must have the exact same keys.\n        If `add_auto_key_as_first_param` is True: Add auto-key as very first parameter in each\n        returned parameter tuple.\n\n        Args:\n            *ops (op): The primitive ops to split.\n            **kwarg_ops (op): More primitive ops to split (but by named key).\n\n        Returns:\n            Union[FlattenedDataOp,Tuple[DataOp]]: The sorted parameter tuples (by flat-key) to use as api_methods in the\n                calls to the graph_fn.\n                If no FlattenedDataOp is in ops, returns ops as-is.\n\n        Raises:\n            RLGraphError: If there are more than 1 flattened ops in ops and their keys don\'t match 100%.\n        """"""\n        assert all(op is not None for op in ops)  # just make sure\n\n        # Collect FlattenedDataOp for checking their keys (must match).\n        flattened = []\n        for op in ops:\n            if isinstance(op, dict) and (len(op) > 1 or """" not in op):\n                flattened.append(op)\n\n        # If it\'s more than 1, make sure they match. If they don\'t match: raise Error.\n        if len(flattened) > 1:\n            # Loop through the non-first ones and make sure all keys match vs the first one.\n            lead_arg_dict = flattened[0]\n            for other in flattened[1:]:\n                other_arg_iter = iter(other)\n                for key in lead_arg_dict.keys():\n                    k_other = next(other_arg_iter)\n                    if key != k_other:  # or get_shape(v_other) != get_shape(value):\n                        raise RLGraphError(""ERROR: Flattened ops have a key mismatch ({} vs {})!"".format(key, k_other))\n\n        # We have one or many (matching) ContainerDataOps: Split the calls.\n        if len(flattened) > 0:\n            # The first op that is a FlattenedDataOp.\n            guide_op = next(op for op in ops if len(op) > 1 or """" not in op)\n            # Re-create our iterators.\n            collected_call_params = FlattenedDataOp()\n            # Do the single split calls to our computation func.\n            for key in guide_op.keys():\n                # Prep input params for a single call.\n                params = [key] if self.add_auto_key_as_first_param is True else []\n                kwargs = {}\n                for op in ops:\n                    # Check first, do not try to check key into tensor (not iterable):\n                    if isinstance(op, dict):\n                        params.append(op[key] if key in op else op[""""])\n                    else:\n                        # E.g. tuple args.\n                        params.append(op)\n\n                # Add kwarg_ops.\n                for kwarg_key, kwarg_op in kwarg_ops.items():\n                    kwargs[kwarg_key] = kwarg_ops[kwarg_key][key] \\\n                        if key in kwarg_ops[kwarg_key] else kwarg_ops[kwarg_key][""""]\n                # Now do the single call.\n                collected_call_params[key] = (params, kwargs)\n            return collected_call_params\n        # We don\'t have any container ops: No splitting possible. Return args and kwargs as is.\n        else:\n            params = [""""] if self.add_auto_key_as_first_param is True else []\n            params += [op[""""] if isinstance(op, dict) else op for op in ops]\n            return tuple(params), {key: value[""""] for key, value in kwarg_ops.items()}\n\n    @staticmethod\n    def unflatten_output_ops(*ops):\n        """"""\n        Re-creates the originally nested input structure (as DataOpDict/DataOpTuple) of the given op-record column.\n        Process all FlattenedDataOp with auto-generated keys, and leave the others untouched.\n\n        Args:\n            ops (DataOp): The ops that need to be unflattened (only process the FlattenedDataOp\n                amongst these and ignore all others).\n\n        Returns:\n            Tuple[DataOp]: A tuple containing the ops as they came in, except that all FlattenedDataOp\n                have been un-flattened (re-nested) into their original structures.\n        """"""\n        # The returned sequence of output ops.\n        ret = []\n\n        for i, op in enumerate(ops):\n            # A FlattenedDataOp: Try to re-nest it.\n            if isinstance(op, FlattenedDataOp):\n                ret.append(unflatten_op(op))\n            # All others are left as-is.\n            else:\n                ret.append(op)\n\n        # Always return a tuple for indexing into the return values.\n        return tuple(ret)\n\n    def __str__(self):\n        return ""OpRecCol(ops: {})->GraphFn(\'{}\')"".format(self.op_id_list, self.graph_fn.__name__)\n\n\nclass DataOpRecordColumnFromGraphFn(DataOpRecordColumn):\n    """"""\n    An array of return values from a graph_fn pass through.\n    """"""\n    def __init__(self, num_op_records, component, graph_fn_name, in_graph_fn_column, summary_ops):\n        """"""\n        Args:\n            graph_fn_name (str): The name of the graph_fn that returned the ops going into `self.op_records`.\n        """"""\n        super(DataOpRecordColumnFromGraphFn, self).__init__(\n            num_op_records=num_op_records, component=component\n        )\n        # The graph_fn that our ops come from.\n        self.graph_fn_name = graph_fn_name\n        # The column after passing this one through the graph_fn.\n        self.in_graph_fn_column = in_graph_fn_column\n        self.summary_ops = summary_ops\n\n    def __str__(self):\n        return ""GraphFn(\'{}\')->OpRecCol(ops: {}, #summary_ops: {})"".format(\n            self.graph_fn_name, self.op_id_list, len(self.summary_ops))\n\n\nclass DataOpRecordColumnIntoAPIMethod(DataOpRecordColumn):\n    """"""\n    An array of input parameters (DataOpRecord objects) that will go in a single call into an API-method.\n\n    API-methods are called and run through during meta-graph assembly time.\n\n    Stores the api method record and all DataOpRecords used for the call.\n    """"""\n    def __init__(self, component, api_method_rec, args=None, kwargs=None):\n        self.api_method_rec = api_method_rec\n        super(DataOpRecordColumnIntoAPIMethod, self).__init__(component=component, args=args, kwargs=kwargs)\n\n    def __str__(self):\n        return ""OpRecCol(ops: {})->APIMethod(\'{}\')"".format(self.op_id_list, self.api_method_rec.name)\n\n\nclass DataOpRecordColumnFromAPIMethod(DataOpRecordColumn):\n    """"""\n    An array of return values from an API-method pass through.\n    """"""\n    def __init__(self, component, api_method_name, args=None, kwargs=None):\n        self.api_method_name = api_method_name\n        super(DataOpRecordColumnFromAPIMethod, self).__init__(component, args=args, kwargs=kwargs)\n\n    def __str__(self):\n        return ""APIMethod(\'{}\')->OpRecCol(ops: {})"".format(self.api_method_name, self.op_id_list)\n\n\nclass APIMethodRecord(object):\n    def __init__(self, func, wrapper_func, name,\n                 component=None, must_be_complete=True, ok_to_overwrite=False,\n                 is_graph_fn_wrapper=False, is_class_method=True,\n                 flatten_ops=False, split_ops=False, add_auto_key_as_first_param=False,\n                 requires_variable_completeness=False):\n        """"""\n        Args:\n            func (callable): The actual API-method (callable).\n            component (Component): The Component this API-method belongs to.\n            must_be_complete (bool): Whether the Component can only be input-complete if at least one\n                input op-record column is complete.\n            TODO: documentation.\n        """"""\n        self.func = func\n        self.wrapper_func = wrapper_func\n        self.name = name\n        self.component = component\n        self.must_be_complete = must_be_complete\n        self.ok_to_overwrite = ok_to_overwrite\n\n        self.is_class_method = is_class_method\n\n        self.is_graph_fn_wrapper = is_graph_fn_wrapper\n        self.flatten_ops = flatten_ops\n        self.split_ops = split_ops\n        self.add_auto_key_as_first_param = add_auto_key_as_first_param\n\n        self.requires_variable_completeness = requires_variable_completeness\n\n        # List of the input-parameter names (str) of this API-method.\n        self.input_names = []\n        # Name of the *args arg (usually ""args"") w/o the *.\n        self.args_name = None\n        # Name of the **kwargs arg (usually ""kwargs"") w/o the *.\n        self.kwargs_name = None\n        # List of the names of all non-arg/non-kwarg arguments (the ones that come first in the signature).\n        self.non_args_kwargs = []\n        # List of the names of all keyword-only arguments (ones that come after *args, but have a default value and\n        # must be set via a keyword).\n        self.keyword_only = []\n        # List of args that have a default value.\n        self.default_args = []\n        self.default_values = []  # and their actual default values\n\n        self.in_op_columns = []\n        self.out_op_columns = []\n\n        # Update the api_method_inputs dict (with empty Spaces if not defined yet).\n        skip_args = 1\n        skip_args += (self.is_graph_fn_wrapper and self.add_auto_key_as_first_param)\n        param_list = list(inspect.signature(self.func).parameters.values())[skip_args:]\n\n        for param in param_list:\n            # This param has a default value.\n            if param.default != inspect.Parameter.empty:\n                if self.args_name is not None:\n                    self.keyword_only.append(param.name)\n                else:\n                    self.non_args_kwargs.append(param.name)\n                self.default_args.append(param.name)\n                self.default_values.append(param.default)\n            # *args\n            elif param.kind == inspect.Parameter.VAR_POSITIONAL:\n                self.args_name = param.name\n            # **kwargs\n            elif param.kind == inspect.Parameter.VAR_KEYWORD:\n                self.kwargs_name = param.name\n            # Normal, non-default.\n            else:\n                self.non_args_kwargs.append(param.name)\n\n    def __str__(self):\n        return ""APIMethodRecord({} {} called {}x)"".format(self.name, self.input_names, len(self.in_op_columns))\n\n\nclass GraphFnRecord(object):\n    def __init__(self, func, wrapper_func, component=None, is_class_method=True,\n                 flatten_ops=False, split_ops=False, add_auto_key_as_first_param=False,\n                 requires_variable_completeness=False):\n        self.func = func\n        self.wrapper_func = wrapper_func\n        self.name = self.func.__name__\n        self.component = component\n\n        self.is_class_method = is_class_method\n\n        self.flatten_ops = flatten_ops\n        self.split_ops = split_ops\n        self.add_auto_key_as_first_param = add_auto_key_as_first_param\n        self.requires_variable_completeness = requires_variable_completeness\n\n        self.in_op_columns = []\n        self.out_op_columns = []\n\n\ndef get_call_param_name(op_rec):\n    api_method_rec = op_rec.column.api_method_rec  # type: APIMethodRecord\n    pos_past_normals = None if op_rec.position is None else op_rec.position - len(api_method_rec.non_args_kwargs)\n\n    # There are *args in the signature.\n    if api_method_rec.args_name is not None:\n        # op_rec has no name -> Can only be normal arg or one of *args.\n        if op_rec.kwarg is None:\n            # Position is higher than number of ""normal"" args -> must be one of *args.\n            if pos_past_normals >= 0:\n                param_name = api_method_rec.args_name + ""[{}]"".format(pos_past_normals)\n            # Normal arg (not part of *args).\n            else:\n                param_name = api_method_rec.input_names[op_rec.position]\n        # op_rec has name -> Can only be normal arg of one of **kwargs (if any).\n        else:\n            if op_rec.kwarg in api_method_rec.non_args_kwargs + api_method_rec.keyword_only:\n                param_name = op_rec.kwarg\n            else:\n                if api_method_rec.kwargs_name is None:\n                    raise RLGraphAPICallParamError(\n                        ""ERROR: API-method \'{}\' has no **kwargs, but op-rec {} indicates that it has kwarg \'{}\'!"".\n                        format(api_method_rec.name, op_rec.id, op_rec.kwarg)\n                    )\n                param_name = api_method_rec.kwargs_name + ""[{}]"".format(op_rec.kwarg)\n    # There are *kwargs in the signature.\n    elif api_method_rec.kwargs_name is not None:\n        # op_rec has no name -> Can only be a normal arg.\n        if op_rec.kwarg is None:\n            # Position is higher than number of ""normal"" args -> ERROR.\n            if pos_past_normals >= 0:\n                raise RLGraphAPICallParamError(\n                    ""Op-rec \'{}\' has no kwarg, but its position ({}) indicates that it\'s part ""\n                    ""of {}\'s **kwargs!"".format(op_rec.id, op_rec.position, api_method_rec.name)\n                )\n            # Normal arg (by position).\n            else:\n                param_name = api_method_rec.input_names[op_rec.position]\n        # op_rec has name -> Can only be normal arg of one of **kwargs.\n        else:\n            if op_rec.kwarg in api_method_rec.non_args_kwargs:\n                param_name = op_rec.kwarg\n            else:\n                param_name = api_method_rec.kwargs_name + ""[{}]"".format(op_rec.kwarg)\n    else:\n        # op_rec has no name -> Can only be normal arg.\n        if op_rec.kwarg is None:\n            # Position is higher than number of ""normal"" args -> ERROR.\n            if pos_past_normals >= 0:\n                raise RLGraphAPICallParamError(\n                    ""Op-rec {}\'s position ({}) is higher than {}\'s number of args!"".\n                    format(op_rec.id, op_rec.position, api_method_rec.name)\n                )\n            # Normal arg (by position).\n            else:\n                param_name = api_method_rec.input_names[op_rec.position]\n        # op_rec has name -> Can only be normal arg.\n        else:\n            if op_rec.kwarg in api_method_rec.non_args_kwargs:\n                param_name = op_rec.kwarg\n            else:\n                raise RLGraphAPICallParamError(\n                    ""Op-rec\'s kwarg ({}) is not a parameter of API-method {}/{}\'s signature!"".\n                    format(op_rec.kwarg, api_method_rec.component.global_scope, api_method_rec.name)\n                )\n\n    return param_name\n\n\ndef gather_summaries(op_recs):\n    summaries = []\n    processed_columns = []\n    for op_rec in op_recs:\n        _gather_summaries(op_rec.column, processed_columns, summaries)\n    return summaries\n\n\ndef _gather_summaries(op_rec_column, processed_columns, summaries):\n    if op_rec_column is None or op_rec_column in processed_columns:\n        return\n    processed_columns.append(op_rec_column)\n    if isinstance(op_rec_column, DataOpRecordColumnFromGraphFn):\n        summaries.extend(op_rec_column.summary_ops)\n    for op_rec in op_rec_column.op_records:\n        if op_rec.previous is not None:\n            _gather_summaries(op_rec.previous.column, processed_columns, summaries)\n'"
rlgraph/utils/ops.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport re\nfrom collections import OrderedDict\n\n# Defines how to generate auto-keys for flattened Tuple-Space items.\n# _T\\d+_\nFLAT_TUPLE_OPEN = ""_T""\nFLAT_TUPLE_CLOSE = ""_""\nFLATTEN_SCOPE_PREFIX = ""/""\n\n\nclass TraceContext(object):\n    """"""\n    Contains static trace context. Used to reconstruct data-flow in cases where normal\n    stack-frame inspection fails, e.g. because when calling from within a lambda.\n    """"""\n\n    # Prior caller.\n    PREV_CALLER = None\n\n    # Define by run build tracing.\n    DEFINE_BY_RUN_CONTEXT = None\n\n    # Is there an active call context?\n    ACTIVE_CALL_CONTEXT = False\n    CONTEXT_START = None\n\n\nclass DataOp(object):\n    """"""\n    The basic class for any Socket-held operation or variable, or collection thereof.\n    Each Socket (in or out) holds either one DataOp or a set of alternative DataOps.\n    """"""\n    def map(self, mapping):\n        """"""\n        A convenience method for mapping all (flattened) contents (primitive DataOps) of primitive\n        as well as ContainerDataOps to something else.\n\n        Args:\n            mapping (callable): The mapping function taking a key and a DataOp as args and returning another DataOp.\n\n        Returns:\n            DataOp: The mapped DataOp (or ContainerDataOp).\n        """"""\n        return mapping("""", self)\n\n\nclass SingleDataOp(DataOp):\n    """"""\n    A placeholder class for a simple (non-container) Tensor going into a GraphFunction or coming out of a GraphFunction,\n    or a tf.no_op-like item.\n    """"""\n    pass\n\n\nclass ContainerDataOp(DataOp):\n    """"""\n    A placeholder class for any DataOp that\'s not a SingleDataOp, but a (possibly nested) container structure\n    containing SingleDataOps as leave nodes.\n    """"""\n    def flat_key_lookup(self, flat_key, custom_scope_separator=None):\n        """"""\n        Returns an element within this DataOp following a given flat-key.\n\n        Args:\n            flat_key (str): The flat key to lookup (e.g. ""/a/_T0_/b"").\n            custom_scope_separator (Optional[str]): The scope separator used in the flat-key. It\'s usually ""/"".\n\n        Returns:\n            any: The looked up item or None if nothing found.\n        """"""\n        return flat_key_lookup(self, flat_key, custom_scope_separator=custom_scope_separator)\n\n\nclass DataOpDict(ContainerDataOp, dict):\n    """"""\n    A hashable dict that\'s used to make (possibly nested) dicts of SingleDataOps hashable, so that\n    we can store them in sets and use them as lookup keys in other dicts.\n    Dict() Spaces produce DataOpDicts when methods like `get_variable` are called on them.\n    """"""\n    def map(self, mapping):\n        """"""\n        Maps this DataOpDict via a given mapping function to another, corresponding DataOpDict where all individual\n        SingleDataOps are mapped to new SingleDataOps.\n\n        Args:\n            mapping (callable): The mapping function to use on each SingeDataOp.\n\n        Returns:\n            DataOpDict: A copy of this DataOpDict, but all SingeDataOps are mapped via the given mapping function.\n        """"""\n        flattened_self = flatten_op(self)\n        ret = {}\n        for key, value in flattened_self.items():\n            ret[key] = mapping(key, value)\n        return DataOpDict(dict(unflatten_op(ret)))\n\n    def __hash__(self):\n        """"""\n        Hash based on sequence of sorted items (keys are all strings, values are always other DataOps).\n        """"""\n        return hash(tuple(sorted(self.items())))\n\n\nclass DataOpTuple(ContainerDataOp, tuple):\n    """"""\n    A simple wrapper for a (possibly nested) tuple that contains other DataOps.\n    """"""\n    def map(self, mapping):\n        """"""\n        Maps this DataOpTuple via a given mapping function to another, corresponding DataOpTuple where all individual\n        SingleDataOps are mapped to new SingleDataOps.\n\n        Args:\n            mapping (callable): The mapping function to use on each SingeDataOp.\n\n        Returns:\n            DataOpTuple: A copy of this DataOpTuple, but all SingeDataOps are mapped via the given mapping function.\n        """"""\n        flattened_self = flatten_op(self)\n        ret = {}\n        for key, value in flattened_self.items():\n            ret[key] = mapping(key, value)\n        return DataOpTuple(*unflatten_op(ret))\n\n    def __new__(cls, *components):\n        if isinstance(components[0], (list, tuple)):\n            assert len(components) == 1\n            components = components[0]\n\n        return tuple.__new__(cls, components)\n\n\nclass FlattenedDataOp(DataOp, OrderedDict):\n    """"""\n    An OrderedDict-type placeholder class that only contains str as keys and SingleDataOps\n    (as opposed to ContainerDataOps) as values.\n    """"""\n    # TODO: enforce str as keys?\n    pass\n\n\ndef flatten_op(\n        op, key_scope="""", op_tuple_list=None, custom_scope_separator=None, scope_separator_at_start=True, mapping=None,\n        flatten_alongside=None\n):\n    """"""\n    Flattens a single ContainerDataOp or a native python dict/tuple into a FlattenedDataOp with auto-key generation.\n\n    Args:\n        op (Union[ContainerDataOp,dict,tuple]): The item to flatten.\n        key_scope (str): The recursive scope for auto-key generation.\n        op_tuple_list (list): The list of tuples (key, value) to be converted into the final FlattenedDataOp.\n\n        custom_scope_separator (str): The separator to use in the returned dict for scopes.\n            Default: \'/\'.\n\n        scope_separator_at_start (bool): Whether to add the scope-separator also at the beginning.\n            Default: False.\n\n        mapping (Optional[callable]): An optional mapping function for op (and all nested ops) to be passed through.\n\n        flatten_alongside (Optional[dict]): If given, flatten only according to this dictionary, not any further down\n            the nested input structure of `op`. This is useful to flatten e.g. along some action-space, but not\n            further down (e.g. into the tuple of a distribution\'s parameters).\n\n    Returns:\n        FlattenedDataOp: The flattened representation of the op.\n    """"""\n    ret = False\n\n    # Are we in the non-recursive (first) call?\n    if op_tuple_list is None:\n        # Flatten a SingleDataOp -> return FlattenedDataOp with only-key="""".\n        # OR: flatten_alongside something, and that something is not flattened (its only key is """").\n        if not isinstance(op, (ContainerDataOp, dict, tuple)) or \\\n                (flatten_alongside is not None and len(flatten_alongside) == 1 and """" in flatten_alongside):\n            return FlattenedDataOp([("""", op)])\n        op_tuple_list = []\n        ret = True\n\n    if mapping is not None:\n        op = mapping(op)\n\n    flatten_scope_prefix = custom_scope_separator or FLATTEN_SCOPE_PREFIX\n\n    if isinstance(op, dict):\n        if scope_separator_at_start:\n            key_scope += flatten_scope_prefix\n        else:\n            key_scope = """"\n        for key in sorted(op.keys()):\n            # Make sure we have no double slashes from flattening an already FlattenedDataOp.\n            scope = (key_scope[:-1] if len(key) == 0 or key[0] == ""/"" else key_scope) + key\n            # If current flat-key is already in `alongside` structure, stop the flattening here.\n            if flatten_alongside is not None and scope in flatten_alongside:\n                op_tuple_list.append((scope, op[key]))\n            else:\n                flatten_op(\n                        op[key], key_scope=scope, op_tuple_list=op_tuple_list, scope_separator_at_start=True,\n                        mapping=mapping, flatten_alongside=flatten_alongside\n                )\n    elif isinstance(op, tuple):\n        if scope_separator_at_start:\n            key_scope += flatten_scope_prefix + FLAT_TUPLE_OPEN\n        else:\n            key_scope += """" + FLAT_TUPLE_OPEN\n        for i, c in enumerate(op):\n            # If current flat-key is already in `alongside` structure, stop the flattening here.\n            if flatten_alongside is not None and key_scope in flatten_alongside:\n                op_tuple_list.append((key_scope, c))\n            else:\n                flatten_op(\n                    c, key_scope=key_scope + str(i) + FLAT_TUPLE_CLOSE, op_tuple_list=op_tuple_list,\n                    scope_separator_at_start=True, mapping=mapping, flatten_alongside=flatten_alongside\n                )\n    else:\n        op_tuple_list.append((key_scope, op))\n\n    # Non recursive (first) call -> Return the final FlattenedDataOp.\n    if ret:\n        return FlattenedDataOp(op_tuple_list)\n\n\ndef unflatten_op(op, custom_scope_separator=None):\n    """"""\n    Takes a FlattenedDataOp with auto-generated keys and returns the corresponding\n    unflattened DataOp.\n    If the only key in the input FlattenedDataOp is """", it returns the SingleDataOp under\n    that key.\n\n    Args:\n        op (dict): The item to be unflattened (re-nested) into any DataOp. Usually a FlattenedDataOp, but can also\n            be a plain dict.\n\n    Returns:\n        DataOp: The unflattened (re-nested) item.\n    """"""\n    # Special case: FlattenedDataOp with only 1 SingleDataOp (key="""").\n    if len(op) == 1 and """" in op:\n        return op[""""]\n\n    # Normal case: FlattenedDataOp that came from a ContainerItem.\n    base_structure = None\n\n    flatten_scope_prefix = custom_scope_separator or FLATTEN_SCOPE_PREFIX\n\n    op_names = sorted(op.keys())\n    for op_name in op_names:\n        op_val = op[op_name]\n        parent_structure = None\n        parent_key = None\n        current_structure = None\n        op_type = None\n\n        # N.b. removed this because we do not prepend / any more before first key.\n        if op_name.startswith(flatten_scope_prefix):\n            op_name = op_name[1:]\n        op_key_list = op_name.split(""/"")  # skip 1st char (/)\n        for sub_key in op_key_list:\n            mo = re.match(r\'^{}(\\d+){}$\'.format(FLAT_TUPLE_OPEN, FLAT_TUPLE_CLOSE), sub_key)\n            if mo:\n                op_type = list\n                idx = int(mo.group(1))\n            else:\n                op_type = DataOpDict\n                idx = sub_key\n\n            if current_structure is None:\n                if base_structure is None:\n                    base_structure = [None] if op_type == list else DataOpDict()\n                current_structure = base_structure\n            elif parent_key is not None:\n                # DEBUG:\n                #if isinstance(parent_structure, list) and len(parent_structure) <= parent_key:\n                #    print(""WARNING: parent_structure={} parent_key={} to-be-flattened-op={}"".format(parent_structure, parent_key, op))\n                # END: DEBUG\n\n                if (isinstance(parent_structure, list) and (parent_structure[parent_key] is None)) or \\\n                        (isinstance(parent_structure, DataOpDict) and parent_key not in parent_structure):\n                    current_structure = [None] if op_type == list else DataOpDict()\n                    parent_structure[parent_key] = current_structure\n                else:\n                    current_structure = parent_structure[parent_key]\n                    if op_type == list and len(current_structure) == idx:\n                        current_structure.append(None)\n\n            parent_structure = current_structure\n            parent_key = idx\n            if isinstance(parent_structure, list) and len(parent_structure) == parent_key:\n                parent_structure.append(None)\n\n        if op_type == list and len(current_structure) == parent_key:\n            current_structure.append(None)\n        current_structure[parent_key] = op_val\n\n    # Deep conversion from list to tuple.\n    return deep_tuple(base_structure)\n\n\ndef flat_key_lookup(container, flat_key, default=None, custom_scope_separator=None):\n    """"""\n    Looks up a flattened key (a sequence of simple lookups inside a deep nested dict/tuple)\n    and returns the found item. Returns a  default value if no item under that flat-key was found.\n\n    Args:\n        container (any): The (non-flattened) structure (can be a dict, a ).\n        flat_key (str): The flat key to look for.\n        default (any): The default value if nothing was found. Default: None.\n        custom_scope_separator (Optional[str]): The scope separator that\'s used in the flat-key string.\n            Default: Value of `FLATTEN_SCOPE_PREFIX`.\n\n    Returns:\n        any: The found item under the flat key or a default value if nothing was found.\n    """"""\n    flatten_scope_prefix = custom_scope_separator or FLATTEN_SCOPE_PREFIX\n\n    # Ignore starting scope-separators.\n    if flat_key.startswith(flatten_scope_prefix):\n        flat_key = flat_key[1:]\n\n    key_sequence = flat_key.split(flatten_scope_prefix)\n    result = container\n    for key in key_sequence:\n        mo = re.match(r\'^{}(\\d+){}$\'.format(FLAT_TUPLE_OPEN, FLAT_TUPLE_CLOSE), key)\n        # Tuple.\n        if mo is not None:\n            slot = int(mo.group(1))\n            if len(result) > slot and default is not None:\n                return default\n            result = result[slot]\n        # Dict.\n        else:\n            if key not in result and default is not None:\n                return default\n            result = result[key]\n    return result\n\n\ndef deep_tuple(x):\n    """"""\n    Converts all lists inside the input into a DataOpTuple.\n\n    Args:\n        x (list): The arbitrarily nested input structure to be converted.\n\n    Returns:\n        any: The corresponding new structure for x.\n    """"""\n    # A list -> convert to DataOpTuple.\n    if isinstance(x, list):\n        return DataOpTuple(list(map(deep_tuple, x)))\n    # A dict -> leave type as is and keep converting recursively.\n    elif isinstance(x, dict):\n        # type(x) b/c x could be DataOpDict as well.\n        return type(x)(dict(map(lambda i: (i[0], deep_tuple(i[1])), x.items())))\n    # A primitive -> keep as is.\n    else:\n        return x\n\n\ndef is_constant(op):\n    return type(op).__name__ in [""int"", ""float"", ""bool"", ""ndarray""]\n'"
rlgraph/utils/pytorch_util.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nimport numpy as np\nimport copy\n\n\nif get_backend() == ""pytorch"":\n    import torch\n\n\nclass PyTorchVariable(object):\n    """"""\n    Wrapper to connect PyTorch parameters to names so they can be included\n    in variable registries.\n    """"""\n    def __init__(self, name, ref):\n        """"""\n\n        Args:\n            name (str): Name of this variable.\n            ref (torch.nn.Module): Ref to the layer or network object.\n        """"""\n        self.name = name\n        self.ref = ref\n\n    def get_value(self):\n        if get_backend() == ""pytorch"":\n            if isinstance(self.ref, torch.nn.Module):\n                return self.ref.weight.detach()\n\n    def set_value(self, value):\n        if get_backend() == ""pytorch"":\n            if isinstance(self.ref, torch.nn.Module):\n                if isinstance(value, torch.nn.Parameter):\n                    self.ref.weight = copy.deepcopy(value)\n                elif isinstance(value, torch.Tensor):\n                    self.ref.weight = torch.nn.Parameter(copy.deepcopy(value), requires_grad=True)\n                else:\n                    raise ValueError(""Value assigned must be torch.Tensor or Parameter but is {}."".format(\n                        type(value)\n                    ))\n\n\ndef pytorch_one_hot(index_tensor, depth=0):\n    """"""\n    One-hot utility function for PyTorch.\n\n    Args:\n        index_tensor (torch.Tensor): The input to be one-hot.\n        depth (int): The max. number to be one-hot encoded (size of last rank).\n\n    Returns:\n        torch.Tensor: The one-hot encoded equivalent of the input array.\n    """"""\n    if get_backend() == ""pytorch"":\n        # Do converts.\n        if isinstance(index_tensor, torch.FloatTensor):\n            index_tensor = index_tensor.long()\n        if isinstance(index_tensor, torch.IntTensor):\n            index_tensor = index_tensor.long()\n\n        out = torch.zeros(index_tensor.size() + torch.Size([depth]))\n        dim = len(index_tensor.size())\n        index = index_tensor.unsqueeze(-1)\n        return out.scatter_(dim, index, 1)\n\n\ndef pytorch_tile(tensor, n_tile, dim=0):\n    """"""\n    Tile utility as there is not `torch.tile`.\n    Args:\n        tensor (torch.Tensor): Tensor to tile.\n        n_tile (int): Num tiles.\n        dim (int): Dim to tile.\n\n    Returns:\n        torch.Tensor: Tiled tensor.\n    """"""\n    if isinstance(n_tile, torch.Size):\n        n_tile = n_tile[0]\n    init_dim = tensor.size(dim)\n    repeat_idx = [1] * tensor.dim()\n    repeat_idx[dim] = n_tile\n    tensor = tensor.repeat(*(repeat_idx))\n    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)]))\n    return torch.index_select(tensor, dim, order_index)\n\n\n# TODO remove when we have handled pytorch placeholder inference better.\ndef get_input_channels(shape):\n    """"""\n    Helper for temporary issues with PyTorch shape inference.\n\n    Args:\n        shape (Tuple): Shape tuple.\n\n    Returns:\n        int: Num input channels.\n    """"""\n    # Have batch rank from placeholder space reconstruction:\n    if len(shape) == 4:\n        # Batch rank and channels first.\n        return shape[1]\n    elif len(shape) == 3:\n        # No batch rank and channels first.\n        return shape[0]\n\n\nif get_backend() == ""pytorch"":\n    SMALL_NUMBER_TORCH = torch.tensor([1e-6])\n    LOG_SMALL_NUMBER = torch.log(SMALL_NUMBER_TORCH)\n\n    class SamePaddedConv2d(torch.nn.Module):\n        """"""\n        Implements a Conv2d layer with padding \'same\' as PyTorch does not have\n        padding options like TF.\n        """"""\n        def __init__(self, in_channels, out_channels, kernel_size, bias=True, stride=1,\n                     transpose=False, padding_layer=torch.nn.ReflectionPad2d):\n            super(SamePaddedConv2d, self).__init__()\n            ka = kernel_size // 2\n            kb = ka - 1 if kernel_size % 2 == 0 else ka\n\n            if transpose is True:\n                self.layer = torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, bias=bias, stride=stride)\n            else:\n                self.layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size, bias=bias, stride=stride)\n            self.net = torch.nn.Sequential(\n                padding_layer((ka, kb, ka, kb)),\n                self.layer\n            )\n\n            self.weight = self.layer.weight\n            self.bias = self.layer.bias\n\n        def forward(self, x):\n            return self.net(x)\n\n        def parameters(self):\n            return self.layer.parameters()\n\n\n'"
rlgraph/utils/rlgraph_errors.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\n\nclass RLGraphError(Exception):\n    """"""\n    Simple Error class.\n    """"""\n    pass\n\n\nclass RLGraphAPICallParamError(RLGraphError):\n    """"""\n    Raised if two sub-Components in a Stack do not have matching API-methods (sub-Component A\'s API output does not\n    go well into sub-Component B\'s API input).\n    """"""\n    pass\n\n\nclass RLGraphBuildError(RLGraphError):\n    """"""\n    Raised if the build of a model cannot be completed properly e.g. due to input/variable-incompleteness of some\n    components.\n    """"""\n    pass\n\n\nclass RLGraphInputIncompleteError(RLGraphError):\n    """"""\n    Raised if the build of a model cannot\n    """"""\n    def __init__(self, component, msg=None):\n        """"""\n        Args:\n            component (Component): The Component that is not input complete.\n            msg (Optional[str]): The error message.\n        """"""\n        msg = msg or ""Component \'{}\' is input incomplete, but its input Spaces are needed in an API-call during the "" \\\n                     ""build procedure!"".format(component.global_scope)\n        super(RLGraphInputIncompleteError, self).__init__(msg)\n        self.component = component\n\n\nclass RLGraphVariableIncompleteError(RLGraphError):\n    """"""\n\n    """"""\n    def __init__(self, component, msg=None):\n        """"""\n        Args:\n            component (Component): The Component that is not input complete.\n            msg (Optional[str]): The error message.\n        """"""\n        msg = msg or ""Component \'{}\' is variable incomplete, but its variables are needed in an API-call during the "" \\\n                     ""build procedure!"".format(component.global_scope)\n        super(RLGraphVariableIncompleteError, self).__init__(msg)\n        self.component = component\n\n\nclass RLGraphObsoletedError(RLGraphError):\n    """"""\n    An error raised when some obsoleted method, property, etc. is used.\n    """"""\n    def __init__(self, type_, old_value, new_value):\n        """"""\n        Args:\n            type_ (str): Some type description of what exactly is obsoleted.\n            old_value (str): The obsoleted value used.\n            new_value (str): The new (replacement) value that should have been used instead.\n        """"""\n        msg = ""The {} \'{}\' you are using has been obsoleted! Use \'{}\' instead."".format(type_, old_value, new_value)\n        super(RLGraphObsoletedError, self).__init__(msg)\n\n\nclass RLGraphSpaceError(RLGraphError):\n    """"""\n    A Space related error. Raises together with a message and Space information.\n    """"""\n    def __init__(self, space, msg=None):\n        """"""\n        Args:\n            space (Space): The Space that failed some check.\n            input_arg (Optional[str]): An optional API-method input arg name.\n            msg (Optional[str]): The error message.\n        """"""\n        super(RLGraphSpaceError, self).__init__(msg)\n        self.space = space\n\n\nclass RLGraphKerasStyleAssemblyError(RLGraphError):\n    """"""\n    Special error to raise when constructing a NeuralNetwork using our Keras-style assembly support.\n    """"""\n    pass\n'"
rlgraph/utils/specifiable.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport importlib\nimport json\nimport logging\nimport os\nimport re\nfrom copy import deepcopy\nfrom functools import partial\n\nimport yaml\n\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\nfrom rlgraph.utils.util import default_dict, force_list\n\n\nclass Specifiable(object):\n    """"""\n    Members of this class support the methods from_file, from_spec and from_mixed.\n    """"""\n    # An optional python dict with supported str-to-ctor mappings for this class.\n    __lookup_classes__ = None\n    # An optional default constructor to use without any arguments in case `spec` is None\n    # and args/kwargs are both empty. This may be a functools.partial object.\n    __default_constructor__ = None\n\n    # TODO: Remove from here. Only Specifyables that really need a Logger should have this. Creates problems when\n    # TODO: deepcopying.\n    logger = logging.getLogger(__name__)\n\n    # Analogous to: http://effbot.org/pyfaq/how-do-i-get-a-list-of-all-instances-of-a-given-class.htm\n    #_instances = list()\n\n    def __init__(self):\n        pass\n        #self._instances.append(weakref.ref(self))\n\n    @classmethod\n    def from_spec(cls, spec=None, **kwargs):\n        """"""\n        Uses the given spec to create an object.\n        If `spec` is a dict, an optional ""type"" key can be used as a ""constructor hint"" to specify a certain class\n        of the object.\n        If `spec` is not a dict, `spec`\'s value is used directly as the ""constructor hint"".\n\n        The rest of `spec` (if it\'s a dict) will be used as kwargs for the (to-be-determined) constructor.\n        Additional keys in **kwargs will always have precedence (overwrite keys in `spec` (if a dict)).\n        Also, if the spec-dict or **kwargs contains the special key ""_args"", it will be popped from the dict\n        and used as *args list to be passed separately to the constructor.\n\n        The following constructor hints are valid:\n        - None: Use `cls` as constructor.\n        - An already instantiated object: Will be returned as is; no constructor call.\n        - A string or an object that is a key in `cls`\'s `__lookup_classes__` dict: The value in `__lookup_classes__`\n            for that key will be used as the constructor.\n        - A python callable: Use that as constructor.\n        - A string: Either a json filename or the name of a python module+class (e.g. ""rlgraph.components.Component"")\n            to be Will be used to\n\n        Args:\n            spec (Optional[dict]): The specification dict.\n\n        Keyword Args:\n            kwargs (any): Optional possibility to pass the c\'tor arguments in here and use spec as the type-only info.\n                Then we can call this like: from_spec([type]?, [**kwargs for ctor])\n                If `spec` is already a dict, then `kwargs` will be merged with spec (overwriting keys in `spec`) after\n                ""type"" has been popped out of `spec`.\n                If a constructor of a Specifiable needs an *args list of items, the special key `_args` can be passed\n                inside `kwargs` with a list type value (e.g. kwargs={""_args"": [arg1, arg2, arg3]}).\n\n        Returns:\n            The object generated from the spec.\n        """"""\n        # specifiable_type is already a created object of this class -> Take it as is.\n        if isinstance(spec, cls):\n            return spec\n\n        # `specifiable_type`: Indicator for the Specifiable\'s constructor.\n        # `ctor_args`: *args arguments for the constructor.\n        # `ctor_kwargs`: **kwargs arguments for the constructor.\n        # Copy so caller can reuse safely.\n        spec = deepcopy(spec)\n        if isinstance(spec, dict):\n            if ""type"" in spec:\n                specifiable_type = spec.pop(""type"", None)\n            else:\n                specifiable_type = None\n            ctor_kwargs = spec\n            ctor_kwargs.update(kwargs)  # give kwargs priority\n        else:\n            specifiable_type = spec\n            if specifiable_type is None and ""type"" in kwargs:\n                specifiable_type = kwargs.pop(""type"")\n            ctor_kwargs = kwargs\n        # Special `_args` field in kwargs for *args-utilizing constructors.\n        ctor_args = force_list(ctor_kwargs.pop(""_args"", []))\n\n        # Figure out the actual constructor (class) from `type_`.\n        # None: Try __default__object (if no args/kwargs), only then constructor of cls (using args/kwargs).\n        if specifiable_type is None:\n            # We have a default constructor that was defined directly by cls (not by its children).\n            if cls.__default_constructor__ is not None and ctor_args == [] and \\\n                    (not hasattr(cls.__bases__[0], ""__default_constructor__"") or\n                     cls.__bases__[0].__default_constructor__ is None or\n                     cls.__bases__[0].__default_constructor__ is not cls.__default_constructor__\n                    ):\n                constructor = cls.__default_constructor__\n                # Default partial\'s keywords into ctor_kwargs.\n                if isinstance(constructor, partial):\n                    kwargs = default_dict(ctor_kwargs, constructor.keywords)\n                    constructor = partial(constructor.func, **kwargs)\n                    ctor_kwargs = {} # erase to avoid duplicate kwarg error\n            # Try our luck with this class itself.\n            else:\n                constructor = cls\n        # Try the __lookup_classes__ of this class.\n        else:\n            constructor = cls.lookup_class(specifiable_type)\n\n            # Found in cls.__lookup_classes__.\n            if constructor is not None:\n                pass\n            # Python callable.\n            elif callable(specifiable_type):\n                constructor = specifiable_type\n            # A string: Filename or a python module+class.\n            elif isinstance(specifiable_type, str):\n                if re.search(r\'\\.(yaml|yml|json)$\', specifiable_type):\n                    return cls.from_file(specifiable_type, *ctor_args, **ctor_kwargs)\n                elif specifiable_type.find(\'.\') != -1:\n                    module_name, function_name = specifiable_type.rsplit(""."", 1)\n                    module = importlib.import_module(module_name)\n                    constructor = getattr(module, function_name)\n                else:\n                    raise RLGraphError(\n                        ""ERROR: String specifier ({}) in from_spec must be a filename, a module+class, or a key ""\n                        ""into {}.__lookup_classes__!"".format(specifiable_type, cls.__name__)\n                    )\n\n        if not constructor:\n            raise RLGraphError(""Invalid type: {}"".format(specifiable_type))\n\n        # Create object with inferred constructor.\n        specifiable_object = constructor(*ctor_args, **ctor_kwargs)\n        assert isinstance(specifiable_object, constructor.func if isinstance(constructor, partial) else constructor)\n\n        return specifiable_object\n\n    @classmethod\n    def from_file(cls, filename, *args, **kwargs):\n        """"""\n        Create object from spec saved in filename. Expects json or yaml format.\n\n        Args:\n            filename: file containing the spec (json or yaml)\n\n        Keyword Args:\n            Used as additional parameters for call to constructor.\n\n        Returns:\n            object\n        """"""\n        path = os.path.join(os.getcwd(), filename)\n        if not os.path.isfile(path):\n            raise RLGraphError(\'No such file: {}\'.format(filename))\n\n        with open(path, \'rt\') as fp:\n            if path.endswith(\'.yaml\') or path.endswith(\'.yml\'):\n                spec = yaml.load(fp)\n            else:\n                spec = json.load(fp)\n\n        # Add possible *args.\n        spec[""_args""] = args\n        return cls.from_spec(spec=spec, **kwargs)\n\n    @classmethod\n    def lookup_class(cls, lookup_type):\n        if isinstance(cls.__lookup_classes__, dict) and \\\n            (lookup_type in cls.__lookup_classes__ or (isinstance(lookup_type, str)\n             and re.sub(r\'[\\W_]\', \'\', lookup_type.lower()) in cls.__lookup_classes__)):\n            available_class_for_type = cls.__lookup_classes__.get(lookup_type)\n            if available_class_for_type is None:\n                available_class_for_type = cls.__lookup_classes__[re.sub(r\'[\\W_]\', \'\', lookup_type.lower())]\n            return available_class_for_type\n        return None\n'"
rlgraph/utils/specifiable_server.py,4,"b'# Copyright 2018/2019 The RLgraph authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport multiprocessing\n\nfrom rlgraph import get_backend\nfrom rlgraph.spaces.space import Space\nfrom rlgraph.spaces.containers import ContainerSpace\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\nfrom rlgraph.utils.specifiable import Specifiable\nfrom rlgraph.utils.util import force_list, convert_dtype\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass SpecifiableServer(Specifiable):\n    """"""\n    A class that creates a separate python process (""server"") which runs an arbitrary Specifiable object.\n\n    This is useful - for example - to run RLgraph Environments (which are Specifiables) in a highly parallelized and\n    in-graph fashion for faster Agent-Environment stepping.\n    """"""\n\n    # Class instances get registered/deregistered here.\n    INSTANCES = []\n\n    def __init__(self, specifiable_class, spec, output_spaces, shutdown_method=None):\n        """"""\n        Args:\n            specifiable_class (type): The class to use for constructing the Specifiable from spec. This class needs to be\n                a child class of Specifiable (with a __lookup_classes__ property).\n            spec (dict): The specification dict that will be used to construct the Specifiable.\n            output_spaces (Union[callable,Dict[str,Space]]): A callable that takes a method_name as argument\n                and returns the Space(s) that this method (on the Specifiable object) would return. Alternatively:\n                A dict with key=method name and value=Space(s).\n            shutdown_method (Optional[str]): An optional name of a shutdown method that will be called on the\n                Specifiable object before ""server"" shutdown to give the Specifiable a chance to clean up.\n                The Specifiable must implement this method.\n            #flatten_output_dicts (bool): Whether output dictionaries should be flattened to tuples and then\n            #    returned.\n        """"""\n        super(SpecifiableServer, self).__init__()\n\n        self.specifiable_class = specifiable_class\n        self.spec = spec\n        # If dict: Process possible specs so we don\'t have to do this during calls.\n        if isinstance(output_spaces, dict):\n            self.output_spaces = {}\n            for method_name, space_spec in output_spaces.items():\n                if isinstance(space_spec, (tuple, list)):\n                    self.output_spaces[method_name] = [Space.from_spec(spec) if spec is not None else\n                                                       None for spec in space_spec]\n                else:\n                    self.output_spaces[method_name] = Space.from_spec(space_spec) if space_spec is not None else None\n        else:\n            self.output_spaces = output_spaces\n        self.shutdown_method = shutdown_method\n\n        # The process in which the Specifiable will run.\n        self.process = None\n        # The out-pipe to send commands (method calls) to the server process.\n        self.out_pipe = None\n        # The in-pipe to receive ""ready"" signal from the server process.\n        self.in_pipe = None\n\n        # Register this object with the class.\n        self.INSTANCES.append(self)\n\n    def __getattr__(self, method_name):\n        """"""\n        Returns a function that will create a server-call (given method_name must be one of the Specifiable object)\n        from within the backend-specific graph.\n\n        Args:\n            method_name (str): The method to call on the Specifiable.\n            #return_slots (Optional[List[int]]): An optional list of return slots to use. None for using all return\n            #    values.\n\n        Returns:\n            callable: The callable to be executed when getting the given method name (of the Specifiable object\n                (running inside the SpecifiableServer).\n        """"""\n        # Only seems to be necessary on Windows as multiprocessing calls __getstate__ on the SpecifiableServer object.\n        # (and `__getstate__` is not in self.output_spaces).\n        if method_name not in self.output_spaces:\n            def func():\n                pass\n            return func\n\n        def call(*args):\n            if isinstance(self.output_spaces, dict):\n                assert method_name in self.output_spaces, ""ERROR: Method \'{}\' not specified in output_spaces: {}!"".\\\n                    format(method_name, self.output_spaces)\n                specs = self.output_spaces[method_name]\n            else:\n                specs = self.output_spaces(method_name)\n\n            if specs is None:\n                raise RLGraphError(\n                    ""No Space information received for method \'{}:{}\'"".format(self.specifiable_class.__name__, method_name)\n                )\n\n            dtypes = []\n            shapes = []\n            return_slots = []\n            for i, space in enumerate(force_list(specs)):\n                assert not isinstance(space, ContainerSpace)\n                # Expecting an op (space 0).\n                if space == 0:\n                    dtypes.append(0)\n                    shapes.append(0)\n                    return_slots.append(i)\n                # Expecting a tensor.\n                elif space is not None:\n                    dtypes.append(convert_dtype(space.dtype))\n                    shapes.append(space.shape)\n                    return_slots.append(i)\n\n            if get_backend() == ""tf"":\n                # This function will send the method-call-comment via the out-pipe to the remote (server) Specifiable\n                # object - all in-graph - and return the results to be used further by other graph ops.\n                def py_call(*call_args):\n                    call_args = [arg.decode(\'UTF-8\') if isinstance(arg, bytes) else arg for arg in call_args]\n                    try:\n                        self.out_pipe.send(call_args)\n                        received_results = self.out_pipe.recv()\n\n                        # If an error occurred, it\'ll be passed back through the pipe.\n                        if isinstance(received_results, Exception):\n                            raise received_results\n                        elif received_results is not None:\n                            return received_results\n\n                    except Exception as e:\n                        if isinstance(e, IOError):\n                            raise StopIteration()  # Clean exit.\n                        else:\n                            print(""ERROR: Sent={} Exception={}"".format(call_args, e))\n                            raise\n\n                results = tf.py_func(py_call, (method_name,) + tuple(args), dtypes, name=method_name)\n\n                # Force known shapes on the returned tensors.\n                for i, (result, shape) in enumerate(zip(results, shapes)):\n                    # Not an op (which have shape=0).\n                    if shape != 0:\n                        result.set_shape(shape)\n            else:\n                raise NotImplementedError\n\n            return results[0] if len(dtypes) == 1 else tuple(results)\n\n        return call\n\n    def start_server(self):\n        # Create the in- and out- pipes to communicate with the proxy-Specifiable.\n        self.out_pipe, self.in_pipe = multiprocessing.Pipe()\n        # Create and start the process passing it the spec to construct the desired Specifiable object..\n        self.process = multiprocessing.Process(\n            target=self.run_server, args=(self.specifiable_class, self.spec, self.in_pipe, self.shutdown_method)\n        )\n        self.process.start()\n\n        # Wait for the ""ready"" signal (which is None).\n        result = self.out_pipe.recv()\n\n        # Check whether there were construction errors.\n        if isinstance(result, Exception):\n            raise result\n\n    def stop_server(self):  #, session):\n        try:\n            self.out_pipe.send(None)\n            self.out_pipe.close()\n        except IOError:\n            pass\n        self.process.join()\n\n    def run_server(self, class_, spec, in_pipe, shutdown_method=None):\n        proxy_object = None\n        try:\n\n            # Construct the Specifiable object.\n            proxy_object = class_.from_spec(spec)\n\n            # Send the ready signal (no errors).\n            in_pipe.send(None)\n\n            # Start a server-loop waiting for method call requests.\n            while True:\n                command = in_pipe.recv()\n\n                # ""close"" signal (None) -> End this process.\n                if command is None:\n                    # Give the proxy_object a chance to clean up via some `shutdown_method`.\n                    if shutdown_method is not None and hasattr(proxy_object, shutdown_method):\n                        getattr(proxy_object, shutdown_method)()\n                    in_pipe.close()\n                    return\n\n                # Call the method with the given args.\n                method_name = str(command[0])  # must decode here as method_name comes in as bytes\n                inputs = command[1:]\n                results = getattr(proxy_object, method_name)(*inputs)\n\n                # Send return values back to caller.\n                in_pipe.send(results)\n\n        # If something happens during the construction and proxy run phase, pass the exception back through our pipe.\n        except Exception as e:\n            print(""ERROR: Last called={} Sent={}"".format(method_name, inputs))\n            # Try to clean up.\n            if proxy_object is not None and shutdown_method is not None and hasattr(proxy_object, shutdown_method):\n                try:\n                    getattr(proxy_object, shutdown_method)()\n                except:\n                    pass\n            # Send the exception back so the main process knows what\'s going on.\n            in_pipe.send(e)\n\n\nif get_backend() == ""tf"":\n    class SpecifiableServerHook(tf.train.SessionRunHook):\n        """"""\n        A hook for a tf.MonitoredSession that takes care of automatically starting and stopping\n        SpecifiableServer objects.\n        """"""\n        def __init__(self):\n            self.specifiable_buffer = []\n\n        def begin(self):\n            """"""\n            Starts all registered RLGraphProxyProcess processes.\n            """"""\n            tp = multiprocessing.pool.ThreadPool()\n            tp.map(lambda server: server.start_server(), SpecifiableServer.INSTANCES)\n\n            tp.close()\n            tp.join()\n            tf.logging.info(\'Started all server hooks.\')\n\n            # Erase all SpecifiableServers as we open the Session (after having started all of them),\n            # so new ones can get registered.\n            self.specifiable_buffer = SpecifiableServer.INSTANCES[:]  # copy list\n            SpecifiableServer.INSTANCES.clear()\n\n        def end(self, session):\n            tp = multiprocessing.pool.ThreadPool()\n            tp.map(lambda server: server.stop_server(), self.specifiable_buffer)\n            tp.close()\n            tp.join()\n'"
rlgraph/utils/tf_util.py,6,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\n\nfrom rlgraph import get_backend\n\n# TF specific scope/device utilities.\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n    @contextlib.contextmanager\n    def pin_global_variables(device):\n        """"""Pins global variables to the specified device.""""""\n\n        def getter(getter, *args, **kwargs):\n            var_collections = kwargs.get(\'collections\', None)\n            if var_collections is None:\n                var_collections = [tf.GraphKeys.GLOBAL_VARIABLES]\n            if tf.GraphKeys.GLOBAL_VARIABLES in var_collections:\n                with tf.device(device):\n                    return getter(*args, **kwargs)\n            else:\n                return getter(*args, **kwargs)\n\n        with tf.variable_scope(\'\', custom_getter=getter) as vs:\n            yield vs\n\n\ndef ensure_batched(tensor):\n    """"""\n    Expands dim to batch dim if needed.\n\n    Args:\n        tensor (tf.Tensor): TensorFlow tensor.\n\n    Returns:\n        Tensor with batch dim added if input rank was 1.\n    """"""\n    if str(tensor.shape) == ""<unknown>"" or len(tensor.shape.as_list()) == 1:\n        print(""Adjusting tensor {}, shape = {} "".format(tensor, tensor.shape))\n        return tf.expand_dims(tensor, axis=1)\n    else:\n        return tensor\n'"
rlgraph/utils/util.py,14,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom functools import partial\nimport logging\nimport inspect\nimport re\nimport sys\n\nimport numpy as np\nfrom rlgraph import get_backend\nfrom rlgraph.utils.define_by_run_ops import define_by_run_flatten\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n# Some small floating point number. Can be used as a small epsilon for numerical stability purposes.\nSMALL_NUMBER = 1e-6\n# Some large int number. May be increased here, if needed.\nLARGE_INTEGER = 100000000\n# Min and Max outputs (clipped) from an NN-output layer interpreted as the log(stddev) of some loc + scale distribution.\nMIN_LOG_STDDEV = -20\nMAX_LOG_STDDEV = 2\n\n# Logging config for testing.\nlogging_formatter = logging.Formatter(\'%(asctime)s:%(levelname)s:%(message)s\', datefmt=\'%y-%m-%d %H:%M:%S\')\nroot_logger = logging.getLogger(\'\')\nroot_logger.setLevel(level=logging.INFO)\ntf_logger = logging.getLogger(\'tensorflow\')\ntf_logger.setLevel(level=logging.INFO)\n\nprint_logging_handler = logging.StreamHandler(stream=sys.stdout)\nprint_logging_handler.setFormatter(logging_formatter)\nprint_logging_handler.setLevel(level=logging.INFO)\nroot_logger.addHandler(print_logging_handler)\n\n\n# TODO: Consider making ""to"" non-optional: https://github.com/rlgraph/rlgraph/issues/34\ndef convert_dtype(dtype, to=""tf""):\n    """"""\n    Translates any type (tf, numpy, python, etc..) into the respective tensorflow/numpy data type.\n\n    Args:\n        dtype (any): String describing a numerical type (e.g. \'float\'), numpy data type, tf dtype,\n            pytorch data-type, or python numerical type.\n        to (str): Either one of \'tf\' (tensorflow), \'pt\' (pytorch), \'np\' (numpy), \'str\' (string).\n            Default=""tf"".\n\n    Returns:\n        TensorFlow, Numpy, pytorch or string, representing a data type (depending on `to` parameter).\n    """"""\n    # Bool: tensorflow.\n    if get_backend() == ""tf"":\n        if dtype in [""bool"", bool, np.bool_, tf.bool]:\n            return np.bool_ if to == ""np"" else tf.bool\n        elif dtype in [""float"", ""float32"", float, np.float32, tf.float32]:\n            return np.float32 if to == ""np"" else tf.float32\n        if dtype in [""float64"", np.float64, tf.float64]:\n            return np.float64 if to == ""np"" else tf.float64\n        elif dtype in [""int"", ""int32"", int, np.int32, tf.int32]:\n            return np.int32 if to == ""np"" else tf.int32\n        elif dtype in [""int64"", np.int64]:\n            return np.int64 if to == ""np"" else tf.int64\n        elif dtype in [""uint8"", np.uint8]:\n            return np.uint8 if to == ""np"" else tf.uint8\n        elif dtype in [""str"", np.str_]:\n            return np.unicode_ if to == ""np"" else tf.string\n        elif dtype in [""int16"", np.int16]:\n            return np.int16 if to == ""np"" else tf.int16\n    elif get_backend() == ""pytorch"":\n        # N.b. this behaves differently than other bools, careful with Python bool comparisons.\n        if dtype in [""bool"", bool, np.bool_] or dtype is torch.uint8:\n            return np.bool_ if to == ""np"" else torch.uint8\n        elif dtype in [""float"", ""float32"", float, np.float32] or dtype is torch.float32:\n            return np.float32 if to == ""np"" else torch.float32\n        if dtype in [""float64"", np.float64] or dtype is torch.float64:\n            return np.float64 if to == ""np"" else torch.float64\n        elif dtype in [""int"", ""int32"", int, np.int32] or dtype is torch.int32:\n            return np.int32 if to == ""np"" else torch.int32\n        elif dtype in [""int64"", np.int64] or dtype is torch.int64:\n            return np.int64 if to == ""np"" else torch.int64\n        elif dtype in [""uint8"", np.uint8] or dtype is torch.uint8:\n            return np.uint8 if to == ""np"" else torch.uint8\n        elif dtype in [""int16"", np.int16] or dtype is torch.int16:\n            return np.int16 if to == ""np"" else torch.int16\n\n        # N.b. no string tensor type.\n\n    raise RLGraphError(""Error: Type conversion to \'{}\' for type \'{}\' not supported."".format(to, str(dtype)))\n\n\ndef get_rank(tensor):\n    """"""\n    Returns the rank (as a single int) of an input tensor.\n\n    Args:\n        tensor (Union[tf.Tensor,torch.Tensor,np.ndarray]): The input tensor.\n\n    Returns:\n        int: The rank of the given tensor.\n    """"""\n    if isinstance(tensor, np.ndarray) or get_backend() == ""python"":\n        return tensor.ndim\n    elif get_backend() == ""tf"":\n        return tensor.get_shape().ndims\n    elif get_backend() == ""pytorch"":\n        # No rank or ndim in PyTorch apparently.\n        return tensor.dim()\n\n\ndef get_shape(op, flat=False, no_batch=False):\n    """"""\n    Returns the (static) shape of a given DataOp as a tuple.\n\n    Args:\n        op (DataOp): The input op.\n        flat (bool): Whether to return the flattened shape (the product of all ints in the shape tuple).\n            Default: False.\n        no_batch (bool): Whether to exclude a possible 0th batch rank from the returned shape.\n            Default: False.\n\n    Returns:\n        tuple: The shape of the given op.\n        int: The flattened dim of the given op (if flat=True).\n    """"""\n    # Dict.\n    if isinstance(op, dict):\n        shape = tuple([get_shape(op[key]) for key in sorted(op.keys())])\n    # Tuple-op.\n    elif isinstance(op, tuple):\n        shape = tuple([get_shape(i) for i in op])\n    # Numpy ndarrays.\n    elif isinstance(op, np.ndarray):\n        shape = op.shape\n    # Primitive op (e.g. tensorflow)\n    else:\n        if get_backend() == ""tf"":\n            op_shape = op.get_shape()\n            # Unknown shape (e.g. a cond op).\n            if op_shape.ndims is None:\n                return None\n            shape = tuple(op_shape.as_list())\n        elif get_backend() == ""pytorch"":\n            op_shape = op.shape\n            shape = list(op_shape)\n    # Remove batch rank?\n    if no_batch is True and shape[0] is None:\n        shape = shape[1:]\n\n    # Return as-is or as flat shape?\n    if flat is False:\n        return shape\n    else:\n        return int(np.prod(shape))\n\n\ndef get_batch_size(tensor):\n    """"""\n    Returns the (dynamic) batch size (dim of 0th rank) of an input tensor.\n\n    Args:\n        tensor (SingleDataOp): The input tensor.\n\n    Returns:\n        SingleDataOp: The op holding the batch size information of the given tensor.\n    """"""\n    if get_backend() == ""tf"":\n        return tf.shape(tensor)[0]\n    elif get_backend() == ""pytorch"":\n        return tensor.shape[0]\n\n\ndef force_list(elements=None, to_tuple=False):\n    """"""\n    Makes sure `elements` is returned as a list, whether `elements` is a single item, already a list, or a tuple.\n\n    Args:\n        elements (Optional[any]): The inputs as single item, list, or tuple to be converted into a list/tuple.\n            If None, returns empty list/tuple.\n        to_tuple (bool): Whether to use tuple (instead of list).\n\n    Returns:\n        Union[list,tuple]: All given elements in a list/tuple depending on `to_tuple`\'s value. If elements is None,\n            returns an empty list/tuple.\n    """"""\n    ctor = list\n    if to_tuple is True:\n        ctor = tuple\n    return ctor() if elements is None else ctor(elements) \\\n        if type(elements) in [list, tuple] else ctor([elements])\n\n\nforce_tuple = partial(force_list, to_tuple=True)\n\n\ndef strip_list(elements):\n    """"""\n    Loops through elements if it\'s a tuple, otherwise processes elements as follows:\n    If a list (or np.ndarray) of length 1, extracts that single item, otherwise leaves\n    the list/np.ndarray untouched.\n\n    Args:\n        elements (any): The input single item, list, or np.ndarray to be converted into\n            single item(s) (if length is 1).\n\n    Returns:\n        any: Single element(s) (the only one in input) or the original input list.\n    """"""\n    # `elements` is a tuple (e.g. from a function return). Process each element separately.\n    if isinstance(elements, tuple):\n        ret = []\n        for el in elements:\n            ret.append(el[0] if isinstance(el, (np.ndarray, list)) and len(el) == 1 else el)\n        return tuple(ret)\n    # `elements` is not a tuple: Process only `elements`.\n    else:\n        return elements[0] if isinstance(elements, (np.ndarray, list)) and len(elements) == 1 else \\\n            elements\n\n\ndef default_dict(original, defaults):\n    """"""\n    Updates the `original` dict with values from `defaults`, but only for those keys that\n    do not exist yet in `original`.\n    Changes `original` in place, but leaves `defaults` as is.\n\n    Args:\n        original (Optional[dict]): The dict to (soft)-update. If None, return `defaults`.\n        defaults (dict): The dict to update from.\n    """"""\n    if original is None:\n        return defaults\n\n    for key in defaults:\n        if key not in original:\n            original[key] = defaults[key]\n    return original\n\n\ndef clip(x, min_val, max_val):\n    """"""\n    Clips x between min_ and max_val.\n\n    Args:\n        x (float): The input to be clipped.\n        min_val (float): The min value for x.\n        max_val (float): The max value for x.\n\n    Returns:\n        float: The clipped value.\n    """"""\n    return max(min_val, min(x, max_val))\n\n\ndef get_method_type(method):\n    """"""\n    Returns either ""graph_fn"" OR ""api"" OR ""other"" depending on which method (and method\n    name) is passed in.\n\n    Args:\n        method (callable): The actual method to analyze.\n\n    Returns:\n        Union[str,None]: ""graph_fn"", ""api"" or ""other"". None if method is not a callable.\n    """"""\n    # Not a callable: Return None.\n    if not callable(method):\n        return None\n    # Simply recognize graph_fn by their name.\n    elif method.__name__[:9] == ""_graph_fn"":\n        return ""graph_fn""\n    else:\n        return ""unknown""\n\n\ndef does_method_call_graph_fns(method):\n    """"""\n    Inspects the source code of a method and returns whether this method calls graph_fns inside its code.\n\n    Args:\n        method (callable): The method to inspect.\n\n    Returns:\n        bool: Whether at least one graph_fn is called somewhere in the source code.\n    """"""\n    src = strip_source_code(method)\n\n    mo = re.search(r\'\\.call\\([^,\\)]+\\._graph_fn_\', src)\n    return mo is not None\n\n\ndef get_num_return_values(method):\n    """"""\n    Does a regexp-based source code inspection and tries to figure out the number of values that the method\n    will return (assuming, that the method always returns the same number of values, regardless of its inputs).\n\n    Args:\n        method (callable): The method to get the number of returns  values for.\n\n    Returns:\n        int: The number of return values of `method`.\n    """"""\n    src = strip_source_code(method)\n\n    mo = re.search(r\'.*\\breturn (.+)\', src, flags=re.DOTALL)\n    if mo:\n        return_code = mo.group(1)\n        # TODO: raise error if ""tuple()"" -> means we don\'t know how many return value we will have\n        # TODO: in this case, the Component needs to specify how many output records it produces.\n        # Resolve tricky things (parentheses, etc..).\n        while re.search(r\'[\\(\\)""]\', return_code):\n            return_code = re.sub(r\'\\([^\\(\\)]*\\)\', \'\', return_code)\n            return_code = re.sub(r\'""[^""]*""\', \'\', return_code)\n            # Probably have to add more resolution code here.\n\n        # Count the commas and return.\n        num_return_values = len(return_code.split("",""))\n        return num_return_values\n    else:\n        return 0\n\n\ndef strip_source_code(method, remove_nested_functions=True):\n    """"""\n    Strips the source code of a method by everything that\'s ""not important"", like comments.\n\n    Args:\n        method (Union[callable,str]): The method to strip the source code for (or the source code directly as string).\n        remove_nested_functions (bool): Whether to remove nested functions from the source code as well.\n            These usually confuse the analysis of a method\'s source code as they comprise a method within a method.\n\n    Returns:\n        str: The stripped source code.\n    """"""\n    if callable(method):\n        src = inspect.getsource(method)\n    else:\n        src = method\n\n    # Resolve \'\\\' at end of lines.\n    src = re.sub(r\'\\\\\\s*\\n\', """", src)\n    # Remove single line comments.\n    src = re.sub(r\'\\n\\s*#.+\', """", src)\n    # Remove multi-line comments.\n    src = re.sub(r\'""""""(.|\\n)*?""""""\', """", src)\n    # Remove nested functions.\n    if remove_nested_functions is True:\n        src = re.sub(r\'\\n(\\s*)def \\w+\\((.|\\n)+?\\n\\1[^\\s\\n]\', """", src)\n\n    return src\n\n\ndef force_torch_tensors(params, requires_grad=False):\n    """"""\n    Converts input params to torch tensors\n    Args:\n        params (list): Input args.\n        requires_grad (bool): If gradients need to be computed from these arguments.\n\n    Returns:\n        list: List of Torch tensors.\n    """"""\n    if get_backend() == ""pytorch"":\n        tensor_params = []\n        for param in params:\n            if isinstance(param, dict):\n                # Flatten dict.\n                param = define_by_run_flatten(param)\n                ret = {}\n                for key, value in param.items():\n                    ret[key] = convert_param(value, requires_grad)\n                tensor_params.append(ret)\n            else:\n                tensor_params.append(convert_param(param, requires_grad))\n        return tensor_params\n\n\ndef convert_param(param, requires_grad):\n    if get_backend() == ""pytorch"":\n        # Do nothing.\n        if isinstance(param, torch.Tensor):\n            return param\n        if isinstance(param, list):\n            param = np.asarray(param)\n        if isinstance(param, np.ndarray):\n            param_type = param.dtype\n        else:\n            param_type = type(param)\n        convert_type = convert_dtype(param_type, to=""pytorch"")\n\n        # PyTorch cannot convert from a np.bool_, must be uint.\n        if isinstance(param, np.ndarray) and param.dtype == np.bool_:\n            param = param.astype(np.uint8)\n\n        if convert_type == torch.float32 or convert_type == torch.float or convert_type == torch.float16:\n            # Only floats can require grad.\n            return torch.tensor(param, dtype=convert_type, requires_grad=requires_grad)\n        else:\n            return torch.tensor(param, dtype=convert_type)'"
rlgraph/utils/visualization_util.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport re\n\ntry:\n    import graphviz\nexcept ImportError:\n    graphviz = None\n\nfrom rlgraph import rlgraph_dir\nfrom rlgraph.components.component import Component\nfrom rlgraph.spaces import IntBox, BoolBox, FloatBox, TextBox\nfrom rlgraph.utils.op_records import DataOpRecord, DataOpRecordColumnIntoAPIMethod, DataOpRecordColumnFromAPIMethod, \\\n    DataOpRecordColumnIntoGraphFn, DataOpRecordColumnFromGraphFn\n\n# Define some colors for the debug graphs.\n_API_COLOR = ""#f4c542""\n_GRAPH_FN_COLOR = ""#ede2c9""\n_COMPONENT_SUBGRAPH_FACTORS_RGB = [0.0, 0.666, 1.0]\n_INTBOX_COLOR = ""#e5dd69""\n_FLOATBOX_COLOR = ""#62ce54""\n_BOOLBOX_COLOR = ""#d8565d""\n_TEXTBOX_COLOR = ""#3b6ddb""\n_OTHER_TYPE_COLOR = ""#777777""\n_FONTSIZE_COMPONENTS = ""24""\n_FONTSIZE_APIS_AND_GRAPH_FNS = ""18""\n\n\ndef draw_meta_graph(component, *, output=None, apis=True, graph_fns=False, render=True,\n                    component_filter=""auto"", column_filter=None, connection_filter=None,\n                    _graphviz_graph=None, _all_connections=None, _max_nesting_level=None, highlighted_connections=None):\n    """"""\n    Creates and draws the GraphViz component graph for the given Component (and all its sub-components).\n    - An (outermost) placeholder is represented by a node named `Placeholder_[arg-name]`.\n    - Each Component is a subgraph inside the graph, each one named exactly like the `global_scope` of its Component\n      (with root being called `root` if no name was given to it) and the prefix ""cluster_"" (GraphViz needs this\n      to allow styling of the subgraph).\n    - A Component\'s APIs are subgraphs (within the Component\'s subgraph) and named `api:[global_scope]/[api_name]`.\n    - An API\'s in- and out-columns are nodes and named after their str(`id`) property, which is unique across\n        the entire graph.\n    - graph_fns are subgraphs (within the Component\'s subgraph) and named `graph:[global_scope]/[graph_fn_name]`.\n    - A graph_fn\'s in- and out-columns are nodes and named via their `id` property (same global counter as\n        API\'s in/out columns).\n\n    Args:\n        component (Component): The Component object to draw the component-graph for.\n        output (Optional[str]): The output filename to write to. Default: ~/.rlgraph/rlgraph_debug_draw.gv.pdf\n\n        apis (Union[bool,Set[str]]): Whether to include API methods. If Set of str: Only include APIs whose names are in\n            the given set.\n\n        graph_fns (Union[bool,str]): Whether to include GraphFns. If Set of str: Only include GraphFns whose names are\n            in the given set.\n\n        component_filter (Optional[Set[Component]]): If provided, only Components in the filter set will be rendered.\n            Set to ""auto"" for only allowing given `component` plus sub-components to be rendered.\n\n        column_filter (Optional[Set[int]]): If provided, only columns (by their `id`) in the filter set will be\n            rendered.\n\n        connection_filter (Optional[Set[str]]): If provided, only connections in the filter set will be rendered.\n            Connections are specified by the two connected DataOpRecord `id`s.\n\n        render (bool): Whether to show the resulting graph as pdf in the browser.\n        _graphviz_graph (Digraph): Whether this is a recursive (sub-component) call.\n\n        _all_connections (Optional[Set[Tuple[str]]]): An set of connection-defining tuples (from-col, to-col).\n            Connections are generated (as graphviz edges) at the very end, on the top-most graph level.\n\n        _max_nesting_level (Optional[int]): The maximum nesting level of the original call\'s `component` and all\n            its sub-Components. Used for finding the correct color shade for the Component subgraphs.\n\n        highlighted_connections (Optional[set]): Connections that should be highlighted (e.g. in red), indicating\n            shape-, type- or other Space-related problems in the build process.\n    """"""\n    if graphviz is None:\n        return\n\n    # _return is True if we are in the first original call (not one of the recursive ones).\n    return_ = False\n    if _graphviz_graph is None:\n        assert _all_connections is None and _max_nesting_level is None  # only for recursive calls\n        if component_filter == ""auto"":\n            component_filter = set(component.get_all_sub_components(exclude_self=False))\n        _graphviz_graph, _all_connections, _max_nesting_level = \\\n            _init_meta_graph(component, apis=apis, graph_fns=graph_fns, component_filter=component_filter,\n                             column_filter=column_filter,\n                             connection_filter=connection_filter, highlighted_connections=highlighted_connections)\n        return_ = True\n\n    for sub_component in component.sub_components.values():\n        # Skip filtered components and all helper Components.\n        if (component_filter is not None and sub_component not in component_filter) or \\\n                re.match(r\'^\\.helper-\', sub_component.scope):\n            continue\n        with _graphviz_graph.subgraph(name=""cluster_"" + sub_component.global_scope) as sg:\n            # Set some attributes of the sub-graph for display purposes.\n            sg.attr(style=""rounded"", label=sub_component.scope, penwidth=""3"", fontsize=_FONTSIZE_COMPONENTS)\n            sg.attr(color=_get_api_subgraph_color(sub_component.nesting_level, _max_nesting_level))\n            # Add all APIs of this sub-Component.\n            if apis is not False and sub_component.graph_builder is not None:\n                for api_name in sub_component.api_methods:\n                    if apis is not False and \\\n                            (apis is True or ""api:"" + sub_component.global_scope + ""/"" + api_name in apis):\n                        _add_api_or_graph_fn_to_graph(\n                            sub_component, sg, api_name=api_name, apis=apis, graph_fns=graph_fns,\n                            component_filter=component_filter, column_filter=column_filter,\n                            connection_filter=connection_filter, _connections=_all_connections,\n                            highlighted_connections=highlighted_connections\n                        )\n            else:\n                # Add a fake node to make the Component\'s subgraph visible.\n                sg.node(\n                    sub_component.global_scope, label="""", _attributes=dict(shape=""point"", color=""#ffffff"")\n                )\n\n            # Add all GraphFns of this sub-Component.\n            if graph_fns is not False and sub_component.graph_builder is not None:\n                for graph_fn_name in sub_component.graph_fns:\n                    if graph_fns is not False and \\\n                            (graph_fns is True or ""graph:"" + sub_component.global_scope + ""/"" + graph_fn_name in graph_fns):\n                        _add_api_or_graph_fn_to_graph(\n                            sub_component, sg, graph_fn_name=graph_fn_name, apis=apis, graph_fns=graph_fns,\n                            component_filter=component_filter, column_filter=column_filter,\n                            connection_filter=connection_filter, _connections=_all_connections,\n                            highlighted_connections=highlighted_connections\n                        )\n\n            # Call recursively on sub-Components of this sub-Component.\n            draw_meta_graph(\n                sub_component, apis=apis, graph_fns=graph_fns, connection_filter=connection_filter,\n                component_filter=component_filter, column_filter=column_filter,\n                render=False, _graphviz_graph=sg, _all_connections=_all_connections,\n                _max_nesting_level=_max_nesting_level, highlighted_connections=highlighted_connections\n            )\n\n    # We are in the first original call (not one of the recursive ones).\n    if return_ is True:\n        # Do all connections between nodes on the top-level graph.\n        # Connections are encoded as tuples: (from, to, label?, color?, style?)\n        for connection in _all_connections:\n            attributed_dict = dict(color=(connection[3] if len(connection) > 3 else None), penwidth=""3"")\n            if len(connection) > 4:\n                attributed_dict[""style""] = connection[4]\n            _graphviz_graph.edge(\n                connection[0], connection[1],\n                label=(connection[2] if len(connection) > 2 else """"),\n                _attributes=attributed_dict\n            )\n\n        # Render the graph as pdf (in browser).\n        if render is True:\n            if output is None:\n                output = os.path.join(rlgraph_dir, ""rlgraph_debug_draw.gv"")\n            _render(_graphviz_graph, output, view=True)\n\n\ndef draw_sub_meta_graph_from_op_rec(op_rec, meta_graph):\n    """"""\n    Traces back an op-rec through the meta-graph drawing everything on the path til reaching the leftmost placeholders.\n\n    Args:\n        op_rec (DataOpRecord): The DataOpRecord object to trace back to the beginning of the graph.\n        meta_graph (MetaGraph): The Meta-graph, of which `op_rec` is a part.\n\n    Returns:\n        str: Meta-graph markup string.\n    """"""\n    components, api_methods, graph_fns, columns, connections, highlighted_connection = _backtrace_op_rec(op_rec)\n    draw_meta_graph(\n        meta_graph.root_component,\n        apis=api_methods, graph_fns=graph_fns,\n        component_filter=components,\n        column_filter=columns, connection_filter=connections,\n        highlighted_connections={highlighted_connection}\n    )\n\n\ndef _backtrace_op_rec(op_rec, _components=None, _api_methods=None, _graph_fns=None, _columns=None, _connections=None):\n    """"""\n    Returns all component/API-method/graph_fn strings in a set that lie on the way back of the given op-rec till\n    the beginning of the meta-graph (placeholders).\n\n    Args:\n        op_rec (DataOpRecord): The DataOpRecord to backtrace.\n\n        _api_methods (Optional[Set[str]]): Set of all API-methods (plus global_scope of their Component)\n            that lie on the backtraced path of the op-rec.\n\n        _graph_fns (Optional[Set[str]]): Set of all GraphFns (plus global_scope of their Component)\n            that lie on the backtraced path of the op-rec.\n\n        _connections (Optional[Set[str]]): All connections that lie in the backtraced path.\n\n    Returns:\n        tuple:\n            - Set[str]: The set of `global_scope/[API-method|graph_fn]-name` that lie on the backtraced path.\n            - Set[str]: The set of connections between API-methods/graph_fns backtraced from the given op_rec.\n    """"""\n    highlighted_connection = None\n\n    if _components is None:\n        _components = set()\n\n        api_input = op_rec\n        # TODO: This could be risky, if op_rec is an graph-fn-output.\n        while api_input is not None and not isinstance(api_input.column, DataOpRecordColumnIntoAPIMethod):\n            api_input = api_input.previous\n        if api_input is not None:\n            highlighted_connection = (api_input.previous.id, api_input.id)\n\n        assert _api_methods is None and _graph_fns is None and _columns is None and _connections is None\n        _api_methods = set()\n        _graph_fns = set()\n        _connections = set()\n        _columns = set()\n\n    # If column is None, we have reached the leftmost (placeholder) op-recs.\n    while op_rec is not None and op_rec.column is not None:\n        col = op_rec.column\n        if col is not None:\n            _components.add(col.component)\n        column_type, column_scope, _ = _get_column_type_scope_and_component(col)\n        if column_type == ""API"":\n            _api_methods.add(column_scope)\n        else:\n            assert column_type == ""GF"" or column_type == """"  # could be a non-specific key-lookup column as well.\n            _graph_fns.add(column_scope)\n\n        if op_rec.previous is None:\n            # Graph_fn.\n            if isinstance(op_rec.column, DataOpRecordColumnFromGraphFn):\n                # Add all inputs recursively to our sets and stop here.\n                for in_op_rec in op_rec.column.in_graph_fn_column.op_records:\n                    _components, _api_methods, _graph_fns, _columns, _connections, _ = _backtrace_op_rec(\n                        in_op_rec, _components, _api_methods, _graph_fns, _columns, _connections\n                    )\n            # Leftmost placeholder.\n            else:\n                pass\n        else:\n            # Store the connection between op_rec and previous op_rec (by their int id).\n            prev_col = op_rec.previous.column\n            if prev_col is not None:\n                _connections.add((op_rec.previous.id, op_rec.id))\n                _columns.add(prev_col.id)\n            else:\n                _connections.add((""Placeholder_"" + op_rec.previous.placeholder, op_rec.id))\n            _columns.add(col.id)\n\n        # Process next op-rec on left side.\n        op_rec = op_rec.previous\n\n    return _components, _api_methods, _graph_fns, _columns, _connections, highlighted_connection\n\n\ndef _add_api_or_graph_fn_to_graph(\n        component, graphviz_graph, *, api_name=None, graph_fn_name=None, draw_columns=True, apis=True, graph_fns=False,\n        component_filter=None, column_filter=None,\n        connection_filter=None, _connections=None, highlighted_connections=None\n):\n    """"""\n    Args:\n        component (Component):  The Component whose API method `api_name` needs to be added to the graph.\n        api_name (str): The name of the API method to add to the graph.\n        graphviz_graph (Digraph): The GraphViz Digraph object to add the API-method\'s nodes and edges to.\n        draw_columns (bool): Whether to draw in/out-columns as nodes or not draw them at all.\n        component_filter\n        component_filter (Optional[Set[Component]]): If provided, only Components in the filter set will be rendered.\n        column_filter (Optional[Set[int]]): If provided, only Component IDs in the filter set will be rendered.\n        connection_filter (Optional[Set[str]]): If provided, only draw connections that are in this set.\n        _connections (Set[Tuple[str]]): Set of connection tuples to add to. Connections will only be done on the\n            top-level.\n    """"""\n    # One must be provided.\n    assert api_name is not None or graph_fn_name is not None\n    name = (api_name or graph_fn_name)\n    type_ = ""api"" if api_name else ""graph""\n\n    record = component.api_methods[api_name] if api_name else component.graph_fns[graph_fn_name]\n    # Draw only when called at least once.\n    num_calls = len(record.out_op_columns)\n\n    if num_calls > 0:\n        # Create API as subgraph of graphviz_graph.\n        # Make all connections coming into all in columns and out-columns of this API.\n        with graphviz_graph.subgraph(name=""cluster_{}:{}/{}"".format(type_, component.global_scope, name)) as sg:\n            # Style the API/graph-fn subgraph.\n            color = _API_COLOR if api_name else _GRAPH_FN_COLOR\n            sg.attr(color=color, bgcolor=color, bb=""0px"", label=name, style=""rounded"",\n                    fontsize=_FONTSIZE_APIS_AND_GRAPH_FNS)\n\n            # Draw all in/out columns as nodes.\n            if draw_columns is True:\n                # Draw in/out columns as one node (op-recs going in and coming out will be edges).\n                # Draw Placeholders as nodes (named: ""Placeholder:[arg-name]"").\n                for call_id in reversed(range(len(record.out_op_columns))):\n                    out_col = record.out_op_columns[call_id]\n                    # Filter by column.\n                    if column_filter is not None and out_col.id not in column_filter:\n                        continue\n                    assert len(out_col.op_records) > 0\n                    sg.node(str(out_col.id), label=""out-"" + str(call_id))\n                    # Make all connections going into this column (all its op-recs).\n                    for op_rec in out_col.op_records:\n                        # Column is None: graph-in to graph-out -> Use simple non-labeled, non-colored connector.\n                        if op_rec.previous is None:\n                            _connections.add((str(op_rec.column.in_graph_fn_column.id), str(out_col.id)))\n                            continue\n\n                        # Possible filtering by API/graph-fn.\n                        prev_type, prev_scope, prev_component = _get_column_type_scope_and_component(\n                            op_rec.previous.column\n                        )\n                        if (component_filter is not None and prev_component not in component_filter) or \\\n                                (prev_type == ""GF"" and\n                                 (graph_fns is False or (graph_fns is not True and prev_scope not in graph_fns))) \\\n                                or (prev_type == ""API"" and\n                                    (apis is False or (apis is not True and prev_scope not in apis))):\n                            continue\n                        # Check connection-filter as well.\n                        if connection_filter is None or (op_rec.previous.id, op_rec.id) in connection_filter:\n                            color, shape = _get_color_and_shape_from_space(op_rec.space)\n                            style = None\n                            # Highlight this connection?\n                            if highlighted_connections is not None and \\\n                                    (op_rec.previous.id, op_rec.id) in highlighted_connections:\n                                color = ""#ff0000""\n                                style = ""dotted""\n                            _connections.add((str(op_rec.previous.column.id), str(out_col.id)) + (shape, color, style))\n\n                # In columns.\n                for call_id in reversed(range(len(record.in_op_columns))):\n                    in_col = record.in_op_columns[call_id]\n                    # Filter by column.\n                    if column_filter is not None and in_col.id not in column_filter:\n                        continue\n                    if len(in_col.op_records) > 0:\n                        sg.node(str(in_col.id), label=""in-"" + str(call_id))\n                    # Make all connections going into this column (all its op-recs).\n                    for op_rec in in_col.op_records:\n                        # Possible filtering by Component/API/graph-fn.\n                        prev_type, prev_scope, prev_component = _get_column_type_scope_and_component(\n                            op_rec.previous.column\n                        )\n                        # Placeholder have component=None, do not filter placeholder connections because of that.\n                        if prev_component is None:\n                            prev_component = component\n                        if (component_filter is not None and prev_component not in component_filter) or \\\n                                (prev_type == ""GF"" and\n                                 (graph_fns is False or (graph_fns is not True and prev_scope not in graph_fns))) \\\n                                or (prev_type == ""API"" and\n                                    (apis is False or (apis is not True and prev_scope not in apis))):\n                            continue\n\n                        color, shape = _get_color_and_shape_from_space(op_rec.space)\n                        style = None\n\n                        # GraphFn -> Use input arg position as label.\n                        if type_ == ""graph"":\n                            label = str(op_rec.previous.position)\n                        # API no kwarg -> Use input arg\'s name.\n                        elif op_rec.kwarg is None:\n                            label = op_rec.column.api_method_rec.input_names[op_rec.position]\n                        # API kwarg -> Use kwarg.\n                        else:\n                            label = op_rec.kwarg\n\n                        if op_rec.previous.placeholder is None:\n                            from_column = op_rec.previous.column.id\n                            from_op_rec_id = op_rec.previous.id\n                            label += "" ("" + shape + "")""\n                        else:\n                            from_column = from_op_rec_id = ""Placeholder_{}"".format(op_rec.previous.placeholder)\n                            label = """"  # Set label to None (placeholder\'s node already describes itself)\n\n                        # Check connection-filter as well.\n                        if connection_filter is None or (from_op_rec_id, op_rec.id) in connection_filter:\n                            # Highlight this connection?\n                            if highlighted_connections is not None and \\\n                                    (from_op_rec_id, op_rec.id) in highlighted_connections:\n                                color = ""#ff0000""\n                                style = ""dotted""\n                            _connections.add((str(from_column), str(in_col.id)) + (label, color, style))\n\n            # Add a fake node to make this API/graph-fn\'s node visible.\n            else:\n                sg.node(\n                    type_ + "":"" + component.global_scope + ""/"" + api_name, label="""",\n                    _attributes=dict(shape=""point"", color=""#ffffff"")\n                )\n\n\ndef _init_meta_graph(component, *, apis, graph_fns, component_filter, column_filter,\n                     connection_filter, highlighted_connections=None):\n    _all_connections = set()\n    graphviz_graph = graphviz.Digraph(name=component.scope if len(component.scope) > 0 else ""root"")\n    graphviz_graph.attr(label=graphviz_graph.name, labelloc=""t"", rankdir=""LR"", fontsize=_FONTSIZE_COMPONENTS)\n    #graphviz_graph.attr(bgcolor=""#000000"")\n\n    # If we need to draw all APIs (and are already in build-phase).\n    if apis is not False and component.graph_builder is not None:\n        placeholders = set()  # Already generated placeholders (only one per unique input-arg name).\n        # Auto-create a placeholder filter if there is a connection filter.\n        placeholder_filter = None\n        if connection_filter is not None:\n            placeholder_filter = [p[0] for p in connection_filter if re.match(r\'^Placeholder_.+$\', str(p[0]))]\n        for api_name in component.api_methods:\n            # If this is the root, add placeholder nodes to graphviz graph.\n            if component.nesting_level == 0:\n                for call_id in reversed(range(len(component.api_methods[api_name].in_op_columns))):\n                    in_col = component.api_methods[api_name].in_op_columns[call_id]\n                    # Filter by column.\n                    if column_filter is not None and in_col.id not in column_filter:\n                        continue\n                    # Make all connections going into this column (all its op-recs).\n                    for incoming_op_rec in in_col.op_records:\n                        if incoming_op_rec.previous.placeholder is not None:\n                            placeholder = incoming_op_rec.previous.placeholder\n                            # Filter by placeholder.\n                            if placeholder not in placeholders and \\\n                                    (placeholder_filter is None or ""Placeholder_"" + placeholder in placeholder_filter):\n                                color, shape = _get_color_and_shape_from_space(incoming_op_rec.previous.space)\n                                graphviz_graph.node(\n                                    ""Placeholder_"" + placeholder, label=placeholder + "" ({})"".format(shape),\n                                    _attributes=dict(color=color, style=""filled"")\n                                )\n                                placeholders.add(incoming_op_rec.previous.placeholder)\n\n            if apis is not False and (apis is True or ""api:"" + component.global_scope + ""/"" + api_name in apis):\n                _add_api_or_graph_fn_to_graph(\n                    component, graphviz_graph,  # draw_columns=(apis is True),\n                    api_name=api_name, apis=apis, graph_fns=graph_fns,\n                    component_filter=component_filter, column_filter=column_filter, connection_filter=connection_filter,\n                    _connections=_all_connections, highlighted_connections=highlighted_connections\n                )\n    # Calculate the max. nesting level of the Component and all its sub-components and subtract\n    # by the Component\'s nesting_level.\n    _max_nesting_level = 0\n    for sub_component in component.get_all_sub_components(exclude_self=False):\n        if component_filter is not None and sub_component not in component_filter:\n            continue\n        if (sub_component.nesting_level or 0) > _max_nesting_level:\n            _max_nesting_level = sub_component.nesting_level\n    _max_nesting_level -= (component.nesting_level or 0)\n\n    # Now that we know the max nesting level, set this Component\'s subgraph color.\n    graphviz_graph.attr(color=_get_api_subgraph_color((component.nesting_level or 0), _max_nesting_level))\n\n    return graphviz_graph, _all_connections, _max_nesting_level\n\n\ndef _get_column_type_scope_and_component(col):\n    if isinstance(col, DataOpRecordColumnIntoAPIMethod):\n        return ""API"", ""api:"" + col.component.global_scope + ""/"" + col.api_method_rec.name, col.component\n    elif isinstance(col, DataOpRecordColumnFromAPIMethod):\n        return ""API"", ""api:"" + col.component.global_scope + ""/"" + col.api_method_name, col.component\n    elif isinstance(col, DataOpRecordColumnIntoGraphFn):\n        return ""GF"", ""graph:"" + col.component.global_scope + ""/"" + col.graph_fn.__name__, col.component\n    elif isinstance(col, DataOpRecordColumnFromGraphFn):\n        return ""GF"", ""graph:"" + col.component.global_scope + ""/"" + col.graph_fn_name, col.component\n    else:\n        return """", """", None\n\n\ndef _get_color_and_shape_from_space(space):\n    """"""\n    Returns a color code and shape description given some Space.\n\n    Args:\n        space (Space): The Space to derive the information from.\n\n    Returns:\n        tuple:\n            - str: Color code.\n            - str: Shape descriptor.\n    """"""\n    if space is None:\n        return None, """"\n\n    # Space is a `component:var`-specifier (like ""variables:policy"").\n    if isinstance(space, str):\n        assert re.match(r\'^variables:.+$\', space)\n        return _OTHER_TYPE_COLOR, """"\n\n    shape_str = ""{}"".format(list(space.shape)) + ("" +B"" if space.has_batch_rank else """") + \\\n                ("" +T"" if space.has_time_rank else """")\n\n    # Int -> dark green.\n    if isinstance(space, IntBox):\n        return _INTBOX_COLOR, shape_str\n    # Float -> light green\n    elif isinstance(space, FloatBox):\n        return _FLOATBOX_COLOR, shape_str\n    # Bool -> red.\n    elif isinstance(space, BoolBox):\n        return _BOOLBOX_COLOR, shape_str\n    # Text -> blue.\n    elif isinstance(space, TextBox):\n        return _TEXTBOX_COLOR, shape_str\n    # Else -> gray.\n    else:\n        return _OTHER_TYPE_COLOR, shape_str\n\n\ndef _get_api_subgraph_color(nesting_level, max_nesting_level):\n    # Get the ratio of nesting_level with respect to max_nesting_level and make lighter (towards 255)\n    # the higher this ratio.\n    # -> Higher level components are then darker than lower level ones.\n    r = int(255 - 64 * min(nesting_level / max_nesting_level, 1.0))\n    color = ""#{:02X}{:02X}{:02X}"".format(\n        int(r * _COMPONENT_SUBGRAPH_FACTORS_RGB[0]),\n        int(r * _COMPONENT_SUBGRAPH_FACTORS_RGB[1]),\n        int(r * _COMPONENT_SUBGRAPH_FACTORS_RGB[2])\n    )\n    return color\n\n\ndef _render(graphviz_graph, file, view=False):\n    # Try rendering with the installed GraphViz engine.\n    try:\n        graphviz_graph.render(file, view=view)\n    except graphviz.backend.ExecutableNotFound as e:\n        print(\n            ""Error: GraphViz (backend engine) executable missing: Try the following steps to enable automated ""\n            ""graphviz-backed debug drawings:\\n""\n            ""1) Install the GraphViz engine on your local machine: https://graphviz.gitlab.io/download/\\n""\n            ""2) Add the GraphViz `bin` directory (created in step 2) when installing GraphViz) to your `PATH` ""\n            ""environment variable\\n""\n            ""3) Reopen your `python` shell and try again.\\n\\n""\n        )\n        print(graphviz_graph.source)  # debug printout\n'"
rlgraph/components/action_adapters/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.action_adapters.action_adapter import ActionAdapter\nfrom rlgraph.components.action_adapters.action_adapter_utils import get_action_adapter_type_from_distribution_type, \\\n    get_distribution_spec_from_action_adapter\nfrom rlgraph.components.action_adapters.bernoulli_distribution_adapter import BernoulliDistributionAdapter\nfrom rlgraph.components.action_adapters.beta_distribution_adapter import BetaDistributionAdapter\nfrom rlgraph.components.action_adapters.categorical_distribution_adapter import CategoricalDistributionAdapter\nfrom rlgraph.components.action_adapters.gumbel_softmax_distribution_adapter import GumbelSoftmaxDistributionAdapter\nfrom rlgraph.components.action_adapters.normal_distribution_adapter import NormalDistributionAdapter\nfrom rlgraph.components.action_adapters.squashed_normal_distribution_adapter import SquashedNormalDistributionAdapter\n\nActionAdapter.__lookup_classes__ = dict(\n    actionadapter=ActionAdapter,\n    bernoullidistributionadapter=BernoulliDistributionAdapter,\n    categoricaldistributionadapter=CategoricalDistributionAdapter,\n    betadistributionadapter=BetaDistributionAdapter,\n    gumbelsoftmaxdistributionadapter=GumbelSoftmaxDistributionAdapter,\n    gumbelsoftmaxadapter=GumbelSoftmaxDistributionAdapter,\n    normaldistributionadapter=NormalDistributionAdapter,\n    squashednormaladapter=SquashedNormalDistributionAdapter,\n    squashednormaldistributionadapter=SquashedNormalDistributionAdapter,\n)\n\n__all__ = [""ActionAdapter"", ""get_action_adapter_type_from_distribution_type"",\n           ""get_distribution_spec_from_action_adapter""] + \\\n          list(set(map(lambda x: x.__name__, ActionAdapter.__lookup_classes__.values())))\n'"
rlgraph/components/action_adapters/action_adapter.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph.components.layers.nn.dense_layer import DenseLayer\nfrom rlgraph.components.layers.preprocessing.reshape import ReShape\nfrom rlgraph.components.neural_networks.neural_network import NeuralNetwork\nfrom rlgraph.spaces import Space, ContainerSpace\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import graph_fn, rlgraph_api\nfrom rlgraph.utils.rlgraph_errors import RLGraphObsoletedError\n\n\n# TODO: Create a more primitive base class only defining the API-methods.\n# Then rename this into `SingleLayerActionAdapter`.\nclass ActionAdapter(NeuralNetwork):\n    """"""\n    A Component that cleans up a neural network\'s flat output and gets it ready for parameterizing a\n    Distribution Component.\n    Processing steps include:\n    - Sending the raw, flattened NN output through a Dense layer whose number of units matches the flattened\n    action space.\n    - Reshaping (according to the action Space).\n    - Translating the reshaped outputs (logits) into probabilities (by softmaxing) and log-probabilities (log).\n    """"""\n    def __init__(self, action_space, weights_spec=None, biases_spec=None, activation=None,\n                 pre_network_spec=None, scope=""action-adapter"", **kwargs):\n        """"""\n        Args:\n            action_space (Optional[Space]): The action Space within which this Component will create actions.\n                NOTE: Exactly one of `action_space` of `final_shape` must be provided.\n\n            #final_shape (Optional[Tuple[int]): An optional final output shape (in case action_space is not provided).\n            #    If None, will calculate the shape automatically from the given `action_space`.\n            #    NOTE: Exactly one of `action_space` of `final_shape` must be provided.\n\n            weights_spec (Optional[any]): An optional RLGraph Initializer spec that will be used to initialize the\n                weights of `self.action layer`. Default: None (use default initializer).\n\n            biases_spec (Optional[any]): An optional RLGraph Initializer spec that will be used to initialize the\n                biases of `self.action layer`. Default: None (use default initializer, which is usually 0.0).\n\n            activation (Optional[str]): The activation function to use for `self.action_layer`.\n                Default: None (=linear).\n\n            pre_network_spec (Optional[dict,NeuralNetwork]): A spec dict for a neural network coming before the\n                last action layer. If None, only the action layer itself is applied.\n        """"""\n        # Build the action layer for this adapter based on the given action-space.\n        self.action_space = action_space.with_batch_rank()\n        assert not isinstance(self.action_space, ContainerSpace),\\\n            ""ERROR: ActionAdapter cannot handle ContainerSpaces!""\n\n        units, self.final_shape = self.get_units_and_shape()\n        assert isinstance(units, int) and units > 0, ""ERROR: `units` must be int and larger 0!""\n\n        action_layer = DenseLayer(\n            units=units,\n            activation=activation,\n            weights_spec=weights_spec,\n            biases_spec=biases_spec,\n            scope=""action-layer""\n        )\n\n        # Do we have a pre-NN?\n        self.network = NeuralNetwork.from_spec(pre_network_spec, scope=""action-network"")  # type: NeuralNetwork\n        self.network.add_layer(action_layer)\n\n        # Add the reshape layer to match the action space\'s shape.\n        self.network.add_layer(ReShape(new_shape=self.final_shape))\n\n        super(ActionAdapter, self).__init__(self.network, scope=scope, **kwargs)\n\n    def get_units_and_shape(self):\n        """"""\n        Returns the number of units in the layer that will be added and the shape of the output according to the\n        action space.\n\n        Returns:\n            Tuple:\n                int: The number of units for the action layer.\n                shape: The final shape for the output space.\n        """"""\n        raise NotImplementedError\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        # Check the input Space.\n        last_nn_layer_space = input_spaces[""inputs[0]""]  # type: Space\n        sanity_check_space(last_nn_layer_space, non_allowed_types=[ContainerSpace])\n        # Check the action Space.\n        #sanity_check_space(self.action_space, must_have_batch_rank=True)\n\n    #@rlgraph_api\n    #def get_action_adapter_outputs(self, nn_input, original_nn_input=None):\n    #    """"""\n    #    Args:\n    #        nn_input (DataOpRecord): The NN output of the preceding neural network.\n    #        original_nn_input (DataOpRecord): The NN input of the preceding neural network.\n    #            Only needed if unfold_time_rank is True.\n\n    #    Returns:\n    #        SingleDataOp: The logits (action layer outputs + reshaped).\n    #    """"""\n    #    # If we are unfolding and NOT folding -> pass original input in as well.\n    #    if self.api_methods_options[0].get(""unfold_time_rank"") and \\\n    #            not self.api_methods_options[0].get(""fold_time_rank""):\n    #        logits_out = self.call(nn_input, original_nn_input)\n    #    else:\n    #        logits_out = self.call(nn_input)\n    #    return logits_out\n\n    @rlgraph_api\n    def get_parameters(self, *inputs):  #, original_nn_input=None):\n        """"""\n        Args:\n            inputs (DataOpRecord): The NN output(s) of the preceding neural network.\n            #original_nn_input (DataOpRecord): The NN input  of the preceding neural network (needed for optional time-rank\n            #    folding/unfolding purposes).\n\n        Returns:\n            Dict[str,SingleDataOp]:\n                - ""adapter_outputs"": The raw nn_input, only reshaped according to the action_space.\n                - ""parameters"": The raw parameters to pass into the distribution Component for generating an actual\n                    backend-distribution object.\n                - ""probs"": The action-probabilities iff discrete actions. None otherwise.\n                - ""log_probs"": log([action probabilities]) iff discrete actions. None otherwise.\n        """"""\n        #nn_outputs = self.get_action_adapter_outputs(nn_input, original_nn_input)\n        adapter_outputs = self.call(*inputs)  #, original_nn_input)\n        out = self.get_parameters_from_adapter_outputs(adapter_outputs)\n        return dict(\n            adapter_outputs=adapter_outputs, parameters=out[""parameters""],\n            probabilities=out[""probabilities""], log_probs=out[""log_probs""]\n        )\n\n    @rlgraph_api(must_be_complete=False)\n    def get_parameters_from_adapter_outputs(self, adapter_outputs):\n        """"""\n        Args:\n            adapter_outputs (SingleDataOp): The (action-space reshaped) output of the action adapter\'s action layer.\n        """"""\n        parameters, probs, log_probs = self._graph_fn_get_parameters_from_adapter_outputs(adapter_outputs)\n        return dict(parameters=parameters, probabilities=probs, log_probs=log_probs)\n\n    @graph_fn\n    def _graph_fn_get_parameters_from_adapter_outputs(self, adapter_outputs):\n        """"""\n        Creates properties/parameters and log-probs from some reshaped output.\n\n        Args:\n            adapter_outputs (SingleDataOp): The output of some layer that is already reshaped\n                according to our action Space.\n\n        Returns:\n            tuple (2x SingleDataOp):\n                parameters (DataOp): The parameters, ready to be passed to a Distribution object\'s\n                    get_distribution API-method (usually some probabilities or loc/scale pairs).\n                log_probs (DataOp): log(probs) in categorical case. In all other cases: None.\n        """"""\n        raise NotImplementedError\n\n    # OBSOLETED API methods. Raise ERROR when called.\n    def get_logits(self, nn_input, original_nn_input=None):\n        raise RLGraphObsoletedError(""API-method"", ""get_logits"", ""call"")\n\n    def get_logits_parameters_log_probs(self, nn_input, original_nn_input=None):\n        raise RLGraphObsoletedError(""API method"", ""get_logits_parameters_log_probs"", ""get_parameters"")\n\n    def get_logits_probabilities_log_probs(self, nn_input, original_nn_input=None):\n        raise RLGraphObsoletedError(""API method"", ""get_logits_probabilities_log_probs"", ""get_parameters"")\n\n    def get_parameters_log_probs(self, logits):\n        raise RLGraphObsoletedError(""API-method"", ""get_parameters_log_probs"", ""get_parameters_from_adapter_outputs"")\n'"
rlgraph/components/action_adapters/action_adapter_utils.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\n\n\ndef get_action_adapter_type_from_distribution_type(distribution_type_str):\n    """"""\n    Args:\n        distribution_type_str (str): The type (str) of the Distribution object, for which to return an appropriate\n            ActionAdapter lookup-class string.\n\n    Returns:\n        str: The lookup-class string for an action-adapter.\n    """"""\n    # IntBox: Categorical.\n    if distribution_type_str == ""Categorical"":\n        return ""categorical-distribution-adapter""\n    elif distribution_type_str == ""GumbelSoftmax"":\n        return ""gumbel-softmax-distribution-adapter""\n    # BoolBox: Bernoulli.\n    elif distribution_type_str == ""Bernoulli"":\n        return ""bernoulli-distribution-adapter""\n    # Continuous action space: Normal/Beta/etc. distribution.\n    # Unbounded -> Normal distribution.\n    elif distribution_type_str == ""Normal"":\n        return ""normal-distribution-adapter""\n    # Bounded -> Beta.\n    elif distribution_type_str == ""Beta"":\n        return ""beta-distribution-adapter""\n    # Bounded -> Squashed Normal.\n    elif distribution_type_str == ""SquashedNormal"":\n        return ""squashed-normal-distribution-adapter""\n    else:\n        raise RLGraphError(""\'{}\' is an unknown Distribution type!"".format(distribution_type_str))\n\n\ndef get_distribution_spec_from_action_adapter(action_adapter):\n    action_adapter_type_str = type(action_adapter).__name__\n    if action_adapter_type_str == ""CategoricalDistributionAdapter"":\n        return dict(type=""categorical"")\n    elif action_adapter_type_str == ""GumbelSoftmaxDistributionAdapter"":\n        return dict(type=""gumbel-softmax"")\n    elif action_adapter_type_str == ""BernoulliDistributionAdapter"":\n        return dict(type=""bernoulli"")\n    # TODO: What about multi-variate normal with non-trivial co-var matrices?\n    elif action_adapter_type_str == ""NormalDistributionAdapter"":\n        return dict(type=""normal"")\n    elif action_adapter_type_str == ""BetaDistributionAdapter"":\n        return dict(type=""beta"")\n    elif action_adapter_type_str == ""SquashedNormalDistributionAdapter"":\n        return dict(type=""squashed-normal"")\n    elif action_adapter_type_str == ""NormalMixtureDistributionAdapter"":\n        # TODO: MixtureDistribution is generic (any sub-distributions, but its AA is not (only supports mixture-Normal))\n        return dict(type=""mixture"", _args=[""multivariate-normal"" for _ in range(action_adapter.size_mixture)])\n    else:\n        raise RLGraphError(""\'{}\' is an unknown ActionAdapter type!"".format(action_adapter_type_str))\n'"
rlgraph/components/action_adapters/bernoulli_distribution_adapter.py,2,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.action_adapters import ActionAdapter\nfrom rlgraph.utils.decorators import graph_fn\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass BernoulliDistributionAdapter(ActionAdapter):\n    """"""\n    Action adapter for the Bernoulli distribution.\n    """"""\n    def get_units_and_shape(self):\n        units = self.action_space.flat_dim\n        new_shape = self.action_space.get_shape()\n        return units, new_shape\n\n    @graph_fn\n    def _graph_fn_get_parameters_from_adapter_outputs(self, adapter_outputs):\n        parameters = adapter_outputs\n        probs = None\n        log_probs = None\n\n        if get_backend() == ""tf"":\n            parameters._batch_rank = 0\n            # Probs (sigmoid).\n            probs = tf.nn.sigmoid(parameters)\n            probs._batch_rank = 0\n            # Log probs.\n            log_probs = tf.log(x=probs)\n            log_probs._batch_rank = 0\n\n        elif get_backend() == ""pytorch"":\n            # Probs (sigmoid).\n            probs = torch.sigmoid(parameters)\n            # Log probs.\n            log_probs = torch.log(probs)\n\n        # TODO: For Bernoulli, params are probs (not raw logits!). Therefore, must return here parameters==probs.\n        # TODO: Change like in Categorical such that parameters are raw logits.\n        return probs, probs, log_probs\n'"
rlgraph/components/action_adapters/beta_distribution_adapter.py,5,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom math import log\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.action_adapters import ActionAdapter\nfrom rlgraph.utils.decorators import graph_fn\nfrom rlgraph.utils.ops import DataOpTuple\nfrom rlgraph.utils.util import SMALL_NUMBER\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass BetaDistributionAdapter(ActionAdapter):\n    """"""\n    Action adapter for the Beta distribution\n    """"""\n    def get_units_and_shape(self):\n        units = 2 * self.action_space.flat_dim  # Those two dimensions are the mean and log sd\n        # Add moments (2x for each action item).\n        if self.action_space.shape == ():\n            new_shape = (2,)\n        else:\n            new_shape = tuple(list(self.action_space.shape[:-1]) + [self.action_space.shape[-1] * 2])\n        return units, new_shape\n\n    @graph_fn\n    def _graph_fn_get_parameters_from_adapter_outputs(self, adapter_outputs):\n        parameters = None\n\n        full_action_space_rank = len(self.action_space.get_shape(with_batch_rank=True, with_time_rank=True))\n\n        if get_backend() == ""tf"":\n            # Stabilize both alpha and beta (currently together in last_nn_layer_output).\n            parameters = tf.clip_by_value(\n                adapter_outputs, clip_value_min=log(SMALL_NUMBER), clip_value_max=-log(SMALL_NUMBER)\n            )\n            parameters = tf.log((tf.exp(parameters) + 1.0)) + 1.0\n            alpha, beta = tf.split(parameters, num_or_size_splits=2, axis=-1)\n\n            # If action_space is 0D, we have to squeeze the params by 1 dim to make them match our action_space.\n            if full_action_space_rank != len(alpha.shape.as_list()):\n                # Assume a difference by exactly 1 dim.\n                assert len(alpha.shape.as_list()) == full_action_space_rank + 1\n                alpha = tf.squeeze(alpha, axis=-1)\n                beta = tf.squeeze(beta, axis=-1)\n\n            alpha._batch_rank = 0\n            beta._batch_rank = 0\n\n            parameters = DataOpTuple([alpha, beta])\n\n        elif get_backend() == ""pytorch"":\n            # Stabilize both alpha and beta (currently together in last_nn_layer_output).\n            parameters = torch.clamp(\n                adapter_outputs, min=log(SMALL_NUMBER), max=-log(SMALL_NUMBER)\n            )\n            parameters = torch.log((torch.exp(parameters) + 1.0)) + 1.0\n\n            # Split in the middle.\n            alpha, beta = torch.split(parameters, split_size_or_sections=int(parameters.shape[0] / 2), dim=-1)\n\n            # If action_space is 0D, we have to squeeze the params by 1 dim to make them match our action_space.\n            if full_action_space_rank != len(list(alpha.size)):\n                # Assume a difference by exactly 1 dim.\n                assert len(list(alpha.size)) == full_action_space_rank + 1\n                alpha = torch.squeeze(alpha, dim=-1)\n                beta = torch.squeeze(beta, dim=-1)\n\n            parameters = DataOpTuple([alpha, beta])\n\n        return parameters, None, None\n'"
rlgraph/components/action_adapters/categorical_distribution_adapter.py,2,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.action_adapters import ActionAdapter\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import graph_fn\nfrom rlgraph.utils.util import SMALL_NUMBER\n\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n    from rlgraph.utils.pytorch_util import SMALL_NUMBER_TORCH\n\n\nclass CategoricalDistributionAdapter(ActionAdapter):\n    """"""\n    Action adapter for the Categorical distribution.\n    """"""\n    def check_input_spaces(self, input_spaces, action_space=None):\n        super(CategoricalDistributionAdapter, self).check_input_spaces(input_spaces, action_space)\n        # IntBoxes must have categories.\n        sanity_check_space(self.action_space, must_have_categories=True)\n\n    def get_units_and_shape(self):\n        units = self.action_space.flat_dim_with_categories\n        new_shape = self.action_space.get_shape(with_category_rank=True)\n        return units, new_shape\n\n    @graph_fn\n    def _graph_fn_get_parameters_from_adapter_outputs(self, adapter_outputs):\n        """"""\n        Returns:\n            Tuple:\n                - DataOp: Raw logits (parameters for a Categorical Distribution).\n                - DataOp: log-probs: log(softmaxed_logits).\n        """"""\n        parameters = adapter_outputs\n        probs = None\n        log_probs = None\n\n        if get_backend() == ""tf"":\n            parameters._batch_rank = 0\n            # Probs (softmax).\n            probs = tf.maximum(x=tf.nn.softmax(logits=parameters, axis=-1), y=SMALL_NUMBER)\n            probs._batch_rank = 0\n            # Log probs.\n            log_probs = tf.log(x=probs)\n            log_probs._batch_rank = 0\n\n        elif get_backend() == ""pytorch"":\n            # Probs (softmax).\n            probs = torch.max(torch.softmax(parameters, dim=-1), SMALL_NUMBER_TORCH)\n            # Log probs.\n            log_probs = torch.log(probs)\n\n        return parameters, probs, log_probs\n'"
rlgraph/components/action_adapters/gumbel_softmax_distribution_adapter.py,2,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.action_adapters import ActionAdapter\nfrom rlgraph.utils import SMALL_NUMBER\nfrom rlgraph.utils.decorators import graph_fn\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    from rlgraph.utils.pytorch_util import SMALL_NUMBER_TORCH\n    import torch\n\n\nclass GumbelSoftmaxDistributionAdapter(ActionAdapter):\n    """"""\n    Action adapter for the GumbelSoftmax distribution\n    """"""\n    def get_units_and_shape(self):\n        units = self.action_space.flat_dim_with_categories\n        new_shape = self.action_space.get_shape(with_category_rank=True)\n        return units, new_shape\n\n    @graph_fn\n    def _graph_fn_get_parameters_from_adapter_outputs(self, adapter_outputs):\n        parameters = adapter_outputs\n        probs = None\n        log_probs = None\n\n        if get_backend() == ""tf"":\n            parameters._batch_rank = 0\n            # Probs (softmax).\n            probs = tf.maximum(x=tf.nn.softmax(logits=parameters, axis=-1), y=SMALL_NUMBER)\n            probs._batch_rank = 0\n            # Log probs.\n            log_probs = tf.log(x=probs)\n            log_probs._batch_rank = 0\n\n        elif get_backend() == ""pytorch"":\n            # Probs (softmax).\n            probs = torch.max(torch.softmax(parameters, dim=-1), SMALL_NUMBER_TORCH)\n            # Log probs.\n            log_probs = torch.log(probs)\n\n        return parameters, probs, log_probs\n\n'"
rlgraph/components/action_adapters/normal_distribution_adapter.py,5,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom math import log\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.action_adapters import ActionAdapter\nfrom rlgraph.utils.decorators import graph_fn\nfrom rlgraph.utils.ops import DataOpTuple\nfrom rlgraph.utils.util import SMALL_NUMBER\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass NormalDistributionAdapter(ActionAdapter):\n    """"""\n    Action adapter for the Normal distribution\n    """"""\n    def get_units_and_shape(self):\n        # Mean and log sd.\n        units = 2 * self.action_space.flat_dim\n        if self.action_space.shape == ():\n            new_shape = (2,)\n        else:\n            new_shape = tuple(list(self.action_space.shape[:-1]) + [self.action_space.shape[-1] * 2])\n        return units, new_shape\n\n    @graph_fn\n    def _graph_fn_get_parameters_from_adapter_outputs(self, adapter_outputs):\n        parameters = None\n\n        full_action_space_rank = len(self.action_space.get_shape(with_batch_rank=True, with_time_rank=True))\n\n        if get_backend() == ""tf"":\n            mean, log_sd = tf.split(adapter_outputs, num_or_size_splits=2, axis=-1)\n            # If action_space is 0D, we have to squeeze the params by 1 dim to make them match our action_space.\n            if full_action_space_rank != len(mean.shape.as_list()):\n                # Assume a difference by exactly 1 dim.\n                assert len(mean.shape.as_list()) == full_action_space_rank + 1\n                mean = tf.squeeze(mean, axis=-1)\n                log_sd = tf.squeeze(log_sd, axis=-1)\n            log_sd = tf.clip_by_value(log_sd, log(SMALL_NUMBER), -log(SMALL_NUMBER))\n\n            # Turn log sd into sd to ascertain always positive stddev values.\n            sd = tf.exp(log_sd)\n\n            mean._batch_rank = 0\n            sd._batch_rank = 0\n\n            parameters = DataOpTuple([mean, sd])\n\n        elif get_backend() == ""pytorch"":\n            # Continuous actions.\n            mean, log_sd = torch.split(\n                adapter_outputs, split_size_or_sections=int(parameters.shape[0] / 2), dim=-1\n            )\n            # If action_space is 0D, we have to squeeze the params by 1 dim to make them match our action_space.\n            if full_action_space_rank != len(list(mean.size)):\n                # Assume a difference by exactly 1 dim.\n                assert len(list(mean.size)) == full_action_space_rank + 1\n                mean = torch.squeeze(mean, dim=-1)\n                log_sd = torch.squeeze(log_sd, dim=-1)\n\n            log_sd = torch.clamp(log_sd, min=log(SMALL_NUMBER), max=-log(SMALL_NUMBER))\n\n            # Turn log sd into sd.\n            sd = torch.exp(log_sd)\n\n            parameters = DataOpTuple([mean, sd])\n\n        return parameters, None, None\n'"
rlgraph/components/action_adapters/squashed_normal_distribution_adapter.py,5,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.action_adapters import ActionAdapter\nfrom rlgraph.utils.decorators import graph_fn\nfrom rlgraph.utils.ops import DataOpTuple\nfrom rlgraph.utils.util import MIN_LOG_STDDEV, MAX_LOG_STDDEV\n\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass SquashedNormalDistributionAdapter(ActionAdapter):\n    """"""\n    Action adapter for the Squashed-normal distribution.\n    """"""\n    def get_units_and_shape(self):\n        # Add moments (2x for each action item).\n        units = 2 * self.action_space.flat_dim  # Those two dimensions are the mean and log sd\n        if self.action_space.shape == ():\n            new_shape = (2,)\n        else:\n            new_shape = tuple(list(self.action_space.shape[:-1]) + [self.action_space.shape[-1] * 2])\n        return units, new_shape\n\n    @graph_fn\n    def _graph_fn_get_parameters_from_adapter_outputs(self, adapter_outputs):\n\n        full_action_space_rank = len(self.action_space.get_shape(with_batch_rank=True, with_time_rank=True))\n\n        if get_backend() == ""tf"":\n            mean, log_sd = tf.split(adapter_outputs, num_or_size_splits=2, axis=-1)\n\n            # If action_space is 0D, we have to squeeze the params by 1 dim to make them match our action_space.\n            if full_action_space_rank != len(mean.shape.as_list()):\n                # Assume a difference by exactly 1 dim.\n                assert len(mean.shape.as_list()) == full_action_space_rank + 1\n                mean = tf.squeeze(mean, axis=-1)\n                log_sd = tf.squeeze(log_sd, axis=-1)\n\n            log_sd = tf.clip_by_value(log_sd, MIN_LOG_STDDEV, MAX_LOG_STDDEV)\n\n            # Turn log sd into sd to ascertain always positive stddev values.\n            sd = tf.exp(log_sd)\n\n            mean._batch_rank = 0\n            sd._batch_rank = 0\n\n        elif get_backend() == ""pytorch"":\n            mean, log_sd = torch.split(adapter_outputs, split_size_or_sections=2, dim=1)\n\n            # If action_space is 0D, we have to squeeze the params by 1 dim to make them match our action_space.\n            if full_action_space_rank != len(list(mean.size)):\n                # Assume a difference by exactly 1 dim.\n                assert len(list(mean.size)) == full_action_space_rank + 1\n                mean = torch.squeeze(mean, dim=-1)\n                log_sd = torch.squeeze(log_sd, dim=-1)\n\n            log_sd = torch.clamp(log_sd, min=MIN_LOG_STDDEV, max=MAX_LOG_STDDEV)\n\n            # Turn log sd into sd.\n            sd = torch.exp(log_sd)\n\n        parameters = DataOpTuple([mean, sd])\n        return parameters, None, None\n'"
rlgraph/components/common/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.common.batch_apply import BatchApply\nfrom rlgraph.components.common.batch_splitter import BatchSplitter\nfrom rlgraph.components.common.container_merger import ContainerMerger\n# TODO: Obsoleted classes.\nfrom rlgraph.components.common.decay_components import DecayComponent, ConstantDecay\nfrom rlgraph.components.common.multi_gpu_synchronizer import MultiGpuSynchronizer\nfrom rlgraph.components.common.noise_components import NoiseComponent, ConstantNoise, GaussianNoise, \\\n    OrnsteinUhlenbeckNoise\nfrom rlgraph.components.common.repeater_stack import RepeaterStack\nfrom rlgraph.components.common.sampler import Sampler\nfrom rlgraph.components.common.slice import Slice\nfrom rlgraph.components.common.staging_area import StagingArea\nfrom rlgraph.components.common.synchronizable import Synchronizable\nfrom rlgraph.components.common.time_dependent_parameters import TimeDependentParameter, Constant, \\\n    LinearDecay, PolynomialDecay, ExponentialDecay\n\nTimeDependentParameter.__lookup_classes__ = dict(\n    parameter=TimeDependentParameter,\n    constant=Constant,\n    constantparameter=Constant,\n    constantdecay=Constant,\n    lineardecay=LinearDecay,\n    polynomialdecay=PolynomialDecay,\n    exponentialdecay=ExponentialDecay\n)\nTimeDependentParameter.__default_constructor__ = Constant\n\n# TODO: Obsoleted classes.\nDecayComponent.__lookup_classes__ = dict(\n    decay=DecayComponent\n)\n\nNoiseComponent.__lookup_classes__ = dict(\n    noise=NoiseComponent,\n    constantnoise=ConstantNoise,\n    gaussiannoise=GaussianNoise,\n    ornsteinuhlenbeck=OrnsteinUhlenbeckNoise,\n    ornsteinuhlenbecknoise=OrnsteinUhlenbeckNoise\n)\nNoiseComponent.__default_constructor__ = GaussianNoise\n\n\n__all__ = [""BatchApply"", ""ContainerMerger"",\n           ""Synchronizable"", ""RepeaterStack"", ""Slice"",\n           ""Sampler"", ""BatchSplitter"", ""MultiGpuSynchronizer""] + \\\n          list(set(map(lambda x: x.__name__, list(TimeDependentParameter.__lookup_classes__.values()) +\n                       list(DecayComponent.__lookup_classes__.values()) +\n                       list(NoiseComponent.__lookup_classes__.values()))))\n'"
rlgraph/components/common/batch_apply.py,0,"b'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.component import Component\nfrom rlgraph.components.layers.preprocessing.reshape import ReShape\nfrom rlgraph.utils.decorators import rlgraph_api\n\n\nclass BatchApply(Component):\n    """"""\n    Takes an input with batch and time ranks, then folds the time rank into the batch rank,\n    calls a certain API of some arbitrary child component, and unfolds the time rank again.\n    """"""\n    def __init__(self, sub_component, api_method_name, scope=""batch-apply"", **kwargs):\n        """"""\n        Args:\n            sub_component (Component): The sub-Component to apply the batch to.\n            api_method_name (str): The name of the API-method to call on the sub-component.\n        """"""\n        super(BatchApply, self).__init__(scope=scope, **kwargs)\n\n        self.sub_component = sub_component\n        self.api_method_name = api_method_name\n\n        # Create the necessary reshape components.\n        self.folder = ReShape(fold_time_rank=True, scope=""folder"")\n        self.unfolder = ReShape(unfold_time_rank=True, scope=""unfolder"")\n\n        self.add_components(self.sub_component, self.folder, self.unfolder)\n\n    @rlgraph_api\n    def call(self, input_):\n        folded = self.folder.call(input_)\n        applied = getattr(self.sub_component, self.api_method_name)(folded)\n        unfolded = self.unfolder.call(applied, input_before_time_rank_folding=input_)\n        return unfolded\n'"
rlgraph/components/common/batch_splitter.py,1,"b'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.ops import FlattenedDataOp, unflatten_op, DataOpTuple\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass BatchSplitter(Component):\n    """"""\n    Splits a number of incoming DataOps along their batch dimension.\n    """"""\n    def __init__(self, num_shards, shard_size, scope=""batch-splitter"", **kwargs):\n        """"""\n        Args:\n            num_shards (int): Number of shards to split the batch dimension into.\n            shard_size (int): The number of samples in a per-GPU shard.\n        """"""\n        super(BatchSplitter, self).__init__(\n            scope=scope, graph_fn_num_outputs=dict(_graph_fn_split_batch=num_shards), **kwargs\n        )\n\n        assert num_shards > 1, ""ERROR: num shards must be greater than 1 but is {}."".format(\n            num_shards\n        )\n        self.num_shards = num_shards\n        self.shard_size = shard_size\n\n    @rlgraph_api(flatten_ops=True)\n    def _graph_fn_split_batch(self, *inputs):\n        """"""\n        Splits all DataOps in *inputs along their batch dimension into n equally sized shards. The number of shards\n        is determined by `self.num_shards` (int) and the size of each shard depends on the incoming batch size with\n        possibly a few superfluous items in the batch being discarded\n        (effective batch size = num_shards x shard_size).\n\n        Args:\n            *input (FlattenedDataOp): Input tensors which must all have the same batch dimension.\n\n        Returns:\n            tuple:\n                # Each shard consisting of: A DataOpTuple with len = number of input args.\n                # - Each item in the DataOpTuple is a FlattenedDataOp with (flat) key (describing the input-piece\n                # (e.g. ""/states1"")) and values being the (now sharded) batch data for that input piece.\n\n                # e.g. return (for 2 shards):\n                # tuple(DataOpTuple(input1_flatdict, input2_flatdict, input3_flatdict, input4_flatdict), DataOpTuple([same]))\n\n\n                List of FlattenedDataOps () containing DataOpTuples containing the input shards.\n        """"""\n        if get_backend() == ""tf"":\n            # Must be evenly divisible so we slice out an evenly divisible tensor.\n            # E.g. 203 items in batch with 4 shards -> Only 4 x 50 = 200 are usable.\n            usable_size = self.shard_size * self.num_shards\n\n            # List (one item for each input arg). Each item in the list looks like:\n            # A FlattenedDataOp with (flat) keys (describing the input-piece (e.g. ""/states1"")) and values being\n            # lists of len n for the n shards\' data.\n            inputs_flattened_and_split = list()\n\n            for input_arg_data in inputs:\n                shard_dict = FlattenedDataOp()\n                for flat_key, data in input_arg_data.items():\n                    usable_input_tensor = data[:usable_size]\n                    shard_dict[flat_key] = tf.split(value=usable_input_tensor, num_or_size_splits=self.num_shards)\n                inputs_flattened_and_split.append(shard_dict)\n\n            # Flip the list to generate a new list where each item represents one shard.\n            shard_list = list()\n            for shard_idx in range(self.num_shards):\n                # To be converted into FlattenedDataOps over the input-arg-pieces once complete.\n                input_arg_list = list()\n                for input_elem in range(len(inputs)):\n                    sharded_data_dict = FlattenedDataOp()\n                    for flat_key, shards in inputs_flattened_and_split[input_elem].items():\n                        sharded_data_dict[flat_key] = shards[shard_idx]\n                    input_arg_list.append(unflatten_op(sharded_data_dict))\n                # Must store everything as FlattenedDataOp otherwise the re-nesting will not work.\n                shard_list.append(DataOpTuple(input_arg_list))\n\n            # Return n values (n = number of batch shards).\n            return tuple(shard_list)\n\n'"
rlgraph/components/common/container_merger.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.ops import DataOpDict, DataOpTuple, FLATTEN_SCOPE_PREFIX\n\n\nclass ContainerMerger(Component):\n    """"""\n    Merges incoming items into one FlattenedDataOp.\n    """"""\n    def __init__(self, *input_names_or_num_items, **kwargs):\n        """"""\n        Args:\n            *input_names_or_num_items (Union[str,int]): List of the names of the different inputs in the\n                order they will be passed into the `merge` API-method in the returned merged Dict.\n                Or the number of items in the Tuple to be merged.\n                Example:\n                input_names_or_num_items = [""A"", ""B""]\n                - merge(Dict(c=1, d=2), Tuple(3, 4))\n                - returned value: Dict(A=Dict(c=1, d=2), B=Tuple(3, 4))\n                input_names_or_num_items = 3: 3 items will be merged into a Tuple.\n\n        Keyword Args:\n            merge_tuples_into_one (bool): Whether to merge incoming DataOpTuples into one single DataOpTuple.\n                If True:  tupleA + tupleB -> (tupleA[0] + tupleA[1] + tupleA[...] + tupleB[0] + tupleB[1] ...).\n                If False: tupleA + tupleB -> (tupleA + tupleB).\n\n            is_tuple (bool): Whether we should merge a tuple.\n        """"""\n        self.merge_tuples_into_one = kwargs.pop(""merge_tuples_into_one"", False)\n        self.is_tuple = kwargs.pop(""is_tuple"", self.merge_tuples_into_one)\n\n        super(ContainerMerger, self).__init__(scope=kwargs.pop(""scope"", ""container-merger""), **kwargs)\n\n        self.dict_keys = None\n\n        if len(input_names_or_num_items) == 1 and isinstance(input_names_or_num_items[0], int):\n            self.is_tuple = True\n        else:\n            # and not re.search(r\'/\', i)\n            # or some of them have \'/\' characters in them, which are not allowed\n            assert all(isinstance(i, str) for i in input_names_or_num_items), \\\n                ""ERROR: Not all input names of DictMerger Component \'{}\' are strings."".format(self.global_scope)\n            self.dict_keys = input_names_or_num_items\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        spaces = []\n        idx = 0\n        while True:\n            key = ""inputs[{}]"".format(idx)\n            if key not in input_spaces:\n                break\n            spaces.append(input_spaces[key])\n            idx += 1\n\n        # If Tuple -> Incoming inputs could be of any number.\n        if self.dict_keys:\n            len_ = len(self.dict_keys)\n            assert len(spaces) == len_,\\\n                ""ERROR: Number of incoming Spaces ({}) does not match number of given `dict_keys` ({}) in"" \\\n                ""ContainerMerger Component \'{}\'!"".format(len(spaces), len_, self.global_scope)\n\n    @rlgraph_api\n    def _graph_fn_merge(self, *inputs):\n        """"""\n        Merges the inputs into a single DataOpDict OR DataOpTuple with the flat keys given in `self.dict_keys`.\n\n        Args:\n            *inputs (FlattenedDataOp): The input items to be merged into a ContainerDataOp.\n\n        Returns:\n            ContainerDataOp: The DataOpDict or DataOpTuple as a merger of all *inputs.\n        """"""\n        if self.is_tuple is True:\n            ret = []\n            for op in inputs:\n                # Merge single items inside a DataOpTuple into resulting tuple.\n                if self.merge_tuples_into_one and isinstance(op, DataOpTuple):\n                    ret.extend(list(op))\n                # Strict by-input merging.\n                else:\n                    ret.append(op)\n            return DataOpTuple(ret)\n        else:\n            ret = DataOpDict()\n            for i, op in enumerate(inputs):\n                if get_backend() == ""pytorch"" and self.execution_mode == ""define_by_run"":\n                    ret[FLATTEN_SCOPE_PREFIX + self.dict_keys[i]] = op\n                else:\n                    ret[self.dict_keys[i]] = op\n            return ret\n\n'"
rlgraph/components/common/decay_components.py,11,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.spaces.int_box import IntBox\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils import util\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.pytorch_util import pytorch_tile\nfrom rlgraph.utils.rlgraph_errors import RLGraphObsoletedError\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass DecayComponent(Component):\n    """"""\n    A base class Component that takes a time input and outputs some decaying-over-time value.\n\n    API:\n        value([current-time-step]): The current decayed value based on the time step and c\'tor settings.\n    """"""\n    def __init__(self, from_=None, to_=None, start_timestep=0, num_timesteps=10000,\n                 scope=""decay"", **kwargs):\n        """"""\n        Args:\n            from_ (float): The max value returned between 0 and `start_timestep`.\n            to_ (float): The min value returned from [`start_timestep`+`num_timesteps`] onwards.\n            start_timestep (int): The timestep at which to start the decay process.\n            num_timesteps (int): The number of time steps over which to decay. Outputs will be stationary before and\n                after this decaying period.\n\n        Keyword Args:\n            from (float): See `from_`. For additional support to specify without the underscore.\n            to (float): See `to_`. For additional support to specify without the underscore.\n        """"""\n        raise RLGraphObsoletedError(""DecayComponent"", ""DecayComponent"", ""TimeDependentParameter"")\n        kwargs_from = kwargs.pop(""from"", None)\n        kwargs_to = kwargs.pop(""to"", None)\n\n        super(DecayComponent, self).__init__(scope=scope, **kwargs)\n\n        self.from_ = kwargs_from if kwargs_from is not None else from_ if from_ is not None else 1.0\n        self.to_ = kwargs_to if kwargs_to is not None else to_ if to_ is not None else 0.0\n        self.start_timestep = start_timestep\n        self.num_timesteps = num_timesteps\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        time_step_space = input_spaces[""time_step""]  # type: Space\n        sanity_check_space(\n            time_step_space, allowed_types=[IntBox], must_have_batch_rank=False,\n            must_have_categories=False, rank=0\n        )\n\n    @rlgraph_api\n    def _graph_fn_decayed_value(self, time_step):\n        """"""\n        Args:\n            time_step (DataOp): The int-type DataOp that holds the current global time_step.\n\n        Returns:\n            DataOp: The decay\'d value depending on the current time step.\n        """"""\n        if get_backend() == ""tf"":\n            smaller_than_start = time_step <= self.start_timestep\n\n            shape = tf.shape(time_step)\n            # time_step comes in as a time-sequence of time-steps.\n            if shape.shape[0] > 0:\n                return tf.where(\n                    condition=smaller_than_start,\n                    # We are still in pre-decay time.\n                    x=tf.tile(tf.constant([self.from_]), multiples=shape),\n                    # We are past pre-decay time.\n                    y=tf.where(\n                        condition=(time_step >= self.start_timestep + self.num_timesteps),\n                        # We are in post-decay time.\n                        x=tf.tile(tf.constant([self.to_]), multiples=shape),\n                        # We are inside the decay time window.\n                        y=self._graph_fn_decay(\n                            tf.cast(x=time_step - self.start_timestep, dtype=util.convert_dtype(""float""))\n                        ),\n                        name=""cond-past-end-time""\n                    ),\n                    name=""cond-before-start-time""\n                )\n            # Single 0D time step.\n            else:\n                return tf.cond(\n                    pred=smaller_than_start,\n                    # We are still in pre-decay time.\n                    true_fn=lambda: self.from_,\n                    # We are past pre-decay time.\n                    false_fn=lambda: tf.cond(\n                        pred=(time_step >= self.start_timestep + self.num_timesteps),\n                        # We are in post-decay time.\n                        true_fn=lambda: self.to_,\n                        # We are inside the decay time window.\n                        false_fn=lambda: self._graph_fn_decay(\n                            tf.cast(x=time_step - self.start_timestep, dtype=util.convert_dtype(""float""))\n                        ),\n                    ),\n                )\n        elif get_backend() == ""pytorch"":\n            if time_step is None:\n                time_step = torch.tensor([0])\n            smaller_than_start = time_step <= self.start_timestep\n            if time_step.dim() == 0:\n                time_step = time_step.unsqueeze(-1)\n            shape = time_step.shape\n            # time_step comes in as a time-sequence of time-steps.\n            # TODO tile shape is confusing -> num tiles should be shape[0] not shape?\n            if shape[0] > 0:\n                past_decay = torch.where(\n                    (time_step >= self.start_timestep + self.num_timesteps),\n                    # We are in post-decay time.\n                    pytorch_tile(torch.tensor([self.to_]), shape),\n                    # We are inside the decay time window.\n                    torch.tensor(self._graph_fn_decay(torch.FloatTensor([time_step - self.start_timestep])))\n                )\n                return torch.where(\n                    smaller_than_start,\n                    # We are still in pre-decay time.\n                    pytorch_tile(torch.tensor([self.from_]), shape),\n                    # We are past pre-decay time.\n                    past_decay\n                )\n            # Single 0D time step.\n            else:\n                if smaller_than_start:\n                    return self.from_\n                else:\n                    if time_step >= self.start_timestep + self.num_timesteps:\n                        return self.to_\n                    else:\n                        return self._graph_fn_decay(\n                            torch.FloatTensor([time_step - self.start_timestep])\n                        )\n\n    @graph_fn\n    def _graph_fn_decay(self, time_steps_in_decay_window):\n        """"""\n        The function that returns the DataOp to actually compute the decay during the decay time period.\n\n        Args:\n            time_steps_in_decay_window (DataOp): The time-step value (already cast to float) based on\n                `self.start_timestep` (not the global time-step value).\n                E.g. time_step=10.0 if global-timestep=100 and `self.start_timestep`=90.\n\n        Returns:\n            DataOp: The decay\'d value (may be based on time_steps_in_decay_window).\n        """"""\n        raise NotImplementedError\n\n\nclass ConstantDecay(DecayComponent):\n    # TODO this naming hierarchy is not ideal, we are not decaying here.\n    """"""\n    Returns a constant value.\n    """"""\n    def __init__(self, constant_value, scope=""constant-decay"", **kwargs):\n        """"""\n        Args:\n            constant_value (float): Constant value for exploration.\n        """"""\n        raise RLGraphObsoletedError(""DecayComponent"", ""ConstantDecay"", ""Constant"")\n\n        super(ConstantDecay, self).__init__(scope=scope, **kwargs)\n        self.constant_value = constant_value\n\n    @graph_fn\n    def _graph_fn_decay(self, time_steps_in_decay_window):\n        return self.constant_value\n\n\n#class PolynomialDecay(DecayComponent):\n#    """"""\n#    Component that takes a time input and outputs a linearly decaying value (using init-, and final values).\n#    The formula is:\n#    out = (t/T) * (from - to) + to\n#    where\n#    - t=time step (counting from the decay start-time, which is not necessarily 0)\n#    - T=the number of timesteps over which to decay.\n#    - from=start value\n#    - to=end value\n#    """"""\n#    def __init__(self, power=1.0, scope=""polynomial-decay"", **kwargs):\n#        """"""\n#        Args:\n#            power (float): The polynomial power to use (e.g. 1.0 for linear).\n#        """"""\n#        super(PolynomialDecay, self).__init__(scope=scope, **kwargs)\n\n#        self.power = power\n\n#    @graph_fn\n#    def _graph_fn_decay(self, time_steps_in_decay_window):\n#        if get_backend() == ""tf"":\n#            return tf.train.polynomial_decay(\n#                learning_rate=self.from_,\n#                global_step=time_steps_in_decay_window,\n#                decay_steps=self.num_timesteps,\n#                end_learning_rate=self.to_,\n#                power=self.power\n#            )\n#        elif get_backend() == ""pytorch"":\n#            decay_steps = self.num_timesteps * torch.ceil(time_steps_in_decay_window / self.num_timesteps)\n#            return (self.from_ - self.to_) \\\n#                * torch.pow((1.0 - time_steps_in_decay_window / decay_steps), self.power) + self.to_\n\n\n#class LinearDecay(PolynomialDecay):\n#    def __init__(self, scope=""linear-decay"", **kwargs):\n#        super(LinearDecay, self).__init__(power=1.0, scope=scope, **kwargs)\n\n\n#class ExponentialDecay(DecayComponent):\n#    """"""\n#    Component that takes a time input and outputs an exponentially decaying value (using a half-life parameter and\n#    init-, and final values).\n#    The formula is:\n#    out = 2exp(-t/h) * (from - to) + to\n#    where\n#    - t=time step (counting from the decay start-time, which is not necessarily 0)\n#    - h=the number of timesteps over which the decay is 50%.\n#    - from=start value\n#    - to=end value\n#    """"""\n#    def __init__(self, half_life=None, num_half_lives=10, scope=""exponential-decay"", **kwargs):\n#        """"""\n#        Args:\n#            half_life (Optional[int]): The half life period in number of timesteps. Use `num_half_lives` for a relative\n#                measure against `num_timesteps`.\n#            num_half_lives (Optional[int]): The number of sub-periods into which `num_timesteps` will be divided, each\n#                division being the length of time in which we decay 50%. This is an alternative to `half_life`.\n\n#        Keyword Args:\n#            see DecayComponent\n#        """"""\n#        assert isinstance(half_life, int) or isinstance(num_half_lives, int)\n\n#        super(ExponentialDecay, self).__init__(scope=scope, **kwargs)\n\n#        self.half_life_timesteps = half_life if half_life is not None else self.num_timesteps / num_half_lives\n\n#    @graph_fn\n#    def _graph_fn_decay(self, time_steps_in_decay_window):\n#        if get_backend() == ""tf"":\n#            return tf.train.exponential_decay(\n#                learning_rate=self.from_,\n#                global_step=time_steps_in_decay_window,\n#                decay_steps=self.half_life_timesteps,\n#                decay_rate=0.5\n#            )\n#        elif get_backend() == ""pytorch"":\n#            power = time_steps_in_decay_window / self.half_life_timesteps\n#            return self.from_ * torch.pow(0.5, power)\n'"
rlgraph/components/common/environment_stepper.py,20,"b'# Copyright 2018/2019 The RLgraph authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import OrderedDict\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.components.neural_networks.actor_component import ActorComponent\nfrom rlgraph.environments.environment import Environment\nfrom rlgraph.spaces import Space, Dict\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.ops import DataOpTuple, DataOpDict, flatten_op, unflatten_op\nfrom rlgraph.utils.specifiable_server import SpecifiableServer\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n    nest = tf.contrib.framework.nest\n\n\nclass EnvironmentStepper(Component):\n    """"""\n    A Component that takes an Environment object, a PreprocessorStack and a Policy to step\n    n times through the environment, each time picking actions depending on the states that the environment produces.\n    """"""\n\n    def __init__(self, environment_spec, actor_component_spec, num_steps=20,\n                 state_space=None, action_space=None, reward_space=None,\n                 internal_states_space=None,\n                 add_action_probs=False, action_probs_space=None,\n                 add_action=False, add_reward=False,\n                 add_previous_action_to_state=False, add_previous_reward_to_state=False,\n                 max_timesteps=None,\n                 scope=""environment-stepper"",\n                 **kwargs):\n        """"""\n        Args:\n            environment_spec (dict): A specification dict for constructing an Environment object that will be run\n                inside a SpecifiableServer for in-graph stepping.\n            actor_component_spec (Union[ActorComponent,dict]): A specification dict to construct this EnvStepper\'s\n                ActionComponent (to generate actions) or an already constructed ActionComponent object.\n            num_steps (int): The number of steps to perform per `step` call.\n            state_space (Optional[Space]): The state Space of the Environment. If None, will construct a dummy\n                environment to get the state Space from there.\n            action_space (Optional[Space]): The action Space of the Environment. If None, will construct a dummy\n                environment to get the action Space from there.\n            reward_space (Optional[Space]): The reward Space of the Environment. If None, will construct a dummy\n                environment to get the reward Space from there.\n            internal_states_space (Optional[Space]): The internal states Space (when using an RNN inside the\n                ActorComponent).\n            add_action_probs (bool): Whether to add all action probabilities for each step to the ActionComponent\'s\n                outputs at each step. These will be added as additional tensor inside the\n                Default: False.\n            action_probs_space (Optional[Space]): If add_action_probs is True, the Space that the action_probs will have.\n                This is usually just the flattened (one-hot) action space.\n            add_action (bool): Whether to add the action to the output of the `step` API-method.\n                Default: False.\n            add_reward (bool): Whether to add the reward to the output of the `step` API-method.\n                Default: False.\n            add_previous_reward_to_state (bool): Whether to add the previous reward as another input channel to the\n                ActionComponent\'s (NN\'s) input at each step. This is only possible if the state space is already a Dict.\n                It will be added under the key ""previous_reward"". Default: False.\n            add_previous_action_to_state (bool): Whether to add the previous action as another input channel to the\n                ActionComponent\'s (NN\'s) input at each step. This is only possible if the state space is already a Dict.\n                It will be added under the key ""previous_action"". Default: False.\n            add_previous_reward_to_state (bool): Whether to add the previous reward as another input channel to the\n                ActionComponent\'s (NN\'s) input at each step. This is only possible if the state space is already a Dict.\n                It will be added under the key ""previous_reward"". Default: False.\n            max_timesteps (Optional[int]): An optional max. timestep estimate to use for calculating `time_percentage`\n                values.\n        """"""\n        super(EnvironmentStepper, self).__init__(scope=scope, **kwargs)\n\n        # Create the SpecifiableServer with the given env spec.\n        if state_space is None or reward_space is None or action_space is None:\n            # Only to retrieve some information about the particular Env.\n            dummy_env = Environment.from_spec(environment_spec)  # type: Environment\n            if state_space is None:\n                state_space = dummy_env.state_space\n            if action_space is None:\n                action_space = dummy_env.action_space\n            if reward_space is None:\n                _, reward, _, _ = dummy_env.step(actions=action_space.sample())\n                # TODO: this may break on non 64-bit machines. tf seems to interpret a python float as tf.float64.\n                reward_space = Space.from_spec(\n                    ""float64"" if type(reward) == float else float, shape=(1,)\n                ).with_batch_rank()\n            dummy_env.terminate()\n\n        self.reward_space = Space.from_spec(reward_space).with_batch_rank()\n        self.action_space = Space.from_spec(action_space)\n        # The state that the environment produces.\n        self.state_space_env = Space.from_spec(state_space)\n        # The state that must be fed into the actor-component to produce an action.\n        # May contain prev_action and prev_reward.\n        self.state_space_actor = Space.from_spec(state_space)\n        self.add_previous_action_to_state = add_previous_action_to_state\n        self.add_previous_reward_to_state = add_previous_reward_to_state\n\n        # Circle actions and/or rewards in `step` API-method?\n        self.add_action = add_action\n        self.add_reward = add_reward\n\n        # The Problem with ContainerSpaces here is that py_func (SpecifiableServer) cannot handle container\n        # spaces, which is why we need to painfully convert these into flat spaces and tuples here whenever\n        # we make a call to the env. So to keep things unified, we treat all container spaces\n        # (state space, preprocessed state) from here on as tuples of primitive spaces sorted by their would be\n        # flat-keys in a flattened dict).\n        self.state_space_env_flattened = self.state_space_env.flatten()\n        # Need to flatten the state-space in case it\'s a ContainerSpace for the return dtypes.\n        self.state_space_env_list = list(self.state_space_env_flattened.values())\n\n        # TODO: automate this by lookup from the NN Component\n        self.internal_states_space = None\n        if internal_states_space is not None:\n            self.internal_states_space = internal_states_space.with_batch_rank(add_batch_rank=1)\n\n        # Add the action/reward spaces to the state space (must be Dict).\n        if self.add_previous_action_to_state is True:\n            assert isinstance(self.state_space_actor, Dict),\\\n                ""ERROR: If `add_previous_action_to_state` is True as input, state_space must be a Dict!""\n            self.state_space_actor[""previous_action""] = self.action_space\n        if self.add_previous_reward_to_state is True:\n            assert isinstance(self.state_space_actor, Dict),\\\n                ""ERROR: If `add_previous_reward_to_state` is True as input, state_space must be a Dict!""\n            self.state_space_actor[""previous_reward""] = self.reward_space\n        self.state_space_actor_flattened = self.state_space_actor.flatten()\n        self.state_space_actor_list = list(self.state_space_actor_flattened.values())\n\n        self.add_action_probs = add_action_probs\n        # TODO: Auto-infer of self.action_probs_space from action_space.\n        self.action_probs_space = action_probs_space\n        if self.add_action_probs is True:\n            assert isinstance(self.action_probs_space, Space),\\\n                ""ERROR: If `add_action_probs` is True, must provide an `action_probs_space`!""\n\n        self.environment_spec = environment_spec\n        self.environment_server = SpecifiableServer(\n            specifiable_class=Environment,\n            spec=environment_spec,\n            output_spaces=dict(\n                step_flow=self.state_space_env_list + [self.reward_space, bool],\n                reset_flow=self.state_space_env_list\n            ),\n            shutdown_method=""terminate""\n        )\n        # Add the sub-components.\n        self.actor_component = ActorComponent.from_spec(actor_component_spec)  # type: ActorComponent\n        self.preprocessed_state_space = self.actor_component.preprocessor.get_preprocessed_space(self.state_space_actor)\n\n        self.num_steps = num_steps\n\n        # Variables that hold information of last step through Env.\n        self.current_state = None\n        self.current_internal_states = None\n        self.time_step = None\n\n        self.max_timesteps = max_timesteps\n\n        self.has_rnn = self.actor_component.policy.neural_network.has_rnn()\n\n        # Add all sub-components (only ActorComponent).\n        self.add_components(self.actor_component)\n\n    def create_variables(self, input_spaces, action_space=None):\n        self.time_step = self.get_variable(\n            name=""time-step"", dtype=""int32"", initializer=0, trainable=False, local=True, use_resource=True\n        )\n        self.current_state = self.get_variable(\n            name=""current-state"", from_space=self.state_space_actor, initializer=0, flatten=True, trainable=False,\n            local=True, use_resource=True\n        )\n        if self.has_rnn:\n            self.current_internal_states = self.get_variable(\n                name=""current-internal-states"", from_space=self.internal_states_space,\n                initializer=0.0, flatten=True, trainable=False, local=True, use_resource=True,\n                add_batch_rank=1\n            )\n\n    @rlgraph_api(returns=1)\n    def _graph_fn_step(self):\n        if get_backend() == ""tf"":\n            def scan_func(accum, time_delta):\n                # Not needed: preprocessed-previous-states (tuple!)\n                # `state` is a tuple as well. See comment in ctor for why tf cannot use ContainerSpaces here.\n                internal_states = None\n                state = accum[1]\n                if self.has_rnn:\n                    internal_states = accum[-1]\n\n                state = tuple(tf.convert_to_tensor(value=s) for s in state)\n\n                flat_state = OrderedDict()\n                for i, flat_key in enumerate(self.state_space_actor_flattened.keys()):\n                    # Add a simple (size 1) batch rank to the state so it\'ll pass through the NN.\n                    # - Also have to add a time-rank for RNN processing.\n                    expanded = state[i]\n                    for _ in range(1 if self.has_rnn is False else 2):\n                        expanded = tf.expand_dims(input=expanded, axis=0)\n                    # Make None so it\'ll be recognized as batch-rank by the auto-Space detector.\n                    flat_state[flat_key] = tf.placeholder_with_default(\n                        input=expanded, shape=(None,) + ((None,) if self.has_rnn is True else ()) +\n                                              self.state_space_actor_list[i].shape\n                    )\n\n                # Recreate state as the original Space to pass it into the actor-component.\n                state = unflatten_op(flat_state)\n\n                # Get action and preprocessed state (as batch-size 1).\n                out = (self.actor_component.get_preprocessed_state_and_action if self.add_action_probs is False else\n                       self.actor_component.get_preprocessed_state_action_and_action_probs)(\n                    state,\n                    # Add simple batch rank to internal_states.\n                    None if internal_states is None else DataOpTuple(internal_states),  # <- None for non-RNN systems\n                    time_precentage=(((self.time_step + time_delta) / self.max_timesteps) if\n                                     self.max_timesteps is not None else None)\n                )\n\n                # Get output depending on whether it contains internal_states or not.\n                a = out[""action""]\n                action_probs = out.get(""action_probs"")\n                current_internal_states = out.get(""last_internal_states"")\n\n                # Strip the batch (and maybe time) ranks again from the action in case the Env doesn\'t like it.\n                a_no_extra_ranks = a[0, 0] if self.has_rnn is True else a[0]\n                # Step through the Env and collect next state (tuple!), reward and terminal as single values\n                # (not batched).\n                out = self.environment_server.step_flow(a_no_extra_ranks)\n                s_, r, t_ = out[:-2], out[-2], out[-1]\n                r = tf.cast(r, dtype=""float32"")\n\n                # Add a and/or r to next_state?\n                if self.add_previous_action_to_state is True:\n                    assert isinstance(s_, tuple), ""ERROR: Cannot add previous action to non tuple!""\n                    s_ = s_ + (a_no_extra_ranks,)\n                if self.add_previous_reward_to_state is True:\n                    assert isinstance(s_, tuple), ""ERROR: Cannot add previous reward to non tuple!""\n                    s_ = s_ + (r,)\n\n                # Note: s_ is packed as tuple.\n                ret = [t_, s_] + \\\n                    ([a_no_extra_ranks] if self.add_action else []) + \\\n                    ([r] if self.add_reward else []) + \\\n                    ([(action_probs[0][0] if self.has_rnn is True else action_probs[0])] if\n                     self.add_action_probs is True else []) + \\\n                    ([tuple(current_internal_states)] if self.has_rnn is True else [])\n\n                return tuple(ret)\n\n            # Initialize the tf.scan run.\n            initializer = [\n                # terminals\n                tf.zeros(shape=(), dtype=tf.bool),\n                # current (raw) state (flattened components if ContainerSpace).\n                tuple(map(lambda x: x.read_value(), self.current_state.values()))\n            ]\n            # Append actions and rewards if needed.\n            if self.add_action:\n                initializer.append(tf.zeros(shape=self.action_space.shape, dtype=self.action_space.dtype))\n            if self.add_reward:\n                initializer.append(tf.zeros(shape=self.reward_space.shape))\n            # Append action probs if needed.\n            if self.add_action_probs is True:\n                initializer.append(tf.zeros(shape=self.action_probs_space.shape))\n            # Append internal states if needed.\n            if self.current_internal_states is not None:\n                initializer.append(tuple(\n                    tf.placeholder_with_default(\n                        internal_s.read_value(), shape=(None,) + tuple(internal_s.shape.as_list()[1:])\n                    ) for internal_s in self.current_internal_states.values()\n                ))\n\n            # Scan over n time-steps (tf.range produces the time_delta with respect to the current time_step).\n            # NOTE: Changed parallel to 1, to resolve parallel issues.\n            step_results = list(tf.scan(\n                fn=scan_func, elems=tf.range(self.num_steps, dtype=""int32""), initializer=tuple(initializer),\n                back_prop=False\n            ))\n\n            # Assign all values that need to be passed again into the next scan.\n            assigns = [tf.assign_add(self.time_step, self.num_steps)]  # time step\n            # State (or flattened state components).\n            for flat_key, var_ref, state_comp in zip(\n                    self.state_space_actor_flattened.keys(), self.current_state.values(), step_results[1]\n            ):\n                assigns.append(self.assign_variable(var_ref, state_comp[-1]))  # -1: current state (last observed)\n\n            # Current internal state.\n            if self.current_internal_states is not None:\n                # TODO: What if internal states is not the last item in the list anymore due to some change.\n                slot = -1\n                # TODO: What if internal states is a dict? Right now assume some tuple.\n                # Remove batch rank from internal states again.\n                internal_states_wo_batch = list()\n                for i, var_ref in enumerate(self.current_internal_states.values()):  #range(len(step_results[slot])):\n                    # 1=batch axis (which has dim=1); 0=time axis.\n                    internal_states_component = tf.squeeze(step_results[slot][i], axis=1)\n                    assigns.append(self.assign_variable(var_ref, internal_states_component[-1:]))\n                    internal_states_wo_batch.append(internal_states_component)\n                step_results[slot] = tuple(internal_states_wo_batch)\n\n            # Concatenate first and rest (and make the concatenated tensors (which are the important return information)\n            # dependent on the assigns).\n            with tf.control_dependencies(control_inputs=assigns):\n                full_results = []\n                for slot in range(len(step_results)):\n                    first_values, rest_values = initializer[slot], step_results[slot]\n                    # Internal states need a slightly different concatenating as the batch rank is missing.\n                    if self.current_internal_states is not None and slot == len(step_results) - 1:\n                        full_results.append(nest.map_structure(self._concat, first_values, rest_values))\n                    # States need concatenating (first state needed).\n                    elif slot == 1:\n                        full_results.append(nest.map_structure(\n                            lambda first, rest: tf.concat([[first], rest], axis=0), first_values, rest_values)\n                        )\n                    # Everything else does not need concatenating (saves one op).\n                    else:\n                        full_results.append(step_results[slot])\n\n            # Re-build DataOpDicts of states (from tuple right now).\n            rebuild_s = DataOpDict()\n            for flat_key, var_ref, s_comp in zip(\n                    self.state_space_actor_flattened.keys(), self.current_state.values(), full_results[1]\n            ):\n                rebuild_s[flat_key] = s_comp\n            rebuild_s = unflatten_op(rebuild_s)\n            full_results[1] = rebuild_s\n\n            # Let the auto-infer system know, what time rank we have.\n            full_results = DataOpTuple(full_results)\n            for o in flatten_op(full_results).values():\n                o._time_rank = 0  # which position in the shape is the time-rank?\n\n            return full_results\n\n    @staticmethod\n    def _concat(first, rest):\n        """"""\n        Helper method to concat initial value and scanned collected results.\n        """"""\n        shape = first.shape.as_list()\n        first.set_shape(shape=(1,) + tuple(shape[1:]))\n        return tf.concat([first, rest], axis=0)\n\n    #@rlgraph_api\n    def step_with_dict_return(self):\n        """"""\n        Simple wrapper to get a dict returned instead of a tuple of values.\n\n        Returns:\n            Dict:\n                - `terminals`: The is-terminal signals.\n                - `states`: The states.\n                - `actions` (optional): The actions actually taken.\n                - `rewards` (optional): The rewards actually received.\n                - `action_probs` (optional): The action probabilities.\n                - `internal_states` (optional): The internal-states (only for RNN type NNs in the ActorComponent).\n        """"""\n        """"""\n        out = self.step()\n        ret = dict(\n            terminals=out[0],\n            states=out[1]\n        )\n        if self.has_rnn:\n            ret[""internal_states""] = out[-1]\n\n        plus = 0\n        if self.add_action:\n            ret[""actions""] = out[2]\n            plus += 1\n        if self.add_reward:\n            ret[""rewards""] = out[2 + plus]\n            plus += 1\n        if self.add_action_probs:\n            ret[""action_probs""] = out[2 + plus]\n            plus += 1\n\n        return ret\n        """"""\n        pass'"
rlgraph/components/common/iterative_optimization.py,14,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import graph_fn\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass IterativeOptimization(Component):\n    """"""\n    Sub-sampling optimizer loop.\n    """"""\n    def __init__(self, num_iterations, sample_size, optimizer, policy, loss, value_function,\n                 vf_optimizer=None, scope=""opt-loop"", **kwargs):\n        """"""\n        Args:\n            num_iterations (int): How often to call the optimizer step function.\n            sample_size (int):\n            optimizer (Optimizer): Optimizer\n        """"""\n        assert num_iterations > 0\n        super(IterativeOptimization, self).__init__(scope=scope, **kwargs)\n\n        self.num_iterations = num_iterations\n        self.sample_size = sample_size\n\n        self.optimizer = optimizer\n        self.policy = policy\n        self.loss = loss\n        self.value_function = value_function\n        self.vf_optimizer = vf_optimizer\n        if vf_optimizer:\n            self.add_components(self.vf_optimizer)\n        self.add_components(optimizer)\n\n    @graph_fn\n    def _graph_fn_iterative_opt(self,  preprocessed_states, actions, rewards, terminals, time_percentage=None):\n        """"""\n        Calls iterative optimization by repeatedly subsampling.\n        Returns:\n            any: Result of the call.\n        """"""\n\n        if get_backend() == ""tf"":\n            # Compute loss once to initialize loop.\n            batch_size = tf.shape(preprocessed_states)[0]\n            sample_indices = tf.random_uniform(shape=(self.sample_size,), maxval=batch_size, dtype=tf.int32)\n            sample_states = tf.gather(params=preprocessed_states, indices=sample_indices)\n            sample_actions = tf.gather(params=actions, indices=sample_indices)\n            sample_rewards = tf.gather(params=rewards, indices=sample_indices)\n            sample_terminals = tf.gather(params=terminals, indices=sample_indices)\n\n            action_log_probs = self.policy.get_log_likelihood(sample_states, sample_actions)[""log_likelihood""]\n            baseline_values = self.value_function.value_output(sample_states)\n\n            loss, loss_per_item, vf_loss, vf_loss_per_item = self.loss.loss(\n                action_log_probs, baseline_values, actions, sample_rewards, sample_terminals\n            )\n\n            # Args are passed in again because some device strategies may want to split them to different devices.\n            policy_vars = self.policy.variables()\n            vf_vars = self.value_function.variables()\n            vf_step_op = self.vf_optimizer.step(vf_vars, vf_loss, vf_loss_per_item, time_percentage)\n            step_op = self.optimizer.step(policy_vars, loss, loss_per_item, time_percentage)\n\n            def opt_body(index, step_op, loss, loss_per_item, vf_step_op, vf_loss, vf_loss_per_item):\n                with tf.control_dependencies([step_op, loss, loss_per_item, vf_step_op, vf_loss, vf_loss_per_item]):\n                    batch_size = tf.shape(preprocessed_states)[0]\n                    sample_indices = tf.random_uniform(shape=(self.sample_size,), maxval=batch_size, dtype=tf.int32)\n                    sample_states = tf.gather(params=preprocessed_states, indices=sample_indices)\n                    sample_actions = tf.gather(params=actions, indices=sample_indices)\n                    sample_rewards = tf.gather(params=rewards, indices=sample_indices)\n                    sample_terminals = tf.gather(params=terminals, indices=sample_indices)\n\n                    action_log_probs = self.policy.get_log_likelihood(sample_states, sample_actions)[""log_likelihood""]\n                    baseline_values = self.value_function.value_output(sample_states)\n\n                    loss, loss_per_item, vf_loss, vf_loss_per_item = self.loss.loss(\n                        action_log_probs, baseline_values, actions, sample_rewards, sample_terminals\n                    )\n\n                    # Args are passed in again because some device strategies may want to split them to different devices.\n                    policy_vars = self.policy.variables()\n                    vf_vars = self.value_function.variables()\n\n                    vf_step_op = self.vf_optimizer.step(vf_vars, vf_loss, vf_loss_per_item, time_percentage)\n                    step_op = self.optimizer.step(policy_vars, loss, loss_per_item, time_percentage)\n                    return index, step_op, loss, loss_per_item, vf_step_op, vf_loss, vf_loss_per_item\n\n            def cond(index, step_op, loss, loss_per_item, vf_step_op, v_loss, v_loss_per_item):\n                return index < self.num_iterations\n\n            index, step_op, loss, loss_per_item, vf_step_op, vf_loss, vf_loss_per_item = tf.while_loop(\n                cond=cond,\n                body=opt_body,\n                # Start with 1.\n                loop_vars=[1, step_op, loss, loss_per_item, vf_step_op, vf_loss, vf_loss_per_item],\n                parallel_iterations=1\n            )\n\n            return step_op, loss, loss_per_item, vf_step_op, vf_loss, vf_loss_per_item\n'"
rlgraph/components/common/multi_gpu_synchronizer.py,14,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.common.batch_splitter import BatchSplitter\nfrom rlgraph.components.component import Component\nfrom rlgraph.spaces import Dict\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.ops import DataOpTuple, DataOpDict\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass MultiGpuSynchronizer(Component):\n    """"""\n    The Multi-GPU optimizer parallelizes synchronous optimization across multiple GPUs.\n    Serves as a replacement pipeline for an Agent\'s `update_from_external_batch` method, which\n    needs to be rerouted through this Component\'s `calculate_update_from_external_batch` method.\n    """"""\n    def __init__(self, batch_size, scope=""multi-gpu-synchronizer"", **kwargs):\n        """"""\n        Args:\n            batch_size (int): The batch size that will need to be split between the different GPUs\n                (each GPU will receive a shard of this batch).\n        """"""\n        super(MultiGpuSynchronizer, self).__init__(graph_fn_num_outputs=dict(\n            _graph_fn_calculate_update_from_external_batch=3  # TODO: <- This is currently hardcoded for DQN-type agents\n        ), scope=scope, **kwargs)\n\n        self.batch_size = batch_size\n        self.shard_size = 0\n\n        # The list of GPU-towers (copies of the original agent root-component) that are sub-Components of this one.\n        self.towers = None\n        self.batch_splitter = None\n\n        # Device names and variables.\n        self.gpu_devices = None\n        self.num_gpus = 0\n\n        self.tower_placeholders = list()\n        self.device_input_space = None\n\n    def setup_towers(self, towers, devices):\n        """"""\n        Provides the optimizer with sub-graphs, batch splitting, name of the loss to split over,\n        and devices to split over.\n\n        Args:\n            towers (list): List of GPU-towers (copies of the original root-component).\n            devices (list): List of device names.\n        """"""\n        self.gpu_devices = devices\n        self.num_gpus = len(devices)\n        assert self.num_gpus > 1,\\\n            ""ERROR: The MultiGPUSyncOptimizer requires as least two GPUs but only {} device ids were passed "" \\\n            ""in."".format(self.num_gpus)\n        self.shard_size = int(self.batch_size / self.num_gpus)\n\n        # Add our GPU-towers (copies of the original agent root-component).\n        self.towers = towers\n        self.add_components(*self.towers)\n\n        # Splits input shards of `update_from_external_batch`.\n        self.batch_splitter = BatchSplitter(self.num_gpus, self.shard_size)\n        self.add_components(self.batch_splitter)\n\n    # TODO: Solve this problem via staging areas (one per GPU).\n    # TODO: Stuff coming from the batch-splitter can then be staged/unstaged (previous batch shard).\n    def create_variables(self, input_spaces, action_space=None):\n        # Get input space to load device fun.\n        device_input_space = {}\n        idx = 0\n        while True:\n            key = ""inputs[{}]"".format(idx)\n            if key not in input_spaces:\n                break\n            device_input_space[str(idx)] = input_spaces[key]\n            idx += 1\n        # Turn into container space for easy variable creation.\n        self.device_input_space = Dict(device_input_space)\n\n        # Create input variables for devices.\n        for i, device in enumerate(self.gpu_devices):\n            with tf.device(device):\n                device_variable = self.get_variable(\n                    name=""gpu-placeholder-{}"".format(i),\n                    trainable=False,\n                    from_space=self.device_input_space,\n                    flatten=True,\n                    add_batch_rank=self.shard_size,\n                    initializer=0\n                )\n                self.tower_placeholders.append(tuple(device_variable.values()))\n\n    # TODO: This is DQN-specific and should not be here.\n    @rlgraph_api\n    def sync_target_qnets(self):\n        tower_ops = list()\n        for i in range(self.num_gpus):\n            op = self.towers[i].sync_target_qnet()\n            tower_ops.append(op)\n        group_op = self._graph_fn_group(*tower_ops)\n        return group_op\n\n    @rlgraph_api\n    def calculate_update_from_external_batch(self, variables, *inputs, apply_postprocessing=True, time_percentage=None):\n        out = self._graph_fn_calculate_update_from_external_batch(\n            variables, *inputs, apply_postprocessing=apply_postprocessing, time_percentage=time_percentage\n        )\n        ret = dict(avg_grads_and_vars_by_component=out[0], loss=out[1], loss_per_item=out[2])\n        for i in range(3, len(out)):\n            ret[""additional_return_{}"".format(i - 3)] = out[i]\n        return ret\n\n    @graph_fn\n    def _graph_fn_calculate_update_from_external_batch(self, variables_by_component, *inputs, apply_postprocessing,\n                                                       time_percentage):\n        """"""\n        Args:\n            variables_by_component (DataOpDict): Dict with each key representing one syncable Component (e.g. Policy) and values\n                being dicts of named variables.\n            *inputs (DataOp): Any sequence of DataOps to be passed into each towers\' `update_from_external_batch`\n                API-method.\n\n        Returns:\n            tuple:\n                - DataOpDict: Corresponding to `variables`, a dict of averaged grads_and_vars to be applied to the main\n                    root Component\'s policies/NNs.\n                - DataOp: Total loss averaged over all GPUs.\n                - DataOp: Loss per item for the original, unsplit batch.\n                - depends on algo: TODO: This is DQN specific atm and needs to be rethought.\n        """"""\n        # Split the incoming batch into its per-GPU shards.\n        input_batches = self.batch_splitter.split_batch(*inputs)\n\n        # Load shards to the different devices.\n        per_device_assign_ops, loaded_input_batches = self._load_to_device(*input_batches)\n\n        all_grads_and_vars_by_component = dict()\n        for component_key in variables_by_component.keys():\n            all_grads_and_vars_by_component[component_key] = []\n        all_loss = []\n        all_loss_per_item = []\n        all_rest = None\n\n        assert len(loaded_input_batches) == self.num_gpus\n        for gpu, shard_data in enumerate(loaded_input_batches):\n            with tf.control_dependencies([per_device_assign_ops[gpu]]):\n                shard_data_stopped = tuple([tf.stop_gradient(datum.read_value()) for datum in shard_data])\n                return_values_to_be_averaged = self.towers[gpu].update_from_external_batch(\n                    *shard_data_stopped, apply_postprocessing=apply_postprocessing, time_percentage=time_percentage\n                )\n\n                grads_and_vars_by_component = return_values_to_be_averaged[0]\n                loss = return_values_to_be_averaged[1]\n                loss_per_item = return_values_to_be_averaged[2]\n                rest = return_values_to_be_averaged[3:]\n                if all_rest is None:\n                    all_rest = [list() for _ in rest]\n\n                for component_key, value in grads_and_vars_by_component.items():\n                    all_grads_and_vars_by_component[component_key].append(value)\n                all_loss.append(loss)\n                all_loss_per_item.append(loss_per_item)\n                for i, r in enumerate(rest):\n                    all_rest[i].append(r)\n\n        ret = []\n        ret.append(self._average_grads_and_vars(variables_by_component, all_grads_and_vars_by_component))\n\n        # Simple average over all GPUs.\n        ret.append(tf.reduce_mean(tf.stack(all_loss, axis=0)))\n        # concatenate the loss_per_item to regenerate original (un-split) batch\n        ret.append(tf.concat(all_loss_per_item, axis=0))\n        # For the remaining return items, do like for loss-per-item (regenerate values for original, unsplit batch).\n        for rest_list in all_rest:\n            ret.append(tf.stack(rest_list, axis=0))\n\n        # Return averaged gradients.\n        return tuple(ret)\n\n    @graph_fn\n    def _graph_fn_group(self, *tower_ops):\n        if get_backend() == ""tf"":\n            return tf.group(*tower_ops)\n\n    @rlgraph_api(must_be_complete=False)\n    def _graph_fn_sync_variables_to_towers(self, optimizer_step_op, variables):\n        # Wait for the optimizer update, then sync all variables from the main (root) policy to each tower.\n        with tf.control_dependencies([optimizer_step_op]):\n            sync_ops = []\n            for i, tower in enumerate(self.towers):\n                # Sync weights to shards\n                sync_op = self.towers[i].set_weights(\n                    variables[""policy""], value_function_weights=variables.get(""vf"")\n                )\n                sync_ops.append(sync_op)\n\n            return tf.group(*sync_ops)\n\n    def _load_to_device(self, *device_inputs):\n        """"""\n        Loads inputs to device memories by splitting data across configured devices.\n\n        Args:\n            *device_inputs (Tuple[DataOpTuple]): One or more DataOpTuples, each one representing the data for a single\n                GPU device.\n\n        Returns:\n            Tuple[Tuple[DataOpTuple]]:\n                - Tuple of assign-ops: One for each GPU.\n                - Tuple: The device allocated variables (""GPU placeholder"" vars).\n        """"""\n        if get_backend() == ""tf"":\n            # Assign shard values to device.\n            per_device_assign_ops = []\n            for gpu, shard in enumerate(device_inputs):\n                assign_ops = []\n                for i, var in enumerate(self.tower_placeholders[gpu]):\n                    assign_op = tf.assign(var, shard[i])\n                    assign_ops.append(assign_op)\n                per_device_assign_ops.append(tf.group(*assign_ops, name=""load-placeholders-gpu{}"".format(gpu)))\n\n            return tuple(per_device_assign_ops), tuple(self.tower_placeholders)\n\n    def _average_grads_and_vars(self, variables_by_component, grads_and_vars_all_gpus_by_component):\n        """"""\n        Utility to average gradients (per var) across towers.\n\n        Args:\n            variables_by_component (DataOpDict[Dict[str,DataOp]]): Dict of Dict of variables.\n            grads_and_vars_all_gpus_by_component (DataOpDict[??]]): Dict of grads_and_vars lists.\n\n        Returns:\n            DataOpDict[str,list]: DataOpDict with keys=component keys, values=list of grads_and_vars tuples averaged\n                across our GPUs.\n        """"""\n        if get_backend() == ""tf"":\n            ret = dict()\n            for component_key in variables_by_component.keys():\n                gpu_grad_averages = []\n                for i, grads_and_vars in enumerate(zip(*grads_and_vars_all_gpus_by_component[component_key])):\n                    gpu_grads = []\n\n                    for grad, var in grads_and_vars:\n                        if grad is not None:\n                            # Add batch dimension.\n                            batch_grad = tf.expand_dims(input=grad, axis=0)\n\n                            # Add along axis for that gpu.\n                            gpu_grads.append(batch_grad)\n\n                    if not gpu_grads:\n                        continue\n\n                    aggregate_grads = tf.concat(axis=0, values=gpu_grads)\n                    mean_grad = tf.reduce_mean(input_tensor=aggregate_grads, axis=0)\n                    # Need the actual main policy vars, as these are the ones that should be updated.\n                    # TODO: This is a hack and needs to be changed, but it works for now to look up main policy variables.\n                    main_variable_key = re.sub(r\'{}/tower-0/\'.format(self.global_scope), """", grads_and_vars[0][1].op.name)\n                    main_variable_key = re.sub(r\'/\', ""-"", main_variable_key)\n                    var = variables_by_component[component_key][main_variable_key]\n                    gpu_grad_averages.append((mean_grad, var))\n\n                ret[component_key] = DataOpTuple(gpu_grad_averages)\n\n            return DataOpDict(ret)\n'"
rlgraph/components/common/noise_components.py,4,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.util import convert_dtype\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass NoiseComponent(Component):\n    """"""\n    A base class Component that takes an action input and outputs some noise value.\n\n    API:\n    ins:\n        action (float): The action value input.\n    outs:\n        noise (float): The noise value to be added to the action.\n    """"""\n    def __init__(self, scope=""noise"", **kwargs):\n        super(NoiseComponent, self).__init__(scope=scope, **kwargs)\n\n    @rlgraph_api\n    def _graph_fn_get_noise(self):\n        """"""\n        The function that returns the DataOp to actually compute the noise.\n\n        Returns:\n            DataOp: The noise value.\n        """"""\n        raise NotImplementedError\n\n\nclass ConstantNoise(NoiseComponent):\n    """"""\n    Simple constant noise component.\n    """"""\n    def __init__(self, value=0.0, scope=""constant_noise"", **kwargs):\n        super(ConstantNoise, self).__init__(scope=scope, **kwargs)\n\n        self.value = value\n\n    @rlgraph_api\n    def _graph_fn_get_noise(self):\n        if get_backend() == ""tf"":\n            return tf.constant(self.value)\n\n\nclass GaussianNoise(NoiseComponent):\n    """"""\n    Simple Gaussian noise component.\n    """"""\n    def __init__(self, mean=0.0, stddev=1.0, scope=""gaussian_noise"", **kwargs):\n        super(GaussianNoise, self).__init__(scope=scope, **kwargs)\n\n        self.mean = mean\n        self.stddev = stddev\n\n        self.action_space = None\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        assert action_space is not None\n        self.action_space = action_space\n\n    @rlgraph_api\n    def _graph_fn_get_noise(self):\n        if get_backend() == ""tf"":\n            return tf.random_normal(\n                shape=(1,) + self.action_space.shape,\n                mean=self.mean,\n                stddev=self.stddev,\n                dtype=convert_dtype(self.action_space.dtype)\n            )\n\n\nclass OrnsteinUhlenbeckNoise(NoiseComponent):\n    """"""\n    Ornstein-Uhlenbeck noise component emitting a mean-reverting time-correlated stochastic noise.\n    """"""\n    def __init__(self, sigma=0.3, mu=0.0, theta=0.15, scope=""ornstein-uhlenbeck-noise"", **kwargs):\n        """"""\n        Args:\n            sigma (float): FixMe: missing documentation.\n            mu (float): The mean reversion level.\n            theta (float): The mean reversion rate.\n        """"""\n        super(OrnsteinUhlenbeckNoise, self).__init__(scope=scope, **kwargs)\n\n        self.sigma = sigma\n        self.mu = mu\n        self.theta = theta\n\n        self.ou_state = None\n        self.action_space = None\n\n    def create_variables(self, input_spaces, action_space=None):\n        assert action_space is not None\n        self.action_space = action_space\n\n        self.ou_state = self.get_variable(\n            name=""ou_state"",\n            from_space=self.action_space,\n            add_batch_rank=False,\n            initializer=self.mu\n        )\n\n    @rlgraph_api\n    def _graph_fn_get_noise(self):\n        drift = self.theta * (self.mu - self.ou_state)\n        if get_backend() == ""tf"":\n            diffusion = self.sigma * tf.random_normal(\n                shape=self.action_space.shape, dtype=convert_dtype(self.action_space.dtype)\n            )\n            delta = drift + diffusion\n            return tf.assign_add(ref=self.ou_state, value=delta)\n'"
rlgraph/components/common/repeater_stack.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.neural_networks.stack import Stack\n\n\nclass RepeaterStack(Stack):\n    """"""\n    A repeater is a special Stack that copies one(!) sub-Component n times and calls certain API-method(s) n times.\n    n is the number of repeats.\n\n    API:\n        call(input_) -> call\'s some API-method on the ""repeat-unit"" (another Component) n times, each time passing the\n            result of the previous repeat and then returning the result of the last repeat.\n    """"""\n    def __init__(self, sub_component, repeats=2, scope=""repeater"", **kwargs):\n        """"""\n        Args:\n            sub_component (Component): The single sub-Component to repeat (and deepcopy) n times.\n            repeats (int): The number of times that the `sub_component`\'s API-method(s) should be called.\n        """"""\n        self.repeats = repeats\n        # Deep copy the sub-Component n times (including riginal sub-Component).\n        sub_components = [sub_component] + [\n            sub_component.copy(scope=sub_component.scope+""-rep""+str(i+1)) for i in range(self.repeats - 1)\n        ]\n        # Call the Stack ctor to handle sub-component adding API-method creation.\n        super(RepeaterStack, self).__init__(*sub_components, scope=scope, **kwargs)\n'"
rlgraph/components/common/sampler.py,3,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.ops import FlattenedDataOp\nfrom rlgraph.utils.util import get_batch_size\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass Sampler(Component):\n    """"""\n    A Sampling component can be used to sample entries from an input op, e.g.\n    to repeatedly perform sub-sampling.\n    """"""\n    def __init__(self, sampling_strategy=""uniform"", scope=""sampler"", **kwargs):\n        """"""\n        Args:\n            # TODO potentially pass in distribution?\n            sampling_strategy (str): Sampling strategy.\n        """"""\n        super(Sampler, self).__init__(scope=scope, **kwargs)\n        self.sampling_strategy = sampling_strategy\n\n    # {1} = only flatten the 1st input arg (indexing starts from 0).\n    @rlgraph_api(flatten_ops={1})\n    def _graph_fn_sample(self, sample_size, inputs):\n        """"""\n        Takes a set of input tensors and uniformly samples a subset of the\n        specified size from them.\n\n        Args:\n            sample_size (SingleDataOp[int]): Subsample size.\n            inputs (FlattenedDataOp): Input tensors (in a FlattenedDataOp) to sample from.\n                All values (tensors) should all be the same size.\n\n        Returns:\n            FlattenedDataOp: The sub-sampled api_methods (will be unflattened automatically).\n        """"""\n        batch_size = get_batch_size(next(iter(inputs.values())))\n\n        if get_backend() == ""tf"":\n            sample_indices = tf.random_uniform(\n                shape=(sample_size,),\n                maxval=batch_size,\n                dtype=tf.int32\n            )\n            sample = FlattenedDataOp()\n            for key, tensor in inputs.items():\n                sample[key] = tf.gather(params=tensor, indices=sample_indices)\n            return sample\n'"
rlgraph/components/common/slice.py,4,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass Slice(Component):\n    """"""\n    A simple slicer layer. Slices off a piece from the input along the 0th rank returns it.\n    """"""\n    def __init__(self, squeeze=False, scope=""slice"", **kwargs):\n        """"""\n        Args:\n            squeeze (bool): Whether to squeeze a possibly size=1 slice so that its rank disappears.\n                Default: False.\n        """"""\n        super(Slice, self).__init__(scope=scope, **kwargs)\n\n        self.squeeze = squeeze\n\n    @rlgraph_api(flatten_ops=True, split_ops=True)\n    def _graph_fn_slice(self, inputs, start_index=0, end_index=None):\n        slice_ = None\n        if end_index is None:\n            # Return a single slice removing the rank.\n            if self.squeeze is True:\n                slice_ = inputs[start_index]\n            # Return a single slice but embedded in the rank now with dim=1.\n            else:\n                if self.backend == ""python"" or get_backend() == ""python"":\n                    slice_ = inputs[start_index:(start_index+1)]\n                elif get_backend() == ""tf"":\n                    # Special case: tf does not know how to do: array[-1:0] (must be array[-1:]).\n                    if isinstance(start_index, (int, np.ndarray)) and start_index == -1:\n                        slice_ = inputs[start_index:]\n                    else:\n                        slice_ = inputs[start_index:(start_index + 1)]\n        else:\n            slice_ = inputs[start_index:end_index]\n\n            if self.squeeze is True:\n                if self.backend == ""python"" or get_backend() == ""python"":\n                    if end_index is None or end_index - start_index == 1:\n                        slice_ = np.squeeze(slice_, axis=0)\n                elif get_backend() == ""tf"":\n                    if end_index is None:\n                        slice_ = tf.squeeze(slice_, axis=0)\n                    else:\n                        slice_ = tf.cond(\n                            pred=tf.equal(end_index - start_index, 1),\n                            true_fn=lambda: tf.squeeze(slice_, axis=0),\n                            false_fn=lambda: slice_\n                    )\n        return slice_\n'"
rlgraph/components/common/softmax.py,2,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.util import SMALL_NUMBER\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass Softmax(Component):\n    """"""\n    A simple softmax component that produces probabilities from logits.\n    """"""\n    def __init__(self, scope=""softmax"", **kwargs):\n        super(Softmax, self).__init__(scope=scope, **kwargs)\n\n    @rlgraph_api(flatten_ops=True, split_ops=True)\n    def _graph_fn_softmax(self, logits):\n        """"""\n        Creates properties/parameters and log-probs from some reshaped logits output.\n\n        Args:\n            logits (SingleDataOp): The output of some layer that is already reshaped.\n\n        Returns:\n            tuple:\n                probabilities (DataOp): The probabilities, ready to be passed to a Distribution object\'s\n                    get_distribution API-method (usually some probabilities or loc/scale pairs).\n\n                log_probs (DataOp): Simply the log(probabilities).\n        """"""\n        if get_backend() == ""tf"":\n            # Discrete actions.\n            probabilities = tf.maximum(x=tf.nn.softmax(logits=logits, axis=-1), y=SMALL_NUMBER)\n            # Log probs.\n            log_probs = tf.log(x=probabilities)\n            return probabilities, log_probs\n\n        elif get_backend() == ""pytorch"":\n            # Discrete actions.\n            probabilities = torch.max(torch.softmax(logits, dim=-1), torch.tensor(SMALL_NUMBER))\n            # Log probs.\n            log_probs = torch.log(probabilities)\n\n            return probabilities, log_probs\n'"
rlgraph/components/common/staging_area.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.ops import flatten_op, unflatten_op, FlattenedDataOp\nfrom rlgraph.utils.util import convert_dtype as dtype_\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass StagingArea(Component):\n    """"""\n    Stages an arbitrary number of incoming ops for next-step processing.\n    This allows for more efficient handling of dequeued (or otherwise pipelined) data: The data can\n    be prepared and then staged while a training step is still taking place, the next training step can then\n    immediately take the staged data, aso.asf..\n    """"""\n    def __init__(self, num_data=1, device=""/device:GPU:0"", scope=""staging-area"", **kwargs):\n        """"""\n        Args:\n            num_data (int): The number of data items to stage. Each item can be a ContainerDataOp (which\n                will be flattened (stage) and unflattened (unstage) automatically).\n        """"""\n        super(StagingArea, self).__init__(\n            graph_fn_num_outputs=dict(_graph_fn_unstage=num_data),\n            device=device,\n            scope=scope, **kwargs\n        )\n\n        # The actual backend-dependent StagingArea object.\n        self.area = None\n        # List of lists of flat keys of all input DataOps.\n        self.flat_keys = list()\n\n    def create_variables(self, input_spaces, action_space=None):\n        # Store the original structure for later recovery.\n        dtypes = list()\n        shapes = list()\n        idx = 0\n        while True:\n            key = ""inputs[{}]"".format(idx)\n            if key not in input_spaces:\n                break\n            flat_keys = list()\n            for flat_key, flat_space in input_spaces[key].flatten().items():\n                dtypes.append(dtype_(flat_space.dtype))\n                shapes.append(flat_space.get_shape(with_batch_rank=True, with_time_rank=True))\n                flat_keys.append(flat_key)\n            self.flat_keys.append(flat_keys)\n            idx += 1\n\n        if get_backend() == ""tf"":\n            self.area = tf.contrib.staging.StagingArea(dtypes, shapes)\n\n    @rlgraph_api\n    def _graph_fn_stage(self, *inputs):\n        """"""\n        Stages all incoming ops (after flattening them).\n\n        Args:\n            inputs (DataOp): The incoming ops to be (flattened and) staged.\n\n        Returns:\n            DataOp: The staging op.\n        """"""\n        # Flatten inputs and stage them.\n        # TODO: Build equivalent to nest.flatten ()\n        flattened_ops = list()\n        for input_ in inputs:\n            flat_list = list(flatten_op(input_).values())\n            flattened_ops.extend(flat_list)\n        stage_op = self.area.put(flattened_ops)\n        return stage_op\n\n    @rlgraph_api\n    def _graph_fn_unstage(self):\n        """"""\n        Unstages (and unflattens) all staged data.\n\n        Returns:\n            Tuple[DataOp]: All previously staged ops.\n        """"""\n        unstaged_data = self.area.get()\n        unflattened_data = list()\n        idx = 0\n        # Unflatten all data and return.\n        for flat_key_list in self.flat_keys:\n            flat_dict = FlattenedDataOp({flat_key: item for flat_key, item in zip(flat_key_list, unstaged_data[idx:idx + len(flat_key_list)])})\n            unflattened_data.append(unflatten_op(flat_dict))\n            idx += len(flat_key_list)\n\n        return tuple(unflattened_data)\n'"
rlgraph/components/common/synchronizable.py,2,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.ops import DataOpDict\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\nfrom rlgraph.utils.util import get_shape\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass Synchronizable(Component):\n    """"""\n    The Synchronizable Component adds a simple synchronization API to arbitrary Components to which this\n    Synchronizable is added (and connected via `connections=CONNECT_ALL`).\n    This is useful for constructions like a target network in DQN or for distributed setups where e.g.\n    local policies need to be sync\'d from a global model from time to time.\n    """"""\n    def __init__(self, *args, **kwargs):\n        """"""\n        Keyword Args:\n            collections (set): A set of specifiers (currently only tf), that determine which Variables\n                of the parent Component to synchronize.\n        """"""\n        self.collections = kwargs.pop(""collections"", None)\n\n        super(Synchronizable, self).__init__(*args, scope=kwargs.pop(""scope"", ""synchronizable""), **kwargs)\n\n    def check_variable_completeness(self):\n        # Overwrites this method as any Synchronizable should only be input-complete once the parent\n        # component is variable-complete (not counting this component!).\n\n        # Shortcut.\n        if self.variable_complete:\n            return True\n\n        # If this component is not used at all (no calls to API-method: `sync` are made), return True.\n        if len(self.api_methods[""sync""].in_op_columns) == 0:\n            return super(Synchronizable, self).check_variable_completeness()\n\n        # Recheck input-completeness of parent.\n        if self.parent_component.input_complete is False:\n            if self.parent_component.check_input_completeness():\n                self.parent_component.when_input_complete()\n\n        # Pretend we are input- and variable-complete (which we may not be) and then check parent\'s variable\n        # completeness under this condition.\n        parent_was_variable_complete = True\n        if self.parent_component.variable_complete is False:\n            parent_was_variable_complete = False\n            self.variable_complete = True\n            self.parent_component.check_variable_completeness()\n            self.variable_complete = False\n\n        if self.parent_component.variable_complete is True:\n            # Set back parent\'s variable completeness to where it was before, no matter what.\n            # To not interfere with variable complete checking logic of graph-builder after this\n            # Synchronizable Component becomes input-complete.\n            if parent_was_variable_complete is False:\n                self.parent_component.variable_complete = False\n            return super(Synchronizable, self).check_variable_completeness()\n\n        return False\n\n    @rlgraph_api(must_be_complete=False, returns=1, requires_variable_completeness=True)\n    def _graph_fn_sync(self, values_):\n        """"""\n        Generates the op that syncs this Synchronizable\'s parent\'s variable values from another Synchronizable\n        Component.\n\n        Args:\n            values_ (DataOpDict): The dict of variable values (coming from the ""variables""-Socket of any other\n                Component) that need to be assigned to this Component\'s parent\'s variables.\n                The keys in the dict refer to the names of our parent\'s variables and must match their names.\n\n        Returns:\n            DataOp: The op that executes the syncing.\n        """"""\n        # Loop through all incoming vars and our own and collect assign ops.\n        syncs = []\n        # Sanity checking\n        if get_backend() == ""tf"":\n            parents_vars = self.parent_component.get_variables(collections=self.collections, custom_scope_separator=""-"")\n            syncs_from, syncs_to = (sorted(values_.items()), sorted(parents_vars.items()))\n            if len(syncs_from) != len(syncs_to):\n                raise RLGraphError(""ERROR: Number of Variables to sync must match! ""\n                                   ""We have {} syncs_from and {} syncs_to."".format(len(syncs_from), len(syncs_to)))\n            for (key_from, var_from), (key_to, var_to) in zip(syncs_from, syncs_to):\n                # Sanity checking. TODO: Check the names\' ends? Without the global scope?\n                #if key_from != key_to:\n                #    raise RLGraphError(""ERROR: Variable names for syncing must match in order and name! ""\n                #                    ""Mismatch at from={} and to={}."".format(key_from, key_to))\n                    if get_shape(var_from) != get_shape(var_to):\n                        raise RLGraphError(""ERROR: Variable shapes for syncing must match! ""\n                                           ""Shape mismatch between from={} ({}) and to={} ({})."".\n                                           format(key_from, get_shape(var_from), key_to, get_shape(var_to)))\n                    syncs.append(self.assign_variable(var_to, var_from))\n\n            # Bundle everything into one ""sync""-op.\n            with tf.control_dependencies(syncs):\n                return tf.no_op(name=""sync-to-{}"".format(self.parent_component.name))\n\n        elif get_backend() == ""pytorch"":\n            # Get refs(!)\n            parents_vars = self.parent_component.get_variables(collections=self.collections,\n                                                               custom_scope_separator=""-"", get_ref=True)\n            syncs_from, sync_to_ref = (sorted(values_.items()), sorted(parents_vars.items()))\n\n            # Assign parameters of layers.\n            for (key_from, var_from), (key_to, ref_to) in zip(syncs_from, sync_to_ref):\n                ref_to.set_value(var_from)\n            return None\n'"
rlgraph/components/common/time_dependent_parameters.py,8,"b'# Copyright 2018/2019 The Rlgraph Authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.spaces import FloatBox\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.rlgraph_errors import RLGraphObsoletedError\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass TimeDependentParameter(Component):\n    """"""\n    A time-dependent or constant parameter that can be used to implement learning rate or other decaying parameters.\n    """"""\n    def __init__(self, from_=None, to_=None, max_time_steps=None, resolution=1000, scope=""parameter"", **kwargs):\n        """"""\n        Args:\n            from_ (float): The constant value or start value to use.\n            to_ (Optional[float]): The value to move towards if this parameter is time-dependent.\n            max_time_steps (Optional[int]): The maximum number of time-steps to use for percentage/decay calculations.\n                IF not provided, the time-step percentage must be passed into API-calls to `get`.\n            resolution (int): The resolution to use as ""max time steps"" value when calculating the current time step\n                from the `time_percentage` parameter. The exact formula is: current_ts=time_percentage\n\n        Keyword Args:\n            from: Instead of arg `from_`.\n            to: Instead of arg `to`.\n        """"""\n        # Kwargs alternative args (instead of the \'_\'-underscore versions).\n        kwargs_from = kwargs.pop(""from"", None)\n        kwargs_to = kwargs.pop(""to"", None)\n\n        super(TimeDependentParameter, self).__init__(scope=scope, **kwargs)\n\n        self.from_ = kwargs_from if kwargs_from is not None else from_ if from_ is not None else 1.0\n        self.to_ = kwargs_to if kwargs_to is not None else to_ if to_ is not None else 0.0\n\n        self.max_time_steps = max_time_steps\n        self.resolution = resolution\n\n        self.variable_complete = True\n\n    def check_input_completeness(self):\n        # If max_time_steps is not given, we will rely on time_percentage input, therefore, it must be given.\n        if self.max_time_steps is None and self.api_method_inputs[""time_percentage""] is ""flex"":\n            return False\n        return super(TimeDependentParameter, self).check_input_completeness()\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        time_pct_space = input_spaces[""time_percentage""]\n\n        # Time percentage is only not needed, iff backend=tf and we have a max_timesteps property with which we\n        # can derive the percentage from the tf GLOBAL_TIMESTEP variable.\n        if time_pct_space == ""flex"":\n            assert get_backend() == ""tf"", ""ERROR: `time_percentage` can only be left out if using tf as backend!""\n            assert self.max_time_steps is not None, \\\n                ""ERROR: `time_percentage` can only be left out if `self.max_time_steps` is not None!""\n        else:\n            sanity_check_space(time_pct_space, allowed_types=[FloatBox], rank=0)\n\n    @rlgraph_api\n    def get(self, time_percentage=None):\n        raise NotImplementedError\n\n    def placeholder(self):\n        """"""\n        Creates a connection to a tf placeholder (completely outside the RLgraph meta-graph).\n        Passes that placeholder through one run of our `_graph_fn_get` function and then returns the output op.\n        That way, this parameter can be used inside a tf.optimizer object as the learning rate tensor.\n\n        Returns:\n            The tf op to calculate the learning rate from the `time_percentage` placeholder.\n        """"""\n        assert get_backend() == ""tf""  # We must use tf for this to work.\n        assert self.graph_builder is not None  # We must be in the build phase.\n        # Get the placeholder (always the same!) for the `time_percentage` input.\n        placeholder = self.graph_builder.get_placeholder(""time_percentage"", float, self)\n        # Do the actual computation to get the current value for the parameter.\n        op = self.api_methods[""get""].func(self, placeholder)\n        # Return the tf op.\n        return op\n\n    @classmethod\n    def from_spec(cls, spec=None, **kwargs):\n        """"""\n        Convenience method to allow for simplified list/tuple specs (apart from full dict specs):\n        For example:\n        [from, to] <- linear decay\n        [""linear"", from, to]\n        [""polynomial"", from, to, (power; default=2.0)?]\n        [""exponential"", from, to, (decay_rate; default=0.1)?]\n        """"""\n        map_ = {\n            ""lin"": ""linear-decay"",\n            ""linear"": ""linear-decay"",\n            ""polynomial"": ""polynomial-decay"",\n            ""poly"": ""polynomial-decay"",\n            ""exp"": ""exponential-decay"",\n            ""exponential"": ""exponential-decay""\n        }\n        # Single float means constant parameter.\n        if isinstance(spec, float):\n            spec = dict(constant_value=spec, type=""constant"")\n        # List/tuple means simple (type)?/from/to setup.\n        elif isinstance(spec, (tuple, list)):\n            if len(spec) == 2:\n                spec = dict(from_=spec[0], to_=spec[1], type=""linear-decay"")\n            elif len(spec) == 3:\n                spec = dict(from_=spec[1], to_=spec[2], type=map_.get(spec[0], spec[0]))\n            elif len(spec) == 4:\n                type_ = map_.get(spec[0], spec[0])\n                spec = dict(from_=spec[1], to_=spec[2], type=type_)\n                if type_ == ""polynomial-decay"":\n                    spec[""power""] = spec[3]\n                elif type_ == ""exponential-decay"":\n                    spec[""decay_rate""] = spec[3]\n        return super(TimeDependentParameter, cls).from_spec(spec, **kwargs)\n\n\nclass Constant(TimeDependentParameter):\n    """"""\n    Always returns a constant value no matter what value `time_percentage` or GLOBAL_STEP have.\n    """"""\n    def __init__(self, constant_value, scope=""constant"", **kwargs):\n        super(Constant, self).__init__(from_=constant_value, scope=scope, **kwargs)\n\n    @rlgraph_api\n    def _graph_fn_get(self, time_percentage=None):\n        if get_backend() == ""tf"":\n            if time_percentage is not None:\n                return tf.fill(tf.shape(time_percentage), self.from_)\n            else:\n                return self.from_\n        elif get_backend() == ""pytorch"":\n            return torch.full(time_percentage.size(), self.from_)\n\n    def placeholder(self):\n        return self.from_\n\n\nclass PolynomialDecay(TimeDependentParameter):\n    """"""\n    Returns the result of:\n    to_ + (from_ - to_) * (1 - `time_percentage`) ** power\n    """"""\n    def __init__(self, power=2.0, scope=""polynomial-decay"", **kwargs):\n        """"""\n        Args:\n            power (float): The power with which to decay polynomially (see formula above).\n        """"""\n        super(PolynomialDecay, self).__init__(scope=scope, **kwargs)\n\n        self.power = power\n\n    @rlgraph_api\n    def _graph_fn_get(self, time_percentage=None):\n        if time_percentage is None:\n            assert get_backend() == ""tf""  # once more, just in case\n            return tf.train.polynomial_decay(\n                learning_rate=self.from_, global_step=tf.train.get_global_step(),\n                decay_steps=self.max_time_steps,\n                end_learning_rate=self.to_,\n                power=self.power\n            )\n        else:\n            if get_backend() == ""tf"":\n                # Get the fake current time-step from the percentage value.\n                current_timestep = self.resolution * time_percentage\n                return tf.train.polynomial_decay(\n                    learning_rate=self.from_, global_step=current_timestep,\n                    decay_steps=self.resolution,\n                    end_learning_rate=self.to_,\n                    power=self.power\n                )\n            elif get_backend() == ""pytorch"":\n                return self.to_ + (self.from_ - self.to_) * (1.0 - time_percentage) ** self.power\n\n\nclass LinearDecay(PolynomialDecay):\n    """"""\n    Same as polynomial with power=1.0. Returns the result of:\n    from_ - `time_percentage` * (from_ - to_)\n    """"""\n    def __init__(self, scope=""linear-decay"", **kwargs):\n        super(LinearDecay, self).__init__(power=1.0, scope=scope, **kwargs)\n\n\nclass ExponentialDecay(TimeDependentParameter):\n    """"""\n    Returns the result of:\n    to_ + (from_ - to_) * decay_rate ** `time_percentage`\n    """"""\n    def __init__(self, decay_rate=0.1, scope=""exponential-decay"", **kwargs):\n        """"""\n        Args:\n            decay_rate (float): The percentage of the original value after 100% of the time has been reached (see\n                formula above).\n                >0.0: The smaller the decay-rate, the stronger the decay.\n                1.0: No decay at all.\n        """"""\n        # Obsoleted decay component args:\n        if ""half_life"" in kwargs:\n            raise RLGraphObsoletedError(\n                ""ExponentialDecay"", ""half_life"",\n                ""decay_rate (according to to_ + (from_ - to_) * decay_rate ** `time_percentage`)""\n            )\n        elif ""num_half_lives"" in kwargs:\n            raise RLGraphObsoletedError(\n                ""ExponentialDecay"", ""num_half_lives"",\n                ""decay_rate (according to to_ + (from_ - to_) * decay_rate ** `time_percentage`)""\n            )\n\n        super(ExponentialDecay, self).__init__(scope=scope, **kwargs)\n\n        self.decay_rate = decay_rate\n\n    @rlgraph_api\n    def _graph_fn_get(self, time_percentage=None):\n        if time_percentage is None:\n            assert get_backend() == ""tf""  # once more, just in case\n            return tf.train.exponential_decay(\n                learning_rate=self.from_ - self.to_, global_step=tf.train.get_global_step(),\n                decay_steps=self.max_time_steps,\n                decay_rate=self.decay_rate\n            ) + self.to_\n        else:\n            if get_backend() == ""tf"":\n                # Get the fake current time-step from the percentage value.\n                current_timestep = self.resolution * time_percentage\n                return tf.train.exponential_decay(\n                    learning_rate=self.from_ - self.to_, global_step=current_timestep,\n                    decay_steps=self.resolution,\n                    decay_rate=self.decay_rate\n                ) + self.to_\n            if get_backend() == ""tf"":\n                return self.to_ + (self.from_ - self.to_) * self.decay_rate ** time_percentage\n'"
rlgraph/components/distributions/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.distributions.bernoulli import Bernoulli\nfrom rlgraph.components.distributions.beta import Beta\nfrom rlgraph.components.distributions.categorical import Categorical\nfrom rlgraph.components.distributions.distribution import Distribution\nfrom rlgraph.components.distributions.gumbel_softmax import GumbelSoftmax\nfrom rlgraph.components.distributions.joint_cumulative_distribution import JointCumulativeDistribution\nfrom rlgraph.components.distributions.mixture_distribution import MixtureDistribution\nfrom rlgraph.components.distributions.multivariate_normal import MultivariateNormal\nfrom rlgraph.components.distributions.normal import Normal\nfrom rlgraph.components.distributions.squashed_normal import SquashedNormal\n\nDistribution.__lookup_classes__ = dict(\n    bernoulli=Bernoulli,\n    bernoullidistribution=Bernoulli,\n    categorical=Categorical,\n    categoricaldistribution=Categorical,\n    gaussian=Normal,\n    gaussiandistribution=Normal,\n    gumbelsoftmax=GumbelSoftmax,\n    gumbelsoftmaxdistribution=GumbelSoftmax,\n    jointcumulative=JointCumulativeDistribution,\n    jointcumulativedistribution=JointCumulativeDistribution,\n    mixture=MixtureDistribution,\n    mixturedistribution=MixtureDistribution,\n    multivariatenormal=MultivariateNormal,\n    multivariategaussian=MultivariateNormal,\n    normal=Normal,\n    normaldistribution=Normal,\n    beta=Beta,\n    betadistribution=Beta,\n    squashed=SquashedNormal,\n    squashednormal=SquashedNormal,\n    squashednormaldistribution=SquashedNormal\n)\n\n__all__ = [""Distribution""] + list(set(map(lambda x: x.__name__, Distribution.__lookup_classes__.values())))\n\n'"
rlgraph/components/distributions/bernoulli.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.distributions.distribution import Distribution\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.util import convert_dtype\n\nif get_backend() == ""tf"":\n    import tensorflow_probability as tfp\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass Bernoulli(Distribution):\n    """"""\n    A Bernoulli distribution object defined by a single value p, the probability for True (rather than False).\n    """"""\n    def __init__(self, scope=""bernoulli"", **kwargs):\n        super(Bernoulli, self).__init__(scope=scope, **kwargs)\n\n    @rlgraph_api\n    def _graph_fn_get_distribution(self, parameters):\n        """"""\n        Args:\n            parameters (DataOp): The p value (probability that distribution returns True).\n        """"""\n        if get_backend() == ""tf"":\n            return tfp.distributions.Bernoulli(probs=parameters, dtype=convert_dtype(""bool""))\n        elif get_backend() == ""pytorch"":\n            return torch.distributions.Bernoulli(probs=parameters)\n\n    @graph_fn\n    def _graph_fn_sample_deterministic(self, distribution):\n        return distribution.prob(True) >= 0.5\n'"
rlgraph/components/distributions/beta.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.distributions.distribution import Distribution\nfrom rlgraph.spaces import Tuple, FloatBox\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\n\nif get_backend() == ""tf"":\n    import tensorflow_probability as tfp\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass Beta(Distribution):\n    """"""\n    A Beta distribution is defined on the interval [0, 1] and parameterized by shape parameters\n    alpha and beta (also called concentration parameters).\n\n    PDF(x; alpha, beta) = x**(alpha - 1) (1 - x)**(beta - 1) / Z\n        with Z = Gamma(alpha) Gamma(beta) / Gamma(alpha + beta)\n        and Gamma(n) = (n - 1)!\n\n    """"""\n    def __init__(self, scope=""beta"", low=0.0, high=1.0, **kwargs):\n        # Do not flatten incoming DataOps as we need more than one parameter in our parameterize graph_fn.\n        self.low = low\n        self.high = high\n        super(Beta, self).__init__(scope=scope, **kwargs)\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        # Must be a Tuple of len 2 (alpha and beta).\n        in_space = input_spaces[""parameters""]\n        sanity_check_space(in_space, allowed_types=[Tuple])\n        assert len(in_space) == 2, ""ERROR: Expected Tuple of len=2 as input Space to Beta!""\n        sanity_check_space(in_space[0], allowed_types=[FloatBox])\n        sanity_check_space(in_space[1], allowed_types=[FloatBox])\n\n    @rlgraph_api\n    def _graph_fn_get_distribution(self, parameters):\n        """"""\n        Args:\n            parameters (DataOpTuple): Tuple holding the alpha and beta parameters.\n        """"""\n        if get_backend() == ""tf"":\n            # Note: concentration0==beta, concentration1=alpha (!)\n            return tfp.distributions.Beta(concentration1=parameters[0], concentration0=parameters[1])\n        elif get_backend() == ""pytorch"":\n            return torch.distributions.Beta(parameters[0], parameters[1])\n\n    @graph_fn\n    def _graph_fn_squash(self, raw_values):\n        return raw_values * (self.high - self.low) + self.low\n\n    @graph_fn\n    def _graph_fn_unsquash(self, values):\n        return (values - self.low) / (self.high - self.low)\n\n    @graph_fn\n    def _graph_fn_sample_deterministic(self, distribution):\n        mean = None\n        if get_backend() == ""tf"":\n            mean = distribution.mean()\n        elif get_backend() == ""pytorch"":\n            mean = distribution.mean\n        return self._graph_fn_squash(mean)\n\n    @graph_fn\n    def _graph_fn_sample_stochastic(self, distribution):\n        raw_values = super(Beta, self)._graph_fn_sample_stochastic(distribution)\n        return self._graph_fn_squash(raw_values)\n\n    @graph_fn\n    def _graph_fn_log_prob(self, distribution, values):\n        raw_values = self._graph_fn_unsquash(values)\n        return super(Beta, self)._graph_fn_log_prob(distribution, raw_values)\n'"
rlgraph/components/distributions/categorical.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.distributions.distribution import Distribution\nfrom rlgraph.utils import util\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n    import tensorflow_probability as tfp\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass Categorical(Distribution):\n    """"""\n    A categorical distribution object defined by a n values {p0, p1, ...} that add up to 1, the probabilities\n    for picking one of the n categories.\n    """"""\n    def __init__(self, scope=""categorical"", **kwargs):\n        super(Categorical, self).__init__(scope=scope, **kwargs)\n\n    @rlgraph_api\n    def _graph_fn_get_distribution(self, parameters):\n        if get_backend() == ""tf"":\n            return tfp.distributions.Categorical(logits=parameters, dtype=util.convert_dtype(""int""))\n        elif get_backend() == ""pytorch"":\n            return torch.distributions.Categorical(logits=parameters)\n\n    @graph_fn\n    def _graph_fn_sample_deterministic(self, distribution):\n        if get_backend() == ""tf"":\n            return tf.argmax(input=distribution.probs, axis=-1, output_type=util.convert_dtype(""int""))\n        elif get_backend() == ""pytorch"":\n            return torch.argmax(distribution.probs, dim=-1).int()\n'"
rlgraph/components/distributions/distribution.py,5,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass Distribution(Component):\n    """"""\n    A distribution wrapper class that can incorporate a backend-specific distribution object that gets its parameters\n    from an external source (e.g. a NN).\n\n    API:\n        get_distribution(parameters): The backend-specific distribution object.\n        sample_stochastic(parameters): Returns a stochastic sample from the distribution.\n        sample_deterministic(parameters): Returns the max-likelihood value (deterministic) from the distribution.\n\n        draw(parameters, deterministic): Draws a sample from the distribution (if `deterministic` is True,\n            this is will be a deterministic draw, otherwise a stochastic sample).\n\n        entropy(parameters): The entropy value of the distribution.\n        log_prob(parameters): The log probabilities for given values.\n\n        kl_divergence(parameters, other_parameters): The Kullback-Leibler Divergence between a Distribution and\n            another one.\n    """"""\n    def __init__(self, scope=""distribution"", **kwargs):\n        """"""\n        Keyword Args:\n            seed (Optional[int]): An optional random seed to use when sampling stochastically.\n        """"""\n        self.seed = kwargs.pop(""seed"", None)\n        super(Distribution, self).__init__(scope=scope, **kwargs)\n\n        # TEST\n        self.variable_complete = True\n        # END: TEST\n\n        # For define-by-run to avoid creating new objects when calling `get_distribution`.\n        self.dist_object = None\n\n    @rlgraph_api\n    def get_distribution(self, parameters):\n        """"""\n        Parameterizes this distribution (normally from an NN-output vector). Returns\n        the backend-distribution object (a DataOp).\n\n        Args:\n            parameters (DataOpRec): The input(s) used to parameterize this distribution. This is normally a cleaned up\n                single NN-output (e.g.: the two values for mean and variance for a univariate Gaussian\n                distribution).\n\n        Returns:\n            The parameterized backend-specific distribution object.\n        """"""\n        raise NotImplementedError\n\n    # Now use that API-method to get the distribution object to implement all other API-methods.\n    @rlgraph_api\n    def sample_stochastic(self, parameters):\n        distribution = self.get_distribution(parameters)\n        return self._graph_fn_sample_stochastic(distribution)\n\n    @rlgraph_api\n    def sample_deterministic(self, parameters):\n        distribution = self.get_distribution(parameters)\n        return self._graph_fn_sample_deterministic(distribution)\n\n    @rlgraph_api\n    def draw(self, parameters, deterministic=True):\n        distribution = self.get_distribution(parameters)\n        return self._graph_fn_draw(distribution, deterministic)\n\n    @rlgraph_api\n    def sample_and_log_prob(self, parameters, deterministic=True):\n        distribution = self.get_distribution(parameters)\n        actions = self._graph_fn_draw(distribution, deterministic)\n        log_probs = self._graph_fn_log_prob(distribution, actions)\n        return actions, log_probs\n\n    @rlgraph_api\n    def entropy(self, parameters):\n        distribution = self.get_distribution(parameters)\n        return self._graph_fn_entropy(distribution)\n\n    @rlgraph_api(must_be_complete=False)\n    def log_prob(self, parameters, values):\n        distribution = self.get_distribution(parameters)\n        return self._graph_fn_log_prob(distribution, values)\n\n    @rlgraph_api(must_be_complete=False)\n    def kl_divergence(self, parameters, other_parameters):\n        distribution = self.get_distribution(parameters)\n        other_distribution = self.get_distribution(other_parameters)\n        return self._graph_fn_kl_divergence(distribution, other_distribution)\n\n    @graph_fn\n    def _graph_fn_draw(self, distribution, deterministic):\n        """"""\n        Takes a sample from the (already parameterized) distribution. The parameterization also includes a possible\n        batch size.\n\n        Args:\n            distribution (DataOp): The (already parameterized) backend-specific distribution DataOp to use for\n                sampling. This is simply the output of `self._graph_fn_parameterize`.\n\n            deterministic (Union[bool,DataOp]): Whether to return the maximum-likelihood result, instead of a random\n                sample. Can be used to pick deterministic actions from discrete (""greedy"") or continuous (mean-value)\n                distributions.\n\n        Returns:\n            DataOp: The taken sample(s).\n        """"""\n        # Fixed boolean input (not a DataOp/Tensor).\n        if get_backend() == ""pytorch"" or isinstance(deterministic, (bool, np.ndarray)):\n            # Don\'t do `is True` here in case `deterministic` is np.ndarray!\n            if deterministic:\n                return self._graph_fn_sample_deterministic(distribution)\n            else:\n                return self._graph_fn_sample_stochastic(distribution)\n\n        # For static graphs, `deterministic` could be a tensor.\n        if get_backend() == ""tf"":\n            return tf.cond(\n                pred=deterministic,\n                true_fn=lambda: self._graph_fn_sample_deterministic(distribution),\n                false_fn=lambda: self._graph_fn_sample_stochastic(distribution)\n            )\n\n    @graph_fn(flatten_ops=True, split_ops=True)\n    def _graph_fn_sample_deterministic(self, distribution):\n        """"""\n        Returns the maximum-likelihood value for a given distribution.\n\n        Args:\n            distribution (DataOp): The (already parameterized) backend-specific distribution whose max-likelihood value\n                to calculate. This is simply the output of `self._graph_fn_parameterize`.\n\n        Returns:\n            DataOp: The max-likelihood value.\n        """"""\n        raise NotImplementedError\n\n    @graph_fn(flatten_ops=True, split_ops=True)\n    def _graph_fn_sample_stochastic(self, distribution):\n        """"""\n        Returns an actual sample for a given distribution.\n\n        Args:\n            distribution (DataOp): The (already parameterized) backend-specific distribution from which a sample\n                should be drawn. This is simply the output of `self._graph_fn_parameterize`.\n\n        Returns:\n            DataOp: The drawn sample.\n        """"""\n        if get_backend() == ""tf"":\n            return distribution.sample(seed=self.seed)\n        elif get_backend() == ""pytorch"":\n            return distribution.sample()\n\n    @graph_fn\n    def _graph_fn_log_prob(self, distribution, values):\n        """"""\n        Probability density/mass function.\n\n        Args:\n            distribution (DataOp): The (already parameterized) backend-specific distribution for which the log\n                probabilities should be calculated. This is simply the output of `self._graph_fn_get_distribution`.\n\n            values (SingleDataOp): Values for which to compute the log probabilities given `distribution`.\n\n        Returns:\n            DataOp: The log probability of the given values.\n        """"""\n        return distribution.log_prob(value=values)\n\n    @graph_fn\n    def _graph_fn_entropy(self, distribution):\n        """"""\n        Returns the DataOp holding the entropy value of the distribution.\n\n        Args:\n            distribution (DataOp): The (already parameterized) backend-specific distribution whose entropy to\n                calculate. This is simply the output of `self._graph_fn_parameterize`.\n\n        Returns:\n            DataOp: The distribution\'s entropy.\n        """"""\n        return distribution.entropy()\n\n    @graph_fn\n    def _graph_fn_kl_divergence(self, distribution, distribution_b):\n        """"""\n        Kullback-Leibler divergence between two distribution objects.\n\n        Args:\n            distribution (tf.Distribution): The (already parameterized) backend-specific distribution 1.\n            distribution_b (tf.Distribution): The other distribution object.\n\n        Returns:\n            DataOp: (batch-wise) KL-divergence between the two distributions.\n        """"""\n        if get_backend() == ""tf"":\n            return tf.no_op()\n            # TODO: never tested. tf throws error: NotImplementedError: No KL(distribution_a || distribution_b) registered for distribution_a type Bernoulli and distribution_b type ndarray\n            #return tf.distributions.kl_divergence(\n            #    distribution_a=distribution_a,\n            #    distribution_b=distribution_b,\n            #    allow_nan_stats=True,\n            #    name=None\n            #)\n'"
rlgraph/components/distributions/gumbel_softmax.py,5,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.distributions.distribution import Distribution\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n    import tensorflow_probability as tfp\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass GumbelSoftmax(Distribution):\n    """"""\n    The Gumbel Softmax distribution is also known as a relaxed one-hot categorical or concrete distribution.\n\n    Gumbel Softmax: https://arxiv.org/abs/1611.01144\n\n    Concrete: https://arxiv.org/abs/1611.00712\n    """"""\n    def __init__(self, scope=""gumbel-softmax"", temperature=1.0, **kwargs):\n        """"""\n\n        Args:\n            temperature (float): Temperature parameter. For low temperatures, the expected value approaches\n                a categorical random variable. For high temperatures, the expected value approaches a uniform\n                distribution.\n        """"""\n        self.temperature = temperature\n        super(GumbelSoftmax, self).__init__(scope=scope, **kwargs)\n\n    @rlgraph_api\n    def _graph_fn_get_distribution(self, parameters):\n        if get_backend() == ""tf"":\n            return tfp.distributions.RelaxedOneHotCategorical(temperature=self.temperature, logits=parameters)\n        elif get_backend() == ""pytorch"":\n            return torch.distributions.RelaxedOneHotCategorical(temperature=self.temperature, logits=parameters)\n\n    @graph_fn\n    def _graph_fn_sample_deterministic(self, distribution):\n        """"""\n        Returns the argmax (int) of a relaxed one-hot vector. See `_graph_fn_sample_stochastic` for details.\n        """"""\n        if get_backend() == ""tf"":\n            # Cast to float again because this is called from a tf.cond where the other option calls a stochastic\n            # sample returning a float.\n            argmax = tf.argmax(input=distribution._distribution.probs, axis=-1, output_type=tf.int32)\n            sample = tf.cast(argmax, dtype=tf.float32)\n            # Argmax turns (?, n) into (?,), not (?, 1)\n            # TODO: What if we have a time rank as well?\n            if len(sample.shape) == 1:\n                sample = tf.expand_dims(sample, -1)\n            return sample\n        elif get_backend() == ""pytorch"":\n            # TODO: keepdims?\n            return torch.argmax(distribution.probs, dim=-1).int()\n\n    @graph_fn\n    def _graph_fn_sample_stochastic(self, distribution):\n        """"""\n        Returns a relaxed one-hot vector representing a quasi-one-hot action.\n        To get the actual int action, one would have to take the argmax over\n        these output values. However, argmax would break the differentiability and should\n        thus only be used right before applying the action in e.g. an env.\n        """"""\n        if get_backend() == ""tf"":\n            return distribution.sample(seed=self.seed)\n        elif get_backend() == ""pytorch"":\n            return distribution.sample()\n\n    @graph_fn\n    def _graph_fn_log_prob(self, distribution, values):\n        if get_backend() == ""tf"":\n            return distribution.log_prob(values)\n        elif get_backend() == ""pytorch"":\n            return distribution.log_prob(values)\n\n    @graph_fn\n    def _graph_fn_entropy(self, distribution):\n        return distribution.entropy()\n\n    @graph_fn\n    def _graph_fn_kl_divergence(self, distribution, distribution_b):\n        if get_backend() == ""tf"":\n            return tf.no_op()\n        elif get_backend() == ""pytorch"":\n            return None\n\n'"
rlgraph/components/distributions/joint_cumulative_distribution.py,4,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.distributions.distribution import Distribution\nfrom rlgraph.components.layers.preprocessing.reshape import ReShape\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.ops import flatten_op, FlattenedDataOp\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass JointCumulativeDistribution(Distribution):\n    """"""\n    A joint cumulative distribution consisting of an arbitrarily nested container of n sub-distributions\n    assumed to be all independent(!) of each other, such that:\n    For e.g. n=2 and random variables X and Y: P(X and Y) = P(X)*P(Y) for all x and y.\n    - Sampling returns a ContainerDataOp.\n    - log_prob returns the sum of all single log prob terms (joint log prob).\n    - entropy returns the sum of all single entropy terms (joint entropy).\n    """"""\n    def __init__(self, distribution_specs, scope=""joint-cumulative-distribution"", **kwargs):\n        """"""\n        Args:\n            distribution_specs (dict): Dict with flat-keys containing the specifications of the single\n                sub-distributions.\n        """"""\n        super(JointCumulativeDistribution, self).__init__(scope=scope, **kwargs)\n\n        # Create the flattened sub-distributions and add them.\n        self.flattened_sub_distributions = \\\n            {flat_key: Distribution.from_spec(spec, scope=""sub-distribution-{}"".format(i))\n             for i, (flat_key, spec) in enumerate(distribution_specs.items())\n            }\n        self.flattener = ReShape(flatten=True)\n\n        self.add_components(self.flattener, *list(self.flattened_sub_distributions.values()))\n\n    @rlgraph_api\n    def sample_deterministic(self, parameters):\n        return self._graph_fn_sample_deterministic(parameters)\n\n    @rlgraph_api\n    def sample_stochastic(self, parameters):\n        return self._graph_fn_sample_stochastic(parameters)\n\n    @rlgraph_api\n    def draw(self, parameters, deterministic=True):\n        return self._graph_fn_draw(parameters, deterministic)\n\n    @rlgraph_api\n    def sample_and_log_prob(self, parameters, deterministic=True):\n        #distribution = self.get_distribution(parameters)\n        actions = self._graph_fn_draw(parameters, deterministic)\n        log_probs = self._graph_fn_log_prob(parameters, actions)\n        return actions, log_probs\n\n    #@rlgraph_api\n    #def entropy(self, parameters):\n    #    return self._graph_fn_entropy(parameters)\n\n    @rlgraph_api\n    def log_prob(self, parameters, values):\n        """"""\n        Override log_prob API as we have to add all the resulting log-probs together\n        (joint log-prob of individual ones).\n        """"""\n        #distributions = self.get_distribution(parameters)\n        all_log_probs = self._graph_fn_log_prob(parameters, values)\n        return self._graph_fn_reduce_over_sub_distributions(all_log_probs)\n\n    #@rlgraph_api(must_be_complete=False)\n    #def kl_divergence(self, parameters, other_parameters):\n    #    distribution = self.get_distribution(parameters)\n    #    other_distribution = self.get_distribution(other_parameters)\n    #    return self._graph_fn_kl_divergence(distribution, other_distribution)\n\n    # Flatten only alongside `self.flattened_sub_distributions`, not any further.\n    @rlgraph_api(flatten_ops=""flattened_sub_distributions"", split_ops=True, add_auto_key_as_first_param=True, ok_to_overwrite=True)\n    def _graph_fn_get_distribution(self, key, parameters):\n        return self.flattened_sub_distributions[key].get_distribution(parameters)\n\n    @graph_fn(flatten_ops=""flattened_sub_distributions"")\n    def _graph_fn_sample_deterministic(self, parameters):\n        ret = {}\n        for key in parameters:\n            ret[key] = self.flattened_sub_distributions[key].sample_deterministic(parameters[key])\n        return FlattenedDataOp(ret)\n\n    @graph_fn(flatten_ops=""flattened_sub_distributions"")\n    def _graph_fn_sample_stochastic(self, parameters):\n        ret = {}\n        for key in parameters:\n            ret[key] = self.flattened_sub_distributions[key].sample_stochastic(parameters[key])\n        return FlattenedDataOp(ret)\n\n    @graph_fn(flatten_ops=""flattened_sub_distributions"")\n    def _graph_fn_draw(self, parameters, deterministic):\n        ret = {}\n        for key in parameters:\n            ret[key] = self.flattened_sub_distributions[key].draw(parameters[key], deterministic)\n        return FlattenedDataOp(ret)\n\n    # Flatten only alongside `self.flattened_sub_distributions`, not any further.\n    @graph_fn(flatten_ops=""flattened_sub_distributions"")\n    def _graph_fn_log_prob(self, parameters, values):\n        ret = {}\n        for key in parameters:\n            #d = self.flattened_sub_distributions[key].get_distribution(parameters[key])\n            #return self.flattened_sub_distributions[key]._graph_fn_log_prob(distribution, values)\n            ret[key] = self.flattened_sub_distributions[key].log_prob(parameters[key], values[key])\n        return FlattenedDataOp(ret)\n\n    @graph_fn(flatten_ops=True)\n    def _graph_fn_reduce_over_sub_distributions(self, log_probs):\n        params_space = next(iter(flatten_op(self.api_method_inputs[""parameters""]).values()))\n        num_ranks_to_keep = (1 if params_space.has_batch_rank else 0) + (1 if params_space.has_time_rank else 0)\n        log_probs_list = []\n        if get_backend() == ""tf"":\n            for log_prob in log_probs.values():\n                # Reduce sum over all ranks to get the joint log llh.\n                log_prob = tf.reduce_sum(log_prob, axis=list(range(len(log_prob.shape) - 1, num_ranks_to_keep - 1, -1)))\n                log_probs_list.append(log_prob)\n            return tf.reduce_sum(tf.stack(log_probs_list, axis=0), axis=0)\n\n        elif get_backend() == ""pytorch"":\n            for log_prob in log_probs.values():\n                # Reduce sum over all ranks to get the joint log llh.\n                log_prob = torch.sum(log_prob, dim=list(range(len(log_prob.shape) - 1, num_ranks_to_keep - 1, -1)))\n                log_probs_list.append(log_prob)\n\n            return torch.sum(torch.stack(log_probs_list, dim=0), dim=0)\n\n    # Flatten only alongside `self.flattened_sub_distributions`, not any further.\n    @graph_fn(flatten_ops=""flattened_sub_distributions"")\n    def _graph_fn_entropy(self, distribution):\n        params_space = next(iter(flatten_op(self.api_method_inputs[""parameters""]).values()))\n        num_ranks_to_keep = (1 if params_space.has_batch_rank else 0) + (1 if params_space.has_time_rank else 0)\n        all_entropies = []\n        if get_backend() == ""tf"":\n            for key, distr in distribution.items():\n                entropy = distr.entropy()\n                # Reduce sum over all ranks to get the joint entropy.\n                entropy = tf.reduce_sum(entropy, axis=list(range(len(entropy.shape) - 1, num_ranks_to_keep - 1, -1)))\n                all_entropies.append(entropy)\n            return tf.reduce_sum(tf.stack(all_entropies, axis=0), axis=0)\n\n        elif get_backend() == ""pytorch"":\n            for key, distr in distribution.items():\n                entropy = distr.entropy()\n                # Reduce sum over all ranks to get the joint log llh.\n                entropy = torch.sum(entropy, dim=list(range(len(entropy.shape) - 1, num_ranks_to_keep - 1, -1)))\n                all_entropies.append(entropy)\n\n            # TODO: flatten all all_log_probs (or expand in last dim) so we can concat, then reduce_sum to get the joint probs.\n            return torch.sum(torch.stack(all_entropies, dim=0), dim=0)\n'"
rlgraph/components/distributions/mixture_distribution.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.distributions.categorical import Categorical\nfrom rlgraph.components.distributions.distribution import Distribution\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\n\nif get_backend() == ""tf"":\n    import tensorflow_probability as tfp\nelif get_backend() == ""pytorch"":\n    pass\n\n\nclass MixtureDistribution(Distribution):\n    """"""\n    A mixed distribution of n sub-distribution components and a categorical which determines,\n    from which sub-distribution we sample.\n    """"""\n    def __init__(self, *sub_distributions, **kwargs):\n        """"""\n        Args:\n            sub_distributions (List[Union[string,Distribution]]): The type-strings or actual Distribution objects\n                that define the n sub-distributions of this MixtureDistribution.\n        """"""\n        super(MixtureDistribution, self).__init__(scope=kwargs.pop(""scope"", ""mixed-distribution""), **kwargs)\n\n        self.sub_distributions = []\n        for i, s in enumerate(sub_distributions):\n            if isinstance(s, str):\n                self.sub_distributions.append(Distribution.from_spec(\n                    {""type"": s, ""scope"": ""sub-distribution-{}"".format(i)}\n                ))\n            else:\n                self.sub_distributions.append(Distribution.from_spec(s))\n\n        self.categorical = Categorical(scope=""main-categorical"")\n\n        self.add_components(self.categorical, *self.sub_distributions)\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        # Must be a Dict with keys: \'categorical\', \'parameters0\', \'parameters1\', etc...\n        in_space = input_spaces[""parameters""]\n\n        assert ""categorical"" in in_space, ""ERROR: in_space for Mixed needs parameter key: \'categorical\'!""\n\n        for i, s in enumerate(self.sub_distributions):\n            sub_space = in_space.get(""parameters{}"".format(i))\n            if sub_space is None:\n                raise RLGraphError(""ERROR: in_space for Mixed needs parameter key: \'parameters{}\'!"".format(i))\n\n    @rlgraph_api\n    def _graph_fn_get_distribution(self, parameters):\n        """"""\n        Args:\n            parameters (DataOpDict): The parameters to use for parameterizations of the different sub-distributions\n                including the main-categorical one. Keys must be ""categorical"", ""parameters0"", ""parameters1"", etc..\n\n        Returns:\n            DataOp: The ready-to-be-sampled mixed distribution.\n        """"""\n        if get_backend() == ""tf"":\n            components = []\n            for i, s in enumerate(self.sub_distributions):\n                components.append(s.get_distribution(parameters[""parameters{}"".format(i)]))\n\n            return tfp.distributions.Mixture(\n                cat=self.categorical.get_distribution(parameters[""categorical""]),\n                components=components\n            )\n\n    @rlgraph_api\n    def entropy(self, parameters):\n        """"""\n        Not implemented for Mixed. Raise error for now (safer than returning wrong values).\n        """"""\n        raise NotImplementedError\n\n    @graph_fn\n    def _graph_fn_sample_deterministic(self, distribution):\n        if get_backend() == ""tf"":\n            return distribution.mean()\n        elif get_backend() == ""pytorch"":\n            return distribution.mean\n'"
rlgraph/components/distributions/multivariate_normal.py,2,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.distributions.distribution import Distribution\nfrom rlgraph.spaces import Tuple, FloatBox\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\n\nif get_backend() == ""tf"":\n    import tensorflow_probability as tfp\nelif get_backend() == ""pytorch"":\n    pass\n\n\nclass MultivariateNormal(Distribution):\n    """"""\n    A multivariate Gaussian distribution.\n    """"""\n    def __init__(self, parameterize_via_diagonal=True,\n                 parameterize_via_covariance=False, scope=""multivariate-normal"", **kwargs):\n        """"""\n        Args:\n            parameterize_via_diagonal (bool): Whether we are parameterizing via the diagonal stddev values.\n                Note that\n            parameterize_via_covariance (bool): Whether we are parameterizing via the full covariance values.\n        """"""\n        super(MultivariateNormal, self).__init__(scope=scope, **kwargs)\n        self.parameterize_via_diagonal = parameterize_via_diagonal\n        self.parameterize_via_covariance = parameterize_via_covariance\n        assert self.parameterize_via_diagonal != self.parameterize_via_covariance, \\\n            ""ERROR: Exactly one of `parameterize_via_diagonal` and `parameterize_via_covariance` must be True!""\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        # Must be a Tuple of len 2 (mean and stddev OR mean and full co-variance matrix).\n        in_space = input_spaces[""parameters""]\n        sanity_check_space(in_space, allowed_types=[Tuple])\n        assert len(in_space) == 2, ""ERROR: Expected Tuple of len=2 as input Space to MultivariateNormal!""\n        sanity_check_space(in_space[0], allowed_types=[FloatBox])\n        sanity_check_space(in_space[1], allowed_types=[FloatBox])\n\n        if self.parameterize_via_diagonal:\n            # Make sure mean and stddev have the same last rank.\n            assert in_space[0].shape[-1] == in_space[1].shape[-1],\\\n                ""ERROR: `parameters` in_space must have the same last rank for mean as for (diagonal) stddev values!""\n\n    @rlgraph_api\n    def _graph_fn_get_distribution(self, parameters):\n        if get_backend() == ""tf"":\n            if self.parameterize_via_diagonal:\n                return tfp.distributions.MultivariateNormalDiag(\n                    loc=parameters[0], scale_diag=parameters[1]\n                )\n            # TODO: support parameterization through full covariance matrix.\n            #else:\n            #    mean, covariance_matrix = tf.split(parameters, num_or_size_splits=[1, self.num_events], axis=-1)\n            #    mean = tf.squeeze(mean, axis=-1)\n            #    return tfp.distributions.MultivariateNormalFullCovariance(\n            #        loc=mean, covariance_matrix=covariance_matrix\n            #    )\n\n    @graph_fn\n    def _graph_fn_sample_deterministic(self, distribution):\n            return distribution.mean()\n'"
rlgraph/components/distributions/normal.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.distributions.distribution import Distribution\nfrom rlgraph.spaces import Tuple, FloatBox\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\n\nif get_backend() == ""tf"":\n    import tensorflow_probability as tfp\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass Normal(Distribution):\n    """"""\n    A Gaussian Normal distribution object defined by a tuple: mean, variance,\n    which is the same as ""loc_and_scale"".\n    """"""\n    def __init__(self, scope=""normal"", **kwargs):\n        # Do not flatten incoming DataOps as we need more than one parameter in our parameterize graph_fn.\n        super(Normal, self).__init__(scope=scope, **kwargs)\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        # Must be a Tuple of len 2 (loc and scale).\n        in_space = input_spaces[""parameters""]\n        sanity_check_space(in_space, allowed_types=[Tuple])\n        assert len(in_space) == 2, ""ERROR: Expected Tuple of len=2 as input Space to Normal!""\n        sanity_check_space(in_space[0], allowed_types=[FloatBox])\n        sanity_check_space(in_space[1], allowed_types=[FloatBox])\n\n    @rlgraph_api\n    def _graph_fn_get_distribution(self, parameters):\n        """"""\n        Args:\n            parameters (DataOpTuple): Tuple holding the mean and stddev parameters.\n        """"""\n        if get_backend() == ""tf"":\n            return tfp.distributions.Normal(loc=parameters[0], scale=parameters[1])\n        elif get_backend() == ""pytorch"":\n            return torch.distributions.Normal(parameters[0], parameters[1])\n\n    @graph_fn\n    def _graph_fn_sample_deterministic(self, distribution):\n        if get_backend() == ""tf"":\n            return distribution.mean()\n        elif get_backend() == ""pytorch"":\n            return distribution.mean\n'"
rlgraph/components/distributions/squashed_normal.py,11,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nfrom rlgraph import get_backend\nfrom rlgraph.components.distributions.distribution import Distribution\nfrom rlgraph.spaces import Tuple, FloatBox\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.util import SMALL_NUMBER\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n    import tensorflow_probability as tfp\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass SquashedNormal(Distribution):\n    """"""\n    A Squashed with tanh Normal distribution object defined by a tuple: mean, standard deviation.\n    """"""\n    def __init__(self, scope=""squashed-normal"", low=-1.0, high=1.0, **kwargs):\n        # Do not flatten incoming DataOps as we need more than one parameter in our parameterize graph_fn.\n        assert np.all(np.less(low, high))\n        self.low = low\n        self.high = high\n        super(SquashedNormal, self).__init__(scope=scope, **kwargs)\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        # Must be a Tuple of len 2 (loc and scale).\n        in_space = input_spaces[""parameters""]\n        sanity_check_space(in_space, allowed_types=[Tuple])\n        assert len(in_space) == 2, ""ERROR: Expected Tuple of len=2 as input Space to SquashedNormal!""\n        sanity_check_space(in_space[0], allowed_types=[FloatBox])\n        sanity_check_space(in_space[1], allowed_types=[FloatBox])\n\n    @rlgraph_api\n    def sample_and_log_prob(self, parameters, deterministic=True):\n        distribution = self.get_distribution(parameters)\n        scaled_actions, log_prob = self._graph_fn_sample_and_log_prob(distribution, deterministic)\n        return scaled_actions, log_prob\n\n    @rlgraph_api\n    def _graph_fn_get_distribution(self, parameters):\n        if get_backend() == ""tf"":\n            return tfp.distributions.Normal(loc=parameters[0], scale=parameters[1])\n        elif get_backend() == ""pytorch"":\n            return torch.distributions.Normal(parameters[0], parameters[1])\n\n    @graph_fn\n    def _graph_fn_sample_deterministic(self, distribution):\n        mean = None\n        if get_backend() == ""tf"":\n            mean = distribution.mean()\n        elif get_backend() == ""pytorch"":\n            mean = distribution.mean\n        return self._graph_fn_squash(mean)\n\n    @graph_fn\n    def _graph_fn_sample_stochastic(self, distribution):\n        if get_backend() == ""tf"":\n            return self._graph_fn_squash(distribution.sample(seed=self.seed))\n        elif get_backend() == ""pytorch"":\n            return self._graph_fn_squash(distribution.sample())\n\n    @graph_fn\n    def _graph_fn_log_prob(self, distribution, values):\n        unsquashed_values = self._graph_fn_unsquash(values)\n        log_prob = distribution.log_prob(value=unsquashed_values)\n        if get_backend() == ""tf"":\n            unsquashed_values_tanhd = tf.tanh(unsquashed_values)\n            log_prob -= tf.reduce_sum(tf.log(1 - unsquashed_values_tanhd ** 2 + SMALL_NUMBER), axis=-1, keepdims=True)\n        elif get_backend() == ""pytorch"":\n            unsquashed_values_tanhd = torch.tanh(unsquashed_values)\n            log_prob -= torch.sum(tf.log(1 - unsquashed_values_tanhd ** 2 + SMALL_NUMBER), axis=-1, keepdims=True)\n        return log_prob\n\n    @graph_fn\n    def _graph_fn_entropy(self, distribution):\n        return distribution.entropy()\n\n    @graph_fn\n    def _graph_fn_kl_divergence(self, distribution, distribution_b):\n        if get_backend() == ""tf"":\n            return tf.no_op()\n        elif get_backend() == ""pytorch"":\n            return None\n\n    @graph_fn\n    def _graph_fn_sample_and_log_prob(self, distribution, deterministic):\n        action = None\n        log_prob = None\n        if get_backend() == ""tf"":\n            raw_action = tf.cond(\n                pred=deterministic,\n                true_fn=lambda: distribution.mean(),\n                false_fn=lambda: distribution.sample()\n            )\n            action = tf.tanh(raw_action)\n            log_prob = distribution.log_prob(raw_action)\n            log_prob -= tf.reduce_sum(tf.log(1 - action ** 2 + SMALL_NUMBER), axis=-1, keepdims=True)\n\n        elif get_backend() == ""pytorch"":\n            if deterministic:\n                raw_action = distribution.mean()\n            else:\n                raw_action = distribution.sample()\n            action = torch.tanh(raw_action)\n            log_prob = distribution.log_prob(raw_action)\n            log_prob -= torch.sum(torch.log(1 - action ** 2 + SMALL_NUMBER), dim=-1, keepdims=True)\n\n        scaled_action = (action + 1) / 2 * (self.high - self.low) + self.low\n        return scaled_action, log_prob\n\n    #@graph_fn\n    #def _graph_fn_sample_and_log_prob(self, distribution, deterministic):\n    #    unsquashed = self._graph_fn_draw(distribution, deterministic)\n    #    action = None\n    #    log_prob = None\n\n    #    if get_backend() == ""tf"":\n    #        log_prob = distribution.log_prob(unsquashed)\n    #        action = tf.tanh(unsquashed)\n    #        log_prob -= tf.reduce_sum(tf.log(1 - action ** 2 + SMALL_NUMBER), axis=-1, keepdims=True)\n\n    #    elif get_backend() == ""pytorch"":\n    #        log_prob = distribution.log_prob(unsquashed)\n    #        action = torch.tanh(unsquashed)\n    #        log_prob -= torch.sum(torch.log(1 - action ** 2 + SMALL_NUMBER), dim=-1, keepdims=True)\n\n    #    scaled_action = (action + 1) / 2 * (self.high - self.low) + self.low\n    #    return scaled_action, log_prob\n\n    @graph_fn\n    def _graph_fn_squash(self, raw_values):\n        """"""\n        Makes sure all incoming values are between `self.low` and `self.high` using a tanh squasher function.\n\n        Args:\n            raw_values (DataOp): The values to squash.\n\n        Returns:\n            The squashed values.\n        """"""\n        if get_backend() == ""tf"":\n            return (tf.tanh(raw_values) + 1.0) / 2.0 * (self.high - self.low) + self.low\n        elif get_backend() == ""pytorch"":\n            return (torch.tanh(raw_values) + 1.0) / 2.0 * (self.high - self.low) + self.low\n\n    @graph_fn\n    def _graph_fn_unsquash(self, values):\n        """"""\n        Reverse operation as _graph_fn_squash (using argus tanh).\n\n        Args:\n            values (DataOp): The values to unsquash.\n\n        Returns:\n            The unsquashed values.\n        """"""\n        if get_backend() == ""tf"":\n            return tf.atanh((values - self.low) / (self.high - self.low) * 2.0 - 1.0)\n        elif get_backend() == ""tf"":\n            return torch.atanh((values - self.low) / (self.high - self.low) * 2.0 - 1.0)\n'"
rlgraph/components/explorations/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.explorations.exploration import Exploration\nfrom rlgraph.components.explorations.epsilon_exploration import EpsilonExploration\n\n__all__ = [""Exploration"", ""EpsilonExploration""]\n\n'"
rlgraph/components/explorations/epsilon_exploration.py,2,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.common.time_dependent_parameters import TimeDependentParameter\nfrom rlgraph.components.component import Component\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass EpsilonExploration(Component):\n    """"""\n    A component to handle epsilon-exploration functionality. It takes the current time step and outputs a bool\n    on whether to explore (uniformly random) or not (greedy or sampling).\n    The time step is used by a epsilon-decay component to determine the current epsilon value between 1.0\n    and 0.0. The result of this decay is the probability, with which we output ""True"" (meaning: do explore),\n    vs ""False"" (meaning: do not explore).\n\n    API:\n    ins:\n        time_step (int): The current time step.\n    outs:\n        do_explore (bool): The decision whether to explore (do_explore=True; pick uniformly randomly) or\n            whether to use a sample (or max-likelihood value) from a distribution (do_explore=False).\n    """"""\n    def __init__(self, decay_spec=None, scope=""epsilon-exploration"", **kwargs):\n        """"""\n        Args:\n            decay_spec (Optional[dict,DecayComponent]): The spec-dict for the DecayComponent to use or a DecayComponent\n                object directly.\n\n        Keyword Args:\n            Used as decay_spec (only if `decay_spec` not given) to construct the DecayComponent.\n        """"""\n        super(EpsilonExploration, self).__init__(scope=scope, **kwargs)\n\n        # The space of the samples that we have to produce epsilon decisions for.\n        self.sample_space = None\n        self.flat_sample_space = None\n\n        # Our (epsilon) Decay-Component.\n        self.decay = TimeDependentParameter.from_spec(decay_spec)\n\n        # Add the decay component and make time_step our (only) input.\n        self.add_components(self.decay)\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        # Require at least a batch-rank in the incoming samples.\n        self.sample_space = input_spaces[""sample""]\n        self.flat_sample_space = self.sample_space.flatten()\n        if get_backend() == ""tf"":\n            sanity_check_space(self.sample_space, must_have_batch_rank=True)\n\n    @rlgraph_api\n    def do_explore(self, sample, time_percentage=None):\n        """"""\n        API-method taking a timestep and returning a bool type tensor on whether to explore or not (per batch item).\n\n        Args:\n            sample (SingleDataOp): A data sample from which we can extract the batch size.\n\n            time_percentage (SingleDataOp): The current consumed time (0.0 to 1.0) with respect to the max timestep\n                value.\n\n        Returns:\n            SingleDataOp: Single decisions over a batch on whether to explore or not.\n        """"""\n        #decayed_value = self.decay_component.decayed_value(time_step)\n        return self._graph_fn_get_random_actions(self.decay.get(time_percentage), sample)\n\n    @graph_fn(flatten_ops=True, split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_get_random_actions(self, key, decayed_value, sample):\n        if get_backend() == ""tf"":\n            shape = tf.shape(sample)\n            batch_time_shape = (shape[0],) + ((shape[1],) if self.flat_sample_space[key].has_time_rank is True else ())\n            return tf.random_uniform(shape=batch_time_shape) < decayed_value\n        elif get_backend() == ""pytorch"":\n            if sample.dim() == 0:\n                sample = sample.unsqueeze(-1)\n            shape = sample.shape\n            batch_time_shape = (shape[0],) + ((shape[1],) if self.flat_sample_space[key].has_time_rank is True else ())\n            x = torch.rand(batch_time_shape) < decayed_value\n            return x\n'"
rlgraph/components/explorations/exploration.py,5,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.common.noise_components import NoiseComponent\nfrom rlgraph.components.component import Component\nfrom rlgraph.components.explorations.epsilon_exploration import EpsilonExploration\nfrom rlgraph.spaces import IntBox, FloatBox\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\nfrom rlgraph.utils.util import convert_dtype\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass Exploration(Component):\n    """"""\n    A Component that can be plugged on top of a Policy\'s output to produce action choices.\n    It includes noise and/or epsilon-based exploration options as well as an API to draw actions from\n    the Policy\'s distribution - either by sampling or by deterministically choosing the max-likelihood value.\n    """"""\n    def __init__(self, epsilon_spec=None, noise_spec=None, scope=""exploration"", **kwargs):\n        """"""\n        Args:\n            epsilon_spec (any): The spec or Component object itself to construct an EpsilonExploration Component.\n            noise_spec (dict): The specification dict for a noise generator that adds noise to the NN\'s output.\n        """"""\n        super(Exploration, self).__init__(scope=scope, **kwargs)\n\n        self.action_space = None  # The actual action space (may not have batch-rank, just the plain space)\n        self.flat_action_space = None\n\n        self.epsilon_exploration = None\n        self.noise_component = None\n\n        # For define-by-run sampling.\n        self.sample_obj = None\n\n        # Don\'t allow both epsilon and noise component\n        if epsilon_spec and noise_spec:\n            raise RLGraphError(""Cannot use both epsilon exploration and a noise component at the same time."")\n\n        # Add epsilon component.\n        if epsilon_spec:\n            self.epsilon_exploration = EpsilonExploration.from_spec(epsilon_spec)\n            self.add_components(self.epsilon_exploration)\n\n            # Define our interface.\n            @rlgraph_api(component=self)\n            def get_action(self, actions, time_percentage=None, use_exploration=True):\n                """"""\n                Action depends on `time_percentage` (e.g. epsilon-decay).\n                """"""\n                epsilon_decisions = self.epsilon_exploration.do_explore(actions, time_percentage)\n                return self._graph_fn_pick(use_exploration, epsilon_decisions, actions)\n\n        # Add noise component.\n        elif noise_spec:\n            self.noise_component = NoiseComponent.from_spec(noise_spec)\n            self.add_components(self.noise_component)\n\n            @rlgraph_api(component=self)\n            def get_action(self, actions, time_percentage=None, use_exploration=True):\n                """"""\n                Noise is added to the sampled action.\n                """"""\n                noise = self.noise_component.get_noise()\n                return self._graph_fn_add_noise(use_exploration, noise, actions)\n\n        # Don\'t explore at all. Simple pass-through.\n        else:\n            @rlgraph_api(component=self)\n            def get_action(self, actions, time_percentage=None, use_exploration=False):\n                """"""\n                Action is returned as is.\n                """"""\n                return actions\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        action_sample_space = input_spaces[""actions""]\n\n        if get_backend() == ""tf"":\n            sanity_check_space(action_sample_space, must_have_batch_rank=True)\n\n        assert action_space is not None\n        self.action_space = action_space\n        self.flat_action_space = action_space.flatten()\n\n        if self.epsilon_exploration and self.noise_component:\n            # Check again at graph creation? This is currently redundant to the check in __init__\n            raise RLGraphError(""Cannot use both epsilon exploration and a noise component at the same time."")\n\n        if self.epsilon_exploration:\n            sanity_check_space(self.action_space, must_have_categories=True, num_categories=(1, None),\n                               allowed_sub_types=[IntBox])\n        elif self.noise_component:\n            sanity_check_space(self.action_space, allowed_sub_types=[FloatBox])\n\n    @graph_fn(flatten_ops=True, split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_pick(self, key, use_exploration, epsilon_decisions, sample):\n        """"""\n        Exploration for discrete action spaces.\n        Either pick a random action (if `use_exploration` and `epsilon_decision` are True),\n            or return non-exploratory action.\n\n        Args:\n            use_exploration (DataOp): The master switch determining, whether to use exploration or not.\n            epsilon_decisions (DataOp): The bool coming from the epsilon-exploration component specifying\n                whether to use exploration or not (per batch item).\n            sample (DataOp): The output from a distribution\'s ""sample_deterministic"" OR ""sample_stochastic"".\n\n        Returns:\n            DataOp: The DataOp representing the action. This will match the shape of self.action_space.\n        """"""\n        if get_backend() == ""tf"":\n            if use_exploration is False:\n                return sample\n            else:\n                random_actions = tf.random_uniform(\n                    shape=tf.shape(sample),\n                    maxval=self.flat_action_space[key].num_categories,\n                    dtype=convert_dtype(""int"")\n                )\n\n                return tf.where(\n                    # `use_exploration` given as actual bool or as tensor?\n                    condition=epsilon_decisions if use_exploration is True else tf.logical_and(\n                        use_exploration, epsilon_decisions\n                    ),\n                    x=random_actions,\n                    y=sample\n                )\n        elif get_backend() == ""pytorch"":\n            # N.b. different order versus TF because we dont want to execute the sampling below.\n            if bool(use_exploration) is False:\n                return sample\n\n            if self.sample_obj is None:\n                # Don\'t create new sample objects very time.\n                self.sample_obj = torch.distributions.Uniform(0, self.flat_action_space[key].num_categories)\n\n            random_actions = self.sample_obj.sample(sample.shape).int()\n            if bool(use_exploration) is True:\n                return torch.where(epsilon_decisions, random_actions, sample)\n            else:\n                if not isinstance(use_exploration, torch.ByteTensor):\n                    use_exploration = use_exploration.byte()\n                if not isinstance(epsilon_decisions, torch.ByteTensor):\n                    epsilon_decisions = epsilon_decisions.byte()\n                return torch.where(use_exploration & epsilon_decisions, random_actions, sample)\n\n    @graph_fn\n    def _graph_fn_add_noise(self, use_exploration, noise, sample):\n        """"""\n        Noise for continuous action spaces.\n        Return the action with added noise.\n\n        Args:\n            use_exploration (DataOp): The master switch determining, whether to add noise or not.\n            noise (DataOp): The noise coming from the noise component.\n            sample (DataOp): The output from a distribution\'s ""sample_deterministic"" or ""sample_stochastic"" API-method.\n\n        Returns:\n            DataOp: The DataOp representing the action. This will match the shape of self.action_space.\n\n        """"""\n        if get_backend() == ""tf"":\n            return tf.cond(\n                use_exploration, true_fn=lambda: sample + noise, false_fn=lambda: sample\n            )\n        elif get_backend() == ""pytorch"":\n            if use_exploration:\n                return sample + noise\n            else:\n                return sample\n'"
rlgraph/components/helpers/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.helpers.mem_segment_tree import MemSegmentTree\nfrom rlgraph.components.helpers.segment_tree import SegmentTree\nfrom rlgraph.components.helpers.softmax import SoftMax\nfrom rlgraph.components.helpers.v_trace_function import VTraceFunction\nfrom rlgraph.components.helpers.sequence_helper import SequenceHelper\nfrom rlgraph.components.helpers.clipping import Clipping\nfrom rlgraph.components.helpers.generalized_advantage_estimation import GeneralizedAdvantageEstimation\n\n\n__all__ = [""MemSegmentTree"", ""SegmentTree"", ""SoftMax"", ""VTraceFunction"", ""SequenceHelper"",\n           ""GeneralizedAdvantageEstimation"", ""Clipping""]\n'"
rlgraph/components/helpers/clipping.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass Clipping(Component):\n    """"""\n    Clipping utility (e.g. to clip rewards).\n\n    API:\n        clip(values) -> returns clipped values.\n    """"""\n    def __init__(self, clip_value=0.0, scope=""clipping"", **kwargs):\n        super(Clipping, self).__init__(scope=scope, **kwargs)\n        self.clip_value = clip_value\n\n    @rlgraph_api(must_be_complete=False)\n    def _graph_fn_clip_if_needed(self, values):\n        """"""\n        Clips values if cli pvalue specified, otherwise passes through.\n        Args:\n            values (SingleDataOp): Values to clip.\n\n        Returns:\n            SingleDataOp: Clipped values.\n\n        """"""\n        if self.clip_value == 0.0:\n            return values\n        elif get_backend() == ""tf"":\n            return tf.clip_by_value(t=values, clip_value_min=-self.clip_value, clip_value_max=self.clip_value)\n        elif get_backend() == ""pytorch"":\n            torch.clamp(values, min=-self.clip_value, max=-self.clip_value)\n\n'"
rlgraph/components/helpers/dynamic_batching.py,9,"b'# Copyright 2018/2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Dynamic batching.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nfrom rlgraph import get_backend\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n    # TODO handle this?\n    try:\n        batcher_ops = tf.load_op_library(\'/home/rlgraph/deepmind/deepmind-scalable-agent/batcher.so\')\n    except:\n        try:\n            batcher_ops = tf.load_op_library(\'/root/scalable_agent/batcher.so\')\n        except:\n            pass\n\n    nest = tf.contrib.framework.nest\n\n\nclass Batcher(object):\n    """"""\n    A thin layer around the Batcher TensorFlow operations.\n\n    It shares some of the interface with queues (close(), name) to be able to use\n    it correctly as the input to a QueueRunner.\n    """"""\n\n    def __init__(self, minimum_batch_size, maximum_batch_size, timeout_ms):\n        self.handle = batcher_ops.batcher(\n            minimum_batch_size, maximum_batch_size, timeout_ms or -1\n        )\n\n    @property\n    def name(self):\n        return ""batcher""\n\n    def get_inputs(self, input_dtypes):\n        return batcher_ops.batcher_get_inputs(self.handle, input_dtypes)\n\n    def set_outputs(self, flat_result, computation_id):\n        return batcher_ops.batcher_set_outputs(self.handle, flat_result, computation_id)\n\n    def compute(self, flat_args, output_dtypes):\n        return batcher_ops.batcher_compute(self.handle, flat_args, output_dtypes)\n\n    def close(self, cancel_pending_enqueues=False, name=None):\n        del cancel_pending_enqueues\n        return batcher_ops.batcher_close(self.handle, name=name)\n\n\ndef batch_fn(f):\n    """"""\n    See `batch_fn_with_options` for details.\n    """"""\n    return batch_fn_with_options()(f)\n\n\ndef batch_fn_with_options(minimum_batch_size=1, maximum_batch_size=1024, timeout_ms=100):\n    """"""\n    Python decorator that automatically batches computations.\n\n    When the decorated function is called, it creates an operation that adds the\n    inputs to a queue, waits until the computation is done, and returns the\n    tensors. The inputs must be nests (see `tf.contrib.framework.nest`) and the\n    first dimension of each tensor in the nest must have size 1.\n\n    It adds a QueueRunner that asynchronously keeps fetching batches of data,\n    computes the results and pushes the results back to the caller.\n\n    Example usage:\n        @dynamic_batching.batch_fn_with_options(minimum_batch_size=10, timeout_ms=100)\n        def fn(a, b):\n            return a + b\n\n        output0 = fn(tf.constant([1]), tf.constant([2]))  # Will be batched with the next call.\n        output1 = fn(tf.constant([3]), tf.constant([4]))\n\n    Note, gradients are currently not supported.\n    Note, if minimum_batch_size == maximum_batch_size and timeout_ms=None, then the batch size of input arguments\n    will be set statically. Otherwise, it will be None.\n\n    Args:\n        minimum_batch_size: The minimum batch size before processing starts.\n        maximum_batch_size: The maximum batch size.\n        timeout_ms: Milliseconds after a batch of samples is requested before it is\n            processed, even if the batch size is smaller than `minimum_batch_size`. If\n            None, there is no timeout.\n\n    Returns:\n        The decorator.\n    """"""\n    def decorator(f):\n        """"""Decorator.""""""\n        batcher = [None]\n        batched_output = [None]\n\n        @functools.wraps(f)\n        def wrapper(*args):\n            """"""Wrapper.""""""\n\n            self_arg = args[0]\n            args = args[1:]\n\n            flat_args = [tf.convert_to_tensor(arg) for arg in nest.flatten(args)]\n            #flat_args = nest.flatten(args)\n\n            if batcher[0] is None:\n                # Remove control dependencies which is necessary when created in loops,\n                # etc.\n                with tf.control_dependencies(None):\n                    input_dtypes = [t.dtype for t in flat_args]\n                    batcher[0] = Batcher(minimum_batch_size, maximum_batch_size, timeout_ms)\n\n                    # Compute in batches using a queue runner.\n\n                    if minimum_batch_size == maximum_batch_size and timeout_ms is None:\n                        batch_size = minimum_batch_size\n                    else:\n                        batch_size = None\n\n                    # Dequeue batched input.\n                    inputs, computation_id = batcher[0].get_inputs(input_dtypes)\n                    nest.map_structure(\n                        lambda i, a: i.set_shape([batch_size] + a.shape.as_list()[1:]),\n                        inputs, flat_args\n                    )\n\n                    # Compute result.\n                    result = f(self_arg, *nest.pack_sequence_as(args, inputs))\n                    batched_output[0] = result\n                    flat_result = nest.flatten(result)\n\n                    # Insert results back into batcher.\n                    set_op = batcher[0].set_outputs(flat_result, computation_id)\n\n                    tf.train.add_queue_runner(tf.train.QueueRunner(batcher[0], [set_op]))\n\n            # Insert inputs into input queue.\n            flat_result = batcher[0].compute(\n                flat_args,\n                [t.dtype for t in nest.flatten(batched_output[0])])\n\n            # Restore structure and shapes.\n            result = nest.pack_sequence_as(batched_output[0], flat_result)\n            static_batch_size = nest.flatten(args)[0].shape[0]\n\n            nest.map_structure(\n                lambda t, b: t.set_shape([static_batch_size] + b.shape[1:].as_list()),\n                result, batched_output[0]\n            )\n            return result\n\n        return wrapper\n\n    return decorator\n\n\ndef convert_graph_fn_to_dynamic_batched(graph_fn, device=""gpu""):\n    return\n'"
rlgraph/components/helpers/generalized_advantage_estimation.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.component import Component\nfrom rlgraph.components.helpers import SequenceHelper\nfrom rlgraph.components.helpers.clipping import Clipping\nfrom rlgraph.utils.decorators import rlgraph_api\n\n\nclass GeneralizedAdvantageEstimation(Component):\n    """"""\n    A Helper Component that contains a graph_fn to generalized advantage estimation (GAE) [1].\n\n    [1] High-Dimensional Continuous Control Using Generalized Advantage Estimation - Schulman et al.\n    - 2015 (https://arxiv.org/abs/1506.02438)\n    """"""\n\n    def __init__(self, gae_lambda=1.0, discount=1.0, clip_rewards=0.0,\n                 scope=""generalized-advantage-estimation"", **kwargs):\n        """"""\n        Args:\n            gae_lambda (float): GAE-lambda. See paper for details.\n            discount (float): Discount gamma.\n        """"""\n        super(GeneralizedAdvantageEstimation, self).__init__(scope=scope, **kwargs)\n        self.gae_lambda = gae_lambda\n        self.discount = discount\n        self.sequence_helper = SequenceHelper()\n        self.clipping = Clipping(clip_value=clip_rewards)\n\n        self.add_components(self.sequence_helper, self.clipping)\n\n    @rlgraph_api\n    def calc_td_errors(self, baseline_values, rewards, terminals, sequence_indices):\n        """"""\n        Returns 1-step TD Errors (delta = r + gamma V(s\') - V(s)). Clips rewards if specified.\n\n        Args:\n            baseline_values (DataOp): Baseline predictions V(s).\n            rewards (DataOp): Rewards in sample trajectory.\n            terminals (DataOp): Terminals in sample trajectory.\n            sequence_indices (DataOp): Int indices denoting sequences (which may be non-terminal episode fragments\n                from multiple environments.\n\n        Returns:\n            1-Step TD Errors for the sequence.\n        """"""\n        rewards = self.clipping.clip_if_needed(rewards)\n\n        # Next, we need to set the next value after the end of each sub-sequence to 0/its prior value\n        # depending on terminal, then compute 1-step TD-errors: delta = r[:] + gamma * v[1:] - v[:-1]\n        # -> where len(v) = len(r) + 1 b/c v contains the extra (bootstrapped) last value.\n        # Terminals indicate boot-strapping. Sequence indices indicate episode fragments in case of a multi-environment.\n        deltas = self.sequence_helper.bootstrap_values(\n            rewards, baseline_values, terminals, sequence_indices, self.discount\n        )\n        return deltas\n\n    @rlgraph_api\n    def calc_gae_values(self, baseline_values, rewards, terminals, sequence_indices):\n        """"""\n        Returns advantage values based on GAE. Clips rewards if specified.\n\n        Args:\n            baseline_values (DataOp): Baseline predictions V(s).\n            rewards (DataOp): Rewards in sample trajectory.\n            terminals (DataOp): Terminals in sample trajectory.\n            sequence_indices (DataOp): Int indices denoting sequences (which may be non-terminal episode fragments\n                from multiple environments.\n        Returns:\n            PG-advantage values used for training via policy gradient with baseline.\n        """"""\n        deltas = self.calc_td_errors(baseline_values, rewards, terminals, sequence_indices)\n\n        gae_discount = self.gae_lambda * self.discount\n        # Apply gae discount to each sub-sequence.\n        # Note: sequences are indicated by sequence indices, which may not be terminal.\n        gae_values = self.sequence_helper.reverse_apply_decays_to_sequence(deltas, sequence_indices, gae_discount)\n        return gae_values\n'"
rlgraph/components/helpers/mem_segment_tree.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport operator\n\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\n\n\nclass MemSegmentTree(object):\n    """"""\n    In-memory Segment tree for prioritized replay.\n\n    Note: The pure TensorFlow segment tree is much slower because variable updating is expensive,\n    and in scenarios like Ape-X, memory and update are separated processes, so there is little to be gained\n    from inserting into the graph.\n    """"""\n\n    def __init__(\n            self,\n            values,\n            capacity,\n            operator=operator.add\n    ):\n        """"""\n        Helper to represent a segment tree.\n\n        Args:\n            values (list): Storage for the segment tree.\n            capacity (int): Capacity of segment tree.\n            operator (callable): Reduce operation of the segment tree.\n        """"""\n        self.values = values\n        self.capacity = capacity\n        self.operator = operator\n\n    def insert(self, index, element):\n        """"""\n        Inserts an element into the segment tree by determining\n        its position in the tree.\n\n        Args:\n            index (int): Insertion index.\n            element (any): Element to insert.\n        """"""\n        index += self.capacity\n        self.values[index] = element\n\n        # Bit shift should be slightly faster here than division.\n        index = index >> 1\n        while index >= 1:\n            # No shift because small multiplications are optimized.\n            update_index = 2 * index\n            self.values[index] = self.operator(\n                self.values[update_index],\n                self.values[update_index + 1]\n            )\n            index = index >> 1\n\n    def get(self, index):\n        """"""\n        Reads an item from the segment tree.\n\n        Args:\n            index (int):\n\n        Returns: The element.\n\n        """"""\n        return self.values[self.capacity + index]\n\n    def index_of_prefixsum(self, prefix_sum):\n        """"""\n        Identifies the highest index which satisfies the condition that the sum\n        over all elements from 0 till the index is <= prefix_sum.\n\n        Args:\n            prefix_sum .float): Upper bound on prefix we are allowed to select.\n\n        Returns:\n            int: Index/indices satisfying prefix sum condition.\n        """"""\n        assert 0 <= prefix_sum <= self.get_sum() + 1e-5\n        index = 1\n\n        while index < self.capacity:\n            update_index = 2 * index\n            if self.values[update_index] > prefix_sum:\n                index = update_index\n            else:\n                prefix_sum -= self.values[update_index]\n                index = update_index + 1\n        return index - self.capacity\n\n    def reduce(self, start, limit, reduce_op=operator.add):\n        """"""\n        Applies an operation to specified segment.\n\n        Args:\n            start (int): Start index to apply reduction to.\n            limit (end): End index to apply reduction to.\n            reduce_op (Union(operator.add, min, max)): Reduce op to apply.\n\n        Returns:\n            Number: Result of reduce operation\n        """"""\n        if limit is None:\n            limit = self.capacity\n        if limit < 0:\n            limit += self.capacity\n\n        # Init result with neutral element of reduce op.\n        # Note that all of these are commutative reduce ops.\n        if reduce_op == operator.add:\n            result = 0.0\n        elif reduce_op == min:\n            result = float(\'inf\')\n        elif reduce_op == max:\n            result = float(\'-inf\')\n        else:\n            raise RLGraphError(""Unsupported reduce OP. Support ops are [add, min, max]."")\n        start += self.capacity\n        limit += self.capacity\n\n        while start < limit:\n            if start & 1:\n                result = reduce_op(result, self.values[start])\n                start += 1\n            if limit & 1:\n                limit -= 1\n                result = reduce_op(result, self.values[limit])\n            start = start >> 1\n            limit = limit >> 1\n        return result\n\n    def get_min_value(self, start=0, stop=None):\n        """"""\n        Returns min value of storage variable.\n        """"""\n        return self.reduce(start, stop, reduce_op=min)\n\n    def get_sum(self, start=0, stop=None):\n        """"""\n        Returns sum value of storage variable.\n        """"""\n        return self.reduce(start, stop, reduce_op=operator.add)\n\n\nclass MinSumSegmentTree(object):\n    """"""\n    This class merges two segment trees\' operations for performance reasons to avoid\n    unnecessary duplication of the insert loops.\n    """"""\n\n    def __init__(\n            self,\n            sum_tree,\n            min_tree,\n            capacity,\n    ):\n        self.sum_segment_tree = sum_tree\n        self.min_segment_tree = min_tree\n        self.capacity = capacity\n\n    def insert(self, index, element):\n        """"""\n        Inserts an element into both segment trees by determining\n        its position in the trees.\n\n        Args:\n            index (int): Insertion index.\n            element (any): Element to insert.\n        """"""\n        index += self.capacity\n        self.sum_segment_tree.values[index] = element\n        self.min_segment_tree.values[index] = element\n\n        index = index >> 1\n        while index >= 1:\n            update_index = 2 * index\n            self.sum_segment_tree.values[index] = self.sum_segment_tree.values[update_index] +\\\n                self.sum_segment_tree.values[update_index + 1]\n            self.min_segment_tree.values[index] = min(self.min_segment_tree.values[update_index],\n                                                      self.min_segment_tree.values[update_index + 1])\n            index = index >> 1\n'"
rlgraph/components/helpers/segment_tree.py,38,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass SegmentTree(object):\n    """"""\n    TensorFlow Segment tree for prioritized replay.\n    """"""\n    def __init__(\n            self,\n            storage_variable,\n            capacity=1048\n    ):\n        """"""\n        Helper to represent a segment tree in pure TensorFlow.\n\n        Args:\n            storage_variable (tf.Variable): TensorFlow variable to use for storage.\n            capacity (int): Capacity of the segment tree.\n        """"""\n        self.values = storage_variable\n        self.capacity = capacity\n\n    def insert(self, index, element, insert_op=None):\n        """"""\n        Inserts an element into the segment tree by determining\n        its position in the tree.\n\n        Args:\n            index (int): Insertion index.\n            element (any): Element to insert.\n            insert_op (Union(tf.add, tf.minimum, tf, maximum)): Insert operation on the tree.\n        """"""\n        insert_op = insert_op or tf.add\n\n        index += self.capacity\n\n        # Use a TensorArray to collect updates to the segment tree, then perform them all at once.\n        index_updates = tf.TensorArray(\n            dtype=tf.int32,\n            infer_shape=False,\n            size=1,\n            dynamic_size=True,\n            clear_after_read=False\n        )\n        element_updates = tf.TensorArray(\n            dtype=tf.float32,\n            infer_shape=False,\n            size=1,\n            dynamic_size=True,\n            clear_after_read=False\n        )\n\n        index_updates = index_updates.write(index=0, value=index)\n        element_updates = element_updates.write(index=0, value=element)\n\n        # Search and update values while index >=1\n        loop_update_index = tf.div(x=index, y=2)\n\n        def insert_body(loop_update_index, index_updates, element_updates, call_index):\n            # This is the index we just updated.\n            prev_index = index_updates.read(call_index - 1)\n            prev_val = element_updates.read(call_index - 1)\n            update_val = tf.where(\n                condition=tf.greater(x=prev_index % 2, y=0),\n                # Previous index was odd because of loop init -> 2 * index + 1 is in element_updates,\n                # 2 * index is in variable values\n                x=insert_op(x=self.values[2 * loop_update_index],\n                            y=prev_val),\n\n                # Previous index was even -> 2 * index is in element updates, 2 * index + 1 in variable values.\n                y=insert_op(x=prev_val,\n                            y=self.values[2 * loop_update_index + 1])\n            )\n            index_updates = index_updates.write(call_index, loop_update_index)\n            element_updates = element_updates.write(call_index, update_val)\n\n            return tf.div(x=loop_update_index, y=2), index_updates, element_updates, call_index + 1\n\n        def cond(loop_update_index, index_updates, element_updates, call_index):\n            return loop_update_index >= 1\n\n        # Return the TensorArrays containing the updates.\n        loop_update_index, index_updates, element_updates, _ = tf.while_loop(\n            cond=cond,\n            body=insert_body,\n            loop_vars=[loop_update_index, index_updates, element_updates, 1],\n            parallel_iterations=1,\n            back_prop=False\n        )\n        indices = index_updates.stack()\n        updates = element_updates.stack()\n\n        assignment = tf.scatter_update(ref=self.values, indices=indices, updates=updates)\n\n        with tf.control_dependencies(control_inputs=[assignment]):\n            return tf.no_op()\n\n    def get(self, index):\n        """"""\n        Reads an item from the segment tree.\n\n        Args:\n            index (int):\n\n        Returns: The element.\n\n        """"""\n        return self.values[self.capacity + index]\n\n    def index_of_prefixsum(self, prefix_sum):\n        """"""\n        Identifies the highest index which satisfies the condition that the sum\n        over all elements from 0 till the index is <= prefix_sum.\n\n        Args:\n            prefix_sum .float): Upper bound on prefix we are allowed to select.\n\n        Returns:\n            int: Index/indices satisfying prefix sum condition.\n        """"""\n        assert_ops = list()\n        # 0 <= prefix_sum <= sum(priorities)\n        priority_sum = tf.reduce_sum(input_tensor=self.values, axis=0)\n        # priority_sum_tensor = tf.fill(dims=tf.shape(prefix_sum), value=priority_sum)\n        assert_ops.append(tf.Assert(\n            condition=tf.less_equal(x=prefix_sum, y=priority_sum),\n            data=[prefix_sum]\n        ))\n\n        # Vectorized loop -> initialize all indices matching elements in prefix-sum,\n        index = 1\n\n        def search_body(index, prefix_sum):\n            # Is the value at position 2 * index > prefix sum?\n            compare_value = self.values[2 * index]\n\n            def update_prefix_sum_fn(index, prefix_sum):\n                # \'Use up\' values in this segment, then jump to next.\n                prefix_sum -= self.values[2 * index]\n                return 2 * index + 1, prefix_sum\n\n            index, prefix_sum = tf.cond(\n                pred=compare_value > prefix_sum,\n                # If over prefix sum, jump index.\n                true_fn=lambda: (2 * index, prefix_sum),\n                # Else adjust prefix sum until done.\n                false_fn=lambda: update_prefix_sum_fn(index, prefix_sum)\n            )\n            return index, prefix_sum\n\n        def cond(index, prefix_sum):\n            return index < self.capacity\n\n        with tf.control_dependencies(control_inputs=assert_ops):\n            index, _ = tf.while_loop(cond=cond, body=search_body, loop_vars=[index, prefix_sum])\n\n        return index - self.capacity\n\n    def reduce(self, start, limit, reduce_op=None):\n        """"""\n        Applies an operation to specified segment.\n\n        Args:\n            start (int): Start index to apply reduction to.\n            limit (end): End index to apply reduction to.\n            reduce_op (Union(tf.add, tf.minimum, tf.maximum)): Reduce op to apply.\n\n        Returns:\n            Number: Result of reduce operation\n        """"""\n        reduce_op = reduce_op or tf.add\n\n        # Init result with neutral element of reduce op.\n        # Note that all of these are commutative reduce ops.\n        if reduce_op == tf.add:\n            result = 0.0\n        elif reduce_op == tf.minimum:\n            result = float(\'inf\')\n        elif reduce_op == tf.maximum:\n            result = float(\'-inf\')\n        else:\n            raise ValueError(""Unsupported reduce OP. Support ops are [tf.add, tf.minimum, tf.maximum]"")\n\n        start += self.capacity\n        limit += self.capacity\n\n        def reduce_body(start, limit, result):\n            start_mod = tf.mod(x=start, y=2)\n\n            def update_start_fn(start, result):\n                result = reduce_op(x=result, y=self.values[start])\n                start += 1\n                return start, result\n\n            start, result = tf.cond(\n                pred=tf.equal(x=start_mod, y=0),\n                true_fn=lambda: (start, result),\n                false_fn=lambda: update_start_fn(start, result)\n            )\n\n            end_mod = tf.mod(x=limit, y=2)\n\n            def update_limit_fn(limit, result):\n                limit -= 1\n                result = reduce_op(x=result, y=self.values[limit])\n                return limit, result\n\n            limit, result = tf.cond(\n                pred=tf.equal(x=end_mod, y=0),\n                true_fn=lambda: (limit, result),\n                false_fn=lambda: update_limit_fn(limit, result)\n            )\n            return tf.div(x=start, y=2), tf.div(x=limit, y=2), result\n\n        def cond(start, limit, result):\n            return start < limit\n\n        _, _, result = tf.while_loop(cond=cond, body=reduce_body, loop_vars=(start, limit, result))\n\n        return result\n\n    def get_min_value(self):\n        """"""\n        Returns min value of storage variable.\n        """"""\n        return self.reduce(0, self.capacity - 1, reduce_op=tf.minimum)\n\n    def get_sum(self):\n        """"""\n        Returns sum value of storage variable.\n        """"""\n        return self.reduce(0, self.capacity - 1, reduce_op=tf.add)\n'"
rlgraph/components/helpers/sequence_helper.py,51,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass SequenceHelper(Component):\n    """"""\n    A helper Component that helps manipulate sequences with various utilities, e.g. for discounting.\n    """"""\n\n    def __init__(self, scope=""sequence-helper"", **kwargs):\n        super(SequenceHelper, self).__init__(space_agnostic=True, scope=scope, **kwargs)\n\n    @rlgraph_api\n    def _graph_fn_calc_sequence_lengths(self, sequence_indices):\n        """"""\n        Computes sequence lengths for a tensor containing sequence indices, where 1 indicates start\n        of a new sequence.\n        Args:\n            sequence_indices (DataOp): Indices denoting sequences, e.g. terminal values.\n        Returns:\n            Sequence lengths.\n        """"""\n        if get_backend() == ""tf"":\n            # TensorArray:\n            elems = tf.shape(input=sequence_indices)[0]\n            sequence_lengths = tf.TensorArray(\n                dtype=tf.int32,\n                infer_shape=False,\n                size=1,\n                dynamic_size=True,\n                clear_after_read=False\n            )\n\n            def update(write_index, sequence_array, length):\n                # Write to index, increase\n                sequence_array = sequence_array.write(write_index, length)\n                return sequence_array, write_index + 1, 0\n\n            def insert_body(index, length, sequence_lengths, write_index):\n                length += 1\n\n                # Update tensor array, reset length to 0.\n                sequence_lengths, write_index, length = tf.cond(\n                    pred=tf.equal(sequence_indices[index], tf.ones_like(sequence_indices[index])),\n                    true_fn=lambda: update(write_index, sequence_lengths, length),\n                    false_fn=lambda: (sequence_lengths, write_index, length)\n                )\n                return index + 1, length, sequence_lengths, write_index\n\n            def cond(index, length, sequence_lengths, write_index):\n                return index < elems\n\n            _, final_length, sequence_lengths, write_index = tf.while_loop(\n                cond=cond,\n                body=insert_body,\n                loop_vars=[0, 0, sequence_lengths, 0],\n                back_prop=False\n            )\n            # If the final element was terminal -> already included.\n            sequence_lengths, _, _ = tf.cond(\n                pred=tf.greater(final_length, 0),\n                true_fn=lambda: update(write_index, sequence_lengths, final_length),\n                false_fn=lambda: (sequence_lengths, write_index, final_length)\n            )\n            return sequence_lengths.stack()\n        elif get_backend() == ""pytorch"":\n            sequence_lengths = []\n            length = 0\n            for index in sequence_indices:\n                length += 1\n                if index == 1:\n                    sequence_lengths.append(length)\n                    length = 0\n            # Append final sequence.\n            if length > 0:\n                sequence_lengths.append(length)\n            return torch.tensor(sequence_lengths, dtype=torch.int32)\n\n    @rlgraph_api(returns=2)\n    def _graph_fn_calc_sequence_decays(self, sequence_indices, decay=0.9):\n        """"""\n        Computes decays for sequence indices, e.g. for generalized advantage estimation.\n        That is, a sequence with terminals is used to compute for each subsequence the decay\n        values and the length of the sequence.\n\n        Example:\n        decay = 0.5, sequence_indices = [0 0 1 0 1] will return lengths [3, 2] and\n        decays [1 0.5 0.25 1 0.5] (decay^0, decay^1, ..decay^k) where k = sequence length for\n        each sub-sequence.\n\n        Args:\n            sequence_indices (DataOp): Indices denoting sequences, e.g. terminal values.\n            decay (float): Initial decay value to start sub-sequence with.\n\n        Returns:\n            tuple:\n                - Sequence lengths.\n                - Decays.\n        """"""\n        if get_backend() == ""tf"":\n            elems = tf.shape(input=sequence_indices)[0]\n            sequence_indices = tf.cast(sequence_indices, dtype=tf.int32)\n\n            # TensorArray:\n            sequence_lengths = tf.TensorArray(\n                dtype=tf.int32,\n                infer_shape=False,\n                size=1,\n                dynamic_size=True,\n                clear_after_read=False\n            )\n            decays = tf.TensorArray(\n                dtype=tf.float32,\n                infer_shape=False,\n                size=1,\n                dynamic_size=True,\n                clear_after_read=False\n            )\n\n            def update(write_index, sequence_array, length):\n                # Write to index, increase\n                sequence_array = sequence_array.write(write_index, length)\n                return sequence_array, write_index + 1, 0\n\n            def insert_body(index, length, sequence_lengths, write_index, decays):\n                # Decay is based on length, so val = decay^length\n                decay_val = tf.pow(x=decay, y=tf.cast(length, dtype=tf.float32))\n\n                # Write decay val into array.\n                decays = decays.write(index, decay_val)\n                length += 1\n\n                # Update tensor array, reset length to 0.\n                sequence_lengths, write_index, length = tf.cond(\n                    pred=tf.equal(sequence_indices[index], 1),\n                    true_fn=lambda: update(write_index, sequence_lengths, length),\n                    false_fn=lambda: (sequence_lengths, write_index, length)\n                )\n                return index + 1, length, sequence_lengths, write_index, decays\n\n            def cond(index, length, sequence_lengths, write_index, decays):\n                return index < elems\n\n            index, final_length, sequence_lengths, write_index, decays = tf.while_loop(\n                cond=cond,\n                body=insert_body,\n                loop_vars=[0, 0, sequence_lengths, 0, decays],\n                back_prop=False\n            )\n\n            # If the final element was terminal -> already included.\n            # Decays need no updating because we just wrote them always.\n            sequence_lengths, _, _ = tf.cond(\n                pred=tf.greater(final_length, 0),\n                true_fn=lambda: update(write_index, sequence_lengths, final_length),\n                false_fn=lambda: (sequence_lengths, write_index, final_length)\n            )\n            return tf.stop_gradient(sequence_lengths.stack()), tf.stop_gradient(decays.stack())\n        elif get_backend() == ""pytorch"":\n            sequence_lengths = []\n            decays = []\n\n            length = 0\n            for index in sequence_indices:\n                # Compute decay based on sequence length.\n                decays.append(pow(decay, length))\n                length += 1\n                if index == 1:\n                    sequence_lengths.append(length)\n                    length = 0\n\n            # Append final sequence.\n            if length > 0:\n                sequence_lengths.append(length)\n            return torch.tensor(sequence_lengths, dtype=torch.int32), torch.tensor(decays, dtype=torch.int32)\n\n    @rlgraph_api\n    def _graph_fn_reverse_apply_decays_to_sequence(self, values, sequence_indices, decay=0.9):\n        """"""\n        Computes decays for sequence indices and applies them (in reverse manner to a sequence of values).\n        Useful to compute discounted reward estimates across a sequence of estimates.\n\n        Args:\n            values (DataOp): Values to apply decays to.\n            sequence_indices (DataOp): Indices denoting sequences, e.g. terminal values.\n            decay (float): Initial decay value to start sub-sequence with.\n\n        Returns:\n            Decayed sequence values.\n        """"""\n        if get_backend() == ""tf"":\n            elems = tf.shape(input=sequence_indices)[0]\n            decayed_values = tf.TensorArray(\n                dtype=tf.float32,\n                infer_shape=False,\n                size=1,\n                dynamic_size=True,\n                clear_after_read=False\n            )\n            sequence_indices = tf.cast(sequence_indices, dtype=tf.int32)\n\n            def insert_body(index, prev_v, decayed_values):\n                # NOTE: We cannot prev_v to 0.0 because values[index] might have a more complex shape,\n                # so this violates shape checks.\n                prev_v = tf.cond(\n                    pred=tf.equal(sequence_indices[index], tf.ones_like(sequence_indices[index])),\n                    true_fn=lambda: tf.zeros_like(prev_v),\n                    false_fn=lambda: prev_v\n                )\n                # index = tf.Print(index, [index, prev_v], summarize=100, message=""index, prev = "")\n\n                # Decay is based on length, so val = decay^length\n                accum_v = values[index] + decay * prev_v\n\n                # Write decayed val into array.\n                decayed_values = decayed_values.write(index, accum_v)\n                prev_v = accum_v\n\n                # Increase write-index and length of sub-sequence, decrease loop index in reverse iteration.\n                return index - 1, prev_v, decayed_values\n\n            def cond(index, prev_v, decayed_values):\n                # Scan in reverse.\n                return index >= 0\n\n            _, _, decayed_values = tf.while_loop(\n                cond=cond,\n                body=insert_body,\n                # loop index, index writing to tensor array, current length of sub-sequence, previous val (float)\n                loop_vars=[elems - 1, tf.zeros_like(values[-1]), decayed_values],\n                back_prop=False\n            )\n\n            decayed_values = decayed_values.stack()\n            return tf.stop_gradient(decayed_values)\n\n        elif get_backend() == ""pytorch"":\n            # Scan sequences in reverse:\n            decayed_values = []\n            i = len(values.data) - 1\n            prev_v = 0\n            for v in reversed(values.data):\n                # Arrived at new sequence, start over.\n                if sequence_indices[i] == 1:\n                    prev_v = 0\n\n                # Accumulate prior value.\n                accum_v = v + decay * prev_v\n                decayed_values.append(accum_v)\n                prev_v = accum_v\n\n                i -= 1\n\n            # Reverse, convert, and return final.\n            return torch.tensor(list(reversed(decayed_values)), dtype=torch.float32)\n\n    @rlgraph_api\n    def _graph_fn_bootstrap_values(self, rewards, values, terminals, sequence_indices, discount=0.99):\n        """"""\n        Inserts value estimates at the end of each sub-sequence for a given sequence and computes deltas\n        for generalized advantage estimation. That is, 0 is inserted after teach terminal and the final value of the\n        sub-sequence if the sequence does not end with a terminal. We then compute for each subsequence\n\n        delta = reward + discount * bootstrapped_values[1:] - bootstrapped_values[:-1]\n\n\n        Args:\n            rewards (DataOp): Rewards for the observed sequences.\n            values (DataOp): Value estimates for the observed sequences.\n            terminals (DataOp): Terminals in sequences\n            sequence_indices (DataOp): Int indices denoting sequences (which may be non-terminal episode fragments\n                from multiple environments.\n            discount (float): Discount to apply to delta computation.\n\n        Returns:\n            Sequence of deltas.\n        """"""\n        if get_backend() == ""tf"":\n            values = tf.squeeze(input=values)\n            num_values = tf.shape(input=values)[0]\n\n            # Again ensure last index is 1 for any sub-sample arriving here.\n            last_sequence = tf.expand_dims(sequence_indices[-1], -1)\n            sequence_indices = tf.concat([sequence_indices[:-1], tf.ones_like(last_sequence, dtype=tf.bool)], axis=0)\n\n            # Cannot use 0.0 because unknown shape.\n            deltas = tf.TensorArray(dtype=tf.float32, infer_shape=False, size=num_values,\n                                    dynamic_size=False, clear_after_read=False, name=""bootstrap-deltas"")\n\n            # Boot-strap with 0 only if terminals[i] and sequence_indices[i] are both true.\n            boot_strap_zeros = tf.where(\n                condition=tf.logical_and(sequence_indices, terminals),\n                x=tf.ones_like(sequence_indices),\n                y=tf.zeros_like(sequence_indices),\n            )\n\n            def write(index, deltas, start_index):\n                # First: Concat the slice of values representing the current sequence with bootstrap value.\n                baseline_slice = values[start_index:index + 1]\n                # Expand so value has a batch dim when we concat.\n\n                # If true terminal, append 0. Otherwise, append boot-strap val -> last observed val.\n                bootstrap_value = tf.cond(\n                    pred=tf.equal(boot_strap_zeros[index], tf.ones_like(sequence_indices[index])),\n                    true_fn=lambda: tf.zeros_like(tensor=values[index], dtype=tf.float32),\n                    false_fn=lambda: values[index],\n                )\n\n                value = tf.expand_dims(bootstrap_value, 0)\n                adjusted_v = tf.concat([baseline_slice, value], axis=0)\n\n                # Compute deltas for this sequence.\n                sequence_deltas = rewards[start_index:index + 1] + discount * adjusted_v[1:] - adjusted_v[:-1]\n\n                # Write delta to tensor-array.\n                write_indices = tf.range(start=start_index, limit=index + 1)\n                deltas = deltas.scatter(write_indices, sequence_deltas)\n\n                start_index = index + 1\n                # Set start-index for the next sub-sequence to index + 1\n                return deltas, start_index\n\n            def body(index, start_index, deltas):\n                # Whenever we encounter a sequence end, we compute deltas for that sequence.\n                deltas, start_index = tf.cond(\n                    pred=tf.equal(sequence_indices[index], tf.ones_like(sequence_indices[index])),\n                    true_fn=lambda: write(index, deltas, start_index),\n                    false_fn=lambda: (deltas, start_index)\n                )\n                return index + 1, start_index, deltas\n\n            def cond(index, start_index, deltas):\n                return index < num_values\n\n            index, start_index, deltas = tf.while_loop(\n                cond=cond,\n                body=body,\n                loop_vars=[0, 0, deltas],\n                parallel_iterations=1,\n                back_prop=False\n            )\n\n            deltas = deltas.stack()\n            # Squeeze because we inserted\n            return tf.squeeze(deltas)\n        elif get_backend() == ""pytorch"":\n            deltas = []\n            discount_tensor = torch.tensor(discount)\n            start_index = 0\n            i = 0\n            if len(values) > 1:\n                last_sequence = torch.unsqueeze(sequence_indices[-1], -1)\n                sequence_indices = torch.cat((sequence_indices[:-1], torch.ones_like(last_sequence)), 0)\n\n            for _ in range(len(values)):\n                if sequence_indices[i]:\n                    # Compute deltas for this sub-sequence.\n                    # Cannot do this all at once because we would need the correct offsets for each sub-sequence.\n                    baseline_slice = list(values[start_index:i + 1])\n                    if terminals[i]:\n                        baseline_slice.append(0)\n                    else:\n                        baseline_slice.append(values[-1])\n                    adjusted_v = torch.tensor(baseline_slice)\n\n                    # +1 because we want to include i-th value.\n                    delta = rewards[start_index:i + 1] + discount_tensor * adjusted_v[1:] - adjusted_v[:-1]\n                    deltas.extend(delta)\n                    start_index = i + 1\n                i += 1\n\n            return torch.tensor(deltas)\n\n'"
rlgraph/components/helpers/softmax.py,2,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.util import SMALL_NUMBER\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass SoftMax(Component):\n    """"""\n    A simple softmax component that translates logits into probabilities (and log-probabilities).\n\n    API:\n        call(logits) -> returns probabilities (softmaxed) and log-probabilities.\n    """"""\n    def __init__(self, scope=""softmax"", **kwargs):\n        super(SoftMax, self).__init__(scope=scope, **kwargs)\n\n    @rlgraph_api(must_be_complete=False)\n    def _graph_fn_get_probabilities_and_log_probs(self, logits):\n        """"""\n        Creates properties/parameters and log-probs from some reshaped output.\n\n        Args:\n            logits (SingleDataOp): The (already reshaped) logits.\n\n        Returns:\n            tuple (2x SingleDataOp):\n                probabilities (DataOp): The probabilities after softmaxing the logits.\n                log_probs (DataOp): Simply the log(probabilities).\n        """"""\n        if get_backend() == ""tf"":\n            # Translate logits into probabilities in a save way (SMALL_NUMBER trick).\n            probabilities = tf.maximum(x=tf.nn.softmax(logits=logits, axis=-1), y=SMALL_NUMBER)\n            # Log probs.\n            log_probs = tf.log(x=probabilities)\n\n            return probabilities, log_probs\n'"
rlgraph/components/helpers/v_trace_function.py,16,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.numpy import softmax\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass VTraceFunction(Component):\n    """"""\n    A Helper Component that contains a graph_fn to calculate V-trace values from importance ratios (rhos).\n    Based on [1] and coded analogously to: https://github.com/deepmind/scalable_agent\n\n    [1] IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures - Espeholt, Soyer,\n        Munos et al. - 2018 (https://arxiv.org/abs/1802.01561)\n    """"""\n\n    def __init__(self, rho_bar=1.0, rho_bar_pg=1.0, c_bar=1.0, device=""/device:CPU:0"",\n                 scope=""v-trace-function"", **kwargs):\n        """"""\n        Args:\n            rho_bar (float): The maximum values of the IS-weights for the temporal differences of V.\n                Use None for not applying any clipping.\n            rho_bar_pg (float): The maximum values of the IS-weights for the policy-gradient loss:\n                rho_s delta log pi(a|x) (r + gamma v_{s+1} - V(x_s))\n                Use None for not applying any clipping.\n            c_bar (float): The maximum values of the IS-weights for the time trace.\n                Use None for not applying any clipping.\n        """"""\n        super(VTraceFunction, self).__init__(device=device, space_agnostic=True, scope=scope, **kwargs)\n\n        self.rho_bar = rho_bar\n        self.rho_bar_pg = rho_bar_pg\n        self.c_bar = c_bar\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        pass\n        # TODO: Complete all input arg checks.\n        #log_is_weight_space = input_spaces[""log_is_weights""]\n        #discounts_space, rewards_space, values_space, bootstrap_value_space = \\\n        #    input_spaces[""discounts""], input_spaces[""rewards""], \\\n        #    input_spaces[""values""], input_spaces[""bootstrapped_values""]\n\n        #sanity_check_space(log_is_weight_space, must_have_batch_rank=True)\n        #log_is_weight_rank = log_is_weight_space.rank\n\n        # Sanity check our input Spaces for consistency (amongst each other).\n        #sanity_check_space(values_space, rank=log_is_weight_rank, must_have_batch_rank=True, must_have_time_rank=True)\n        #sanity_check_space(bootstrap_value_space, must_have_batch_rank=True, must_have_time_rank=True)\n        #sanity_check_space(discounts_space, rank=log_is_weight_rank,\n        #                   must_have_batch_rank=True, must_have_time_rank=True)\n        #sanity_check_space(rewards_space, rank=log_is_weight_rank, must_have_batch_rank=True, must_have_time_rank=True)\n\n    @rlgraph_api(must_be_complete=False)\n    def _graph_fn_calc_v_trace_values(self, logits_actions_pi, log_probs_actions_mu, actions, actions_flat,\n                                      discounts, rewards,\n                                      values, bootstrapped_values):\n        """"""\n        Returns the V-trace values calculated from log importance weights (see [1] for details).\n        Calculation:\n        vs = V(xs) + SUM[t=s to s+N-1]( gamma^t-s * ( PROD[i=s to t-1](ci) ) * dt_V )\n        with:\n            dt_V = rho_t * (rt + gamma V(xt+1) - V(xt))\n            rho_t and ci being the clipped IS weights\n\n        Args:\n            logits_actions_pi (SingleDataOp): The raw logits output of the pi-network (one logit per discrete action).\n            log_probs_actions_mu (SingleDataOp): The log-probs of the mu-network (one log-prob per discrete action).\n            actions (SingleDataOp): The (int encoded) actually taken actions.\n            actions_flat (SingleDataOp): The one-hot converted actually taken actions.\n            discounts (SingleDataOp): DataOp (time x batch x values) holding the discounts collected when stepping\n                through the environment (for the timesteps s=t to s=t+N-1).\n            rewards (SingleDataOp): DataOp (time x batch x values) holding the rewards collected when stepping\n                through the environment (for the timesteps s=t to s=t+N-1).\n            values (SingleDataOp): DataOp (time x batch x values) holding the the value function estimates\n                wrt. the learner\'s policy (pi) (for the timesteps s=t to s=t+N-1).\n            bootstrapped_values (SingleDataOp): DataOp (time(1) x batch x values) holding the last (bootstrapped)\n                value estimate to use as a value function estimate after n time steps (V(xs) for s=t+N).\n\n        Returns:\n            tuple:\n                - v-trace values (vs) in time x batch dimensions used to train the value-function (baseline).\n                - PG-advantage values in time x batch dimensions used for training via policy gradient with baseline.\n        """"""\n        # Simplified (not performance optimized!) numpy implementation of v-trace for testing purposes.\n        if get_backend() == ""python"" or self.backend == ""python"":\n            probs_actions_pi = softmax(logits_actions_pi, axis=-1)\n            log_probs_actions_pi = np.log(probs_actions_pi)\n\n            log_is_weights = log_probs_actions_pi - log_probs_actions_mu  # log(a/b) = log(a) - log(b)\n            log_is_weights_actions_taken = np.sum(log_is_weights * actions_flat, axis=-1, keepdims=True)\n            is_weights = np.exp(log_is_weights_actions_taken)\n\n            # rho_t = min(rho_bar, is_weights) = [1.0, 1.0], [0.67032005, 1.0], [1.0, 0.36787944]\n            if self.rho_bar is not None:\n                rho_t = np.minimum(self.rho_bar, is_weights)\n            else:\n                rho_t = is_weights\n\n            # Same for rho-PG (policy gradients).\n            if self.rho_bar_pg is not None:\n                rho_t_pg = np.minimum(self.rho_bar_pg, is_weights)\n            else:\n                rho_t_pg = is_weights\n\n            # Calculate ci terms for all timesteps:\n            # ci = min(c_bar, is_weights) = [1.0, 1.0], [0.67032005, 1.0], [1.0, 0.36787944]\n            if self.c_bar is not None:\n                c_i = np.minimum(self.c_bar, is_weights)\n            else:\n                c_i = is_weights\n\n            # Values t+1 -> shift by one time step.\n            values_t_plus_1 = np.concatenate((values[1:], bootstrapped_values), axis=0)\n            deltas = rho_t * (rewards + discounts * values_t_plus_1 - values)\n\n            # Reverse everything for recursive v_s calculation.\n            discounts_reversed = discounts[::-1]\n            c_i_reversed = c_i[::-1]\n            deltas_reversed = deltas[::-1]\n\n            vs_minus_v_xs = [np.zeros_like(np.squeeze(bootstrapped_values, axis=0))]\n\n            # Do the recursive calculations.\n            for d, c, delta in zip(discounts_reversed, c_i_reversed, deltas_reversed):\n                vs_minus_v_xs.append(delta + d * c * vs_minus_v_xs[-1])\n\n            # Convert into numpy array and revert back.\n            vs_minus_v_xs = np.array(vs_minus_v_xs[::-1])[:-1]\n\n            # Add V(x_s) to get v_s.\n            vs = vs_minus_v_xs + values\n\n            # Advantage for policy gradient.\n            vs_t_plus_1 = np.concatenate([vs[1:], bootstrapped_values], axis=0)\n            pg_advantages = (rho_t_pg * (rewards + discounts * vs_t_plus_1 - values))\n\n            return vs, pg_advantages\n\n        elif get_backend() == ""tf"":\n            # Calculate the log IS-weight values via: logIS = log(pi(a|s)) - log(mu(a|s)).\n            # Use the action_probs_pi values only of the actions actually taken.\n            log_probs_actions_taken_pi = tf.expand_dims(-tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=logits_actions_pi, labels=actions\n            ), axis=-1)\n            log_probs_actions_taken_mu = tf.reduce_sum(\n                input_tensor=log_probs_actions_mu * actions_flat, axis=-1, keepdims=True, name=""log-probs-actions-taken-mu""\n            )\n            log_is_weights = log_probs_actions_taken_pi - log_probs_actions_taken_mu\n\n            is_weights = tf.exp(x=log_is_weights, name=""is-weights-from-logs"")\n\n            # Apply rho-bar (also for PG) and c-bar clipping to all IS-weights.\n            if self.rho_bar is not None:\n                rho_t = tf.minimum(x=self.rho_bar, y=is_weights, name=""clip-rho-bar"")\n            else:\n                rho_t = is_weights\n\n            if self.rho_bar_pg is not None:\n                rho_t_pg = tf.minimum(x=self.rho_bar_pg, y=is_weights, name=""clip-rho-bar-pg"")\n            else:\n                rho_t_pg = is_weights\n\n            if self.c_bar is not None:\n                c_i = tf.minimum(x=self.c_bar, y=is_weights, name=""clip-c-bar"")\n            else:\n                c_i = is_weights\n\n            # This is the same vector as `values` except that it will be shifted by 1 timestep to the right and\n            # include - as the last item - the bootstrapped V value at s=t+N.\n            values_t_plus_1 = tf.concat(values=[values[1:], bootstrapped_values], axis=0, name=""values-t-plus-1"")\n            # Calculate the temporal difference terms (delta-t-V in the paper) for each s=t to s=t+N-1.\n            dt_vs = rho_t * (rewards + discounts * values_t_plus_1 - values)\n\n            # V-trace values can be calculated recursively (starting from the end of a trajectory) via:\n            #    vs = V(xs) + dsV + gamma * cs * (vs+1 - V(s+1))\n            # => (vs - V(xs)) = dsV + gamma * cs * (vs+1 - V(s+1))\n            # We will thus calculate all terms: [vs - V(xs)] for all timesteps first, then add V(xs) again to get the\n            # v-traces.\n            elements = (\n                tf.reverse(tensor=discounts, axis=[0], name=""revert-discounts""),\n                tf.reverse(tensor=c_i, axis=[0], name=""revert-c-i""),\n                tf.reverse(tensor=dt_vs, axis=[0], name=""revert-dt-vs"")\n            )\n\n            def scan_func(vs_minus_v_xs_, elements_):\n                gamma_t, c_t, dt_v = elements_\n                return dt_v + gamma_t * c_t * vs_minus_v_xs_\n\n            vs_minus_v_xs = tf.scan(\n                fn=scan_func,\n                elems=elements,\n                initializer=tf.zeros_like(tensor=tf.squeeze(bootstrapped_values, axis=0)),\n                parallel_iterations=1,\n                back_prop=False,\n                name=""v-trace-scan""\n            )\n            # Reverse the results back to original order.\n            vs_minus_v_xs = tf.reverse(tensor=vs_minus_v_xs, axis=[0], name=""revert-vs-minus-v-xs"")\n\n            # Add V(xs) to get vs.\n            vs = tf.add(x=vs_minus_v_xs, y=values)\n\n            # Calculate the advantage values (for policy gradient loss term) according to:\n            # A = Q - V with Q based on vs (v-trace) values: qs = rs + gamma * vs and V being the\n            # approximate value function output.\n            vs_t_plus_1 = tf.concat(values=[vs[1:], bootstrapped_values], axis=0)\n            pg_advantages = rho_t_pg * (rewards + discounts * vs_t_plus_1 - values)\n\n            # Return v-traces and policy gradient advantage values based on: A=r+gamma*v-trace(s+1) - V(s).\n            # With `r+gamma*v-trace(s+1)` also called `qs` in the paper.\n            return tf.stop_gradient(vs), tf.stop_gradient(pg_advantages)\n'"
rlgraph/components/layers/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# Basics.\nfrom rlgraph.components.layers.layer import Layer\n# NN-Layers.\nfrom rlgraph.components.layers.nn import *\n# Preprocessing Layers.\nfrom rlgraph.components.layers.preprocessing import *\n# String Layers.\nfrom rlgraph.components.layers.strings import *\nfrom rlgraph.utils.util import default_dict\n\n# The Layers.\nLayer.__lookup_classes__ = dict(\n    nnlayer=NNLayer,\n    preprocesslayer=PreprocessLayer\n)\n# Add all specific Layer sub-classes to this one.\ndefault_dict(Layer.__lookup_classes__, NNLayer.__lookup_classes__)\ndefault_dict(Layer.__lookup_classes__, PreprocessLayer.__lookup_classes__)\ndefault_dict(Layer.__lookup_classes__, StringLayer.__lookup_classes__)\n\n\n__all__ = [""Layer""] + \\\n          list(set(map(lambda x: x.__name__, Layer.__lookup_classes__.values())))\n'"
rlgraph/components/layers/layer.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport copy\n\nfrom rlgraph.components.component import Component\nfrom rlgraph.spaces.space import Space\nfrom rlgraph.utils.decorators import rlgraph_api\n\n\nclass Layer(Component):\n    """"""\n    A Layer is a simple Component that implements the `call` method with n inputs and m return values.\n    """"""\n    def __init__(self, scope=""layer"", **kwargs):\n        # Assume 1 return value when calling `call`.\n        graph_fn_num_outputs = kwargs.pop(""graph_fn_num_outputs"", {""_graph_fn_call"": 1})\n        super(Layer, self).__init__(scope=scope, graph_fn_num_outputs=graph_fn_num_outputs, **kwargs)\n\n    def get_preprocessed_space(self, space):\n        """"""\n        Returns the Space obtained after pushing the space input through this layer.\n\n        Args:\n            space (Space): The incoming Space object.\n\n        Returns:\n            Space: The Space after preprocessing.\n        """"""\n        return space\n\n    @rlgraph_api\n    def _graph_fn_call(self, *inputs):\n        """"""\n        Applies the layer\'s logic to the inputs and returns one or more result values.\n\n        Args:\n            *inputs (any): The input(s) to this layer.\n\n        Returns:\n            DataOp: The output(s) of this layer.\n        """"""\n        raise NotImplementedError\n\n    def __call__(self, *args, **kwargs):\n        """"""\n        Makes all Layers callable for the Keras-style functional API.\n\n        Args:\n            *args (any): The args passed in to the layer when ""called"" via the Keras-style functional API.\n\n        Keyword Args:\n            **kwargs (any): The kwargs passed in to the layer when ""called"" via the Keras-style functional API.\n\n        Returns:\n            List[LayerCallOutput]: n LayerCallOutput objects, where n=number of return values of `self._graph_fn_call`.\n        """"""\n        # If Spaces are given, add this information already to `self.api_method_records`.\n        for i, arg in enumerate(args):\n            if isinstance(arg, Space):\n                if self.api_methods[""call""].args_name is None:\n                    self.api_method_inputs[self.api_methods[""call""].input_names[i]] = arg\n                else:\n                    self.api_method_inputs[""{}[{}]"".format(self.api_methods[""call""].args_name, i)] = arg\n\n        for key, value in kwargs.items():\n            if isinstance(value, Space):\n                self.api_method_inputs[key] = value\n\n        inputs_list = list(args) + list(kwargs.values())\n        # Translate all incoming Spaces also into a LayerCallOutput, where the `space` property is set\n        # and `inputs` is [].\n        if len(inputs_list) > 0:\n            inputs_list = [\n                LayerCallOutput([], [], self, output_slot=i, num_outputs=len(inputs_list), space=s)\n                if isinstance(s, Space) else s\n                for i, s in enumerate(inputs_list)\n            ]\n        kwarg_strings = ["""" for _ in range(len(args))]+[k + ""="" for k in kwargs.keys()]\n\n        # Need to return as many return values as `call` returns.\n        num_outputs = self.graph_fn_num_outputs.get(""_graph_fn_call"", 1)\n        if num_outputs > 1:\n            siblings = [\n                LayerCallOutput(inputs_list, kwarg_strings, self, output_slot=i, num_outputs=num_outputs)\n                for i in range(num_outputs)\n            ]\n            for s in siblings:\n                s.inputs_needed.extend(siblings)\n                s.inputs_needed.remove(s)\n            return tuple(siblings)\n        else:\n            return LayerCallOutput(inputs_list, kwarg_strings, self)\n\n\nclass LayerCallOutput(object):\n    def __init__(self, inputs, kwarg_strings, component, output_slot=0, num_outputs=1, space=None):\n        """"""\n        Args:\n            inputs (list[LayerCallOutput,Space]): The inputs to the `call` method.\n            kwarg_strings (List[str]): The kwargs corresponding to each input in `inputs` (use """" for no kwarg).\n            component (Component): The Component, whose `call` method returns this output.\n            output_slot (int): The position in the return tuple of the call.\n            num_outputs (int): The overall number of return values that the call returns.\n            space(Optional[Space]): The Space this object represents iff at the input arg side of the call.\n        """"""\n        self.inputs = inputs\n        self.inputs_needed = copy.copy(self.inputs)\n        self.kwarg_strings = kwarg_strings\n        self.component = component\n        self.output_slot = output_slot\n        self.num_outputs = num_outputs\n        self.space = space\n\n        for i in self.inputs:\n            self.inputs_needed.extend(i.inputs_needed)\n\n        self.var_name = None\n\n    #def __eq__(self, other):\n    #    if other in self.inputs_needed or self.output_slot != other.output_slot:\n    #        return False\n    #    return True\n\n    def __lt__(self, other):\n        # If `self` is dependent on the `other`, put self first.\n        if other in self.inputs_needed:\n            return True\n        # Otherwise, sort by output-slot.\n        return self.output_slot < other.output_slot\n\n    def __gt__(self, other):\n        # If `self` is dependent on the `other`, put self first.\n        if other in self.inputs_needed:\n            return False\n        # Otherwise, sort by output-slot.\n        return self.output_slot > other.output_slot\n\n    #def __le__(self, other):\n    #    if other in self.inputs_needed:\n    #        return True\n    #    return self.output_slot <= other.output_slot\n\n    #def __ge__(self, other):\n    #    if other in self.inputs_needed:\n    #        return False\n    #    return self.output_slot <= other.output_slot\n'"
rlgraph/components/loss_functions/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph.components.loss_functions.loss_function import LossFunction\nfrom rlgraph.components.loss_functions.actor_critic_loss_function import ActorCriticLossFunction\nfrom rlgraph.components.loss_functions.categorical_cross_entropy_loss import CategoricalCrossEntropyLoss\nfrom rlgraph.components.loss_functions.container_loss_function import ContainerLossFunction\nfrom rlgraph.components.loss_functions.dqn_loss_function import DQNLossFunction\nfrom rlgraph.components.loss_functions.dqfd_loss_function import DQFDLossFunction\nfrom rlgraph.components.loss_functions.euclidian_distance_loss import EuclidianDistanceLoss\nfrom rlgraph.components.loss_functions.impala_loss_function import IMPALALossFunction\nfrom rlgraph.components.loss_functions.neg_log_likelihood_loss import NegativeLogLikelihoodLoss\nfrom rlgraph.components.loss_functions.ppo_loss_function import PPOLossFunction\nfrom rlgraph.components.loss_functions.sac_loss_function import SACLossFunction\n\nLossFunction.__lookup_classes__ = dict(\n    actorcriticlossfunction=ActorCriticLossFunction,\n    categoricalcrossentropy=CategoricalCrossEntropyLoss,\n    categoricalcrossentropyloss=CategoricalCrossEntropyLoss,\n    containerloss=ContainerLossFunction,\n    dqnlossfunction=DQNLossFunction,\n    dqfdlossfunction=DQFDLossFunction,\n    euclidiandistance=EuclidianDistanceLoss,\n    euclidiandistanceloss=EuclidianDistanceLoss,\n    impalalossfunction=IMPALALossFunction,\n    negativeloglikelihoodloss=NegativeLogLikelihoodLoss,\n    negativeloglikelihood=NegativeLogLikelihoodLoss,\n    negloglikelihoodloss=NegativeLogLikelihoodLoss,\n    negloglikelihood=NegativeLogLikelihoodLoss,\n    ppolossfunction=PPOLossFunction,\n    saclossfunction=SACLossFunction\n)\n\n__all__ = [""LossFunction""] + \\\n          list(set(map(lambda x: x.__name__, LossFunction.__lookup_classes__.values())))\n\n'"
rlgraph/components/loss_functions/actor_critic_loss_function.py,3,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.common.time_dependent_parameters import TimeDependentParameter\nfrom rlgraph.components.loss_functions import LossFunction\nfrom rlgraph.spaces import IntBox\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass ActorCriticLossFunction(LossFunction):\n    """"""\n    A basic actor critic policy gradient loss function, including entropy regularization and\n    generalized advantage estimation. Suitable for A2C, A3C etc.\n\n    The three terms of the loss function are:\n    1) The policy gradient term:\n        L[pg] = advantages * nabla log(pi(a|s)).\n    2) The value-function baseline term:\n        L[V] = 0.5 (vs - V(xs))^2, such that dL[V]/dtheta = (vs - V(xs)) nabla V(xs)\n    3) The entropy regularizer term:\n        L[E] = - SUM[all actions a] pi(a|s) * log pi(a|s)\n\n    """"""\n    def __init__(self, weight_pg=None, weight_vf=None, weight_entropy=None, **kwargs):\n        """"""\n        Args:\n            discount (float): The discount factor (gamma) to use.\n            gae_lambda (float): Optional GAE discount factor.\n            reward_clipping (Optional[str]): One of None, ""clamp_one"" or ""soft_asymmetric"". Default: ""clamp_one"".\n            weight_pg (float): The coefficient used for the policy gradient loss term (L[PG]).\n            weight_vf (float): The coefficient used for the value function term (L[V]).\n            weight_entropy (float): The coefficient used for the entropy regularization term (L[E]).\n                In the paper, values between 0.01 and 0.00005 are used via log-uniform search.\n        """"""\n        super(ActorCriticLossFunction, self).__init__(scope=kwargs.pop(""scope"", ""actor-critic-loss-func""), **kwargs)\n\n        self.weight_pg = TimeDependentParameter.from_spec(weight_pg if weight_pg is not None else 1.0,\n                                                          scope=""weight-pg"")\n        self.weight_vf = TimeDependentParameter.from_spec(\n            weight_vf if weight_vf is not None else 0.5, scope=""weight-vf""\n        )\n        self.weight_entropy = TimeDependentParameter.from_spec(\n            weight_entropy if weight_entropy is not None else 0.00025, scope=""weight-entropy""\n        )\n\n        self.add_components(self.weight_pg, self.weight_vf, self.weight_entropy)\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        assert action_space is not None\n        self.action_space = action_space\n        # Check for IntBox and num_categories.\n        sanity_check_space(\n            self.action_space, allowed_types=[IntBox], must_have_categories=True\n        )\n\n    @rlgraph_api\n    def loss(self, log_probs, values, rewards, entropy, time_percentage=None):\n        """"""\n        API-method that calculates the total loss (average over per-batch-item loss) from the original input to\n        per-item-loss.\n\n        Args: see `self._graph_fn_loss_per_item`.\n\n        Returns:\n            SingleDataOp: The tensor specifying the final loss (over the entire batch).\n        """"""\n        loss_per_item, vf_loss_per_item = self.loss_per_item(\n            log_probs, values, rewards, entropy, time_percentage\n        )\n        total_loss = self.loss_average(loss_per_item)\n        vf_total_loss = self.loss_average(vf_loss_per_item)\n\n        return total_loss, loss_per_item, vf_total_loss, vf_loss_per_item\n\n    @rlgraph_api\n    def loss_per_item(self, log_probs, state_values, advantages, entropy, time_percentage=None):\n        # Get losses for each action.\n        # Baseline loss for V(s) does not depend on actions, only on state.\n        vf_loss_per_item = self._graph_fn_state_value_function_loss_per_item(state_values, advantages, time_percentage)\n        loss_per_item = self._graph_fn_loss_per_item(log_probs, advantages, entropy, time_percentage)\n\n        # Average across actions.\n        loss_per_item = self._graph_fn_average_over_container_keys(loss_per_item)\n\n        return loss_per_item, vf_loss_per_item\n\n    @graph_fn(flatten_ops=True, split_ops=True)\n    def _graph_fn_loss_per_item(self, log_probs, advantages, entropy, time_percentage):\n        """"""\n        Calculates the loss per batch item (summed over all timesteps) using the formula described above in\n        the docstring to this class.\n\n        Args:\n            log_probs (DataOp): Log-likelihood of actions.\n            advantages (DataOp): The received rewards.\n            entropy (DataOp): Policy entropy\n\n        Returns:\n            SingleDataOp: The loss values per item in the batch, but summed over all timesteps.\n        """"""\n        if get_backend() == ""tf"":\n            # # Let the gae-helper function calculate the pg-advantages.\n            advantages = tf.stop_gradient(advantages)\n\n        elif get_backend() == ""pytorch"":\n            advantages = advantages.detach()\n\n        # The policy gradient loss.\n        loss = advantages * -log_probs\n        loss = self.weight_pg.get(time_percentage) * loss\n\n        # Subtract the entropy bonus from the loss (the larger the entropy the smaller the loss).\n        loss -= self.weight_entropy.get(time_percentage) * entropy\n        return loss\n\n    @rlgraph_api\n    def _graph_fn_state_value_function_loss_per_item(self, state_values, advantages, time_percentage=None):\n        """"""\n        Computes the loss for V(s).\n\n        Args:\n            state_values (SingleDataOp): Baseline predictions V(s).\n            advantages (SingleDataOp): Advantage values.\n\n        Returns:\n            SingleDataOp: Baseline loss per item.\n        """"""\n        v_targets = None\n        if get_backend() == ""tf"":\n            state_values = tf.squeeze(input=state_values, axis=-1)\n            v_targets = advantages + state_values\n            v_targets = tf.stop_gradient(input=v_targets)\n        elif get_backend() == ""pytorch"":\n            state_values = torch.squeeze(state_values, dim=-1)\n            v_targets = advantages + state_values\n            v_targets = v_targets.detach()\n\n        vf_loss = (v_targets - state_values) ** 2\n        return self.weight_vf.get(time_percentage) * vf_loss\n'"
rlgraph/components/loss_functions/categorical_cross_entropy_loss.py,12,"b'# Copyright 2018/2019 ducandu GmbH, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.loss_functions.supervised_loss_function import SupervisedLossFunction\nfrom rlgraph.spaces import IntBox, FloatBox\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.util import SMALL_NUMBER\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass CategoricalCrossEntropyLoss(SupervisedLossFunction):\n\n    def __init__(self, sparse=True, with_kl_regularizer=True, average_time_steps=False, scope=""cross-entropy-loss"",\n                 **kwargs):\n        """"""\n        Args:\n            sparse (bool): Whether we have sparse labels. Sparse labels can only assign one category to each\n                sample, so labels are ints. If False, labels are already softmaxed categorical distribution probs\n                OR simple logits.\n\n            average_time_steps (bool): Whether, if a time rank is given, to divide by th esequence lengths to get\n                the mean or not (leave as sum).\n        """"""\n        super(CategoricalCrossEntropyLoss, self).__init__(scope=scope, **kwargs)\n\n        self.sparse = sparse\n        self.with_kl_regularizer = with_kl_regularizer\n        self.average_time_steps = average_time_steps\n        #self.reduce_ranks = None\n\n        #self.time_rank = None\n        #self.time_major = None\n\n        #self.is_bool = None\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        labels_space = input_spaces[""labels""]\n        if self.sparse is True:\n            sanity_check_space(labels_space, allowed_types=IntBox, must_have_batch_rank=True)\n        else:\n            sanity_check_space(labels_space, allowed_types=FloatBox, must_have_batch_rank=True)\n\n    @rlgraph_api\n    def _graph_fn_loss_per_item(self, parameters, labels, sequence_length=None, time_percentage=None):\n        """"""\n        Supervised cross entropy classification loss.\n\n        Args:\n            parameters (SingleDataOp): The parameters output by a DistributionAdapter (before sampling from a\n                possible distribution).\n\n            labels (SingleDataOp): The corresponding labels (ideal probabilities) or int categorical labels.\n            sequence_length (SingleDataOp[int]): The lengths of each sequence (if applicable) in the given batch.\n\n            time_percentage (SingleDataOp[bool]): The time-percentage (0.0 to 1.0) with respect to the max number of\n                timesteps.\n\n        Returns:\n            SingleDataOp: The loss values vector (one single value for each batch item).\n        """"""\n        if get_backend() == ""tf"":\n            batch_rank = parameters._batch_rank\n            time_rank = 0 if batch_rank == 1 else 1\n\n            # TODO: This softmaxing is duplicate computation (waste) as `parameters` are already softmaxed.\n            if self.sparse is True:\n                cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=parameters)\n            else:\n                cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=parameters)\n\n            # TODO: Make it possible to customize the time-step decay (or increase?) behavior.\n            # Weight over time-steps (linearly decay weighting over time rank, cutting out entirely values past the\n            # sequence length).\n            if sequence_length is not None:\n                # Add KL Divergence between given distribution and uniform.\n                if self.with_kl_regularizer is True:\n                    uniform_probs = tf.fill(tf.shape(parameters), 1.0 / float(parameters.shape.as_list()[-1]))\n                    # Subtract KL-divergence from loss term such that\n                    kl = - tf.reduce_sum(uniform_probs * tf.log((tf.maximum(parameters, SMALL_NUMBER)) / uniform_probs), axis=-1)\n                    cross_entropy += kl\n\n                max_time_steps = tf.cast(tf.shape(labels)[time_rank], dtype=tf.float32)\n                sequence_mask = tf.sequence_mask(sequence_length, max_time_steps, dtype=tf.float32)\n                # no sequence decay anymore (no one does this):\n                # sequence_decay = tf.range(start=1.0, limit=0.0, delta=-1.0 / max_time_steps, dtype=tf.float32)\n                # sequence_decay = tf.range(start=0.5, limit=1.0, delta=0.5 / max_time_steps, dtype=tf.float32)\n                weighting = sequence_mask  # * sequence_decay\n                cross_entropy = tf.multiply(cross_entropy, weighting)\n\n                # Reduce away the time-rank.\n                cross_entropy = tf.reduce_sum(cross_entropy, axis=time_rank)\n                # Average?\n                if self.average_time_steps is True:\n                    cross_entropy = tf.divide(cross_entropy, tf.cast(sequence_length, dtype=tf.float32))\n            else:\n                # Reduce away the time-rank.\n                if hasattr(parameters, ""_time_rank""):\n                    cross_entropy = tf.reduce_sum(cross_entropy, axis=time_rank)\n\n            return cross_entropy\n'"
rlgraph/components/loss_functions/container_loss_function.py,0,"b'# Copyright 2018/2019 ducandu GmbH, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom rlgraph.components.loss_functions.loss_function import LossFunction\nfrom rlgraph.components.loss_functions.supervised_loss_function import SupervisedLossFunction\nfrom rlgraph.utils.decorators import rlgraph_api\n\n\nclass ContainerLossFunction(SupervisedLossFunction):\n    """"""\n    A loss function consisting of n sub-loss functions whose weighted sum is used as the total loss.\n    """"""\n    def __init__(self, loss_functions_spec, weights=None, scope=""mixture-loss"", **kwargs):\n        """"""\n        Args:\n            loss_functions_spec (Union[Dict[str,dict],Tuple[dict]]): A specification dict or tuple with values being\n                the spec dicts for the single loss functions. The `loss` methods expect a dict input or a single\n                tuple input (not as *args) in its first parameter.\n\n            weights (Optional[List[float]]): If given, sum over all sub loss function will be weighted.\n        """"""\n        super(ContainerLossFunction, self).__init__(scope=scope, **kwargs)\n\n        # Create all component loss functions and store each one\'s weight.\n        if isinstance(loss_functions_spec, dict):\n            weights_ = {}\n            self.loss_functions = {}\n            for i, (key, loss_fn_spec) in enumerate(loss_functions_spec.items()):\n                if weights is None and ""weight"" in loss_fn_spec:\n                    weights_[key] = loss_fn_spec.pop(""weight"")\n                # Change scope.\n                if isinstance(loss_fn_spec, LossFunction):\n                    loss_fn_spec.scope = loss_fn_spec.global_scope = loss_fn_spec.name = ""loss-function-{}"".format(i)\n                    loss_fn_spec.propagate_scope(None)\n                self.loss_functions[key] = LossFunction.from_spec(loss_fn_spec, scope=""loss-function-{}"".format(i))\n        else:\n            assert isinstance(loss_functions_spec, (list, tuple)),\\\n                ""ERROR: `loss_functions_spec` must be dict or tuple/list!""\n            weights_ = []\n            self.loss_functions = []\n            for i, loss_fn_spec in enumerate(loss_functions_spec):\n                if weights is None and ""weight"" in loss_fn_spec:\n                    weights_.append(loss_fn_spec.pop(""weight""))\n                # Change scope.\n                if isinstance(loss_fn_spec, LossFunction):\n                    loss_fn_spec.scope = loss_fn_spec.global_scope = loss_fn_spec.name = ""loss-function-{}"".format(i)\n                    loss_fn_spec.propagate_scope(None)\n                self.loss_functions.append(LossFunction.from_spec(loss_fn_spec, scope=""loss-function-{}"".format(i)))\n\n        # Weights were given per component loss? If no weights given at all, use 1.0 for all loss components.\n        if weights is None and len(weights_) > 0:\n            weights = weights_\n        self.weights = weights\n\n        # Add all sub-Components.\n        self.add_components(\n            *list(self.loss_functions.values() if isinstance(loss_functions_spec, dict) else self.loss_functions)\n        )\n\n    @rlgraph_api\n    def _graph_fn_loss_per_item(self, parameters, labels, sequence_length=None, time_percentage=None):\n        """"""\n        Args:\n            predictions (ContainerDataOp): The container parameters, each one represents the input for one of our sub\n                loss functions.\n\n            labels (ContainerDataOp): The container labels.\n            sequence_length: The lengths of each sequence (if applicable) in the given batch.\n        """"""\n        weighted_sum_loss_per_item = None\n        # Feed all inputs through their respective loss function and do the weighted sum.\n        if isinstance(self.loss_functions, dict):\n            for key, loss_fn in self.loss_functions.items():\n                loss_per_item = loss_fn.loss_per_item(parameters[key], labels[key], sequence_length, time_percentage)\n                if self.weights is not None:\n                    loss_per_item *= self.weights[key]\n                if weighted_sum_loss_per_item is None:\n                    weighted_sum_loss_per_item = loss_per_item\n                else:\n                    weighted_sum_loss_per_item += loss_per_item\n        else:\n            for i, loss_fn in enumerate(self.loss_functions):\n                loss_per_item = loss_fn.loss_per_item(parameters[i], labels[i], sequence_length, time_percentage)\n                if self.weights is not None:\n                    loss_per_item *= self.weights[i]\n                if weighted_sum_loss_per_item is None:\n                    weighted_sum_loss_per_item = loss_per_item\n                else:\n                    weighted_sum_loss_per_item += loss_per_item\n\n        return weighted_sum_loss_per_item\n'"
rlgraph/components/loss_functions/dqfd_loss_function.py,13,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.loss_functions.dqn_loss_function import DQNLossFunction\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.util import get_rank\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass DQFDLossFunction(DQNLossFunction):\n    """"""\n    The DQFD-loss extends the (dueling) DQN loss by a supervised loss to leverage expert demonstrations. Paper:\n\n    https://arxiv.org/abs/1704.03732\n\n    API:\n        loss_per_item(q_values_s, actions, rewards, terminals, qt_values_sp, q_values_sp=None): The DQN loss per batch\n            item.\n    """"""\n    def __init__(self,  supervised_weight=1.0, scope=""dqfd-loss-function"", **kwargs):\n        """"""\n        Args:\n            supervised_weight (float): Indicates weight of the expert loss.\n        """"""\n        super(DQFDLossFunction, self).__init__(scope=scope, **kwargs)\n\n        self.supervised_weight = supervised_weight\n\n    @rlgraph_api\n    def loss(self, q_values_s, actions, rewards, terminals, qt_values_sp, expert_margins,\n             q_values_sp=None, importance_weights=None, apply_demo_loss=False):\n        loss_per_item = self.loss_per_item(\n            q_values_s, actions, rewards, terminals, qt_values_sp, expert_margins, q_values_sp,\n            importance_weights, apply_demo_loss\n        )\n        total_loss = self.loss_average(loss_per_item)\n        return total_loss, loss_per_item\n\n    @rlgraph_api\n    def loss_per_item(self, q_values_s, actions, rewards, terminals, qt_values_sp, expert_margins, q_values_sp=None,\n                      importance_weights=None, apply_demo_loss=False):\n        # Get the targets per action.\n        td_targets = self._graph_fn_get_td_targets(rewards, terminals, qt_values_sp, q_values_sp)\n        # Average over container sub-actions.\n        if self.shared_container_action_target is True:\n            td_targets = self._graph_fn_average_over_container_keys(td_targets)\n\n        # Calculate the loss per item.\n        loss_per_item = self._graph_fn_loss_per_item(td_targets, q_values_s, actions, expert_margins,\n                                                     importance_weights, apply_demo_loss)\n        # Average over container sub-actions.\n        loss_per_item = self._graph_fn_average_over_container_keys(loss_per_item)\n\n        # Apply huber loss.\n        loss_per_item = self._graph_fn_apply_huber_loss_if_necessary(loss_per_item)\n\n        return loss_per_item\n\n    @graph_fn(flatten_ops=True, split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_loss_per_item(self, key, td_targets, q_values_s, actions, expert_margins,\n                                importance_weights=None, apply_demo_loss=False):\n        """"""\n        Args:\n            td_targets (SingleDataOp): The already calculated TD-target terms (r + gamma maxa\'Qt(s\',a\')\n                OR for double Q: r + gamma Qt(s\',argmaxa\'(Q(s\',a\'))))\n            q_values_s (SingleDataOp): The batch of Q-values representing the expected accumulated discounted returns\n                when in s and taking different actions a.\n            actions (SingleDataOp): The batch of actions that were actually taken in states s (from a memory).\n            importance_weights (Optional[SingleDataOp]): If \'self.importance_weights\' is True: The batch of weights to\n                apply to the losses.\n            apply_demo_loss (Optional[SingleDataOp]): If \'apply_demo_loss\' is True: The large-margin loss is applied.\n                Should be set to True when updating from demo data, False when updating from online data.\n            expert_margins (SingleDataOp): The expert margin enforces a distance in Q-values between expert action and\n                all other actions.\n        Returns:\n            SingleDataOp: The loss values vector (one single value for each batch item).\n        """"""\n        if get_backend() == ""tf"":\n            # Q(s,a) -> Use the Q-value of the action actually taken before.\n            one_hot = tf.one_hot(indices=actions, depth=self.flat_action_space[key].num_categories)\n            q_s_a_values = tf.reduce_sum(input_tensor=(q_values_s * one_hot), axis=-1)\n\n            # Calculate the TD-delta (targets - current estimate).\n            td_delta = td_targets - q_s_a_values\n\n            # Calculate the demo-loss.\n            #  J_E(Q) = max_a([Q(s, a_taken) + l(s, a_expert, a_taken)] - Q(s, a_expert)\n            mask = tf.ones_like(tensor=one_hot, dtype=tf.float32)\n            action_mask = mask - one_hot\n\n            # Margin mask: allow custom per-sample expert margins -> requires creating a margin matrix.\n            # Instead of applying the same margin to all samples, users can pass a margin vector.\n            # Broadcast to one hot shape\n            expert_margins = tf.expand_dims(expert_margins, -1)\n            expert_margins = tf.broadcast_to(input=expert_margins, shape=tf.shape(one_hot))\n            margin_mask = expert_margins - one_hot\n\n            # margin_mask = tf.Print(margin_mask, [margin_mask], summarize=100, message=""margin mask ="")\n            margin_val = action_mask * margin_mask\n            loss_input = q_values_s + margin_val\n\n            # Apply margin.\n            def map_margins(x):\n                element_margin = x[0]\n                element_loss = x[1]\n                # Positive margins: apply max.\n                # Negative margins: apply min.\n                return tf.cond(\n                    pred=tf.reduce_sum(element_margin) > 0,\n                    true_fn=lambda: tf.reduce_max(element_loss),\n                    false_fn=lambda: tf.reduce_min(element_loss),\n                )\n            supervised_loss = tf.map_fn(map_margins, (margin_val, loss_input), dtype=tf.float32)\n\n            # Subtract Q-values of action actually taken.\n            supervised_delta = supervised_loss - q_s_a_values\n            td_delta = tf.cond(\n                pred=apply_demo_loss,\n                true_fn=lambda: td_delta + self.supervised_weight * supervised_delta,\n                false_fn=lambda: td_delta\n            )\n\n            # Reduce over the composite actions, if any.\n            if get_rank(td_delta) > 1:\n                td_delta = tf.reduce_mean(input_tensor=td_delta, axis=list(range(1, self.ranks_to_reduce + 1)))\n\n            # Apply importance-weights from a prioritized replay to the loss.\n            if self.importance_weights:\n                return importance_weights * td_delta\n            else:\n                return td_delta\n'"
rlgraph/components/loss_functions/dqn_loss_function.py,16,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.loss_functions import LossFunction\nfrom rlgraph.spaces import IntBox\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils import pytorch_one_hot\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.pytorch_util import pytorch_tile\nfrom rlgraph.utils.util import get_rank\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass DQNLossFunction(LossFunction):\n    """"""\n    The classic 2015 DQN Loss Function [1] with options for ""double"" Q-losses [2], Huber loss [3], and container\n    actions [4]:\n\n    L = Expectation-over-uniform-batch(r + gamma x max_a\'Qt(s\',a\') - Qn(s,a))^2\n    Where Qn is the ""normal"" Q-network and Qt is the ""target"" net (which is a little behind Qn for stability purposes).\n\n    [1] Human-level control through deep reinforcement learning. Mnih, Kavukcuoglu, Silver et al. - 2015\n    [2] Deep Reinforcement Learning with Double Q-learning. v. Hasselt, Guez, Silver - 2015\n    [3] https://en.wikipedia.org/wiki/Huber_loss\n    [4] Action Branching Architectures for Deep Reinforcement Learning. Tavakoli, Pardo, and Kormushev - 2017\n    """"""\n    def __init__(self, double_q=False, huber_loss=False, importance_weights=False, n_step=1,\n                 shared_container_action_target=True, scope=""dqn-loss-function"", **kwargs):\n        """"""\n        Args:\n            double_q (bool): Whether to use the double DQN loss function ([2]).\n            huber_loss (bool): Whether to apply a huber loss correction ([3]).\n            importance_weights (bool): Where to use importance weights from a prioritized replay.\n            n_step (int): n-step adjustment to discounting.\n\n            shared_container_action_target (bool): Whether - only in the case of container actions - the target term\n                should be shared (average) over all action components\' single loss terms. Default: True.\n        """"""\n        self.double_q = double_q\n        self.huber_loss = huber_loss\n        assert n_step >= 1, ""Number of steps for n-step learning must be >= 1, is {}"".format(n_step)\n        # TODO reward must be preprocessed to work correctly for n-step.\n        # For Apex, this is done in the worker - do we want to move this as an in-graph option too?\n        self.n_step = n_step\n        self.shared_container_action_target = shared_container_action_target\n\n        # Clip value, see: https://en.wikipedia.org/wiki/Huber_loss\n        self.huber_delta = kwargs.get(""huber_delta"", 1.0)\n        self.importance_weights = importance_weights\n\n        super(DQNLossFunction, self).__init__(scope=scope, **kwargs)\n\n        self.flat_action_space = None\n        self.ranks_to_reduce = 0  # How many ranks do we have to reduce to get down to the final loss per batch item?\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        """"""\n        Do some sanity checking on the incoming Spaces:\n        """"""\n        assert action_space is not None\n        self.action_space = action_space\n        self.flat_action_space = action_space.flatten()\n        # Check for IntBox and num_categories.\n        sanity_check_space(self.action_space, must_have_categories=True, allowed_sub_types=[IntBox])\n        self.ranks_to_reduce = len(self.action_space.get_shape(with_batch_rank=True)) - 1\n\n    @rlgraph_api\n    def loss(self, q_values_s, actions, rewards, terminals, qt_values_sp, q_values_sp=None, importance_weights=None):\n        loss_per_item = self.loss_per_item(\n            q_values_s, actions, rewards, terminals, qt_values_sp, q_values_sp, importance_weights\n        )\n        total_loss = self.loss_average(loss_per_item)\n        return total_loss, loss_per_item\n\n    @rlgraph_api\n    def loss_per_item(self, q_values_s, actions, rewards, terminals, qt_values_sp, q_values_sp=None,\n                      importance_weights=None):\n        # Get the targets per action.\n        td_targets = self._graph_fn_get_td_targets(rewards, terminals, qt_values_sp, q_values_sp)\n        # Average over container sub-actions.\n        if self.shared_container_action_target is True:\n            td_targets = self._graph_fn_average_over_container_keys(td_targets)\n\n        # Calculate the loss per item.\n        loss_per_item = self._graph_fn_loss_per_item(td_targets, q_values_s, actions, importance_weights)\n        # Average over container sub-actions.\n        loss_per_item = self._graph_fn_average_over_container_keys(loss_per_item)\n\n        # Apply huber loss.\n        loss_per_item = self._graph_fn_apply_huber_loss_if_necessary(loss_per_item)\n\n        return loss_per_item\n\n    @graph_fn(flatten_ops=True, split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_get_td_targets(self, key, rewards, terminals, qt_values_sp, q_values_sp=None):\n        """"""\n        Args:\n            rewards (SingleDataOp): The batch of rewards that we received after having taken a in s (from a memory).\n            terminals (SingleDataOp): The batch of terminal signals that we received after having taken a in s\n                (from a memory).\n            qt_values_sp (SingleDataOp): The batch of Q-values representing the expected accumulated discounted\n                returns (estimated by the target net) when in s\' and taking different actions a\'.\n            q_values_sp (Optional[SingleDataOp]): If `self.double_q` is True: The batch of Q-values representing the\n                expected accumulated discounted returns (estimated by the (main) policy net) when in s\' and taking\n                different actions a\'.\n\n        Returns:\n            SingleDataOp: The target values vector.\n        """"""\n        qt_sp_ap_values = None\n\n        # Numpy backend primarily for testing purposes.\n        if self.backend == ""python"" or get_backend() == ""python"":\n            from rlgraph.utils.numpy import one_hot\n            if self.double_q:\n                a_primes = np.argmax(q_values_sp, axis=-1)\n                a_primes_one_hot = one_hot(a_primes, depth=self.flat_action_space[key].num_categories)\n                qt_sp_ap_values = np.sum(qt_values_sp * a_primes_one_hot, axis=-1)\n            else:\n                qt_sp_ap_values = np.max(qt_values_sp, axis=-1)\n\n            for _ in range(qt_sp_ap_values.ndim - 1):\n                rewards = np.expand_dims(rewards, axis=1)\n\n            qt_sp_ap_values = np.where(terminals, np.zeros_like(qt_sp_ap_values), qt_sp_ap_values)\n\n        elif get_backend() == ""tf"":\n            # Make sure the target policy\'s outputs are treated as constant when calculating gradients.\n            qt_values_sp = tf.stop_gradient(qt_values_sp)\n\n            if self.double_q:\n                # For double-Q, we no longer use the max(a\')Qt(s\'a\') value.\n                # Instead, the a\' used to get the Qt(s\'a\') is given by argmax(a\') Q(s\',a\') <- Q=q-net, not target net!\n                a_primes = tf.argmax(input=q_values_sp, axis=-1)\n\n                # Now lookup Q(s\'a\') with the calculated a\'.\n                one_hot = tf.one_hot(indices=a_primes, depth=self.flat_action_space[key].num_categories)\n                qt_sp_ap_values = tf.reduce_sum(input_tensor=(qt_values_sp * one_hot), axis=-1)\n            else:\n                # Qt(s\',a\') -> Use the max(a\') value (from the target network).\n                qt_sp_ap_values = tf.reduce_max(input_tensor=qt_values_sp, axis=-1)\n\n            # Make sure the rewards vector (batch) is broadcast correctly.\n            for _ in range(get_rank(qt_sp_ap_values) - 1):\n                rewards = tf.expand_dims(rewards, axis=1)\n\n            # Ignore Q(s\'a\') values if s\' is a terminal state. Instead use 0.0 as the state-action value for s\'a\'.\n            # Note that in that case, the next_state (s\') is not the correct next state and should be disregarded.\n            # See Chapter 3.4 in ""RL - An Introduction"" (2017 draft) by A. Barto and R. Sutton for a detailed analysis.\n            qt_sp_ap_values = tf.where(\n                condition=terminals, x=tf.zeros_like(qt_sp_ap_values), y=qt_sp_ap_values\n            )\n\n        elif get_backend() == ""pytorch"":\n            if not isinstance(terminals, torch.ByteTensor):\n                terminals = terminals.byte()\n            # Add batch dim in case of single sample.\n            if qt_values_sp.dim() == 1:\n                rewards = rewards.unsqueeze(-1)\n                terminals = terminals.unsqueeze(-1)\n                q_values_sp = q_values_sp.unsqueeze(-1)\n                qt_values_sp = qt_values_sp.unsqueeze(-1)\n\n            # Make sure the target policy\'s outputs are treated as constant when calculating gradients.\n            qt_values_sp = qt_values_sp.detach()\n            if self.double_q:\n                # For double-Q, we no longer use the max(a\')Qt(s\'a\') value.\n                # Instead, the a\' used to get the Qt(s\'a\') is given by argmax(a\') Q(s\',a\') <- Q=q-net, not target net!\n                a_primes = torch.argmax(q_values_sp, dim=-1, keepdim=True)\n\n                # Now lookup Q(s\'a\') with the calculated a\'.\n                one_hot = pytorch_one_hot(a_primes, depth=self.flat_action_space[key].num_categories)\n                qt_sp_ap_values = torch.sum(qt_values_sp * one_hot.squeeze(), dim=-1)\n            else:\n                # Qt(s\',a\') -> Use the max(a\') value (from the target network).\n                qt_sp_ap_values = torch.max(qt_values_sp, -1)[0]\n\n            # Make sure the rewards vector (batch) is broadcast correctly.\n            for _ in range(get_rank(qt_sp_ap_values) - 1):\n                rewards = torch.unsqueeze(rewards, dim=1)\n\n            # Ignore Q(s\'a\') values if s\' is a terminal state. Instead use 0.0 as the state-action value for s\'a\'.\n            # Note that in that case, the next_state (s\') is not the correct next state and should be disregarded.\n            # See Chapter 3.4 in ""RL - An Introduction"" (2017 draft) by A. Barto and R. Sutton for a detailed analysis.\n            # torch.where cannot broadcast here, so tile and reshape to same shape.\n            if qt_sp_ap_values.dim() > 1:\n                num_tiles = np.prod(qt_sp_ap_values.shape[1:])\n                terminals = pytorch_tile(terminals, num_tiles, -1).reshape(qt_sp_ap_values.shape)\n            qt_sp_ap_values = torch.where(\n                terminals, torch.zeros_like(qt_sp_ap_values), qt_sp_ap_values\n            )\n        td_targets = (rewards + (self.discount ** self.n_step) * qt_sp_ap_values)\n        return td_targets\n\n    @graph_fn(flatten_ops=True, split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_loss_per_item(self, key, td_targets, q_values_s, actions, importance_weights=None):\n        """"""\n        Args:\n            td_targets (SingleDataOp): The already calculated TD-target terms (r + gamma maxa\'Qt(s\',a\')\n                OR for double Q: r + gamma Qt(s\',argmaxa\'(Q(s\',a\'))))\n\n            q_values_s (SingleDataOp): The batch of Q-values representing the expected accumulated discounted returns\n                when in s and taking different actions a.\n\n            actions (SingleDataOp): The batch of actions that were actually taken in states s (from a memory).\n\n            importance_weights (Optional[SingleDataOp]): If \'self.importance_weights\' is True: The batch of weights to\n                apply to the losses.\n\n        Returns:\n            SingleDataOp: The loss values vector (one single value for each batch item).\n        """"""\n        # Numpy backend primarily for testing purposes.\n        if self.backend == ""python"" or get_backend() == ""python"":\n            from rlgraph.utils.numpy import one_hot\n\n            actions_one_hot = one_hot(actions, depth=self.flat_action_space[key].num_categories)\n            q_s_a_values = np.sum(q_values_s * actions_one_hot, axis=-1)\n\n            td_delta = td_targets - q_s_a_values\n\n            if td_delta.ndim > 1:\n                if self.importance_weights:\n                    td_delta = np.mean(\n                        td_delta * importance_weights,\n                        axis=list(range(1, self.ranks_to_reduce + 1))\n                    )\n\n                else:\n                    td_delta = np.mean(td_delta, axis=list(range(1, self.ranks_to_reduce + 1)))\n\n        elif get_backend() == ""tf"":\n            # Q(s,a) -> Use the Q-value of the action actually taken before.\n            one_hot = tf.one_hot(indices=actions, depth=self.flat_action_space[key].num_categories)\n            q_s_a_values = tf.reduce_sum(input_tensor=(q_values_s * one_hot), axis=-1)\n\n            # Calculate the TD-delta (target - current estimate).\n            td_delta = td_targets - q_s_a_values\n\n            # Reduce over the composite actions, if any.\n            if get_rank(td_delta) > 1:\n                td_delta = tf.reduce_mean(input_tensor=td_delta, axis=list(range(1, self.ranks_to_reduce + 1)))\n\n        elif get_backend() == ""pytorch"":\n            # Add batch dim in case of single sample.\n            if q_values_s.dim() == 1:\n                q_values_s = q_values_s.unsqueeze(-1)\n                actions = actions.unsqueeze(-1)\n                if self.importance_weights:\n                    importance_weights = importance_weights.unsqueeze(-1)\n\n            # Q(s,a) -> Use the Q-value of the action actually taken before.\n            one_hot = pytorch_one_hot(actions, depth=self.flat_action_space[key].num_categories)\n            q_s_a_values = torch.sum((q_values_s * one_hot), -1)\n\n            # Calculate the TD-delta (target - current estimate).\n            td_delta = td_targets - q_s_a_values\n\n            # Reduce over the composite actions, if any.\n            if get_rank(td_delta) > 1:\n                td_delta = torch.mean(td_delta, tuple(range(1, self.ranks_to_reduce + 1)), keepdim=False)\n\n        # Apply importance-weights from a prioritized replay to the loss.\n        if self.importance_weights:\n            return importance_weights * td_delta\n        else:\n            return td_delta\n\n    @graph_fn\n    def _graph_fn_apply_huber_loss_if_necessary(self, td_delta):\n        if self.backend == ""python"" or get_backend() == ""python"":\n            if self.huber_loss:\n                return np.where(\n                    condition=np.abs(td_delta) <= self.huber_delta,\n                    x=0.5 * np.square(td_delta),\n                    y=self.huber_delta * (np.abs(td_delta) - 0.5 * self.huber_delta)\n                )\n            else:\n                return 0.5 * np.square(x=td_delta)\n        elif get_backend() == ""tf"":\n            if self.huber_loss:\n                return tf.where(\n                    condition=tf.abs(x=td_delta) <= self.huber_delta,\n                    x=0.5 * tf.square(x=td_delta),\n                    y=self.huber_delta * (tf.abs(x=td_delta) - 0.5 * self.huber_delta)\n                )\n            else:\n                return 0.5 * tf.square(x=td_delta)\n        elif get_backend() == ""pytorch"":\n            if self.huber_loss:\n                # Not certain if arithmetics need to be expressed via torch operators.\n                return torch.where(\n                    torch.abs(td_delta) <= self.huber_delta,\n                    # PyTorch has no `square`\n                    0.5 * torch.pow(td_delta, 2),\n                    self.huber_delta * (torch.abs(td_delta) - 0.5 * self.huber_delta)\n                )\n            else:\n                return 0.5 * td_delta * td_delta\n'"
rlgraph/components/loss_functions/euclidian_distance_loss.py,13,"b'# Copyright 2018/2019 ducandu GmbH, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.loss_functions.supervised_loss_function import SupervisedLossFunction\nfrom rlgraph.spaces.bool_box import BoolBox\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass EuclidianDistanceLoss(SupervisedLossFunction):\n    """"""\n    Calculates the loss between two vectors (prediction and label) via their Euclidian distance:\n    d(v,w) = SQRT(SUMi( (vi - wi)\xc2\xb2 ))\n    """"""\n    def __init__(self, time_steps=None, scope=""euclidian-distance"", **kwargs):\n        """"""\n        Args:\n            time_steps (Optional[int]): If given, reduce-sum linearly over this many timesteps with weights going\n                from 0.0 (first time-step) to 1.0 (last-timestep).\n        """"""\n        super(EuclidianDistanceLoss, self).__init__(scope=scope, **kwargs)\n\n        self.time_steps = time_steps\n        self.reduce_ranks = None\n\n        self.time_rank = None\n        self.time_major = None\n\n        self.is_bool = None\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        in_space = input_spaces[""labels""]\n        self.is_bool = isinstance(in_space, BoolBox)  # Need to cast (to 0.0 and 1.0) in graph_fn?\n        self.reduce_ranks = np.array(list(range(in_space.rank)))\n        if in_space.has_batch_rank:\n            self.reduce_ranks += 1\n        if in_space.has_time_rank:\n            self.reduce_ranks += 1\n\n        self.time_rank = in_space.has_time_rank\n        self.time_major = in_space.time_major\n\n    @rlgraph_api\n    def _graph_fn_loss_per_item(self, parameters, labels, sequence_length=None, time_percentage=None):\n        """"""\n        Euclidian distance loss.\n\n        Args:\n            parameters (SingleDataOp): Output predictions.\n            labels (SingleDataOp): Labels.\n            sequence_length (SingleDataOp): The lengths of each sequence (if applicable) in the given batch.\n\n        Returns:\n            SingleDataOp: The loss values vector (one single value for each batch item).\n        """"""\n        batch_rank = 0 if self.time_major is False else 1\n        time_rank = 0 if batch_rank == 1 else 1\n\n        if get_backend() == ""tf"":\n            # Reduce over last rank (vector axis) and take the square root.\n            if self.is_bool:\n                labels = tf.cast(labels, tf.float32)\n                parameters = tf.cast(parameters, tf.float32)\n            euclidian_distance = tf.square(tf.subtract(parameters, labels))\n            euclidian_distance = tf.reduce_sum(euclidian_distance, axis=self.reduce_ranks)\n            euclidian_distance = tf.sqrt(euclidian_distance)\n\n            # TODO: Make it possible to customize the time-step decay (or increase?) behavior.\n            # Weight over time-steps (linearly decay weighting over time rank, cutting out entirely values past the\n            # sequence length).\n            if sequence_length is not None:\n                max_time_steps = tf.cast(tf.shape(labels)[time_rank], dtype=tf.float32)\n                sequence_mask = tf.sequence_mask(sequence_length, max_time_steps, dtype=tf.float32)\n                sequence_decay = tf.expand_dims(\n                    tf.range(start=1.0, limit=0.0, delta=-1.0 / max_time_steps, dtype=tf.float32), axis=batch_rank\n                )\n                weighting = sequence_mask * sequence_decay\n                euclidian_distance = tf.multiply(euclidian_distance, weighting)\n                # Reduce away the time-rank.\n                euclidian_distance = tf.reduce_sum(euclidian_distance, axis=time_rank)\n                euclidian_distance = tf.divide(euclidian_distance, tf.cast(sequence_length, dtype=tf.float32))\n            else:\n                # Reduce away the time-rank.\n                if hasattr(parameters, ""_time_rank""):\n                    euclidian_distance = tf.reduce_mean(euclidian_distance, axis=time_rank)\n\n            return euclidian_distance\n'"
rlgraph/components/loss_functions/impala_loss_function.py,20,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.helpers.v_trace_function import VTraceFunction\nfrom rlgraph.components.loss_functions import LossFunction\nfrom rlgraph.spaces import IntBox\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.util import get_rank\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass IMPALALossFunction(LossFunction):\n    """"""\n    The IMPALA loss function based on v-trace off-policy policy gradient corrections, described in detail in [1].\n\n    The three terms of the loss function are:\n    1) The policy gradient term:\n        L[pg] = (rho_pg * advantages) * nabla log(pi(a|s)), where (rho_pg * advantages)=pg_advantages in code below.\n    2) The value-function baseline term:\n        L[V] = 0.5 (vs - V(xs))^2, such that dL[V]/dtheta = (vs - V(xs)) nabla V(xs)\n    3) The entropy regularizer term:\n        L[E] = - SUM[all actions a] pi(a|s) * log pi(a|s)\n\n    [1] IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures - Espeholt, Soyer,\n        Munos et al. - 2018 (https://arxiv.org/abs/1802.01561)\n    """"""\n    def __init__(self, discount=0.99, reward_clipping=""clamp_one"",\n                 weight_pg=None, weight_baseline=None, weight_entropy=None, slice_actions=False,\n                 slice_rewards=False, **kwargs):\n        """"""\n        Args:\n            discount (float): The discount factor (gamma) to use.\n            reward_clipping (Optional[str]): One of None, ""clamp_one"" or ""soft_asymmetric"". Default: ""clamp_one"".\n            weight_pg (float): The coefficient used for the policy gradient loss term (L[PG]).\n            weight_baseline (float): The coefficient used for the Value-function baseline term (L[V]).\n\n            weight_entropy (float): The coefficient used for the entropy regularization term (L[E]).\n                In the paper, values between 0.01 and 0.00005 are used via log-uniform search.\n\n            slice_actions (bool): Whether to slice off the very first action coming in from the\n                caller. This must be True if actions/rewards are part of the state (via the keys ""previous_action"" and\n                ""previous_reward""). Default: False.\n\n            slice_rewards (bool): Whether to slice off the very first reward coming in from the\n                caller. This must be True if actions/rewards are part of the state (via the keys ""previous_action"" and\n                ""previous_reward""). Default: False.\n        """"""\n        super(IMPALALossFunction, self).__init__(scope=kwargs.pop(""scope"", ""impala-loss-func""), **kwargs)\n\n        self.discount = discount\n        self.v_trace_function = VTraceFunction()\n\n        self.reward_clipping = reward_clipping\n\n        self.weight_pg = weight_pg if weight_pg is not None else 1.0\n        self.weight_baseline = weight_baseline if weight_baseline is not None else 0.5\n        self.weight_entropy = weight_entropy if weight_entropy is not None else 0.00025\n\n        self.slice_actions = slice_actions\n        self.slice_rewards = slice_rewards\n\n        self.action_space = None\n\n        self.add_components(self.v_trace_function)\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        assert action_space is not None\n        self.action_space = action_space\n        # Check for IntBox and num_categories.\n        sanity_check_space(\n            self.action_space, allowed_types=[IntBox], must_have_categories=True\n        )\n\n    @rlgraph_api\n    def loss(self, logits_actions_pi, action_probs_mu, values, actions, rewards, terminals):\n        """"""\n        API-method that calculates the total loss (average over per-batch-item loss) from the original input to\n        per-item-loss.\n\n        Args: see `self._graph_fn_loss_per_item`.\n\n        Returns:\n            SingleDataOp: The tensor specifying the final loss (over the entire batch).\n        """"""\n        loss_per_item = self.loss_per_item(\n            logits_actions_pi, action_probs_mu, values, actions, rewards, terminals\n        )\n        total_loss = self.loss_average(loss_per_item)\n\n        return total_loss, loss_per_item\n\n    @rlgraph_api\n    def _graph_fn_loss_per_item(self, logits_actions_pi, action_probs_mu, values, actions,\n                                rewards, terminals):\n        """"""\n        Calculates the loss per batch item (summed over all timesteps) using the formula described above in\n        the docstring to this class.\n\n        Args:\n            logits_actions_pi (DataOp): The logits for all possible actions coming from the learner\'s\n                policy (pi). Dimensions are: (time+1) x batch x action-space+categories.\n                +1 b/c last-next-state (aka ""bootstrapped"" value).\n            action_probs_mu (DataOp): The probabilities for all actions coming from the\n                actor\'s policies (mu). Dimensions are: time x batch x action-space+categories.\n            values (DataOp): The state value estimates coming from baseline node of the learner\'s policy (pi).\n                Dimensions are: (time+1) x batch x 1.\n            actions (DataOp): The actually taken actions.\n                Both one-hot actions as well as discrete int actions are allowed.\n                Dimensions are: time x batch x (one-hot values)?.\n            rewards (DataOp): The received rewards. Dimensions are: time x batch.\n            terminals (DataOp): The observed terminal signals. Dimensions are: time x batch.\n\n        Returns:\n            SingleDataOp: The loss values per item in the batch, but summed over all timesteps.\n        """"""\n        if get_backend() == ""tf"":\n            values, bootstrapped_values = values[:-1], values[-1:]\n\n            logits_actions_pi = logits_actions_pi[:-1]\n            # Ignore very first actions/rewards (these are the previous ones only used as part of the state input\n            # for the network).\n            if self.slice_actions:\n                actions = actions[1:]\n            if self.slice_rewards:\n                rewards = rewards[1:]\n            # If already given as flat (e.g. cycled from previous_action via env-stepper) ->\n            # Need to revert as well here for v-trace function.\n            if actions.dtype == tf.float32:\n                actions_flat = actions\n                actions = tf.argmax(actions_flat, axis=2)\n            else:\n                actions_flat = tf.one_hot(actions, depth=self.action_space.num_categories)\n\n            # Discounts are simply 0.0, if there is a terminal, otherwise: `self.discount`.\n            discounts = tf.expand_dims(tf.to_float(~terminals) * self.discount, axis=-1, name=""discounts"")\n            # `clamp_one`: Clamp rewards between -1.0 and 1.0.\n            if self.reward_clipping == ""clamp_one"":\n                rewards = tf.clip_by_value(rewards, -1, 1, name=""reward-clipping"")\n            # `soft_asymmetric`: Negative rewards are less negative than positive rewards are positive.\n            elif self.reward_clipping == ""soft_asymmetric"":\n                squeezed = tf.tanh(rewards / 5.0)\n                rewards = tf.where(rewards < 0.0, 0.3 * squeezed, squeezed) * 5.0\n\n            # Let the v-trace  helper function calculate the v-trace values (vs) and the pg-advantages\n            # (already multiplied by rho_t_pg): A = rho_t_pg * (rt + gamma*vt - V(t)).\n            # Both vs and pg_advantages will block the gradient as they should be treated as constants by the gradient\n            # calculator of this loss func.\n            if get_rank(rewards) == 2:\n                rewards = tf.expand_dims(rewards, axis=-1)\n            vs, pg_advantages = self.v_trace_function.calc_v_trace_values(\n                logits_actions_pi, tf.log(action_probs_mu), actions, actions_flat, discounts, rewards, values,\n                bootstrapped_values\n            )\n\n            cross_entropy = tf.expand_dims(tf.nn.sparse_softmax_cross_entropy_with_logits(\n                labels=actions, logits=logits_actions_pi\n            ), axis=-1)\n\n            # Make sure vs and advantage values are treated as constants for the gradient calculation.\n            #vs = tf.stop_gradient(vs)\n            pg_advantages = tf.stop_gradient(pg_advantages)\n\n            # The policy gradient loss.\n            loss_pg = pg_advantages * cross_entropy\n            loss = tf.reduce_sum(loss_pg, axis=0)  # reduce over the time-rank\n            if self.weight_pg != 1.0:\n                loss = self.weight_pg * loss\n\n            # The value-function baseline loss.\n            loss_baseline = 0.5 * tf.square(x=tf.subtract(vs, values))\n            loss_baseline = tf.reduce_sum(loss_baseline, axis=0)  # reduce over the time-rank\n            loss += self.weight_baseline * loss_baseline\n\n            # The entropy regularizer term.\n            policy = tf.nn.softmax(logits=logits_actions_pi)\n            log_policy = tf.nn.log_softmax(logits=logits_actions_pi)\n            loss_entropy = tf.reduce_sum(-policy * log_policy, axis=-1, keepdims=True)\n            loss_entropy = -tf.reduce_sum(loss_entropy, axis=0)  # reduce over the time-rank\n            loss += self.weight_entropy * loss_entropy\n\n            return tf.squeeze(loss, axis=-1)\n'"
rlgraph/components/loss_functions/loss_function.py,3,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.spaces import ContainerSpace\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass LossFunction(Component):\n    """"""\n    A loss function component offers a simple interface into some error/loss calculation function.\n    """"""\n    def __init__(self, discount=0.98, **kwargs):\n        """"""\n        Args:\n            discount (float): The discount factor (gamma).\n        """"""\n        super(LossFunction, self).__init__(scope=kwargs.pop(""scope"", ""loss-function""), **kwargs)\n\n        self.discount = discount\n        self.action_space = None\n\n    @rlgraph_api\n    def loss(self, *inputs):\n        """"""\n        API-method that calculates the total loss (average over per-batch-item loss) from the original input to\n        per-item-loss.\n\n        Args:\n            see `self._graph_fn_loss_per_item`.\n\n        Returns:\n            Tuple (2x SingleDataOp):\n                - The tensor specifying the final loss (over the entire batch).\n                - The loss values vector (one single value for each batch item).\n        """"""\n        raise NotImplementedError\n\n    @rlgraph_api\n    def _graph_fn_loss_per_item(self, *inputs):\n        """"""\n        Returns the single loss values (one for each item in a batch).\n\n        Args:\n            *inputs (DataOpTuple): The various data that this function needs to calculate the loss.\n\n        Returns:\n            SingleDataOp: The tensor specifying the loss per item. The batch dimension of this tensor corresponds\n                to the number of items in the batch.\n        """"""\n        raise NotImplementedError\n\n    @rlgraph_api(must_be_complete=False)\n    def _graph_fn_loss_average(self, loss_per_item):\n        """"""\n        The actual loss function that an optimizer will try to minimize. This is usually the average over a batch.\n\n        Args:\n            loss_per_item (SingleDataOp): The output of our loss_per_item graph_fn.\n\n        Returns:\n            SingleDataOp: The final loss tensor holding the average loss over the entire batch.\n        """"""\n        if get_backend() == ""tf"":\n            return tf.reduce_mean(input_tensor=loss_per_item, axis=0)\n        elif get_backend() == ""pytorch"":\n            return torch.mean(loss_per_item, 0)\n\n    @graph_fn(flatten_ops=True)\n    def _graph_fn_average_over_container_keys(self, loss_per_item):\n        """"""\n        If action space is a container space, average losses for each action.\n        """"""\n        if isinstance(self.action_space, ContainerSpace):\n            if get_backend() == ""tf"":\n                loss_per_item = tf.stack(list(loss_per_item.values()))\n                loss_per_item = tf.reduce_mean(loss_per_item, axis=0)\n            elif get_backend() == ""pytorch"":\n                loss_per_item = torch.stack(list(loss_per_item.values()))\n                loss_per_item = torch.mean(loss_per_item, 0)\n\n        return loss_per_item\n'"
rlgraph/components/loss_functions/neg_log_likelihood_loss.py,7,"b'# Copyright 2018/2019 ducandu GmbH, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.distributions.distribution import Distribution\nfrom rlgraph.components.loss_functions.supervised_loss_function import SupervisedLossFunction\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass NegativeLogLikelihoodLoss(SupervisedLossFunction):\n    """"""\n    Calculates the negative log-likelihood loss by passing the labels through a given distribution\n    (parameterized by `predictions`) and inverting the sign.\n\n    L(params,labels) = -log(Dparams.pdf(labels))\n    Where:\n        Dparams: Parameterized distribution object.\n        pdf: Prob. density function of the distribution.\n    """"""\n    def __init__(self, distribution_spec, average_time_steps=False, scope=""negative-log-likelihood-loss"", **kwargs):\n        """"""\n        Args:\n            average_time_steps (bool): Whether, if a time rank is given, to divide by th esequence lengths to get\n                the mean or not (leave as sum).\n        """"""\n        super(NegativeLogLikelihoodLoss, self).__init__(scope=scope, **kwargs)\n\n        self.distribution = Distribution.from_spec(distribution_spec)\n        self.average_time_steps = average_time_steps\n\n        self.add_components(self.distribution)\n\n        #self.reduce_ranks = None\n\n        self.time_rank = None\n        self.time_major = None\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        in_space = input_spaces[""labels""]\n\n        #self.reduce_ranks = np.array(list(range(in_space.rank)))\n        #if in_space.has_batch_rank:\n        #    self.reduce_ranks += 1\n        #if in_space.has_time_rank:\n        #    self.reduce_ranks += 1\n\n        self.time_rank = in_space.has_time_rank\n        self.time_major = in_space.time_major\n\n    @rlgraph_api\n    def _graph_fn_loss_per_item(self, parameters, labels, sequence_length=None, time_percentage=None):\n        """"""\n        Args:\n            parameters (SingleDataOp): Output parameters for a distribution.\n            labels (SingleDataOp): Labels that will be passed through the pdf function of the distribution.\n            sequence_length (SingleDataOp): The lengths of each sequence (if applicable) in the given batch.\n            time_percentage (Optional[SingleDataOp]): The time_percentage of the update. May be used e.g. for decaying\n                some weight parameter.\n\n        Returns:\n            SingleDataOp: The loss values vector (one single value for each batch item).\n        """"""\n        batch_rank = 0 if self.time_major is False else 1\n        #time_rank = 0 if batch_rank == 1 else 1\n\n        params_space = next(iter(self.api_method_inputs[""parameters""].flatten().values()))\n        num_ranks_to_keep = 2 if params_space.has_time_rank else 1\n\n        if get_backend() == ""tf"":\n            # Get the distribution\'s log-likelihood for the labels, given the parameterized distribution.\n            neg_log_likelihood = -self.distribution.log_prob(parameters, labels)\n\n            # If necessary, reduce over all non-batch/non-time ranks.\n            neg_log_likelihood = tf.reduce_sum(\n                neg_log_likelihood,\n                axis=list(range(len(neg_log_likelihood.shape) - 1, num_ranks_to_keep - 1, -1))\n            )\n\n            # TODO: Here, we use no time-decay and just sum up the valid time-steps.\n            #if sequence_length is not None:\n            #    max_time_steps = tf.cast(tf.shape(labels)[time_rank], dtype=tf.float32)\n            #    sequence_mask = tf.sequence_mask(sequence_length, max_time_steps, dtype=tf.float32)\n            #    neg_log_likelihoods = tf.multiply(neg_log_likelihoods, sequence_mask)\n            #    # Reduce away the time-rank.\n            #    neg_log_likelihoods = tf.reduce_sum(neg_log_likelihoods, axis=time_rank)\n            #    # Average?\n            #    if self.average_time_steps is True:\n            #        neg_log_likelihoods = tf.divide(neg_log_likelihoods, tf.cast(sequence_length, dtype=tf.float32))\n            ## Reduce away the time-rank.\n            #elif self.time_rank:\n            #    neg_log_likelihoods = tf.reduce_mean(neg_log_likelihoods, axis=time_rank)\n\n            neg_log_likelihood._batch_rank = batch_rank\n\n            return neg_log_likelihood\n'"
rlgraph/components/loss_functions/ppo_loss_function.py,11,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.common.time_dependent_parameters import TimeDependentParameter\nfrom rlgraph.components.loss_functions import LossFunction\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.util import get_rank\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass PPOLossFunction(LossFunction):\n    """"""\n    Loss function for proximal policy optimization:\n\n    https://arxiv.org/abs/1707.06347\n    """"""\n    def __init__(self, clip_ratio=0.2, value_function_clipping=None, weight_entropy=None,\n                 scope=""ppo-loss-function"", **kwargs):\n        """"""\n        Args:\n            clip_ratio (Spec[TimeDependentParameter]): How much to clip the likelihood ratio between old and new policy when\n                updating.\n\n            value_function_clipping (float): Clipping value for the ValueFunction component.\n                If None, no clipping is applied.\n\n            weight_entropy (Optional[Spec[TimeDependentParameter]]): The weight with which to multiply the entropy and subtract\n                from the loss.\n        """"""\n        super(PPOLossFunction, self).__init__(scope=scope, **kwargs)\n\n        self.clip_ratio = TimeDependentParameter.from_spec(clip_ratio, scope=""clip-ratio"")\n        self.weight_entropy = TimeDependentParameter.from_spec(\n            weight_entropy if weight_entropy is not None else 0.00025, scope=""weight-entropy""\n        )\n        self.value_function_clipping = value_function_clipping\n\n        self.add_components(self.clip_ratio, self.weight_entropy)\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        """"""\n        Do some sanity checking on the incoming Spaces:\n        """"""\n        assert action_space is not None\n        self.action_space = action_space.with_batch_rank()\n\n    @rlgraph_api\n    def loss(self, log_probs, prev_log_probs, state_values, prev_state_values, advantages, entropy, time_percentage):\n        """"""\n        API-method that calculates the total loss (average over per-batch-item loss) from the original input to\n        per-item-loss.\n\n        Args: see `self._graph_fn_loss_per_item`.\n\n        Returns:\n            Total loss, loss per item, total value-function loss, value-function loss per item.\n        """"""\n        loss_per_item, vf_loss_per_item = self.loss_per_item(\n            log_probs, prev_log_probs, state_values, prev_state_values, advantages, entropy, time_percentage\n        )\n        total_loss = self.loss_average(loss_per_item)\n        total_vf_loss = self.loss_average(vf_loss_per_item)\n\n        return total_loss, loss_per_item, total_vf_loss, vf_loss_per_item\n\n    @rlgraph_api\n    def loss_per_item(self, log_probs, prev_log_probs, state_values, prev_state_values, advantages,\n                      entropy, time_percentage):\n        # Get losses for each action.\n        # Baseline loss for V(s) does not depend on actions, only on state.\n        pg_loss_per_item = self.pg_loss_per_item(log_probs, prev_log_probs, advantages, entropy, time_percentage)\n        vf_loss_per_item = self.value_function_loss_per_item(state_values, prev_state_values, advantages)\n\n        # Average PG-loss across action components.\n        pg_loss_per_item = self._graph_fn_average_over_container_keys(pg_loss_per_item)\n\n        return pg_loss_per_item, vf_loss_per_item\n\n    @rlgraph_api(flatten_ops=True, split_ops=True)\n    def _graph_fn_pg_loss_per_item(self, log_probs, prev_log_probs, advantages, entropy, time_percentage):\n        """"""\n        Args:\n            log_probs (SingleDataOp): Log-likelihoods of actions under policy.\n            prev_log_probs (SingleDataOp) Log-likelihoods of actions under policy before this update step.\n            advantages (SingleDataOp): The batch of post-processed generalized advantage estimations (GAEs).\n            entropy (SingleDataOp): Policy entropy.\n\n        Returns:\n            SingleDataOp: The loss values vector (one single value for each batch item).\n        """"""\n        if get_backend() == ""tf"":\n            # N.b.: Many implementations do the following:\n            # Sample action -> return policy log probs with action -> feed both back in from memory/via placeholders.\n            # This creates the same effect as just stopping the gradients on the log-probs.\n            # Saving them would however remove necessity for an extra forward pass.\n            # Likelihood ratio and clipped objective.\n            ratio = tf.exp(x=log_probs - prev_log_probs)\n\n            # Make sure the pg_advantages vector (batch) is broadcast correctly.\n            for _ in range(get_rank(ratio) - 1):\n                advantages = tf.expand_dims(advantages, axis=-1)\n\n            clipped_advantages = tf.where(\n                condition=advantages > 0,\n                x=(1 + self.clip_ratio.get(time_percentage)) * advantages,\n                y=(1 - self.clip_ratio.get(time_percentage)) * advantages\n            )\n            #clipped_advantages = tf.clip_by_value(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * advantages\n            loss = -tf.minimum(x=ratio * advantages, y=clipped_advantages)\n\n            # Subtract the entropy bonus from the loss (the larger the entropy the smaller the loss).\n            loss -= self.weight_entropy.get(time_percentage) * entropy\n\n            # Reduce over the composite actions, if any.\n            if get_rank(ratio) > 1:\n                loss = tf.reduce_mean(loss, axis=list(range(1, get_rank(ratio))))\n\n            return loss\n\n        elif get_backend() == ""pytorch"":\n            # Likelihood ratio and clipped objective.\n            ratio = torch.exp(log_probs - prev_log_probs)\n\n            # Make sure the pg_advantages vector (batch) is broadcast correctly.\n            for _ in range(get_rank(ratio) - 1):\n                advantages = torch.unsqueeze(advantages, -1)\n\n            clipped_advantages = torch.where(\n                advantages > 0,\n                (1 + self.clip_ratio.get(time_percentage)) * advantages,\n                (1 - self.clip_ratio.get(time_percentage)) * advantages\n            )\n            #clipped_advantages = torch.clamp(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * advantages\n            loss = -torch.min(ratio * advantages, clipped_advantages)\n\n            # Subtract the entropy bonus from the loss (the larger the entropy the smaller the loss).\n            loss -= self.weight_entropy.get(time_percentage) * entropy\n\n            # Reduce over the composite actions, if any.\n            if get_rank(ratio) > 1:\n                loss = torch.mean(loss, tuple(range(1, get_rank(ratio))), keepdim=False)\n\n            return loss\n\n    @rlgraph_api\n    def _graph_fn_value_function_loss_per_item(self, state_values, prev_state_values, advantages):\n        """"""\n        Computes the loss for V(s).\n\n        Args:\n            state_values (SingleDataOp): State value predictions V(s).\n            prev_state_values (SingleDataOp): Previous state value predictions V(s) (before the update).\n            advantages (SingleDataOp): GAE (advantage) values.\n\n        Returns:\n            SingleDataOp: Value function loss per item.\n        """"""\n        if get_backend() == ""tf"":\n            state_values = tf.squeeze(input=state_values, axis=-1)\n            prev_state_values = tf.squeeze(input=prev_state_values, axis=-1)\n            v_targets = advantages + prev_state_values\n            v_targets = tf.stop_gradient(input=v_targets)\n            vf_loss = (state_values - v_targets) ** 2\n            if self.value_function_clipping:\n                vf_clipped = prev_state_values + tf.clip_by_value(\n                    state_values - prev_state_values, -self.value_function_clipping, self.value_function_clipping\n                )\n                clipped_loss = (vf_clipped - v_targets) ** 2\n                return tf.maximum(vf_loss, clipped_loss)\n            else:\n                return vf_loss\n\n        elif get_backend() == ""pytorch"":\n            state_values = torch.squeeze(state_values, dim=-1)\n            prev_state_values = torch.squeeze(input=prev_state_values, dim=-1)\n            v_targets = advantages + prev_state_values\n            v_targets = v_targets.detach()\n            vf_loss = (state_values - v_targets) ** 2\n            if self.value_function_clipping:\n                vf_clipped = prev_state_values + torch.clamp(\n                    state_values - prev_state_values, -self.value_function_clipping, self.value_function_clipping\n                )\n                clipped_loss = (vf_clipped - v_targets) ** 2\n                return torch.max(vf_loss, clipped_loss)\n            else:\n                return vf_loss\n'"
rlgraph/components/loss_functions/sac_loss_function.py,18,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.loss_functions.loss_function import LossFunction\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass SACLossFunction(LossFunction):\n    """"""\n    TODO: docs\n    """"""\n    def __init__(self, target_entropy=None, discount=0.99, num_q_functions=2, scope=""sac-loss-function"", **kwargs):\n        super(SACLossFunction, self).__init__(discount=discount, scope=scope, **kwargs)\n        self.num_q_functions = num_q_functions\n        self.target_entropy = target_entropy\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        # All the following need a batch rank.\n        self.action_space = action_space\n        for in_space_name in [""log_probs_sampled"", ""log_probs_next_sampled"", ""q_values"", ""q_values_sampled"",\n                              ""q_values_next_sampled"", ""rewards"", ""terminals""]:\n            in_space = input_spaces[in_space_name]\n            sanity_check_space(in_space, must_have_batch_rank=True, must_have_time_rank=False)\n\n        # All the following need shape==().\n        for in_space_name in [""alpha"", ""rewards"", ""terminals""]:\n            in_space = input_spaces[in_space_name]\n            sanity_check_space(in_space, shape=())\n\n        # All the following need shape==(1,).\n        for in_space_name in [""q_values"", ""q_values_sampled"", ""q_values_next_sampled""]:\n            in_space = input_spaces[in_space_name]\n            sanity_check_space(in_space, shape=(1,))\n\n    @rlgraph_api\n    def loss(self, alpha, log_probs_next_sampled, q_values_next_sampled, q_values, log_probs_sampled,\n             q_values_sampled, rewards, terminals):\n        actor_loss_per_item, critic_loss_per_item, alpha_loss_per_item = self.loss_per_item(\n            alpha, log_probs_next_sampled, q_values_next_sampled, q_values, log_probs_sampled,\n            q_values_sampled, rewards, terminals\n        )\n\n        # Average across batch.\n        actor_loss = self.loss_average(actor_loss_per_item)\n        critic_loss = self.loss_average(critic_loss_per_item)\n        alpha_loss = self.loss_average(alpha_loss_per_item)\n        return actor_loss, actor_loss_per_item, critic_loss, critic_loss_per_item, alpha_loss, alpha_loss_per_item\n\n    @rlgraph_api\n    def loss_per_item(self, alpha, log_probs_next_sampled, q_values_next_sampled, q_values, log_probs_sampled,\n                      q_values_sampled, rewards, terminals):\n        critic_loss_per_item = self._graph_fn_critic_loss(log_probs_next_sampled, q_values_next_sampled,\n            q_values, rewards, terminals, alpha)\n        critic_loss_per_item = self._graph_fn_average_over_container_keys(critic_loss_per_item)\n\n        actor_loss_per_item = self._graph_fn_actor_loss(log_probs_sampled, q_values_sampled, alpha)\n        actor_loss_per_item = self._graph_fn_average_over_container_keys(actor_loss_per_item)\n\n        alpha_loss_per_item = self._graph_fn_alpha_loss(log_probs_sampled, alpha)\n        alpha_loss_per_item = self._graph_fn_average_over_container_keys(alpha_loss_per_item)\n\n        return actor_loss_per_item, critic_loss_per_item, alpha_loss_per_item\n\n    @graph_fn(flatten_ops={0}, split_ops=True)\n    def _graph_fn_critic_loss(self, log_probs_next_sampled, q_values_next_sampled, q_values, rewards, terminals, alpha):\n        # In case log_probs come in as shape=(), expand last rank to 1.\n        if log_probs_next_sampled.shape.as_list()[-1] is None:\n            log_probs_next_sampled = tf.expand_dims(log_probs_next_sampled, axis=-1)\n\n        log_probs_next_sampled = tf.reduce_sum(log_probs_next_sampled, axis=1, keepdims=True)\n        rewards = tf.expand_dims(rewards, axis=-1)\n        terminals = tf.expand_dims(terminals, axis=-1)\n\n        q_min_next = tf.reduce_min(tf.concat(q_values_next_sampled, axis=1), axis=1, keepdims=True)\n        assert q_min_next.shape.as_list() == [None, 1]\n        soft_state_value = q_min_next - alpha * log_probs_next_sampled\n        q_target = rewards + self.discount * (1.0 - tf.cast(terminals, tf.float32)) * soft_state_value\n        total_loss = 0.0\n        if self.num_q_functions < 2:\n            q_values = [q_values]\n        for i, q_value in enumerate(q_values):\n            loss = 0.5 * (q_value - tf.stop_gradient(q_target)) ** 2\n            loss = tf.identity(loss, ""critic_loss_per_item_{}"".format(i + 1))\n            total_loss += loss\n        return tf.squeeze(total_loss, axis=1)\n\n    @graph_fn(flatten_ops={0}, split_ops=True)\n    def _graph_fn_actor_loss(self, log_probs_sampled, q_values_sampled, alpha):\n        if log_probs_sampled.shape.as_list()[-1] is None:\n            log_probs_sampled = tf.expand_dims(log_probs_sampled, axis=-1)\n        log_probs_sampled = tf.reduce_sum(log_probs_sampled, axis=1, keepdims=True)\n\n        q_min = tf.reduce_min(tf.concat(q_values_sampled, axis=1), axis=1, keepdims=True)\n        assert q_min.shape.as_list() == [None, 1]\n        loss = alpha * log_probs_sampled - q_min\n        loss = tf.identity(loss, ""actor_loss_per_item"")\n        return tf.squeeze(loss, axis=1)\n\n    @graph_fn(flatten_ops=True, split_ops=True)\n    def _graph_fn_alpha_loss(self, log_probs_sampled, alpha):\n        if self.target_entropy is None:\n            return tf.zeros([tf.shape(log_probs_sampled)[0]])\n        else:\n            # in the paper this is -alpha * (log_pi + target entropy), however the implementation uses log_alpha\n            # see the discussion in https://github.com/rail-berkeley/softlearning/issues/37\n            loss = -tf.log(alpha) * tf.stop_gradient(log_probs_sampled + self.target_entropy)\n            loss = tf.identity(loss, ""alpha_loss_per_item"")\n            return tf.squeeze(loss)\n'"
rlgraph/components/loss_functions/supervised_loss_function.py,0,"b'# Copyright 2018/2019 ducandu GmbH, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom rlgraph.components.loss_functions.loss_function import LossFunction\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api\n\n\nclass SupervisedLossFunction(LossFunction):\n    """"""\n    Calculates a loss based on predictions and labels.\n    Should also optionally support time-ranked data.\n    """"""\n    def __init__(self, scope=""supervised-loss-function"", **kwargs):\n        super(SupervisedLossFunction, self).__init__(scope=scope, **kwargs)\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        sanity_check_space(input_spaces[""parameters""], must_have_batch_rank=True)\n        sanity_check_space(input_spaces[""labels""], must_have_batch_rank=True)\n\n    @rlgraph_api\n    def loss(self, parameters, labels, sequence_length=None, time_percentage=None):\n        loss_per_item = self.loss_per_item(parameters, labels, sequence_length=sequence_length,\n                                           time_percentage=time_percentage)\n        total_loss = self.loss_average(loss_per_item)\n        return total_loss, loss_per_item\n'"
rlgraph/components/memories/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.memories.memory import Memory\nfrom rlgraph.components.memories.fifo_queue import FIFOQueue\nfrom rlgraph.components.memories.prioritized_replay import PrioritizedReplay\nfrom rlgraph.components.memories.replay_memory import ReplayMemory\nfrom rlgraph.components.memories.ring_buffer import RingBuffer\nfrom rlgraph.components.memories.mem_prioritized_replay import MemPrioritizedReplay\n\n# TODO backend reorg.\nif get_backend() == ""tf"":\n    Memory.__lookup_classes__ = dict(\n        fifo=FIFOQueue,\n        fifoqueue=FIFOQueue,\n        prioritized=PrioritizedReplay,\n        prioritizedreplay=PrioritizedReplay,\n        prioritizedreplaybuffer=PrioritizedReplay,\n        mem_prioritized_replay=MemPrioritizedReplay,\n        replay=ReplayMemory,\n        replaybuffer=ReplayMemory,\n        replaymemory=ReplayMemory,\n        ringbuffer=RingBuffer\n    )\nelif get_backend() == ""pytorch"":\n    Memory.__lookup_classes__ = dict(\n        prioritized=MemPrioritizedReplay,\n        prioritizedreplay=MemPrioritizedReplay,\n        prioritizedreplaybuffer=MemPrioritizedReplay,\n        mem_prioritized_replay=MemPrioritizedReplay,\n        replay=ReplayMemory,\n        replaybuffer=ReplayMemory,\n        replaymemory=ReplayMemory,\n        ringbuffer=RingBuffer\n    )\nMemory.__default_constructor__ = ReplayMemory\n\n__all__ = [""Memory"", ""PrioritizedReplay""] + \\\n          list(set(map(lambda x: x.__name__, Memory.__lookup_classes__.values())))\n\n'"
rlgraph/components/memories/fifo_queue.py,2,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.memories.memory import Memory\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.ops import FlattenedDataOp, flatten_op\nfrom rlgraph.utils.util import convert_dtype as dtype_\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass FIFOQueue(Memory):\n    """"""\n    A wrapper for a simple in-graph FIFOQueue.\n    """"""\n    def __init__(self, record_space=None, only_insert_single_records=False, **kwargs):\n        """"""\n        Args:\n            record_space (Space): The Space of a single record to be pushed to or pulled from the queue.\n\n            only_insert_single_records (bool): Whether insertion will always only happen with single records.\n                If True, will add a batch=1 rank to each to-be-inserted sample.\n        """"""\n        super(FIFOQueue, self).__init__(scope=kwargs.pop(""scope"", ""fifo-queue""), **kwargs)\n\n        # The record Space must be provided for clients of the Queue that only use it for retrieving records, but never\n        # inserting any. This way, RLgraph cannot infer the input space itself.\n        self.record_space = record_space\n\n        self.only_insert_single_records = only_insert_single_records\n        # Holds the actual backend-specific queue object.\n        self.queue = None\n\n        # If record space given, overwrite the insert method as ""must_be_complete=False"".\n        if self.record_space is not None:\n            @rlgraph_api(component=self, must_be_complete=False, ok_to_overwrite=True)\n            def _graph_fn_insert_records(self, records):\n                flattened_records = flatten_op(records)\n                flattened_stopped_records = {key: tf.stop_gradient(op) for key, op in flattened_records.items()}\n                # Records is just one record.\n                if self.only_insert_single_records is True:\n                    return self.queue.enqueue(flattened_stopped_records)\n                # Insert many records (with batch rank).\n                else:\n                    return self.queue.enqueue_many(flattened_stopped_records)\n\n    def create_variables(self, input_spaces, action_space=None):\n        # Overwrite parent\'s method as we don\'t need a custom registry.\n        if self.record_space is None:\n            self.record_space = input_spaces[""records""]\n\n        # Make sure all input-records have a batch rank and determine the shapes and dtypes.\n        shapes = []\n        dtypes = []\n        names = []\n        for key, value in self.record_space.flatten().items():\n            # TODO: what if single items come in without a time-rank? Then this check here will fail.\n            # We are expecting single items. The incoming batch-rank is actually a time-rank: Add the batch rank.\n            sanity_check_space(value, must_have_batch_rank=self.only_insert_single_records is False)\n            shape = value.get_shape(with_time_rank=value.has_time_rank)\n            shapes.append(shape)\n            dtypes.append(dtype_(value.dtype))\n            names.append(key)\n\n        # Construct the wrapped FIFOQueue object.\n        if get_backend() == ""tf"":\n            if self.reuse_variable_scope:\n                shared_name = self.reuse_variable_scope + (""/"" + self.scope if self.scope else """")\n            else:\n                shared_name = self.global_scope\n\n            self.queue = tf.FIFOQueue(\n                capacity=self.capacity,\n                dtypes=dtypes,\n                shapes=shapes,\n                names=names,\n                shared_name=shared_name\n            )\n\n    @rlgraph_api\n    def _graph_fn_get_records(self, num_records=1):\n        # Get the records as dict.\n        record_dict = self.queue.dequeue_many(num_records)\n        # Return a FlattenedDataOp.\n        flattened_records = FlattenedDataOp(record_dict)\n        # Add batch and (possible) time rank to output ops for the auto-Space-inference.\n        flat_record_space = self.record_space.flatten()\n        for flat_key, op in record_dict.items():\n            if flat_record_space[flat_key].has_time_rank:\n                op._batch_rank = 0\n                op._time_rank = 1\n                flattened_records[flat_key] = op\n            else:\n                op._batch_rank = 0\n                flattened_records[flat_key] = op\n        return flattened_records\n\n    @rlgraph_api\n    def _graph_fn_get_size(self):\n        """"""\n        Returns the current size of the queue.\n\n        Returns:\n            DataOp: The current size of the queue (how many items are in it).\n        """"""\n        return self.queue.size()\n'"
rlgraph/components/memories/mem_prioritized_replay.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport operator\n\nimport numpy as np\nfrom six.moves import xrange as range_\nfrom rlgraph import get_backend\nfrom rlgraph.utils import util, DataOpDict\nfrom rlgraph.utils.define_by_run_ops import define_by_run_unflatten\nfrom rlgraph.utils.util import SMALL_NUMBER, get_rank\nfrom rlgraph.components.memories.memory import Memory\nfrom rlgraph.components.helpers.mem_segment_tree import MemSegmentTree, MinSumSegmentTree\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""pytorch"":\n    import torch\n\n\nclass MemPrioritizedReplay(Memory):\n    """"""\n    Implements an in-memory  prioritized replay.\n\n    API:\n        update_records(indices, update) -> Updates the given indices with the given priority scores.\n    """"""\n    def __init__(self, capacity=1000, next_states=True, alpha=1.0, beta=0.0):\n        super(MemPrioritizedReplay, self).__init__()\n\n        self.memory_values = []\n        self.index = 0\n        self.capacity = capacity\n\n        self.size = 0\n        self.max_priority = 1.0\n        self.alpha = alpha\n        self.beta = beta\n        self.next_states = next_states\n\n        self.default_new_weight = np.power(self.max_priority, self.alpha)\n\n    def create_variables(self, input_spaces, action_space=None):\n        super(MemPrioritizedReplay, self).create_variables(input_spaces, action_space)\n        self.priority_capacity = 1\n        while self.priority_capacity < self.capacity:\n            self.priority_capacity *= 2\n\n        # Create segment trees, initialize with neutral elements.\n        sum_values = [0.0 for _ in range_(2 * self.priority_capacity)]\n        sum_segment_tree = MemSegmentTree(sum_values, self.priority_capacity, operator.add)\n        min_values = [float(\'inf\') for _ in range_(2 * self.priority_capacity)]\n        min_segment_tree = MemSegmentTree(min_values, self.priority_capacity, min)\n\n        self.merged_segment_tree = MinSumSegmentTree(\n            sum_tree=sum_segment_tree,\n            min_tree=min_segment_tree,\n            capacity=self.priority_capacity\n        )\n\n    @rlgraph_api(flatten_ops=True)\n    def _graph_fn_insert_records(self, records):\n        if records is None or get_rank(records[self.terminal_key]) == 0:\n            return\n        num_records = len(records[self.terminal_key])\n\n        if num_records == 1:\n            if self.index >= self.size:\n                self.memory_values.append(records)\n            else:\n                self.memory_values[self.index] = records\n            self.merged_segment_tree.insert(self.index, self.default_new_weight)\n        else:\n            insert_indices = np.arange(start=self.index, stop=self.index + num_records) % self.capacity\n            i = 0\n            for insert_index in insert_indices:\n                self.merged_segment_tree.insert(insert_index, self.default_new_weight)\n                record = {}\n                for name, record_values in records.items():\n                    record[name] = record_values[i]\n                if insert_index >= self.size:\n                    self.memory_values.append(record)\n                else:\n                    self.memory_values[insert_index] = record\n                i += 1\n\n        # Update indices\n        self.index = (self.index + num_records) % self.capacity\n        self.size = min(self.size + num_records, self.capacity)\n\n        return None\n\n    @rlgraph_api\n    def _graph_fn_get_records(self, num_records=1):\n        available_records = min(num_records, self.size)\n        indices = []\n        prob_sum = self.merged_segment_tree.sum_segment_tree.get_sum(0, self.size - 1)\n        samples = np.random.random(size=(available_records,)) * prob_sum\n        for sample in samples:\n            indices.append(self.merged_segment_tree.sum_segment_tree.index_of_prefixsum(prefix_sum=sample))\n\n        sum_prob = self.merged_segment_tree.sum_segment_tree.get_sum() + SMALL_NUMBER\n        min_prob = self.merged_segment_tree.min_segment_tree.get_min_value() / sum_prob\n        max_weight = (min_prob * self.size) ** (-self.beta)\n        weights = []\n        for index in indices:\n            sample_prob = self.merged_segment_tree.sum_segment_tree.get(index) / sum_prob\n            weight = (sample_prob * self.size) ** (-self.beta)\n            weights.append(weight / max_weight)\n\n        if get_backend() == ""pytorch"":\n            indices = torch.tensor(indices)\n            weights = torch.tensor(weights)\n        else:\n            indices = np.asarray(indices)\n            weights = np.asarray(weights)\n\n        records = DataOpDict()\n        for name, variable in self.memory.items():\n            records[name] = self.read_variable(variable, indices, dtype=\n            util.convert_dtype(self.flat_record_space[name].dtype, to=""pytorch""))\n        records = define_by_run_unflatten(records)\n        return records, indices, weights\n\n    @rlgraph_api(must_be_complete=False)\n    def _graph_fn_update_records(self, indices, update):\n        for index, loss in zip(indices, update):\n            priority = np.power(loss, self.alpha)\n            self.merged_segment_tree.insert(index, priority)\n            self.max_priority = max(self.max_priority, priority)\n\n    def get_state(self):\n        return {\n            ""size"": self.size,\n            ""index"": self.index,\n            ""max_priority"": self.max_priority\n        }'"
rlgraph/components/memories/memory.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph.utils.ops import FLATTEN_SCOPE_PREFIX\nfrom rlgraph.components.component import Component, rlgraph_api\nfrom rlgraph.utils import FlattenedDataOp\n\n\nclass Memory(Component):\n    """"""\n    Abstract memory component.\n\n    API:\n        insert_records(records) -> Triggers an insertion of records into the memory.\n        get_records(num_records) -> Returns `num_records` records from the memory.\n    """"""\n    def __init__(self, capacity=1000, scope=""memory"", **kwargs):\n        """"""\n        Args:\n            capacity (int): Maximum capacity of the memory.\n        """"""\n        super(Memory, self).__init__(scope=scope, **kwargs)\n\n        # Variables (will be populated in create_variables).\n        self.record_space = None\n        self.memory = None\n        self.flat_record_space = None\n        self.capacity = capacity\n        # The current size of the memory.\n        self.size = None\n\n        # Use this to get batch size.\n        self.terminal_key = FLATTEN_SCOPE_PREFIX + ""terminals""\n\n    def create_variables(self, input_spaces, action_space=None):\n        # Store our record-space for convenience.\n        self.record_space = input_spaces[""records""]\n        self.flat_record_space = self.record_space.flatten()\n\n        # Create the main memory as a flattened OrderedDict from any arbitrarily nested Space.\n        self.memory = self.get_variable(\n            name=""memory"", trainable=False,\n            from_space=self.record_space,\n            flatten=True,\n            add_batch_rank=self.capacity,\n            initializer=0\n        )\n        # Number of elements present.\n        self.size = self.get_variable(name=""size"", dtype=int, trainable=False, initializer=0)\n\n    @rlgraph_api(flatten_ops=True)\n    def _graph_fn_insert_records(self, records):\n        """"""\n        Inserts one or more complex records.\n\n        Args:\n            records (FlattenedDataOp): FlattenedDataOp containing record data. Keys must match keys in record\n                space.\n        """"""\n        raise NotImplementedError\n\n    @rlgraph_api\n    def _graph_fn_get_records(self, num_records=1):\n        """"""\n        Returns a number of records according to the retrieval strategy implemented by\n        the memory.\n\n        Args:\n            num_records (int): Number of records to return.\n\n        Returns:\n            DataOpDict: The retrieved records.\n        """"""\n        raise NotImplementedError\n\n    @rlgraph_api(returns=0)\n    def _graph_fn_get_episodes(self, num_episodes=1):\n        """"""\n        Retrieves a given number of episodes.\n\n        Args:\n            num_episodes (int): Number of episodes to retrieve.\n\n        Returns: The retrieved episodes.\n        """"""\n        pass\n\n    def _read_records(self, indices):\n        """"""\n        Obtains record values for the provided indices.\n\n        Args:\n            indices (Union[ndarray,tf.Tensor]): Indices to read. Assumed to be not contiguous.\n\n        Returns:\n             FlattenedDataOp: Record value dict.\n        """"""\n        records = FlattenedDataOp()\n        for name, variable in self.memory.items():\n            records[name] = self.read_variable(variable, indices)\n        return records\n\n    @rlgraph_api\n    def _graph_fn_get_size(self):\n        """"""\n        Returns the current size of the memory.\n\n        Returns:\n            SingleDataOp: The size (int) of the memory.\n        """"""\n        return self.read_variable(self.size)\n'"
rlgraph/components/memories/prioritized_replay.py,33,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.memories.memory import Memory\nfrom rlgraph.components.helpers.segment_tree import SegmentTree\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.util import get_batch_size\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass PrioritizedReplay(Memory):\n    """"""\n    Implements pure TensorFlow prioritized replay.\n\n    API:\n        update_records(indices, update) -> Updates the given indices with the given priority scores.\n    """"""\n    def __init__(self, capacity=1000, alpha=1.0, beta=0.0, scope=""prioritized-replay"", **kwargs):\n        """"""\n        Args:\n            next_states (bool): Whether to include s\' in the return values of the out-Socket ""get_records"".\n            alpha (float): Degree to which prioritization is applied, 0.0 implies no\n                prioritization (uniform), 1.0 full prioritization.\n            beta (float): Importance weight factor, 0.0 for no importance correction, 1.0\n                for full correction.\n        """"""\n        super(PrioritizedReplay, self).__init__(capacity, scope=scope, **kwargs)\n\n        # Variables.\n        self.index = None\n        self.max_priority = None\n        self.sum_segment_buffer = None\n        self.sum_segment_tree = None\n        self.min_segment_buffer = None\n        self.min_segment_tree = None\n\n        # List of flattened keys in our state Space.\n        self.flat_state_keys = None\n\n        self.priority_capacity = 0\n\n        # TODO check if we allow 0.0 as well.\n        assert alpha > 0.0\n        # Priority weight.\n        self.alpha = alpha\n        self.beta = beta\n\n    def create_variables(self, input_spaces, action_space=None):\n        super(PrioritizedReplay, self).create_variables(input_spaces, action_space)\n\n        # Record space must contain \'terminals\' for a replay memory.\n        assert \'terminals\' in self.record_space\n\n        # Main buffer index.\n        self.index = self.get_variable(name=""index"", dtype=int, trainable=False, initializer=0)\n\n        self.max_priority = self.get_variable(name=""max-priority"", dtype=float, trainable=False, initializer=1.0)\n\n        # Segment tree must be full binary tree.\n        self.priority_capacity = 1\n        while self.priority_capacity < self.capacity:\n            self.priority_capacity *= 2\n\n        # 1. Create a variable for a sum-segment tree.\n        self.sum_segment_buffer = self.get_variable(\n                name=""sum-segment-tree"",\n                shape=(2 * self.priority_capacity,),\n                dtype=tf.float32,\n                trainable=False,\n                initializer=tf.zeros_initializer()\n        )\n        self.sum_segment_tree = SegmentTree(self.sum_segment_buffer, self.priority_capacity)\n\n        # 2. Create a variable for a min-segment tree.\n        self.min_segment_buffer = self.get_variable(\n                name=""min-segment-tree"",\n                dtype=tf.float32,\n                trainable=False,\n                # Neutral element of min()\n                shape=(2 * self.priority_capacity,),\n                initializer=tf.constant_initializer(np.full((2 * self.priority_capacity,), float(\'inf\')))\n        )\n        self.min_segment_tree = SegmentTree(self.min_segment_buffer, self.priority_capacity)\n\n    @rlgraph_api(flatten_ops=True)\n    def _graph_fn_insert_records(self, records):\n        num_records = get_batch_size(records[self.terminal_key])\n        index = self.read_variable(self.index)\n        update_indices = tf.range(start=index, limit=index + num_records) % self.capacity\n\n        # Updates all the necessary sub-variables in the record.\n        record_updates = list()\n        for key in self.memory:\n            record_updates.append(self.scatter_update_variable(\n                variable=self.memory[key],\n                indices=update_indices,\n                updates=records[key]\n            ))\n\n        # Update indices and size.\n        with tf.control_dependencies(control_inputs=record_updates):\n            index_updates = list()\n            index_updates.append(self.assign_variable(ref=self.index, value=(index + num_records) % self.capacity))\n            update_size = tf.minimum(x=(self.read_variable(self.size) + num_records), y=self.capacity)\n            index_updates.append(self.assign_variable(self.size, value=update_size))\n\n        weight = tf.pow(x=self.max_priority, y=self.alpha)\n\n        # Insert new priorities into segment tree.\n        def insert_body(i):\n            sum_insert = self.sum_segment_tree.insert(update_indices[i], weight, tf.add)\n            with tf.control_dependencies(control_inputs=[sum_insert]):\n                return i + 1\n\n        def cond(i):\n            return i < num_records\n\n        with tf.control_dependencies(control_inputs=index_updates):\n            sum_insert = tf.while_loop(cond=cond, body=insert_body, loop_vars=[0])\n\n        def insert_body(i):\n            min_insert = self.min_segment_tree.insert(update_indices[i], weight, tf.minimum)\n            with tf.control_dependencies(control_inputs=[min_insert]):\n                return i + 1\n\n        def cond(i):\n            return i < num_records\n\n        with tf.control_dependencies(control_inputs=[sum_insert]):\n            min_insert = tf.while_loop(cond=cond, body=insert_body, loop_vars=[0])\n\n        # Nothing to return.\n        with tf.control_dependencies(control_inputs=[min_insert]):\n            return tf.no_op()\n\n    @rlgraph_api\n    def _graph_fn_get_records(self, num_records=1):\n        # Sum total mass.\n        current_size = self.read_variable(self.size)\n        stored_elements_prob_sum = self.sum_segment_tree.reduce(start=0, limit=current_size - 1)\n\n        # Sample the entire batch.\n        sample = stored_elements_prob_sum * tf.random_uniform(shape=(num_records, ))\n\n        # Sample by looking up prefix sum.\n        sample_indices = tf.map_fn(fn=self.sum_segment_tree.index_of_prefixsum, elems=sample, dtype=tf.int32)\n        # sample_indices = self.sum_segment_tree.index_of_prefixsum(sample)\n\n        # Importance correction.\n        total_prob = self.sum_segment_tree.reduce(start=0, limit=self.priority_capacity - 1)\n        min_prob = self.min_segment_tree.get_min_value() / total_prob\n        max_weight = tf.pow(x=min_prob * tf.cast(current_size, tf.float32), y=-self.beta)\n\n        def importance_sampling_fn(sample_index):\n            sample_prob = self.sum_segment_tree.get(sample_index) / stored_elements_prob_sum\n            weight = tf.pow(x=sample_prob * tf.cast(current_size, tf.float32), y=-self.beta)\n\n            return weight / max_weight\n\n        corrected_weights = tf.map_fn(\n            fn=importance_sampling_fn,\n            elems=sample_indices,\n            dtype=tf.float32\n        )\n        # sample_indices = tf.Print(sample_indices, [sample_indices, self.sum_segment_tree.values], summarize=1000,\n        #                           message=\'sample indices, segment tree values = \')\n        return self._read_records(indices=sample_indices), sample_indices, corrected_weights\n\n    @rlgraph_api(must_be_complete=False)\n    def _graph_fn_update_records(self, indices, update):\n        num_records = get_batch_size(indices)\n        max_priority = 0.0\n\n        # Update has to be sequential.\n        def insert_body(i, max_priority_):\n            priority = tf.pow(x=update[i], y=self.alpha)\n\n            sum_insert = self.sum_segment_tree.insert(\n                index=indices[i],\n                element=priority,\n                insert_op=tf.add\n            )\n            min_insert = self.min_segment_tree.insert(\n                index=indices[i],\n                element=priority,\n                insert_op=tf.minimum\n            )\n            # Keep track of current max priority element.\n            max_priority_ = tf.maximum(x=max_priority_, y=priority)\n\n            with tf.control_dependencies(control_inputs=[tf.group(sum_insert, min_insert)]):\n                # TODO: This confuses the auto-return value detector.\n                return i + 1, max_priority_\n\n        def cond(i, max_priority_):\n            return i < num_records - 1\n\n        _, max_priority = tf.while_loop(\n            cond=cond,\n            body=insert_body,\n            loop_vars=(0, max_priority)\n        )\n\n        assignment = self.assign_variable(ref=self.max_priority, value=max_priority)\n        with tf.control_dependencies(control_inputs=[assignment]):\n            return tf.no_op()\n'"
rlgraph/components/memories/queue_runner.py,3,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.ops import flatten_op\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass QueueRunner(Component):\n    """"""\n    A queue runner that contains n sub-components, of which an API-method is called. The return values are bundled\n    into a FIFOQueue as inputs. Queue runner uses multi-threading and is started after session creation.\n\n    API:\n    enqueue() -> Returns a noop, but creates the enqueue ops for enqueuing data into the queue and hands these\n        to the underlying queue-runner object.\n    """"""\n    def __init__(self, queue, api_method_name, return_slot,\n                 # TODO: move these into data_producing_components-wrapper components\n                 env_output_splitter,\n                 fifo_input_merger,\n                 internal_states_slicer,\n                 *data_producing_components, **kwargs):\n        """"""\n        Args:\n            queue (Queue-like): The Queue (FIFOQueue), whose underlying `queue` object to use to enqueue items into.\n            api_method_name (str): The name of the API method to call on all `sub_components` to get ops from\n                which we will create enqueue ops for the queue.\n            return_slot (int): The slot of the returned values to use as to-be-inserted record into the queue.\n                Set to -1 if only one value is expected.\n            #input_merger (Component): The record input-merger to use for merging things into a dict-record\n            #    before inserting it into the queue.\n            data_producing_components (Component): The components of this QueueRunner that produce the data to\n                be enqueued.\n        """"""\n        super(QueueRunner, self).__init__(scope=kwargs.pop(""scope"", ""queue-runner""), **kwargs)\n\n        self.queue = queue\n        self.api_method_name = api_method_name\n        self.return_slot = return_slot\n\n        self.env_output_splitter = env_output_splitter\n        self.fifo_input_merger = fifo_input_merger\n        self.internal_states_slicer = internal_states_slicer\n\n        # The actual backend-dependent queue object.\n        self.queue_runner = None\n\n        self.data_producing_components = data_producing_components\n\n        # Add our sub-components (not the queue!).\n        self.add_components(\n            self.env_output_splitter, self.internal_states_slicer, self.fifo_input_merger,\n            *self.data_producing_components\n        )\n\n        self.input_complete = False\n\n    def check_input_completeness(self):\n        # The queue must be ready before we are (even though it\'s not a sub-component).\n        if self.queue.input_complete is False or self.queue.built is False:\n            return False\n        return super(QueueRunner, self).check_input_completeness()\n\n    @rlgraph_api\n    def _graph_fn_setup(self):\n        enqueue_ops = list()\n\n        if get_backend() == ""tf"":\n            for data_producing_component in self.data_producing_components:\n                record = getattr(data_producing_component, self.api_method_name)()\n                if self.return_slot != -1:\n                    # Only care about one slot of the return values.\n                    record = record[self.return_slot]\n\n                # TODO: specific for IMPALA problem: needs to be generalized.\n                if self.internal_states_slicer is not None:\n                    outs = self.env_output_splitter.call(record)\n\n                    # Assume that internal_states are the last item coming from the env-stepper.\n                    initial_internal_states = self.internal_states_slicer.slice(outs[-1], 0)\n                    record = self.fifo_input_merger.merge(*(outs[:-1] + (initial_internal_states,)))\n                else:\n                    terminals, states, actions, rewards, action_log_probs = self.env_output_splitter.call(record)\n                    record = self.fifo_input_merger.merge(\n                        terminals, states, actions, rewards, action_log_probs\n                    )\n\n                # Create enqueue_op from api_return.\n                # TODO: This is kind of cheating, as we are producing an op from a component that\'s not ours.\n                enqueue_op = self.queue.queue.enqueue(flatten_op(record))\n                enqueue_ops.append(enqueue_op)\n\n            self.queue_runner = tf.train.QueueRunner(self.queue.queue, enqueue_ops)\n            # Add to standard collection, so all queue-runners will be started after session creation.\n            tf.train.add_queue_runner(self.queue_runner)\n\n            return tf.no_op()\n'"
rlgraph/components/memories/replay_memory.py,7,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.memories.memory import Memory\nfrom rlgraph.utils import util, DataOpDict\nfrom rlgraph.utils.define_by_run_ops import define_by_run_unflatten\nfrom rlgraph.utils.util import get_batch_size\nfrom rlgraph.utils.decorators import rlgraph_api\n\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n    import numpy as np\n\n\nclass ReplayMemory(Memory):\n    """"""\n    Implements a standard replay memory to sample randomized batches.\n    """"""\n    def __init__(\n        self,\n        capacity=1000,\n        scope=""replay-memory"",\n        **kwargs\n    ):\n        """"""\n        Args:\n            next_states (bool): If true include next states in the return values of the API-method ""get_records"".\n        """"""\n        super(ReplayMemory, self).__init__(capacity, scope=scope, **kwargs)\n\n        self.index = None\n        self.states = None\n        self.flat_record_space = None\n\n    def create_variables(self, input_spaces, action_space=None):\n        super(ReplayMemory, self).create_variables(input_spaces, action_space)\n\n        # Record space must contain \'terminals\' for a replay memory.\n        assert \'terminals\' in self.record_space\n        # Main buffer index.\n        self.index = self.get_variable(name=""index"", dtype=int, trainable=False, initializer=0)\n\n    @rlgraph_api(flatten_ops=True)\n    def _graph_fn_insert_records(self, records):\n        num_records = get_batch_size(records[self.terminal_key])\n        if get_backend() == ""tf"":\n            # List of indices to update (insert from `index` forward and roll over at `self.capacity`).\n            update_indices = tf.range(start=self.index, limit=self.index + num_records) % self.capacity\n\n            # Updates all the necessary sub-variables in the record.\n            record_updates = []\n            for key in self.memory:\n                record_updates.append(self.scatter_update_variable(\n                    variable=self.memory[key],\n                    indices=update_indices,\n                    updates=records[key]\n                ))\n\n            # Update indices and size.\n            with tf.control_dependencies(control_inputs=record_updates):\n                index_updates = [self.assign_variable(ref=self.index, value=(self.index + num_records) % self.capacity)]\n                update_size = tf.minimum(x=(self.read_variable(self.size) + num_records), y=self.capacity)\n                index_updates.append(self.assign_variable(self.size, value=update_size))\n\n            # Nothing to return.\n            with tf.control_dependencies(control_inputs=index_updates):\n                return tf.no_op()\n        elif get_backend() == ""pytorch"":\n            update_indices = torch.arange(self.index, self.index + num_records) % self.capacity\n            for key in self.memory:\n                for i, val in zip(update_indices, records[key]):\n                    self.memory[key][i] = val\n            self.index = (self.index + num_records) % self.capacity\n            self.size = min(self.size + num_records, self.capacity)\n            return None\n\n    @rlgraph_api\n    def _graph_fn_get_records(self, num_records=1):\n        if get_backend() == ""tf"":\n            size = self.read_variable(self.size)\n\n            # Sample and retrieve a random range, including terminals.\n            index = self.read_variable(self.index)\n            indices = tf.random_uniform(shape=(num_records,), maxval=size, dtype=tf.int32)\n            indices = (index - 1 - indices) % self.capacity\n\n            # Return default importance weight one.\n            return self._read_records(indices=indices), indices, tf.ones_like(tensor=indices, dtype=tf.float32)\n        elif get_backend() == ""pytorch"":\n            indices = []\n            if self. size > 0:\n                indices = np.random.choice(np.arange(0, self.size), size=int(num_records))\n                indices = (self.index - 1 - indices) % self.capacity\n            records = DataOpDict()\n            for name, variable in self.memory.items():\n                records[name] = self.read_variable(variable, indices, dtype=\n                                                   util.convert_dtype(self.flat_record_space[name].dtype, to=""pytorch""),\n                                                   shape=self.flat_record_space[name].shape)\n            records = define_by_run_unflatten(records)\n            weights = torch.ones(indices.shape, dtype=torch.float32) if len(indices) > 0 \\\n                else torch.ones(1, dtype=torch.float32)\n            return records, indices, weights\n\n    def get_state(self):\n        return {\n            ""index"": self.index,\n            ""size"": self.size,\n            ""memory"": self.memory\n        }\n'"
rlgraph/components/memories/ring_buffer.py,21,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.memories.memory import Memory\nfrom rlgraph.utils import util, DataOpDict\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.define_by_run_ops import define_by_run_unflatten\nfrom rlgraph.utils.util import get_batch_size\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import numpy as np\n    import torch\n\n\nclass RingBuffer(Memory):\n    """"""\n    Simple ring-buffer to be used for on-policy sampling based on sample count or episodes.\n    Fetches most recently added memories.\n    """"""\n    def __init__(self, capacity=1000, scope=""ring-buffer"", **kwargs):  #auto_sequence_indices=True\n        """"""\n        Args:\n            #auto_sequence_indices (bool): Whether to add sequence_indices=True automatically whenever data is inserted\n            #    and the last item does not have `terminal`=True. This requires `sequence_indices` to be a key in\n            #    `self.record_space`.\n        """"""\n        super(RingBuffer, self).__init__(capacity, scope=scope, **kwargs)\n\n        #self.auto_sequence_indices = auto_sequence_indices\n\n        self.index = None\n        self.states = None\n        self.num_episodes = None\n        self.episode_indices = None\n        self.flat_record_space = None\n\n    def create_variables(self, input_spaces, action_space=None):\n        super(RingBuffer, self).create_variables(input_spaces, action_space)\n\n        # Record space must contain \'terminals\' for a ring buffer memory.\n        assert ""terminals"" in self.record_space\n        #if self.auto_sequence_indices is True:\n        #    assert ""sequence_indices"" in self.record_space\n\n        self.index = self.get_variable(name=""index"", dtype=int, trainable=False, initializer=0)\n        # Num episodes present.\n        self.num_episodes = self.get_variable(name=""num-episodes"", dtype=int, trainable=False, initializer=0)\n\n        # Terminal indices contiguously arranged.\n        self.episode_indices = self.get_variable(name=""episode-indices"", shape=(self.capacity,),\n                                                 dtype=int, trainable=False)\n\n    @rlgraph_api(flatten_ops=True)\n    def _graph_fn_insert_records(self, records):\n        if get_backend() == ""tf"":\n            num_records = get_batch_size(records[self.terminal_key])\n            index = self.read_variable(self.index)\n\n            # Episodes before inserting these records.\n            prev_num_episodes = self.read_variable(self.num_episodes)\n            update_indices = tf.range(start=index, limit=index + num_records) % self.capacity\n\n            # Episodes previously existing in the range we inserted to as indicated\n            # by count of terminals in that slice.\n            insert_terminal_slice = self.read_variable(self.memory[self.terminal_key], update_indices)\n\n            # Shift episode indices.\n            with tf.control_dependencies([update_indices, index, prev_num_episodes, insert_terminal_slice]):\n                index_updates = []\n\n                # Newly inserted episodes.\n                inserted_episodes = tf.reduce_sum(input_tensor=tf.cast(records[self.terminal_key], dtype=tf.int32), axis=0)\n                episodes_in_insert_range = tf.reduce_sum(\n                    input_tensor=tf.cast(insert_terminal_slice, dtype=tf.int32), axis=0\n                )\n                num_episode_update = prev_num_episodes - episodes_in_insert_range + inserted_episodes\n\n                # Shift contiguous episode indices.\n                # prev_num_episodes = tf.Print(prev_num_episodes, [inserted_episodes, episodes_in_insert_range], summarize=100,\n                #                              message=""inserted_episodes, episodes in range = "")\n                index_updates.append(self.assign_variable(\n                        ref=self.episode_indices[:prev_num_episodes - episodes_in_insert_range],\n                        value=self.episode_indices[episodes_in_insert_range:prev_num_episodes]\n                ))\n\n                # Insert new episodes starting at previous count minus the ones we removed,\n                # ending at previous count minus removed + inserted.\n                slice_start = prev_num_episodes - episodes_in_insert_range\n                slice_end = num_episode_update\n\n            # Update indices and size.\n            with tf.control_dependencies(index_updates):\n                index_updates = []\n\n                # Actually update indices.\n                mask = tf.boolean_mask(tensor=update_indices, mask=records[self.terminal_key])\n                # mask = tf.Print(mask, [self.episode_indices, update_indices, mask, slice_start, slice_end],\n                #                 summarize=100, message=""update, mask, start, end"")\n                index_updates.append(self.assign_variable(\n                    ref=self.episode_indices[slice_start:slice_end],\n                    value=mask\n                ))\n\n                # Assign final new episode count.\n                index_updates.append(self.assign_variable(self.num_episodes, num_episode_update))\n\n                index_updates.append(self.assign_variable(ref=self.index, value=(index + num_records) % self.capacity))\n                update_size = tf.minimum(x=(self.read_variable(self.size) + num_records), y=self.capacity)\n                index_updates.append(self.assign_variable(self.size, value=update_size))\n\n            # Updates all the necessary sub-variables in the record.\n            with tf.control_dependencies(index_updates):\n                record_updates = []\n                for key in self.memory:\n                    record_updates.append(self.scatter_update_variable(\n                        variable=self.memory[key],\n                        indices=update_indices,\n                        updates=records[key]\n                    ))\n\n            # Nothing to return.\n            with tf.control_dependencies(control_inputs=record_updates):\n                return tf.no_op()\n        elif get_backend() == ""pytorch"":\n            # TODO: Unclear if we should do this in numpy and then convert to torch once we sample.\n            num_records = get_batch_size(records[self.terminal_key])\n            update_indices = torch.arange(self.index, self.index + num_records) % self.capacity\n\n            # Newly inserted episodes.\n            inserted_episodes = torch.sum(records[self.terminal_key].int(), 0)\n\n            # Episodes previously existing in the range we inserted to as indicated\n            # by count of terminals in the that slice.\n            episodes_in_insert_range = 0\n            # Count terminals in inserted range.\n            for index in update_indices:\n                episodes_in_insert_range += int(self.memory[self.terminal_key][index])\n            num_episode_update = self.num_episodes - episodes_in_insert_range + inserted_episodes\n            self.episode_indices[:self.num_episodes - episodes_in_insert_range] = \\\n                self.episode_indices[episodes_in_insert_range:self.num_episodes]\n\n            # Insert new episodes starting at previous count minus the ones we removed,\n            # ending at previous count minus removed + inserted.\n            slice_start = self.num_episodes - episodes_in_insert_range\n            slice_end = num_episode_update\n\n            byte_terminals = records[self.terminal_key].byte()\n            mask = torch.masked_select(update_indices, byte_terminals)\n            self.episode_indices[slice_start:slice_end] = mask\n\n            # Update indices.\n            self.num_episodes = int(num_episode_update)\n            self.index = (self.index + num_records) % self.capacity\n            self.size = min(self.size + num_records, self.capacity)\n\n            # Updates all the necessary sub-variables in the record.\n            for key in self.memory:\n                for i, val in zip(update_indices, records[key]):\n                    self.memory[key][i] = val\n\n            # The TF version returns no-op, return None so return-val inference system does not throw error.\n            return None\n\n    @rlgraph_api\n    def _graph_fn_get_records(self, num_records=1):\n        if get_backend() == ""tf"":\n            stored_records = self.read_variable(self.size)\n            available_records = tf.minimum(x=num_records, y=stored_records)\n            index = self.read_variable(self.index)\n            indices = tf.range(start=index - available_records, limit=index) % self.capacity\n            return self._read_records(indices=indices)\n        elif get_backend() == ""pytorch"":\n            available_records = min(num_records, self.size)\n            indices = np.arange(self.index - available_records, self.index) % self.capacity\n            records = DataOpDict()\n\n            for name, variable in self.memory.items():\n                records[name] = self.read_variable(variable, indices, dtype=\n                                                   util.convert_dtype(self.flat_record_space[name].dtype, to=""pytorch""),\n                                                   shape=self.flat_record_space[name].shape)\n\n            records = define_by_run_unflatten(records)\n            return records\n\n    @rlgraph_api(ok_to_overwrite=True)\n    def _graph_fn_get_episodes(self, num_episodes=1):\n        if get_backend() == ""tf"":\n            stored_episodes = self.read_variable(self.num_episodes)\n            available_episodes = tf.minimum(x=num_episodes, y=stored_episodes)\n\n            # Say we have two episodes with this layout:\n            # terminals = [0 0 1 0 1]\n            # episode_indices = [2, 4]\n            # If we want to fetch the most recent episode, the start index is:\n            # stored_episodes - 1 - num_episodes = 2 - 1 - 1 = 0, which points to buffer index 2\n            # The next episode starts one element after this, hence + 1.\n            # However, this points to index -1 if stored_episodes = available_episodes,\n            # in this case we want start = 0 to get everything.\n            start = tf.cond(\n                pred=tf.equal(x=stored_episodes, y=available_episodes),\n                true_fn=lambda: 0,\n                false_fn=lambda: self.episode_indices[stored_episodes - available_episodes - 1] + 1\n            )\n            # End index is just the pointer to the most recent episode.\n            limit = self.episode_indices[stored_episodes - 1]\n\n            limit += tf.where(condition=(start < limit), x=0, y=self.capacity - 1)\n            # limit = tf.Print(limit, [stored_episodes, start, limit], summarize=100, message=""start | limit"")\n            indices = tf.range(start=start, limit=limit + 1) % self.capacity\n            return self._read_records(indices=indices)\n        elif get_backend() == ""pytorch"":\n            stored_episodes = self.num_episodes\n            available_episodes = min(num_episodes, self.num_episodes)\n\n            if stored_episodes == available_episodes:\n                start = 0\n            else:\n                start = self.episode_indices[stored_episodes - available_episodes - 1] + 1\n\n            # End index is just the pointer to the most recent episode.\n            limit = self.episode_indices[stored_episodes - 1]\n            if start >= limit:\n                limit += self.capacity - 1\n            indices = torch.arange(start, limit + 1) % self.capacity\n\n            records = DataOpDict()\n            for name, variable in self.memory.items():\n                records[name] = self.read_variable(variable, indices, dtype=\n                                                   util.convert_dtype(self.flat_record_space[name].dtype, to=""pytorch""),\n                                                   shape=self.flat_record_space[name].shape)\n            records = define_by_run_unflatten(records)\n            return records\n\n    def get_state(self):\n        return {\n            ""index"": self.index,\n            ""size"": self.size,\n            ""num_episodes"": self.num_episodes,\n            ""episode_indices"": self.episode_indices,\n            ""memory"": self.memory\n        }\n'"
rlgraph/components/models/__init__.py,0,"b'# Copyright 2018/2019 ducandu GmbH. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\n'"
rlgraph/components/models/intrinsic_curiosity_world_option_model.py,2,"b'# Copyright 2018/2019 ducandu GmbH. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.policies.supervised_predictor import SupervisedPredictor\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.nn.concat_layer import ConcatLayer\nfrom rlgraph.components.layers.preprocessing.reshape import ReShape\nfrom rlgraph.components.loss_functions.neg_log_likelihood_loss import NegativeLogLikelihoodLoss\nfrom rlgraph.components.models.supervised_model import SupervisedModel\nfrom rlgraph.components.neural_networks.neural_network import NeuralNetwork\nfrom rlgraph.spaces import *\nfrom rlgraph.spaces.space_utils import get_default_distribution_from_space\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass IntrinsicCuriosityWorldOptionModel(SupervisedModel):\n    """"""\n    Combines an inverse dynamics network, predicting actions that lead from state s to state s\', with a forward\n    model (""world option model""), predicting a distribution over a latent feature-vector for s\', when given s and a.\n    Uses a single loss function, combining the loss terms for both these prediction tasks, and one optimizer.\n\n    Codename: LEM (Latent Space Exploration/Establishing Model).\n\n    Based on:\n    [1] Curiosity-driven Exploration by Self-supervised Prediction - Pathak et al. - UC Berkeley 2017\n    [2] World Models - Ha, Schmidhuber - 2018\n    """"""\n    def __init__(\n            self, action_space, world_option_model_network, encoder_network, num_features, num_mixtures, beta=0.2,\n            post_phi_concat_network=None,\n            reward_clipping=1.0,\n            intrinsic_rewards_weight=0.1,\n            concat_with_command_vector=False,\n            optimizer=None, deterministic=False, scope=""intrinsic-curiosity-world-option-model"",\n            **kwargs\n    ):\n        """"""\n        Args:\n            action_space (Space): The action Space to be fed into the model together with the latent feature vector\n                for the states. Will be flattened automatically and then concatenated by this component.\n\n            world_option_model_network (Union[NeuralNetwork,dict]): A specification dict (or NN object directly) to\n                construct the world-option-model\'s neural network.\n\n            encoder_network (Union[NeuralNetwork,dict]): A specification dict (or NN object directly) to\n                construct the inverse dynamics model\'s encoder network leading from s to phi (feature vector).\n\n            num_features (int): The size of the feature vectors phi.\n\n            num_mixtures (int): The number of mixture Normals to use for the next-state distribution output.\n\n            beta (float): The weight for the phi\' loss (action loss is then 1.0 - beta).\n\n            post_phi_concat_network\n\n            reward_clipping (float): 0.0 for no clipping, some other value for +/- reward value clipping.\n                Default: 1.0.\n\n            concat_with_command_vector (bool): If True, this model needs an additional command vector (coming from the\n                policy above) to concat it together with the latent state vector.\n\n            optimizer (Optional[Optimizer]): The optimizer to use for supervised learning of the two networks\n                (ICM and WOM).\n        """"""\n        self.num_features = num_features\n        self.num_mixtures = num_mixtures\n        self.deterministic = deterministic\n        self.beta = beta\n        assert 0.0 < self.beta < 1.0, ""ERROR: `beta` must be between 0 and 1!""\n        self.reward_clipping = reward_clipping\n        self.intrinsic_rewards_weight = intrinsic_rewards_weight\n\n        # Create the encoder network inside a SupervisedPredictor (so we get the adapter + distribution with it).\n        self.state_encoder = SupervisedPredictor(\n            network_spec=encoder_network, output_space=FloatBox(shape=(num_features,), add_batch_rank=True),\n            scope=""state-encoder""\n        )\n\n        # Create the container loss function for the two prediction tasks:\n        # a) Action prediction and b) next-state prediction, each of them using a simple neg log likelihood loss\n        # comparing the actual action and s\' with their log-likelihood value vs the respective distributions.\n        self.loss_functions = dict(\n            # Action prediction loss (neg log likelihood of observed action vs the parameterized distribution).\n            predicted_actions=NegativeLogLikelihoodLoss(\n                distribution_spec=get_default_distribution_from_space(action_space),\n                scope=""action-loss""\n            ),\n            # s\' prediction loss (neg log likelihood of observed s\' vs the parameterized mixed normal distribution).\n            predicted_phi_=NegativeLogLikelihoodLoss(distribution_spec=dict(type=""mixture"", _args=[\n                ""multi-variate-normal"" for _ in range(num_mixtures)\n            ]), scope=""phi-loss"")\n        )\n\n        # TODO: Support for command vector concatenation.\n        #self.concat_with_command_vector = concat_with_command_vector\n\n        # Define the Model\'s network\'s custom call method.\n        def custom_call(self, inputs):\n            phi = inputs[""phi""]\n            actions = inputs[""actions""]\n            phi_ = inputs[""phi_""]\n            actions_flat = self.get_sub_component_by_name(""action-flattener"").call(actions)\n            concat_phis = self.get_sub_component_by_name(""concat-phis"").call(phi, phi_)\n            # Predict the action that lead from s to s\'.\n            predicted_actions = self.get_sub_component_by_name(""post-phi-concat-nn"").call(concat_phis)\n\n            # Concat phi with flattened actions.\n            phi_and_actions = self.get_sub_component_by_name(""concat-states-and-actions"").call(\n                phi, actions_flat\n            )\n            # Add stop-gradient to phi here before predicting phi\'\n            # (the phis should only be trained by the inverse dynamics model, not by the world option model).\n            # NOT DONE IN ORIGINAL PAPER\'s CODE AND ALSO NOT IN MLAGENTS EQUIVALENT.\n            # phi_and_actions = self.get_sub_component_by_name(""stop-gradient"").stop(phi_and_actions)\n            # Predict phi\' (through a mixture gaussian distribution).\n            predicted_phi_ = self.get_sub_component_by_name(""wom-nn"").call(phi_and_actions)\n\n            return dict(\n                # Predictions (actions and next-state-features (mixture distribution)).\n                predicted_actions=predicted_actions,\n                predicted_phi_=predicted_phi_\n                ## Also return the two feature vectors for s and s\'.\n                #phi=phi, phi_=phi_\n            )\n\n        # Create the SupervisedPredictor\'s neural network.\n        predictor_network = NeuralNetwork(\n            # The world option model network taking action-cat-phi and mapping them to the predicted phi\'.\n            NeuralNetwork.from_spec(world_option_model_network, scope=""wom-nn""),\n            # The concat component concatenating both latent state vectors (phi and phi\').\n            ConcatLayer(scope=""concat-phis""),\n            # The NN mapping from phi-cat-phi\' to the action prediction.\n            NeuralNetwork.from_spec(post_phi_concat_network, scope=""post-phi-concat-nn""),\n            # The ReShape component for flattening all actions in arbitrary action spaces.\n            ReShape(flatten=True, flatten_categories=True, flatten_containers=True, scope=""action-flattener""),\n            # The concat component concatenating latent state feature vector and incoming (flattened) actions.\n            ConcatLayer(scope=""concat-states-and-actions""),\n            # Set the `call` method.\n            api_methods={(""call"", custom_call)}\n        )\n\n        if optimizer is None:\n            optimizer = dict(type=""adam"", learning_rate=3e-4)\n\n        super(IntrinsicCuriosityWorldOptionModel, self).__init__(\n            predictor=dict(\n                network_spec=predictor_network,\n                output_space=Dict({\n                    ""predicted_actions"": action_space,\n                    ""predicted_phi_"": FloatBox(shape=(self.num_features,))\n                }, add_batch_rank=action_space.has_batch_rank, add_time_rank=action_space.has_time_rank),\n                distribution_adapter_spec=dict(\n                    # for `predicted_actions`: use default adapter\n                    # for predicted_phi\': use normal-mixture adapter & distribution.\n                    predicted_phi_={""type"": ""normal-mixture-adapter"", ""num_mixtures"": num_mixtures}\n                ),\n                deterministic=deterministic\n            ),\n            loss_function=self.loss_functions[""predicted_actions""],\n            optimizer=optimizer, scope=scope, **kwargs\n        )\n\n        self.add_components(self.state_encoder, self.loss_functions[""predicted_phi_""])\n\n    @rlgraph_api\n    def get_phi(self, states, deterministic=None):\n        """"""\n        Returns the (automatically learnt) feature vector given some state (s).\n\n        Args:\n            states (DataOpRec): The states to encode to phi (feature vector).\n            deterministic (DataOpRec[bool]): Whether to sample from the distribution output of the encoder network\n                deterministically (max-likelihood) or not.\n\n        Returns:\n            dict:\n                - phi: The feature vector for s.\n        """"""\n        deterministic = self.deterministic if deterministic is None else deterministic\n        phi = self.state_encoder.predict(states, deterministic=deterministic)\n        phi[""phi""] = phi[""predictions""]\n        return phi\n\n    @rlgraph_api\n    def get_phis_from_nn_inputs(self, nn_inputs, deterministic=None):\n        """"""\n        Returns the (automatically learnt) feature vectors given some state (s) and next state (s\').\n        Action inputs within `nn_inputs` are just passed through.\n\n        Args:\n            nn_inputs (DataOpRec[dict]): The NN input (as dict) with the keys: ""states"", ""next_states"", and ""actions"".\n\n            deterministic (DataOpRec[bool]): Whether to sample from the distribution output of the encoder network\n                deterministically (max-likelihood) or not.\n\n        Returns:\n            dict:\n                - phi: The feature vector for s.\n                - phi_: The feature vector for s\'.\n                - actions: The actions from `nn_inputs` unchanged.\n        """"""\n        states = nn_inputs[""states""]\n        next_states = nn_inputs[""next_states""]\n        actions = nn_inputs[""actions""]\n        phi = self.get_phi(states, deterministic=deterministic)[""predictions""]\n        phi_ = self.get_phi(next_states, deterministic=deterministic)[""predictions""]\n        return dict(phi=phi, actions=actions, phi_=phi_)\n\n    @rlgraph_api\n    def predict(self, nn_inputs, deterministic=None):\n        """"""\n        Returns:\n\n        """"""\n        nn_inputs = self.get_phis_from_nn_inputs(nn_inputs, deterministic)\n        return self.predictor.predict(nn_inputs, deterministic=deterministic)\n\n    @rlgraph_api\n    def get_distribution_parameters(self, nn_inputs, deterministic=None):\n        nn_inputs = self.get_phis_from_nn_inputs(nn_inputs, deterministic=deterministic)\n        return self.predictor.get_distribution_parameters(nn_inputs)\n\n    @rlgraph_api\n    def update(self, nn_inputs, labels=None, time_percentage=None):\n        """"""\n        Returns:\n            dict:\n                see SupervisedModel outputs\n                - intrinsic_rewards: The intrinsic rewards\n        """"""\n        # Update the model (w/o labels b/c labels are already included in the nn_inputs).\n        assert labels is None, ""ERROR: `labels` arg not needed for {}.`update()`!"".format(type(self).__name__)\n        nn_inputs = self.get_phis_from_nn_inputs(nn_inputs, deterministic=False)\n        parameters = self.predictor.get_distribution_parameters(nn_inputs)\n        # Construct labels from nn_inputs.\n        labels = dict(predicted_actions=nn_inputs[""actions""], predicted_phi_=nn_inputs[""phi_""])\n\n        # Get two losses from both parts of the loss function.\n        action_loss, action_loss_per_item = self.loss_functions[""predicted_actions""].loss(\n            parameters[""predicted_actions""], labels[""predicted_actions""], time_percentage\n        )\n        phi_loss, phi_loss_per_item = self.loss_functions[""predicted_phi_""].loss(\n            parameters[""predicted_phi_""], labels[""predicted_phi_""], time_percentage\n        )\n        # Average all the losses.\n        loss, loss_per_item, intrinsic_rewards = self._graph_fn_average_losses_and_calc_rewards(\n            action_loss, action_loss_per_item, phi_loss, phi_loss_per_item\n        )\n        step_op = self.optimizer.step(self.variables(), loss, loss_per_item, time_percentage)\n\n        return dict(\n            step_op=step_op,\n            loss=loss,\n            loss_per_item=loss_per_item,\n            intrinsic_rewards=intrinsic_rewards,\n            parameters=parameters\n        )\n\n    # TODO: Move this to Component base class.\n    @graph_fn\n    def _graph_fn_stop_gradient(self, input_):\n        if get_backend() == ""tf"":\n            return tf.stop_gradient(input_)\n\n    @graph_fn\n    def _graph_fn_average_losses_and_calc_rewards(self, action_loss, action_loss_per_item, phi_loss, phi_loss_per_item):\n        loss = (1.0 - self.beta) * action_loss + self.beta * phi_loss\n        loss_per_item = (1.0 - self.beta) * action_loss_per_item + self.beta * phi_loss_per_item\n        intrinsic_rewards = None\n        if get_backend() == ""tf"":\n            if self.reward_clipping > 0.0:\n                intrinsic_rewards = tf.clip_by_value(\n                    phi_loss_per_item, clip_value_min=-self.reward_clipping, clip_value_max=self.reward_clipping\n                )\n            else:\n                intrinsic_rewards = phi_loss_per_item\n            intrinsic_rewards *= self.intrinsic_rewards_weight\n        return loss, loss_per_item, intrinsic_rewards\n\n'"
rlgraph/components/models/model.py,0,"b'# Copyright 2018/2019 ducandu GmbH. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api\n\n\nclass Model(Component):\n    """"""\n    A Model is an abstract Component class that must implement the APIs `predict`, `get_distribution_parameters`, and\n    `update`.\n    """"""\n\n    @rlgraph_api\n    def predict(self, nn_inputs, deterministic=None):\n        """"""\n        Args:\n            nn_inputs (any): The input to our neural network.\n\n            deterministic (Optional[bool]): Whether to draw the prediction sample deterministically\n                (max likelihood) from the parameterized distribution or not.\n\n        Returns:\n            dict:\n                - predictions: The final sample from the Distribution (including the entire output of the neural network).\n                - nn_outputs: The raw NN outputs\n                - adapter_outputs: The NN outputs passed through the action adapter.\n                - parameters: The parameters for the distributions.\n                - log_likelihood: The log-likelihood (in case of categorical distribution(s)).\n        """"""\n        raise NotImplementedError\n\n    @rlgraph_api\n    def get_distribution_parameters(self, nn_inputs):\n        """"""\n        Args:\n            nn_inputs (any): The input to our neural network.\n\n        Returns:\n            any: The raw (parameter) output of the DistributionAdapter layer\n                (including possibly the last internal states of an RNN-based NN).\n        """"""\n        raise NotImplementedError\n\n    @rlgraph_api\n    def update(self, nn_inputs, labels, timestep=None):\n        """"""\n        Updates the model by doing a forward pass through the predictor, passing its outputs and the\n        `loss_function_inputs` through the loss function, then performing the optimizer update.\n\n        Args:\n            nn_inputs (any): The inputs to the predictor. Will be passed as one DataOpRecord into the NN.\n            labels (any): The corresponding labels for the prediction inputs.\n            timestep (Optional[int]): The timestep of the update. Can be used e.g. for decaying learning rates.\n\n        Returns:\n            dict with keys:\n                - step_op\n                - loss\n                - loss_per_item\n                - parameters: Distribution parameters before the outputs.\n        """"""\n        raise NotImplementedError\n'"
rlgraph/components/models/supervised_model.py,0,"b'# Copyright 2018/2019 ducandu GmbH. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph.components.models.model import Model\nfrom rlgraph.components.loss_functions.loss_function import LossFunction\nfrom rlgraph.components.optimizers.optimizer import Optimizer\nfrom rlgraph.components.policies.supervised_predictor import SupervisedPredictor\nfrom rlgraph.utils.decorators import rlgraph_api\n\n\nclass SupervisedModel(Model):\n    """"""\n    A Model is a Component that holds a Predictor, a LossFunction and an Optimizer Component and exposes\n    the Predictor\'s API plus some methods to update the Predictor\'s parameters.\n    """"""\n    def __init__(self, predictor, loss_function, optimizer, scope=""supervised-model"", **kwargs):\n        super(SupervisedModel, self).__init__(scope=scope, **kwargs)\n\n        self.predictor = SupervisedPredictor.from_spec(predictor)\n        self.loss_function = LossFunction.from_spec(loss_function)\n        self.optimizer = Optimizer.from_spec(optimizer)\n\n        self.add_components(self.predictor)\n        self.add_components(self.loss_function, self.optimizer)\n\n    @rlgraph_api\n    def predict(self, nn_inputs, deterministic=None):\n        return self.predictor.predict(nn_inputs, deterministic=deterministic)\n\n    @rlgraph_api\n    def get_distribution_parameters(self, nn_inputs):\n        return self.predictor.get_distribution_parameters(nn_inputs)\n\n    @rlgraph_api\n    def update(self, nn_inputs, labels, timestep=None):\n        parameters = self.predictor.get_distribution_parameters(nn_inputs)\n        loss, loss_per_item = self.loss_function.loss(parameters, labels, timestep)\n        step_op, _, _ = self.optimizer.step(self.predictor.variables(), loss, loss_per_item)\n\n        return dict(\n            step_op=step_op,\n            loss=loss,\n            loss_per_item=loss_per_item,\n            parameters=parameters\n        )\n'"
rlgraph/components/neural_networks/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph.components.neural_networks.stack import Stack\nfrom rlgraph.components.neural_networks.dict_preprocessor_stack import DictPreprocessorStack\nfrom rlgraph.components.neural_networks.preprocessor_stack import PreprocessorStack\nfrom rlgraph.components.neural_networks.neural_network import NeuralNetwork\nfrom rlgraph.components.neural_networks.multi_input_stream_neural_network import MultiInputStreamNeuralNetwork\nfrom rlgraph.components.neural_networks.variational_auto_encoder import VariationalAutoEncoder\nfrom rlgraph.components.neural_networks.value_function import ValueFunction\nfrom rlgraph.components.neural_networks.sac.sac_networks import SACValueNetwork\n\n# NeuralNetworks\nNeuralNetwork.__lookup_classes__ = dict(\n    neuralnetwork=NeuralNetwork,\n    multiinputstreamneuralnetwork=MultiInputStreamNeuralNetwork,\n    multiinputstreamnn=MultiInputStreamNeuralNetwork,\n    variationalautoencoder=VariationalAutoEncoder\n)\n\n# The Stacks.\nStack.__lookup_classes__ = dict(\n    dictpreprocessorstack=DictPreprocessorStack,\n    preprocessorstack=PreprocessorStack\n)\n\nValueFunction.__lookup_classes__ = dict(\n    sacvaluefunction=SACValueNetwork,\n    valuefunction=ValueFunction,\n)\nValueFunction.__default_constructor__ = ValueFunction\n\n\n__all__ = [""NeuralNetwork"", ""ValueFunction""] + \\\n          [""Stack""] + \\\n          list(set(map(lambda x: x.__name__, NeuralNetwork.__lookup_classes__.values()))) + \\\n          list(set(map(lambda x: x.__name__, Stack.__lookup_classes__.values()))) + \\\n          list(set(map(lambda x: x.__name__, ValueFunction.__lookup_classes__.values())))\n\n\n'"
rlgraph/components/neural_networks/actor_component.py,0,"b'# Copyright 2018/2019 The RLgraph authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph.components.common.container_merger import ContainerMerger\nfrom rlgraph.components.component import Component\nfrom rlgraph.components.explorations.exploration import Exploration\nfrom rlgraph.components.neural_networks.preprocessor_stack import PreprocessorStack\nfrom rlgraph.components.policies.policy import Policy\nfrom rlgraph.utils.decorators import rlgraph_api\n\n\nclass ActorComponent(Component):\n    """"""\n    A Component that incorporates an entire pipeline from env state to an action choice.\n    Includes preprocessor, policy and exploration sub-components.\n    """"""\n    def __init__(self, preprocessor_spec, policy_spec, exploration_spec=None, **kwargs):\n        """"""\n        Args:\n            preprocessor_spec (Union[list,dict,PreprocessorSpec]):\n                - A dict if the state from the Env will come in as a ContainerSpace (e.g. Dict). In this case, each\n                    each key in this dict specifies, which value in the incoming dict should go through which PreprocessorStack.\n                - A list with layer specs.\n                - A PreprocessorStack object.\n\n            policy_spec (Union[dict,Policy]): A specification dict for a Policy object or a Policy object directly.\n\n            exploration_spec (Union[dict,Exploration]): A specification dict for an Exploration object or an Exploration\n                object directly.\n        """"""\n        super(ActorComponent, self).__init__(scope=kwargs.pop(""scope"", ""actor-component""), **kwargs)\n\n        self.preprocessor = PreprocessorStack.from_spec(preprocessor_spec)\n        self.policy = Policy.from_spec(policy_spec)\n        self.exploration = Exploration.from_spec(exploration_spec)\n\n        self.tuple_merger = ContainerMerger(is_tuple=True, merge_tuples_into_one=True)\n\n        self.add_components(self.policy, self.exploration, self.preprocessor, self.tuple_merger)\n\n    @rlgraph_api\n    def get_preprocessed_state_and_action(\n            self, states, other_nn_inputs=None, time_percentage=None, use_exploration=True\n    ):\n        """"""\n        API-method to get the preprocessed state and an action based on a raw state from an Env.\n\n        Args:\n            states (DataOp): The states coming directly from the environment.\n            other_nn_inputs (Optional[DataOpTuple]): Inputs to the NN that don\'t have to be pushed through the preprocessor.\n\n            time_percentage (SingleDataOp): The current consumed time (0.0 to 1.0) with respect to a max timestep\n                value.\n\n            use_exploration (Optional[DataOp]): Whether to use exploration or not.\n\n        Returns:\n            dict (3x DataOp):\n                `preprocessed_state` (DataOp): The preprocessed states.\n                `action` (DataOp): The chosen action.\n                #`last_internal_states` (DataOp): If RNN-based, the last internal states after passing through\n                #states. Or None.\n        """"""\n        preprocessed_states = self.preprocessor.preprocess(states)\n        nn_inputs = preprocessed_states\n\n        if other_nn_inputs is not None:\n            # TODO: Do this automatically when using the `+` operator on DataOpRecords.\n            nn_inputs = self.tuple_merger.merge(nn_inputs, other_nn_inputs)\n\n        out = self.policy.get_action(nn_inputs)\n\n        actions = self.exploration.get_action(out[""action""], time_percentage, use_exploration)\n        return dict(\n            preprocessed_state=preprocessed_states, action=actions, nn_outputs=out[""nn_outputs""]\n        )\n\n    @rlgraph_api\n    def get_preprocessed_state_action_and_action_probs(\n            self, states, other_nn_inputs=None, time_percentage=None, use_exploration=True\n    ):\n        """"""\n        API-method to get the preprocessed state, one action and all possible action\'s probabilities based on a\n        raw state from an Env.\n\n        Args:\n            states (DataOp): The states coming directly from the environment.\n            other_nn_inputs (DataOp): Inputs to the NN that don\'t have to be pushed through the preprocessor.\n\n            time_percentage (SingleDataOp): The current consumed time (0.0 to 1.0) with respect to a max timestep\n                value.\n\n            use_exploration (Optional[DataOp]): Whether to use exploration or not.\n\n        Returns:\n            dict (4x DataOp):\n                `preprocessed_state` (DataOp): The preprocessed states.\n                `action` (DataOp): The chosen action.\n                `action_probs` (DataOp): The different action probabilities.\n                #`last_internal_states` (DataOp): If RNN-based, the last internal states after passing through\n                #states. Or None.\n        """"""\n        preprocessed_states = self.preprocessor.preprocess(states)\n        nn_inputs = preprocessed_states\n        # merge preprocessed_states + other_nn_inputs\n        if other_nn_inputs is not None:\n            # TODO: Do this automatically when using the `+` operator on DataOpRecords.\n            nn_inputs = self.tuple_merger.merge(nn_inputs, other_nn_inputs)\n\n        # TODO: Dynamic Batching problem. State-value is not really needed, but dynamic batching will require us to\n        # TODO: run through the exact same partial-graph as the learner (which does need the extra state-value output).\n        # if isinstance(self.policy, SharedValueFunctionPolicy):\n        #    out = self.policy.get_state_values_logits_probabilities_log_probs(preprocessed_states, internal_states)\n        # else:\n        # out = self.policy.get_logits_parameters_log_probs(preprocessed_states, internal_states)\n        # action_sample = self.policy.get_action_from_logits_and_parameters(out[""logits""], out[""parameters""])\n\n        out = self.policy.get_action_and_log_likelihood(nn_inputs)\n\n        actions = self.exploration.get_action(out[""action""], time_percentage, use_exploration)\n\n        return dict(\n            preprocessed_state=preprocessed_states, action=actions, action_probs=out[""action_probabilities""],\n            nn_outputs=out[""nn_outputs""]\n        )\n'"
rlgraph/components/neural_networks/dict_preprocessor_stack.py,2,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing import PreprocessLayer\nfrom rlgraph.components.neural_networks.preprocessor_stack import PreprocessorStack\nfrom rlgraph.spaces import ContainerSpace, Dict\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.ops import flatten_op, unflatten_op\nfrom rlgraph.utils.util import default_dict\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass DictPreprocessorStack(PreprocessorStack):\n    """"""\n    A generic PreprocessorStack that can handle Dict/Tuple Spaces and parallelly preprocess different Spaces within\n    different (and separate) single PreprocessorStack components.\n    The output is again a dict of preprocessed inputs.\n    """"""\n    def __init__(self, preprocessors, **kwargs):\n        """"""\n        Args:\n            preprocessors (dict):\n\n        Raises:\n            RLGraphError: If a sub-component is not a PreprocessLayer object.\n        """"""\n        # Create one separate PreprocessorStack per given key.\n        # All possibly other keys in an input will be pass through un-preprocessed.\n        self.flattened_preprocessors = flatten_op(preprocessors)\n        for i, (flat_key, spec) in enumerate(self.flattened_preprocessors.items()):\n            self.flattened_preprocessors[flat_key] = PreprocessorStack.from_spec(\n                spec, scope=""preprocessor-stack-{}"".format(i)\n            )\n\n        # NOTE: No automatic API-methods. Define them all ourselves.\n        kwargs[""api_methods""] = {}\n        default_dict(kwargs, dict(scope=kwargs.pop(""scope"", ""dict-preprocessor-stack"")))\n        super(DictPreprocessorStack, self).__init__(*list(self.flattened_preprocessors.values()), **kwargs)\n\n    @rlgraph_api(flatten_ops=True, split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_preprocess(self, flat_key, inputs):\n        # Is a PreprocessorStack defined for this key?\n        if flat_key in self.flattened_preprocessors:\n            return self.flattened_preprocessors[flat_key].preprocess(inputs)\n        # Simple pass through, no preprocessing.\n        else:\n            return inputs\n\n    @rlgraph_api\n    def reset(self):\n        # TODO: python-Components: For now, we call each preprocessor\'s graph_fn directly.\n        if self.backend == ""python"" or get_backend() == ""python"":\n            for preprocessor in self.flattened_preprocessors.values():  # type: PreprocessLayer\n                preprocessor.reset()\n\n        elif get_backend() == ""tf"":\n            # Connect each pre-processor\'s ""reset"" output op via our graph_fn into one op.\n            resets = list()\n            for preprocessor in self.flattened_preprocessors.values():  # type: PreprocessorStack\n                resets.append(preprocessor.reset())\n            reset_op = self._graph_fn_reset(*resets)\n            return reset_op\n\n    @graph_fn\n    def _graph_fn_reset(self, *preprocessor_resets):\n        if get_backend() == ""tf"":\n            with tf.control_dependencies(preprocessor_resets):\n                return tf.no_op()\n\n    def get_preprocessed_space(self, space):\n        """"""\n        Returns the Space obtained after pushing the input through all layers of this Stack.\n\n        Args:\n            space (Dict): The incoming Space object.\n\n        Returns:\n            Space: The Space after preprocessing.\n        """"""\n        assert isinstance(space, ContainerSpace)\n        dict_spec = dict()\n        for flat_key, sub_space in space.flatten().items():\n            if flat_key in self.flattened_preprocessors:\n                dict_spec[flat_key] = self.flattened_preprocessors[flat_key].get_preprocessed_space(sub_space)\n            else:\n                dict_spec[flat_key] = sub_space\n        dict_spec = unflatten_op(dict_spec)\n        return Dict(dict_spec)\n'"
rlgraph/components/neural_networks/multi_input_stream_neural_network.py,0,"b'# Copyright 2018/2019 ducandu GmbH. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph.components.layers.nn.concat_layer import ConcatLayer\nfrom rlgraph.components.neural_networks.neural_network import NeuralNetwork\nfrom rlgraph.utils.ops import flatten_op\nfrom rlgraph.utils.decorators import rlgraph_api\n\n\nclass MultiInputStreamNeuralNetwork(NeuralNetwork):\n    """"""\n    A NeuralNetwork that takes n separate input-streams and feeds each of them separately through a different NN.\n    The final outputs of these NNs are then all concatenated and fed further through an (optional) post-network.\n    """"""\n    def __init__(self, input_network_specs, post_network_spec=None, **kwargs):\n        """"""\n        Args:\n            input_network_specs (Union[Dict[str,dict],Tuple[dict]]): A specification dict or tuple with values being\n                the spec dicts for the single streams. The `call` method expects a dict input or a single tuple input\n                (not as *args) in its first parameter.\n\n            post_network_spec (Optional[]): The specification dict of the post-concat network or the post-concat\n                network object itself.\n        """"""\n        super(MultiInputStreamNeuralNetwork, self).__init__(scope=""multi-input-stream-nn"", **kwargs)\n\n        # Create all streams\' networks.\n        if isinstance(input_network_specs, dict):\n            self.input_stream_nns = {}\n            for i, (flat_key, nn_spec) in enumerate(flatten_op(input_network_specs).items()):\n                self.input_stream_nns[flat_key] = NeuralNetwork.from_spec(nn_spec, scope=""input-stream-nn-{}"".format(i))\n            # Create the concat layer to merge all streams.\n            self.concat_layer = ConcatLayer(dict_keys=list(self.input_stream_nns.keys()), axis=-1)\n        else:\n            assert isinstance(input_network_specs, (list, tuple)),\\\n                ""ERROR: `input_network_specs` must be dict or tuple/list!""\n            self.input_stream_nns = []\n            for i, nn_spec in enumerate(input_network_specs):\n                self.input_stream_nns.append(NeuralNetwork.from_spec(nn_spec, scope=""input-stream-nn-{}"".format(i)))\n            # Create the concat layer to merge all streams.\n            self.concat_layer = ConcatLayer(axis=-1)\n\n        # Create the post-network (after the concat).\n        self.post_nn = NeuralNetwork.from_spec(post_network_spec, scope=""post-concat-nn"")  # type: NeuralNetwork\n\n        # Add all sub-Components.\n        self.add_components(\n            self.post_nn, self.concat_layer,\n            *list(self.input_stream_nns.values() if isinstance(input_network_specs, dict) else self.input_stream_nns)\n        )\n\n    @rlgraph_api\n    def call(self, inputs):\n        """"""\n        Feeds all inputs through the sub networks\' apply methods and concats their outputs and sends that\n        concat\'d output through the post-network.\n        """"""\n        # Feed all inputs through their respective NNs.\n        if isinstance(self.input_stream_nns, dict):\n            outputs = {}\n            # TODO: Support last-timestep returning LSTMs in input-stream-networks.\n            for input_stream_flat_key, input_stream_nn in self.input_stream_nns.items():\n                outputs[input_stream_flat_key] = input_stream_nn.call(inputs[input_stream_flat_key])\n            # Concat everything.\n            concat_output = self.concat_layer.call(outputs)\n        else:\n            outputs = []\n            # TODO: Support last-timestep returning LSTMs in input-stream-networks.\n            for i, input_stream_nn in enumerate(self.input_stream_nns):\n                outputs.append(input_stream_nn.call(inputs[i]))\n            # Concat everything.\n            concat_output = self.concat_layer.call(*outputs)\n\n        # Send everything through post-network.\n        post_nn_out = self.post_nn.call(concat_output)\n\n        return post_nn_out\n\n    def add_layer(self, layer_component):\n        """"""\n        Overwrite this by adding any new layer to the post-network (most obvious behavior).\n        """"""\n        return self.post_nn.add_layer(layer_component)\n'"
rlgraph/components/neural_networks/neural_network.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport copy\nimport re\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing.container_splitter import ContainerSplitter\nfrom rlgraph.components.component import Component\nfrom rlgraph.components.layers.nn.lstm_layer import LSTMLayer\nfrom rlgraph.components.neural_networks.stack import Stack\nfrom rlgraph.spaces.containers import ContainerSpace\nfrom rlgraph.utils import force_tuple, force_list\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.rlgraph_errors import RLGraphKerasStyleAssemblyError\n\nif get_backend() == ""pytorch"":\n    import torch\n\n\nclass NeuralNetwork(Stack):\n    """"""\n    A NeuralNetwork is a Stack, in which the `call` method is defined either by custom-API-method OR by connecting\n    through all sub-Components\' `call` methods. The signature of the `call` method is always (self, *inputs).\n    In all cases, 1 or more values may be returned by `call`.\n    No other API methods other than `call` should be defined/used.\n    """"""\n    def __init__(self, *layers, **kwargs):\n        """"""\n        Args:\n            *layers (Component): Same as `sub_components` argument of Stack. Can be used to add Layer Components\n                (or any other Components) to this Network.\n\n        Keyword Args:\n            layers (Optional[list]): An optional list of Layer objects or spec-dicts to overwrite(!)\n                *layers.\n\n            inputs (Optional[List[Space]]): A list of Spaces or a single Space object defining the input spaces for\n                the `call` method of this network. Must be provided, if more than one input arg are needed by `call`\n                to determine the order in which these inputs will come in.\n\n            outputs (Optional[List[NNCallOutput]]): A list or single output NNCallOutput object,\n                indicating that we have to infer the `call` method from the graph given by these outputs.\n                This is used iff a NN is constructed by the Keras-style functional API.\n\n            num_inputs (Optional[int]): An optional number of inputs the `call` method will take as `*inputs`.\n                If not given, NN will try to infer this value automatically.\n\n            fold_time_rank (bool): Whether to overwrite the `fold_time_rank` option for the apply method.\n                Only for auto-generated `call` method. Default: None.\n\n            unfold_time_rank (bool): Whether to overwrite the `unfold_time_rank` option for the `call` method.\n                Only for auto-generated `call` method. Default: None.\n        """"""\n        # In case layers come in via a spec dict -> push it into *layers.\n        layers_args = kwargs.pop(""layers"", layers)\n        # Add a default scope (if not given) and pass on via kwargs.\n        kwargs[""scope""] = kwargs.get(""scope"", ""neural-network"")\n        self.keras_style_api_outputs = force_list(kwargs.pop(""outputs"", None))\n        self.keras_style_api_inputs = force_list(kwargs.pop(""inputs"", []))\n        # If Keras-style inputs are given, just count those, otherwise allow for `num_inputs` hint (default: 1).\n        self.num_inputs = len(self.keras_style_api_inputs)\n        if self.num_inputs == 0:\n            self.num_inputs = kwargs.pop(""num_inputs"", 1)\n        self.num_outputs = min(len(self.keras_style_api_outputs), 1)\n\n        # Force the only API-method to be `call`. No matter whether custom-API or auto-generated (via Stack).\n        self.custom_call_given = True\n        if not hasattr(self, ""call""):\n            # Automatically create the `call` stack.\n            if ""api_methods"" not in kwargs:\n                kwargs[""api_methods""] = [dict(api=""call_shadowed_"", component_api=""call"")]\n                self.custom_call_given = False\n            # Sanity check `api_method` to contain only specifications on `call`.\n            else:\n                assert len(kwargs[""api_methods""]) == 1, \\\n                    ""ERROR: Only 0 or 1 given API-methods are allowed in NeuralNetwork ctor! You provided "" \\\n                    ""\'{}\'."".format(kwargs[""api_methods""])\n                # Make sure the only allowed api_method is `call`.\n                assert next(iter(kwargs[""api_methods""]))[0] == ""call"", \\\n                    ""ERROR: NeuralNetwork\'s custom API-method must be called `call`! You named it \'{}\'."". \\\n                    format(next(iter(kwargs[""api_methods""]))[0])\n\n            # Follow given options.\n            fold_time_rank = kwargs.pop(""fold_time_rank"", None)\n            if fold_time_rank is not None:\n                kwargs[""api_methods""][0][""fold_time_rank""] = fold_time_rank\n            unfold_time_rank = kwargs.pop(""unfold_time_rank"", None)\n            if unfold_time_rank is not None:\n                kwargs[""api_methods""][0][""unfold_time_rank""] = unfold_time_rank\n\n        assert len(self.keras_style_api_outputs) == 0 or self.custom_call_given is False, \\\n            ""ERROR: If functional API is used to construct network, a custom `call` method must not be provided!""\n\n        # Pytorch specific objects.\n        self.network_obj = None\n        self.non_layer_components = None\n\n        super(NeuralNetwork, self).__init__(*layers_args, **kwargs)\n\n        # In case we have more than one input (and not using Keras-style assembly),\n        # add another input splitter here.\n        self.inputs_splitter = None\n        if self.num_inputs > 1:\n            self.inputs_splitter = ContainerSplitter(tuple_length=self.num_inputs, scope="".helper-inputs-splitter"")\n            self.add_components(self.inputs_splitter)\n\n    def build_auto_api_method(self, stack_api_method_name, component_api_method_name, fold_time_rank=False,\n                              unfold_time_rank=False, ok_to_overwrite=False):\n\n        if get_backend() == ""pytorch"" and self.execution_mode == ""define_by_run"":\n            @rlgraph_api(name=stack_api_method_name, component=self, ok_to_overwrite=ok_to_overwrite)\n            def method(self, nn_input, *nn_inputs, **kwargs):\n                # Avoid jumping back between layers and calls at runtime.\n                return self._pytorch_fast_path_exec(*([nn_input] + list(nn_inputs)), **kwargs)\n\n        # Functional API (Keras Style assembly). TODO: Add support for pytorch.\n        elif len(self.keras_style_api_outputs) > 0:\n            self._build_call_via_keras_style_functional_api(*self.keras_style_api_outputs)\n\n        # Auto call-API -> Handle LSTMs correctly.\n        elif self.custom_call_given is False:\n            self._build_auto_call_method(fold_time_rank, unfold_time_rank)\n\n        # Have super class (Stack) handle registration of given custom `call` method.\n        else:\n            super(NeuralNetwork, self).build_auto_api_method(\n                stack_api_method_name, component_api_method_name, fold_time_rank, unfold_time_rank, True\n            )\n\n    def _unfold(self, original_input, *args_, **kwargs_):\n        if args_ == ():\n            assert len(kwargs_) == 1, \\\n                ""ERROR: time-rank-unfolding not supported for more than one NN-return value!""\n            key = next(iter(kwargs_))\n            kwargs_ = {key: self.unfolder.call(kwargs_[key], original_input)}\n        else:\n            assert len(args_) == 1, \\\n                ""ERROR: time-rank-unfolding not supported for more than one NN-return value!""\n        args_ = (self.unfolder.call(args_[0], original_input),)\n        return args_, kwargs_\n\n    def _fold(self, *args_, **kwargs_):\n        if args_ == ():\n            assert len(kwargs_) == 1, \\\n                ""ERROR: time-rank-unfolding not supported for more than one NN-return value!""\n            key = next(iter(kwargs_))\n            kwargs_ = {key: self.folder.call(kwargs_[key])}\n        else:\n            args_ = (self.folder.call(args_[0]),)\n        return args_, kwargs_\n\n    def add_layer(self, layer_component):\n        """"""\n        Adds an additional Layer Component (even after c\'tor execution) to this NN.\n        TODO: Currently, layers are always added to the end.\n\n        Args:\n            layer_component (Layer): The Layer object to be added to this NN.\n        """"""\n        assert self.custom_call_given is False,\\\n            ""ERROR: Cannot add layer to neural network if `call` API-method is a custom one!""\n        assert hasattr(layer_component, self.map_api_to_sub_components_api[""call_shadowed_""]), \\\n            ""ERROR: Layer to be added ({}) does not have an API-method called \'{}\'!"".format(\n                layer_component.scope, self.map_api_to_sub_components_api[""call_shadowed_""]\n            )\n        self.add_components(layer_component)\n        self.build_auto_api_method(""call_shadowed_"", self.map_api_to_sub_components_api[""call_shadowed_""],\n                                   ok_to_overwrite=True)\n\n    def _pytorch_fast_path_exec(self, *inputs, **kwargs):\n        """"""\n        Builds a fast-path execution method for pytorch / eager.\n        """"""\n        inputs = inputs[0]\n        forward_inputs = []\n        for v in inputs:\n            if v is not None:\n                if isinstance(v, tuple):\n                    # Unitary tuples\n                    forward_inputs.append(v[0])\n                else:\n                    forward_inputs.append(v)\n        result = self.network_obj.forward(*forward_inputs)\n        # Problem: Not everything in the neural network stack is a true layer.\n        for c in self.non_layer_components:\n            result = getattr(c, ""call"")(*force_list(result))\n        return result\n\n    def post_define_by_run_build(self):\n        # Layer objects only exist after build - define torch neural network.\n        layer_objects = []\n        self.non_layer_components = []\n        for component in self.sub_components.values():\n            if hasattr(component, ""layer""):\n                # Store Layer object itself.\n                layer_objects.append(component.layer)\n\n                # Append activation fn if needed.\n                # N.b. linear returns None here.\n                if component.activation_fn is not None:\n                    layer_objects.append(component.activation_fn)\n            else:\n                self.non_layer_components.append(component)\n        self.network_obj = torch.nn.Sequential(*layer_objects)\n\n    def has_rnn(self):\n        """"""\n        Returns:\n            True if one of our sub-Components is an LSTMLayer, False otherwise.\n        """"""\n        # TODO: Maybe it would be better to create a child class (RecurrentNeuralNetwork with has_rrn=True and\n        # TODO: other available information for its API-clients such as internal_states_space, etc..)\n        return any(isinstance(sc, LSTMLayer) for sc in self.get_all_sub_components())\n\n    def _build_call_via_keras_style_functional_api(self, *layer_call_outputs):\n        """"""\n        Automatically builds our `call` method by traversing the given graph depth first via the following iterative\n        procedure:\n\n        Add given `layer_call_outputs` to a set.\n        While still items in set that are not Spaces:\n            For o in set:\n                If o is lone output for its call OR all outputs are in set.\n                    write call to code\n                    erase outs from set\n                    add ins to set\n        Write `def call(self, ...)` from given Spaces.\n        """"""\n        output_set = set(layer_call_outputs)\n        output_id = 0\n        sub_components = set()\n\n        def _all_siblings_in_set(output, set_):\n            siblings = []\n            need_to_find = output.num_outputs\n            for o in set_:\n                if o.component == output.component:\n                    siblings.append(o)\n            return len(siblings) == need_to_find, sorted(siblings, key=lambda s: s.output_slot)\n\n        # Initialize var names for final outputs.\n        for out in sorted(output_set):\n            out.var_name = ""out{}"".format(output_id)\n            output_id += 1\n\n        # Write this NN\'s `call` API-method code dynamically, then execute it.\n        call_code = ""\\treturn {}\\n"".format("", "".join([o.var_name for o in layer_call_outputs]))\n\n        prev_output_set = None\n\n        # Input Space-IDs that we know will be used.\n        functional_api_input_ids = [space.id for space in self.keras_style_api_inputs]\n        # If no inputs given -> Allow only a single-input arg setup (otherwise, there would be\n        # ambiguity).\n        auto_functional_api_single_input = None\n\n        # Loop through all nodes.\n        while len(output_set) > 0:\n            output_list = list(output_set)\n\n            output = next(iter(sorted(output_list)))\n\n            # If only one output OR all outputs are in set -> Write the call.\n            found_all, siblings = _all_siblings_in_set(output, output_set)\n            if found_all is True:\n                siblings_str = "", "".join([o.var_name for o in siblings])\n            # Nothing has changed and it\'s the only output in list\n            # Some output(s) may be dead ends (construct those as `_`).\n            elif prev_output_set == output_set or (prev_output_set is None and len(output_set) == 1):\n                indices = [s.output_slot for s in siblings]\n                siblings_str = """"\n                for i in range(output.num_outputs):\n                    siblings_str += "", "" + (siblings[indices.index(i)].var_name if i in indices else ""_"")\n                siblings_str = siblings_str[2:]  # cut preceding "", ""\n            else:\n                continue\n\n            # Remove outs from set.\n            for sibling in siblings:\n                output_set.remove(sibling)\n            # Add `ins` to set (or set to one of the `inputs[?]` for the `call` method.\n            for pos, in_ in enumerate(output.inputs):\n                # This input is a Space -> If we can find it in `self.keras_style_api_inputs`, use the correct\n                # `inputs[?]` reference here, if not, may be a child of a container input, in which case:\n                # tag it for now with `inputs[Space.id]`.\n                if in_.space is not None:\n                    # Given Space is in this NeuralNetwork\'s given inputs. Use its `*inputs`-index directly.\n                    if in_.space.id in functional_api_input_ids:\n                        in_.var_name = ""inputs[{}]"".format(functional_api_input_ids.index(in_.space.id))\n                    # A child of an input container space. Add the necessary ContainerSplitter and `inputs`-index\n                    # automatically.\n                    else:\n                        if len(functional_api_input_ids) == 0:\n                            top_level_container_space = in_.space.get_top_level_container()\n                            # Make sure it\'s always the same top-level container (only single input allowed in this\n                            # case, due to arg-order ambiguity otherwise).\n                            if auto_functional_api_single_input is not None:\n                                if top_level_container_space != auto_functional_api_single_input:\n                                    raise RLGraphKerasStyleAssemblyError(\n                                        ""When creating NeuralNetwork \'{}\' in Keras-style assembly and not providing ""\n                                        ""the `inputs` arg, only one single input into the Network is allowed! You have ""\n                                        ""{} and {}."".format(self.global_scope, auto_functional_api_single_input,\n                                                            top_level_container_space)\n                                    )\n                            else:\n                                auto_functional_api_single_input = top_level_container_space\n                        # Look for this Space in `self.keras_style_api_inputs`.\n                        index_chain = []\n                        if self._get_container_space_index_chain(\n                                self.keras_style_api_inputs if len(self.keras_style_api_inputs) > 0\n                                else [auto_functional_api_single_input],\n                                in_.space.id, index_chain\n                        ) is False:\n                            raise RLGraphKerasStyleAssemblyError(\n                                ""Input \'{}\' into NeuralNetwork \'{}\' was not found in any of the provided `inputs` ""\n                                ""(or in the auto-derived input)!"".format(in_.space, self.global_scope)\n                            )\n                        in_.var_name = ""inputs[{}]"".format(""]["".join(index_chain))\n                elif in_.var_name is None:\n                    in_.var_name = ""out{}"".format(output_id)\n                    output_id += 1\n                    output_set.add(in_)\n\n            inputs_str = "", "".join([k + i.var_name for i, k in zip(output.inputs, output.kwarg_strings)])\n            call_code = ""\\t{} = self.get_sub_component_by_name(\'{}\').call({})\\n"".format(\n                siblings_str, output.component.scope, inputs_str) + call_code\n            sub_components.add(output.component)\n\n            # Store previous state of our set.\n            prev_output_set = output_set\n\n        # Prepend inputs from left-over Space objects in set.\n        call_code = \\\n            ""@rlgraph_api(component=self, ok_to_overwrite=True)\\n"" + \\\n            ""def call(self, *inputs):\\n"" + \\\n            call_code\n\n        # Add all sub-components to this NN.\n        self.add_components(*list(sub_components))\n\n        # Execute the code and assign self.call to it.\n        print(""`call_code` for NN:"")\n        print(call_code)\n        exec(call_code, globals(), locals())\n\n    def _build_auto_call_method(self, fold_time_rank, unfold_time_rank):\n        @rlgraph_api(component=self, ok_to_overwrite=True)\n        def call(self_, *inputs):\n            # Everything is lumped together in inputs[0] but is supposed to be split -> Do this here.\n            if len(inputs) == 1 and self.num_inputs > 1:\n                inputs = self.inputs_splitter.call(inputs[0])\n\n            inputs = list(inputs)\n            original_input = inputs[0]\n\n            # Keep track of the folding status.\n            fold_status = ""unfolded"" if self.has_rnn() else None\n            # Fold time rank? For now only support 1st arg folding/unfolding.\n            if fold_time_rank is True:\n                args_ = tuple([self.folder.call(original_input)] + list(inputs[1:]))\n                fold_status = ""folded""\n            else:\n                # TODO: If only unfolding: Assume for now that 2nd input is the original one (so we can infer\n                # TODO: batch/time dims).\n                if unfold_time_rank is True:\n                    assert len(inputs) >= 2, \\\n                        ""ERROR: In Stack: If unfolding w/o folding, second arg must be the original input!""\n                    original_input = inputs[1]\n                    args_ = tuple([inputs[0]] + list(inputs[2:]))\n                else:\n                    args_ = inputs\n\n            kwargs_ = {}\n\n            # TODO: keep track of LSTMLayers that only return the last time-step (outputs after these Layers\n            # TODO: can no longer be folded, their time-rank is gone for the rest of the NN.\n            for i, sub_component in enumerate(self_.sub_components.values()):  # type: Component\n                if re.search(r\'^\\.helper-\', sub_component.scope):\n                    continue\n\n                # Unfold before an LSTM.\n                if isinstance(sub_component, LSTMLayer) and fold_status != ""unfolded"":\n                    args_, kwargs_ = self._unfold(original_input, *args_, **kwargs_)\n                    fold_status = ""unfolded""\n                # Fold before a non-LSTM if not already done so.\n                elif not isinstance(sub_component, LSTMLayer) and fold_status == ""unfolded"":\n                    args_, kwargs_ = self._fold(*args_, **kwargs_)\n                    fold_status = ""folded""\n\n                results = sub_component.call(*args_, **kwargs_)\n\n                # Recycle args_, kwargs_ for reuse in next sub-Component\'s API-method call.\n                if isinstance(results, dict):\n                    args_ = ()\n                    kwargs_ = results\n                else:\n                    args_ = force_tuple(results)\n                    kwargs_ = {}\n\n            if unfold_time_rank:\n                args_, kwargs_ = self._unfold(original_input, *args_, **kwargs_)\n            if args_ == ():\n                return kwargs_\n            elif len(args_) == 1:\n                return args_[0]\n            else:\n                self.num_outputs = len(args_)\n                return args_\n\n    @staticmethod\n    def _get_container_space_index_chain(spaces, space_id, _index_chain=None):\n        """"""\n        Finds `space_id` in `spaces` and returns the actual path from the top-level Space till the child-Space\n        with id=space_id.\n\n        Args:\n            spaces (Union[List[Space],Tuple[Space],Dict[str,Space]]): The container Space or list of Spaces to look\n                through.\n\n            space_id (int): The ID of the Space, we are trying to find in `spaces`.\n\n            _index_chain (List[str,int]): The indexing chain so far. Starts with the index of the matching parent Space\n                in `spaces`. E.g. given:\n                spaces=(Tuple([spaceA(id=0),Dict(a=SpaceB(id=2), b=SpaceC(id=5))]))\n                space_id=5\n                -> returns: [1, ""b""] -> pick index 1 in Tuple, then key ""b"" in Dict.\n\n        Returns:\n            List[str]: A list of inputs indices, e.g. [""0"", ""\'img\'"", ""2""] to go from the top-level Space in `spaces`\n                to the given Space\'s id.\n        """"""\n        assert isinstance(spaces, (tuple, list, dict)), \\\n            ""ERROR: `spaces` must be tuple/list (Tuple Space) OR dict (Dict Space)!""\n\n        for idx, in_space in (spaces.items() if isinstance(spaces, dict) else enumerate(spaces)):\n            index_chain_copy = copy.deepcopy(_index_chain)\n            # Found the ID.\n            if in_space.id == space_id:\n                _index_chain.append(str(idx) if isinstance(idx, int) else ""\\"""" + idx + ""\\"""")\n                return True\n            # Another container -> recurse.\n            elif isinstance(in_space, ContainerSpace):\n                index_chain_copy.append(str(idx) if isinstance(idx, int) else ""\\"""" + idx + ""\\"""")\n                if NeuralNetwork._get_container_space_index_chain(in_space, space_id, index_chain_copy):\n                    _index_chain[:] = index_chain_copy\n                    return True\n\n        # Not found -> Return False.\n        return False\n'"
rlgraph/components/neural_networks/preprocessor_stack.py,2,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing import PreprocessLayer\nfrom rlgraph.components.neural_networks.stack import Stack\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.util import default_dict\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass PreprocessorStack(Stack):\n    """"""\n    A special Stack that only carries PreprocessLayer Components and bundles all their `reset` output ops\n    into one exposed `reset` output op. Otherwise, behaves like a Stack in feeding the outputs\n    of one sub-Component to the inputs of the next sub-Component, etc..\n    """"""\n    def __init__(self, *preprocessors, **kwargs):\n        """"""\n        Args:\n            preprocessors (PreprocessorLayer): The PreprocessorLayers to add to the Stack and connect to each other.\n\n        Keyword Args:\n            fold_time_rank (bool): Whether to fold the time rank for the `preprocess` API-method stack.\n            unfold_time_rank (bool): Whether to unfold the time rank for the `preprocess` API-method stack.\n\n        Raises:\n            RLGraphError: If a sub-component is not a PreprocessLayer object.\n        """"""\n        self.fold_time_rank = kwargs.get(""fold_time_rank"", False)\n        self.unfold_time_rank = kwargs.get(""unfold_time_rank"", False)\n        # Link sub-Components\' `call` methods together to yield PreprocessorStack\'s `preprocess` method.\n        # NOTE: Do not include `reset` here as it is defined explicitly below.\n        kwargs[""api_methods""] = [dict(api=""preprocess"", component_api=""call"", fold_time_rank=self.fold_time_rank,\n                                      unfold_time_rank=self.unfold_time_rank)]\n        default_dict(kwargs, dict(scope=kwargs.pop(""scope"", ""preprocessor-stack"")))\n        super(PreprocessorStack, self).__init__(*preprocessors, **kwargs)\n\n    @rlgraph_api\n    def reset(self):\n        # TODO: python-Components: For now, we call each preprocessor\'s graph_fn directly.\n        if self.backend == ""python"" or get_backend() == ""python"":\n            for preprocess_layer in self.sub_components.values():  # type: PreprocessLayer\n                if re.search(r\'^\\.helper-\', preprocess_layer.scope):\n                    continue\n                preprocess_layer._graph_fn_reset()\n\n        elif get_backend() == ""tf"":\n            # Connect each pre-processor\'s ""reset"" output op via our graph_fn into one op.\n            resets = list()\n            for preprocess_layer in self.sub_components.values():  # type: PreprocessLayer\n                if re.search(r\'^\\.helper-\', preprocess_layer.scope):\n                    continue\n                resets.append(preprocess_layer.reset())\n            reset_op = self._graph_fn_reset(*resets)\n            return reset_op\n\n    @graph_fn\n    def _graph_fn_reset(self, *preprocessor_resets):\n        if get_backend() == ""tf"":\n            with tf.control_dependencies(preprocessor_resets):\n                return tf.no_op()\n\n    def get_preprocessed_space(self, space):\n        """"""\n        Returns the Space obtained after pushing the input through all layers of this Stack.\n\n        Args:\n            space (Space): The incoming Space object.\n\n        Returns:\n            Space: The Space after preprocessing.\n        """"""\n        for pp in self.sub_components.values():\n            # not in [""time-rank-folder_"", ""time-rank-unfolder_""] or \\\n            if not re.search(r\'\\.helper-\', pp.scope) or \\\n                    (pp.scope == "".helper-time-rank-folder"" and self.fold_time_rank) or \\\n                    (pp.scope == "".helper-time-rank-unfolder"" and self.unfold_time_rank):\n                space = pp.get_preprocessed_space(space)\n        return space\n'"
rlgraph/components/neural_networks/stack.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport re\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.component import Component\nfrom rlgraph.components.layers.preprocessing import ReShape\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.util import force_tuple, force_list\n\n\nclass Stack(Component):\n    """"""\n    A component container stack that incorporates one or more sub-components some of whose API-methods\n    (default: only `call`) are automatically connected with each other (in the sequence the sub-Components are given\n    in the c\'tor), resulting in an API of the Stack.\n    All sub-components\' API-methods need to match in the number of input and output values. E.g. the third\n    sub-component\'s api-metehod\'s number of return values has to match the forth sub-component\'s api-method\'s number of\n    input parameters.\n    """"""\n    def __init__(self, *sub_components, **kwargs):\n        """"""\n        Args:\n            sub_components (Union[Component,List[Component]]): The sub-components to add to the Stack and connect\n                to each other.\n\n        Keyword Args:\n            api_methods (List[Union[str,Tuple[str,str],dict]]): A list of strings of API-methods names to connect\n                through the stack.\n                Defaults to {""call""}. All sub-Components must implement all API-methods in this set.\n                Alternatively, this set may contain tuples (1st item is the final Stack\'s API method name, 2nd item\n                is the name of the API-methods of the sub-Components to connect through).\n                E.g. api_methods={(""stack_run"", ""run"")}. This will create ""stack_run"" for the Stack, which will call\n                - one by one - all the ""run"" methods of the sub-Components.\n                Alternatively, this set may contain spec-dicts with keys:\n                `api` (exposed final API-method name), `component_api` (sub-Components API-method names to connect\n                through), `function` (the custom API-function to use), `fold_time_rank` (whether to fold a time\n                rank into a batch rank at the beginning), `unfold_time_rank` (whether to unfold the time rank\n                at the end).\n\n                Connecting always works by first calling the first sub-Component\'s API-method, then - with the\n                result - calling the second sub-Component\'s API-method, etc..\n                This is done for all API-methods in the given set, plus - optionally - time rank folding and unfolding\n                at the beginning and/or end.\n        """"""\n        self.api_methods_options = kwargs.pop(""api_methods"", [""call""])\n        super(Stack, self).__init__(*sub_components, scope=kwargs.pop(""scope"", ""stack""), **kwargs)\n\n        self.map_api_to_sub_components_api = dict()\n\n        self.folder = ReShape(fold_time_rank=True, scope="".helper-time-rank-folder"")\n        self.unfolder = ReShape(unfold_time_rank=True, scope="".helper-time-rank-unfolder"")\n\n        self.add_components(self.folder, self.unfolder)\n\n        self._build_stack(self.api_methods_options)\n\n    def _build_stack(self, api_methods):\n        """"""\n        For each api-method in set `api_methods`, automatically create this Stack\'s own API-method by connecting\n        through all sub-Component\'s API-methods. This is skipped if this Stack already has a custom API-method\n        by that name.\n\n        Args:\n            api_methods (List[Union[str,Tuple[str,str],dict]]): See ctor kwargs.\n        """"""\n        # Loop through the API-method set and register each one.\n        for api_method_spec in api_methods:\n            function_to_use = None\n            fold_time_rank = False\n            unfold_time_rank = False\n\n            # Detailed spec-dict given.\n            if isinstance(api_method_spec, dict):\n                stack_api_method_name = api_method_spec[""api""]\n                component_api_method_name = api_method_spec.get(""component_api"", stack_api_method_name)\n                function_to_use = api_method_spec.get(""function"")\n                fold_time_rank = api_method_spec.get(""fold_time_rank"", False)\n                unfold_time_rank = api_method_spec.get(""unfold_time_rank"", False)\n            # API-method of sub-Components and this Stack should have different names.\n            elif isinstance(api_method_spec, tuple):\n                # Custom method given, use that instead of creating one automatically.\n                if callable(api_method_spec[1]):\n                    stack_api_method_name = component_api_method_name = api_method_spec[0]\n                    function_to_use = api_method_spec[1]\n                else:\n                    stack_api_method_name, component_api_method_name = api_method_spec[0], api_method_spec[1]\n            # API-method of sub-Components and this Stack should have the same name.\n            else:\n                stack_api_method_name = component_api_method_name = api_method_spec\n\n            self.map_api_to_sub_components_api[stack_api_method_name] = component_api_method_name\n\n            # API-method for this Stack does not exist yet -> Automatically create it.\n            if not hasattr(self, stack_api_method_name):\n                # Custom API-method is given (w/o decorator) -> Call the decorator directly here to register it.\n                if function_to_use is not None:\n                    rlgraph_api(api_method=function_to_use, component=self, name=stack_api_method_name)\n                # No API-method given -> Create auto-API-method and set it up through decorator.\n                else:\n                    self.build_auto_api_method(\n                        stack_api_method_name, component_api_method_name, fold_time_rank=fold_time_rank,\n                        unfold_time_rank=unfold_time_rank\n                    )\n\n    def build_auto_api_method(self, stack_api_method_name, sub_components_api_method_name,\n                              fold_time_rank=False, unfold_time_rank=False, ok_to_overwrite=False):\n        """"""\n        Creates and registers an auto-API method for this stack.\n\n        Args:\n            stack_api_method_name (str): The name for the (exposed) API-method of the Stack.\n\n            sub_components_api_method_name (str): The name of the single sub-components\' API-methods to call one after\n                another.\n\n            ok_to_overwrite (Optional[bool]): Set to True if we know we are overwriting\n        """"""\n        @rlgraph_api(name=stack_api_method_name, component=self, ok_to_overwrite=ok_to_overwrite)\n        def method(self_, *inputs, **kwargs):\n            # Fold time rank? For now only support 1st arg folding/unfolding.\n            original_input = inputs[0]\n            if fold_time_rank is True:\n                args_ = tuple([self.folder.call(original_input)] + list(inputs[1:]))\n            else:\n                # TODO: If only unfolding: Assume for now that 2nd input is the original one (so we can infer\n                # TODO: batch/time dims).\n                if unfold_time_rank is True:\n                    assert len(inputs) >= 2, \\\n                        ""ERROR: In Stack: If unfolding w/o folding, second arg must be the original input!""\n                    original_input = inputs[1]\n                    args_ = tuple([inputs[0]] + list(inputs[2:]))\n                else:\n                    args_ = inputs\n            kwargs_ = kwargs\n\n            for i, sub_component in enumerate(self_.sub_components.values()):  # type: Component\n                if re.search(r\'^\\.helper-\', sub_component.scope):\n                    continue\n                # TODO: python-Components: For now, we call each preprocessor\'s graph_fn\n                #  directly (assuming that inputs are not ContainerSpaces).\n                if self_.backend == ""python"" or get_backend() == ""python"":\n                    graph_fn = getattr(sub_component, ""_graph_fn_"" + sub_components_api_method_name)\n                    # if sub_component.api_methods[components_api_method_name].add_auto_key_as_first_param:\n                    #    results = graph_fn("""", *args_)  # TODO: kwargs??\n                    # else:\n                    results = graph_fn(*args_)\n                elif get_backend() == ""pytorch"":\n                    # Do NOT convert to tuple, has to be in unpacked again immediately.n\n                    results = getattr(sub_component, sub_components_api_method_name)(*force_list(args_))\n                else:  # if get_backend() == ""tf"":\n                    results = getattr(sub_component, sub_components_api_method_name)(*args_, **kwargs_)\n\n                # Recycle args_, kwargs_ for reuse in next sub-Component\'s API-method call.\n                if isinstance(results, dict):\n                    args_ = ()\n                    kwargs_ = results\n                else:\n                    args_ = force_tuple(results)\n                    kwargs_ = {}\n\n            if args_ == ():\n                # Unfold time rank? For now only support 1st arg folding/unfolding.\n                if unfold_time_rank is True:\n                    assert len(kwargs_) == 1,\\\n                        ""ERROR: time-rank-unfolding not supported for more than one NN-return value!""\n                    key = next(iter(kwargs_))\n                    kwargs_ = {key: self.unfolder.call(kwargs_[key], original_input)}\n                return kwargs_\n            else:\n                # Unfold time rank? For now only support 1st arg folding/unfolding.\n                if unfold_time_rank is True:\n                    assert len(args_) == 1,\\\n                        ""ERROR: time-rank-unfolding not supported for more than one NN-return value!""\n                    args_ = tuple([self.unfolder.call(args_[0], original_input)] +\n                                  list(args_[1 if fold_time_rank is True else 2:]))\n                if len(args_) == 1:\n                    return args_[0]\n                else:\n                    return args_\n\n    @classmethod\n    def from_spec(cls, spec=None, **kwargs):\n        spec_deepcopy = copy.deepcopy(spec)\n        if isinstance(spec, dict):\n            kwargs[""_args""] = list(spec_deepcopy.pop(""layers"", []))\n        elif isinstance(spec, (tuple, list)):\n            kwargs[""_args""] = spec_deepcopy\n            spec_deepcopy = None\n        return super(Stack, cls).from_spec(spec_deepcopy, **kwargs)\n'"
rlgraph/components/neural_networks/value_function.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph.components.component import Component\nfrom rlgraph.components.layers.layer import Layer\nfrom rlgraph.components.neural_networks.neural_network import NeuralNetwork\nfrom rlgraph.utils.decorators import rlgraph_api\n\n\nclass ValueFunction(Component):\n    """"""\n    A Value-function is a wrapper Component that contains a NeuralNetwork and adds a value-function output.\n    """"""\n    def __init__(self, network_spec, scope=""value-function"", **kwargs):\n        """"""\n        Args:\n            network_spec (list): Layer specification for baseline network.\n        """"""\n        super(ValueFunction, self).__init__(scope=scope, **kwargs)\n        self.network_spec = network_spec\n\n        # If first layer is conv, build image stack.\n        # TODO: This should not be checked in this way. What if network_spec if a NN Component, a dict, etc..?\n        self.use_image_stack = False\n        if isinstance(self.network_spec, (list, tuple)) and self.network_spec[0][""type""] == ""conv2d"":\n            self.use_image_stack = True\n        self.image_stack = None\n        self.dense_stack = None\n\n        self.neural_network = None\n        self.value_layer_spec = {\n            ""type"": ""dense"",\n            ""units"": 1,\n            ""activation"": ""linear"",\n            ""scope"": ""value-function-output""\n        }\n        self.build_value_function()\n\n    def build_value_function(self):\n        # Attach VF output to hidden layers.\n        self.neural_network = NeuralNetwork.from_spec(self.network_spec)\n        self.neural_network.add_layer(Layer.from_spec(self.value_layer_spec))\n        self.add_components(self.neural_network)\n\n    @rlgraph_api\n    def value_output(self, nn_inputs):\n        """"""\n        Args:\n            nn_inputs (any): The inputs to our neural network.\n\n        Returns:\n            any: Value function estimate V(s) for inputs s.\n        """"""\n        return self.neural_network.call(nn_inputs)\n'"
rlgraph/components/neural_networks/variational_auto_encoder.py,1,"b'# Copyright 2018/2019 ducandu GmbH. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.distributions.normal import Normal\nfrom rlgraph.components.layers.nn.dense_layer import DenseLayer\nfrom rlgraph.components.neural_networks.neural_network import NeuralNetwork\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\n\n# import importlib\n\n#_BACKEND_MOD = importlib.import_module(\n#    ""rlcore.components.neural_networks.""+get_backend()+"".variational_auto_encoder""\n#)\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass VariationalAutoEncoder(NeuralNetwork):\n    def __init__(self, z_units, encoder_network_spec, decoder_network_spec, **kwargs):\n        """"""\n        Args:\n            z_units (int): Number of units of the latent (z) vectors that the encoder will produce.\n\n            encoder_network_spec (Union[dict,NeuralNetwork]): Specification dict to construct an encoder\n                NeuralNetwork object from or a NeuralNetwork Component directly.\n\n            decoder_network_spec (Union[dict,NeuralNetwork]): Specification dict to construct a decoder\n                NeuralNetwork object from or a NeuralNetwork Component directly.\n        """"""\n        super(VariationalAutoEncoder, self).__init__(scope=""variational-auto-encoder"", **kwargs)\n\n        self.z_units = z_units\n\n        # Create encoder and decoder networks.\n        self.encoder_network = NeuralNetwork.from_spec(encoder_network_spec, scope=""encoder-network"")\n        self.decoder_network = NeuralNetwork.from_spec(decoder_network_spec, scope=""decoder-network"")\n\n        # Create the two Gaussian layers.\n        self.mean_layer = DenseLayer(units=self.z_units, scope=""mean-layer"")\n        self.stddev_layer = DenseLayer(units=self.z_units, scope=""stddev-layer"")\n\n        # Create the Normal Distribution from which to sample.\n        self.normal_distribution = Normal()\n\n        # A concat layer to concat mean and stddev before passing it to the Normal distribution.\n        # No longer needed: Pass Tuple (mean + stddev) into API-method instead of concat\'d tensor.\n        #self.concat_layer = ConcatLayer(axis=-1)\n\n        # Add all sub-Components.\n        self.add_components(\n            self.encoder_network, self.decoder_network, self.mean_layer, self.stddev_layer,\n            self.normal_distribution#, self.concat_layer\n        )\n\n    @rlgraph_api\n    def call(self, input_):\n        """"""\n        Our custom `call` method.\n        """"""\n        encoder_out = self.encode(input_)\n        decoder_out = self.decode(encoder_out[""z_sample""])\n        return decoder_out\n\n    @rlgraph_api\n    def encode(self, input_):\n        # Get the encoder raw output.\n        encoder_output = self.encoder_network.call(input_)\n        # Push it through our two mean/std layers.\n        mean = self.mean_layer.call(encoder_output)\n        log_stddev = self.stddev_layer.call(encoder_output)\n        stddev = self._graph_fn_exp(log_stddev)\n        # Generate a Tuple to be passed into `sample_stochastic` as parameters of a Normal distribution.\n        z_sample = self.normal_distribution.sample_stochastic(tuple([mean, stddev]))\n        return dict(z_sample=z_sample, mean=mean, stddev=stddev)\n\n    @rlgraph_api\n    def decode(self, z_vector):\n        return self.decoder_network.call(z_vector)\n\n    @graph_fn\n    def _graph_fn_exp(self, input_):\n        if get_backend() == ""tf"":\n            return tf.exp(input_)\n        elif get_backend() == ""pytorch"":\n            return torch.exp(input_)\n'"
rlgraph/components/optimizers/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom functools import partial\n\nfrom rlgraph.components.optimizers.horovod_optimizer import HorovodOptimizer\nfrom rlgraph.components.optimizers.local_optimizers import *\nfrom rlgraph.components.common.multi_gpu_synchronizer import MultiGpuSynchronizer\nfrom rlgraph.components.optimizers.optimizer import Optimizer\n\n\nOptimizer.__lookup_classes__ = dict(\n    horovod=HorovodOptimizer,\n    #multigpu=MultiGpuSynchronizer,\n    #multigpusync=MultiGpuSynchronizer,\n    # LocalOptimizers.\n    gradientdescent=GradientDescentOptimizer,\n    adagrad=AdagradOptimizer,\n    adadelta=AdadeltaOptimizer,\n    adam=AdamOptimizer,\n    nadam=NadamOptimizer,\n    sgd=SGDOptimizer,\n    rmsprop=RMSPropOptimizer\n)\n\n# The default Optimizer to use if a spec is None and no args/kwars are given.\nOptimizer.__default_constructor__ = partial(GradientDescentOptimizer, learning_rate=0.0001)\n\n__all__ = [""Optimizer"", ""LocalOptimizer"", ""MultiGpuSynchronizer""] + \\\n          list(set(map(lambda x: x.__name__, Optimizer.__lookup_classes__.values())))\n'"
rlgraph/components/optimizers/horovod_optimizer.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend, get_distributed_backend\nfrom rlgraph.components.optimizers.optimizer import Optimizer\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""tf"" and get_distributed_backend() == ""horovod"":\n    import horovod.tensorflow as hvd\nelif get_backend() == ""pytorch"" and get_distributed_backend() == ""horovod"":\n    import horovod.pytorch as hvd\n\n\nclass HorovodOptimizer(Optimizer):\n    """"""\n    This Optimizer provides a wrapper for the horovod optimizer package:\n\n    https://github.com/uber/horovod\n\n    Horovod is meant to be used as an alternative to distributed TensorFlow as it implements\n    communication in a different way, as explained in the Horovod paper:\n\n    arXiv:1802.05799\n\n    This Horovod Optimizer expects a local LocalOptimizer spec (tensorflow) as input.\n    """"""\n    def __init__(self, local_optimizer=None, **kwargs):\n        """"""\n        Initializes a distributed horovod optimizer by wrapping a local optimizer.\n\n        Args:\n            local_optimizer (Optional[dict,LocalOptimizer]): The spec-dict for the wrapped LocalOptimizer object or\n                a LocalOptimizer object itself.\n        """"""\n        super(HorovodOptimizer, self).__init__(**kwargs)\n\n        # Create the horovod wrapper.\n        wrapped_local_optimizer = Optimizer.from_spec(local_optimizer)\n        self.local_optimizer = hvd.DistributedOptimizer(wrapped_local_optimizer)\n\n        @rlgraph_api\n        def step(self, variables, loss, time_percentage, *inputs):\n            grads_and_vars = self._graph_fn_calculate_gradients(variables, loss, time_percentage, *inputs)\n            return self._graph_fn_apply_gradients(grads_and_vars)\n\n    def _graph_fn_calculate_gradients(self, variables, loss, time_percentage, *inputs):\n        return self.local_optimizer._graph_fn_calculate_gradients(variables, loss, time_percentage, *inputs)\n\n    def _graph_fn_apply_gradients(self, grads_and_vars):\n        return self.local_optimizer._graph_fn_apply_gradients(grads_and_vars)\n\n'"
rlgraph/components/optimizers/local_optimizers.py,9,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.common.time_dependent_parameters import Constant\nfrom rlgraph.components.optimizers.optimizer import Optimizer\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.ops import DataOpTuple\nfrom rlgraph.utils.util import force_list\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass LocalOptimizer(Optimizer):\n    """"""\n    A local optimizer performs optimization irrespective of any distributed semantics, i.e.\n    it has no knowledge of other machines and does not implement any communications with them.\n    """"""\n    def __init__(self, learning_rate, clip_grad_norm=None, **kwargs):\n        super(LocalOptimizer, self).__init__(\n            learning_rate=learning_rate, scope=kwargs.pop(""scope"", ""local-optimizer""), **kwargs\n        )\n\n        self.clip_grad_norm = clip_grad_norm\n        if self.clip_grad_norm is not None:\n            assert isinstance(self.clip_grad_norm, float) or isinstance(self.clip_grad_norm, int),\\\n                ""ERROR: \'clip_grad_norm\' must be of type float or int but is type {}"".format(type(self.clip_grad_norm))\n\n        self.input_complete = True\n\n        # The wrapped, backend-specific optimizer object.\n        self.optimizer = None\n\n        # For define-by-run instances.\n        self.optimizer_obj = None\n\n    @rlgraph_api(must_be_complete=False)\n    def _graph_fn_step(self, variables, loss, loss_per_item, time_percentage, *inputs):\n        # TODO n.b. PyTorch does not call api functions because other optimization semantics.\n        if get_backend() == ""tf"":\n            grads_and_vars = self._graph_fn_calculate_gradients(variables, loss, time_percentage)\n            step_op = self._graph_fn_apply_gradients(grads_and_vars)\n            return step_op\n        elif get_backend() == ""pytorch"":\n            # Instantiate optimizer with variables.\n            if self.optimizer_obj is None:\n                # self.optimizer is a lambda creating the respective optimizer\n                # with params pre-filled.\n                parameters = variables.values()\n                self.optimizer_obj = self.optimizer(parameters)\n            # Reset gradients.\n            self.optimizer_obj.zero_grad()\n            if not torch.isnan(loss):\n                loss.backward()\n            # Adjust learning rate via time-dependent parameter if not a constant.\n            if not isinstance(self.learning_rate, Constant):\n                lr = self.learning_rate.get(time_percentage)\n                for param_group in self.optimizer_obj.param_groups:\n                    param_group[""lr""] = lr\n            # Do the optimizer step.\n            return self.optimizer_obj.step()\n\n    @rlgraph_api(must_be_complete=False)\n    def _graph_fn_calculate_gradients(self, variables, loss, time_percentage):\n        """"""\n        Args:\n            variables (DataOpTuple): A list of variables to calculate gradients for.\n            loss (SingeDataOp): The total loss over a batch to be minimized.\n        """"""\n        if get_backend() == ""tf"":\n            var_list = list(variables.values()) if isinstance(variables, dict) else force_list(variables)\n            grads_and_vars = self.optimizer.compute_gradients(\n                loss=loss,\n                var_list=var_list\n            )\n            if self.clip_grad_norm is not None:\n                for i, (grad, var) in enumerate(grads_and_vars):\n                    if grad is not None:\n                        grads_and_vars[i] = (tf.clip_by_norm(t=grad, clip_norm=self.clip_grad_norm), var)\n            return DataOpTuple(grads_and_vars)\n\n    @rlgraph_api(must_be_complete=False)\n    def _graph_fn_apply_gradients(self, grads_and_vars):\n        if get_backend() == ""tf"":\n            return self.optimizer.apply_gradients(\n                grads_and_vars=grads_and_vars\n            )\n\n    def get_optimizer_variables(self):\n        if get_backend() == ""tf"":\n            return self.optimizer.variables()\n        # Nothing to do for PyTorch -> no variable init issues.\n\n\nclass GradientDescentOptimizer(LocalOptimizer):\n    """"""\n    Classic gradient descent optimizer:\n    ""Stochastic Estimation of the Maximum of a Regression Function."" - Kiefer and Wolfowitz, 1952\n    """"""\n    def __init__(self, learning_rate, **kwargs):\n        super(GradientDescentOptimizer, self).__init__(\n            learning_rate=learning_rate,\n            scope=kwargs.pop(""scope"", ""gradient-descent-optimizer""),\n            **kwargs\n        )\n\n    def create_variables(self, input_spaces, action_space=None):\n        if get_backend() == ""tf"":\n            self.optimizer = tf.train.GradientDescentOptimizer(\n                learning_rate=self.learning_rate.placeholder()\n            )\n        elif get_backend() == ""pytorch"":\n            raise NotImplementedError\n\n\nclass AdamOptimizer(LocalOptimizer):\n    """"""\n    Adaptive momentum optimizer:\n    https://arxiv.org/abs/1412.6980\n    """"""\n    def __init__(self, learning_rate, **kwargs):\n        self.beta1 = kwargs.pop(""beta_1"", kwargs.pop(""beta1"", 0.9))\n        self.beta2 = kwargs.pop(""beta_2"", kwargs.pop(""beta2"", 0.999))\n\n        super(AdamOptimizer, self).__init__(\n            learning_rate=learning_rate, scope=kwargs.pop(""scope"", ""adam-optimizer""), **kwargs\n        )\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        if get_backend() == ""tf"":\n            self.optimizer = tf.train.AdamOptimizer(\n                learning_rate=self.learning_rate.placeholder(),\n                beta1=self.beta1,\n                beta2=self.beta2\n            )\n        elif get_backend() == ""pytorch"":\n            # Cannot instantiate yet without weights.\n            self.optimizer = lambda parameters: torch.optim.Adam(\n                parameters,\n                lr=self.learning_rate.from_,\n                betas=(self.beta1, self.beta2)\n            )\n\n\nclass NadamOptimizer(LocalOptimizer):\n    """"""\n    Nesterov-adaptive momentum optimizer which applies Nesterov\'s accelerated gradient to Adam:\n\n    http://cs229.stanford.edu/proj2015/054_report.pdf\n    """"""\n    def __init__(self, learning_rate, **kwargs):\n        self.beta1 = kwargs.pop(""beta_1"", kwargs.pop(""beta1"", 0.9))\n        self.beta2 = kwargs.pop(""beta_2"", kwargs.pop(""beta2"", 0.999))\n        self.schedule_decay = kwargs.pop(""schedule_decay"", 0.004)\n\n        super(NadamOptimizer, self).__init__(\n            learning_rate=learning_rate, scope=kwargs.pop(""scope"", ""nadam-optimizer""), **kwargs\n        )\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        if get_backend() == ""tf"":\n            self.optimizer = tf.keras.optimizers.Nadam(\n                lr=self.learning_rate.placeholder(),\n                beta_1=self.beta1,\n                beta_2=self.beta2,\n                schedule_decay=self.schedule_decay\n            )\n        elif get_backend() == ""pytorch"":\n            raise NotImplementedError\n\n\nclass AdagradOptimizer(LocalOptimizer):\n    """"""\n    Adaptive gradient optimizer which sets small learning rates for frequently appearing features\n    and large learning rates for rare features:\n\n    http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\n    """"""\n    def __init__(self, learning_rate, **kwargs):\n        self.initial_accumulator_value = kwargs.pop(""initial_accumulator_value"", 0.1)\n\n        super(AdagradOptimizer, self).__init__(\n            learning_rate=learning_rate.placeholder(),\n            scope=kwargs.pop(""scope"", ""adagrad-optimizer""),\n            **kwargs\n        )\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        if get_backend() == ""tf"":\n            self.optimizer = tf.train.AdagradOptimizer(\n                learning_rate=self.learning_rate.placeholder(),\n                initial_accumulator_value=self.initial_accumulator_value\n            )\n        elif get_backend() == ""pytorch"":\n            # Cannot instantiate yet without weights.\n            self.optimizer = lambda parameters: torch.optim.Adagrad(\n                parameters,\n                lr=self.learning_rate.from_,\n                initial_accumulator_value=self.initial_accumulator_value\n            )\n\n\nclass AdadeltaOptimizer(LocalOptimizer):\n    """"""\n    Adadelta optimizer which adapts learning rate over time:\n\n    https://arxiv.org/abs/1212.5701\n    """"""\n    def __init__(self, learning_rate, **kwargs):\n        self.rho = kwargs.pop(""rho"", 0.95)\n\n        super(AdadeltaOptimizer, self).__init__(\n            learning_rate=learning_rate, scope=kwargs.pop(""scope"", ""adadelta-optimizer""), **kwargs\n        )\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        if get_backend() == ""tf"":\n            self.optimizer = tf.train.AdadeltaOptimizer(\n                learning_rate=self.learning_rate.placeholder(),\n                rho=self.rho\n            )\n        elif get_backend() == ""pytorch"":\n            # Cannot instantiate yet without weights.\n            self.optimizer = lambda parameters: torch.optim.Adadelta(\n                parameters,\n                lr=self.learning_rate.from_,\n                rho=self.rho\n            )\n\n\nclass SGDOptimizer(LocalOptimizer):\n    """"""\n    Stochastic gradient descent optimizer from tf.keras including support for momentum,\n    learning-rate-decay and Nesterov momentum.\n    """"""\n    def __init__(self, learning_rate, **kwargs):\n        self.momentum = kwargs.pop(""momentum"", 0.0)\n        self.decay = kwargs.pop(""decay"", 0.0)\n        self.nesterov = kwargs.pop(""nesterov"", False)\n\n        super(SGDOptimizer, self).__init__(\n            learning_rate=learning_rate, scope=kwargs.pop(""scope"", ""sgd-optimizer""), **kwargs\n        )\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        if get_backend() == ""tf"":\n            self.optimizer = tf.keras.optimizers.SGD(\n                lr=self.learning_rate.placeholder(),\n                momentum=self.momentum,\n                decay=self.decay,\n                nesterov=self.nesterov\n            )\n        elif get_backend() == ""pytorch"":\n            # Cannot instantiate yet without weights.\n            self.optimizer = lambda parameters: torch.optim.SGD(\n                parameters,\n                lr=self.learning_rate.from_,\n                momentum=self.momentum,\n                weight_decay=self.decay,\n                nesterov=self.nesterov\n            )\n\n\nclass RMSPropOptimizer(LocalOptimizer):\n    """"""\n    RMSProp Optimizer as discussed by Hinton:\n\n    https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n    """"""\n    def __init__(self, learning_rate, **kwargs):\n        self.decay = kwargs.pop(""decay"", 0.99)\n        self.momentum = kwargs.pop(""momentum"", 0.0)\n        self.epsilon = kwargs.pop(""epsilon"", 0.1)\n\n        super(RMSPropOptimizer, self).__init__(\n            learning_rate=learning_rate, scope=kwargs.pop(""scope"", ""rms-prop-optimizer""), **kwargs\n        )\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        if get_backend() == ""tf"":\n            self.optimizer = tf.train.RMSPropOptimizer(\n                learning_rate=self.learning_rate,\n                decay=self.decay,\n                momentum=self.momentum,\n                epsilon=self.epsilon\n            )\n        elif get_backend() == ""pytorch"":\n            # Cannot instantiate yet without weights.\n            self.optimizer = lambda parameters: torch.optim.RMSprop(\n                parameters,\n                lr=self.learning_rate.from_,\n                momentum=self.momentum,\n                weight_decay=self.decay,\n            )\n\n'"
rlgraph/components/optimizers/optimizer.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.common.time_dependent_parameters import TimeDependentParameter\nfrom rlgraph.components.component import Component\nfrom rlgraph.utils.decorators import rlgraph_api\n\n\nclass Optimizer(Component):\n    """"""\n    A component that takes a tuple of variables as in-Sockets and optimizes them according to some loss function\n    or another criterion or method.\n    """"""\n    def __init__(self, learning_rate=None, **kwargs):\n        """"""\n        Args:\n            learning_rate (Optional[float]): The learning rate to use.\n        """"""\n        super(Optimizer, self).__init__(scope=kwargs.pop(""scope"", ""optimizer""), **kwargs)\n\n        self.learning_rate = TimeDependentParameter.from_spec(learning_rate, scope=""learning-rate"")\n\n        self.add_components(self.learning_rate)\n\n    # Make all API-methods `must_be_complete`=False as optimizers don\'t implement `create_variables`.\n    @rlgraph_api(must_be_complete=False)\n    def _graph_fn_step(self, *inputs):\n        """"""\n        Applies an optimization step to a list of variables via a loss.\n\n        Args:\n            inputs (SingleDataOp): Any args to the optimizer to be able to perform gradient calculations from\n                losses and then apply these gradients to some variables.\n\n        Returns:\n\n        """"""\n        raise NotImplementedError\n\n    @rlgraph_api(must_be_complete=False)\n    def _graph_fn_calculate_gradients(self, *inputs):\n        """"""\n        Calculates the gradients for the given variables and the loss function (and maybe other child-class\n        specific input parameters).\n\n        Args:\n            inputs (SingleDataOp): Custom SingleDataOp parameters, dependent on the optimizer type.\n\n        Returns:\n            DataOpTuple: The list of gradients and variables to be optimized.\n        """"""\n        raise NotImplementedError\n\n    @rlgraph_api(must_be_complete=False)\n    def _graph_fn_apply_gradients(self, grads_and_vars):\n        """"""\n        Changes the given variables based on the previously calculated gradients. `gradients` is the output of\n        `self._graph_fn_calculate_gradients`.\n\n        Args:\n            grads_and_vars (DataOpTuple): The list of gradients and variables to be optimized.\n\n        Returns:\n            DataOp: The op to trigger the gradient-application step.\n        """"""\n        raise NotImplementedError\n\n    def get_optimizer_variables(self):\n        """"""\n        Returns this optimizer\'s variables. This extra utility function is necessary because\n        some frameworks like TensorFlow create optimizer variables ""late"", e.g. Adam variables,\n        so they cannot be fetched at graph build time yet.\n\n        Returns:\n            list: List of variables.\n        """"""\n        pass\n'"
rlgraph/components/policies/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.policies.policy import Policy\nfrom rlgraph.components.policies.shared_value_function_policy import SharedValueFunctionPolicy\nfrom rlgraph.components.policies.dueling_policy import DuelingPolicy\n\n# The Stacks.\nPolicy.__lookup_classes__ = dict(\n    policy=Policy,\n    sharedvaluefunctionpolicy=SharedValueFunctionPolicy,\n    duelingpolicy=DuelingPolicy\n)\n\n__all__ = [""Policy""] + \\\n          list(set(map(lambda x: x.__name__, Policy.__lookup_classes__.values())))\n'"
rlgraph/components/policies/dueling_policy.py,2,"b'# Copyright 2018/2019 The Rlgraph Authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.common.softmax import Softmax\nfrom rlgraph.components.layers.nn.dense_layer import DenseLayer\nfrom rlgraph.components.policies.policy import Policy\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.rlgraph_errors import RLGraphObsoletedError\nfrom rlgraph.utils.util import get_rank\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass DuelingPolicy(Policy):\n    def __init__(self, network_spec, units_state_value_stream,\n                 weights_spec_state_value_stream=None, biases_spec_state_value_stream=None,\n                 activation_state_value_stream=""relu"", scope=""dueling-policy"", **kwargs):\n        super(DuelingPolicy, self).__init__(network_spec, scope=scope, **kwargs)\n\n        self.action_space_flattened = self.action_space.flatten()\n\n        # The state-value stream.\n        self.units_state_value_stream = units_state_value_stream\n        self.weights_spec_state_value_stream = weights_spec_state_value_stream\n        self.biases_spec_state_value_stream = biases_spec_state_value_stream\n        self.activation_state_value_stream = activation_state_value_stream\n\n        # Our softmax component to produce probabilities.\n        self.softmax = Softmax()\n\n        # Create all state value extra Layers.\n        # TODO: Make this a NN-spec as well (right now it\'s one layer fixed plus the final value node).\n        self.dense_layer_state_value_stream = DenseLayer(\n            units=self.units_state_value_stream, weights_spec=self.weights_spec_state_value_stream,\n            biases_spec=self.biases_spec_state_value_stream,\n            activation=self.activation_state_value_stream,\n            scope=""dense-layer-state-value-stream""\n        )\n        self.state_value_node = DenseLayer(\n            units=1,\n            activation=""linear"",\n            scope=""state-value-node""\n        )\n\n        self.add_components(self.dense_layer_state_value_stream, self.state_value_node)\n\n    @rlgraph_api\n    def get_state_values(self, nn_inputs):  #, internal_states=None):\n        """"""\n        Returns the state value node\'s output passing some nn-input through the policy and the state-value\n        stream.\n\n        Args:\n            nn_inputs (any): The input to our neural network.\n            #internal_states (Optional[any]): The initial internal states going into an RNN-based neural network.\n\n        Returns:\n            Dict:\n                state_values: The single (but batched) value function node output.\n        """"""\n        nn_outputs = self.get_nn_outputs(nn_inputs)\n        state_values_tmp = self.dense_layer_state_value_stream.call(nn_outputs)\n        state_values = self.state_value_node.call(state_values_tmp)\n\n        return dict(state_values=state_values, nn_outputs=nn_outputs)\n\n    @rlgraph_api\n    def get_state_values_adapter_outputs_and_parameters(self, nn_inputs):\n        """"""\n        Similar to `get_values_logits_probabilities_log_probs`, but also returns in the return dict under key\n        `state_value` the output of our state-value function node.\n\n        Args:\n            nn_inputs (any): The input to our neural network.\n            #internal_states (Optional[any]): The initial internal states going into an RNN-based neural network.\n\n        Returns:\n            Dict:\n                state_values: The single (but batched) value function node output.\n                action_adapter_outputs: The (reshaped) logits from the ActionAdapter.\n                parameters: The parameters for the distribution (gained from the softmaxed logits or interpreting\n                    logits as mean and stddev for a normal distribution).\n                log_probs: The log(probabilities) values.\n                last_internal_states: The last internal states (if network is RNN-based).\n        """"""\n        nn_outputs = self.get_nn_outputs(nn_inputs)\n        advantages, _, _, _ = self._graph_fn_get_adapter_outputs_and_parameters(nn_outputs)\n        state_values_tmp = self.dense_layer_state_value_stream.call(nn_outputs)\n        state_values = self.state_value_node.call(state_values_tmp)\n\n        q_values = self._graph_fn_calculate_q_values(state_values, advantages)\n\n        parameters, probs, log_probs = self._graph_fn_get_parameters_from_q_values(q_values)\n\n        return dict(\n            nn_outputs=nn_outputs, adapter_outputs=q_values, state_values=state_values,\n            parameters=parameters, probabilities=probs, log_probs=log_probs,\n            advantages=advantages, q_values=q_values\n        )\n\n    @rlgraph_api\n    def get_adapter_outputs(self, nn_inputs):\n        """"""\n        Args:\n            nn_inputs (any): The input to our neural network.\n\n        Returns:\n            Dict:\n                nn_outputs: The raw NN outputs.\n                adapter_outputs: The q-values after adding advantages to state values (and subtracting the\n                    mean advantage).\n                advantages:\n                q_values:\n        """"""\n        nn_outputs = self.get_nn_outputs(nn_inputs)\n        advantages, _, _, _ = self._graph_fn_get_adapter_outputs_and_parameters(nn_outputs)\n        state_values_tmp = self.dense_layer_state_value_stream.call(nn_outputs)\n        state_values = self.state_value_node.call(state_values_tmp)\n\n        q_values = self._graph_fn_calculate_q_values(state_values, advantages)\n\n        return dict(\n            nn_outputs=nn_outputs,\n            adapter_outputs=q_values,\n            advantages=advantages,\n            q_values=q_values\n        )\n\n    @rlgraph_api\n    def get_adapter_outputs_and_parameters(self, nn_inputs):\n        """"""\n        Args:\n            nn_inputs (any): The input to our neural network.\n            #internal_states (Optional[any]): The initial internal states going into an RNN-based neural network.\n\n        Returns:\n            Dict:\n                nn_outputs: The raw NN outputs.\n                adapter_outputs: The q-values after adding advantages to state values (and subtracting the\n                    mean advantage).\n                parameters: The parameters for the distribution (gained from the softmaxed logits or interpreting\n                    logits as mean and stddev for a normal distribution).\n                log_probs: The log(probabilities) values iff we have a discrete action space.\n        """"""\n        out = self.get_state_values_adapter_outputs_and_parameters(nn_inputs)\n        return dict(\n            nn_outputs=out[""nn_outputs""],\n            adapter_outputs=out[""adapter_outputs""],\n            parameters=out[""parameters""],\n            log_probs=out[""log_probs""]\n        )\n\n    @graph_fn(flatten_ops=True, split_ops=True)\n    def _graph_fn_calculate_q_values(self, state_value, advantage_values):\n        """"""\n        Args:\n            state_value (SingleDataOp): The single node state-value output.\n            advantage_values (SingleDataOp): The already reshaped advantage-values.\n\n        Returns:\n            SingleDataOp: The calculated, reshaped Q values (for each composite action) based on:\n                Q = V + [A - mean(A)]\n        """"""\n        # Use the very first node as value function output.\n        # Use all following nodes as advantage function output.\n        if get_backend() == ""tf"":\n            # Calculate the q-values according to [1] and return.\n            mean_advantages = tf.reduce_mean(input_tensor=advantage_values, axis=-1, keepdims=True)\n\n            # Make sure we broadcast the state_value correctly for the upcoming q_value calculation.\n            state_value_expanded = state_value\n            for _ in range(get_rank(advantage_values) - 2):\n                state_value_expanded = tf.expand_dims(state_value_expanded, axis=1)\n            q_values = state_value_expanded + advantage_values - mean_advantages\n\n            # q-values\n            return q_values\n\n        elif get_backend() == ""pytorch"":\n            mean_advantages = torch.mean(advantage_values, dim=-1, keepdim=True)\n\n            # Make sure we broadcast the state_value correctly for the upcoming q_value calculation.\n            state_value_expanded = state_value\n            for _ in range(get_rank(advantage_values) - 2):\n                state_value_expanded = torch.unsqueeze(state_value_expanded, dim=1)\n            q_values = state_value_expanded + advantage_values - mean_advantages\n\n            # q-values\n            return q_values\n\n    @graph_fn(flatten_ops=True, split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_get_parameters_from_q_values(self, key, q_values):\n        """"""\n        """"""\n        out = self.action_adapters[key].get_parameters_from_adapter_outputs(q_values)\n        return out[""parameters""], out[""probabilities""], out[""log_probs""]\n\n    def get_state_values_logits_probabilities_log_probs(self, nn_input, internal_states=None):\n        raise RLGraphObsoletedError(\n            ""API method"", ""get_state_values_logits_probabilities_log_probs"",\n            ""get_state_values_adpater_outputs_and_parameters""\n        )\n\n    def get_logits_probabilities_log_probs(self, nn_input, internal_states=None):\n        raise RLGraphObsoletedError(\n            ""API method"", ""get_logits_probabilities_log_probs"",\n            ""get_adapter_outputs_and_parameters""\n        )\n'"
rlgraph/components/policies/dynamic_batching_policy.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.component import Component\nfrom rlgraph.components.policies.policy import Policy\nfrom rlgraph.components.helpers import dynamic_batching\n\n\n# Wrap in dynamic batching module.\n@dynamic_batching.batch_fn\ndef get_state_values_logits_probabilities_log_probs(self_, nn_input_, internal_states_):\n    return self_.policy.get_state_values_logits_probabilities_log_probs(nn_input_, internal_states_)\n\n\nclass DynamicBatchingPolicy(Component):\n    """"""\n    A dynamic batching policy wraps a policy with DeepMind\'s custom\n    dynamic batching ops which are provided as part of their IMPALA open source\n    implementation.\n    """"""\n    def __init__(self, policy_spec, minimum_batch_size=1, maximum_batch_size=1024, timeout_ms=100,\n                 scope=""dynamic-batching-policy"", **kwargs):\n        """"""\n        Args:\n            policy_spec (Union[Optimizer,dict]): A spec dict to construct the Policy that is wrraped by this\n                DynamicBatchingPolicy or a Policy object directly.\n            minimum_batch_size (int): The minimum batch size to use. Default: 1.\n            maximum_batch_size (int): The maximum batch size to use. Default: 1024\n            timeout_ms (int): The time out in ms to use when waiting for items on the queue.\n                Default: 100ms.\n        """"""\n        super(DynamicBatchingPolicy, self).__init__(\n            # 3=states, logits, internal_states\n            graph_fn_num_outputs=dict(_graph_fn_get_state_values_logits_probabilities_log_probs=5), scope=scope, **kwargs\n        )\n\n        # The wrapped, backend-specific policy object.\n        self.policy = Policy.from_spec(policy_spec)\n\n        # hack: link in case parent components call APIs of the distribution directly\n        self.action_adapter = self.policy.action_adapter\n        self.distribution = self.policy.distribution\n        self.deterministic = True\n\n        # Dynamic batching options.\n        self.minimum_batch_size = minimum_batch_size\n        self.maximum_batch_size = maximum_batch_size\n        self.timeout_ms = timeout_ms\n\n        self.add_components(self.policy)\n\n        # TODO: for now, only define this one API-method as this is the only one used in IMPALA.\n        # TODO: Generalize this component so it can wrap arbitrary other components and simulate their API.\n        self.define_api_method(""get_state_values_logits_probabilities_log_probs"",\n                               self._graph_fn_get_state_values_logits_probabilities_log_probs)\n\n    def _graph_fn_get_state_values_logits_probabilities_log_probs(self, nn_input, internal_states=None):\n        out = get_state_values_logits_probabilities_log_probs(self, nn_input, internal_states)\n        return out\n\n    #def _graph_fn_get_logits_probabilities_log_probs(self, nn_input, internal_states=None):\n    #    # Wrap in dynamic batching module.\n    #    @dynamic_batching.batch_fn_with_options(minimum_batch_size=self.minimum_batch_size,\n    #                                            maximum_batch_size=self.maximum_batch_size,\n    #                                            timeout_ms=self.timeout_ms)\n    #    def get_logits_probabilities_log_probs(nn_input_, internal_states_):\n    #        # TODO potentially assign device\n    #        ret = self.policy.get_logits_probabilities_log_probs(nn_input_, internal_states_)\n    #        return ret\n    #    out = get_logits_probabilities_log_probs(nn_input, internal_states)\n    #    return out\n'"
rlgraph/components/policies/policy.py,8,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nfrom rlgraph import get_backend\nfrom rlgraph.components.action_adapters.action_adapter import ActionAdapter\nfrom rlgraph.components.action_adapters.action_adapter_utils import get_action_adapter_type_from_distribution_type, \\\n    get_distribution_spec_from_action_adapter\nfrom rlgraph.components.component import Component\nfrom rlgraph.components.distributions import Distribution\nfrom rlgraph.components.neural_networks.neural_network import NeuralNetwork\nfrom rlgraph.spaces import Space, BoolBox, IntBox, ContainerSpace\nfrom rlgraph.spaces.space_utils import get_default_distribution_from_space\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.ops import FlattenedDataOp, flat_key_lookup\nfrom rlgraph.utils.rlgraph_errors import RLGraphError, RLGraphObsoletedError\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass Policy(Component):\n    """"""\n    A Policy is a wrapper Component that contains a NeuralNetwork, an ActionAdapter and a Distribution Component.\n    """"""\n    def __init__(self, network_spec, action_space=None, action_adapter_spec=None,\n                 deterministic=True, scope=""policy"", distributions_spec=None, **kwargs):\n        """"""\n        Args:\n            network_spec (Union[NeuralNetwork,dict]): The NeuralNetwork Component or a specification dict to build\n                one.\n\n            action_space (Union[dict,Space]): A specification dict to create the Space within which this Component\n                will create actions or the action Space object directly.\n\n            action_adapter_spec (Optional[dict]): A spec-dict to create an ActionAdapter. Use None for the default\n                ActionAdapter object.\n\n            deterministic (bool): Whether to pick actions according to the max-likelihood value or via sampling.\n                Default: True.\n\n            distributions_spec (dict): Specifies bounded and discrete distribution types, and optionally additional\n                configuration parameters such as temperature.\n\n            batch_apply (bool): Whether to wrap both the NN and the ActionAdapter with a BatchApply Component in order\n                to fold time rank into batch rank before a forward pass.\n        """"""\n        super(Policy, self).__init__(scope=scope, **kwargs)\n\n        self.neural_network = NeuralNetwork.from_spec(network_spec)  # type: NeuralNetwork\n        self.deterministic = deterministic\n        self.action_adapters = {}\n        self.distributions = {}\n\n        self.distributions_spec = distributions_spec if distributions_spec is not None else {}\n        self.bounded_distribution_type = self.distributions_spec.get(""bounded_distribution_type"", ""beta"")\n        self.discrete_distribution_type = self.distributions_spec.get(""discrete_distribution_type"", ""categorical"")\n        # For discrete approximations.\n        self.gumbel_softmax_temperature = self.distributions_spec.get(""gumbel_softmax_temperature"", 1.0)\n\n        self.action_space = None\n        self.flat_action_space = None\n        self._create_action_adapters_and_distributions(\n            action_space=action_space, action_adapter_spec=action_adapter_spec\n        )\n\n        self.add_components(\n            *[self.neural_network] + list(self.action_adapters.values()) + list(self.distributions.values())\n        )\n\n    def _create_action_adapters_and_distributions(self, action_space, action_adapter_spec):\n        if action_space is None:\n            adapter = ActionAdapter.from_spec(action_adapter_spec)\n            self.action_space = adapter.action_space\n            # Assert single component action space.\n            assert len(self.action_space.flatten()) == 1, \\\n                ""ERROR: Action space must not be ContainerSpace if no `action_space` is given in Policy constructor!""\n        else:\n            self.action_space = Space.from_spec(action_space)\n\n        self.flat_action_space = self.action_space.flatten()\n\n        # Figure out our Distributions.\n        for i, (flat_key, action_component) in enumerate(self.flat_action_space.items()):\n            # Spec dict.\n            if isinstance(action_adapter_spec, dict):\n                aa_spec = flat_key_lookup(action_adapter_spec, flat_key, action_adapter_spec)\n                aa_spec[""action_space""] = action_component\n            # Simple type spec.\n            elif not isinstance(action_adapter_spec, ActionAdapter):\n                aa_spec = dict(action_space=action_component)\n            # Direct object.\n            else:\n                aa_spec = action_adapter_spec\n\n            if isinstance(aa_spec, dict) and ""type"" not in aa_spec:\n                dist_spec = get_default_distribution_from_space(\n                    action_component, self.bounded_distribution_type, self.discrete_distribution_type,\n                    self.gumbel_softmax_temperature\n                )\n\n                self.distributions[flat_key] = Distribution.from_spec(\n                    dist_spec, scope=""{}-{}"".format(dist_spec[""type""], i)\n                )\n                if self.distributions[flat_key] is None:\n                    raise RLGraphError(\n                        ""ERROR: `action_component` is of type {} and not allowed in {} Component!"".\n                            format(type(action_space).__name__, self.name)\n                    )\n                aa_spec[""type""] = get_action_adapter_type_from_distribution_type(\n                    type(self.distributions[flat_key]).__name__\n                )\n                self.action_adapters[flat_key] = ActionAdapter.from_spec(aa_spec, scope=""action-adapter-{}"".format(i))\n            else:\n                self.action_adapters[flat_key] = ActionAdapter.from_spec(aa_spec, scope=""action-adapter-{}"".format(i))\n                dist_spec = get_distribution_spec_from_action_adapter(self.action_adapters[flat_key])\n                self.distributions[flat_key] = Distribution.from_spec(\n                    dist_spec, scope=""{}-{}"".format(dist_spec[""type""], i)\n                )\n\n    # Define our interface.\n    @rlgraph_api\n    def get_nn_outputs(self, nn_inputs):\n        """"""\n        Args:\n            nn_inputs (any): The inputs to our neural network.\n\n        Returns:\n            any: The raw outputs of the neural network (before it\'s cleaned-up and passed through the ActionAdapter).\n        """"""\n        return self.neural_network.call(nn_inputs)\n\n    @rlgraph_api\n    def get_adapter_outputs(self, nn_inputs):\n        """"""\n        Args:\n            nn_inputs (any): The inputs to our neural network.\n\n        Returns:\n            any: The (reshaped) outputs of the action layer of the ActionAdapter.\n        """"""\n        nn_outputs = self.get_nn_outputs(nn_inputs)\n        nn_main_outputs = nn_outputs\n        if self.neural_network.num_outputs > 1:\n            nn_main_outputs = nn_outputs[0]\n        action_layer_outputs = self._graph_fn_get_adapter_outputs(nn_main_outputs)\n        # Add last internal states to return value.\n        return dict(adapter_outputs=action_layer_outputs, nn_outputs=nn_outputs)\n\n    @rlgraph_api\n    def get_adapter_outputs_and_parameters(self, nn_inputs):\n        """"""\n        Args:\n            nn_inputs (any): The input to our neural network.\n            #internal_states (Optional[any]): The initial internal states going into an RNN-based neural network.\n\n        Returns:\n            Dict:\n                logits: The (reshaped) logits from the ActionAdapter.\n                parameters: The parameters for the distribution.\n                log_probs (Optional): The log(probabilities) values for discrete distributions.\n        """"""\n        nn_outputs = self.get_nn_outputs(nn_inputs)\n        nn_main_outputs = nn_outputs\n        if self.neural_network.num_outputs > 1:\n            nn_main_outputs = nn_outputs[0]\n        out = self._graph_fn_get_adapter_outputs_and_parameters(nn_main_outputs)\n        return dict(\n            nn_outputs=nn_outputs, adapter_outputs=out[0], parameters=out[1],\n            action_probabilities=out[2], log_probs=out[3]\n        )\n\n    @rlgraph_api\n    def get_action(self, nn_inputs, deterministic=None):  # other_nn_inputs=None,\n        """"""\n        Returns an action based on NN output, action adapter output and distribution sampling.\n\n        Args:\n            nn_inputs (any): The input to our neural network.\n            #other_nn_inputs (DataOp): Inputs to the NN that don\'t have to be pushed through the preprocessor.\n            deterministic (Optional[bool]): If not None, use this to determine whether actions should be drawn\n                from the distribution in max-likelihood (deterministic) or stochastic fashion.\n\n        Returns:\n            dict:\n                `action`: The drawn action.\n                #`last_internal_states`: The last internal states (if NN is RNN based, otherwise: None).\n        """"""\n        deterministic = self.deterministic if deterministic is None else deterministic\n\n        out = self.get_adapter_outputs_and_parameters(nn_inputs)  #, other_nn_inputs)\n        action = self._graph_fn_get_action_components(out[""adapter_outputs""], out[""parameters""], deterministic)\n\n        return dict(\n            action=action,\n            nn_outputs=out[""nn_outputs""],\n            adapter_outputs=out[""adapter_outputs""],\n            parameters=out[""parameters""],\n            log_probs=out.get(""log_probs""),\n            action_probabilities=out.get(""action_probabilities"")\n        )\n\n    @rlgraph_api\n    def get_action_and_log_likelihood(self, nn_inputs, deterministic=None):\n        """"""\n        Args:\n            nn_inputs (any): The input to our neural network.\n            #internal_states (Optional[any]): The initial internal states going into an RNN-based neural network.\n            deterministic (Optional[bool]): If not None, use this to determine whether actions should be drawn\n                from the distribution in max-likelihood (deterministic) or stochastic fashion.\n        Returns:\n            dict:\n                `nn_outputs`: The raw output of the neural network.\n                `adapter_outputs`: The (reshaped) raw action adapter output.\n                `action`: The drawn action.\n                `log_likelihood`: The log probability/log likelihood of the drawn action.\n        """"""\n        deterministic = self.deterministic if deterministic is None else deterministic\n        out = self.get_adapter_outputs_and_parameters(nn_inputs)\n        action, log_likelihood = self._graph_fn_get_action_and_log_likelihood(out[""parameters""], deterministic)\n        log_likelihood = self._graph_fn_combine_log_likelihood_over_container_keys(log_likelihood)\n\n        return dict(\n            nn_outputs=out[""nn_outputs""],\n            adapter_outputs=out[""adapter_outputs""],\n            action=action,\n            # log-llh of the drawn action.\n            log_likelihood=log_likelihood,\n            # All action probabilities (discrete case).\n            action_probabilities=out.get(""action_probabilities"")\n        )\n\n    @rlgraph_api(must_be_complete=False)\n    def get_log_likelihood(self, nn_inputs, actions):\n        """"""\n        Computes the log-likelihood for a given set of actions under the distribution induced by a set of states.\n\n        Args:\n            nn_inputs (any): The input to our neural network.\n            actions (any): The actions for which to get the log-likelihood.\n\n        Returns:\n            Log-probs of actions under current policy\n        """"""\n        out = self.get_adapter_outputs_and_parameters(nn_inputs)\n\n        # Probabilities under current action.\n        log_likelihood = self._graph_fn_get_distribution_log_likelihood(out[""parameters""], actions)\n        log_likelihood = self._graph_fn_combine_log_likelihood_over_container_keys(log_likelihood)\n\n        return dict(log_likelihood=log_likelihood, adapter_outputs=out[""adapter_outputs""])\n\n    @rlgraph_api\n    def get_deterministic_action(self, nn_inputs):\n        """"""\n        Args:\n            nn_inputs (any): The input to our neural network.\n            #internal_states (Optional[any]): The initial internal states going into an RNN-based neural network.\n\n        Returns:\n            any: See `get_action`, but with deterministic force set to True.\n        """"""\n        out = self.get_adapter_outputs_and_parameters(nn_inputs)\n        action = self._graph_fn_get_action_components(out[""adapter_outputs""], out[""parameters""], True)\n\n        return dict(action=action, nn_outputs=out[""nn_outputs""])\n\n    @rlgraph_api\n    def get_stochastic_action(self, nn_inputs): #, internal_states=None):\n        """"""\n        Args:\n            nn_inputs (any): The input to our neural network.\n            #internal_states (Optional[any]): The initial internal states going into an RNN-based neural network.\n\n        Returns:\n            any: See `get_action`, but with deterministic force set to False.\n        """"""\n        out = self.get_adapter_outputs_and_parameters(nn_inputs)  # internal_states\n        action = self._graph_fn_get_action_components(out[""adapter_outputs""], out[""parameters""], False)\n\n        return dict(action=action, nn_outputs=out[""nn_outputs""])\n\n    @rlgraph_api\n    def get_entropy(self, nn_inputs):\n        """"""\n        Args:\n            nn_inputs (any): The inputs to our neural network.\n            #internal_states (Optional[any]): The initial internal states going into an RNN-based neural network.\n\n        Returns:\n            any: See Distribution component.\n        """"""\n        out = self.get_adapter_outputs_and_parameters(nn_inputs)  #, internal_states)\n        entropy = self._graph_fn_get_distribution_entropies(out[""parameters""])\n\n        return dict(entropy=entropy, nn_outputs=out[""nn_outputs""])\n\n    @graph_fn(flatten_ops=True, split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_get_adapter_outputs(self, flat_key, nn_outputs):  #, nn_inputs):\n        """"""\n        Pushes the given nn_output through all our action adapters and returns a DataOpDict with the keys corresponding\n        to our `action_space`.\n\n        Args:\n            nn_outputs (DataOp): The output of our neural network.\n            #nn_inputs (DataOp): The original inputs of the NN (that produced the `nn_outputs`).\n\n        Returns:\n            FlattenedDataOp: A DataOpDict with the different action adapter outputs (keys correspond to\n                structure of `self.action_space`).\n        """"""\n        # NN outputs are already split -> Feed flat-key NN output directly into its corresponding action_adapter.\n        if flat_key in self.action_adapters:\n            return self.action_adapters[flat_key].call(nn_outputs)\n        # Many NN outputs, but no action adapters specified for this one -> return nn_outputs as is.\n        elif flat_key != """":\n            return nn_outputs, nn_outputs, None\n\n        ret = FlattenedDataOp()\n        for aa_flat_key, action_adapter in self.action_adapters.items():\n            ret[aa_flat_key] = action_adapter.call(nn_outputs)\n\n        return ret\n\n    @graph_fn(flatten_ops=True, split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_get_adapter_outputs_and_parameters(self, flat_key, nn_outputs):\n        """"""\n        Pushes the given nn_output through all our action adapters\' get_logits_parameters_log_probs API\'s and\n        returns a DataOpDict with the keys corresponding to our `action_space`.\n\n        Args:\n            nn_outputs (DataOp): The output of our neural network.\n            #nn_inputs (DataOp): The original inputs of the NN (that produced the `nn_outputs`).\n\n        Returns:\n            tuple:\n                - FlattenedDataOp: A DataOpDict with the different action adapters\' logits outputs.\n                - FlattenedDataOp: A DataOpDict with the different action adapters\' parameters outputs.\n                - FlattenedDataOp: A DataOpDict with the different action adapters\' log_probs outputs.\n            Note: Keys always correspond to structure of `self.action_space`.\n        """"""\n        # NN outputs are already split -> Feed flat-key NN output directly into its corresponding action_adapter.\n        if flat_key in self.action_adapters:\n            adapter_outs = self.action_adapters[flat_key].call(nn_outputs)\n            params = self.action_adapters[flat_key].get_parameters_from_adapter_outputs(adapter_outs)\n            return adapter_outs, params[""parameters""], params.get(""probabilities""), params.get(""log_probs"")\n        # Many NN outputs, but no action adapters specified for this one -> return nn_outputs as is.\n        elif flat_key != """":\n            return nn_outputs, nn_outputs, None, None\n\n        # There is only a single NN-output, but many action adapters.\n        adapter_outputs = FlattenedDataOp()\n        parameters = FlattenedDataOp()\n        probs = FlattenedDataOp()\n        log_probs = FlattenedDataOp()\n        for aa_flat_key, action_adapter in self.action_adapters.items():\n            adapter_outs = action_adapter.call(nn_outputs)\n            params = action_adapter.get_parameters_from_adapter_outputs(adapter_outs)\n            #out = action_adapter.get_adapter_outputs_and_parameters(nn_outputs, nn_inputs)\n            adapter_outputs[aa_flat_key], parameters[aa_flat_key], probs[aa_flat_key], log_probs[aa_flat_key] = \\\n                adapter_outs, params[""parameters""], params.get(""probabilities""), params.get(""log_probs"")\n\n        return adapter_outputs, parameters, probs, log_probs\n\n    @graph_fn(flatten_ops=""flat_action_space"", split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_get_distribution_entropies(self, flat_key, parameters):\n        """"""\n        Pushes `parameters` through the respective self.distributions\' `entropy` API-methods and returns a\n        DataOp with the entropy values.\n\n        Args:\n            parameters (DataOp): The parameters to define a distribution. This could be a ContainerDataOp, which\n                container the parameter pieces for each action component.\n\n        Returns:\n            SingleDataOp: The DataOp with the `entropy` outputs for the given flat_key distribution.\n        """"""\n        return self.distributions[flat_key].entropy(parameters)\n\n    @graph_fn(flatten_ops=""flat_action_space"", split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_get_distribution_log_likelihood(self, flat_key, parameters, actions):\n        """"""\n        Pushes the given `probabilities` and actions through all our distributions\' `log_prob` API-methods and returns a\n        DataOpDict with the keys corresponding to our `action_space`.\n\n        Args:\n            parameters (DataOp): The parameters to define a distribution.\n            actions (DataOp): The actions for which to return the log-probs.\n\n        Returns:\n            FlattenedDataOp: A DataOpDict with the different distributions\' `log_prob` outputs. Keys always correspond\n                to structure of `self.action_space`.\n        """"""\n        return self.distributions[flat_key].log_prob(parameters, actions)\n\n    @graph_fn(flatten_ops=True)\n    def _graph_fn_combine_log_likelihood_over_container_keys(self, log_likelihoods):\n        """"""\n        If action space is a container space, add log-likelihoods (assuming all distribution components are\n        independent).\n        """"""\n        if isinstance(self.action_space, ContainerSpace):\n            if get_backend() == ""tf"":\n                log_likelihoods = tf.stack(list(log_likelihoods.values()))\n                log_likelihoods = tf.reduce_sum(log_likelihoods, axis=0)\n            elif get_backend() == ""pytorch"":\n                log_likelihoods = torch.stack(list(log_likelihoods.values()))\n                log_likelihoods = torch.sum(log_likelihoods, 0)\n\n        return log_likelihoods\n\n    @graph_fn(flatten_ops=""flat_action_space"", split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_get_action_components(self, flat_key, logits, parameters, deterministic):\n        action_space_component = self.flat_action_space[flat_key]\n\n        # Skip our distribution, iff discrete action-space and deterministic acting (greedy).\n        # In that case, one does not need to create a distribution in the graph each act (only to get the argmax\n        # over the logits, which is the same as the argmax over the probabilities (or log-probabilities)).\n        if isinstance(action_space_component, IntBox) and \\\n                (deterministic is True or (isinstance(deterministic, np.ndarray) and deterministic)):\n            return self._graph_fn_get_deterministic_action_wo_distribution(logits)\n        # Bernoulli: Sigmoid derived p must be larger 0.5.\n        elif isinstance(action_space_component, BoolBox) and \\\n                (deterministic is True or (isinstance(deterministic, np.ndarray) and deterministic)):\n            # Note: Change 0.5 to 1.0, once parameters are logits, not probs anymore (so far, parameters for\n            # Bernoulli distributions are still probs).\n            if get_backend() == ""tf"":\n                return tf.greater(parameters, 0.5)\n            elif get_backend() == ""pytorch"":\n                return torch.gt(parameters, 0.5)\n        # Deterministic is tensor or False. Pass through graph.\n        else:\n            return self.distributions[flat_key].draw(parameters, deterministic)\n\n    @graph_fn(returns=2, flatten_ops=""flat_action_space"", split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_get_action_and_log_likelihood(self, flat_key, parameters, deterministic):\n        # TODO: Utilize same logic in _graph_fn_get_action_components.\n        # TODO: Not working right now, because we would split twice (here and in _graph_fn_get_action_components).\n        action = None\n        log_prob_or_likelihood = None\n\n        action_space_component = self.flat_action_space[flat_key]\n\n        # Categorical: Argmax over raw logits.\n        if isinstance(action_space_component, IntBox) and \\\n                (deterministic is True or (isinstance(deterministic, np.ndarray) and deterministic)):\n            action = self._graph_fn_get_deterministic_action_wo_distribution(parameters)\n            if get_backend() == ""tf"":\n                log_prob_or_likelihood = tf.log(tf.reduce_max(tf.nn.softmax(parameters, axis=-1), axis=-1))\n            elif get_backend() == ""pytorch"":\n                log_prob_or_likelihood = torch.log(torch.max(torch.softmax(parameters, dim=-1), dim=-1)[0])\n        # Bernoulli: Sigmoid derived p must be larger 0.5.\n        elif isinstance(action_space_component, BoolBox) and \\\n                (deterministic is True or (isinstance(deterministic, np.ndarray) and deterministic)):\n            # Note: Change 0.5 to 1.0, once parameters are logits, not probs anymore (so far, parameters for\n            # Bernoulli distributions are still probs).\n            if get_backend() == ""tf"":\n                action = tf.greater(parameters, 0.5)\n                log_prob_or_likelihood = tf.log(tf.where(parameters > 0.5, parameters, 1.0 - parameters))\n            elif get_backend() == ""pytorch"":\n                action = torch.gt(parameters, 0.5)\n                log_prob_or_likelihood = torch.log(torch.where(parameters > 0.5, parameters, 1.0 - parameters))\n        # Deterministic is tensor or False. Pass through graph.\n        else:\n            action, log_prob_or_likelihood = self.distributions[flat_key].sample_and_log_prob(\n                parameters, deterministic\n            )\n\n        return action, log_prob_or_likelihood\n\n    @graph_fn(flatten_ops=True, split_ops=True)\n    def _graph_fn_get_deterministic_action_wo_distribution(self, logits):\n        """"""\n        Use this function only for discrete action spaces to circumvent using a full-blown\n        backend-specific distribution object (e.g. tf.distribution.Multinomial).\n\n        Args:\n            logits (SingleDataOp): Logits over which to pick the argmax (greedy action).\n\n        Returns:\n            SingleDataOp: The argmax over the last rank of the input logits.\n        """"""\n        if get_backend() == ""tf"":\n            return tf.argmax(logits, axis=-1, output_type=tf.int32)\n        elif get_backend() == ""pytorch"":\n            return torch.argmax(logits, dim=-1).int()\n\n    def get_logits_parameters_log_probs(self, nn_inputs, internal_states=None):\n        raise RLGraphObsoletedError(""API-method"", ""get_logits_parameters_log_probs"",\n                                    ""get_adapter_outputs_and_parameters"")\n\n    def get_logits_probabilities_log_probs(self, nn_inputs, internal_states=None):\n        raise RLGraphObsoletedError(""API-method"", ""get_logits_probabilities_log_probs"",\n                                    ""get_adapter_outputs_and_parameters"")\n\n    def get_action_and_log_params(self, nn_inputs, internal_states=None, deterministic=None):\n        raise RLGraphObsoletedError(""API-method"", ""get_action_and_log_params"", ""get_action_and_log_likelihood"")\n\n    def get_action_log_probs(self, nn_inputs, actions):\n        raise RLGraphObsoletedError(""API-method"", ""get_action_log_probs"", ""get_log_likelihood"")\n\n    def get_action_layer_output(self, nn_inputs):\n        raise RLGraphObsoletedError(""API-method"", ""get_action_layer_output"", ""get_adapter_outputs"")\n\n    def get_nn_output(self, nn_inputs):\n        raise RLGraphObsoletedError(""API-method"", ""get_nn_output"", ""get_nn_outputs"")\n'"
rlgraph/components/policies/shared_value_function_policy.py,0,"b'# Copyright 2018/2019 The Rlgraph Authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.layers.nn.dense_layer import DenseLayer\nfrom rlgraph.components.neural_networks.neural_network import NeuralNetwork\nfrom rlgraph.components.policies.policy import Policy\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.rlgraph_errors import RLGraphObsoletedError\n\n\nclass SharedValueFunctionPolicy(Policy):\n    def __init__(self, network_spec, value_weights_spec=None, value_biases_spec=None, value_activation=None,\n                 value_fold_time_rank=False, value_unfold_time_rank=False,\n                 scope=""shared-value-function-policy"", **kwargs):\n        super(SharedValueFunctionPolicy, self).__init__(network_spec, scope=scope, **kwargs)\n\n        # Create the extra value dense layer with 1 node.\n        self.value_unfold_time_rank = value_unfold_time_rank\n        self.value_network = NeuralNetwork(DenseLayer(\n            units=1,\n            activation=value_activation,\n            weights_spec=value_weights_spec,\n            biases_spec=value_biases_spec,\n        ), fold_time_rank=value_fold_time_rank, unfold_time_rank=value_unfold_time_rank,\n            scope=""value-function-node"")\n\n        self.add_components(self.value_network)\n\n    @rlgraph_api\n    def get_state_values(self, nn_inputs):  # , internal_states=None\n        """"""\n        Returns the state value node\'s output.\n\n        Args:\n            nn_inputs (any): The input to our neural network.\n            #internal_states (Optional[any]): The initial internal states going into an RNN-based neural network.\n\n        Returns:\n            Dict:\n                state_values: The single (but batched) value function node output.\n        """"""\n        nn_outputs = self.get_nn_outputs(nn_inputs)\n        #if self.value_unfold_time_rank is True:\n        #    state_values = self.value_network.call(nn_outputs, nn_inputs)\n        #else:\n        state_values = self.value_network.call(nn_outputs)\n\n        return dict(state_values=state_values, nn_outputs=nn_outputs)\n\n    @rlgraph_api\n    def get_state_values_adapter_outputs_and_parameters(self, nn_inputs):  #, internal_states=None\n        """"""\n        Similar to `get_values_logits_probabilities_log_probs`, but also returns in the return dict under key\n        `state_value` the output of our state-value function node.\n\n        Args:\n            nn_inputs (any): The input to our neural network.\n            #internal_states (Optional[any]): The initial internal states going into an RNN-based neural network.\n\n        Returns:\n            Dict:\n                nn_outputs: The raw NN outputs.\n                state_values: The single (but batched) value function node output.\n                adapter_outputs: The (reshaped) logits from the ActionAdapter.\n                parameters: The parameters for the distribution (gained from the softmaxed logits or interpreting\n                    logits as mean and stddev for a normal distribution).\n                log_probs: The log(probabilities) values.\n        """"""\n        nn_outputs = self.get_nn_outputs(nn_inputs)\n        adapter_outputs, parameters, probs, log_probs = self._graph_fn_get_adapter_outputs_and_parameters(nn_outputs)\n        #if self.value_unfold_time_rank is True:\n        #    state_values = self.value_network.call(nn_outputs, nn_inputs)\n        #else:\n        state_values = self.value_network.call(nn_outputs)\n\n        return dict(\n            nn_outputs=nn_outputs, state_values=state_values, adapter_outputs=adapter_outputs,\n            parameters=parameters, probabilities=probs, log_probs=log_probs\n        )\n\n    def get_state_values_logits_probabilities_log_probs(self, nn_input, internal_states=None):\n        raise RLGraphObsoletedError(\n            ""API-method"", ""get_state_values_logits_probabilities_log_probs"",\n            ""get_state_values_adapter_outputs_and_parameters""\n        )\n\n    def get_state_values_logits_parameters_log_probs(self, nn_input, internal_states=None):\n        raise RLGraphObsoletedError(\n            ""API-method"", ""get_state_values_logits_parameters_log_probs"",\n            ""get_state_values_adapter_outputs_and_parameters""\n        )\n'"
rlgraph/components/queues/__init__.py,0,"b'# Copyright 2018/2019 The Rlgraph Authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# TODO: Move all Queues and QueueRunners here'"
rlgraph/execution/distributed_tf/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
rlgraph/execution/ray/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.execution.ray.ray_executor import RayExecutor\nfrom rlgraph.execution.ray.ray_value_worker import RayValueWorker\n\nfrom rlgraph.execution.ray.apex import ApexExecutor, ApexMemory, RayMemoryActor\nfrom rlgraph.execution.ray.sync_batch_executor import SyncBatchExecutor\n\nRayExecutor.__lookup_classes__ = dict(\n    apex=ApexExecutor,\n    apexecutor=ApexExecutor,\n    syncbatch=SyncBatchExecutor,\n    syncbatchexecutor=SyncBatchExecutor\n)\n\n__all__ = [""RayExecutor"", ""RayValueWorker"", ""ApexExecutor"", ""ApexMemory"", ""RayMemoryActor""]\n'"
rlgraph/execution/ray/ray_actor.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom copy import deepcopy\n\nfrom rlgraph.components import PreprocessorStack\n\n\nclass RayActor(object):\n    """"""\n    Generic Ray actor. All classes implementing ray actors should inherit from this.\n    """"""\n    def get_host(self):\n        """"""\n        Returns host node identifier.\n\n        Returns:\n            str: Node name this agent is running on.\n        """"""\n        return os.uname()[1]\n\n    def setup_preprocessor(self, preprocessing_spec, in_space):\n        if preprocessing_spec is not None:\n            preprocessing_spec = deepcopy(preprocessing_spec)\n            in_space = deepcopy(in_space)\n            # Set scopes.\n            scopes = [preprocessor[""scope""] for preprocessor in preprocessing_spec]\n            # Set backend to python.\n            for spec in preprocessing_spec:\n                spec[""backend""] = ""python""\n            processor_stack = PreprocessorStack(*preprocessing_spec, backend=""python"")\n            build_space = in_space\n            for sub_comp_scope in scopes:\n                processor_stack.sub_components[sub_comp_scope].create_variables(input_spaces=dict(\n                    inputs=build_space\n                ), action_space=None)\n                build_space = processor_stack.sub_components[sub_comp_scope].get_preprocessed_space(build_space)\n            processor_stack.reset()\n            return processor_stack\n        else:\n            return None\n'"
rlgraph/execution/ray/ray_executor.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom copy import deepcopy\nfrom six.moves import xrange as range_\nimport logging\nimport numpy as np\nimport time\n\nfrom rlgraph import get_distributed_backend\nfrom rlgraph.agents import Agent\nfrom rlgraph.environments import Environment\nfrom rlgraph.execution.ray.ray_util import worker_exploration\n\nif get_distributed_backend() == ""ray"":\n    import ray\n\n\nclass RayExecutor(object):\n    """"""\n    Abstract distributed Ray executor.\n\n    A Ray executor implements a specific distributed learning semantic by delegating\n    distributed state management and execution to the Ray execution engine.\n    """"""\n    def __init__(self, executor_spec, environment_spec, worker_spec):\n        """"""\n        Args:\n            executor_spec (dict): Contains all information necessary to set up and execute\n                agents on a Ray cluster.\n            environment_spec (dict): Environment spec. Each worker in the cluster will instantiate\n                an environment using this spec.\n            worker_spec (dict): Worker spec to read out for reporting.\n        """"""\n        self.logger = logging.getLogger(__name__)\n\n        # Ray workers for remote data collection.\n        self.ray_env_sample_workers = None\n        self.executor_spec = executor_spec\n        self.num_cpus_per_worker = executor_spec.get(""num_cpus_per_worker"", 1)\n        self.num_gpus_per_worker = executor_spec.get(""num_gpus_per_worker"", 0)\n        self.environment_spec = environment_spec\n\n        # Global performance metrics.\n        self.sample_iteration_throughputs = None\n        self.update_iteration_throughputs = None\n        self.iteration_times = None\n        self.worker_frame_skip = worker_spec.get(""worker_frame_skip"", 1)\n        self.env_internal_frame_skip = worker_spec.get(""env_internal_frame_skip"", 1)\n\n        # Map worker objects to host ids.\n        self.worker_ids = {}\n\n    def ray_init(self):\n        """"""\n        Connects to a Ray cluster or starts one if none exists.\n        """"""\n        self.logger.info(""Initializing Ray cluster with executor spec:"")\n        for spec_key, value in self.executor_spec.items():\n            self.logger.info(""{}: {}"".format(spec_key, value))\n\n        # Avoiding accidentally starting local redis clusters.\n        if \'redis_address\' not in self.executor_spec:\n            self.logger.warning(""Warning: No redis address provided, starting local redis server."")\n        ray.init(\n            redis_address=self.executor_spec.get(\'redis_address\', None),\n            num_cpus=self.executor_spec.get(\'num_cpus\', None),\n            num_gpus=self.executor_spec.get(\'num_gpus\', None)\n        )\n\n    def create_remote_workers(self, cls, num_actors, agent_config, worker_spec, *args):\n        """"""\n        Creates Ray actors for remote execution.\n\n        Args:\n            cls (Union[RayValueWorker, RayPolicyWorker]): RayActor class.\n            num_actors (int): Num RayActor to create.\n            agent_config (dict): Agent config.\n            worker_spec (dict): Worker spec.\n            *args (any): Arguments for RayActor class.\n\n        Returns:\n            list: Remote Ray actors.\n        """"""\n        workers = []\n        cls_as_remote = cls.as_remote(num_cpus=self.num_cpus_per_worker, num_gpus=self.num_gpus_per_worker).remote\n\n        # Create remote objects and schedule init tasks.\n        ray_constant_exploration = worker_spec.get(""ray_constant_exploration"", False)\n        for i in range_(num_actors):\n            if ray_constant_exploration is True:\n                exploration_val = worker_exploration(i, num_actors)\n                worker_spec[""ray_exploration""] = exploration_val\n            worker = cls_as_remote(deepcopy(agent_config), worker_spec, *args)\n            self.worker_ids[worker] = ""worker_{}"".format(i)\n            workers.append(worker)\n            self.logger.info(""Successfully built agent num {}."".format(i))\n\n        return workers\n\n    def test_worker_init(self):\n        """"""\n        Tests every worker for successful constructor call (which may otherwise fail silently.\n        """"""\n        for ray_worker in self.ray_env_sample_workers:\n            self.logger.info(""Testing worker for successful init: {}"".format(self.worker_ids[ray_worker]))\n            task = ray_worker.get_constructor_success.remote()\n            result = ray.get(task)\n            assert result is True, ""ERROR: constructor failed, attribute returned: {}"" \\\n                                   ""instead of True"".format(result)\n\n    def setup_execution(self):\n        """"""\n        Creates and initializes all remote agents on the Ray cluster. Does not\n        schedule any tasks yet.\n        """"""\n        raise NotImplementedError\n\n    def init_tasks(self):\n        """"""\n        Initializes Remote ray worker tasks. Calling this method will result in\n        actually scheduling tasks on Ray, as opposed to setup_execution which just\n        creates the relevant remote actors.\n        """"""\n        pass\n\n    def execute_workload(self, workload):\n        """"""\n        Executes a workload on Ray and measures worker statistics. Workload semantics\n        are decided via the private implementer, _execute_step().\n\n        Args:\n            workload (dict): Workload parameters, primarily \'num_timesteps\' and \'report_interval\'\n                to indicate how many steps to execute and how often to report results.\n        """"""\n        self.sample_iteration_throughputs = []\n        self.update_iteration_throughputs = []\n        self.iteration_times = []\n\n        # Assume time step based initially.\n        num_timesteps = workload[""num_timesteps""]\n\n        # Performance reporting granularity.\n        report_interval = workload[""report_interval""]\n        report_interval_min_seconds = workload[""report_interval_min_seconds""]\n        timesteps_executed = 0\n        iteration_time_steps = []\n        iteration_update_steps = []\n\n        start = time.monotonic()\n        # Call _execute_step as many times as required.\n        while timesteps_executed < num_timesteps:\n            iteration_step = 0\n            iteration_updates = 0\n            iteration_discarded = 0\n            iteration_queue_inserted = 0\n            iteration_start = time.monotonic()\n\n            # Last episode rewards seen during iteration.\n            iteration_rewards = []\n\n            # Record sampling and learning throughput every interval.\n            while (iteration_step < report_interval) or\\\n                    time.monotonic() - iteration_start < report_interval_min_seconds:\n                worker_steps_executed, update_steps, stats = self._execute_step()\n                iteration_step += worker_steps_executed\n                iteration_updates += update_steps\n                iteration_discarded += stats[""discarded""]\n                iteration_queue_inserted += stats[""queue_inserts""]\n                iteration_rewards.extend(stats[""rewards""])\n\n            iteration_end = time.monotonic() - iteration_start\n            timesteps_executed += iteration_step\n\n            # Append raw values, compute stats after experiment is done.\n            self.iteration_times.append(iteration_end)\n            iteration_update_steps.append(iteration_updates)\n            iteration_time_steps.append(iteration_step)\n\n            self.logger.info(""Executed {} Ray worker steps, {} update steps, ({} of {} ({} %), discarded = {},""\n                             "" inserts = {})"".format(iteration_step, iteration_updates, timesteps_executed,\n                             num_timesteps, (100 * timesteps_executed / num_timesteps), iteration_discarded,\n                             iteration_queue_inserted))\n            if len(iteration_rewards) > 0:\n                self.logger.info(""Min iteration reward: {}, mean iteration reward: {}, max iteration reward: {}.""\n                                 ""Stats from {} episodes."".format(np.min(iteration_rewards), np.mean(iteration_rewards),\n                                                                  np.max(iteration_rewards), len(iteration_rewards)))\n\n        total_time = (time.monotonic() - start) or 1e-10\n        self.logger.info(""Time steps executed: {} ({} ops/s)"".\n                         format(timesteps_executed, timesteps_executed / total_time))\n        all_updates = np.sum(iteration_update_steps)\n        self.logger.info(""Updates executed: {}, ({} updates/s)"".format(\n            all_updates, all_updates / total_time\n        ))\n        for i in range_(len(self.iteration_times)):\n            it_time = self.iteration_times[i]\n            # Note: these are samples, not internal environment frames.\n            self.sample_iteration_throughputs.append(iteration_time_steps[i] / it_time)\n            self.update_iteration_throughputs.append(iteration_update_steps[i] / it_time)\n\n        worker_stats = self.get_aggregate_worker_results()\n        self.logger.info(""Retrieved worker stats for {} workers:"".format(len(self.ray_env_sample_workers)))\n        self.logger.info(worker_stats)\n\n        return dict(\n            runtime=total_time,\n            timesteps_executed=timesteps_executed,\n            ops_per_second=(timesteps_executed / total_time),\n            # Multiply sample throughput by these env_frames = samples * env_internal * worker_frame_skip:\n            env_internal_frame_skip=self.env_internal_frame_skip,\n            worker_frame_skip=self.worker_frame_skip,\n            min_iteration_sample_throughput=np.min(self.sample_iteration_throughputs),\n            max_iteration_sample_throughput=np.max(self.sample_iteration_throughputs),\n            mean_iteration_sample_throughput=np.mean(self.sample_iteration_throughputs),\n            min_iteration_update_throughput=np.min(self.update_iteration_throughputs),\n            max_iteration_update_throughput=np.max(self.update_iteration_throughputs),\n            mean_iteration_update_throughput=np.mean(self.update_iteration_throughputs),\n            # Worker stats.\n            mean_worker_op_throughput=worker_stats[""mean_worker_op_throughput""],\n            # N.b. these are already corrected.\n            mean_worker_env_frames_throughput=worker_stats[""mean_worker_env_frame_throughput""],\n            max_worker_op_throughput=worker_stats[""max_worker_op_throughput""],\n            min_worker_op_throughput=worker_stats[""min_worker_op_throughput""],\n            mean_worker_reward=worker_stats[""mean_reward""],\n            max_worker_reward=worker_stats[""max_reward""],\n            min_worker_reward=worker_stats[""min_reward""],\n            # This is the mean final episode over all workers.\n            mean_final_reward=worker_stats[""mean_final_reward""]\n        )\n\n    def sample_metrics(self):\n        return self.sample_iteration_throughputs\n\n    def update_metrics(self):\n        return self.update_iteration_throughputs\n\n    def get_iteration_times(self):\n        return self.iteration_times\n\n    def _execute_step(self):\n        """"""\n        Actual private implementer of each step of the workload executed.\n        """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def build_agent_from_config(agent_config):\n        """"""\n        Builds agent without using from_spec as Ray cannot handle kwargs correctly\n        at the moment.\n\n        Args:\n            agent_config (dict): Agent config. Must contain \'type\' field to lookup constructor.\n\n        Returns:\n            Agent: RLGraph agent object.\n        """"""\n        config = deepcopy(agent_config)\n        # Pop type on a copy because this may be called by multiple classes/worker types.\n        agent_cls = Agent.__lookup_classes__.get(config.pop(\'type\'))\n        return agent_cls(**config)\n\n    def result_by_worker(self, worker_index=None):\n        """"""\n        Retrieves full episode-reward time series for a worker by id (or first worker in registry if None).\n\n        Args:\n            worker_index (Optional[int]): Index of worker to fetch.\n\n        Returns:\n            dict: Full results for this worker.\n        """"""\n        if worker_index is not None:\n            ray_worker = self.ray_env_sample_workers[worker_index]\n        else:\n            # Otherwise just pick  first.\n            ray_worker = self.ray_env_sample_workers[0]\n\n        task = ray_worker.get_workload_statistics.remote()\n        metrics = ray.get(task)\n\n        # Return full reward series.\n        return dict(\n            episode_rewards=metrics[""episode_rewards""],\n            episode_timesteps=metrics[""episode_timesteps""]\n        )\n\n    def get_all_worker_results(self):\n        """"""\n        Retrieves full episode-reward time series for all workers.\n\n        Returns:\n            list: List dicts with worker results (timesteps and rewards)\n        """"""\n        results = list()\n        for ray_worker in self.ray_env_sample_workers:\n            task = ray_worker.get_workload_statistics.remote()\n            metrics = ray.get(task)\n            results.append(dict(\n                episode_rewards=metrics[""episode_rewards""],\n                episode_timesteps=metrics[""episode_timesteps""],\n                episode_total_times=metrics[""episode_total_times""],\n                episode_sample_times=metrics[""episode_sample_times""]\n            ))\n        return results\n\n    def get_sample_worker_ids(self):\n        """"""\n        Returns identifiers of all sample workers.\n\n        Returns:\n            list: List of worker name strings in case individual analysis of one worker\'s results are required via\n                \'result_by_worker\'.\n        """"""\n        return list(self.worker_ids.keys())\n\n    def get_aggregate_worker_results(self):\n        """"""\n        Fetches execution statistics from remote workers and aggregates them.\n\n        Returns:\n            dict: Aggregate worker statistics.\n        """"""\n        min_rewards = []\n        max_rewards = []\n        mean_rewards = []\n        final_rewards = []\n        worker_op_throughputs = []\n        worker_env_frame_throughputs = []\n        episodes_executed = []\n        steps_executed = 0\n\n        for ray_worker in self.ray_env_sample_workers:\n            self.logger.info(""Retrieving workload statistics for worker: {}"".format(\n                self.worker_ids[ray_worker])\n            )\n            task = ray_worker.get_workload_statistics.remote()\n            metrics = ray.get(task)\n            if metrics[""mean_episode_reward""] is not None:\n                min_rewards.append(metrics[""min_episode_reward""])\n                max_rewards.append(metrics[""max_episode_reward""])\n                mean_rewards.append(metrics[""mean_episode_reward""])\n                final_rewards.append(metrics[""final_episode_reward""])\n            else:\n                self.logger.warning(""Warning: No episode rewards available for worker {}. Steps executed: {}"".\n                                    format(self.worker_ids[ray_worker], metrics[""worker_steps""]))\n            episodes_executed.append(metrics[""episodes_executed""])\n            steps_executed += metrics[""worker_steps""]\n            worker_op_throughputs.append(metrics[""mean_worker_ops_per_second""])\n            worker_env_frame_throughputs.append(metrics[""mean_worker_env_frames_per_second""])\n\n        return dict(\n            min_reward=np.min(min_rewards),\n            max_reward=np.max(max_rewards),\n            mean_reward=np.mean(mean_rewards),\n            mean_final_reward=np.mean(final_rewards),\n            min_worker_episodes=np.min(episodes_executed),\n            max_worker_episodes=np.max(episodes_executed),\n            mean_worker_episodes=np.mean(episodes_executed),\n            total_episodes_executed=np.sum(episodes_executed),\n            steps_executed=steps_executed,\n            # Identify potential straggling workers.\n            mean_worker_op_throughput=np.mean(worker_op_throughputs),\n            min_worker_op_throughput=np.min(worker_op_throughputs),\n            max_worker_op_throughput=np.max(worker_op_throughputs),\n            mean_worker_env_frame_throughput=np.mean(worker_env_frame_throughputs)\n        )\n'"
rlgraph/execution/ray/ray_policy_worker.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom copy import deepcopy\nimport time\n\nimport numpy as np\nfrom six.moves import xrange as range_\n\nfrom rlgraph.utils import util\nfrom rlgraph import get_distributed_backend\nfrom rlgraph.environments.sequential_vector_env import SequentialVectorEnv\nfrom rlgraph.execution.environment_sample import EnvironmentSample\nfrom rlgraph.execution.ray import RayExecutor\nfrom rlgraph.execution.ray.ray_actor import RayActor\nfrom rlgraph.execution.ray.ray_util import ray_compress\n\nif get_distributed_backend() == ""ray"":\n    import ray\n\n\nclass RayPolicyWorker(RayActor):\n    """"""\n    A Ray policy worker for distributed policy optimisation. Unlike the value worker,\n    this worker does not need to explicitly store next-states, thus reducing communication overhead and\n    memory usage in the object store.\n    """"""\n\n    def __init__(self, agent_config, worker_spec, env_spec, frameskip=1):\n        """"""\n        Creates agent and environment for Ray worker.\n\n        Args:\n            agent_config (dict): Agent configuration dict.\n            worker_spec (dict): Worker parameters.\n            env_spec (dict): Environment config for environment to run.\n            frameskip (int): How often actions are repeated after retrieving them from the agent.\n        """"""\n        assert get_distributed_backend() == ""ray""\n        # Internal frameskip of env.\n        self.env_frame_skip = worker_spec.get(""env_internal_frame_skip"", 1)\n        # Worker computes weights for prioritized sampling.\n        worker_spec = deepcopy(worker_spec)\n        self.num_environments = worker_spec.pop(""num_worker_environments"", 1)\n        self.worker_sample_size = worker_spec.pop(""worker_sample_size"") * self.num_environments\n        self.worker_executes_postprocessing = worker_spec.pop(""worker_executes_postprocessing"", True)\n\n        self.compress = worker_spec.pop(""compress_states"", False)\n        self.env_ids = [""env_{}"".format(i) for i in range_(self.num_environments)]\n        num_background_envs = worker_spec.pop(""num_background_envs"", 1)\n\n        self.vector_env = SequentialVectorEnv(self.num_environments, env_spec, num_background_envs)\n\n        # Then update agent config.\n        agent_config[\'state_space\'] = self.vector_env.state_space\n        agent_config[\'action_space\'] = self.vector_env.action_space\n\n        # Python based preprocessor as image resizing is broken in TF.\n        self.preprocessors = {}\n        preprocessing_spec = agent_config.get(""preprocessing_spec"", None)\n        self.is_preprocessed = {}\n        for env_id in self.env_ids:\n            self.preprocessors[env_id] = self.setup_preprocessor(\n                preprocessing_spec, self.vector_env.state_space.with_batch_rank()\n            )\n            self.is_preprocessed[env_id] = False\n        self.agent = self.setup_agent(agent_config, worker_spec)\n        self.worker_frameskip = frameskip\n\n        #  Flag for container actions.\n        self.container_actions = self.agent.flat_action_space is not None\n        self.action_space = self.agent.flat_action_space\n\n        # Save these so they can be fetched after training if desired.\n        self.finished_episode_rewards = [[] for _ in range_(self.num_environments)]\n        self.finished_episode_timesteps = [[] for _ in range_(self.num_environments)]\n        # Total times sample the ""real"" wallclock time from start to end for each episode.\n        self.finished_episode_total_times = [[] for _ in range_(self.num_environments)]\n        # Sample times stop the wallclock time counter between runs, so only the sampling time is accounted for.\n        self.finished_episode_sample_times = [[] for _ in range_(self.num_environments)]\n\n        self.total_worker_steps = 0\n        self.episodes_executed = 0\n\n        # Step time and steps done per call to execute_and_get to measure throughput of this worker.\n        self.sample_times = []\n        self.sample_steps = []\n        self.sample_env_frames = []\n\n        # To continue running through multiple exec calls.\n        self.last_states = self.vector_env.reset_all()\n\n        self.zero_batched_state = np.zeros((1,) + self.agent.preprocessed_state_space.shape)\n        self.zero_unbatched_state = np.zeros(self.agent.preprocessed_state_space.shape)\n        self.preprocessed_states_buffer = np.zeros(\n            shape=(self.num_environments,) + self.agent.preprocessed_state_space.shape,\n            dtype=self.agent.preprocessed_state_space.dtype\n        )\n        self.last_ep_timesteps = [0 for _ in range_(self.num_environments)]\n        self.last_ep_rewards = [0 for _ in range_(self.num_environments)]\n        self.last_ep_start_timestamps = [0.0 for _ in range_(self.num_environments)]\n        self.last_ep_start_initialized = False  # initialize on first `execute_and_get_timesteps()` call\n        self.last_ep_sample_times = [0.0 for _ in range_(self.num_environments)]\n\n        # Was the last state a terminal state so env should be reset in next call?\n        self.last_terminals = [False for _ in range_(self.num_environments)]\n\n    def get_constructor_success(self):\n        """"""\n        For debugging: fetch the last attribute. Will fail if constructor failed.\n        """"""\n        return not self.last_terminals[0]\n\n    @classmethod\n    def as_remote(cls, num_cpus=None, num_gpus=None):\n        return ray.remote(num_cpus=num_cpus, num_gpus=num_gpus)(cls)\n\n    def setup_agent(self, agent_config, worker_spec):\n        """"""\n        Sets up agent, potentially modifying its configuration via worker specific settings.\n        """"""\n        # Worker execution spec may differ from controller/learner.\n        worker_exec_spec = worker_spec.get(""execution_spec"", None)\n        if worker_exec_spec is not None:\n            agent_config.update(execution_spec=worker_exec_spec)\n\n        # Build lazily per default.\n        return RayExecutor.build_agent_from_config(agent_config)\n\n    def execute_and_get_timesteps(\n        self,\n        num_timesteps,\n        max_timesteps_per_episode=0,\n        use_exploration=True,\n        break_on_terminal=False\n    ):\n        """"""\n        Collects and returns time step experience.\n\n        Args:\n            break_on_terminal (Optional[bool]): If true, breaks when a terminal is encountered. If false,\n                executes exactly \'num_timesteps\' steps.\n        """"""\n        # Initialize start timestamps. Initializing the timestamps here should make the observed execution timestamps\n        # more accurate, as there might be delays between the worker initialization and actual sampling start.\n        if not self.last_ep_start_initialized:\n            for i, timestamp in enumerate(self.last_ep_start_timestamps):\n                self.last_ep_start_timestamps[i] = time.perf_counter()\n            self.last_ep_start_initialized = True\n\n        start = time.perf_counter()\n        timesteps_executed = 0\n        episodes_executed = [0] * self.num_environments\n        env_frames = 0\n\n        # Final result batch.\n        if self.container_actions:\n            batch_actions = {k: [] for k in self.action_space.keys()}\n        else:\n            batch_actions = []\n        batch_states, batch_rewards, batch_next_states, batch_terminals = [], [], [], []\n\n        # Running trajectories.\n        sample_states, sample_actions, sample_rewards, sample_terminals = {}, {}, {}, {}\n\n        # Reset envs and Agent either if finished an episode in current loop or if last state\n        # from previous execution was terminal for that environment.\n        for i, env_id in enumerate(self.env_ids):\n            sample_states[env_id] = []\n            if self.container_actions:\n                sample_actions[env_id] = {k: [] for k in self.action_space.keys()}\n            else:\n                sample_actions[env_id] = []\n            sample_rewards[env_id] = []\n            sample_terminals[env_id] = []\n\n        env_states = self.last_states\n        last_episode_rewards = []\n        current_episode_rewards = self.last_ep_rewards\n        current_episode_timesteps = self.last_ep_timesteps\n        current_episode_start_timestamps = self.last_ep_start_timestamps\n        current_episode_sample_times = self.last_ep_sample_times\n\n        # Whether the episode in each env has terminated.\n        terminals = [False] * self.num_environments\n\n        while timesteps_executed < num_timesteps:\n            current_iteration_start_timestamp = time.perf_counter()\n            for i, env_id in enumerate(self.env_ids):\n                state, _ = self.agent.state_space.force_batch(env_states[i])\n                if self.preprocessors[env_id] is not None:\n                    if self.is_preprocessed[env_id] is False:\n                        self.preprocessed_states_buffer[i] = self.preprocessors[env_id].preprocess(state)\n                        self.is_preprocessed[env_id] = True\n                else:\n                    self.preprocessed_states_buffer[i] = env_states[i]\n\n            actions = self.agent.get_action(states=self.preprocessed_states_buffer,\n                                            use_exploration=use_exploration, apply_preprocessing=False)\n\n            if self.agent.flat_action_space is not None:\n                some_key = next(iter(actions))\n                assert isinstance(actions, dict) and isinstance(actions[some_key], np.ndarray),\\\n                    ""ERROR: Cannot flip container-action batch with dict keys if returned value is not a dict OR "" \\\n                    ""values of returned value are not np.ndarrays!""\n                if hasattr(actions[some_key], ""__len__""):\n                    env_actions = [{key: value[i] for key, value in actions.items()} for i in range(len(actions[some_key]))]\n                else:\n                    # Action was not array type.\n                    env_actions = actions\n            # No flipping necessary.\n            else:\n                env_actions = actions\n                if self.num_environments == 1 and env_actions.shape == ():\n                    env_actions = [env_actions]\n            next_states, step_rewards, terminals, infos = self.vector_env.step(actions=env_actions)\n            # Worker frameskip not needed as done in env.\n            # for _ in range_(self.worker_frameskip):\n            #     next_states, step_rewards, terminals, infos = self.vector_env.step(actions=actions)\n            #     env_frames += self.num_environments\n            #\n            #     for i, env_id in enumerate(self.env_ids):\n            #         rewards[env_id] += step_rewards[i]\n            #     if np.any(terminals):\n            #         break\n\n            timesteps_executed += self.num_environments\n            env_frames += self.num_environments\n            env_states = next_states\n            current_iteration_time = time.perf_counter() - current_iteration_start_timestamp\n\n            # Do accounting for each environment.\n            state_buffer = np.array(self.preprocessed_states_buffer)\n            for i, env_id in enumerate(self.env_ids):\n                # Set is preprocessed to False because env_states are currently NOT preprocessed.\n                self.is_preprocessed[env_id] = False\n                current_episode_timesteps[i] += 1\n                # Each position is the running episode reward of that episode. Add step reward.\n                current_episode_rewards[i] += step_rewards[i]\n                sample_states[env_id].append(state_buffer[i])\n                if self.container_actions:\n                    for name in self.action_space.keys():\n                        sample_actions[env_id][name].append(env_actions[i][name])\n                else:\n                    sample_actions[env_id].append(env_actions[i])\n                sample_rewards[env_id].append(step_rewards[i])\n                sample_terminals[env_id].append(terminals[i])\n                current_episode_sample_times[i] += current_iteration_time\n\n                # Terminate and reset episode for that environment.\n                if terminals[i] or (0 < max_timesteps_per_episode <= current_episode_timesteps[i]):\n                    self.finished_episode_rewards[i].append(current_episode_rewards[i])\n\n                    self.finished_episode_timesteps[i].append(current_episode_timesteps[i])\n                    self.finished_episode_total_times[i].append(time.perf_counter() - current_episode_start_timestamps[i])\n                    self.finished_episode_sample_times[i].append(current_episode_sample_times[i])\n                    episodes_executed[i] += 1\n                    self.episodes_executed += 1\n                    last_episode_rewards.append(current_episode_rewards[i])\n\n                    # Append to final result trajectories.\n                    batch_states.extend(sample_states[env_id])\n                    if self.agent.flat_action_space is not None:\n                        # Use actions here, not env actions.\n                        for name in self.agent.flat_action_space.keys():\n                            batch_actions[name].extend(sample_actions[env_id][name])\n                    else:\n                        batch_actions.extend(sample_actions[env_id])\n                    batch_rewards.extend(sample_rewards[env_id])\n                    batch_terminals.extend(sample_terminals[env_id])\n\n                    # Reset running trajectory for this env.\n                    sample_states[env_id] = []\n                    if self.container_actions:\n                        sample_actions[env_id] = {k: [] for k in self.action_space.keys()}\n                    else:\n                        sample_actions[env_id] = []\n                    sample_rewards[env_id] = []\n                    sample_terminals[env_id] = []\n\n                    # Reset this environment and its pre-processor stack.\n                    env_states[i] = self.vector_env.reset(i)\n                    if self.preprocessors[env_id] is not None:\n                        self.preprocessors[env_id].reset()\n                        # This re-fills the sequence with the reset state.\n                        state, _ = self.agent.state_space.force_batch(env_states[i])\n                        # Pre - process, add to buffer\n                        self.preprocessed_states_buffer[i] = np.array(self.preprocessors[env_id].preprocess(state))\n                        self.is_preprocessed[env_id] = True\n                    current_episode_rewards[i] = 0\n                    current_episode_timesteps[i] = 0\n                    current_episode_start_timestamps[i] = time.perf_counter()\n                    current_episode_sample_times[i] = 0.0\n\n            if 0 < num_timesteps <= timesteps_executed or (break_on_terminal and np.any(terminals)):\n                self.total_worker_steps += timesteps_executed\n                break\n\n        self.last_terminals = terminals\n        self.last_states = env_states\n        self.last_ep_rewards = current_episode_rewards\n        self.last_ep_timesteps = current_episode_timesteps\n        self.last_ep_start_timestamps = current_episode_start_timestamps\n        self.last_ep_sample_times = current_episode_sample_times\n\n        # Sequence indices are the same as terminals for terminal episodes.\n        batch_sequence_indices = batch_terminals.copy()\n\n        # We already accounted for all terminated episodes. This means we only\n        # have to do accounting for any unfinished fragments.\n        for i, env_id in enumerate(self.env_ids):\n            # This env was not terminal -> need to process remaining trajectory\n            if not terminals[i]:\n                batch_states.extend(sample_states[env_id])\n                if self.agent.flat_action_space is not None:\n                    # Use actions here, not env actions.\n                    for name in self.agent.flat_action_space.keys():\n                        batch_actions[name].extend(sample_actions[env_id][name])\n                else:\n                    batch_actions.extend(sample_actions[env_id])\n                batch_rewards.extend(sample_rewards[env_id])\n                batch_terminals.extend(sample_terminals[env_id])\n\n                # Take terminals thus far - all zero.\n                batch_sequence_indices.extend(sample_terminals[env_id].copy())\n                # Set final elem to true because sub-sequence ends here.\n                batch_sequence_indices[-1] = True\n\n        # Perform final batch-processing once.\n        sample_batch, batch_size = self._process_policy_trajectories(batch_states, batch_actions,\n                                                                     batch_rewards, batch_terminals,\n                                                                     batch_sequence_indices)\n\n        total_time = (time.perf_counter() - start) or 1e-10\n        self.sample_steps.append(timesteps_executed)\n        self.sample_times.append(total_time)\n        self.sample_env_frames.append(env_frames)\n\n        # Note that the controller already evaluates throughput so there is no need\n        # for each worker to calculate expensive statistics now.\n        return EnvironmentSample(\n            sample_batch=sample_batch,\n            batch_size=len(batch_rewards),\n            metrics=dict(\n                last_rewards=last_episode_rewards,\n                runtime=total_time,\n                # Agent act/observe throughput.\n                timesteps_executed=timesteps_executed,\n                ops_per_second=(timesteps_executed / total_time),\n            )\n        )\n\n    @ray.method(num_return_vals=2)\n    def execute_and_get_with_count(self):\n        sample = self.execute_and_get_timesteps(num_timesteps=self.worker_sample_size)\n        return sample, sample.batch_size\n\n    def set_weights(self, weights):\n        policy_weights = {k: v for k,v in zip(weights.policy_vars, weights.policy_values)}\n        vf_weights = None\n        if weights.has_vf:\n            vf_weights = {k: v for k, v in zip(weights.value_function_vars, weights.value_function_values)}\n        self.agent.set_weights(policy_weights, value_function_weights=vf_weights)\n\n    def get_workload_statistics(self):\n        """"""\n        Returns performance results for this worker.\n\n        Returns:\n            dict: Performance metrics.\n        """"""\n        # Adjust env frames for internal env frameskip:\n        adjusted_frames = [env_frames * self.env_frame_skip for env_frames in self.sample_env_frames]\n        if len(self.finished_episode_rewards) > 0:\n            all_finished_rewards = []\n            for env_reward_list in self.finished_episode_rewards:\n                all_finished_rewards.extend(env_reward_list)\n            min_episode_reward = np.min(all_finished_rewards)\n            max_episode_reward = np.max(all_finished_rewards)\n            mean_episode_reward = np.mean(all_finished_rewards)\n            # Mean of final episode rewards over all envs\n            final_episode_reward = np.mean([env_rewards[-1] for env_rewards in self.finished_episode_rewards])\n        else:\n            # Will be aggregated in executor.\n            min_episode_reward = None\n            max_episode_reward = None\n            mean_episode_reward = None\n            final_episode_reward = None\n\n        return dict(\n            episode_timesteps=self.finished_episode_timesteps,\n            episode_rewards=self.finished_episode_rewards,\n            episode_total_times=self.finished_episode_total_times,\n            episode_sample_times=self.finished_episode_sample_times,\n            min_episode_reward=min_episode_reward,\n            max_episode_reward=max_episode_reward,\n            mean_episode_reward=mean_episode_reward,\n            final_episode_reward=final_episode_reward,\n            episodes_executed=self.episodes_executed,\n            worker_steps=self.total_worker_steps,\n            mean_worker_ops_per_second=sum(self.sample_steps) / sum(self.sample_times),\n            mean_worker_env_frames_per_second=sum(adjusted_frames) / sum(self.sample_times)\n        )\n\n    def _process_policy_trajectories(self, states, actions, rewards, terminals, sequence_indices):\n        """"""\n        Post-processes policy trajectories.\n        """"""\n        if self.worker_executes_postprocessing:\n            rewards = self.agent.post_process(\n                dict(\n                    states=states,\n                    rewards=rewards,\n                    terminals=terminals,\n                    sequence_indices=sequence_indices\n                )\n            )\n\n        if self.compress:\n            env_dtype = self.vector_env.state_space.dtype\n            states = [ray_compress(np.asarray(state, dtype=util.convert_dtype(dtype=env_dtype, to=\'np\')))\n                      for state in states]\n        return dict(\n            states=states,\n            actions=actions,\n            rewards=rewards,\n            terminals=terminals\n        ), len(rewards)\n\n'"
rlgraph/execution/ray/ray_util.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport base64\nimport numpy as np\nfrom six import string_types\nfrom rlgraph import get_distributed_backend\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\n\nif get_distributed_backend() == ""ray"":\n    import ray\n    import lz4.frame\n    import pyarrow\n\n\n# Follows utils used in Ray RLlib.\nclass RayWeight(object):\n    """"""\n    Wrapper to transport TF weights to deal with serialisation bugs in Ray/Arrow.\n\n    #TODO investigate serialisation bugs in Ray/flatten values.\n    """"""\n\n    def __init__(self, weights):\n        self.policy_vars = []\n        self.policy_values = []\n\n        for k, v in weights[""policy_weights""].items():\n            self.policy_vars.append(k)\n            self.policy_values.append(v)\n\n        self.has_vf = False\n        if ""value_function_weights"" in weights:\n            self.value_function_vars = []\n            self.value_function_values = []\n            self.has_vf = True\n            for k, v in weights[""value_function_weights""].items():\n                self.value_function_vars.append(k)\n                self.value_function_values.append(v)\n\n\nclass RayTaskPool(object):\n    """"""\n    Manages a set of Ray tasks currently being executed (i.e. the RayAgent tasks).\n    """"""\n\n    def __init__(self):\n        self.ray_tasks = {}\n        self.ray_objects = {}\n\n    def add_task(self, worker, ray_object_ids):\n        """"""\n        Adds a task to the task pool.\n        Args:\n            worker (any): Worker completing the task, must use the @ray.remote decorator.\n            ray_object_ids (Union[str, list]): Ray object id. See ray documentation for how these are used.\n        """"""\n        # Map which worker is responsible for completing the Ray task.\n        if isinstance(ray_object_ids, list):\n            ray_object_id = ray_object_ids[0]\n        else:\n            ray_object_id = ray_object_ids\n        self.ray_tasks[ray_object_id] = worker\n        self.ray_objects[ray_object_id] = ray_object_ids\n\n    def get_completed(self):\n        """"""\n        Waits on pending tasks and yields them upon completion.\n\n        Returns:\n            generator: Yields completed tasks.\n        """"""\n\n        pending_tasks = list(self.ray_tasks)\n        if pending_tasks:\n            # This ray function checks tasks and splits into ready and non-ready tasks.\n            ready, not_ready = ray.wait(pending_tasks, num_returns=len(pending_tasks), timeout=0.01)\n            for obj_id in ready:\n                yield (self.ray_tasks.pop(obj_id), self.ray_objects.pop(obj_id))\n\n\ndef create_colocated_ray_actors(cls, config, num_agents, max_attempts=10):\n    """"""\n    Creates a specified number of co-located RayActors.\n\n    Args:\n        cls (class): Actor class to create\n        config (dict): Config for actor.\n        num_agents (int): Number of worker agents to create.\n        max_attempts (Optional[int]): Max number of attempts to create colocated agents, will raise\n            an error if creation was not successful within this number.\n\n    Returns:\n        list: List of created agents.\n\n    Raises:\n        RLGraph-Error if not enough agents could be created within the specified number of attempts.\n    """"""\n    agents = []\n    attempt = 1\n\n    while len(agents) < num_agents and attempt <= max_attempts:\n        ray_agents = [cls.remote(config) for _ in range(attempt * num_agents)]\n        local_agents, _ = split_local_non_local_agents(ray_agents)\n        agents.extend(local_agents)\n\n    if len(agents) < num_agents:\n        raise RLGraphError(""Could not create the specified number ({}) of agents."".format(\n            num_agents\n        ))\n\n    return agents[:num_agents]\n\n\ndef split_local_non_local_agents(ray_agents):\n    """"""\n    Splits agents in local and non-local agents based on localhost string and ray remote\n    hsots.\n\n    Args:\n        ray_agents (list): List of RayAgent objects.\n\n    Returns:\n        (list, list): Local and non-local agents.\n    """"""\n    localhost = os.uname()[1]\n    hosts = ray.get([agent.get_host.remote() for agent in ray_agents])\n    local = []\n    non_local = []\n\n    for host, a in zip(hosts, ray_agents):\n        if host == localhost:\n            local.append(a)\n        else:\n            non_local.append(a)\n    return local, non_local\n\n\n# Ported Ray compression utils, encoding apparently necessary for Redis.\ndef ray_compress(data):\n    data = pyarrow.serialize(data).to_buffer().to_pybytes()\n    data = lz4.frame.compress(data)\n    # Unclear why ascii decoding.\n    data = base64.b64encode(data).decode(""ascii"")\n    # data = base64.b64encode(data)\n    return data\n\n\ndef ray_decompress(data):\n    if isinstance(data, bytes) or isinstance(data, string_types):\n        data = base64.b64decode(data)\n        data = lz4.frame.decompress(data)\n        data = pyarrow.deserialize(data)\n    return data\n\n\n# Ray\'s magic constant worker explorations..\ndef worker_exploration(worker_index, num_workers):\n    """"""\n    Computes an exploration value for a worker\n    Args:\n        worker_index (int): This worker\'s integer index.\n        num_workers (int): Total number of workers.\n    Returns:\n        float: Constant epsilon value to use.\n    """"""\n    exponent = (1.0 + worker_index / float(num_workers - 1) * 7)\n    return 0.4 ** exponent\n\n\ndef merge_samples(samples, decompress=False):\n    """"""\n    Merges list of samples into a final batch.\n    Args:\n        samples (list): List of EnvironmentSamples\n        decompress (bool): If true, assume states are compressed and decompress them.\n\n    Returns:\n        dict: Sample batch of numpy arrays.\n    """"""\n    batch = {}\n    sample_layout = samples[0].sample_batch\n    for key in sample_layout.keys():\n        # E.g. action dict.\n        if isinstance(sample_layout[key], dict):\n            batch[key] = {}\n            for name in sample_layout[key].keys():\n                batch[key][name] = np.concatenate([sample.sample_batch[key][name] for sample in samples])\n        else:\n            batch[key] = np.concatenate([sample.sample_batch[key] for sample in samples])\n\n    if decompress:\n        assert ""states"" in batch\n        batch[""states""] = np.asarray([ray_decompress(state) for state in batch[""states""]])\n    return batch\n'"
rlgraph/execution/ray/ray_value_worker.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom copy import deepcopy\nimport time\n\nimport numpy as np\nfrom six.moves import xrange as range_\n\nfrom rlgraph.utils import util\nfrom rlgraph import get_distributed_backend\nfrom rlgraph.utils.util import SMALL_NUMBER\nfrom rlgraph.environments.sequential_vector_env import SequentialVectorEnv\nfrom rlgraph.execution.environment_sample import EnvironmentSample\nfrom rlgraph.execution.ray import RayExecutor\nfrom rlgraph.execution.ray.ray_actor import RayActor\nfrom rlgraph.execution.ray.ray_util import ray_compress\n\nif get_distributed_backend() == ""ray"":\n    import ray\n\n\nclass RayValueWorker(RayActor):\n    """"""\n    Ray worker for value-based algorithms, e.g. for distributed Q-learning variants\n    such as Ape-X.\n    """"""\n\n    def __init__(self, agent_config, worker_spec, env_spec, frameskip=1):\n        """"""\n        Creates agent and environment for Ray worker.\n\n        Args:\n            agent_config (dict): Agent configuration dict.\n            worker_spec (dict): Worker parameters.\n            env_spec (dict): Environment config for environment to run.\n            frameskip (int): How often actions are repeated after retrieving them from the agent.\n        """"""\n        assert get_distributed_backend() == ""ray""\n        # Internal frameskip of env.\n        self.env_frame_skip = worker_spec.get(""env_internal_frame_skip"", 1)\n        # Worker computes weights for prioritized sampling.\n        worker_spec = deepcopy(worker_spec)\n        self.num_environments = worker_spec.pop(""num_worker_environments"", 1)\n\n        # Make sample size proportional to num envs.\n        self.worker_sample_size = worker_spec.pop(""worker_sample_size"") * self.num_environments\n        self.worker_executes_postprocessing = worker_spec.pop(""worker_executes_postprocessing"", True)\n        self.n_step_adjustment = worker_spec.pop(""n_step_adjustment"", 1)\n        self.env_ids = [""env_{}"".format(i) for i in range_(self.num_environments)]\n        num_background_envs = worker_spec.pop(""num_background_envs"", 1)\n\n        # TODO from spec once we decided on generic vectorization.\n        self.vector_env = SequentialVectorEnv(self.num_environments, env_spec, num_background_envs)\n\n        # Then update agent config.\n        agent_config[\'state_space\'] = self.vector_env.state_space\n        agent_config[\'action_space\'] = self.vector_env.action_space\n\n        ray_exploration = worker_spec.pop(""ray_exploration"", None)\n        self.worker_executes_exploration = worker_spec.pop(""worker_executes_exploration"", False)\n        self.ray_exploration_set = False\n        if ray_exploration is not None:\n            # Update worker with worker specific constant exploration value.\n            # TODO too many levels?\n            assert agent_config[""exploration_spec""][""epsilon_spec""][""decay_spec""][""type""] == ""constant_decay"", \\\n                ""ERROR: If using Ray\'s constant exploration, exploration type must be \'constant_decay\'.""\n            if self.worker_executes_exploration:\n                agent_config[""exploration_spec""] = None\n                self.exploration_epsilon = ray_exploration\n            else:\n                agent_config[""exploration_spec""][""epsilon_spec""][""decay_spec""][""constant_value""] = ray_exploration\n                self.ray_exploration_set = True\n\n        self.discount = agent_config.get(""discount"", 0.99)\n        # Python based preprocessor as image resizing is broken in TF.\n\n        self.preprocessors = {}\n        preprocessing_spec = agent_config.get(""preprocessing_spec"", None)\n        self.is_preprocessed = {}\n        for env_id in self.env_ids:\n            self.preprocessors[env_id] = self.setup_preprocessor(\n                preprocessing_spec, self.vector_env.state_space.with_batch_rank()\n            )\n            self.is_preprocessed[env_id] = False\n        self.agent = self.setup_agent(agent_config, worker_spec)\n        self.worker_frameskip = frameskip\n\n        #  Flag for container actions.\n        self.container_actions = self.agent.flat_action_space is not None\n        self.action_space = self.agent.flat_action_space\n\n        # Save these so they can be fetched after training if desired.\n        self.finished_episode_rewards = [[] for _ in range_(self.num_environments)]\n        self.finished_episode_timesteps = [[] for _ in range_(self.num_environments)]\n        # Total times sample the ""real"" wallclock time from start to end for each episode.\n        self.finished_episode_total_times = [[] for _ in range_(self.num_environments)]\n        # Sample times stop the wallclock time counter between runs, so only the sampling time is accounted for.\n        self.finished_episode_sample_times = [[] for _ in range_(self.num_environments)]\n\n        self.total_worker_steps = 0\n        self.episodes_executed = 0\n\n        # Step time and steps done per call to execute_and_get to measure throughput of this worker.\n        self.sample_times = []\n        self.sample_steps = []\n        self.sample_env_frames = []\n\n        # To continue running through multiple exec calls.\n        self.last_states = self.vector_env.reset_all()\n\n        self.zero_batched_state = np.zeros((1,) + self.agent.preprocessed_state_space.shape)\n        self.zero_unbatched_state = np.zeros(self.agent.preprocessed_state_space.shape)\n        self.preprocessed_states_buffer = np.zeros(\n            shape=(self.num_environments,) + self.agent.preprocessed_state_space.shape,\n            dtype=self.agent.preprocessed_state_space.dtype\n        )\n        self.last_ep_timesteps = [0 for _ in range_(self.num_environments)]\n        self.last_ep_rewards = [0 for _ in range_(self.num_environments)]\n        self.last_ep_start_timestamps = [0.0 for _ in range_(self.num_environments)]\n        self.last_ep_start_initialized = False  # initialize on first `execute_and_get_timesteps()` call\n        self.last_ep_sample_times = [0.0 for _ in range_(self.num_environments)]\n\n        # Was the last state a terminal state so env should be reset in next call?\n        self.last_terminals = [False for _ in range_(self.num_environments)]\n\n    def get_constructor_success(self):\n        """"""\n        For debugging: fetch the last attribute. Will fail if constructor failed.\n        """"""\n        return not self.last_terminals[0]\n\n    @classmethod\n    def as_remote(cls, num_cpus=None, num_gpus=None):\n        return ray.remote(num_cpus=num_cpus, num_gpus=num_gpus)(cls)\n\n    def setup_agent(self, agent_config, worker_spec):\n        """"""\n        Sets up agent, potentially modifying its configuration via worker specific settings.\n        """"""\n        sample_exploration = worker_spec.pop(""sample_exploration"", False)\n        # Adjust exploration for this worker.\n        if sample_exploration:\n            assert self.ray_exploration_set is False, ""ERROR: Cannot sample exploration if ray exploration is used.""\n            exploration_min_value = worker_spec.pop(""exploration_min_value"", 0.0)\n            epsilon_spec = agent_config[""exploration_spec""][""epsilon_spec""]\n\n            if epsilon_spec is not None and ""decay_spec"" in epsilon_spec:\n                decay_from = epsilon_spec[""decay_spec""][""from""]\n                assert decay_from >= exploration_min_value, \\\n                    ""Min value for exploration sampling must be smaller than"" \\\n                    ""decay_from {} in exploration_spec but is {}."".format(decay_from, exploration_min_value)\n\n                # Sample a new initial epsilon from the interval [exploration_min_value, decay_from).\n                sampled_from = np.random.uniform(low=exploration_min_value, high=decay_from)\n                epsilon_spec[""decay_spec""][""from""] = sampled_from\n\n        # Worker execution spec may differ from controller/learner.\n        worker_exec_spec = worker_spec.get(""execution_spec"", None)\n        if worker_exec_spec is not None:\n            agent_config.update(execution_spec=worker_exec_spec)\n\n        # Build lazily per default.\n        return RayExecutor.build_agent_from_config(agent_config)\n\n    def execute_and_get_timesteps(\n        self,\n        num_timesteps,\n        max_timesteps_per_episode=0,\n        use_exploration=True,\n        break_on_terminal=False\n    ):\n        """"""\n        Collects and returns time step experience.\n\n        Args:\n            break_on_terminal (Optional[bool]): If true, breaks when a terminal is encountered. If false,\n                executes exactly \'num_timesteps\' steps.\n        """"""\n        # Initialize start timestamps. Initializing the timestamps here should make the observed execution timestamps\n        # more accurate, as there might be delays between the worker initialization and actual sampling start.\n        if not self.last_ep_start_initialized:\n            for i, timestamp in enumerate(self.last_ep_start_timestamps):\n                self.last_ep_start_timestamps[i] = time.perf_counter()\n            self.last_ep_start_initialized = True\n\n        start = time.monotonic()\n        timesteps_executed = 0\n        episodes_executed = [0 for _ in range_(self.num_environments)]\n        env_frames = 0\n        last_episode_rewards = []\n        # Final result batch.\n        if self.container_actions:\n            batch_actions = {k: [] for k in self.action_space.keys()}\n        else:\n            batch_actions = []\n        batch_states, batch_rewards, batch_next_states, batch_terminals = [], [], [], []\n\n        # Running trajectories.\n        sample_states, sample_actions, sample_rewards, sample_terminals = {}, {}, {}, {}\n        next_states = [np.zeros_like(self.last_states) for _ in range_(self.num_environments)]\n\n        # Reset envs and Agent either if finished an episode in current loop or if last state\n        # from previous execution was terminal for that environment.\n        for i, env_id in enumerate(self.env_ids):\n            sample_states[env_id] = []\n            if self.container_actions:\n                sample_actions[env_id] = {k: [] for k in self.action_space.keys()}\n            else:\n                sample_actions[env_id] = []\n            sample_rewards[env_id] = []\n            sample_terminals[env_id] = []\n\n        env_states = self.last_states\n        current_episode_rewards = self.last_ep_rewards\n        current_episode_timesteps = self.last_ep_timesteps\n        current_episode_start_timestamps = self.last_ep_start_timestamps\n        current_episode_sample_times = self.last_ep_sample_times\n\n        # Whether the episode in each env has terminated.\n        terminals = [False for _ in range_(self.num_environments)]\n        while timesteps_executed < num_timesteps:\n            current_iteration_start_timestamp = time.perf_counter()\n            for i, env_id in enumerate(self.env_ids):\n                state, _ = self.agent.state_space.force_batch(env_states[i])\n                if self.preprocessors[env_id] is not None:\n                    if self.is_preprocessed[env_id] is False:\n                        self.preprocessed_states_buffer[i] = self.preprocessors[env_id].preprocess(state)\n                        self.is_preprocessed[env_id] = True\n                else:\n                    self.preprocessed_states_buffer[i] = env_states[i]\n\n            actions = self.get_action(states=self.preprocessed_states_buffer,\n                                      use_exploration=use_exploration, apply_preprocessing=False)\n            if self.agent.flat_action_space is not None:\n                some_key = next(iter(actions))\n                assert isinstance(actions, dict) and isinstance(actions[some_key], np.ndarray),\\\n                    ""ERROR: Cannot flip container-action batch with dict keys if returned value is not a dict OR "" \\\n                    ""values of returned value are not np.ndarrays!""\n                if hasattr(actions[some_key], ""__len__""):\n                    env_actions = [{key: value[i] for key, value in actions.items()} for i in range(len(actions[some_key]))]\n                else:\n                    # Action was not array type.\n                    env_actions = actions\n            # No flipping necessary.\n            else:\n                env_actions = actions\n                if self.num_environments == 1 and env_actions.shape == ():\n                    env_actions = [env_actions]\n\n            next_states, step_rewards, terminals, infos = self.vector_env.step(actions=env_actions)\n            # Worker frameskip not needed as done in env.\n            # for _ in range_(self.worker_frameskip):\n            #     next_states, step_rewards, terminals, infos = self.vector_env.step(actions=actions)\n            #     env_frames += self.num_environments\n            #\n            #     for i, env_id in enumerate(self.env_ids):\n            #         rewards[env_id] += step_rewards[i]\n            #     if np.any(terminals):\n            #         break\n\n            timesteps_executed += self.num_environments\n            env_frames += self.num_environments\n            env_states = next_states\n            current_iteration_time = time.perf_counter() - current_iteration_start_timestamp\n\n            # Do accounting for each environment.\n            state_buffer = np.array(self.preprocessed_states_buffer)\n            for i, env_id in enumerate(self.env_ids):\n                # Set is preprocessed to False because env_states are currently NOT preprocessed.\n                self.is_preprocessed[env_id] = False\n                current_episode_timesteps[i] += 1\n                # Each position is the running episode reward of that episode. Add step reward.\n                current_episode_rewards[i] += step_rewards[i]\n                sample_states[env_id].append(state_buffer[i])\n\n                if self.container_actions:\n                    for name in self.action_space.keys():\n                        sample_actions[env_id][name].append(env_actions[i][name])\n                else:\n                    sample_actions[env_id].append(env_actions[i])\n                sample_rewards[env_id].append(step_rewards[i])\n                sample_terminals[env_id].append(terminals[i])\n                current_episode_sample_times[i] += current_iteration_time\n\n                # Terminate and reset episode for that environment.\n                if terminals[i] or (0 < max_timesteps_per_episode <= current_episode_timesteps[i]):\n                    self.finished_episode_rewards[i].append(current_episode_rewards[i])\n                    self.finished_episode_timesteps[i].append(current_episode_timesteps[i])\n                    self.finished_episode_total_times[i].append(time.perf_counter() - current_episode_start_timestamps[i])\n                    self.finished_episode_sample_times[i].append(current_episode_sample_times[i])\n                    episodes_executed[i] += 1\n                    self.episodes_executed += 1\n                    last_episode_rewards.append(current_episode_rewards[i])\n\n                    env_sample_states = sample_states[env_id]\n                    # Get next states for this environment\'s trajectory.\n                    env_sample_next_states = env_sample_states[1:]\n\n                    next_state, _ = self.agent.state_space.force_batch(next_states[i])\n                    if self.preprocessors[env_id] is not None:\n                        next_state = self.preprocessors[env_id].preprocess(next_state)\n\n                    # Extend because next state has a batch dim.\n                    env_sample_next_states.extend(next_state)\n\n                    # Post-process this trajectory via n-step discounting.\n                    # print(""processing terminal episode of length:"", len(env_sample_states))\n                    post_s, post_a, post_r, post_next_s, post_t = self._truncate_n_step(env_sample_states,\n                        sample_actions[env_id], sample_rewards[env_id], env_sample_next_states,\n                        sample_terminals[env_id], was_terminal=True)\n\n                    # Append to final result trajectories.\n                    batch_states.extend(post_s)\n                    if self.agent.flat_action_space is not None:\n                        # Use actions here, not env actions.\n                        for name in self.agent.flat_action_space.keys():\n                            batch_actions[name].extend(post_a[name])\n                    else:\n                        batch_actions.extend(post_a)\n                    batch_rewards.extend(post_r)\n                    batch_next_states.extend(post_next_s)\n                    batch_terminals.extend(post_t)\n\n                    # Reset running trajectory for this env.\n                    sample_states[env_id] = []\n                    if self.container_actions:\n                        sample_actions[env_id] = {k: [] for k in self.action_space.keys()}\n                    else:\n                        sample_actions[env_id] = []\n                    sample_rewards[env_id] = []\n                    sample_terminals[env_id] = []\n\n                    # Reset this environment and its pre-processor stack.\n                    env_states[i] = self.vector_env.reset(i)\n                    if self.preprocessors[env_id] is not None:\n                        self.preprocessors[env_id].reset()\n                        # This re-fills the sequence with the reset state.\n                        state, _ = self.agent.state_space.force_batch(env_states[i])\n                        # Pre - process, add to buffer\n                        self.preprocessed_states_buffer[i] = np.array(self.preprocessors[env_id].preprocess(state))\n                        self.is_preprocessed[env_id] = True\n                    current_episode_rewards[i] = 0\n                    current_episode_timesteps[i] = 0\n                    current_episode_start_timestamps[i] = time.perf_counter()\n                    current_episode_sample_times[i] = 0.0\n\n            if 0 < num_timesteps <= timesteps_executed or (break_on_terminal and np.any(terminals)):\n                self.total_worker_steps += timesteps_executed\n                break\n\n        self.last_terminals = terminals\n        self.last_states = env_states\n        self.last_ep_rewards = current_episode_rewards\n        self.last_ep_timesteps = current_episode_timesteps\n        self.last_ep_start_timestamps = current_episode_start_timestamps\n        self.last_ep_sample_times = current_episode_sample_times\n\n        # We already accounted for all terminated episodes. This means we only\n        # have to do accounting for any unfinished fragments.\n        for i, env_id in enumerate(self.env_ids):\n            # This env was not terminal -> need to process remaining trajectory\n            if not terminals[i]:\n                env_sample_states = sample_states[env_id]\n                # Get next states for this environment\'s trajectory.\n                env_sample_next_states = env_sample_states[1:]\n                next_state, _ = self.agent.state_space.force_batch(next_states[i])\n                if self.preprocessors[env_id] is not None:\n                    next_state = self.preprocessors[env_id].preprocess(next_state)\n                    # This is the env state in the next call so avoid double preprocessing\n                    # by adding to buffer.\n                    self.preprocessed_states_buffer[i] = np.array(next_state)\n                    self.is_preprocessed[env_id] = True\n\n                # Extend because next state has a batch dim.\n                env_sample_next_states.extend(next_state)\n                post_s, post_a, post_r, post_next_s, post_t = self._truncate_n_step(env_sample_states,\n                    sample_actions[env_id], sample_rewards[env_id], env_sample_next_states,\n                    sample_terminals[env_id], was_terminal=False)\n\n                batch_states.extend(post_s)\n                if self.agent.flat_action_space is not None:\n                    # Use actions here, not env actions.\n                    for name in self.agent.flat_action_space.keys():\n                        batch_actions[name].extend(post_a[name])\n                else:\n                    batch_actions.extend(post_a)\n                batch_rewards.extend(post_r)\n                batch_next_states.extend(post_next_s)\n                batch_terminals.extend(post_t)\n\n        # Perform final batch-processing once.\n        sample_batch, batch_size = self._batch_process_sample(batch_states, batch_actions,\n                                                              batch_rewards, batch_next_states, batch_terminals)\n\n        total_time = (time.monotonic() - start) or 1e-10\n        self.sample_steps.append(timesteps_executed)\n        self.sample_times.append(total_time)\n        self.sample_env_frames.append(env_frames)\n\n        # Note that the controller already evaluates throughput so there is no need\n        # for each worker to calculate expensive statistics now.\n        return EnvironmentSample(\n            sample_batch=sample_batch,\n            batch_size=batch_size,\n            metrics=dict(\n                last_rewards=last_episode_rewards,\n                runtime=total_time,\n                # Agent act/observe throughput.\n                timesteps_executed=timesteps_executed,\n                ops_per_second=(timesteps_executed / total_time),\n            )\n        )\n\n    @ray.method(num_return_vals=2)\n    def execute_and_get_with_count(self):\n        sample = self.execute_and_get_timesteps(num_timesteps=self.worker_sample_size)\n\n        # Return count and reward as separate task so learner thread does not need to download them before\n        # inserting to buffers..\n        return sample, {""batch_size"": sample.batch_size, ""last_rewards"": sample.metrics[""last_rewards""]}\n\n    def set_weights(self, weights):\n        policy_weights = {k: v for k,v in zip(weights.policy_vars, weights.policy_values)}\n        vf_weights = None\n        if weights.has_vf:\n            vf_weights = {k: v for k, v in zip(weights.value_function_vars, weights.value_function_values)}\n        self.agent.set_weights(policy_weights, value_function_weights=vf_weights)\n\n    def get_workload_statistics(self):\n        """"""\n        Returns performance results for this worker.\n\n        Returns:\n            dict: Performance metrics.\n        """"""\n        # Adjust env frames for internal env frameskip:\n        adjusted_frames = [env_frames * self.env_frame_skip for env_frames in self.sample_env_frames]\n        if len(self.finished_episode_rewards) > 0:\n            all_finished_rewards = []\n            for env_reward_list in self.finished_episode_rewards:\n                all_finished_rewards.extend(env_reward_list)\n            min_episode_reward = np.min(all_finished_rewards)\n            max_episode_reward = np.max(all_finished_rewards)\n            mean_episode_reward = np.mean(all_finished_rewards)\n            # Mean of final episode rewards over all envs\n            final_episode_reward = np.mean([env_rewards[-1] for env_rewards in self.finished_episode_rewards])\n        else:\n            # Will be aggregated in executor.\n            min_episode_reward = None\n            max_episode_reward = None\n            mean_episode_reward = None\n            final_episode_reward = None\n\n        return dict(\n            episode_timesteps=self.finished_episode_timesteps,\n            episode_rewards=self.finished_episode_rewards,\n            episode_total_times=self.finished_episode_total_times,\n            episode_sample_times=self.finished_episode_sample_times,\n            min_episode_reward=min_episode_reward,\n            max_episode_reward=max_episode_reward,\n            mean_episode_reward=mean_episode_reward,\n            final_episode_reward=final_episode_reward,\n            episodes_executed=self.episodes_executed,\n            worker_steps=self.total_worker_steps,\n            mean_worker_ops_per_second=sum(self.sample_steps) / sum(self.sample_times),\n            mean_worker_env_frames_per_second=sum(adjusted_frames) / sum(self.sample_times)\n        )\n\n    def _truncate_n_step(self, states, actions, rewards, next_states, terminals, was_terminal=True):\n        """"""\n        Computes n-step truncation for exactly one episode segment of one environment.\n\n        Returns:\n             n-step truncated (shortened) version.\n        """"""\n        if self.n_step_adjustment > 1:\n            new_len = len(states) - self.n_step_adjustment + 1\n\n            # There are 2 cases. If the trajectory did not end in a terminal,\n            # we just have to move states forward and truncate.\n            if was_terminal:\n                # We know the ONLY last terminal is True.\n                terminal_position = len(rewards) - 1\n                for i in range(len(rewards)):\n                    for j in range(1, self.n_step_adjustment):\n                        # Outside sample data. Stop inner loop and set truncate = True\n                        if i + j >= len(next_states):\n                            break\n                        # Normal case: No terminal ahead (so far) in n-step sequence.\n                        if i + j < terminal_position:\n                            next_states[i] = next_states[i + j]\n                            rewards[i] += self.discount ** j * rewards[i + j]\n                        # Terminal ahead: Don\'t go beyond it.\n                        # Repeat it for the remaining n-steps and always assume r=0.0.\n                        else:\n                            next_states[i] = next_states[terminal_position]\n                            terminals[i] = True\n                            if i + j <= terminal_position:\n                                rewards[i] += self.discount ** j * rewards[i + j]\n            else:\n                # We know this segment does not contain any terminals so we simply have to adjust next\n                # states and rewards.\n                for i in range_(len(rewards) - self.n_step_adjustment + 1):\n                    for j in range_(1, self.n_step_adjustment):\n                        next_states[i] = next_states[i + j]\n                        rewards[i] += self.discount ** j * rewards[i + j]\n\n                if self.agent.flat_action_space is not None:\n                    for arr in [states, rewards, next_states, terminals]:\n                        del arr[new_len:]\n                    # Delete container actions separately.\n                    for name in self.agent.flat_action_space.keys():\n                        del actions[name][new_len:]\n                else:\n                    for arr in [states, actions, rewards, next_states, terminals]:\n                        del arr[new_len:]\n\n        return states, actions, rewards, next_states, terminals\n\n    def _batch_process_sample(self, states, actions, rewards, next_states, terminals):\n        """"""\n        Batch Post-processes sample, e.g. by computing priority weights, and compressing.\n\n        Args:\n            states (list): List of states.\n            actions (list, dict): List of actions or dict of lists  for container actions.\n            rewards (list): List of rewards.\n            next_states: (list): List of next_states.\n            terminals (list): List of terminals.\n\n        Returns:\n            dict: Sample batch dict.\n        """"""\n        weights = np.ones_like(rewards)\n\n        # Compute loss-per-item.\n        if self.worker_executes_postprocessing:\n            # Next states were just collected, we batch process them here.\n            _, loss_per_item = self.agent.post_process(\n                dict(\n                    states=states,\n                    actions=actions,\n                    rewards=rewards,\n                    terminals=terminals,\n                    next_states=next_states,\n                    importance_weights=weights\n                )\n            )\n            weights = np.abs(loss_per_item) + SMALL_NUMBER\n        env_dtype = self.vector_env.state_space.dtype\n        compressed_states = [ray_compress(np.asarray(state, dtype=util.convert_dtype(dtype=env_dtype, to=\'np\')))\n                             for state in states]\n\n        compressed_next_states = compressed_states[self.n_step_adjustment:] + \\\n                                 [ray_compress(np.asarray(next_s,dtype=util.convert_dtype(dtype=env_dtype, to=\'np\')))\n                                  for next_s in next_states[-self.n_step_adjustment:]]\n        if self.container_actions:\n            for name in self.action_space.keys():\n                actions[name] = np.array(actions[name])\n        else:\n            actions = np.array(actions)\n        return dict(\n            states=compressed_states,\n            actions=actions,\n            rewards=np.array(rewards),\n            terminals=np.array(terminals),\n            next_states=compressed_next_states,\n            importance_weights=np.array(weights)\n        ), len(rewards)\n\n    def get_action(self, states, use_exploration, apply_preprocessing):\n        if self.worker_executes_exploration:\n            # Only once for all actions otherwise we would have to call a session anyway.\n            if np.random.random() <= self.exploration_epsilon:\n                    action = self.agent.action_space.sample(size=self.num_environments)\n            else:\n                    action = self.agent.get_action(states=states, use_exploration=use_exploration,\n                                                    apply_preprocessing=apply_preprocessing)\n            return action\n        else:\n            return self.agent.get_action(states=states, use_exploration=use_exploration,\n                                         apply_preprocessing=apply_preprocessing)\n\n'"
rlgraph/execution/ray/sync_batch_executor.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.environments import Environment\nfrom rlgraph.execution.ray.ray_policy_worker import RayPolicyWorker\n\nfrom rlgraph import get_distributed_backend\nfrom rlgraph.execution.ray.ray_executor import RayExecutor\nfrom rlgraph.execution.ray.ray_util import merge_samples, RayWeight\n\nif get_distributed_backend() == ""ray"":\n    import ray\n\n\nclass SyncBatchExecutor(RayExecutor):\n    """"""\n    Implements distributed synchronous execution.\n    """"""\n    def __init__(self, environment_spec, agent_config):\n        """"""\n        Args:\n            environment_spec (dict, callable): Environment spec or callable returning an environment. Each worker\n                in the cluster will instantiate an environment using this spec or callable.\n            agent_config (dict): Config dict containing agent and execution specs.\n        """"""\n        ray_spec = agent_config[""execution_spec""].pop(""ray_spec"")\n        self.worker_spec = ray_spec.pop(""worker_spec"")\n        self.compress_states = self.worker_spec[""compress_states""]\n        super(SyncBatchExecutor, self).__init__(executor_spec=ray_spec.pop(""executor_spec""),\n                                                environment_spec=environment_spec,\n                                                worker_spec=self.worker_spec)\n\n        # Must specify an agent type.\n        assert ""type"" in agent_config\n        self.agent_config = agent_config\n        environment = None\n        if isinstance(self.environment_spec, dict):\n            environment = Environment.from_spec(self.environment_spec)\n        elif hasattr(self.environment_spec, \'__call__\'):\n            environment = self.environment_spec()\n        self.agent_config[""state_space""] = environment.state_space\n        self.agent_config[""action_space""] = environment.action_space\n\n        self.local_agent = self.build_agent_from_config(self.agent_config)\n        self.update_batch_size = self.agent_config[""update_spec""][""batch_size""]\n\n        # Create remote sample workers based on ray cluster spec.\n        self.num_sample_workers = self.executor_spec[""num_sample_workers""]\n\n        # These are the tasks actually interacting with the environment.\n        self.worker_sample_size = self.executor_spec[""num_worker_samples""]\n\n        assert not ray_spec, ""ERROR: ray_spec still contains items: {}"".format(ray_spec)\n        self.logger.info(""Setting up execution for Apex executor."")\n        self.setup_execution()\n\n    def setup_execution(self):\n        # Start Ray cluster and connect to it.\n        self.ray_init()\n\n        # Create remote workers for data collection.\n        self.worker_spec[""worker_sample_size""] = self.worker_sample_size\n        self.logger.info(""Initializing {} remote data collection agents, sample size: {}"".format(\n            self.num_sample_workers, self.worker_spec[""worker_sample_size""]))\n        self.ray_env_sample_workers = self.create_remote_workers(\n            RayPolicyWorker, self.num_sample_workers, self.agent_config,\n            # *args\n            self.worker_spec, self.environment_spec, self.worker_frame_skip\n        )\n\n    def _execute_step(self):\n        """"""\n        Executes a workload on Ray. The main loop performs the following\n        steps until the specified number of steps or episodes is finished:\n\n        - Sync weights to policy workers.\n        - Schedule a set of samples\n        - Wait until enough samples tasks are complete to form an update batch\n        - Merge samples\n        - Perform local update(s)\n        """"""\n        # Env steps done during this rollout.\n        env_steps = 0\n\n        # 1. Sync local learners weights to remote workers.\n        weights = ray.put(RayWeight(self.local_agent.get_weights()))\n        for ray_worker in self.ray_env_sample_workers:\n            ray_worker.set_weights.remote(weights)\n\n        # 2. Schedule samples and fetch results from RayWorkers.\n        sample_batches = []\n        num_samples = 0\n        while num_samples < self.update_batch_size:\n            batches = ray.get([worker.execute_and_get_timesteps.remote(self.worker_sample_size)\n                              for worker in self.ray_env_sample_workers])\n            # Each batch has exactly worker_sample_size length.\n            num_samples += len(batches) * self.worker_sample_size\n            sample_batches.extend(batches)\n\n        env_steps += num_samples\n        # 3. Merge samples\n        rewards = []\n        for sample in sample_batches:\n            if len(sample.metrics[""last_rewards""]) > 0:\n                rewards.extend(sample.metrics[""last_rewards""])\n        batch = merge_samples(sample_batches, decompress=self.compress_states)\n\n        # 4. Update from merged batch.\n        self.local_agent.update(batch, apply_postprocessing=False)\n        return env_steps, 1, {\n            ""discarded"": 0,\n            ""queue_inserts"": 0,\n            ""rewards"": rewards\n        }\n\n\n'"
rlgraph/tests/agent_functionality/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n'"
rlgraph/tests/agent_functionality/test_all_compile.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nfrom rlgraph import get_backend\nfrom rlgraph.agents import DQNAgent, ApexAgent, IMPALAAgent, ActorCriticAgent, PPOAgent, SACAgent\nfrom rlgraph.environments import OpenAIGymEnv, GridWorld, GaussianDensityAsRewardEnv\nfrom rlgraph.spaces import FloatBox, Tuple\nfrom rlgraph.tests.test_util import config_from_path\nfrom rlgraph.utils.util import default_dict\n\n\nclass TestAllCompile(unittest.TestCase):\n    """"""\n    Tests if all agents compile correctly on relevant configurations.\n    """"""\n    impala_cluster_spec = dict(learner=[""localhost:22222""], actor=[""localhost:22223""])\n\n    def test_dqn_compilation(self):\n        """"""\n        Tests DQN Agent compilation.\n        """"""\n        env = OpenAIGymEnv(""Pong-v0"", frameskip=4, max_num_noops=30, episodic_life=True)\n        agent_config = config_from_path(""configs/dqn_agent_for_pong.json"")\n        agent = DQNAgent.from_spec(\n            # Uses 2015 DQN parameters as closely as possible.\n            agent_config,\n            state_space=env.state_space,\n            # Try with ""reduced"" action space (actually only 3 actions, up, down, no-op)\n            action_space=env.action_space\n        )\n        print(""Compiled {}"".format(agent))\n\n    def test_apex_compilation(self):\n        """"""\n        Tests agent compilation without Ray to ease debugging on Windows.\n        """"""\n        agent_config = config_from_path(""configs/ray_apex_for_pong.json"")\n        agent_config[""execution_spec""].pop(""ray_spec"")\n        environment = OpenAIGymEnv(""Pong-v0"", frameskip=4)\n\n        agent = ApexAgent.from_spec(\n            agent_config, state_space=environment.state_space,\n            action_space=environment.action_space\n        )\n        print(""Compiled {}"".format(agent))\n\n    def test_actor_critic_compilation(self):\n        """"""\n        Tests Policy gradient agent compilation.\n        """"""\n        env = OpenAIGymEnv(""Pong-v0"", frameskip=4, max_num_noops=30, episodic_life=True)\n        agent_config = config_from_path(""configs/actor_critic_agent_for_pong.json"")\n        agent = ActorCriticAgent.from_spec(\n            agent_config,\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n        print(""Compiled {}"".format(agent))\n\n    def test_ppo_compilation(self):\n        """"""\n        Tests PPO agent compilation.\n        """"""\n        env = OpenAIGymEnv(""Pong-v0"", frameskip=4, max_num_noops=30, episodic_life=True)\n        agent_config = config_from_path(""configs/ppo_agent_for_pong.json"")\n        agent = PPOAgent.from_spec(\n            agent_config,\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n        print(""Compiled {}"".format(agent))\n\n    def test_impala_single_agent_compilation(self):\n        """"""\n        Tests IMPALA agent compilation (single-node mode).\n        """"""\n        return\n        if get_backend() == ""pytorch"":\n            return\n        env = GridWorld(""2x2"")\n        agent = IMPALAAgent.from_spec(\n            config_from_path(""configs/impala_agent_for_2x2_gridworld.json""),\n            state_space=env.state_space,\n            action_space=env.action_space,\n            update_spec=dict(batch_size=16),\n            optimizer_spec=dict(type=""adam"", learning_rate=0.05),\n            # Make session-creation hang in docker.\n            execution_spec=dict(disable_monitoring=True)\n        )\n        agent.terminate()\n        print(""Compiled {}"".format(agent))\n\n    def test_impala_actor_compilation(self):\n        """"""\n        Tests IMPALA agent compilation (actor).\n        """"""\n        return\n        if get_backend() == ""pytorch"":\n            return\n        try:\n            from rlgraph.environments.deepmind_lab import DeepmindLabEnv\n        except ImportError:\n            print(""Deepmind Lab not installed: Will skip this test."")\n            return\n\n        agent_config = config_from_path(""configs/impala_agent_for_deepmind_lab_env.json"")\n        env_spec = dict(level_id=""seekavoid_arena_01"", observations=[""RGB_INTERLEAVED"", ""INSTR""], frameskip=4)\n        dummy_env = DeepmindLabEnv.from_spec(env_spec)\n        agent = IMPALAAgent.from_spec(\n            agent_config,\n            type=""actor"",\n            state_space=dummy_env.state_space,\n            action_space=dummy_env.action_space,\n            internal_states_space=Tuple(FloatBox(shape=(256,)), FloatBox(shape=(256,)), add_batch_rank=False),\n            environment_spec=default_dict(dict(type=""deepmind-lab""), env_spec),\n            # Make session-creation hang in docker.\n            execution_spec=dict(\n                session_config=dict(\n                    type=""monitored-training-session"",\n                    auto_start=False\n                ),\n                disable_monitoring=True\n            )\n        )\n        # Start Specifiable Server with Env manually (monitoring is disabled).\n        agent.environment_stepper.environment_server.start_server()\n        print(""Compiled {}"".format(agent))\n        agent.environment_stepper.environment_server.stop_server()\n        agent.terminate()\n\n    def test_impala_learner_compilation(self):\n        """"""\n        Tests IMPALA agent compilation (learner).\n        """"""\n        return\n        if get_backend() == ""pytorch"":\n            return\n        try:\n            from rlgraph.environments.deepmind_lab import DeepmindLabEnv\n        except ImportError:\n            print(""Deepmind Lab not installed: Will skip this test."")\n            return\n\n        agent_config = config_from_path(""configs/impala_agent_for_deepmind_lab_env.json"")\n        env_spec = dict(level_id=""seekavoid_arena_01"", observations=[""RGB_INTERLEAVED"", ""INSTR""], frameskip=4)\n        dummy_env = DeepmindLabEnv.from_spec(env_spec)\n        learner_agent = IMPALAAgent.from_spec(\n            agent_config,\n            type=""learner"",\n            state_space=dummy_env.state_space,\n            action_space=dummy_env.action_space,\n            internal_states_space=IMPALAAgent.default_internal_states_space,\n            environment_spec=default_dict(dict(type=""deepmind-lab""), env_spec),\n            # Setup distributed tf.\n            execution_spec=dict(\n                mode=""distributed"",\n                #gpu_spec=dict(\n                #    gpus_enabled=True,\n                #    max_usable_gpus=1,\n                #    num_gpus=1\n                #),\n                distributed_spec=dict(job=""learner"", task_index=0, cluster_spec=self.impala_cluster_spec),\n                session_config=dict(\n                    type=""monitored-training-session"",\n                    allow_soft_placement=True,\n                    log_device_placement=True,\n                    auto_start=False\n                ),\n                disable_monitoring=True,\n                enable_timeline=True,\n            )\n        )\n        print(""Compiled IMPALA type=learner agent without starting the session (would block waiting for actor)."")\n\n        ## Take one batch from the filled up queue and run an update_from_memory with the learner.\n        #update_steps = 10\n        #time_start = time.perf_counter()\n        #for _ in range(update_steps):\n        #    agent.call_api_method(""update_from_memory"")\n        #time_total = time.perf_counter() - time_start\n        #print(""Done learning {}xbatch-of-{} in {}sec ({} updates/sec)."".format(\n        #    update_steps, agent.update_spec[""batch_size""], time_total , update_steps / time_total)\n        #)\n\n        learner_agent.terminate()\n\n    def test_sac_compilation(self):\n        # TODO: support SAC on pytorch.\n        if get_backend() == ""pytorch"":\n            return\n\n        env = GaussianDensityAsRewardEnv(episode_length=5)\n        agent = SACAgent.from_spec(\n            config_from_path(""configs/sac_agent_for_gaussian_density_env.json""),\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n        print(""Compiled {}"".format(agent))\n'"
rlgraph/tests/agent_functionality/test_apex_agent_functionality.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport unittest\nfrom rlgraph.agents import Agent\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.tests import recursive_assert_almost_equal\nfrom rlgraph.tests.test_util import config_from_path\nfrom rlgraph.utils import root_logger\n\nimport numpy as np\n\n\nclass TestApexAgentFunctionality(unittest.TestCase):\n    """"""\n    Tests Ape-X specific functionality.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    def test_apex_weight_syncing(self):\n        agent_config = config_from_path(""configs/ray_apex_for_pong.json"")\n        agent_config[""execution_spec""].pop(""ray_spec"")\n        environment = OpenAIGymEnv(""Pong-v0"", frameskip=4)\n\n        agent = Agent.from_spec(\n            agent_config,\n            state_space=environment.state_space,\n            action_space=environment.action_space\n        )\n\n        weights = agent.get_weights()[""policy_weights""]\n        print(""type weights = "", type(weights))\n        for variable, value in weights.items():\n            print(""Type value = "", type(value))\n            value += 0.01\n        agent.set_weights(weights)\n\n        new_weights = agent.get_weights()[""policy_weights""]\n        recursive_assert_almost_equal(weights, new_weights)\n\n    def test_update_from_external(self):\n        agent_config = config_from_path(""configs/ray_apex_for_pong.json"")\n        agent_config[""execution_spec""].pop(""ray_spec"")\n        environment = OpenAIGymEnv(""Pong-v0"", frameskip=4)\n\n        agent = Agent.from_spec(\n            agent_config,\n            state_space=environment.state_space,\n            action_space=environment.action_space\n        )\n\n        batch = {\n            ""states"": agent.preprocessed_state_space.sample(200),\n            ""actions"": environment.action_space.sample(200),\n            ""rewards"": np.zeros(200, dtype=np.float32),\n            ""terminals"": [False] * 200,\n            ""next_states"": agent.preprocessed_state_space.sample(200),\n            ""importance_weights"":  np.ones(200, dtype=np.float32)\n        }\n\n        agent.update(batch)\n'"
rlgraph/tests/agent_functionality/test_base_agent_functionality.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport unittest\n\nfrom rlgraph.agents import Agent, PPOAgent\nfrom rlgraph.environments import GridWorld, OpenAIGymEnv\nfrom rlgraph.tests.test_util import config_from_path, recursive_assert_almost_equal\nfrom rlgraph.utils import root_logger\n\n\nclass TestBaseAgentFunctionality(unittest.TestCase):\n    """"""\n    Tests the base Agent\'s functionality.\n    """"""\n    root_logger.setLevel(level=logging.DEBUG)\n\n    def test_weights_getting_setting(self):\n        """"""\n        Tests getting and setting of the Agent\'s weights.\n        """"""\n        env = GridWorld(world=""2x2"")\n        agent = Agent.from_spec(\n            config_from_path(""configs/dqn_agent_for_functionality_test.json""),\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n\n        weights = agent.get_weights()\n        new_weights = {}\n        for key, weight in weights[""policy_weights""].items():\n            new_weights[key] = weight + 0.01\n\n        agent.set_weights(new_weights)\n        new_actual_weights = agent.get_weights()\n\n        recursive_assert_almost_equal(new_actual_weights[""policy_weights""], new_weights)\n\n    def test_value_function_weights(self):\n        """"""\n        Tests changing of value function weights.\n        """"""\n        env = OpenAIGymEnv(""Pong-v0"")\n        agent_config = config_from_path(""configs/ppo_agent_for_pong.json"")\n        agent = PPOAgent.from_spec(\n            agent_config,\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n        weights = agent.get_weights()\n        assert ""value_function_weights"" in weights\n        assert ""policy_weights"" in weights\n\n        policy_weights = weights[""policy_weights""]\n        value_function_weights = weights[""value_function_weights""]\n\n        # Just change vf weights.\n        for key, weight in value_function_weights.items():\n            value_function_weights[key] = weight + 0.01\n        agent.set_weights(policy_weights, value_function_weights)\n        new_actual_weights = agent.get_weights()\n\n        recursive_assert_almost_equal(new_actual_weights[""value_function_weights""],\n                                      value_function_weights)\n\n    def test_build_overhead(self):\n        """"""\n        Tests build timing on agents with nested graph function calls.\n        """"""\n        env = OpenAIGymEnv(""Pong-v0"")\n        agent_config = config_from_path(""configs/ppo_agent_for_pong.json"")\n        agent_config[""auto_build""] = False\n        agent = Agent.from_spec(\n            agent_config,\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n        build = agent.build()\n        print(""Build stats = "", build)\n        # Check no negative times (e.g. from wrong start/stop times in recursive calls).\n        self.assertGreater(build[""total_build_time""], 0.0)\n        build_times = build[""build_times""][0]\n        self.assertGreater(build_times[""build_overhead""], 0.0)\n        self.assertGreater(build_times[""total_build_time""], 0.0)\n        self.assertGreater(build_times[""op_creation""], 0.0)\n        self.assertGreater(build_times[""var_creation""], 0.0)\n        self.assertGreater(build_times[""total_build_time""], build_times[""build_overhead""])\n'"
rlgraph/tests/agent_functionality/test_dqfd_agent_functionality.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\n\nfrom rlgraph.agents import DQFDAgent\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.spaces import BoolBox, FloatBox, IntBox, Dict\nfrom rlgraph.tests.test_util import config_from_path, recursive_assert_almost_equal\n\n\nclass TestDQFDAgentFunctionality(unittest.TestCase):\n    """"""\n    Tests the DQFD Agent\'s functionality.\n    """"""\n    env_spec = dict(type=""openai"", gym_env=""CartPole-v0"")\n\n    def test_container_actions(self):\n        # Test container actions with embedding.\n\n        vocab_size = 100\n        embed_dim = 128\n        # ID/state space.\n        state_space = IntBox(vocab_size, shape=(10,))\n        # Container action space.\n        actions_space = {}\n        num_outputs = 3\n        for i in range(3):\n            actions_space[\'action_{}\'.format(i)] = IntBox(\n                low=0,\n                high=num_outputs\n            )\n        actions_space = Dict(actions_space)\n\n        agent_config = config_from_path(""configs/dqfd_container.json"")\n        agent_config[""network_spec""] = [\n                dict(type=""embedding"", embed_dim=embed_dim, vocab_size=vocab_size),\n                dict(type=""reshape"", flatten=True),\n                dict(type=""dense"", units=embed_dim, activation=""relu"", scope=""dense_1"")\n            ]\n        agent = DQFDAgent.from_spec(\n            agent_config,\n            state_space=state_space,\n            action_space=actions_space\n        )\n        terminals = BoolBox(add_batch_rank=True)\n        rewards = FloatBox(add_batch_rank=True)\n\n        agent.observe_demos(\n            preprocessed_states=agent.preprocessed_state_space.with_batch_rank().sample(1),\n            actions=actions_space.with_batch_rank().sample(1),\n            rewards=rewards.sample(1),\n            next_states=agent.preprocessed_state_space.with_batch_rank().sample(1),\n            terminals=terminals.sample(1),\n        )\n\n    def test_insert_demos(self):\n        """"""\n        Tests inserting into the demo memory.\n        """"""\n        env = OpenAIGymEnv.from_spec(self.env_spec)\n\n        agent_config = config_from_path(""configs/dqfd_agent_for_cartpole.json"")\n        agent = DQFDAgent.from_spec(\n            agent_config,\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n        terminals = BoolBox(add_batch_rank=True)\n        rewards = FloatBox(add_batch_rank=True)\n\n        # Observe a single data point.\n        agent.observe_demos(\n            preprocessed_states=agent.preprocessed_state_space.with_batch_rank().sample(1),\n            actions=env.action_space.with_batch_rank().sample(1),\n            rewards=rewards.sample(1),\n            next_states=agent.preprocessed_state_space.with_batch_rank().sample(1),\n            terminals=terminals.sample(1),\n        )\n\n        # Observe a batch of demos.\n        agent.observe_demos(\n            preprocessed_states=agent.preprocessed_state_space.sample(10),\n            actions=env.action_space.sample(10),\n            rewards=FloatBox().sample(10),\n            terminals=terminals.sample(10),\n            next_states=agent.preprocessed_state_space.sample(10)\n        )\n\n    def test_update_from_demos(self):\n        """"""\n        Tests the separate API method to update from demos.\n        """"""\n        env = OpenAIGymEnv.from_spec(self.env_spec)\n        agent_config = config_from_path(""configs/dqfd_agent_for_cartpole.json"")\n        agent = DQFDAgent.from_spec(\n            agent_config,\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n        terminals = BoolBox(add_batch_rank=True)\n        rewards = FloatBox(add_batch_rank=True)\n        state_1 = agent.preprocessed_state_space.with_batch_rank().sample(1)\n        action_1 = [1]\n        state_2 = agent.preprocessed_state_space.with_batch_rank().sample(1)\n        action_2 = [0]\n\n        # Insert two states with fixed actions and a few random examples.\n        for _ in range(10):\n            # State with correct action\n            agent.observe_demos(\n                preprocessed_states=state_1,\n                actions=action_1,\n                rewards=rewards.sample(1),\n                next_states=agent.preprocessed_state_space.with_batch_rank().sample(1),\n                terminals=terminals.sample(1),\n            )\n            agent.observe_demos(\n                preprocessed_states=state_2,\n                actions=action_2,\n                rewards=rewards.sample(1),\n                next_states=agent.preprocessed_state_space.with_batch_rank().sample(1),\n                terminals=terminals.sample(1),\n            )\n\n        # Update.\n        agent.update_from_demos(batch_size=8, num_updates=1000)\n\n        # Test if fixed states and actions map.\n        action = agent.get_action(states=state_1, apply_preprocessing=False, use_exploration=False)\n        self.assertEqual(action, action_1)\n\n        action = agent.get_action(states=state_2, apply_preprocessing=False, use_exploration=False)\n        self.assertEqual(action, action_2)\n\n    def test_demos_with_container_actions(self):\n        # Tests if dqfd can fit a set of states to a set of actions.\n        vocab_size = 100\n        embed_dim = 128\n        # ID/state space.\n        state_space = IntBox(vocab_size, shape=(10,))\n        # Container action space.\n        actions_space = {}\n        num_outputs = 3\n        for i in range(3):\n            actions_space[\'action_{}\'.format(i)] = IntBox(\n                low=0,\n                high=num_outputs\n            )\n        actions_space = Dict(actions_space)\n\n        agent_config = config_from_path(""configs/dqfd_container.json"")\n        agent_config[""network_spec""] = [\n                dict(type=""embedding"", embed_dim=embed_dim, vocab_size=vocab_size),\n                dict(type=""reshape"", flatten=True),\n                dict(type=""dense"", units=embed_dim, activation=""relu"", scope=""dense_1"")\n            ]\n        agent = DQFDAgent.from_spec(\n            agent_config,\n            state_space=state_space,\n            action_space=actions_space\n        )\n        terminals = BoolBox(add_batch_rank=True)\n        rewards = FloatBox(add_batch_rank=True)\n\n        # Create a set of demos.\n        demo_states = agent.preprocessed_state_space.with_batch_rank().sample(20)\n        demo_actions = actions_space.with_batch_rank().sample(20)\n        demo_rewards = rewards.sample(20, fill_value=1.0)\n        demo_next_states = agent.preprocessed_state_space.with_batch_rank().sample(20)\n        demo_terminals = terminals.sample(20, fill_value=False)\n\n        # Insert.\n        agent.observe_demos(\n            preprocessed_states=demo_states,\n            actions=demo_actions,\n            rewards=demo_rewards,\n            next_states=demo_next_states,\n            terminals=demo_terminals,\n        )\n\n        # Fit demos.\n        agent.update_from_demos(batch_size=20, num_updates=5000)\n\n        # Evaluate demos:\n        agent_actions = agent.get_action(demo_states, apply_preprocessing=False, use_exploration=False)\n        recursive_assert_almost_equal(agent_actions, demo_actions)\n\n    def test_update_online(self):\n        """"""\n        Tests if joint updates from demo and online memory work.\n        """"""\n        env = OpenAIGymEnv.from_spec(self.env_spec)\n        agent_config = config_from_path(""configs/dqfd_agent_for_cartpole.json"")\n        agent = DQFDAgent.from_spec(\n            agent_config,\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n        terminals = BoolBox(add_batch_rank=True)\n\n        # Observe a batch of demos.\n        agent.observe_demos(\n            preprocessed_states=agent.preprocessed_state_space.sample(32),\n            actions=env.action_space.sample(32),\n            rewards=FloatBox().sample(32),\n            terminals=terminals.sample(32),\n            next_states=agent.preprocessed_state_space.sample(32)\n        )\n\n        # Observe a batch of online data.\n        agent._observe_graph(\n            preprocessed_states=agent.preprocessed_state_space.sample(32),\n            actions=env.action_space.sample(32),\n            rewards=FloatBox().sample(32),\n            internals=[],\n            terminals=terminals.sample(32),\n            next_states=agent.preprocessed_state_space.sample(32)\n        )\n        # Call update.\n        agent.update()\n\n    def test_custom_margin_demos_with_container_actions(self):\n        # Tests if using different margins per sample works.\n        # Same state, but different\n        vocab_size = 100\n        embed_dim = 8\n        # ID/state space.\n        state_space = IntBox(vocab_size, shape=(10,))\n        # Container action space.\n        actions_space = {}\n        num_outputs = 3\n        for i in range(3):\n            actions_space[\'action_{}\'.format(i)] = IntBox(\n                low=0,\n                high=num_outputs\n            )\n        actions_space = Dict(actions_space)\n\n        agent_config = config_from_path(""configs/dqfd_container.json"")\n        agent_config[""network_spec""] = [\n            dict(type=""embedding"", embed_dim=embed_dim, vocab_size=vocab_size),\n            dict(type=""reshape"", flatten=True),\n            dict(type=""dense"", units=embed_dim, activation=""relu"", scope=""dense_1"")\n        ]\n        agent = DQFDAgent.from_spec(\n            agent_config,\n            state_space=state_space,\n            action_space=actions_space\n        )\n        terminals = BoolBox(add_batch_rank=True)\n        rewards = FloatBox(add_batch_rank=True)\n\n        # Create a set of demos.\n        demo_states = agent.preprocessed_state_space.with_batch_rank().sample(2)\n        # Same state.\n        demo_states[1] = demo_states[0]\n        demo_actions = actions_space.with_batch_rank().sample(2)\n\n        for name, action in actions_space.items():\n            demo_actions[name][0] = 0\n            demo_actions[name][1] = 1\n\n        demo_rewards = rewards.sample(2, fill_value=.0)\n        # One action has positive reward, one negative\n        demo_rewards[0] = 0\n        demo_rewards[1] = 0\n\n        # One action is encouraged, one is discouraged.\n        margins = np.asarray([0.5, -0.5])\n\n        demo_next_states = agent.preprocessed_state_space.with_batch_rank().sample(2)\n        demo_terminals = terminals.sample(2, fill_value=False)\n\n        # When using margins, need to use external batch.\n        batch = dict(\n            states=demo_states,\n            actions=demo_actions,\n            rewards=demo_rewards,\n            next_states=demo_next_states,\n            importance_weights=np.ones_like(demo_rewards),\n            terminals=demo_terminals,\n        )\n        # Fit demos with custom margins.\n        for _ in range(10000):\n            agent.update(batch=batch, update_from_demos=False, apply_demo_loss_to_batch=True, expert_margins=margins)\n\n        # Evaluate demos for the state -> should have action with positive reward.\n        agent_actions = agent.get_action(np.array([demo_states[0]]), apply_preprocessing=False, use_exploration=False)\n        print(""learned action = "", agent_actions)\n'"
rlgraph/tests/agent_functionality/test_dqn_agent_functionality.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport unittest\n\nimport numpy as np\n\nimport rlgraph.spaces as spaces\nfrom rlgraph.agents import Agent\nfrom rlgraph.components.loss_functions.dqn_loss_function import DQNLossFunction\nfrom rlgraph.environments import GridWorld\nfrom rlgraph.execution.single_threaded_worker import SingleThreadedWorker\nfrom rlgraph.tests.agent_test import AgentTest\nfrom rlgraph.tests.test_util import config_from_path\nfrom rlgraph.utils import root_logger, one_hot\n\n\n#TODO this stepping needs to be redone, variables should not be accessed this way.\nclass TestDQNAgentFunctionality(unittest.TestCase):\n    """"""\n    Tests the DQN Agent\'s functionality.\n    """"""\n    root_logger.setLevel(level=logging.DEBUG)\n\n    def test_dqn_functionality(self):\n        """"""\n        Creates a DQNAgent and runs it for a few steps in a GridWorld to vigorously test\n        all steps of the learning process.\n        """"""\n        env = GridWorld(world=""2x2"", save_mode=True)  # no holes, just fire\n        agent = Agent.from_spec(  # type: DQNAgent\n            config_from_path(""configs/dqn_agent_for_functionality_test.json""),\n            double_q=True,\n            dueling_q=True,\n            state_space=env.state_space,\n            action_space=env.action_space,\n            discount=0.95\n        )\n        worker = SingleThreadedWorker(env_spec=lambda: GridWorld(world=""2x2"", save_mode=True), agent=agent)\n        test = AgentTest(worker=worker)\n\n        # Helper python DQNLossFunc object.\n        loss_func = DQNLossFunction(backend=""python"", double_q=True, discount=agent.discount)\n        loss_func.when_input_complete(input_spaces=dict(\n            loss_per_item=[\n                spaces.FloatBox(shape=(4,), add_batch_rank=True),\n                spaces.IntBox(4, add_batch_rank=True),\n                spaces.FloatBox(add_batch_rank=True),\n                spaces.BoolBox(add_batch_rank=True),\n                spaces.FloatBox(shape=(4,), add_batch_rank=True),\n                spaces.FloatBox(shape=(4,), add_batch_rank=True)\n            ]\n        ), action_space=env.action_space)\n\n        matrix1_qnet = np.array([[0.9] * 2] * 4)\n        matrix2_qnet = np.array([[0.8] * 5] * 2)\n        matrix1_target_net = np.array([[0.9] * 2] * 4)\n        matrix2_target_net = np.array([[0.8] * 5] * 2)\n\n        a = self._calculate_action(0, matrix1_qnet, matrix2_qnet)\n\n        # 1st step -> Expect insert into python-buffer.\n        # action: up (0)\n        test.step(1, reset=True)\n        # Environment\'s new state.\n        test.check_env(""state"", 0)\n        # Agent\'s buffer.\n        test.check_agent(""states_buffer"", [[1.0, 0.0, 0.0, 0.0]], key_or_index=""env_0"")  # <- prev state (preprocessed)\n        test.check_agent(""actions_buffer"", [a],  key_or_index=""env_0"")\n        test.check_agent(""rewards_buffer"", [-1.0], key_or_index=""env_0"")\n        test.check_agent(""terminals_buffer"", [False], key_or_index=""env_0"")\n        # Memory contents.\n        test.check_var(""replay-memory/index"", 0)\n        test.check_var(""replay-memory/size"", 0)\n        test.check_var(""replay-memory/memory/states"", np.array([[0] * 4] * agent.memory.capacity))\n        test.check_var(""replay-memory/memory/actions"", np.array([0] * agent.memory.capacity))\n        test.check_var(""replay-memory/memory/rewards"", np.array([0] * agent.memory.capacity))\n        test.check_var(""replay-memory/memory/terminals"", np.array([False] * agent.memory.capacity))\n        # Check policy and target-policy weights (should be the same).\n        test.check_var(""dueling-policy/neural-network/hidden/dense/kernel"", matrix1_qnet)\n        test.check_var(""target-policy/neural-network/hidden/dense/kernel"", matrix1_qnet)\n        test.check_var(""dueling-policy/action-adapter-0/action-network/action-layer/dense/kernel"", matrix2_qnet)\n        test.check_var(""target-policy/action-adapter-0/action-network/action-layer/dense/kernel"", matrix2_qnet)\n\n        # 2nd step -> expect insert into memory (and python buffer should be empty again).\n        # action: up (0)\n        # Also check the policy and target policy values (Should be equal at this point).\n        test.step(1)\n        test.check_env(""state"", 0)\n        test.check_agent(""states_buffer"", [], key_or_index=""env_0"")\n        test.check_agent(""actions_buffer"", [], key_or_index=""env_0"")\n        test.check_agent(""rewards_buffer"", [], key_or_index=""env_0"")\n        test.check_agent(""terminals_buffer"", [], key_or_index=""env_0"")\n        test.check_var(""replay-memory/index"", 2)\n        test.check_var(""replay-memory/size"", 2)\n        test.check_var(""replay-memory/memory/states"", np.array([[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0]] +\n                                                               [[0.0, 0.0, 0.0, 0.0]] * (agent.memory.capacity - 2)))\n        test.check_var(""replay-memory/memory/actions"", np.array([0, 0] + [0] * (agent.memory.capacity - 2)))\n        test.check_var(""replay-memory/memory/rewards"", np.array([-1.0, -1.0] + [0.0] * (agent.memory.capacity - 2)))\n        test.check_var(""replay-memory/memory/terminals"", np.array([False, True] + [False] * (agent.memory.capacity - 2)))\n        # Check policy and target-policy weights (should be the same).\n        test.check_var(""dueling-policy/neural-network/hidden/dense/kernel"", matrix1_qnet)\n        test.check_var(""target-policy/neural-network/hidden/dense/kernel"", matrix1_qnet)\n        test.check_var(""dueling-policy/action-adapter-0/action-network/action-layer/dense/kernel"", matrix2_qnet)\n        test.check_var(""target-policy/action-adapter-0/action-network/action-layer/dense/kernel"", matrix2_qnet)\n\n        # 3rd and 4th step -> expect another insert into memory (and python buffer should be empty again).\n        # actions: down (2), up (0)  <- exploring is True = more random actions\n        # Expect an update to the policy variables (leave target as is (no sync yet)).\n        test.step(2, use_exploration=True)\n        test.check_env(""state"", 0)\n        test.check_agent(""states_buffer"", [], key_or_index=""env_0"")\n        test.check_agent(""actions_buffer"", [], key_or_index=""env_0"")\n        test.check_agent(""rewards_buffer"", [], key_or_index=""env_0"")\n        test.check_agent(""terminals_buffer"", [], key_or_index=""env_0"")\n        test.check_var(""replay-memory/index"", 4)\n        test.check_var(""replay-memory/size"", 4)\n        test.check_var(""replay-memory/memory/states"", np.array([[1.0, 0.0, 0.0, 0.0]] * 3 +\n                                                               [[0.0, 1.0, 0.0, 0.0]] +\n                                                               [[0.0, 0.0, 0.0, 0.0]] * (agent.memory.capacity - 4)))\n        test.check_var(""replay-memory/memory/actions"", np.array([0, 0, 2, 0] + [0] * (agent.memory.capacity - 4)))\n        test.check_var(""replay-memory/memory/rewards"", np.array([-1.0] * 4 +  # + [-3.0] +\n                                                                [0.0] * (agent.memory.capacity - 4)))\n        test.check_var(""replay-memory/memory/terminals"", np.array([False, True] * 2 +\n                                                                  [False] * (agent.memory.capacity - 4)))\n        # Get the latest memory batch.\n        expected_batch = dict(\n            states=np.array([[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0]]),\n            actions=np.array([0, 1]),\n            rewards=np.array([-1.0, -3.0]),\n            terminals=np.array([False, True]),\n            next_states=np.array([[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]])\n        )\n        test.check_agent(""last_memory_batch"", expected_batch)\n\n        # Calculate the weight updates and check against actually update weights by the AgentDQN.\n        mat_updated = self._helper_update_matrix(expected_batch, matrix1_qnet, matrix2_qnet, matrix1_target_net,\n                                                 matrix2_target_net, agent, loss_func)\n        # Check policy and target-policy weights (policy should be updated now).\n        test.check_var(""dueling-policy/neural-network/hidden/dense/kernel"", mat_updated[0], decimals=4)\n        test.check_var(""target-policy/neural-network/hidden/dense/kernel"", matrix1_target_net)\n        test.check_var(""dueling-policy/action-adapter-0/action-network/action-layer/dense/kernel"", mat_updated[1], decimals=4)\n        test.check_var(""target-policy/action-adapter-0/action-network/action-layer/dense/kernel"", matrix2_target_net)\n\n        matrix1_qnet = mat_updated[0]\n        matrix2_qnet = mat_updated[1]\n\n        # 5th step -> Another buffer update check.\n        # action: down (2) (weights have been updated -> different actions)\n        test.step(1)\n        test.check_env(""state"", 3)\n        test.check_agent(""states_buffer"", [], key_or_index=""env_0"")  # <- all empty b/c we reached end of episode (buffer gets force-flushed)\n        test.check_agent(""actions_buffer"", [], key_or_index=""env_0"")\n        test.check_agent(""rewards_buffer"", [], key_or_index=""env_0"")\n        test.check_agent(""terminals_buffer"", [], key_or_index=""env_0"")\n        test.check_agent(""last_memory_batch"", expected_batch)\n        test.check_var(""replay-memory/index"", 5)\n        test.check_var(""replay-memory/size"", 5)\n        test.check_var(""replay-memory/memory/states"", np.array([[1.0, 0.0, 0.0, 0.0]] * 4 + [[0.0, 0.0, 1.0, 0.0]] +\n                                                               [[0.0, 0.0, 0.0, 0.0]] * (agent.memory.capacity - 5)))\n        test.check_var(""replay-memory/memory/actions"", np.array([0, 0, 0, 1, 2, 0]))\n        test.check_var(""replay-memory/memory/rewards"", np.array([-1.0] * 3 + [-3.0, 1.0, 0.0]))\n        test.check_var(""replay-memory/memory/terminals"", np.array([False, True] * 2 + [True, False]))\n        test.check_var(""dueling-policy/neural-network/hidden/dense/kernel"", matrix1_qnet, decimals=4)\n        test.check_var(""target-policy/neural-network/hidden/dense/kernel"", matrix1_target_net)\n        test.check_var(""dueling-policy/action-adapter-0/action-network/action-layer/dense/kernel"", mat_updated[1], decimals=4)\n        test.check_var(""target-policy/action-adapter-0/action-network/action-layer/dense/kernel"", matrix2_target_net)\n\n        # 6th/7th step (with exploration enabled) -> Another buffer update check.\n        # action: up, down (0, 2)\n        test.step(2, use_exploration=True)\n        test.check_env(""state"", 1)\n        test.check_agent(""states_buffer"", [], key_or_index=""env_0"")  # <- all empty again; flushed after 6th step (when buffer was full).\n        test.check_agent(""actions_buffer"", [], key_or_index=""env_0"")\n        test.check_agent(""rewards_buffer"", [], key_or_index=""env_0"")\n        test.check_agent(""terminals_buffer"", [], key_or_index=""env_0"")\n        test.check_agent(""last_memory_batch"", expected_batch)\n        test.check_var(""replay-memory/index"", 1)  # index has been rolled over (memory capacity is 6)\n        test.check_var(""replay-memory/size"", 6)\n        test.check_var(""replay-memory/memory/states"", np.array([[1.0, 0.0, 0.0, 0.0]] * 4 +\n                                                               [[0.0, 0.0, 1.0, 0.0]] +\n                                                               [[1.0, 0.0, 0.0, 0.0]]))\n        test.check_var(""replay-memory/memory/actions"", np.array([2, 0, 0, 1, 2, 0]))\n        test.check_var(""replay-memory/memory/rewards"", np.array([-1.0] * 3 + [-3.0, 1.0, -1.0]))\n        test.check_var(""replay-memory/memory/terminals"", np.array([True, True, False, True, True, False]))\n\n        test.check_var(""dueling-policy/neural-network/hidden/dense/kernel"", matrix1_qnet, decimals=4)\n        test.check_var(""target-policy/neural-network/hidden/dense/kernel"", matrix1_target_net)\n        test.check_var(""dueling-policy/dueling-action-adapter/action-layer/dense/kernel"", matrix2_qnet, decimals=4)\n        test.check_var(""target-policy/dueling-action-adapter/action-layer/dense/kernel"", matrix2_target_net)\n\n        # 8th step -> Another buffer update check and weights update and sync.\n        # action: down (2)\n        test.step(1)\n        test.check_env(""state"", 1)\n        test.check_agent(""states_buffer"", [1], key_or_index=""env_0"")\n        test.check_agent(""actions_buffer"", [2], key_or_index=""env_0"")\n        test.check_agent(""rewards_buffer"", [-1.0], key_or_index=""env_0"")\n        test.check_agent(""terminals_buffer"", [False], key_or_index=""env_0"")\n        expected_batch = dict(\n            states=np.array([[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0]]),\n            actions=np.array([0, 1]),\n            rewards=np.array([-1.0, -3.0]),\n            terminals=np.array([True, True]),\n            next_states=np.array([[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]])\n            # TODO: <- This is wrong and must be fixed\n            # (next-state of first item is from a previous insert and unrelated to first item)\n        )\n        test.check_agent(""last_memory_batch"", expected_batch)\n        test.check_var(""replay-memory/index"", 1)\n        test.check_var(""replay-memory/size"", 6)\n        test.check_var(""replay-memory/memory/states"", np.array([[1.0, 0.0, 0.0, 0.0]] * 4 +\n                                                               [[0.0, 0.0, 1.0, 0.0]] +\n                                                               [[1.0, 0.0, 0.0, 0.0]]))\n        test.check_var(""replay-memory/memory/actions"", np.array([2, 0, 0, 1, 2, 0]))\n        test.check_var(""replay-memory/memory/rewards"", np.array([-1.0, -1.0, -1.0, -3.0, 1.0, -1.0]))\n        test.check_var(""replay-memory/memory/terminals"", np.array([True, True, False, True, True, False]))\n\n        # Assume that the sync happens first (matrices are already the same when updating).\n        mat_updated = self._helper_update_matrix(expected_batch, matrix1_qnet, matrix2_qnet, matrix1_qnet,\n                                                 matrix2_qnet, agent, loss_func)\n\n        # Now target-net should be again 1 step behind policy-net.\n        test.check_var(""dueling-policy/neural-network/hidden/dense/kernel"", mat_updated[0], decimals=2)\n        test.check_var(""target-policy/neural-network/hidden/dense/kernel"", matrix1_qnet, decimals=2)  # again: old matrix\n        test.check_var(""dueling-policy/dueling-action-adapter/action-layer/dense/kernel"", mat_updated[1], decimals=2)\n        test.check_var(""target-policy/dueling-action-adapter/action-layer/dense/kernel"", matrix2_qnet, decimals=2)\n\n    def _calculate_action(self, state, matrix1, matrix2):\n        s = np.asarray([state])\n        s_flat = one_hot(s, depth=4)\n        q_values = self._helper_get_q_values(s_flat, matrix1, matrix2)\n        # Assume greedy.\n        return np.argmax(q_values)\n\n    @staticmethod\n    def _helper_get_q_values(input_, matrix1, matrix2):\n        """"""\n        Calculates the q-values for a given simple 1-hidden 1-action-layer (both linear w/o biases) setup.\n\n        Args:\n            input_ (np.ndarray): The input array (batch x in-nodes).\n            matrix1 (np.ndarray): The weights matrix of the hidden layer.\n            matrix2 (np.ndarray): The weights matrix of the action-layer.\n\n        Returns:\n            np.ndarray: The calculated q-values.\n        """"""\n        # Simple NN implementation.\n        nn_output = np.matmul(np.matmul(input_, matrix1), matrix2)\n        # Simple dueling layer implementation.\n        state_values = np.expand_dims(nn_output[:, 0], axis=-1)\n        q_values = state_values + nn_output[:, 1:] - np.mean(nn_output[:, 1:], axis=-1, keepdims=True)\n        return q_values\n\n    def _helper_update_matrix(self, expected_batch, matrix1_qnet, matrix2_qnet, matrix1_target_net, matrix2_target_net,\n                              agent, loss_func):\n        # Calculate gradient per weight based on the above batch.\n        q_s = self._helper_get_q_values(expected_batch[""states""], matrix1_qnet, matrix2_qnet)\n        q_sp = self._helper_get_q_values(expected_batch[""next_states""], matrix1_qnet, matrix2_qnet)\n        qt_sp = self._helper_get_q_values(expected_batch[""next_states""], matrix1_target_net, matrix2_target_net)\n\n        # The loss without weight changes.\n        loss = np.mean(loss_func._graph_fn_loss_per_item(\n            q_s, expected_batch[""actions""], expected_batch[""rewards""], expected_batch[""terminals""], qt_sp, q_sp\n        ))\n\n        # Calculate the dLoss/dw for all individual weights (w) and apply [- LR * dLoss/dw] to each weight.\n        # Then check again against the actual, now optimized weights.\n        mat_updated = list()\n        for i, mat in enumerate([matrix1_qnet, matrix2_qnet]):\n            mat_updated.append(mat.copy())\n            for index in np.ndindex(mat.shape):\n                mat_w_plus_d = mat.copy()\n                mat_w_plus_d[index] += 0.0001\n                if i == 0:\n                    q_s_plus_d = self._helper_get_q_values(expected_batch[""states""], mat_w_plus_d, matrix2_qnet)\n                    q_sp_plus_d = self._helper_get_q_values(expected_batch[""next_states""], mat_w_plus_d, matrix2_qnet)\n                else:\n                    q_s_plus_d = self._helper_get_q_values(expected_batch[""states""], matrix1_qnet, mat_w_plus_d)\n                    q_sp_plus_d = self._helper_get_q_values(expected_batch[""next_states""], matrix1_qnet, mat_w_plus_d)\n\n                loss_w_plus_d = np.mean(loss_func._graph_fn_loss_per_item(\n                    q_s_plus_d,\n                    expected_batch[""actions""], expected_batch[""rewards""], expected_batch[""terminals""],\n                    qt_sp, q_sp_plus_d\n                ))\n                dl_over_dw = (loss - loss_w_plus_d) / 0.0001\n\n                # Apply the changes to our matrices, then check their actual values.\n                mat_updated[i][index] += agent.optimizer.learning_rate * dl_over_dw\n\n        return mat_updated\n'"
rlgraph/tests/agent_functionality/test_impala_agent_functionality.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport time\nimport unittest\n\nimport numpy as np\n\nfrom rlgraph.agents.impala_agents import IMPALAAgent, SingleIMPALAAgent\nfrom rlgraph.components.common.environment_stepper import EnvironmentStepper\nfrom rlgraph.components.neural_networks.actor_component import ActorComponent\nfrom rlgraph.components.neural_networks.impala.impala_networks import LargeIMPALANetwork\nfrom rlgraph.components.policies.shared_value_function_policy import SharedValueFunctionPolicy\nfrom rlgraph.spaces import *\nfrom rlgraph.tests.component_test import ComponentTest\nfrom rlgraph.tests.test_util import recursive_assert_almost_equal, config_from_path\nfrom rlgraph.utils import root_logger, default_dict\nfrom rlgraph.utils.ops import DataOpTuple\n\n\nclass TestIMPALAAgentFunctionality(unittest.TestCase):\n    """"""\n    Tests the LargeIMPALANetwork functionality and IMPALAAgent assembly on the RandomEnv.\n    For details on the IMPALA algorithm, see [1]:\n\n    [1] IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures - Espeholt, Soyer,\n        Munos et al. - 2018 (https://arxiv.org/abs/1802.01561)\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    # Use the exact same Spaces as in the IMPALA paper.\n    action_space = IntBox(9, add_batch_rank=True, add_time_rank=True, time_major=True)\n    action_probs_space = FloatBox(shape=(9,), add_batch_rank=True)\n    input_space = Dict(\n        RGB_INTERLEAVED=FloatBox(shape=(96, 72, 3)),\n        INSTR=TextBox(),\n        previous_action=FloatBox(shape=(9,)),\n        previous_reward=FloatBox(shape=(1,)),  # add the extra rank for proper concatenating with the other inputs.\n        add_batch_rank=True,\n        add_time_rank=True,\n        time_major=False\n    )\n    parameters_and_logits_space = FloatBox(shape=(9,), add_batch_rank=True)\n    internal_states_space = Tuple(FloatBox(shape=(256,)), FloatBox(shape=(256,)), add_batch_rank=True)\n    cluster_spec = dict(learner=[""localhost:22222""], actor=[""localhost:22223""])\n    cluster_spec_single_actor = dict(actor=[""localhost:22223""])\n\n    def test_large_impala_network_without_agent(self):\n        """"""\n        Creates a large IMPALA architecture network and runs an input sample through it.\n        """"""\n        # Create the network (with a small time-step value for this test).\n        large_impala_architecture = LargeIMPALANetwork(worker_sample_size=2)\n        test = ComponentTest(\n            large_impala_architecture,\n            input_spaces=dict(input_dict=self.input_space),\n            execution_spec=dict(disable_monitoring=True)\n        )\n\n        # Send a 2x3 sample through the network (2=sequence-length (time-rank), 3=batch-size).\n        sample_input = self.input_space.sample(size=(2, 3))\n        expected = None\n        ret = test.test((""call"", sample_input), expected_outputs=expected)\n        # Check shape of outputs.\n        self.assertEquals(ret[""output""].shape, (2, 3, 256))\n        # Check shapes of current internal_states (c and h).\n        self.assertEquals(ret[""last_internal_states""][0].shape, (3, 256))\n        self.assertEquals(ret[""last_internal_states""][1].shape, (3, 256))\n\n        test.terminate()\n\n    def test_large_impala_policy_without_agent(self):\n        """"""\n        Creates a large IMPALA architecture network inside a policy and runs a few input samples through it.\n        """"""\n        # Create the network.\n        large_impala_architecture = LargeIMPALANetwork(worker_sample_size=1)\n        # IMPALA uses a baseline action adapter (v-trace off-policy PG with baseline value function).\n        policy = SharedValueFunctionPolicy(\n            network_spec=large_impala_architecture, action_space=self.action_space,\n            switched_off_apis={\n                #""get_action_from_logits_and_parameters"", ""get_action_from_logits_and_probabilities"",\n                ""get_log_likelihood""\n            }\n        )\n        test = ComponentTest(\n            policy,\n            input_spaces=dict(\n                nn_input=self.input_space,\n                internal_states=self.internal_states_space,\n                #parameters=self.parameters_and_logits_space,\n                #logits=self.parameters_and_logits_space\n            ),\n            action_space=self.action_space,\n            execution_spec=dict(disable_monitoring=True)\n        )\n\n        # Send a 1x1 sample through the network (1=sequence-length (time-rank), 1=batch-size).\n        nn_input = self.input_space.sample(size=(1, 1))\n        initial_internal_states = self.internal_states_space.zeros(size=1)\n        expected = None\n        out = test.test((""get_action"", [nn_input, initial_internal_states]), expected_outputs=expected)\n        print(""First action: {}"".format(out[""action""]))\n        self.assertEquals(out[""action""].shape, (1, 1))\n        self.assertEquals(out[""last_internal_states""][0].shape, (1, 256))\n        self.assertEquals(out[""last_internal_states""][1].shape, (1, 256))\n\n        # Send another 1x1 sample through the network using the previous internal-state.\n        next_nn_input = self.input_space.sample(size=(1, 1))\n        expected = None\n        out = test.test((""get_action"", [next_nn_input, out[""last_internal_states""]]), expected_outputs=expected)\n        print(""Second action: {}"".format(out[""action""]))\n        self.assertEquals(out[""action""].shape, (1, 1))\n        self.assertEquals(out[""last_internal_states""][0].shape, (1, 256))\n        self.assertEquals(out[""last_internal_states""][1].shape, (1, 256))\n\n        test.terminate()\n\n    def test_large_impala_actor_component_without_agent(self):\n        """"""\n        Creates a large IMPALA architecture network inside a policy inside an actor component and runs a few input\n        samples through it.\n        """"""\n        batch_size = 4\n        time_steps = 1\n\n        # IMPALA uses a baseline action adapter (v-trace off-policy PG with baseline value function).\n        policy = SharedValueFunctionPolicy(LargeIMPALANetwork(worker_sample_size=time_steps),\n                                           action_space=self.action_space, deterministic=False)\n        actor_component = ActorComponent(preprocessor_spec=None, policy_spec=policy, exploration_spec=None)\n\n        test = ComponentTest(\n            actor_component, input_spaces=dict(\n                states=self.input_space,\n                internal_states=self.internal_states_space\n            ),\n            action_space=self.action_space,\n            execution_spec=dict(disable_monitoring=True)\n        )\n\n        # Send a sample through the network (sequence-length (time-rank) x batch-size).\n        nn_dict_input = self.input_space.sample(size=(time_steps, batch_size))\n        initial_internal_states = self.internal_states_space.zeros(size=batch_size)\n        expected = None\n        out = test.test(\n            (""get_preprocessed_state_and_action"", [nn_dict_input, initial_internal_states]), expected_outputs=expected\n        )\n        print(""First action: {}"".format(out[""action""]))\n        self.assertEquals(out[""action""].shape, (time_steps, batch_size))\n        self.assertEquals(out[""last_internal_states""][0].shape, (batch_size, 256))\n        self.assertEquals(out[""last_internal_states""][1].shape, (batch_size, 256))\n        # Check preprocessed state (all the same except \'image\' channel).\n        recursive_assert_almost_equal(\n            out[""preprocessed_state""], dict(\n                RGB_INTERLEAVED=nn_dict_input[""RGB_INTERLEAVED""],\n                INSTR=nn_dict_input[""INSTR""],\n                previous_action=nn_dict_input[""previous_action""],\n                previous_reward=nn_dict_input[""previous_reward""],\n            )\n        )\n\n        # Send another 1x1 sample through the network using the previous internal-state.\n        next_nn_input = self.input_space.sample(size=(time_steps, batch_size))\n        expected = None\n        out = test.test(\n            (""get_preprocessed_state_and_action"", [next_nn_input, out[""last_internal_states""]]), expected_outputs=expected\n        )\n        print(""Second action: {}"".format(out[""action""]))\n        self.assertEquals(out[""action""].shape, (time_steps, batch_size))\n        self.assertEquals(out[""last_internal_states""][0].shape, (batch_size, 256))\n        self.assertEquals(out[""last_internal_states""][1].shape, (batch_size, 256))\n        # Check preprocessed state (all the same except \'image\' channel, which gets divided by 255).\n        recursive_assert_almost_equal(\n            out[""preprocessed_state""], dict(\n                RGB_INTERLEAVED=next_nn_input[""RGB_INTERLEAVED""],\n                INSTR=next_nn_input[""INSTR""],\n                previous_action=next_nn_input[""previous_action""],\n                previous_reward=next_nn_input[""previous_reward""],\n            )\n        )\n\n        test.terminate()\n\n    def test_environment_stepper_component_with_large_impala_architecture(self):\n        try:\n            from rlgraph.environments.deepmind_lab import DeepmindLabEnv\n        except ImportError:\n            print(""DeepmindLab not installed: Skipping this test case."")\n            return\n\n        worker_sample_size = 100\n        env_spec = dict(\n            type=""deepmind_lab"", level_id=""seekavoid_arena_01"", observations=[""RGB_INTERLEAVED"", ""INSTR""],\n            frameskip=4\n        )\n        dummy_env = DeepmindLabEnv.from_spec(env_spec)\n        state_space = dummy_env.state_space\n        action_space = dummy_env.action_space\n        actor_component = ActorComponent(\n            # Preprocessor spec (only for image and prev-action channel).\n            dict(\n                type=""dict-preprocessor-stack"",\n                preprocessors=dict(\n                    # The prev. action/reward from the env must be flattened/bumped-up-to-(1,).\n                    previous_action=[dict(type=""reshape"", flatten=True, flatten_categories=action_space.num_categories)],\n                    previous_reward=[dict(type=""reshape"", new_shape=(1,)), dict(type=""convert_type"", to_dtype=""float32"")],\n                )\n            ),\n            # Policy spec. worker_sample_size=1 as its an actor network.\n            dict(network_spec=LargeIMPALANetwork(worker_sample_size=1), action_space=action_space)\n        )\n        environment_stepper = EnvironmentStepper(\n            environment_spec=env_spec,\n            actor_component_spec=actor_component,\n            state_space=state_space,\n            reward_space=""float32"",\n            internal_states_space=self.internal_states_space,\n            num_steps=worker_sample_size,\n            # Add both prev-action and -reward into the state sent through the network.\n            add_previous_action_to_state=True,\n            add_previous_reward_to_state=True,\n            add_action_probs=True,\n            action_probs_space=self.action_probs_space\n        )\n\n        test = ComponentTest(\n            component=environment_stepper,\n            action_space=action_space,\n            execution_spec=dict(disable_monitoring=True)\n        )\n\n        environment_stepper.environment_server.start_server()\n\n        # Step n times through the Env and collect results.\n        # 1st return value is the step-op (None), 2nd return value is the tuple of items (3 steps each), with each\n        # step containing: Preprocessed state, actions, rewards, episode returns, terminals, (raw) next-states.\n        time_start = time.perf_counter()\n        steps = 10\n        for _ in range(steps):\n            out = test.test(""step"")\n        time_total = time.perf_counter() - time_start\n        print(""Done running {}x{} steps in Deepmind Lab env using IMPALA network in {}sec ({} actions/sec)."".format(\n            steps, environment_stepper.num_steps, time_total , environment_stepper.num_steps * steps / time_total)\n        )\n\n        # Check types of outputs.\n        self.assertTrue(isinstance(out, DataOpTuple))  # the step results as a tuple (see below)\n\n        # Check types of single data.\n        self.assertTrue(out[0].dtype == np.bool_)  # next-state is terminal?\n        self.assertTrue(out[1][""INSTR""].dtype == np.object)\n        self.assertTrue(out[1][""RGB_INTERLEAVED""].dtype == np.uint8)\n        self.assertTrue(out[1][""RGB_INTERLEAVED""].shape == (worker_sample_size + 1,) + state_space[""RGB_INTERLEAVED""].shape)\n        self.assertTrue(out[1][""RGB_INTERLEAVED""].min() >= 0)  # make sure we have pixels\n        self.assertTrue(out[1][""RGB_INTERLEAVED""].max() <= 255)\n        self.assertTrue(out[1][""previous_action""].dtype == np.int32)  # actions\n        self.assertTrue(out[1][""previous_action""].shape == (worker_sample_size + 1,))\n        self.assertTrue(out[1][""previous_reward""].dtype == np.float32)  # rewards\n        self.assertTrue(out[1][""previous_reward""].shape == (worker_sample_size + 1,))\n        # action probs (test whether sum to one).\n        self.assertTrue(out[2].dtype == np.float32)\n        self.assertTrue(out[2].shape == (100, action_space.num_categories))\n        self.assertTrue(out[2].min() >= 0.0)\n        self.assertTrue(out[2].max() <= 1.0)\n        recursive_assert_almost_equal(out[2].sum(axis=-1, keepdims=False),\n                                      np.ones(shape=(worker_sample_size,)), decimals=4)\n        # internal states (c- and h-state)\n        self.assertTrue(out[3][0].dtype == np.float32)\n        self.assertTrue(out[3][0].shape == (worker_sample_size + 1, 256))\n        self.assertTrue(out[3][1].dtype == np.float32)\n        self.assertTrue(out[3][1].shape == (worker_sample_size + 1, 256))\n\n        environment_stepper.environment_server.stop_server()\n\n        test.terminate()\n\n    def test_single_impala_agent_functionality(self):\n        """"""\n        Creates a single IMPALAAgent and runs it for a few steps in a DeepMindLab Env to test\n        all steps of the actor and learning process.\n        """"""\n        try:\n            from rlgraph.environments.deepmind_lab import DeepmindLabEnv\n        except ImportError:\n            print(""Deepmind Lab not installed: Will skip this test."")\n            return\n\n        agent_config = config_from_path(""configs/impala_agent_for_deepmind_lab_env.json"")\n        env_spec = dict(level_id=""lt_hallway_slope"", observations=[""RGB_INTERLEAVED"", ""INSTR""], frameskip=4)\n        dummy_env = DeepmindLabEnv.from_spec(env_spec)\n\n        agent = SingleIMPALAAgent.from_spec(\n            default_dict(dict(type=""single-impala-agent""), agent_config),\n            architecture=""large"",\n            environment_spec=default_dict(dict(type=""deepmind-lab""), env_spec),\n            state_space=dummy_env.state_space,\n            action_space=dummy_env.action_space,\n            # TODO: automate this (by lookup from NN).\n            internal_states_space=IMPALAAgent.default_internal_states_space,\n            # Summarize time-steps to have an overview of the env-stepping speed.\n            summary_spec=dict(summary_regexp=""time-step"", directory=""/home/rlgraph/""),\n            dynamic_batching=False,\n            num_workers=4\n        )\n        # Count items in the queue.\n        print(""Items in queue: {}"".format(agent.call_api_method(""get_queue_size"")))\n\n        updates = 5\n        update_times = list()\n        print(""Updating from queue ..."")\n        for _ in range(updates):\n            start_time = time.monotonic()\n            agent.update()\n            update_times.append(time.monotonic() - start_time)\n\n        print(""Updates per second (including waiting for enqueued items): {}/s"".format(updates / np.sum(update_times)))\n\n        time.sleep(5)\n\n        agent.terminate()\n\n    def test_isolated_impala_actor_agent_functionality(self):\n        """"""\n        Creates a non-distributed IMPALAAgent (actor) and runs it for a few steps in a DeepMindLab Env to test\n        all steps of the learning process.\n        """"""\n        try:\n            from rlgraph.environments.deepmind_lab import DeepmindLabEnv\n        except ImportError:\n            print(""Deepmind Lab not installed: Will skip this test."")\n            return\n\n        agent_config = config_from_path(""configs/impala_agent_for_deepmind_lab_env.json"")\n        env_spec = dict(level_id=""seekavoid_arena_01"", observations=[""RGB_INTERLEAVED"", ""INSTR""], frameskip=4)\n        dummy_env = DeepmindLabEnv.from_spec(env_spec)\n\n        agent = IMPALAAgent.from_spec(\n            agent_config,\n            type=""actor"",\n            architecture=""large"",\n            environment_spec=default_dict(dict(type=""deepmind-lab""), env_spec),\n            state_space=dummy_env.state_space,\n            action_space=dummy_env.action_space,\n            # TODO: automate this (by lookup from NN).\n            internal_states_space=IMPALAAgent.default_internal_states_space,\n            execution_spec=dict(\n                #mode=""distributed"",\n                #distributed_spec=dict(job=""actor"", task_index=0, cluster_spec=self.cluster_spec_single_actor),\n                disable_monitoring=True\n            ),\n            # Need large queue to be able to fill it up (don\'t have a learner).\n            fifo_queue_spec=dict(capacity=10000)\n        )\n        # Start Specifiable Server with Env manually (monitoring is disabled).\n        agent.environment_stepper.environment_server.start_server()\n        time_start = time.perf_counter()\n        steps = 5\n        for _ in range(steps):\n            agent.call_api_method(""perform_n_steps_and_insert_into_fifo"")\n        time_total = time.perf_counter() - time_start\n        print(""Done running {}x{} steps in Deepmind Lab env using IMPALA network in {}sec ({} actions/sec)."".format(\n            steps, agent.worker_sample_size, time_total , agent.worker_sample_size * steps / time_total)\n        )\n        agent.environment_stepper.environment_server.stop_server()\n        agent.terminate()\n\n    #def test_distributed_impala_agent_functionality_actor_part(self):\n    #    """"""\n    #    Creates an IMPALAAgent (actor) and starts it without the learner piece.\n    #    Distributed actor agents are able to run autonomously as they don\'t require the learner to be present\n    #    and connected to the server.\n    #    """"""\n    #    try:\n    #        from rlgraph.environments.deepmind_lab import DeepmindLabEnv\n    #    except ImportError:\n    #        print(""Deepmind Lab not installed: Will skip this test."")\n    #        return\n\n    #    agent_config = config_from_path(""configs/impala_agent_for_deepmind_lab_env.json"")\n    #    env_spec = dict(level_id=""seekavoid_arena_01"", observations=[""RGB_INTERLEAVED"", ""INSTR""], frameskip=4)\n    #    dummy_env = DeepmindLabEnv.from_spec(env_spec)\n    #    agent = IMPALAAgent.from_spec(\n    #        agent_config,\n    #        type=""actor"",\n    #        architecture=""large"",\n    #        environment_spec=default_dict(dict(type=""deepmind-lab""), env_spec),\n    #        state_space=dummy_env.state_space,\n    #        action_space=dummy_env.action_space,\n    #        # TODO: automate this (by lookup from NN).\n    #        internal_states_space=IMPALAAgent.default_internal_states_space,\n    #        # Setup distributed tf.\n    #        execution_spec=dict(\n    #            mode=""distributed"",\n    #            distributed_spec=dict(job=""actor"", task_index=0, cluster_spec=self.cluster_spec),\n    #            session_config=dict(\n    #                type=""monitored-training-session"",\n    #                #log_device_placement=True\n    #            ),\n    #            #enable_profiler=True,\n    #            #profiler_frequency=1\n    #        ),\n    #        fifo_queue_spec=dict(capacity=10000)\n    #    )\n    #    time_start = time.perf_counter()\n    #    steps = 50\n    #    for _ in range(steps):\n    #        agent.call_api_method(""perform_n_steps_and_insert_into_fifo"")\n    #    time_total = time.perf_counter() - time_start\n    #    print(""Done running {}x{} steps in Deepmind Lab env using IMPALAAgent in {}sec ({} actions/sec)."".format(\n    #        steps, agent.worker_sample_size, time_total, agent.worker_sample_size * steps / time_total)\n    #    )\n    #    agent.terminate()\n\n    #def test_isolated_impala_learner_agent_functionality(self):\n    #    """"""\n    #    Creates a IMPALAAgent (learner), inserts some dummy records and ""learns"" from them.\n    #    """"""\n    #    agent_config = config_from_path(""configs/impala_agent_for_deepmind_lab_env.json"")\n    #    environment_spec = dict(\n    #        type=""deepmind-lab"", level_id=""lt_hallway_slope"", observations=[""RGB_INTERLEAVED"", ""INSTR""], frameskip=4\n    #    )\n    #    env = DeepmindLabEnv.from_spec(environment_spec)\n\n    #    agent = IMPALAAgent.from_spec(\n    #        agent_config,\n    #        type=""learner"",\n    #        architecture=""small"",\n    #        environment_spec=environment_spec,\n    #        state_space=env.state_space,\n    #        action_space=env.action_space,\n    #        # TODO: automate this (by lookup from NN).\n    #        internal_states_space=IMPALAAgent.standard_internal_states_space,\n    #    )\n    #    agent.call_api_method(""insert_dummy_records"")\n    #    agent.call_api_method(""update_from_memory"")\n'"
rlgraph/tests/agent_functionality/test_ppo_agent_functionality.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport logging\nimport unittest\n\nfrom rlgraph.agents import PPOAgent\nfrom rlgraph.environments import OpenAIGymEnv, RandomEnv\nfrom rlgraph.execution.single_threaded_worker import SingleThreadedWorker\nfrom rlgraph.spaces import Dict, FloatBox, BoolBox\nfrom rlgraph.tests.test_util import config_from_path\nfrom rlgraph.utils import root_logger\n\n\nclass TestPPOAgentFunctionality(unittest.TestCase):\n    """"""\n    Tests the PPO Agent\'s functionality.\n    """"""\n    root_logger.setLevel(level=logging.DEBUG)\n\n    def test_post_processing(self):\n        """"""\n        Tests external batch post-processing for the PPO agent.\n        """"""\n        env = OpenAIGymEnv(""Pong-v0"", frameskip=4, max_num_noops=30, episodic_life=True)\n        agent_config = config_from_path(""configs/ppo_agent_for_pong.json"")\n        agent = PPOAgent.from_spec(\n            agent_config,\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n        num_samples = 200\n        states = agent.preprocessed_state_space.sample(num_samples)\n        reward_space = FloatBox(add_batch_rank=True)\n        terminal_space = BoolBox(add_batch_rank=True)\n        sequence_indices_space = BoolBox(add_batch_rank=True)\n\n        # GAE is separately tested, just testing if this API method returns results.\n        pg_advantages = agent.post_process(dict(\n            states=states,\n            rewards=reward_space.sample(num_samples),\n            terminals=terminal_space.sample(num_samples, fill_value=0),\n            sequence_indices=sequence_indices_space.sample(num_samples, fill_value=0)\n        ))\n\n    def test_ppo_on_container_state_and_action_spaces_and_very_large_rewards(self):\n        """"""\n        Tests stability of PPO on an extreme env producing strange container states and large rewards and requiring\n        container actions.\n        """"""\n        env = RandomEnv(\n            state_space=Dict({""F_position"": FloatBox(shape=(2,), low=0.01, high=0.02)}),\n            action_space=Dict({""F_direction_low-1.0_high1.0"": FloatBox(shape=(), low=-1.0, high=1.0),\n                               ""F_forward_direction_low-1.0_high1.0"": FloatBox(shape=(), low=-1.0, high=1.0),\n                               ""B_jump"": BoolBox()\n                               }),\n            reward_space=FloatBox(low=-1000.0, high=-100000.0),  # hugely negative rewards\n            terminal_prob=0.0000001\n        )\n\n        agent_config = config_from_path(""configs/ppo_agent_for_random_env_with_container_spaces.json"")\n        agent = PPOAgent.from_spec(\n            agent_config,\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n\n        worker = SingleThreadedWorker(\n            env_spec=lambda: env,\n            agent=agent,\n            preprocessing_spec=None,\n            worker_executes_preprocessing=True,\n            #episode_finish_callback=lambda episode_return, duration, timesteps, env_num:\n            #print(""episode return {}; steps={}"".format(episode_return, timesteps))\n        )\n        results = worker.execute_timesteps(num_timesteps=int(1e6), use_exploration=True)\n\n        print(results)\n\n'"
rlgraph/tests/agent_functionality/test_sac_agent_functionality.py,0,"b'import logging\nimport unittest\n\nimport numpy as np\n\nfrom rlgraph.agents.sac_agent import SACAgentComponent, SyncSpecification, SACAgent\nfrom rlgraph.components import Policy, PreprocessorStack, ReplayMemory, AdamOptimizer, Synchronizable, \\\n    SACValueNetwork\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.spaces import FloatBox, BoolBox\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.tests.test_util import config_from_path, recursive_assert_almost_equal\nfrom rlgraph.utils import root_logger\n\n\nclass TestSACAgentFunctionality(unittest.TestCase):\n    """"""\n    Tests the SAC Agent\'s functionality.\n    """"""\n    root_logger.setLevel(level=logging.DEBUG)\n\n    def test_sac_agent_component_functionality(self):\n        config = config_from_path(""configs/sac_component_for_fake_env_test.json"")\n\n        # Arbitrary state space, state should not be used in this example.\n        state_space = FloatBox(shape=(8,))\n        continuous_action_space = FloatBox(shape=(1,), low=-2.0, high=2.0)\n        terminal_space = BoolBox(add_batch_rank=True)\n        rewards_space = FloatBox(add_batch_rank=True)\n        policy = Policy.from_spec(config[""policy""], action_space=continuous_action_space)\n        policy.add_components(Synchronizable(), expose_apis=""sync"")\n        q_function = SACValueNetwork.from_spec(config[""value_function""])\n\n        class DummyAgent(object):\n            def __init__(self):\n                self.graph_executor = None\n\n        dummy_agent = DummyAgent()\n        agent_component = SACAgentComponent(\n            agent=dummy_agent,\n            policy=policy,\n            q_function=q_function,\n            preprocessor=PreprocessorStack.from_spec([]),\n            memory=ReplayMemory.from_spec(config[""memory""]),\n            discount=config[""discount""],\n            initial_alpha=config[""initial_alpha""],\n            target_entropy=None,\n            optimizer=AdamOptimizer.from_spec(config[""optimizer""]),\n            vf_optimizer=AdamOptimizer.from_spec(config[""value_function_optimizer""], scope=""vf-optimizer""),\n            alpha_optimizer=None,\n            q_sync_spec=SyncSpecification(sync_interval=10, sync_tau=1.0),\n            num_q_functions=2\n        )\n\n        test = ComponentTest(\n            component=agent_component,\n            input_spaces=dict(\n                increment=int,\n                episode_reward=float,\n                states=state_space.with_batch_rank(),\n                preprocessed_states=state_space.with_batch_rank(),\n                env_actions=continuous_action_space.with_batch_rank(),\n                actions=continuous_action_space.with_batch_rank(),\n                rewards=rewards_space,\n                next_states=state_space.with_batch_rank(),\n                terminals=terminal_space,\n                batch_size=int,\n                importance_weights=FloatBox(add_batch_rank=True),\n                deterministic=bool,\n                weights=""variables:{}"".format(policy.scope),\n                time_percentage=float\n                # TODO: how to provide the space for multiple component variables?\n                #q_weights=Dict(\n                #    q_0=""variables:{}"".format(q_function.scope),\n                #    q_1=""variables:{}"".format(agent_component._q_functions[1].scope),\n                #)\n            ),\n            action_space=continuous_action_space,\n            build_kwargs=dict(\n                optimizer=agent_component._optimizer,\n                build_options=dict(\n                    vf_optimizer=agent_component.vf_optimizer,\n                ),\n            ),\n            auto_build=False\n        )\n        dummy_agent.graph_executor = test.graph_executor\n        test.build()\n\n        batch_size = 10\n        action_sample = continuous_action_space.with_batch_rank().sample(batch_size)\n        rewards = rewards_space.sample(batch_size)\n        # Check, whether an update runs ok.\n        result = test.test((""update_from_external_batch"", [\n            state_space.sample(batch_size),\n            action_sample,\n            rewards,\n            [True] * batch_size,\n            state_space.sample(batch_size),\n            [1.0] * batch_size  # importance\n        ]))\n        self.assertTrue(result[""actor_loss""].dtype == np.float32)\n        self.assertTrue(result[""critic_loss""].dtype == np.float32)\n\n        action_sample = np.linspace(-1, 1, batch_size).reshape((batch_size, 1))\n        q_values = test.test((""get_q_values"", [state_space.sample(batch_size), action_sample]))\n        for q_val in q_values:\n            self.assertTrue(q_val.dtype == np.float32)\n            self.assertTrue(q_val.shape == (batch_size, 1))\n\n        action_sample, _ = test.test((""action_from_preprocessed_state"", [state_space.sample(batch_size), False]))\n        self.assertTrue(action_sample.dtype == np.float32)\n        self.assertTrue(action_sample.shape == (batch_size, 1))\n\n    def test_policy_sync(self):\n        """"""\n        Tests weight syncing of policy (and only policy, not Q-functions).\n        """"""\n        env = OpenAIGymEnv(""CartPole-v0"")\n        agent = SACAgent.from_spec(\n            config_from_path(""configs/sac_agent_for_cartpole.json""),\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n\n        weights = agent.get_weights()\n        print(""weights ="", weights.keys())\n\n        new_weights = {}\n        for key, value in weights[""policy_weights""].items():\n            new_weights[key] = value + 0.01\n\n        agent.set_weights(policy_weights=new_weights, value_function_weights=None)\n\n        updated_weights = agent.get_weights()[""policy_weights""]\n        recursive_assert_almost_equal(updated_weights, new_weights)\n\n    def test_image_value_functions(self):\n        """"""\n        Tests if actions and states are successfully merged on image inputs to compute Q(s,a).\n        """"""\n        env = OpenAIGymEnv(""Pong-v0"", frameskip=4, max_num_noops=30, episodic_life=True)\n        agent = SACAgent.from_spec(\n            config_from_path(""configs/sac_agent_for_pong.json""),\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n\n        # Test updating from image batch.\n        batch = dict(\n            states=agent.preprocessed_state_space.sample(32),\n            actions=env.action_space.sample(32),\n            rewards=np.ones((32,)),\n            terminals=np.zeros((32,)),\n            next_states=agent.preprocessed_state_space.sample(32),\n        )\n        print(agent.update(batch))\n\n    def test_apex_integration(self):\n        from rlgraph.execution.ray import ApexExecutor\n        env_spec = dict(\n            type=""openai"",\n            gym_env=""PongNoFrameskip-v4"",\n            # The frameskip in the agent config will trigger worker skips, this\n            # is used for internal env.\n            frameskip=4,\n            max_num_noops=30,\n            episodic_life=False,\n            fire_reset=True\n        )\n\n        # Not a learning config, just testing integration.\n        executor = ApexExecutor(\n            environment_spec=env_spec,\n            agent_config=config_from_path(""configs/ray_sac_pong_test.json""),\n        )\n\n        # Tests short execution.\n        result = executor.execute_workload(workload=dict(\n            num_timesteps=5000, report_interval=100, report_interval_min_seconds=1)\n        )\n        print(result)\n'"
rlgraph/tests/agent_learning/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n'"
rlgraph/tests/components/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n'"
rlgraph/tests/components/test_action_adapters.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport unittest\n\nimport numpy as np\nfrom rlgraph.components.action_adapters import BernoulliDistributionAdapter, CategoricalDistributionAdapter\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.utils.numpy import softmax, sigmoid, relu\n\n\nclass TestActionAdapters(unittest.TestCase):\n    """"""\n    Tests for the different ActionAdapter setups.\n    """"""\n    def test_bernoulli_action_adapter(self):\n        # Last NN layer.\n        previous_nn_layer_space = FloatBox(shape=(16,), add_batch_rank=True)\n        adapter_outputs_space = FloatBox(shape=(2,), add_batch_rank=True)\n        # Action Space.\n        action_space = BoolBox(shape=(2,))\n\n        action_adapter = BernoulliDistributionAdapter(action_space=action_space, activation=""relu"")\n        test = ComponentTest(\n            component=action_adapter, input_spaces=dict(\n                inputs=previous_nn_layer_space,\n                adapter_outputs=adapter_outputs_space,\n            ), action_space=action_space\n        )\n        action_adapter_params = test.read_variable_values(action_adapter.variable_registry)\n\n        # Batch of n samples.\n        inputs = previous_nn_layer_space.sample(32)\n\n        expected_logits = relu(np.matmul(\n            inputs, action_adapter_params[""action-adapter/action-network/action-layer/dense/kernel""]\n        ))\n        test.test((""call"", inputs), expected_outputs=expected_logits, decimals=5)\n\n        expected_probs = sigmoid(expected_logits)\n        expected_log_probs = np.log(expected_probs)\n        test.test((""get_parameters"", inputs), expected_outputs=dict(\n            adapter_outputs=expected_logits, parameters=expected_probs, probabilities=expected_probs,\n            log_probs=expected_log_probs\n        ), decimals=5)\n\n    def test_simple_action_adapter(self):\n        # Last NN layer.\n        previous_nn_layer_space = FloatBox(shape=(16,), add_batch_rank=True)\n        adapter_outputs_space = FloatBox(shape=(3, 2, 2), add_batch_rank=True)\n        # Action Space.\n        action_space = IntBox(2, shape=(3, 2))\n\n        action_adapter = CategoricalDistributionAdapter(action_space=action_space, weights_spec=1.0, biases_spec=False,\n                                                        activation=""relu"")\n        test = ComponentTest(\n            component=action_adapter, input_spaces=dict(\n                inputs=previous_nn_layer_space,\n                adapter_outputs=adapter_outputs_space,\n            ), action_space=action_space\n        )\n        action_adapter_params = test.read_variable_values(action_adapter.variable_registry)\n\n        # Batch of 2 samples.\n        inputs = previous_nn_layer_space.sample(2)\n\n        expected_action_layer_output = np.matmul(\n            inputs, action_adapter_params[""action-adapter/action-network/action-layer/dense/kernel""]\n        )\n        expected_logits = np.reshape(expected_action_layer_output, newshape=(2, 3, 2, 2))\n        test.test((""call"", inputs), expected_outputs=expected_logits, decimals=5)\n        #test.test((""get_logits"", inputs), expected_outputs=expected_logits, decimals=5)  # w/o the dict\n\n        expected_probs = softmax(expected_logits)\n        expected_log_probs = np.log(expected_probs)\n        test.test((""get_parameters"", inputs), expected_outputs=dict(\n            adapter_outputs=expected_logits, parameters=expected_logits, probabilities=expected_probs,\n            log_probs=expected_log_probs\n        ), decimals=5)\n\n    def test_simple_action_adapter_with_batch_apply(self):\n        # Last NN layer.\n        previous_nn_layer_space = FloatBox(shape=(16,), add_batch_rank=True, add_time_rank=True, time_major=True)\n        adapter_outputs_space = FloatBox(shape=(3, 2, 2), add_batch_rank=True)\n        # Action Space.\n        action_space = IntBox(2, shape=(3, 2))\n\n        action_adapter = CategoricalDistributionAdapter(\n            action_space=action_space, weights_spec=1.0, biases_spec=False, fold_time_rank=True, unfold_time_rank=True,\n            activation=""relu""\n        )\n        test = ComponentTest(\n            component=action_adapter, input_spaces=dict(\n                inputs=previous_nn_layer_space,\n                adapter_outputs=adapter_outputs_space\n            ), action_space=action_space\n        )\n        action_adapter_params = test.read_variable_values(action_adapter.variable_registry)\n\n        # Batch of (4, 5).\n        inputs = previous_nn_layer_space.sample(size=(4, 5))\n        inputs_folded = np.reshape(inputs, newshape=(20, -1))\n\n        expected_action_layer_output = np.matmul(\n            inputs_folded, action_adapter_params[""action-adapter/action-network/action-layer/dense/kernel""]\n        )\n        expected_logits = np.reshape(expected_action_layer_output, newshape=(4, 5, 3, 2, 2))\n\n        test.test((""call"", inputs), expected_outputs=expected_logits, decimals=4)\n\n        expected_probs = softmax(expected_logits)\n        expected_log_probs = np.log(expected_probs)\n        test.test((""get_parameters"", inputs), expected_outputs=dict(\n            adapter_outputs=expected_logits, parameters=expected_logits, probabilities=expected_probs,\n            log_probs=expected_log_probs\n        ), decimals=4)\n\n    def test_action_adapter_with_complex_lstm_output(self):\n        # Last NN layer (LSTM with time rank).\n        previous_nn_layer_space = FloatBox(shape=(4,), add_batch_rank=True, add_time_rank=True, time_major=True)\n        adapter_outputs_space = FloatBox(shape=(3, 2, 2), add_batch_rank=True)\n        # Action Space.\n        action_space = IntBox(2, shape=(3, 2))\n\n        action_adapter = CategoricalDistributionAdapter(action_space=action_space, biases_spec=False)\n        test = ComponentTest(\n            component=action_adapter, input_spaces=dict(\n                inputs=previous_nn_layer_space,\n                adapter_outputs=adapter_outputs_space\n            ), action_space=action_space\n        )\n        action_adapter_params = test.read_variable_values(action_adapter.variable_registry)\n\n        # Batch of 2 samples, 3 timesteps.\n        inputs = previous_nn_layer_space.sample(size=(3, 2))\n        # Fold time rank before the action layer pass through.\n        inputs_reshaped = np.reshape(inputs, newshape=(6, -1))\n        # Action layer pass through and unfolding of time rank.\n        expected_action_layer_output = np.matmul(\n            inputs_reshaped, action_adapter_params[""action-adapter/action-network/action-layer/dense/kernel""]\n        ).reshape((3, 2, -1))\n        # Logits (already well reshaped (same as action space)).\n        expected_logits = np.reshape(expected_action_layer_output, newshape=(3, 2, 3, 2, 2))\n        test.test((""call"", inputs), expected_outputs=expected_logits)\n        #test.test((""get_logits"", inputs), expected_outputs=expected_logits)\n\n        # Softmax (probs).\n        expected_probs = softmax(expected_logits)\n        # Log probs.\n        expected_log_probs = np.log(expected_probs)\n        test.test((""get_parameters"", inputs), expected_outputs=dict(\n            adapter_outputs=expected_logits, parameters=expected_logits, probabilities=expected_probs,\n            log_probs=expected_log_probs\n        ), decimals=5)\n\n'"
rlgraph/tests/components/test_actor_components.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\nfrom rlgraph.components.explorations.exploration import Exploration\nfrom rlgraph.components.neural_networks.actor_component import ActorComponent\nfrom rlgraph.components.neural_networks.preprocessor_stack import PreprocessorStack\nfrom rlgraph.components.policies.policy import Policy\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.tests.dummy_components_with_sub_components import DummyNNWithDictInput\nfrom rlgraph.tests.test_util import config_from_path, recursive_assert_almost_equal\nfrom rlgraph.utils.numpy import softmax\n\n\nclass TestActorComponents(unittest.TestCase):\n\n    def test_simple_actor_component(self):\n        # state_space (NN is a simple single fc-layer relu network (2 units), random biases, random weights).\n        state_space = FloatBox(shape=(5,), add_batch_rank=True)\n        # action_space.\n        action_space = IntBox(10)\n\n        preprocessor = PreprocessorStack.from_spec(\n            [dict(type=""convert_type"", to_dtype=""float""), dict(type=""multiply"", factor=2)]\n        )\n        policy = Policy(network_spec=config_from_path(""configs/test_simple_nn.json""), action_space=action_space)\n        exploration = Exploration()  # no exploration\n        actor_component = ActorComponent(preprocessor, policy, exploration)\n        test = ComponentTest(\n            component=actor_component,\n            input_spaces=dict(states=state_space),\n            action_space=action_space\n        )\n        # Get and check some actions.\n        actor_component_params = test.read_variable_values(actor_component.variable_registry)\n\n        # Some state inputs (5 input nodes, batch size=2).\n        states = state_space.sample(2)\n        # Expected NN-output.\n        expected_nn_output = np.matmul(\n            states * 2, actor_component_params[""actor-component/policy/test-network/hidden-layer/dense/kernel""]\n        )\n        # Raw action layer output.\n        expected_action_layer_output = np.matmul(\n            expected_nn_output,\n            actor_component_params[""actor-component/policy/action-adapter-0/action-network/action-layer/dense/kernel""]\n        )\n        # Final actions (max-likelihood/greedy pick).\n        expected_actions = np.argmax(expected_action_layer_output, axis=-1)\n        expected_preprocessed_state = states * 2\n        test.test((""get_preprocessed_state_and_action"", states), expected_outputs=dict(\n            preprocessed_state=expected_preprocessed_state, action=expected_actions,\n            nn_outputs=expected_nn_output\n        ), decimals=5)\n\n        # Get actions and action-probs by calling a different API-method.\n        states = state_space.sample(5)\n        # Get and check some actions.\n        actor_component_params = test.read_variable_values(actor_component.variable_registry)\n        # Expected NN-output.\n        expected_nn_output = np.matmul(\n            states * 2, actor_component_params[""actor-component/policy/test-network/hidden-layer/dense/kernel""]\n        )\n        # Raw action layer output.\n        expected_action_layer_output = np.matmul(\n            expected_nn_output,\n            actor_component_params[""actor-component/policy/action-adapter-0/action-network/action-layer/dense/kernel""]\n        )\n        # No reshape necessary (simple action space), softmax to get probs.\n        expected_action_probs = softmax(expected_action_layer_output)\n        # Final actions (max-likelihood/greedy pick).\n        expected_actions = np.argmax(expected_action_layer_output, axis=-1)\n        expected_preprocessed_state = states * 2\n        test.test((""get_preprocessed_state_action_and_action_probs"", states), expected_outputs=dict(\n            preprocessed_state=expected_preprocessed_state, action=expected_actions, action_probs=expected_action_probs,\n            nn_outputs=expected_nn_output\n        ), decimals=5)\n\n    def test_actor_component_with_lstm_network(self):\n        # state space and internal state space\n        state_space = FloatBox(shape=(2,), add_batch_rank=True, add_time_rank=True, time_major=False)\n        internal_states_space = Tuple(FloatBox(shape=(3,)), FloatBox(shape=(3,)), add_batch_rank=True)\n        time_percentages_space = FloatBox()\n        # action_space.\n        action_space = IntBox(2, add_batch_rank=True, add_time_rank=True)\n\n        preprocessor = PreprocessorStack.from_spec(\n            [dict(type=""convert_type"", to_dtype=""float""), dict(type=""divide"", divisor=10)]\n        )\n        policy = Policy(network_spec=config_from_path(""configs/test_lstm_nn.json""), action_space=action_space)\n        exploration = Exploration(epsilon_spec=dict(decay_spec=dict(\n            type=""linear_decay"", from_=1.0, to_=0.1)\n        ))\n        actor_component = ActorComponent(preprocessor, policy, exploration)\n        test = ComponentTest(\n            component=actor_component,\n            input_spaces=dict(\n                states=state_space,\n                other_nn_inputs=Tuple(internal_states_space, add_batch_rank=True),\n                time_percentage=time_percentages_space\n            ),\n            action_space=action_space\n        )\n        # Some state inputs (batch size=2, seq-len=1000; batch-major).\n        np.random.seed(10)\n        states = state_space.sample(size=(1000, 2))\n        initial_internal_states = internal_states_space.zeros(size=2)  # only batch\n        time_percentages = time_percentages_space.sample(1000)\n\n        # Run n times a single time-step to simulate acting and env interaction with an LSTM.\n        preprocessed_states = np.ndarray(shape=(1000, 2, 2), dtype=np.float)\n        actions = np.ndarray(shape=(1000, 2, 1), dtype=np.int)\n        for i, time_percentage in enumerate(time_percentages):\n            ret = test.test((\n                ""get_preprocessed_state_and_action"",\n                # expand time dim at 1st slot as we are time-major == False\n                [np.expand_dims(states[i], 1), tuple([initial_internal_states]), time_percentage]\n            ))\n            preprocessed_states[i] = ret[""preprocessed_state""][:, 0, :]  # take out time-rank again ()\n            actions[i] = ret[""action""]\n            # Check c/h-state shape.\n            self.assertEqual(ret[""nn_outputs""][1][0].shape, (2, 3))  # batch-size=2, LSTM units=3\n            self.assertEqual(ret[""nn_outputs""][1][1].shape, (2, 3))\n\n        # Check all preprocessed states (easy: just divided by 10).\n        expected_preprocessed_state = states / 10\n        recursive_assert_almost_equal(preprocessed_states, expected_preprocessed_state)\n\n        # Check the exploration functionality over the actions.\n        # Not checking mean as we are mostly in the non-exploratory region, that\'s why the stddev should be small.\n        stddev_actions = actions.std()\n        self.assertGreater(stddev_actions, 0.4)\n        self.assertLess(stddev_actions, 0.6)\n\n    def test_actor_component_with_dict_preprocessor(self):\n        # state_space (a complex Dict Space, that will be partially preprocessed).\n        state_space = Dict(\n            a=FloatBox(shape=(2,)),\n            b=FloatBox(shape=(5,)),\n            add_batch_rank=True\n        )\n        # action_space.\n        action_space = IntBox(2, add_batch_rank=True)\n\n        preprocessor_spec = dict(\n            type=""dict-preprocessor-stack"",\n            preprocessors=dict(\n                a=[\n                    dict(type=""convert_type"", to_dtype=""float""),\n                    dict(type=""multiply"", factor=0.5)\n                ]\n            )\n        )\n        # Simple custom NN with dict input (splits into 2 streams (simple dense layers) and concats at the end).\n        policy = Policy(network_spec=DummyNNWithDictInput(num_units_a=2, num_units_b=3, scope=""dummy-nn""),\n                        action_space=action_space)\n        exploration = None  # no exploration\n\n        actor_component = ActorComponent(preprocessor_spec, policy, exploration)\n\n        test = ComponentTest(\n            component=actor_component,\n            input_spaces=dict(states=state_space),\n            action_space=action_space\n        )\n\n        # Some state inputs (batch size=4).\n        states = state_space.sample(size=4)\n        # Get and check some actions.\n        actor_component_params = test.read_variable_values(actor_component.variable_registry)\n        # Expected NN-output.\n        expected_nn_output_stream_a = np.matmul(\n            states[""a""] * 0.5, actor_component_params[""actor-component/policy/dummy-nn/dense-a/dense/kernel""]\n        )\n        expected_nn_output_stream_b = np.matmul(\n            states[""b""], actor_component_params[""actor-component/policy/dummy-nn/dense-b/dense/kernel""]\n        )\n        expected_nn_output = np.concatenate((expected_nn_output_stream_a, expected_nn_output_stream_b), axis=-1)\n\n        # Raw action layer output.\n        expected_action_layer_output = np.matmul(\n            expected_nn_output,\n            actor_component_params[""actor-component/policy/action-adapter-0/action-network/action-layer/dense/kernel""]\n        )\n        # Final actions (max-likelihood/greedy pick).\n        expected_actions = np.argmax(expected_action_layer_output, axis=-1)\n        expected_preprocessed_state = dict(a=states[""a""] * 0.5, b=states[""b""])\n        test.test(\n            (""get_preprocessed_state_and_action"", states),\n            expected_outputs=dict(\n                preprocessed_state=expected_preprocessed_state, action=expected_actions,\n                nn_outputs=expected_nn_output\n            )\n        )\n'"
rlgraph/tests/components/test_batch_apply.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\n\nfrom rlgraph.components import BatchApply, DenseLayer, DictPreprocessorStack, Multiply, Divide\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.utils.numpy import dense_layer\n\n\nclass TestBatchApply(unittest.TestCase):\n    """"""\n    Tests the BatchApply Component.\n    """"""\n    def test_batch_apply_component_with_simple_input_space(self):\n        input_space = FloatBox(shape=(3,), add_batch_rank=True, add_time_rank=True)\n\n        sub_component = DenseLayer(units=4, biases_spec=False)\n\n        batch_apply = BatchApply(sub_component=sub_component, api_method_name=""call"")\n        test = ComponentTest(component=batch_apply, input_spaces=dict(input_=input_space))\n        weights = test.read_variable_values(batch_apply.variable_registry[""batch-apply/dense-layer/dense/kernel""])\n\n        sample = input_space.sample(size=(5, 10))\n        sample_folded = np.reshape(sample, newshape=(50, 3))\n\n        expected = dense_layer(sample_folded, weights)\n        expected = np.reshape(expected, newshape=(5, 10, 4))\n\n        test.test((""call"", sample), expected_outputs=expected)\n\n    def test_batch_apply_component_with_dict_input_space(self):\n        input_space = Dict(dict(\n            a=FloatBox(shape=(3,)),\n            b=FloatBox(shape=(1, 2))\n        ), add_batch_rank=True, add_time_rank=True)\n\n        sub_component = DictPreprocessorStack(preprocessors=dict(\n            a=[Multiply(factor=2.0)],\n            b=[Divide(divisor=2.0), Multiply(factor=2.0)]\n        ))\n\n        batch_apply = BatchApply(sub_component=sub_component, api_method_name=""preprocess"")\n        test = ComponentTest(component=batch_apply, input_spaces=dict(input_=input_space))\n\n        sample = input_space.sample(size=(5, 10))\n        sample_folded_a = np.reshape(sample[""a""], newshape=(50, 3))\n        sample_folded_b = np.reshape(sample[""b""], newshape=(50, 1, 2))\n\n        expected_a = sample_folded_a * 2.0\n        expected_a = np.reshape(expected_a, newshape=(5, 10, 3))\n        expected_b = sample_folded_b\n        expected_b = np.reshape(expected_b, newshape=(5, 10, 1, 2))\n\n        test.test((""call"", sample), expected_outputs=dict(a=expected_a, b=expected_b))\n\n'"
rlgraph/tests/components/test_batch_splitter.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nfrom rlgraph.components import BatchSplitter\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\n\n\nclass TestBatchSplitter(unittest.TestCase):\n    """"""\n    Tests the BatchSplitter Component.\n    """"""\n    def test_batch_splitter_component(self):\n        num_shards = 4\n        space = Dict(\n            states=dict(state1=float, state2=float),\n            actions=dict(action1=float),\n            rewards=float,\n            terminals=BoolBox(),\n            add_batch_rank=True\n        )\n\n        sample = space.sample(size=21)\n\n        test_inputs = [sample[""states""], sample[""actions""], sample[""rewards""], sample[""terminals""]]\n        splitter = BatchSplitter(num_shards=num_shards, shard_size=5)\n        test = ComponentTest(component=splitter, input_spaces=dict(\n            inputs=[space[""states""], space[""actions""], space[""rewards""], space[""terminals""]]\n        ))\n\n        # Expect 4 shards.\n        expected = tuple(\n            (\n                dict(state1=sample[""states""][""state1""][start:stop], state2=sample[""states""][""state2""][start:stop]),\n                dict(action1=sample[""actions""][""action1""][start:stop]),\n                sample[""rewards""][start:stop],\n                sample[""terminals""][start:stop]\n            ) for start, stop in [(0, 5), (5, 10), (10, 15), (15, 20)]\n        )\n\n        test.test((""split_batch"", test_inputs), expected_outputs=expected)\n'"
rlgraph/tests/components/test_component_copy.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\nfrom rlgraph.components.component import Component\nfrom rlgraph.components.layers.preprocessing import ReShape\nfrom rlgraph.spaces import FloatBox\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom six.moves import xrange as range_\n\n\nclass TestComponentCopy(unittest.TestCase):\n    """"""\n    Tests copying a constructed Component and adding the copy as well as the original into another Component.\n    """"""\n    def test_copying_a_component(self):\n        # Flatten a simple 2x2 FloatBox to (4,).\n        space = FloatBox(shape=(2, 2), add_batch_rank=False)\n\n        flatten_orig = ReShape(flatten=True, scope=""A"")\n        flatten_copy = flatten_orig.copy(scope=""B"")\n        container = Component(flatten_orig, flatten_copy)\n\n        @rlgraph_api(component=container)\n        def flatten1(self, input_):\n            return self.sub_components[""A""].call(input_)\n\n        @rlgraph_api(component=container)\n        def flatten2(self, input_):\n            return self.sub_components[""B""].call(input_)\n\n        test = ComponentTest(component=container, input_spaces=dict(input_=space))\n\n        input_ = dict(\n            input1=np.array([[0.5, 2.0], [1.0, 2.0]]),\n            input2=np.array([[1.0, 2.0], [3.0, 4.0]])\n        )\n        expected = dict(\n            output1=np.array([0.5, 2.0, 1.0, 2.0]),\n            output2=np.array([1.0, 2.0, 3.0, 4.0])\n        )\n        for i in range_(1, 3):\n            test.test((""flatten""+str(i), input_[""input""+str(i)]), expected_outputs=expected[""output""+str(i)])\n\n'"
rlgraph/tests/components/test_container_merger.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nfrom rlgraph.components.common.container_merger import ContainerMerger\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\n\n\nclass TestContainerMergerComponents(unittest.TestCase):\n    """"""\n    Tests the ContainerMerger Component.\n    """"""\n\n    def test_dict_merger_component(self):\n        space = Tuple(\n            dict(a=bool, b=float),\n            dict(c=bool),\n            float,\n            IntBox(low=0, high=255),\n            IntBox(2),\n            FloatBox(shape=(3, 2)),\n            Dict(d=bool, e=FloatBox(shape=())),\n            add_batch_rank=False\n        )\n        merger = ContainerMerger(""1"", ""2"", ""3"", ""test"", ""5"", ""6"", ""7"")\n        test = ComponentTest(component=merger, input_spaces=dict(inputs=[s for s in space]))\n\n        # Get a single sample.\n        sample = space.sample()\n        expected_outputs = dict({""1"": sample[0],\n                                 ""2"": sample[1],\n                                 ""3"": sample[2],\n                                 ""test"": sample[3],\n                                 ""5"": sample[4],\n                                 ""6"": sample[5],\n                                 ""7"": sample[6]})\n\n        test.test((""merge"", list(sample)), expected_outputs=expected_outputs)\n\n    def test_tuple_merger_component(self):\n        space = Tuple(\n            dict(a=bool, b=float),\n            dict(c=bool),\n            float,\n            IntBox(low=0, high=255),\n            IntBox(2),\n            FloatBox(shape=(3, 2)),\n            Dict(d=bool, e=FloatBox(shape=())),\n            add_batch_rank=False\n        )\n        merger = ContainerMerger(7)\n        test = ComponentTest(component=merger, input_spaces=dict(inputs=[s for s in space]))\n\n        # Get a single sample.\n        sample = space.sample()\n        expected_outputs = tuple([sample[0], sample[1], sample[2], sample[3], sample[4], sample[5], sample[6]])\n\n        test.test((""merge"", list(sample)), expected_outputs=expected_outputs)\n\n    def test_tuple_merger_component_merging_two_data_op_tuples(self):\n        space = Tuple(\n            Tuple(IntBox(2), IntBox(3)),\n            IntBox(3),\n            Tuple(FloatBox(shape=(3,)), BoolBox(shape=(1,))),\n            add_batch_rank=False\n        )\n        merger = ContainerMerger(merge_tuples_into_one=True)\n        test = ComponentTest(component=merger, input_spaces=dict(inputs=[s for s in space]))\n\n        # Get a single sample.\n        sample = space.sample()\n        expected_outputs = tuple([sample[0][0], sample[0][1], sample[1], sample[2][0], sample[2][1]])\n\n        test.test((""merge"", list(sample)), expected_outputs=expected_outputs)\n'"
rlgraph/tests/components/test_container_splitter.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nfrom rlgraph.components.layers.preprocessing.container_splitter import ContainerSplitter\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\n\n\nclass TestContainerSplitterComponents(unittest.TestCase):\n    """"""\n    Tests the ContainerSplitter Component.\n    """"""\n\n    def test_dict_splitter(self):\n        space = Dict(\n            a=dict(aa=bool, ab=float),\n            b=dict(ba=bool),\n            c=float,\n            d=IntBox(low=0, high=255),\n            e=IntBox(2),\n            f=FloatBox(shape=(3, 2)),\n            g=Tuple(bool, FloatBox(shape=())),\n            add_batch_rank=True\n        )\n        # Define the output-order.\n        splitter = ContainerSplitter(""g"", ""a"", ""b"", ""c"", ""d"", ""e"", ""f"")\n        test = ComponentTest(component=splitter, input_spaces=dict(inputs=space))\n\n        # Get a batch of samples.\n        input_ = space.sample(size=3)\n        expected_output = [\n            input_[""g""],\n            input_[""a""],\n            input_[""b""],\n            input_[""c""],\n            input_[""d""],\n            input_[""e""],\n            input_[""f""]\n        ]\n        test.test((""call"", input_), expected_outputs=expected_output)\n\n    def test_dict_splitter_with_different_input_space(self):\n        space = Dict(\n            a=Tuple(bool, FloatBox(shape=())),\n            b=FloatBox(shape=()),\n            c=bool,\n            d=IntBox(low=0, high=255),\n            e=dict(ea=float),\n            f=FloatBox(shape=(3, 2)),\n            add_batch_rank=False\n        )\n        # Define the output-order.\n        splitter = ContainerSplitter(""b"", ""c"", ""d"", ""a"", ""f"", ""e"")\n        test = ComponentTest(component=splitter, input_spaces=dict(inputs=space))\n\n        # Single sample (no batch rank).\n        input_ = space.sample()\n        expected_outputs = [\n            input_[""b""],\n            input_[""c""],\n            input_[""d""],\n            input_[""a""],\n            input_[""f""],\n            input_[""e""]\n        ]\n\n        test.test((""call"", input_), expected_outputs=expected_outputs)\n\n    def test_tuple_splitter(self):\n        space = Tuple(FloatBox(shape=()), bool, IntBox(low=0, high=255), add_batch_rank=True)\n        # Define the output-order.\n        splitter = ContainerSplitter(tuple_length=len(space))\n        test = ComponentTest(component=splitter, input_spaces=dict(inputs=space))\n\n        # Single sample (batch size=6).\n        input_ = space.sample(size=6)\n        expected_outputs = [\n            input_[0],\n            input_[1],\n            input_[2]\n        ]\n\n        test.test((""call"", (input_,)), expected_outputs=expected_outputs)\n'"
rlgraph/tests/components/test_decay_components.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\n\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\n\n\nclass TestDecayComponents(unittest.TestCase):\n    """"""\n    Tests RLGraph\'s decay components.\n    """"""\n\n    # Decaying a value always without batch dimension (does not make sense for global time step).\n    time_step_space = IntBox(1000)\n    time_step_space_with_time_rank = IntBox(1000, add_time_rank=True)\n\n    def test_linear_decay(self):\n        return  # Obsoleted!\n        decay_component = LinearDecay(from_=1.0, to_=0.0, start_timestep=100, num_timesteps=100)\n        test = ComponentTest(component=decay_component, input_spaces=dict(time_step=self.time_step_space))\n\n        # Values to pass as single items.\n        input_ = np.array([0, 1, 2, 25, 50, 100, 110, 112, 120, 130, 150, 180, 190, 195, 200, 201, 210, 250, 1000])\n        expected = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.88, 0.8, 0.7, 0.5, 0.2, 0.1, 0.05, 0.0, 0.0, 0.0,\n                             0.0, 0.0])\n        for i, e in zip(input_, expected):\n            test.test((""decayed_value"", i), expected_outputs=e)\n\n    def test_linear_decay_with_time_rank(self):\n        return  # Obsoleted!\n        decay_component = LinearDecay(from_=1.0, to_=0.0, start_timestep=0, num_timesteps=100)\n        test = ComponentTest(component=decay_component, input_spaces=dict(\n            time_step=self.time_step_space_with_time_rank\n        ))\n\n        # Values to pass all at once.\n        input_ = np.array([0, 1, 2, 25, 50, 100, 110, 112, 120, 130, 150, 180, 190, 195, 200, 201, 210, 250, 1000])\n        expected = np.array([1.0, 0.99, 0.98, 0.75, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                             0.0, 0.0])\n        test.test((""decayed_value"", input_), expected_outputs=expected)\n\n    def test_exponential_decay(self):\n        return  # Obsoleted!\n        decay_component = ExponentialDecay(from_=1.0, to_=0.0, start_timestep=0, num_timesteps=100, half_life=50)\n        test = ComponentTest(component=decay_component, input_spaces=dict(time_step=self.time_step_space))\n\n        # Values to pass as single items.\n        input_ = np.array([0, 1, 2, 25, 50, 75, 80, 90, 99, 100])\n        expected = np.array([1.0, 0.9862327, 0.97265494, 0.70710677, 0.5, 0.35355338, 0.329877, 0.2871746, 0.25348988,\n                             0.0])\n        for i, e in zip(input_, expected):\n            test.test((""decayed_value"", i), expected_outputs=e)\n\n'"
rlgraph/tests/components/test_dict_preprocessor_stack.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport unittest\n\nfrom rlgraph.components.neural_networks.dict_preprocessor_stack import DictPreprocessorStack\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\n\n\nclass TestDictPreprocessorStacks(unittest.TestCase):\n    """"""\n    Tests dict preprocessor stacks.\n    """"""\n    def test_dict_preprocessor_stack(self):\n        """"""\n        Tests if Python and TensorFlow backend return the same output\n        for a standard DQN-style preprocessing stack.\n        """"""\n        input_space = Dict(\n            a=FloatBox(shape=(2, 3)),\n            b=IntBox(3),\n            c=FloatBox(shape=(4, 5, 6)),\n            add_batch_rank=True\n        )\n        preprocessors = dict(\n            a=[dict(type=""divide"", divisor=2), dict(type=""multiply"", factor=4)],\n            c=[dict(type=""reshape"", flatten=True)]\n        )\n\n        dict_preprocessor_stack = DictPreprocessorStack(preprocessors)\n\n        test = ComponentTest(component=dict_preprocessor_stack, input_spaces=dict(inputs=input_space))\n\n        # Run the test.\n        batch_size = 5\n        inputs = input_space.sample(batch_size)\n        expected = dict(a=inputs[""a""] * 2, b=inputs[""b""], c=np.reshape(inputs[""c""], newshape=(batch_size, 120,)))\n        test.test(""reset"")\n        test.test((""preprocess"", inputs), expected_outputs=expected)\n'"
rlgraph/tests/components/test_distributions.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport unittest\n\nimport numpy as np\nfrom scipy.stats import norm, beta\nfrom rlgraph.components.distributions import *\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest, recursive_assert_almost_equal\nfrom rlgraph.utils.numpy import softmax\n\n\nclass TestDistributions(unittest.TestCase):\n\n    # TODO also not portable to PyTorch due to batch shapes.\n    # TODO: test entropies.\n\n    def test_bernoulli(self):\n        # Create 5 bernoulli distributions (or a multiple thereof if we use batch-size > 1).\n        param_space = FloatBox(shape=(5,), add_batch_rank=True)\n        values_space = BoolBox(shape=(5,), add_batch_rank=True)\n\n        # The Component to test.\n        bernoulli = Bernoulli(switched_off_apis={""kl_divergence""})\n        input_spaces = dict(\n            parameters=param_space,\n            values=values_space,\n            deterministic=bool,\n        )\n        test = ComponentTest(component=bernoulli, input_spaces=input_spaces)\n\n        # Batch of size=6 and deterministic (True).\n        input_ = [input_spaces[""parameters""].sample(6), True]\n        expected = input_[0] > 0.5\n        # Sample n times, expect always max value (max likelihood for deterministic draw).\n        for _ in range(10):\n            test.test((""draw"", input_), expected_outputs=expected)\n            test.test((""sample_deterministic"", input_[0]), expected_outputs=expected)\n\n        # Batch of size=6 and non-deterministic -> expect roughly the mean.\n        input_ = [input_spaces[""parameters""].sample(6), False]\n        outs = []\n        for _ in range(20):\n            out = test.test((""draw"", input_))\n            outs.append(out)\n            out = test.test((""sample_stochastic"", input_[0]))\n            outs.append(out)\n\n        recursive_assert_almost_equal(np.mean(outs), 0.5, decimals=1)\n\n        # Test log-likelihood outputs.\n        test.test((""log_prob"", [\n            np.array([[0.1, 0.2, 0.3, 0.4, 0.5]]),\n            np.array([[True, False, False, True, True]])\n            # probability that result is the given value\n        ]), expected_outputs=np.log(np.array([[0.1, 0.8, 0.7, 0.4, 0.5]])))\n\n        # Test entropy outputs.\n        input_ = np.array([[0.1, 0.2, 0.3, 0.4, 0.5]])\n        # Binary Entropy with natural log.\n        expected_entropy = -(input_ * np.log(input_)) - ((1.0 - input_) * np.log(1.0 - input_))\n        test.test((""entropy"", input_), expected_outputs=expected_entropy)\n\n    def test_categorical(self):\n        # Create 5 categorical distributions of 3 categories each.\n        param_space = FloatBox(shape=(5, 3), low=-1.0, high=2.0, add_batch_rank=True)\n        values_space = IntBox(3, shape=(5,), add_batch_rank=True)\n\n        # The Component to test.\n        categorical = Categorical(switched_off_apis={""kl_divergence""})\n        input_spaces = dict(\n            parameters=param_space,\n            values=values_space,\n            deterministic=bool,\n        )\n        test = ComponentTest(component=categorical, input_spaces=input_spaces)\n\n        # Batch of size=3 and deterministic (True).\n        input_ = [input_spaces[""parameters""].sample(3), True]\n        expected = np.argmax(input_[0], axis=-1)\n        # Sample n times, expect always max value (max likelihood for deterministic draw).\n        for _ in range(10):\n            test.test((""draw"", input_), expected_outputs=expected)\n            test.test((""sample_deterministic"", input_[0]), expected_outputs=expected)\n\n        # Batch of size=3 and non-deterministic -> expect roughly the mean.\n        input_ = [input_spaces[""parameters""].sample(3), False]\n        outs = []\n        for _ in range(20):\n            out = test.test((""draw"", input_))\n            outs.append(out)\n            out = test.test((""sample_stochastic"", input_[0]))\n            outs.append(out)\n\n        recursive_assert_almost_equal(np.mean(outs), 1.0, decimals=1)\n\n        # Test log-likelihood outputs.\n        input_ = param_space.sample(1)\n        labels = values_space.sample(1)\n        probs = softmax(input_)\n        test.test((""log_prob"", [input_, labels]), expected_outputs=np.log(np.array([[\n            probs[0][0][labels[0][0]], probs[0][1][labels[0][1]], probs[0][2][labels[0][2]],\n            probs[0][3][labels[0][3]], probs[0][4][labels[0][4]]\n        ]])), decimals=4)\n\n    def test_normal(self):\n        # Create 5 normal distributions (2 parameters (mean and stddev) each).\n        param_space = Tuple(\n            FloatBox(shape=(5,)),  # mean\n            FloatBox(shape=(5,)),  # stddev\n            add_batch_rank=True\n        )\n        values_space = FloatBox(shape=(5,), add_batch_rank=True)\n        input_spaces = dict(\n            parameters=param_space,\n            values=values_space,\n            deterministic=bool,\n        )\n\n        # The Component to test.\n        normal = Normal(switched_off_apis={""kl_divergence""})\n        test = ComponentTest(component=normal, input_spaces=input_spaces)\n\n        # Batch of size=2 and deterministic (True).\n        input_ = [param_space.sample(2), True]\n        expected = input_[0][0]  # 0 = mean\n        # Sample n times, expect always mean value (deterministic draw).\n        for _ in range(50):\n            test.test((""draw"", input_), expected_outputs=expected)\n            test.test((""sample_deterministic"", tuple([input_[0]])), expected_outputs=expected)\n\n        # Batch of size=1 and non-deterministic -> expect roughly the mean.\n        input_ = [param_space.sample(1), False]\n        expected = input_[0][0]  # 0 = mean\n        outs = []\n        for _ in range(50):\n            out = test.test((""draw"", input_))\n            outs.append(out)\n            out = test.test((""sample_stochastic"", tuple([input_[0]])))\n            outs.append(out)\n\n        recursive_assert_almost_equal(np.mean(outs), expected.mean(), decimals=1)\n\n        # Test log-likelihood outputs.\n        means = np.array([[0.1, 0.2, 0.3, 0.4, 100.0]])\n        stds = np.array([[0.8, 0.2, 0.3, 2.0, 50.0]])\n        values = np.array([[1.0, 2.0, 0.4, 10.0, 5.4]])\n        test.test(\n            (""log_prob"", [tuple([means, stds]), values]),\n            expected_outputs=np.log(norm.pdf(values, means, stds)), decimals=4\n        )\n\n    def test_multivariate_normal(self):\n        # Create batch0=n (batch-rank), batch1=2 (can be used for m mixed Gaussians), num-events=3 (trivariate)\n        # distributions (2 parameters (mean and stddev) each).\n        num_events = 3  # 3=trivariate Gaussian\n        num_mixed_gaussians = 2  # 2x trivariate Gaussians (mixed)\n        param_space = Tuple(\n            FloatBox(shape=(num_mixed_gaussians, num_events)),  # mean\n            FloatBox(shape=(num_mixed_gaussians, num_events)),  # diag (variance)\n            add_batch_rank=True\n        )\n        values_space = FloatBox(shape=(num_mixed_gaussians, num_events), add_batch_rank=True)\n        input_spaces = dict(\n            parameters=param_space,\n            values=values_space,\n            deterministic=bool,\n        )\n\n        # The Component to test.\n        multivariate_normal = MultivariateNormal(switched_off_apis={""kl_divergence""})\n        test = ComponentTest(component=multivariate_normal, input_spaces=input_spaces)\n\n        input_ = [input_spaces[""parameters""].sample(4), True]\n        expected = input_[0][0]  # 0=mean\n        # Sample n times, expect always mean value (deterministic draw).\n        for _ in range(50):\n            test.test((""draw"", input_), expected_outputs=expected)\n            test.test((""sample_deterministic"", tuple([input_[0]])), expected_outputs=expected)\n\n        # Batch of size=1 and non-deterministic -> expect roughly the mean.\n        input_ = [input_spaces[""parameters""].sample(1), False]\n        expected = input_[0][0]  # 0=mean\n        outs = []\n        for _ in range(50):\n            out = test.test((""draw"", input_))\n            outs.append(out)\n            out = test.test((""sample_stochastic"", tuple([input_[0]])))\n            outs.append(out)\n\n        recursive_assert_almost_equal(np.mean(outs), expected.mean(), decimals=1)\n\n        # Test log-likelihood outputs (against scipy).\n        means = values_space.sample(2)\n        stds = values_space.sample(2)\n        values = values_space.sample(2)\n        test.test(\n            (""log_prob"", [tuple([means, stds]), values]),\n            # Sum up the individual log-probs as we have a diag (independent) covariance matrix.\n            expected_outputs=np.sum(np.log(norm.pdf(values, means, stds)), axis=-1), decimals=4\n        )\n\n    def test_beta(self):\n        # Create 5 beta distributions (2 parameters (alpha and beta) each).\n        param_space = Tuple(\n            FloatBox(shape=(5,)),  # alpha\n            FloatBox(shape=(5,)),  # beta\n            add_batch_rank=True\n        )\n        values_space = FloatBox(shape=(5,), add_batch_rank=True)\n        input_spaces = dict(\n            parameters=param_space,\n            values=values_space,\n            deterministic=bool,\n        )\n\n        # The Component to test.\n        low, high = -1.0, 2.0\n        beta_distribution = Beta(low=low, high=high, switched_off_apis={""kl_divergence""})\n        test = ComponentTest(component=beta_distribution, input_spaces=input_spaces)\n\n        # Batch of size=2 and deterministic (True).\n        input_ = [input_spaces[""parameters""].sample(2), True]\n        # Mean for a Beta distribution: 1 / [1 + (beta/alpha)]\n        expected = (1.0 / (1.0 + input_[0][1] / input_[0][0])) * (high - low) + low\n        # Sample n times, expect always mean value (deterministic draw).\n        for _ in range(50):\n            test.test((""draw"", input_), expected_outputs=expected, decimals=5)\n            test.test((""sample_deterministic"", tuple([input_[0]])), expected_outputs=expected, decimals=5)\n\n        # Batch of size=1 and non-deterministic -> expect roughly the mean.\n        input_ = [input_spaces[""parameters""].sample(1), False]\n        expected = (1.0 / (1.0 + input_[0][1] / input_[0][0])) * (high - low) + low\n        outs = []\n        for _ in range(50):\n            out = test.test((""draw"", input_))\n            outs.append(out)\n            out = test.test((""sample_stochastic"", tuple([input_[0]])))\n            outs.append(out)\n\n        recursive_assert_almost_equal(np.mean(outs), expected.mean(), decimals=1)\n\n        # Test log-likelihood outputs (against scipy).\n        alpha_ = values_space.sample(1)\n        beta_ = values_space.sample(1)\n        values = values_space.sample(1)\n        values_scaled = values * (high - low) + low\n        test.test(\n            (""log_prob"", [tuple([alpha_, beta_]), values_scaled]),\n            expected_outputs=np.log(beta.pdf(values, alpha_, beta_)), decimals=4\n        )\n\n    def test_mixture(self):\n        # Create a mixture distribution consisting of 3 bivariate normals.\n        num_distributions = 3\n        num_events_per_multivariate = 2  # 2=bivariate\n        param_space = Dict(\n            {\n                ""categorical"": FloatBox(shape=(num_distributions,), low=-1.5, high=2.3),\n                ""parameters0"": Tuple(\n                    FloatBox(shape=(num_events_per_multivariate,)),  # mean\n                    FloatBox(shape=(num_events_per_multivariate,)),  # diag\n                ),\n                ""parameters1"": Tuple(\n                    FloatBox(shape=(num_events_per_multivariate,)),  # mean\n                    FloatBox(shape=(num_events_per_multivariate,)),  # diag\n                ),\n                ""parameters2"": Tuple(\n                    FloatBox(shape=(num_events_per_multivariate,)),  # mean\n                    FloatBox(shape=(num_events_per_multivariate,)),  # diag\n                ),\n            },\n            add_batch_rank=True\n        )\n        values_space = FloatBox(shape=(num_events_per_multivariate,), add_batch_rank=True)\n        input_spaces = dict(\n            parameters=param_space,\n            values=values_space,\n            deterministic=bool,\n        )\n\n        # The Component to test.\n        mixture = MixtureDistribution(\n            # Try different spec types.\n            MultivariateNormal(), ""multi-variate-normal"", ""multivariate_normal"",\n            switched_off_apis={""entropy"", ""kl_divergence""}\n        )\n        test = ComponentTest(component=mixture, input_spaces=input_spaces)\n\n        # Batch of size=n and deterministic (True).\n        input_ = [input_spaces[""parameters""].sample(1), True]\n        # Make probs for categorical.\n        categorical_probs = softmax(input_[0][""categorical""])\n\n        # Note: Usually, the deterministic draw should return the max-likelihood value\n        # Max-likelihood for a 3-Mixed Bivariate: mean-of-argmax(categorical)()\n        # argmax = np.argmax(input_[0][""categorical""], axis=-1)\n        #expected = np.array([input_[0][""parameters{}"".format(idx)][0][i] for i, idx in enumerate(argmax)])\n        #    input_[0][""categorical""][:, 1:2] * input_[0][""parameters1""][0] + \\\n        #    input_[0][""categorical""][:, 2:3] * input_[0][""parameters2""][0]\n\n        # The mean value is a 2D vector (bivariate distribution).\n        expected = categorical_probs[:, 0:1] * input_[0][""parameters0""][0] + \\\n            categorical_probs[:, 1:2] * input_[0][""parameters1""][0] + \\\n            categorical_probs[:, 2:3] * input_[0][""parameters2""][0]\n\n        for _ in range(50):\n            test.test((""draw"", input_), expected_outputs=expected)\n            test.test((""sample_deterministic"", tuple([input_[0]])), expected_outputs=expected)\n\n        # Batch of size=1 and non-deterministic -> expect roughly the mean.\n        input_ = [input_spaces[""parameters""].sample(1), False]\n        # Make probs for categorical.\n        categorical_probs = softmax(input_[0][""categorical""])\n\n        expected = categorical_probs[:, 0:1] * input_[0][""parameters0""][0] + \\\n            categorical_probs[:, 1:2] * input_[0][""parameters1""][0] + \\\n            categorical_probs[:, 2:3] * input_[0][""parameters2""][0]\n        outs = []\n        for _ in range(50):\n            out = test.test((""draw"", input_))\n            outs.append(out)\n            out = test.test((""sample_stochastic"", tuple([input_[0]])))\n            outs.append(out)\n\n        recursive_assert_almost_equal(np.mean(np.array(outs), axis=0), expected, decimals=1)\n\n        # Test log-likelihood outputs (against scipy).\n        params = param_space.sample(1)\n        # Make sure categorical params are softmaxed.\n        category_probs = softmax(params[""categorical""][0])\n        values = values_space.sample(1)\n        expected = \\\n            category_probs[0] * \\\n            np.sum(np.log(norm.pdf(values[0], params[""parameters0""][0][0], params[""parameters0""][1][0])), axis=-1) + \\\n            category_probs[1] * \\\n            np.sum(np.log(norm.pdf(values[0], params[""parameters1""][0][0], params[""parameters1""][1][0])), axis=-1) + \\\n            category_probs[2] * \\\n            np.sum(np.log(norm.pdf(values[0], params[""parameters2""][0][0], params[""parameters2""][1][0])), axis=-1)\n        test.test((""log_prob"", [params, values]), expected_outputs=np.array([expected]), decimals=1)\n\n    def test_squashed_normal(self):\n        param_space = Tuple(\n            FloatBox(shape=(5,)),\n            FloatBox(shape=(5,)),\n            add_batch_rank=True\n        )\n        values_space = FloatBox(shape=(5,), add_batch_rank=True)\n        input_spaces = dict(\n            parameters=param_space,\n            deterministic=bool,\n            values=values_space\n        )\n\n        low, high = -2.0, 1.0\n        squashed_distribution = SquashedNormal(switched_off_apis={""kl_divergence""}, low=low, high=high)\n        test = ComponentTest(component=squashed_distribution, input_spaces=input_spaces)\n\n        # Batch of size=2 and deterministic (True).\n        input_ = [param_space.sample(2), True]\n        expected = ((np.tanh(input_[0][0]) + 1.0) / 2.0) * (high - low) + low   # [0] = mean\n        # Sample n times, expect always mean value (deterministic draw).\n        for _ in range(50):\n            test.test((""draw"", input_), expected_outputs=expected, decimals=5)\n            test.test((""sample_deterministic"", tuple([input_[0]])), expected_outputs=expected, decimals=5)\n\n        # Batch of size=1 and non-deterministic -> expect roughly the mean.\n        input_ = [param_space.sample(1), False]\n        expected = ((np.tanh(input_[0][0]) + 1.0) / 2.0) * (high - low) + low  # [0] = mean\n        outs = []\n        for _ in range(500):\n            out = test.test((""draw"", input_))\n            outs.append(out)\n            self.assertTrue(out.max() <= high)\n            self.assertTrue(out.min() >= low)\n            out = test.test((""sample_stochastic"", tuple([input_[0]])))\n            outs.append(out)\n            self.assertTrue(out.max() <= high)\n            self.assertTrue(out.min() >= low)\n\n        recursive_assert_almost_equal(np.mean(outs), expected.mean(), decimals=1)\n\n        # Test log-likelihood outputs.\n        means = np.array([[0.1, 0.2, 0.3, 0.4, 5.0]])\n        stds = np.array([[0.8, 0.2, 0.3, 2.0, 4.0]])\n        # Make sure values are within low and high.\n        values = np.array([[0.9, 0.2, 0.4, -0.1, -1.05]])\n\n        # TODO: understand and comment the following formula to get the log-prob.\n        # Unsquash values, then get log-llh from regular gaussian.\n        unsquashed_values = np.arctanh((values - low) / (high - low) * 2.0 - 1.0)\n        log_prob_unsquashed = np.log(norm.pdf(unsquashed_values, means, stds))\n        log_prob = log_prob_unsquashed - np.sum(np.log(1 - np.tanh(unsquashed_values) ** 2), axis=-1, keepdims=True)\n\n        test.test((""log_prob"", [tuple([means, stds]), values]), expected_outputs=log_prob, decimals=4)\n\n    def test_gumbel_softmax_distribution(self):\n        # 5-categorical Gumble-Softmax.\n        param_space = Tuple(FloatBox(shape=(5,)), add_batch_rank=True)\n        values_space = FloatBox(shape=(5,), add_batch_rank=True)\n        input_spaces = dict(parameters=param_space, deterministic=bool, values=values_space)\n\n        gumble_softmax_distribution = GumbelSoftmax(switched_off_apis={""kl_divergence"", ""entropy""}, temperature=1.0)\n        test = ComponentTest(component=gumble_softmax_distribution, input_spaces=input_spaces)\n\n        # Batch of size=2 and deterministic (True).\n        input_ = [param_space.sample(2), True]\n        expected = np.argmax(input_[0], axis=-1)\n        # Sample n times, expect always argmax value (deterministic draw).\n        for _ in range(50):\n            test.test((""draw"", input_), expected_outputs=expected, decimals=5)\n            test.test((""sample_deterministic"", tuple([input_[0]])), expected_outputs=expected, decimals=5)\n\n        # TODO: finish this test case, using an actual Gumble-Softmax distribution from the\n        # paper: https://arxiv.org/pdf/1611.01144.pdf.\n        return\n\n        # Batch of size=1 and non-deterministic -> expect roughly the mean.\n        input_ = [param_space.sample(1), False]\n        expected = ""???""\n        outs = []\n        for _ in range(100):\n            out = test.test((""draw"", input_))\n            outs.append(np.argmax(out, axis=-1))\n            out = test.test((""sample_stochastic"", tuple([input_[0]])))\n            outs.append(np.argmax(out, axis=-1))\n\n        recursive_assert_almost_equal(np.mean(outs), expected.mean(), decimals=1)\n\n        # Test log-likelihood outputs.\n        means = np.array([[0.1, 0.2, 0.3, 0.4, 5.0]])\n        stds = np.array([[0.8, 0.2, 0.3, 2.0, 4.0]])\n        # Make sure values are within low and high.\n        values = np.array([[0.9, 0.2, 0.4, -0.1, -1.05]])\n\n        # TODO: understand and comment the following formula to get the log-prob.\n        # Unsquash values, then get log-llh from regular gaussian.\n        unsquashed_values = np.arctanh((values - low) / (high - low) * 2.0 - 1.0)\n        log_prob_unsquashed = np.log(norm.pdf(unsquashed_values, means, stds))\n        log_prob = log_prob_unsquashed - np.sum(np.log(1 - np.tanh(unsquashed_values) ** 2), axis=-1, keepdims=True)\n\n        test.test((""log_prob"", [tuple([means, stds]), values]), expected_outputs=log_prob, decimals=4)\n\n    def test_joint_cumulative_distribution(self):\n        param_space = Dict({\n            ""a"": FloatBox(shape=(4,)),  # 4-discrete\n            ""b"": Dict({""ba"": Tuple([FloatBox(shape=(3,)), FloatBox(0.1, 1.0, shape=(3,))]),  # 3-variate normal\n                       ""bb"": Tuple([FloatBox(shape=(2,)), FloatBox(shape=(2,))]),  # beta -1 to 1\n                       ""bc"": Tuple([FloatBox(shape=(4,)), FloatBox(0.1, 1.0, shape=(4,))]),  # normal (dim=4)\n                       })\n        }, add_batch_rank=True)\n\n        values_space = Dict({\n            ""a"": IntBox(4),\n            ""b"": Dict({\n                ""ba"": FloatBox(shape=(3,)),\n                ""bb"": FloatBox(shape=(2,)),\n                ""bc"": FloatBox(shape=(4,))\n            })\n        }, add_batch_rank=True)\n\n        input_spaces = dict(\n            parameters=param_space,\n            values=values_space,\n            deterministic=bool\n        )\n\n        low, high = -1.0, 1.0\n        joined_cumulative_distribution = JointCumulativeDistribution(distribution_specs={\n            ""/a"": Categorical(), ""/b/ba"": MultivariateNormal(), ""/b/bb"": Beta(low=low, high=high), ""/b/bc"": Normal()\n        }, switched_off_apis={""kl_divergence""})\n        test = ComponentTest(component=joined_cumulative_distribution, input_spaces=input_spaces)\n\n        # Batch of size=2 and deterministic (True).\n        input_ = [param_space.sample(2), True]\n        input_[0][""a""] = softmax(input_[0][""a""])\n        expected_mean = {\n            ""a"": np.argmax(input_[0][""a""], axis=-1),\n            ""b"": {\n                ""ba"": input_[0][""b""][""ba""][0],  # [0]=Mean\n                # Mean for a Beta distribution: 1 / [1 + (beta/alpha)] * range + low\n                ""bb"": (1.0 / (1.0 + input_[0][""b""][""bb""][1] / input_[0][""b""][""bb""][0])) * (high - low) + low,\n                ""bc"": input_[0][""b""][""bc""][0],\n            }\n        }\n        # Sample n times, expect always mean value (deterministic draw).\n        for _ in range(50):\n            test.test((""draw"", input_), expected_outputs=expected_mean)\n            test.test((""sample_deterministic"", tuple([input_[0]])), expected_outputs=expected_mean)\n\n        # Batch of size=1 and non-deterministic -> expect roughly the mean.\n        input_ = [param_space.sample(1), False]\n        input_[0][""a""] = softmax(input_[0][""a""])\n        expected_mean = {\n            ""a"": np.sum(input_[0][""a""] * np.array([0, 1, 2, 3])),\n            ""b"": {\n                ""ba"": input_[0][""b""][""ba""][0],  # [0]=Mean\n                # Mean for a Beta distribution: 1 / [1 + (beta/alpha)] * range + low\n                ""bb"": (1.0 / (1.0 + input_[0][""b""][""bb""][1] / input_[0][""b""][""bb""][0])) * (high - low) + low,\n                ""bc"": input_[0][""b""][""bc""][0],\n            }\n        }\n\n        outs = []\n        for _ in range(100):\n            out = test.test((""draw"", input_))\n            outs.append(out)\n            out = test.test((""sample_stochastic"", tuple([input_[0]])))\n            outs.append(out)\n\n        recursive_assert_almost_equal(np.mean(np.stack([o[""a""][0] for o in outs], axis=0), axis=0), expected_mean[""a""], atol=0.2)\n        recursive_assert_almost_equal(np.mean(np.stack([o[""b""][""ba""][0] for o in outs], axis=0), axis=0),\n                                      expected_mean[""b""][""ba""][0], decimals=1)\n        recursive_assert_almost_equal(np.mean(np.stack([o[""b""][""bb""][0] for o in outs], axis=0), axis=0),\n                                      expected_mean[""b""][""bb""][0], decimals=1)\n        recursive_assert_almost_equal(np.mean(np.stack([o[""b""][""bc""][0] for o in outs], axis=0), axis=0),\n                                      expected_mean[""b""][""bc""][0], decimals=1)\n\n        # Test log-likelihood outputs.\n        params = param_space.sample(1)\n        params[""a""] = softmax(params[""a""])\n        # Make sure beta-values are within 0.0 and 1.0 for the numpy calculation (which doesn\'t have scaling).\n        values = values_space.sample(1)\n        log_prob_beta = np.log(beta.pdf(values[""b""][""bb""], params[""b""][""bb""][0], params[""b""][""bb""][1]))\n        # Now do the scaling for b/bb (beta values).\n        values[""b""][""bb""] = values[""b""][""bb""] * (high - low) + low\n        expected_log_llh = np.log(params[""a""][0][values[""a""][0]]) + \\\n            np.sum(np.log(norm.pdf(values[""b""][""ba""][0], params[""b""][""ba""][0], params[""b""][""ba""][1]))) + \\\n            np.sum(log_prob_beta) + \\\n            np.sum(np.log(norm.pdf(values[""b""][""bc""][0], params[""b""][""bc""][0], params[""b""][""bc""][1])))\n\n        test.test((""log_prob"", [params, values]), expected_outputs=expected_log_llh, decimals=1)\n'"
rlgraph/tests/components/test_dqn_loss_functions.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\nfrom rlgraph.components.loss_functions import DQNLossFunction\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\n\n\nclass TestDQNLossFunctions(unittest.TestCase):\n\n    reward_space = FloatBox(add_batch_rank=True)\n    terminal_space = BoolBox(add_batch_rank=True)\n    loss_per_item_space = FloatBox(add_batch_rank=True)\n\n    def test_dqn_loss_function_on_int_action_space(self):\n        # Create a shape=() 2-action discrete-space.\n        # Thus, each action pick consists of one single binary action (0 or 1).\n        action_space = IntBox(2, shape=(), add_batch_rank=True)\n        q_values_space = FloatBox(shape=action_space.get_shape(with_category_rank=True), add_batch_rank=True)\n        dqn_loss_function = DQNLossFunction(discount=1.0)  # gamma=1.0: keep it simple\n\n        test = ComponentTest(\n            component=dqn_loss_function,\n            input_spaces=dict(\n                q_values_s=q_values_space,\n                actions=action_space,\n                rewards=self.reward_space,\n                terminals=self.terminal_space,\n                qt_values_sp=q_values_space,\n                loss_per_item=self.loss_per_item_space\n            ),\n            action_space=action_space\n        )\n\n        # Batch of size=2.\n        input_ = [\n            np.array([[10.0, -10.0], [-0.101, -90.6]]),\n            np.array([0, 1]),\n            np.array([9.4, -1.23]),\n            np.array([False, False]),\n            np.array([[12.0, -8.0], [22.3, 10.5]])\n        ]\n        """"""\n        Calculation:\n        batch of 2, gamma=1.0\n        Qt(s\'a\') = [12 -8] [22.3 10.5] -> max(a\') = [12] [22.3]\n        Q(s,a)  = [10.0] [-90.6]\n        L = E(batch)| 0.5((r + gamma max(a\')Qt(s\'a\') ) - Q(s,a))^2 |\n        L = (0.5(9.4 + 1.0*12 - 10.0)^2 + 0.5(-1.23 + 1.0*22.3 - -90.6)^2) / 2\n        L = (0.5(129.96) + 0.5(12470.1889)) / 2\n        L = (64.98 + 6235.09445) / 2\n        L = 3150.037225\n        """"""\n\n        # Batch size=2 -> Expect 2 values returned by `loss_per_item`.\n        expected_loss_per_item = np.array([64.979996, 6235.09445], dtype=np.float32)\n        test.test((""loss_per_item"", input_), expected_outputs=expected_loss_per_item)\n        # Expect the mean over the batch.\n        expected_loss = expected_loss_per_item.mean()\n        test.test((""loss_average"", expected_loss_per_item), expected_outputs=expected_loss)\n        # Both.\n        test.test((""loss"", input_), expected_outputs=[expected_loss, expected_loss_per_item])\n\n    def test_double_dqn_loss_function_on_int_action_space(self):\n        # Create a shape=() 3-action discrete-space.\n        # Thus, each action pick consists of one single action (0, 1, or 2).\n        action_space = IntBox(3, shape=(), add_batch_rank=True)\n        q_values_space = FloatBox(shape=action_space.get_shape(with_category_rank=True), add_batch_rank=True)\n        dqn_loss_function = DQNLossFunction(double_q=True, discount=0.9)\n\n        test = ComponentTest(\n            component=dqn_loss_function,\n            input_spaces=dict(\n                q_values_s=q_values_space,\n                actions=action_space,\n                rewards=self.reward_space,\n                terminals=self.terminal_space,\n                qt_values_sp=q_values_space,\n                q_values_sp=q_values_space,\n                loss_per_item=self.loss_per_item_space\n            ),\n            action_space=action_space\n        )\n\n        # Batch of size=2.\n        input_ = [\n            np.array([[10.0, -10.0, 12.4], [-0.101, -4.6, -9.3]]),\n            np.array([2, 1]),\n            np.array([10.3, -4.25]),\n            np.array([False, True]),\n            np.array([[-12.3, 1.2, 1.4], [12.2, -11.5, 9.2]]),\n            np.array([[-10.3, 1.5, 1.4], [8.2, -10.9, 9.3]])\n        ]\n        """"""\n        Calculation:\n        batch of 2, gamma=0.9\n        a\' = [1 2]  <- argmax(a\')Q(s\'a\')\n        Qt(s\'.) = [-12.3  1.2  1.4] [12.2  -11.5  9.2] -> Qt(s\'a\') = [1.2] [0.0 <- normally 9.2, but terminal(!) = True]\n        a = [2 1]\n        Q(s,a)  = [12.4] [-4.6]\n        L = E(batch)| 0.5((r + gamma Qt(s\'( argmax(a\') Q(s\'a\') )) ) - Q(s,a))^2 |\n        L = (0.5(10.3 + 0.9*1.2 - 12.4)^2 + 0.5(-4.25 + 0.9*0.0 - -4.6)^2) / 2\n        L = (0.5(1.0404) + 0.5(0.1224999)) / 2\n        L = (0.5202 + 0.06124995) / 2 \n        L = 0.290725\n        """"""\n\n        # Batch size=2 -> Expect 2 values returned by `loss_per_item`.\n        expected_loss_per_item = np.array([0.5202, 0.06125], dtype=np.float32)\n        test.test((""loss_per_item"", input_), expected_outputs=expected_loss_per_item, decimals=4)\n        # Expect the mean over the batch.\n        expected_loss = expected_loss_per_item.mean()\n        test.test((""loss_average"", expected_loss_per_item), expected_outputs=expected_loss, decimals=4)\n        # Both.\n        test.test((""loss"", input_), expected_outputs=[expected_loss, expected_loss_per_item], decimals=4)\n\n    def test_dqn_loss_function_in_multi_action_space(self):\n        # Create a shape=(3,) 4-action discrete-space.\n        # Thus, each action pick consists of 2 composite-actions chosen from a set of 4 possible single actions.\n        action_space = IntBox(4, shape=(3,), add_batch_rank=True)\n        q_values_space = FloatBox(shape=action_space.get_shape(with_category_rank=True), add_batch_rank=True)\n        dqn_loss_function = DQNLossFunction(discount=0.8)\n\n        test = ComponentTest(\n            component=dqn_loss_function,\n            input_spaces=dict(\n                q_values_s=q_values_space,\n                actions=action_space,\n                rewards=self.reward_space,\n                terminals=self.terminal_space,\n                qt_values_sp=q_values_space,\n                loss_per_item=self.loss_per_item_space\n            ),\n            action_space=action_space\n        )\n\n        # Batch of size=2.\n        input_ = [\n            np.array([[[10.0, -10.0, 9.8, 2.0], [20.2, -0.6, 0.001, 98.1], [10.0, -10.0, 9.8, 2.0]],\n                               [[4.1, -11.1, 7.5, 2.1], [21.3, 9.5, -0.101, -90.6], [21.3, 9.5, -0.101, -90.6]]]),\n            np.array([[0, 3, 2], [1, 2, 0]]),\n            np.array([9.4, -1.23]),\n            np.array([False, True]),\n            np.array([[[12.0, -8.0, 7.8, 4.0], [16.2, -2.6, -6.001, 90.1], [12.0, -8.0, 7.8, 4.0]],\n                                   [[5.1, -12.1, 8.5, 3.1], [22.3, 10.5, 1.098, -89.2], [22.3, 10.5, 1.098, -89.2]]])\n        ]\n\n        """"""\n        Calculation:\n        batch of 2, gamma=0.8\n        Qt(s\'a\') = [[12 -8 7.8 4] [16.2 -2.6 -6.001 90.1] .. ] [[5.1 -12.1 8.5 3.1] [22.3 10.5 1.098 -89.2] ..] ->\n            max(a\')Qt(s\'a\') = [[12 90.1 12] [0 0 0] <- would have been [8.5 22.3], but terminal=True]\n        Q(s,a)  = [10.0 98.1 9.8] [-11.1 -0.101 21.3]\n        L = E(batch)| 0.5((r + gamma max(a\')Qt(s\'a\') ) - Q(s,a))^2 |\n        L = (0.5((9.4 + 0.8*12 - 10.0) + (9.4 + 0.8*90.1 - 98.1) + (9.4 + 0.8*12 - 9.8))/3)^2 +\n            (0.5(((-1.23 + 0.8*0.0 - -11.1) + (-1.23 + 0.8*0.0 - -0.101) + (-1.23 + 0.8*0.0 - 21.3))/3)^2) / 2\n        L = (0.5((9.0) + (-16.62) + (9.2))/3)^2 +\n            (0.5(((-1.23 + 0.8*0.0 - -11.1) + (-1.23 + 0.8*0.0 - -0.101) + (-1.23 + 0.8*0.0 - 21.3))/3)^2) / 2\n        L = (0.5(0.5267)^2 + 0.5(-4.59633)^2) / 2\n        L = (0.138689725 + 10.563136) / 2\n        L = 5.3509128625\n        """"""\n\n        # Batch size=2 -> Expect 2 values returned by `loss_per_item`.\n        expected_loss_per_item = np.array([0.138689725, 10.563136], dtype=np.float32)\n        print(test.test((""loss_per_item"", input_), expected_outputs=None))\n        # Just expect the mean over the batch.\n        expected_loss = expected_loss_per_item.mean()\n        test.test((""loss_average"", expected_loss_per_item), expected_outputs=expected_loss)\n        # Both.\n        test.test((""loss"", input_), expected_outputs=[expected_loss, expected_loss_per_item])\n\n    def test_double_dqn_loss_function_on_multi_int_action_space(self):\n        # Create a shape=(2,2) 3-action discrete-space.\n        # Thus, each action pick consists of one single action (0, 1, or 2).\n        action_space = IntBox(3, shape=(2,2), add_batch_rank=True)\n        q_values_space = FloatBox(shape=action_space.get_shape(with_category_rank=True), add_batch_rank=True)\n        dqn_loss_function = DQNLossFunction(double_q=True, discount=1.0)\n\n        test = ComponentTest(\n            component=dqn_loss_function,\n            input_spaces=dict(\n                q_values_s=q_values_space,\n                actions=action_space,\n                rewards=self.reward_space,\n                terminals=self.terminal_space,\n                qt_values_sp=q_values_space,\n                q_values_sp=q_values_space,\n                loss_per_item=self.loss_per_item_space\n            ),\n            action_space=action_space\n        )\n\n        # Batch of size=4.\n        input_ = [\n            np.array(\n                [\n                    [[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10., 11., 12.]]],\n                    [[[2.0, 3.0, 4.0], [5.0, 6.0, 7.0]], [[8.0, 9.0, 10.], [11., 12., 13.]]],\n                    [[[3.0, 4.0, 5.0], [6.0, 7.0, 8.0]], [[9.0, 10., 11.], [12., 13., 14.]]],\n                    [[[4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], [[10., 11., 12.], [13., 14., 15.]]],\n                ]\n            ),\n            np.array(\n                [\n                    [[0, 1], [2, 0]],\n                    [[1, 2], [0, 1]],\n                    [[2, 0], [1, 2]],\n                    [[0, 1], [1, 1]]\n                ]\n            ),\n            np.array([-1.0, -2.0, -3.0, -4.0]),\n            np.array([False, False, True, False]),\n            np.array(\n                [\n                    [[[1.5, 2.5, 3.5], [4.5, 5.5, 6.5]], [[7.5, 8.5, 9.5], [10.5, 11.5, 12.5]]],\n                    [[[2.5, 3.5, 4.5], [5.5, 6.5, 7.5]], [[8.5, 9.5, 10.5], [11.5, 12.5, 13.5]]],\n                    [[[3.5, 4.5, 5.5], [6.5, 7.5, 8.5]], [[9.5, 10.5, 11.5], [12.5, 13.5, 14.5]]],\n                    [[[4.5, 5.5, 6.5], [7.5, 8.5, 9.5]], [[10.5, 11.5, 12.5], [13.5, 14.5, 15.5]]],\n                ]\n            ),\n            np.array(\n                [\n                    [[[1.6, 2.6, 3.6], [4.6, 5.6, 6.6]], [[7.6, 8.6, 9.6], [10.6, 11.6, 12.6]]],\n                    [[[2.6, 3.6, 4.6], [5.6, 6.6, 7.6]], [[8.6, 9.6, 10.6], [11.6, 12.6, 13.6]]],\n                    [[[3.6, 4.6, 5.6], [6.6, 7.6, 8.6]], [[9.6, 10.6, 11.6], [12.6, 13.6, 14.6]]],\n                    [[[4.6, 5.6, 6.6], [7.6, 8.6, 9.6]], [[10.6, 11.6, 12.6], [13.6, 14.6, 15.6]]],\n                ]\n            )\n        ]\n        """"""\n        Calculation:\n        batch of 4, gamma=1.0\n        a\' = [[2 2] [2 2]], [[2 2] [2 2]], [[2 2] [2 2]], [[2 2] [2 2]]  <- argmax(a\')Q(s\'a\')\n        Qt(s\'a\') = [[3.5 6.5] [9.5 12.5]], [[4.5 7.5] [10.5 13.5]], [[0.0 0.0] [0.0 0.0]], [[6.5 9.5] [12.5 15.5]]\n        a = [[0 1] [2 0]], [[1 2] [0 1]], [[2 0] [1 2]], [[0 1] [1 1]]\n        Q(s,a) = [[1 5] [9 10]], [[3 7] [8 12]], [[5 6] [10 14]], [[4 8] [11 14]]\n        L = E(batch)| 0.5((r + gamma Qt(s\'( argmax(a\') Q(s\'a\') )) ) - Q(s,a))^2 |\n        L = [\n            0.5(((-1 + 3.5 - 1) + (-1 + 6.5 - 5) + (-1 + 9.5 - 9) + (-1 + 12.5 - 10))/4)^2 +   -> 0.28125\n            0.5(((-2 + 4.5 - 3) + (-2 + 7.5 - 7) + (-2 + 10.5 - 8) + (-2 + 13.5 - 12))/4)^2 +  -> 0.125\n            0.5(((-3 + 0.0 - 5) + (-3 + 0.0 - 6) + (-3 + 0.0 - 10) + (-3 + 0.0 - 14))/4)^2 +   -> 69.03125\n            0.5(((-4 + 6.5 - 4) + (-4 + 9.5 - 8) + (-4 + 12.5 - 11) + (-4 + 15.5 - 14))/4)^2   -> 2.53125\n            ] / 4\n        L = 17.9921875\n        """"""\n\n        # Batch size=2 -> Expect 2 values returned by `loss_per_item`.\n        expected_loss_per_item = np.array([0.28125, 0.125, 69.03125, 2.53125], dtype=np.float32)\n        test.test((""loss_per_item"", input_), expected_outputs=expected_loss_per_item)\n        # Expect the mean over the batch.\n        expected_loss = expected_loss_per_item.mean()\n        test.test((""loss_average"", expected_loss_per_item), expected_outputs=expected_loss)\n        # Both.\n        test.test((""loss"", input_), expected_outputs=[expected_loss, expected_loss_per_item])\n\n    def test_dqn_loss_function_in_container_action_space(self):\n        action_space = Dict({""a"": IntBox(2), ""b"": {""ba"": IntBox(3), ""bb"": IntBox(2)}}, add_batch_rank=True)\n        q_values_space = Dict({""a"": FloatBox(shape=(2,)), ""b"": {""ba"": FloatBox(shape=(3,)),\n                                                                ""bb"": FloatBox(shape=(2, ))}}, add_batch_rank=True)\n        dqn_loss_function = DQNLossFunction(discount=1.0)\n\n        test = ComponentTest(\n            component=dqn_loss_function,\n            input_spaces=dict(\n                q_values_s=q_values_space,\n                actions=action_space,\n                rewards=self.reward_space,\n                terminals=self.terminal_space,\n                qt_values_sp=q_values_space,\n                loss_per_item=self.loss_per_item_space\n            ),\n            action_space=action_space\n        )\n\n        # Batch of size=1.\n        input_ = [\n            # q(s)-values\n            {""a"": np.array([[1.0, 2.0]]), ""b"": {""ba"": np.array([[0.0, -0.5, 1.2]]), ""bb"": np.array([[-1.0, -2.0]])}},\n            # actions\n            {""a"": np.array([0]), ""b"": {""ba"": np.array([0]), ""bb"": np.array([1])}},\n            np.array([1.0]),\n            np.array([False]),\n            # qt(s\')-values\n            {""a"": np.array([[-1.0, -2.0]]), ""b"": {""ba"": np.array([[3.0, 3.1, 3.2]]), ""bb"": np.array([[1.0, -5.2]])}}\n        ]\n\n        """"""\n        Calculation:\n        batch of 1, gamma=1.0\n        TDtarget (global across all sub-actions) = 1/N SUMd ( r + gamma max(a\')Qdt(s\'a\') )\n          = 1/3 [(1.0 + 1.0*-1.0) + (1.0 + 1.0*3.2) + (1.0 + 1.0*1.0)] = 2.06667\n        L = E(batch) | 1/N SUMd ( 0.5*(TDtarget - Qd(s,a))^2 ) |   # d=action components\n        L = 1/3 SUMd ( 0.5[a]^2 + 0.5[ba]^2 + 0.5[bb]^2 )\n            a=2.06667 - 1.0=1.06667\n            ba=2.06667 - 0.0= 2.06667\n            bb=2.06667 - -2.0=4.06667\n            \n            Huberloss/square before aggregation:\n            SUM=0.5(1.06667)^2 + 0.5(2.06667)^2 + 0.5(4.06667)^2 = 10.973357\n            10.973357/3 = 3.657786\n\n            Huber loss/square after aggregation: \n            SUM=0.5 * ((1.06667 + 2.06667 + 4.06667)/3) ^ 2\n               =2.88\n        """"""\n\n        # Batch size=2 -> Expect 2 values returned by `loss_per_item`.\n        expected_loss_per_item = np.array([2.8800], dtype=np.float32)\n        test.test((""loss_per_item"", input_), expected_outputs=expected_loss_per_item, decimals=4)\n        # Just expect the mean over the batch.\n        expected_loss = expected_loss_per_item.mean()\n        test.test((""loss_average"", expected_loss_per_item), expected_outputs=expected_loss, decimals=4)\n        # Both.\n        test.test((""loss"", input_), expected_outputs=[expected_loss, expected_loss_per_item], decimals=2)\n\n    def test_double_dqn_loss_function_in_container_action_space(self):\n        action_space = Dict({""a"": IntBox(2), ""b"": {""ba"": IntBox(3), ""bb"": IntBox(2)}}, add_batch_rank=True)\n        q_values_space = Dict({""a"": FloatBox(shape=(2,)), ""b"": {""ba"": FloatBox(shape=(3,)),\n                                                                ""bb"": FloatBox(shape=(2, ))}}, add_batch_rank=True)\n        dqn_loss_function = DQNLossFunction(discount=0.99, double_q=True)\n\n        test = ComponentTest(\n            component=dqn_loss_function,\n            input_spaces=dict(\n                q_values_s=q_values_space,\n                actions=action_space,\n                rewards=self.reward_space,\n                terminals=self.terminal_space,\n                qt_values_sp=q_values_space,\n                q_values_sp=q_values_space,\n                loss_per_item=self.loss_per_item_space\n            ),\n            action_space=action_space\n        )\n\n        # Batch of size=2.\n        input_ = [\n            # q(s)-values\n            {""a"": np.array([[1.0, 2.0], [3.0, 4.0]]), ""b"": {""ba"": np.array([[0.0, -0.5, 1.2], [-0.1, -0.2, -0.3]]),\n                                                            ""bb"": np.array([[-1.0, -2.0], [0.5, 0.6]])}},\n            # actions\n            {""a"": np.array([0, 1]), ""b"": {""ba"": np.array([0, 2]), ""bb"": np.array([1, 0])}},\n            np.array([1.0, -1.0]),\n            np.array([False, True]),\n            # qt(s\')-values\n            {""a"": np.array([[-1.0, -2.0], [5.0, 6.0]]), ""b"": {""ba"": np.array([[3.0, 3.1, 3.2], [4.1, 4.2, 4.3]]),\n                                                              ""bb"": np.array([[1.0, -5.2], [-0.5, -0.6]])}},\n            # q(s\')-values\n            {""a"": np.array([[-1.0, 100.0], [5.0, 60.0]]), ""b"": {""ba"": np.array([[3.0, 3.1, 3.2], [4.5, 4.4, 4.3]]),\n                                                                ""bb"": np.array([[1.0, -5.2], [-0.5, 0.6]])}}\n        ]\n\n        """"""\n        Calculation:\n        batch of 2, gamma=0.99\n        argmaxa\'Qd(s\',a\'): a=[1, 1] ba=[2, 0] bb=[0, 1]\n        TDtarget (global across all sub-actions) = 1/N SUMd ( r + gamma Qdt(s\',argmaxa\'Qd(s\',a\')) )\n          = [\n                1/3 [(1.0 + 0.99*-2.0) + (1.0 + 0.99*3.2) + (1.0 + 0.99*1.0)],\n                1/3 [(-1.0 + 0.99*0.0) + (-1.0 + 0.99*0.1) + (-1.0 + 0.99*0.0)]  # all 0 due to terminal=True\n            ]\n          = [1/3(-0.98 + 4.168 + 1.99), 1/3(-1.0 + -1.0 + -1.0)]\n          = [1.726, -1.0]\n        L = E(batch) | 1/N SUMd ( 0.5*(TDtarget - Qd(s,a))^2 ) |   # d=action components\n        L = 1/3 SUMd ( 0.5[a]^2 + 0.5[ba]^2 + 0.5[bb]^2 )\n            a=[1.726-1.0, -1.0-4.0]=[0.726, -5.0]\n            ba=[1.726-0.0, -1.0+0.3]=[1.726, -0.7]\n            bb=[1.726+2.0, -1.0-0.5]=[3.726, -1.5]\n            \n            Huberloss/square before aggregation:\n            SUM=[\n                0.5(0.726)^2 + 0.5(1.726)^2 + 0.5(3.726)^2,\n                0.5(-5.0)^2 + 0.5(-0.7)^2 + 0.5(-1.5)^2\n                ] = [8.694614, 13.87]\n            /3 = [2.89820467, 4.62333]\n            \n            Huberloss/square after aggregation:\n             =[\n                0.5 * ((0.726 + 1.726 + 3.726) / 3)^2,\n                0.5 * ((-5.0 + -0.7 + -1.5) / 3)^2\n                ] = [2.1204 2.88]\n            \n        """"""\n\n        # Batch size=2 -> Expect 2 values returned by `loss_per_item`.\n        expected_loss_per_item = np.array([2.1204, 2.88], dtype=np.float32)\n        test.test((""loss_per_item"", input_), expected_outputs=expected_loss_per_item, decimals=4, print=True)\n        # Just expect the mean over the batch.\n        expected_loss = expected_loss_per_item.mean()\n        test.test((""loss_average"", expected_loss_per_item), expected_outputs=expected_loss, decimals=4)\n        # Both.\n        test.test((""loss"", input_), expected_outputs=[expected_loss, expected_loss_per_item], decimals=2)\n\n'"
rlgraph/tests/components/test_environment_stepper.py,0,"b'# Copyright 2018/2019 The RLgraph authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport unittest\n\nimport numpy as np\n\nfrom rlgraph.components.common.environment_stepper import EnvironmentStepper\nfrom rlgraph.components.explorations.exploration import Exploration\nfrom rlgraph.components.neural_networks.actor_component import ActorComponent\nfrom rlgraph.environments.environment import Environment\nfrom rlgraph.spaces import FloatBox, IntBox, Tuple\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.tests.test_util import config_from_path, recursive_assert_almost_equal\nfrom rlgraph.utils.numpy import dense_layer, softmax, lstm_layer\nfrom rlgraph.utils.ops import DataOpTuple\n\n\nclass TestEnvironmentStepper(unittest.TestCase):\n    """"""\n    Tests for the EnvironmentStepper Component using a simple RandomEnv.\n    """"""\n    deterministic_env_state_space = FloatBox(shape=(1,))\n    deterministic_env_action_space = IntBox(2)\n    deterministic_action_probs_space = FloatBox(shape=(2,), add_batch_rank=True)\n\n    grid_world_2x2_state_space = IntBox(4)\n    grid_world_2x2_action_space = IntBox(4)\n    grid_world_2x2_action_probs_space = FloatBox(shape=(4,), add_batch_rank=True)\n\n    internal_states_space = Tuple(FloatBox(shape=(256,)), FloatBox(shape=(256,)), add_batch_rank=True)\n    internal_states_space_test_lstm = Tuple(FloatBox(shape=(3,)), FloatBox(shape=(3,)), add_batch_rank=True)\n\n    action_probs_space = FloatBox(shape=(4,), add_batch_rank=True)\n\n    time_steps = 500\n\n    def test_environment_stepper_on_deterministic_env(self):\n        preprocessor_spec = None\n        network_spec = config_from_path(""configs/test_simple_nn.json"")\n        exploration_spec = None\n        actor_component = ActorComponent(\n            preprocessor_spec,\n            dict(network_spec=network_spec, action_space=self.deterministic_env_action_space),\n            exploration_spec\n        )\n        environment_stepper = EnvironmentStepper(\n            environment_spec=dict(type=""deterministic_env"", steps_to_terminal=5),\n            actor_component_spec=actor_component,\n            state_space=self.deterministic_env_state_space,\n            reward_space=""float32"",\n            num_steps=3\n        )\n\n        test = ComponentTest(\n            component=environment_stepper,\n            action_space=self.deterministic_env_action_space,\n        )\n\n        # Step 3 times through the Env and collect results.\n        expected = (\n            np.array([False, False, False]),  # t_\n            np.array([[0.0], [1.0], [2.0], [3.0]]),  # s\' (raw)\n        )\n        test.test(""step"", expected_outputs=expected)\n\n        # Step again, check whether stitching of states/etc.. works.\n        expected = (\n            np.array([False, True, False]),  # t_\n            np.array([[3.0], [4.0], [0.0], [1.0]]),  # s\' (raw)\n        )\n        test.test(""step"", expected_outputs=expected)\n\n        # Make sure we close the session (to shut down the Env on the server).\n        test.terminate()\n\n    def test_environment_stepper_on_2x2_grid_world(self):\n        preprocessor_spec = [dict(\n            type=""reshape"", flatten=True, flatten_categories=self.grid_world_2x2_action_space.num_categories\n        )]\n        network_spec = config_from_path(""configs/test_simple_nn.json"")\n        # Try to find a NN that outputs greedy actions down in start state and right in state=1 (to reach goal).\n        network_spec[""layers""][0][""weights_spec""] = [[0.5, -0.5], [-0.1, 0.1], [-0.2, 0.2], [-0.4, 0.2]]\n        network_spec[""layers""][0][""biases_spec""] = False\n        exploration_spec = None\n        actor_component = ActorComponent(\n            preprocessor_spec,\n            dict(network_spec=network_spec, action_adapter_spec=dict(\n                weights_spec=[[0.1, -0.5, 0.5, 0.1], [0.4, 0.2, -0.2, 0.2]],\n                biases_spec=False\n            ), action_space=self.grid_world_2x2_action_space, deterministic=True),\n            exploration_spec\n        )\n        environment_stepper = EnvironmentStepper(\n            environment_spec=dict(type=""grid_world"", world=""2x2""),\n            actor_component_spec=actor_component,\n            state_space=self.grid_world_2x2_state_space,\n            reward_space=""float32"",\n            add_action_probs=True,\n            action_probs_space=self.grid_world_2x2_action_probs_space,\n            num_steps=5\n        )\n\n        test = ComponentTest(\n            component=environment_stepper,\n            action_space=self.grid_world_2x2_action_space,\n        )\n\n        # Step 5 times through the Env and collect results.\n        expected = (\n            np.array([False, True, False, True, False]),  # t_\n            np.array([0, 1, 0, 1, 0, 1]),  # s\' (raw)\n            np.array([[0.21869287, 0.17905058, 0.36056358, 0.24169299],\n                      [0.2547221, 0.2651175, 0.23048209, 0.24967825],\n                      [0.21869287, 0.17905058, 0.36056358, 0.24169299],\n                      [0.2547221, 0.2651175, 0.23048209, 0.24967825],\n                      [0.21869287, 0.17905058, 0.36056358, 0.24169299]], dtype=np.float32)\n        )\n        out = test.test(""step"", expected_outputs=expected, decimals=2)\n        print(out)\n\n        # Step again, check whether stitching of states/etc.. works.\n        expected = (\n            np.array([True, False, True, False, True]),  # t_\n            np.array([1, 0, 1, 0, 1, 0]),  # s\' (raw)\n            np.array([[0.2547221, 0.2651175, 0.23048209, 0.24967825],\n                      [0.21869287, 0.17905058, 0.36056358, 0.24169299],\n                      [0.2547221, 0.2651175, 0.23048209, 0.24967825],\n                      [0.21869287, 0.17905058, 0.36056358, 0.24169299],\n                      [0.2547221, 0.2651175, 0.23048209, 0.24967825]], dtype=np.float32)\n        )\n        out = test.test(""step"", expected_outputs=expected)\n        print(out)\n\n        # Make sure we close the session (to shut down the Env on the server).\n        test.terminate()\n\n    def test_environment_stepper_on_2x2_grid_world_returning_actions_and_rewards(self):\n        preprocessor_spec = [dict(\n            type=""reshape"", flatten=True, flatten_categories=self.grid_world_2x2_action_space.num_categories\n        )]\n        network_spec = config_from_path(""configs/test_simple_nn.json"")\n        # Try to find a NN that outputs greedy actions down in start state and right in state=1 (to reach goal).\n        network_spec[""layers""][0][""weights_spec""] = [[0.5, -0.5], [-0.1, 0.1], [-0.2, 0.2], [-0.4, 0.2]]\n        network_spec[""layers""][0][""biases_spec""] = False\n        exploration_spec = None\n        actor_component = ActorComponent(\n            preprocessor_spec,\n            dict(network_spec=network_spec, action_adapter_spec=dict(\n                weights_spec=[[0.1, -0.5, 0.5, 0.1], [0.4, 0.2, -0.2, 0.2]],\n                biases_spec=False\n            ), action_space=self.grid_world_2x2_action_space, deterministic=True),\n            exploration_spec\n        )\n        environment_stepper = EnvironmentStepper(\n            environment_spec=dict(type=""grid_world"", world=""2x2""),\n            actor_component_spec=actor_component,\n            state_space=self.grid_world_2x2_state_space,\n            reward_space=""float32"",\n            add_action=True,\n            add_reward=True,\n            num_steps=5\n        )\n\n        test = ComponentTest(\n            component=environment_stepper,\n            action_space=self.grid_world_2x2_action_space,\n        )\n\n        # Step 5 times through the Env and collect results.\n        expected = (\n            np.array([False, True, False, True, False]),  # t_\n            np.array([0, 1, 0, 1, 0, 1]),  # s\' (raw)\n            np.array([2, 1, 2, 1, 2]),  # actions taken\n            np.array([-1.0, 1.0, -1.0, 1.0, -1.0])  # rewards\n        )\n        out = test.test(""step"", expected_outputs=expected, decimals=2)\n        print(out)\n\n        # Make sure we close the session (to shut down the Env on the server).\n        test.terminate()\n\n    def test_environment_stepper_on_deterministic_env_with_returning_action_probs(self):\n        preprocessor_spec = [dict(type=""divide"", divisor=2)]\n        network_spec = config_from_path(""configs/test_simple_nn.json"")\n        exploration_spec = None\n        actor_component = ActorComponent(\n            preprocessor_spec,\n            dict(network_spec=network_spec, action_space=self.deterministic_env_action_space),\n            exploration_spec\n        )\n        environment_stepper = EnvironmentStepper(\n            environment_spec=dict(type=""deterministic_env"", steps_to_terminal=6),\n            actor_component_spec=actor_component,\n            state_space=self.deterministic_env_state_space,\n            reward_space=""float32"",\n            add_action_probs=True,\n            action_probs_space=self.deterministic_action_probs_space,\n            num_steps=3\n        )\n\n        test = ComponentTest(\n            component=environment_stepper,\n            action_space=self.deterministic_env_action_space,\n        )\n\n        weights = test.read_variable_values(environment_stepper.actor_component.policy.variable_registry)\n        policy_scope = ""environment-stepper/actor-component/policy/""\n        weights_hid = weights[policy_scope+""test-network/hidden-layer/dense/kernel""]\n        biases_hid = weights[policy_scope+""test-network/hidden-layer/dense/bias""]\n        weights_action = weights[policy_scope+""action-adapter-0/action-network/action-layer/dense/kernel""]\n        biases_action = weights[policy_scope+""action-adapter-0/action-network/action-layer/dense/bias""]\n\n        # Step 3 times through the Env and collect results.\n        expected = (\n            # t_\n            np.array([False, False, False]),\n            # s\' (raw)\n            np.array([[0.0], [1.0], [2.0], [3.0]]),\n            # action probs\n            np.array([\n                softmax(dense_layer(dense_layer(np.array([0.0]), weights_hid, biases_hid), weights_action, biases_action)),\n                softmax(dense_layer(dense_layer(np.array([0.5]), weights_hid, biases_hid), weights_action, biases_action)),\n                softmax(dense_layer(dense_layer(np.array([1.0]), weights_hid, biases_hid), weights_action, biases_action))\n            ])\n        )\n        test.test(""step"", expected_outputs=expected, decimals=3)\n\n        # Step again, check whether stitching of states/etc.. works.\n        expected = (\n            np.array([False, False, True]),\n            np.array([[3.0], [4.0], [5.0], [0.0]]),  # s\' (raw)\n            np.array([\n                softmax(dense_layer(dense_layer(np.array([1.5]), weights_hid, biases_hid), weights_action, biases_action)),\n                softmax(dense_layer(dense_layer(np.array([2.0]), weights_hid, biases_hid), weights_action, biases_action)),\n                softmax(dense_layer(dense_layer(np.array([2.5]), weights_hid, biases_hid), weights_action, biases_action))\n            ])\n        )\n        test.test(""step"", expected_outputs=expected, decimals=3)\n\n        # Make sure we close the session (to shut down the Env on the server).\n        test.terminate()\n\n    def test_environment_stepper_on_deterministic_env_with_action_probs_lstm(self):\n        internal_states_space = Tuple(FloatBox(shape=(3,)), FloatBox(shape=(3,)))\n        preprocessor_spec = [dict(type=""multiply"", factor=0.1)]\n        network_spec = config_from_path(""configs/test_lstm_nn.json"")\n        exploration_spec = None\n        actor_component = ActorComponent(\n            preprocessor_spec,\n            dict(network_spec=network_spec, action_space=self.deterministic_env_action_space),\n            exploration_spec\n        )\n        environment_stepper = EnvironmentStepper(\n            environment_spec=dict(type=""deterministic_env"", steps_to_terminal=3),\n            actor_component_spec=actor_component,\n            state_space=self.deterministic_env_state_space,\n            reward_space=""float32"",\n            internal_states_space=internal_states_space,\n            add_action_probs=True,\n            action_probs_space=self.deterministic_action_probs_space,\n            num_steps=4,\n        )\n\n        test = ComponentTest(\n            component=environment_stepper,\n            action_space=self.deterministic_env_action_space,\n        )\n\n        weights = test.read_variable_values(environment_stepper.actor_component.policy.variable_registry)\n        policy_scope = ""environment-stepper/actor-component/policy/""\n        weights_lstm = weights[policy_scope+""test-lstm-network/lstm-layer/lstm-cell/kernel""]\n        biases_lstm = weights[policy_scope+""test-lstm-network/lstm-layer/lstm-cell/bias""]\n        weights_action = weights[policy_scope+""action-adapter-0/action-network/action-layer/dense/kernel""]\n        biases_action = weights[policy_scope+""action-adapter-0/action-network/action-layer/dense/bias""]\n\n        # Step 3 times through the Env and collect results.\n        lstm_1 = lstm_layer(np.array([[[0.0]]]), weights_lstm, biases_lstm)\n        lstm_2 = lstm_layer(np.array([[[0.1]]]), weights_lstm, biases_lstm, lstm_1[1])\n        lstm_3 = lstm_layer(np.array([[[0.2]]]), weights_lstm, biases_lstm, lstm_2[1])\n        lstm_4 = lstm_layer(np.array([[[0.0]]]), weights_lstm, biases_lstm, lstm_3[1])\n        expected = (\n            np.array([False, False, True, False]),\n            np.array([[0.0], [1.0], [2.0], [0.0], [1.0]]),  # s\' (raw)\n            np.array([\n                softmax(dense_layer(np.squeeze(lstm_1[0]), weights_action, biases_action)),\n                softmax(dense_layer(np.squeeze(lstm_2[0]), weights_action, biases_action)),\n                softmax(dense_layer(np.squeeze(lstm_3[0]), weights_action, biases_action)),\n                softmax(dense_layer(np.squeeze(lstm_4[0]), weights_action, biases_action)),\n            ]),  # action probs\n            # internal states\n            (\n                np.squeeze(np.array([[[0.0, 0.0, 0.0]], lstm_1[1][0], lstm_2[1][0], lstm_3[1][0], lstm_4[1][0]])),\n                np.squeeze(np.array([[[0.0, 0.0, 0.0]], lstm_1[1][1], lstm_2[1][1], lstm_3[1][1], lstm_4[1][1]]))\n            )\n        )\n        test.test(""step"", expected_outputs=expected)\n\n        # Make sure we close the session (to shut down the Env on the server).\n        test.terminate()\n\n    def test_environment_stepper_on_pong(self):\n        environment_spec = dict(type=""openai-gym"", gym_env=""Pong-v0"", frameskip=4, seed=10)\n        dummy_env = Environment.from_spec(environment_spec)\n        state_space = dummy_env.state_space\n        action_space = dummy_env.action_space\n        agent_config = config_from_path(""configs/dqn_agent_for_pong.json"")\n        actor_component = ActorComponent(\n            agent_config[""preprocessing_spec""],\n            dict(network_spec=agent_config[""network_spec""],\n                 action_space=action_space,\n                 **agent_config[""policy_spec""]),\n            agent_config[""exploration_spec""]\n        )\n        environment_stepper = EnvironmentStepper(\n            environment_spec=environment_spec,\n            actor_component_spec=actor_component,\n            state_space=state_space,\n            reward_space=""float"",\n            add_reward=True,\n            num_steps=self.time_steps\n        )\n\n        test = ComponentTest(\n            component=environment_stepper,\n            action_space=action_space,\n        )\n\n        # Step 30 times through the Env and collect results.\n        # 1st return value is the step-op (None), 2nd return value is the tuple of items (3 steps each), with each\n        # step containing: Preprocessed state, actions, rewards, episode returns, terminals, (raw) next-states.\n        time_start = time.monotonic()\n        out = test.test(""step"")\n        time_end = time.monotonic()\n        print(""Done running {} steps in env-stepper env in {}sec."".format(\n            environment_stepper.num_steps, time_end - time_start\n        ))\n\n        # Check types of outputs.\n        self.assertTrue(isinstance(out, DataOpTuple))  # the step results as a tuple (see below)\n\n        # Check types of single data.\n        self.assertTrue(out[0].dtype == np.bool_)  # next-state is terminal?\n        self.assertTrue(out[1].dtype == np.uint8)  # next state (raw, not preprocessed)\n        self.assertTrue(out[1].min() >= 0)  # make sure we have pixels\n        self.assertTrue(out[1].max() <= 255)\n        self.assertTrue(out[2].dtype == np.float32)  # rewards\n        self.assertTrue(out[2].min() >= -1.0)  # -1.0 to 1.0\n        self.assertTrue(out[2].max() <= 1.0)\n\n        # Make sure we close the session (to shut down the Env on the server).\n        test.terminate()\n\n    def test_compare_with_non_env_stepper(self):\n        environment_spec = dict(type=""openai_gym"", gym_env=""Pong-v0"", frameskip=4, seed=10)\n        dummy_env = Environment.from_spec(environment_spec)\n        state_space = dummy_env.state_space.with_batch_rank()\n        action_space = dummy_env.action_space\n        agent_config = config_from_path(""configs/dqn_agent_for_pong.json"")\n        actor_component = ActorComponent(\n            agent_config[""preprocessing_spec""],\n            dict(network_spec=agent_config[""network_spec""],\n                 action_space=action_space,\n                 **agent_config[""policy_spec""]),\n            agent_config[""exploration_spec""]\n        )\n        test = ComponentTest(\n            component=actor_component,\n            input_spaces=dict(states=state_space),\n            action_space=action_space,\n        )\n        s = dummy_env.reset()\n        time_start = time.monotonic()\n        for i in range(self.time_steps):\n            out = test.test((""get_preprocessed_state_and_action"", np.array([s])))\n            a = out[""action""]\n            # Act in env.\n            s, r, t, _ = dummy_env.step(a[0])  # remove batch\n            if t is True:\n                s = dummy_env.reset()\n        time_end = time.monotonic()\n        print(""Done running {} steps in bare-metal env in {}sec."".format(self.time_steps, time_end - time_start))\n        test.terminate()\n\n    def test_environment_stepper_on_deepmind_lab(self):\n        try:\n            from rlgraph.environments.deepmind_lab import DeepmindLabEnv\n        except ImportError:\n            print(""DeepmindLab not installed: Skipping this test case."")\n            return\n\n        env_spec = dict(\n            type=""deepmind_lab"", level_id=""seekavoid_arena_01"", observations=[""RGB_INTERLEAVED""], frameskip=4\n        )\n        dummy_env = Environment.from_spec(env_spec)\n        state_space = dummy_env.state_space\n        action_space = dummy_env.action_space\n        actor_component = ActorComponent(\n            # Preprocessor spec (only divide and flatten the image).\n            [\n                {\n                    ""type"": ""divide"",\n                    ""divisor"": 255\n                },\n                {\n                    ""type"": ""reshape"",\n                    ""flatten"": True\n                }\n            ],\n            # Policy spec.\n            dict(network_spec=""../configs/test_lstm_nn.json"", action_space=action_space),\n            # Exploration spec.\n            Exploration(epsilon_spec=dict(decay_spec=dict(\n                type=""linear_decay"", from_=1.0, to_=0.1)\n            ))\n        )\n        environment_stepper = EnvironmentStepper(\n            environment_spec=env_spec,\n            actor_component_spec=actor_component,\n            state_space=state_space,\n            reward_space=""float32"",\n            internal_states_space=self.internal_states_space_test_lstm,\n            num_steps=1000,\n            # Add both prev-action and -reward into the state sent through the network.\n            #add_previous_action_to_state=True,\n            #add_previous_reward_to_state=True,\n            add_action_probs=True,\n            action_probs_space=FloatBox(shape=(9,), add_batch_rank=True)\n        )\n\n        test = ComponentTest(\n            component=environment_stepper,\n            action_space=action_space,\n        )\n        # Step n times through the Env and collect results.\n        # 1st return value is the step-op (None), 2nd return value is the tuple of items (3 steps each), with each\n        # step containing: Preprocessed state, actions, rewards, episode returns, terminals, (raw) next-states.\n        time_start = time.monotonic()\n        steps = 10\n        out = None\n        for _ in range(steps):\n            out = test.test(""step"")\n        time_total = time.monotonic() - time_start\n        print(""Done running {}x{} steps in Deepmind Lab env using IMPALA network in {}sec. ({} actions/sec)"".format(\n            steps, environment_stepper.num_steps, time_total, environment_stepper.num_steps * steps / time_total)\n        )\n\n        # Check types of outputs.\n        self.assertTrue(isinstance(out, DataOpTuple))  # the step results as a tuple (see below)\n\n        # Check types of single data.\n        self.assertTrue(out[0].dtype == np.bool_)  # next-state is terminal?\n        self.assertTrue(out[1].dtype == np.uint8)  # next state (raw, not preprocessed)\n        self.assertTrue(out[1].min() >= 0)  # make sure we have pixels\n        self.assertTrue(out[1].max() <= 255)\n        # action probs (test whether sum to one).\n        self.assertTrue(out[2].dtype == np.float32)\n        self.assertTrue(out[2].min() >= 0.0)\n        self.assertTrue(out[2].max() <= 1.0)\n        recursive_assert_almost_equal(\n            out[2].sum(axis=-1, keepdims=False), np.ones(shape=(environment_stepper.num_steps,)), decimals=4\n        )\n        # internal states (c- and h-state)\n        self.assertTrue(out[3][0].dtype == np.float32)\n        self.assertTrue(out[3][1].dtype == np.float32)\n        self.assertTrue(out[3][0].shape == (environment_stepper.num_steps, 3))\n        self.assertTrue(out[3][1].shape == (environment_stepper.num_steps, 3))\n\n        test.terminate()\n'"
rlgraph/tests/components/test_epsilon_exploration.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\n\nfrom rlgraph.components.common.time_dependent_parameters import LinearDecay\nfrom rlgraph.components.explorations.exploration import EpsilonExploration\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\n\n\nclass TestEpslionExploration(unittest.TestCase):\n\n    def test_epsilon_exploration_at_single_time_steps(self):\n        time_percentage_space = FloatBox(add_batch_rank=True)\n        sample_space = FloatBox(add_batch_rank=True)\n\n        np.random.seed(10)\n\n        # The Component(s) to test.\n        decay_component = LinearDecay(from_=1.0, to_=0.15)\n        epsilon_component = EpsilonExploration(decay_spec=decay_component)\n        test = ComponentTest(component=epsilon_component, input_spaces=dict(\n            sample=sample_space, time_percentage=float\n        ))\n\n        # Take n uniform samples over the time space and then check,\n        # whether we have a decent distribution of do_explore values.\n        time_percentages = time_percentage_space.sample(100)\n        out = np.ndarray(shape=(100, 10), dtype=np.bool_)\n        for i, time_percentage in enumerate(time_percentages):\n            # Each time step, get epsilon decisions for a batch of samples.\n            sample = sample_space.sample(10)\n            out[i, :] = test.test((""do_explore"", [sample, time_percentage]))\n\n        # As we are going from epsilon 1.0 to 0.1, assert that we are slightly smaller than 0.5.\n        mean = out.mean()\n        self.assertAlmostEqual(mean, 0.6, places=1)\n        self.assertGreater(mean, 0.5)\n\n        # Take n samples at the end (+10000) of the exploration (almost no exploration).\n        time_percentages = time_percentage_space.sample(100) + 0.95\n        out = np.ndarray(shape=(100, 10), dtype=np.bool_)\n        for i, time_percentage in enumerate(time_percentages):\n            # Each time step, get epsilon decisions for a batch of samples.\n            sample = sample_space.sample(10)\n            out[i, :] = test.test((""do_explore"", [sample, time_percentage]))\n\n        # As we are going from epsilon 1.0 to 0.0, assert that we are slightly smaller than 0.5.\n        mean = out.mean()\n        self.assertAlmostEqual(mean, 0.15, places=1)\n\n    """"""\n    TAKE OUT FOR NOW: doesn\'t make sense to explore with a sequence of time-steps at the same time.\n    Will revisit when we look into multi-agent RL.\n    def test_epsilon_exploration_with_time_rank(self):\n        time_step_space = IntBox(add_time_rank=True)\n        sample_space = FloatBox(add_batch_rank=True, add_time_rank=True, time_major=True)\n\n        # The Component(s) to test.\n        decay_component = LinearDecay(from_=1.0, to_=0.0, start_timestep=0, num_timesteps=1000)\n        epsilon_component = EpsilonExploration(decay_spec=decay_component)\n        test = ComponentTest(component=epsilon_component, input_spaces=dict(\n            sample=sample_space, time_step=time_step_space\n        ))\n\n        # Values to pass at once (one time x batch pass).\n        time_steps = np.array([0, 1, 2, 25, 50, 100, 110, 112, 120, 130, 150, 180, 190, 195, 200, 201, 210, 250, 386,\n                               670, 789, 900, 923, 465, 894, 91, 1000])\n\n        # Only pass in sample (zeros) for the batch rank (5).\n        out = test.test((""do_explore"", [np.zeros(shape=(27, 5)), time_steps]), expected_outputs=None)\n        recursive_assert_almost_equal(out[0], [True] * 5)\n        recursive_assert_almost_equal(out[-1], [False] * 5)\n        mean = out.mean()\n        self.assertAlmostEqual(mean, 0.65, places=1)\n    """"""\n'"
rlgraph/tests/components/test_explorations.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport unittest\n\nimport numpy as np\nfrom six.moves import xrange as range_\n\nfrom rlgraph.components.action_adapters.categorical_distribution_adapter import CategoricalDistributionAdapter\nfrom rlgraph.components.component import Component\nfrom rlgraph.components.distributions import Categorical, Normal\nfrom rlgraph.components.explorations.exploration import Exploration\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.ops import DataOpDict\n\n\nclass TestExplorations(unittest.TestCase):\n\n    def test_exploration_with_discrete_action_space(self):\n        nn_output_space = FloatBox(shape=(13,), add_batch_rank=True)\n        time_percentage_space = FloatBox(add_batch_rank=True)\n        # 2x2 action-pick, each composite action with 5 categories.\n        action_space = IntBox(5, shape=(2, 2), add_batch_rank=True)\n\n        # Our distribution to go into the Exploration object.\n        distribution = Categorical()\n        action_adapter = CategoricalDistributionAdapter(action_space=action_space)\n\n        exploration = Exploration.from_spec(dict(\n            epsilon_spec=dict(\n                decay_spec=dict(\n                    type=""linear_decay"",\n                    from_=1.0,\n                    to_=0.0\n                )\n            )\n        ))\n        # The Component to test.\n        exploration_pipeline = Component(action_adapter, distribution, exploration, scope=""exploration-pipeline"")\n\n        @rlgraph_api(component=exploration_pipeline)\n        def get_action(self_, nn_output, time_percentage):\n            out = action_adapter.get_parameters(nn_output)\n            sample = distribution.sample_deterministic(out[""parameters""])\n            action = exploration.get_action(sample, time_percentage)\n            return action\n\n        test = ComponentTest(\n            component=exploration_pipeline,\n            input_spaces=dict(nn_output=nn_output_space, time_percentage=float),\n            action_space=action_space\n        )\n\n        # With exploration: Check, whether actions are equally distributed.\n        nn_outputs = nn_output_space.sample(2)\n        sample_size = 100\n        time_percentages = time_percentage_space.sample(sample_size)\n        # Collect action-batch-of-2 for each of our various random time steps.\n        # Each action is an int box of shape=(2,2)\n        actions = np.ndarray(shape=(sample_size, 2, 2, 2), dtype=np.int)\n        actions_wo_exploration = np.mean(test.test((""get_action"", [nn_outputs, 1.0])))\n        for i, time_percentage in enumerate(time_percentages):\n            actions[i] = test.test((""get_action"", [nn_outputs, time_percentage]), expected_outputs=None)\n\n        # Assert some distribution of the actions.\n        mean_action = actions.mean()\n        stddev_action = actions.std()\n        self.assertAlmostEqual(mean_action, 0.5 * 2.0 + 0.5 * actions_wo_exploration, places=1)\n        self.assertAlmostEqual(stddev_action, 1.0, places=0)\n\n        # Without exploration (epsilon is force-set to 0.0): Check, whether actions are always the same\n        # (given same nn_output all the time).\n        nn_outputs = nn_output_space.sample(2)\n        # +1.0: Use high time-percentage (>100%) to force no exploration.\n        time_percentages = time_percentage_space.sample(sample_size) + 1.0\n        # Collect action-batch-of-2 for each of our various random time steps.\n        # Each action is an int box of shape=(2,2)\n        actions = np.ndarray(shape=(sample_size, 2, 2, 2), dtype=np.int)\n        for i, time_percentage in enumerate(time_percentages):\n            actions[i] = test.test((""get_action"", [nn_outputs, time_percentage]), expected_outputs=None)\n\n        # Assert almost zero stddev of the single action components.\n        stddev_action_a = actions[:, 0, 0, 0].std()  # batch item 0, action-component (0,0)\n        self.assertAlmostEqual(stddev_action_a, 0.0, places=1)\n        stddev_action_b = actions[:, 1, 1, 0].std()  # batch item 1, action-component (1,0)\n        self.assertAlmostEqual(stddev_action_b, 0.0, places=1)\n        stddev_action_c = actions[:, 0, 0, 1].std()  # batch item 0, action-component (0,1)\n        self.assertAlmostEqual(stddev_action_c, 0.0, places=1)\n        stddev_action_d = actions[:, 1, 1, 1].std()  # batch item 1, action-component (1,1)\n        self.assertAlmostEqual(stddev_action_d, 0.0, places=1)\n        self.assertAlmostEqual(actions.std(), 1.0, places=0)\n\n    def test_exploration_with_discrete_container_action_space(self):\n        nn_output_space = FloatBox(shape=(12,), add_batch_rank=True)\n        time_percentage_space = FloatBox(add_batch_rank=True)\n        # Some container action space.\n        action_space = Dict(dict(a=IntBox(3), b=IntBox(2), c=IntBox(4)), add_batch_rank=True)\n\n        # Our distribution to go into the Exploration object.\n        distribution_a = Categorical(scope=""d_a"")\n        distribution_b = Categorical(scope=""d_b"")\n        distribution_c = Categorical(scope=""d_c"")\n        action_adapter_a = CategoricalDistributionAdapter(action_space=action_space[""a""], scope=""aa_a"")\n        action_adapter_b = CategoricalDistributionAdapter(action_space=action_space[""b""], scope=""aa_b"")\n        action_adapter_c = CategoricalDistributionAdapter(action_space=action_space[""c""], scope=""aa_c"")\n\n        exploration = Exploration.from_spec(dict(\n            epsilon_spec=dict(\n                decay_spec=dict(\n                    type=""linear_decay"",\n                    from_=1.0,\n                    to_=0.0\n                )\n            )\n        ))\n        # The Component to test.\n        exploration_pipeline = Component(\n            action_adapter_a, action_adapter_b, action_adapter_c, distribution_a, distribution_b, distribution_c,\n            exploration, scope=""exploration-pipeline""\n        )\n\n        @rlgraph_api(component=exploration_pipeline)\n        def get_action(self_, nn_output, time_percentage):\n            out_a = action_adapter_a.get_parameters(nn_output)\n            out_b = action_adapter_b.get_parameters(nn_output)\n            out_c = action_adapter_c.get_parameters(nn_output)\n            sample_a = distribution_a.sample_deterministic(out_a[""parameters""])\n            sample_b = distribution_b.sample_deterministic(out_b[""parameters""])\n            sample_c = distribution_c.sample_deterministic(out_c[""parameters""])\n            sample = self_._graph_fn_merge_actions(sample_a, sample_b, sample_c)\n            action = exploration.get_action(sample, time_percentage)\n            return action\n\n        @graph_fn(component=exploration_pipeline)\n        def _graph_fn_merge_actions(self, a, b, c):\n            return DataOpDict(a=a, b=b, c=c)\n\n        test = ComponentTest(\n            component=exploration_pipeline,\n            input_spaces=dict(nn_output=nn_output_space, time_percentage=float),\n            action_space=action_space\n        )\n\n        # With (random) exploration (in the beginning of decay interval): Check, whether actions are equally and\n        # more or less randomly distributed.\n        batch_size = 2\n        num_time_steps = 30\n        nn_outputs = nn_output_space.sample(batch_size)\n        time_percentages = np.maximum(time_percentage_space.sample(num_time_steps) - 0.8, 0)\n        # Collect action-batch-of-2 for each of our various random time steps.\n        actions_a = np.ndarray(shape=(num_time_steps, batch_size), dtype=np.int)\n        actions_b = np.ndarray(shape=(num_time_steps, batch_size), dtype=np.int)\n        actions_c = np.ndarray(shape=(num_time_steps, batch_size), dtype=np.int)\n        for i, t in enumerate(time_percentages):\n            a = test.test((""get_action"", [nn_outputs, t]), expected_outputs=None)\n            actions_a[i] = a[""a""]\n            actions_b[i] = a[""b""]\n            actions_c[i] = a[""c""]\n\n        # Assert some distribution of the actions.\n        mean_action_a = actions_a.mean()\n        stddev_action_a = actions_a.std()\n        self.assertAlmostEqual(mean_action_a, 1.0, places=0)\n        self.assertAlmostEqual(stddev_action_a, 1.0, places=0)\n        mean_action_b = actions_b.mean()\n        stddev_action_b = actions_b.std()\n        self.assertAlmostEqual(mean_action_b, 0.5, places=0)\n        self.assertAlmostEqual(stddev_action_b, 0.5, places=0)\n        mean_action_c = actions_c.mean()\n        stddev_action_c = actions_c.std()\n        self.assertAlmostEqual(mean_action_c, 1.5, places=0)\n        self.assertAlmostEqual(stddev_action_c, 1.0, places=0)\n\n        # Without exploration (epsilon is force-set to 0.0): Check, whether actions are always the same\n        # (given same nn_output all the time).\n        nn_outputs = nn_output_space.sample(batch_size)\n        time_percentages = time_percentage_space.sample(num_time_steps) + 1.0\n        # Collect action-batch-of-2 for each of our various random time steps.\n        actions_a = np.ndarray(shape=(num_time_steps, batch_size), dtype=np.int)\n        actions_b = np.ndarray(shape=(num_time_steps, batch_size), dtype=np.int)\n        actions_c = np.ndarray(shape=(num_time_steps, batch_size), dtype=np.int)\n        for i, t in enumerate(time_percentages):\n            a = test.test((""get_action"", [nn_outputs, t]), expected_outputs=None)\n            actions_a[i] = a[""a""]\n            actions_b[i] = a[""b""]\n            actions_c[i] = a[""c""]\n\n        # Assert zero stddev of the single action components.\n        stddev_action = actions_a[:, 0].std()  # batch item 0, action-component a\n        self.assertAlmostEqual(stddev_action, 0.0, places=1)\n        stddev_action = actions_a[:, 1].std()  # batch item 1, action-component a\n        self.assertAlmostEqual(stddev_action, 0.0, places=1)\n\n        stddev_action = actions_b[:, 0].std()  # batch item 0, action-component b\n        self.assertAlmostEqual(stddev_action, 0.0, places=1)\n        stddev_action = actions_b[:, 1].std()  # batch item 1, action-component b\n        self.assertAlmostEqual(stddev_action, 0.0, places=1)\n\n        stddev_action = actions_c[:, 0].std()  # batch item 0, action-component c\n        self.assertAlmostEqual(stddev_action, 0.0, places=1)\n        stddev_action = actions_c[:, 1].std()  # batch item 1, action-component c\n        self.assertAlmostEqual(stddev_action, 0.0, places=1)\n\n    def test_exploration_with_continuous_action_space(self):\n        # TODO not portable, redo with more general mean/stddev checks over a sample of distributed outputs.\n        return\n        # 2x2 action-pick, each composite action with 5 categories.\n        action_space = FloatBox(shape=(2,2), add_batch_rank=True)\n\n        distribution = Normal()\n        action_adapter = ActionAdapter(action_space=action_space)\n\n        # Our distribution to go into the Exploration object.\n        nn_output_space = FloatBox(shape=(13,), add_batch_rank=True)  # 13: Any flat nn-output should be ok.\n\n        exploration = Exploration.from_spec(dict(noise_spec=dict(type=""gaussian_noise"", mean=10.0, stddev=2.0)))\n\n        # The Component to test.\n        exploration_pipeline = Component(scope=""continuous-plus-noise"")\n        exploration_pipeline.add_components(action_adapter, distribution, exploration, scope=""exploration-pipeline"")\n\n        @rlgraph_api(component=exploration_pipeline)\n        def get_action(self_, nn_output):\n            parameters = action_adapter.get_adapter_outputs_and_parameters(nn_output)[""parameters""]\n            sample_stochastic = distribution.sample_stochastic(parameters)\n            sample_deterministic = distribution.sample_deterministic(parameters)\n            action = exploration.get_action(sample_stochastic, sample_deterministic)\n            return action\n\n        @rlgraph_api(component=exploration_pipeline)\n        def get_noise(self_):\n            return exploration.noise_component.get_noise()\n\n        test = ComponentTest(component=exploration_pipeline, input_spaces=dict(nn_output=nn_output_space),\n                             action_space=action_space)\n\n        # Collect outputs in `collected` list to compare moments.\n        collected = list()\n        for _ in range_(1000):\n            test.test(""get_noise"", fn_test=lambda component_test, outs: collected.append(outs))\n\n        self.assertAlmostEqual(10.0, np.mean(collected), places=1)\n        self.assertAlmostEqual(2.0, np.std(collected), places=1)\n\n        np.random.seed(10)\n        input_ = nn_output_space.sample(size=3)\n        expected = np.array([[[13.163095, 8.46925],\n                              [10.375976, 5.4675055]],\n                             [[13.239931, 7.990649],\n                              [10.03761, 10.465796]],\n                             [[10.280741, 7.2384844],\n                              [10.040194, 8.248206]]], dtype=np.float32)\n        test.test((""get_action"", input_), expected_outputs=expected, decimals=3)\n'"
rlgraph/tests/components/test_fifo_queue.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport threading\nimport time\nimport unittest\n\nfrom rlgraph.components.memories.fifo_queue import FIFOQueue\nfrom rlgraph.spaces import Dict, BoolBox, Tuple\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.utils.ops import flatten_op, unflatten_op\n\n\nclass TestFIFOQueue(unittest.TestCase):\n    """"""\n    Tests sampling and insertion behaviour of the FIFOQueue class.\n    """"""\n    record_space = Dict(\n        states=dict(state1=float, state2=float, state3=bool),\n        actions=dict(action1=float, action2=Tuple(float, float)),\n        reward=float,\n        terminals=BoolBox(),\n        add_batch_rank=True\n    )\n    capacity = 10\n\n    input_spaces = dict(\n        records=record_space,\n        num_records=int\n    )\n\n    def test_enqueue_dequeue(self):\n        """"""\n        Simply tests insert op without checking internal logic.\n        """"""\n        fifo_queue = FIFOQueue(capacity=self.capacity, record_space=self.record_space)\n        test = ComponentTest(component=fifo_queue, input_spaces=self.input_spaces)\n\n        first_record = self.record_space.sample(size=1)\n        test.test((""insert_records"", first_record), expected_outputs=None)\n        test.test(""get_size"", expected_outputs=1)\n\n        further_records = self.record_space.sample(size=5)\n        test.test((""insert_records"", further_records), expected_outputs=None)\n        test.test(""get_size"", expected_outputs=6)\n\n        expected = dict()\n        for (k1, v1), (k2, v2) in zip(flatten_op(first_record).items(), flatten_op(further_records).items()):\n            expected[k1] = np.concatenate((v1, v2[:4]))\n        expected = unflatten_op(expected)\n\n        test.test((""get_records"", 5), expected_outputs=expected)\n        test.test(""get_size"", expected_outputs=1)\n\n    def test_capacity(self):\n        """"""\n        Tests if insert correctly blocks when capacity is reached.\n        """"""\n        fifo_queue = FIFOQueue(capacity=self.capacity, record_space=self.record_space)\n        test = ComponentTest(component=fifo_queue, input_spaces=self.input_spaces)\n\n        def run(expected_):\n            # Wait n seconds.\n            time.sleep(2)\n            # Pull something out of the queue again to continue.\n            test.test((""get_records"", 2), expected_outputs=expected_)\n\n        # Insert one more element than capacity\n        records = self.record_space.sample(size=self.capacity + 1)\n\n        expected = dict()\n        for key, value in flatten_op(records).items():\n            expected[key] = value[:2]\n        expected = unflatten_op(expected)\n\n        # Start thread to save this one from getting stuck due to capacity overflow.\n        thread = threading.Thread(target=run, args=(expected,))\n        thread.start()\n\n        print(""Going over capacity: blocking ..."")\n        test.test((""insert_records"", records), expected_outputs=None)\n        print(""Dequeued some items in another thread. Unblocked."")\n\n        thread.join()\n\n    def test_fifo_queue_with_distributed_tf(self):\n        """"""\n        Tests if FIFO is correctly shared between two processes running in distributed tf.\n        """"""\n        cluster_spec = dict(source=[""localhost:22222""], target=[""localhost:22223""])\n\n        def run1():\n            fifo_queue_1 = FIFOQueue(capacity=self.capacity, device=""/job:source/task:0/cpu"",\n                                     record_space=self.record_space)\n            test_1 = ComponentTest(component=fifo_queue_1, input_spaces=self.input_spaces, execution_spec=dict(\n                mode=""distributed"",\n                distributed_spec=dict(job=""source"", task_index=0, cluster_spec=cluster_spec)\n            ))\n            # Insert elements from source.\n            records = self.record_space.sample(size=self.capacity)\n            print(""inserting into source-side queue ..."")\n            test_1.test((""insert_records"", records), expected_outputs=None)\n            print(""size of source-side queue:"")\n            print(test_1.test(""get_size"", expected_outputs=None))\n            # Pull one sample out.\n            print(""pulling from source-side queue:"")\n            print(test_1.test((""get_records"", 2), expected_outputs=None))\n\n            test_1.terminate()\n\n        def run2():\n            fifo_queue_2 = FIFOQueue(capacity=self.capacity, device=""/job:source/task:0/cpu"",\n                                     record_space=self.record_space)\n            test_2 = ComponentTest(component=fifo_queue_2, input_spaces=self.input_spaces, execution_spec=dict(\n                mode=""distributed"",\n                distributed_spec=dict(job=""target"", task_index=0, cluster_spec=cluster_spec)\n            ))\n            # Dequeue elements in target.\n            print(""size of target-side queue:"")\n            print(test_2.test(""get_size"", expected_outputs=None))\n            print(""pulling from target-side queue:"")\n            print(test_2.test((""get_records"", 5), expected_outputs=None))\n\n            test_2.terminate()\n\n        # Start thread to save this one from getting stuck due to capacity overflow.\n        thread_1 = threading.Thread(target=run1)\n        thread_2 = threading.Thread(target=run2)\n        thread_1.start()\n        thread_2.start()\n\n        thread_1.join()\n        thread_2.join()\n'"
rlgraph/tests/components/test_generalized_advantage_estimation.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\nfrom rlgraph.components.helpers import GeneralizedAdvantageEstimation\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest, recursive_assert_almost_equal\n\n\nclass TestGeneralizedAdvantageEstimation(unittest.TestCase):\n\n    gamma = 0.99\n    gae_lambda = 1.0\n\n    rewards = FloatBox(add_batch_rank=True)\n    baseline_values = FloatBox(add_batch_rank=True)\n    terminals = BoolBox(add_batch_rank=True)\n    sequence_indices = BoolBox(add_batch_rank=True)\n\n    input_spaces = dict(\n        rewards=rewards,\n        baseline_values=baseline_values,\n        terminals=terminals,\n        sequence_indices=sequence_indices\n    )\n\n    @staticmethod\n    def discount(x, gamma):\n        # Discounts a single sequence.\n        discounted = []\n        prev = 0\n        index = 0\n        # Apply discount to value.\n        for val in reversed(x):\n            decayed = prev + val * pow(gamma, index)\n            discounted.append(decayed)\n            index += 1\n            prev = decayed\n        return list(reversed(discounted))\n\n    @staticmethod\n    def discount_all(values, decay, terminal):\n        # Discounts multiple sub-sequences by keeping track of terminals.\n        discounted = []\n        i = len(values) - 1\n        prev_v = 0.0\n        for v in reversed(values):\n            # Arrived at new sequence, start over.\n            if np.all(terminal[i]):\n                print(""Resetting discount after processing i = "", i)\n                prev_v = 0.0\n\n            # Accumulate prior value.\n            accum_v = v + decay * prev_v\n            discounted.append(accum_v)\n            prev_v = accum_v\n\n            i -= 1\n        return list(reversed(discounted))\n\n    def gae_helper(self, baseline, reward, gamma, gae_lambda, terminals, sequence_indices):\n        # Bootstrap adjust.\n        deltas = []\n        start_index = 0\n        i = 0\n        sequence_indices[-1] = True\n        for _ in range(len(baseline)):\n            if np.all(sequence_indices[i]):\n                # Compute deltas for this subsequence.\n                # Cannot do this all at once because we would need the correct offsets for each sub-sequence.\n                baseline_slice = list(baseline[start_index:i + 1])\n\n                if np.all(terminals[i]):\n                    print(""Appending boot-strap val 0 at index."", i)\n                    baseline_slice.append(0)\n                else:\n                    print(""Appending boot-strap val {} at index {}."".format(baseline[i], i+1))\n                    baseline_slice.append(baseline[i])\n                adjusted_v = np.asarray(baseline_slice)\n\n                # +1 because we want to include i-th value.\n                delta = reward[start_index:i + 1] + gamma * adjusted_v[1:] - adjusted_v[:-1]\n                print(""Length for sequence deltas: "", len(delta))\n                deltas.extend(delta)\n                start_index = i + 1\n            i += 1\n\n        deltas = np.asarray(deltas)\n        print(""len deltas = "", len(deltas))\n        return np.asarray(self.discount_all(deltas, gamma * gae_lambda, terminals))\n\n    def test_with_manual_numbers_and_lambda_0_5(self):\n        lambda_ = 0.5\n        lg = lambda_ * self.gamma\n        gae = GeneralizedAdvantageEstimation(gae_lambda=lambda_, discount=self.gamma)\n\n        test = ComponentTest(component=gae, input_spaces=self.input_spaces)\n\n        # Batch of 2 sequences.\n        rewards_ = np.array([0.1, 0.2, 0.3])\n        baseline_values_ = np.array([1.0, 2.0, 3.0])\n        terminals_ = np.array([False, False, False])\n\n        # Final sequence index must always be true.\n        sequence_indices = np.array([False, False, True])\n        input_ = [baseline_values_, rewards_, terminals_, sequence_indices]\n\n        # Test TD-error outputs.\n        td = np.array([1.08, 1.17, 0.27])\n        test.test((""calc_td_errors"", input_), expected_outputs=td, decimals=5)\n\n        expected_gaes_manual = np.array([\n            td[0] + lg * td[1] + lg * lg * td[2],\n            td[1] + lg * td[2],\n            td[2]\n        ])\n        expected_gaes_helper = self.gae_helper(\n            baseline_values_, rewards_, self.gamma, lambda_, terminals_, sequence_indices\n        )\n        recursive_assert_almost_equal(expected_gaes_manual, expected_gaes_helper, decimals=5)\n        advantages = test.test((""calc_gae_values"", input_), expected_outputs=expected_gaes_manual)\n\n        print(""Rewards:"", rewards_)\n        print(""Baseline-values:"", baseline_values_)\n        print(""Terminals:"", terminals_)\n        print(""Expected advantage:"", expected_gaes_manual)\n        print(""Got advantage:"", advantages)\n\n        test.terminate()\n\n    def test_single_non_terminal_sequence(self):\n        gae = GeneralizedAdvantageEstimation(gae_lambda=self.gae_lambda, discount=self.gamma)\n\n        test = ComponentTest(component=gae, input_spaces=self.input_spaces)\n\n        rewards_ = self.rewards.sample(10)  #, fill_value=0.5)\n        baseline_values_ = self.baseline_values.sample(10)  #, fill_value=1.0)\n        terminals_ = self.terminals.sample(size=10, fill_value=False)\n\n        # Final sequence index must always be true.\n        sequence_indices = [False] * 10\n        # Assume sequence indices = terminals here.\n        input_ = [baseline_values_, rewards_, terminals_, sequence_indices]\n\n        advantage_expected = self.gae_helper(\n            baseline=baseline_values_,\n            reward=rewards_,\n            gamma=self.gamma,\n            gae_lambda=self.gae_lambda,\n            terminals=terminals_,\n            sequence_indices=sequence_indices\n        )\n\n        advantage = test.test((""calc_gae_values"", input_))\n        recursive_assert_almost_equal(advantage_expected, advantage, decimals=5)\n        print(""Rewards:"", rewards_)\n        print(""Baseline-values:"", baseline_values_)\n        print(""Terminals:"", terminals_)\n        print(""Expected advantage:"", advantage_expected)\n        print(""Got advantage:"", advantage)\n\n        test.terminate()\n\n    def test_multiple_sequences(self):\n        gae = GeneralizedAdvantageEstimation(gae_lambda=self.gae_lambda, discount=self.gamma)\n\n        test = ComponentTest(component=gae, input_spaces=self.input_spaces)\n\n        rewards_ = self.rewards.sample(10)  #, fill_value=0.5)\n        baseline_values_ = self.baseline_values.sample(10)  #, fill_value=1.0)\n        terminals_ = [False] * 10\n        terminals_[5] = True\n        sequence_indices = [False] * 10\n        sequence_indices[5] = True\n        terminals_ = np.asarray(terminals_)\n\n        input_ = [baseline_values_, rewards_, terminals_, sequence_indices]\n        advantage_expected = self.gae_helper(\n            baseline=baseline_values_,\n            reward=rewards_,\n            gamma=self.gamma,\n            gae_lambda=self.gae_lambda,\n            terminals=terminals_,\n            sequence_indices=sequence_indices\n        )\n\n        print(""Advantage expected:"", advantage_expected)\n        advantage = test.test((""calc_gae_values"", input_))\n        print(""Got advantage = "", advantage)\n        recursive_assert_almost_equal(advantage_expected, advantage, decimals=5)\n\n        test.terminate()\n'"
rlgraph/tests/components/test_impala_loss_function.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport unittest\n\nfrom rlgraph.components.loss_functions.impala_loss_function import IMPALALossFunction\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\n\n\nclass TestIMPALALossFunction(unittest.TestCase):\n\n    reward_space = FloatBox(add_batch_rank=True, add_time_rank=True, time_major=True)\n    terminal_space = BoolBox(add_batch_rank=True, add_time_rank=True, time_major=True)\n    values_space = FloatBox(shape=(1,), add_batch_rank=True, add_time_rank=True, time_major=True)\n    loss_per_item_space = FloatBox(add_batch_rank=True)\n\n    def test_impala_loss_function(self):\n        action_space = IntBox(4, shape=(), add_batch_rank=True)\n        impala_loss_function = IMPALALossFunction(discount=0.99)\n        action_probs_space = FloatBox(shape=(4,), add_batch_rank=True, add_time_rank=True, time_major=True)\n\n        test = ComponentTest(\n            component=impala_loss_function,\n            input_spaces=dict(\n                logits_actions_pi=action_probs_space,\n                action_probs_mu=action_probs_space,\n                values=self.values_space,\n                actions=action_space.with_extra_ranks(add_time_rank=True, time_major=True),\n                rewards=self.reward_space,\n                terminals=self.terminal_space,\n                loss_per_item=self.loss_per_item_space\n            ),\n            action_space=action_space\n        )\n\n        # Some crazy time/batch combination.\n        size = (14, 34)\n        size_state = (size[0]+1, size[1])  # States have one more slot (see env-stepper `step` output).\n        input_ = [\n            action_probs_space.sample(size=size_state),  # logits actions pi\n            action_probs_space.sample(size=size),  # action probs mu\n            self.values_space.sample(size=size_state),  # values\n            action_space.with_extra_ranks(add_time_rank=True, time_major=True).sample(size=size),  # actions\n            self.reward_space.sample(size=size),  # rewards\n            self.terminal_space.sample(size=size),  # terminals\n        ]\n\n        # Batch size=2 -> Expect 2 values in the `loss_per_item` out-Socket.\n        expected_loss_per_item = np.array([\n            3.163107, 0.7833158, 4.313544, 4.540819, 2.8402734, 4.001171, 3.4556258, 4.4281297, 4.2689576, 7.52999,\n            4.073623, 8.217271, 4.697727, 7.644105, 11.693808, 9.09968, 8.663768, 3.2075443, 6.819166, 4.017282,\n            0.96279377, 2.6984437, 1.3387702, 5.492561, 3.0185113, -0.1206184, 5.096336, 5.011886, 2.627573, 3.4739096,\n            6.6671352, 2.14344, 4.222275, 1.3773788\n        ], dtype=np.float32)\n\n        test.test((""loss_per_item"", input_), expected_outputs=expected_loss_per_item, decimals=5)\n        # Expect the mean over the batch.\n        expected_loss = expected_loss_per_item.mean()\n        test.test((""loss_average"", expected_loss_per_item), expected_outputs=expected_loss, decimals=5)\n        # Both.\n        test.test((""loss"", input_), expected_outputs=[expected_loss, expected_loss_per_item], decimals=5)\n\n        test.terminate()\n\n'"
rlgraph/tests/components/test_local_optimizers.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nfrom rlgraph.spaces import FloatBox\nfrom rlgraph.tests import ComponentTest, DummyWithOptimizer, recursive_assert_almost_equal\n\n\nclass TestLocalOptimizers(unittest.TestCase):\n\n    def test_calculate_gradients(self):\n        component = DummyWithOptimizer()\n\n        test = ComponentTest(component=component, input_spaces=dict(\n            input_=FloatBox(add_batch_rank=True), time_percentage=float\n        ))\n\n        expected_outputs = [0.73240823, 3.0]\n        test.test((""calc_grads""), expected_outputs=expected_outputs)\n\n    def test_apply_gradients(self):\n        component = DummyWithOptimizer(variable_value=2.0)\n\n        test = ComponentTest(component=component, input_spaces=dict(\n            input_=FloatBox(add_batch_rank=True), time_percentage=float\n        ))\n\n        expected_grad = 0.69314718\n        expected_outputs = [expected_grad, 2.0]\n        test.test(""calc_grads"", expected_outputs=expected_outputs)\n\n        # Now apply the grad and check the variable value.\n        var_values_before = test.read_variable_values(component.variable_registry)\n        test.test(""step"")\n\n        # Check against variable now. Should change by -learning_rate*grad.\n        var_values_after = test.read_variable_values(component.variable_registry)\n        expected_new_value = var_values_before[""dummy-with-optimizer/variable""] - (\n            component.optimizer.learning_rate.from_ * expected_grad\n        )\n        recursive_assert_almost_equal(\n            var_values_after[""dummy-with-optimizer/variable""], expected_new_value, decimals=5\n        )\n\n'"
rlgraph/tests/components/test_multi_input_stream_nn.py,0,"b'# Copyright 2018/2019 ducandu GmbH. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\nfrom rlgraph.components.neural_networks.multi_input_stream_neural_network import MultiInputStreamNeuralNetwork\nfrom rlgraph.spaces import FloatBox, IntBox, Dict, Tuple\nfrom rlgraph.tests.component_test import ComponentTest\nfrom rlgraph.utils.numpy import dense_layer, one_hot\n\n\nclass TestMultiInputStreamNeuralNetwork(unittest.TestCase):\n    """"""\n    Tests for the VariationalAutoEncoder class.\n    """"""\n    def test_multi_input_stream_neural_network_with_tuple(self):\n        # Space must contain batch dimension (otherwise, NNLayer will complain).\n        input_space = Tuple(\n            IntBox(3, shape=()),\n            FloatBox(shape=(8,)),\n            IntBox(4, shape=()),\n            add_batch_rank=True\n        )\n\n        multi_input_nn = MultiInputStreamNeuralNetwork(\n            input_network_specs=(\n                [{""type"": ""reshape"", ""flatten"": True, ""flatten_categories"": True}],  # intbox -> flatten\n                [{""type"": ""dense"", ""units"": 2}],  # floatbox -> dense\n                [{""type"": ""reshape"", ""flatten"": True, ""flatten_categories"": True}]  # inbox -> flatten\n            ),\n            post_network_spec=[{""type"": ""dense"", ""units"": 3}],\n        )\n\n        test = ComponentTest(component=multi_input_nn, input_spaces=dict(inputs=input_space))\n\n        # Batch of size=n.\n        nn_inputs = input_space.sample(3)\n\n        global_scope_pre = ""multi-input-stream-nn/input-stream-nn-""\n        global_scope_post = ""multi-input-stream-nn/post-concat-nn/dense-layer/dense/""\n        # Calculate output manually.\n        var_dict = test.read_variable_values()\n\n        flat_0 = one_hot(nn_inputs[0], depth=3)\n        dense_1 = dense_layer(\n            nn_inputs[1], var_dict[global_scope_pre+""1/dense-layer/dense/kernel""],\n            var_dict[global_scope_pre+""1/dense-layer/dense/bias""]\n        )\n        flat_2 = one_hot(nn_inputs[2], depth=4)\n        concat_out = np.concatenate((flat_0, dense_1, flat_2), axis=-1)\n        expected = dense_layer(concat_out, var_dict[global_scope_post+""kernel""], var_dict[global_scope_post+""bias""])\n\n        test.test((""call"", tuple([nn_inputs])), expected_outputs=expected)\n\n        test.terminate()\n\n    def test_multi_input_stream_neural_network_with_dict(self):\n        # Space must contain batch dimension (otherwise, NNlayer will complain).\n        input_space = Dict(\n            a=FloatBox(shape=(3,)),\n            b=IntBox(4, shape=()),\n            add_batch_rank=True\n        )\n\n        multi_input_nn = MultiInputStreamNeuralNetwork(\n            input_network_specs=dict(\n                a=[],\n                b=[{""type"": ""reshape"", ""flatten"": True, ""flatten_categories"": True}]\n            ),\n            post_network_spec=[{""type"": ""dense"", ""units"": 2}],\n        )\n\n        test = ComponentTest(component=multi_input_nn, input_spaces=dict(inputs=input_space))\n\n        # Batch of size=n.\n        nn_inputs = input_space.sample(5)\n\n        global_scope = ""multi-input-stream-nn/post-concat-nn/dense-layer/dense/""\n        # Calculate output manually.\n        var_dict = test.read_variable_values()\n\n        b_flat = one_hot(nn_inputs[""b""], depth=4)\n        concat_out = np.concatenate((nn_inputs[""a""], b_flat), axis=-1)\n        expected = dense_layer(concat_out, var_dict[global_scope+""kernel""], var_dict[global_scope+""bias""])\n\n        test.test((""call"", nn_inputs), expected_outputs=expected)\n\n        test.terminate()\n'"
rlgraph/tests/components/test_neural_networks.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\n\nfrom rlgraph.components.layers.nn.dense_layer import DenseLayer\nfrom rlgraph.components.layers.nn.lstm_layer import LSTMLayer\nfrom rlgraph.components.neural_networks import NeuralNetwork\nfrom rlgraph.spaces import FloatBox, Tuple\nfrom rlgraph.tests.component_test import ComponentTest\nfrom rlgraph.tests.test_util import config_from_path\nfrom rlgraph.utils.numpy import dense_layer, lstm_layer\n\n\nclass TestNeuralNetworks(unittest.TestCase):\n    """"""\n    Tests for assembling from json and running different NeuralNetworks.\n    """"""\n    def test_simple_nn_using_layers(self):\n        # Space must contain batch dimension (otherwise, NNlayer will complain).\n        space = FloatBox(shape=(4,), add_batch_rank=True)\n\n        # Create a simple neural net from json.\n        nn_layers = config_from_path(""configs/test_simple_nn.json"")\n        neural_net = NeuralNetwork(*nn_layers[""layers""])\n\n        # Do not seed, we calculate expectations manually.\n        test = ComponentTest(component=neural_net, input_spaces=dict(inputs=space))\n\n        # Batch of size=3.\n        input_ = space.sample(4)\n        # Calculate output manually.\n        var_dict = neural_net.get_variables(""hidden-layer/dense/kernel"", ""hidden-layer/dense/bias"", global_scope=False)\n        w1_value = test.read_variable_values(var_dict[""hidden-layer/dense/kernel""])\n        b1_value = test.read_variable_values(var_dict[""hidden-layer/dense/bias""])\n\n        expected = dense_layer(input_, w1_value, b1_value)\n\n        test.test((""call"", input_), expected_outputs=expected, decimals=5)\n\n        test.terminate()\n\n    def test_simple_nn_using_from_spec(self):\n        # Space must contain batch dimension (otherwise, NNlayer will complain).\n        space = FloatBox(shape=(3,), add_batch_rank=True)\n\n        # Create a simple neural net from json.\n        neural_net = NeuralNetwork.from_spec(config_from_path(""configs/test_simple_nn.json""))  # type: NeuralNetwork\n\n        # Do not seed, we calculate expectations manually.\n        test = ComponentTest(component=neural_net, input_spaces=dict(inputs=space))\n\n        # Batch of size=3.\n        input_ = np.array([[0.1, 0.2, 0.3], [1.0, 2.0, 3.0], [10.0, 20.0, 30.0]])\n        # Calculate output manually.\n        var_dict = neural_net.get_variables(""hidden-layer/dense/kernel"", ""hidden-layer/dense/bias"", global_scope=False)\n        w1_value = test.read_variable_values(var_dict[""hidden-layer/dense/kernel""])\n        b1_value = test.read_variable_values(var_dict[""hidden-layer/dense/bias""])\n\n        expected = dense_layer(input_, w1_value, b1_value)\n\n        test.test((""call"", input_), expected_outputs=expected, decimals=5)\n\n        test.terminate()\n\n    def test_add_layer_to_simple_nn(self):\n        # Space must contain batch dimension (otherwise, NNlayer will complain).\n        space = FloatBox(shape=(3,), add_batch_rank=True)\n\n        # Create a simple neural net from json.\n        neural_net = NeuralNetwork.from_spec(config_from_path(""configs/test_simple_nn.json""))  # type: NeuralNetwork\n        # Add another layer to it.\n        neural_net.add_layer(DenseLayer(units=10, scope=""last-layer""))\n\n        # Do not seed, we calculate expectations manually.\n        test = ComponentTest(component=neural_net, input_spaces=dict(inputs=space))\n\n        # Batch of size=3.\n        input_ = space.sample(3)\n        # Calculate output manually.\n        var_dict = test.read_variable_values(neural_net.variable_registry)\n\n        expected = dense_layer(\n            dense_layer(input_, var_dict[""test-network/hidden-layer/dense/kernel""],\n                        var_dict[""test-network/hidden-layer/dense/bias""]),\n            var_dict[""test-network/last-layer/dense/kernel""], var_dict[""test-network/last-layer/dense/bias""]\n        )\n\n        test.test((""call"", input_), expected_outputs=expected, decimals=5)\n\n        test.terminate()\n\n    def test_lstm_nn(self):\n        # Space must contain batch dimension (otherwise, NNlayer will complain).\n        #units = 3\n        batch_size = 2\n        time_steps = 4\n        input_nodes = 2\n        input_space = FloatBox(shape=(input_nodes,), add_batch_rank=True, add_time_rank=True)\n        #internal_states_space = Tuple(FloatBox(shape=(units,)), FloatBox(shape=(units,)), add_batch_rank=True)\n\n        neural_net = NeuralNetwork.from_spec(config_from_path(""configs/test_dense_to_lstm_nn.json""))\n\n        # Do not seed, we calculate expectations manually.\n        test = ComponentTest(component=neural_net, input_spaces=dict(\n            inputs=input_space\n        ))\n\n        # Batch of size=2, time-steps=3.\n        input_ = input_space.sample((batch_size, time_steps))\n\n        # Calculate output manually.\n        w0_value = test.read_variable_values(neural_net.variable_registry[""test-lstm-network/dense-layer/dense/kernel""])\n        b0_value = test.read_variable_values(neural_net.variable_registry[""test-lstm-network/dense-layer/dense/bias""])\n        lstm_w_value = test.read_variable_values(neural_net.variable_registry[""test-lstm-network/lstm-layer/lstm-cell/kernel""])\n        lstm_b_value = test.read_variable_values(neural_net.variable_registry[""test-lstm-network/lstm-layer/lstm-cell/bias""])\n\n        d0_out = dense_layer(input_, w0_value, b0_value)\n        lstm_out, last_internal_states = lstm_layer(d0_out, lstm_w_value, lstm_b_value, time_major=False)\n\n        expected = [lstm_out, last_internal_states]\n        test.test((""call"", input_), expected_outputs=tuple(expected), decimals=5)\n\n        test.terminate()\n\n    def test_lstm_nn_with_custom_call(self):\n        # Space must contain batch dimension (otherwise, NNlayer will complain).\n        units = 3\n        batch_size = 2\n        time_steps = 4\n        input_nodes = 2\n        input_space = FloatBox(shape=(input_nodes,), add_batch_rank=True, add_time_rank=True)\n        internal_states_space = Tuple(FloatBox(shape=(units,)), FloatBox(shape=(units,)), add_batch_rank=True)\n\n        def custom_call(self, input_, internal_states=None):\n            d0_out = self.get_sub_component_by_name(""d0"").call(input_)\n            lstm_out = self.get_sub_component_by_name(""lstm"").call(d0_out, internal_states)\n            d1_out = self.get_sub_component_by_name(""d1"").call(lstm_out[0])\n            return d1_out, lstm_out[1]\n\n        # Create a simple neural net with the above custom API-method.\n        neural_net = NeuralNetwork(\n            DenseLayer(units, scope=""d0""),\n            LSTMLayer(units, scope=""lstm""),\n            DenseLayer(units, scope=""d1""),\n            api_methods={(""call"", custom_call)}\n        )\n\n        # Do not seed, we calculate expectations manually.\n        test = ComponentTest(component=neural_net, input_spaces=dict(\n            input_=input_space, internal_states=internal_states_space\n        ))\n\n        # Batch of size=2, time-steps=3.\n        input_ = input_space.sample((batch_size, time_steps))\n        internal_states = internal_states_space.sample(batch_size)\n\n        # Calculate output manually.\n        w0_value = test.read_variable_values(neural_net.variable_registry[""neural-network/d0/dense/kernel""])\n        b0_value = test.read_variable_values(neural_net.variable_registry[""neural-network/d0/dense/bias""])\n        w1_value = test.read_variable_values(neural_net.variable_registry[""neural-network/d1/dense/kernel""])\n        b1_value = test.read_variable_values(neural_net.variable_registry[""neural-network/d1/dense/bias""])\n        lstm_w_value = test.read_variable_values(neural_net.variable_registry[""neural-network/lstm/lstm-cell/kernel""])\n        lstm_b_value = test.read_variable_values(neural_net.variable_registry[""neural-network/lstm/lstm-cell/bias""])\n\n        d0_out = dense_layer(input_, w0_value, b0_value)\n        lstm_out, last_internal_states = lstm_layer(\n            d0_out, lstm_w_value, lstm_b_value, initial_internal_states=internal_states, time_major=False\n        )\n        d1_out = dense_layer(lstm_out, w1_value, b1_value)\n\n        expected = [d1_out, last_internal_states]\n        test.test((""call"", [input_, internal_states]), expected_outputs=tuple(expected), decimals=5)\n\n        test.terminate()\n\n    def test_dictionary_input_nn(self):\n        pass\n'"
rlgraph/tests/components/test_neural_networks_keras_style_assembly.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport unittest\n\nimport numpy as np\nfrom rlgraph.components.layers.nn import DenseLayer, LSTMLayer, ConcatLayer, Conv2DLayer\nfrom rlgraph.components.layers.preprocessing.reshape import ReShape\nfrom rlgraph.components.layers.strings import StringToHashBucket, EmbeddingLookup\nfrom rlgraph.components.neural_networks import NeuralNetwork\nfrom rlgraph.spaces import FloatBox, TextBox, IntBox, Tuple, Dict\nfrom rlgraph.tests.component_test import ComponentTest\nfrom rlgraph.utils.numpy import dense_layer, relu, lstm_layer, one_hot\n\n\nclass TestNeuralNetworkKerasStyleAssembly(unittest.TestCase):\n    """"""\n    Tests for assembling from json and running different NeuralNetworks.\n    """"""\n    def test_keras_style_simple_nn(self):\n        # Input Space of the network.\n        input_space = FloatBox(shape=(3,), add_batch_rank=True)\n\n        # Create a DenseLayer with a fixed `call` method input space for the arg `inputs`.\n        output1 = DenseLayer(units=5, activation=""linear"", scope=""a"")(input_space)\n        # Create a DenseLayer whose `inputs` arg is the resulting DataOpRec of output1\'s `call` output.\n        output2 = DenseLayer(units=7, activation=""relu"", scope=""b"")(output1)\n\n        # This will trace back automatically through the given output DataOpRec(s) and add all components\n        # on the way to the input-space to this network.\n        neural_net = NeuralNetwork(outputs=output2)\n\n        test = ComponentTest(component=neural_net, input_spaces=dict(inputs=input_space))\n\n        # Batch of size=n.\n        input_ = input_space.sample(5)\n        # Calculate output manually.\n        var_dict = neural_net.get_variables(""a/dense/kernel"", ""a/dense/bias"", ""b/dense/kernel"", ""b/dense/bias"", global_scope=False)\n        w1_value = test.read_variable_values(var_dict[""a/dense/kernel""])\n        b1_value = test.read_variable_values(var_dict[""a/dense/bias""])\n        w2_value = test.read_variable_values(var_dict[""b/dense/kernel""])\n        b2_value = test.read_variable_values(var_dict[""b/dense/bias""])\n\n        expected = relu(dense_layer(dense_layer(input_, w1_value, b1_value), w2_value, b2_value))\n\n        test.test((""call"", input_), expected_outputs=expected, decimals=5)\n\n        test.terminate()\n\n    def test_keras_style_one_output_is_discarded(self):\n        # Input Space of the network.\n        input_space = FloatBox(shape=(3,), add_batch_rank=True, add_time_rank=True)\n\n        # Pass input through an LSTM and get two outputs (output and internal states), only one of which will be used.\n        lstm_out, _ = LSTMLayer(units=2, return_sequences=False)(input_space)\n\n        # A NN with 1 output (don\'t return internal_states of LSTM).\n        neural_net = NeuralNetwork(outputs=lstm_out)\n\n        test = ComponentTest(component=neural_net, input_spaces=dict(inputs=input_space))\n\n        # Batch of size=n.\n        input_ = input_space.sample((5, 3))\n        # Calculate output manually.\n        var_dict = neural_net.variable_registry\n        w1_value = test.read_variable_values(var_dict[""neural-network/lstm-layer/lstm-cell/kernel""])\n        b1_value = test.read_variable_values(var_dict[""neural-network/lstm-layer/lstm-cell/bias""])\n\n        expected_out, _ = lstm_layer(input_, w1_value, b1_value)\n        expected_out = expected_out[:, -1, :]  # last time step only\n\n        # Don\'t expect internal states (our NN does not return these as per the functional API definition above).\n        test.test((""call"", input_), expected_outputs=expected_out, decimals=5)\n\n        test.terminate()\n\n    def test_keras_style_two_separate_input_spaces(self):\n        # Define two input Spaces first. Independently (no container).\n        input_space_1 = IntBox(3, add_batch_rank=True)\n        input_space_2 = FloatBox(shape=(4,), add_batch_rank=True)\n\n        # One-hot flatten the int tensor.\n        flatten_layer_out = ReShape(flatten=True, flatten_categories=True)(input_space_1)\n        # Run the float tensor through two dense layers.\n        dense_1_out = DenseLayer(units=3, scope=""d1"")(input_space_2)\n        dense_2_out = DenseLayer(units=5, scope=""d2"")(dense_1_out)\n        # Concat everything.\n        cat_out = ConcatLayer()(flatten_layer_out, dense_2_out)\n\n        # Use the `outputs` arg to allow your network to trace back the data flow until the input space.\n        neural_net = NeuralNetwork(inputs=[input_space_1, input_space_2], outputs=cat_out)\n\n        test = ComponentTest(component=neural_net, input_spaces=dict(inputs=[input_space_1, input_space_2]))\n\n        var_dict = neural_net.variable_registry\n        w1_value = test.read_variable_values(var_dict[""neural-network/d1/dense/kernel""])\n        b1_value = test.read_variable_values(var_dict[""neural-network/d1/dense/bias""])\n        w2_value = test.read_variable_values(var_dict[""neural-network/d2/dense/kernel""])\n        b2_value = test.read_variable_values(var_dict[""neural-network/d2/dense/bias""])\n\n        # Batch of size=n.\n        input_ = [input_space_1.sample(4), input_space_2.sample(4)]\n\n        expected = np.concatenate([  # concat everything\n            one_hot(input_[0]),  # int flattening\n            dense_layer(dense_layer(input_[1], w1_value, b1_value), w2_value, b2_value)  # float -> 2 x dense\n        ], axis=-1)\n        out = test.test((""call"", input_), expected_outputs=expected)\n\n        test.terminate()\n\n    def test_keras_style_one_container_input_space(self):\n        # Define one container input Space.\n        input_space = Tuple(IntBox(3), FloatBox(shape=(4,)), add_batch_rank=True)\n\n        # One-hot flatten the int tensor.\n        flatten_layer_out = ReShape(flatten=True, flatten_categories=True)(input_space[0])\n        # Run the float tensor through two dense layers.\n        dense_1_out = DenseLayer(units=3, scope=""d1"")(input_space[1])\n        dense_2_out = DenseLayer(units=5, scope=""d2"")(dense_1_out)\n        # Concat everything.\n        cat_out = ConcatLayer()(flatten_layer_out, dense_2_out)\n\n        # Use the `outputs` arg to allow your network to trace back the data flow until the input space.\n        # `inputs` is not needed  here as we only have one single input (the Tuple).\n        neural_net = NeuralNetwork(outputs=cat_out)\n\n        test = ComponentTest(component=neural_net, input_spaces=dict(inputs=input_space))\n\n        var_dict = neural_net.variable_registry\n        w1_value = test.read_variable_values(var_dict[""neural-network/d1/dense/kernel""])\n        b1_value = test.read_variable_values(var_dict[""neural-network/d1/dense/bias""])\n        w2_value = test.read_variable_values(var_dict[""neural-network/d2/dense/kernel""])\n        b2_value = test.read_variable_values(var_dict[""neural-network/d2/dense/bias""])\n\n        # Batch of size=n.\n        input_ = input_space.sample(4)\n\n        expected = np.concatenate([  # concat everything\n            one_hot(input_[0]),  # int flattening\n            dense_layer(dense_layer(input_[1], w1_value, b1_value), w2_value, b2_value)  # float -> 2 x dense\n        ], axis=-1)\n        out = test.test((""call"", tuple([input_])), expected_outputs=expected)\n\n        test.terminate()\n\n    def test_keras_style_multi_stream_nn(self):\n        # Input Space of the network.\n        input_space = Dict({\n            ""img"": FloatBox(shape=(6, 6, 3)),  # some RGB img\n            ""txt"": TextBox()  # some text\n        }, add_batch_rank=True, add_time_rank=True)\n\n        # Complex NN assembly via our Keras-style functional API.\n        # Fold text input into single batch rank.\n        folded_text = ReShape(fold_time_rank=True)(input_space[""txt""])\n        # String layer will create batched AND time-ranked (individual words) hash outputs (int64).\n        string_bucket_out, lengths = StringToHashBucket(num_hash_buckets=5)(folded_text)\n        # Batched and time-ranked embedding output (floats) with embed dim=n.\n        embedding_out = EmbeddingLookup(embed_dim=10, vocab_size=5)(string_bucket_out)\n        # Pass embeddings through a text LSTM and use last output (reduce time-rank).\n        string_lstm_out, _ = LSTMLayer(units=2, return_sequences=False, scope=""lstm-layer-txt"")(\n            embedding_out, sequence_length=lengths\n        )\n        # Unfold to get original time-rank back.\n        string_lstm_out_unfolded = ReShape(unfold_time_rank=True)(string_lstm_out, input_space[""txt""])\n\n        # Parallel image stream via 1 CNN layer plus dense.\n        folded_img = ReShape(fold_time_rank=True, scope=""img-fold"")(input_space[""img""])\n        cnn_out = Conv2DLayer(filters=1, kernel_size=2, strides=2)(folded_img)\n        unfolded_cnn_out = ReShape(unfold_time_rank=True, scope=""img-unfold"")(cnn_out, input_space[""img""])\n        unfolded_cnn_out_flattened = ReShape(flatten=True, scope=""img-flat"")(unfolded_cnn_out)\n        dense_out = DenseLayer(units=2, scope=""dense-0"")(unfolded_cnn_out_flattened)\n\n        # Concat everything.\n        concat_out = ConcatLayer()(string_lstm_out_unfolded, dense_out)\n\n        # LSTM output has batch+time.\n        main_lstm_out, internal_states = LSTMLayer(units=2, scope=""lstm-layer-main"")(concat_out)\n\n        dense1_after_lstm_out = DenseLayer(units=3, scope=""dense-1"")(main_lstm_out)\n        dense2_after_lstm_out = DenseLayer(units=2, scope=""dense-2"")(dense1_after_lstm_out)\n        dense3_after_lstm_out = DenseLayer(units=1, scope=""dense-3"")(dense2_after_lstm_out)\n\n        # A NN with 3 outputs.\n        neural_net = NeuralNetwork(outputs=[dense3_after_lstm_out, main_lstm_out, internal_states])\n\n        test = ComponentTest(component=neural_net, input_spaces=dict(inputs=input_space))\n\n        # Batch of size=n.\n        sample_shape = (4, 2)\n        input_ = input_space.sample(sample_shape)\n\n        out = test.test((""call"", input_), expected_outputs=None)\n        # Main output (Dense out after LSTM).\n        self.assertTrue(out[0].shape == sample_shape + (1,))  # 1=1 unit in dense layer\n        self.assertTrue(out[0].dtype == np.float32)\n        # main-LSTM out.\n        self.assertTrue(out[1].shape == sample_shape + (2,))  # 2=2 LSTM units\n        self.assertTrue(out[1].dtype == np.float32)\n        # main-LSTM internal-states.\n        self.assertTrue(out[2][0].shape == sample_shape[:1] + (2,))  # 2=2 LSTM units\n        self.assertTrue(out[2][0].dtype == np.float32)\n        self.assertTrue(out[2][1].shape == sample_shape[:1] + (2,))  # 2=2 LSTM units\n        self.assertTrue(out[2][1].dtype == np.float32)\n\n        test.terminate()\n\n    def test_keras_style_complex_multi_stream_nn(self):\n        # 3 inputs.\n        input_spaces = [\n            Dict({\n                ""img"": FloatBox(shape=(6, 6, 3)),\n                ""int"": IntBox(3)\n            }, add_batch_rank=True, add_time_rank=True),\n            FloatBox(shape=(2,), add_batch_rank=True),\n            Tuple(IntBox(2), TextBox(), add_batch_rank=True, add_time_rank=True)\n        ]\n\n        # Same NN as in test above, only using some of the sub-Spaces from the input spaces.\n        # Tests whether this NN can add automatically the correct splitters.\n        folded_text = ReShape(fold_time_rank=True)(input_spaces[2][1])\n        # String layer will create batched AND time-ranked (individual words) hash outputs (int64).\n        string_bucket_out, lengths = StringToHashBucket(num_hash_buckets=5)(folded_text)\n        # Batched and time-ranked embedding output (floats) with embed dim=n.\n        embedding_out = EmbeddingLookup(embed_dim=10, vocab_size=5)(string_bucket_out)\n        # Pass embeddings through a text LSTM and use last output (reduce time-rank).\n        string_lstm_out, _ = LSTMLayer(units=2, return_sequences=False, scope=""lstm-layer-txt"")(\n            embedding_out, sequence_length=lengths\n        )\n        # Unfold to get original time-rank back.\n        string_lstm_out_unfolded = ReShape(unfold_time_rank=True)(string_lstm_out, input_spaces[2][1])\n\n        # Parallel image stream via 1 CNN layer plus dense.\n        folded_img = ReShape(fold_time_rank=True, scope=""img-fold"")(input_spaces[0][""img""])\n        cnn_out = Conv2DLayer(filters=1, kernel_size=2, strides=2)(folded_img)\n        unfolded_cnn_out = ReShape(unfold_time_rank=True, scope=""img-unfold"")(cnn_out, input_spaces[0][""img""])\n        unfolded_cnn_out_flattened = ReShape(flatten=True, scope=""img-flat"")(unfolded_cnn_out)\n        dense_out = DenseLayer(units=2, scope=""dense-0"")(unfolded_cnn_out_flattened)\n\n        # Concat everything.\n        concat_out = ConcatLayer()(string_lstm_out_unfolded, dense_out)\n\n        # LSTM output has batch+time.\n        main_lstm_out, internal_states = LSTMLayer(units=2, scope=""lstm-layer-main"")(concat_out)\n\n        dense1_after_lstm_out = DenseLayer(units=3, scope=""dense-1"")(main_lstm_out)\n        dense2_after_lstm_out = DenseLayer(units=2, scope=""dense-2"")(dense1_after_lstm_out)\n        dense3_after_lstm_out = DenseLayer(units=1, scope=""dense-3"")(dense2_after_lstm_out)\n\n        # A NN with 3 outputs.\n        neural_net = NeuralNetwork(inputs=input_spaces, outputs=[dense3_after_lstm_out, main_lstm_out, internal_states])\n\n        test = ComponentTest(component=neural_net, input_spaces=dict(inputs=input_spaces))\n\n        # Batch of size=n.\n        sample_shape = (4, 2)\n        input_ = [input_spaces[0].sample(sample_shape), input_spaces[1].sample(sample_shape[0]),\n                  input_spaces[2].sample(sample_shape)]\n\n        out = test.test((""call"", tuple(input_)), expected_outputs=None)\n        # Main output (Dense out after LSTM).\n        self.assertTrue(out[0].shape == sample_shape + (1,))  # 1=1 unit in dense layer\n        self.assertTrue(out[0].dtype == np.float32)\n        # main-LSTM out.\n        self.assertTrue(out[1].shape == sample_shape + (2,))  # 2=2 LSTM units\n        self.assertTrue(out[1].dtype == np.float32)\n        # main-LSTM internal-states.\n        self.assertTrue(out[2][0].shape == sample_shape[:1] + (2,))  # 2=2 LSTM units\n        self.assertTrue(out[2][0].dtype == np.float32)\n        self.assertTrue(out[2][1].shape == sample_shape[:1] + (2,))  # 2=2 LSTM units\n        self.assertTrue(out[2][1].dtype == np.float32)\n\n        test.terminate()\n'"
rlgraph/tests/components/test_nn_layers.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\n\nfrom rlgraph.components.layers.nn import NNLayer, DenseLayer, Conv2DLayer, ConcatLayer, MaxPool2DLayer, \\\n    LSTMLayer, ResidualLayer, LocalResponseNormalizationLayer, MultiLSTMLayer\nfrom rlgraph.spaces import FloatBox, Dict, Tuple\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.utils.numpy import sigmoid, relu, lstm_layer\n\n\nclass TestNNLayer(unittest.TestCase):\n    """"""\n    Tests for the different NNLayer Components. Each layer is tested separately.\n    """"""\n    def test_dummy_nn_layer(self):\n        # Tests simple pass through (no activation, no layer (graph_fn) computation).\n        space = FloatBox(shape=(3,), add_batch_rank=True)\n\n        # - fixed 1.0 weights, no biases\n        dummy_layer = NNLayer(activation=None)\n        test = ComponentTest(component=dummy_layer, input_spaces=dict(inputs=space))\n\n        input_ = space.sample(size=5)\n        test.test((""call"", input_), expected_outputs=input_)\n\n    def test_activation_functions(self):\n        # Test single activation functions (no other custom computations in layer).\n        space = FloatBox(shape=(3,), add_batch_rank=True)\n\n        # ReLU.\n        relu_layer = NNLayer(activation=""relu"")\n        test = ComponentTest(component=relu_layer, input_spaces=dict(inputs=space))\n\n        input_ = space.sample(size=5)\n        expected = relu(input_)\n        test.test((""call"", input_), expected_outputs=expected)\n\n        # Again manually in case util numpy-relu is broken.\n        input_ = np.array([[1.0, 2.0, -5.0], [-10.0, -100.1, 4.5]])\n        expected = np.array([[1.0, 2.0, 0.0], [0.0, 0.0, 4.5]])\n        test.test((""call"", input_), expected_outputs=expected)\n\n        # Sigmoid.\n        sigmoid_layer = NNLayer(activation=""sigmoid"")\n        test = ComponentTest(component=sigmoid_layer, input_spaces=dict(inputs=space))\n\n        input_ = space.sample(size=10)\n        expected = sigmoid(input_)\n        test.test((""call"", input_), expected_outputs=expected)\n\n    def test_dense_layer(self):\n        # Space must contain batch dimension (otherwise, NNLayer will complain).\n        space = FloatBox(shape=(2,), add_batch_rank=True)\n\n        # - fixed 1.0 weights, no biases\n        dense_layer = DenseLayer(units=2, weights_spec=1.0, biases_spec=False)\n        test = ComponentTest(component=dense_layer, input_spaces=dict(inputs=space))\n\n        # Batch of size=1 (can increase this to any larger number).\n        input_ = np.array([[0.5, 2.0]])\n        expected = np.array([[2.5, 2.5]])\n        test.test((""call"", input_), expected_outputs=expected)\n\n    def test_dense_layer_with_leaky_relu_activation(self):\n        input_space = FloatBox(shape=(3,), add_batch_rank=True)\n\n        dense_layer = DenseLayer(units=4, weights_spec=2.0, biases_spec=0.5, activation=""lrelu"")\n        test = ComponentTest(component=dense_layer, input_spaces=dict(inputs=input_space))\n\n        # Batch of size=1 (can increase this to any larger number).\n        input_ = np.array([[0.5, 2.0, 1.5], [-1.0, -2.0, -1.5]])\n        expected = np.array([[8.5, 8.5, 8.5, 8.5], [-8.5*0.2, -8.5*0.2, -8.5*0.2, -8.5*0.2]],\n                            dtype=np.float32)  # 0.2=leaky-relu\n        test.test((""call"", input_), expected_outputs=expected)\n\n    def test_conv2d_layer(self):\n        # Space must contain batch dimension (otherwise, NNlayer will complain).\n        space = FloatBox(shape=(2, 2, 3), add_batch_rank=True)  # e.g. a simple 3-color image\n\n        conv2d_layer = Conv2DLayer(filters=4, kernel_size=2, strides=1, padding=""valid"",\n                                   kernel_spec=0.5, biases_spec=False)\n        test = ComponentTest(component=conv2d_layer, input_spaces=dict(inputs=space))\n\n        # Batch of 2 samples.\n        input_ = np.array([[[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],  # sample 1 (2x2x3)\n                            [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]],\n                           [[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]],  # sample 2 (2x2x3)\n                            [[0.7, 0.8, 0.9], [1.00, 1.10, 1.20]]]\n                           ])\n        expected = np.array([[[[39.0, 39.0, 39.0, 39.0]]],  # output 1 (1x1x4)\n                             [[[3.9, 3.9, 3.9, 3.9]]],  # output 2 (1x1x4)\n                             ])\n        test.test((""call"", input_), expected_outputs=expected)\n\n    def test_maxpool2d_layer(self):\n        space = FloatBox(shape=(2, 2, 3), add_batch_rank=True)  # e.g. a simple 3-color image\n\n        # NOTE: Strides shouldn\'t matter.\n        maxpool2d_layer = MaxPool2DLayer(pool_size=2, strides=2, padding=""valid"")\n        test = ComponentTest(component=maxpool2d_layer, input_spaces=dict(inputs=space))\n\n        # Batch of 2 sample.\n        input_ = space.sample(2)\n        item0_ch0 = max(input_[0][0][0][0], input_[0][0][1][0], input_[0][1][0][0], input_[0][1][1][0])\n        item0_ch1 = max(input_[0][0][0][1], input_[0][0][1][1], input_[0][1][0][1], input_[0][1][1][1])\n        item0_ch2 = max(input_[0][0][0][2], input_[0][0][1][2], input_[0][1][0][2], input_[0][1][1][2])\n        item1_ch0 = max(input_[1][0][0][0], input_[1][0][1][0], input_[1][1][0][0], input_[1][1][1][0])\n        item1_ch1 = max(input_[1][0][0][1], input_[1][0][1][1], input_[1][1][0][1], input_[1][1][1][1])\n        item1_ch2 = max(input_[1][0][0][2], input_[1][0][1][2], input_[1][1][0][2], input_[1][1][1][2])\n        expected = np.array([[[[item0_ch0, item0_ch1, item0_ch2]]], [[[item1_ch0, item1_ch1, item1_ch2]]]])\n        test.test((""call"", input_), expected_outputs=expected)\n\n    def test_local_response_normalization_layer(self):\n        space = FloatBox(shape=(2, 2, 3), add_batch_rank=True)  # e.g. a simple 3-color image\n\n        # Todo: This is a very simple example ignoring the depth radius, which is the main idea of this normalization\n        # Also, 0.0 depth_radius doesn\'t run on GPU with cuDNN\n        depth_radius = 0.0\n        bias = np.random.random() + 1.0\n        alpha = np.random.random() + 1.0\n        beta = np.random.random() + 1.0\n\n        test_local_response_normalization_layer = LocalResponseNormalizationLayer(\n            depth_radius=depth_radius, bias=bias, alpha=alpha, beta=beta\n        )\n        test = ComponentTest(component=test_local_response_normalization_layer, input_spaces=dict(inputs=space))\n\n        # Batch of 2 sample.\n        input_ = space.sample(2)\n\n        calculated = input_ / (bias + alpha * np.square(input_)) ** beta\n\n        expected = np.array(calculated)\n        test.test((""call"", input_), expected_outputs=expected)\n\n    def test_concat_layer(self):\n        # Spaces must contain batch dimension (otherwise, NNlayer will complain).\n        space0 = FloatBox(shape=(2, 3), add_batch_rank=True)\n        space1 = FloatBox(shape=(2, 1), add_batch_rank=True)\n        space2 = FloatBox(shape=(2, 2), add_batch_rank=True)\n\n        concat_layer = ConcatLayer()\n        test = ComponentTest(component=concat_layer, input_spaces=dict(inputs=[space0, space1, space2]))\n\n        # Batch of 2 samples to concatenate.\n        inputs = (\n            np.array([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[1.1, 2.1, 3.1], [4.1, 5.1, 6.1]]], dtype=np.float32),\n            np.array([[[1.0], [2.0]], [[3.0], [4.0]]], dtype=np.float32),\n            np.array([[[1.2, 2.2], [3.2, 4.2]], [[1.3, 2.3], [3.3, 4.3]]], dtype=np.float32)\n        )\n        expected = np.concatenate((inputs[0], inputs[1], inputs[2]), axis=-1)\n        test.test((""call"", inputs), expected_outputs=expected)\n\n    def test_concat_layer_with_dict_input(self):\n        # Spaces must contain batch dimension (otherwise, NNlayer will complain).\n        input_space = Dict({\n            ""a"": FloatBox(shape=(2, 3)),\n            ""b"": FloatBox(shape=(2, 1)),\n            ""c"": FloatBox(shape=(2, 2)),\n        }, add_batch_rank=True)\n\n        concat_layer = ConcatLayer(dict_keys=[""c"", ""a"", ""b""])  # some crazy order\n        test = ComponentTest(component=concat_layer, input_spaces=dict(inputs=input_space))\n\n        # Batch of n samples to concatenate.\n        inputs = input_space.sample(4)\n        expected = np.concatenate((inputs[""c""], inputs[""a""], inputs[""b""]), axis=-1)\n        test.test((""call"", tuple([inputs])), expected_outputs=expected)\n\n    def test_residual_layer(self):\n        # Input space to residual layer (with 2-repeat [simple Conv2D layer]-residual-unit).\n        input_space = FloatBox(shape=(2, 2, 3), add_batch_rank=True)\n\n        residual_unit = Conv2DLayer(filters=3, kernel_size=1, strides=1, padding=""same"",\n                                    kernel_spec=0.5, biases_spec=1.0)\n        residual_layer = ResidualLayer(residual_unit=residual_unit, repeats=2)\n        test = ComponentTest(component=residual_layer, input_spaces=dict(inputs=input_space))\n\n        # Batch of 2 samples.\n        inputs = np.array(\n            [\n                [[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8, 0.9], [1.1, 1.2, 1.3]]],\n                [[[1.1, 1.2, 1.3], [2.4, 2.5, 2.6]], [[-0.7, -0.8, -0.9], [3.1, 3.2, 3.3]]]\n            ]\n        )\n\n        """"""\n        Calculation:\n        1st_conv2d = sum-over-last-axis(input) * 0.5 + 1.0 -> tile last axis 3x\n        2nd_conv2d = sum-over-last-axis(2nd_conv2d) * 0.5 + 1.0 -> tile last axis 3x\n        output: 2nd_conv2d + input\n        """"""\n        conv2d_1 = np.tile(np.sum(inputs, axis=3, keepdims=True) * 0.5 + 1.0, (1, 1, 1, 3))\n        conv2d_2 = np.tile(np.sum(conv2d_1, axis=3, keepdims=True) * 0.5 + 1.0, (1, 1, 1, 3))\n        expected = conv2d_2 + inputs\n        test.test((""call"", inputs), expected_outputs=expected, decimals=5)\n\n    def test_lstm_layer(self):\n        # 0th rank=batch-rank; 1st rank=time/sequence-rank; 2nd-nth rank=data.\n        batch_size = 3\n        sequence_length = 2\n        input_space = FloatBox(shape=(3,), add_batch_rank=True, add_time_rank=True)\n\n        lstm_layer_component = LSTMLayer(units=5)\n        test = ComponentTest(component=lstm_layer_component, input_spaces=dict(inputs=input_space))\n\n        # Batch of n samples.\n        inputs = np.ones(shape=(batch_size, sequence_length, 3))\n\n        # First matmul the inputs times the LSTM matrix:\n        var_values = test.read_variable_values(lstm_layer_component.variable_registry)\n        lstm_matrix = var_values[""lstm-layer/lstm-cell/kernel""]\n        lstm_biases = var_values[""lstm-layer/lstm-cell/bias""]\n\n        expected_outputs, expected_internal_states = lstm_layer(inputs, lstm_matrix, lstm_biases, time_major=False)\n\n        expected = [expected_outputs, expected_internal_states]\n        test.test((""call"", inputs), expected_outputs=tuple(expected))\n\n    def test_multi_lstm_layer(self):\n        return  # TODO: finish this test case\n        # Tests a double MultiLSTMLayer.\n        input_spaces = dict(\n            inputs=FloatBox(shape=(3,), add_batch_rank=True, add_time_rank=True),\n            initial_c_and_h_states=Tuple(\n                Tuple(FloatBox(shape=(5,)), FloatBox(shape=(5,))),\n                Tuple(FloatBox(shape=(5,)), FloatBox(shape=(5,))),\n                add_batch_rank=True\n            )\n        )\n\n        multi_lstm_layer = MultiLSTMLayer(\n            num_lstms=2,\n            units=5,\n            # Full skip connections (x goes into both layers, out0 goes into layer1).\n            skip_connections=[[True, False], [True, True]]\n        )\n\n        # Do not seed, we calculate expectations manually.\n        test = ComponentTest(component=multi_lstm_layer, input_spaces=input_spaces)\n\n        # Batch of size=n, time-steps=m.\n        input_ = input_spaces[""inputs""].sample((2, 3))\n\n        global_scope = ""variational-auto-encoder/""\n        # Calculate output manually.\n        var_dict = test.read_variable_values(multi_lstm_layer.variable_registry)\n\n        encoder_network_out = dense_layer(\n            input_, var_dict[global_scope+""encoder-network/encoder-layer/dense/kernel""],\n            var_dict[global_scope+""encoder-network/encoder-layer/dense/bias""]\n        )\n        expected_mean = dense_layer(\n            encoder_network_out, var_dict[global_scope+""mean-layer/dense/kernel""],\n            var_dict[global_scope+""mean-layer/dense/bias""]\n        )\n        expected_stddev = dense_layer(\n            encoder_network_out, var_dict[global_scope + ""stddev-layer/dense/kernel""],\n            var_dict[global_scope + ""stddev-layer/dense/bias""]\n        )\n        out = test.test((""encode"", input_), expected_outputs=None)\n        recursive_assert_almost_equal(out[""mean""], expected_mean, decimals=5)\n        recursive_assert_almost_equal(out[""stddev""], expected_stddev, decimals=5)\n        self.assertTrue(out[""z_sample""].shape == (3, 1))\n\n        test.terminate()\n\n'"
rlgraph/tests/components/test_noise_components.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom six.moves import xrange as range_\nimport unittest\n\nfrom rlgraph.components.common.noise_components import *\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\n\n\nclass TestNoiseComponents(unittest.TestCase):\n    """"""\n    Tests RLGraph\'s noise components.\n    """"""\n\n    # Decaying a value always without batch dimension (does not make sense for global time step).\n    action_input_space = FloatBox(1000, add_batch_rank=False)\n\n    def test_constant_noise(self):\n        real_noise = 200.0\n\n        noise_component = ConstantNoise(value=real_noise)\n        test = ComponentTest(component=noise_component, action_space=self.action_input_space)\n\n        for _ in range_(1000):\n            test.test((""get_noise"", None), expected_outputs=real_noise)\n\n    def test_gaussian_noise(self):\n        real_mean = 10.0\n        real_sd = 2.0\n\n        noise_component = GaussianNoise(mean=real_mean, stddev=real_sd)\n        test = ComponentTest(component=noise_component, input_spaces=None, action_space=self.action_input_space)\n\n        # Collect outputs in `collected` list to compare moments.\n        collected = list()\n        collect_outs = lambda component_test, outs: collected.append(outs)\n\n        for _ in range_(1000):\n            test.test((""get_noise"", None), fn_test=collect_outs)\n\n        test_mean = np.mean(collected)\n        test_sd = np.std(collected)\n\n        # Empiric mean should be within 2 sd of real mean\n        self.assertGreater(real_mean, test_mean - test_sd * 2)\n        self.assertLess(real_mean, test_mean + test_sd * 2)\n\n        # Empiric sd should be within 80 % and 120 % interval\n        self.assertGreater(real_sd, test_sd * 0.8)\n        self.assertLess(real_sd, test_sd * 1.2)\n\n    def test_ornstein_uhlenbeck_noise(self):\n        ou_theta = 0.15\n        ou_mu = 10.0\n        ou_sigma = 2.0\n\n        noise_component = OrnsteinUhlenbeckNoise(\n            theta=ou_theta, mu=ou_mu, sigma=ou_sigma\n        )\n        test = ComponentTest(component=noise_component, action_space=self.action_input_space)\n\n        # Collect outputs in `collected` list to compare moments.\n        collected = list()\n        collect_outs = lambda component_test, outs: collected.append(outs)\n\n        for _ in range_(1000):\n            test.test((""get_noise"", None), fn_test=collect_outs)\n\n        test_mean = np.mean(collected)\n        test_sd = np.std(collected)\n\n        print(""Moments: {} / {}"".format(test_mean, test_sd))\n\n        # Empiric mean should be within 2 sd of real mean.\n        self.assertGreater(ou_mu, test_mean - test_sd * 2)\n        self.assertLess(ou_mu, test_mean + test_sd * 2)\n\n        # Empiric sd should be within 45% and 200% interval.\n        self.assertGreater(ou_sigma, test_sd * 0.45)\n        self.assertLess(ou_sigma, test_sd * 2.0)\n\n        # TODO: Maybe test time correlation?\n'"
rlgraph/tests/components/test_policies.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport unittest\n\nimport numpy as np\nfrom rlgraph.components.policies import Policy, SharedValueFunctionPolicy, DuelingPolicy\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.tests.test_util import config_from_path, recursive_assert_almost_equal\nfrom rlgraph.utils import sigmoid, softmax, relu, MAX_LOG_STDDEV, MIN_LOG_STDDEV, SMALL_NUMBER\nfrom scipy.stats import beta, norm\n\n\nclass TestPolicies(unittest.TestCase):\n\n    def test_policy_for_boolean_action_space(self):\n        # state_space (NN is a simple single fc-layer relu network (2 units), random biases, random weights).\n        state_space = FloatBox(shape=(4,), add_batch_rank=True)\n\n        # action_space (simple boolean).\n        action_space = BoolBox(add_batch_rank=True)\n\n        policy = Policy(network_spec=config_from_path(""configs/test_simple_nn.json""), action_space=action_space)\n        test = ComponentTest(\n            component=policy,\n            input_spaces=dict(\n                nn_inputs=state_space,\n                actions=action_space,\n            ),\n            action_space=action_space\n        )\n        policy_params = test.read_variable_values(policy.variable_registry)\n\n        # Some NN inputs.\n        batch_size = 32\n        states = state_space.sample(batch_size)\n        # Raw NN-output.\n        expected_nn_output = np.matmul(\n            states, ComponentTest.read_params(""policy/test-network/hidden-layer"", policy_params)\n        )\n\n        test.test((""get_nn_outputs"", states), expected_outputs=expected_nn_output, decimals=5)\n\n        # Raw action layer output; Expected shape=(): 2=batch\n        expected_action_layer_output = np.squeeze(np.matmul(\n            expected_nn_output,\n            ComponentTest.read_params(""policy/action-adapter-0/action-network/action-layer"", policy_params)\n        ), axis=-1)\n        test.test(\n            (""get_adapter_outputs"", states), expected_outputs=dict(\n                adapter_outputs=expected_action_layer_output, nn_outputs=expected_nn_output\n            ), decimals=5\n        )\n\n        # Logits, parameters (probs) and skip log-probs (numerically unstable for small probs).\n        expected_probs_output = sigmoid(expected_action_layer_output)\n        test.test(\n            (""get_adapter_outputs_and_parameters"", states, [""adapter_outputs"", ""parameters"", ""log_probs""]),\n            expected_outputs=dict(\n                adapter_outputs=expected_action_layer_output,\n                parameters=expected_probs_output,\n                log_probs=np.log(expected_probs_output)\n            ), decimals=5\n        )\n\n        expected_actions = expected_action_layer_output > 0.0\n        test.test((""get_action"", states, [""action""]), expected_outputs=dict(action=expected_actions))\n\n        # Get action AND log-llh.\n        out = test.test((""get_action_and_log_likelihood"", states))\n        action = out[""action""]\n        llh = out[""log_likelihood""]\n\n        # Action log-probs.\n        expected_action_log_llh_output = np.log(np.array([\n            expected_probs_output[i] if action[i] else 1.0 - expected_probs_output[i] for i in range(batch_size)\n        ]))\n        test.test(\n            (""get_log_likelihood"", [states, action], ""log_likelihood""),\n            expected_outputs=dict(log_likelihood=expected_action_log_llh_output),\n            decimals=5\n        )\n        recursive_assert_almost_equal(expected_action_log_llh_output, llh, decimals=5)\n\n        # Stochastic sample.\n        out = test.test((""get_stochastic_action"", states), expected_outputs=None)\n        self.assertTrue(out[""action""].dtype == np.bool_)\n        self.assertTrue(out[""action""].shape == (batch_size,))\n\n        # Deterministic sample.\n        test.test((""get_deterministic_action"", states), expected_outputs=None)\n        self.assertTrue(out[""action""].dtype == np.bool_)\n        self.assertTrue(out[""action""].shape == (batch_size,))\n\n        # Distribution\'s entropy.\n        out = test.test((""get_entropy"", states), expected_outputs=None)\n        self.assertTrue(out[""entropy""].dtype == np.float32)\n        self.assertTrue(out[""entropy""].shape == (batch_size,))\n\n    def test_policy_for_discrete_action_space(self):\n        # state_space (NN is a simple single fc-layer relu network (2 units), random biases, random weights).\n        state_space = FloatBox(shape=(4,), add_batch_rank=True)\n\n        # action_space (5 possible actions).\n        action_space = IntBox(5, add_batch_rank=True)\n\n        policy = Policy(network_spec=config_from_path(""configs/test_simple_nn.json""), action_space=action_space)\n        test = ComponentTest(\n            component=policy,\n            input_spaces=dict(\n                nn_inputs=state_space,\n                actions=action_space,\n            ),\n            action_space=action_space\n        )\n        policy_params = test.read_variable_values(policy.variable_registry)\n\n        # Some NN inputs (4 input nodes, batch size=2).\n        states = np.array([[-0.08, 0.4, -0.05, -0.55], [13.0, -14.0, 10.0, -16.0]])\n        # Raw NN-output.\n        expected_nn_output = np.matmul(\n            states, ComponentTest.read_params(""policy/test-network/hidden-layer"", policy_params)\n        )\n\n        test.test((""get_nn_outputs"", states), expected_outputs=expected_nn_output, decimals=5)\n\n        # Raw action layer output; Expected shape=(2,5): 2=batch, 5=action categories\n        expected_action_layer_output = np.matmul(\n            expected_nn_output,\n            ComponentTest.read_params(""policy/action-adapter-0/action-network/action-layer"", policy_params)\n        )\n        test.test(\n            (""get_adapter_outputs"", states), expected_outputs=dict(\n                adapter_outputs=expected_action_layer_output, nn_outputs=expected_nn_output\n            ), decimals=5\n        )\n\n        # Logits, parameters (probs) and skip log-probs (numerically unstable for small probs).\n        expected_probs_output = softmax(expected_action_layer_output, axis=-1)\n        test.test(\n            (""get_adapter_outputs_and_parameters"", states, [""adapter_outputs"", ""parameters"", ""log_probs""]),\n            expected_outputs=dict(\n                adapter_outputs=expected_action_layer_output,\n                parameters=np.array(expected_action_layer_output, dtype=np.float32),\n                log_probs=np.log(expected_probs_output)\n            ), decimals=5\n        )\n\n        expected_actions = np.argmax(expected_action_layer_output, axis=-1)\n        test.test((""get_action"", states, [""action""]), expected_outputs=dict(action=expected_actions))\n\n        # Get action AND log-llh.\n        out = test.test((""get_action_and_log_likelihood"", states))\n        action = out[""action""]\n        llh = out[""log_likelihood""]\n\n        # Action log-probs.\n        expected_action_log_llh_output = np.log(np.array([\n            expected_probs_output[0][action[0]],\n            expected_probs_output[1][action[1]]\n        ]))\n        test.test(\n            (""get_log_likelihood"", [states, action], ""log_likelihood""),\n            expected_outputs=dict(log_likelihood=expected_action_log_llh_output),\n            decimals=5\n        )\n        recursive_assert_almost_equal(expected_action_log_llh_output, llh, decimals=5)\n\n        # Stochastic sample.\n        out = test.test((""get_stochastic_action"", states), expected_outputs=None)\n        self.assertTrue(out[""action""].dtype == np.int32 or (out[""action""].dtype == np.int64))\n        self.assertTrue(out[""action""].shape == (2,))\n\n        # Deterministic sample.\n        test.test((""get_deterministic_action"", states), expected_outputs=None)\n        self.assertTrue(out[""action""].dtype == np.int32 or (out[""action""].dtype == np.int64))\n        self.assertTrue(out[""action""].shape == (2,))\n\n        # Distribution\'s entropy.\n        out = test.test((""get_entropy"", states), expected_outputs=None)\n        self.assertTrue(out[""entropy""].dtype == np.float32)\n        self.assertTrue(out[""entropy""].shape == (2,))\n\n    def test_shared_value_function_policy_for_discrete_action_space(self):\n        # state_space (NN is a simple single fc-layer relu network (2 units), random biases, random weights).\n        state_space = FloatBox(shape=(4,), add_batch_rank=True)\n\n        # action_space (3 possible actions).\n        action_space = IntBox(3, add_batch_rank=True)\n\n        # Policy with baseline action adapter.\n        shared_value_function_policy = SharedValueFunctionPolicy(\n            network_spec=config_from_path(""configs/test_lrelu_nn.json""),\n            action_space=action_space\n        )\n        test = ComponentTest(\n            component=shared_value_function_policy,\n            input_spaces=dict(\n                nn_inputs=state_space,\n                actions=action_space,\n            ),\n            action_space=action_space,\n        )\n        policy_params = test.read_variable_values(shared_value_function_policy.variable_registry)\n\n        # Some NN inputs (4 input nodes, batch size=3).\n        states = state_space.sample(size=3)\n        # Raw NN-output (3 hidden nodes). All weights=1.5, no biases.\n        expected_nn_output = relu(np.matmul(\n            states, ComponentTest.read_params(""shared-value-function-policy/test-network/hidden-layer"", policy_params)\n        ), 0.1)\n\n        test.test((""get_nn_outputs"", states), expected_outputs=expected_nn_output, decimals=5)\n\n        # Raw action layer output; Expected shape=(3,3): 3=batch, 2=action categories + 1 state value\n        expected_action_layer_output = np.matmul(\n            expected_nn_output,\n            ComponentTest.read_params(""shared-value-function-policy/action-adapter-0/action-network/action-layer/"",\n                                      policy_params)\n        )\n        test.test((""get_adapter_outputs"", states),\n                  expected_outputs=dict(adapter_outputs=expected_action_layer_output, nn_outputs=expected_nn_output),\n                  decimals=5)\n\n        # State-values: One for each item in the batch.\n        expected_state_value_output = np.matmul(\n            expected_nn_output,\n            ComponentTest.read_params(""shared-value-function-policy/value-function-node/dense-layer"", policy_params)\n        )\n        test.test((""get_state_values"", states, [""state_values""]),\n                  expected_outputs=dict(state_values=expected_state_value_output), decimals=5)\n\n        # Logits-values.\n        test.test((""get_state_values_adapter_outputs_and_parameters"", states,\n                   [""state_values"", ""adapter_outputs""]),\n                  expected_outputs=dict(\n                      state_values=expected_state_value_output, adapter_outputs=expected_action_layer_output\n                  ),\n                  decimals=5)\n\n        # Parameter (probabilities). Softmaxed logits.\n        expected_probs_output = softmax(expected_action_layer_output, axis=-1)\n        test.test((""get_adapter_outputs_and_parameters"", states, [""adapter_outputs"", ""parameters""]),\n                  expected_outputs=dict(\n                      adapter_outputs=expected_action_layer_output,\n                      parameters=expected_action_layer_output\n                  ), decimals=5)\n\n        print(""Probs: {}"".format(expected_probs_output))\n\n        expected_actions = np.argmax(expected_action_layer_output, axis=-1)\n        test.test((""get_action"", states, [""action""]), expected_outputs=dict(action=expected_actions))\n\n        # Get action AND log-llh.\n        out = test.test((""get_action_and_log_likelihood"", states))\n        action = out[""action""]\n        llh = out[""log_likelihood""]\n\n        # Action log-llh.\n        expected_action_log_llh_output = np.log(np.array([\n            expected_probs_output[0][action[0]],\n            expected_probs_output[1][action[1]],\n            expected_probs_output[2][action[2]],\n        ]))\n        test.test(\n            (""get_log_likelihood"", [states, action], ""log_likelihood""),\n            expected_outputs=dict(log_likelihood=expected_action_log_llh_output),\n            decimals=5\n        )\n        recursive_assert_almost_equal(expected_action_log_llh_output, llh)\n\n        # Stochastic sample.\n        out = test.test((""get_stochastic_action"", states), expected_outputs=None)\n        self.assertTrue(out[""action""].dtype == np.int32 or (out[""action""].dtype == np.int64))\n        self.assertTrue(out[""action""].shape == (3,))\n\n        # Deterministic sample.\n        out = test.test((""get_deterministic_action"", states), expected_outputs=None)\n        self.assertTrue(out[""action""].dtype == np.int32 or (out[""action""].dtype == np.int64))\n        self.assertTrue(out[""action""].shape == (3,))\n\n        # Distribution\'s entropy.\n        out = test.test((""get_entropy"", states), expected_outputs=None)\n        self.assertTrue(out[""entropy""].dtype == np.float32)\n        self.assertTrue(out[""entropy""].shape == (3,))\n\n    def test_shared_value_function_policy_for_discrete_action_space_with_time_rank_folding(self):\n        # state_space (NN is a simple single fc-layer relu network (2 units), random biases, random weights).\n        state_space = FloatBox(shape=(3,), add_batch_rank=True, add_time_rank=True)\n\n        # action_space (4 possible actions).\n        action_space = IntBox(4, add_batch_rank=True, add_time_rank=True)\n        flat_float_action_space = FloatBox(shape=(4,), add_batch_rank=True, add_time_rank=True)\n\n        # Policy with baseline action adapter AND batch-apply over the entire policy (NN + ActionAdapter + distr.).\n        network_spec = config_from_path(""configs/test_lrelu_nn.json"")\n        # Add folding and unfolding to network.\n        network_spec[""fold_time_rank""] = True\n        network_spec[""unfold_time_rank""] = True\n        shared_value_function_policy = SharedValueFunctionPolicy(\n            network_spec=network_spec,\n            action_adapter_spec=dict(fold_time_rank=True, unfold_time_rank=True),\n            action_space=action_space,\n            value_fold_time_rank=True,\n            value_unfold_time_rank=True\n        )\n        test = ComponentTest(\n            component=shared_value_function_policy,\n            input_spaces=dict(\n                nn_inputs=state_space,\n                actions=action_space,\n            ),\n            action_space=action_space,\n        )\n        policy_params = test.read_variable_values(shared_value_function_policy.variable_registry)\n\n        # Some NN inputs.\n        states = state_space.sample(size=(2, 3))\n        states_folded = np.reshape(states, newshape=(6, 3))\n        # Raw NN-output (3 hidden nodes). All weights=1.5, no biases.\n        expected_nn_output = np.reshape(relu(np.matmul(\n            states_folded,\n            ComponentTest.read_params(""shared-value-function-policy/test-network/hidden-layer"", policy_params)\n        ), 0.1), newshape=states.shape)\n        test.test((""get_nn_outputs"", states), expected_outputs=expected_nn_output, decimals=5)\n\n        # Raw action layer output; Expected shape=(3,3): 3=batch, 2=action categories + 1 state value\n        expected_action_layer_output = np.matmul(\n            expected_nn_output,\n            ComponentTest.read_params(""shared-value-function-policy/action-adapter-0/action-network/action-layer/"",\n                                      policy_params))\n\n        expected_action_layer_output = np.reshape(expected_action_layer_output, newshape=(2, 3, 4))\n        test.test(\n            (""get_adapter_outputs"", states),\n            expected_outputs=dict(adapter_outputs=expected_action_layer_output, nn_outputs=expected_nn_output),\n            decimals=5\n        )\n\n        # State-values: One for each item in the batch.\n        expected_state_value_output = np.matmul(\n            expected_nn_output,\n            ComponentTest.read_params(""shared-value-function-policy/value-function-node/dense-layer"", policy_params)\n        )\n        expected_state_value_output_unfolded = np.reshape(expected_state_value_output, newshape=(2, 3, 1))\n        test.test((""get_state_values"", states, [""state_values""]),\n                  expected_outputs=dict(state_values=expected_state_value_output_unfolded),\n                  decimals=5)\n\n        expected_action_layer_output_unfolded = np.reshape(expected_action_layer_output, newshape=(2, 3, 4))\n        test.test((\n            ""get_state_values_adapter_outputs_and_parameters"", states, [""state_values"", ""adapter_outputs""]\n        ), expected_outputs=dict(\n            state_values=expected_state_value_output_unfolded,\n            adapter_outputs=expected_action_layer_output_unfolded\n        ), decimals=5)\n\n        # Parameter (probabilities). Softmaxed logits.\n        expected_probs_output = softmax(expected_action_layer_output_unfolded, axis=-1)\n        test.test(\n            (""get_adapter_outputs_and_parameters"", states, [""adapter_outputs"", ""parameters"", ""nn_outputs""]),\n            expected_outputs=dict(\n                nn_outputs=expected_nn_output,\n                adapter_outputs=expected_action_layer_output_unfolded,\n                parameters=expected_action_layer_output_unfolded\n            ), decimals=5\n        )\n\n        print(""Probs: {}"".format(expected_probs_output))\n\n        expected_actions = np.argmax(expected_action_layer_output_unfolded, axis=-1)\n        test.test((""get_action"", states, [""action""]), expected_outputs=dict(action=expected_actions))\n\n        out = test.test((""get_action_and_log_likelihood"", states))\n        action = out[""action""]\n        llh = out[""log_likelihood""]\n\n        # Action log-llh.\n        expected_action_log_llh_output = np.log(np.array([[\n            expected_probs_output[0][0][action[0][0]],\n            expected_probs_output[0][1][action[0][1]],\n            expected_probs_output[0][2][action[0][2]],\n        ], [\n            expected_probs_output[1][0][action[1][0]],\n            expected_probs_output[1][1][action[1][1]],\n            expected_probs_output[1][2][action[1][2]],\n        ]]))\n        test.test((""get_log_likelihood"", [states, action]), expected_outputs=dict(\n            log_likelihood=expected_action_log_llh_output,\n            adapter_outputs=expected_action_layer_output_unfolded\n        ), decimals=5)\n        recursive_assert_almost_equal(expected_action_log_llh_output, llh, decimals=5)\n\n        # Deterministic sample.\n        out = test.test((""get_deterministic_action"", states), expected_outputs=None)\n        self.assertTrue(out[""action""].dtype == np.int32 or (out[""action""].dtype == np.int64))\n        self.assertTrue(out[""action""].shape == (2, 3))  # Make sure output is unfolded.\n\n        # Stochastic sample.\n        out = test.test((""get_stochastic_action"", states), expected_outputs=None)\n        self.assertTrue(out[""action""].dtype == np.int32 or (out[""action""].dtype == np.int64))\n        self.assertTrue(out[""action""].shape == (2, 3))  # Make sure output is unfolded.\n\n        # Distribution\'s entropy.\n        out = test.test((""get_entropy"", states), expected_outputs=None)\n        self.assertTrue(out[""entropy""].dtype == np.float32)\n        self.assertTrue(out[""entropy""].shape == (2, 3))  # Make sure output is unfolded.\n\n    def test_policy_for_discrete_action_space_with_dueling_layer(self):\n        # np.random.seed(10)\n        # state_space (NN is a simple single fc-layer relu network (2 units), random biases, random weights).\n        nn_input_space = FloatBox(shape=(3,), add_batch_rank=True)\n\n        # action_space (2 possible actions).\n        action_space = IntBox(2, add_batch_rank=True)\n        # flat_float_action_space = FloatBox(shape=(2,), add_batch_rank=True)\n\n        # Policy with dueling logic.\n        policy = DuelingPolicy(\n            network_spec=config_from_path(""configs/test_lrelu_nn.json""),\n            action_adapter_spec=dict(\n                pre_network_spec=[\n                    dict(type=""dense"", units=10, activation=""lrelu"", activation_params=[0.1])\n                ]\n            ),\n            units_state_value_stream=10,\n            action_space=action_space\n        )\n        test = ComponentTest(\n            component=policy,\n            input_spaces=dict(\n                nn_inputs=nn_input_space,\n                actions=action_space,\n            ),\n            action_space=action_space\n        )\n        policy_params = test.read_variable_values(policy.variable_registry)\n\n        # Some NN inputs.\n        nn_input = nn_input_space.sample(size=3)\n        # Raw NN-output.\n        expected_nn_output = relu(np.matmul(\n            nn_input,\n            ComponentTest.read_params(""dueling-policy/test-network/hidden-layer"", policy_params)), 0.1\n        )\n        test.test((""get_nn_outputs"", nn_input), expected_outputs=expected_nn_output)\n\n        # Single state values.\n        expected_state_values = np.matmul(relu(np.matmul(\n            expected_nn_output,\n            ComponentTest.read_params(""dueling-policy/dense-layer-state-value-stream"", policy_params)\n        )),\n            ComponentTest.read_params(""dueling-policy/state-value-node"", policy_params))\n        test.test(\n            (""get_state_values"", nn_input, [""state_values"", ""nn_outputs""]),\n            expected_outputs=dict(state_values=expected_state_values, nn_outputs=expected_nn_output),\n            decimals=5\n        )\n\n        # Raw action layer output.\n        expected_raw_advantages = np.matmul(relu(np.matmul(\n            expected_nn_output,\n            ComponentTest.read_params(""dueling-policy/action-adapter-0/action-network/dense-layer"", policy_params)\n        ), 0.1),\n            ComponentTest.read_params(""dueling-policy/action-adapter-0/action-network/action-layer"", policy_params))\n\n        # Q-values: One for each item in the batch.\n        expected_q_values_output = expected_state_values + expected_raw_advantages - \\\n            np.mean(expected_raw_advantages, axis=-1, keepdims=True)\n        test.test(\n            (""get_adapter_outputs"", nn_input, [""adapter_outputs"", ""advantages""]),\n            expected_outputs=dict(adapter_outputs=expected_q_values_output, advantages=expected_raw_advantages),\n            decimals=5\n        )\n\n        # Parameter (probabilities). Softmaxed q_values.\n        expected_probs_output = softmax(expected_q_values_output, axis=-1)\n        test.test(\n            (""get_adapter_outputs_and_parameters"", nn_input, [""adapter_outputs"", ""parameters""]),\n            expected_outputs=dict(adapter_outputs=expected_q_values_output, parameters=expected_q_values_output),\n            decimals=5\n        )\n\n        print(""Probs: {}"".format(expected_probs_output))\n\n        expected_actions = np.argmax(expected_q_values_output, axis=-1)\n        test.test((""get_action"", nn_input, [""action""]), expected_outputs=dict(action=expected_actions))\n\n        out = test.test((""get_action_and_log_likelihood"", nn_input))\n        action = out[""action""]\n        llh = out[""log_likelihood""]\n\n        # Action log-probs.\n        expected_action_log_llh_output = np.log(np.array([\n            expected_probs_output[0][action[0]],\n            expected_probs_output[1][action[1]],\n            expected_probs_output[2][action[2]],\n        ]))\n        test.test(\n            (""get_log_likelihood"", [nn_input, action]),\n            expected_outputs=dict(\n                log_likelihood=expected_action_log_llh_output, adapter_outputs=expected_q_values_output\n            ),\n            decimals=5\n        )\n        recursive_assert_almost_equal(expected_action_log_llh_output, llh, decimals=5)\n\n        # Stochastic sample.\n        out = test.test((""get_stochastic_action"", nn_input), expected_outputs=None)\n        self.assertTrue(out[""action""].dtype == np.int32 or (out[""action""].dtype == np.int64))\n        self.assertTrue(out[""action""].shape == (3,))\n\n        # Deterministic sample.\n        out = test.test((""get_deterministic_action"", nn_input), expected_outputs=None)\n        self.assertTrue(out[""action""].dtype == np.int32 or (out[""action""].dtype == np.int64))\n        self.assertTrue(out[""action""].shape == (3,))\n\n        # Distribution\'s entropy.\n        out = test.test((""get_entropy"", nn_input), expected_outputs=None)\n        self.assertTrue(out[""entropy""].dtype == np.float32)\n        self.assertTrue(out[""entropy""].shape == (3,))\n\n    def test_policy_for_bounded_continuous_action_space_using_beta(self):\n        """"""\n        https://github.com/rlgraph/rlgraph/issues/43\n        """"""\n        nn_input_space = FloatBox(shape=(4,), add_batch_rank=True)\n        action_space = FloatBox(low=-1.0, high=1.0, shape=(1,), add_batch_rank=True)\n        # Double the shape for alpha/beta params.\n        # action_space_parameters = Tuple(FloatBox(shape=(1,)), FloatBox(shape=(1,)), add_batch_rank=True)\n\n        policy = Policy(network_spec=config_from_path(""configs/test_simple_nn.json""), action_space=action_space)\n        test = ComponentTest(\n            component=policy,\n            input_spaces=dict(\n                nn_inputs=nn_input_space,\n                actions=action_space,\n            ),\n            action_space=action_space\n        )\n\n        policy_params = test.read_variable_values(policy.variable_registry)\n\n        # Some NN inputs.\n        nn_input = nn_input_space.sample(size=3)\n        # Raw NN-output.\n        expected_nn_output = np.matmul(nn_input,\n                                       ComponentTest.read_params(""policy/test-network/hidden-layer"", policy_params))\n        test.test((""get_nn_outputs"", nn_input), expected_outputs=expected_nn_output)\n\n        # Raw action layer output.\n        expected_raw_logits = np.matmul(\n            expected_nn_output,\n            ComponentTest.read_params(""policy/action-adapter-0/action-network/action-layer"", policy_params)\n        )\n        test.test(\n            (""get_adapter_outputs"", nn_input),\n            expected_outputs=dict(adapter_outputs=expected_raw_logits, nn_outputs=expected_nn_output),\n            decimals=5\n        )\n\n        # Parameter (alpha/betas).\n        expected_alpha_parameters = np.log(np.exp(expected_raw_logits[:, 0:1]) + 1.0) + 1.0\n        expected_beta_parameters = np.log(np.exp(expected_raw_logits[:, 1:]) + 1.0) + 1.0\n        expected_parameters = tuple([expected_alpha_parameters, expected_beta_parameters])\n        test.test(\n            (""get_adapter_outputs_and_parameters"", nn_input, [""adapter_outputs"", ""parameters""]),\n            expected_outputs=dict(adapter_outputs=expected_raw_logits, parameters=expected_parameters),\n            decimals=5\n        )\n\n        print(""Params: {}"".format(expected_parameters))\n\n        action = test.test((""get_action"", nn_input))[""action""]\n        self.assertTrue(action.dtype == np.float32)\n        self.assertGreaterEqual(action.min(), -1.0)\n        self.assertLessEqual(action.max(), 1.0)\n        self.assertTrue(action.shape == (3, 1))\n\n        out = test.test((""get_action_and_log_likelihood"", nn_input))\n        action = out[""action""]\n        llh = out[""log_likelihood""]\n\n        # Action log-probs.\n        actions_scaled_back = (action + 1.0) / 2.0\n        expected_action_log_llh_output = np.log(\n            beta.pdf(actions_scaled_back, expected_alpha_parameters, expected_beta_parameters)\n        )\n        # expected_action_log_prob_output = np.array([[expected_action_log_prob_output[0][0]],\n        # [expected_action_log_prob_output[1][1]], [expected_action_log_prob_output[2][2]]])\n        test.test(\n            (""get_log_likelihood"", [nn_input, action], ""log_likelihood""),\n            expected_outputs=dict(log_likelihood=expected_action_log_llh_output),\n            decimals=5\n        )\n        recursive_assert_almost_equal(expected_action_log_llh_output, llh, decimals=5)\n\n        # Stochastic sample.\n        actions = test.test((""get_stochastic_action"", nn_input))[""action""]\n        self.assertTrue(actions.dtype == np.float32)\n        self.assertGreaterEqual(actions.min(), -1.0)\n        self.assertLessEqual(actions.max(), 1.0)\n        self.assertTrue(actions.shape == (3, 1))\n\n        # Deterministic sample.\n        actions = test.test((""get_deterministic_action"", nn_input))[""action""]\n        self.assertTrue(actions.dtype == np.float32)\n        self.assertGreaterEqual(actions.min(), -1.0)\n        self.assertLessEqual(actions.max(), 1.0)\n        self.assertTrue(actions.shape == (3, 1))\n\n        # Distribution\'s entropy.\n        entropy = test.test((""get_entropy"", nn_input))[""entropy""]\n        self.assertTrue(entropy.dtype == np.float32)\n        self.assertTrue(entropy.shape == (3, 1))\n\n    def test_policy_for_bounded_continuous_action_space_using_squashed_normal(self):\n        """"""\n        Same test case, but with different bounded continuous distribution (squashed normal).\n        """"""\n        nn_input_space = FloatBox(shape=(4,), add_batch_rank=True)\n        action_space = FloatBox(low=-2.0, high=1.0, shape=(1,), add_batch_rank=True)\n\n        policy = Policy(network_spec=config_from_path(""configs/test_simple_nn.json""), action_space=action_space,\n                        distributions_spec=dict(bounded_distribution_type=""squashed-normal""))\n        test = ComponentTest(\n            component=policy,\n            input_spaces=dict(\n                nn_inputs=nn_input_space,\n                actions=action_space,\n            ),\n            action_space=action_space\n        )\n\n        policy_params = test.read_variable_values(policy.variable_registry)\n\n        # Some NN inputs.\n        nn_input = nn_input_space.sample(size=3)\n        # Raw NN-output.\n        expected_nn_output = np.matmul(\n            nn_input, ComponentTest.read_params(""policy/test-network/hidden-layer"", policy_params)\n        )\n        test.test((""get_nn_outputs"", nn_input), expected_outputs=expected_nn_output)\n\n        # Raw action layer output.\n        expected_raw_logits = np.matmul(\n            expected_nn_output,\n            ComponentTest.read_params(""policy/action-adapter-0/action-network/action-layer"", policy_params)\n        )\n        test.test(\n            (""get_adapter_outputs"", nn_input),\n            expected_outputs=dict(adapter_outputs=expected_raw_logits, nn_outputs=expected_nn_output),\n            decimals=5\n        )\n\n        # Parameter (mean/stddev).\n        expected_mean_parameters = expected_raw_logits[:, 0:1]\n        expected_log_stddev_parameters = np.clip(expected_raw_logits[:, 1:2], MIN_LOG_STDDEV, MAX_LOG_STDDEV)\n        expected_parameters = tuple([expected_mean_parameters, np.exp(expected_log_stddev_parameters)])\n        test.test(\n            (""get_adapter_outputs_and_parameters"", nn_input, [""adapter_outputs"", ""parameters""]),\n            expected_outputs=dict(adapter_outputs=expected_raw_logits, parameters=expected_parameters),\n            decimals=5\n        )\n\n        print(""Params: {}"".format(expected_parameters))\n\n        action = test.test((""get_action"", nn_input))[""action""]\n        self.assertTrue(action.dtype == np.float32)\n        self.assertGreaterEqual(action.min(), -2.0)\n        self.assertLessEqual(action.max(), 1.0)\n        self.assertTrue(action.shape == (3, 1))\n\n        out = test.test((""get_action_and_log_likelihood"", nn_input))\n        action = out[""action""]\n        llh = out[""log_likelihood""]\n\n        # Action log-probs.\n        actions_tanh_d = (action + 2.0) / 3.0 * 2.0 - 1.0\n        actions_unsquashed = np.arctanh(actions_tanh_d)\n        expected_action_log_llh_output = np.log(\n            norm.pdf(actions_unsquashed, loc=expected_parameters[0], scale=expected_parameters[1])\n        )\n        expected_action_log_llh_output -= np.sum(np.log(1 - actions_tanh_d ** 2 + SMALL_NUMBER), axis=-1, keepdims=True)\n        # expected_action_log_prob_output = np.array([[expected_action_log_prob_output[0][0]],\n        # [expected_action_log_prob_output[1][1]], [expected_action_log_prob_output[2][2]]])\n        test.test(\n            (""get_log_likelihood"", [nn_input, action], ""log_likelihood""),\n            expected_outputs=dict(log_likelihood=expected_action_log_llh_output),\n            decimals=5\n        )\n        recursive_assert_almost_equal(expected_action_log_llh_output, llh, decimals=5)\n\n        # Stochastic sample.\n        actions = test.test((""get_stochastic_action"", nn_input))[""action""]\n        self.assertTrue(actions.dtype == np.float32)\n        self.assertGreaterEqual(actions.min(), -2.0)\n        self.assertLessEqual(actions.max(), 1.0)\n        self.assertTrue(actions.shape == (3, 1))\n\n        # Deterministic sample.\n        actions = test.test((""get_deterministic_action"", nn_input))[""action""]\n        self.assertTrue(actions.dtype == np.float32)\n        self.assertGreaterEqual(actions.min(), -2.0)\n        self.assertLessEqual(actions.max(), 1.0)\n        self.assertTrue(actions.shape == (3, 1))\n\n        # Distribution\'s entropy.\n        entropy = test.test((""get_entropy"", nn_input))[""entropy""]\n        self.assertTrue(entropy.dtype == np.float32)\n        self.assertTrue(entropy.shape == (3, 1))\n'"
rlgraph/tests/components/test_policies_on_container_actions.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport unittest\n\nimport numpy as np\n\nfrom rlgraph.components.policies import Policy, SharedValueFunctionPolicy, DuelingPolicy\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.tests.test_util import config_from_path, recursive_assert_almost_equal\nfrom rlgraph.utils import sigmoid, softmax, relu, SMALL_NUMBER\n\n\nclass TestPoliciesOnContainerActions(unittest.TestCase):\n\n    def test_policy_for_discrete_container_action_space(self):\n        # state_space.\n        state_space = FloatBox(shape=(4,), add_batch_rank=True)\n\n        # Container action space.\n        action_space = dict(\n            type=""dict"",\n            a=BoolBox(),\n            b=IntBox(3),\n            add_batch_rank=True\n        )\n\n        policy = Policy(network_spec=config_from_path(""configs/test_simple_nn.json""), action_space=action_space)\n        test = ComponentTest(\n            component=policy,\n            input_spaces=dict(\n                nn_inputs=state_space,\n                actions=action_space,\n            ),\n            action_space=action_space\n        )\n        policy_params = test.read_variable_values(policy.variable_registry)\n\n        # Some NN inputs (batch size=32).\n        batch_size = 32\n        states = state_space.sample(batch_size)\n        # Raw NN-output.\n        expected_nn_output = np.matmul(states, policy_params[""policy/test-network/hidden-layer/dense/kernel""])\n        test.test((""get_nn_outputs"", states), expected_outputs=expected_nn_output, decimals=6)\n\n        # Raw action layers\' output.\n        expected_action_layer_outputs = dict(\n            a=np.squeeze(np.matmul(expected_nn_output, policy_params[""policy/action-adapter-0/action-network/action-layer/dense/kernel""])),\n            b=np.matmul(expected_nn_output, policy_params[""policy/action-adapter-1/action-network/action-layer/dense/kernel""])\n        )\n        test.test(\n            (""get_adapter_outputs"", states),\n            expected_outputs=dict(adapter_outputs=expected_action_layer_outputs, nn_outputs=expected_nn_output),\n            decimals=5\n        )\n\n        # Logits, parameters (probs) and skip log-probs (numerically unstable for small probs).\n        expected_probs_output = dict(\n            a=np.array(sigmoid(expected_action_layer_outputs[""a""]), dtype=np.float32),\n            b=np.array(softmax(expected_action_layer_outputs[""b""], axis=-1), dtype=np.float32)\n        )\n        test.test(\n            (""get_adapter_outputs_and_parameters"", states, [""adapter_outputs"", ""parameters""]),\n            expected_outputs=dict(\n                adapter_outputs=expected_action_layer_outputs,\n                parameters=dict(a=expected_probs_output[""a""], b=expected_action_layer_outputs[""b""])\n            ), decimals=5\n        )\n\n        print(""Probs: {}"".format(expected_probs_output))\n\n        expected_actions = dict(\n            a=expected_probs_output[""a""] > 0.5,\n            b=np.argmax(expected_action_layer_outputs[""b""], axis=-1)\n        )\n        test.test((""get_action"", states, [""action""]), expected_outputs=dict(action=expected_actions))\n\n        out = test.test((""get_action_and_log_likelihood"", states))\n        action = out[""action""]\n        llh = out[""log_likelihood""]\n\n        # Action log-likelihood (sum of the composite llhs).\n        expected_action_llh_output = \\\n            np.log(np.array([expected_probs_output[""a""][i] if action[""a""][i] else 1.0 - expected_probs_output[""a""][i] for i in range(batch_size)])) + \\\n            np.log(np.array([expected_probs_output[""b""][i][action[""b""][i]] for i in range(batch_size)]))\n        test.test(\n            (""get_log_likelihood"", [states, action]), expected_outputs=dict(\n                log_likelihood=expected_action_llh_output, adapter_outputs=expected_action_layer_outputs\n            ), decimals=5\n        )\n        recursive_assert_almost_equal(expected_action_llh_output, llh, decimals=5)\n\n        # Stochastic sample.\n        out = test.test((""get_stochastic_action"", states), expected_outputs=None)  # dict(action=expected_actions))\n        self.assertTrue(out[""action""][""a""].dtype == np.bool_)\n        self.assertTrue(out[""action""][""a""].shape == (batch_size,))\n        self.assertTrue(out[""action""][""b""].dtype == np.int32)\n        self.assertTrue(out[""action""][""b""].shape == (batch_size,))\n\n        # Deterministic sample.\n        test.test((""get_deterministic_action"", states), expected_outputs=None)  # dict(action=expected_actions))\n        self.assertTrue(out[""action""][""a""].dtype == np.bool_)\n        self.assertTrue(out[""action""][""a""].shape == (batch_size,))\n        self.assertTrue(out[""action""][""b""].dtype == np.int32)\n        self.assertTrue(out[""action""][""b""].shape == (batch_size,))\n\n        # Distribution\'s entropy.\n        out = test.test((""get_entropy"", states), expected_outputs=None)  # dict(entropy=expected_h), decimals=3)\n        self.assertTrue(out[""entropy""][""a""].dtype == np.float32)\n        self.assertTrue(out[""entropy""][""a""].shape == (batch_size,))\n        self.assertTrue(out[""entropy""][""b""].dtype == np.float32)\n        self.assertTrue(out[""entropy""][""b""].shape == (batch_size,))\n\n    def test_shared_value_function_policy_for_discrete_container_action_space(self):\n        # state_space (NN is a simple single fc-layer relu network (2 units), random biases, random weights).\n        state_space = FloatBox(shape=(5,), add_batch_rank=True)\n\n        # action_space (complex nested container action space).\n        action_space = dict(\n            type=""dict"",\n            a=IntBox(2),\n            b=Dict(b1=IntBox(3), b2=IntBox(4)),\n            add_batch_rank=True\n        )\n        #flat_float_action_space = dict(\n        #    type=""dict"",\n        #    a=FloatBox(shape=(2,)),\n        #    b=Dict(b1=FloatBox(shape=(3,)), b2=FloatBox(shape=(4,))),\n        #    add_batch_rank=True\n        #)\n\n        # Policy with baseline action adapter.\n        shared_value_function_policy = SharedValueFunctionPolicy(\n            network_spec=config_from_path(""configs/test_lrelu_nn.json""),\n            action_space=action_space\n        )\n        test = ComponentTest(\n            component=shared_value_function_policy,\n            input_spaces=dict(\n                nn_inputs=state_space,\n                actions=action_space,\n            ),\n            action_space=action_space,\n        )\n        policy_params = test.read_variable_values(shared_value_function_policy.variable_registry)\n\n        base_scope = ""shared-value-function-policy/action-adapter-""\n\n        # Some NN inputs (batch size=2).\n        states = state_space.sample(size=2)\n        # Raw NN-output.\n        expected_nn_output = relu(np.matmul(\n            states, policy_params[""shared-value-function-policy/test-network/hidden-layer/dense/kernel""]\n        ), 0.1)\n        test.test((""get_nn_outputs"", states), expected_outputs=expected_nn_output, decimals=5)\n\n        # Raw action layers\' output.\n        expected_action_layer_outputs = dict(\n            a=np.matmul(expected_nn_output, policy_params[base_scope + ""0/action-network/action-layer/dense/kernel""]),\n            b=dict(b1=np.matmul(expected_nn_output, policy_params[base_scope + ""1/action-network/action-layer/dense/kernel""]),\n                   b2=np.matmul(expected_nn_output, policy_params[base_scope + ""2/action-network/action-layer/dense/kernel""]))\n        )\n        test.test(\n            (""get_adapter_outputs"", states),\n            expected_outputs=dict(adapter_outputs=expected_action_layer_outputs, nn_outputs=expected_nn_output),\n                decimals=5\n        )\n\n        # State-values.\n        expected_state_value_output = np.matmul(\n            expected_nn_output, policy_params[""shared-value-function-policy/value-function-node/dense-layer/dense/kernel""]\n        )\n        test.test(\n            (""get_state_values"", states, [""state_values""]),\n            expected_outputs=dict(state_values=expected_state_value_output),\n            decimals=5\n        )\n\n        # logits-values: One for each action-choice per item in the batch (simply take the remaining out nodes).\n        test.test(\n            (""get_state_values_adapter_outputs_and_parameters"", states, [""state_values"", ""adapter_outputs""]),\n            expected_outputs=dict(\n                state_values=expected_state_value_output, adapter_outputs=expected_action_layer_outputs\n            ),\n            decimals=5\n        )\n\n        # Parameter (probabilities). Softmaxed logits.\n        expected_probs_output = dict(\n            a=softmax(expected_action_layer_outputs[""a""], axis=-1),\n            b=dict(\n                b1=softmax(expected_action_layer_outputs[""b""][""b1""], axis=-1),\n                b2=softmax(expected_action_layer_outputs[""b""][""b2""], axis=-1)\n            )\n        )\n        test.test((""get_adapter_outputs_and_parameters"", states, [""adapter_outputs"", ""parameters""]), expected_outputs=dict(\n            adapter_outputs=expected_action_layer_outputs,\n            parameters=expected_action_layer_outputs\n        ), decimals=5)\n\n        print(""Probs: {}"".format(expected_probs_output))\n\n        # Action sample.\n        expected_actions = dict(\n            a=np.argmax(expected_action_layer_outputs[""a""], axis=-1),\n            b=dict(\n                b1=np.argmax(expected_action_layer_outputs[""b""][""b1""], axis=-1),\n                b2=np.argmax(expected_action_layer_outputs[""b""][""b2""], axis=-1)\n            )\n        )\n        test.test((""get_action"", states, [""action""]), expected_outputs=dict(action=expected_actions))\n\n        out = test.test((""get_action_and_log_likelihood"", states))\n        action = out[""action""]\n        llh = out[""log_likelihood""]\n\n        # Action log-likelihood.\n        expected_action_llh_output = np.log(np.array([expected_probs_output[""a""][0][action[""a""][0]],\n                                                      expected_probs_output[""a""][1][action[""a""][1]]])) + \\\n                                     np.log(np.array([expected_probs_output[""b""][""b1""][0][action[""b""][""b1""][0]],\n                                                      expected_probs_output[""b""][""b1""][1][action[""b""][""b1""][1]]\n                                                      ])\n                                            ) + \\\n                                     np.log(np.array([expected_probs_output[""b""][""b2""][0][action[""b""][""b2""][0]],\n                                                      expected_probs_output[""b""][""b2""][1][action[""b""][""b2""][1]],\n                                                      ])\n                                            )\n        test.test(\n            (""get_log_likelihood"", [states, action]), expected_outputs=dict(\n                log_likelihood=expected_action_llh_output, adapter_outputs=expected_action_layer_outputs\n            ), decimals=5\n        )\n        recursive_assert_almost_equal(expected_action_llh_output, llh, decimals=5)\n\n        # Stochastic sample.\n        out = test.test((""get_stochastic_action"", states), expected_outputs=None)[""action""]\n        self.assertTrue(out[""a""].dtype == np.int32)\n        self.assertTrue(out[""a""].shape == (2,))\n        self.assertTrue(out[""b""][""b1""].dtype == np.int32)\n        self.assertTrue(out[""b""][""b1""].shape == (2,))\n        self.assertTrue(out[""b""][""b2""].dtype == np.int32)\n        self.assertTrue(out[""b""][""b2""].shape == (2,))\n\n        # Deterministic sample.\n        out = test.test((""get_deterministic_action"", states), expected_outputs=None)[""action""]\n        self.assertTrue(out[""a""].dtype == np.int32)\n        self.assertTrue(out[""a""].shape == (2,))\n        self.assertTrue(out[""b""][""b1""].dtype == np.int32)\n        self.assertTrue(out[""b""][""b1""].shape == (2,))\n        self.assertTrue(out[""b""][""b2""].dtype == np.int32)\n        self.assertTrue(out[""b""][""b2""].shape == (2,))\n\n        # Distribution\'s entropy.\n        out = test.test((""get_entropy"", states), expected_outputs=None)[""entropy""]\n        self.assertTrue(out[""a""].dtype == np.float32)\n        self.assertTrue(out[""a""].shape == (2,))\n        self.assertTrue(out[""b""][""b1""].dtype == np.float32)\n        self.assertTrue(out[""b""][""b1""].shape == (2,))\n        self.assertTrue(out[""b""][""b2""].dtype == np.float32)\n        self.assertTrue(out[""b""][""b2""].shape == (2,))\n\n    def test_shared_value_function_policy_for_discrete_container_action_space_with_time_rank_folding(self):\n        # state_space (NN is a simple single fc-layer relu network (2 units), random biases, random weights).\n        state_space = FloatBox(shape=(6,), add_batch_rank=True, add_time_rank=True)\n\n        # Action_space.\n        action_space = Tuple(\n            IntBox(2),\n            IntBox(3),\n            Dict(\n                a=IntBox(4),\n            ),\n            add_batch_rank=True,\n            add_time_rank=True\n        )\n        #flat_float_action_space = Tuple(\n        #    FloatBox(shape=(2,)),\n        #    FloatBox(shape=(3,)),\n        #    Dict(\n        #        a=FloatBox(shape=(4,)),\n        #    ),\n        #    add_batch_rank=True,\n        #    add_time_rank=True\n        #)\n\n        # Policy with baseline action adapter AND batch-apply over the entire policy (NN + ActionAdapter + distr.).\n        network_spec = config_from_path(""configs/test_lrelu_nn.json"")\n        network_spec[""fold_time_rank""] = True\n        network_spec[""unfold_time_rank""] = True\n        shared_value_function_policy = SharedValueFunctionPolicy(\n            network_spec=network_spec,\n            action_adapter_spec=dict(fold_time_rank=True, unfold_time_rank=True),\n            action_space=action_space,\n            value_fold_time_rank=True,\n            value_unfold_time_rank=True\n        )\n        test = ComponentTest(\n            component=shared_value_function_policy,\n            input_spaces=dict(\n                nn_inputs=state_space,\n                actions=action_space,\n            ),\n            action_space=action_space,\n        )\n        policy_params = test.read_variable_values(shared_value_function_policy.variable_registry)\n        base_scope = ""shared-value-function-policy/action-adapter-""\n\n        # Some NN inputs.\n        states = state_space.sample(size=(2, 3))\n        states_folded = np.reshape(states, newshape=(6, 6))\n        # Raw NN-output (still folded).\n        expected_nn_output = np.reshape(relu(np.matmul(\n            states_folded, policy_params[""shared-value-function-policy/test-network/hidden-layer/dense/kernel""]\n        ), 0.1), newshape=(2, 3, 3))\n        test.test((""get_nn_outputs"", states), expected_outputs=expected_nn_output, decimals=5)\n\n        # Raw action layer output; Expected shape=(3,3): 3=batch, 2=action categories + 1 state value\n        expected_action_layer_output = tuple([\n            np.matmul(expected_nn_output, policy_params[base_scope + ""0/action-network/action-layer/dense/kernel""]),\n            np.matmul(expected_nn_output, policy_params[base_scope + ""1/action-network/action-layer/dense/kernel""]),\n            dict(\n                a=np.matmul(expected_nn_output, policy_params[base_scope + ""2/action-network/action-layer/dense/kernel""])\n            )\n        ])\n        expected_action_layer_output_unfolded = tuple([\n            np.reshape(expected_action_layer_output[0], newshape=(2, 3, 2)),\n            np.reshape(expected_action_layer_output[1], newshape=(2, 3, 3)),\n            dict(\n                a=np.reshape(expected_action_layer_output[2][""a""], newshape=(2, 3, 4))\n            )\n        ])\n        test.test(\n            (""get_adapter_outputs"", states),\n            expected_outputs=dict(adapter_outputs=expected_action_layer_output_unfolded, nn_outputs=expected_nn_output),\n            decimals=5\n        )\n\n        # State-values: One for each item in the batch.\n        expected_state_value_output = np.matmul(\n            expected_nn_output,\n            policy_params[""shared-value-function-policy/value-function-node/dense-layer/dense/kernel""]\n        )\n        expected_state_value_output_unfolded = np.reshape(expected_state_value_output, newshape=(2, 3, 1))\n        test.test(\n            (""get_state_values"", states, [""state_values""]),\n            expected_outputs=dict(state_values=expected_state_value_output_unfolded),\n            decimals=5\n        )\n\n        test.test(\n            (""get_state_values_adapter_outputs_and_parameters"", states, [""state_values"", ""adapter_outputs""]),\n            expected_outputs=dict(\n                state_values=expected_state_value_output_unfolded, adapter_outputs=expected_action_layer_output_unfolded\n            ),\n            decimals=5\n        )\n\n        # Parameter (probabilities). Softmaxed logits.\n        expected_probs_output = tuple([\n            softmax(expected_action_layer_output_unfolded[0], axis=-1),\n            softmax(expected_action_layer_output_unfolded[1], axis=-1),\n            dict(\n                a=softmax(expected_action_layer_output_unfolded[2][""a""], axis=-1)\n            )\n        ])\n        test.test(\n            (""get_adapter_outputs_and_parameters"", states, [""adapter_outputs"", ""parameters""]),\n            expected_outputs=dict(\n                adapter_outputs=expected_action_layer_output_unfolded, parameters=expected_action_layer_output_unfolded\n            ),\n            decimals=5\n        )\n\n        print(""Probs: {}"".format(expected_probs_output))\n\n        expected_actions = tuple([\n            np.argmax(expected_action_layer_output_unfolded[0], axis=-1),\n            np.argmax(expected_action_layer_output_unfolded[1], axis=-1),\n            dict(\n                a=np.argmax(expected_action_layer_output_unfolded[2][""a""], axis=-1),\n            )\n        ])\n        test.test((""get_action"", states, [""action""]), expected_outputs=dict(action=expected_actions))\n\n        out = test.test((""get_action_and_log_likelihood"", states))\n        action = out[""action""]\n        llh = out[""log_likelihood""]\n\n        # Action log-likelihood.\n        expected_action_llh_output = np.log(np.array([[\n            expected_probs_output[0][0][0][action[0][0][0]],\n            expected_probs_output[0][0][1][action[0][0][1]],\n            expected_probs_output[0][0][2][action[0][0][2]],\n        ], [\n            expected_probs_output[0][1][0][action[0][1][0]],\n            expected_probs_output[0][1][1][action[0][1][1]],\n            expected_probs_output[0][1][2][action[0][1][2]],\n        ]])) + np.log(np.array([[\n            expected_probs_output[1][0][0][action[1][0][0]],\n            expected_probs_output[1][0][1][action[1][0][1]],\n            expected_probs_output[1][0][2][action[1][0][2]],\n        ], [\n            expected_probs_output[1][1][0][action[1][1][0]],\n            expected_probs_output[1][1][1][action[1][1][1]],\n            expected_probs_output[1][1][2][action[1][1][2]],\n        ]])) + np.log(np.array([[\n            expected_probs_output[2][""a""][0][0][action[2][""a""][0][0]],\n            expected_probs_output[2][""a""][0][1][action[2][""a""][0][1]],\n            expected_probs_output[2][""a""][0][2][action[2][""a""][0][2]],\n        ], [\n            expected_probs_output[2][""a""][1][0][action[2][""a""][1][0]],\n            expected_probs_output[2][""a""][1][1][action[2][""a""][1][1]],\n            expected_probs_output[2][""a""][1][2][action[2][""a""][1][2]],\n        ]]))\n        test.test(\n            (""get_log_likelihood"", [states, action]), expected_outputs=dict(\n                log_likelihood=expected_action_llh_output, adapter_outputs=expected_action_layer_output_unfolded\n            ), decimals=5\n        )\n        recursive_assert_almost_equal(expected_action_llh_output, llh, decimals=5)\n\n        # Deterministic sample.\n        out = test.test((""get_deterministic_action"", states), expected_outputs=None)\n        self.assertTrue(out[""action""][0].dtype == np.int32)\n        self.assertTrue(out[""action""][0].shape == (2, 3))  # Make sure output is unfolded.\n        self.assertTrue(out[""action""][1].dtype == np.int32)\n        self.assertTrue(out[""action""][1].shape == (2, 3))  # Make sure output is unfolded.\n        self.assertTrue(out[""action""][2][""a""].dtype == np.int32)\n        self.assertTrue(out[""action""][2][""a""].shape == (2, 3))  # Make sure output is unfolded.\n\n        # Stochastic sample.\n        out = test.test((""get_stochastic_action"", states), expected_outputs=None)\n        self.assertTrue(out[""action""][0].dtype == np.int32)\n        self.assertTrue(out[""action""][0].shape == (2, 3))  # Make sure output is unfolded.\n        self.assertTrue(out[""action""][1].dtype == np.int32)\n        self.assertTrue(out[""action""][1].shape == (2, 3))  # Make sure output is unfolded.\n        self.assertTrue(out[""action""][2][""a""].dtype == np.int32)\n        self.assertTrue(out[""action""][2][""a""].shape == (2, 3))  # Make sure output is unfolded.\n\n        # Distribution\'s entropy.\n        out = test.test((""get_entropy"", states), expected_outputs=None)\n        self.assertTrue(out[""entropy""][0].dtype == np.float32)\n        self.assertTrue(out[""entropy""][0].shape == (2, 3))  # Make sure output is unfolded.\n        self.assertTrue(out[""entropy""][1].dtype == np.float32)\n        self.assertTrue(out[""entropy""][1].shape == (2, 3))  # Make sure output is unfolded.\n        self.assertTrue(out[""entropy""][2][""a""].dtype == np.float32)\n        self.assertTrue(out[""entropy""][2][""a""].shape == (2, 3))  # Make sure output is unfolded.\n\n    def test_policy_for_discrete_action_space_with_dueling_layer(self):\n        # state_space (NN is a simple single fc-layer relu network (2 units), random biases, random weights).\n        nn_input_space = FloatBox(shape=(5,), add_batch_rank=True)\n\n        # Action space.\n        action_space = Dict(dict(\n            a=Tuple(IntBox(2), IntBox(3)),\n            b=Dict(dict(ba=IntBox(4)))\n        ), add_batch_rank=True)\n        #flat_float_action_space = Dict(dict(\n        #    a=Tuple(FloatBox(shape=(2,)), FloatBox(shape=(3,))),\n        #    b=Dict(dict(ba=FloatBox(shape=(4,))))\n        #), add_batch_rank=True)\n\n        # Policy with dueling logic.\n        policy = DuelingPolicy(\n            network_spec=config_from_path(""configs/test_lrelu_nn.json""),\n            # Make all sub action adapters the same.\n            action_adapter_spec=dict(\n                pre_network_spec=[\n                    dict(type=""dense"", units=5, activation=""lrelu"", activation_params=[0.2])\n                ]\n            ),\n            units_state_value_stream=2,\n            action_space=action_space\n        )\n        test = ComponentTest(\n            component=policy,\n            input_spaces=dict(\n                nn_inputs=nn_input_space,\n                actions=action_space,\n                #logits=flat_float_action_space,\n                #parameters=flat_float_action_space\n            ),\n            action_space=action_space\n        )\n        policy_params = test.read_variable_values(policy.variable_registry)\n\n        # Some NN inputs.\n        nn_input = nn_input_space.sample(size=3)\n        # Raw NN-output.\n        expected_nn_output = relu(np.matmul(\n            nn_input, policy_params[""dueling-policy/test-network/hidden-layer/dense/kernel""]), 0.2\n        )\n        test.test((""get_nn_outputs"", nn_input), expected_outputs=expected_nn_output, decimals=5)\n\n        # Raw action layer output.\n        expected_raw_advantages = dict(\n            a=(\n                np.matmul(\n                    relu(np.matmul(\n                        expected_nn_output,\n                        policy_params[""dueling-policy/action-adapter-0/action-network/dense-layer/dense/kernel""]\n                    ), 0.2), policy_params[""dueling-policy/action-adapter-0/action-network/action-layer/dense/kernel""]\n                ),\n                np.matmul(\n                    relu(np.matmul(\n                        expected_nn_output,\n                        policy_params[""dueling-policy/action-adapter-1/action-network/dense-layer/dense/kernel""]\n                    ), 0.2), policy_params[""dueling-policy/action-adapter-1/action-network/action-layer/dense/kernel""]\n                ),\n            ),\n            b=dict(ba=np.matmul(\n                relu(np.matmul(\n                    expected_nn_output,\n                    policy_params[""dueling-policy/action-adapter-2/action-network/dense-layer/dense/kernel""]\n                ), 0.2), policy_params[""dueling-policy/action-adapter-2/action-network/action-layer/dense/kernel""]\n            ))\n        )\n\n        # Single state values.\n        expected_state_values = np.matmul(relu(np.matmul(\n            expected_nn_output,\n            policy_params[""dueling-policy/dense-layer-state-value-stream/dense/kernel""]\n        )), policy_params[""dueling-policy/state-value-node/dense/kernel""])\n        test.test(\n            (""get_state_values"", nn_input, [""state_values""]),\n            expected_outputs=dict(state_values=expected_state_values),\n            decimals=5\n        )\n\n        # State-values: One for each item in the batch.\n        expected_q_values_output = dict(\n            a=(\n                expected_state_values + expected_raw_advantages[""a""][0] - np.mean(expected_raw_advantages[""a""][0],\n                                                                                  axis=-1, keepdims=True),\n                expected_state_values + expected_raw_advantages[""a""][1] - np.mean(expected_raw_advantages[""a""][1],\n                                                                                  axis=-1, keepdims=True),\n            ),\n            b=dict(ba=expected_state_values + expected_raw_advantages[""b""][""ba""] - np.mean(\n                expected_raw_advantages[""b""][""ba""], axis=-1, keepdims=True))\n        )\n        test.test(\n            (""get_adapter_outputs"", nn_input),\n            expected_outputs=dict(\n                adapter_outputs=expected_q_values_output, nn_outputs=expected_nn_output,\n                advantages=expected_raw_advantages, q_values=expected_q_values_output\n            ),\n            decimals=5\n        )\n\n        test.test(\n            (""get_adapter_outputs_and_parameters"", nn_input, [""adapter_outputs""]),\n            expected_outputs=dict(adapter_outputs=expected_q_values_output),\n            decimals=5\n        )\n\n        # Parameter (probabilities). Softmaxed q_values.\n        expected_probs_output = dict(\n            a=(\n                softmax(expected_q_values_output[""a""][0], axis=-1),\n                softmax(expected_q_values_output[""a""][1], axis=-1)\n            ),\n            b=dict(ba=np.maximum(softmax(expected_q_values_output[""b""][""ba""], axis=-1), SMALL_NUMBER))\n        )\n        expected_log_probs_output = dict(\n            a=(np.log(expected_probs_output[""a""][0]),\n               np.log(expected_probs_output[""a""][1])),\n            b=dict(ba=np.log(expected_probs_output[""b""][""ba""]))\n        )\n        test.test(\n            (""get_adapter_outputs_and_parameters"", nn_input, [""adapter_outputs"", ""parameters"", ""log_probs""]),\n            expected_outputs=dict(\n                adapter_outputs=expected_q_values_output, parameters=expected_q_values_output,\n                log_probs=expected_log_probs_output\n            ),\n            decimals=5\n        )\n\n        print(""Probs: {}"".format(expected_probs_output))\n\n        expected_actions = dict(\n            a=(np.argmax(expected_q_values_output[""a""][0], axis=-1),\n               np.argmax(expected_q_values_output[""a""][1], axis=-1)),\n            b=dict(ba=np.argmax(expected_q_values_output[""b""][""ba""], axis=-1))\n        )\n        test.test((""get_action"", nn_input, [""action""]), expected_outputs=dict(action=expected_actions))\n\n        out = test.test((""get_action_and_log_likelihood"", nn_input))\n        action = out[""action""]\n        llh = out[""log_likelihood""]\n\n        # Action log-likelihood.\n        expected_action_llh_output = np.array([\n            expected_log_probs_output[""a""][0][0][action[""a""][0][0]],\n            expected_log_probs_output[""a""][0][1][action[""a""][0][1]],\n            expected_log_probs_output[""a""][0][2][action[""a""][0][2]],\n        ]) + np.array([\n            expected_log_probs_output[""a""][1][0][action[""a""][1][0]],\n            expected_log_probs_output[""a""][1][1][action[""a""][1][1]],\n            expected_log_probs_output[""a""][1][2][action[""a""][1][2]],\n        ]) + np.array([\n            expected_log_probs_output[""b""][""ba""][0][action[""b""][""ba""][0]],\n            expected_log_probs_output[""b""][""ba""][1][action[""b""][""ba""][1]],\n            expected_log_probs_output[""b""][""ba""][2][action[""b""][""ba""][2]],\n        ])\n        test.test(\n            (""get_log_likelihood"", [nn_input, action]), expected_outputs=dict(\n                log_likelihood=expected_action_llh_output, adapter_outputs=expected_q_values_output\n            ), decimals=5\n        )\n        recursive_assert_almost_equal(expected_action_llh_output, llh, decimals=5)\n\n        # Stochastic sample.\n        out = test.test((""get_stochastic_action"", nn_input), expected_outputs=None)\n        self.assertTrue(out[""action""][""a""][0].dtype == np.int32)\n        self.assertTrue(out[""action""][""a""][0].shape == (3,))\n        self.assertTrue(out[""action""][""a""][1].dtype == np.int32)\n        self.assertTrue(out[""action""][""a""][1].shape == (3,))\n        self.assertTrue(out[""action""][""b""][""ba""].dtype == np.int32)\n        self.assertTrue(out[""action""][""b""][""ba""].shape == (3,))\n\n        # Deterministic sample.\n        out = test.test((""get_deterministic_action"", nn_input), expected_outputs=None)\n        self.assertTrue(out[""action""][""a""][0].dtype == np.int32)\n        self.assertTrue(out[""action""][""a""][0].shape == (3,))\n        self.assertTrue(out[""action""][""a""][1].dtype == np.int32)\n        self.assertTrue(out[""action""][""a""][1].shape == (3,))\n        self.assertTrue(out[""action""][""b""][""ba""].dtype == np.int32)\n        self.assertTrue(out[""action""][""b""][""ba""].shape == (3,))\n\n        # Distribution\'s entropy.\n        out = test.test((""get_entropy"", nn_input), expected_outputs=None)\n        self.assertTrue(out[""entropy""][""a""][0].dtype == np.float32)\n        self.assertTrue(out[""entropy""][""a""][0].shape == (3,))\n        self.assertTrue(out[""entropy""][""a""][1].dtype == np.float32)\n        self.assertTrue(out[""entropy""][""a""][1].shape == (3,))\n        self.assertTrue(out[""entropy""][""b""][""ba""].dtype == np.float32)\n        self.assertTrue(out[""entropy""][""b""][""ba""].shape == (3,))\n'"
rlgraph/tests/components/test_ppo_loss_functions.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\nfrom math import log\n\nimport numpy as np\nfrom rlgraph.components.loss_functions import PPOLossFunction\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\n\n\nclass TestPPOLossFunctions(unittest.TestCase):\n\n    input_spaces = dict(\n        loss_per_item=FloatBox(add_batch_rank=True),\n        log_probs=FloatBox(shape=(1,), add_batch_rank=True),\n        prev_log_probs=FloatBox(shape=(1,), add_batch_rank=True),\n        state_values=FloatBox(shape=(1,), add_batch_rank=True),\n        prev_state_values=FloatBox(shape=(1,), add_batch_rank=True),\n        advantages=FloatBox(add_batch_rank=True),\n        entropy=FloatBox(add_batch_rank=True),\n        time_percentage=float\n    )\n\n    def test_ppo_loss_function_on_int_action_space(self):\n        action_space = IntBox(2, add_batch_rank=True)\n        clip_ratio = 0.2\n\n        ppo_loss_function = PPOLossFunction(clip_ratio=clip_ratio, value_function_clipping=False)\n\n        test = ComponentTest(component=ppo_loss_function, input_spaces=self.input_spaces, action_space=action_space)\n\n        # Batch of size=n.\n        log_probs = np.array([[log(0.4)], [log(0.9)], [log(0.1)]])\n        prev_log_probs = np.array([[log(0.3)], [log(0.95)], [log(0.2)]])\n        state_values = np.array([[-2.0], [-1.0], [1.0]])\n        prev_state_values = np.array([[-3.4], [-1.3], [0.3]])\n        advantages = np.array([1.0, 3.0, 2.0])\n        entropy = np.array([0.7, 0.3, 3.2])\n\n        """"""\n        Calculation of PG loss term:\n        # IS ratios\n        rhos = probs / prev_probs = exp(log(probs/prev_probs)) = exp(log_probs - prev_log_probs)\n        # clipping around 1.0\n        clipped = clip(rhos, 1.0-clip_ratio, 1.0+clip_ratio)\n        # entropy loss term\n        Le = - weight * entropy\n        \n        L = min(clipped * A, rhos * A) + Le \n        """"""\n        rhos = np.exp(log_probs - prev_log_probs)\n        clipped_rhos = np.clip(rhos, 1.0 - clip_ratio, 1.0 + clip_ratio)\n        expanded_advantages = np.expand_dims(advantages, axis=-1)\n        clipped_advantages = -np.minimum(rhos * expanded_advantages, clipped_rhos * expanded_advantages)\n        entropy_term = -0.00025 * np.expand_dims(entropy, axis=-1)  # 0.00025 == default entropy weight\n\n        expected_pg_loss_per_item = np.squeeze(clipped_advantages + entropy_term)\n\n        test.test(\n            (""pg_loss_per_item"", [log_probs, prev_log_probs, advantages, entropy]),\n            expected_outputs=expected_pg_loss_per_item, decimals=3\n        )\n\n        v_targets = advantages + np.squeeze(prev_state_values)  # Q-value targets\n        expected_value_loss_per_item = np.square(np.squeeze(state_values) - v_targets)\n\n        test.test(\n            (""value_function_loss_per_item"", [state_values, prev_state_values, advantages]),\n            expected_outputs=expected_value_loss_per_item, decimals=3\n        )\n\n        # All together.\n        test.test(\n            (""loss_per_item"", [log_probs, prev_log_probs, state_values, prev_state_values, advantages, entropy]),\n            expected_outputs=[expected_pg_loss_per_item, expected_value_loss_per_item], decimals=3\n        )\n\n        # Expect the mean over the batch.\n        test.test((""loss_average"", expected_pg_loss_per_item), expected_outputs=expected_pg_loss_per_item.mean())\n\n        # Both.\n        test.test(\n            (""loss"", [log_probs, prev_log_probs, state_values, prev_state_values, advantages, entropy]),\n            expected_outputs=[\n                expected_pg_loss_per_item.mean(), expected_pg_loss_per_item,\n                expected_value_loss_per_item.mean(), expected_value_loss_per_item\n            ], decimals=3\n        )\n\n'"
rlgraph/tests/components/test_preprocess_layers.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport unittest\n\nimport cv2\nimport numpy as np\n\nfrom rlgraph.components.layers import GrayScale, ReShape, Multiply, Divide, Clip, ImageBinary, ImageResize, ImageCrop, \\\n    MovingStandardize\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest, recursive_assert_almost_equal\nfrom rlgraph.utils import SMALL_NUMBER\n\n\nclass TestPreprocessLayers(unittest.TestCase):\n\n    def test_multiply(self):\n        multiply = Multiply(factor=2.0)\n        test = ComponentTest(component=multiply, input_spaces=dict(inputs=FloatBox(\n            shape=(2, 1), add_batch_rank=True)\n        ))\n\n        test.test(""reset"")\n        # Batch=2\n        input_ = np.array([[[1.0], [2.0]], [[3.0], [4.0]]])\n        expected = np.array([[[2.0], [4.0]], [[6.0], [8.0]]])\n        test.test((""call"", input_), expected_outputs=expected)\n\n    def test_divide(self):\n        divide = Divide(divisor=10.0)\n        test = ComponentTest(component=divide, input_spaces=dict(inputs=FloatBox(shape=(1, 2),\n                                                                                               add_batch_rank=False)))\n\n        test.test(""reset"")\n\n        input_ = np.array([[10.0, 100.0]])\n        expected = np.array([[1.0, 10.0]])\n        test.test((""call"", input_), expected_outputs=expected)\n\n    def test_clip(self):\n        clip = Clip(min=0.0, max=1.0)\n        # Grayscale image of 2x2 size.\n        test = ComponentTest(\n            component=clip,\n            input_spaces=dict(inputs=FloatBox(shape=(2, 2), add_batch_rank=True))\n        )\n\n        test.test(""reset"")\n        # Batch=3\n        input_images = np.array([\n            [[125.6, 10.3], [-45, 5.234]],\n            [[-10.0, 1.0004], [0.0, -0.0003]],\n            [[0.0005, 0.00000009], [90.0, 10000901.347]]\n        ])\n        expected = np.array([\n            [[1.0, 1.0], [0.0, 1.0]],\n            [[0.0, 1.0], [0.0, 0.0]],\n            [[0.0005, 0.00000009], [1.0, 1.0]]\n        ])\n        test.test((""call"", input_images), expected_outputs=expected)\n\n    def test_grayscale_with_uint8_image(self):\n        # last rank is always the color rank (its dim must match len(grayscale-weights))\n        space = IntBox(256, shape=(1, 1, 2), dtype=""uint8"", add_batch_rank=True)\n        grayscale = GrayScale(weights=(0.5, 0.5), keep_rank=False)\n\n        test = ComponentTest(component=grayscale, input_spaces=dict(inputs=space))\n\n        # Run the test (batch of 3 images).\n        input_ = space.sample(size=3)\n        expected = np.sum(input_, axis=-1, keepdims=False)\n        expected = (expected / 2).astype(input_.dtype)\n        test.test(""reset"")\n        print(test.test((""call"", input_), expected_outputs=expected))\n\n    def test_grayscale_python_with_uint8_image(self):\n        # last rank is always the color rank (its dim must match len(grayscale-weights))\n        space = IntBox(256, shape=(1, 1, 3), dtype=""uint8"", add_batch_rank=True)\n        grayscale = GrayScale(keep_rank=False, backend=""python"")\n\n        # Run the test (batch of 2 images).\n        input_ = space.sample(size=2)\n        expected = np.round(np.dot(input_[:, :, :, :3], [0.299, 0.587, 0.114]), 0).astype(dtype=input_.dtype)\n\n        out = grayscale._graph_fn_call(input_)\n        recursive_assert_almost_equal(out, expected)\n\n    def test_split_inputs_on_grayscale(self):\n        # last rank is always the color rank (its dim must match len(grayscale-weights))\n        space = Dict.from_spec(dict(\n            a=Tuple(FloatBox(shape=(1, 1, 2)), FloatBox(shape=(1, 2, 2))),\n            b=FloatBox(shape=(2, 2, 2, 2)),\n            c=dict(type=float, shape=(2,))  # single scalar pixel\n        ))\n        grayscale = GrayScale(weights=(0.5, 0.5), keep_rank=False)\n\n        test = ComponentTest(component=grayscale, input_spaces=dict(inputs=space))\n\n        # Run the test.\n        input_ = dict(\n            a=(\n                np.array([[[3.0, 5.0]]]), np.array([[[3.0, 5.0], [1.0, 5.0]]])\n            ),\n            b=np.array([[[[2.0, 4.0], [2.0, 4.0]],\n                         [[2.0, 4.0], [2.0, 4.0]]],\n                        [[[2.0, 4.0], [2.0, 4.0]],\n                         [[2.0, 4.0], [2.0, 4.0]]]]\n                       ),\n            c=np.array([0.6, 0.8])\n        )\n        expected = dict(\n            a=(\n                np.array([[4.0]]), np.array([[4.0, 3.0]])\n            ),\n            b=np.array([[[3.0, 3.0], [3.0, 3.0]], [[3.0, 3.0], [3.0, 3.0]]]),\n            c=0.7\n        )\n        test.test(""reset"")\n        test.test((""call"", input_), expected_outputs=expected)\n\n    def test_split_graph_on_reshape_flatten(self):\n        space = Dict.from_spec(\n            dict(\n                a=Tuple(FloatBox(shape=(1, 1, 2)), FloatBox(shape=(1, 2, 2))),\n                b=FloatBox(shape=(2, 2, 3)),\n                c=dict(type=float, shape=(2,)),\n                d=IntBox(3)\n            ),\n            add_batch_rank=True\n        )\n        flatten = ReShape(flatten=True, flatten_categories={""d"": 3})\n\n        test = ComponentTest(component=flatten, input_spaces=dict(inputs=space))\n\n        input_ = dict(\n            a=(\n                np.array([[[[3.0, 5.0]]], [[[1.0, 5.2]]]]), np.array([[[[3.1, 3.2], [3.3, 3.4]]],\n                                                                      [[[3.5, 3.6], [3.7, 3.8]]]])\n            ),\n            b=np.array([[[[0.01, 0.02, 0.03], [0.04, 0.05, 0.06]], [[0.07, 0.08, 0.09], [0.10, 0.11, 0.12]]],\n                        [[[0.13, 0.14, 0.15], [0.16, 0.17, 0.18]], [[0.19, 0.20, 0.21], [0.22, 0.23, 0.24]]]]),\n            c=np.array([[0.1, 0.2], [0.3, 0.4]]),\n            d=np.array([2, 0])\n        )\n        expected = dict(\n            a=(\n                np.array([[3.0, 5.0], [1.0, 5.2]], dtype=np.float32), np.array([[3.1, 3.2, 3.3, 3.4], [3.5, 3.6, 3.7, 3.8]], dtype=np.float32)\n            ),\n            b=np.array([[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12],\n                        [0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24]]\n            ),\n            c=np.array([[0.1, 0.2], [0.3, 0.4]], dtype=np.float32),\n            d=np.array([[0.0, 0.0, 1.0], [1.0, 0.0, 0.0]])  # category (one-hot) flatten\n        )\n        test.test(""reset"")\n        test.test((""call"", input_), expected_outputs=expected)\n\n    def test_image_resize(self):\n        image_resize = ImageResize(width=4, height=4, interpolation=""bilinear"")\n        # Some image of 16x16x3 size.\n        test = ComponentTest(\n            component=image_resize, input_spaces=dict(inputs=FloatBox(shape=(16, 16, 3), add_batch_rank=False))\n        )\n\n        test.test(""reset"")\n\n        input_image = cv2.imread(os.path.join(os.path.dirname(__file__), ""images/16x16x3_image.bmp""))\n        expected = cv2.imread(os.path.join(os.path.dirname(__file__), ""images/4x4x3_image_resized.bmp""))\n        assert expected is not None\n\n        test.test((""call"", input_image), expected_outputs=expected)\n\n    def test_image_crop(self):\n        image_crop = ImageCrop(x=7, y=1, width=8, height=12)\n\n        # Some image of 16x16x3 size.\n        test = ComponentTest(\n            component=image_crop, input_spaces=dict(inputs=FloatBox(shape=(16, 16, 3),\n                                                                                  add_batch_rank=False))\n        )\n\n        test.test(""reset"")\n\n        input_image = cv2.imread(os.path.join(os.path.dirname(__file__), ""images/16x16x3_image.bmp""))\n        expected = cv2.imread(os.path.join(os.path.dirname(__file__), ""images/8x12x3_image_cropped.bmp""))\n        assert expected is not None\n\n        test.test((""call"", input_image), expected_outputs=expected)\n\n    def test_python_image_crop(self):\n        image_crop = ImageCrop(x=7, y=1, width=8, height=12, backend=""python"")\n        image_crop.create_variables(input_spaces=dict(\n            inputs=FloatBox(shape=(16, 16, 3)), add_batch_rank=False)\n        )\n\n        input_image = cv2.imread(os.path.join(os.path.dirname(__file__), ""images/16x16x3_image.bmp""))\n        expected = cv2.imread(os.path.join(os.path.dirname(__file__), ""images/8x12x3_image_cropped.bmp""))\n        assert expected is not None\n\n        out = image_crop._graph_fn_call(input_image)\n        recursive_assert_almost_equal(out, expected)\n\n    def test_black_and_white(self):\n        binary = ImageBinary()\n        # Color image of 2x2x3 size.\n        test = ComponentTest(component=binary, input_spaces=dict(inputs=FloatBox(shape=(2, 2, 3), add_batch_rank=True)))\n\n        test.test(""reset"")\n        # Batch=2\n        input_images = np.array([\n            [[[0, 1, 0], [10, 9, 5]], [[0, 0, 0], [0, 0, 1]]],\n            [[[255, 255, 255], [0, 0, 0]], [[0, 0, 0], [255, 43, 0]]]\n        ])\n        expected = np.array([\n            [[1, 1], [0, 1]],\n            [[1, 0], [0, 1]]\n        ])\n        test.test((""call"", input_images), expected_outputs=expected)\n\n    def test_moving_standardize_python(self):\n        env = OpenAIGymEnv(""Pong-v0"")\n        space = env.state_space\n\n        moving_standardize = MovingStandardize(backend=""python"")\n        moving_standardize.create_variables(input_spaces=dict(\n                    inputs=space\n                ), action_space=None)\n        samples = [space.sample() for _ in range(100)]\n        out = None\n        for sample in samples:\n            out = moving_standardize._graph_fn_call(sample)\n\n        # Assert shape remains intact.\n        expected_shape = (1, ) + space.shape\n        self.assertEqual(expected_shape,  moving_standardize.mean_est.shape)\n        # Assert mean estimate.\n        expected_mean = np.mean(samples, axis=0)\n        self.assertTrue(np.allclose(moving_standardize.mean_est, expected_mean))\n\n        expected_variance = np.var(samples, ddof=1, axis=0)\n        variance_estimate = moving_standardize.std_sum_est / (moving_standardize.sample_count - 1.0)\n        self.assertEqual(expected_shape,  variance_estimate.shape)\n        self.assertTrue(np.allclose(variance_estimate, expected_variance))\n\n        std = np.sqrt(variance_estimate) + SMALL_NUMBER\n\n        # Final output.\n        expected_out = (samples[-1] - moving_standardize.mean_est) / std\n        self.assertTrue(np.allclose(out, expected_out))\n'"
rlgraph/tests/components/test_preprocessor_stacks.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\nfrom copy import deepcopy\n\nimport numpy as np\nfrom six.moves import xrange as range_\n\nfrom rlgraph.agents import ApexAgent\nfrom rlgraph.components.layers import GrayScale, Multiply\nfrom rlgraph.components.neural_networks import PreprocessorStack\nfrom rlgraph.environments import SequentialVectorEnv\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest, recursive_assert_almost_equal\nfrom rlgraph.tests.test_util import config_from_path\n\n\nclass TestPreprocessorStacks(unittest.TestCase):\n    """"""\n    Tests preprocessor stacks using different backends.\n    """"""\n    batch_size = 4\n\n    # All preprocessors\n    preprocessing_spec = [\n        # Remove image-resize (make crop 80 instead 160) as tf\'s erroneous resize implementation\n        # screws up everything.\n        {\n            ""type"": ""image_crop"",\n            ""x"": 0,\n            ""y"": 25,\n            ""width"": 160,\n            ""height"": 160,\n            ""scope"": ""image_crop""\n        },\n        #{\n        #    ""type"": ""image_resize"",\n        #    ""width"": 80,\n        #    ""height"": 80,\n        #    ""scope"": ""image_resize""\n        #},\n        {\n            ""type"": ""grayscale"",\n            ""keep_rank"": True,\n            ""scope"": ""grayscale""\n        },\n        {\n            ""type"": ""divide"",\n            ""divisor"": 255,\n            ""scope"": ""divide""\n        },\n        {\n            ""type"": ""sequence"",\n            ""sequence_length"": 4,\n            ""batch_size"": batch_size,\n            ""add_rank"": False,\n            ""scope"": ""sequence""\n        }\n    ]\n\n    preprocessing_spec_ray_pong = [\n        {\n            ""type"": ""image_resize"",\n            ""width"": 84,\n            ""height"": 84,\n            ""scope"": ""image_resize""\n        },\n        {\n            ""type"": ""grayscale"",\n            ""keep_rank"": True,\n            ""scope"": ""grayscale""\n        },\n        {\n            ""type"": ""sequence"",\n            ""sequence_length"": 4,\n            ""batch_size"": 1,\n            ""add_rank"": False,\n            ""scope"": ""sequence""\n        }\n    ]\n\n    # TODO: Make tests backend independent so we can use the same tests for everything.\n    def test_backend_equivalence(self):\n        """"""\n        Tests if Python and TensorFlow backend return the same output\n        for a standard DQN-style preprocessing stack.\n        """"""\n        in_space = IntBox(256, shape=(210, 160, 3), dtype=""uint8"", add_batch_rank=True)\n\n        # Regression test: Incrementally add preprocessors.\n        to_use = []\n        for i, decimals in zip(range_(len(self.preprocessing_spec)), [0, 0, 2, 2]):\n            to_use.append(i)\n            incremental_spec = []\n            incremental_scopes = []\n            for index in to_use:\n                incremental_spec.append(deepcopy(self.preprocessing_spec[index]))\n                incremental_scopes.append(self.preprocessing_spec[index][""scope""])\n\n            print(""Comparing incremental spec: {}"".format(incremental_scopes))\n\n            # Set up python preprocessor.\n            # Set backend to python.\n            for spec in incremental_spec:\n                spec[""backend""] = ""python""\n            python_preprocessor = PreprocessorStack(*incremental_spec, backend=""python"")\n            for sub_comp_scope in incremental_scopes:\n                python_preprocessor.sub_components[sub_comp_scope].create_variables(\n                    input_spaces=dict(inputs=in_space), action_space=None\n                )\n                python_preprocessor.sub_components[sub_comp_scope].check_input_spaces(\n                    input_spaces=dict(inputs=in_space), action_space=None\n                )\n                #build_space = python_processor.sub_components[sub_comp_scope].get_preprocessed_space(build_space)\n                python_preprocessor.reset()\n\n            # To compare to tf, use an equivalent tf PreprocessorStack.\n            # Switch back to tf.\n            for spec in incremental_spec:\n                spec[""backend""] = ""tf""\n            tf_preprocessor = PreprocessorStack(*incremental_spec, backend=""tf"")\n\n            test = ComponentTest(component=tf_preprocessor, input_spaces=dict(\n                inputs=in_space\n            ))\n\n            # Generate a few states from random set points. Test if preprocessed states are almost equal\n            states = in_space.sample(size=self.batch_size)\n            python_preprocessed_states = python_preprocessor.preprocess(states)\n            tf_preprocessed_states = test.test((""preprocess"", states), expected_outputs=None)\n\n            print(""Asserting (almost) equal values:"")\n            for tf_state, python_state in zip(tf_preprocessed_states, python_preprocessed_states):\n                recursive_assert_almost_equal(tf_state, python_state, decimals=decimals)\n            print(""Success comparing: {}"".format(incremental_scopes))\n\n    def test_ray_pong_preprocessor_config_in_python(self):\n        in_space = IntBox(256, shape=(210, 160, 3), dtype=""uint8"", add_batch_rank=True)\n\n        # Regression test: Incrementally add preprocessors.\n        specs = [spec for spec in self.preprocessing_spec_ray_pong]\n        scopes = [spec[""scope""] for spec in self.preprocessing_spec_ray_pong]\n\n        # Set up python preprocessor.\n        # Set backend to python.\n        for spec in specs:\n            spec[""backend""] = ""python""\n        python_preprocessor = PreprocessorStack(*specs, backend=""python"")\n        for sub_comp_scope in scopes:\n            python_preprocessor.sub_components[sub_comp_scope].create_variables(\n                input_spaces=dict(inputs=in_space), action_space=None\n            )\n            python_preprocessor.reset()\n\n        # Generate a few states from random set points. Test if preprocessed states are almost equal\n        states = in_space.sample(size=self.batch_size)\n        python_preprocessed_states = python_preprocessor.preprocess(states)\n        # TODO: add more checks here besides the shape.\n        self.assertEqual(python_preprocessed_states.shape, (4, 84, 84, 4))\n\n    def test_batched_backend_equivalence(self):\n        return\n        """"""\n        Tests if Python and TensorFlow backend return the same output\n        for a standard DQN-style preprocessing stack.\n        """"""\n        env_spec = dict(\n            type=""openai"",\n            gym_env=""Pong-v0"",\n            frameskip=4,\n            max_num_noops=30,\n            episodic_life=True\n        )\n        # Test with batching because we assume vector environments to be the normal case going forward.\n        env = SequentialVectorEnv(num_environments=4, env_spec=env_spec, num_background_envs=2)\n        in_space = env.state_space\n\n        agent_config = config_from_path(""configs/ray_apex_for_pong.json"")\n        preprocessing_spec = deepcopy(agent_config[""preprocessing_spec""])\n\n        # Set up python preprocessor.\n        scopes = [preprocessor[""scope""] for preprocessor in preprocessing_spec]\n        # Set backend to python.\n        for spec in preprocessing_spec:\n            spec[""backend""] = ""python""\n        python_processor = PreprocessorStack(*preprocessing_spec, backend=""python"")\n        for sub_comp_scope in scopes:\n            python_processor.sub_components[sub_comp_scope].create_variables(dict(inputs=in_space))\n        python_processor.reset()\n\n        # To have the use case we considered so far, use agent interface for TF backend.\n        agent_config.pop(""type"")\n        agent = ApexAgent(state_space=env.state_space, action_space=env.action_space, **agent_config)\n\n        # Generate a few states from random set points. Test if preprocessed states are almost equal\n        states = np.asarray(env.reset_all())\n        actions, agent_preprocessed_states = agent.get_action(\n            states=states, use_exploration=False, extra_returns=""preprocessed_states"")\n        print(""TensorFlow preprocessed shape: {}"".format(np.asarray(agent_preprocessed_states).shape))\n        python_preprocessed_states = python_processor.preprocess(states)\n        print(""Python preprocessed shape: {}"".format(np.asarray(python_preprocessed_states).shape))\n        print(""Asserting (almost) equal values:"")\n        for tf_state, python_state in zip(agent_preprocessed_states, python_preprocessed_states):\n            flat_tf = np.ndarray.flatten(tf_state)\n            flat_python = np.ndarray.flatten(python_state)\n            for x, y in zip(flat_tf, flat_python):\n                recursive_assert_almost_equal(x, y, decimals=3)\n\n        states, _, _, _ = env.step(actions)\n        actions, agent_preprocessed_states = agent.get_action(\n            states=states, use_exploration=False, extra_returns=""preprocessed_states"")\n        print(""TensorFlow preprocessed shape: {}"".format(np.asarray(agent_preprocessed_states).shape))\n        python_preprocessed_states = python_processor.preprocess(states)\n        print(""Python preprocessed shape: {}"".format(np.asarray(python_preprocessed_states).shape))\n        print(""Asserting (almost) equal values:"")\n        recursive_assert_almost_equal(agent_preprocessed_states, python_preprocessed_states, decimals=3)\n\n    def test_simple_preprocessor_stack_with_one_preprocess_layer(self):\n        stack = PreprocessorStack(dict(type=""multiply"", factor=0.5))\n\n        test = ComponentTest(component=stack, input_spaces=dict(inputs=float))\n\n        test.test(""reset"")\n        test.test((""preprocess"", 2.0), expected_outputs=1.0)\n\n    # TODO: Make it irrelevent whether we test a python or a tf Component (API and handling should be 100% identical)\n    def test_simple_python_preprocessor_stack(self):\n        """"""\n        Tests a pure python preprocessor stack.\n        """"""\n        space = FloatBox(shape=(2,), add_batch_rank=True)\n        # python PreprocessorStack\n        multiply = dict(type=""multiply"", factor=0.5, scope=""m"")\n        divide = dict(type=""divide"", divisor=0.5, scope=""d"")\n        stack = PreprocessorStack(multiply, divide, backend=""python"")\n        for sub_comp_scope in [""m"", ""d""]:\n            stack.sub_components[sub_comp_scope].create_variables(input_spaces=dict(inputs=space))\n\n        #test = ComponentTest(component=stack, input_spaces=dict(inputs=float))\n\n        for _ in range_(3):\n            # Call fake API-method directly (ok for PreprocessorStack).\n            stack.reset()\n            input_ = np.asarray([[1.0], [2.0], [3.0], [4.0]])\n            expected = input_\n            #test.test((""preprocess"", input_), expected_outputs=expected)\n            out = stack.preprocess(input_)\n            recursive_assert_almost_equal(out, input_)\n\n            input_ = space.sample()\n            #test.test((""preprocess"", input_), expected_outputs=expected)\n            out = stack.preprocess(input_)\n            recursive_assert_almost_equal(out, input_)\n\n    def test_preprocessor_from_list_spec(self):\n        space = FloatBox(shape=(2,))\n        stack = PreprocessorStack.from_spec([\n            dict(type=""grayscale"", keep_rank=False, weights=(0.5, 0.5)),\n            dict(type=""divide"", divisor=2),\n        ])\n        test = ComponentTest(component=stack, input_spaces=dict(inputs=space))\n\n        # Run the test.\n        input_ = np.array([3.0, 5.0])\n        expected = np.array(2.0)\n        test.test(""reset"")\n        test.test((""preprocess"", input_), expected_outputs=expected)\n\n    def test_two_preprocessor_layers_in_a_preprocessor_stack(self):\n        space = Dict(\n            a=FloatBox(shape=(1, 2)),\n            b=FloatBox(shape=(2, 2, 2)),\n            c=Tuple(FloatBox(shape=(2,)), Dict(ca=FloatBox(shape=(3, 3, 2))))\n        )\n\n        # Construct the Component to test (PreprocessorStack).\n        scale = Multiply(factor=2)\n        gray = GrayScale(weights=(0.5, 0.5), keep_rank=False)\n        stack = PreprocessorStack(scale, gray)\n        test = ComponentTest(component=stack, input_spaces=dict(inputs=space))\n\n        input_ = dict(\n            a=np.array([[3.0, 5.0]]),\n            b=np.array([[[2.0, 4.0], [2.0, 4.0]], [[2.0, 4.0], [2.0, 4.0]]]),\n            c=(np.array([10.0, 20.0]), dict(ca=np.array([[[1.0, 2.0], [1.0, 2.0], [1.0, 2.0]],\n                                                         [[1.0, 2.0], [1.0, 2.0], [1.0, 2.0]],\n                                                         [[1.0, 2.0], [1.0, 2.0], [1.0, 2.0]]])))\n        )\n        expected = dict(\n            a=np.array([8.0]),\n            b=np.array([[6.0, 6.0], [6.0, 6.0]]),\n            c=(30.0, dict(ca=np.array([[3.0, 3.0, 3.0],\n                                       [3.0, 3.0, 3.0],\n                                       [3.0, 3.0, 3.0]])))\n        )\n        test.test(""reset"")\n        test.test((""preprocess"", input_), expected_outputs=expected)\n'"
rlgraph/tests/components/test_prioritized_replay.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\nimport numpy as np\n\nfrom rlgraph.components.memories import PrioritizedReplay\nfrom rlgraph.spaces import Dict, IntBox, BoolBox, FloatBox\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.tests.test_util import non_terminal_records\n\n\nclass TestPrioritizedReplay(unittest.TestCase):\n    """"""\n    Tests sampling and insertion behaviour of the prioritized_replay module.\n    """"""\n    record_space = Dict(\n        states=dict(state1=float, state2=float),\n        actions=dict(action1=float),\n        reward=float,\n        terminals=BoolBox(),\n        add_batch_rank=True\n    )\n    memory_variables = [""size"", ""index"", ""max-priority""]\n\n    capacity = 10\n    alpha = 1.0\n    beta = 1.0\n\n    max_priority = 1.0\n\n    input_spaces = dict(\n        # insert: records\n        records=record_space,\n        # get_records: num_records\n        num_records=int,\n        # update_records: indices, update\n        indices=IntBox(add_batch_rank=True),\n        update=FloatBox(add_batch_rank=True)\n    )\n\n    def test_insert(self):\n        """"""\n        Simply tests insert op without checking internal logic.\n        """"""\n        memory = PrioritizedReplay(\n            capacity=self.capacity,\n            alpha=self.alpha,\n            beta=self.beta\n        )\n        test = ComponentTest(component=memory, input_spaces=self.input_spaces)\n\n        observation = self.record_space.sample(size=1)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n    def test_capacity(self):\n        """"""\n        Tests if insert correctly manages capacity.\n        """"""\n        memory = PrioritizedReplay(\n            capacity=self.capacity,\n            alpha=self.alpha,\n            beta=self.beta\n        )\n        test = ComponentTest(component=memory, input_spaces=self.input_spaces)\n\n        # Internal state variables.\n        memory_variables = memory.get_variables(self.memory_variables, global_scope=False)\n        buffer_size = memory_variables[\'size\']\n        buffer_index = memory_variables[\'index\']\n        max_priority = memory_variables[\'max-priority\']\n\n        size_value, index_value, max_priority_value = test.read_variable_values(buffer_size, buffer_index, max_priority)\n\n        # Assert indices 0 before insert.\n        self.assertEqual(size_value, 0)\n        self.assertEqual(index_value, 0)\n        self.assertEqual(max_priority_value, 1.0)\n\n        # Insert one more element than capacity\n        observation = self.record_space.sample(size=self.capacity + 1)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        size_value, index_value = test.read_variable_values(buffer_size, buffer_index)\n        # Size should be equivalent to capacity when full.\n        self.assertEqual(size_value, self.capacity)\n\n        # Index should be one over capacity due to modulo.\n        self.assertEqual(index_value, 1)\n\n    def test_batch_retrieve(self):\n        """"""\n        Tests if retrieval correctly manages capacity.\n        """"""\n        memory = PrioritizedReplay(\n            capacity=self.capacity,\n            alpha=self.alpha,\n            beta=self.beta\n        )\n        test = ComponentTest(component=memory, input_spaces=self.input_spaces)\n\n        # Insert 2 Elements.\n        observation = non_terminal_records(self.record_space, 2)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        # Assert we can now fetch 2 elements.\n        num_records = 2\n        batch = test.test((""get_records"", num_records), expected_outputs=None)\n        records = batch[0]\n        print(\'Result batch = {}\'.format(records))\n        self.assertEqual(2, len(records[\'terminals\']))\n\n        # We allow repeat indices in sampling.\n        num_records = 5\n        batch = test.test((""get_records"", num_records), expected_outputs=None)\n        records = batch[0]\n        self.assertEqual(5, len(records[\'terminals\']))\n\n        # Now insert over capacity, note all elements here are non-terminal.\n        observation = non_terminal_records(self.record_space, self.capacity)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        # Assert we can fetch exactly capacity elements.\n        num_records = self.capacity\n        batch = test.test((""get_records"", num_records), expected_outputs=None)\n        records = batch[0]\n        self.assertEqual(self.capacity, len(records[\'terminals\']))\n\n    def test_update_records(self):\n        """"""\n        Tests update records logic.\n        """"""\n        memory = PrioritizedReplay(\n            capacity=self.capacity\n        )\n        test = ComponentTest(component=memory, input_spaces=self.input_spaces)\n\n        # Insert a few Elements.\n        observation = non_terminal_records(self.record_space, 2)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        # Fetch elements and their indices.\n        num_records = 2\n        batch = test.test((""get_records"", num_records), expected_outputs=None)\n        indices = batch[1]\n        self.assertEqual(num_records, len(indices))\n        # 0.3, 0.5, 1.0])\n        input_params = [indices, np.asarray([0.1, 0.2])]\n        # Does not return anything\n        test.test((""update_records"", input_params), expected_outputs=None)\n\n    def test_segment_tree_insert_values(self):\n        """"""\n        Tests if segment tree inserts into correct positions.\n        """"""\n        memory = PrioritizedReplay(\n            capacity=self.capacity,\n            alpha=self.alpha,\n            beta=self.beta\n        )\n        test = ComponentTest(component=memory, input_spaces=self.input_spaces)\n        priority_capacity = 1\n        while priority_capacity < self.capacity:\n            priority_capacity *= 2\n\n        memory_variables = memory.get_variables([""sum-segment-tree"", ""min-segment-tree""], global_scope=False)\n        sum_segment_tree = memory_variables[\'sum-segment-tree\']\n        min_segment_tree = memory_variables[\'min-segment-tree\']\n        sum_segment_values, min_segment_values = test.read_variable_values(sum_segment_tree, min_segment_tree)\n\n        self.assertEqual(sum(sum_segment_values), 0)\n        self.assertEqual(sum(min_segment_values), float(\'inf\'))\n        self.assertEqual(len(sum_segment_values), 2 * priority_capacity)\n        self.assertEqual(len(min_segment_values), 2 * priority_capacity)\n        # Insert 1 Element.\n        observation = non_terminal_records(self.record_space, 1)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        # Fetch segment tree.\n        sum_segment_values, min_segment_values = test.read_variable_values(sum_segment_tree, min_segment_tree)\n\n        # Check insert positions\n        # Initial insert is at priority capacity\n        print(sum_segment_values)\n        print(min_segment_values)\n        start = priority_capacity\n\n        while start >= 1:\n            self.assertEqual(sum_segment_values[start], 1.0)\n            self.assertEqual(min_segment_values[start], 1.0)\n            start = int(start / 2)\n\n        # Insert another Element.\n        observation = non_terminal_records(self.record_space, 1)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        # Fetch segment tree.\n        sum_segment_values, min_segment_values = test.read_variable_values(sum_segment_tree, min_segment_tree)\n        print(sum_segment_values)\n        print(min_segment_values)\n\n        # Index shifted 1\n        start = priority_capacity + 1\n        self.assertEqual(sum_segment_values[start], 1.0)\n        self.assertEqual(min_segment_values[start], 1.0)\n        start = int(start / 2)\n        while start >= 1:\n            # 1 + 1 is 2 on the segment.\n            self.assertEqual(sum_segment_values[start], 2.0)\n            # min is still 1.\n            self.assertEqual(min_segment_values[start], 1.0)\n            start = int(start / 2)'"
rlgraph/tests/components/test_python_prioritized_replay.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\nimport numpy as np\nfrom six.moves import xrange as range_\nfrom rlgraph.components.memories.mem_prioritized_replay import MemPrioritizedReplay\nfrom rlgraph.execution.ray.apex.apex_memory import ApexMemory\nfrom rlgraph.execution.ray.ray_util import ray_compress\nfrom rlgraph.spaces import Dict, IntBox, BoolBox, FloatBox\n\n\n# TODO (Michael): Clean up memory semantics and tests re:\n# next states, python memories.\nclass TestPythonPrioritizedReplay(unittest.TestCase):\n    """"""\n    Tests sampling and insertion behaviour of the mem_prioritized_replay module.\n    """"""\n    record_space = Dict(\n        states=dict(state1=float, state2=float),\n        actions=dict(action1=float),\n        reward=float,\n        terminals=BoolBox(),\n        add_batch_rank=True\n    )\n    apex_space = Dict(\n        states=FloatBox(shape=(4,)),\n        actions=FloatBox(shape=(2,)),\n        reward=float,\n        terminals=BoolBox(),\n        weights=FloatBox(),\n        add_batch_rank=True\n    )\n\n    memory_variables = [""size"", ""index"", ""max-priority""]\n\n    capacity = 10\n    alpha = 1.0\n    beta = 1.0\n\n    max_priority = 1.0\n\n    input_spaces = dict(\n        # insert: records\n        records=record_space,\n        # get_records: num_records\n        num_records=int,\n        # update_records: indices, update\n        indices=IntBox(add_batch_rank=True),\n        update=FloatBox(add_batch_rank=True)\n    )\n\n    # TODO These methods are all graph fns now -> unify backend tests.\n    def test_insert(self):\n        """"""\n        Simply tests insert op without checking internal logic.\n        """"""\n        memory = MemPrioritizedReplay(\n            capacity=self.capacity,\n            next_states=True,\n            alpha=self.alpha,\n            beta=self.beta\n        )\n        memory.create_variables(self.input_spaces)\n\n        observation = memory.record_space_flat.sample(size=1)\n        memory.insert_records(observation)\n\n        # Test chunked insert\n        observation = memory.record_space_flat.sample(size=5)\n        memory.insert_records(observation)\n\n        # Also test Apex version\n        memory = ApexMemory(\n            capacity=self.capacity,\n            alpha=self.alpha,\n            beta=self.beta\n        )\n        observation = self.apex_space.sample(size=5)\n        for i in range_(5):\n            memory.insert_records((\n                observation[\'states\'][i],\n                observation[\'actions\'][i],\n                observation[\'reward\'][i],\n                observation[\'terminals\'][i],\n                observation[\'states\'][i],\n                observation[""weights""][i]\n            ))\n\n    def test_update_records(self):\n        """"""\n        Tests update records logic.\n        """"""\n        memory = MemPrioritizedReplay(\n            capacity=self.capacity,\n            next_states=True\n        )\n        memory.create_variables(self.input_spaces)\n\n        # Insert a few Elements.\n        observation = memory.record_space_flat.sample(size=2)\n        memory.insert_records(observation)\n\n        # Fetch elements and their indices.\n        num_records = 2\n        batch = memory.get_records(num_records)\n        indices = batch[1]\n        self.assertEqual(num_records, len(indices))\n\n        # Does not return anything.\n        memory.update_records(indices, np.asarray([0.1, 0.2]))\n\n        # Test apex memory.\n        memory = ApexMemory(\n            capacity=self.capacity,\n            alpha=self.alpha,\n            beta=self.beta\n        )\n        observation = self.apex_space.sample(size=5)\n        for i in range_(5):\n            memory.insert_records((\n                ray_compress(observation[""states""][i]),\n                observation[""actions""][i],\n                observation[""reward""][i],\n                observation[""terminals""][i],\n                observation[""weights""][i]\n            ))\n\n        # Fetch elements and their indices.\n        num_records = 5\n        batch = memory.get_records(num_records)\n        indices = batch[1]\n        self.assertEqual(num_records, len(indices))\n\n        # Does not return anything\n        memory.update_records(indices, np.random.uniform(size=10))\n\n    def test_segment_tree_insert_values(self):\n        """"""\n        Tests if segment tree inserts into correct positions.\n        """"""\n        memory = MemPrioritizedReplay(\n            capacity=self.capacity,\n            next_states=True,\n            alpha=self.alpha,\n            beta=self.beta\n        )\n        memory.create_variables(self.input_spaces)\n\n        priority_capacity = 1\n        while priority_capacity < self.capacity:\n            priority_capacity *= 2\n\n        sum_segment_values = memory.merged_segment_tree.sum_segment_tree.values\n        min_segment_values = memory.merged_segment_tree.min_segment_tree.values\n\n        self.assertEqual(sum(sum_segment_values), 0)\n        self.assertEqual(sum(min_segment_values), float(\'inf\'))\n        self.assertEqual(len(sum_segment_values), 2 * priority_capacity)\n        self.assertEqual(len(min_segment_values), 2 * priority_capacity)\n\n        # Insert 1 Element.\n        observation = memory.record_space_flat.sample(size=1)\n        memory.insert_records(observation)\n\n        # Check insert positions\n        # Initial insert is at priority capacity\n        print(sum_segment_values)\n        print(min_segment_values)\n        start = priority_capacity\n\n        while start >= 1:\n            self.assertEqual(sum_segment_values[start], 1.0)\n            self.assertEqual(min_segment_values[start], 1.0)\n            start = int(start / 2)\n\n        # Insert another Element.\n        observation =  memory.record_space_flat.sample(size=1)\n        memory.insert_records(observation)\n\n        # Index shifted 1\n        start = priority_capacity + 1\n        self.assertEqual(sum_segment_values[start], 1.0)\n        self.assertEqual(min_segment_values[start], 1.0)\n        start = int(start / 2)\n        while start >= 1:\n            # 1 + 1 is 2 on the segment.\n            self.assertEqual(sum_segment_values[start], 2.0)\n            # min is still 1.\n            self.assertEqual(min_segment_values[start], 1.0)\n            start = int(start / 2)\n\n    def test_tree_insert(self):\n        """"""\n        Tests inserting into the segment tree and querying segments.\n        """"""\n        memory = ApexMemory(\n            capacity=4\n        )\n        tree = memory.merged_segment_tree.sum_segment_tree\n        tree.insert(2, 1.0)\n        tree.insert(3, 3.0)\n        assert np.isclose(tree.get_sum(), 4.0)\n        assert np.isclose(tree.get_sum(0, 2), 0.0)\n        assert np.isclose(tree.get_sum(0, 3), 1.0)\n        assert np.isclose(tree.get_sum(2, 3), 1.0)\n        assert np.isclose(tree.get_sum(2, -1), 1.0)\n        assert np.isclose(tree.get_sum(2, 4), 4.0)\n\n    def test_prefixsum_idx(self):\n        """"""\n        Tests fetching the index corresponding to a prefix sum.\n        """"""\n        memory = ApexMemory(\n            capacity=4\n        )\n        tree = memory.merged_segment_tree.sum_segment_tree\n        tree.insert(2, 1.0)\n        tree.insert(3, 3.0)\n\n        self.assertEqual(tree.index_of_prefixsum(0.0), 2)\n        self.assertEqual(tree.index_of_prefixsum(0.5), 2)\n        self.assertEqual(tree.index_of_prefixsum(0.99), 2)\n        self.assertEqual(tree.index_of_prefixsum(1.01), 3)\n        self.assertEqual(tree.index_of_prefixsum(3.0), 3)\n        self.assertEqual(tree.index_of_prefixsum(4.0), 3)\n\n        memory = ApexMemory(\n            capacity=4\n        )\n        tree = memory.merged_segment_tree.sum_segment_tree\n        tree.insert(0, 0.5)\n        tree.insert(1, 1.0)\n        tree.insert(2, 1.0)\n        tree.insert(3, 3.0)\n        self.assertEqual(tree.index_of_prefixsum(0.0), 0)\n        self.assertEqual(tree.index_of_prefixsum(0.55), 1)\n        self.assertEqual(tree.index_of_prefixsum(0.99), 1)\n        self.assertEqual(tree.index_of_prefixsum(1.51), 2)\n        self.assertEqual(tree.index_of_prefixsum(3.0), 3)\n        self.assertEqual(tree.index_of_prefixsum(5.50), 3)\n'"
rlgraph/tests/components/test_replay_memory.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nfrom rlgraph.components.memories.replay_memory import ReplayMemory\nfrom rlgraph.spaces import Dict, BoolBox\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.tests.test_util import non_terminal_records\n\n\nclass TestReplayMemory(unittest.TestCase):\n    """"""\n    Tests sampling and insertion behaviour of the replay_memory module.\n    """"""\n    record_space = Dict(\n        states=dict(state1=float, state2=float),\n        actions=dict(action1=float),\n        reward=float,\n        terminals=BoolBox(),\n        next_states=dict(state1=float, state2=float),\n        add_batch_rank=True\n    )\n    memory_variables = [""size"", ""index""]\n    capacity = 10\n\n    input_spaces = dict(\n        records=record_space,\n        num_records=int\n    )\n\n    def test_insert(self):\n        """"""\n        Simply tests insert op without checking internal logic.\n        """"""\n        memory = ReplayMemory(\n            capacity=self.capacity,\n        )\n        test = ComponentTest(component=memory, input_spaces=self.input_spaces)\n\n        observation = self.record_space.sample(size=1)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        observation = self.record_space.sample(size=100)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n    def test_capacity(self):\n        """"""\n        Tests if insert correctly manages capacity.\n        """"""\n        memory = ReplayMemory(\n            capacity=self.capacity\n        )\n        test = ComponentTest(component=memory, input_spaces=self.input_spaces)\n        # Internal state variables.\n        variables = test.get_variable_values(memory, self.memory_variables)\n        size_value = variables[""size""]\n        index_value = variables[""index""]\n        # Assert indices 0 before insert.\n        self.assertEqual(size_value, 0)\n        self.assertEqual(index_value, 0)\n\n        # Insert one more element than capacity\n        observation = self.record_space.sample(size=self.capacity + 1)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        variables = test.get_variable_values(memory, self.memory_variables)\n        size_value = variables[""size""]\n        index_value = variables[""index""]\n        # Size should be equivalent to capacity when full.\n        self.assertEqual(size_value, self.capacity)\n\n        # Index should be one over capacity due to modulo.\n        self.assertEqual(index_value, 1)\n\n    def test_batch_retrieve(self):\n        """"""\n        Tests if retrieval correctly manages capacity.\n        """"""\n        memory = ReplayMemory(\n            capacity=self.capacity\n        )\n        test = ComponentTest(component=memory, input_spaces=self.input_spaces)\n\n        # Insert 2 Elements.\n        observation = non_terminal_records(self.record_space, 2)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        # Assert we can now fetch 2 elements.\n        num_records = 2\n        batch, _, _ = test.test((""get_records"", num_records), expected_outputs=None)\n        print(\'Result batch = {}\'.format(batch))\n        self.assertEqual(2, len(batch[\'terminals\']))\n        # Assert next states key is there\n        self.assertTrue(\'next_states\' in batch)\n\n        # Test duplicate sampling.\n        num_records = 5\n        batch, _, _ = test.test((""get_records"", num_records), expected_outputs=None)\n        self.assertEqual(5, len(batch[\'terminals\']))\n\n        # Now insert over capacity.\n        observation = non_terminal_records(self.record_space, self.capacity)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        # Assert we can fetch exactly capacity elements.\n        num_records = self.capacity\n        batch, _, _ = test.test((""get_records"", num_records), expected_outputs=None)\n        self.assertEqual(self.capacity, len(batch[\'terminals\']))\n'"
rlgraph/tests/components/test_reshape_preprocessor.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\n\nfrom rlgraph.components.layers import ReShape\nfrom rlgraph.components.neural_networks.stack import Stack\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest, recursive_assert_almost_equal\nfrom rlgraph.utils.numpy import one_hot\n\n\nclass TestReShapePreprocessors(unittest.TestCase):\n\n    def test_reshape(self):\n        reshape = ReShape(new_shape=(3, 2))\n        test = ComponentTest(component=reshape, input_spaces=dict(\n            inputs=FloatBox(shape=(6,), add_batch_rank=True)\n        ))\n\n        test.test(""reset"")\n        # Batch=2\n        inputs = np.array([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12]])\n        expected = np.array([[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11, 12]]])\n        test.test((""call"", inputs), expected_outputs=expected)\n\n    def test_reshape_with_flatten_option(self):\n        # Test flattening while leaving batch and time rank as is.\n        in_space = FloatBox(shape=(2, 3, 4), add_batch_rank=True, add_time_rank=True, time_major=True)\n        reshape = ReShape(flatten=True)\n        test = ComponentTest(component=reshape, input_spaces=dict(\n            inputs=in_space\n        ))\n\n        test.test(""reset"")\n        # Time-rank=5, Batch=2\n        inputs = in_space.sample(size=(5, 2))\n        expected = np.reshape(inputs, newshape=(5, 2, 24))\n        test.test((""call"", inputs), expected_outputs=expected)\n\n    def test_reshape_with_flatten_option_only_time_rank(self):\n        # Test flattening while leaving batch and time rank as is.\n        in_space = FloatBox(shape=(2, 3), add_batch_rank=False, add_time_rank=True)\n        reshape = ReShape(flatten=True)\n        test = ComponentTest(component=reshape, input_spaces=dict(\n            inputs=in_space\n        ))\n\n        test.test(""reset"")\n        # Time-rank=5, Batch=2\n        inputs = in_space.sample(size=3)\n        expected = np.reshape(inputs, newshape=(3, 6))\n        test.test((""call"", inputs), expected_outputs=expected)\n\n    def test_reshape_with_flatten_option_with_0D_shape(self):\n        # Test flattening int with shape=().\n        in_space = IntBox(3, shape=(), add_batch_rank=True)\n        reshape = ReShape(flatten=True, flatten_categories=3)\n        test = ComponentTest(component=reshape, input_spaces=dict(\n            inputs=in_space\n        ))\n\n        test.test(""reset"")\n        # Time-rank=5, Batch=2\n        inputs = in_space.sample(size=4)\n        # Expect a by-int-category one-hot flattening.\n        expected = one_hot(inputs, depth=3)\n        test.test((""call"", inputs), expected_outputs=expected)\n\n    def test_reshape_with_flatten_option_with_categories(self):\n        # Test flattening while leaving batch and time rank as is, but flattening out int categories.\n        in_space = IntBox(2, shape=(2, 3, 4), add_batch_rank=True, add_time_rank=True, time_major=False)\n        reshape = ReShape(flatten=True, flatten_categories=2)\n        test = ComponentTest(component=reshape, input_spaces=dict(\n            inputs=in_space\n        ))\n\n        test.test(""reset"")\n        # Batch=3, time-rank=5\n        inputs = in_space.sample(size=(3, 5))\n        expected = np.reshape(one_hot(inputs, depth=2), newshape=(3, 5, 48)).astype(dtype=np.float32)\n        test.test((""call"", inputs), expected_outputs=expected)\n\n    def test_reshape_with_flatten_option_without_categories(self):\n        # Test flattening while leaving batch and time rank as is.\n        in_space = IntBox(3, shape=(2, 3, 4), add_batch_rank=True, add_time_rank=True, time_major=False)\n        reshape = ReShape(flatten=True, flatten_categories=False)\n        test = ComponentTest(component=reshape, input_spaces=dict(\n            inputs=in_space\n        ))\n\n        test.test(""reset"")\n        # Batch=3, time-rank=5\n        inputs = in_space.sample(size=(3, 5))\n        expected = np.reshape(inputs, newshape=(3, 5, 24)).astype(dtype=np.float32)\n        test.test((""call"", inputs), expected_outputs=expected)\n\n    def test_reshape_with_time_rank(self):\n        # Test with time-rank instead of batch-rank.\n        in_space = FloatBox(shape=(4,), add_batch_rank=False, add_time_rank=True)\n        reshape = ReShape(new_shape=(2, 2))\n        test = ComponentTest(component=reshape, input_spaces=dict(\n            inputs=in_space\n        ))\n\n        test.test(""reset"")\n        inputs = in_space.sample(size=3)\n        expected = np.reshape(inputs, newshape=(3, 2, 2))\n        test.test((""call"", inputs), expected_outputs=expected)\n\n    def test_reshape_with_time_rank_folding(self):\n        # Fold time rank into batch rank.\n        in_space = FloatBox(shape=(4, 4), add_batch_rank=True, add_time_rank=True, time_major=True)\n        reshape = ReShape(fold_time_rank=True)\n        test = ComponentTest(component=reshape, input_spaces=dict(\n            inputs=in_space\n        ))\n\n        test.test(""reset"")\n        # seq-len=3, batch-size=2\n        inputs = in_space.sample(size=(3, 2))\n        expected = np.reshape(inputs, newshape=(6, 4, 4))\n        test.test((""call"", inputs), expected_outputs=expected)\n\n    def test_reshape_with_time_rank_unfolding(self):\n        # Unfold time rank from batch rank with given time-dimension (2 out of 8 -> batch will be 4 after unfolding).\n        in_space = FloatBox(shape=(4, 4), add_batch_rank=True, add_time_rank=False)\n        in_space_before_folding = FloatBox(shape=(4, 4), add_batch_rank=True, add_time_rank=True)\n        reshape = ReShape(unfold_time_rank=True)\n        test = ComponentTest(component=reshape, input_spaces=dict(\n            inputs=in_space, input_before_time_rank_folding=in_space_before_folding\n        ))\n\n        test.test(""reset"")\n        # seq-len=2, batch-size=4 -> unfold from 8.\n        inputs = in_space.sample(size=8)\n        inputs_before_folding = in_space_before_folding.sample(size=(4, 2))\n        expected = np.reshape(inputs, newshape=(4, 2, 4, 4))\n        test.test((""call"", (inputs, inputs_before_folding)), expected_outputs=expected)\n\n    def test_reshape_python_with_time_rank_unfolding(self):\n        # Unfold time rank from batch rank with given time-dimension (2 out of 8 -> batch will be 4 after unfolding).\n        in_space = FloatBox(shape=(4, 4), add_batch_rank=True, add_time_rank=False)\n        in_space_before_folding = FloatBox(shape=(4, 4), add_batch_rank=True, add_time_rank=True)\n        reshape = ReShape(unfold_time_rank=True, backend=""python"")\n        reshape.create_variables(dict(\n            inputs=in_space, input_before_time_rank_folding=in_space_before_folding\n        ))\n\n        # seq-len=2, batch-size=4 -> unfold from 8.\n        inputs = in_space.sample(size=8)\n        inputs_before_folding = in_space_before_folding.sample(size=(4, 2))\n        expected = np.reshape(inputs, newshape=(4, 2, 4, 4))\n        out = reshape._graph_fn_call(inputs, inputs_before_folding)\n\n        recursive_assert_almost_equal(out, expected)\n\n    def test_reshape_with_time_and_batch_ranks_and_reshaping(self):\n        in_space = FloatBox(shape=(5, 8), add_batch_rank=True, add_time_rank=True, time_major=True)\n        reshape = ReShape(new_shape=(4, 10))\n        test = ComponentTest(component=reshape, input_spaces=dict(\n            inputs=in_space\n        ))\n\n        test.test(""reset"")\n        # seq-len=2, batch-size=4\n        inputs = in_space.sample(size=(2, 4))\n        # Reshape without the first two ranks.\n        expected = np.reshape(inputs, newshape=(2, 4, 4, 10))\n        test.test((""call"", inputs), expected_outputs=expected)\n\n    def test_reshape_with_batch_and_time_ranks_and_flattening(self):\n        in_space = FloatBox(shape=(6, 4, 2), add_batch_rank=True, add_time_rank=True, time_major=False)\n        reshape = ReShape(flatten=True)\n        test = ComponentTest(component=reshape, input_spaces=dict(\n            inputs=in_space\n        ))\n\n        test.test(""reset"")\n        # batch-size=1, seq-len=3\n        inputs = in_space.sample(size=(1, 3))\n        # Reshape without the first two ranks.\n        expected = np.reshape(inputs, newshape=(1, 3, 48))\n        test.test((""call"", inputs), expected_outputs=expected)\n\n    def test_reshape_with_batch_and_time_ranks_and_with_folding_and_unfolding(self):\n        # Flip time and batch rank via folding, then unfolding.\n        in_space = FloatBox(shape=(3, 2), add_batch_rank=True, add_time_rank=True, time_major=False)\n        reshape_fold = ReShape(fold_time_rank=True)\n        reshape_unfold = ReShape(unfold_time_rank=True, time_major=False)\n\n        def custom_call(self_, inputs):\n            folded = reshape_fold.call(inputs)\n            unfolded = reshape_unfold.call(folded, inputs)\n            return unfolded\n\n        stack = Stack(reshape_fold, reshape_unfold, api_methods={(""call"", custom_call)})\n\n        test = ComponentTest(component=stack, input_spaces=dict(inputs=in_space))\n\n        # batch-size=4, seq-len=2\n        inputs = in_space.sample(size=(4, 2))\n\n        test.test((""call"", inputs), expected_outputs=inputs)\n\n    def test_reshape_with_batch_and_time_ranks_with_folding_and_unfolding_0D_shape(self):\n        # Flip time and batch rank via folding, then unfolding.\n        in_space = FloatBox(shape=(), add_batch_rank=True, add_time_rank=True, time_major=True)\n        reshape_fold = ReShape(fold_time_rank=True, scope=""fold-time-rank"")\n        reshape_unfold = ReShape(unfold_time_rank=True, scope=""unfold-time-rank"", time_major=True)\n\n        def custom_call(self_, inputs):\n            folded = reshape_fold.call(inputs)\n            unfolded = reshape_unfold.call(folded, inputs)\n            return unfolded\n\n        stack = Stack(reshape_fold, reshape_unfold, api_methods={(""call"", custom_call)})\n\n        test = ComponentTest(component=stack, input_spaces=dict(inputs=in_space))\n\n        # seq-len=16, batch-size=8\n        inputs = in_space.sample(size=(16, 8))\n\n        test.test((""call"", inputs), expected_outputs=inputs)\n\n    def test_reshape_with_batch_and_time_ranks_with_folding_and_explicit_unfolding(self):\n        time_rank = 8\n        in_space = FloatBox(shape=(2, 3), add_batch_rank=True, add_time_rank=True, time_major=True)\n        reshape_fold = ReShape(fold_time_rank=True, scope=""fold-time-rank"")\n        reshape_unfold = ReShape(unfold_time_rank=time_rank, scope=""unfold-time-rank"", time_major=True)\n\n        def custom_call(self_, inputs):\n            folded = reshape_fold.call(inputs)\n            unfolded = reshape_unfold.call(folded)  # no need for orig input here as unfolding is explicit\n            return unfolded\n\n        stack = Stack(reshape_fold, reshape_unfold, api_methods={(""call"", custom_call)})\n\n        test = ComponentTest(component=stack, input_spaces=dict(inputs=in_space))\n\n        # seq-len=time_rank, batch-size=n\n        inputs = in_space.sample(size=(time_rank, 12))\n\n        test.test((""call"", inputs), expected_outputs=inputs)\n'"
rlgraph/tests/components/test_ring_buffer.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\nfrom rlgraph.components.memories.ring_buffer import RingBuffer\nfrom rlgraph.spaces import Dict, BoolBox\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.tests.test_util import non_terminal_records, terminal_records, recursive_assert_almost_equal\nfrom six.moves import xrange as range_\n\n\nclass TestRingBufferMemory(unittest.TestCase):\n    """"""\n    Tests the ring buffer. The ring buffer has very similar tests to\n    the replay memory as it supports similar insertion and retrieval semantics,\n    but needs additional tests on episode indexing and its latest semantics.\n    """"""\n\n    record_space = Dict(\n        states=dict(state1=float, state2=float),\n        actions=dict(action1=float),\n        rewards=float,\n        terminals=BoolBox(),\n        add_batch_rank=True\n    )\n    # Generic memory variables.\n    memory_variables = [""size"", ""index""]\n\n    # Ring buffer variables\n    ring_buffer_variables = [""size"", ""index"", ""num-episodes"", ""episode-indices""]\n    capacity = 10\n\n    input_spaces = dict(\n        records=record_space,\n        num_records=int,\n        num_episodes=int\n    )\n    input_spaces_no_episodes = dict(\n        records=record_space,\n        num_records=int,\n    )\n\n    def test_capacity_with_episodes(self):\n        """"""\n        Tests if inserts of non-terminals work.\n\n        Note that this does not test episode semantics itself, which are tested below.\n        """"""\n        ring_buffer = RingBuffer(capacity=self.capacity)\n        test = ComponentTest(component=ring_buffer, input_spaces=self.input_spaces)\n        # Internal memory variables.\n        ring_buffer_variables = test.get_variable_values(ring_buffer, self.ring_buffer_variables)\n        size_value = ring_buffer_variables[""size""]\n        index_value = ring_buffer_variables[""index""]\n        num_episodes_value = ring_buffer_variables[""num-episodes""]\n        episode_index_values = ring_buffer_variables[""episode-indices""]\n\n        # Assert indices 0 before insert.\n        self.assertEqual(size_value, 0)\n        self.assertEqual(index_value, 0)\n        self.assertEqual(num_episodes_value, 0)\n        self.assertEqual(np.sum(episode_index_values), 0)\n\n        # Insert one more element than capacity. Note: this is different than\n        # replay test because due to episode semantics, it matters if\n        # these are terminal or not. This tests if episode index updating\n        # causes problems if none of the inserted elements are terminal.\n        observation = non_terminal_records(self.record_space, self.capacity + 1)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        ring_buffer_variables = test.get_variable_values(ring_buffer, self.ring_buffer_variables)\n        size_value = ring_buffer_variables[""size""]\n        index_value = ring_buffer_variables[""index""]\n        num_episodes_value = ring_buffer_variables[""num-episodes""]\n        episode_index_values = ring_buffer_variables[""episode-indices""]\n\n        # Size should be equivalent to capacity when full.\n        self.assertEqual(size_value, self.capacity)\n\n        # Index should be one over capacity due to modulo.\n        self.assertEqual(index_value, 1)\n        self.assertEqual(num_episodes_value, 0)\n        self.assertEqual(np.sum(episode_index_values), 0)\n\n        # If we fetch n elements, we expect to see exactly the last n.\n        for last_n in range(1, 6):\n            batch = test.test((""get_records"", last_n), expected_outputs=None)\n            recursive_assert_almost_equal(batch[""actions""][""action1""], observation[""actions""][""action1""][-last_n:])\n            recursive_assert_almost_equal(batch[""states""][""state2""], observation[""states""][""state2""][-last_n:])\n            recursive_assert_almost_equal(batch[""terminals""], observation[""terminals""][-last_n:])\n\n    def test_episode_indices_when_inserting(self):\n        """"""\n        Tests if episodes indices and counts are set correctly when inserting\n        terminals.\n        """"""\n        ring_buffer = RingBuffer(capacity=self.capacity)\n        test = ComponentTest(component=ring_buffer, input_spaces=self.input_spaces)\n\n        # First, we insert a single terminal record.\n        observation = terminal_records(self.record_space, 1)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        # Internal memory variables.\n        ring_buffer_variables = test.get_variable_values(ring_buffer, self.ring_buffer_variables)\n        num_episodes_value = ring_buffer_variables[""num-episodes""]\n        episode_index_values = ring_buffer_variables[""episode-indices""]\n\n        # One episode should be present.\n        self.assertEqual(num_episodes_value, 1)\n        # However, the index of that episode is 0, so we cannot fetch it.\n        self.assertEqual(sum(episode_index_values), 0)\n\n        # Next, we insert 1 non-terminal, then 1 terminal element.\n        observation = non_terminal_records(self.record_space, 1)\n        test.test((""insert_records"", observation), expected_outputs=None)\n        observation = terminal_records(self.record_space, 1)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        # Now, we expect to have 2 episodes with episode indices at 0 and 2.\n        ring_buffer_variables = test.get_variable_values(ring_buffer, self.ring_buffer_variables)\n        num_episodes_value = ring_buffer_variables[""num-episodes""]\n        episode_index_values = ring_buffer_variables[""episode-indices""]\n\n        print(\'Episode indices after = {}\'.format(episode_index_values))\n        self.assertEqual(num_episodes_value, 2)\n        self.assertEqual(episode_index_values[1], 2)\n\n    def test_only_terminal_with_episodes(self):\n        """"""\n        Edge case: What if only terminals are inserted when episode\n        semantics are enabled?\n        """"""\n        ring_buffer = RingBuffer(capacity=self.capacity)\n        test = ComponentTest(component=ring_buffer, input_spaces=self.input_spaces)\n        observation = terminal_records(self.record_space, self.capacity)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        ring_buffer_variables = test.get_variable_values(ring_buffer, self.ring_buffer_variables)\n        num_episodes_value = ring_buffer_variables[""num-episodes""]\n        episode_index_values = ring_buffer_variables[""episode-indices""]\n\n        self.assertEqual(num_episodes_value, self.capacity)\n        # Every episode index should correspond to its position\n        for i in range_(self.capacity):\n            self.assertEqual(episode_index_values[i], i)\n\n    def test_episode_fetching(self):\n        """"""\n        Test if we can accurately fetch most recent episodes.\n        """"""\n        ring_buffer = RingBuffer(capacity=self.capacity)\n        test = ComponentTest(component=ring_buffer, input_spaces=self.input_spaces)\n\n        # Insert 2 non-terminals, 1 terminal\n        observation = non_terminal_records(self.record_space, 2)\n        test.test((""insert_records"", observation), expected_outputs=None)\n        observation = terminal_records(self.record_space, 1)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        ring_buffer_variables = test.get_variable_values(ring_buffer, self.ring_buffer_variables)\n        num_episodes_value = ring_buffer_variables[""num-episodes""]\n        episode_index_values = ring_buffer_variables[""episode-indices""]\n\n        # One episode.\n        self.assertEqual(num_episodes_value, 1)\n        expected_indices = [0] * self.capacity\n        expected_indices[0] = 2\n        recursive_assert_almost_equal(episode_index_values, expected_indices)\n\n        # We should now be able to retrieve one episode of length 3.\n        episode = test.test((""get_episodes"", 1), expected_outputs=None)\n        expected_terminals = [0, 0, 1]\n        recursive_assert_almost_equal(episode[""terminals""], expected_terminals)\n\n        # We should not be able to retrieve two episodes, and still return just one.\n        episode = test.test((""get_episodes"", 2), expected_outputs=None)\n        expected_terminals = [0, 0, 1]\n        recursive_assert_almost_equal(episode[""terminals""], expected_terminals)\n\n        # Insert 7 non-terminals.\n        observation = non_terminal_records(self.record_space, 7)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        ring_buffer_variables = test.get_variable_values(ring_buffer, self.ring_buffer_variables)\n        index_value = ring_buffer_variables[""index""]\n        episode_index_values = ring_buffer_variables[""episode-indices""]\n\n        # Episode indices should not have changed.\n        expected_indices[0] = 2\n        recursive_assert_almost_equal(episode_index_values, expected_indices)\n        # Inserted 2 non-terminal, 1 terminal, 7 non-terminal at capacity 10 -> should be at 0 again.\n        self.assertEqual(index_value, 0)\n\n        # Now inserting one terminal so the terminal buffer has layout [1 0 1 0 0 0 0 0 0 0]\n        observation = terminal_records(self.record_space, 1)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        # Episode indices:\n        ring_buffer_variables = test.get_variable_values(ring_buffer, self.ring_buffer_variables)\n        num_episodes_value = ring_buffer_variables[""num-episodes""]\n        recursive_assert_almost_equal(num_episodes_value, 2)\n\n        # # Check if we can fetch 2 episodes:\n        episodes = test.test((""get_episodes"", 2), expected_outputs=None)\n        #\n        # # We now expect to have retrieved:\n        # # - 10 time steps\n        # # - 2 terminal values 1\n        # # - Terminal values spaced apart 1 index due to the insertion order\n        self.assertEqual(len(episodes[\'terminals\']), self.capacity)\n        self.assertEqual(episodes[\'terminals\'][0], True)\n        self.assertEqual(episodes[\'terminals\'][2], True)\n\n    def test_latest_batch(self):\n        """"""\n        Tests if we can fetch latest steps.\n        """"""\n        ring_buffer = RingBuffer(capacity=self.capacity)\n        test = ComponentTest(component=ring_buffer, input_spaces=self.input_spaces)\n\n        # Insert 5 random elements.\n        observation = non_terminal_records(self.record_space, 5)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        # First, test if the basic computation works.\n        batch = test.test((""get_records"", 5), expected_outputs=None)\n        recursive_assert_almost_equal(batch, observation)\n\n        # Next, insert capacity more elements:\n        observation = non_terminal_records(self.record_space, self.capacity)\n        test.test((""insert_records"", observation), expected_outputs=None)\n\n        # If we now fetch capacity elements, we expect to see exactly the last 10.\n        batch = test.test((""get_records"", self.capacity), expected_outputs=None)\n        recursive_assert_almost_equal(batch, observation)\n\n        # If we fetch n elements, we expect to see exactly the last n.\n        for last_n in range(1, 6):\n            batch = test.test((""get_records"", last_n), expected_outputs=None)\n            recursive_assert_almost_equal(batch[""actions""][""action1""], observation[""actions""][""action1""][-last_n:])\n            recursive_assert_almost_equal(batch[""states""][""state2""], observation[""states""][""state2""][-last_n:])\n            recursive_assert_almost_equal(batch[""terminals""], observation[""terminals""][-last_n:])\n\n'"
rlgraph/tests/components/test_sac_loss_function.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\nimport logging\nimport numpy as np\n\nfrom rlgraph.agents.sac_agent import SACLossFunction\nfrom rlgraph.spaces import FloatBox, BoolBox, Tuple, IntBox\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.utils import root_logger\n\n\nclass TestSACLossFunction(unittest.TestCase):\n    """"""\n    Tests the SAC Agent\'s functionality.\n    """"""\n    root_logger.setLevel(level=logging.DEBUG)\n\n    @staticmethod\n    def _prepare_loss_function_test(loss_function):\n        test = ComponentTest(\n            component=loss_function,\n            input_spaces=dict(\n                alpha=float,\n                log_probs_next_sampled=FloatBox(shape=(1,), add_batch_rank=True),\n                q_values_next_sampled=Tuple(FloatBox(shape=(1,)), FloatBox(shape=(1,)), add_batch_rank=True),\n                q_values=Tuple(FloatBox(shape=(1,)), FloatBox(shape=(1,)), add_batch_rank=True),\n                log_probs_sampled=FloatBox(shape=(1,), add_batch_rank=True),\n                q_values_sampled=Tuple(FloatBox(shape=(1,)), FloatBox(shape=(1,)), add_batch_rank=True),\n                rewards=FloatBox(add_batch_rank=True),\n                terminals=BoolBox(add_batch_rank=True),\n                loss_per_item=FloatBox(add_batch_rank=True)\n            ),\n            action_space=IntBox(2, shape=(), add_batch_rank=True)\n        )\n        return test\n\n    def test_sac_loss_function(self):\n        loss_function = SACLossFunction(\n            target_entropy=0.1, discount=0.8\n        )\n        test = self._prepare_loss_function_test(loss_function)\n\n        batch_size = 10\n        inputs = [\n            0.9,  # alpha\n            [[0.5]] * batch_size,  # log_probs_next_sampled\n            ([[0.0]] * batch_size, [[1.0]] * batch_size),  # q_values_next_sampled\n            ([[0.0]] * batch_size, [[1.0]] * batch_size),  # q_values\n            [[0.5]] * batch_size,  # log_probs_sampled\n            ([[.1]] * batch_size, [[.2]] * batch_size),  # q_values_sampled\n            [1.0] * batch_size,  # rewards\n            [False] * batch_size  # terminals\n        ]\n\n        policy_loss_per_item = [.35] * batch_size\n        values_loss_per_item = [.2696] * batch_size\n        alpha_loss_per_item = [.06321] * batch_size\n\n        test.test(\n            (loss_function.loss, inputs),\n            expected_outputs=[\n                np.mean(policy_loss_per_item),\n                policy_loss_per_item,\n                np.mean(values_loss_per_item),\n                values_loss_per_item,\n                np.mean(alpha_loss_per_item),\n                alpha_loss_per_item\n            ],\n            decimals=5\n        )\n\n        test.test(\n            (loss_function.loss_per_item, inputs),\n            expected_outputs=[\n                policy_loss_per_item,\n                values_loss_per_item,\n                alpha_loss_per_item\n            ],\n            decimals=5\n        )\n\n        test.test(\n            (loss_function.loss_average, [policy_loss_per_item]),\n            expected_outputs=[np.mean(policy_loss_per_item)],\n            decimals=5\n        )\n\n    def test_sac_loss_function_no_target_entropy(self):\n        loss_function = SACLossFunction(\n            target_entropy=None, discount=0.8\n        )\n        test = self._prepare_loss_function_test(loss_function)\n\n        batch_size = 10\n        inputs = [\n            0.9,  # alpha\n            [[0.5]] * batch_size,  # log_probs_next_sampled\n            ([[0.0]] * batch_size, [[1.0]] * batch_size),  # q_values_next_sampled\n            ([[0.0]] * batch_size, [[1.0]] * batch_size),  # q_values\n            [[0.5]] * batch_size,  # log_probs_sampled\n            ([[.1]] * batch_size, [[.2]] * batch_size),  # q_values_sampled\n            [1.0] * batch_size,  # rewards\n            [False] * batch_size  # terminals\n        ]\n\n        policy_loss_per_item = [.35] * batch_size\n        values_loss_per_item = [.2696] * batch_size\n        alpha_loss_per_item = [0.0] * batch_size\n\n        test.test(\n            (loss_function.loss, inputs),\n            expected_outputs=[\n                np.mean(policy_loss_per_item),\n                policy_loss_per_item,\n                np.mean(values_loss_per_item),\n                values_loss_per_item,\n                np.mean(alpha_loss_per_item),\n                alpha_loss_per_item\n            ],\n            decimals=5\n        )\n\n        test.test(\n            (loss_function.loss_per_item, inputs),\n            expected_outputs=[\n                policy_loss_per_item,\n                values_loss_per_item,\n                alpha_loss_per_item\n            ],\n            decimals=5\n        )\n\n        test.test(\n            (loss_function.loss_average, [policy_loss_per_item]),\n            expected_outputs=[np.mean(policy_loss_per_item)],\n            decimals=5\n        )\n'"
rlgraph/tests/components/test_sampler_component.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nfrom rlgraph.components import Sampler\nfrom rlgraph.spaces import BoolBox, Dict\nfrom rlgraph.tests import ComponentTest\n\n\nclass TestSamplerComponent(unittest.TestCase):\n    """"""\n    Tests the sampler component.\n    """"""\n    def test_sampler_component(self):\n        input_space = Dict(\n            states=dict(state1=float, state2=float),\n            actions=dict(action1=float),\n            reward=float,\n            terminals=BoolBox(),\n            add_batch_rank=True\n        )\n\n        sampler = Sampler()\n        test = ComponentTest(component=sampler, input_spaces=dict(sample_size=int, inputs=input_space))\n\n        samples = input_space.sample(size=100)\n        sample = test.test((""sample"", [10, samples]), expected_outputs=None)\n\n        self.assertEqual(len(sample[""actions""][""action1""]), 10)\n        self.assertEqual(len(sample[""states""][""state1""]), 10)\n        self.assertEqual(len(sample[""terminals""]), 10)\n\n        print(sample)\n'"
rlgraph/tests/components/test_sequence_helper.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\nfrom rlgraph.components.helpers.sequence_helper import SequenceHelper\nfrom rlgraph.spaces import FloatBox, BoolBox\nfrom rlgraph.tests import ComponentTest, recursive_assert_almost_equal\n\n\nclass TestSequenceHelper(unittest.TestCase):\n\n    input_spaces = dict(\n        sequence_indices=BoolBox(add_batch_rank=True),\n        terminals=BoolBox(add_batch_rank=True),\n        values=FloatBox(add_batch_rank=True),\n        rewards=FloatBox(add_batch_rank=True),\n        decay=float\n    )\n\n    @staticmethod\n    def decay_td_sequence(td_errors, decay=0.99, value_next=0.0):\n        discounted_td_errors = np.zeros_like(td_errors)\n        running_add = value_next\n        for t in reversed(range(0, td_errors.size)):\n            running_add = running_add * decay + td_errors[t]\n            discounted_td_errors[t] = running_add\n        return discounted_td_errors\n\n    @staticmethod\n    def deltas(baseline, reward, discount, terminals, sequence_values):\n        """"""\n        Computes expected 1-step TD errors over a sequence of rewards, terminals, sequence-indices:\n\n        delta = reward + discount * bootstrapped_values[1:] - bootstrapped_values[:-1]\n        """"""\n        deltas = []\n        start_index = 0\n        i = 0\n        for _ in range(len(baseline)):\n            if np.all(sequence_values[i]):\n                # Compute deltas for this sub-sequence.\n                # Cannot do this all at once because we would need the correct offsets for each sub-sequence.\n                baseline_slice = list(baseline[start_index:i + 1])\n\n                # Boot-strap: If also terminal, with 0, else with last value.\n                if np.all(terminals[i]):\n                    print(""Appending boot-strap val 0 at index."", i)\n                    baseline_slice.append(0)\n                else:\n                    print(""Appending boot-strap val {} at index {}."".format(baseline[i], i))\n                    baseline_slice.append(baseline[i])\n\n                adjusted_v = np.asarray(baseline_slice)\n\n                print(""adjusted_v"", adjusted_v)\n                print(""adjusted_v[1:]"", adjusted_v[1:])\n                print(""adjusted_v[:-1]"",  adjusted_v[:-1])\n\n                # +1 because we want to include i-th value.\n                delta = reward[start_index:i + 1] + discount * adjusted_v[1:] - adjusted_v[:-1]\n                deltas.extend(delta)\n                start_index = i + 1\n            i += 1\n\n        return np.array(deltas)\n\n    def test_calc_sequence_lengths(self):\n        """"""\n        Tests counting sequence lengths based on terminal configurations.\n        """"""\n        sequence_helper = SequenceHelper()\n        test = ComponentTest(component=sequence_helper, input_spaces=self.input_spaces)\n        input_ = np.asarray([0, 0, 0, 0])\n        test.test((""calc_sequence_lengths"", input_), expected_outputs=[4])\n\n        input_ = np.asarray([0, 0, 1, 0])\n        test.test((""calc_sequence_lengths"", input_), expected_outputs=[3, 1])\n\n        input_ = np.asarray([1, 1, 1, 1])\n        test.test((""calc_sequence_lengths"", input_), expected_outputs=[1, 1, 1, 1])\n\n        input_ = np.asarray([1, 0, 0, 1])\n        test.test((""calc_sequence_lengths"", input_), expected_outputs=[1, 3])\n\n    def test_bootstrapping(self):\n        """"""\n        Tests boot-strapping for GAE purposes.\n        """"""\n        sequence_helper = SequenceHelper()\n        discount = 0.99\n\n        test = ComponentTest(component=sequence_helper, input_spaces=self.input_spaces)\n\n        # No terminals - just boot-strap with final sequence index.\n        values = np.asarray([1.0, 2.0, 3.0, 4.0])\n        rewards = np.asarray([0, 0, 0, 0])\n        sequence_indices = np.asarray([0, 0, 0, 1])\n        terminals = np.asarray([0, 0, 0, 0])\n\n        expected_deltas = self.deltas(values, rewards, discount, terminals, sequence_indices)\n        deltas = test.test((""bootstrap_values"", [rewards, values, terminals, sequence_indices]))\n        recursive_assert_almost_equal(expected_deltas, deltas, decimals=5)\n\n        # Final index is also terminal.\n        values = np.asarray([1.0, 2.0, 3.0, 4.0])\n        rewards = np.asarray([0, 0, 0, 0])\n        sequence_indices = np.asarray([0, 0, 0, 1])\n        terminals = np.asarray([0, 0, 0, 1])\n\n        expected_deltas = self.deltas(values, rewards, discount, terminals, sequence_indices)\n        deltas = test.test((""bootstrap_values"", [rewards, values, terminals, sequence_indices]))\n        recursive_assert_almost_equal(expected_deltas, deltas, decimals=5)\n\n        # Mixed: i = 1 is also terminal, i = 3 is only sequence.\n        values = np.asarray([1.0, 2.0, 3.0, 4.0])\n        rewards = np.asarray([0, 0, 0, 0])\n        sequence_indices = np.asarray([0, 1, 0, 1])\n        terminals = np.asarray([0, 1, 0, 0])\n\n        expected_deltas = self.deltas(values, rewards, discount, terminals, sequence_indices)\n        deltas = test.test((""bootstrap_values"", [rewards, values, terminals, sequence_indices]))\n        recursive_assert_almost_equal(expected_deltas, deltas, decimals=5)\n\n    def test_calc_decays(self):\n        """"""\n        Tests counting sequence lengths based on terminal configurations.\n        """"""\n        sequence_helper = SequenceHelper()\n        decay_value = 0.5\n\n        test = ComponentTest(component=sequence_helper, input_spaces=self.input_spaces)\n        input_ = np.asarray([0, 0, 0, 0])\n        expected_decays = [1.0, 0.5, 0.25, 0.125]\n        lengths, decays = test.test((""calc_sequence_decays"", [input_, decay_value]))\n\n        # Check lengths and decays.\n        recursive_assert_almost_equal(x=lengths, y=[4])\n        recursive_assert_almost_equal(x=decays, y=expected_decays)\n\n        input_ = np.asarray([0, 0, 1, 0])\n        expected_decays = [1.0, 0.5, 0.25, 1.0]\n        lengths, decays = test.test((""calc_sequence_decays"", [input_, decay_value]))\n\n        recursive_assert_almost_equal(x=lengths, y=[3, 1])\n        recursive_assert_almost_equal(x=decays, y=expected_decays)\n\n        input_ = np.asarray([1, 1, 1, 1])\n        expected_decays = [1.0, 1.0, 1.0, 1.0]\n        lengths, decays = test.test((""calc_sequence_decays"", [input_, decay_value]))\n\n        recursive_assert_almost_equal(x=lengths, y=[1, 1, 1, 1])\n        recursive_assert_almost_equal(x=decays, y=expected_decays)\n\n    def test_reverse_apply_decays_to_sequence(self):\n        """"""\n        Tests reverse decaying a sequence of 1-step TD errors for GAE.\n        """"""\n        sequence_helper = SequenceHelper()\n        decay_value = 0.5\n\n        test = ComponentTest(component=sequence_helper, input_spaces=self.input_spaces)\n        td_errors = np.asarray([0.1, 0.2, 0.3, 0.4])\n        indices = np.array([0, 0, 0, 1])\n        expected_output_sequence_manual = np.asarray([\n            0.1 + 0.5 * 0.2 + 0.25 * 0.3 + 0.125 * 0.4,\n            0.2 + 0.5 * 0.3 + 0.25 * 0.4,\n            0.3 + 0.5 * 0.4,\n            0.4\n        ])\n        expected_output_sequence_numpy = self.decay_td_sequence(td_errors, decay=decay_value)\n        recursive_assert_almost_equal(expected_output_sequence_manual, expected_output_sequence_numpy)\n        test.test(\n            (""reverse_apply_decays_to_sequence"", [td_errors, indices, decay_value]),\n            expected_outputs=expected_output_sequence_manual\n        )\n'"
rlgraph/tests/components/test_sequence_preprocessor.py,0,"b'# Copyright 2018/2019 The RLgraph authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\nfrom six.moves import xrange as range_\n\nfrom rlgraph.components.layers import Sequence\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest, recursive_assert_almost_equal\n\n\nclass TestSequencePreprocessor(unittest.TestCase):\n\n    def test_sequence_preprocessor(self):\n        space = FloatBox(shape=(1,), add_batch_rank=True)\n        sequencer = Sequence(sequence_length=3, add_rank=True)\n        test = ComponentTest(component=sequencer, input_spaces=dict(inputs=space))\n\n        vars = sequencer.get_variables(""index"", ""buffer"", global_scope=False)\n        index, buffer = vars[""index""], vars[""buffer""]\n\n        for _ in range_(3):\n            test.test(""reset"")\n            index_value, buffer_value = test.read_variable_values(index, buffer)\n            self.assertEqual(index_value, -1)\n            test.test((""call"", np.array([[0.1]])),\n                      expected_outputs=np.array([[[0.1, 0.1, 0.1]]]))\n            index_value, buffer_value = test.read_variable_values(index, buffer)\n            self.assertEqual(index_value, 0)\n            test.test((""call"", np.array([[0.2]])),\n                      expected_outputs=np.array([[[0.1, 0.1, 0.2]]]))\n            index_value, buffer_value = test.read_variable_values(index, buffer)\n            self.assertEqual(index_value, 1)\n            test.test((""call"", np.array([[0.3]])),\n                      expected_outputs=np.array([[[0.1, 0.2, 0.3]]]))\n            index_value, buffer_value = test.read_variable_values(index, buffer)\n            self.assertEqual(index_value, 2)\n            test.test((""call"", np.array([[0.4]])),\n                      expected_outputs=np.array([[[0.2, 0.3, 0.4]]]))\n            index_value, buffer_value = test.read_variable_values(index, buffer)\n            self.assertEqual(index_value, 0)\n            test.test((""call"", np.array([[0.5]])),\n                      expected_outputs=np.array([[[0.3, 0.4, 0.5]]]))\n            index_value, buffer_value = test.read_variable_values(index, buffer)\n            self.assertEqual(index_value, 1)\n\n        test.terminate()\n\n    # TODO: Make it irrelevent whether we test a python or a tf Component (API and handling should be 100% identical)\n    def test_python_sequence_preprocessor(self):\n        seq_len = 3\n        space = FloatBox(shape=(1,), add_batch_rank=True)\n        sequencer = Sequence(sequence_length=seq_len, batch_size=4, add_rank=True, backend=""python"")\n        sequencer.create_variables(input_spaces=dict(inputs=space))\n\n        #test = ComponentTest(component=sequencer, input_spaces=dict(call=space))\n\n        for _ in range_(3):\n            sequencer._graph_fn_reset()\n            self.assertEqual(sequencer.index, -1)\n            input_ = np.asarray([[1.0], [2.0], [3.0], [4.0]])\n            out = sequencer._graph_fn_call(input_)\n            self.assertEqual(sequencer.index, 0)\n            recursive_assert_almost_equal(\n                out, np.asarray([[[1.0, 1.0, 1.0]], [[2.0, 2.0, 2.0]], [[3.0, 3.0, 3.0]], [[4.0, 4.0, 4.0]]])\n            )\n            input_ = np.asarray([[1.1], [2.2], [3.3], [4.4]])\n            out = sequencer._graph_fn_call(input_)\n            self.assertEqual(sequencer.index, 1)\n            recursive_assert_almost_equal(\n                out, np.asarray([[[1.0, 1.0, 1.1]], [[2.0, 2.0, 2.2]], [[3.0, 3.0, 3.3]], [[4.0, 4.0, 4.4]]])\n            )\n            input_ = np.asarray([[1.11], [2.22], [3.33], [4.44]])\n            out = sequencer._graph_fn_call(input_)\n            self.assertEqual(sequencer.index, 2)\n            recursive_assert_almost_equal(\n                out, np.asarray([[[1.0, 1.1, 1.11]], [[2.0, 2.2, 2.22]], [[3.0, 3.3, 3.33]], [[4.0, 4.4, 4.44]]])\n            )\n            input_ = np.asarray([[10], [20], [30], [40]])\n            out = sequencer._graph_fn_call(input_)\n            self.assertEqual(sequencer.index, 0)\n            recursive_assert_almost_equal(\n                out, np.asarray([[[1.1, 1.11, 10]], [[2.2, 2.22, 20]], [[3.3, 3.33, 30]], [[4.4, 4.44, 40]]])\n            )\n\n    def test_sequence_preprocessor_with_batch(self):\n        space = FloatBox(shape=(2,), add_batch_rank=True)\n        sequencer = Sequence(sequence_length=2, batch_size=3, add_rank=True)\n        test = ComponentTest(component=sequencer, input_spaces=dict(inputs=space))\n\n        vars = sequencer.get_variables(""index"", ""buffer"", global_scope=False)\n        index, buffer = vars[""index""], vars[""buffer""]\n\n        for _ in range_(3):\n            test.test(""reset"")\n            index_value, buffer_value = test.read_variable_values(index, buffer)\n            self.assertEqual(index_value, -1)\n\n            test.test((""call"", np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])),\n                      expected_outputs=np.array([\n                          [[1.0, 1.0], [2.0, 2.0]],\n                          [[3.0, 3.0], [4.0, 4.0]],\n                          [[5.0, 5.0], [6.0, 6.0]]\n                      ]))\n            index_value, buffer_value = test.read_variable_values(index, buffer)\n            self.assertEqual(index_value, 0)\n\n            test.test((""call"", np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])),\n                      expected_outputs=np.array([\n                          [[1.0, 0.1], [2.0, 0.2]],\n                          [[3.0, 0.3], [4.0, 0.4]],\n                          [[5.0, 0.5], [6.0, 0.6]]\n                      ]))\n            index_value, buffer_value = test.read_variable_values(index, buffer)\n            self.assertEqual(index_value, 1)\n\n            test.test((""call"", np.array([[10.0, 20.0], [30.0, 40.0], [50.0, 60.0]])),\n                      expected_outputs=np.array([\n                          [[0.1, 10.0], [0.2, 20.0]],\n                          [[0.3, 30.0], [0.4, 40.0]],\n                          [[0.5, 50.0], [0.6, 60.0]]\n                      ]))\n            index_value, buffer_value = test.read_variable_values(index, buffer)\n            self.assertEqual(index_value, 0)\n\n            test.test((""call"", np.array([[100.0, 200.0], [300.0, 400.0], [500.0, 600.0]])),\n                      expected_outputs=np.array([\n                          [[10.0, 100.0], [20.0, 200.0]],\n                          [[30.0, 300.0], [40.0, 400.0]],\n                          [[50.0, 500.0], [60.0, 600.0]]\n                      ]))\n            index_value, buffer_value = test.read_variable_values(index, buffer)\n            self.assertEqual(index_value, 1)\n\n        test.terminate()\n\n    def test_sequence_preprocessor_with_container_space(self):\n        # Test with no batch rank.\n        space = Tuple(\n            FloatBox(shape=(1,)),\n            FloatBox(shape=(2, 2)),\n            add_batch_rank=False\n        )\n\n        component_to_test = Sequence(sequence_length=4, add_rank=False)\n        test = ComponentTest(component=component_to_test, input_spaces=dict(inputs=space))\n\n        for i in range_(3):\n            test.test(""reset"")\n\n            test.test((""call"", (tuple([np.array([0.5]), np.array([[0.6, 0.7], [0.8, 0.9]])]),)),\n                      expected_outputs=(np.array([0.5, 0.5, 0.5, 0.5]), np.array([[0.6, 0.7] * 4,\n                                                                                  [0.8, 0.9] * 4])))\n            test.test((""call"", (tuple([np.array([0.6]), np.array([[1.1, 1.1], [1.1, 1.1]])]),)),\n                      expected_outputs=(np.array([0.5, 0.5, 0.5, 0.6]), np.array([[0.6, 0.7, 0.6, 0.7,\n                                                                                   0.6, 0.7, 1.1, 1.1],\n                                                                                  [0.8, 0.9, 0.8, 0.9,\n                                                                                   0.8, 0.9, 1.1, 1.1]])))\n            test.test((""call"", (tuple([np.array([0.7]), np.array([[2.0, 2.1], [2.2, 2.3]])]),)),\n                      expected_outputs=(np.array([0.5, 0.5, 0.6, 0.7]), np.array([[0.6, 0.7, 0.6, 0.7,\n                                                                                   1.1, 1.1, 2.0, 2.1],\n                                                                                  [0.8, 0.9, 0.8, 0.9,\n                                                                                   1.1, 1.1, 2.2, 2.3]])))\n\n        test.terminate()\n'"
rlgraph/tests/components/test_slice.py,0,"b'# Copyright 2018/2019 The Rlgraph Authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport unittest\n\nfrom rlgraph.components.common.slice import Slice\nfrom rlgraph.spaces import FloatBox, IntBox\nfrom rlgraph.tests.component_test import ComponentTest\n\n\nclass TestSlice(unittest.TestCase):\n\n    def test_slice_with_squeeze(self):\n        slicer = Slice(squeeze=True)\n        input_space = FloatBox(shape=(2, 2, 3), add_batch_rank=True, add_time_rank=True, time_major=True)\n        test = ComponentTest(component=slicer, input_spaces=dict(\n            inputs=input_space,\n            start_index=IntBox(),\n            end_index=IntBox()\n        ))\n\n        # Time-steps=3, Batch=5\n        inputs = input_space.sample(size=(3, 5))\n        expected = inputs[1]\n        test.test((""slice"", [inputs, 1, 2]), expected_outputs=expected)\n\n        expected = inputs[0:2]\n        test.test((""slice"", [inputs, 0, 2]), expected_outputs=expected)\n\n        expected = inputs[0]\n        test.test((""slice"", [inputs, 0, 1]), expected_outputs=expected)\n\n    def test_slice_without_squeeze(self):\n        slicer = Slice(squeeze=False)\n        input_space = FloatBox(shape=(1, 4, 5), add_batch_rank=True)\n        test = ComponentTest(component=slicer, input_spaces=dict(\n            inputs=input_space,\n            start_index=IntBox(),\n            end_index=IntBox()\n        ))\n\n        # Time-steps=3, Batch=5\n        inputs = input_space.sample(size=4)\n        expected = np.asarray([inputs[1]])  # Add the not-squeezed rank back to expected.\n        test.test((""slice"", [inputs, 1, 2]), expected_outputs=expected)\n\n        expected = inputs[0:2]\n        test.test((""slice"", [inputs, 0, 2]), expected_outputs=expected)\n\n        expected = np.asarray([inputs[0]])\n        test.test((""slice"", [inputs, 0, 1]), expected_outputs=expected)\n'"
rlgraph/tests/components/test_softmax.py,0,"b'# Copyright 2018/2019 The Rlgraph Authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport unittest\n\nfrom rlgraph.components.common.softmax import Softmax\nfrom rlgraph.spaces import FloatBox, IntBox, Dict\nfrom rlgraph.tests.component_test import ComponentTest\nfrom rlgraph.utils.numpy import softmax as softmax_\n\n\nclass TestSoftmax(unittest.TestCase):\n\n    def test_softmax_on_simple_inputs(self):\n        softmax = Softmax()\n        input_space = FloatBox(shape=(2, 2, 3), add_batch_rank=True)\n        test = ComponentTest(component=softmax, input_spaces=dict(logits=input_space))\n\n        # Batch=5\n        inputs = input_space.sample(5)\n        expected = softmax_(inputs)\n        test.test((""softmax"", inputs), expected_outputs=(expected, np.log(expected)))\n\n    def test_softmax_on_complex_inputs(self):\n        softmax = Softmax()\n        input_space = Dict(dict(a=FloatBox(shape=(4, 5)), b=FloatBox(shape=(3,))),\n                                add_batch_rank=True, add_time_rank=True)\n        test = ComponentTest(component=softmax, input_spaces=dict(logits=input_space))\n\n        inputs = input_space.sample(size=(4, 5))\n        expected = dict(\n            a=softmax_(inputs[""a""]),\n            b=softmax_(inputs[""b""])\n        )\n        expected_logs = dict(\n            a=np.log(expected[""a""]),\n            b=np.log(expected[""b""])\n        )\n        test.test((""softmax"", inputs), expected_outputs=(expected, expected_logs), decimals=5)\n\n'"
rlgraph/tests/components/test_stack.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\nimport numpy as np\n\nfrom rlgraph.utils.rlgraph_errors import RLGraphAPICallParamError\nfrom rlgraph.components.neural_networks import Stack\nfrom rlgraph.components.common import RepeaterStack\nfrom rlgraph.tests.dummy_components import Dummy1To1, Dummy2To1, Dummy1To2, Dummy2To2, Dummy0To1\nfrom rlgraph.spaces import FloatBox\nfrom rlgraph.tests import ComponentTest\n\n\nclass TestStack(unittest.TestCase):\n    """"""\n    Tests for Stack Components of different length (number of sub-Components).\n    """"""\n    def test_one_sub_component(self):\n        stack = Stack(Dummy1To1(constant_value=3.0), api_methods={""run""})\n        test = ComponentTest(component=stack, input_spaces=dict(inputs=float))\n\n        test.test((""run"", 4.6), expected_outputs=7.6)\n\n    def test_two_sub_components(self):\n        stack = Stack(Dummy1To1(scope=""A"", constant_value=3.0),\n                      Dummy1To1(scope=""B"", constant_value=1.0),\n                      api_methods={""run""})\n        test = ComponentTest(component=stack, input_spaces=dict(inputs=[float]))\n\n        test.test((""run"", 4.6), expected_outputs=np.array(8.6, dtype=np.float32))\n\n    def test_two_sub_components_and_time_rank_folding(self):\n        stack = Stack(Dummy1To1(scope=""A"", constant_value=3.0),\n                      Dummy1To1(scope=""B"", constant_value=1.0),\n                      api_methods=[dict(api=""run"", fold_time_rank=True)])\n        test = ComponentTest(\n            component=stack, input_spaces=dict(inputs=[FloatBox(add_time_rank=True, add_batch_rank=True)])\n        )\n\n        test.test((""run"", np.array([[4.6, 5.2], [1.0, 2.0]])),\n                  expected_outputs=np.array([8.6, 9.2, 5.0, 6.0], dtype=np.float32))\n\n    def test_two_sub_components_and_time_rank_unfolding(self):\n        stack = Stack(Dummy1To1(scope=""A"", constant_value=3.0),\n                      Dummy1To1(scope=""B"", constant_value=1.0),\n                      api_methods=[dict(api=""run"", unfold_time_rank=True)])\n        input_space = FloatBox(add_batch_rank=True)\n        test = ComponentTest(\n            component=stack, input_spaces=dict(inputs=[input_space, input_space.with_time_rank()])\n        )\n\n        input_ = input_space.sample(size=4)\n        input_before_folding = input_.reshape((2, 2))\n\n        test.test((""run"", [np.array([4.6, 5.2, 1.0, 2.0]), input_before_folding]),\n                  expected_outputs=np.array([[8.6, 9.2], [5.0, 6.0]], dtype=np.float32))\n\n    def test_two_sub_components_1to2_2to1(self):\n        stack = Stack(Dummy1To2(scope=""A"", constant_value=1.5),\n                      Dummy2To1(scope=""B""),\n                      api_methods={""run""})\n        test = ComponentTest(component=stack, input_spaces=dict(inputs=FloatBox()))\n\n        # Expect: (in + 1.5, in * 1.5) -> (1.5, 0.0) -> (1.5 + 0.0) = 1.5\n        test.test((""run"", 0.0), expected_outputs=np.array(1.5, dtype=np.float32))\n\n    def test_two_sub_components_2to1_1to2(self):\n        # Try different ctor with list as first item.\n        stack = Stack([Dummy2To1(scope=""A""), Dummy1To2(scope=""B"", constant_value=2.3)], api_methods={""run""})\n        test = ComponentTest(component=stack, input_spaces=dict(inputs=[FloatBox(), float]))\n\n        # Expect: (in1 + in2) -> 0.1 -> (0.1 + 2.3, 0.1 * 2.3)\n        test.test((""run"", [0.0, 0.1]), expected_outputs=(2.4, 0.23))\n\n    def test_two_sub_components_1to2_2to1_time_rank_folding_and_unfolding(self):\n        stack = Stack([Dummy1To2(scope=""A"", constant_value=1.5), Dummy2To1(scope=""B"")],\n                      api_methods=[dict(api=""run"", fold_time_rank=True, unfold_time_rank=True)])\n        input_space = FloatBox(add_batch_rank=True, add_time_rank=True, time_major=True)\n        test = ComponentTest(component=stack, input_spaces=dict(inputs=[input_space]))\n\n        input_ = input_space.sample(size=(2, 3))\n        expected_outputs = input_ + 1.5 + (input_ * 1.5)\n\n        test.test((""run"", input_), expected_outputs=expected_outputs)\n\n    def test_repeater_stack_with_n_sub_components(self):\n        repeater_stack = RepeaterStack(sub_component=Dummy2To2(scope=""2To2"", constant_value=0.5), repeats=10,\n                                       api_methods={(""some_crazy_new_api_method_name"", ""run"")})\n        test = ComponentTest(component=repeater_stack, input_spaces=dict(inputs=[FloatBox(), float]))\n\n        # 1st return value: 10 times add 0.5 (to initially 0.0).\n        # 2nd return value: 10 times multiply with 0.5 (to initially 2048)\n        expected_outputs = (5.0, 2.0)\n        test.test((""some_crazy_new_api_method_name"", [0.0, 2048]), expected_outputs=expected_outputs)\n\n    def test_non_matching_sub_components_in_stack(self):\n        stack = Stack(Dummy2To1(scope=""A""), Dummy2To1(scope=""B""), api_methods={""run""})\n        try:\n            ComponentTest(component=stack, input_spaces=dict(inputs=[float, float]))\n        except RLGraphAPICallParamError:\n            print(""expected this error."")\n            return\n        else:\n            # Something went wrong.\n            assert False, ""Expected Error on non-matching sub-Components in Stack, but none was thrown!""\n\n    def test_other_non_matching_sub_components_in_stack(self):\n        stack = Stack(Dummy1To2(scope=""A""), Dummy2To1(scope=""B""), Dummy1To1(scope=""C""), Dummy0To1(scope=""D""),\n                      api_methods={""run""})\n        try:\n            ComponentTest(component=stack, input_spaces=dict(inputs=FloatBox()))\n        except RLGraphAPICallParamError:\n            print(""expected this error."")\n            return\n        else:\n            # Something went wrong.\n            assert False, ""Expected Error on non-matching sub-Components in Stack, but none was thrown!""\n\n'"
rlgraph/tests/components/test_staging_area.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport unittest\n\nfrom rlgraph.components.common.staging_area import StagingArea\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest, recursive_assert_almost_equal\n\n\nclass TestStagingArea(unittest.TestCase):\n\n    def test_staging_area(self):\n        input_spaces = Tuple(\n            FloatBox(shape=(3, 2)),\n            Dict(a=FloatBox(shape=(1,)), b=bool),\n            bool,\n            IntBox(shape=(2,))\n        )\n        staging_area = StagingArea(num_data=len(input_spaces))\n        test = ComponentTest(component=staging_area, input_spaces=dict(inputs=[i for i in input_spaces]),\n                             auto_build=False)\n\n        inputs = input_spaces.sample()\n\n        # Build manually, then do one step_fn for the initial staging.\n        test.build()\n        stage_op = test.graph_builder.api[""stage""][1][0].op  # first (0) output (1) of stage API.\n        test.graph_executor.monitored_session.run_step_fn(lambda step_context: step_context.session.run(\n            stage_op, feed_dict={\n                test.graph_builder.api[""stage""][0][0].op: inputs[0],\n                test.graph_builder.api[""stage""][0][1].op: inputs[1],\n                test.graph_builder.api[""stage""][0][2].op: inputs[2],\n                test.graph_builder.api[""stage""][0][3].op: inputs[3],\n            }\n        ))\n\n        # Unstage the inputs.\n        test.test(""unstage"", expected_outputs=inputs)\n'"
rlgraph/tests/components/test_string_layers.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\n\nfrom rlgraph.components.layers.strings import *\nfrom rlgraph.spaces import IntBox, TextBox\nfrom rlgraph.tests import ComponentTest\n\n\nclass TestStringLayers(unittest.TestCase):\n    """"""\n    Tests for the different StringLayer Components. Each layer is tested separately.\n    """"""\n    def test_embedding_lookup_layer(self):\n        # Input space for lookup indices (double indices for picking 2 rows per batch item).\n        input_space = IntBox(shape=(2,), add_batch_rank=True)\n\n        embedding = EmbeddingLookup(embed_dim=5, vocab_size=4, initializer_spec=np.array([\n            [1.0, 2.0, 3.0, 4.0, 5.0],\n            [6.0, 7.0, 8.0, 9.0, 10.0],\n            [11.0, 12.0, 13.0, 14.0, 15.0],\n            [16.0, 17.0, 18.0, 19.0, 20.0]\n        ]))\n        test = ComponentTest(component=embedding, input_spaces=dict(ids=input_space))\n\n        # Pull a batch of 3 (2 vocabs each) from the embedding matrix.\n        inputs = np.array(\n            [[0, 1], [3, 2], [2, 1]]\n        )\n\n        expected = np.array([\n            [\n                [1.0, 2.0, 3.0, 4.0, 5.0],\n                [6.0, 7.0, 8.0, 9.0, 10.0]\n            ], [\n                [16.0, 17.0, 18.0, 19.0, 20.0],\n                [11.0, 12.0, 13.0, 14.0, 15.0]\n            ], [\n                [11.0, 12.0, 13.0, 14.0, 15.0],\n                [6.0, 7.0, 8.0, 9.0, 10.0],\n            ]\n        ])\n        test.test((""call"", inputs), expected_outputs=expected, decimals=5)\n\n    def test_string_to_hash_bucket_layer(self):\n        # Input space: Batch of strings.\n        input_space = TextBox(add_batch_rank=True)\n\n        # Use a fast-hash function with 10 possible buckets to put a word into.\n        string_to_hash_bucket = StringToHashBucket(num_hash_buckets=10, hash_function=""fast"")\n        test = ComponentTest(component=string_to_hash_bucket, input_spaces=dict(text_inputs=input_space))\n\n        # Send a batch of 3 strings through the hash-bucket generator.\n        inputs = np.array([\n            ""text A"",\n            ""test B"",\n            ""text C  D and E""\n        ])\n\n        # NOTE that some different words occupy the same hash bucket (e.g. \'C\' and \'and\' (7) OR \'text\' and [empty] (3)).\n        # This can be avoided by 1) picking a larger `num_hash_buckets` or 2) using the ""strong"" hash function.\n        expected_hash_bucket = np.array([\n            [3, 4, 3, 3, 3],  # text A .  .  .\n            [6, 8, 3, 3, 3],  # test B .  .  .\n            [3, 7, 5, 7, 2],  # text C D and E\n        ])\n        expected_lengths = np.array([2, 2, 5])\n        test.test((""call"", inputs), expected_outputs=(expected_hash_bucket, expected_lengths))\n\n    def test_string_to_hash_bucket_layer_with_different_ctor_params(self):\n        # Input space: Batch of strings.\n        input_space = TextBox(add_batch_rank=True)\n\n        # Construct a strong hash bucket with different delimiter, larger number of buckets, string algo and\n        # int16 dtype.\n        string_to_hash_bucket = StringToHashBucket(delimiter=""-"", num_hash_buckets=20, hash_function=""strong"",\n                                                   dtype=""int16"")\n        test = ComponentTest(component=string_to_hash_bucket, input_spaces=dict(text_inputs=input_space))\n\n        # Send a batch of 5 strings through the hash-bucket generator.\n        inputs = np.array([\n            ""text-A"",\n            ""test-B"",\n            ""text-C--D-and-E"",\n            ""bla bla-D""\n        ])\n\n        # NOTE that some different words occupy the same hash bucket (e.g. \'C\' and \'and\' OR \'text\' and [empty]).\n        # This can be avoided by 1) picking a larger `num_hash_buckets` or 2) using the ""strong"" hash function.\n        expected_hash_bucket = np.array([\n            [2, 6, 18, 18, 18],    # text    A .  .  .\n            [12, 7, 18, 18, 18],   # test    B .  .  .\n            [2, 6, 13, 19, 15],    # text    C D and E\n            [13, 13, 18, 18, 18],  # bla bla D .  .  .  <- Note that ""bla bla"" and ""D"" still have the same bucket (13)\n        ])\n        expected_lengths = np.array([2, 2, 5, 2])\n        test.test((""call"", inputs), expected_outputs=(expected_hash_bucket, expected_lengths))\n'"
rlgraph/tests/components/test_supervised_loss_functions.py,0,"b'# Copyright 2018/2019 ducandu GmbH. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\nimport scipy.stats as sts\n\nfrom rlgraph.components.loss_functions.categorical_cross_entropy_loss import CategoricalCrossEntropyLoss\nfrom rlgraph.components.loss_functions.container_loss_function import ContainerLossFunction\nfrom rlgraph.components.loss_functions.euclidian_distance_loss import EuclidianDistanceLoss\nfrom rlgraph.components.loss_functions.neg_log_likelihood_loss import NegativeLogLikelihoodLoss\nfrom rlgraph.spaces import *\nfrom rlgraph.spaces.space_utils import get_default_distribution_from_space\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.utils.numpy import softmax\n\n\nclass TestSupervisedLossFunctions(unittest.TestCase):\n\n    def test_categorical_cross_entropy_loss_wo_time_rank(self):\n        #time_steps = 3\n        labels_space = IntBox(2, shape=(), add_batch_rank=True)  #, add_time_rank=time_steps)\n        parameters_space = labels_space.as_one_hot_float_space()\n        loss_per_item_space = FloatBox(shape=(), add_batch_rank=True)\n        #sequence_length_space = IntBox(low=1, high=time_steps+1, shape=(), add_batch_rank=True)\n\n        categorical_x_entropy_loss_function = CategoricalCrossEntropyLoss()\n\n        test = ComponentTest(\n            component=categorical_x_entropy_loss_function,\n            input_spaces=dict(\n                labels=labels_space,\n                loss_per_item=loss_per_item_space,\n                #sequence_length=sequence_length_space,\n                parameters=parameters_space\n            )\n        )\n\n        batch_size = 4\n        parameters = parameters_space.sample(batch_size)  #, time_steps)))\n        probs = softmax(parameters)\n        positive_probs = probs[:, 1]  # parameters[:, :, 1]\n        labels = labels_space.sample(batch_size)  #, time_steps))\n\n        # Calculate binary x-entropy manually here: \xe2\x88\x92[ylog(p) + (1-y)log(1-p)]\n        # iff label (y) is 0: \xe2\x88\x92log(1\xe2\x88\x92[predicted prob for 1])\n        # iff label (y) is 1: \xe2\x88\x92log([predicted prob for 1])\n        cross_entropy = np.where(labels == 0, -np.log(1.0 - positive_probs), -np.log(positive_probs))\n\n        #sequence_length = sequence_length_space.sample(batch_size)\n\n        # This code here must be adapted to the exact time-rank reduction schema set within the loss function\n        # in case there is a time-rank. For now, test w/o time rank.\n        #ces = []\n        #for batch_item, sl in enumerate(sequence_length):\n        #    weight = 0.5\n        #    ce_sum = 0.0\n        #    for ce in cross_entropy[batch_item][:sl]:\n        #        ce_sum += ce * weight\n        #        weight += 0.5 / sequence_length[batch_item]\n        #    ces.append(ce_sum / sl)\n\n        expected_loss_per_item = cross_entropy  # np.asarray(ces)\n        expected_loss = np.mean(expected_loss_per_item, axis=0, keepdims=False)\n\n        test.test((""loss_per_item"", [parameters, labels]),  #, sequence_length]),\n                  expected_outputs=expected_loss_per_item, decimals=4)\n        test.test((""loss_average"", expected_loss_per_item), expected_outputs=expected_loss, decimals=4)\n        # Both.\n        test.test((""loss"", [parameters, labels]),  #, sequence_length]),\n                  expected_outputs=[expected_loss, expected_loss_per_item], decimals=4)\n\n    def test_euclidian_distance_loss_function_wo_time_rank(self):\n        input_space = FloatBox(shape=(5, 4, 3), add_batch_rank=True)\n        loss_per_item_space = FloatBox(shape=(), add_batch_rank=True)\n\n        euclidian_distance_loss_function = EuclidianDistanceLoss()\n\n        test = ComponentTest(\n            component=euclidian_distance_loss_function,\n            input_spaces=dict(\n                parameters=input_space,\n                labels=input_space,\n                loss_per_item=loss_per_item_space\n            )\n        )\n\n        parameters = input_space.sample(10)\n        labels = input_space.sample(10)\n\n        expected_loss_per_item = np.sqrt(np.sum(np.square(parameters - labels), axis=(-1, -2, -3),\n                                                keepdims=False))\n        expected_loss = np.mean(expected_loss_per_item, axis=0, keepdims=False)\n\n        test.test((""loss_per_item"", [parameters, labels]), expected_outputs=expected_loss_per_item, decimals=4)\n        test.test((""loss_average"", expected_loss_per_item), expected_outputs=expected_loss, decimals=4)\n        # Both.\n        test.test((""loss"", [parameters, labels]), expected_outputs=[expected_loss, expected_loss_per_item], decimals=4)\n\n    def test_neg_log_likelihood_loss_function_w_simple_space(self):\n        shape = (5, 4, 3)\n        parameters_space = Tuple(FloatBox(shape=shape), FloatBox(shape=shape), add_batch_rank=True)\n        labels_space = FloatBox(shape=shape, add_batch_rank=True)\n        loss_per_item_space = FloatBox(add_batch_rank=True)\n\n        loss_function = NegativeLogLikelihoodLoss(distribution_spec=get_default_distribution_from_space(labels_space))\n\n        test = ComponentTest(\n            component=loss_function,\n            input_spaces=dict(\n                parameters=parameters_space,\n                labels=labels_space,\n                loss_per_item=loss_per_item_space\n            )\n        )\n\n        parameters = parameters_space.sample(10)\n        # Make sure stddev params are not too crazy (just like our adapters do clipping for the raw NN output).\n        parameters = (parameters[0], np.clip(parameters[1], 0.1, 1.0))\n        labels = labels_space.sample(10)\n\n        expected_loss_per_item = np.sum(-np.log(sts.norm.pdf(labels, parameters[0], parameters[1])), axis=(-1, -2, -3))\n        expected_loss = np.mean(expected_loss_per_item, axis=0, keepdims=False)\n\n        test.test((""loss_per_item"", [parameters, labels]), expected_outputs=expected_loss_per_item, decimals=4)\n        test.test((""loss_average"", expected_loss_per_item), expected_outputs=expected_loss, decimals=4)\n        # Both.\n        test.test((""loss"", [parameters, labels]), expected_outputs=[expected_loss, expected_loss_per_item], decimals=4)\n\n    def test_neg_log_likelihood_loss_function_w_container_space(self):\n        parameters_space = Dict({\n            # Make sure stddev params are not too crazy (just like our adapters do clipping for the raw NN output).\n            ""a"": Tuple(FloatBox(shape=(2, 3)), FloatBox(0.5, 1.0, shape=(2, 3))),  # normal (0.0 to 1.0)\n            ""b"": FloatBox(shape=(4,), low=-1.0, high=1.0)  # 4-discrete\n        }, add_batch_rank=True)\n        labels_space = Dict({\n            ""a"": FloatBox(shape=(2, 3)),\n            ""b"": IntBox(4)\n        }, add_batch_rank=True)\n        loss_per_item_space = FloatBox(add_batch_rank=True)\n\n        loss_function = NegativeLogLikelihoodLoss(distribution_spec=get_default_distribution_from_space(labels_space))\n\n        test = ComponentTest(\n            component=loss_function,\n            input_spaces=dict(\n                parameters=parameters_space,\n                labels=labels_space,\n                loss_per_item=loss_per_item_space\n            )\n        )\n\n        parameters = parameters_space.sample(2)\n        # Softmax the discrete params.\n        probs_b = softmax(parameters[""b""])\n        #probs_b = parameters[""b""]\n        labels = labels_space.sample(2)\n\n        # Expected loss: Sum of all -log(llh)\n        log_prob_per_item_a = np.sum(np.log(sts.norm.pdf(labels[""a""], parameters[""a""][0], parameters[""a""][1])), axis=(-1, -2))\n        log_prob_per_item_b = np.array([np.log(probs_b[0][labels[""b""][0]]), np.log(probs_b[1][labels[""b""][1]])])\n\n        expected_loss_per_item = - (log_prob_per_item_a + log_prob_per_item_b)\n        expected_loss = np.mean(expected_loss_per_item, axis=0, keepdims=False)\n\n        test.test((""loss_per_item"", [parameters, labels]), expected_outputs=expected_loss_per_item, decimals=4)\n        test.test((""loss_average"", expected_loss_per_item), expected_outputs=expected_loss, decimals=4)\n        # Both.\n        test.test((""loss"", [parameters, labels]), expected_outputs=[expected_loss, expected_loss_per_item], decimals=4)\n\n    def test_container_loss_function(self):\n        # Regular float layer output.\n        input_space_euclidian = FloatBox(shape=(5, 3))\n        # Tuple output (parameters) for a Normal distribution.\n        input_space_neg_log_llh = Tuple(FloatBox(shape=(5, 3)), FloatBox(shape=(5, 3)))\n        # The predictions are a=output, b=parameters.\n        input_space_parameters = Dict({""a"": input_space_euclidian, ""b"": input_space_neg_log_llh}, add_batch_rank=True)\n\n        input_space_labels = Dict(\n            {\n                ""a"": input_space_euclidian,\n                ""b"": input_space_neg_log_llh[0]\n            }, add_batch_rank=True\n        )\n        loss_per_item_space = FloatBox(shape=(), add_batch_rank=True)\n\n        container_loss_function = ContainerLossFunction(\n            loss_functions_spec=dict(\n                a=dict(type=""euclidian-distance-loss""),\n                b=dict(type=""neg-log-likelihood-loss"", distribution_spec=dict(type=""normal-distribution""))\n            ), weights=dict(a=0.2, b=0.4))\n\n        test = ComponentTest(\n            component=container_loss_function,\n            input_spaces=dict(\n                parameters=input_space_parameters,\n                labels=input_space_labels,\n                loss_per_item=loss_per_item_space\n            )\n        )\n\n        predictions = input_space_parameters.sample(3)\n        labels = input_space_labels.sample(3)\n\n        expected_euclidian = 0.2 * np.sqrt(\n            np.sum(np.square(predictions[""a""] - labels[""a""]), axis=(-1, -2), keepdims=False)\n        )\n        expected_neg_log_llh = 0.4 * np.sum(\n            (- np.log(sts.norm.pdf(labels[""b""], predictions[""b""][0], predictions[""b""][1]))),\n            axis=(-1, -2), keepdims=False\n        )\n\n        expected_loss_per_item = expected_euclidian + expected_neg_log_llh\n        expected_loss = np.mean(expected_loss_per_item, axis=0, keepdims=False)\n\n        test.test((""loss_per_item"", [predictions, labels]), expected_outputs=expected_loss_per_item, decimals=4)\n        test.test((""loss_average"", expected_loss_per_item), expected_outputs=expected_loss, decimals=4)\n        # Both.\n        test.test((""loss"", [predictions, labels]), expected_outputs=[expected_loss, expected_loss_per_item], decimals=4)\n\n'"
rlgraph/tests/components/test_synchronizable.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\nfrom rlgraph.components.common.synchronizable import Synchronizable\nfrom rlgraph.components.component import Component\nfrom rlgraph.spaces import FloatBox\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.utils.decorators import rlgraph_api\n\nVARIABLE_NAMES = [""variable_to_sync1"", ""variable_to_sync2""]\n\n\nclass MyCompWithVars(Component):\n    """"""\n    The Component with variables to test. Synchronizable can be added later as a drop-in via add_component.\n    """"""\n    def __init__(self, initializer1=0.0, initializer2=1.0, synchronizable=False, **kwargs):\n        super(MyCompWithVars, self).__init__(**kwargs)\n        self.space = FloatBox(shape=(4, 5))\n        self.initializer1 = initializer1\n        self.initializer2 = initializer2\n        self.dummy_var_1 = None\n        self.dummy_var_2 = None\n\n        if synchronizable is True:\n            self.add_components(Synchronizable(), expose_apis=""sync"")\n\n    def create_variables(self, input_spaces, action_space=None):\n        # create some dummy var to sync from/to.\n        self.dummy_var_1 = self.get_variable(name=VARIABLE_NAMES[0], from_space=self.space,\n                                             initializer=self.initializer1, trainable=True)\n        self.dummy_var_2 = self.get_variable(name=VARIABLE_NAMES[1], from_space=self.space,\n                                             initializer=self.initializer2, trainable=True)\n\n\nclass TestSynchronizableComponent(unittest.TestCase):\n\n    def test_variables_api_method(self):\n        # Proof that all Components can push out their variable values.\n        component_to_test = MyCompWithVars(synchronizable=False)\n        test = ComponentTest(component=component_to_test)\n\n        # Test pulling the variable values from the sync_out socket.\n        expected1 = np.zeros(shape=component_to_test.space.shape)\n        expected2 = np.ones(shape=component_to_test.space.shape)\n        expected = dict({""variable-to-sync1"": expected1, ""variable-to-sync2"": expected2})\n\n        test.test(""variables"", expected_outputs=expected)\n\n    def test_sync_functionality(self):\n        # Two Components, one with Synchronizable dropped in:\n        # A: Can only push out values.\n        # B: To be synced by A\'s values.\n        sync_from = MyCompWithVars(scope=""sync-from"")\n        sync_to = MyCompWithVars(initializer1=8.0, initializer2=7.0, scope=""sync-to"", synchronizable=True)\n\n        # Create a dummy test component that contains our two Synchronizables.\n        container = Component(name=""container"")\n        container.add_components(sync_from, sync_to)\n\n        @rlgraph_api(component=container)\n        def execute_sync(self):\n            values_ = sync_from.variables()\n            return sync_to.sync(values_)\n\n        test = ComponentTest(component=container)\n\n        # Test syncing the variable from->to and check them before and after the sync.\n        # Before the sync.\n        test.variable_test(sync_to.get_variables(VARIABLE_NAMES), {\n            ""sync-to/""+VARIABLE_NAMES[0]: np.full(shape=sync_from.space.shape, fill_value=8.0),\n            ""sync-to/""+VARIABLE_NAMES[1]: np.full(shape=sync_from.space.shape, fill_value=7.0)\n        })\n\n        # Now sync and re-check.\n        test.test(""execute_sync"", expected_outputs=None)\n\n        # After the sync.\n        test.variable_test(sync_to.get_variables(VARIABLE_NAMES), {\n            ""sync-to/""+VARIABLE_NAMES[0]: np.zeros(shape=sync_from.space.shape),\n            ""sync-to/""+VARIABLE_NAMES[1]: np.ones(shape=sync_from.space.shape)\n        })\n\n    def test_sync_between_2_identical_comps_that_have_vars_only_in_their_sub_comps(self):\n        """"""\n        Similar to the Policy scenario, where the Policy Component owns a NeuralNetwork (which has vars)\n        and has to be synced with other Policies.\n        """"""\n        # Create 2x: A custom Component (with vars) that holds another Component (with vars).\n        # Then sync between them.\n        comp1 = MyCompWithVars(scope=""A"")\n        comp1.add_components(MyCompWithVars(scope=""sub-of-A-with-vars""))\n\n        comp2_writable = MyCompWithVars(scope=""B"", initializer1=3.0, initializer2=4.2, synchronizable=True)\n        comp2_writable.add_components(MyCompWithVars(scope=""sub-of-B-with-vars"", initializer1=5.0, initializer2=6.2))\n\n        container = Component(comp1, comp2_writable, scope=""container"")\n\n        @rlgraph_api(component=container)\n        def execute_sync(self):\n            values_ = comp1.variables()\n            return comp2_writable.sync(values_)\n\n        test = ComponentTest(component=container)\n\n        # Before the sync.\n        test.variable_test(comp2_writable.get_variables([\n            ""container/B/variable_to_sync1"",\n            ""container/B/variable_to_sync2"",\n            ""container/B/sub-of-B-with-vars/variable_to_sync1"",\n            ""container/B/sub-of-B-with-vars/variable_to_sync2""\n        ]), {\n            ""container/B/variable_to_sync1"": np.full(shape=comp1.space.shape, fill_value=3.0, dtype=np.float32),\n            ""container/B/variable_to_sync2"": np.full(shape=comp1.space.shape, fill_value=4.2, dtype=np.float32),\n            ""container/B/sub-of-B-with-vars/variable_to_sync1"": np.full(shape=comp1.space.shape, fill_value=5.0,\n                                                                        dtype=np.float32),\n            ""container/B/sub-of-B-with-vars/variable_to_sync2"": np.full(shape=comp1.space.shape, fill_value=6.2,\n                                                                        dtype=np.float32)\n        })\n\n        # Now sync and re-check.\n        test.test((""execute_sync"", None), expected_outputs=None)\n\n        # After the sync.\n        test.variable_test(comp2_writable.get_variables([\n            ""container/B/variable_to_sync1"",\n            ""container/B/variable_to_sync2"",\n            ""container/B/sub-of-B-with-vars/variable_to_sync1"",\n            ""container/B/sub-of-B-with-vars/variable_to_sync2""\n        ]), {\n            ""container/B/variable_to_sync1"": np.zeros(shape=comp1.space.shape, dtype=np.float32),\n            ""container/B/variable_to_sync2"": np.ones(shape=comp1.space.shape, dtype=np.float32),\n            ""container/B/sub-of-B-with-vars/variable_to_sync1"": np.zeros(shape=comp1.space.shape, dtype=np.float32),\n            ""container/B/sub-of-B-with-vars/variable_to_sync2"": np.ones(shape=comp1.space.shape, dtype=np.float32)\n        })\n\n\n\n\n'"
rlgraph/tests/components/test_time_dependent_parameters.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport unittest\n\nimport numpy as np\nfrom rlgraph.components.common.time_dependent_parameters import TimeDependentParameter\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\n\n\nclass TestParameters(unittest.TestCase):\n    """"""\n    Tests time-step dependent TimeDependentParameter Component classes.\n    """"""\n\n    input_space_pct = dict(time_percentage=FloatBox(add_batch_rank=True))\n\n    def test_constant_parameter(self):\n        constant = TimeDependentParameter.from_spec(2.0)\n        test = ComponentTest(component=constant, input_spaces=self.input_space_pct)\n\n        input_ = np.array([0.5, 0.1, 1.0, 0.9, 0.02, 0.01, 0.99, 0.23])\n\n        test.test((""get"", input_), expected_outputs=[\n            2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0\n        ])\n\n    def test_linear_parameter(self):\n        linear_parameter = TimeDependentParameter.from_spec((2.0, 0.5))\n        test = ComponentTest(component=linear_parameter, input_spaces=self.input_space_pct)\n\n        input_ = np.array([0.5, 0.1, 1.0, 0.9, 0.02, 0.01, 0.99, 0.23])\n\n        test.test((""get"", input_), expected_outputs=2.0 - input_ * (2.0 - 0.5))\n\n    def test_linear_parameter_using_global_timestep(self):\n        linear_parameter = TimeDependentParameter.from_spec(""linear-decay"", from_=2.0, to_=0.5, max_time_steps=100)\n        test = ComponentTest(component=linear_parameter, input_spaces=None)\n\n        # Call without any parameters -> force component to use GLOBAL_STEP, which should be 0 right now -> no decay.\n        for _ in range(10):\n            test.test(""get"", expected_outputs=2.0)\n\n    def test_polynomial_parameter(self):\n        polynomial_parameter = TimeDependentParameter.from_spec(type=""polynomial-decay"", from_=2.0, to_=0.5, power=2.0)\n        test = ComponentTest(component=polynomial_parameter, input_spaces=self.input_space_pct)\n\n        input_ = np.array([0.5, 0.1, 1.0, 0.9, 0.02, 0.01, 0.99, 0.23])\n\n        test.test((""get"", input_), expected_outputs=(2.0 - 0.5) * (1.0 - input_) ** 2 + 0.5)\n\n    def test_polynomial_parameter_using_global_timestep(self):\n        polynomial_parameter = TimeDependentParameter.from_spec(""polynomial-decay"", from_=3.0, to_=0.5, max_time_steps=100)\n        test = ComponentTest(component=polynomial_parameter, input_spaces=None)\n\n        # Call without any parameters -> force component to use GLOBAL_STEP, which should be 0 right now -> no decay.\n        for _ in range(10):\n            test.test(""get"", expected_outputs=3.0)\n\n    def test_exponential_parameter(self):\n        exponential_parameter = TimeDependentParameter.from_spec(type=""exponential-decay"", from_=2.0, to_=0.5, decay_rate=0.5)\n        test = ComponentTest(component=exponential_parameter, input_spaces=self.input_space_pct)\n\n        input_ = np.array([0.5, 0.1, 1.0, 0.9, 0.02, 0.01, 0.99, 0.23])\n\n        test.test((""get"", input_), expected_outputs=0.5 + (2.0 - 0.5) * 0.5 ** input_, decimals=5)\n\n    def test_exponential_parameter_using_global_timestep(self):\n        exponential_parameter = TimeDependentParameter.from_spec(""exponential-decay"", from_=3.0, to_=0.5, max_time_steps=100)\n        test = ComponentTest(component=exponential_parameter, input_spaces=None)\n\n        # Call without any parameters -> force component to use GLOBAL_STEP, which should be 0 right now -> no decay.\n        for _ in range(10):\n            test.test(""get"", expected_outputs=3.0)\n'"
rlgraph/tests/components/test_v_trace_function.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport unittest\n\nfrom rlgraph.components.helpers.v_trace_function import VTraceFunction\nfrom rlgraph.spaces import *\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.utils.numpy import one_hot, softmax\n\n\nclass TestVTraceFunctions(unittest.TestCase):\n\n    time_x_batch_x_2_space = FloatBox(shape=(2,), add_batch_rank=True, add_time_rank=True, time_major=True)\n    time_x_batch_x_9_space = FloatBox(shape=(9,), add_batch_rank=True, add_time_rank=True, time_major=True)\n    time_x_batch_x_1_space = FloatBox(shape=(1,), add_batch_rank=True, add_time_rank=True, time_major=True)\n\n    def test_v_trace_function(self):\n        v_trace_function = VTraceFunction()\n        v_trace_function_reference = VTraceFunction(backend=""python"")\n\n        action_space = IntBox(2, add_batch_rank=True, add_time_rank=True, time_major=True)\n        action_space_flat = FloatBox(shape=(2,), add_batch_rank=True, add_time_rank=True, time_major=True)\n        input_spaces = dict(\n            logits_actions_pi=self.time_x_batch_x_2_space,\n            log_probs_actions_mu=self.time_x_batch_x_2_space,\n            actions=action_space,\n            actions_flat=action_space_flat,\n            discounts=self.time_x_batch_x_1_space,\n            rewards=self.time_x_batch_x_1_space,\n            values=self.time_x_batch_x_1_space,\n            bootstrapped_values=self.time_x_batch_x_1_space\n        )\n\n        test = ComponentTest(component=v_trace_function, input_spaces=input_spaces)\n\n        size = (3, 2)\n        logits_actions_pi = self.time_x_batch_x_2_space.sample(size=size)\n        logits_actions_mu = self.time_x_batch_x_2_space.sample(size=size)\n        log_probs_actions_mu = np.log(softmax(logits_actions_mu))\n        actions = action_space.sample(size=size)\n        actions_flat = one_hot(actions, depth=action_space.num_categories)\n        # Set some discounts to 0.0 (these will mark the end of episodes, where the value is 0.0).\n        discounts = np.random.choice([0.0, 0.99], size=size + (1,), p=[0.2, 0.8])\n        rewards = self.time_x_batch_x_1_space.sample(size=size)\n        values = self.time_x_batch_x_1_space.sample(size=size)\n        bootstrapped_values = self.time_x_batch_x_1_space.sample(size=(1, size[1]))\n\n        input_ = [\n            logits_actions_pi, log_probs_actions_mu, actions, actions_flat, discounts, rewards, values,\n            bootstrapped_values\n        ]\n\n        vs_expected, pg_advantages_expected = v_trace_function_reference._graph_fn_calc_v_trace_values(*input_)\n\n        test.test((""calc_v_trace_values"", input_), expected_outputs=[vs_expected, pg_advantages_expected], decimals=4)\n\n    def test_v_trace_function_more_complex(self):\n        v_trace_function = VTraceFunction()\n        v_trace_function_reference = VTraceFunction(backend=""python"")\n\n        action_space = IntBox(9, add_batch_rank=True, add_time_rank=True, time_major=True)\n        action_space_flat = FloatBox(shape=(9,), add_batch_rank=True, add_time_rank=True, time_major=True)\n        input_spaces = dict(\n            logits_actions_pi=self.time_x_batch_x_9_space,\n            log_probs_actions_mu=self.time_x_batch_x_9_space,\n            actions=action_space,\n            actions_flat=action_space_flat,\n            discounts=self.time_x_batch_x_1_space,\n            rewards=self.time_x_batch_x_1_space,\n            values=self.time_x_batch_x_1_space,\n            bootstrapped_values=self.time_x_batch_x_1_space\n        )\n\n        test = ComponentTest(component=v_trace_function, input_spaces=input_spaces)\n\n        size = (100, 16)\n        logits_actions_pi = self.time_x_batch_x_9_space.sample(size=size)\n        logits_actions_mu = self.time_x_batch_x_9_space.sample(size=size)\n        log_probs_actions_mu = np.log(softmax(logits_actions_mu))\n        actions = action_space.sample(size=size)\n        actions_flat = one_hot(actions, depth=action_space.num_categories)\n        # Set some discounts to 0.0 (these will mark the end of episodes, where the value is 0.0).\n        discounts = np.random.choice([0.0, 0.99], size=size + (1,), p=[0.1, 0.9])\n        rewards = self.time_x_batch_x_1_space.sample(size=size)\n        values = self.time_x_batch_x_1_space.sample(size=size)\n        bootstrapped_values = self.time_x_batch_x_1_space.sample(size=(1, size[1]))\n\n        input_ = [\n            logits_actions_pi, log_probs_actions_mu, actions, actions_flat, discounts, rewards, values,\n            bootstrapped_values\n        ]\n\n        vs_expected, pg_advantages_expected = v_trace_function_reference._graph_fn_calc_v_trace_values(*input_)\n\n        test.test((""calc_v_trace_values"", input_), expected_outputs=[vs_expected, pg_advantages_expected], decimals=4)\n\n'"
rlgraph/tests/components/test_variational_auto_encoders.py,0,"b'# Copyright 2018/2019 ducandu GmbH. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport unittest\n\nimport numpy as np\nfrom rlgraph.components.neural_networks.variational_auto_encoder import VariationalAutoEncoder\nfrom rlgraph.spaces import FloatBox\nfrom rlgraph.tests.component_test import ComponentTest\nfrom rlgraph.tests.test_util import config_from_path, recursive_assert_almost_equal\nfrom rlgraph.utils.numpy import dense_layer\n\n\nclass TestVariationalAutoEncoders(unittest.TestCase):\n    """"""\n    Tests for the VariationalAutoEncoder class.\n    """"""\n    def test_simple_variational_auto_encoder(self):\n        # Space must contain batch dimension (otherwise, NNlayer will complain).\n        input_spaces = dict(\n            input_=FloatBox(shape=(3,), add_batch_rank=True), z_vector=FloatBox(shape=(1,), add_batch_rank=True)\n        )\n\n        variational_auto_encoder = VariationalAutoEncoder(\n            z_units=1,\n            encoder_network_spec=config_from_path(""configs/test_vae_encoder_network.json""),\n            decoder_network_spec=config_from_path(""configs/test_vae_decoder_network.json"")\n        )\n\n        # Do not seed, we calculate expectations manually.\n        test = ComponentTest(component=variational_auto_encoder, input_spaces=input_spaces)\n\n        # Batch of size=3.\n        input_ = np.array([[0.1, 0.2, 0.3], [1.0, 2.0, 3.0], [10.0, 20.0, 30.0]])\n        global_scope = ""variational-auto-encoder/""\n        # Calculate output manually.\n        var_dict = test.read_variable_values(variational_auto_encoder.variable_registry)\n\n        encoder_network_out = dense_layer(\n            input_, var_dict[global_scope+""encoder-network/encoder-layer/dense/kernel""],\n            var_dict[global_scope+""encoder-network/encoder-layer/dense/bias""]\n        )\n        expected_mean = dense_layer(\n            encoder_network_out, var_dict[global_scope+""mean-layer/dense/kernel""],\n            var_dict[global_scope+""mean-layer/dense/bias""]\n        )\n        expected_stddev = dense_layer(\n            encoder_network_out, var_dict[global_scope + ""stddev-layer/dense/kernel""],\n            var_dict[global_scope + ""stddev-layer/dense/bias""]\n        )\n        out = test.test((""encode"", input_), expected_outputs=None)\n        recursive_assert_almost_equal(out[""mean""], expected_mean, decimals=5)\n        recursive_assert_almost_equal(out[""stddev""], np.exp(expected_stddev), decimals=5)\n        self.assertTrue(out[""z_sample""].shape == (3, 1))\n\n        test.terminate()\n\n'"
rlgraph/tests/core/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n'"
rlgraph/tests/core/test_api_methods.py,6,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport logging\nimport unittest\n\nimport mock\nfrom tensorflow.core.framework import summary_pb2\n\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.tests.test_util import regex_pattern\nfrom rlgraph.utils import root_logger\nfrom rlgraph.tests.dummy_components import *\nfrom rlgraph.tests.dummy_components_with_sub_components import *\n\n\nclass TestAPIMethods(unittest.TestCase):\n    """"""\n    Tests for different ways to place and then connect two or more sub-Components into the root-Component.\n    Tests different ways of defining these connections using API-methods.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    def test_component_with_sub_component(self):\n        a = DummyWithSubComponents(scope=""A"")\n        test = ComponentTest(component=a, input_spaces=dict(input_=float))\n\n        # Expected: (1): in + 2.0  (2): [result of (1)] + 1.0\n        test.test((""run1"", 1.1), expected_outputs=[3.1, 4.1], decimals=4)\n        # Expected: in - 2.0 + 1.0\n        test.test((""run2"", 1.1), expected_outputs=0.1, decimals=4)\n\n    def test_connecting_two_1to1_components(self):\n        """"""\n        Adds two components with 1-to-1 graph_fns to the core, connects them and passes a value through it.\n        """"""\n        core = Component(scope=""container"")\n        sub_comp1 = Dummy1To1(scope=""comp1"")\n        sub_comp2 = Dummy1To1(scope=""comp2"")\n        core.add_components(sub_comp1, sub_comp2)\n\n        @rlgraph_api(component=core)\n        def run(self_, input_):\n            out = sub_comp1.run(input_)\n            return sub_comp2.run(out)\n\n        test = ComponentTest(component=core, input_spaces=dict(input_=float))\n\n        # Expected output: input + 1.0 + 1.0\n        test.test((""run"", 1.1), expected_outputs=3.1)\n        test.test((""run"", -5.1), expected_outputs=-3.1)\n\n    def test_connecting_1to2_to_2to1(self):\n        """"""\n        Adds two components with 1-to-2 and 2-to-1 graph_fns to the core, connects them and passes a value through it.\n        """"""\n        core = Component(scope=""container"")\n        sub_comp1 = Dummy1To2(scope=""comp1"")  # outs=in,in+1\n        sub_comp2 = Dummy2To1(scope=""comp2"")  # out =in1+in2\n        core.add_components(sub_comp1, sub_comp2)\n\n        @rlgraph_api(component=core)\n        def run(self_, input_):\n            out1, out2 = sub_comp1.run(input_)\n            return sub_comp2.run(out1, out2)\n\n        test = ComponentTest(component=core, input_spaces=dict(input_=float))\n\n        # Expected output: input + (input + 1.0)\n        test.test((""run"", 100.9), expected_outputs=np.float32(202.8))\n        test.test((""run"", -5.1), expected_outputs=np.float32(-9.2))\n\n    def test_1to1_to_2to1_component_with_constant_input_value(self):\n        """"""\n        Adds two components in sequence, 1-to-1 and 2-to-1, to the core and blocks one of the api_methods of 2-to-1\n        with a constant value (so that this constant value is not at the border of the root-component).\n        """"""\n        core = Component(scope=""container"")\n        sub_comp1 = Dummy1To1(scope=""A"")\n        sub_comp2 = Dummy2To1(scope=""B"")\n        core.add_components(sub_comp1, sub_comp2)\n\n        @rlgraph_api(component=core)\n        def run(self_, input_):\n            out = sub_comp1.run(input_)\n            return sub_comp2.run(out, 1.1)\n\n        test = ComponentTest(component=core, input_spaces=dict(input_=float))\n\n        # Expected output: (input + 1.0) + 1.1\n        test.test((""run"", 78.4), expected_outputs=80.5)\n        test.test((""run"", -5.2), expected_outputs=-3.1)\n\n    def test_diamond_4x_sub_component_setup(self):\n        """"""\n        Adds 4 sub-components (A, B, C, D) with 1-to-1 graph_fns to the core.\n        in1 -> A (like preprocessor in DQN)\n        in2 -> A\n        A -> B (like policy in DQN)\n        A -> C (like target policy in DQN)\n        B -> Din1 (like loss func in DQN: q_vals_s)\n        C -> Din2 (q_vals_sp)\n        """"""\n        container = Component(scope=""container"")\n        a = Dummy1To1(scope=""A"")\n        b = Dummy1To1(scope=""B"")\n        c = Dummy1To1(scope=""C"")\n        d = Dummy2To1(scope=""D"")\n\n        # Throw in the sub-components.\n        container.add_components(a, b, c, d)\n\n        # Define container\'s API:\n        @rlgraph_api(name=""run"", component=container)\n        def container_run(self_, input1, input2):\n            """"""\n            Describes the diamond setup in1->A->B; in2->A->C; C,B->D->output\n            """"""\n            # Adds constant value 1.0  to 1.1 -> 2.1\n            in1_past_a = self_.sub_components[""A""].run(input1)\n            # 0.5 + 1.0 = 1.5\n            in2_past_a = self_.sub_components[""A""].run(input2)\n            # 2.1 + 1.0 = 3.1\n            past_b = self_.sub_components[""B""].run(in1_past_a)\n            # 1.5 + 1.0 = 2.5\n            past_c = self_.sub_components[""C""].run(in2_past_a)\n            # 3.1 + 2.5 = 5.6\n            past_d = self_.sub_components[""D""].run(past_b, past_c)\n            return past_d\n\n        test = ComponentTest(component=container, input_spaces=dict(input1=float, input2=float))\n\n        # Push both api_methods through graph to receive correct (single-op) output calculation.\n        test.test((""run"", [1.1, 0.5]), expected_outputs=5.6)\n\n    def test_calling_sub_components_api_from_within_graph_fn(self):\n        a = DummyCallingSubComponentsAPIFromWithinGraphFn(scope=""A"")\n        test = ComponentTest(component=a, input_spaces=dict(input_=float))\n\n        # Expected: (1): 2*in + 10\n        test.test((""run"", 1.1), expected_outputs=12.2, decimals=4)\n\n    def test_component_that_defines_custom_api_methods(self):\n        a = DummyThatDefinesCustomAPIMethod()\n\n        test = ComponentTest(component=a, input_spaces=dict(input_=float))\n\n        test.test((""some_custom_api_method"", 1.23456), expected_outputs=1.23456, decimals=5)\n\n    def test_call_in_comprehension(self):\n        """"""\n        Tests calling graph functions within list comprehensions.\n\n        https://github.com/rlgraph/rlgraph/issues/41\n        """"""\n        container = Component(scope=""container"")\n        sub_comps = [Dummy1To1(scope=""dummy-{}"".format(i)) for i in range(3)]\n        container.add_components(*sub_comps)\n\n        # Define container\'s API:\n        @rlgraph_api(name=""test"", component=container)\n        def container_test(self_, input_):\n            # results = []\n            # for i in range(len(sub_comps)):\n            #     results.append(sub_comps[i].run(input_))\n            results = [x.run(input_) for x in sub_comps]\n            return self_._graph_fn_sum(*results)\n\n        @graph_fn(component=container)\n        def _graph_fn_sum(self_, *inputs):\n            return sum(inputs)\n\n        test = ComponentTest(component=container, input_spaces=dict(input_=float))\n        test.test((""test"", 1.23), expected_outputs=len(sub_comps) * (1.23 + 1), decimals=2)\n\n    def test_providing_method_as_argument(self):\n        component = DummyWithVar()\n        test = ComponentTest(component=component, input_spaces=dict(input_=float))\n\n        test.test((component.run_plus, 1.23456), expected_outputs=3.23456, decimals=5)\n        test.test((component.run_minus, 1.23456), expected_outputs=-0.7654, decimals=4)\n\n    @staticmethod\n    def _parse_summary_if_needed(summary):\n        """"""\n        Parses the summary if it is provided in serialized form (bytes).\n        This code is copied from tensorflow\'s SummaryToEventTransformer::add_summary\n        :param summary:\n        :return:\n        """"""\n        if isinstance(summary, bytes):\n            summ = summary_pb2.Summary()\n            summ.ParseFromString(summary)\n            summary = summ\n        return summary\n\n    def test_summaries(self):\n        container = Component(scope=""container"")\n\n        # Define container\'s API:\n        @rlgraph_api(component=container)\n        def add(self_, value, value2):\n            return self_._graph_fn_sum(value, value2)\n\n        @graph_fn(component=container)\n        def _graph_fn_sum(self_, *inputs):\n            summary_op = tf.summary.histogram(""summary_sum"", inputs)\n            self_.register_summary_op(summary_op)\n            return sum(inputs)\n\n        @rlgraph_api(component=container)\n        def _graph_fn_graph_api(self_):\n            summary_op = tf.summary.scalar(""summary_graph_api"", 1.0)\n            self_.register_summary_op(summary_op)\n            return tf.constant(1.0)\n\n        @rlgraph_api(component=container)\n        def api_method_double(self_, value):\n            return self_.add(value, value)\n\n        @rlgraph_api(component=container)\n        def _graph_fn_api_method_complex(self_, value):\n            doubled = self_._graph_fn_sum(value, value)\n            incremented = self_._graph_fn_inc(doubled)\n            denominator = self_.graph_api()\n            return incremented / denominator\n\n        @rlgraph_api(component=container)\n        def increment(self_, value):\n            return self_._graph_fn_inc(value)\n\n        test = ComponentTest(component=container, input_spaces=dict(\n            value=float,\n            value2=float\n        ), auto_build=False)\n\n        @graph_fn(component=container)\n        def _graph_fn_inc(self_, value):\n            summary_op = tf.summary.scalar(""summary_inc"", value)\n            self_.register_summary_op(summary_op)\n            assign_op = tf.assign_add(test.graph_executor.global_training_timestep, 1)\n            with tf.control_dependencies([assign_op]):\n                return value + 1\n\n        test.build()\n\n        test.graph_executor.summary_writer = mock.Mock(\n            spec=test.graph_executor.summary_writer,\n            wraps=test.graph_executor.summary_writer\n        )\n        test.graph_executor.summary_writer.add_summary = mock.Mock()\n\n        test.test((add, [1.0, 2.0]), expected_outputs=3.0, decimals=2)\n        assert test.graph_executor.summary_writer.add_summary.call_count == 1\n        summary, step = test.graph_executor.summary_writer.add_summary.call_args[0]\n        summary = self._parse_summary_if_needed(summary)\n        assert len(summary.value) == 1\n        assert summary.value[0].tag == regex_pattern(container.scope + ""/summary_sum"" + r""(_\\d)?"")\n        assert step == 0\n\n        test.graph_executor.summary_writer.add_summary.reset_mock()\n        test.test((increment, [6.0]), expected_outputs=7.0, decimals=2)\n        assert test.graph_executor.summary_writer.add_summary.call_count == 1\n        summary, step = test.graph_executor.summary_writer.add_summary.call_args[0]\n        summary = self._parse_summary_if_needed(summary)\n        assert len(summary.value) == 1\n        assert summary.value[0].tag == container.scope + ""/summary_inc""\n        assert step == 1\n\n        test.graph_executor.summary_writer.add_summary.reset_mock()\n        test.test(""graph_api"", expected_outputs=1.0, decimals=2)\n        assert test.graph_executor.summary_writer.add_summary.call_count == 1\n        summary, step = test.graph_executor.summary_writer.add_summary.call_args[0]\n        summary = self._parse_summary_if_needed(summary)\n        assert len(summary.value) == 1\n        assert summary.value[0].tag == container.scope + ""/summary_graph_api""\n        assert step == 1\n\n        test.graph_executor.summary_writer.add_summary.reset_mock()\n        test.test((api_method_double, [3.0]), expected_outputs=6.0, decimals=2)\n        assert test.graph_executor.summary_writer.add_summary.call_count == 1\n        summary, step = test.graph_executor.summary_writer.add_summary.call_args[0]\n        summary = self._parse_summary_if_needed(summary)\n        assert len(summary.value) == 1\n        assert summary.value[0].tag == regex_pattern(container.scope + ""/summary_sum"" + r""(_\\d)?"")\n        assert step == 1\n\n        test.graph_executor.summary_writer.add_summary.reset_mock()\n        test.test((""api_method_complex"", [3.0]), expected_outputs=7.0, decimals=2)\n        assert test.graph_executor.summary_writer.add_summary.call_count == 1\n        summary, step = test.graph_executor.summary_writer.add_summary.call_args[0]\n        summary = self._parse_summary_if_needed(summary)\n        assert len(summary.value) == 3\n        assert summary.value[0].tag == regex_pattern(container.scope + ""/summary_sum"" + r""(_\\d)?"")\n        assert summary.value[1].tag == regex_pattern(container.scope + ""/summary_inc"" + r""(_\\d)?"")\n        assert summary.value[2].tag == regex_pattern(container.scope + ""/summary_graph_api"" + r""(_\\d)?"")\n        assert step == 2\n\n\n    #def test_kwargs_in_api_call(self):\n    #    core = Component(scope=""container"")\n    #    sub_comp = Dummy2To2(scope=""comp1"")\n    #    core.add_components(sub_comp)\n\n    #    @api(component=core)\n    #    def run(self_, input1=1.0, input2=2.0):\n    #        return sub_comp.run(input1, input2)\n\n    #    test = ComponentTest(component=core, input_spaces=dict(input1=float, input2=float))\n\n    #    # Expected output: input + 1.0 + 1.0\n    #    test.test((""run"", [1.1, None]), expected_outputs=(2.1, 4.1))\n    #    #test.test((""run"", -5.1), expected_outputs=-3.1)\n\n'"
rlgraph/tests/core/test_device_placements.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport unittest\n\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.utils import root_logger\nfrom rlgraph.tests.dummy_components import *\nfrom rlgraph.tests.dummy_components_with_sub_components import *\n\n\nclass TestDevicePlacements(unittest.TestCase):\n    """"""\n    Tests different ways to place Components and their ops/variables on different devices.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    def test_single_component(self):\n        """"""\n        Place the entire Component on its own device.\n        """"""\n        a = Dummy1To1(scope=""A"", device=""/device:CPU:0"")\n        test = ComponentTest(component=a, input_spaces=dict(input_=float))\n        # Actually check the device of the ops in a.\n        self.assertEqual(a.api_methods[""run""].in_op_columns[0].op_records[0].op.device, ""/device:CPU:0"")\n        self.assertEqual(a.api_methods[""run""].out_op_columns[0].op_records[0].op.device, ""/device:CPU:0"")\n        # Expected: in + 1.0\n        test.test((""run"", 1.1), expected_outputs=2.1)\n\n    def test_single_component_with_variables(self):\n        """"""\n        Place variables on CPU, ops on GPU (if exists).\n        """"""\n        var_device = ""/device:CPU:0""\n        op_device = ""/device:GPU:0""\n        a = DummyWithVar(scope=""A"", device=dict(variables=var_device, ops=op_device))\n        test = ComponentTest(component=a, input_spaces=dict(input_=float))\n\n        # Vars -> CPU.\n        self.assertEqual(a.variable_registry[""A/constant-variable""].device, var_device)\n        # Placeholders -> GPU.\n        self.assertEqual(a.api_methods[""run_plus""].in_op_columns[0].op_records[0].op.device, op_device)\n        self.assertEqual(a.api_methods[""run_minus""].in_op_columns[0].op_records[0].op.device, op_device)\n        # Actual ops -> GPU.\n        self.assertEqual(a.api_methods[""run_plus""].out_op_columns[0].op_records[0].op.device, op_device)\n        self.assertEqual(a.api_methods[""run_minus""].out_op_columns[0].op_records[0].op.device, op_device)\n\n        # Expected: in + 2.0\n        test.test((""run_plus"", 1.1), expected_outputs=3.1)\n\n    def test_sub_components_with_device_map(self):\n        """"""\n        Place variables on CPU, ops on GPU (if exists).\n        """"""\n        a = DummyWithSubComponents(scope=""A"")\n        comp_device = ""/device:GPU:0""\n        sub_comp_device = ""/device:CPU:0""\n        test = ComponentTest(component=a, input_spaces=dict(input_=float),\n                             device_strategy=""custom"",\n                             device_map=dict({""A/dummy-with-var"": sub_comp_device, ""A"": comp_device}))\n        # Actually check the device of the variables and ops in a.\n        actual_comp_device = ""/device:GPU:0"" if ""/device:GPU:0"" in test.graph_builder.available_devices else \\\n            ""/device:CPU:0""\n        self.assertEqual(a.api_methods[""run1""].in_op_columns[0].op_records[0].op.device, actual_comp_device)\n        self.assertEqual(a.api_methods[""run1""].out_op_columns[0].op_records[0].op.device, sub_comp_device)\n        self.assertEqual(a.api_methods[""run1""].out_op_columns[0].op_records[1].op.device, actual_comp_device)\n        self.assertEqual(a.api_methods[""run2""].in_op_columns[0].op_records[0].op.device, actual_comp_device)\n        self.assertEqual(a.api_methods[""run2""].out_op_columns[0].op_records[0].op.device, actual_comp_device)\n\n        test.test((""run1"", 1.1), expected_outputs=[3.1, 4.1])\n        test.test((""run2"", -1.1), expected_outputs=-2.1)\n\n'"
rlgraph/tests/core/test_graph_fns.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport unittest\n\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.utils import root_logger\nimport rlgraph.spaces as spaces\nfrom rlgraph.tests.dummy_components import *\n\n\nclass TestGraphFns(unittest.TestCase):\n    """"""\n    Tests for different ways to send DataOps through GraphFunctions.\n    Tests flattening, splitting, etc.. operations.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    def test_2_containers_flattening_splitting(self):\n        """"""\n        Adds a single component with 2-to-2 graph_fn to the core and passes two containers through it\n        with flatten/split options enabled.\n        """"""\n        input1_space = spaces.Dict(a=float, b=spaces.FloatBox(shape=(1, 2)))\n        input2_space = spaces.Dict(a=float, b=float)\n\n        component = FlattenSplitDummy()\n        test = ComponentTest(\n            component=component,\n            input_spaces=dict(input1=input1_space, input2=input2_space)\n        )\n\n        # Options: fsu=flat/split/un-flat.\n        in1_fsu = dict(a=np.array(0.234), b=np.array([[0.0, 3.0]]))\n        in2_fsu = dict(a=np.array(5.0), b=np.array(5.5))\n        # Result of sending \'a\' keys through graph_fn: (in1[a]+1.0=1.234, in1[a]+in2[a]=5.234)\n        # Result of sending \'b\' keys through graph_fn: (in1[b]+1.0=[[1, 4]], in1[b]+in2[b]=[[5.5, 8.5]])\n        out1_fsu = dict(a=1.234, b=np.array([[1.0, 4.0]]))\n        out2_fsu = dict(a=np.array(5.234, dtype=np.float32), b=np.array([[5.5, 8.5]]))\n        test.test((""run"", [in1_fsu, in2_fsu]), expected_outputs=[out1_fsu, out2_fsu])\n\n    def test_1_containers_1_float_flattening_splitting(self):\n        """"""\n        Adds a single component with 2-to-2 graph_fn to the core and passes one container and one float through it\n        with flatten/split options all disabled.\n        """"""\n        input1_space = spaces.Dict(a=float, b=spaces.FloatBox(shape=(1, 2)))\n        input2_space = spaces.FloatBox(shape=(1,1))\n\n        component = FlattenSplitDummy()\n        test = ComponentTest(component=component, input_spaces=dict(input1=input1_space, input2=input2_space))\n\n        # Options: fsu=flat/split/un-flat.\n        in1_fsu = dict(a=np.array(0.234), b=np.array([[0.0, 3.0]]))\n        in2_fsu = np.array([[2.0]])\n        # Result of sending \'a\' keys through graph_fn: (in1[a]+1.0=1.234, in1[a]+in2=2.234)\n        # Result of sending \'b\' keys through graph_fn: (in1[b]+1.0=[[1, 4]], in1[b]+in2=[[2.0, 5.0]])\n        out1_fsu = dict(a=1.234, b=np.array([[1.0, 4.0]]))\n        out2_fsu = dict(a=np.array([[2.234]], dtype=np.float32), b=np.array([[2.0, 5.0]]))\n        test.test((""run"", [in1_fsu, in2_fsu]), expected_outputs=[out1_fsu, out2_fsu])\n\n    def test_2_containers_no_options(self):\n        """"""\n        Adds a single component with 2-to-2 graph_fn to the core and passes one container and one float through it\n        with no flatten/split options enabled.\n        """"""\n        input1_space = spaces.Dict(a=int, b=bool)\n        input2_space = spaces.Dict(c=bool, d=int)\n\n        component = NoFlattenNoSplitDummy()\n        test = ComponentTest(component=component, input_spaces=dict(input1=input1_space, input2=input2_space))\n\n        # Options: fsu=flat/split.\n        in1 = dict(a=5, b=True)\n        in2 = dict(c=False, d=3)\n        # Expect reversal (see graph_fn)\n        out1 = in2\n        out2 = in1\n        test.test((""run"", [in1, in2]), expected_outputs=[out1, out2])\n\n    def test_1_container_1_float_only_flatten(self):\n        """"""\n        Adds a single component with 2-to-3 graph_fn to the core and passes one container and one float through it\n        with only the flatten option enabled.\n        """"""\n        input1_space = spaces.Dict(a=float, b=float, c=spaces.Tuple(float))\n        input2_space = spaces.FloatBox(shape=(1,))\n\n        component = OnlyFlattenDummy(constant_value=5.0)\n        test = ComponentTest(component=component, input_spaces=dict(input1=input1_space, input2=input2_space))\n\n        # Options: only flatten_ops=True.\n        in1 = dict(a=5.4, b=3.4, c=tuple([3.2]))\n        in2 = np.array([1.2])\n        # out1: dict(in1_f key: in1_f value + in2_f[""""])\n        # out2: in2_f\n        # out3: self.constant_value\n        out1 = dict(a=in1[""a""] + in2, b=in1[""b""] + in2, c=tuple([in1[""c""][0] + in2]))\n        out2 = dict(a=in1[""a""] - in2, b=in1[""b""] - in2, c=tuple([in1[""c""][0] - in2]))\n        out3 = in2\n        test.test((""run"", [in1, in2]), expected_outputs=[out1, out2, out3], decimals=5)\n\n    def test_calling_graph_fn_from_inside_another_graph_fn(self):\n        """"""\n        One graph_fn gets called from within another. Must return actual ops from inner one so that the outer one\n        can handle it.\n        """"""\n        input_space = spaces.FloatBox(shape=(2,))\n        component = Dummy2NestedGraphFnCalls()\n        test = ComponentTest(component=component, input_spaces=dict(\n            input_=input_space\n        ))\n\n        input_ = input_space.sample()\n        expected = input_ - 1.0\n        test.test((""run"", input_), expected_outputs=expected, decimals=5)\n\n    def test_component_that_defines_custom_graph_fns(self):\n        a = DummyThatDefinesCustomGraphFn()\n\n        test = ComponentTest(component=a, input_spaces=dict(input_=float))\n\n        test.test((""run"", 3.4567), expected_outputs=3.4567, decimals=3)\n\n    def test_calling_graph_fn_with_default_args_in_middle(self):\n        a = Dummy3To1WithDefaultValues()\n        test = ComponentTest(component=a, input_spaces=dict(input1=float))\n        # Will put default float into input2.\n        test.test((""run"", 1.0), expected_outputs=2.0, decimals=3)\n\n        b = Dummy3To1WithDefaultValues()\n        test = ComponentTest(component=b, input_spaces=dict(input1=int, input3=int))\n        test.test((""run"", [5, 6]), expected_outputs=6, decimals=0)\n\n        c = Dummy3To1WithDefaultValues()\n        test = ComponentTest(component=c, input_spaces=dict(input1=float, input4=float))  # TODO: if we leave out input4, should create a default-placeholder with default value = 1.0 (see api-method)\n        test.test((""run2"", [1.0, 2.0]), expected_outputs=3.0, decimals=3)\n'"
rlgraph/tests/core/test_input_incomplete_build.py,0,"b'# Copyright 2018/2019 The Rlgraph Authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport unittest\n\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.utils import root_logger, RLGraphError, RLGraphBuildError\nfrom rlgraph.tests.dummy_components_with_sub_components import *\n\n\nclass TestInputIncompleteTest(unittest.TestCase):\n    """"""\n    Tests for different scenarios, where a model is build, but pieces remain input-incomplete (these should be\n    reported then by meaningful error messages).\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    def test_inner_deadlock_of_component(self):\n        """"""\n        Component cannot be built due to its sub-component remaining input incomplete.\n        """"""\n        a = DummyProducingInputIncompleteBuild(scope=""A"")\n        try:\n            test = ComponentTest(component=a, input_spaces=dict(input_=float))\n        except RLGraphBuildError as e:\n            print(""Seeing expected RLGraphBuildError ({}). Test ok."".format(e))\n        else:\n            raise RLGraphError(""Not seeing expected RLGraphBuildError with input-incomplete model!"")\n\n    def test_solution_of_inner_deadlock_of_component_with_must_be_complete_false(self):\n        """"""\n        Component can be built due to its sub-component resolving a deadlock with `must_be_complete`.\n        """"""\n        a = DummyProducingInputIncompleteBuild(scope=""A"")\n        deadlock_component = a.sub_components[""dummy-calling-one-api-from-within-other""]\n        # Manually set the must_be_complete flag to false.\n        deadlock_component.api_methods[""run_inner""].must_be_complete = False\n        test = ComponentTest(component=a, input_spaces=dict(input_=float))\n        print(""Not seeing RLGraphBuildError. Test ok."")\n'"
rlgraph/tests/core/test_input_space_checking.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport logging\nimport unittest\n\nfrom rlgraph.components.layers.nn.dense_layer import DenseLayer\nfrom rlgraph.components.layers.strings.embedding_lookup import EmbeddingLookup\nimport rlgraph.spaces as spaces\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.tests.dummy_components import *\nfrom rlgraph.utils import root_logger\nfrom rlgraph.utils.visualization_util import draw_meta_graph\n\n\nclass TestInputSpaceChecking(unittest.TestCase):\n    """"""\n    Tests whether faulty ops are caught after calling `sanity_check_space` in `check_input_spaces` of a Component.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    def test_faulty_op_catching(self):\n        """"""\n        Adds a single component with 2-to-2 graph_fn to the core and passes two containers through it\n        with flatten/split options enabled.\n        """"""\n        # Construct some easy component containing a sub-component.\n        dense_layer = DenseLayer(units=2, scope=""dense-layer"")\n        string_layer = EmbeddingLookup(embed_dim=3, vocab_size=4, scope=""embed-layer"")\n        container_component = Component(dense_layer, string_layer)\n\n        # Add the component\'s API method.\n        @rlgraph_api(component=container_component)\n        def test_api(self, a):\n            dense_result = self.get_sub_component_by_name(""dense-layer"").call(a)\n            # First call dense to get a vector output, then call embedding, which is expecting an int input.\n            # This should fail EmbeddingLookup\'s input space checking (only during the build phase).\n            return self.get_sub_component_by_name(""embed-layer"").call(dense_result)\n\n        # Test graphviz component graph drawing.\n        draw_meta_graph(container_component, apis=True)\n\n        test = ComponentTest(\n            component=container_component,\n            input_spaces=dict(a=spaces.FloatBox(shape=(4,), add_batch_rank=True))\n        )\n'"
rlgraph/tests/core/test_pytorch_backend.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport time\nimport unittest\n\nfrom rlgraph.agents import DQNAgent, ApexAgent\nfrom rlgraph.components import Policy, MemPrioritizedReplay\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.spaces import FloatBox, IntBox, Dict, BoolBox\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.tests.dummy_components import *\nfrom rlgraph.tests.dummy_components_with_sub_components import *\nfrom rlgraph.tests.test_util import config_from_path\nfrom rlgraph.utils import root_logger, softmax\nfrom rlgraph.utils.define_by_run_ops import print_call_chain\n\n\nclass TestPytorchBackend(unittest.TestCase):\n    """"""\n    Tests PyTorch component execution.\n\n    # TODO: This is a temporary test. We will later run all backend-specific\n    tests via setting the executor in the component-test.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    def test_api_call_no_variables(self):\n        """"""\n        Tests define-by-run call of api method via defined_api method on a\n        component without variables.\n        """"""\n        a = Dummy2To1()\n        test = ComponentTest(component=a, input_spaces=dict(input1=float, input2=float))\n        test.test((""run"", [1.0, 2.0]), expected_outputs=3.0, decimals=4)\n\n    def test_connecting_1to2_to_2to1(self):\n        """"""\n        Adds two components with 1-to-2 and 2-to-1 graph_fns to the core, connects them and passes a value through it.\n        """"""\n        core = Component(scope=""container"")\n        sub_comp1 = Dummy1To2(scope=""comp1"")  # outs=in,in+1\n        sub_comp2 = Dummy2To1(scope=""comp2"")  # out =in1+in2\n        core.add_components(sub_comp1, sub_comp2)\n\n        @rlgraph_api(component=core)\n        def run(self_, input_):\n            out1, out2 = sub_comp1.run(input_)\n            return sub_comp2.run(out1, out2)\n\n        test = ComponentTest(component=core, input_spaces=dict(input_=float))\n\n        # Expected output: input + (input + 1.0)\n        test.test((""run"", 100.9), expected_outputs=np.array(202.8, dtype=np.float32))\n        test.test((""run"", -5.1), expected_outputs=np.array(-9.2, dtype=np.float32))\n\n    def test_calling_sub_components_api_from_within_graph_fn(self):\n        a = DummyCallingSubComponentsAPIFromWithinGraphFn(scope=""A"")\n        test = ComponentTest(component=a, input_spaces=dict(input_=float))\n\n        # Expected: (1): 2*in + 10\n        test.test((""run"", 1.1), expected_outputs=12.2, decimals=4)\n\n    def test_1to1_to_2to1_component_with_constant_input_value(self):\n        """"""\n        Adds two components in sequence, 1-to-1 and 2-to-1, to the core and blocks one of the api_methods of 2-to-1\n        with a constant value (so that this constant value is not at the border of the root-component).\n        """"""\n        core = Component(scope=""container"")\n        sub_comp1 = Dummy1To1(scope=""A"")\n        sub_comp2 = Dummy2To1(scope=""B"")\n        core.add_components(sub_comp1, sub_comp2)\n\n        @rlgraph_api(component=core)\n        def run(self_, input_):\n            out = sub_comp1.run(input_)\n            return sub_comp2.run(out, 1.1)\n\n        test = ComponentTest(component=core, input_spaces=dict(input_=float))\n\n        # Expected output: (input + 1.0) + 1.1\n        test.test((""run"", 78.4), expected_outputs=80.5)\n        test.test((""run"", -5.2), expected_outputs=-3.1)\n\n    def test_dqn_compilation(self):\n        """"""\n        Creates a DQNAgent and runs it via a Runner on an openAI Pong Env.\n        """"""\n        env = OpenAIGymEnv(""Pong-v0"", frameskip=4, max_num_noops=30, episodic_life=True)\n        agent_config = config_from_path(""configs/dqn_pytorch_test.json"")\n        agent = DQNAgent.from_spec(\n            # Uses 2015 DQN parameters as closely as possible.\n            agent_config,\n            state_space=env.state_space,\n            # Try with ""reduced"" action space (actually only 3 actions, up, down, no-op)\n            action_space=env.action_space\n        )\n\n    def test_memory_compilation(self):\n        # Builds a memory and returns build stats.\n        env = OpenAIGymEnv(""Pong-v0"", frameskip=4, max_num_noops=30, episodic_life=True)\n\n        record_space = Dict(\n            states=env.state_space,\n            actions=env.action_space,\n            rewards=float,\n            terminals=BoolBox(),\n            add_batch_rank=True\n        )\n        input_spaces = dict(\n            # insert: records\n            records=record_space,\n            # get_records: num_records\n            num_records=int,\n            # update_records: indices, update\n            indices=IntBox(add_batch_rank=True),\n            update=FloatBox(add_batch_rank=True)\n        )\n\n        input_spaces.pop(""num_records"")\n        memory = MemPrioritizedReplay(\n            capacity=20000,\n        )\n        test = ComponentTest(component=memory, input_spaces=input_spaces, auto_build=False)\n        return test.build()\n\n    # TODO -> batch dim works differently in pytorch -> have to squeeze.\n    def test_dense_layer(self):\n        # Space must contain batch dimension (otherwise, NNLayer will complain).\n        space = FloatBox(shape=(2,), add_batch_rank=True)\n\n        # - fixed 1.0 weights, no biases\n        dense_layer = DenseLayer(units=2, weights_spec=1.0, biases_spec=False)\n        test = ComponentTest(component=dense_layer, input_spaces=dict(inputs=space))\n\n        # Batch of size=1 (can increase this to any larger number).\n        input_ = np.array([0.5, 2.0])\n        expected = np.array([2.5, 2.5])\n        test.test((""call"", input_), expected_outputs=expected)\n\n    def test_nn_assembly_from_file(self):\n        # Space must contain batch dimension (otherwise, NNlayer will complain).\n        space = FloatBox(shape=(3,), add_batch_rank=True)\n\n        # Create a simple neural net from json.\n        neural_net = NeuralNetwork.from_spec(config_from_path(""configs/test_simple_nn.json""))  # type: NeuralNetwork\n\n        # Do not seed, we calculate expectations manually.\n        test = ComponentTest(component=neural_net, input_spaces=dict(inputs=space), seed=None)\n\n        # Batch of size=3.\n        input_ = np.array([[0.1, 0.2, 0.3], [1.0, 2.0, 3.0], [10.0, 20.0, 30.0]])\n\n        # Cant fetch variables here.\n\n        out = test.test((""call"", input_), decimals=5)\n        print(out)\n\n    def test_policy_for_discrete_action_space(self):\n        # state_space (NN is a simple single fc-layer relu network (2 units), random biases, random weights).\n        state_space = FloatBox(shape=(4,), add_batch_rank=True)\n\n        # action_space (5 possible actions).\n        action_space = IntBox(5, add_batch_rank=True)\n\n        policy = Policy(network_spec=config_from_path(""configs/test_simple_nn.json""), action_space=action_space)\n        test = ComponentTest(\n            component=policy,\n            input_spaces=dict(nn_input=state_space),\n            action_space=action_space\n        )\n        policy_params = test.read_variable_values(policy.variable_registry)\n\n        # Some NN inputs (4 input nodes, batch size=2).\n        states = np.array([[-0.08, 0.4, -0.05, -0.55], [13.0, -14.0, 10.0, -16.0]])\n        # Raw NN-output.\n        expected_nn_output = np.matmul(states, policy_params[""policy/test-network/hidden-layer/dense/kernel""])\n        test.test((""get_nn_output"", states), expected_outputs=expected_nn_output, decimals=6)\n\n        # Raw action layer output; Expected shape=(2,5): 2=batch, 5=action categories\n        expected_action_layer_output = np.matmul(\n            expected_nn_output, policy_params[""policy/action-adapter/action-layer/dense/kernel""]\n        )\n        expected_action_layer_output = np.reshape(expected_action_layer_output, newshape=(2, 5))\n        test.test(\n            (""get_adapter_outputs"", states, [""adapter_outputs""]),\n            expected_outputs=dict(adapter_outputs=expected_action_layer_output),\n            decimals=5\n        )\n\n        expected_actions = np.argmax(expected_action_layer_output, axis=-1)\n        test.test((""get_action"", states, [""action""]), expected_outputs=dict(action=expected_actions))\n\n        # Logits, parameters (probs) and skip log-probs (numerically unstable for small probs).\n        expected_probabilities_output = softmax(expected_action_layer_output, axis=-1)\n        test.test((""get_adapter_outputs_and_parameters"", states, [0, 1, 2]), expected_outputs=dict(\n            adapter_outputs=expected_action_layer_output,\n            parameters=expected_probabilities_output,\n            log_probs=np.log(expected_probabilities_output)\n        ), decimals=5)\n\n        print(""Probs: {}"".format(expected_probabilities_output))\n\n        # Deterministic sample.\n        out = test.test((""get_deterministic_action"", states), expected_outputs=None)\n        self.assertTrue(out[""action""].dtype == np.int32)\n        self.assertTrue(out[""action""].shape == (2,))\n\n        # Stochastic sample.\n        out = test.test((""get_stochastic_action"", states), expected_outputs=None)\n        self.assertTrue(out[""action""].dtype == np.int32)\n        self.assertTrue(out[""action""].shape == (2,))\n\n        # Distribution\'s entropy.\n        out = test.test((""get_entropy"", states), expected_outputs=None)\n        self.assertTrue(out[""entropy""].dtype == np.float32)\n        self.assertTrue(out[""entropy""].shape == (2,))\n\n    def test_act(self):\n        env = OpenAIGymEnv(""Pong-v0"", frameskip=4, max_num_noops=30, episodic_life=True)\n        agent_config = config_from_path(""configs/ray_apex_for_pong.json"")\n        if get_backend() == ""pytorch"":\n            agent_config[""memory_spec""][""type""] = ""mem_prioritized_replay""\n        agent = DQNAgent.from_spec(\n            # Uses 2015 DQN parameters as closely as possible.\n            agent_config,\n            state_space=env.state_space,\n            # Try with ""reduced"" action space (actually only 3 actions, up, down, no-op)\n            action_space=env.action_space\n        )\n        state = env.reset()\n        action = agent.get_action(state)\n        print(""Component call count = {}"".format(Component.call_count))\n\n        state_space = env.state_space\n        count = 200\n\n        samples = state_space.sample(count)\n        start = time.perf_counter()\n        for s in samples:\n            action = agent.get_action(s)\n        end = time.perf_counter() - start\n\n        print(""Took {} s for {} separate actions, mean = {}"".format(end, count, end / count))\n\n        # Now instead test 100 batch actions\n        samples = state_space.sample(count)\n        start = time.perf_counter()\n        action = agent.get_action(samples)\n        end = time.perf_counter() - start\n        print(""Took {} s for {} batched actions."".format(end, count))\n        profile = Component.call_times\n        print_call_chain(profile, False, 0.03)\n\n    def test_post_processing(self):\n        env = OpenAIGymEnv(""Pong-v0"", frameskip=4, max_num_noops=30, episodic_life=True)\n        agent_config = config_from_path(""configs/ray_apex_for_pong.json"")\n\n        # Test cpu settings for batching here.\n        agent_config[""memory_spec""][""type""] = ""mem_prioritized_replay""\n        agent_config[""execution_spec""][""torch_num_threads""] = 1\n        agent_config[""execution_spec""][""OMP_NUM_THREADS""] = 1\n\n        agent = ApexAgent.from_spec(\n            # Uses 2015 DQN parameters as closely as possible.\n            agent_config,\n            state_space=env.state_space,\n            # Try with ""reduced"" action space (actually only 3 actions, up, down, no-op)\n            action_space=env.action_space\n        )\n        samples = 200\n        rewards = np.random.random(size=samples)\n        states = list(agent.preprocessed_state_space.sample(samples))\n        actions = agent.action_space.sample(samples)\n        terminals = np.zeros(samples, dtype=np.uint8)\n        next_states = states[1:]\n        next_states.extend([agent.preprocessed_state_space.sample(1)])\n        next_states = np.asarray(next_states)\n        states = np.asarray(states)\n        weights = np.ones_like(rewards)\n\n        for _ in range(1):\n            start = time.perf_counter()\n            _, loss_per_item = agent.post_process(\n                dict(\n                    states=states,\n                    actions=actions,\n                    rewards=rewards,\n                    terminals=terminals,\n                    next_states=next_states,\n                    importance_weights=weights\n                )\n            )\n            print(""post process time = {}"".format(time.perf_counter() - start))\n        profile = Component.call_times\n        print_call_chain(profile, False, 0.003)\n'"
rlgraph/tests/core/test_pytorch_util.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport unittest\n\nfrom rlgraph import get_backend\nfrom rlgraph.tests import recursive_assert_almost_equal\nfrom rlgraph.utils import root_logger, pytorch_one_hot\n\nif get_backend() == ""pytorch"":\n    import torch\n\n\nclass TestPyTorchUtil(unittest.TestCase):\n    """"""\n    Tests some torch utils.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    def test_one_hot(self):\n        """"""\n        Tests a torch one hot function.\n        """"""\n        if get_backend() == ""pytorch"":\n            # Flat action array.\n            inputs = torch.tensor([0, 1], dtype=torch.int32)\n            one_hot = pytorch_one_hot(inputs, depth=2)\n\n            expected = torch.tensor([[1., 0.], [0., 1.]])\n            recursive_assert_almost_equal(one_hot, expected)\n\n            # Container space.\n            inputs = torch.tensor([[0, 3, 2],[1, 2, 0]], dtype=torch.int32)\n            one_hot = pytorch_one_hot(inputs, depth=4)\n\n            expected = torch.tensor([[[1, 0, 0, 0],[0, 0, 0, 1],[0, 0, 1, 0]],[[0, 1, 0, 0],[0, 0, 1, 0],[1, 0, 0, 0,]]],\n                                    dtype=torch.int32)\n            recursive_assert_almost_equal(one_hot, expected)\n'"
rlgraph/tests/core/test_single_components.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport unittest\n\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.utils import root_logger\nfrom rlgraph.tests.dummy_components import *\n\n\nclass TestSingleComponents(unittest.TestCase):\n    """"""\n    Tests for different ways to place different, but single sub-Components into the core.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    def test_single_component_with_single_api_method(self):\n        """"""\n        \'A\' is 1to1: send ""input"" through A, receive output.\n        """"""\n        a = Dummy1To1(scope=""A"")\n        test = ComponentTest(component=a, input_spaces=dict(input_=float))\n        # Expected: in + 1.0\n        test.test((""run"", 1.1), expected_outputs=2.1)\n\n    def test_1to1_component(self):\n        """"""\n        Adds a single component with 1-to-1 graph_fn to the core and passes a value through it.\n        """"""\n        component = Dummy1To1(scope=""dummy"")\n        test = ComponentTest(component=component, input_spaces=dict(input_=float))\n\n        # Expected output: input + 1.0\n        test.test((""run"", 1.0), expected_outputs=2.0)\n        test.test((""run"", -5.0), expected_outputs=-4.0)\n\n    def test_2to1_component(self):\n        """"""\n        Adds a single component with 2-to-1 graph_fn to the core and passes 2 values through it.\n        """"""\n        component = Dummy2To1(scope=""dummy"")\n        test = ComponentTest(component=component, input_spaces=dict(input1=float, input2=float))\n\n        # Expected output: input1 + input2\n        test.test((""run"", [1.0, 2.9]), expected_outputs=3.9)\n        test.test((""run"", [4.9, -0.1]), expected_outputs=np.array(4.8, dtype=np.float32))\n\n    def test_1to2_component(self):\n        """"""\n        Adds a single component with 1-to-2 graph_fn to the core and passes a value through it.\n        """"""\n        component = Dummy1To2(scope=""dummy"", constant_value=1.3)\n        test = ComponentTest(component=component, input_spaces=dict(input_=float))\n\n        # Expected outputs: (input, input+1.0)\n        test.test((""run"", 1.0), expected_outputs=[2.3, 1.3])\n        test.test((""run"", 4.6), expected_outputs=[5.9, 5.98], decimals=3)\n\n    def test_0to1_component(self):\n        """"""\n        Adds a single component with 0-to-1 graph_fn to the core and passes a value through it.\n        """"""\n        component = Dummy0To1(scope=""dummy"", var_value=5.0)\n        test = ComponentTest(component=component, input_spaces=None)\n\n        # Expected outputs: `var_value` passed into ctor.\n        test.test((""run"", None), expected_outputs=5.0)\n\n    def test_2to1_component_with_int_input_space(self):\n        """"""\n        Adds a single component with 1-to-1 graph_fn to the core and blocks the input with a constant value.\n        """"""\n        component = Dummy2To1(scope=""dummy"")\n        test = ComponentTest(component=component, input_spaces=dict(input1=int, input2=int))\n\n        # Expected output: in1 + in2\n        test.test((""run"", [5, 4]), expected_outputs=9)\n\n    def test_2to1_component_with_1_constant_input(self):\n        """"""\n        TODO: Same as above test case: Rather make input2 have placeholder_with_default so that we don\'t need to\n        TODO: provide a value for it necessarily.\n\n        Adds a single component with 2-to-1 graph_fn to the core, and the second input to the\n        graph_fn is already blocked by the component.\n        """"""\n        component = Dummy2To1(scope=""dummy"")\n        test = ComponentTest(component=component, input_spaces=dict(input1=int, input2=int))\n\n        # Expected output: in1 + (const 1.0)\n        test.test((""run"", [4, 5]), expected_outputs=9)\n'"
rlgraph/tests/core/test_spaces.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nfrom six.moves import xrange as range_\n\nfrom rlgraph.spaces import *\nfrom rlgraph.utils.ops import FLAT_TUPLE_CLOSE, FLAT_TUPLE_OPEN\n\n\nclass TestSpaces(unittest.TestCase):\n    """"""\n    Tests creation, sampling and shapes of Spaces.\n    """"""\n    def test_box_spaces(self):\n        """"""\n        Tests all BoxSpaces via sample/contains loop. With and without batch-rank,\n        different batch sizes, and different los/high combinations (including no bounds).\n        """"""\n        for class_ in [FloatBox, IntBox, BoolBox, TextBox]:\n            for add_batch_rank in [False, True]:\n                # TODO: Test time-rank more thoroughly.\n                for add_time_rank in [False, True]:\n                    if class_ != BoolBox and class_ != TextBox:\n                        for low, high in [(None, None), (-1.0, 10.0), ((1.0, 2.0), (3.0, 4.0)),\n                                          (((1.0, 2.0, 3.0), (4.0, 5.0, 6.0)), ((7.0, 8.0, 9.0), (10.0, 11.0, 12.0)))]:\n                            space = class_(low=low, high=high, add_batch_rank=add_batch_rank,\n                                           add_time_rank=add_time_rank)\n                            if add_batch_rank is False:\n                                sample = space.sample()\n                                self.assertTrue(space.contains(sample))\n                            else:\n                                for batch_size in range_(1, 4):\n                                    samples = space.sample(size=batch_size)\n                                    for s in samples:\n                                        self.assertTrue(space.contains(s))\n                            # TODO: test zero() method perperly for all cases\n                            #all_0s = space.zeros()\n                            #self.assertTrue(all(v == 0 for v in all_0s))\n                    else:\n                        space = class_(add_batch_rank=add_batch_rank, add_time_rank=add_time_rank)\n                        if add_batch_rank is False:\n                            sample = space.sample()\n                            self.assertTrue(space.contains(sample))\n                        else:\n                            for batch_size in range_(1, 4):\n                                samples = space.sample(size=batch_size)\n                                for s in samples:\n                                    self.assertTrue(space.contains(s))\n\n    def test_complex_space_sampling_and_check_via_contains(self):\n        """"""\n        Tests a complex Space on sampling and `contains` functionality.\n        """"""\n        space = Dict(\n            a=dict(aa=float, ab=bool),\n            b=dict(ba=float),\n            c=float,\n            d=IntBox(low=0, high=1),\n            e=IntBox(5),\n            f=FloatBox(shape=(2, 2)),\n            g=Tuple(float, FloatBox(shape=())),\n            add_batch_rank=True\n        )\n\n        samples = space.sample(size=100, horizontal=True)\n        for i in range_(len(samples)):\n            self.assertTrue(space.contains(samples[i]))\n\n    def test_container_space_flattening_with_mapping(self):\n        space = Tuple(\n            Dict(\n                a=bool,\n                b=IntBox(4),\n                c=Dict(\n                    d=FloatBox(shape=())\n                )\n            ),\n            BoolBox(),\n            IntBox(2),\n            FloatBox(shape=(3, 2)),\n            Tuple(\n                BoolBox(), BoolBox()\n            )\n        )\n\n        def mapping_func(key, primitive_space):\n            # Just map a primitive Space to its flat_dim property.\n            return primitive_space.flat_dim\n\n        result = """"\n        flat_space_and_mapped = space.flatten(mapping=mapping_func, scope_separator_at_start=False)\n        for key, value in flat_space_and_mapped.items():\n            result += ""{}:{},"".format(key, value)\n\n        tuple_txt = [FLAT_TUPLE_OPEN, FLAT_TUPLE_CLOSE] * 10\n        expected = ""{}0{}/a:1,{}0{}/b:1,{}0{}/c/d:1,{}1{}:1,{}2{}:1,{}3{}:6,{}4{}/{}0{}:1,{}4{}/{}1{}:1,"".\\\n            format(*tuple_txt)\n\n        self.assertTrue(result == expected)\n\n    def test_container_space_mapping(self):\n        space = Tuple(\n            Dict(\n                a=bool,\n                b=IntBox(4),\n                c=Dict(\n                    d=FloatBox(shape=())\n                )\n            ),\n            BoolBox(),\n            IntBox(2),\n            FloatBox(shape=(3, 2)),\n            Tuple(\n                BoolBox(), BoolBox()\n            )\n        )\n\n        def mapping_func(key, primitive_space):\n            # Change each primitive space to IntBox(5).\n            return IntBox(5)\n\n        mapped_space = space.map(mapping=mapping_func)\n\n        self.assertTrue(isinstance(mapped_space[0][""a""], IntBox))\n        self.assertTrue(mapped_space[0][""a""].num_categories == 5)\n        self.assertTrue(mapped_space[3].num_categories == 5)\n        self.assertTrue(mapped_space[4][0].num_categories == 5)\n        self.assertTrue(mapped_space[4][1].num_categories == 5)\n\n        # Same on Dict.\n        space = Dict(\n            a=bool,\n            b=IntBox(4),\n            c=Dict(\n                d=FloatBox(shape=())\n            )\n        )\n        mapped_space = space.map(mapping=mapping_func)\n\n        self.assertTrue(isinstance(mapped_space[""a""], IntBox))\n        self.assertTrue(mapped_space[""a""].num_categories == 5)\n        self.assertTrue(isinstance(mapped_space[""b""], IntBox))\n        self.assertTrue(mapped_space[""c""][""d""].num_categories == 5)\n'"
rlgraph/tests/core/test_specifiable_server.py,1,"b'# Copyright 2018/2019 The RLgraph authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nimport unittest\n\nfrom rlgraph.environments.environment import Environment\nfrom rlgraph.utils.specifiable_server import SpecifiableServer, SpecifiableServerHook\nfrom rlgraph.utils.util import convert_dtype\nfrom rlgraph.spaces import IntBox, FloatBox\n\n\nclass TestSpecifiableServer(unittest.TestCase):\n    """"""\n    Tests a SpecifiableServer with a simple environment and make some calls to it to see how it reacts.\n    """"""\n    def test_specifiable_server(self):\n        action_space = IntBox(2)\n        state_space = FloatBox()\n        env_spec = dict(type=""random_env"", state_space=state_space, action_space=action_space, deterministic=True)\n        # Create the server, but don\'t start it yet. This will be done fully automatically by the tf-Session.\n        specifiable_server = SpecifiableServer(Environment, env_spec, dict(\n            step_flow=[state_space, float, bool]\n        ), ""terminate"")\n\n        # ret are ops now in the graph.\n        ret1 = specifiable_server.step_flow(action_space.sample())\n        ret2 = specifiable_server.step_flow(action_space.sample())\n\n        # Check all 3 outputs of the Env step (next state, reward, terminal).\n        self.assertEqual(ret1[0].shape, ())\n        self.assertEqual(ret1[0].dtype, convert_dtype(""float32""))\n        self.assertEqual(ret1[1].shape, ())\n        self.assertEqual(ret1[1].dtype, convert_dtype(""float32""))\n        self.assertEqual(ret1[2].shape, ())\n        self.assertEqual(ret1[2].dtype, convert_dtype(""bool""))\n        self.assertEqual(ret2[0].shape, ())\n        self.assertEqual(ret2[0].dtype, convert_dtype(""float32""))\n        self.assertEqual(ret2[1].shape, ())\n        self.assertEqual(ret2[1].dtype, convert_dtype(""float32""))\n        self.assertEqual(ret2[2].shape, ())\n        self.assertEqual(ret2[2].dtype, convert_dtype(""bool""))\n\n        # Start the session and run the op, then check its actual values.\n        with tf.train.SingularMonitoredSession(hooks=[SpecifiableServerHook()]) as sess:\n            out1 = sess.run(ret1)\n            out2 = sess.run(ret2)\n\n        # next state\n        self.assertAlmostEqual(out1[0], 0.7713, places=4)\n        self.assertAlmostEqual(out2[0], 0.7488, places=4)\n        # reward\n        self.assertAlmostEqual(out1[1], 0.0208, places=4)\n        self.assertAlmostEqual(out2[1], 0.4985, places=4)\n        # terminal\n        self.assertTrue(out1[2] is np.bool_(False))\n        self.assertTrue(out2[2] is np.bool_(False))\n\n'"
rlgraph/tests/core/test_specifiables.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport unittest\n\nfrom rlgraph.spaces import *\nfrom rlgraph.tests.test_util import recursive_assert_almost_equal\n\n\nclass TestSpecifiables(unittest.TestCase):\n    """"""\n    Tests creation of Specifiable objects via from_spec.\n    """"""\n    def test_specifiable_on_spaces(self):\n        """"""\n        Tests complex Container Spaces for being constructable from_spec.\n        """"""\n        np.random.seed(10)\n\n        space = Dict.from_spec(\n            dict(\n                a=Tuple(FloatBox(shape=(1, 1, 2))),\n                b=float,\n                c=dict(type=float, shape=(2,))\n            ), add_batch_rank=True\n        )\n        recursive_assert_almost_equal(\n            space.sample(),\n            dict(\n                a=(np.array([[[0.77132064, 0.02075195]]]),),\n                b=0.6336482349262754,\n                c=np.array([0.74880388, 0.49850701])\n            )\n        )\n\n        space = Space.from_spec(dict(type=""tuple"", _args=[\n            Dict(\n                a=bool,\n                b=IntBox(4),\n                c=Dict(\n                    d=FloatBox(shape=())\n                )\n            ),\n            BoolBox(),\n            FloatBox(shape=(3, 2)),\n            Tuple(\n                bool, BoolBox()\n            )]\n        ))\n        recursive_assert_almost_equal(\n            space.sample(),\n            (\n                dict(\n                    a=False,\n                    b=0,\n                    c=dict(d=0.709208009843012)\n                ),\n                True,\n                np.array(\n                    [[0.16911084, 0.08833981], [0.68535982, 0.95339335], [0.00394827, 0.51219226]], dtype=np.float32\n                ),\n                (True, False)\n            )\n        )\n\n        space = Dict.from_spec(dict(\n            a=Tuple(float, FloatBox(shape=(1, 2, 2))),\n            b=FloatBox(shape=(2, 2, 2, 2)),\n            c=dict(type=float, shape=(2,))\n        ))\n        self.assertEqual(space.rank, ((0, 3), 4, 1))\n        self.assertEqual(space.shape, (((), (1, 2, 2)), (2, 2, 2, 2), (2,)))\n        self.assertEqual(space.get_shape(with_batch_rank=True), (((), (1, 2, 2)), (2, 2, 2, 2), (2,)))\n\n        space = Dict(\n            a=Tuple(int, IntBox(2), FloatBox(shape=(4, 2))),\n            b=FloatBox(shape=(2, 2)),\n            c=dict(type=float, shape=(4,)),\n            add_batch_rank=True,\n            add_time_rank=True\n        )\n        self.assertEqual(space.rank, ((0, 0, 2), 2, 1))\n        self.assertEqual(space.shape, (((), (), (4, 2)), (2, 2), (4,)))\n        self.assertEqual(space.get_shape(with_batch_rank=True), (((None,), (None,), (None, 4, 2)),\n                                                                 (None, 2, 2), (None, 4)))\n        self.assertEqual(space.get_shape(with_time_rank=True), (((None,), (None,), (None, 4, 2)),\n                                                                (None, 2, 2), (None, 4)))\n        self.assertEqual(space.get_shape(with_batch_rank=True, with_time_rank=True),\n                         (((None, None), (None, None), (None, None, 4, 2)), (None, None, 2, 2), (None, None, 4)))\n        self.assertEqual(space.get_shape(with_batch_rank=True, with_time_rank=10, time_major=True),\n                         (((10, None), (10, None), (10, None, 4, 2)), (10, None, 2, 2), (10, None, 4)))\n        self.assertEqual(space.get_shape(with_batch_rank=5, with_time_rank=10, time_major=False),\n                         (((5, 10), (5, 10), (5, 10, 4, 2)), (5, 10, 2, 2), (5, 10, 4)))\n'"
rlgraph/tests/environments/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n'"
rlgraph/tests/environments/test_deepmind_lab.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport time\nimport unittest\n\nfrom rlgraph.spaces.int_box import IntBox\n\n\nclass TestDeepmindLabEnv(unittest.TestCase):\n    """"""\n    Tests creation, resetting and stepping through a deepmind Lab Env.\n    """"""\n    def test_deepmind_lab_env(self):\n        try:\n            from rlgraph.environments import DeepmindLabEnv\n        except ImportError:\n            print(""Deepmind Lab not installed -> skipping this test case."")\n            return\n\n        frameskip = 4\n        env = DeepmindLabEnv(""seekavoid_arena_01"", observations=[""RGB_INTERLEAVED"", ""MAP_FRAME_NUMBER""],\n                             frameskip=frameskip)\n\n        # Assert action Space is IntBox(9). 9 default actions from IMPALA paper.\n        self.assertTrue(env.action_space == IntBox(9))\n\n        # Simple test runs with fixed actions.\n        s = env.reset()\n        # Assert we have pixels.\n        self.assertGreaterEqual(np.mean(s[""RGB_INTERLEAVED""]), 0)\n        self.assertLessEqual(np.mean(s[""RGB_INTERLEAVED""]), 255)\n        accum_reward = 0.0\n        frame = 0\n        for i in range(2000):\n            s, r, t, _ = env.step(env.action_space.sample())\n            assert isinstance(r, np.ndarray)\n            assert r.dtype == np.float32\n            assert isinstance(t, bool)\n            # Assert we have pixels.\n            self.assertGreaterEqual(np.mean(s[""RGB_INTERLEAVED""]), 0)\n            self.assertLessEqual(np.mean(s[""RGB_INTERLEAVED""]), 255)\n            accum_reward += r\n            if t is True:\n                s = env.reset()\n                frame = 0\n                # Assert we have pixels.\n                self.assertGreaterEqual(np.mean(s[""RGB_INTERLEAVED""]), 0)\n                self.assertLessEqual(np.mean(s[""RGB_INTERLEAVED""]), 255)\n                # Assert the env-observed timestep counter.\n                self.assertEqual(s[""MAP_FRAME_NUMBER""], 0)\n            else:\n                frame += frameskip\n                self.assertEqual(s[""MAP_FRAME_NUMBER""], frame)\n\n        print(""Accumulated Reward: "".format(accum_reward))\n\n    def test_deepmind_lab_env_performance(self):\n        try:\n            from rlgraph.environments import DeepmindLabEnv\n        except ImportError:\n            print(""Deepmind Lab not installed -> skipping this test case."")\n            return\n\n        frameskip = 4\n        env = DeepmindLabEnv(""seekavoid_arena_01"", observations=[""RGB_INTERLEAVED"", ""INSTR""],\n                             frameskip=frameskip, seed=1)\n\n        # Simple test runs with fixed actions.\n        num_steps = 1000\n        s = env.reset()\n        start_time = time.perf_counter()\n        for _ in range(num_steps):\n            img, text, r, t = env.step_flow(env.action_space.sample())\n        run_time = time.perf_counter() - start_time\n        print(""\\n{} Steps took {}sec ({:.2f} actions/sec)."".format(num_steps, run_time, num_steps / run_time))\n'"
rlgraph/tests/environments/test_deterministic_env.py,0,"b'# Copyright 2018/2019 The RLgraph Authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import print_function\n\nimport unittest\n\nfrom rlgraph.environments.deterministic_env import DeterministicEnv\nfrom rlgraph.tests.test_util import recursive_assert_almost_equal\n\n\n# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nclass TestDeterministicEnv(unittest.TestCase):\n    """"""\n    Tests creation, resetting and stepping through a DeterministicEnv.\n    """"""\n    def test_deterministic_env(self):\n        """"""\n        Tests functionality of DeterministicEnv.\n        """"""\n        env = DeterministicEnv(state_start=0.0, reward_start=50.0, steps_to_terminal=5)\n\n        # Simple test runs with random actions.\n        s = env.reset()\n        recursive_assert_almost_equal(s, [0.0])\n\n        # Perform 5 steps\n        for i in range(5):\n            s, r, t, _ = env.step(env.action_space.sample())\n            recursive_assert_almost_equal(s, [1.0 + i])\n            recursive_assert_almost_equal(r, 50.0 + i)\n            if i == 4:\n                self.assertTrue(t)\n            else:\n                self.assertFalse(t)\n\n        s = env.reset()\n        recursive_assert_almost_equal(s, [0.0])\n\n        # Perform another 5 steps.\n        for i in range(5):\n            s, r, t, _ = env.step(env.action_space.sample())\n            recursive_assert_almost_equal(s, [1.0 + i])\n            recursive_assert_almost_equal(r, 50.0 + i)\n            if i == 4:\n                self.assertTrue(t)\n            else:\n                self.assertFalse(t)\n\n'"
rlgraph/tests/environments/test_grid_world.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport unittest\n\nfrom rlgraph.environments import GridWorld\nfrom rlgraph.tests.test_util import recursive_assert_almost_equal\n\n\nclass TestGridWorld(unittest.TestCase):\n    """"""\n    Tests creation, resetting and stepping through a deterministic GridWorld.\n    """"""\n    def test_2x2_grid_world(self):\n        """"""\n        Tests a minimalistic 2x2 GridWorld.\n        """"""\n        env = GridWorld(world=""2x2"")\n\n        # Simple test runs with fixed actions.\n        # X=player\'s position\n        s = env.reset()  # [""XH"", "" G""]  X=player\'s position\n        self.assertTrue(s == 0)\n        s, r, t, _ = env.step(2)  # down: ["" H"", ""XG""]\n        self.assertTrue(s == 1)\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n        s, r, t, _ = env.step(1)  # right: ["" H"", "" X""]\n        self.assertTrue(s == 3)\n        recursive_assert_almost_equal(r, 1.0)\n        self.assertTrue(t)\n\n        env.reset()  # [""XH"", "" G""]  X=player\'s position\n        s, r, t, _ = env.step(1)  # right: ["" X"", "" G""] -> in the hole\n        self.assertTrue(s == 2)\n        self.assertTrue(r == -5.0)\n        self.assertTrue(t)\n\n        # Run against a wall.\n        env.reset()  # [""XH"", "" G""]  X=player\'s position\n        s, r, t, _ = env.step(3)  # left: [""XH"", "" G""]\n        self.assertTrue(s == 0)\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n        s, r, t, _ = env.step(2)  # down: ["" H"", ""XG""]\n        self.assertTrue(s == 1)\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n        s, r, t, _ = env.step(0)  # up: [""XH"", "" G""]\n        self.assertTrue(s == 0)\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n\n    def test_2x2_grid_world_using_flow_methods(self):\n        """"""\n        Tests a minimalistic 2x2 GridWorld.\n        """"""\n        env = GridWorld(world=""2x2"")\n\n        # Simple test runs with fixed actions.\n        # X=player\'s position\n        s, r, t = env.step_flow(2)  # down: ["" H"", ""XG""]\n        self.assertTrue(s == 1)\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n        s, r, t = env.step_flow(1)  # right: ["" H"", "" X""]\n        self.assertTrue(s == 0)\n        self.assertTrue(r == 1.0)\n        self.assertTrue(t)\n\n        s, r, t = env.step_flow(1)  # right: ["" X"", "" G""] -> in the hole\n        self.assertTrue(s == 0)\n        self.assertTrue(r == -5.0)\n        self.assertTrue(t)\n\n        # Run against a wall.\n        s, r, t = env.step_flow(3)  # left: [""XH"", "" G""]\n        self.assertTrue(s == 0)\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n        s, r, t = env.step_flow(2)  # down: ["" H"", ""XG""]\n        self.assertTrue(s == 1)\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n        s, r, t = env.step_flow(0)  # up: [""XH"", "" G""]\n        self.assertTrue(s == 0)\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n\n    def test_4x4_grid_world_with_container_actions(self):\n        """"""\n        Tests a 4x4 GridWorld using forward+turn+jump container actions.\n        """"""\n        env = GridWorld(world=""4x4"", action_type=""ftj"", state_representation=""xy+orientation"")\n\n        # Simple test runs with fixed actions.\n\n        # Fall into hole.\n        s = env.reset()  # [0, 0, 0] (x, y, orientation)\n        recursive_assert_almost_equal(s, [0, 0, 0, 1])\n        s, r, t, _ = env.step(dict(turn=2, forward=2))  # turn=2 (right), move=2 (forward), jump=0\n        recursive_assert_almost_equal(s, [1, 0, 1, 0])\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n        s, r, t, _ = env.step(dict(turn=2, forward=1))  # turn=2 (right), move=1 (stay), jump=0\n        recursive_assert_almost_equal(s, [1, 0, 0, -1])\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n        s, r, t, _ = env.step(dict(turn=1, forward=2))  # turn=1 (no turn), move=2 (forward), jump=0\n        recursive_assert_almost_equal(s, [1, 1, 0, -1])\n        self.assertTrue(r == -5.0)\n        self.assertTrue(t)\n\n        # Jump quite a lot and reach goal.\n        env.reset()  # [0, 0, 0] (x, y, orientation)\n        s, r, t, _ = env.step(dict(turn=2, forward=1))\n        recursive_assert_almost_equal(s, [0, 0, 1, 0])\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n        s, r, t, _ = env.step(dict(turn=1, forward=1, jump=1))\n        recursive_assert_almost_equal(s, [2, 0, 1, 0])\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n        s, r, t, _ = env.step(dict(turn=2, forward=2))\n        recursive_assert_almost_equal(s, [2, 1, 0, -1])\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n        s, r, t, _ = env.step(dict(turn=1, forward=2, jump=1))\n        recursive_assert_almost_equal(s, [2, 3, 0, -1])\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n        s, r, t, _ = env.step(dict(turn=2, forward=0))\n        recursive_assert_almost_equal(s, [3, 3, -1, 0])\n        self.assertTrue(r == 1.0)\n        self.assertTrue(t)\n\n        # Run against a wall.\n        env.reset()  # [0, 0, 0] (x, y, orientation)\n        s, r, t, _ = env.step(dict(turn=1, forward=0))\n        recursive_assert_almost_equal(s, [0, 1, 0, 1])\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n        s, r, t, _ = env.step(dict(turn=0, forward=2))\n        recursive_assert_almost_equal(s, [0, 1, -1, 0])\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n\n        # Jump over a hole (no reset).\n        s, r, t, _ = env.step(dict(turn=2, forward=1))  # turn around\n        s, r, t, _ = env.step(dict(turn=2, forward=1))\n        recursive_assert_almost_equal(s, [0, 1, 1, 0])\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n        s, r, t, _ = env.step(dict(turn=1, forward=1, jump=1))\n        recursive_assert_almost_equal(s, [2, 1, 1, 0])\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n\n    def test_long_chain_grid_world(self):\n        """"""\n        Tests a minimalistic long-chain GridWorld.\n        """"""\n        env = GridWorld(world=""long-chain"")\n\n        # Simple test runs with fixed actions.\n        # X=player\'s position\n        s = env.reset()  # [""X                                              G""]\n        self.assertTrue(s == 33)\n        s, r, t, _ = env.step(2)  # down: [""X                                              G""]\n        self.assertTrue(s == 33)\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n        s, r, t, _ = env.step(1)  # right: [""SX                                             G""]\n        self.assertTrue(s == 34)\n        recursive_assert_almost_equal(r, -0.1)\n        self.assertTrue(not t)\n\n        env.reset()  # [""X                                              G""]\n        # Right, left, down, up, right -> Move one right each iteration.\n        for x in range(20):\n            s, r, t, _ = env.step(1)\n            self.assertTrue(s == x + 33 + 1)\n            recursive_assert_almost_equal(r, -0.1)\n            self.assertTrue(not t)\n            s, r, t, _ = env.step(3)\n            self.assertTrue(s == x + 33)\n            recursive_assert_almost_equal(r, -0.1)\n            self.assertTrue(not t)\n            s, r, t, _ = env.step(2)\n            self.assertTrue(s == x + 33)\n            recursive_assert_almost_equal(r, -0.1)\n            self.assertTrue(not t)\n            s, r, t, _ = env.step(0)\n            self.assertTrue(s == x + 33)\n            recursive_assert_almost_equal(r, -0.1)\n            self.assertTrue(not t)\n            s, r, t, _ = env.step(1)\n            self.assertTrue(s == x + 33 + 1)\n            recursive_assert_almost_equal(r, -0.1)\n            self.assertTrue(not t)\n'"
rlgraph/tests/environments/test_ml_agents_env.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\n\n\nclass TestMLAgentsEnv(unittest.TestCase):\n    """"""\n    Tests creation, resetting and stepping through an openAI Atari Env.\n    """"""\n    def test_ml_agents_env(self):\n        try:\n            from rlgraph.environments import MLAgentsEnv\n        except ImportError:\n            print(""MLAgents not installed -> skipping this test case."")\n            return\n\n        env = MLAgentsEnv()\n\n        # Simple test runs with fixed actions.\n        env.reset()\n        for _ in range(100):\n            actions = [env.action_space.sample() for _ in range(env.num_environments)]\n            s, r, t, _ = env.step(actions)\n            assert all(isinstance(r_, np.ndarray) for r_ in r)\n            assert all(r_.dtype == np.float32 for r_ in r)\n            assert all(isinstance(t_, bool) for t_ in t)\n\n        env.terminate()\n'"
rlgraph/tests/environments/test_openai_gym_atari.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport unittest\n\nfrom rlgraph.environments import OpenAIGymEnv\n\n\nclass TestOpenAIAtariEnv(unittest.TestCase):\n    """"""\n    Tests creation, resetting and stepping through an openAI Atari Env.\n    """"""\n    def test_openai_atari_env(self):\n        env = OpenAIGymEnv(""Pong-v0"")\n\n        # Simple test runs with fixed actions.\n        s = env.reset()\n        # Assert we have pixels.\n        self.assertGreaterEqual(np.mean(s), 0)\n        self.assertLessEqual(np.mean(s), 255)\n        accum_reward = 0.0\n        for _ in range(100):\n            s, r, t, _ = env.step(env.action_space.sample())\n            assert isinstance(r, np.ndarray)\n            assert r.dtype == np.float32\n            assert isinstance(t, bool)\n            self.assertGreaterEqual(np.mean(s), 0)\n            self.assertLessEqual(np.mean(s), 255)\n            accum_reward += r\n\n        print(""Accumulated Reward: "".format(accum_reward))\n\n        env.terminate()\n'"
rlgraph/tests/environments/test_random_env.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport unittest\n\nfrom rlgraph.spaces import IntBox, FloatBox\nfrom rlgraph.environments import RandomEnv\nfrom rlgraph.tests.test_util import recursive_assert_almost_equal\n\n\nclass TestRandomEnv(unittest.TestCase):\n    """"""\n    Tests creation, resetting and stepping through a deterministic RandomEnv.\n    """"""\n    def test_random_env(self):\n        """"""\n        Tests deterministic functionality of RandomEnv.\n        """"""\n        env = RandomEnv(state_space=FloatBox(shape=(2, 2)), action_space=IntBox(2), deterministic=True)\n\n        # Simple test runs with fixed actions.\n        s = env.reset()\n        recursive_assert_almost_equal(s, np.array([[0.77132064, 0.02075195], [0.63364823, 0.74880388]]))\n        s, r, t, _ = env.step(env.action_space.sample())\n        recursive_assert_almost_equal(s, np.array([[0.1980629, 0.7605307], [0.1691108, 0.0883398]]))\n        s, r, t, _ = env.step(env.action_space.sample())\n        recursive_assert_almost_equal(r, np.array(0.7217553))\n        s, r, t, _ = env.step(env.action_space.sample())\n        self.assertEqual(t, False)\n        s, r, t, _ = env.step(env.action_space.sample())\n        recursive_assert_almost_equal(s, np.array([[0.4418332, 0.434014], [0.617767 , 0.5131382]]))\n        s, r, t, _ = env.step(env.action_space.sample())\n'"
rlgraph/tests/environments/test_readme_example.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nfrom rlgraph.tests.test_util import config_from_path\n\n\nclass TestReadmeExample(unittest.TestCase):\n    """"""\n    Tests if the readme example runs.\n    """"""\n\n    def test_readme_example(self):\n        """"""\n        Tests deterministic functionality of RandomEnv.\n        """"""\n        from rlgraph.agents import DQNAgent\n        from rlgraph.environments import OpenAIGymEnv\n\n        environment = OpenAIGymEnv(\'CartPole-v0\')\n        config = config_from_path(""../../examples/configs/dqn_cartpole.json"")\n\n        # Create from .json file or dict, see agent API for all\n        # possible configuration parameters.\n        agent = DQNAgent.from_spec(\n            config,\n            state_space=environment.state_space,\n            action_space=environment.action_space\n        )\n\n        # Get an action, take a step, observe reward.\n        state = environment.reset()\n        action, preprocessed_state = agent.get_action(\n            states=state,\n            extra_returns=""preprocessed_states""\n        )\n\n        # Execute step in environment.\n        next_state, reward, terminal, info = environment.step(action)\n\n        # Observe result.\n        agent.observe(\n            preprocessed_states=preprocessed_state,\n            actions=action,\n            internals=[],\n            next_states=next_state,\n            rewards=reward,\n            terminals=terminal\n        )\n\n        # Call update when desired:\n        loss = agent.update()\n'"
rlgraph/tests/environments/test_sequential_vector_env.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nfrom rlgraph.environments import SequentialVectorEnv\nfrom rlgraph.tests.test_util import recursive_assert_almost_equal\n\n\nclass TestSequentialVectorEnv(unittest.TestCase):\n    """"""\n    Tests creation, resetting and stepping through a sequential vectorized Env with GridWorld entities.\n    """"""\n    def test_sequential_vector_env(self):\n        num_envs = 4\n        env = SequentialVectorEnv(num_environments=num_envs, env_spec={""type"": ""gridworld"", ""world"": ""2x2""})\n\n        # Simple test runs with fixed actions.\n        # X=player\'s position\n        s = env.reset(index=0)  # [""XH"", "" G""]  X=player\'s position\n        self.assertTrue(s == 0)\n\n        s = env.reset_all()\n        all(self.assertTrue(s_ == 0) for s_ in s)\n\n        s, r, t, _ = env.step([2 for _ in range(num_envs)])  # down: ["" H"", ""XG""]\n        all(self.assertTrue(s_ == 1) for s_ in s)\n        all(recursive_assert_almost_equal(r_, -0.1) for r_ in r)\n        all(self.assertTrue(not t_) for t_ in t)\n\n        s, r, t, _ = env.step([1 for _ in range(num_envs)])  # right: ["" H"", "" X""]\n        all(self.assertTrue(s_ == 3) for s_ in s)\n        all(recursive_assert_almost_equal(r_, 1.0) for r_ in r)\n        all(self.assertTrue(t_) for t_ in t)\n\n        [env.reset(index=i) for i in range(num_envs)]  # [""XH"", "" G""]  X=player\'s position\n        s, r, t, _ = env.step([1 for _ in range(num_envs)])  # right: ["" X"", "" G""] -> in the hole\n        all(self.assertTrue(s_ == 2) for s_ in s)\n        all(self.assertTrue(r_ == -5.0) for r_ in r)\n        all(self.assertTrue(t_) for t_ in t)\n\n        # Run against a wall.\n        env.reset_all()  # [""XH"", "" G""]  X=player\'s position\n        s, r, t, _ = env.step([3 for _ in range(num_envs)])  # left: [""XH"", "" G""]\n        all(self.assertTrue(s_ == 0) for s_ in s)\n        all(recursive_assert_almost_equal(r_, -0.1) for r_ in r)\n        all(self.assertTrue(not t_) for t_ in t)\n        s, r, t, _ = env.step([2 for _ in range(num_envs)])  # down: ["" H"", ""XG""]\n        all(self.assertTrue(s_ == 1) for s_ in s)\n        all(recursive_assert_almost_equal(r_, -0.1) for r_ in r)\n        all(self.assertTrue(not t_) for t_ in t)\n        s, r, t, _ = env.step([0 for _ in range(num_envs)])  # up: [""XH"", "" G""]\n        all(self.assertTrue(s_ == 0) for s_ in s)\n        all(recursive_assert_almost_equal(r_, -0.1) for r_ in r)\n        all(self.assertTrue(not t_) for t_ in t)\n\n'"
rlgraph/tests/execution/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n'"
rlgraph/tests/execution/test_apex_executor.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport unittest\nfrom copy import deepcopy\n\nimport numpy as np\n\nfrom rlgraph.components import PreprocessorStack\nfrom rlgraph.environments import OpenAIGymEnv, Environment\nfrom rlgraph.execution.ray.apex import ApexExecutor\nfrom rlgraph.tests.test_util import config_from_path, recursive_assert_almost_equal\n\n\nclass TestApexExecutor(unittest.TestCase):\n    """"""\n    Tests the ApexExecutor which provides an interface for distributing Apex-style workloads\n    via Ray.\n    """"""\n    def test_learning_2x2_grid_world(self):\n        """"""\n        Tests if apex can learn a simple environment using a single worker, thus replicating\n        dqn.\n        """"""\n        env_spec = dict(\n            type=""grid-world"",\n            world=""2x2"",\n            save_mode=False\n        )\n        agent_config = config_from_path(""configs/apex_agent_for_2x2_gridworld.json"")\n        executor = ApexExecutor(\n            environment_spec=env_spec,\n            agent_config=agent_config,\n        )\n        # Define executor, test assembly.\n        print(""Successfully created executor."")\n\n        # Executes actual workload.\n        result = executor.execute_workload(workload=dict(\n            num_timesteps=5000, report_interval=100, report_interval_min_seconds=1)\n        )\n        full_worker_stats = executor.result_by_worker()\n        print(""All finished episode rewards"")\n        print(full_worker_stats[""episode_rewards""])\n\n        print(""STATES:\\n{}"".format(executor.local_agent.last_q_table[""states""]))\n        print(""\\n\\nQ(s,a)-VALUES:\\n{}"".format(np.round_(executor.local_agent.last_q_table[""q_values""], decimals=2)))\n\n        # Check q-table for correct values.\n        expected_q_values_per_state = {\n            (1.0, 0, 0, 0): (-1, -5, 0, -1),\n            (0, 1.0, 0, 0): (-1, 1, 0, 0)\n        }\n        for state, q_values in zip(\n                executor.local_agent.last_q_table[""states""], executor.local_agent.last_q_table[""q_values""]\n        ):\n            state, q_values = tuple(state), tuple(q_values)\n            assert state in expected_q_values_per_state, \\\n                ""ERROR: state \'{}\' not expected in q-table as it\'s a terminal state!"".format(state)\n            recursive_assert_almost_equal(q_values, expected_q_values_per_state[state], decimals=0)\n\n    def test_learning_2x2_grid_world_container_actions(self):\n        """"""\n        Tests Apex container action functionality.\n        """"""\n        env_spec = dict(\n            type=""grid-world"",\n            world=""2x2"",\n            save_mode=False,\n            action_type=""ftj"",\n            state_representation=""xy+orientation""\n        )\n        agent_config = config_from_path(""configs/apex_agent_for_2x2_gridworld_with_container_actions.json"")\n        executor = ApexExecutor(\n            environment_spec=env_spec,\n            agent_config=agent_config,\n        )\n        # Define executor, test assembly.\n        print(""Successfully created executor."")\n\n        # Executes actual workload.\n        result = executor.execute_workload(workload=dict(\n            num_timesteps=10000, report_interval=100, report_interval_min_seconds=1)\n        )\n        print(result)\n\n    def test_learning_cartpole(self):\n        """"""\n        Tests if apex can learn a simple environment using a single worker, thus replicating\n        dqn.\n        """"""\n        env_spec = dict(\n            type=""openai"",\n            gym_env=""CartPole-v0""\n        )\n        agent_config = config_from_path(""configs/apex_agent_cartpole.json"")\n        executor = ApexExecutor(\n            environment_spec=env_spec,\n            agent_config=agent_config,\n        )\n        # Define executor, test assembly.\n        print(""Successfully created executor."")\n\n        # Executes actual workload.\n        result = executor.execute_workload(workload=dict(num_timesteps=20000, report_interval=1000,\n                                                         report_interval_min_seconds=1))\n        print(""Finished executing workload:"")\n        print(result)\n\n    def test_from_callable_env_spec(self):\n        env_spec = dict(\n            type=""openai"",\n            gym_env=""CartPole-v0""\n        )\n        agent_config = config_from_path(""configs/apex_agent_cartpole.json"")\n\n        def create_env():\n            return Environment.from_spec(env_spec)\n\n        # Pass lambda, not spec.\n        executor = ApexExecutor(\n            environment_spec=create_env,\n            agent_config=agent_config,\n        )\n\n        # Define executor, test assembly.\n        print(""Successfully created executor."")\n\n        # Executes actual workload.\n        result = executor.execute_workload(workload=dict(num_timesteps=20000, report_interval=1000,\n                                                         report_interval_min_seconds=1))\n        print(""Finished executing workload:"")\n        print(result)\n\n    def test_learning_cartpole_n_step(self):\n        """"""\n        Tests if apex can learn a simple environment using a single worker, thus replicating\n        DQN.\n        """"""\n        env_spec = dict(\n            type=""openai"",\n            gym_env=""CartPole-v0""\n        )\n        agent_config = config_from_path(""configs/apex_agent_cartpole.json"")\n\n        # Use n-step adjustments.\n        agent_config[""execution_spec""][""ray_spec""][""worker_spec""][""n_step_adjustment""] = 3\n        agent_config[""execution_spec""][""ray_spec""][""apex_replay_spec""][""n_step_adjustment""] = 3\n        agent_config[""n_step""] = 3\n\n        executor = ApexExecutor(\n            environment_spec=env_spec,\n            agent_config=agent_config,\n        )\n        # Define executor, test assembly.\n        print(""Successfully created executor."")\n\n        # Executes actual workload.\n        result = executor.execute_workload(workload=dict(num_timesteps=20000, report_interval=1000,\n                                                         report_interval_min_seconds=1))\n        print(""Finished executing workload:"")\n        print(result)\n\n    def test_with_final_eval(self):\n        """"""\n        Tests if apex can learn a simple environment using a single worker, thus replicating\n        DQN.\n        """"""\n        env_spec = dict(\n            type=""openai"",\n            gym_env=""CartPole-v0""\n        )\n        agent_config = config_from_path(""configs/apex_agent_cartpole.json"")\n\n        # Use n-step adjustments.\n        agent_config[""execution_spec""][""ray_spec""][""worker_spec""][""n_step_adjustment""] = 3\n        agent_config[""execution_spec""][""ray_spec""][""apex_replay_spec""][""n_step_adjustment""] = 3\n        agent_config[""n_step""] = 3\n\n        executor = ApexExecutor(\n            environment_spec=env_spec,\n            agent_config=agent_config,\n        )\n        # Define executor, test assembly.\n        print(""Successfully created executor."")\n\n        # Executes actual workload.\n        result = executor.execute_workload(workload=dict(num_timesteps=20000, report_interval=1000,\n                                                         report_interval_min_seconds=1))\n        print(""Finished executing workload:"")\n        print(result)\n\n        # Get agent.\n        agent = executor.local_agent\n        preprocessing_spec = agent_config[""preprocessing_spec""]\n\n        # Create env.\n        env = OpenAIGymEnv.from_spec(env_spec)\n\n        if preprocessing_spec is not None:\n            preprocessing_spec = deepcopy(preprocessing_spec)\n            in_space = env.state_space.with_batch_rank()\n            in_space = deepcopy(in_space)\n            # Set scopes.\n            scopes = [preprocessor[""scope""] for preprocessor in preprocessing_spec]\n            # Set backend to python.\n            for spec in preprocessing_spec:\n                spec[""backend""] = ""python""\n            processor_stack = PreprocessorStack(*preprocessing_spec, backend=""python"")\n            build_space = in_space\n            for sub_comp_scope in scopes:\n                processor_stack.sub_components[sub_comp_scope].create_variables(input_spaces=dict(\n                    inputs=build_space\n                ), action_space=None)\n                build_space = processor_stack.sub_components[sub_comp_scope].get_preprocessed_space(build_space)\n            processor_stack.reset()\n        else:\n            processor_stack = None\n\n        ep_rewards = []\n        print(""finished learning, starting eval"")\n        for _ in range(10):\n            state = env.reset()\n            terminal = False\n            ep_reward = 0\n            while not terminal:\n                state, _ = agent.state_space.force_batch(state)\n                if processor_stack is not None:\n                    state = processor_stack.preprocess(state)\n\n                actions = agent.get_action(states=state, use_exploration=False, apply_preprocessing=False)\n                next_state, step_reward, terminal, info = env.step(actions=actions[0])\n                ep_reward += step_reward\n\n                state = next_state\n                if terminal:\n                    ep_rewards.append(ep_reward)\n                    break\n\n        print(""Eval episode rewards:"")\n        print(ep_rewards)\n'"
rlgraph/tests/execution/test_gpu_strategies.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, softwamre\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\nfrom logging import DEBUG\n\nimport numpy as np\n\nfrom rlgraph.agents import ApexAgent, DQNAgent, PPOAgent\nfrom rlgraph.environments import OpenAIGymEnv, RandomEnv, GridWorld\nfrom rlgraph.execution.single_threaded_worker import SingleThreadedWorker\nfrom rlgraph.spaces import *\nfrom rlgraph.tests.test_util import config_from_path\nfrom rlgraph.tests.test_util import recursive_assert_almost_equal\nfrom rlgraph.utils import root_logger\nfrom rlgraph.utils.numpy import one_hot\n\n\nclass TestGpuStrategies(unittest.TestCase):\n    """"""\n    Tests gpu strategies.\n    """"""\n    env_spec = dict(\n        type=""openai"",\n        gym_env=""PongNoFrameskip-v4"",\n        # The frameskip in the agent config will trigger worker skips, this\n        # is used for internal env.\n        frameskip=4,\n        max_num_noops=30,\n        episodic_life=True\n    )\n    random_env_spec = dict(type=""random"", state_space=FloatBox(shape=(2,)), action_space=IntBox(2))\n    grid_world_2x2_flattened_state_space = FloatBox(shape=(4,), add_batch_rank=True)\n    grid_world_4x4_flattened_state_space = FloatBox(shape=(16,), add_batch_rank=True)\n\n    def test_multi_gpu_dqn_agent_compilation(self):\n        """"""\n        Tests if the multi gpu strategy can compile successfully on a multi gpu system, but\n        also runs on a CPU-only system using fake-GPU logic for testing purposes.\n        """"""\n        root_logger.setLevel(DEBUG)\n        agent_config = config_from_path(""configs/multi_gpu_dqn_for_random_env.json"")\n        environment = RandomEnv.from_spec(self.random_env_spec)\n\n        agent = DQNAgent.from_spec(\n            agent_config, state_space=environment.state_space, action_space=environment.action_space\n        )\n        print(""Compiled DQN agent on multi-GPU system"")\n\n        # Do an update from external batch.\n        batch_size = agent_config[""update_spec""][""batch_size""]\n        external_batch = dict(\n            states=environment.state_space.sample(size=batch_size),\n            actions=environment.action_space.sample(size=batch_size),\n            rewards=np.random.sample(size=batch_size),\n            terminals=np.random.choice([True, False], size=batch_size),\n            next_states=environment.state_space.sample(size=batch_size),\n            importance_weights=np.zeros(shape=(batch_size,))\n        )\n        agent.update(batch=external_batch)\n        print(""Performed an update from external batch"")\n\n    def test_multi_gpu_apex_agent_compilation(self):\n        """"""\n        Tests if the multi gpu strategy can compile successfully on a multi gpu system, but\n        also runs on a CPU-only system using fake-GPU logic for testing purposes.\n        """"""\n        root_logger.setLevel(DEBUG)\n        agent_config = config_from_path(""configs/multi_gpu_ray_apex_for_pong.json"")\n        agent_config[""execution_spec""].pop(""ray_spec"")\n        environment = OpenAIGymEnv(""Pong-v0"", frameskip=4)\n\n        agent = ApexAgent.from_spec(\n            agent_config, state_space=environment.state_space, action_space=environment.action_space\n        )\n        print(""Compiled Apex agent"")\n\n    def test_multi_gpu_dqn_agent_learning_test_gridworld_2x2(self):\n        """"""\n        Tests if the multi gpu strategy can learn successfully on a multi gpu system, but\n        also runs on a CPU-only system using fake-GPU logic for testing purposes.\n        """"""\n        env_spec = dict(type=""grid-world"", world=""2x2"")\n        dummy_env = GridWorld.from_spec(env_spec)\n        agent_config = config_from_path(""configs/multi_gpu_dqn_for_2x2_gridworld.json"")\n        preprocessing_spec = agent_config.pop(""preprocessing_spec"")\n        agent = DQNAgent.from_spec(\n            agent_config,\n            state_space=self.grid_world_2x2_flattened_state_space,\n            action_space=dummy_env.action_space,\n        )\n\n        time_steps = 2000\n        worker = SingleThreadedWorker(\n            env_spec=env_spec,\n            agent=agent,\n            worker_executes_preprocessing=True,\n            preprocessing_spec=preprocessing_spec\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        self.assertEqual(results[""timesteps_executed""], time_steps)\n        self.assertEqual(results[""env_frames""], time_steps)\n        self.assertGreaterEqual(results[""mean_episode_reward""], -4.5)\n        self.assertGreaterEqual(results[""max_episode_reward""], 0.0)\n        self.assertLessEqual(results[""episodes_executed""], time_steps / 2)\n\n        # Check all learnt Q-values.\n        q_values = agent.graph_executor.execute((""get_q_values"", one_hot(np.array([0, 1]), depth=4)))[:]\n        recursive_assert_almost_equal(q_values[0], (0.8, -5, 0.9, 0.8), decimals=1)\n        recursive_assert_almost_equal(q_values[1], (0.8, 1.0, 0.9, 0.9), decimals=1)\n\n    def test_apex_multi_gpu_update(self):\n        """"""\n        Tests if the multi GPU optimizer can perform successful updates, using the apex executor.\n        Also runs on a CPU-only system using fake-GPU logic for testing purposes.\n        """"""\n        agent_config = config_from_path(""configs/multi_gpu_ray_apex_for_pong.json"")\n        executor = ApexExecutor(\n            environment_spec=self.env_spec,\n            agent_config=agent_config,\n        )\n\n        # Executes actual workload.\n        result = executor.execute_workload(workload=dict(\n            num_timesteps=100000, report_interval=10000, report_interval_min_seconds=10)\n        )\n\n    def test_multi_gpu_ppo_agent_learning_test_gridworld_2x2(self):\n        """"""\n        Tests if the multi gpu strategy can learn successfully on a multi gpu system, but\n        also runs on a CPU-only system using fake-GPU logic for testing purposes.\n        """"""\n        env_spec = dict(type=""grid-world"", world=""2x2"")\n        dummy_env = GridWorld.from_spec(env_spec)\n        agent_config = config_from_path(""configs/multi_gpu_ppo_for_2x2_gridworld.json"")\n        preprocessing_spec = agent_config.pop(""preprocessing_spec"")\n        agent = PPOAgent.from_spec(\n            agent_config,\n            state_space=self.grid_world_2x2_flattened_state_space,\n            action_space=dummy_env.action_space,\n        )\n\n        time_steps = 10000\n        worker = SingleThreadedWorker(\n            env_spec=env_spec,\n            agent=agent,\n            worker_executes_preprocessing=True,\n            preprocessing_spec=preprocessing_spec\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        # Assume we have learned something.\n        # TODO: This test needs more tuning. -1.0 is not great for the 2x2 grid world.\n        self.assertGreater(results[""mean_episode_reward""], -1.0)\n'"
rlgraph/tests/execution/test_ray_policy_worker.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nfrom rlgraph.execution.ray.ray_policy_worker import RayPolicyWorker\nfrom rlgraph.execution.ray.ray_util import RayWeight\nfrom rlgraph.tests.test_util import config_from_path\n\nfrom rlgraph import get_distributed_backend\nfrom rlgraph.agents import Agent\nfrom rlgraph.environments import Environment\n\nif get_distributed_backend() == ""ray"":\n    import ray\n\n\nclass TestRayPolicyWorker(unittest.TestCase):\n\n    env_spec = dict(\n      type=""openai"",\n      gym_env=""CartPole-v0""\n    )\n\n    def setUp(self):\n        """"""\n        Inits a local redis and scheduler.\n        """"""\n        ray.init()\n\n    def test_policy_and_vf_weight_syncing(self):\n        """"""\n        Tests weight synchronization with a local agent and a remote worker.\n        """"""\n        # First, create a local agent\n        env_spec = dict(\n            type=""openai"",\n            gym_env=""CartPole-v0""\n        )\n        env = Environment.from_spec(env_spec)\n        agent_config = config_from_path(""configs/sync_batch_ppo_cartpole.json"")\n\n        ray_spec = agent_config[""execution_spec""].pop(""ray_spec"")\n        local_agent = Agent.from_spec(\n            agent_config,\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n        ray_spec[""worker_spec""][""worker_sample_size""] = 50\n        # Create a remote worker with the same agent config.\n        worker = RayPolicyWorker.as_remote().remote(agent_config, ray_spec[""worker_spec""],\n                                                   self.env_spec)\n\n        # This imitates the initial executor sync without ray.put\n        weights = RayWeight(local_agent.get_weights())\n        print(\'Weight type in init sync = {}\'.format(type(weights)))\n        print(""Weights = "", weights)\n        worker.set_weights.remote(weights)\n        print(\'Init weight sync successful.\')\n\n        # Replicate worker syncing steps as done in e.g. Ape-X executor:\n        weights = RayWeight(local_agent.get_weights())\n        print(\'Weight type returned by ray put = {}\'.format(type(weights)))\n        print(weights)\n        ret = worker.set_weights.remote(weights)\n        ray.wait([ret])\n        print(\'Object store weight sync successful.\')\n\n'"
rlgraph/tests/execution/test_ray_value_worker.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\nfrom time import sleep\n\nfrom rlgraph.execution.ray.ray_value_worker import RayValueWorker\nfrom rlgraph.execution.ray.ray_util import RayWeight\nfrom rlgraph.tests.test_util import recursive_assert_almost_equal, config_from_path\nimport numpy as np\n\nfrom rlgraph import get_distributed_backend\nfrom rlgraph.agents import Agent\nfrom rlgraph.environments import Environment\n\nif get_distributed_backend() == ""ray"":\n    import ray\n\n\nclass TestRayWorker(unittest.TestCase):\n\n    env_spec = dict(\n      type=""openai"",\n      gym_env=""CartPole-v0""\n    )\n\n    def setUp(self):\n        """"""\n        Inits a local redis and scheduler.\n        """"""\n        ray.init()\n\n    def test_get_timesteps(self):\n        """"""\n        Simply tests if time-step execution loop works and returns the samples.\n        """"""\n        agent_config = config_from_path(""configs/apex_agent_cartpole.json"")\n        ray_spec = agent_config[""execution_spec""].pop(""ray_spec"")\n\n        # 2 workers.\n        ray_spec[""worker_spec""][""worker_sample_size""] = 50\n        worker = RayValueWorker.as_remote().remote(agent_config, ray_spec[""worker_spec""],\n                                                   self.env_spec)\n\n        # Test when breaking on terminal.\n        # Init remote task.\n        task = worker.execute_and_get_timesteps.remote(100, break_on_terminal=True)\n        sleep(5)\n        # Retrieve result.\n        result = ray.get(task)\n        observations = result.get_batch()\n        print(\'Task results, break on terminal = True:\')\n        print(observations)\n        print(result.get_metrics())\n\n        self.assertLessEqual(len(observations[\'terminals\']), 100)\n        # There can only be one terminal in there because we break on terminals:\n        terminals = 0\n        for elem in observations[\'terminals\']:\n            if np.alltrue(elem):\n                terminals += 1\n        print(observations[\'terminals\'])\n        self.assertEqual(1, terminals)\n\n        # Now run exactly 100 steps.\n        task = worker.execute_and_get_timesteps.remote(100, break_on_terminal=False)\n        sleep(5)\n        # Retrieve result.\n        result = ray.get(task)\n        observations = result.get_batch()\n        print(\'Task results, break on terminal = False:\')\n        print(result.get_metrics())\n\n        # We do not break on terminal so there should be exactly 100 steps.\n        self.assertEqual(len(observations[\'terminals\']), 100)\n\n        # Test with count.\n        task = worker.execute_and_get_with_count.remote()\n        # Retrieve result.\n        result, size = ray.get(task)\n        observations = result.get_batch()\n        print(""Returned exact count: "")\n        print(size)\n\n        # We do not break on terminal so there should be exactly 100 steps.\n        self.assertEqual(len(observations[""terminals""]), size)\n\n    def test_metrics(self):\n        """"""\n        Tests metric collection for 1 and multiple environments.\n        """"""\n        agent_config = config_from_path(""configs/apex_agent_cartpole.json"")\n\n        ray_spec = agent_config[""execution_spec""].pop(""ray_spec"")\n        ray_spec[""worker_spec""][""worker_sample_size""] = 50\n        worker_spec = ray_spec[""worker_spec""]\n        worker = RayValueWorker.as_remote().remote(agent_config, ray_spec[""worker_spec""],\n                                                   self.env_spec,  auto_build=True)\n\n        print(""Testing statistics for 1 environment:"")\n        # Run for a while:\n        task = worker.execute_and_get_timesteps.remote(100, break_on_terminal=False)\n        sleep(1)\n        # Include a transition between calls.\n        task = worker.execute_and_get_timesteps.remote(100, break_on_terminal=False)\n        sleep(1)\n        # Retrieve result.\n        result = ray.get(task)\n        print(\'Task results:\')\n        print(result.get_metrics())\n\n        # Get worker metrics.\n        task = worker.get_workload_statistics.remote()\n        result = ray.get(task)\n        print(""Worker statistics:"")\n\n        # In cartpole, num timesteps = reward -> must be the same.\n        print(""Cartpole episode rewards: {}"".format(result[""episode_rewards""]))\n        print(""Cartpole episode timesteps: {}"".format(result[""episode_timesteps""]))\n        recursive_assert_almost_equal(result[""episode_rewards""], result[""episode_timesteps""])\n\n        # Now repeat this but for multiple environments.\n        print(""Testing statistics for 4 environments:"")\n        worker_spec[""num_worker_environments""] = 4\n        worker_spec[""num_background_environments""] = 2\n        worker = RayValueWorker.as_remote().remote(agent_config, ray_spec[""worker_spec""],\n                                                   self.env_spec,  auto_build=True)\n\n        task = worker.execute_and_get_timesteps.remote(100, break_on_terminal=False)\n        sleep(1)\n        result = ray.get(task)\n        task = worker.execute_and_get_timesteps.remote(100, break_on_terminal=False)\n        sleep(1)\n        result = ray.get(task)\n        task = worker.get_workload_statistics.remote()\n        result = ray.get(task)\n        print(""Multi-env statistics:"")\n        print(""Cartpole episode rewards: {}"".format(result[""episode_rewards""]))\n        print(""Cartpole episode timesteps: {}"".format(result[""episode_timesteps""]))\n        recursive_assert_almost_equal(result[""episode_rewards""], result[""episode_timesteps""])\n\n    def test_worker_weight_syncing(self):\n        """"""\n        Tests weight synchronization with a local agent and a remote worker.\n        """"""\n        # First, create a local agent\n        env_spec = dict(\n            type=""openai"",\n            gym_env=""PongNoFrameskip-v4"",\n            # The frameskip in the agent config will trigger worker skips, this\n            # is used for internal env.\n            frameskip=4,\n            max_num_noops=30,\n            episodic_life=True\n        )\n        env = Environment.from_spec(env_spec)\n        agent_config = config_from_path(""configs/ray_apex_for_pong.json"")\n\n        # Remove unneeded apex params.\n        if ""apex_replay_spec"" in agent_config:\n            agent_config.pop(""apex_replay_spec"")\n\n        ray_spec = agent_config[""execution_spec""].pop(""ray_spec"")\n        local_agent = Agent.from_spec(\n            agent_config,\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n\n        ray_spec[""worker_spec""][""worker_sample_size""] = 50\n        # Create a remote worker with the same agent config.\n        worker = RayValueWorker.as_remote().remote(agent_config, ray_spec[""worker_spec""], env_spec)\n\n        # This imitates the initial executor sync without ray.put\n        weights = RayWeight(local_agent.get_weights())\n        print(\'Weight type in init sync = {}\'.format(type(weights)))\n        ret = worker.set_weights.remote(weights)\n        ray.wait([ret])\n        print(\'Init weight sync successful.\')\n\n        # Replicate worker syncing steps as done in e.g. Ape-X executor:\n        weights = RayWeight(local_agent.get_weights())\n        print(\'Weight type returned by ray put = {}\'.format(type(weights)))\n        ret = worker.set_weights.remote(weights)\n        ray.wait([ret])\n        print(\'Object store weight sync successful.\')\n\n'"
rlgraph/tests/execution/test_single_threaded_worker.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nfrom rlgraph.agents.random_agent import RandomAgent\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.execution.single_threaded_worker import SingleThreadedWorker\n\n\nclass TestSingleThreadedWorker(unittest.TestCase):\n\n    environment = OpenAIGymEnv(gym_env=\'CartPole-v0\')\n\n    def test_timesteps(self):\n        """"""\n        Simply tests if timestep execution loop works and returns a result.\n        """"""\n        agent = RandomAgent(\n            action_space=self.environment.action_space,\n            state_space=self.environment.state_space\n        )\n        worker = SingleThreadedWorker(\n            env_spec=lambda: self.environment,\n            agent=agent,\n            frameskip=1,\n            worker_executes_preprocessing=False\n        )\n\n        result = worker.execute_timesteps(100)\n        self.assertEqual(result[\'timesteps_executed\'], 100)\n        self.assertGreater(result[\'episodes_executed\'], 0)\n        self.assertLessEqual(result[\'episodes_executed\'], 100)\n        self.assertGreaterEqual(result[\'env_frames\'], 100)\n        self.assertGreaterEqual(result[\'runtime\'], 0.0)\n\n    def test_episodes(self):\n        """"""\n        Simply tests if episode execution loop works and returns a result.\n        """"""\n        agent = RandomAgent(\n            action_space=self.environment.action_space,\n            state_space=self.environment.state_space\n        )\n        worker = SingleThreadedWorker(\n            env_spec=lambda: self.environment,\n            agent=agent,\n            frameskip=1,\n            worker_executes_preprocessing=False\n        )\n\n        result = worker.execute_episodes(5, max_timesteps_per_episode=10)\n        # Max 5 * 10.\n        self.assertLessEqual(result[\'timesteps_executed\'], 50)\n        self.assertEqual(result[\'episodes_executed\'], 5)\n        self.assertLessEqual(result[\'env_frames\'], 50)\n        self.assertGreaterEqual(result[\'runtime\'], 0.0)\n'"
rlgraph/tests/execution/test_sync_batch_executor.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\nfrom rlgraph.execution.ray.sync_batch_executor import SyncBatchExecutor\nfrom rlgraph.tests.test_util import config_from_path\n\n\nclass TestSyncBatchExecutor(unittest.TestCase):\n    """"""\n    Tests the synchronous batch executor which provides an interface for executing A2C-style algorithms.\n    via Ray.\n    """"""\n    def test_ppo_learning_cartpole(self):\n        """"""\n        Tests if sync-batch ppo can solve cartpole.\n        """"""\n        env_spec = dict(\n            type=""openai"",\n            gym_env=""CartPole-v0""\n        )\n        agent_config = config_from_path(""configs/sync_batch_ppo_cartpole.json"")\n\n        executor = SyncBatchExecutor(\n            environment_spec=env_spec,\n            agent_config=agent_config,\n        )\n        # Define executor, test assembly.\n        print(""Successfully created executor."")\n\n        # Executes actual workload.\n        result = executor.execute_workload(workload=dict(num_timesteps=20000, report_interval=1000,\n                                                         report_interval_min_seconds=1))\n        print(""Finished executing workload:"")\n        print(result)\n\n    def test_learning_2x2_grid_world_container_actions(self):\n        """"""\n        Tests sync batch container action functionality.\n        """"""\n        env_spec = dict(\n            type=""grid-world"",\n            world=""2x2"",\n            save_mode=False,\n            action_type=""ftj"",\n            state_representation=""xy+orientation""\n        )\n        agent_config = config_from_path(""configs/sync_batch_ppo_gridworld_with_container_actions.json"")\n        executor = SyncBatchExecutor(\n            environment_spec=env_spec,\n            agent_config=agent_config,\n        )\n        # Define executor, test assembly.\n        print(""Successfully created executor."")\n\n        # Executes actual workload.\n        result = executor.execute_workload(workload=dict(\n            num_timesteps=10000, report_interval=100, report_interval_min_seconds=1)\n        )\n        print(result)\n\n    def test_ppo_learning_pendulum(self):\n        """"""\n        Tests if sync-batch ppo can solve Pendulum.\n        """"""\n        env_spec = dict(\n            type=""openai"",\n            gym_env=""Pendulum-v0""\n        )\n        agent_config = config_from_path(""configs/sync_batch_ppo_pendulum.json"")\n\n        executor = SyncBatchExecutor(\n            environment_spec=env_spec,\n            agent_config=agent_config,\n        )\n        # Define executor, test assembly.\n        print(""Successfully created executor."")\n\n        # Executes actual workload.\n        result = executor.execute_workload(workload=dict(num_timesteps=500000, report_interval=25000,\n                                                         report_interval_min_seconds=1))\n        print(""Finished executing workload:"")\n        print(result)\n'"
rlgraph/tests/performance/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n'"
rlgraph/tests/performance/test_backends.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport unittest\n\nfrom rlgraph.agents import DQNAgent\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.execution import SingleThreadedWorker\nfrom rlgraph.tests.test_util import config_from_path\nfrom rlgraph.utils import root_logger\n\n\nclass TestPytorchBackend(unittest.TestCase):\n    """"""\n    Tests PyTorch component execution.\n\n    # TODO: This is a temporary test. We will later run all backend-specific\n    tests via setting the executor in the component-test.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    def test_cartpole_with_worker(self):\n        env = OpenAIGymEnv(""CartPole-v0"")\n        agent_config = config_from_path(""configs/backend_performance_dqn_cartpole.json"")\n\n        # Test cpu settings for batching here.\n        agent_config[""update_spec""] = None\n\n        agent = DQNAgent.from_spec(\n            # Uses 2015 DQN parameters as closely as possible.\n            agent_config,\n            state_space=env.state_space,\n            # Try with ""reduced"" action space (actually only 3 actions, up, down, no-op)\n            action_space=env.action_space\n        )\n\n        worker = SingleThreadedWorker(\n            env_spec=lambda: OpenAIGymEnv(""CartPole-v0""),\n            agent=agent,\n            frameskip=1,\n            num_environments=1,\n            worker_executes_preprocessing=False\n        )\n\n        result = worker.execute_timesteps(1000)\n        print(result)\n\n    def test_pong_with_worker(self):\n        env_spec = dict(\n            type=""openai"",\n            gym_env=""PongNoFrameskip-v4"",\n            # The frameskip in the agent config will trigger worker skips, this\n            # is used for internal env.\n            frameskip=4,\n            max_num_noops=30,\n            episodic_life=False\n        )\n\n        env = OpenAIGymEnv.from_spec(env_spec)\n        agent_config = config_from_path(""configs/backend_performance_dqn_pong.json"")\n\n        # Test cpu settings for batching here.\n        agent_config[""update_spec""] = None\n\n        agent = DQNAgent.from_spec(\n            # Uses 2015 DQN parameters as closely as possible.\n            agent_config,\n            state_space=env.state_space,\n            # Try with ""reduced"" action space (actually only 3 actions, up, down, no-op)\n            action_space=env.action_space\n        )\n\n        worker = SingleThreadedWorker(\n            env_spec=env_spec,\n            agent=agent,\n            frameskip=1,\n            preprocessing_spec=agent_config[""preprocessing_spec""],\n            worker_executes_preprocessing=True\n        )\n\n        result = worker.execute_timesteps(1000)\n        print(result)\n\n'"
rlgraph/tests/performance/test_multi_gpu_updates.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport unittest\nimport numpy as np\n\nfrom rlgraph.agents import Agent\nfrom rlgraph.spaces import Dict, FloatBox, BoolBox, IntBox\nfrom rlgraph.tests.test_util import config_from_path\nfrom rlgraph.environments import Environment\n\n\nclass TestMultiGPUUpdates(unittest.TestCase):\n    """"""\n    Tests multi gpu update throughput.\n    """"""\n    env_spec = dict(\n        type=""openai"",\n        gym_env=""Pong-v0"",\n        frameskip=4,\n        max_num_noops=30,\n        episodic_life=True\n    )\n\n    def test_update_throughput(self):\n        env = Environment.from_spec(self.env_spec)\n        # TODO comment in for multi gpu\n        # config_from_path(""configs/multi_gpu_ray_apex_for_pong.json""),\n        config = config_from_path(""configs/ray_apex_for_pong.json"")\n\n        # Adjust to usable GPUs for test system.\n        num_gpus = [1]\n        for gpu_count in num_gpus:\n            config[""execution_spec""][""gpu_spec""][""num_gpus""] = gpu_count\n            config[""execution_spec""][""gpu_spec""][""per_process_gpu_memory_fraction""] = 1.0 / gpu_count\n\n            agent = Agent.from_spec(\n                # TODO replace with config from above\n                config_from_path(""configs/ray_apex_for_pong.json""),\n                state_space=env.state_space,\n                # Try with ""reduced"" action space (actually only 3 actions, up, down, no-op)\n                action_space=env.action_space\n            )\n\n            batch_space = Dict(\n                states=agent.preprocessed_state_space,\n                actions=env.action_space,\n                rewards=FloatBox(),\n                next_states=agent.preprocessed_state_space,\n                terminals=IntBox(low=0, high=1),\n                importance_weights=FloatBox(),\n                add_batch_rank=True\n            )\n\n            batch_size = 512 * gpu_count\n            num_samples = 50\n            samples = [batch_space.sample(batch_size) for _ in range(num_samples)]\n\n            times = []\n            throughputs = []\n            for sample in samples:\n                start = time.perf_counter()\n                agent.update(sample)\n                runtime = time.perf_counter() - start\n                times.append(runtime)\n                throughputs.append(batch_size / runtime)\n\n            print(""Throughput: {} samples / s ({}) for {} GPUs"".format(np.mean(throughputs),\n                                                                       np.std(throughputs), gpu_count))\n'"
rlgraph/tests/performance/test_python_memory_performance.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\nimport time\n\nfrom six.moves import xrange as range_\n\nfrom rlgraph import get_distributed_backend\nfrom rlgraph.execution.ray.apex.apex_memory import ApexMemory\nfrom rlgraph.execution.ray.ray_util import ray_compress\nfrom rlgraph.spaces import Dict, BoolBox, FloatBox\n\nif get_distributed_backend() == ""ray"":\n    from ray.rllib.optimizers.replay_buffer import PrioritizedReplayBuffer\n\n\nclass TestPythonMemoryPerformance(unittest.TestCase):\n    record_space = Dict(\n        states=FloatBox(shape=(4,)),\n        actions=FloatBox(shape=(2,)),\n        reward=float,\n        terminals=BoolBox(),\n        add_batch_rank=True\n    )\n\n    # Apex params\n    capacity = 2000000\n    chunksize = 64\n    inserts = 1000000\n\n    # Samples.\n    samples = 10000\n    sample_batch_size = 50\n\n    alpha = 0.6\n    beta = 0.4\n    max_priority = 1.0\n\n    def test_ray_prioritized_replay_insert(self):\n        """"""\n        Tests Ray\'s memory performance.\n        """"""\n        assert get_distributed_backend() == ""ray""\n        memory = PrioritizedReplayBuffer(\n            size=self.capacity,\n            alpha=1.0,\n            clip_rewards=True\n        )\n        # Test individual inserts.\n        records = [self.record_space.sample(size=1) for _ in range_(self.inserts)]\n\n        start = time.monotonic()\n        for record in records:\n            memory.add(\n                obs_t=record[\'states\'],\n                action=record[\'actions\'],\n                reward=record[\'reward\'],\n                obs_tp1=record[\'states\'],\n                done=record[\'terminals\'],\n                weight=None\n            )\n        end = time.monotonic() - start\n        tp = len(records) / end\n        print(\'#### Testing Ray Prioritized Replay memory ####\')\n        print(\'Testing insert performance:\')\n        print(\'Inserted {} separate records, throughput: {} records/s, total time: {} s\'.format(\n            len(records), tp, end\n        ))\n\n        memory = PrioritizedReplayBuffer(\n            size=self.capacity,\n            alpha=1.0,\n            clip_rewards=True\n        )\n\n        # Test chunked inserts -> done via external for loop in Ray.\n        chunks = int(self.inserts / self.chunksize)\n        records = [self.record_space.sample(size=self.chunksize) for _ in range_(chunks)]\n        start = time.monotonic()\n        for chunk in records:\n            for i in range_(self.chunksize):\n                memory.add(\n                    obs_t=chunk[\'states\'][i],\n                    action=chunk[\'actions\'][i],\n                    reward=chunk[\'reward\'][i],\n                    obs_tp1=chunk[\'states\'][i],\n                    done=chunk[\'terminals\'][i],\n                    weight=None\n                )\n        end = time.monotonic() - start\n        tp = len(records) * self.chunksize / end\n        print(\'Testing chunked insert performance:\')\n        print(\'Inserted {} chunks, throughput: {} records/s, total time: {} s\'.format(\n            len(records), tp, end\n        ))\n\n    def test_ray_sampling(self):\n        """"""\n        Tests Ray\'s memory performance.\n        """"""\n        assert get_distributed_backend() == ""ray""\n        memory = PrioritizedReplayBuffer(\n            size=self.capacity,\n            alpha=1.0,\n            clip_rewards=True\n        )\n        records = [self.record_space.sample(size=1) for _ in range_(self.inserts)]\n        for record in records:\n            memory.add(\n                obs_t=ray_compress(record[\'states\']),\n                action=record[\'actions\'],\n                reward=record[\'reward\'],\n                obs_tp1=ray_compress(record[\'states\']),\n                done=record[\'terminals\'],\n                weight=None\n            )\n        start = time.monotonic()\n        for _ in range_(self.samples):\n            batch_tuple = memory.sample(self.sample_batch_size, beta=1.0)\n        end = time.monotonic() - start\n        tp = self.samples / end\n        print(\'#### Testing Ray Prioritized Replay memory ####\')\n        print(\'Testing sampling performance:\')\n        print(\'Sampled {} batches, throughput: {} samples/s, total time: {} s\'.format(\n            self.samples, tp, end\n        ))\n\n    def test_ray_updating(self):\n        """"""\n        Tests Ray\'s memory performance.\n        """"""\n        assert get_distributed_backend() == ""ray""\n        memory = PrioritizedReplayBuffer(\n            size=self.capacity,\n            alpha=1.0,\n            clip_rewards=True\n        )\n        records = [self.record_space.sample(size=1) for _ in range_(self.inserts)]\n        for record in records:\n            memory.add(\n                obs_t=record[\'states\'],\n                action=record[\'actions\'],\n                reward=record[\'reward\'],\n                obs_tp1=record[\'states\'],\n                done=record[\'terminals\'],\n                weight=None\n            )\n        loss_values = [np.random.random(size=self.sample_batch_size) for _ in range_(self.samples)]\n        indices = [np.random.randint(low=0, high=self.inserts, size=self.sample_batch_size) for _\n                   in range_(self.samples)]\n\n        start = time.monotonic()\n        for index, loss in zip(indices, loss_values):\n            memory.update_priorities(index, loss)\n        end = time.monotonic() - start\n        tp = len(indices) / end\n        print(\'#### Testing Ray Prioritized Replay memory ####\')\n        print(\'Testing updating performance:\')\n        print(\'Updates {} loss batches, throughput: {} updates/s, total time: {} s\'.format(\n            len(indices), tp, end\n        ))\n\n    def test_rlgraph_apex_insert(self):\n        """"""\n        Tests RLgraph\'s python memory performance.\n        """"""\n        memory = ApexMemory(\n            capacity=self.capacity,\n            alpha=1.0\n        )\n        # Testing insert performance\n        records = [self.record_space.sample(size=1) for _ in range(self.inserts)]\n\n        start = time.monotonic()\n        for record in records:\n            memory.insert_records((\n                 record[\'states\'],\n                 record[\'actions\'],\n                 record[\'reward\'],\n                 record[\'terminals\'],\n                 None\n            ))\n        end = time.monotonic() - start\n        tp = len(records) / end\n\n        print(\'#### Testing RLGraph python prioritized replay ####\')\n        print(\'Testing insert performance:\')\n        print(\'Inserted {} separate records, throughput: {} records/s, total time: {} s\'.format(\n            len(records), tp, end\n        ))\n\n        memory = ApexMemory(\n            capacity=self.capacity,\n            alpha=1.0\n        )\n        chunks = int(self.inserts / self.chunksize)\n        records = [self.record_space.sample(size=self.chunksize) for _ in range_(chunks)]\n        start = time.monotonic()\n        for chunk in records:\n            for i in range_(self.chunksize):\n                memory.insert_records((\n                    chunk[\'states\'][i],\n                    chunk[\'actions\'][i],\n                    chunk[\'reward\'][i],\n                    chunk[\'terminals\'][i],\n                    None\n                ))\n\n        end = time.monotonic() - start\n        tp = len(records) * self.chunksize / end\n        print(\'Testing chunked insert performance:\')\n        print(\'Inserted {} chunks, throughput: {} records/s, total time: {} s\'.format(\n            len(records), tp, end\n        ))\n\n    def test_rlgraph_sampling(self):\n        """"""\n        Tests RLgraph\'s sampling performance.\n        """"""\n        memory = ApexMemory(\n            capacity=self.capacity,\n            alpha=1.0\n        )\n\n        records = [self.record_space.sample(size=1) for _ in range_(self.inserts)]\n        for record in records:\n            memory.insert_records((\n                 ray_compress(record[\'states\']),\n                 record[\'actions\'],\n                 record[\'reward\'],\n                 record[\'terminals\'],\n                 None\n            ))\n        start = time.monotonic()\n        for _ in range_(self.samples):\n            batch_tuple = memory.get_records(self.sample_batch_size)\n        end = time.monotonic() - start\n        tp = self.samples / end\n        print(\'#### Testing RLGraph Prioritized Replay memory ####\')\n        print(\'Testing sampling performance:\')\n        print(\'Sampled {} batches, throughput: {} batches/s, total time: {} s\'.format(\n            self.samples, tp, end\n        ))\n\n    def test_rlgraph_updating(self):\n        """"""\n        Tests RLGraph\'s memory performance.\n        """"""\n        memory = ApexMemory(\n            capacity=self.capacity,\n            alpha=1.0\n        )\n\n        records = [self.record_space.sample(size=1) for _ in range_(self.inserts)]\n        for record in records:\n            memory.insert_records((\n                 record[\'states\'],\n                 record[\'actions\'],\n                 record[\'reward\'],\n                 record[\'terminals\'],\n                 None\n            ))\n        loss_values = [np.random.random(size=self.sample_batch_size) for _ in range_(self.samples)]\n        indices = [np.random.randint(low=0, high=self.inserts, size=self.sample_batch_size) for _\n                   in range_(self.samples)]\n\n        start = time.monotonic()\n        for index, loss in zip(indices, loss_values):\n            memory.update_records(index, loss)\n        end = time.monotonic() - start\n        tp = len(indices) / end\n        print(\'#### Testing RLGraph Prioritized Replay memory ####\')\n        print(\'Testing updating performance:\')\n        print(\'Updates {} loss batches, throughput: {} updates/s, total time: {} s\'.format(\n            len(indices), tp, end\n        ))\n\n    def test_ray_combined_ops(self):\n        """"""\n        Tests a combined workflow of insert, sample, update on the prioritized replay memory.\n        """"""\n        assert get_distributed_backend() == ""ray""\n        memory = PrioritizedReplayBuffer(\n            size=self.capacity,\n            alpha=1.0,\n            clip_rewards=True\n        )\n        chunksize = 32\n\n        # Test chunked inserts -> done via external for loop in Ray.\n        chunks = int(self.inserts / chunksize)\n        records = [self.record_space.sample(size=chunksize) for _ in range_(chunks)]\n        loss_values = [np.random.random(size=self.sample_batch_size) for _ in range_(chunks)]\n        start = time.monotonic()\n\n        for chunk, loss_values in zip(records, loss_values):\n            # Insert.\n            for i in range_(chunksize):\n                memory.add(\n                    obs_t=ray_compress(chunk[\'states\'][i]),\n                    action=chunk[\'actions\'][i],\n                    reward=chunk[\'reward\'][i],\n                    obs_tp1=ray_compress(chunk[\'states\'][i]),\n                    done=chunk[\'terminals\'][i],\n                    weight=None\n                )\n            # Sample.\n            batch_tuple = memory.sample(self.sample_batch_size, beta=1.0)\n            indices = batch_tuple[-1]\n            # Update\n            memory.update_priorities(indices, loss_values)\n\n        end = time.monotonic() - start\n        tp = len(records) / end\n        print(\'Ray: testing combined insert/sample/update performance:\')\n        print(\'Ran {} combined ops, throughput: {} combined ops/s, total time: {} s\'.format(\n            len(records), tp, end\n        ))\n\n    def test_rlgraph_combined_ops(self):\n        """"""\n        Tests a combined workflow of insert, sample, update on the prioritized replay memory.\n        """"""\n        memory = ApexMemory(\n            capacity=self.capacity,\n            alpha=1.0\n        )\n\n        chunksize = 32\n        chunks = int(self.inserts / chunksize)\n        records = [self.record_space.sample(size=chunksize) for _ in range_(chunks)]\n        loss_values = [np.random.random(size=self.sample_batch_size) for _ in range_(chunks)]\n\n        start = time.monotonic()\n        for chunk, loss_values in zip(records, loss_values):\n            # Each record now is a chunk.\n            for i in range_(chunksize):\n                memory.insert_records((\n                    ray_compress(chunk[\'states\'][i]),\n                    chunk[\'actions\'][i],\n                    chunk[\'reward\'][i],\n                    chunk[\'terminals\'][i],\n                    None\n                ))\n            batch, indices, weights = memory.get_records(self.sample_batch_size)\n            memory.update_records(indices, loss_values)\n\n        end = time.monotonic() - start\n        tp = len(records) / end\n        print(\'RLGraph: Testing combined op performance:\')\n        print(\'Ran {} combined ops, throughput: {} combined ops/s, total time: {} s\'.format(\n            len(records), tp, end\n        ))\n'"
rlgraph/tests/performance/test_single_threaded_dqn.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nfrom rlgraph.agents.dqn_agent import DQNAgent\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.execution.single_threaded_worker import SingleThreadedWorker\n\n\nclass TestSingleThreadedDQN(unittest.TestCase):\n\n    # TODO test on the relevant Atari environments.\n    env = OpenAIGymEnv(gym_env=\'Pong-v0\')\n\n    # TODO define classic atari dqn network.\n    network = list()\n\n    def test_replay_memory_atari_throughput(self):\n        """"""\n        Tests throughput on standard Atari environments using the replay memory.\n        """"""\n        agent = DQNAgent(\n            states_spec=self.env.state_space,\n            action_spec=self.env.action_space,\n            network_spec=self.network,\n            memory_spec=dict(\n                type=\'replay_memory\',\n                capacity=100000,\n                next_states=True\n            )\n        )\n        worker = SingleThreadedWorker(\n            env_spec=lambda: self.env,\n            agent=agent,\n            frameskip=1\n        )\n\n        result = worker.execute_timesteps(num_timesteps=1000000, use_exploration=True)\n        print(\'Agent throughput = {} ops/s\'.format(result[\'ops_per_second\']))\n        print(\'Environment throughput = {} frames/s\'.format(result[\'env_frames_per_second\']))\n\n    def test_prioritized_replay_atari_throughput(self):\n        """"""\n        Tests throughput on standard Atari environments using the prioritized replay memory.\n        """"""\n        agent = DQNAgent(\n            states_spec=self.env.state_space,\n            action_spec=self.env.action_space,\n            network_spec=self.network,\n            memory_spec=dict(\n                type=\'prioritized\',\n                capacity=100000,\n                next_states=True\n            )\n        )\n        worker = SingleThreadedWorker(\n            env_spec=lambda: self.env,\n            agent=agent,\n            frameskip=1\n        )\n\n        result = worker.execute_timesteps(num_timesteps=1000000, use_exploration=True)\n        print(\'Agent throughput = {} ops/s\'.format(result[\'ops_per_second\']))\n        print(\'Environment throughput = {} frames/s\'.format(result[\'env_frames_per_second\']))\n'"
rlgraph/tests/performance/test_tf_memory_performance.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport time\n\nfrom rlgraph.components import ReplayMemory, PrioritizedReplay\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.spaces import Dict, BoolBox, FloatBox, IntBox\nfrom rlgraph.tests import ComponentTest\n\n\nclass TestTfMemoryPerformance(unittest.TestCase):\n    """"""\n    Tests performance of pure TensorFlow memories.\n    """"""\n\n    # Note: Using Atari states here has to be done with care because without preprocessing, these will require\n    # large amount sof memory.\n    env = OpenAIGymEnv(gym_env=\'CartPole-v0\')\n\n    # Inserts.\n    capacity = 100000\n    inserts = 1000\n    enable_profiler = True\n    chunk_size = 64\n\n    # Samples.\n    samples = 1000\n    sample_batch_size = 64\n\n    alpha = 1.0\n    beta = 1.0\n    max_priority = 1.0\n\n    def test_replay(self):\n        """"""\n        Tests individual and chunked insert and sampling performance of replay memory.\n        """"""\n        record_space = Dict(\n            states=self.env.state_space,\n            actions=self.env.action_space,\n            reward=float,\n            terminals=BoolBox(),\n            add_batch_rank=True\n        )\n        input_spaces = dict(\n            insert_records=record_space,\n            get_records=int\n        )\n\n        memory = ReplayMemory(\n            capacity=self.capacity,\n            next_states=True\n        )\n        test = ComponentTest(component=memory, input_spaces=input_spaces, enable_profiler=self.enable_profiler)\n\n        records = [record_space.sample(size=1) for _ in range(self.inserts)]\n        start = time.monotonic()\n        for record in records:\n            test.test((""insert_records"", record), expected_outputs=None)\n        end = time.monotonic() - start\n\n        tp = len(records) / end\n        print(\'#### Testing Replay memory ####\')\n        print(\'Testing insert performance:\')\n        print(\'Inserted {} separate records, throughput: {} records/s, total time: {} s\'.format(\n            len(records), tp, end\n        ))\n\n        record_chunks = [record_space.sample(size=self.chunk_size) for _ in range(self.inserts)]\n        start = time.monotonic()\n        for chunk in record_chunks:\n            test.test((""insert_records"", chunk), expected_outputs=None)\n        end = time.monotonic() - start\n\n        tp = len(record_chunks) * self.chunk_size / end\n        print(\'Inserted {} record chunks of size {}, throughput: {} records/s, total time: {} s\'.format(\n            len(record_chunks), self.chunk_size, tp, end\n        ))\n\n        print(\'Testing sample performance:\')\n        start = time.monotonic()\n        for _ in range(self.samples):\n            test.test((""get_records"", self.sample_batch_size), expected_outputs=None)\n        end = time.monotonic() - start\n        tp = self.samples / end\n\n        print(\'Sampled {} batches of size {}, throughput: {} sample-ops/s, total time: {} s\'.format(\n            self.samples, self.sample_batch_size, tp, end\n        ))\n\n    def test_prioritized_replay(self):\n        """"""\n        Tests individual and chunked insert and sampling performance of prioritized replay memory.\n        """"""\n        record_space = Dict(\n            states=self.env.state_space,\n            actions=self.env.action_space,\n            reward=float,\n            terminals=BoolBox(),\n            add_batch_rank=True\n        )\n        input_spaces = dict(\n            insert_records=record_space,\n            get_records=int,\n            update_records=[IntBox(shape=(), add_batch_rank=True), FloatBox(shape=(), add_batch_rank=True)]\n        )\n\n        memory = PrioritizedReplay(\n            capacity=self.capacity,\n            next_states=True,\n            alpha=self.alpha,\n            beta=self.beta\n        )\n        test = ComponentTest(component=memory, input_spaces=input_spaces, enable_profiler=self.enable_profiler)\n\n        records = [record_space.sample(size=1) for _ in range(self.inserts)]\n        start = time.monotonic()\n        for record in records:\n            test.test((""insert_records"", record), expected_outputs=None)\n        end = time.monotonic() - start\n\n        tp = len(records) / end\n        print(\'#### Testing Prioritized Replay memory ####\')\n        print(\'Testing insert performance:\')\n        print(\'Inserted {} separate records, throughput: {} records/s, total time: {} s\'.format(\n            len(records), tp, end\n        ))\n\n        record_chunks = [record_space.sample(size=self.chunk_size) for _ in range(self.inserts)]\n        start = time.monotonic()\n        for chunk in record_chunks:\n            test.test((""insert_records"", chunk), expected_outputs=None)\n        end = time.monotonic() - start\n\n        tp = len(record_chunks) * self.chunk_size / end\n        print(\'Inserted {} record chunks of size {}, throughput: {} records/s, total time: {} s\'.format(\n            len(record_chunks), self.chunk_size, tp, end\n        ))\n\n        print(\'Testing sample performance:\')\n        start = time.monotonic()\n        for _ in range(self.samples):\n            test.test((""get_records"", self.sample_batch_size), expected_outputs=None)\n        end = time.monotonic() - start\n        tp = self.samples / end\n\n        print(\'Sampled {} batches of size {}, throughput: {} sample-ops/s, total time: {} s\'.format(\n            self.samples, self.sample_batch_size, tp, end\n        ))'"
rlgraph/tests/performance/test_time_rank_folding_performance.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport unittest\n\nimport numpy as np\n\nfrom rlgraph.components.neural_networks.neural_network import NeuralNetwork\nfrom rlgraph.spaces import FloatBox\nfrom rlgraph.tests import ComponentTest, recursive_assert_almost_equal\nfrom rlgraph.tests.test_util import config_from_path\n\n\nclass TestTimeRankFoldingPerformance(unittest.TestCase):\n    """"""\n    Tests whether folding (and unfolding) of a time rank is better for certain NNs.\n    """"""\n    def test_time_rank_folding_for_large_dense_nn(self):\n        vector_dim = 256\n        input_space = FloatBox(shape=(vector_dim,), add_batch_rank=True, add_time_rank=True)\n        base_config = config_from_path(""configs/test_large_dense_nn.json"")\n        neural_net_wo_folding = NeuralNetwork.from_spec(base_config)\n\n        test = ComponentTest(component=neural_net_wo_folding, input_spaces=dict(nn_input=input_space))\n\n        # Pull a large batch+time ranked sample.\n        sample_shape = (256, 200)\n        inputs = input_space.sample(sample_shape)\n\n        start = time.monotonic()\n        runs = 10\n        for _ in range(runs):\n            print(""."", flush=True, end="""")\n            test.test((""call"", inputs), expected_outputs=None)\n        runtime_wo_folding = time.monotonic() - start\n\n        print(""\\nTesting large dense NN w/o time-rank folding: {}x pass through with {}-data took ""\n              ""{}s"".format(runs, sample_shape, runtime_wo_folding))\n\n        neural_net_w_folding = NeuralNetwork.from_spec(base_config)\n\n        # Folded space.\n        input_space_folded = FloatBox(shape=(vector_dim,), add_batch_rank=True)\n        inputs = input_space.sample(sample_shape[0] * sample_shape[1])\n\n        test = ComponentTest(component=neural_net_w_folding, input_spaces=dict(nn_input=input_space_folded))\n\n        start = time.monotonic()\n        for _ in range(runs):\n            print(""."", flush=True, end="""")\n            test.test((""call"", inputs), expected_outputs=None)\n        runtime_w_folding = time.monotonic() - start\n\n        print(""\\nTesting large dense NN w/ time-rank folding: {}x pass through with {}-data took ""\n              ""{}s"".format(runs, sample_shape, runtime_w_folding))\n\n        recursive_assert_almost_equal(runtime_w_folding, runtime_wo_folding, decimals=0)\n\n    def test_time_rank_folding_for_large_cnn_nn(self):\n        width = 86\n        height = 86\n        time_rank = 20\n        input_space = FloatBox(shape=(width, height, 3), add_batch_rank=True, add_time_rank=True, time_major=True)\n        base_config = config_from_path(""configs/test_3x_cnn_nn.json"")\n        base_config.insert(0, {""type"": ""reshape"", ""fold_time_rank"": True})\n        base_config.append({""type"": ""reshape"", ""unfold_time_rank"": time_rank, ""time_major"": True})\n        neural_net = NeuralNetwork.from_spec(base_config)\n\n        test = ComponentTest(component=neural_net, input_spaces=dict(nn_input=input_space))\n\n        # Pull a large batch+time ranked sample.\n        sample_shape = (time_rank, 256)\n        inputs = input_space.sample(sample_shape)\n\n        out = test.test((""call"", inputs), expected_outputs=None)[""output""]\n\n        self.assertTrue(out.shape == (time_rank, 256, 7 * 7 * 64))\n        self.assertTrue(out.dtype == np.float32)\n'"
rlgraph/tests/performance/test_vector_env.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport unittest\n\nfrom six.moves import xrange as range_\n\nfrom rlgraph.agents import Agent\nfrom rlgraph.environments import Environment, SequentialVectorEnv\nfrom rlgraph.tests.test_util import config_from_path\n\n\nclass TestVectorEnv(unittest.TestCase):\n    """"""\n    Tests environment throughput of Vector environment versus simple environment.\n    """"""\n    env_spec = dict(\n        type=""openai"",\n        gym_env=""Pong-v0"",\n        frameskip=4,\n        max_num_noops=30,\n        episodic_life=True\n    )\n\n    samples = 50000\n    num_vector_envs = 4\n\n    def test_individual_env(self):\n        env = Environment.from_spec(self.env_spec)\n        agent = Agent.from_spec(\n            # Uses 2015 DQN parameters as closely as possible.\n            config_from_path(""configs/dqn_agent_for_pong.json""),\n            state_space=env.state_space,\n            # Try with ""reduced"" action space (actually only 3 actions, up, down, no-op)\n            action_space=env.action_space\n        )\n\n        state = env.reset()\n        start = time.monotonic()\n        ep_length = 0\n        for _ in range_(self.samples):\n            action = agent.get_action(state)\n            state, reward, terminal, info = env.step(action)\n\n            ep_length += 1\n            if terminal:\n                print(""reset after {} states"".format(ep_length))\n                env.reset()\n                ep_length = 0\n\n        runtime = time.monotonic() - start\n        tp = self.samples / runtime\n\n        print(\'Testing individual env {} performance:\'.format(self.env_spec[""gym_env""]))\n        print(\'Ran {} steps, throughput: {} states/s, total time: {} s\'.format(\n            self.samples, tp, runtime\n        ))\n\n    def test_sequential_vector_env(self):\n        vector_env = SequentialVectorEnv(\n            num_environments=self.num_vector_envs,\n            env_spec=self.env_spec,\n            num_background_envs=2\n        )\n        agent = Agent.from_spec(\n            # Uses 2015 DQN parameters as closely as possible.\n            config_from_path(""configs/dqn_vector_env.json""),\n            state_space=vector_env.state_space,\n            # Try with ""reduced"" action space (actually only 3 actions, up, down, no-op)\n            action_space=vector_env.action_space\n        )\n\n        states = vector_env.reset_all()\n        start = time.monotonic()\n        ep_lengths = [0 for _ in range_(self.num_vector_envs)]\n\n        for _ in range_(int(self.samples/ self.num_vector_envs)):\n            # Sample all envs at once.\n            actions, preprocessed_states = agent.get_action(states, extra_returns=""preprocessed_states"")\n            states, rewards, terminals, infos = vector_env.step(actions)\n            ep_lengths = [ep_length + 1 for ep_length in ep_lengths]\n\n            for i, terminal in enumerate(terminals):\n                if terminal:\n                    print(""reset env {} after {} states"".format(i, ep_lengths[i]))\n                    vector_env.reset(i)\n                    ep_lengths[i] = 0\n\n        runtime = time.monotonic() - start\n        tp = self.samples / runtime\n\n        print(\'Testing vector env {} performance:\'.format(self.env_spec[""gym_env""]))\n        print(\'Ran {} steps, throughput: {} states/s, total time: {} s\'.format(\n            self.samples, tp, runtime\n        ))\n'"
rlgraph/tests/visualization/__init__.py,0,"b'# Copyright 2018/2019 The Rlgraph Authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n'"
rlgraph/tests/visualization/test_visualizations.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport unittest\n\nfrom rlgraph import rlgraph_dir\nfrom rlgraph.agents.ppo_agent import PPOAgent\nfrom rlgraph.environments import GridWorld\nfrom rlgraph.tests.test_util import config_from_path\nfrom rlgraph.utils import root_logger\nfrom rlgraph.utils.rlgraph_errors import RLGraphError, RLGraphSpaceError\nfrom rlgraph.utils.visualization_util import draw_meta_graph\n\n\nclass TestVisualizations(unittest.TestCase):\n    """"""\n    Tests whether components and meta-(sub)-graphs get visualized properly.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    def test_ppo_agent_visualization(self):\n        """"""\n        Creates a PPOAgent and visualizes meta-graph (no APIs) and the NN-component.\n        """"""\n        env = GridWorld(world=""2x2"")\n        env.render()\n        ppo_agent = PPOAgent.from_spec(\n            config_from_path(""configs/ppo_agent_for_2x2_gridworld.json""),\n            state_space=GridWorld.grid_world_2x2_flattened_state_space,\n            action_space=env.action_space\n        )\n\n        # Test graphviz component-graph drawing.\n        draw_meta_graph(ppo_agent.root_component, output=rlgraph_dir + ""/ppo.gv"", apis=False, graph_fns=False)\n        self.assertTrue(os.path.isfile(rlgraph_dir + ""/ppo.gv""))\n        # Test graphviz component-graph w/ API drawing (only the Policy component).\n        draw_meta_graph(ppo_agent.policy.neural_network, output=rlgraph_dir + ""/ppo_nn.gv"", apis=True)\n        self.assertTrue(os.path.isfile(rlgraph_dir + ""/ppo_nn.gv""))\n\n    def test_ppo_agent_faulty_op_visualization(self):\n        """"""\n        Creates a PPOAgent with a badly connected network and visualizes the root component.\n        """"""\n        agent_config = config_from_path(""configs/ppo_agent_for_2x2_gridworld.json"")\n        # Sabotage the NN.\n        agent_config[""network_spec""] = [\n            {""type"": ""dense"", ""units"": 10},\n            {""type"": ""embedding"", ""embed_dim"": 3, ""vocab_size"": 4}\n        ]\n        env = GridWorld(world=""2x2"")\n        # Build Agent and hence trigger the Space error.\n        try:\n            ppo_agent = PPOAgent.from_spec(\n                agent_config,\n                state_space=GridWorld.grid_world_2x2_flattened_state_space,\n                action_space=env.action_space\n            )\n        except RLGraphSpaceError as e:\n            print(""Seeing expected RLGraphSpaceError ({}). Test ok."".format(e))\n        else:\n            raise RLGraphError(""Not seeing expected RLGraphSpaceError with faulty input Space to embed layer of PPO!"")\n'"
contrib/bitflip_env/rlgraph/environments/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.environments.environment import Environment\nfrom rlgraph.environments.deterministic_env import DeterministicEnv\nfrom rlgraph.environments.grid_world import GridWorld\nfrom rlgraph.environments.openai_gym import OpenAIGymEnv\nfrom rlgraph.environments.random_env import RandomEnv\nfrom rlgraph.environments.vector_env import VectorEnv\nfrom rlgraph.environments.sequential_vector_env import SequentialVectorEnv\nimport contrib.bitflip_env.rlgraph.environments.custom.openai.envs\n\nEnvironment.__lookup_classes__ = dict(\n    deterministic=DeterministicEnv,\n    deterministicenv=DeterministicEnv,\n    gridworld=GridWorld,\n    openai=OpenAIGymEnv,\n    openaigymenv=OpenAIGymEnv,\n    openaigym=OpenAIGymEnv,\n    randomenv=RandomEnv,\n    random=RandomEnv,\n    sequentialvector=SequentialVectorEnv,\n    sequentialvectorenv=SequentialVectorEnv\n)\n\ntry:\n    import deepmind_lab\n\n    # If import works: Can import our Adapter.\n    from rlgraph.environments.deepmind_lab import DeepmindLabEnv\n\n    Environment.__lookup_classes__.update(dict(\n        deepmindlab=DeepmindLabEnv,\n        deepmindlabenv=DeepmindLabEnv,\n    ))\n    # TODO travis error on this, investigate.\nexcept Exception:\n    pass\n\n__all__ = [""Environment""] + \\\n          list(set(map(lambda x: x.__name__, Environment.__lookup_classes__.values())))\n'"
contrib/bitflip_env/rlgraph/examples/random_bitflip.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nExample script for training a DQN agent on an OpenAI gym environment.\n\nUsage:\n\npython dqn_cartpole_with_tf_summaries.py [--config configs/dqn_cartpole.json] [--env CartPole-v0]\n\n```\n# Run script\npython dqn_cartpole_with_tf_summaries.py\n```\n""""""\n\nimport json\nimport os\nimport sys\n\nimport numpy as np\nfrom absl import flags\n\nfrom contrib.bitflip_env.rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.agents import Agent\nfrom rlgraph.execution import SingleThreadedWorker\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'config\', \'./configs/random_bitflip.json\', \'Agent config file.\')\nflags.DEFINE_string(\'env\', \'bitflip-v0\', \'gym environment ID.\')\nflags.DEFINE_boolean(\'render\', True, \'Render the environment.\')\nflags.DEFINE_integer(\'episodes\', 200, \'Number of training episodes.\')\n\n\ndef main(argv):\n    try:\n        FLAGS(argv)\n    except flags.Error as e:\n        print(\'%s\\\\nUsage: %s ARGS\\\\n%s\' % (e, sys.argv[0], FLAGS))\n\n    agent_config_path = os.path.join(os.getcwd(), FLAGS.config)\n    with open(agent_config_path, \'rt\') as fp:\n        agent_config = json.load(fp)\n\n    env = OpenAIGymEnv.from_spec({\n        ""type"": ""openai"",\n        ""gym_env"": FLAGS.env\n    })\n\n    agent = Agent.from_spec(\n        agent_config,\n        state_space=env.state_space,\n        action_space=env.action_space\n    )\n\n    episode_returns = []\n\n    def episode_finished_callback(episode_return, duration, timesteps, **kwargs):\n        episode_returns.append(episode_return)\n        if len(episode_returns) % 10 == 0:\n            print(""Episode {} finished: reward={:.2f}, average reward={:.2f}."".format(\n                len(episode_returns), episode_return, np.mean(episode_returns[-10:])\n            ))\n\n    worker = SingleThreadedWorker(env_spec=lambda: env, agent=agent, render=FLAGS.render,\n                                  worker_executes_preprocessing=False,\n                                  episode_finish_callback=episode_finished_callback)\n    print(""Starting workload, this will take some time for the agents to build."")\n    results = worker.execute_episodes(FLAGS.episodes)\n\n    print(""Mean reward: {:.2f} / over the last 10 episodes: {:.2f}"".format(\n        np.mean(episode_returns), np.mean(episode_returns[-10:])\n    ))\n\n\nif __name__ == \'__main__\':\n    main(sys.argv)\n'"
rlgraph/components/layers/nn/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.layers.nn.concat_layer import ConcatLayer\nfrom rlgraph.components.layers.nn.conv2d_layer import Conv2DLayer\nfrom rlgraph.components.layers.nn.conv2d_transpose_layer import Conv2DTransposeLayer\nfrom rlgraph.components.layers.nn.dense_layer import DenseLayer\nfrom rlgraph.components.layers.nn.local_response_normalization_layer import LocalResponseNormalizationLayer\nfrom rlgraph.components.layers.nn.lstm_layer import LSTMLayer\nfrom rlgraph.components.layers.nn.maxpool2d_layer import MaxPool2DLayer\nfrom rlgraph.components.layers.nn.multi_lstm_layer import MultiLSTMLayer\nfrom rlgraph.components.layers.nn.nn_layer import NNLayer\nfrom rlgraph.components.layers.nn.residual_layer import ResidualLayer\n\nNNLayer.__lookup_classes__ = dict(\n    concat=ConcatLayer,\n    concatlayer=ConcatLayer,\n    conv2d=Conv2DLayer,\n    conv2dlayer=Conv2DLayer,\n    conv2dtranspose=Conv2DTransposeLayer,\n    conv2dtransposelayer=Conv2DTransposeLayer,\n    dense=DenseLayer,\n    denselayer=DenseLayer,\n    fc=DenseLayer,\n    fclayer=DenseLayer,\n    lstm=LSTMLayer,\n    lstmlayer=LSTMLayer,\n    maxpool2d=MaxPool2DLayer,\n    maxpool2dlayer=MaxPool2DLayer,\n    multilstm=MultiLSTMLayer,\n    multilstmlayer=MultiLSTMLayer,\n    residual=ResidualLayer,\n    residuallayer=ResidualLayer,\n    localresponsenormalization=LocalResponseNormalizationLayer,\n    localresponsenormalizationlayer=LocalResponseNormalizationLayer\n)\n\n__all__ = [""NNLayer""] + list(set(map(lambda x: x.__name__, NNLayer.__lookup_classes__.values())))\n'"
rlgraph/components/layers/nn/activation_functions.py,12,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom functools import partial\n\nfrom rlgraph import get_backend\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\n\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch.nn as nn\n\n\ndef get_activation_function(activation_function=None, *other_parameters):\n    """"""\n    Returns an activation function (callable) to use in a NN layer.\n\n    Args:\n        activation_function (Optional[callable,str]): The activation function to lookup. Could be given as:\n            - already a callable (return just that)\n            - a lookup key (str)\n            - None: Use linear activation.\n\n        other_parameters (any): Possible extra parameter(s) used for some of the activation functions.\n\n    Returns:\n        callable: The backend-dependent activation function.\n    """"""\n    if get_backend() == ""tf"":\n        if activation_function is None or callable(activation_function):\n            return activation_function\n        elif activation_function == ""linear"":\n            return tf.identity\n        # Rectifier linear unit (ReLU) : 0 if x < 0 else x\n        elif activation_function == ""relu"":\n            return tf.nn.relu\n        # Exponential linear: exp(x) - 1 if x < 0 else x\n        elif activation_function == ""elu"":\n            return tf.nn.elu\n        # Sigmoid: 1 / (1 + exp(-x))\n        elif activation_function == ""sigmoid"":\n            return tf.sigmoid\n        # Scaled exponential linear unit: scale * [alpha * (exp(x) - 1) if < 0 else x]\n        # https://arxiv.org/pdf/1706.02515.pdf\n        elif activation_function == ""selu"":\n            return tf.nn.selu\n        # Swish function: x * sigmoid(x)\n        # https://arxiv.org/abs/1710.05941\n        elif activation_function == ""swish"":\n            return lambda x: x * tf.sigmoid(x=x)\n        # Leaky ReLU: x * [alpha if x < 0 else 1.0]\n        elif activation_function in [""lrelu"", ""leaky_relu""]:\n            alpha = other_parameters[0] if len(other_parameters) > 0 else 0.2\n            return partial(tf.nn.leaky_relu, alpha=alpha)\n        # Concatenated ReLU:\n        elif activation_function == ""crelu"":\n            return tf.nn.crelu\n        # Softmax function:\n        elif activation_function == ""softmax"":\n            return tf.nn.softmax\n        # Softplus function:\n        elif activation_function == ""softplus"":\n            return tf.nn.softplus\n        # Softsign function:\n        elif activation_function == ""softsign"":\n            return tf.nn.softsign\n        # tanh activation function:\n        elif activation_function == ""tanh"":\n            return tf.nn.tanh\n        else:\n            raise RLGraphError(""ERROR: Unknown activation_function \'{}\' for TensorFlow backend!"".\n                               format(activation_function))\n    elif get_backend() == ""pytorch"":\n        # Have to instantiate objects here.\n        if activation_function is None or callable(activation_function):\n            return activation_function\n        elif activation_function == ""linear"":\n            # Do nothing.\n            return None\n        # Rectifier linear unit (ReLU) : 0 if x < 0 else x\n        elif activation_function == ""relu"":\n            return nn.ReLU()\n        # Exponential linear: exp(x) - 1 if x < 0 else x\n        elif activation_function == ""elu"":\n            return nn.ELU()\n        # Sigmoid: 1 / (1 + exp(-x))\n        elif activation_function == ""sigmoid"":\n            return nn.Sigmoid()\n        # Scaled exponential linear unit: scale * [alpha * (exp(x) - 1) if < 0 else x]\n        # https://arxiv.org/pdf/1706.02515.pdf\n        elif activation_function == ""selu"":\n            return nn.SELU()\n        # Leaky ReLU: x * [alpha if x < 0 else 1.0]\n        elif activation_function in [""lrelu"", ""leaky_relu""]:\n            alpha = other_parameters[0] if len(other_parameters) > 0 else 0.2\n            return nn.LeakyReLU(negative_slope=alpha)\n        # Softmax function:\n        elif activation_function == ""softmax"":\n            return nn.Softmax()\n        # Softplus function:\n        elif activation_function == ""softplus"":\n            return nn.Softplus()\n        # Softsign function:\n        elif activation_function == ""softsign"":\n            return nn.Softsign()\n        # tanh activation function:\n        elif activation_function == ""tanh"":\n            return nn.Tanh()\n        else:\n            raise RLGraphError(""ERROR: Unknown activation_function \'{}\' for PyTorch backend!"".\n                               format(activation_function))\n'"
rlgraph/components/layers/nn/concat_layer.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.nn.nn_layer import NNLayer\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.util import force_list\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass ConcatLayer(NNLayer):\n    """"""\n    A simple concatenation layer wrapper. The ConcatLayer is a Layer without sub-components but with n\n    api_methods and 1 output, where input data is concatenated into one output by its GraphFunction.\n    """"""\n    def __init__(self, axis=-1, dict_keys=None, scope=""concat-layer"", **kwargs):\n        """"""\n        Args:\n            axis (int): The axis along which to concatenate. Use negative numbers to count from end.\n                All api_methods to this layer must have the same shape, except for the `axis` rank.\n                Default: -1.\n\n            dict_keys (Optional[List[str]]): An optional list of dict keys to use to retrieve input data from an\n                incoming dict (instead of a tuple series of inputs).\n        """"""\n        super(ConcatLayer, self).__init__(scope=scope, **kwargs)\n\n        self.axis = axis\n        self.dict_keys = dict_keys\n\n        # Whether input spaces are time-major or not.\n        self.time_major = None\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        super(ConcatLayer, self).check_input_spaces(input_spaces, action_space)\n        # Make sure all inputs have the same shape except for the last rank.\n        if self.dict_keys:\n            self.in_space_0 = input_spaces[""inputs[0]""][self.dict_keys[0]]\n        else:\n            self.in_space_0 = input_spaces[""inputs[0]""]\n        self.time_major = self.in_space_0.time_major\n        # Loop through either all args or all kwargs Spaces.\n        idx = 0\n        while self.dict_keys is None or len(self.dict_keys) > idx:\n            if self.dict_keys is None:\n                key = ""inputs[{}]"".format(idx)\n                if key not in input_spaces:\n                    break\n                in_space = input_spaces[key]\n            else:\n                in_space = input_spaces[""inputs[0]""][self.dict_keys[idx]]\n\n            # Make sure the shapes match (except for last rank).\n            assert self.in_space_0.shape[:-1] == in_space.shape[:-1], \\\n                ""ERROR: Input spaces to ConcatLayer must have same shape except for last rank! "" \\\n                ""0th input\'s shape is {}, but {}st input\'s shape is {} (all shapes here are without "" \\\n                ""batch/time-ranks)."".format(self.in_space_0.shape, idx, in_space.shape)\n            idx += 1\n\n    @rlgraph_api\n    def _graph_fn_call(self, *inputs):\n        # Simple translation from dict to tuple-input.\n        if self.dict_keys is not None:\n            inputs = [inputs[0][key] for key in self.dict_keys]\n\n        if get_backend() == ""tf"":\n            concat_output = tf.concat(values=inputs, axis=self.axis)\n            # Add batch/time-rank information.\n            concat_output._batch_rank = 0 if self.time_major is False else 1\n            if self.in_space_0.has_time_rank:\n                concat_output._time_rank = 0 if self.time_major is True else 1\n            return concat_output\n        elif get_backend() == ""pytorch"":\n            return torch.cat(force_list(inputs))\n'"
rlgraph/components/layers/nn/conv2d_layer.py,3,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.nn.activation_functions import get_activation_function\nfrom rlgraph.components.layers.nn.nn_layer import NNLayer\nfrom rlgraph.utils import PyTorchVariable\nfrom rlgraph.utils.initializer import Initializer\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch.nn as nn\n    from rlgraph.utils.pytorch_util import get_input_channels, SamePaddedConv2d\n\n\nclass Conv2DLayer(NNLayer):\n    """"""\n    A Conv2D NN-layer.\n    """"""\n    def __init__(self, filters, kernel_size, strides, padding=""valid"", data_format=""channels_last"",\n                 kernel_spec=None, biases_spec=None, **kwargs):\n        """"""\n        Args:\n            filters (int): The number of filters to produce in the channel-rank.\n            kernel_size (Union[int,Tuple[int]]): The height and width (or one value for both) of the 2D convolution\n                sliding window.\n            strides (Union[int,Tuple[int]]): Kernel stride size along height and width axis (or one value\n                for both directions).\n            padding (str): One of \'valid\' or \'same\'. Default: \'valid\'.\n            data_format (str): One of \'channels_last\' (default) or \'channels_first\'. Specifies which rank (first or\n                last) is the color-channel. If the input Space is with batch, the batch always has the first rank.\n            kernel_spec (any): A specifier for the kernel-weights initializer. Use None for the default initializer.\n                Default: None.\n            bias_spec (any): A specifier for the biases-weights initializer. Use None for the default initializer.\n                If False, uses no biases. Default: False.\n\n            # TODO: regularization specs\n        """"""\n        super(Conv2DLayer, self).__init__(scope=kwargs.pop(""scope"", ""conv-2d""), **kwargs)\n\n        self.filters = filters\n        self.kernel_size = kernel_size if isinstance(kernel_size, (tuple, list)) else (kernel_size, kernel_size)\n        self.strides = strides if isinstance(strides, (tuple, list)) else (strides, strides)\n        self.padding = padding\n        self.data_format = data_format\n        self.kernel_spec = kernel_spec\n        self.biases_spec = biases_spec\n\n        # At model-build time.\n        self.kernel_init = None\n        self.biases_init = None\n\n    def create_variables(self, input_spaces, action_space=None):\n        in_space = input_spaces[""inputs[0]""]\n\n        # Create kernel and biases initializers.\n        self.kernel_init = Initializer.from_spec(shape=self.kernel_size, specification=self.kernel_spec)\n        self.biases_init = Initializer.from_spec(shape=self.kernel_size, specification=self.biases_spec)\n\n        # Wrapper for backend.\n        if get_backend() == ""tf"":\n            self.layer = tf.layers.Conv2D(\n                filters=self.filters,\n                kernel_size=self.kernel_size,\n                strides=self.strides,\n                padding=self.padding,\n                data_format=self.data_format,\n                activation=get_activation_function(self.activation, *self.activation_params),\n                use_bias=(self.biases_spec is not False),\n                kernel_initializer=self.kernel_init.initializer,\n                bias_initializer=(self.biases_init.initializer or tf.zeros_initializer()),\n                trainable=(False if self.trainable is False else True),\n                _reuse=tf.AUTO_REUSE\n            )\n\n            # Now build the layer so that its variables get created.\n            self.layer.build(in_space.get_shape(with_batch_rank=True))\n            # Register the generated variables with our registry.\n            self.register_variables(*self.layer.variables)\n        elif get_backend() == ""pytorch"":\n            shape = in_space.shape\n            num_channels = get_input_channels(shape)\n            apply_bias = (self.biases_spec is not False)\n\n            # print(""Defining conv2d layer with shape = {} and channels {}"".format(\n            #     shape, num_channels\n            # ))\n            if self.padding == ""same"":\n                # N.b. there is no \'same\' or \'valid\' padding for PyTorch so need custom layer.\n                self.layer = SamePaddedConv2d(\n                    in_channels=num_channels,\n                    out_channels=self.filters,\n                    # Only support square kernels.\n                    kernel_size=self.kernel_size[0],\n                    stride=self.strides,\n                    bias=apply_bias\n                )\n            else:\n                self.layer = nn.Conv2d(\n                    in_channels=num_channels,\n                    out_channels=self.filters,\n                    kernel_size=self.kernel_size,\n                    stride=self.strides,\n                    padding=0,\n                    bias=apply_bias\n                )\n            # Apply weight initializer\n            if self.kernel_init.initializer is not None:\n                # Must be a callable in PyTorch\n                self.kernel_init.initializer(self.layer.weight)\n            if apply_bias:\n                if self.biases_spec is not None and self.biases_init.initializer is not None:\n                    self.biases_init.initializer(self.layer.bias)\n                else:\n                    # Fill with zeros.\n                    self.layer.bias.data.fill_(0)\n            if self.activation is not None:\n                # Activation function will be used in `call`.\n                self.activation_fn = get_activation_function(self.activation, *self.activation_params)\n            self.register_variables(PyTorchVariable(name=self.global_scope, ref=self.layer))\n\n'"
rlgraph/components/layers/nn/conv2d_transpose_layer.py,3,"b'# Copyright 2018/2019 ducandu GmbH, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.nn.activation_functions import get_activation_function\nfrom rlgraph.components.layers.nn.nn_layer import NNLayer\nfrom rlgraph.utils import PyTorchVariable, RLGraphError\nfrom rlgraph.utils.initializer import Initializer\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch.nn as nn\n    from rlgraph.utils.pytorch_util import get_input_channels\n\n\nclass Conv2DTransposeLayer(NNLayer):\n    """"""\n    A Conv2D NN-layer.\n    """"""\n    def __init__(self, filters, kernel_size, strides, padding=""valid"", data_format=""channels_last"",\n                 kernel_spec=None, biases_spec=None, **kwargs):\n        """"""\n        Args:\n            filters (int): The number of filters to produce in the channel-rank.\n            kernel_size (Union[int,Tuple[int]]): The height and width (or one value for both) of the 2D convolution\n                sliding window.\n            strides (Union[int,Tuple[int]]): Kernel stride size along height and width axis (or one value\n                for both directions).\n            padding (str): One of \'valid\' or \'same\'. Default: \'valid\'.\n            data_format (str): One of \'channels_last\' (default) or \'channels_first\'. Specifies which rank (first or\n                last) is the color-channel. If the input Space is with batch, the batch always has the first rank.\n            kernel_spec (any): A specifier for the kernel-weights initializer. Use None for the default initializer.\n                Default: None.\n            bias_spec (any): A specifier for the biases-weights initializer. Use None for the default initializer.\n                If False, uses no biases. Default: False.\n\n            # TODO: regularization specs\n        """"""\n        super(Conv2DTransposeLayer, self).__init__(scope=kwargs.pop(""scope"", ""conv-2d-transpose""), **kwargs)\n\n        self.filters = filters\n        self.kernel_size = kernel_size if isinstance(kernel_size, (tuple, list)) else (kernel_size, kernel_size)\n        self.strides = strides if isinstance(strides, (tuple, list)) else (strides, strides)\n        self.padding = padding\n        self.data_format = data_format\n        self.kernel_spec = kernel_spec\n        self.biases_spec = biases_spec\n\n        # At model-build time.\n        self.kernel_init = None\n        self.biases_init = None\n\n    def create_variables(self, input_spaces, action_space=None):\n        in_space = input_spaces[""inputs[0]""]\n\n        # Create kernel and biases initializers.\n        self.kernel_init = Initializer.from_spec(shape=self.kernel_size, specification=self.kernel_spec)\n        self.biases_init = Initializer.from_spec(shape=self.kernel_size, specification=self.biases_spec)\n\n        # Wrapper for backend.\n        if get_backend() == ""tf"":\n            self.layer = tf.layers.Conv2DTranspose(\n                filters=self.filters,\n                kernel_size=self.kernel_size,\n                strides=self.strides,\n                padding=self.padding,\n                data_format=self.data_format,\n                activation=get_activation_function(self.activation, *self.activation_params),\n                use_bias=(self.biases_spec is not False),\n                kernel_initializer=self.kernel_init.initializer,\n                bias_initializer=(self.biases_init.initializer or tf.zeros_initializer()),\n                trainable=(False if self.trainable is False else True),\n                _reuse=tf.AUTO_REUSE\n            )\n\n            # Now build the layer so that its variables get created.\n            self.layer.build(in_space.get_shape(with_batch_rank=True))\n            # Register the generated variables with our registry.\n            self.register_variables(*self.layer.variables)\n        elif get_backend() == ""pytorch"":\n            shape = in_space.shape\n            num_channels = get_input_channels(shape)\n            apply_bias = (self.biases_spec is not False)\n\n            if self.padding == ""same"":\n                # N.b. there is no \'same\' or \'valid\' padding for PyTorch so need custom layer.\n                raise RLGraphError(""Pytorch nn.ConvTranspose2d does currently not support `same` padding!"")\n            else:\n                self.layer = nn.ConvTranspose2d(\n                    in_channels=num_channels,\n                    out_channels=self.filters,\n                    kernel_size=self.kernel_size,\n                    stride=self.strides,\n                    padding=0,\n                    bias=apply_bias\n                )\n            # Apply weight initializer\n            if self.kernel_init.initializer is not None:\n                # Must be a callable in PyTorch\n                self.kernel_init.initializer(self.layer.weight)\n            if apply_bias:\n                if self.biases_spec is not None and self.biases_init.initializer is not None:\n                    self.biases_init.initializer(self.layer.bias)\n                else:\n                    # Fill with zeros.\n                    self.layer.bias.data.fill_(0)\n            if self.activation is not None:\n                # Activation function will be used in apply.\n                self.activation_fn = get_activation_function(self.activation, *self.activation_params)\n            self.register_variables(PyTorchVariable(name=self.global_scope, ref=self.layer))\n\n'"
rlgraph/components/layers/nn/dense_layer.py,3,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.nn.activation_functions import get_activation_function\nfrom rlgraph.components.layers.nn.nn_layer import NNLayer\nfrom rlgraph.spaces import FloatBox\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils import PyTorchVariable\nfrom rlgraph.utils.initializer import Initializer\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch.nn as nn\n\n\nclass DenseLayer(NNLayer):\n    """"""\n    A dense (or ""fully connected"") NN-layer.\n    """"""\n    def __init__(self, units, weights_spec=None, biases_spec=None, **kwargs):\n        """"""\n        Args:\n            units (int): The number of nodes in this layer.\n            weights_spec (any): A specifier for a weights initializer. If None, use the default initializer.\n            biases_spec (any): A specifier for a biases initializer. If False, use no biases. If None,\n                use the default initializer (0.0).\n        """"""\n        super(DenseLayer, self).__init__(scope=kwargs.pop(""scope"", ""dense-layer""), **kwargs)\n\n        self.weights_spec = weights_spec\n        self.biases_spec = biases_spec\n        # At build time.\n        self.weights_init = None\n        self.biases_init = None\n\n        # Number of nodes in this layer.\n        self.units = units\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        super(DenseLayer, self).check_input_spaces(input_spaces, action_space)\n        in_space = input_spaces[""inputs[0]""]\n        # Rank must at least be 2.\n        sanity_check_space(in_space, allowed_types=[FloatBox], rank=(1, None))\n\n    def create_variables(self, input_spaces, action_space=None):\n        in_space = input_spaces[""inputs[0]""]\n\n        # Create weights matrix and (maybe) biases vector.\n        weights_shape = (in_space.shape[0], self.units)\n        self.weights_init = Initializer.from_spec(shape=weights_shape, specification=self.weights_spec)\n        biases_shape = (self.units,)\n        self.biases_init = Initializer.from_spec(shape=biases_shape, specification=self.biases_spec)\n\n        # Wrapper for backend.\n        if get_backend() == ""tf"":\n            self.layer = tf.layers.Dense(\n                units=self.units,\n                activation=get_activation_function(self.activation, *self.activation_params),\n                kernel_initializer=self.weights_init.initializer,\n                use_bias=(self.biases_spec is not False),\n                bias_initializer=(self.biases_init.initializer or tf.zeros_initializer()),\n                trainable=(False if self.trainable is False else True),\n                _reuse=tf.AUTO_REUSE\n            )\n\n            # Now build the layer so that its variables get created.\n            self.layer.build(in_space.get_shape(with_batch_rank=True))\n            # Register the generated variables with our registry.\n            self.register_variables(*self.layer.variables)\n\n        elif get_backend() == ""pytorch"":\n            # N.b. activation must be added as a separate \'layer\' when assembling a network.\n            # In features is the num of input channels.\n            apply_bias = (self.biases_spec is not False)\n            in_features = in_space.shape[1] if in_space.shape[0] == 1 else in_space.shape[0]\n            # print(""name = {}, ndim = {}, in space.shape = {}, in_features = {}, units = {}"".format(\n            #     self.name, ndim, in_space.shape, in_features, self.units))\n            self.layer = nn.Linear(\n                # In case there is a batch dim here due to missing preprocessing.\n                in_features=in_features,\n                out_features=self.units,\n                bias=apply_bias\n            )\n            # Apply weight initializer\n            if self.weights_init.initializer is not None:\n                # Must be a callable in PyTorch\n                self.weights_init.initializer(self.layer.weight)\n            if apply_bias:\n                if self.biases_spec is not None and self.biases_init.initializer is not None:\n                    self.biases_init.initializer(self.layer.bias)\n                else:\n                    # Fill with zeros.\n                    self.layer.bias.data.fill_(0)\n            if self.activation is not None:\n                # Activation function will be used in apply.\n                self.activation_fn = get_activation_function(self.activation, *self.activation_params)\n            # Use unique scope as name.\n            self.register_variables(PyTorchVariable(name=self.global_scope, ref=self.layer))\n'"
rlgraph/components/layers/nn/local_response_normalization_layer.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.nn.nn_layer import NNLayer\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch.nn as nn\n\n\nclass LocalResponseNormalizationLayer(NNLayer):\n    """"""\n    A max-pooling 2D layer.\n    """"""\n    def __init__(self, depth_radius=5, bias=1, alpha=1, beta=0.5, **kwargs):\n        """"""\n        Args:\n            pool_size (Optional[int,Tuple[int,int]]): An int or tuple of 2 ints (height x width) specifying the\n                size of the pooling window. Use a  single integer to specify the same value for all spatial dimensions.\n            strides (Union[int,Tuple[int]]): Kernel stride size along height and width axis (or one value\n                for both directions).\n            padding (str): One of \'valid\' or \'same\'. Default: \'valid\'.\n            data_format (str): One of \'channels_last\' (default) or \'channels_first\'. Specifies which rank (first or\n                last) is the color-channel. If the input Space is with batch, the batch always has the first rank.\n        """"""\n        super(LocalResponseNormalizationLayer, self).__init__(scope=kwargs.pop(""scope"", ""maxpool-2d""), **kwargs)\n\n        self.depth_radius = depth_radius\n        self.bias = bias\n        self.alpha = alpha\n        self.beta = beta\n\n        if get_backend() == ""pytorch"":\n            # Todo: Ensure channels_first?\n            self.layer = nn.LocalReponseNorm(\n                size=self.depth_radius*2,  # The PyTorch implementation divides the size by 2\n                alpha=self.alpha,\n                beta=self.beta,\n                k=self.bias\n            )\n\n    @rlgraph_api\n    def _graph_fn_call(self, *inputs):\n        if get_backend() == ""tf"":\n            result = tf.nn.local_response_normalization(\n                inputs[0], depth_radius=self.depth_radius, bias=self.bias, alpha=self.alpha, beta=self.beta\n            )\n            # TODO: Move into util function.\n            if hasattr(inputs[0], ""_batch_rank""):\n                result._batch_rank = inputs[0]._batch_rank\n            if hasattr(inputs[0], ""_time_rank""):\n                result._time_rank = inputs[0]._time_rank\n            return result\n'"
rlgraph/components/layers/nn/lstm_layer.py,18,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom packaging import version\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.nn.nn_layer import NNLayer\nfrom rlgraph.spaces import Tuple\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils import PyTorchVariable\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.ops import DataOpTuple\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n    import torch.nn as nn\n\n\nclass LSTMLayer(NNLayer):\n    """"""\n    An LSTM layer processing an initial internal state vector and a batch of sequences to produce\n    a final internal state and a batch of output sequences.\n    """"""\n    def __init__(\n            self, units, use_peepholes=False, cell_clip=None, static_loop=False,\n            forget_bias=1.0, parallel_iterations=32, return_sequences=True,\n            swap_memory=False, time_major=False, **kwargs):  # weights_spec=None, dtype=""float""\n        """"""\n        Args:\n            units (int): The number of units in the LSTM cell.\n            use_peepholes (bool): True to enable diagonal/peephole connections from the c-state into each of\n                the layers. Default: False.\n            cell_clip (Optional[float]): If provided, the cell state is clipped by this value prior to the cell\n                output activation. Default: None.\n            static_loop (Union[bool,int]): If an int, will perform a static RNN loop (with fixed sequence lengths\n                of size `static_loop`) instead of a dynamic one (where the lengths for each input can be different).\n                In this case, time_major must be set to True (as transposing for this case has not been automated yet).\n                Default: False.\n            #weights_spec: A specifier for the weight-matrices\' initializers.\n            #If None, use the default initializers.\n            forget_bias (float): The forget gate bias to use. Default: 1.0.\n            parallel_iterations (int): The number of iterations to run in parallel.\n                Default: 32.\n            return_sequences (bool): Whether to return one output for each input or only the last output.\n                Default: True.\n            swap_memory (bool): Transparently swap the tensors produced in forward inference but needed for back\n                prop from GPU to CPU. This allows training RNNs which would typically not fit on a single GPU,\n                with very minimal (or no) performance penalty.\n                Default: False.\n            #time_major (bool): Whether the time rank is the first rank (vs the batch rank).\n            #    Default: False.\n            #dtype (str): The dtype of this LSTM. Default: ""float"".\n        """"""\n        super(LSTMLayer, self).__init__(\n            graph_fn_num_outputs=dict(_graph_fn_call=2),  # LSTMs return: unrolled output, (final c_state & h_state)\n            scope=kwargs.pop(""scope"", ""lstm-layer""), activation=kwargs.pop(""activation"", ""tanh""), **kwargs\n        )\n\n        self.units = units\n        self.time_major = time_major\n        self.use_peepholes = use_peepholes\n        self.cell_clip = cell_clip\n        self.static_loop = static_loop\n        assert self.static_loop is False or (self.static_loop > 0 and self.static_loop is not True), \\\n            ""ERROR: `static_loop` in LSTMLayer must either be False or an int value (is {})!"".format(self.static_loop)\n        # self.weights_spec = weights_spec\n        # self.weights_init = None\n        self.forget_bias = forget_bias\n\n        self.parallel_iterations = parallel_iterations\n        self.return_sequences = return_sequences\n        self.swap_memory = swap_memory\n        self.in_space = None\n\n        # tf RNNCell\n        # torch lstm and hidden state placeholder\n        self.lstm = None\n        self.hidden_state = None\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        super(LSTMLayer, self).check_input_spaces(input_spaces, action_space)\n\n        # Check correct tuple-internal-states format (if not None, in which case we assume all 0.0s).\n        if ""internal_states"" in input_spaces:\n            sanity_check_space(\n                input_spaces[""internal_states""], allowed_types=[Tuple], must_have_batch_rank=True,\n                must_have_time_rank=False\n            )\n            assert len(input_spaces[""internal_states""]) == 2,\\\n                ""ERROR: If internal_states are provided (which is the case), an LSTMLayer requires the len of "" \\\n                ""this Tuple to be 2 (c- and h-states). Your Space is \'{}\'."".format(input_spaces[""internal_states""])\n\n        # Check for batch AND time-rank.\n        self.in_space = input_spaces[""inputs""]\n        sanity_check_space(self.in_space, must_have_batch_rank=True, must_have_time_rank=True)\n\n    def create_variables(self, input_spaces, action_space=None):\n        self.in_space = input_spaces[""inputs""]\n\n        # Create one weight matrix: [input nodes + internal state nodes, 4 (4 internal layers) * internal state nodes]\n        # weights_shape = (in_space.shape[0] + self.units, 4 * self.units)  # [0]=one past batch rank\n        # self.weights_init = Initializer.from_spec(shape=weights_shape, specification=self.weights_spec)\n\n        # Wrapper for backend.\n        if get_backend() == ""tf"":\n            # dtype arg is only supported from 1.13 on.\n            if version.parse(tf.__version__) >= version.parse(""1.13.0""):\n                self.lstm = tf.contrib.rnn.LSTMBlockCell(\n                    num_units=self.units,\n                    use_peephole=self.use_peepholes,\n                    cell_clip=self.cell_clip,\n                    forget_bias=self.forget_bias,\n                    name=""lstm-cell"",\n                    dtype=tf.float32,\n                    reuse=tf.AUTO_REUSE\n                )\n            else:\n                self.lstm = tf.contrib.rnn.LSTMBlockCell(\n                    num_units=self.units,\n                    use_peephole=self.use_peepholes,\n                    cell_clip=self.cell_clip,\n                    forget_bias=self.forget_bias,\n                    name=""lstm-cell"",\n                    reuse=tf.AUTO_REUSE\n                    # TODO: self.trainable needs to be recognized somewhere here.\n                    # These are all not supported yet for LSTMBlockCell (only for the slower LSTMCell)\n                    # initializer=self.weights_init.initializer,\n                    # activation=get_activation_function(self.activation, *self.activation_params),\n                    # dtype=self.dtype,\n                )\n\n            # Now build the layer so that its variables get created.\n            in_space_without_time_rank = list(self.in_space.get_shape(with_batch_rank=True))\n            self.lstm.build(tf.TensorShape(in_space_without_time_rank))\n            # Register the generated variables with our registry.\n            self.register_variables(*self.lstm.variables)\n\n        elif get_backend() == ""pytorch"":\n            self.lstm = nn.LSTM(self.in_space, self.units)\n            self.hidden_state = (torch.zeros(1, 1, self.units), torch.zeros(1, 1, self.units))\n            self.register_variables(PyTorchVariable(name=self.global_scope, ref=self.lstm))\n\n    @rlgraph_api\n    def _graph_fn_call(self, inputs, initial_c_and_h_states=None, sequence_length=None):\n        """"""\n        Args:\n            inputs (SingleDataOp): The data to pass through the layer (batch of n items, m timesteps).\n                Position of batch- and time-ranks in the input depend on `self.time_major` setting.\n\n            initial_c_and_h_states (DataOpTuple): The initial cell- and hidden-states to use.\n                None for the default behavior (all zeros).\n                The cell-state in an LSTM is passed between cells from step to step and only affected by element-wise\n                operations. The hidden state is identical to the output of the LSTM on the previous time step.\n\n            sequence_length (Optional[SingleDataOp]): An int tensor mapping each batch item to a sequence length\n                such that the remaining time slots for each batch item are filled with zeros.\n\n        Returns:\n            tuple:\n                - The outputs over all timesteps of the LSTM.\n                - DataOpTuple: The final cell- and hidden-states.\n        """"""\n        if get_backend() == ""tf"":\n            # Convert to tf\'s LSTMStateTuple from DataOpTuple.\n            if initial_c_and_h_states is not None:\n                initial_c_and_h_states = tf.nn.rnn_cell.LSTMStateTuple(\n                    initial_c_and_h_states[0], initial_c_and_h_states[1]\n                )\n\n            # We are running the LSTM as a dynamic while-loop.\n            if self.static_loop is False:\n                lstm_out, lstm_state_tuple = tf.nn.dynamic_rnn(\n                    cell=self.lstm,\n                    inputs=inputs,\n                    sequence_length=sequence_length,\n                    initial_state=initial_c_and_h_states,\n                    parallel_iterations=self.parallel_iterations,\n                    swap_memory=self.swap_memory,\n                    time_major=self.time_major ,\n                    dtype=tf.float32\n                )\n            # We are running with a fixed number of time steps (static unroll).\n            else:\n                # Set to zeros as tf lstm object does not handle None.\n                if initial_c_and_h_states is None:\n                    shape = (tf.shape(inputs)[0 if self.time_major is False else 1], self.units)\n                    initial_c_and_h_states = tf.nn.rnn_cell.LSTMStateTuple(\n                        tf.zeros(shape=shape, dtype=tf.float32),\n                        tf.zeros(shape=shape, dtype=tf.float32)\n                    )\n                output_list = list()\n                lstm_state_tuple = initial_c_and_h_states\n                # TODO: Add option to reset the internal state in the middle of this loop iff some reset signal\n                # TODO: (e.g. terminal) is True during the loop.\n                inputs.set_shape([self.static_loop] + inputs.shape.as_list()[1:])\n                #for input_, terminal in zip(tf.unstack(inputs), tf.unstack(terminals)):\n                for input_ in tf.unstack(inputs):\n                    # If the episode ended, the core state should be reset before the next.\n                    #core_state = nest.map_structure(functools.partial(tf.where, d),\n                    #                                initial_core_state, core_state)\n                    output, lstm_state_tuple = self.lstm(input_, lstm_state_tuple)\n                    output_list.append(output)\n                lstm_out = tf.stack(output_list)\n\n            # Only return last value.\n            if self.return_sequences is False:\n                if self.time_major is True:\n                    lstm_out = lstm_out[-1]\n                else:\n                    lstm_out = lstm_out[:,-1]\n                lstm_out._batch_rank = 0\n            # Return entire sequence.\n            else:\n                lstm_out._batch_rank = 0 if self.time_major is False else 1\n                lstm_out._time_rank = 0 if self.time_major is True else 1\n\n            # Returns: Unrolled-outputs (time series of all encountered h-states), final c- and h-states.\n            return lstm_out, DataOpTuple(lstm_state_tuple)\n\n        elif get_backend() == ""pytorch"":\n            # TODO init hidden state has to be available at create variable time to use.\n            inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n            # TODO: support `self.return_sequences` = False\n            out, self.hidden_state = self.lstm(inputs, self.hidden_state)\n            return out, DataOpTuple(self.hidden_state)\n'"
rlgraph/components/layers/nn/maxpool2d_layer.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.nn.nn_layer import NNLayer\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch.nn as nn\n\n\nclass MaxPool2DLayer(NNLayer):\n    """"""\n    A max-pooling 2D layer.\n    """"""\n    def __init__(self, pool_size, strides, padding=""valid"", data_format=""channels_last"", **kwargs):\n        """"""\n        Args:\n            pool_size (Optional[int,Tuple[int,int]]): An int or tuple of 2 ints (height x width) specifying the\n                size of the pooling window. Use a  single integer to specify the same value for all spatial dimensions.\n            strides (Union[int,Tuple[int]]): Kernel stride size along height and width axis (or one value\n                for both directions).\n            padding (str): One of \'valid\' or \'same\'. Default: \'valid\'.\n            data_format (str): One of \'channels_last\' (default) or \'channels_first\'. Specifies which rank (first or\n                last) is the color-channel. If the input Space is with batch, the batch always has the first rank.\n        """"""\n        super(MaxPool2DLayer, self).__init__(scope=kwargs.pop(""scope"", ""maxpool-2d""), **kwargs)\n\n        self.pool_size = pool_size if isinstance(pool_size, (tuple, list)) else (pool_size, pool_size)\n        self.strides = strides if isinstance(strides, (tuple, list)) else (strides, strides)\n        self.padding = padding\n        self.data_format = data_format\n\n        if get_backend() == ""pytorch"":\n            self.layer = nn.MaxPool2d(\n                kernel_size=self.pool_size,\n                stride=self.strides,\n                padding=self.padding\n            )\n\n    @rlgraph_api\n    def _graph_fn_call(self, *inputs):\n        if get_backend() == ""tf"":\n            result = tf.nn.pool(\n                inputs[0], window_shape=self.pool_size, pooling_type=""MAX"", padding=self.padding.upper(),\n                strides=self.strides\n            )\n            # TODO: Move into util function.\n            if hasattr(inputs[0], ""_batch_rank""):\n                result._batch_rank = inputs[0]._batch_rank\n            if hasattr(inputs[0], ""_time_rank""):\n                result._time_rank = inputs[0]._time_rank\n            return result\n'"
rlgraph/components/layers/nn/multi_lstm_layer.py,0,"b'# Copyright 2018/2019 ducandu GmbH, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.nn.concat_layer import ConcatLayer\nfrom rlgraph.components.layers.nn.lstm_layer import LSTMLayer\nfrom rlgraph.components.layers.nn.nn_layer import NNLayer\nfrom rlgraph.spaces import Tuple\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api, graph_fn\nfrom rlgraph.utils.ops import DataOpTuple\n\n\nclass MultiLSTMLayer(NNLayer):\n    """"""\n    A multi-LSTM layer processing an initial internal state vector and a batch of sequences to produce\n    a final internal state and a batch of output sequences.\n    """"""\n    def __init__(\n            self, num_lstms, units, use_peepholes=False, cell_clip=None, static_loop=False,\n            forget_bias=1.0, parallel_iterations=32, return_sequences=True,\n            swap_memory=False, skip_connections=None, **kwargs):\n        """"""\n        Args:\n            num_lstms (int): The number of LSTMs to stack deep.\n            units (Union[List[int],int]): The number of units in the different LSTMLayers\' cells.\n            use_peepholes (Union[List[bool],bool]): True to enable diagonal/peephole connections from the c-state into\n                each of the layers. Default: False.\n            cell_clip (Optional[Union[List[float],float]]): If provided, the cell state is clipped by this value prior\n                to the cell output activation. Default: None.\n            static_loop (Union[bool,int]): If an int, will perform a static RNN loop (with fixed sequence lengths\n                of size `static_loop`) instead of a dynamic one (where the lengths for each input can be different).\n                In this case, time_major must be set to True (as transposing for this case has not been automated yet).\n                Default: False.\n            forget_bias (float): The forget gate bias to use. Default: 1.0.\n            parallel_iterations (int): The number of iterations to run in parallel.\n                Default: 32.\n            return_sequences (bool): Whether to return one output for each input or only the last output.\n                Default: True.\n            swap_memory (bool): Transparently swap the tensors produced in forward inference but needed for back\n                prop from GPU to CPU. This allows training RNNs which would typically not fit on a single GPU,\n                with very minimal (or no) performance penalty.\n                Default: False.\n            skip_connections (Optional[List[List[bool]]]): An optional list of lists (2D) of bools indicating the skip\n                connections for the input as well as outputs of each layer and whether these should be concatenated\n                with the ""regular"" input for each layer. ""Regular"" here means the output from the previous layer.\n                Example:\n                A 4-layer LSTM:\n                skip_connections=[\n                    #   x    out0   out1   out2   out3    <- outputs (or x)\n                                                          # layer 0 (never specified, only takes x as input)\n                    [ True,  True, False, False, False],  # layer 1\n                    True (for all outputs)                # layer 2\n                    [ False, False, False, True, False],  # layer 3\n                    ...\n                ]\n                0) Layer0 does not need to be specified (only takes x, obviously).\n                1) Layer1 takes x concatenated with the output of layer0.\n                2) Layer2 takes x and both out0 and out1, all concatenated.\n                3) Layer3 takes only out2 as input.\n                4) A missing sub-list in the main `skip_connections` list means that this layer only takes the previous\n                    layer\'s output (no further skip connections for that layer).\n        """"""\n        super(MultiLSTMLayer, self).__init__(\n            graph_fn_num_outputs=dict(_graph_fn_apply=2),  # LSTMs: unrolled output, final c_state, final h_state\n            scope=kwargs.pop(""scope"", ""multi-lstm-layer""), activation=kwargs.pop(""activation"", ""tanh""), **kwargs\n        )\n\n        self.num_lstms = num_lstms\n        assert self.num_lstms > 1, ""ERROR: Must have more than 1 LSTM layer for MultiLSTMLayer Component!""\n        self.units = units\n        self.use_peepholes = use_peepholes\n        self.cell_clip = cell_clip\n        self.static_loop = static_loop\n        assert self.static_loop is False or (self.static_loop > 0 and self.static_loop is not True), \\\n            ""ERROR: `static_loop` in LSTMLayer must either be False or an int value (is {})!"".format(self.static_loop)\n        self.forget_bias = forget_bias\n\n        self.parallel_iterations = parallel_iterations\n        self.return_sequences = return_sequences\n        self.swap_memory = swap_memory\n        self.skip_connections = skip_connections or [[] for _ in range(num_lstms + 1)]\n\n        self.in_space = None\n\n        # tf RNNCell\n        # torch lstm and hidden state placeholder\n        self.lstms = []\n        # The concat layers to concat together the different skip_connection outputs.\n        self.concat_layers = []\n        self.hidden_states = None\n\n        for i in range(self.num_lstms):\n            # Per layer or global settings?\n            units = self.units[i] if isinstance(self.units, (list, tuple)) else self.units\n            use_peepholes = self.use_peepholes[i] if isinstance(self.use_peepholes, (list, tuple)) else \\\n                self.use_peepholes\n            cell_clip = self.cell_clip[i] if isinstance(self.cell_clip, (list, tuple)) else self.cell_clip\n            forget_bias = self.forget_bias[i] if isinstance(self.forget_bias, (list, tuple)) else self.forget_bias\n            activation = self.activation[i] if isinstance(self.activation, (list, tuple)) else self.activation\n\n            # Generate the single layers.\n            self.lstms.append(LSTMLayer(\n                units=units,\n                use_peepholes=use_peepholes,\n                cell_clip=cell_clip,\n                static_loop=self.static_loop,\n                parallel_iterations=self.parallel_iterations,\n                forget_bias=forget_bias,\n                # Always return sequences except for last layer (there, return whatever the user wants).\n                return_sequences=True if i < self.num_lstms - 1 else self.return_sequences,\n                scope=""lstm-layer-{}"".format(i),\n                swap_memory=self.swap_memory,\n                activation=activation\n            ))\n            self.concat_layers.append(ConcatLayer(scope=""concat-layer-{}"".format(i)))\n\n        self.add_components(*self.lstms)\n        self.add_components(*self.concat_layers)\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        super(MultiLSTMLayer, self).check_input_spaces(input_spaces, action_space)\n\n        # Check correct tuple-internal-states format (if not None, in which case we assume all 0.0s).\n        if ""internal_states"" in input_spaces:\n            # Check that main space is a Tuple (one item for each layer).\n            sanity_check_space(\n                input_spaces[""internal_states""], allowed_types=[Tuple]\n            )\n            # Check that each layer gets a tuple of 2 values: c- and h-states.\n            for i in range(self.num_lstms):\n                sanity_check_space(\n                    input_spaces[""internal_states""][i], allowed_types=[Tuple], must_have_batch_rank=True,\n                    must_have_time_rank=False\n                )\n                assert len(input_spaces[""internal_states""][i]) == 2,\\\n                    ""ERROR: If internal_states are provided (which is the case), an LSTMLayer requires the len of "" \\\n                    ""this Tuple to be 2 (c- and h-states). Your Space is \'{}\'."".\\\n                    format(input_spaces[""internal_states""][i])\n\n        # Check for batch AND time-rank.\n        self.in_space = input_spaces[""inputs""]\n        sanity_check_space(self.in_space, must_have_batch_rank=True, must_have_time_rank=True)\n\n    @rlgraph_api\n    def apply(self, inputs, initial_c_and_h_states=None, sequence_length=None):\n        output, last_internal_states = self._graph_fn_apply(\n            inputs, initial_c_and_h_states=initial_c_and_h_states, sequence_length=sequence_length\n        )\n        return dict(output=output, last_internal_states=last_internal_states)\n\n    @graph_fn\n    def _graph_fn_apply(self, inputs, initial_c_and_h_states=None, sequence_length=None):\n        """"""\n        Args:\n            inputs (SingleDataOp): The data to pass through the layer (batch of n items, m timesteps).\n                Position of batch- and time-ranks in the input depend on `self.time_major` setting.\n\n            initial_c_and_h_states (DataOpTuple): The initial cell- and hidden-states to use.\n                None for the default behavior (all zeros).\n                The cell-state in an LSTM is passed between cells from step to step and only affected by element-wise\n                operations. The hidden state is identical to the output of the LSTM on the previous time step.\n\n            sequence_length (Optional[SingleDataOp]): An int tensor mapping each batch item to a sequence length\n                such that the remaining time slots for each batch item are filled with zeros.\n\n        Returns:\n            tuple:\n                - The outputs over all timesteps of the LSTM.\n                - DataOpTuple: The final cell- and hidden-states.\n        """"""\n        if get_backend() == ""tf"":\n            # Pass through all layers and concat with respective skip-connections each time.\n            last_internal_states = []\n            inputs_ = inputs\n            outputs = [inputs]\n            for i in range(self.num_lstms):\n                output = self.lstms[i].call(\n                    inputs_,\n                    initial_c_and_h_states=initial_c_and_h_states[i] if initial_c_and_h_states is not None else None,\n                    sequence_length=sequence_length\n                )\n                # Store all outputs for possible future skip_connections.\n                outputs.append(output[""output""])\n                # Store current internal states for each layer.\n                last_internal_states.append(output[""last_internal_states""])\n\n                # Concat with previous (skip-connection) outputs?\n                skip_connections = self.skip_connections[i + 1] if len(self.skip_connections) > i + 1 else [True if j == i + 1 else False for j in range(self.num_lstms + 1)]\n                if isinstance(skip_connections, bool):\n                    skip_connections = [skip_connections for _ in range(self.num_lstms + 1)]\n                concat_inputs = [outputs[j] for j, sc in enumerate(skip_connections) if sc is True]\n\n                if len(concat_inputs) > 1:\n                    next_input = self.concat_layers[i].call(*concat_inputs)\n                else:\n                    next_input = concat_inputs[0]\n                # Set input for next layer.\n                inputs_ = next_input\n\n            return inputs_, DataOpTuple(last_internal_states)\n'"
rlgraph/components/layers/nn/nn_layer.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.layer import Layer\nfrom rlgraph.components.layers.nn.activation_functions import get_activation_function\nfrom rlgraph.spaces import FloatBox, IntBox\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api\n\n\nclass NNLayer(Layer):\n    """"""\n    A generic NN-layer object implementing the `call` graph_fn and offering additional activation function support.\n    Can be used in the following ways:\n\n    - Thin wrapper around a backend-specific layer object (normal use case):\n        Create the backend layer in the `create_variables` method and store it under `self.layer`. Then register\n        the backend layer\'s variables with the RLgraph Component.\n\n    - Custom layer (with custom computation):\n        Create necessary variables in `create_variables` (e.g. matrices), then override `_graph_fn_call`, leaving\n        `self.layer` as None.\n\n    - Single Activation Function:\n        Leave `self.layer` as None and do not override `_graph_fn_call`. It will then only apply the activation\n        function.\n    """"""\n    def __init__(self, **kwargs):\n        # Most NN layers have an activation function (some with parameters e.g. leaky ReLU).\n        self.activation = kwargs.pop(""activation"", None)\n        self.activation_params = kwargs.pop(""activation_params"", [])\n\n        # Activation fn for define-by-run execution.\n        self.activation_fn = None\n\n        # The wrapped backend-layer object.\n        self.layer = None\n        self.in_space_0 = None\n        self.time_major = None\n\n        super(NNLayer, self).__init__(scope=kwargs.pop(""scope"", ""nn-layer""), **kwargs)\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        """"""\n        Do some sanity checking on the incoming Space:\n        Must not be Container (for now) and must have a batch rank.\n        """"""\n        super(NNLayer, self).check_input_spaces(input_spaces, action_space)\n        # Make sure all inputs have the same time/batch ranks.\n        # TODO also check spaces for pytorch once unified space management\n        if get_backend() == ""tf"":\n            if ""inputs[0]"" in input_spaces:\n                self.in_space_0 = input_spaces[""inputs[0]""]\n                self.time_major = self.in_space_0.time_major\n                idx = 0\n                while True:\n                    key = ""inputs[{}]"".format(idx)\n                    if key not in input_spaces:\n                        break\n                    sanity_check_space(\n                        input_spaces[key], allowed_sub_types=[FloatBox, IntBox], must_have_batch_rank=True\n                    )\n                    # Make sure all concat inputs have same batch-/time-ranks.\n                    assert self.in_space_0.has_batch_rank == input_spaces[key].has_batch_rank and \\\n                        self.in_space_0.has_time_rank == input_spaces[key].has_time_rank, \\\n                        ""ERROR: Input spaces to \'{}\' must have same batch-/time-rank structure! "" \\\n                        ""0th input is batch-rank={} time-rank={}, but {}st input is batch-rank={} "" \\\n                        ""time-rank={}."".format(\n                            self.global_scope, self.in_space_0.has_batch_rank, input_spaces[key].has_batch_rank, idx,\n                            self.in_space_0.has_time_rank, input_spaces[key].has_time_rank\n                        )\n\n                    idx += 1\n\n    @rlgraph_api\n    def _graph_fn_call(self, *inputs):\n        """"""\n        The actual calculation on one or more input Ops.\n\n        Args:\n            inputs (SingleDataOp): The single (non-container) input(s) to the layer.\n\n        Returns:\n            The output(s) after having pushed input(s) through the layer.\n        """"""\n        # `self.layer` is not given: Only apply the activation function.\n        if self.layer is None:\n            # No activation function.\n            if self.activation is None:\n                return tuple(inputs)\n            # Pass inputs through activation function.\n            else:\n                activation_function = get_activation_function(self.activation, self.activation_params)\n                output = activation_function(*inputs)\n                # TODO: Move into util function.\n                # Add batch-/time-rank flags.\n                output._batch_rank = 0 if self.time_major is False else 1\n                if self.in_space_0 and self.in_space_0.has_time_rank:\n                    output._time_rank = 0 if self.in_space_0.time_major is True else 1\n                return output\n        # `self.layer` already includes activation function details.\n        else:\n            if get_backend() == ""tf"":\n                output = self.layer.call(*inputs)\n                # Add batch-/time-rank flags.\n                output._batch_rank = 0 if self.time_major is False else 1\n                if self.in_space_0 and self.in_space_0.has_time_rank:\n                    output._time_rank = 0 if self.in_space_0.time_major is True else 1\n                return output\n            elif get_backend() == ""pytorch"":\n                # Strip empty internal states:\n                # Ensure inputs are float tensors.\n                input_tensors = []\n                for value in inputs:\n                    if value is not None and hasattr(value, ""float""):\n                        input_tensors.append(value.float())\n                if not input_tensors:\n                    return None\n\n                # Common debug print:\n                # print(""in net work layer: "", self.name)\n                # import torch\n                # shapes = []\n                # for inp in inputs:\n                #     if hasattr(inp, ""shape""):\n                #         shapes.append(inp.shape)\n                #     else:\n                #         shapes.append(type(inp))\n                # print(""input shapes = "", shapes)\n                # PyTorch layers are called, not `applied`.\n                out = self.layer(*input_tensors)\n                # print(""layer output shape = "", out.shape)\n                if self.activation_fn is None:\n                    return out\n                else:\n                    # Apply activation fn.\n                    return self.activation_fn(out)\n'"
rlgraph/components/layers/nn/residual_layer.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.nn.activation_functions import get_activation_function\nfrom rlgraph.components.layers.nn.nn_layer import NNLayer\nfrom rlgraph.utils.decorators import rlgraph_api\n\n\nclass ResidualLayer(NNLayer):\n    """"""\n    A residual layer that adds the input value to some calculation. Based on:\n\n    [1] Identity Mappings in Deep Residual Networks - He, Zhang, Ren and Sun (Microsoft) 2016\n    (https://arxiv.org/pdf/1603.05027.pdf)\n    """"""\n    def __init__(self, residual_unit, repeats=2, scope=""residual-layer"", **kwargs):\n        """"""\n        Args:\n            residual_unit (NeuralNetwork):\n\n            repeats (int): The number of times that the residual unit should be repeated before applying the addition\n                with the original input and the activation function.\n        """"""\n        super(ResidualLayer, self).__init__(scope=scope, **kwargs)\n\n        self.residual_unit = residual_unit\n        self.repeats = repeats\n\n        # Copy the repeat_units n times and add them to this Component.\n        self.residual_units = [self.residual_unit] + [\n            self.residual_unit.copy(scope=self.residual_unit.scope+""-rep""+str(i)) for i in range(repeats - 1)\n        ]\n        self.add_components(*self.residual_units)\n\n    @rlgraph_api\n    def _graph_fn_call(self, inputs):\n        """"""\n        Args:\n            inputs (SingleDataOp): The flattened inputs to this layer.\n\n        Returns:\n            SingleDataOp: The output after passing the input through n times the residual function, then the\n                activation function.\n        """"""\n        if get_backend() == ""tf"":\n            results = inputs\n            # Apply the residual unit n times to the input.\n            for i in range(self.repeats):\n                results = self.residual_units[i].call(results)\n\n            # Then activate and add up.\n            result = results + inputs\n            activation_function = get_activation_function(self.activation, self.activation_params)\n            if activation_function is not None:\n                result = activation_function(result)\n            # TODO: Move into util function.\n            if hasattr(inputs, ""_batch_rank""):\n                result._batch_rank = inputs._batch_rank\n            if hasattr(inputs, ""_time_rank""):\n                result._time_rank = inputs._time_rank\n            return result'"
rlgraph/components/layers/preprocessing/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.layers.preprocessing.clip import Clip\nfrom rlgraph.components.layers.preprocessing.concat import Concat\nfrom rlgraph.components.layers.preprocessing.container_splitter import ContainerSplitter\nfrom rlgraph.components.layers.preprocessing.convert_type import ConvertType\nfrom rlgraph.components.layers.preprocessing.grayscale import GrayScale\nfrom rlgraph.components.layers.preprocessing.image_binary import ImageBinary\nfrom rlgraph.components.layers.preprocessing.image_crop import ImageCrop\nfrom rlgraph.components.layers.preprocessing.image_resize import ImageResize\nfrom rlgraph.components.layers.preprocessing.moving_standardize import MovingStandardize\nfrom rlgraph.components.layers.preprocessing.multiply_divide import Multiply, Divide\nfrom rlgraph.components.layers.preprocessing.normalize import Normalize\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.components.layers.preprocessing.rank_reinterpreter import RankReinterpreter\nfrom rlgraph.components.layers.preprocessing.reshape import ReShape\nfrom rlgraph.components.layers.preprocessing.sequence import Sequence\nfrom rlgraph.components.layers.preprocessing.transpose import Transpose\n\nPreprocessLayer.__lookup_classes__ = dict(\n    clip=Clip,\n    concat=Concat,\n    divide=Divide,\n    grayscale=GrayScale,\n    imagebinary=ImageBinary,\n    converttype=ConvertType,\n    containersplitter=ContainerSplitter,\n    imagecrop=ImageCrop,\n    imageresize=ImageResize,\n    multiply=Multiply,\n    normalize=Normalize,\n    rankreinterpreter=RankReinterpreter,\n    movingstandardize=MovingStandardize,\n    reshape=ReShape,\n    sequence=Sequence,\n    transpose=Transpose,\n)\n\n\n__all__ = [""PreprocessLayer""] + \\\n          list(set(map(lambda x: x.__name__, PreprocessLayer.__lookup_classes__.values())))\n'"
rlgraph/components/layers/preprocessing/clip.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass Clip(PreprocessLayer):\n    """"""\n    A simple clip-by-value layer. Clips each value in the input tensor between `min` and `max`.\n    """"""\n    def __init__(self, min=0.0, max=1.0, scope=""clip"", **kwargs):\n        """"""\n        Args:\n            min (float): The min value that any value in the input can have.\n            max (float): The max value that any value in the input can have.\n        """"""\n        super(Clip, self).__init__(scope=scope, **kwargs)\n        self.min = min\n        self.max = max\n\n    @rlgraph_api\n    def _graph_fn_call(self, inputs):\n        if self.backend == ""python"" or get_backend() == ""python"":\n            return np.clip(inputs, a_min=self.min, a_max=self.max)\n        elif get_backend() == ""tf"":\n            return tf.clip_by_value(t=inputs, clip_value_min=self.min, clip_value_max=self.max)\n\n'"
rlgraph/components/layers/preprocessing/concat.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.utils.util import force_list\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch.nn as nn\n\n\nclass Concat(PreprocessLayer):\n    """"""\n    A simple concatenation layer wrapper. The ConcatLayer is a Layer without sub-components but with n\n    api_methods and 1 output, where the in-Sockets\'s data are concatenated into one out-Socket by its GraphFunction.\n    """"""\n    def __init__(self, axis=-1, scope=""concat"", **kwargs):\n        """"""\n        Args:\n            axis (int): The axis along which to concatenate. Use negative numbers to count from end.\n                All api_methods to this layer must have the same shape, except for the `axis` rank.\n                Default: -1.\n        """"""\n        super(Concat, self).__init__(flatten_ops=True, split_ops=True, scope=scope, **kwargs)\n        self.axis = axis\n\n        # Whether input spaces are time-major or not.\n        self.time_major = None\n\n        # Wrapper for backend.\n        if get_backend() == ""tf"":\n            self.layer = tf.keras.layers.Concatenate(axis=self.axis)\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        #super(ConcatLayer, self).check_input_spaces(input_spaces, action_space)\n        # Make sure all inputs have the same shape except for the last rank.\n        self.in_space_0 = input_spaces[""inputs[0]""]\n        self.time_major = self.in_space_0.time_major\n\n        # TODO: Reinstante these checks from original ConcatLayer (in nn folder) (for container spaces).\n        #idx = 0\n        #while True:\n        #    key = ""inputs[{}]"".format(idx)\n        #    if key not in input_spaces:\n        #        break\n        #    # Make sure the shapes match (except for last rank).\n        #    assert self.in_space_0.shape[:-1] == input_spaces[key].shape[:-1], \\\n        #        ""ERROR: Input spaces to ConcatLayer must have same shape except for last rank! "" \\\n        #        ""0th input\'s shape is {}, but {}st input\'s shape is {} (all shapes here are without "" \\\n        #        ""batch/time-ranks)."".format(self.in_space_0.shape, idx, input_spaces[key].shape)\n        #    idx += 1\n\n    def _graph_fn_call(self, *inputs):\n        if get_backend() == ""tf"":\n            concat_output = self.layer.call(force_list(inputs))\n            # Add batch/time-rank information.\n            concat_output._batch_rank = 0 if self.time_major is False else 1\n            if self.in_space_0.has_time_rank:\n                concat_output._time_rank = 0 if self.time_major is True else 1\n            return concat_output\n        elif get_backend() == ""pytorch"":\n            return nn.Sequential(force_list(inputs))\n'"
rlgraph/components/layers/preprocessing/container_splitter.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.spaces import Dict, Tuple\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\n\n\nclass ContainerSplitter(PreprocessLayer):\n    """"""\n    Splits an incoming ContainerSpace into all its single primitive Spaces.\n    """"""\n    def __init__(self, *output_order, **kwargs):\n        """"""\n        Args:\n            *output_order (Union[str,int]):\n                For Dict splitting:\n                    List of 0th level keys by which the return values of `split` must be sorted.\n                    Example: output_order=[""B"", ""C"", ""A""]\n                    -> split(Dict(A=o1, B=o2, C=o3))\n                    -> return: list(o2, o3, o1), where o1-3 are ops\n                For Tuple splitting:\n                    List of 0th level indices by which the return values of `split` must be sorted.\n                    Example: output_order=[0, 2, 1]\n                    -> split(Tuple(o1, o2, o3))\n                    -> return: list(o1, o3, o2), where o1-3 are ops\n\n        Keyword Args:\n            tuple_length (Optional[int]): If no output_order is given, use this number to hint how many\n                return values our graph_fn has.\n        """"""\n        self.tuple_length = kwargs.pop(""tuple_length"", None)\n        assert self.tuple_length or len(output_order) > 0, \\\n            ""ERROR: one of **kwargs `tuple_length` or `output_order` must be provided in ContainerSplitter "" \\\n            ""(for tuples)!""\n        num_outputs = self.tuple_length or len(output_order)\n\n        super(ContainerSplitter, self).__init__(\n            scope=kwargs.pop(""scope"", ""container-splitter""), graph_fn_num_outputs=dict(_graph_fn_call=num_outputs),\n            **kwargs\n        )\n        self.output_order = output_order\n\n        # Only for DictSplitter, define this convenience API-method:\n        if self.output_order is not None and len(self.output_order) > 0 and isinstance(self.output_order[0], str):\n            @rlgraph_api(component=self)\n            def split_into_dict(self, inputs):\n                """"""\n                Same as `call`, but returns a dict with keys.\n\n                Args:\n                    inputs ():\n\n                Returns:\n\n                """"""\n                out = self._graph_fn_call(inputs)\n                ret = dict()\n                for i, key in enumerate(self.output_order):\n                    ret[key] = out[i]\n                return ret\n\n        # Dict or Tuple?\n        self.type = None\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        in_space = input_spaces[""inputs""]\n\n        self.type = type(in_space)\n        if self.output_order is None or len(self.output_order) == 0:\n            # Auto-ordering only valid for incoming Tuples.\n            assert self.type == Tuple, \\\n                ""ERROR: Cannot use auto-ordering in ContainerSplitter for input Dict spaces! Only ok for Tuples.""\n            self.output_order = list(range(len(in_space)))\n\n        # Make sure input is a Dict (unsorted).\n        assert self.type == Dict or self.type == Tuple,\\\n            ""ERROR: Input Space for ContainerSplitter ({}) must be Dict or Tuple (but is "" \\\n            ""{})!"".format(self.global_scope, in_space)\n\n        # Keys of in_space must all be part of `self.output_order`.\n        for i, name_or_index in enumerate(self.output_order):\n            if self.type == Dict and name_or_index not in in_space:\n                raise RLGraphError(\n                    ""Name #{} in `output_order` ({}) of ContainerSplitter \'{}\'""\n                    "" is not part of the input Space ""\n                    ""({})!"".format(i, name_or_index, self.scope, in_space)\n                )\n            elif self.type == Tuple and name_or_index >= len(in_space):\n                raise RLGraphError(\n                    ""Index #{} in `output_order` (value={}) of ContainerSplitter \'{}\'""\n                    "" is outside the length of the input ""\n                    ""Space ({})!"".format(i, name_or_index, self.scope, in_space)\n                )\n\n    def create_variables(self, input_spaces, action_space=None):\n        in_space = input_spaces[""inputs""]\n        self.type = type(in_space)\n\n    @rlgraph_api\n    def _graph_fn_call(self, inputs):\n        """"""\n        Splits the inputs at 0th level into the Spaces at that level (may still be ContainerSpaces in returned\n        values).\n\n        Args:\n            inputs (DataOpDict): The input Dict/Tuple to be split by its primary keys or along its indices.\n\n        Returns:\n            tuple: The tuple of the sub-Spaces (may still be Containers) sorted by `self.output_order`.\n        """"""\n        ret = [None] * len(self.output_order)\n        if self.type == Dict:\n            for key, value in inputs.items():\n                ret[self.output_order.index(key)] = value\n        else:\n            # No special ordering -> return as is.\n            #if self.output_order is None:\n            #    for index, value in enumerate(inputs):\n            #        ret[index] = value\n            ## Custom re-ordering of the input tuple.\n            #else:\n            for index, value in enumerate(inputs):\n                ret[self.output_order.index(index)] = value\n\n        return tuple(ret)\n'"
rlgraph/components/layers/preprocessing/convert_type.py,5,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.spaces import IntBox, FloatBox, BoolBox, ContainerSpace\nfrom rlgraph.spaces.space_utils import get_space_from_op\nfrom rlgraph.utils import util\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass ConvertType(PreprocessLayer):\n    """"""\n    Converts data types of inputs for static type checking.\n    """"""\n    def __init__(self, to_dtype, scope=""convert-type"", **kwargs):\n        """"""\n        Args:\n            to_dtype (str): Target data type.\n        """"""\n        super(ConvertType, self).__init__(scope=scope, **kwargs)\n        self.to_dtype = to_dtype\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        assert not isinstance(input_spaces, ContainerSpace)\n\n    def get_preprocessed_space(self, space):\n        # TODO map of allowed conversions in utils?\n        if isinstance(space, IntBox):\n            if self.to_dtype == ""float"" or self.to_dtype == ""float32"" or self.to_dtype == ""np.float""\\\n                    or self.to_dtype == ""tf.float32"" or self.to_dtype == ""torch.float32"":\n                return FloatBox(shape=space.shape, low=space.low, high=space.high,\n                                add_batch_rank=space.has_batch_rank, add_time_rank=space.has_time_rank)\n            elif self.to_dtype == ""bool"":\n                if space.low == 0 and space.high == 1:\n                    return BoolBox(shape=space.shape, add_batch_rank=space.has_batch_rank,\n                                   add_time_rank=space.has_time_rank)\n                else:\n                    raise RLGraphError(""ERROR: Conversion from IntBox to BoolBox not allowed if low is not 0 and ""\n                                       ""high is not 1."")\n        elif isinstance(space, BoolBox):\n            if self.to_dtype == ""float"" or self.to_dtype == ""float32"" or self.to_dtype == ""np.float"" \\\n                 or self.to_dtype == ""tf.float32"" or self.to_dtype == ""torch.float32"":\n                return FloatBox(shape=space.shape, low=0.0, high=1.0,\n                                add_batch_rank=space.has_batch_rank, add_time_rank=space.has_time_rank)\n            elif self.to_dtype == ""int"" or self.to_dtype == ""int32"" or self.to_dtype  == ""np.int32"" or \\\n                    self.to_dtype == ""tf.int32"" or self.to_dtype == ""torch.int32"":\n                return IntBox(shape=space.shape, low=0, high=1,\n                              add_batch_rank=space.has_batch_rank, add_time_rank=space.has_time_rank)\n        elif isinstance(space, FloatBox):\n            if self.to_dtype == ""int"" or self.to_dtype == ""int32"" or self.to_dtype  == ""np.int32"" or \\\n                 self.to_dtype == ""tf.int32"" or self.to_dtype == ""torch.int32"":\n                return IntBox(shape=space.shape, low=space.low, high=space.high,\n                              add_batch_rank=space.has_batch_rank, add_time_rank=space.has_time_rank)\n\n        # Wrong conversion.\n        else:\n            raise RLGraphError(""ERROR: Space conversion from: {} to type {} not supported"".format(\n                space, self.to_dtype\n            ))\n\n        # No conversion.\n        return space\n\n    @rlgraph_api\n    def _graph_fn_call(self, inputs):\n        if self.backend == ""python"" or get_backend() == ""python"":\n            if isinstance(inputs, list):\n                inputs = np.asarray(inputs)\n            return inputs.astype(dtype=util.convert_dtype(self.to_dtype, to=""np""))\n        elif get_backend() == ""pytorch"":\n            torch_dtype = util.convert_dtype(self.to_dtype, to=""pytorch"")\n            if torch_dtype == torch.float or torch.float32:\n                return inputs.float()\n            elif torch_dtype == torch.int or torch.int32:\n                return inputs.int()\n            elif torch_dtype == torch.uint8:\n                return inputs.byte()\n        elif get_backend() == ""tf"":\n            in_space = get_space_from_op(inputs)\n            to_dtype = util.convert_dtype(self.to_dtype, to=""tf"")\n            if inputs.dtype != to_dtype:\n                ret = tf.cast(x=inputs, dtype=to_dtype)\n                if in_space.has_batch_rank is True:\n                    ret._batch_rank = 0 if in_space.time_major is False else 1\n                if in_space.has_time_rank is True:\n                    ret._time_rank = 0 if in_space.time_major is True else 1\n                return ret\n            else:\n                return inputs\n'"
rlgraph/components/layers/preprocessing/grayscale.py,3,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cv2\nimport numpy as np\nfrom six.moves import xrange as range_\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.ops import flatten_op, unflatten_op\nfrom rlgraph.utils.util import get_rank, get_shape, convert_dtype as dtype_\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass GrayScale(PreprocessLayer):\n    """"""\n    A simple grayscale converter for RGB images of arbitrary dimensions (normally, an image is 2D).\n\n    [1]: C Kanan, GW Cottrell: Color-to-Grayscale: Does the Method Matter in Image Recognition? - PLOS One (2012)\n    """"""\n    def __init__(self, weights=None, keep_rank=False, scope=""grayscale"", **kwargs):\n        """"""\n        Args:\n            weights (Optional[tuple,list]): A list/tuple of three items indicating the weights to apply to the 3 color\n                channels (RGB).\n            keep_rank (bool): Whether to keep the color-depth rank in the pre-processed tensor (default: False).\n        """"""\n        super(GrayScale, self).__init__(scope=scope, **kwargs)\n\n        # A list of weights used to reduce the last rank (e.g. color rank) of the inputs.\n        self.weights = weights or (0.299, 0.587, 0.114)  # magic RGB-weights for ""natural"" gray-scaling results\n        # The dimension of the last rank (e.g. color rank of the image).\n        self.last_rank = len(self.weights)\n        # Whether to keep the last rank with dim=1.\n        self.keep_rank = keep_rank\n        # The output spaces after preprocessing (per flat-key).\n        self.output_spaces = None\n\n    def get_preprocessed_space(self, space):\n        ret = {}\n        for key, value in space.flatten().items():\n            shape = list(value.shape)\n            if self.keep_rank is True:\n                shape[-1] = 1\n            else:\n                shape.pop(-1)\n            ret[key] = value.__class__(shape=tuple(shape), add_batch_rank=value.has_batch_rank)\n        return unflatten_op(ret)\n\n    def create_variables(self, input_spaces, action_space=None):\n        in_space = input_spaces[""inputs""]\n        self.output_spaces = flatten_op(self.get_preprocessed_space(in_space))\n\n    @rlgraph_api(flatten_ops=True, split_ops=True)\n    def _graph_fn_call(self, inputs):\n        """"""\n        Gray-scales images of arbitrary rank.\n        Normally, the images\' rank is 3 (width/height/colors), but can also be: batch/width/height/colors, or any other.\n        However, the last rank must be of size: len(self.weights).\n\n        Args:\n            inputs (tensor): Single image or a batch of images to be gray-scaled (last rank=n colors, where\n                n=len(self.weights)).\n\n        Returns:\n            DataOp: The op for processing the images.\n        """"""\n        # The reshaped weights used for the grayscale operation.\n        if isinstance(inputs, list):\n            inputs = np.asarray(inputs)\n        images_shape = get_shape(inputs)\n        assert images_shape[-1] == self.last_rank,\\\n            ""ERROR: Given image\'s shape ({}) does not match number of weights (last rank must be {})!"".\\\n            format(images_shape, self.last_rank)\n        if self.backend == ""python"" or get_backend() == ""python"":\n            if inputs.ndim == 4:\n                grayscaled = []\n                for i in range_(len(inputs)):\n                    scaled = cv2.cvtColor(inputs[i], cv2.COLOR_RGB2GRAY)\n                    grayscaled.append(scaled)\n                scaled_images = np.asarray(grayscaled)\n\n                # Keep last dim.\n                if self.keep_rank:\n                    scaled_images = scaled_images[:, :, :, np.newaxis]\n            else:\n                # Sample by sample.\n                scaled_images = cv2.cvtColor(inputs, cv2.COLOR_RGB2GRAY)\n\n            return scaled_images\n        elif get_backend() == ""pytorch"":\n            if len(inputs.shape) == 4:\n                grayscaled = []\n                for i in range_(len(inputs)):\n                    scaled = cv2.cvtColor(inputs[i].numpy(), cv2.COLOR_RGB2GRAY)\n                    grayscaled.append(scaled)\n                scaled_images = np.asarray(grayscaled)\n                # Keep last dim.\n                if self.keep_rank:\n                    scaled_images = scaled_images[:, :, :, np.newaxis]\n            else:\n                # Sample by sample.\n                scaled_images = cv2.cvtColor(inputs.numpy(), cv2.COLOR_RGB2GRAY)\n            return torch.tensor(scaled_images)\n        elif get_backend() == ""tf"":\n            weights_reshaped = np.reshape(\n                self.weights, newshape=tuple([1] * (get_rank(inputs) - 1)) + (self.last_rank,)\n            )\n\n            # Do we need to convert?\n            # The dangerous thing is that multiplying an int tensor (image) with float weights results in an all\n            # 0 tensor).\n            if ""int"" in str(dtype_(inputs.dtype)):\n                weighted = weights_reshaped * tf.cast(inputs, dtype=dtype_(""float""))\n            else:\n                weighted = weights_reshaped * inputs\n\n            reduced = tf.reduce_sum(weighted, axis=-1, keepdims=self.keep_rank)\n\n            # Cast back to original dtype.\n            if ""int"" in str(dtype_(inputs.dtype)):\n                reduced = tf.cast(reduced, dtype=inputs.dtype)\n\n            return reduced\n'"
rlgraph/components/layers/preprocessing/image_binary.py,4,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass ImageBinary(PreprocessLayer):\n    """"""\n    # TODO: Better to move this into grayscale! When needed.\n    A simple binary converter for images of arbitrary dimensions. All non-black pixels are converted to\n    1.0s, all black pixels (all 0.0 in last rank) remain.\n    """"""\n    def __init__(self, threshold=0.0, keep_rank=False, scope=""image-binary"", **kwargs):\n        """"""\n        Args:\n            keep_rank (bool): Whether to keep the color-depth rank in the pre-processed tensor (default: False).\n        """"""\n        super(ImageBinary, self).__init__(scope=scope, **kwargs)\n\n        # The threshold after which the sum of\n        # The dimension of the last rank (e.g. color rank of the image).\n        self.last_rank = None\n        # Whether to keep the last rank with dim=1.\n        self.keep_rank = keep_rank\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        in_space = input_spaces[""inputs""]\n        self.last_rank = in_space.shape[-1]\n\n    @rlgraph_api\n    def _graph_fn_call(self, inputs):\n        """"""\n        Converts the images into binary images by replacing all non-black (at least one channel value is not 0.0)\n        to 1.0 and leaves all black pixels (all channel values 0.0) as-is.\n\n        Args:\n            inputs (tensor): Single image or a batch of images to be converted into a binary image (last rank=n colors,\n                where n=len(self.weights)).\n\n        Returns:\n            DataOp: The op for processing the images.\n        """"""\n        if get_backend() == ""tf"":\n            # Sum over the color channel.\n            color_channel_sum = tf.reduce_sum(input_tensor=inputs, axis=-1, keepdims=self.keep_rank)\n            # Reduce the image to only 0.0 or 1.0.\n            binary_image = tf.where(\n                tf.greater(color_channel_sum, 0.0), tf.ones_like(color_channel_sum),\n                tf.zeros_like(color_channel_sum)\n            )\n            return binary_image\n\n'"
rlgraph/components/layers/preprocessing/image_crop.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.ops import flatten_op, unflatten_op\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass ImageCrop(PreprocessLayer):\n    """"""\n    Crops one or more images to a new size without touching the color channel.\n    """"""\n    def __init__(self, x=0, y=0, width=0, height=0, scope=""image-crop"", **kwargs):\n        """"""\n        Args:\n            x (int): Start x coordinate.\n            y (int): Start y coordinate.\n            width (int): Width of resulting image.\n            height (int): Height of resulting image.\n        """"""\n        super(ImageCrop, self).__init__(scope=scope, **kwargs)\n        self.x = x\n        self.y = y\n        self.width = width\n        self.height = height\n\n        assert self.x >= 0\n        assert self.y >= 0\n        assert self.width > 0\n        assert self.height > 0\n\n        # The output spaces after preprocessing (per flat-key).\n        self.output_spaces = dict()\n\n    def get_preprocessed_space(self, space):\n        ret = dict()\n        for key, value in space.flatten().items():\n            # Do some sanity checking.\n            rank = value.rank\n            assert rank == 2 or rank == 3, \\\n                ""ERROR: Given image\'s rank (which is {}{}, not counting batch rank) must be either 2 or 3!"".\\\n                format(rank, ("""" if key == """" else "" for key \'{}\'"".format(key)))\n            # Determine the output shape.\n            shape = list(value.shape)\n            shape[0] = self.width\n            shape[1] = self.height\n            ret[key] = value.__class__(shape=tuple(shape), add_batch_rank=value.has_batch_rank)\n        return unflatten_op(ret)\n\n    def create_variables(self, input_spaces, action_space=None):\n        in_space = input_spaces[""inputs""]\n\n        self.output_spaces = flatten_op(self.get_preprocessed_space(in_space))\n\n    @rlgraph_api(flatten_ops=True, split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_call(self, key, inputs):\n        """"""\n        Images come in with either a batch dimension or not.\n        """"""\n        if self.backend == ""python"" or get_backend() == ""python"":\n            if isinstance(inputs, list):\n                inputs = np.asarray(inputs)\n            # Preserve batch dimension.\n            if self.output_spaces[key].has_batch_rank is True:\n                return inputs[:, self.y:self.y + self.height, self.x:self.x + self.width]\n            else:\n                return inputs[self.y:self.y + self.height, self.x:self.x + self.width]\n        elif get_backend() == ""pytorch"":\n            if isinstance(inputs, list):\n                inputs = torch.tensor(inputs)\n\n            # TODO: the reason this key check is there is due to call during meta graph build - > out spaces\n            # do not exist yet  -> need better solution.\n            # Preserve batch dimension.\n            if key in self.output_spaces and self.output_spaces[key].has_batch_rank is True:\n                return inputs[:, self.y:self.y + self.height, self.x:self.x + self.width]\n            else:\n                return inputs[self.y:self.y + self.height, self.x:self.x + self.width]\n        elif get_backend() == ""tf"":\n            return tf.image.crop_to_bounding_box(\n                image=inputs,\n                offset_height=self.y,\n                offset_width=self.x,\n                target_height=self.height,\n                target_width=self.width\n            )\n'"
rlgraph/components/layers/preprocessing/image_resize.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cv2\nimport numpy as np\nfrom six.moves import xrange as range_\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.ops import unflatten_op\nfrom rlgraph.utils.rlgraph_errors import RLGraphError\n\ncv2.ocl.setUseOpenCL(False)\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n    from tensorflow.python.ops.image_ops_impl import ResizeMethod\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass ImageResize(PreprocessLayer):\n    """"""\n    Resizes one or more images to a new size without touching the color channel.\n    """"""\n    def __init__(self, width, height, interpolation=""area"", scope=""image-resize"", **kwargs):\n        """"""\n        Args:\n            width (int): The new width.\n            height (int): The new height.\n            interpolation (str): One of ""bilinear"", ""area"". Default: ""bilinear"" (which is also the default for both\n                cv2 and tf).\n        """"""\n        super(ImageResize, self).__init__(scope=scope, **kwargs)\n        self.width = width\n        self.height = height\n        \n        if interpolation == ""bilinear"":\n            if get_backend() == ""tf"":\n                self.tf_interpolation = ResizeMethod.BILINEAR\n                # All other backends use cv2 currently.\n            # Sometimes we mix python preprocessor stack with tf backend -> always need this.\n            self.cv2_interpolation = cv2.INTER_LINEAR\n        elif interpolation == ""area"":\n            if get_backend() == ""tf"":\n                self.tf_interpolation = ResizeMethod.AREA\n            self.cv2_interpolation = cv2.INTER_AREA\n        else:\n            raise RLGraphError(""Invalid interpolation algorithm {}!. Allowed are \'bilinear\' and ""\n                               ""\'area\'."".format(interpolation))\n\n        # The output spaces after preprocessing (per flat-key).\n        self.output_spaces = None\n\n    def get_preprocessed_space(self, space):\n        ## Test sending np samples to get number of return values and output spaces without having to call\n        ## the tf graph_fn.\n        #backend = self.backend\n        #self.backend = ""python""\n        #sample = space.sample(size=1)\n        #out = self._graph_fn_call(sample)\n        #new_space = get_space_from_op(out)\n        #self.backend = backend\n        #return new_space\n\n        ret = dict()\n        for key, value in space.flatten().items():\n            # Do some sanity checking.\n            rank = value.rank\n            if get_backend() == ""tf"":\n                assert rank == 2 or rank == 3, \\\n                    ""ERROR: Given image\'s rank (which is {}{}, not counting batch rank) must be either 2 or 3!"".\\\n                    format(rank, ("""" if key == """" else "" for key \'{}\'"".format(key)))\n                # Determine the output shape.\n                shape = list(value.shape)\n                shape[0] = self.width\n                shape[1] = self.height\n            elif get_backend() == ""pytorch"":\n                shape = list(value.shape)\n\n                # Determine the output shape.\n                if rank == 3:\n                    shape[0] = self.width\n                    shape[1] = self.height\n                elif rank == 4:\n                    # TODO PyTorch shape inference issue.\n                    shape[1] = self.width\n                    shape[2] = self.height\n            ret[key] = value.__class__(shape=tuple(shape), add_batch_rank=value.has_batch_rank)\n        return unflatten_op(ret)\n\n    def create_variables(self, input_spaces, action_space=None):\n        in_space = input_spaces[""inputs""]\n        self.output_spaces = self.get_preprocessed_space(in_space)\n\n    @rlgraph_api\n    def _graph_fn_call(self, inputs):\n        """"""\n        Images come in with either a batch dimension or not.\n        """"""\n        if self.backend == ""python"" or get_backend() == ""python"":\n            if isinstance(inputs, list):\n                inputs = np.asarray(inputs)\n            had_single_color_dim = (inputs.shape[-1] == 1)\n            # Batch of samples.\n            if inputs.ndim == 4:\n                resized = []\n                for i in range_(len(inputs)):\n                    resized.append(cv2.resize(\n                        inputs[i], dsize=(self.width, self.height), interpolation=self.cv2_interpolation)\n                    )\n                resized = np.asarray(resized)\n            # Single sample.\n            else:\n                resized = cv2.resize(\n                    inputs, dsize=(self.width, self.height), interpolation=self.cv2_interpolation\n                )\n\n            # cv2.resize removes the color rank, if its dimension is 1 (e.g. grayscale), add it back here.\n            if had_single_color_dim is True:\n                resized = np.expand_dims(resized, axis=-1)\n\n            return resized\n        elif get_backend() == ""pytorch"":\n            if isinstance(inputs, list):\n                inputs = torch.tensor(inputs)\n\n            had_single_color_dim = (inputs.shape[-1] == 1)\n            # Batch of samples.\n            if len(inputs.shape) == 4:\n                resized = []\n                for i in range_(len(inputs)):\n                    # Get numpy array.\n                    resized.append(cv2.resize(\n                        inputs[i].numpy(), dsize=(self.width, self.height),\n                        interpolation=self.cv2_interpolation)\n                    )\n                resized = torch.tensor(resized)\n            # Single sample.\n            else:\n                resized = cv2.resize(\n                    inputs.numpy(), dsize=(self.width, self.height), interpolation=self.cv2_interpolation\n                )\n\n            # cv2.resize removes the color rank, if its dimension is 1 (e.g. grayscale), add it back here.\n            if had_single_color_dim is True:\n                resized = torch.unsqueeze(resized, dim=-1)\n\n            return resized\n        elif get_backend() == ""tf"":\n            return tf.image.resize_images(\n                images=inputs, size=(self.width, self.height), method=self.tf_interpolation\n            )\n\n'"
rlgraph/components/layers/preprocessing/moving_standardize.py,14,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.util import SMALL_NUMBER\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass MovingStandardize(PreprocessLayer):\n    """"""\n    Standardizes inputs using a moving estimate of mean and std.\n    """"""\n    def __init__(self, batch_size=1, scope=""moving-standardize"", **kwargs):\n        """"""\n        Args:\n            batch_size (int): Number of samples processed per step.\n        """"""\n        super(MovingStandardize, self).__init__(scope=scope, **kwargs)\n        self.batch_size = batch_size\n        self.sample_count = None\n\n        # Current estimate of state mean.\n        self.mean_est = None\n\n        # Current estimate of sum of stds.\n        self.std_sum_est = None\n        #self.output_spaces = None\n        self.in_shape = None\n\n    def create_variables(self, input_spaces, action_space=None):\n        in_space = input_spaces[""inputs""]\n        #self.output_spaces = in_space\n        self.in_shape = (self.batch_size, ) + in_space.shape\n\n        if self.backend == ""python"" or get_backend() == ""python"" or get_backend() == ""pytorch"":\n            self.sample_count = 0.0\n            self.mean_est = np.zeros(self.in_shape, dtype=np.float32)\n            self.std_sum_est = np.zeros(self.in_shape, dtype=np.float32)\n        elif get_backend() == ""tf"":\n            self.sample_count = self.get_variable(name=""sample-count"", dtype=""float"", initializer=0.0, trainable=False)\n            self.mean_est = self.get_variable(\n                name=""mean-est"",\n                shape=self.in_shape,\n                dtype=tf.float32,\n                trainable=False,\n                initializer=tf.zeros_initializer()\n            )\n            self.std_sum_est = self.get_variable(\n                name=""std-sum-est"",\n                shape= self.in_shape,\n                dtype=tf.float32,\n                trainable=False,\n                initializer=tf.zeros_initializer()\n            )\n\n    @rlgraph_api\n    def _graph_fn_reset(self):\n        if self.backend == ""python"" or get_backend() == ""python"" or get_backend() == ""pytorch"":\n            self.sample_count = 0.0\n            self.mean_est = np.zeros(self.in_shape)\n            self.std_sum_est = np.zeros(self.in_shape)\n        elif get_backend() == ""tf"":\n            return tf.variables_initializer([self.sample_count, self.mean_est, self.std_sum_est])\n\n    @rlgraph_api\n    def _graph_fn_call(self, inputs):\n        if self.backend == ""python"" or get_backend() == ""python"" or get_backend() == ""pytorch"":\n            # https://www.johndcook.com/blog/standard_deviation/\n            inputs = np.asarray(inputs, dtype=np.float32)\n            self.sample_count += 1.0\n            if self.sample_count == 1.0:\n                self.mean_est[...] = inputs\n            else:\n                update = inputs - self.mean_est\n                self.mean_est[...] += update / self.sample_count\n                self.std_sum_est[...] += update * update * (self.sample_count - 1.0) / self.sample_count\n\n            # Subtract mean.\n            result = inputs - self.mean_est\n\n            # Estimate variance via sum of variance.\n            if self.sample_count > 1.0:\n                var_estimate = self.std_sum_est / (self.sample_count - 1.0)\n            else:\n                var_estimate = np.square(self.mean_est)\n            std = np.sqrt(var_estimate) + SMALL_NUMBER\n\n            standardized = result / std\n            if get_backend() == ""pytorch"":\n                standardized = torch.Tensor(standardized)\n            return standardized\n\n        elif get_backend() == ""tf"":\n            assignments = [tf.assign_add(ref=self.sample_count, value=1.0)]\n            with tf.control_dependencies(assignments):\n                # 1. Update vars\n                assignments = []\n                update = inputs - self.mean_est\n                mean_update = tf.cond(\n                    pred=self.sample_count > 1.0,\n                    false_fn=lambda: self.mean_est,\n                    true_fn=lambda: update\n                )\n                var_update = update * update * (self.sample_count - 1) / self.sample_count\n                assignments.append(tf.assign_add(ref=self.mean_est, value=mean_update))\n                assignments.append(tf.assign_add(ref=self.std_sum_est, value=var_update))\n\n            with tf.control_dependencies(assignments):\n                # 2. Compute var estimate after update.\n                var_estimate = tf.cond(\n                    pred=self.sample_count > 1,\n                    false_fn=lambda: tf.square(x=self.mean_est),\n                    true_fn=lambda: self.std_sum_est / (self.sample_count - 1)\n                )\n                result = inputs - self.mean_est\n                std = tf.sqrt(x=var_estimate) + SMALL_NUMBER\n\n                return result / std\n'"
rlgraph/components/layers/preprocessing/multiply_divide.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.spaces.float_box import FloatBox\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.ops import unflatten_op\n\n\nclass Multiply(PreprocessLayer):\n    """"""\n    Scales an input by a constant scaling-factor.\n    """"""\n    def __init__(self, factor, scope=""multiply"", **kwargs):\n        """"""\n        Args:\n            scaling_factor (float): The factor to scale with.\n        """"""\n        super(Multiply, self).__init__(scope=scope, **kwargs)\n        self.factor = factor\n\n    def get_preprocessed_space(self, space):\n        # Translate to corresponding FloatBoxes.\n        ret = dict()\n        for key, value in space.flatten().items():\n            ret[key] = FloatBox(shape=value.shape, add_batch_rank=value.has_batch_rank,\n                                add_time_rank=value.has_time_rank)\n        return unflatten_op(ret)\n\n    @rlgraph_api(flatten_ops=True, split_ops=True)\n    def _graph_fn_call(self, inputs):\n        """"""\n        Multiplies the input with our factor.\n\n        Args:\n            inputs (tensor): The input to be scaled.\n\n        Returns:\n            op: The op to scale the input.\n        """"""\n        result = inputs * self.factor\n        # TODO: Move into util function.\n        if hasattr(inputs, ""_batch_rank""):\n            result._batch_rank = inputs._batch_rank\n        if hasattr(inputs, ""_time_rank""):\n            result._time_rank = inputs._time_rank\n        return result\n\n\nclass Divide(PreprocessLayer):\n    """"""\n    Divides an input by a constant value.\n    """"""\n    def __init__(self, divisor, scope=""divide"", **kwargs):\n        """"""\n        Args:\n            scaling_factor (float): The factor to scale with.\n        """"""\n        super(Divide, self).__init__(scope=scope, **kwargs)\n        self.divisor = divisor\n\n    def get_preprocessed_space(self, space):\n        # Translate to corresponding FloatBoxes.\n        ret = dict()\n        for key, value in space.flatten().items():\n            ret[key] = FloatBox(shape=value.shape, add_batch_rank=value.has_batch_rank,\n                                add_time_rank=value.has_time_rank)\n        return unflatten_op(ret)\n\n    @rlgraph_api(flatten_ops=True, split_ops=True)\n    def _graph_fn_call(self, inputs):\n        """"""\n        Divides the input by with our divisor.\n\n        Args:\n            inputs (tensor): The input to be divided.\n\n        Returns:\n            DataOp: The op to divide the input.\n        """"""\n        result = inputs / self.divisor\n        # TODO: Move into util function.\n        if hasattr(inputs, ""_batch_rank""):\n            result._batch_rank = inputs._batch_rank\n        if hasattr(inputs, ""_time_rank""):\n            result._time_rank = inputs._time_rank\n        return result\n\n'"
rlgraph/components/layers/preprocessing/normalize.py,2,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.spaces import Space\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.util import SMALL_NUMBER\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass Normalize(PreprocessLayer):\n    """"""\n    Normalizes an input over all axes individually (denoted as `Xi` below) according to the following formula:\n\n    Xi = (Xi - min(Xi)) / (max(Xi) - min(Xi) + epsilon),\n        where:\n        Xi is one entire axis of values.\n        max(Xi) is the max value along this axis.\n        min(Xi) is the min value along this axis.\n        epsilon is a very small constant number (to avoid dividing by 0).\n    """"""\n    def __init__(self, scope=""normalize"", **kwargs):\n        super(Normalize, self).__init__(scope=scope, **kwargs)\n        self.axes = None\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        super(Normalize, self).check_input_spaces(input_spaces, action_space)\n\n        in_space = input_spaces[""inputs""]  # type: Space\n        # A list of all axes over which to normalize (exclude batch rank).\n        self.axes = list(range(1 if in_space.has_batch_rank else 0, len(in_space.get_shape(with_batch_rank=False))))\n\n    @rlgraph_api\n    def _graph_fn_call(self, inputs):\n        min_value = inputs\n        max_value = inputs\n\n        if get_backend() == ""tf"":\n            # Iteratively reduce dimensionality across all axes to get the min/max values for each sample in the batch.\n            for axis in self.axes:\n                min_value = tf.reduce_min(input_tensor=min_value, axis=axis, keep_dims=True)\n                max_value = tf.reduce_max(input_tensor=max_value, axis=axis, keep_dims=True)\n        elif get_backend() == ""pytorch"":\n            for axis in self.axes:\n                min_value = torch.min(min_value, axis)\n                max_value = torch.max(max_value, axis)\n\n        # Add some small constant to never let the range be zero.\n        return (inputs - min_value) / (max_value - min_value + SMALL_NUMBER)\n\n'"
rlgraph/components/layers/preprocessing/preprocess_layer.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.layer import Layer\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass PreprocessLayer(Layer):\n    """"""\n    A Layer that - additionally to `call` - implements the `reset` API-method.\n    `call` is usually used for preprocessing inputs. `reset` is used to reset some state information of this\n    preprocessor (e.g reset/reinitialize a variable).\n    """"""\n    def __init__(self, scope=""pre-process"", **kwargs):\n        super(PreprocessLayer, self).__init__(scope=scope, **kwargs)\n\n    @rlgraph_api\n    def _graph_fn_reset(self):\n        """"""\n        Does some reset operations e.g. in case this PreprocessLayer contains variables and state.\n\n        Returns:\n            SingleDataOp: The op that resets this processor to some initial state.\n        """"""\n        if get_backend() == ""tf"":\n            return tf.no_op(name=""reset-op"")  # Not mandatory.\n\n        # TODO: fix for python backend.\n        return\n\n    @rlgraph_api(flatten_ops=True, split_ops=True)\n    def _graph_fn_call(self, *inputs):\n        return super(PreprocessLayer, self)._graph_fn_call(*inputs)\n'"
rlgraph/components/layers/preprocessing/rank_reinterpreter.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass RankReinterpreter(PreprocessLayer):\n    """"""\n    Re-interprets the given ranks (ints) into batch and/or time ranks.\n    """"""\n    def __init__(self, batch_rank=None, time_rank=None, scope=""rank-reinterpreter"",  **kwargs):\n        """"""\n        Args:\n            batch_rank (Optional[int]): The batch rank to set for the input.\n            time_rank (Optional[int]): The time rank to set for the input.\n        """"""\n        super(RankReinterpreter, self).__init__(space_agnostic=True, scope=scope, **kwargs)\n        self.batch_rank = batch_rank\n        self.time_rank = time_rank\n\n    @rlgraph_api\n    def _graph_fn_call(self, inputs):\n        if get_backend() == ""tf"":\n            ret = tf.identity(inputs, name=""rank-reinterpreted"")\n            # We have to re-interpret the batch rank.\n            if self.batch_rank is not None:\n                ret._batch_rank = self.batch_rank\n            # We have to re-interpret the time rank.\n            if self.time_rank is not None:\n                ret._time_rank = self.time_rank\n\n            return ret\n'"
rlgraph/components/layers/preprocessing/reshape.py,4,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.spaces import IntBox, FloatBox\nfrom rlgraph.spaces.space_utils import sanity_check_space, get_space_from_op\nfrom rlgraph.utils import pytorch_one_hot\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.numpy import one_hot\nfrom rlgraph.utils.ops import unflatten_op, FLATTEN_SCOPE_PREFIX\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass ReShape(PreprocessLayer):\n    """"""\n    A reshaping preprocessor that takes an input and reshapes it into a new shape.\n    Also supports special options for time/batch rank manipulations and complete flattening\n    (including IntBox categories).\n    """"""\n    def __init__(self, new_shape=None, flatten=False, flatten_categories=None, fold_time_rank=False,\n                 unfold_time_rank=False, time_major=None, scope=None, **kwargs):\n        """"""\n        Args:\n            new_shape (Optional[Dict[str,Tuple[int]],Tuple[int]]): A dict of str/tuples or a single tuple\n                specifying the new-shape(s) to use (for each auto key in case of a Container input Space).\n                At most one of the ranks in any new_shape may be -1 to indicate flexibility in that dimension.\n                NOTE: Shape does not include batch- or time-ranks. If you want to manipulate these directly, use\n                the fold_time_rank/unfold_time_rank options.\n\n            flatten (bool): Whether to simply flatten the input Space into a single rank. This does not include\n                batch- or time-ranks. These can be processed separately by the other ctor options.\n                If flatten is True, new_shape must be None.\n\n            flatten_categories (Union[Dict[str,int],int]): Only important if `flatten` is True and incoming space is\n                an IntBox. Specifies, how to also flatten IntBox categories by giving the exact number of int\n                categories generally or by flat-dict key.\n                Default: None.\n\n            fold_time_rank (bool): Whether to fold the time rank into a single batch rank.\n                E.g. from (None, None, 2, 3) to (None, 2, 3). Providing both `fold_time_rank` (True) and\n                `new_shape` is allowed.\n\n            unfold_time_rank (Union[bool,int]): Whether to unfold the time rank from a currently common batch+time-rank.\n                The exact size of the time rank to unfold is either directly provided or determined automatically via\n                the original sample.\n                Providing both `unfold_time_rank` (True) and `new_shape` is allowed.\n\n            time_major (Optional[bool]): Only used if not None and if unfold_time_rank is True. Specifies whether the\n                time rank should come before the batch rank after unfolding.\n        """"""\n        scope = scope or (\n            ""reshape-fold"" if fold_time_rank else ""reshape-unfold"" if unfold_time_rank else\n            ""reshape-flatten"" if flatten is True else ""reshape""\n        )\n        super(ReShape, self).__init__(space_agnostic=True, scope=scope, **kwargs)\n\n        assert flatten is False or new_shape is None, ""ERROR: If `flatten` is True, `new_shape` must be None!""\n        assert not fold_time_rank or not unfold_time_rank,\\\n            ""ERROR: Can only either fold or unfold the time-rank! Both `fold_time_rank` and `unfold_time_rank` "" \\\n            ""cannot be True at the same time.""\n\n        # The new shape specifications.\n        self.new_shape = new_shape\n        self.flatten = flatten\n        self.flatten_categories = flatten_categories\n        self.fold_time_rank = fold_time_rank\n        self.unfold_time_rank = unfold_time_rank\n        self.time_major = time_major\n\n    def get_preprocessed_space(self, space):\n        ret = {}\n        for key, single_space in space.flatten().items():\n            class_ = type(single_space)\n\n            # Determine the actual shape (not batch/time ranks).\n            if self.flatten is True:\n                if type(single_space) == IntBox and self.flatten_categories is not False:\n                    assert self.flatten_categories is not None,\\\n                        ""ERROR: `flatten_categories` must not be None if `flatten` is True and input is IntBox!""\n                    new_shape = (self.get_num_categories(key, single_space),)\n                    class_ = FloatBox\n                else:\n                    new_shape = (single_space.flat_dim,)\n            else:\n                new_shape = self.new_shape[key] if isinstance(self.new_shape, dict) else self.new_shape\n\n            # Check the batch/time rank options.\n            if self.fold_time_rank is True:\n                sanity_check_space(single_space, must_have_batch_rank=True, must_have_time_rank=True)\n                ret[key] = class_(\n                    shape=single_space.shape if new_shape is None else new_shape,\n                    add_batch_rank=True, add_time_rank=False\n                )\n            # Time rank should be unfolded from batch rank with the given dimension.\n            elif self.unfold_time_rank:\n                sanity_check_space(single_space, must_have_batch_rank=True, must_have_time_rank=False)\n                ret[key] = class_(\n                    shape=single_space.shape if new_shape is None else new_shape,\n                    add_batch_rank=True, add_time_rank=True,\n                    time_major=self.time_major if self.time_major is not None else False\n                )\n            # Only change the actual shape (leave batch/time ranks as is).\n            else:\n                time_major = single_space.time_major\n                ret[key] = class_(shape=single_space.shape if new_shape is None else new_shape,\n                                  add_batch_rank=single_space.has_batch_rank,\n                                  add_time_rank=single_space.has_time_rank, time_major=time_major)\n        ret = unflatten_op(ret)\n        return ret\n\n    def get_num_categories(self, key, single_space):\n        if self.flatten_categories is True and isinstance(single_space, IntBox):\n            num_categories = single_space.flat_dim_with_categories\n        elif isinstance(self.flatten_categories, dict):\n            if key.startswith(FLATTEN_SCOPE_PREFIX):\n                key = key[1:]\n            num_categories = self.flatten_categories.get(key, 1)\n        else:\n            num_categories = self.flatten_categories\n        return num_categories\n\n    @rlgraph_api(flatten_ops=True, split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_call(self, key, inputs, input_before_time_rank_folding=None):\n        """"""\n        Reshapes the input to the specified new shape.\n\n        Args:\n            inputs (SingleDataOp): The input to reshape.\n            input_before_time_rank_folding (Optional[SingleDataOp]): The original input (before!) the time-rank had\n                been folded (this was done in a different ReShape Component).\n                Used to figure out the exact time-rank dimension to unfold iff `self.unfold_time_rank` is True.\n\n        Returns:\n            SingleDataOp: The reshaped input.\n        """"""\n        assert self.unfold_time_rank is not True or input_before_time_rank_folding is not None\n\n        if self.backend == ""python"" or get_backend() == ""python"":\n            # Create a one-hot axis for the categories at the end?\n            num_categories = self.get_num_categories(key, get_space_from_op(inputs))\n            if num_categories and num_categories > 1:\n                inputs = one_hot(inputs, depth=num_categories)\n\n            if self.unfold_time_rank:\n                new_shape = [-1, -1] + list(inputs.shape[1:])\n                if type(self.unfold_time_rank) == int:\n                    new_shape[0 if self.time_major else 1] = self.unfold_time_rank\n                new_shape = tuple(new_shape)\n            elif self.fold_time_rank:\n                new_shape = (-1,) + inputs.shape[2:]\n            else:\n                new_shape = self.get_preprocessed_space(get_space_from_op(inputs)).get_shape(\n                    with_batch_rank=-1, with_time_rank=-1\n                )\n\n            # Dynamic new shape inference:\n            # If both batch and time rank must be left alone OR the time rank must be unfolded from a currently common\n            # batch+time 0th rank, get these two dynamically.\n            if len(inputs.shape) > 2 and new_shape[0] == -1 and new_shape[1] == -1:\n                # Time rank unfolding. Get the time rank from original input.\n                if self.unfold_time_rank is True:\n                    original_shape = input_before_time_rank_folding.shape\n                    new_shape = (original_shape[0], original_shape[1]) + new_shape[2:]\n                # No time-rank unfolding, but we do have both batch- and time-rank.\n                else:\n                    input_shape = inputs.shape\n                    # Batch and time rank stay as is.\n                    new_shape = (input_shape[0], input_shape[1]) + new_shape[2:]\n\n            return np.reshape(inputs, newshape=new_shape)\n\n        elif get_backend() == ""pytorch"":\n            # Create a one-hot axis for the categories at the end?\n            num_categories = self.get_num_categories(key, get_space_from_op(inputs))\n            if num_categories and num_categories > 1:\n                inputs = pytorch_one_hot(inputs, depth=num_categories)\n\n            if self.unfold_time_rank:\n                new_shape = [-1, -1] + list(inputs.shape[1:])\n                if type(self.unfold_time_rank) == int:\n                    new_shape[0 if self.time_major else 1] = self.unfold_time_rank\n                new_shape = tuple(new_shape)\n            elif self.fold_time_rank:\n                new_shape = (-1,) + inputs.shape[2:]\n            else:\n                new_shape = self.get_preprocessed_space(get_space_from_op(inputs)).get_shape(\n                    with_batch_rank=-1, with_time_rank=-1\n                )\n\n            # Dynamic new shape inference:\n            # If both batch and time rank must be left alone OR the time rank must be unfolded from a currently common\n            # batch+time 0th rank, get these two dynamically.\n            if len(new_shape) > 2 and new_shape[0] == -1 and new_shape[1] == -1:\n                # Time rank unfolding. Get the time rank from original input.\n                if self.unfold_time_rank is True:\n                    original_shape = input_before_time_rank_folding.shape\n                    new_shape = (original_shape[0], original_shape[1]) + new_shape[2:]\n                # No time-rank unfolding, but we do have both batch- and time-rank.\n                else:\n                    input_shape = inputs.shape\n                    # Batch and time rank stay as is.\n                    new_shape = (input_shape[0], input_shape[1]) + new_shape[2:]\n\n            # print(""Reshaping input of shape {} to new shape {} (flatten = {})"".format(inputs.shape,\n            #                                                                           new_shape, self.flatten))\n\n            old_size = np.prod(list(inputs.shape))\n            new_size = np.prod(new_shape)\n\n            # The problem here is the following: Input has dim e.g. [4, 256, 1, 1]\n            # -> If shape inference in spaces failed, output dim is not correct -> reshape will attempt\n            # something like reshaping to [256].\n            if self.flatten and inputs.dim() > 1:\n                flattened_shape_without_batchrank = np.prod(inputs.shape[1:])\n                flattened_shape = (inputs.shape[0],) + (flattened_shape_without_batchrank,)\n                return torch.reshape(inputs, flattened_shape)\n            # If new shape does not fit into old shape, batch inference failed -> try to restore:\n            # Equal except batch rank -> return as is:\n            elif old_size != new_size:\n                if tuple(inputs.shape[1:]) == new_shape:\n                    return inputs\n                else:\n                    # Attempt to rescue reshape by combining new shape with batch dim.\n                    full_new_shape = (inputs.shape[0],) + new_shape\n                    return torch.reshape(inputs, full_new_shape)\n            else:\n                return torch.reshape(inputs, new_shape)\n\n        elif get_backend() == ""tf"":\n            # Create a one-hot axis for the categories at the end?\n            space = get_space_from_op(inputs)\n            num_categories = self.get_num_categories(key, space)\n            if num_categories and num_categories > 1:\n                inputs_ = tf.one_hot(\n                    inputs, depth=num_categories, axis=-1, dtype=""float32""\n                )\n                if hasattr(inputs, ""_batch_rank""):\n                    inputs_._batch_rank = inputs._batch_rank\n                if hasattr(inputs, ""_time_rank""):\n                    inputs_._time_rank = inputs._time_rank\n                inputs = inputs_\n\n            if self.fold_time_rank:\n                new_shape = (-1,) + tuple(inputs.shape.as_list()[2:])\n            else:\n                time_rank = -1\n                if type(self.unfold_time_rank) == int:\n                    time_rank = self.unfold_time_rank\n\n                new_shape = self.get_preprocessed_space(get_space_from_op(inputs)).get_shape(\n                    with_batch_rank=-1, with_time_rank=time_rank\n                )\n\n            # Dynamic new shape inference:\n            # If both batch and time rank must be left alone OR the time rank must be unfolded from a currently common\n            # batch+time 0th rank, get these two dynamically.\n            if len(new_shape) >= 2 and new_shape[0] == -1 and new_shape[1] == -1:\n                # Time rank unfolding. Get the time rank from original input.\n                if self.unfold_time_rank is True:\n                    original_shape = tf.shape(input_before_time_rank_folding)\n                    new_shape = (original_shape[0], original_shape[1]) + new_shape[2:]\n                # No time-rank unfolding, but we do have both batch- and time-rank.\n                else:\n                    input_shape = tf.shape(inputs)\n                    # Batch and time rank stay as is.\n                    new_shape = (input_shape[0], input_shape[1]) + new_shape[2:]\n\n            reshaped = tf.reshape(tensor=inputs, shape=new_shape, name=""reshaped"")\n\n            # Have to place the time rank back in as unknown (for the auto Space inference).\n            if type(self.unfold_time_rank) == int:\n                reshaped._batch_rank = 1 if self.time_major is True else 0\n                reshaped._time_rank = 0 if self.time_major is True else 1\n                return reshaped\n            else:\n                # TODO: add other cases of reshaping and fix batch/time rank hints.\n                if self.fold_time_rank:\n                    reshaped._batch_rank = 0\n                elif self.unfold_time_rank:\n                    reshaped._batch_rank = 1 if self.time_major is True else 0\n                    reshaped._time_rank = 0 if self.time_major is True else 1\n                else:\n                    if space.has_batch_rank is True:\n                        if space.time_major is False:\n                            reshaped._batch_rank = 0\n                        else:\n                            reshaped._time_rank = 0\n                            reshaped._batch_rank = 1\n                    if space.has_time_rank is True:\n                        reshaped._time_rank = 0 if space.time_major is True else 1\n\n                return reshaped\n'"
rlgraph/components/layers/preprocessing/sequence.py,9,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import deque\n\nimport numpy as np\nfrom six.moves import xrange as range_\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.ops import FlattenedDataOp, unflatten_op\nfrom rlgraph.utils.util import get_rank, force_list\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass Sequence(PreprocessLayer):\n    """"""\n    Concatenate `length` state vectors. Example: Used in Atari\n    problems to create the Markov property (velocity of game objects as they move across the screen).\n    """"""\n\n    def __init__(self, sequence_length=2, batch_size=1, add_rank=True, in_data_format=""channels_last"",\n                 out_data_format=""channels_last"", scope=""sequence"",  **kwargs):\n        """"""\n        Args:\n            sequence_length (int): The number of records to always concatenate together within the last rank or\n                in an extra (added) rank.\n            batch_size (int): The batch size for incoming records so multiple inputs can be passed through at once.\n            in_data_format (str): One of \'channels_last\' (default) or \'channels_first\'. Specifies which rank (first or\n                last) is the color-channel. If the input Space is with batch, the batch always has the first rank.\n            out_data_format (str): One of \'channels_last\' (default) or \'channels_first\'. Specifies which rank (first or\n                last) is the color-channel in output. If the input Space is with batch,\n                 the batch always has the first rank.\n            add_rank (bool): Whether to add another rank to the end of the input with dim=length-of-the-sequence.\n                If False, concatenates the sequence within the last rank.\n                Default: True.\n        """"""\n        # Switch off split (it\'s switched on for all LayerComponents by default).\n        # -> accept any Space -> flatten to OrderedDict -> input & return OrderedDict -> re-nest.\n        super(Sequence, self).__init__(scope=scope, **kwargs)\n\n        self.sequence_length = sequence_length\n        self.batch_size = batch_size\n        self.add_rank = add_rank\n\n        self.in_data_format = in_data_format\n        if get_backend() == ""pytorch"":\n            # Always channels first for PyTorch.\n            self.out_data_format = ""channels_first""\n        else:\n            self.out_data_format = out_data_format\n\n        # The sequence-buffer where we store previous inputs.\n        self.buffer = None\n        # The index into the buffer\'s.\n        self.index = None\n        # The output spaces after preprocessing (per flat-key).\n        self.output_spaces = None\n        if self.backend == ""python"" or get_backend() == ""python"" or get_backend() == ""pytorch"":\n            self.deque = deque([], maxlen=self.sequence_length)\n\n    def get_preprocessed_space(self, space):\n        ret = {}\n        for key, value in space.flatten().items():\n            shape = list(value.shape)\n            if self.add_rank:\n                shape.append(self.sequence_length)\n            else:\n                shape[-1] *= self.sequence_length\n\n            # TODO move to transpose component.\n            # Transpose.\n            if self.in_data_format == ""channels_last"" and self.out_data_format == ""channels_first"":\n                shape.reverse()\n                ret[key] = value.__class__(shape=tuple(shape), add_batch_rank=value.has_batch_rank)\n            else:\n                ret[key] = value.__class__(shape=tuple(shape), add_batch_rank=value.has_batch_rank)\n        return unflatten_op(ret)\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        super(Sequence, self).check_input_spaces(input_spaces, action_space)\n        in_space = input_spaces[""inputs""]\n\n        # Require inputs to not have time rank (batch rank doesn\'t matter).\n        sanity_check_space(in_space, must_have_time_rank=False)\n\n    def create_variables(self, input_spaces, action_space=None):\n        in_space = input_spaces[""inputs""]\n        self.output_spaces = self.get_preprocessed_space(in_space)\n        self.index = self.get_variable(name=""index"", dtype=""int"", initializer=-1, trainable=False)\n\n        if get_backend() == ""tf"":\n            self.buffer = self.get_variable(\n                name=""buffer"", trainable=False, from_space=in_space,\n                add_batch_rank=self.batch_size if in_space.has_batch_rank is not False else False,\n                add_time_rank=self.sequence_length, time_major=True, flatten=True\n            )\n\n    @rlgraph_api\n    def _graph_fn_reset(self):\n        if self.backend == ""python"" or get_backend() == ""python"" or get_backend() == ""pytorch"":\n            self.index = -1\n        elif get_backend() == ""tf"":\n            return tf.variables_initializer([self.index])\n\n    @rlgraph_api(flatten_ops=True, split_ops=False)\n    def _graph_fn_call(self, inputs):\n        """"""\n        Sequences (stitches) together the incoming inputs by using our buffer (with stored older records).\n        Sequencing happens within the last rank if `self.add_rank` is False, otherwise a new rank is added at the end\n        for the sequencing.\n\n        Args:\n            inputs (FlattenedDataOp): The FlattenedDataOp to be sequenced.\n                One sequence is generated separately for each SingleDataOp in api_methods.\n\n        Returns:\n            FlattenedDataOp: The FlattenedDataOp holding the sequenced SingleDataOps as values.\n        """"""\n        # A normal (index != -1) assign op.\n        if self.backend == ""python"" or get_backend() == ""python"":\n            if self.index == -1:\n                for _ in range_(self.sequence_length):\n                    self.deque.append(inputs)\n            else:\n                self.deque.append(inputs)\n            self.index = (self.index + 1) % self.sequence_length\n\n            if self.add_rank:\n                sequence = np.stack(self.deque, axis=-1)\n            # Concat the sequence items in the last rank.\n            else:\n                sequence = np.concatenate(self.deque, axis=-1)\n\n            # TODO move into transpose component.\n            if self.in_data_format == ""channels_last"" and self.out_data_format == ""channels_first"":\n                sequence = sequence.transpose((0, 3, 2, 1))\n\n            return sequence\n        elif get_backend() == ""pytorch"":\n            if self.index == -1:\n                for _ in range_(self.sequence_length):\n                    if isinstance(inputs, dict):\n                        for key, value in inputs.items():\n                            self.deque.append(value)\n                    else:\n                        self.deque.append(inputs)\n            else:\n                if isinstance(inputs, dict):\n                    for key, value in inputs.items():\n                        self.deque.append(value)\n                        self.index = (self.index + 1) % self.sequence_length\n                else:\n                    self.deque.append(inputs)\n                    self.index = (self.index + 1) % self.sequence_length\n\n            if self.add_rank:\n                sequence = torch.stack(torch.tensor(self.deque), dim=-1)\n            # Concat the sequence items in the last rank.\n            else:\n                data = []\n                for t in self.deque:\n                    if isinstance(t, torch.Tensor):\n                        data.append(t)\n                    else:\n                        data.append(torch.tensor(t))\n                sequence = torch.cat(data, dim=-1)\n\n            # TODO remove when transpose component implemented.\n            if self.in_data_format == ""channels_last"" and self.out_data_format == ""channels_first"":\n                # Problem: PyTorch does not have data format options in conv layers ->\n                # only channels first supported.\n                # -> Confusingly have to transpose.\n                # B W H C -> B C W H\n                # e.g. atari: [4 84 84 4] -> [4 4 84 84]\n                sequence = sequence.permute(0, 3, 2, 1)\n\n            return sequence\n        elif get_backend() == ""tf"":\n            # Assigns the input_ into the buffer at the current time index.\n            def normal_assign():\n                assigns = list()\n                for key_, value in inputs.items():\n                    assign_op = self.assign_variable(ref=self.buffer[key_][self.index], value=value)\n                    assigns.append(assign_op)\n                return assigns\n\n            # After a reset (time index is -1), fill the entire buffer with `self.sequence_length` x input_.\n            def after_reset_assign():\n                assigns = list()\n                for key_, value in inputs.items():\n                    multiples = (self.sequence_length,) + tuple([1] * get_rank(value))\n                    input_ = tf.expand_dims(input=value, axis=0)\n                    assign_op = self.assign_variable(\n                        ref=self.buffer[key_], value=tf.tile(input=input_, multiples=multiples)\n                    )\n                    assigns.append(assign_op)\n                return assigns\n\n            # Insert the input at the correct index or fill empty buffer entirely with input.\n            insert_inputs = tf.cond(pred=(self.index >= 0), true_fn=normal_assign, false_fn=after_reset_assign)\n\n            # Make sure the input has been inserted.\n            with tf.control_dependencies(control_inputs=force_list(insert_inputs)):\n                # Then increase index by 1.\n                index_plus_1 = self.assign_variable(ref=self.index, value=((self.index + 1) % self.sequence_length))\n\n            # Then gather the output.\n            with tf.control_dependencies(control_inputs=[index_plus_1]):\n                sequences = FlattenedDataOp()\n                # Collect the correct previous inputs from the buffer to form the output sequence.\n                for key in inputs.keys():\n                    n_in = [self.buffer[key][(self.index + n) % self.sequence_length]\n                            for n in range_(self.sequence_length)]\n\n                    # Add the sequence-rank to the end of our inputs.\n                    if self.add_rank:\n                        sequence = tf.stack(values=n_in, axis=-1)\n                    # Concat the sequence items in the last rank.\n                    else:\n                        sequence = tf.concat(values=n_in, axis=-1)\n\n                    # Must pass the sequence through a placeholder_with_default dummy to set back the\n                    # batch rank to \'?\', instead of 1 (1 would confuse the auto Space inference).\n                    #sequences[key] = tf.placeholder_with_default(\n                    #    sequence, shape=(None,) + tuple(get_shape(sequence)[1:])\n                    #)\n                    sequence._batch_rank = 0\n                    sequences[key] = sequence\n\n            # TODO implement transpose\n                return sequences\n\n'"
rlgraph/components/layers/preprocessing/transpose.py,1,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.preprocessing.preprocess_layer import PreprocessLayer\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.ops import unflatten_op\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\nelif get_backend() == ""pytorch"":\n    import torch\n\n\nclass Transpose(PreprocessLayer):\n    """"""\n    """"""\n    def __init__(self, output_is_time_major=True, scope=""transpose"", **kwargs):\n        """"""\n        Args:\n            output_is_time_major (Optional[bool]): Whether the output of this Component will always be time-major.\n                If None, get this information from input-spaces. If given, this Component will be space-agnostic.\n                Default: True (batch-major -> time-major transpose).\n        """"""\n        super(Transpose, self).__init__(space_agnostic=(output_is_time_major is not None), scope=scope, **kwargs)\n\n        # Overrides everything in dict: `self.output_time_majors`.\n        self.output_is_time_major = output_is_time_major\n\n        # Only used if `self.output_is_time_major` is None.\n        self.output_time_majors = {}\n\n    def create_variables(self, input_spaces, action_space=None):\n        in_space = input_spaces[""inputs""]  # type: Space\n        # Make sure output time_majors are stored.\n        self.get_preprocessed_space(in_space)\n\n    def get_preprocessed_space(self, space):\n        ret = {}\n        for key, single_space in space.flatten().items():\n            class_ = type(single_space)\n            # We flip batch and time ranks.\n            time_major = not single_space.time_major\n            ret[key] = class_(shape=single_space.shape,\n                              add_batch_rank=single_space.has_batch_rank,\n                              add_time_rank=single_space.has_time_rank, time_major=time_major)\n            self.output_time_majors[key] = time_major\n        ret = unflatten_op(ret)\n        return ret\n\n    @rlgraph_api(flatten_ops=True, split_ops=True, add_auto_key_as_first_param=True)\n    def _graph_fn_call(self, key, inputs):\n        """"""\n        Transposes the input by flipping batch and time ranks.\n        """"""\n        if get_backend() == ""tf"":\n            # Flip around ranks 0 and 1.\n            transposed = tf.transpose(\n                inputs,\n                perm=(1, 0) + tuple(i for i in range(2, len(inputs.shape.as_list()))), name=""transpose""\n            )\n            if self.output_is_time_major is None:\n                transposed._time_rank = 0 if self.output_time_majors[key] is True else 1\n                transposed._batch_rank = 0 if self.output_time_majors[key] is False else 1\n            else:\n                transposed._time_rank = 0 if self.output_is_time_major is True else 1\n                transposed._batch_rank = 0 if self.output_is_time_major is False else 1\n\n            return transposed\n\n        elif get_backend() == ""pytorch"":\n            perm = (1, 0) + tuple(i for i in range(2, len(list(inputs.shape))))\n            return torch.transpose(inputs, perm)\n'"
rlgraph/components/layers/strings/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.layers.strings.string_layer import StringLayer\nfrom rlgraph.components.layers.strings.embedding_lookup import EmbeddingLookup\nfrom rlgraph.components.layers.strings.string_to_hash_bucket import StringToHashBucket\n\nStringLayer.__lookup_classes__ = dict(\n    embedding=EmbeddingLookup,\n    embeddinglookup=EmbeddingLookup,\n    stringtohashbucket=StringToHashBucket\n)\n\n__all__ = [""StringLayer"", ""EmbeddingLookup"", ""StringToHashBucket""]\n'"
rlgraph/components/layers/strings/embedding_lookup.py,3,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.layer import Layer\nfrom rlgraph.spaces import IntBox\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.initializer import Initializer\nfrom rlgraph.utils.util import convert_dtype\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass EmbeddingLookup(Layer):\n    """"""\n    An embedding lookup layer.\n    A matrix with num-columns = number of encoding value per vocab and num-rows = number of vocabs to encode.\n    Calling `call` will lookup and return rows from this matrix specified via the input to `call` as a simple\n    tensor of row indices.\n    """"""\n    def __init__(self, embed_dim, vocab_size, initializer_spec=""truncated_normal"", partition_strategy=""mod"",\n                 trainable=True, pad_empty=False, **kwargs):\n        """"""\n        Args:\n            embed_dim (int): The number of values (number of columns) to use for the encoding of each vocab. One vocab\n                equals one row in the embedding matrix.\n            vocab_size (int): The number of vocabs (number of rows) in the embedding matrix.\n            initializer_spec (any): A specifier for the embedding matrix initializer.\n                If None, use the default initializer, which is truncated normal with stddev=1/sqrt(vocab_size).\n            partition_strategy (str): One of ""mod"" or ""div"". Default: ""mod"".\n            trainable (bool): Whether the Variable(s) representing the embedding matrix should be trainable or not.\n                Default: True.\n            pad_empty (bool): Whether to pad the output if no lookups take place and the embedding would otherwise\n                return a shape=(embed_dim, 0) output. If True, would then return shape=(embed_dim, 1).\n\n        """"""\n        super(EmbeddingLookup, self).__init__(scope=kwargs.pop(""scope"", ""embedding-lookup""), **kwargs)\n\n        self.embed_dim = embed_dim\n        self.vocab_size = vocab_size\n        self.initializer_spec = initializer_spec\n        self.initializer = None\n        self.partition_strategy = partition_strategy\n        self.trainable = trainable\n        self.pad_empty = pad_empty\n\n        # Our embedding matrix variable.\n        self.embedding_matrix = None\n\n        self.ids_space = None\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        ids_space = input_spaces[""ids""]\n        # Require int with batch-rank.\n        sanity_check_space(ids_space, must_have_batch_rank=True, allowed_sub_types=[IntBox])\n\n    def create_variables(self, input_spaces, action_space=None):\n        # Create weights matrix and (maybe) biases vector.\n        shape = (self.vocab_size, self.embed_dim)\n        self.initializer = Initializer.from_spec(shape=shape, specification=self.initializer_spec)\n        # TODO: For IMPALA partitioner is not needed. Do this later.\n        self.embedding_matrix = self.get_variable(\n            name=""embedding-matrix"", shape=shape, dtype=convert_dtype(""float""), initializer=self.initializer.initializer,\n            #partitioner=self.partitioners, regularizer=self.regularizers,\n            trainable=self.trainable\n        )\n\n        self.ids_space = input_spaces[""ids""]\n\n    @rlgraph_api(flatten_ops=True, split_ops=True)\n    def _graph_fn_call(self, ids):\n        if get_backend() == ""tf"":\n            embedding_lookup_output = tf.nn.embedding_lookup(\n                self.embedding_matrix, ids, partition_strategy=self.partition_strategy, max_norm=None\n            )\n            # Do we need to CONSTANT-pad (with 0s) if empty?\n            if self.pad_empty:\n                padding = tf.cast(x=tf.equal(tf.shape(embedding_lookup_output)[1], 0), dtype=tf.int32)\n                embedding_lookup_output = tf.pad(embedding_lookup_output, [[0, 0], [0, padding], [0, 0]])\n\n            if self.ids_space.has_time_rank is True:\n                embedding_lookup_output._batch_rank = 0 if self.ids_space.time_major is False else 1\n                embedding_lookup_output._time_rank = 0 if self.ids_space.time_major is True else 1\n            elif self.ids_space.has_batch_rank is True:\n                embedding_lookup_output._batch_rank = 0\n            return embedding_lookup_output\n'"
rlgraph/components/layers/strings/string_layer.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.layers.layer import Layer\nfrom rlgraph.spaces import TextBox\nfrom rlgraph.spaces.space_utils import sanity_check_space\n\n\nclass StringLayer(Layer):\n    """"""\n    A generic string processing layer class.\n    """"""\n    def __init__(self, **kwargs):\n        super(StringLayer, self).__init__(scope=kwargs.pop(""scope"", ""string-layer""), **kwargs)\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        """"""\n        Do some sanity checking on the incoming Space:\n        Must be string type.\n        """"""\n        sanity_check_space(input_spaces[""text_inputs""], allowed_types=[TextBox], must_have_batch_rank=True)\n'"
rlgraph/components/layers/strings/string_to_hash_bucket.py,7,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers.strings.string_layer import StringLayer\nfrom rlgraph.spaces.space_utils import sanity_check_space\nfrom rlgraph.utils.decorators import rlgraph_api\nfrom rlgraph.utils.util import convert_dtype as dtype_\n\nif get_backend() == ""tf"":\n    import tensorflow as tf\n\n\nclass StringToHashBucket(StringLayer):\n    """"""\n    A string to hash-bucket converter Component that takes a batch of string inputs (e.g.\n    [""this is string A"", ""this is string B""] <- batch size==2) and creates a table of indices out of it that can be\n    used instead of a static vocabulary list for embedding lookups. The created indices table contains\n    n rows (n = number of items (strings) in the input batch) and m columns (m=max number of words in any of the\n    input strings) of customizable int type.\n    The int numbers in the created table can range from 0 to H (with H being the `num_hash_buckets` parameter).\n    The entire hash bucket can now be fed through an embedding, producing - for each item in the batch - an m x e\n    matrix, where m is the number of words in the batch item (sentence) (corresponds to an LSTM sequence length) and\n    e is the embedding size. The embedding output can then be fed - e.g. - into an LSTM with m being the time rank\n    (n still the batch rank).\n    """"""\n    def __init__(self, delimiter="" "", dtype=""int64"", num_hash_buckets=1000, hash_function=""fast"",\n                 scope=""string-to-hash-bucket"", **kwargs):\n        """"""\n        Args:\n            delimiter (str): The string delimiter used for splitting the input sentences into single ""words"".\n                Default: "" "".\n            dtype (str): The type specifier for the created hash bucket. Default: int64.\n            num_hash_buckets (int): The number of hash buckets to create. This is the maximum value of any number in\n                the created lookup table (lowest value is always 0) minus 1.\n            hash_function (str): The hashing function to use. One of ""fast"" or ""strong"". Default: ""fast"".\n                For details, see: https://www.tensorflow.org/api_docs/python/tf/string_to_hash_bucket_(fast|strong)\n                The ""strong"" method is better at avoiding placing different words into the same bucket, but runs\n                about 4x slower than the ""fast"" one.\n\n        Keyword Args:\n            hash_keys (List[int,int]): Two uint64 keys used by the ""strong"" hashing function.\n        """"""\n        super(StringToHashBucket, self).__init__(graph_fn_num_outputs=dict(_graph_fn_call=2), scope=scope, **kwargs)\n\n        self.delimiter = delimiter\n        self.dtype = dtype\n        assert self.dtype in [""int16"", ""int32"", ""int"", ""int64""],\\\n            ""ERROR: dtype \'{}\' not supported by StringToHashBucket Component!"".format(self.dtype)\n        self.num_hash_buckets = num_hash_buckets\n        self.hash_function = hash_function\n        # Only used in the ""strong"" hash function.\n        self.hash_keys = kwargs.pop(""hash_keys"", [12345, 67890])\n\n    def check_input_spaces(self, input_spaces, action_space=None):\n        super(StringToHashBucket, self).check_input_spaces(input_spaces, action_space)\n\n        # Make sure there is only a batch rank (single text items).\n        # tf.string_split does not support more complex shapes.\n        # Output is then always batch+time ranked data.\n        sanity_check_space(input_spaces[""text_inputs""], must_have_batch_rank=True, must_have_time_rank=False, rank=0)\n\n    @rlgraph_api(flatten_ops=True, split_ops=True)\n    def _graph_fn_call(self, text_inputs):\n        """"""\n        Args:\n            text_inputs (SingleDataOp): The Text input to generate a hash bucket for.\n\n        Returns:\n            tuple:\n                - SingleDataOp: The hash lookup table (int64) that can be used as input to embedding-lookups.\n                - SingleDataOp: The length (number of words) of the longest string in the `text_input` batch.\n        """"""\n        if get_backend() == ""tf"":\n            # Split the input string.\n            split_text_inputs = tf.string_split(source=text_inputs, delimiter=self.delimiter)\n            # Build a tensor of n rows (number of items in text_inputs) words with\n            dense = tf.sparse_tensor_to_dense(sp_input=split_text_inputs, default_value="""")\n\n            length = tf.reduce_sum(input_tensor=tf.cast(x=tf.not_equal(x=dense, y=""""), dtype=tf.int32), axis=-1)\n            if self.hash_function == ""fast"":\n                hash_bucket = tf.string_to_hash_bucket_fast(input=dense, num_buckets=self.num_hash_buckets)\n            else:\n                hash_bucket = tf.string_to_hash_bucket_strong(input=dense,\n                                                              num_buckets=self.num_hash_buckets,\n                                                              key=self.hash_keys)\n\n            # Int64 is tf\'s default for `string_to_hash_bucket` operation: Can leave as is.\n            if self.dtype != ""int64"":\n                hash_bucket = tf.cast(x=hash_bucket, dtype=dtype_(self.dtype))\n\n            # Hash-bucket output is always batch-major.\n            hash_bucket._batch_rank = 0\n            hash_bucket._time_rank = 1\n\n            return hash_bucket, length\n'"
rlgraph/components/neural_networks/impala/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.neural_networks.impala.impala_networks import LargeIMPALANetwork\nfrom rlgraph.components.neural_networks.impala.impala_networks import SmallIMPALANetwork\n\n\n__all__ = [""LargeIMPALANetwork"", ""SmallIMPALANetwork""]\n'"
rlgraph/components/neural_networks/impala/impala_networks.py,0,"b'# Copyright 2018/2019 The RLgraph authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.components.common.repeater_stack import RepeaterStack\nfrom rlgraph.components.layers.nn.concat_layer import ConcatLayer\nfrom rlgraph.components.layers.nn.conv2d_layer import Conv2DLayer\nfrom rlgraph.components.layers.nn.dense_layer import DenseLayer\nfrom rlgraph.components.layers.nn.lstm_layer import LSTMLayer\nfrom rlgraph.components.layers.nn.maxpool2d_layer import MaxPool2DLayer\nfrom rlgraph.components.layers.nn.nn_layer import NNLayer\nfrom rlgraph.components.layers.nn.residual_layer import ResidualLayer\nfrom rlgraph.components.layers.preprocessing.container_splitter import ContainerSplitter\nfrom rlgraph.components.layers.preprocessing.multiply_divide import Divide\nfrom rlgraph.components.layers.preprocessing.reshape import ReShape\nfrom rlgraph.components.layers.strings.embedding_lookup import EmbeddingLookup\nfrom rlgraph.components.layers.strings.string_to_hash_bucket import StringToHashBucket\nfrom rlgraph.components.neural_networks.neural_network import NeuralNetwork\nfrom rlgraph.components.neural_networks.stack import Stack\nfrom rlgraph.utils.decorators import rlgraph_api\n\n\nclass IMPALANetwork(NeuralNetwork):\n    """"""\n    The base class for both ""large and small architecture"" versions of the networks used in [1].\n\n    [1] IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures - Espeholt, Soyer,\n        Munos et al. - 2018 (https://arxiv.org/abs/1802.01561)\n    """"""\n    def __init__(self, worker_sample_size=100, scope=""impala-network"", **kwargs):\n        """"""\n        Args:\n            worker_sample_size (int): How many time-steps an IMPALA actor will have performed in one rollout.\n        """"""\n        super(IMPALANetwork, self).__init__(scope=scope, **kwargs)\n\n        self.worker_sample_size = worker_sample_size\n\n        # Create all needed sub-components.\n\n        # ContainerSplitter for the Env signal (dict of 4 keys: for env image, env text, previous action and reward).\n        self.splitter = ContainerSplitter(""RGB_INTERLEAVED"", ""INSTR"", ""previous_action"", ""previous_reward"",\n                                          scope=""input-splitter"")\n\n        # Fold the time rank into the batch rank.\n        self.time_rank_fold_before_lstm = ReShape(fold_time_rank=True, scope=""time-rank-fold-before-lstm"")\n        self.time_rank_unfold_before_lstm = ReShape(unfold_time_rank=True, time_major=True,\n                                                    scope=""time-rank-unfold-before-lstm"")\n\n        # The Image Processing Stack (left side of ""Large Architecture"" Figure 3 in [1]).\n        # Conv2D column + ReLU + fc(256) + ReLU.\n        self.image_processing_stack = self.build_image_processing_stack()\n\n        # The text processing pipeline: Takes a batch of string tensors as input, creates a hash-bucket thereof,\n        # and passes the output of the hash bucket through an embedding-lookup(20) layer. The output of the embedding\n        # lookup is then passed through an LSTM(64).\n        self.text_processing_stack = self.build_text_processing_stack()\n\n        #self.debug_slicer = Slice(scope=""internal-states-slicer"", squeeze=True)\n\n        # The concatenation layer (concatenates outputs from image/text processing stacks, previous action/reward).\n        self.concat_layer = ConcatLayer()\n\n        # The main LSTM (going into the ActionAdapter (next in the Policy Component that uses this NN Component)).\n        # Use time-major as it\'s faster (say tf docs).\n        self.main_lstm = LSTMLayer(units=256, scope=""lstm-256"", time_major=True, static_loop=self.worker_sample_size)\n\n        # Add all sub-components to this one.\n        self.add_components(\n            self.splitter, self.image_processing_stack, self.text_processing_stack,\n            self.concat_layer,\n            self.main_lstm,\n            self.time_rank_fold_before_lstm, self.time_rank_unfold_before_lstm,\n            #self.debug_slicer\n        )\n\n    @staticmethod\n    def build_image_processing_stack():\n        """"""\n        Builds the image processing pipeline for IMPALA and returns it.\n        """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def build_text_processing_stack():\n        """"""\n        Helper function to build the text processing pipeline for both the large and small architectures, consisting of:\n        - ReShape preprocessor to fold the incoming time rank into the batch rank.\n        - StringToHashBucket Layer taking a batch of sentences and converting them to an indices-table of dimensions:\n          cols=length of longest sentences in input\n          rows=number of items in the batch\n          The cols dimension could be interpreted as the time rank into a consecutive LSTM. The StringToHashBucket\n          Component returns the sequence length of each batch item for exactly that purpose.\n        - Embedding Lookup Layer of embedding size 20 and number of rows == num_hash_buckets (see previous layer).\n        - LSTM processing the batched sequences of words coming from the embedding layer as batches of rows.\n        """"""\n        num_hash_buckets = 1000\n\n        # Create a hash bucket from the sentences and use that bucket to do an embedding lookup (instead of\n        # a vocabulary).\n        string_to_hash_bucket = StringToHashBucket(num_hash_buckets=num_hash_buckets)\n        embedding = EmbeddingLookup(embed_dim=20, vocab_size=num_hash_buckets, pad_empty=True)\n        # The time rank for the LSTM is now the sequence of words in a sentence, NOT the original env time rank.\n        # We will only use the last output of the LSTM-64 for further processing as that is the output after having\n        # seen all words in the sentence.\n        # The original env stepping time rank is currently folded into the batch rank and must be unfolded again before\n        # passing it into the main LSTM.\n        lstm64 = LSTMLayer(units=64, scope=""lstm-64"", time_major=False)\n\n        tuple_splitter = ContainerSplitter(tuple_length=2, scope=""tuple-splitter"")\n\n        def custom_call(self, inputs):\n            hash_bucket, lengths = self.sub_components[""string-to-hash-bucket""].call(inputs)\n\n            embedding_output = self.sub_components[""embedding-lookup""].call(hash_bucket)\n\n            # Return only the last output (sentence of words, where we are not interested in intermediate results\n            # where the LSTM has not seen the entire sentence yet).\n            # Last output is the final internal h-state (slot 1 in the returned LSTM tuple; slot 0 is final c-state).\n            lstm_output = self.sub_components[""lstm-64""].call(embedding_output, sequence_length=lengths)\n            lstm_final_internals = lstm_output[""last_internal_states""]\n\n            # Need to split once more because the LSTM state is always a tuple of final c- and h-states.\n            _, lstm_final_h_state = self.sub_components[""tuple-splitter""].call(lstm_final_internals)\n\n            return lstm_final_h_state\n\n        text_processing_stack = Stack(\n            string_to_hash_bucket, embedding, lstm64, tuple_splitter,\n            api_methods={(""call"", custom_call)}, scope=""text-stack""\n        )\n\n        return text_processing_stack\n\n    @rlgraph_api\n    def call(self, input_dict, internal_states=None):\n        # Split the input dict coming directly from the Env.\n        _, _, _, orig_previous_reward = self.splitter.call(input_dict)\n\n        folded_input = self.time_rank_fold_before_lstm.call(input_dict)\n        image, text, previous_action, previous_reward = self.splitter.call(folded_input)\n\n        # Get the left-stack (image) and right-stack (text) output (see [1] for details).\n        text_processing_output = self.text_processing_stack.call(text)\n        image_processing_output = self.image_processing_stack.call(image)\n\n        # Concat everything together.\n        concatenated_data = self.concat_layer.call(\n            image_processing_output, text_processing_output, previous_action, previous_reward\n        )\n\n        unfolded_concatenated_data = self.time_rank_unfold_before_lstm.call(concatenated_data, orig_previous_reward)\n\n        # Feed concat\'d input into main LSTM(256).\n        lstm_output = self.main_lstm.call(unfolded_concatenated_data, internal_states)\n\n        return lstm_output\n\n\nclass LargeIMPALANetwork(IMPALANetwork):\n\n    @staticmethod\n    def build_image_processing_stack():\n        """"""\n        Constructs a ReShape preprocessor to fold the time rank into the batch rank.\n\n        Then builds the 3 sequential Conv2D blocks that process the image information.\n        Each of these 3 blocks consists of:\n        - 1 Conv2D layer followed by a MaxPool2D\n        - 2 residual blocks, each of which looks like:\n            - ReLU + Conv2D + ReLU + Conv2D + element-wise add with original input\n\n        Then adds: ReLU + fc(256) + ReLU.\n        """"""\n        # Collect components for image stack before unfolding time-rank going into main LSTM.\n        sub_components = list()\n\n        # Divide by 255\n        sub_components.append(Divide(divisor=255, scope=""divide-255""))\n\n        for i, num_filters in enumerate([16, 32, 32]):\n            # Conv2D plus MaxPool2D.\n            conv2d_plus_maxpool = Stack(\n                Conv2DLayer(filters=num_filters, kernel_size=3, strides=1, padding=""same""),\n                MaxPool2DLayer(pool_size=3, strides=2, padding=""same""),\n                scope=""conv-max""\n            )\n\n            # Single unit for the residual layers (ReLU + Conv2D 3x3 stride=1).\n            residual_unit = Stack(\n                NNLayer(activation=""relu""),  # single ReLU\n                Conv2DLayer(filters=num_filters, kernel_size=3, strides=1, padding=""same""),\n                scope=""relu-conv""\n            )\n            # Residual Layer.\n            residual_layer = ResidualLayer(residual_unit=residual_unit, repeats=2)\n            # Repeat same residual layer 2x.\n            residual_repeater = RepeaterStack(sub_component=residual_layer, repeats=2)\n\n            sub_components.append(Stack(conv2d_plus_maxpool, residual_repeater, scope=""conv-unit-{}"".format(i)))\n\n        # A Flatten preprocessor and then an fc block (surrounded by ReLUs) and a time-rank-unfolding.\n        sub_components.extend([\n            ReShape(flatten=True, scope=""flatten""),  # Flattener (to flatten Conv2D output for the fc layer).\n            NNLayer(activation=""relu"", scope=""relu-1""),  # ReLU 1\n            DenseLayer(units=256),  # Dense layer.\n            NNLayer(activation=""relu"", scope=""relu-2""),  # ReLU 2\n        ])\n\n        image_stack = Stack(sub_components, scope=""image-stack"")\n\n        return image_stack\n\n\nclass SmallIMPALANetwork(IMPALANetwork):\n    """"""\n    The ""small/shallow architecture"" version of the network used in [1].\n\n    [1] IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures - Espeholt, Soyer,\n        Munos et al. - 2018 (https://arxiv.org/abs/1802.01561)\n    """"""\n\n    @staticmethod\n    def build_image_processing_stack():\n        """"""\n        Constructs a ReShape preprocessor to fold the time rank into the batch rank.\n\n        Then builds the 2 Conv2D Layers followed by ReLUs.\n\n        Then adds: fc(256) + ReLU.\n        """"""\n        # Collect components for image stack before unfolding time-rank going into main LSTM.\n        sub_components = list()\n\n        # Divide by 255\n        sub_components.append(Divide(divisor=255, scope=""divide-255""))\n\n        for i, (num_filters, kernel_size, stride) in enumerate(zip([16, 32], [8, 4], [4, 2])):\n            # Conv2D plus ReLU activation function.\n            conv2d = Conv2DLayer(\n                filters=num_filters, kernel_size=kernel_size, strides=stride, padding=""same"",\n                activation=""relu"", scope=""conv2d-{}"".format(i)\n            )\n            sub_components.append(conv2d)\n\n        # A Flatten preprocessor and then an fc block (surrounded by ReLUs) and a time-rank-unfolding.\n        sub_components.extend([\n            ReShape(flatten=True, scope=""flatten""),  # Flattener (to flatten Conv2D output for the fc layer).\n            DenseLayer(units=256),  # Dense layer.\n            NNLayer(activation=""relu"", scope=""relu-before-lstm""),\n        ])\n\n        #stack_before_unfold = <- formerly known as\n        image_stack = Stack(sub_components, scope=""image-stack"")\n\n        return image_stack\n'"
rlgraph/components/neural_networks/sac/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
rlgraph/components/neural_networks/sac/sac_networks.py,0,"b'# Copyright 2018/2019 The RLgraph authors, All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom rlgraph import get_backend\nfrom rlgraph.components.layers import Layer, ConcatLayer\nfrom rlgraph.components.neural_networks.stack import Stack\nfrom rlgraph.components.neural_networks.value_function import ValueFunction\nfrom rlgraph.utils.decorators import rlgraph_api\n\nif get_backend() == ""tf"":\n    pass\nelif get_backend() == ""pytorch"":\n    pass\n\n\nclass SACValueNetwork(ValueFunction):\n    """"""\n    Value network for SAC which must be able to merge different input types.\n    """"""\n    def __init__(self, scope=""sac-value-network"", **kwargs):\n        super(SACValueNetwork, self).__init__(scope=scope, **kwargs)\n\n        # Add all sub-components to this one.\n        if self.image_stack is not None:\n            self.add_components(self.image_stack)\n        self.concat_layer = ConcatLayer()\n        self.add_components(self.concat_layer , self.dense_stack)\n\n    def build_value_function(self):\n        """"""\n        Builds a dense stack and optionally an image stack.\n        """"""\n        if self.use_image_stack:\n            image_components = []\n            dense_components = []\n            for layer_spec in self.network_spec:\n                if layer_spec[""type""] in [""conv2d"", ""reshape""]:\n                    image_components.append(Layer.from_spec(layer_spec))\n\n            self.image_stack = Stack(image_components, scope=""image-stack"")\n\n            # Remainings layers should be dense.\n            for layer_spec in self.network_spec[len(image_components):]:\n                assert layer_spec[""type""] == ""dense"", ""Only expecting dense layers after image "" \\\n                                                      ""stack but found spec: {}."".format(layer_spec)\n                dense_components.append(layer_spec)\n\n            dense_components.append(self.value_layer_spec)\n            self.dense_stack = Stack(dense_components, scope=""dense-stack"")\n        else:\n            # Assume dense network otherwise -> onyl a single stack.\n            dense_components = []\n            for layer_spec in self.network_spec:\n                assert layer_spec[""type""] == ""dense"", ""Only dense layers allowed if not using"" \\\n                                                      "" image stack in this network.""\n                dense_components.append(Layer.from_spec(layer_spec))\n            dense_components.append(self.value_layer_spec)\n            self.dense_stack = Stack(dense_components, scope=""dense-stack"")\n\n    @rlgraph_api\n    def state_action_value(self, states, actions, internal_states=None):\n        """"""\n        Computes Q(s,a) by passing states and actions through one or multiple processing stacks..\n        """"""\n        if self.use_image_stack:\n            image_processing_output = self.image_stack.call(states)\n            state_actions = self.concat_layer.call(image_processing_output, actions)\n            dense_output = self.dense_stack.call(state_actions)\n        else:\n            # Concat states and actions, then pass through.\n            state_actions = self.concat_layer.call(states, actions)\n            dense_output = self.dense_stack.call(state_actions)\n        return dense_output\n'"
rlgraph/execution/distributed_tf/impala/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n'"
rlgraph/execution/distributed_tf/impala/impala_worker.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\nfrom rlgraph.agents.impala_agents import IMPALAAgent\nfrom rlgraph.execution.worker import Worker\nfrom rlgraph.utils.util import default_dict\n\n\nclass IMPALAWorker(Worker):\n\n    def __init__(self, agent, **kwargs):\n        """"""\n        Args:\n            agent (IMPALAAgent): The IMPALAAgent object to use.\n            #num_steps (int): The number of steps (actions) to perform in the environment each rollout.\n        """"""\n        assert isinstance(agent, IMPALAAgent)\n\n        frameskip = agent.environment_stepper.environment_spec.get(""frameskip"", 1)\n\n        super(IMPALAWorker, self).__init__(agent=agent, frameskip=frameskip, **kwargs)\n\n        self.logger.info(\n            ""Initialized IMPALA worker (type {}) with 1 environment \'{}\' running inside Agent\'s EnvStepper ""\n            ""component."".format(self.agent.type, self.agent.environment_stepper.environment_spec)\n        )\n\n        # Global statistics.\n        self.env_frames = 0\n        self.finished_episode_rewards = list()\n        self.finished_episode_durations = list()\n        self.finished_episode_steps = list()\n\n        # Accumulated return over the running episode.\n        self.episode_returns = 0\n\n        # The number of steps taken in the running episode.\n        self.episode_timesteps = 0\n        # Wall time of the last start of the running episode.\n        #self.episode_starts = 0\n\n    def execute_timesteps(self, num_timesteps, max_timesteps_per_episode=0, update_spec=None, use_exploration=True,\n                          frameskip=None, reset=True):\n        """"""\n        Args:\n            num_timesteps (Optional[int]): The maximum number of timesteps to run. At least one of `num_timesteps` or\n                `num_episodes` must be provided.\n            use_exploration (Optional[bool]): Indicates whether to utilize exploration (epsilon or noise based)\n                when picking actions. Default: True.\n            max_timesteps_per_episode (Optional[int]): Can be used to limit the number of timesteps per episode.\n                Use None or 0 for no limit. Default: None.\n            update_spec (Optional[dict]): Update parameters. If None, the worker only performs rollouts.\n                Matches the structure of an Agent\'s update_spec dict and will be ""defaulted"" by that dict.\n                See `input_parsing/parse_update_spec.py` for more details.\n            frameskip (Optional[int]): How often actions are repeated after retrieving them from the agent.\n                Rewards are accumulated over the number of skips. Use None for the Worker\'s default value.\n            reset (bool): Whether to reset the environment and all the Worker\'s internal counters.\n                Default: True.\n\n        Returns:\n            dict: Execution statistics.\n        """"""\n        # Are we updating or just acting/observing?\n        update_spec = default_dict(update_spec, self.agent.update_spec)\n        self.set_update_schedule(update_spec)\n\n        num_timesteps = num_timesteps or 0\n        max_timesteps_per_episode = max_timesteps_per_episode or 0\n\n        # Stats.\n        timesteps_executed = 0\n        episodes_executed = 0\n\n        start = time.perf_counter()\n        if reset is True:\n            self.env_frames = 0\n            #self.finished_episode_returns = list()\n            self.finished_episode_steps = list()\n\n            #self.episode_returns = 0\n            self.episode_timesteps = 0\n\n            # TODO: Fix for vectorized Envs.\n            self.agent.call_api_method(""reset"")\n\n        # Only run everything for at most num_timesteps (if defined).\n        while not (0 < num_timesteps <= timesteps_executed):\n            # TODO right now everything comes back as single-env.\n            out = self.agent.call_api_method((""perform_n_steps_and_insert_into_fifo"", None, [0]))\n            timesteps_executed += self.agent.worker_sample_size\n\n            # Accumulate the reward over n env-steps (equals one action pick). n=self.frameskip.\n            #rewards = out[2]\n            terminals = out[3][1:]\n\n            self.env_frames += self.frameskip * self.agent.worker_sample_size\n\n            # Only render once per action.\n            #if self.render:\n            #    self.vector_env.environments[0].render()\n\n            #for i in range_(self.num_environments):\n            #    #self.episode_timesteps[i] += self.agent.worker_sample_size\n\n            for j, terminal in enumerate(terminals):  # TODO: <- [i]\n                self.episode_timesteps += 1\n\n                if 0 < max_timesteps_per_episode <= self.episode_timesteps:\n                    terminal = True\n\n                if terminal:\n                    episodes_executed += 1\n                    self.finished_episode_steps.append(self.episode_timesteps)\n                    self.logger.info(""Finished episode: actions={}."".format(self.episode_timesteps))\n                    self.episode_timesteps = 0\n\n            num_timesteps_reached = (0 < num_timesteps <= timesteps_executed)\n\n            if num_timesteps_reached:\n                break\n\n        total_time = (time.perf_counter() - start) or 1e-10\n\n        # Return values for current episode(s) if None have been completed.\n        #if len(self.finished_episode_returns) == 0:\n        #    #mean_episode_runtime = 0\n        #    mean_episode_reward = np.mean(self.episode_returns)\n        #    max_episode_reward = np.max(self.episode_returns)\n        #    final_episode_reward = self.episode_returns[0]\n        #else:\n        #    #mean_episode_runtime = np.mean(self.finished_episode_durations)\n        #    mean_episode_reward = np.mean(self.finished_episode_returns)\n        #    max_episode_reward = np.max(self.finished_episode_returns)\n        #    final_episode_reward = self.finished_episode_returns[-1]\n\n        results = dict(\n            runtime=total_time,\n            # Agent act/observe throughput.\n            timesteps_executed=timesteps_executed,\n            ops_per_second=(timesteps_executed / total_time),\n            # Env frames including action repeats.\n            env_frames=self.env_frames,\n            env_frames_per_second=(self.env_frames / total_time),\n            episodes_executed=episodes_executed,\n            episodes_per_minute=(episodes_executed/(total_time / 60)),\n            #mean_episode_runtime=mean_episode_runtime,\n            #mean_episode_reward=mean_episode_reward,\n            #max_episode_reward=max_episode_reward,\n            #final_episode_reward=final_episode_reward\n        )\n\n        # Total time of run.\n        self.logger.info(""Finished execution in {} s"".format(total_time))\n        # Total (RL) timesteps (actions) done (and timesteps/sec).\n        self.logger.info(""Time steps (actions) executed: {} ({} ops/s)"".\n                         format(results[\'timesteps_executed\'], results[\'ops_per_second\']))\n        # Total env-timesteps done (including action repeats) (and env-timesteps/sec).\n        self.logger.info(""Env frames executed (incl. action repeats): {} ({} frames/s)"".\n                         format(results[\'env_frames\'], results[\'env_frames_per_second\']))\n        # Total episodes done (and episodes/min).\n        self.logger.info(""Episodes finished: {} ({} episodes/min)"".\n                         format(results[\'episodes_executed\'], results[\'episodes_per_minute\']))\n        #self.logger.info(""Mean episode runtime: {}s"".format(results[\'mean_episode_runtime\']))\n        #self.logger.info(""Mean episode reward: {}"".format(results[\'mean_episode_reward\']))\n        #self.logger.info(""Max. episode reward: {}"".format(results[\'max_episode_reward\']))\n        #self.logger.info(""Final episode reward: {}"".format(results[\'final_episode_reward\']))\n\n        return results\n'"
rlgraph/execution/ray/apex/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom rlgraph.execution.ray.apex.apex_executor import ApexExecutor\nfrom rlgraph.execution.ray.apex.apex_memory import ApexMemory\nfrom rlgraph.execution.ray.apex.ray_memory_actor import RayMemoryActor\n\n__all__ = [""ApexExecutor"", ""ApexMemory"", ""RayMemoryActor""]\n'"
rlgraph/execution/ray/apex/apex_executor.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\nfrom threading import Thread\n\nfrom six.moves import queue\n\nfrom rlgraph import get_distributed_backend\nfrom rlgraph.agents import Agent\nfrom rlgraph.environments import Environment\nfrom rlgraph.execution.ray import RayValueWorker\nfrom rlgraph.execution.ray.apex.ray_memory_actor import RayMemoryActor\nfrom rlgraph.execution.ray.ray_executor import RayExecutor\nfrom rlgraph.execution.ray.ray_util import create_colocated_ray_actors, RayTaskPool, RayWeight\nfrom rlgraph.spaces import Dict\n\nif get_distributed_backend() == ""ray"":\n    import ray\n\n\nclass ApexExecutor(RayExecutor):\n    """"""\n    Implements the distributed update semantics of distributed prioritized experience replay (Ape-X),\n    as described in:\n\n    https://arxiv.org/abs/1803.00933\n    """"""\n    def __init__(self, environment_spec, agent_config, discard_queued_samples=False):\n        """"""\n        Args:\n            environment_spec (dict, callable): Environment spec or callable creating\n            an environment. Each worker in the cluster will instantiate\n                an environment using this spec.\n            agent_config (dict): Config dict containing agent and execution specs.\n            discard_queued_samples (bool): If true, discard samples if the learner queue is full instead\n                of blocking until free.\n        """"""\n        ray_spec = agent_config[""execution_spec""].pop(""ray_spec"")\n        self.apex_replay_spec = ray_spec.pop(""apex_replay_spec"")\n        self.worker_spec = ray_spec.pop(""worker_spec"")\n        self.discard_queued_samples = discard_queued_samples\n        super(ApexExecutor, self).__init__(executor_spec=ray_spec.pop(""executor_spec""),\n                                           environment_spec=environment_spec,\n                                           worker_spec=self.worker_spec)\n\n        # Must specify an agent type.\n        assert ""type"" in agent_config\n        self.agent_config = agent_config\n\n        # These are the Ray remote tasks which sample batches from the replay memory\n        # and pass them to the learner.\n        self.prioritized_replay_tasks = RayTaskPool()\n        self.replay_sampling_task_depth = self.executor_spec[""replay_sampling_task_depth""]\n        self.replay_batch_size = self.agent_config[""update_spec""][""batch_size""]\n        self.num_cpus_per_replay_actor = self.executor_spec.get(""num_cpus_per_replay_actor"",\n                                                                self.replay_sampling_task_depth)\n\n        # How often weights are synced to remote workers.\n        self.weight_sync_steps = self.executor_spec[""weight_sync_steps""]\n\n        # Necessary for target network updates.\n        self.weight_syncs_executed = 0\n        self.steps_since_weights_synced = {}\n\n        # These are the tasks actually interacting with the environment.\n        self.env_sample_tasks = RayTaskPool()\n        self.env_interaction_task_depth = self.executor_spec[""env_interaction_task_depth""]\n        self.worker_sample_size = self.executor_spec[""num_worker_samples""] + self.worker_spec[""n_step_adjustment""] - 1\n\n        assert not ray_spec, ""ERROR: ray_spec still contains items: {}"".format(ray_spec)\n        self.logger.info(""Setting up execution for Apex executor."")\n        self.setup_execution()\n\n    def setup_execution(self):\n        # Create local worker agent according to spec.\n        # Extract states and actions space.\n        environment = None\n        if isinstance(self.environment_spec, dict):\n            environment = Environment.from_spec(self.environment_spec)\n        elif hasattr(self.environment_spec, \'__call__\'):\n            environment = self.environment_spec()\n        self.agent_config[""state_space""] = environment.state_space\n        self.agent_config[""action_space""] = environment.action_space\n        self.apex_replay_spec[""memory_spec""][""state_space""] = environment.state_space\n\n        # Ray cannot serialise Dict, must be dict.\n        if isinstance(environment.action_space, Dict):\n            self.apex_replay_spec[""memory_spec""][""action_space""] = dict(environment.action_space)\n        else:\n            self.apex_replay_spec[""memory_spec""][""action_space""] = environment.action_space\n\n        # Start Ray cluster and connect to it.\n        self.local_agent = Agent.from_spec(self.agent_config)\n\n        # Set up worker thread for performing updates.\n        self.update_worker = UpdateWorker(\n            agent=self.local_agent,\n            in_queue_size=self.executor_spec[""learn_queue_size""]\n        )\n        self.ray_init()\n\n        # Create remote sample workers based on ray cluster spec.\n        self.num_replay_workers = self.executor_spec[""num_replay_workers""]\n        self.num_sample_workers = self.executor_spec[""num_sample_workers""]\n\n        self.logger.info(""Initializing {} local replay memories."".format(self.num_replay_workers))\n        # Update memory size for num of workers\n        shard_size = int(self.apex_replay_spec[""memory_spec""][""capacity""] / self.num_replay_workers)\n        self.apex_replay_spec[""memory_spec""][""capacity""] = shard_size\n        self.logger.info(""Shard size per memory: {}"".format(self.apex_replay_spec[""memory_spec""][""capacity""]))\n        min_sample_size = self.apex_replay_spec[""min_sample_memory_size""]\n        self.apex_replay_spec[""min_sample_memory_size""] = int(min_sample_size / self.num_replay_workers)\n        self.logger.info(""Sampling for learning starts at: {}"".format( self.apex_replay_spec[""min_sample_memory_size""]))\n\n        # Set sample batch size:\n        self.apex_replay_spec[""sample_batch_size""] = self.agent_config[""update_spec""][""batch_size""]\n        self.logger.info(""Sampling batch size {}"".format(self.apex_replay_spec[""sample_batch_size""]))\n\n        self.ray_local_replay_memories = create_colocated_ray_actors(\n            cls=RayMemoryActor.as_remote(num_cpus=self.num_cpus_per_replay_actor),\n            config=self.apex_replay_spec,\n            num_agents=self.num_replay_workers\n        )\n\n        # Create remote workers for data collection.\n        self.worker_spec[""worker_sample_size""] = self.worker_sample_size\n        self.logger.info(""Initializing {} remote data collection agents, sample size: {}"".format(\n            self.num_sample_workers, self.worker_spec[""worker_sample_size""]))\n        self.ray_env_sample_workers = self.create_remote_workers(\n            RayValueWorker, self.num_sample_workers, self.agent_config,\n            # *args\n            self.worker_spec, self.environment_spec, self.worker_frame_skip\n        )\n        self.init_tasks()\n\n    def init_tasks(self):\n        # Start learner thread.\n        self.update_worker.start()\n\n        # Prioritized replay sampling tasks via RayAgents.\n        for ray_memory in self.ray_local_replay_memories:\n            for _ in range(self.replay_sampling_task_depth):\n                # This initializes remote tasks to sample from the prioritized replay memories of each worker.\n                self.prioritized_replay_tasks.add_task(ray_memory, ray_memory.get_batch.remote())\n\n        # Env interaction tasks via RayWorkers which each\n        # have a local agent.\n        weights = RayWeight(self.local_agent.get_weights())\n        for ray_worker in self.ray_env_sample_workers:\n            ray_worker.set_weights.remote(weights)\n            self.steps_since_weights_synced[ray_worker] = 0\n\n            self.logger.info(""Synced worker {} weights, initializing sample tasks."".format(\n                self.worker_ids[ray_worker]))\n            for _ in range(self.env_interaction_task_depth):\n                self.env_sample_tasks.add_task(ray_worker, ray_worker.execute_and_get_with_count.remote())\n\n    def _execute_step(self):\n        """"""\n        Executes a workload on Ray. The main loop performs the following\n        steps until the specified number of steps or episodes is finished:\n\n        - Retrieve sample batches via Ray from remote workers\n        - Insert these into the local memory\n        - Have a separate learn thread sample batches from the memory and compute updates\n        - Sync weights to the shared model so remot eworkers can update their weights.\n        """"""\n        # Env steps done during this rollout.\n        env_steps = 0\n        update_steps = 0\n        discarded = 0\n        queue_inserts = 0\n        rewards = []\n        weights = None\n\n        # 1. Fetch results from RayWorkers.\n        completed_sample_tasks = list(self.env_sample_tasks.get_completed())\n        sample_batch_metrics = ray.get([task[1][1] for task in completed_sample_tasks])\n        for i, (ray_worker, (env_sample_obj_id, sample_size)) in enumerate(completed_sample_tasks):\n            # Randomly add env sample to a local replay actor.\n            random.choice(self.ray_local_replay_memories).observe.remote(env_sample_obj_id)\n            sample_steps = sample_batch_metrics[i][""batch_size""]\n            if len(sample_batch_metrics[i][""last_rewards""]) > 0:\n                rewards.extend(sample_batch_metrics[i][""last_rewards""])\n            env_steps += sample_steps\n\n            self.steps_since_weights_synced[ray_worker] += sample_steps\n            if self.steps_since_weights_synced[ray_worker] >= self.weight_sync_steps:\n                if weights is None or self.update_worker.update_done:\n                    self.update_worker.update_done = False\n                    weights = ray.put(RayWeight(self.local_agent.get_weights()))\n                # self.logger.debug(""Syncing weights for worker {}"".format(self.worker_ids[ray_worker]))\n                # self.logger.debug(""Weights type: {}, weights = {}"".format(type(weights), weights))\n                ray_worker.set_weights.remote(weights)\n                self.weight_syncs_executed += 1\n                self.steps_since_weights_synced[ray_worker] = 0\n\n            # Reschedule environment samples.\n            self.env_sample_tasks.add_task(ray_worker, ray_worker.execute_and_get_with_count.remote())\n\n        # 2. Fetch completed replay priority sampling task, move to worker, reschedule.\n        for ray_memory, replay_remote_task in self.prioritized_replay_tasks.get_completed():\n            # Immediately schedule new batch sampling tasks on these workers.\n            self.prioritized_replay_tasks.add_task(ray_memory, ray_memory.get_batch.remote())\n\n            # Retrieve results via id.\n            # self.logger.info(""replay task obj id {}"".format(replay_remote_task))\n            if self.discard_queued_samples and self.update_worker.input_queue.full():\n                discarded += 1\n            else:\n                sampled_batch = ray.get(object_ids=replay_remote_task)\n                # Pass to the agent doing the actual updates.\n                # The ray worker is passed along because we need to update its priorities later in the subsequent\n                # task (see loop below).\n                # Copy due to memory leaks in Ray, see https://github.com/ray-project/ray/pull/3484/\n                self.update_worker.input_queue.put((ray_memory, sampled_batch and sampled_batch.copy()))\n                queue_inserts += 1\n\n        # 3. Update priorities on priority sampling workers using loss values produced by update worker.\n        while not self.update_worker.output_queue.empty():\n            ray_memory, indices, loss_per_item = self.update_worker.output_queue.get()\n            # self.logger.info(\'indices = {}\'.format(batch[""indices""]))\n            # self.logger.info(\'loss = {}\'.format(loss_per_item))\n\n            ray_memory.update_priorities.remote(indices, loss_per_item)\n            # len of loss per item is update count.\n            update_steps += len(indices)\n\n        return env_steps, update_steps, {\n            ""discarded"": discarded,\n            ""queue_inserts"": queue_inserts,\n            ""rewards"": rewards\n        }\n\n\nclass UpdateWorker(Thread):\n    """"""\n    Executes learning separate from the main event loop as described in the Ape-X paper.\n    Communicates with the main thread via a queue.\n    """"""\n\n    def __init__(self, agent, in_queue_size):\n        """"""\n        Initializes the worker with a RLGraph agent and queues for\n\n        Args:\n            agent (Agent): RLGraph agent used to execute local updates.\n            input_queue (queue.Queue): Input queue the worker will use to poll samples.\n            output_queue (queue.Queue): Output queue the worker will use to push results of local\n                update computations.\n        """"""\n        super(UpdateWorker, self).__init__()\n\n        # Agent to use for updating.\n        self.agent = agent\n        self.input_queue = queue.Queue(maxsize=in_queue_size)\n        self.output_queue = queue.Queue()\n\n        # Terminate when host process terminates.\n        self.daemon = True\n\n        # Flag for main thread.\n        self.update_done = False\n\n    def run(self):\n        while True:\n            self.step()\n\n    def step(self):  # TODO: time-percentage calculation missing here\n        # Fetch input for update:\n        # Replay memory used.\n        memory_actor, sample_batch = self.input_queue.get()\n\n        if sample_batch is not None:\n            losses = self.agent.update(batch=sample_batch)  # TODO: pass in time-percentage\n            # Just pass back indices for updating.\n            self.output_queue.put((memory_actor, sample_batch[""indices""], losses[1]))\n            self.update_done = True\n'"
rlgraph/execution/ray/apex/apex_memory.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport operator\nfrom six.moves import xrange as range_\n\nfrom rlgraph.utils import SMALL_NUMBER\nfrom rlgraph.utils.specifiable import Specifiable\nfrom rlgraph.components.helpers.mem_segment_tree import MemSegmentTree, MinSumSegmentTree\nfrom rlgraph.execution.ray.ray_util import ray_decompress\n\n\nclass ApexMemory(Specifiable):\n    """"""\n    Apex prioritized replay implementing compression.\n    """"""\n    def __init__(self, state_space=None, action_space=None, capacity=1000, alpha=1.0, beta=1.0):\n        """"""\n        Args:\n            state_space (dict): State spec.\n            action_space (dict): Actions spec.\n            capacity (int): Max capacity.\n            alpha (float): Initial weight.\n            beta (float): Prioritisation factor.\n        """"""\n        super(ApexMemory, self).__init__()\n\n        self.state_space = state_space\n        self.action_space = action_space\n        self.container_actions = isinstance(action_space, dict)\n        self.memory_values = []\n        self.index = 0\n        self.capacity = capacity\n        self.size = 0\n        self.max_priority = 1.0\n        self.alpha = alpha\n        self.beta = beta\n\n        self.default_new_weight = np.power(self.max_priority, self.alpha)\n        self.priority_capacity = 1\n        while self.priority_capacity < self.capacity:\n            self.priority_capacity *= 2\n\n        # Create segment trees, initialize with neutral elements.\n        sum_values = [0.0 for _ in range_(2 * self.priority_capacity)]\n        sum_segment_tree = MemSegmentTree(sum_values, self.priority_capacity, operator.add)\n        min_values = [float(\'inf\') for _ in range_(2 * self.priority_capacity)]\n        min_segment_tree = MemSegmentTree(min_values, self.priority_capacity, min)\n        self.merged_segment_tree = MinSumSegmentTree(\n            sum_tree=sum_segment_tree,\n            min_tree=min_segment_tree,\n            capacity=self.priority_capacity\n        )\n\n    def insert_records(self, record):\n        # TODO: This has the record interface, but actually expects a specific structure anyway, so\n        # may as well change API?\n        if self.index >= self.size:\n            self.memory_values.append(record)\n        else:\n            self.memory_values[self.index] = record\n\n        # Weights. # TODO this is problematic due to index not existing.\n        if record[5] is not None:\n            self.merged_segment_tree.insert(self.index, record[5] ** self.alpha)\n        else:\n            self.merged_segment_tree.insert(self.index, self.max_priority ** self.alpha)\n\n        # Update indices.\n        self.index = (self.index + 1) % self.capacity\n        self.size = min(self.size + 1, self.capacity)\n\n    def read_records(self, indices):\n        """"""\n        Obtains record values for the provided indices.\n\n        Args:\n            indices (ndarray): Indices to read. Assumed to be not contiguous.\n\n        Returns:\n             dict: Record value dict.\n        """"""\n        states = []\n        if self.container_actions:\n            actions = {k: [] for k in self.action_space.keys()}\n        else:\n            actions = []\n        rewards = []\n        terminals = []\n        next_states = []\n        for index in indices:\n            state, action, reward, terminal, next_state, weight = self.memory_values[index]\n            states.append(ray_decompress(state))\n\n            if self.container_actions:\n                for name in self.action_space.keys():\n                    actions[name].append(action[name])\n            else:\n                actions.append(action)\n            rewards.append(reward)\n            terminals.append(terminal)\n            next_states.append(ray_decompress(next_state))\n\n        if self.container_actions:\n            for name in self.action_space.keys():\n                actions[name] = np.squeeze(np.array(actions[name]))\n        else:\n            actions = np.array(actions)\n        return dict(\n            states=np.asarray(states),\n            actions=actions,\n            rewards=np.asarray(rewards),\n            terminals=np.asarray(terminals),\n            next_states=np.asarray(next_states)\n        )\n\n    def get_records(self, num_records):\n        indices = []\n        prob_sum = self.merged_segment_tree.sum_segment_tree.get_sum(0, self.size)\n        samples = np.random.random(size=(num_records,)) * prob_sum\n        for sample in samples:\n            indices.append(self.merged_segment_tree.sum_segment_tree.index_of_prefixsum(prefix_sum=sample))\n\n        sum_prob = self.merged_segment_tree.sum_segment_tree.get_sum()\n        min_prob = self.merged_segment_tree.min_segment_tree.get_min_value() / sum_prob + SMALL_NUMBER\n        max_weight = (min_prob * self.size) ** (-self.beta)\n        weights = []\n        for index in indices:\n            sample_prob = self.merged_segment_tree.sum_segment_tree.get(index) / sum_prob\n            weight = (sample_prob * self.size) ** (-self.beta)\n            weights.append(weight / max_weight)\n\n        return self.read_records(indices=indices), np.asarray(indices), np.asarray(weights)\n\n    def update_records(self, indices, update):\n        for index, loss in zip(indices, update):\n            self.merged_segment_tree.insert(index, loss ** self.alpha)\n            self.max_priority = max(self.max_priority, loss)\n'"
rlgraph/execution/ray/apex/ray_memory_actor.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom rlgraph.utils import SMALL_NUMBER\nfrom six.moves import xrange as range_\nfrom rlgraph import get_distributed_backend\nfrom rlgraph.execution.ray.apex.apex_memory import ApexMemory\nfrom rlgraph.execution.ray.ray_actor import RayActor\n\nif get_distributed_backend() == ""ray"":\n    import ray\n\n\nclass RayMemoryActor(RayActor):\n    """"""\n    An in-memory prioritized replay worker used to accelerate memory interaction in Ape-X.\n    """"""\n    def __init__(self, apex_replay_spec):\n        """"""\n        Args:\n            apex_replay_spec (dict): Specifies behaviour of this replay actor. Must contain key ""memory_spec"".\n        """"""\n        # N.b. The memory spec contains type PrioritizedReplay because that is\n        # used for the agent. We hence do not use from_spec but just read the relevant\n        # args.\n        self.min_sample_memory_size = apex_replay_spec[""min_sample_memory_size""]\n        self.clip_rewards = apex_replay_spec.get(""clip_rewards"", True)\n        self.sample_batch_size = apex_replay_spec[""sample_batch_size""]\n        self.memory = ApexMemory(**apex_replay_spec[""memory_spec""])\n\n    @classmethod\n    def as_remote(cls, num_cpus=None, num_gpus=None):\n        return ray.remote(num_cpus=num_cpus, num_gpus=num_gpus)(cls)\n\n    def get_batch(self):\n        """"""\n        Samples a batch from the replay memory.\n\n        Returns:\n            dict: Sample batch\n\n        """"""\n        if self.memory.size < self.min_sample_memory_size:\n            return None\n        else:\n            batch, indices, weights = self.memory.get_records(self.sample_batch_size)\n            # Merge into one dict to only return one future in ray.\n            batch[""indices""] = indices\n            batch[""importance_weights""] = weights\n            return batch\n\n    def observe(self, env_sample):\n        """"""\n        Observes experience(s).\n\n        N.b. For performance reason, data layout is slightly different for apex.\n        """"""\n        records = env_sample.get_batch()\n        num_records = len(records[\'states\'])\n\n        # TODO port to tf PR behaviour.\n        if self.clip_rewards:\n            rewards = np.sign(records[""rewards""])\n        else:\n            rewards = records[""rewards""]\n        for i in range_(num_records):\n            # If Actions is dict with vectors per key, convert to single dict.\n            if isinstance(records[""actions""], dict):\n                action = {k: v[i] for k, v in records[""actions""].items()}\n            else:\n                action = records[""actions""][i]\n\n            self.memory.insert_records((\n                records[""states""][i],\n                action,\n                rewards[i],\n                records[""terminals""][i],\n                records[""next_states""][i],\n                records[""importance_weights""][i]\n            ))\n\n    def update_priorities(self, indices, loss):\n        """"""\n        Updates priorities of provided indices in replay memory via externally\n        provided loss.\n\n        Args:\n            indices (ndarray): Indices to update in replay memory.\n            loss (ndarray):  Loss values for indices.\n        """"""\n        loss = np.abs(loss) + SMALL_NUMBER\n        self.memory.update_records(indices, loss)\n'"
rlgraph/tests/agent_learning/long_tasks/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n'"
rlgraph/tests/agent_learning/long_tasks/test_apex_agent_long_task_learning.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport ray\nimport unittest\nimport time\nfrom rlgraph.agents import ApexAgent\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.execution.ray.apex import ApexExecutor\nfrom rlgraph.execution.ray import RayValueWorker\nfrom rlgraph.tests.test_util import config_from_path\n\n\nclass TestApexAgentLongTaskLearning(unittest.TestCase):\n    """"""\n    Tests whether the Apex Agent can start learning in pong.\n\n    WARNING: This test requires large amounts of memory due to large buffer size.\n    """"""\n    env_spec = dict(\n        type=""openai"",\n        gym_env=""PongNoFrameskip-v4"",\n        # The frameskip in the agent config will trigger worker skips, this\n        # is used for internal env.\n        frameskip=4,\n        max_num_noops=30,\n        episodic_life=False,\n        fire_reset=True\n    )\n\n    def test_worker_init(self):\n        """"""\n        Tests if workers initialize without problems for the pong config.\n        """"""\n        agent_config = config_from_path(""configs/ray_apex_for_pong.json"")\n\n        # Long initialization times can lead to Ray crashes.\n        start = time.monotonic()\n        executor = ApexExecutor(\n            environment_spec=self.env_spec,\n            agent_config=agent_config,\n        )\n        end = time.monotonic() - start\n        print(""Initialized {} workers in {} s."".format(\n            executor.num_sample_workers, end\n        ))\n        executor.test_worker_init()\n\n    def test_worker_update(self):\n        """"""\n        Tests if a worker can update from an external batch correct including all\n        corrections and postprocessing using the pong spec.\n\n        N.b. this test does not use Ray.\n        """"""\n        ray.init()\n        agent_config = config_from_path(""configs/ray_apex_for_pong.json"")\n        ray_spec = agent_config[""execution_spec""].pop(""ray_spec"")\n        worker_cls = RayValueWorker.as_remote().remote\n        ray_spec[""worker_spec""][""worker_sample_size""] = 198\n        ray_spec[""worker_spec""][""worker_executes_exploration""] = True\n        ray_spec[""worker_spec""][""ray_exploration""] = 0.4\n\n        worker = worker_cls(agent_config, ray_spec[""worker_spec""], self.env_spec,)\n        time.sleep(5)\n\n        start = time.perf_counter()\n        task = worker.execute_and_get_with_count.remote()\n        result, count = ray.get(task)\n        task_time = time.perf_counter() - start\n        print(""internal result metrics = {}, external task time = {},""\n              ""external throughput = {}"".format(result.get_metrics(), task_time, 198 / task_time))\n\n    def test_initial_training_pong(self):\n        """"""\n        Tests if Apex can start learning pong effectively on ray.\n        """"""\n        agent_config = config_from_path(""configs/ray_apex_for_pong.json"")\n        executor = ApexExecutor(\n            environment_spec=self.env_spec,\n            agent_config=agent_config,\n        )\n\n        # Executes actual workload.\n        result = executor.execute_workload(workload=dict(\n            num_timesteps=10000000, report_interval=10000, report_interval_min_seconds=10)\n        )\n        print(""Finished executing workload:"")\n        print(result)\n'"
rlgraph/tests/agent_learning/long_tasks/test_dqn_agent_long_task_learning.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\nimport logging\n\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.agents import Agent\nfrom rlgraph.execution import SingleThreadedWorker\nfrom rlgraph.spaces import FloatBox\nfrom rlgraph.utils import root_logger\nfrom rlgraph.tests.test_util import config_from_path\n\n\nclass TestDQNAgentLongTaskLearning(unittest.TestCase):\n    """"""\n    Tests whether the DQNAgent can learn in tough environments.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    pong_preprocessed_state_space = FloatBox(shape=(80, 80, 4), add_batch_rank=True)\n\n    def test_dqn_on_pong(self):\n        """"""\n        Creates a DQNAgent and runs it via a Runner on an openAI Pong Env.\n        """"""\n        env = OpenAIGymEnv(""Pong-v0"", frameskip=4, max_num_noops=30, episodic_life=True, visualize=False)\n        agent_config = config_from_path(""configs/dqn_agent_for_pong.json"")\n        preprocessing_spec = agent_config.pop(""preprocessor_spec"")\n        agent = Agent.from_spec(\n            # Uses 2015 DQN parameters as closely as possible.\n            agent_config,\n            state_space=self.pong_preprocessed_state_space,\n            # Try with ""reduced"" action space (actually only 3 actions, up, down, no-op)\n            action_space=env.action_space\n        )\n\n        time_steps = 4000000\n        worker = SingleThreadedWorker(\n            env_spec=lambda: env, agent=agent, render=True,\n            preprocessing_spec=preprocessing_spec,\n            worker_executes_preprocessing=True\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        #self.assertEqual(results[""timesteps_executed""], time_steps)\n        #self.assertEqual(results[""env_frames""], time_steps)\n        #self.assertAlmostEqual(results[""mean_episode_reward""], -2.9207317073170733)\n        #self.assertAlmostEqual(results[""max_episode_reward""], 0.0)\n        #self.assertEqual(results[""episodes_executed""], 328)\n\n'"
rlgraph/tests/agent_learning/long_tasks/test_impala_agent_long_task_learning.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport numpy as np\nimport time\nimport unittest\n\nfrom rlgraph.environments import OpenAIGymEnv\nfrom rlgraph.agents import IMPALAAgent\nfrom rlgraph.spaces import FloatBox\nfrom rlgraph.utils import root_logger\nfrom rlgraph.tests.test_util import config_from_path\n\n\nclass TestIMPALAAgentLongTaskLearning(unittest.TestCase):\n    """"""\n    Tests whether the DQNAgent can learn in tough environments.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    #atari_preprocessed_state_space = FloatBox(shape=(80, 80, 4), add_batch_rank=True)\n    #atari_preprocessing_spec = [\n    #    dict(type=""image_crop"", x=0, y=25, width=160, height=160),\n    #    dict(type=""image_resize"", width=80, height=80),\n    #    dict(type=""grayscale"", keep_rank=True),\n    #    dict(type=""divide"", divisor=255,),\n    #    dict(type=""sequence"", sequence_length=4, batch_size=1, add_rank=False)\n    #]\n\n    def test_impala_on_outbreak(self):\n        """"""\n        Creates a DQNAgent and runs it via a Runner on an openAI Pong Env.\n        """"""\n        env = OpenAIGymEnv(""Breakout-v0"", frameskip=4, max_num_noops=30, episodic_life=True, visualize=False)\n        config_ = config_from_path(""configs/impala_agent_for_breakout.json"")\n        agent = IMPALAAgent.from_spec(\n            config_,\n            state_space=env.state_space,\n            action_space=env.action_space,\n        )\n\n        learn_updates = 4000000\n        mean_returns = []\n        for i in range(learn_updates):\n            ret = agent.update()\n            mean_return = self._calc_mean_return(ret)\n            mean_returns.append(mean_return)\n            print(""i={} Loss={:.4} Avg-reward={:.2}"".format(i, float(ret[1]), mean_return))\n\n        time.sleep(3)\n        agent.terminate()\n        time.sleep(3)\n\n    @staticmethod\n    def _calc_mean_return(records):\n        size = records[3][""rewards""].size\n        rewards = records[3][""rewards""].reshape((size,))\n        terminals = records[3][""terminals""].reshape((size,))\n        returns = list()\n        return_ = 0.0\n        for r, t in zip(rewards, terminals):\n            return_ += r\n            if t:\n                returns.append(return_)\n                return_ = 0.0\n\n        return np.mean(returns)\n'"
rlgraph/tests/agent_learning/short_tasks/__init__.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n'"
rlgraph/tests/agent_learning/short_tasks/test_actor_critic_agent_short_task_learning.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport unittest\n\nfrom rlgraph.environments import OpenAIGymEnv, GridWorld\nfrom rlgraph.agents import ActorCriticAgent\nfrom rlgraph.execution import SingleThreadedWorker\nfrom rlgraph.utils import root_logger\nfrom rlgraph.tests.test_util import config_from_path, recursive_assert_almost_equal\n\n\nclass TestActorCriticShortTaskLearning(unittest.TestCase):\n    """"""\n    Tests whether the Actor-critic can learn in simple environments.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    is_windows = os.name == ""nt""\n\n    def test_actor_critic_on_2x2_grid_world(self):\n        """"""\n        Creates a Actor-critic and runs it via a Runner on the 2x2 Grid World Env.\n        """"""\n        env = GridWorld(world=""2x2"")\n        agent = ActorCriticAgent.from_spec(\n            config_from_path(""configs/actor_critic_agent_for_2x2_gridworld.json""),\n            state_space=GridWorld.grid_world_2x2_flattened_state_space,\n            action_space=env.action_space,\n            execution_spec=dict(seed=13),\n        )\n\n        time_steps = 30000\n        worker = SingleThreadedWorker(\n            env_spec=lambda: env,\n            agent=agent,\n            worker_executes_preprocessing=True,\n            preprocessing_spec=GridWorld.grid_world_2x2_preprocessing_spec\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        print(results)\n\n        # Assume we have learned something.\n        self.assertGreater(results[""mean_episode_reward""], -0.1)\n\n    def test_actor_critic_on_cart_pole(self):\n        """"""\n        Creates an Actor-critic and runs it via a Runner on the CartPole Env.\n        """"""\n        env_spec = dict(type=""open-ai-gym"", gym_env=""CartPole-v0"", visualize=False)  #self.is_windows)\n        dummy_env = OpenAIGymEnv.from_spec(env_spec)\n        agent = ActorCriticAgent.from_spec(\n            config_from_path(""configs/actor_critic_agent_for_cartpole.json""),\n            state_space=dummy_env.state_space,\n            action_space=dummy_env.action_space\n        )\n\n        time_steps = 20000\n        worker = SingleThreadedWorker(\n            env_spec=env_spec,\n            agent=agent,\n            worker_executes_preprocessing=False\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        print(results)\n\n        self.assertEqual(results[""timesteps_executed""], time_steps)\n        self.assertEqual(results[""env_frames""], time_steps)\n        self.assertGreaterEqual(results[""mean_episode_reward""], 20)\n        self.assertGreaterEqual(results[""max_episode_reward""], 100.0)\n        #self.assertLessEqual(results[""episodes_executed""], time_steps / 30)\n\n\n'"
rlgraph/tests/agent_learning/short_tasks/test_dqn_agent_short_task_learning.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport unittest\n\nimport numpy as np\n\nfrom rlgraph.agents import DQNAgent\nfrom rlgraph.environments import GridWorld, OpenAIGymEnv\nfrom rlgraph.execution import SingleThreadedWorker\nfrom rlgraph.spaces import FloatBox, IntBox\nfrom rlgraph.tests.test_util import config_from_path, recursive_assert_almost_equal\nfrom rlgraph.utils import root_logger\nfrom rlgraph.utils.numpy import one_hot\n\n\nclass TestDQNAgentShortTaskLearning(unittest.TestCase):\n    """"""\n    Tests whether the DQNAgent can learn in simple environments.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n    # Preprocessed state spaces.\n    grid_world_2x2_flattened_state_space = FloatBox(shape=(4,), add_batch_rank=True)\n    grid_world_4x4_flattened_state_space = FloatBox(shape=(16,), add_batch_rank=True)\n    is_windows = os.name == ""nt""\n\n    def test_dqn_on_2x2_grid_world(self):\n        """"""\n        Creates a DQNAgent and runs it via a Runner on a simple 2x2 GridWorld.\n        """"""\n        dummy_env = GridWorld(""2x2"")\n        agent_config = config_from_path(""configs/dqn_agent_for_2x2_gridworld.json"")\n        preprocessing_spec = agent_config.pop(""preprocessing_spec"")\n        agent = DQNAgent.from_spec(\n            agent_config,\n            double_q=False,\n            dueling_q=False,\n            state_space=self.grid_world_2x2_flattened_state_space,\n            action_space=dummy_env.action_space,\n            execution_spec=dict(seed=12),\n            update_spec=dict(update_interval=4, batch_size=24, sync_interval=32),\n            optimizer_spec=dict(type=""adam"", learning_rate=0.05)\n        )\n\n        time_steps = 2000\n        worker = SingleThreadedWorker(\n            env_spec=lambda: GridWorld(""2x2""),\n            agent=agent,\n            preprocessing_spec=preprocessing_spec,\n            worker_executes_preprocessing=True\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        self.assertEqual(results[""timesteps_executed""], time_steps)\n        self.assertEqual(results[""env_frames""], time_steps)\n        self.assertGreaterEqual(results[""mean_episode_reward""], -1.5)\n        self.assertGreaterEqual(results[""max_episode_reward""], 0.0)\n        self.assertLessEqual(results[""episodes_executed""], time_steps / 2)\n\n        # Check all learnt Q-values.\n        q_values = agent.graph_executor.execute((""get_q_values"", one_hot(np.array([0, 1]), depth=4)))[:]\n        recursive_assert_almost_equal(q_values[0], (0.8, -5, 0.9, 0.8), decimals=1)\n        recursive_assert_almost_equal(q_values[1], (0.8, 1.0, 0.9, 0.9), decimals=1)\n\n    def test_double_dqn_on_2x2_grid_world(self):\n        """"""\n        Creates a double DQNAgent and runs it via a Runner on a simple 2x2 GridWorld.\n        """"""\n        env_spec = dict(world=""2x2"")\n        dummy_env = GridWorld.from_spec(env_spec)\n        agent_config = config_from_path(""configs/dqn_agent_for_2x2_gridworld.json"")\n        preprocessing_spec = agent_config.pop(""preprocessing_spec"")\n        agent = DQNAgent.from_spec(\n            agent_config,\n            dueling_q=False,\n            state_space=self.grid_world_2x2_flattened_state_space,\n            action_space=dummy_env.action_space,\n            execution_spec=dict(seed=10),\n            update_spec=dict(update_interval=4, batch_size=24, sync_interval=32),\n            optimizer_spec=dict(type=""adam"", learning_rate=0.05)\n        )\n\n        time_steps = 2000\n        worker = SingleThreadedWorker(\n            env_spec=lambda: GridWorld.from_spec(env_spec),\n            agent=agent,\n            preprocessing_spec=preprocessing_spec,\n            worker_executes_preprocessing=True\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        self.assertEqual(results[""timesteps_executed""], time_steps)\n        self.assertEqual(results[""env_frames""], time_steps)\n        self.assertGreaterEqual(results[""mean_episode_reward""], -1.5)\n        self.assertGreaterEqual(results[""max_episode_reward""], 0.0)\n        self.assertLessEqual(results[""episodes_executed""], time_steps / 2)\n\n        # Check all learnt Q-values.\n        q_values = agent.graph_executor.execute((""get_q_values"", one_hot(np.array([0, 1]), depth=4)))[:]\n        recursive_assert_almost_equal(q_values[0], (0.8, -5, 0.9, 0.8), decimals=1)\n        recursive_assert_almost_equal(q_values[1], (0.8, 1.0, 0.9, 0.9), decimals=1)\n\n    def test_double_dqn_on_2x2_grid_world_with_container_actions(self):\n        """"""\n        Creates a double DQNAgent and runs it via a Runner on a simple 2x2 GridWorld using container actions.\n        """"""\n        # ftj = forward + turn + jump\n        env_spec = dict(world=""2x2"", action_type=""ftj"", state_representation=""xy+orientation"")\n        dummy_env = GridWorld.from_spec(env_spec)\n        agent_config = config_from_path(""configs/dqn_agent_for_2x2_gridworld_with_container_actions.json"")\n        preprocessing_spec = agent_config.pop(""preprocessing_spec"")\n\n        agent = DQNAgent.from_spec(\n            agent_config,\n            double_q=True,\n            dueling_q=False,\n            state_space=FloatBox(shape=(4,)),\n            action_space=dummy_env.action_space,\n            execution_spec=dict(seed=15)\n        )\n\n        time_steps = 10000\n        worker = SingleThreadedWorker(\n            env_spec=lambda: GridWorld.from_spec(env_spec),\n            agent=agent,\n            preprocessing_spec=preprocessing_spec,\n            worker_executes_preprocessing=True,\n            render=False,\n            episode_finish_callback=lambda episode_return, duration, timesteps, env_num:\n            print(""episode return {}; steps={}"".format(episode_return, timesteps))\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        self.assertEqual(results[""timesteps_executed""], time_steps)\n        self.assertEqual(results[""env_frames""], time_steps)\n        self.assertGreaterEqual(results[""mean_episode_reward""], -2.0)\n        self.assertGreaterEqual(results[""max_episode_reward""], -1.0)\n        self.assertLessEqual(results[""episodes_executed""], time_steps / 3)\n\n        # Check q-table for correct values.\n        expected_q_values_per_state = {\n            (0., 0., -1., 0.): {""forward"": (-5.0, -1.0, -1.0), ""jump"": (0.0, -1.0)},  # start <\n            (0., 0., 1., 0.): {""forward"": (0.0, -1.0, -1.0), ""jump"": (0.0, -1.0)},    # start >\n            (0., 0., 0., -1.): {""forward"": (0.0, -1.0, -1.0), ""jump"": (0.0, -1.0)},   # start v\n            (0., 0., 0., 1.): {""forward"": (0.0, -1.0, -1.0), ""jump"": (0.0, -1.0)},    # start ^\n            (0., 1., -1., 0.): {""forward"": (0.0, -1.0, -1.0), ""jump"": (0.0, -1.0)},\n            (0., 1., 1., 0.): {""forward"": (0.0, -1.0, -1.0), ""jump"": (0.0, -1.0)},\n            (0., 1., 0., -1.): {""forward"": (0.0, -1.0, -1.0), ""jump"": (0.0, -1.0)},\n            (0., 1., 0., 1.): {""forward"": (0.0, -1.0, -1.0), ""jump"": (0.0, -1.0)},\n        }\n        for state, q_values_forward, q_values_jump in zip(\n                agent.last_q_table[""states""], agent.last_q_table[""q_values""][""forward""],\n                agent.last_q_table[""q_values""][""jump""]\n        ):\n            state, q_values_forward, q_values_jump = tuple(state), tuple(q_values_forward), tuple(q_values_jump)\n            assert state in expected_q_values_per_state, \\\n                ""ERROR: state \'{}\' not expected in q-table as it\'s a terminal state!"".format(state)\n            recursive_assert_almost_equal(q_values_forward, expected_q_values_per_state[state][""forward""], decimals=0)\n            recursive_assert_almost_equal(q_values_jump, expected_q_values_per_state[state][""jump""], decimals=0)\n\n    def test_double_dqn_on_4x4_grid_world(self):\n        """"""\n        Creates a double DQNAgent and runs it via a Runner on a simple 2x2 GridWorld.\n        """"""\n        dummy_env = GridWorld(""4x4"")\n        agent_config = config_from_path(""configs/dqn_agent_for_4x4_gridworld.json"")\n        preprocessing_spec = agent_config.pop(""preprocessing_spec"")\n        agent = DQNAgent.from_spec(\n            agent_config,\n            dueling_q=False,\n            state_space=self.grid_world_4x4_flattened_state_space,\n            action_space=dummy_env.action_space,\n            update_spec=dict(update_interval=4, batch_size=32, sync_interval=32),\n            optimizer_spec=dict(type=""adam"", learning_rate=[""linear"", 0.01, 0.0001])\n        )\n\n        time_steps = 9000\n        worker = SingleThreadedWorker(\n            env_spec=lambda: GridWorld(""4x4""),\n            agent=agent,\n            preprocessing_spec=preprocessing_spec,\n            worker_executes_preprocessing=True\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        self.assertEqual(results[""timesteps_executed""], time_steps)\n        self.assertEqual(results[""env_frames""], time_steps)\n        self.assertGreaterEqual(results[""mean_episode_reward""], -7.0)\n        self.assertGreaterEqual(results[""max_episode_reward""], 0.0)\n        self.assertLessEqual(results[""episodes_executed""], time_steps / 5)\n\n        # Check all learnt Q-values.\n        q_values = agent.graph_executor.execute(\n            (""get_q_values"", one_hot(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]), depth=16))\n        )\n        # Walking towards goal is always good.\n        self.assertTrue(q_values[0][0] < q_values[0][1])\n        self.assertTrue(q_values[6][3] < q_values[6][1])\n        # Falling into holes.\n        self.assertTrue(q_values[1][1] < -4.0)  # Going right in state 1 is very bad\n        self.assertTrue(q_values[4][2] < -4.0)  # Going down in state 4 is very bad\n        self.assertTrue(q_values[12][2] < -4.0)  # Going down in state 12 is very bad\n        # Reaching goal.\n        self.assertTrue(q_values[11][1] > 0.0)\n\n    def test_dqn_on_cart_pole(self):\n        """"""\n        Creates a DQNAgent and runs it via a Runner on the CartPole Env.\n        """"""\n        dummy_env = OpenAIGymEnv(""CartPole-v0"")\n        agent = DQNAgent.from_spec(\n            config_from_path(""configs/dqn_agent_for_cartpole.json""),\n            double_q=False,\n            dueling_q=False,\n            state_space=dummy_env.state_space,\n            action_space=dummy_env.action_space,\n            execution_spec=dict(seed=15),\n            update_spec=dict(update_interval=4, batch_size=24, sync_interval=64),\n            optimizer_spec=dict(type=""adam"", learning_rate=0.05)\n        )\n\n        time_steps = 3000\n        worker = SingleThreadedWorker(\n            env_spec=lambda: OpenAIGymEnv(""CartPole-v0"", seed=15),\n            agent=agent,\n            render=self.is_windows,\n            worker_executes_preprocessing=False\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        self.assertEqual(results[""timesteps_executed""], time_steps)\n        self.assertEqual(results[""env_frames""], time_steps)\n        self.assertGreaterEqual(results[""mean_episode_reward""], 25)\n        self.assertGreaterEqual(results[""max_episode_reward""], 100.0)\n        self.assertLessEqual(results[""episodes_executed""], 200)\n\n    def test_double_dueling_dqn_on_cart_pole(self):\n        """"""\n        Creates a double and dueling DQNAgent and runs it via a Runner on the CartPole Env.\n        """"""\n        gym_env = ""CartPole-v0""\n        dummy_env = OpenAIGymEnv(gym_env)\n        config_ = config_from_path(""configs/dqn_agent_for_cartpole.json"")\n        # Add dueling config to agent.\n        config_[""policy_spec""] = {""units_state_value_stream"": 3, ""action_adapter_spec"": {""pre_network_spec"": [\n            {""type"": ""dense"", ""units"": 3}\n        ]}}\n        agent = DQNAgent.from_spec(\n            config_,\n            double_q=True,\n            dueling_q=True,\n            state_space=dummy_env.state_space,\n            action_space=dummy_env.action_space,\n            execution_spec=dict(seed=13),\n            update_spec=dict(update_interval=4, batch_size=64, sync_interval=16),\n            optimizer_spec=dict(type=""adam"", learning_rate=[""linear"", 0.01, 0.00001])\n        )\n\n        time_steps = 6000\n        worker = SingleThreadedWorker(\n            env_spec=lambda: OpenAIGymEnv(gym_env, seed=10),\n            agent=agent,\n            render=self.is_windows,\n            worker_executes_preprocessing=False\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        self.assertEqual(results[""timesteps_executed""], time_steps)\n        self.assertEqual(results[""env_frames""], time_steps)\n        self.assertGreaterEqual(results[""mean_episode_reward""], 40)\n        self.assertLessEqual(results[""episodes_executed""], time_steps / 10)\n\n    def test_double_dqn_on_2x2_grid_world_single_action_to_container(self):\n        """"""\n        Tests how dqn solves a mapping of a single integer to multiple actions (as opposed to using container\n        actions).\n        """"""\n        # ftj = forward + turn + jump\n        env_spec = dict(world=""2x2"", action_type=""ftj"", state_representation=""xy+orientation"")\n        agent_config = config_from_path(""configs/dqn_agent_for_2x2_gridworld_single_to_container.json"")\n        preprocessing_spec = agent_config.pop(""preprocessing_spec"")\n\n        action_space = IntBox(0, 18)\n        agent = DQNAgent.from_spec(\n            agent_config,\n            huber_loss=True,\n            double_q=True,\n            dueling_q=True,\n            state_space=FloatBox(shape=(4,)),\n            action_space=action_space\n        )\n\n        time_steps = 10000\n        worker = SingleThreadedWorker(\n            env_spec=lambda: GridWorld.from_spec(env_spec),\n            agent=agent,\n            preprocessing_spec=preprocessing_spec,\n            worker_executes_preprocessing=True,\n            render=False\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n        print(results)\n'"
rlgraph/tests/agent_learning/short_tasks/test_impala_agent_short_task_learning.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport numpy as np\nimport os\nimport time\nimport unittest\n\nfrom rlgraph.environments import GridWorld, OpenAIGymEnv\nfrom rlgraph.agents import IMPALAAgent\nfrom rlgraph.utils import root_logger\nfrom rlgraph.tests.test_util import config_from_path, recursive_assert_almost_equal\n\n\nclass TestIMPALAAgentShortTaskLearning(unittest.TestCase):\n    """"""\n    Tests whether the DQNAgent can learn in simple environments.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    is_windows = os.name == ""nt""\n\n    def test_impala_on_2x2_grid_world(self):\n        """"""\n        Creates a single IMPALAAgent and runs it via a simple loop on a 2x2 GridWorld.\n        """"""\n        env = GridWorld(""2x2"")\n        agent = IMPALAAgent.from_spec(\n            config_from_path(""configs/impala_agent_for_2x2_gridworld.json""),\n            state_space=env.state_space,\n            action_space=env.action_space,\n            execution_spec=dict(seed=12),\n            update_spec=dict(batch_size=16),\n            optimizer_spec=dict(type=""adam"", learning_rate=0.05)\n        )\n\n        learn_updates = 50\n        for i in range(learn_updates):\n            ret = agent.update()\n            mean_return = self._calc_mean_return(ret)\n            print(""i={} Loss={:.4} Avg-reward={:.2}"".format(i, float(ret[1]), mean_return))\n\n        # Assume we have learned something.\n        self.assertGreater(mean_return, -0.1)\n\n        # Check the last action probs for the 2 valid next_states (start (after a reset) and one below start).\n        action_probs = ret[3][""action_probs""].reshape((80, 4))\n        next_states = ret[3][""states""][:, 1:].reshape((80,))\n        for s_, probs in zip(next_states, action_probs):\n            # Start state:\n            # - Assume we picked ""right"" in state=1 (in order to step into goal state).\n            # - OR we picked ""up"" or ""left"" in state=0 (unlikely, but possible).\n            if s_ == 0:\n                recursive_assert_almost_equal(probs[0], 0.0, decimals=2)\n                self.assertTrue(probs[1] > 0.99 or probs[2] > 0.99)\n                recursive_assert_almost_equal(probs[3], 0.0, decimals=2)\n            # One below start:\n            # - Assume we picked ""down"" in start state with very large probability.\n            # - OR we picked ""left"" or ""down"" in state=1 (unlikely, but possible).\n            elif s_ == 1:\n                recursive_assert_almost_equal(probs[0], 0.0, decimals=2)\n                self.assertTrue(probs[1] > 0.99 or probs[2] > 0.99)\n                recursive_assert_almost_equal(probs[3], 0.0, decimals=2)\n\n        agent.terminate()\n\n    def test_impala_on_cart_pole(self):\n        """"""\n        Creates a single IMPALAAgent and runs it via a simple loop on CartPole-v0.\n        """"""\n        env_spec = dict(type=""open-ai-gym"", gym_env=""CartPole-v0"", seed=10, visualize=self.is_windows)\n        config_ = config_from_path(""configs/impala_agent_for_cartpole.json"")\n        config_[""environment_spec""] = env_spec\n        dummy_env = OpenAIGymEnv.from_spec(env_spec)\n        agent = IMPALAAgent.from_spec(\n            config_,\n            state_space=dummy_env.state_space,\n            action_space=dummy_env.action_space,\n            execution_spec=dict(seed=10)\n        )\n\n        learn_updates = 300\n        mean_returns = []\n        for i in range(learn_updates):\n            ret = agent.update()\n            mean_return = self._calc_mean_return(ret)\n            mean_returns.append(mean_return)\n            print(""i={}/{} Loss={:.4} Avg-reward={:.2}"".format(i, learn_updates, float(ret[1]), mean_return))\n\n        # Assume we have learned something.\n        average_return_last_n_episodes = np.nanmean(mean_returns[:-100])\n        print(""Average return over last n episodes: {}"".format(average_return_last_n_episodes))\n        self.assertGreater(average_return_last_n_episodes, 30.0)\n\n        time.sleep(3)\n        agent.terminate()\n        time.sleep(3)\n\n    @staticmethod\n    def _calc_mean_return(records):\n        size = records[3][""rewards""].size\n        rewards = records[3][""rewards""].reshape((size,))\n        terminals = records[3][""terminals""].reshape((size,))\n        returns = list()\n        return_ = 0.0\n        for r, t in zip(rewards, terminals):\n            return_ += r\n            if t:\n                returns.append(return_)\n                return_ = 0.0\n\n        return np.mean(returns)\n'"
rlgraph/tests/agent_learning/short_tasks/test_ppo_agent_short_task_learning.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport logging\nimport os\nimport unittest\n\nimport numpy as np\nfrom rlgraph.agents import PPOAgent\nfrom rlgraph.environments import OpenAIGymEnv, GridWorld\nfrom rlgraph.execution import SingleThreadedWorker\nfrom rlgraph.spaces import FloatBox\nfrom rlgraph.tests.test_util import config_from_path, recursive_assert_almost_equal\nfrom rlgraph.utils import root_logger\n\n\nclass TestPPOShortTaskLearning(unittest.TestCase):\n    """"""\n    Tests whether the PPO agent can learn in simple environments.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    is_windows = os.name == ""nt""\n\n    def test_ppo_on_2x2_grid_world(self):\n        """"""\n        Creates a PPO Agent and runs it via a Runner on the 2x2 Grid World env.\n        """"""\n        env = GridWorld(world=""2x2"")\n        agent = PPOAgent.from_spec(\n            config_from_path(""configs/ppo_agent_for_2x2_gridworld.json""),\n            state_space=GridWorld.grid_world_2x2_flattened_state_space,\n            action_space=env.action_space,\n            execution_spec=dict(seed=15),\n        )\n\n        time_steps = 3000\n        worker = SingleThreadedWorker(\n            env_spec=lambda: env,\n            agent=agent,\n            worker_executes_preprocessing=True,\n            preprocessing_spec=GridWorld.grid_world_2x2_preprocessing_spec\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        print(results)\n\n        # Check value function outputs for states 0 and 1.\n        # NOTE that this test only works if standardize-advantages=False.\n        values = agent.graph_executor.execute(\n            (""get_state_values"", np.array([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]]))\n        )[:, 0]\n        recursive_assert_almost_equal(values[0], 0.9, decimals=1)  # state 0 should have a value of 0.0\n        recursive_assert_almost_equal(values[1], 1.0, decimals=1)  # state 1 should have a value of +1.0\n\n        self.assertEqual(results[""timesteps_executed""], time_steps)\n        self.assertEqual(results[""env_frames""], time_steps)\n        self.assertLessEqual(results[""episodes_executed""], time_steps / 2)\n        # Assume we have learned something.\n        self.assertGreater(results[""mean_episode_reward_last_10_episodes""], 0.0)\n\n    def test_ppo_on_long_chain_grid_world(self):\n        """"""\n        Creates a PPO Agent and runs it via a Runner on the long-chain Grid World env.\n        NOTE: This is a negative learning test showing that if the buffer is too small (meaning we must add artificial\n        terminal signals in the middle of episodes) and the learning rate is large enough, the agent will get stuck\n        in very few (or even just one) states.\n        """"""\n        env = GridWorld(world=""long-chain"")\n        agent = PPOAgent.from_spec(\n            config_from_path(""configs/ppo_agent_for_long_chain_gridworld.json""),\n            state_space=GridWorld.grid_world_long_chain_flattened_state_space,\n            action_space=env.action_space\n        )\n\n        time_steps = 3000\n        worker = SingleThreadedWorker(\n            env_spec=lambda: env,\n            agent=agent,\n            render=False, #self.is_windows,\n            worker_executes_preprocessing=True,\n            preprocessing_spec=GridWorld.grid_world_long_chain_preprocessing_spec\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        print(results)\n\n        self.assertEqual(results[""timesteps_executed""], time_steps)\n        self.assertEqual(results[""env_frames""], time_steps)\n        self.assertLessEqual(results[""episodes_executed""], time_steps / 2)\n        # Assume we have NOT learned anything.\n        self.assertLess(results[""mean_episode_reward_last_10_episodes""], -100.0)\n\n    def test_ppo_on_2x2_grid_world_with_container_actions(self):\n        """"""\n        Creates a PPO agent and runs it via a Runner on a simple 2x2 GridWorld using container actions.\n        """"""\n        # -----\n        # |^|H|\n        # -----\n        # | |G|  ^=start, looking up\n        # -----\n\n        # ftjb = forward + turn + jump (bool!)\n        env_spec = dict(world=""2x2"", action_type=""ftjb"", state_representation=""xy+orientation"")\n        dummy_env = GridWorld.from_spec(env_spec)\n        agent_config = config_from_path(""configs/ppo_agent_for_2x2_gridworld_with_container_actions.json"")\n        preprocessing_spec = agent_config.pop(""preprocessing_spec"")\n\n        agent = PPOAgent.from_spec(\n            agent_config,\n            state_space=FloatBox(shape=(4,)),\n            action_space=dummy_env.action_space\n        )\n\n        time_steps = 5000\n        worker = SingleThreadedWorker(\n            env_spec=lambda: GridWorld.from_spec(env_spec),\n            agent=agent,\n            preprocessing_spec=preprocessing_spec,\n            worker_executes_preprocessing=True,\n            render=False\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        print(results)\n\n        # Check value function outputs for states 0 and 1.\n        # NOTE that this test only works if standardize-advantages=False.\n        #values = agent.graph_executor.execute(\n        #    (""get_state_values"", np.array([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]]))\n        #)[:, 0]\n        #recursive_assert_almost_equal(values[0], 0.9, decimals=1)  # state 0 should have a value of 0.0\n        #recursive_assert_almost_equal(values[1], 1.0, decimals=1)  # state 1 should have a value of +1.0\n\n        self.assertEqual(results[""timesteps_executed""], time_steps)\n        self.assertEqual(results[""env_frames""], time_steps)\n        self.assertLessEqual(results[""episodes_executed""], time_steps)\n        # Assume we have learned something.\n        self.assertGreaterEqual(results[""mean_episode_reward_last_10_episodes""], 0.0)\n\n    def test_ppo_on_4x4_grid_world(self):\n        """"""\n        Creates a PPO Agent and runs it via a Runner on the 4x4 Grid World env.\n        """"""\n        env = GridWorld(world=""4x4"")\n        agent = PPOAgent.from_spec(\n            config_from_path(""configs/ppo_agent_for_4x4_gridworld.json""),\n            state_space=GridWorld.grid_world_4x4_flattened_state_space,\n            action_space=env.action_space\n        )\n\n        time_steps = 3000\n        worker = SingleThreadedWorker(\n            env_spec=lambda: env,\n            agent=agent,\n            worker_executes_preprocessing=True,\n            preprocessing_spec=GridWorld.grid_world_4x4_preprocessing_spec,\n            episode_finish_callback=lambda episode_return, duration, timesteps, **kwargs: print(\n                ""Episode done return={} timesteps={}"".format(episode_return, timesteps))\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        print(results)\n\n        # Test deterministic actions (expected to go towards goal).\n        #recursive_assert_almost_equal(agent.graph_executor.execute(\n        #    (""get_preprocessed_state_and_action"", [one_hot(np.array([1, 2, 4, 7, 8, 9, 10, 11, 12]), depth=16), True])\n        #)[0], [2, 1, 1, 1, 2, 2, 2, 1, 3])\n\n        self.assertEqual(results[""timesteps_executed""], time_steps)\n        self.assertEqual(results[""env_frames""], time_steps)\n        self.assertLessEqual(results[""episodes_executed""], time_steps / 4)\n        # Assume we have learned something.\n        self.assertGreater(results[""mean_episode_reward_last_10_episodes""], -2.0)\n\n    def test_ppo_on_4_room_grid_world(self):\n        """"""\n        Creates a PPO agent and runs it via a Runner on a 4-rooms GridWorld.\n        """"""\n        env_spec = dict(world=""4-room"")\n        dummy_env = GridWorld.from_spec(env_spec)\n        agent_config = config_from_path(""configs/ppo_agent_for_4_room_gridworld.json"")\n        preprocessing_spec = agent_config.pop(""preprocessing_spec"")\n\n        agent = PPOAgent.from_spec(\n            agent_config,\n            state_space=FloatBox(shape=(dummy_env.state_space.num_categories,)),\n            action_space=dummy_env.action_space\n        )\n\n        episodes = 30\n        worker = SingleThreadedWorker(\n            env_spec=lambda: GridWorld.from_spec(env_spec),\n            agent=agent,\n            preprocessing_spec=preprocessing_spec,\n            worker_executes_preprocessing=True,\n            render=False,\n            episode_finish_callback=lambda episode_return, duration, timesteps, **kwargs: print(""Episode done return={}"".format(episode_return)),\n            #update_finish_callback=lambda loss: print(""Update policy+vf-loss={}"".format(loss[0]))\n        )\n        results = worker.execute_episodes(\n            num_episodes=episodes, max_timesteps_per_episode=10000, use_exploration=True\n        )\n\n        print(results)\n\n        # Check value function outputs for states 46 (doorway close to goal) and start state.\n        #values = agent.graph_executor.execute(\n        #    (""get_state_values"", np.array([one_hot(np.array(46), depth=121), one_hot(np.array(30), depth=121)]))\n        #)[:, 0]\n        #recursive_assert_almost_equal(values[0], -1.0, decimals=1)  # state 46 should have a value of -1.0\n        #recursive_assert_almost_equal(values[1], -50, decimals=1)  # state 30 (start) should have a value of ???\n\n        self.assertEqual(results[""episodes_executed""], episodes)\n        # Assume we have learned something.\n        #self.assertGreaterEqual(results[""mean_episode_reward_last_10_episodes""], -2.0)\n\n    def test_ppo_on_cart_pole(self):\n        """"""\n        Creates a PPO Agent and runs it via a Runner on the CartPole env.\n        """"""\n        env = OpenAIGymEnv(""CartPole-v0"", seed=36)\n        agent = PPOAgent.from_spec(\n            config_from_path(""configs/ppo_agent_for_cartpole.json""),\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n\n        time_steps = 5000\n        worker = SingleThreadedWorker(\n            env_spec=lambda: env,\n            agent=agent,\n            worker_executes_preprocessing=False,\n            render=False,  #self.is_windows\n            episode_finish_callback=lambda episode_return, duration, timesteps, env_num:\n            print(""episode return {}; steps={}"".format(episode_return, timesteps))\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n\n        print(results)\n\n        self.assertEqual(results[""timesteps_executed""], time_steps)\n        self.assertEqual(results[""env_frames""], time_steps)\n        self.assertLessEqual(results[""episodes_executed""], time_steps / 10)\n        # Assume we have learned something.\n        self.assertGreaterEqual(results[""mean_episode_reward_last_10_episodes""], 40.0)\n\n    def test_ppo_on_pendulum(self):\n        """"""\n        Creates a PPO Agent and runs it via a Runner on the Pendulum env.\n        """"""\n        env_spec = dict(type=""openai-gym"", gym_env=""Pendulum-v0"")\n        dummy_env = OpenAIGymEnv.from_spec(env_spec)\n        agent_spec = config_from_path(""configs/ppo_agent_for_pendulum.json"")\n        preprocessing_spec = agent_spec.pop(""preprocessing_spec"", None)\n        agent = PPOAgent.from_spec(\n            agent_spec,\n            state_space=dummy_env.state_space,\n            action_space=dummy_env.action_space\n        )\n\n        worker = SingleThreadedWorker(\n            env_spec=env_spec,\n            num_environments=10,\n            agent=agent,\n            preprocessing_spec=preprocessing_spec,\n            worker_executes_preprocessing=True,\n            render=False, #self.is_windows,\n            episode_finish_callback=lambda episode_return, duration, timesteps, env_num:\n            print(""episode return {}; steps={}"".format(episode_return, timesteps))\n        )\n        results = worker.execute_timesteps(num_timesteps=int(5e6), use_exploration=True)\n\n        print(results)\n\n    def test_ppo_on_lunar_lander(self):\n        """"""\n        Creates a PPO Agent and runs it via a Runner on the Pendulum env.\n        """"""\n        env = OpenAIGymEnv(""LunarLander-v2"")\n        agent = PPOAgent.from_spec(\n            config_from_path(""configs/ppo_agent_for_pendulum.json""),\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n\n        worker = SingleThreadedWorker(\n            env_spec=lambda: env,\n            agent=agent,\n            worker_executes_preprocessing=False,\n            render=False,  #self.is_windows,\n            episode_finish_callback=lambda episode_return, duration, timesteps, env_num:\n            print(""episode return {}; steps={}"".format(episode_return, timesteps))\n        )\n        results = worker.execute_timesteps(num_timesteps=int(5e6), use_exploration=True)\n\n        print(results)\n'"
rlgraph/tests/agent_learning/short_tasks/test_sac_agent_short_task_learning.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport logging\nimport os\nimport unittest\n\nimport numpy as np\nfrom scipy import stats\n\nfrom rlgraph.agents.sac_agent import SACAgentComponent, SACAgent, SyncSpecification\nfrom rlgraph.components import Policy, ValueFunction, PreprocessorStack, ReplayMemory, AdamOptimizer, \\\n    Synchronizable\nfrom rlgraph.environments import GaussianDensityAsRewardEnv, OpenAIGymEnv, GridWorld\nfrom rlgraph.execution import SingleThreadedWorker\nfrom rlgraph.spaces import FloatBox, BoolBox\nfrom rlgraph.tests import ComponentTest\nfrom rlgraph.tests.test_util import config_from_path\nfrom rlgraph.utils import root_logger\n\n\nclass TestSACShortTaskLearning(unittest.TestCase):\n    """"""\n    Tests whether the SACAgent and the SACAgentComponent can learn in simple environments.\n    """"""\n    root_logger.setLevel(level=logging.INFO)\n\n    is_windows = os.name == ""nt""\n\n    def test_sac_agent_component_on_fake_env(self):\n        config = config_from_path(""configs/sac_component_for_fake_env_test.json"")\n\n        # Arbitrary state space, state should not be used in this example.\n        state_space = FloatBox(shape=(2,))\n        continuous_action_space = FloatBox(low=-1.0, high=1.0)\n        terminal_space = BoolBox(add_batch_rank=True)\n        policy = Policy.from_spec(config[""policy""], action_space=continuous_action_space)\n        policy.add_components(Synchronizable(), expose_apis=""sync"")\n        q_function = ValueFunction.from_spec(config[""value_function""])\n\n        agent_component = SACAgentComponent(\n            agent=None,\n            policy=policy,\n            q_function=q_function,\n            preprocessor=PreprocessorStack.from_spec([]),\n            memory=ReplayMemory.from_spec(config[""memory""]),\n            discount=config[""discount""],\n            initial_alpha=config[""initial_alpha""],\n            target_entropy=None,\n            optimizer=AdamOptimizer.from_spec(config[""optimizer""]),\n            vf_optimizer=AdamOptimizer.from_spec(config[""value_function_optimizer""], scope=""vf-optimizer""),\n            alpha_optimizer=None,\n            q_sync_spec=SyncSpecification(sync_interval=10, sync_tau=1.0),\n            num_q_functions=2\n        )\n\n        test = ComponentTest(\n            component=agent_component,\n            input_spaces=dict(\n                states=state_space.with_batch_rank(),\n                preprocessed_states=state_space.with_batch_rank(),\n                actions=continuous_action_space.with_batch_rank(),\n                rewards=FloatBox(add_batch_rank=True),\n                next_states=state_space.with_batch_rank(),\n                terminals=terminal_space,\n                batch_size=int,\n                preprocessed_s_prime=state_space.with_batch_rank(),\n                importance_weights=FloatBox(add_batch_rank=True),\n                preprocessed_next_states=state_space.with_batch_rank(),\n                deterministic=bool,\n                weights=""variables:{}"".format(policy.scope),\n                # TODO: how to provide the space for multiple component variables?\n                # q_weights=Dict(\n                #    q_0=""variables:{}"".format(q_function.scope),\n                #    q_1=""variables:{}"".format(agent_component._q_functions[1].scope),\n                # )\n            ),\n            action_space=continuous_action_space,\n            build_kwargs=dict(\n                optimizer=agent_component._optimizer,\n                build_options=dict(\n                    vf_optimizer=agent_component.vf_optimizer,\n                ),\n            )\n        )\n\n        policy_loss = []\n        vf_loss = []\n\n        # This test simulates an env that always requires actions to be close to the max-pdf\n        # value of a loc=0.5, scale=0.2 normal, regardless of any state inputs.\n        # The component should learn to produce actions like that (close to 0.5).\n        true_mean = 0.5\n        target_dist = stats.norm(loc=true_mean, scale=0.2)\n        batch_size = 100\n        for _ in range(5000):\n            action_sample = continuous_action_space.sample(batch_size)\n            rewards = target_dist.pdf(action_sample)\n            result = test.test((""update_from_external_batch"", [\n                state_space.sample(batch_size),\n                action_sample,\n                rewards,\n                [True] * batch_size,\n                state_space.sample(batch_size),\n                [1.0] * batch_size  # importance\n            ]))\n            policy_loss.append(result[""actor_loss""])\n            vf_loss.append(result[""critic_loss""])\n\n        self.assertTrue(np.mean(policy_loss[:100]) > np.mean(policy_loss[-100:]))\n        self.assertTrue(np.mean(vf_loss[:100]) > np.mean(vf_loss[-100:]))\n\n        action_sample = np.linspace(-1, 1, batch_size)\n        q_values = test.test((""get_q_values"", [state_space.sample(batch_size), action_sample]))\n        for q_val in q_values:\n            q_val = q_val.flatten()\n            np.testing.assert_allclose(q_val, target_dist.pdf(action_sample), atol=0.2)\n\n        action_sample, _ = test.test((""action_from_preprocessed_state"", [state_space.sample(batch_size), False]))\n        action_sample = action_sample.flatten()\n        np.testing.assert_allclose(np.mean(action_sample), true_mean, atol=0.1)\n\n    def test_sac_learning_on_gaussian_density_as_reward_env(self):\n        """"""\n        Creates an SAC-Agent and runs it via a Runner on the GaussianDensityAsRewardEnv.\n        """"""\n        env = GaussianDensityAsRewardEnv(episode_length=5)\n        agent = SACAgent.from_spec(\n            config_from_path(""configs/sac_agent_for_gaussian_density_env.json""),\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n\n        worker = SingleThreadedWorker(env_spec=lambda: env, agent=agent)\n        worker.execute_episodes(num_episodes=500)\n        rewards = worker.finished_episode_returns[0]  # 0=1st env in vector-env\n        self.assertTrue(np.mean(rewards[:100]) < np.mean(rewards[-100:]))\n\n        worker.execute_episodes(num_episodes=100, use_exploration=False, update_spec=None)\n        rewards = worker.finished_episode_returns[0]\n        self.assertTrue(len(rewards) == 100)\n        evaluation_score = np.mean(rewards)\n        self.assertTrue(.5 * env.get_max_reward() < evaluation_score <= env.get_max_reward())\n\n    def test_sac_on_pendulum(self):\n        """"""\n        Creates an SAC-Agent and runs it on Pendulum.\n        """"""\n        env = OpenAIGymEnv(""Pendulum-v0"")\n        agent = SACAgent.from_spec(\n            config_from_path(""configs/sac_agent_for_pendulum.json""),\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n\n        worker = SingleThreadedWorker(\n            env_spec=lambda: env,\n            agent=agent,\n            worker_executes_preprocessing=False,\n            render=False,  # self.is_windows\n            episode_finish_callback=lambda episode_return, duration, timesteps, **kwargs:\n            print(""episode: return={} ts={}"".format(episode_return, timesteps))\n        )\n        # Note: SAC is more computationally expensive.\n        episodes = 50\n        results = worker.execute_episodes(episodes)\n\n        print(results)\n\n        self.assertTrue(results[""timesteps_executed""] == episodes * 200)\n        self.assertTrue(results[""episodes_executed""] == episodes)\n        self.assertGreater(results[""mean_episode_reward_last_10_episodes""], -700)\n        self.assertGreater(results[""max_episode_reward""], -100)\n\n    def test_sac_on_cartpole(self):\n        """"""\n        Creates an SAC-Agent and runs it on CartPole.\n        """"""\n        env = OpenAIGymEnv(""CartPole-v0"")\n        agent = SACAgent.from_spec(\n            config_from_path(""configs/sac_agent_for_cartpole.json""),\n            state_space=env.state_space,\n            action_space=env.action_space\n        )\n\n        worker = SingleThreadedWorker(\n            env_spec=lambda: env,\n            agent=agent,\n            worker_executes_preprocessing=False,\n            render=False,  # self.is_windows,\n            episode_finish_callback=lambda episode_return, duration, timesteps, **kwargs:\n            print(""episode: return={} ts={}"".format(episode_return, timesteps))\n        )\n\n        time_steps = 5000\n        results = worker.execute_timesteps(time_steps)\n\n        print(results)\n\n        self.assertTrue(results[""timesteps_executed""] == time_steps)\n        self.assertLessEqual(results[""episodes_executed""], time_steps / 20)\n        self.assertGreater(results[""mean_episode_reward""], 40.0)\n        self.assertGreater(results[""max_episode_reward""], 100.0)\n        self.assertGreater(results[""mean_episode_reward_last_10_episodes""], 100.0)\n\n    def test_sac_2x2_grid_world_with_container_actions(self):\n        """"""\n        Creates a SAC agent and runs it via a Runner on a simple 2x2 GridWorld using container actions.\n        """"""\n        # ftj = forward + turn + jump\n        env_spec = dict(world=""2x2"", action_type=""ftj"", state_representation=""xy+orientation"")\n        dummy_env = GridWorld.from_spec(env_spec)\n        agent_config = config_from_path(""configs/sac_agent_for_2x2_gridworld_with_container_actions.json"")\n        preprocessing_spec = agent_config.pop(""preprocessing_spec"")\n\n        agent = SACAgent.from_spec(\n            agent_config,\n            state_space=FloatBox(shape=(4,)),\n            action_space=dummy_env.action_space,\n        )\n\n        time_steps = 10000\n        worker = SingleThreadedWorker(\n            env_spec=lambda: GridWorld.from_spec(env_spec),\n            agent=agent,\n            preprocessing_spec=preprocessing_spec,\n            worker_executes_preprocessing=False,\n            render=False\n        )\n        results = worker.execute_timesteps(time_steps, use_exploration=True)\n        print(results)\n\n    def test_sac_cartpole_on_ray(self):\n        """"""\n        Tests sac on Ape-X.\n        """"""\n        # Import Ray here so other test cases do not need to import it if not installed.\n        from rlgraph.execution.ray import ApexExecutor\n        env_spec = dict(\n            type=""openai"",\n            gym_env=""CartPole-v0""\n        )\n        agent_config = config_from_path(""configs/sac_cartpole_on_apex.json"")\n        executor = ApexExecutor(\n            environment_spec=env_spec,\n            agent_config=agent_config,\n        )\n        # Define executor, test assembly.\n        print(""Successfully created executor."")\n\n        # Executes actual workload.\n        result = executor.execute_workload(workload=dict(num_timesteps=20000, report_interval=1000,\n                                                         report_interval_min_seconds=1))\n        print(""Finished executing workload:"")\n        print(result)\n'"
contrib/bitflip_env/rlgraph/environments/custom/__init__.py,0,b''
contrib/bitflip_env/rlgraph/tests/environments/test_openai_gym_atari.py,0,"b'# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport unittest\n\nfrom contrib.bitflip_env.rlgraph.environments import OpenAIGymEnv\n\n\nclass TestOpenAIAtariEnv(unittest.TestCase):\n    """"""\n    Tests creation, resetting and stepping through an openAI Atari Env.\n    """"""\n    def test_openai_atari_env(self):\n        env = OpenAIGymEnv(""Pong-v0"")\n\n        # Simple test runs with fixed actions.\n        s = env.reset()\n        # Assert we have pixels.\n        self.assertGreaterEqual(np.mean(s), 0)\n        self.assertLessEqual(np.mean(s), 255)\n        accum_reward = 0.0\n        for _ in range(100):\n            s, r, t, _ = env.step(env.action_space.sample())\n            assert isinstance(r, np.ndarray)\n            assert r.dtype == np.float32\n            assert isinstance(t, bool)\n            self.assertGreaterEqual(np.mean(s), 0)\n            self.assertLessEqual(np.mean(s), 255)\n            accum_reward += r\n\n        print(""Accumulated Reward: {}"".format(accum_reward))\n\n        env.terminate()\n\n    def test_openai_bitflip_env(self):\n        env = OpenAIGymEnv(""bitflip-v0"")\n\n        # Simple test runs with fixed actions.\n        s = env.reset()\n        # Assert we have pixels.\n        self.assertGreaterEqual(np.mean(s), 0)\n        self.assertLessEqual(np.mean(s), 255)\n        accum_reward = 0.0\n        for _ in range(100):\n            s, r, t, _ = env.step(env.action_space.sample())\n            assert isinstance(r, np.ndarray)\n            assert r.dtype == np.float32\n            assert isinstance(t, bool)\n            self.assertGreaterEqual(np.mean(s), 0)\n            self.assertLessEqual(np.mean(s), 255)\n            accum_reward += r\n\n        print(""Accumulated Reward: {}"".format(accum_reward))\n\n        env.terminate()\n'"
contrib/bitflip_env/rlgraph/environments/custom/openai/__init__.py,0,"b""from gym.envs.registration import register\n\nregister(\n    id='bitflip-v0',\n    entry_point='contrib.bitflip_env.rlgraph.environments.custom.openai.envs:BitFlip',\n    max_episode_steps=200,\n    reward_threshold=195.0,\n)"""
contrib/bitflip_env/rlgraph/environments/custom/openai/envs/__init__.py,0,b'from contrib.bitflip_env.rlgraph.environments.custom.openai.envs.bit_flip import BitFlip\n'
contrib/bitflip_env/rlgraph/environments/custom/openai/envs/bit_flip.py,0,"b'import gym\nimport numpy as np\nfrom gym import spaces\nfrom gym.utils import seeding\n\n\nclass BitFlip(gym.GoalEnv):\n    """"""\n    A multi-goal environment based on BitFlip environment from [1]\n\n     [1] Hindsight Experience Replay - https://arxiv.org/abs/1707.01495\n    TODO add support for mean = zero\n    """"""\n\n    def __init__(self, bit_length=16, max_steps=None):\n\n        super(BitFlip, self).__init__()\n\n        assert bit_length >= 1, \'bit_length must be >= 1, found {}\'.format(bit_length)\n\n        self.bit_length = bit_length\n\n        if max_steps is None:\n            self.max_steps = bit_length\n        else:\n            self.max_steps = max_steps\n\n        self.last_action = -1  # -1 for reset\n        self.steps = 0\n        self.seed()\n        self.action_space = spaces.Discrete(bit_length + 1)  # index = n means to not flip any bit\n        # achieved goal and observation are identical in bit_flip environment, however it is made this way to be\n        # compatible with Openai GoalEnv\n        self.observation_space = spaces.Dict(dict(\n            observation=spaces.Box(low=0, high=1, shape=(bit_length,), dtype=np.int32),\n            achieved_goal=spaces.Box(low=0, high=1, shape=(bit_length,), dtype=np.int32),\n            desired_goal=spaces.Box(low=0, high=1, shape=(bit_length,), dtype=np.int32),\n        ))\n\n        self.reset()\n\n    def step(self, action):\n        obs = self._get_obs()\n\n        if not action == self.bit_length:  # ignore moves that do flip\n            self._bitflip(action)\n\n        self.last_action = action\n        self.steps += 1\n\n        reward = self.compute_reward(obs[\'achieved_goal\'], obs[\'desired_goal\'], {})\n        done = self._terminate()\n        info = {}  # TODO add useful debug information in info\n\n        return obs, reward, done, info\n\n    def render(self, mode=\'human\'):\n        print(""Observation_space: {}, last_action: {}, num_steps: {}"".format(self._get_obs(),\n                                                                             self.last_action,\n                                                                             self.steps))\n\n    def compute_reward(self, achieved_goal, desired_goal, info):\n        return 0 if np.array_equal(achieved_goal, desired_goal) else -1\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def _terminate(self):\n        return np.array_equal(self.state, self.goal) or self.steps >= self.max_steps\n\n    def reset(self):\n        self.steps = 0\n        self.last_action = -1  # -1 for reset\n\n        initial_state = self.observation_space.sample()\n\n        self.state = initial_state[\'observation\']\n        self.goal = initial_state[\'desired_goal\']\n\n        return self._get_obs()\n\n    def _get_obs(self):\n        return {\n            \'observation\': self.state.copy(),\n            \'achieved_goal\': self.state.copy(),\n            \'desired_goal\': self.goal.copy(),\n        }\n\n    def _bitflip(self, index):\n        self.state[index] = not self.state[index]\n'"
