file_path,api_count,code
combine_streams.py,0,"b""# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n### This script is only existing for legacy purposes. Use combine_streams2.py\n### future experiments\n\n\nimport numpy as np\nimport h5py\nimport argparse\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Cluster Features')\n    parser.add_argument('-s', '--spatial',\n            type=str, required=True,\n            help='Path to h5 file with spatial fc8.')\n    parser.add_argument('-t', '--temporal',\n            type=str, required=True,\n            help='Path to h5 file with temporal fc8.')\n    parser.add_argument('--feat_name_temporal',\n            type=str, default='stream0/logits',\n            help='Layer name whose features to use from temporal.')\n    parser.add_argument('--feat_name_spatial',\n            type=str, default='stream0/logits',\n            help='Layer name whose features to use from spatial.')\n    parser.add_argument('-f', '--label_file',\n            type=str, required=True,\n            help='Test file with labels (UCF format).')\n    parser.add_argument('-r', '--temporal_ratio',\n            type=float, default=0.667,\n            help='Weight for the temporal stream output.')\n    parser.add_argument('--idt_scores',\n            type=str, default=None,\n            help='H5 file with IDT scores, from Gul.')\n    parser.add_argument('--idt_wt',\n            type=float, default=0.25,\n            help='Wt on the L2 normalized IDT scores, compared to my score.')\n\n    args = vars(parser.parse_args())\n\n    with open(args['label_file'], 'r') as fin:\n      labels = np.array([int(el.split()[-1]) for el in\n                         fin.read().splitlines()])\n\n    with h5py.File(args['spatial'], 'r') as fin:\n      spatial = fin[args['feat_name_spatial']].value\n    with h5py.File(args['temporal'], 'r') as fin:\n      temporal = fin[args['feat_name_temporal']].value\n    final = (spatial * (1.0 - args['temporal_ratio']) \\\n             + temporal * args['temporal_ratio'])\n    acc_spat = np.mean(spatial.argmax(axis=1) == labels)\n    acc_temp = np.mean(temporal.argmax(axis=1) == labels)\n    acc = np.mean(final.argmax(axis=1) == labels)\n    if args['idt_scores'] is not None:\n      with h5py.File(args['idt_scores'], 'r') as fin:\n        idt = fin['idt'].value\n    else:\n      idt = np.zeros(final.shape)\n    acc_onlyIDT = np.mean(idt.argmax(axis=1) == labels)\n\n    idt_wt = args['idt_wt']\n    final = (1-idt_wt) * (\n      final / np.linalg.norm(final, axis=1, keepdims=True)) + idt_wt * (\n        idt / np.linalg.norm(idt, axis=1, keepdims=True))\n    acc_withIDT = np.mean(final.argmax(axis=1) == labels)\n    print('Spatial = %0.6f [*%f]\\nTemporal = %0.6f [*%f]\\nFinal acc = '\n          '%0.6f.\\nonly IDT = %f\\nwith IDT = %f' %\n          (acc_spat, (1.0 - args['temporal_ratio']), acc_temp, args['temporal_ratio'],\n           acc, acc_onlyIDT, acc_withIDT))\n\n\nif __name__ == '__main__':\n    main()\n"""
convert_first_layer_for_flow.py,6,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n""""""A simple script for write out the first layer with 20 channels""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport h5py\nimport numpy as np\n\nimport tensorflow as tf\n\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_string(""file_name"", """", ""Checkpoint filename"")\ntf.app.flags.DEFINE_string(""tensor_name"", """", ""Name of the tensor to modify"")\ntf.app.flags.DEFINE_string(""output_file_name"", """", ""Path to write out the"" \n                           ""first layer weights"")\n\n\ndef get_modified_weights(file_name, tensor_name):\n  try:\n    reader = tf.train.NewCheckpointReader(file_name)\n    T = reader.get_tensor(tensor_name)\n    return np.repeat(np.mean(T, axis=2, keepdims=True), 20, axis=2)\n  except Exception as e:  # pylint: disable=broad-except\n    print(str(e))\n    if ""corrupted compressed block contents"" in str(e):\n      print(""It\'s likely that your checkpoint file has been compressed ""\n            ""with SNAPPY."")\n\n\ndef main(unused_argv):\n  if not FLAGS.file_name:\n    print(""Usage: inspect_checkpoint --file_name=checkpoint_file_name ""\n          ""[--tensor_name=tensor_to_modify]"")\n    sys.exit(1)\n  else:\n    W = get_modified_weights(FLAGS.file_name, FLAGS.tensor_name)\n    with h5py.File(FLAGS.output_file_name, \'w\') as fout:\n      fout.create_dataset(\'feat\', data=W, compression=\'gzip\',\n                          compression_opts=9)\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
eval_image_classifier.py,50,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic evaluation script that evaluates a model using a given dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\nimport sys\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.platform import tf_logging as logging\n\nfrom datasets import dataset_factory\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\nfrom datasets import dataset_data_provider\n\nslim = tf.contrib.slim\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 100, \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n    \'frames_per_video\', 1, \'Number of frames per video.\')\n\ntf.app.flags.DEFINE_string(\n    \'gpus\', \'0\', \'GPUs to use for testing.\')\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = FLAGS.gpus\nif len(FLAGS.gpus.strip().split(\',\')) > 1:\n  print(\'Multi-gpu testing not supported yet. Specify one gpu.\')\n  sys.exit(-1)\n\n\ntf.app.flags.DEFINE_integer(\n    \'max_num_batches\', None,\n    \'Max number of batches to evaluate by default use all.\')\n\ntf.app.flags.DEFINE_string(\n    \'master\', \'\', \'The address of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'/tmp/tfmodel/\',\n    \'The directory where the model was written to or an absolute path to a \'\n    \'checkpoint file.\')\n# tf.app.flags.DEFINE_string(\n#     \'eval_dir\', \'/tmp/tfmodel/\', \'Directory where the results are saved to.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 4,\n    \'The number of threads used to create the batches.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'imagenet\', \'The name of the dataset to load.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_split_name\', \'test\', \'The name of the train/test split.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', None, \'The directory where the dataset files are stored.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_list_dir\', \'/home/rgirdhar/Work/Data/018_VideoVLAD/raw/UCF101/Lists/\',\n    \'The directory where the dataset list files are stored.\')\n\ntf.app.flags.DEFINE_integer(\n    \'labels_offset\', 0,\n    \'An offset for the labels in the dataset. This flag is primarily used to \'\n    \'evaluate the VGG and ResNet architectures which do not use a background \'\n    \'class for the ImageNet dataset.\')\n\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'inception_v3\', \'The name of the architecture to evaluate.\')\n\ntf.app.flags.DEFINE_string(\n    \'preprocessing_name\', None, \'The name of the preprocessing to use. If left \'\n    \'as `None`, then the model_name flag is used.\')\n\ntf.app.flags.DEFINE_string(\n    \'bgr_flip\', None, \'set true/false to turn on/off this, for each stream. Else use default.\')\n\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None,\n    \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\ntf.app.flags.DEFINE_integer(\n    \'eval_image_size\', None, \'Eval image size\')\n\ntf.app.flags.DEFINE_integer(\n    \'split_id\', 1, \'Dataset split id to use.\')\n\n###########\n# Pooling #\n###########\n\ntf.app.flags.DEFINE_string(\n    \'pooling\', None,\n    \'Set =[netvlad/avg-conv] to train with that.\')\n\ntf.app.flags.DEFINE_string(\n    \'classifier_type\', \'linear\',\n    \'Classifier to use with netvlad/avg-conv. Use linear/two-layer.\')\n\ntf.app.flags.DEFINE_string(\'conv_endpoint\', None,\n                           \'Set a non-default conv endpoint for netvlad.\'\n                           \'Default for vgg16: conv5. Can set fc7.\'\n                           \'Default for inceptionV2TSN is inception_5a.\')\n\n##############\n# Store feat #\n##############\n\ntf.app.flags.DEFINE_string(\n    \'store_feat\', None,\n    \'Set to comma sep list of endpoints to store.\')\n\ntf.app.flags.DEFINE_string(\n    \'store_feat_path\', None,\n    \'Set to path of h5 file to write feats into.\')\n\ntf.app.flags.DEFINE_boolean(\n    \'force_random_shuffle\', False,\n    \'Force random shuffle input data. Useful for storing training features for clustering.\')\n\ntf.app.flags.DEFINE_string(\'modality\', \'rgb\',\n                           \'Modality of training data.\')\n\ntf.app.flags.DEFINE_float(\'out_dim_scale\', 1.0,\n                          \'Resize the output image by this scale. Eg, 224x \'\n                          \'with 2 would be 448x images.\')\n\ntf.app.flags.DEFINE_integer(\'ncrops\', 1,\n                            \'Number of image crops in testing. \'\n                            \'Only 1 or 5 work.\')\n\ntf.app.flags.DEFINE_string(\'netvlad_initCenters\', \'\',\n                           \'Path to PKL with the initial centers.\')\n\ntf.app.flags.DEFINE_string(\'stream_pool_type\', None,\n                           \'Pool streams [concat-netvlad].\')\n\ntf.app.flags.DEFINE_integer(\'feat_store_compression_opt\', 9,\n                            \'Compression opt for storing features.\')\n\n\n\ndef main(_):\n  if not FLAGS.dataset_dir:\n    raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n  if not os.path.isfile(FLAGS.checkpoint_path):\n    FLAGS.eval_dir = os.path.join(FLAGS.checkpoint_path, \'eval\')\n  else:\n    FLAGS.eval_dir = os.path.join(\n        os.path.dirname(FLAGS.checkpoint_path), \'eval\')\n\n  try:\n    os.makedirs(FLAGS.eval_dir)\n  except OSError:\n    pass\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n  with tf.Graph().as_default():\n    tf_global_step = slim.get_or_create_global_step()\n\n    ######################\n    # Select the dataset #\n    ######################\n    dataset = dataset_factory.get_dataset(\n        FLAGS.dataset_name, FLAGS.dataset_split_name,\n        FLAGS.dataset_dir.split(\',\'),\n        FLAGS.dataset_list_dir,\n        num_samples=FLAGS.frames_per_video,\n        modality=FLAGS.modality,\n        split_id=FLAGS.split_id)\n\n    ####################\n    # Select the model #\n    ####################\n    network_fn = nets_factory.get_network_fn(\n        FLAGS.model_name,\n        num_classes=(dataset.num_classes - FLAGS.labels_offset),\n        batch_size=FLAGS.batch_size,\n        is_training=False)\n\n    ##############################################################\n    # Create a dataset provider that loads data from the dataset #\n    ##############################################################\n    provider = dataset_data_provider.DatasetDataProvider(\n        dataset,\n        shuffle=FLAGS.force_random_shuffle,\n        common_queue_capacity=2 * FLAGS.batch_size,\n        common_queue_min=FLAGS.batch_size,\n        bgr_flips=FLAGS.bgr_flip)\n    [image, label] = provider.get([\'image\', \'label\'])\n    label = tf.cast(tf.string_to_number(label, tf.int32),\n        tf.int64)\n    label.set_shape(())\n    label -= FLAGS.labels_offset\n\n    #####################################\n    # Select the preprocessing function #\n    #####################################\n    preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n        preprocessing_name,\n        is_training=False)\n\n    eval_image_size = FLAGS.eval_image_size or network_fn.default_image_size\n\n    image = image_preprocessing_fn(image, eval_image_size, eval_image_size,\n                                   model_name=FLAGS.model_name,\n                                   ncrops=FLAGS.ncrops,\n                                   out_dim_scale=FLAGS.out_dim_scale)\n\n    images, labels = tf.train.batch(\n        [image, label],\n        batch_size=FLAGS.batch_size,\n        num_threads=1 if FLAGS.store_feat is not None else FLAGS.num_preprocessing_threads,\n        capacity=5 * FLAGS.batch_size)\n\n    ####################\n    # Define the model #\n    ####################\n    kwargs = {}\n    if FLAGS.conv_endpoint is not None:\n      kwargs[\'conv_endpoint\'] = FLAGS.conv_endpoint\n    logits, end_points = network_fn(\n        images, pool_type=FLAGS.pooling,\n        classifier_type=FLAGS.classifier_type,\n        num_channels_stream=provider.num_channels_stream,\n        netvlad_centers=FLAGS.netvlad_initCenters.split(\',\'),\n        stream_pool_type=FLAGS.stream_pool_type,\n        **kwargs)\n    end_points[\'images\'] = images\n    end_points[\'labels\'] = labels\n\n    if FLAGS.moving_average_decay:\n      variable_averages = tf.train.ExponentialMovingAverage(\n          FLAGS.moving_average_decay, tf_global_step)\n      variables_to_restore = variable_averages.variables_to_restore(\n          slim.get_model_variables())\n      variables_to_restore[tf_global_step.op.name] = tf_global_step\n    else:\n      variables_to_restore = slim.get_variables_to_restore()\n\n    predictions = tf.argmax(logits, 1)\n    # rgirdhar: Because of the following, can\'t use with batch_size=1\n    if FLAGS.batch_size > 1:\n      labels = tf.squeeze(labels)\n\n    # Define the metrics:\n    names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n        \'Accuracy\': slim.metrics.streaming_accuracy(predictions, labels),\n        \'Recall@5\': slim.metrics.streaming_recall_at_k(\n            logits, labels, 5),\n    })\n\n    # Print the summaries to screen.\n    for name, value in names_to_values.iteritems():\n      summary_name = \'eval/%s\' % name\n      op = tf.scalar_summary(summary_name, value, collections=[])\n      op = tf.Print(op, [value], summary_name)\n      tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n\n    # TODO(sguada) use num_epochs=1\n    if FLAGS.max_num_batches:\n      num_batches = FLAGS.max_num_batches\n    else:\n      # This ensures that we make a single pass over all of the data.\n      num_batches = int(math.ceil(dataset.num_samples /\n                                  float(FLAGS.batch_size)))\n\n    if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n      checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n    else:\n      checkpoint_path = FLAGS.checkpoint_path\n\n    tf.logging.info(\'Evaluating %s\' % checkpoint_path)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.allow_soft_placement = True\n\n    if FLAGS.store_feat is not None:\n      assert(FLAGS.store_feat_path is not None)\n      from tensorflow.python.training import supervisor\n      from tensorflow.python.framework import ops\n      import h5py\n      saver = tf.train.Saver(variables_to_restore)\n      sv = supervisor.Supervisor(graph=ops.get_default_graph(),\n                                 logdir=None,\n                                 summary_op=None,\n                                 summary_writer=None,\n                                 global_step=None,\n                                 saver=None)\n      ept_names_to_store = FLAGS.store_feat.split(\',\')\n      try:\n        ept_to_store = [end_points[el] for el in ept_names_to_store]\n      except:\n        logging.error(\'Endpoint not found\')\n        logging.error(\'Choose from %s\' % \',\'.join(end_points.keys()))\n        raise KeyError()\n      res = dict([(epname, []) for epname in ept_names_to_store])\n      with sv.managed_session(\n          FLAGS.master, start_standard_services=False,\n          config=config) as sess:\n        saver.restore(sess, checkpoint_path)\n        sv.start_queue_runners(sess)\n        for j in range(num_batches):\n          if j % 10 == 0:\n            logging.info(\'Doing batch %d/%d\' % (j, num_batches))\n          feats = sess.run(ept_to_store)\n          for eid, epname in enumerate(ept_names_to_store):\n            res[epname].append(feats[eid])\n      logging.info(\'Writing out features to %s\' % FLAGS.store_feat_path)\n      with h5py.File(FLAGS.store_feat_path, \'w\') as fout:\n        for epname in res.keys():\n          fout.create_dataset(epname,\n              data=np.concatenate(res[epname], axis=0),\n              compression=\'gzip\',\n              compression_opts=FLAGS.feat_store_compression_opt)\n    else:\n      slim.evaluation.evaluate_once(\n          master=FLAGS.master,\n          checkpoint_path=checkpoint_path,\n          logdir=FLAGS.eval_dir,\n          num_evals=num_batches,\n          eval_op=names_to_updates.values(),\n          variables_to_restore=variables_to_restore,\n          session_config=config)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
train_image_classifier.py,136,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic training script that trains a model using a given dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nfrom datetime import datetime\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.client import timeline\nfrom datasets import dataset_factory\nfrom datasets import dataset_data_provider\nfrom deployment import model_deploy\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\nfrom restore import model_restorer\n\nslim = tf.contrib.slim\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    \'master\', \'\', \'The address of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\n    \'train_dir\', \'/tmp/tfmodel/\',\n    \'Directory where checkpoints and event logs are written to.\')\n\ntf.app.flags.DEFINE_string(\'gpus\', \'0\',\n                           \'Comma sep list of gpus.\')\n\ntf.app.flags.DEFINE_integer(\'num_clones\', -1,\n                            \'Number of model clones to deploy.\')\n\ntf.app.flags.DEFINE_boolean(\'clone_on_cpu\', False,\n                            \'Use CPUs to deploy clones.\')\n\ntf.app.flags.DEFINE_integer(\'worker_replicas\', 1, \'Number of worker replicas.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_ps_tasks\', 0,\n    \'The number of parameter servers. If the value is 0, then the parameters \'\n    \'are handled locally by the worker.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 4,\n    \'The number of parallel readers that read data from the dataset.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 4,\n    \'The number of threads used to create the batches.\')\n\ntf.app.flags.DEFINE_integer(\n    \'cpu_threads\', 0,\n    \'Max CPU threads used apart from data prefetch. \'\n    \'0 default lets system pick a reasonable number.\')\n\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_summaries_secs\', 30,\n    \'The frequency with which summaries are saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_interval_secs\', 600,\n    \'The frequency with which the model is saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n    \'task\', 0, \'Task id of the replica running the training.\')\n\n######################\n# Optimization Flags #\n######################\n\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 0.00004, \'The weight decay on the model weights.\')\n\ntf.app.flags.DEFINE_string(\n    \'optimizer\', \'rmsprop\',\n    \'The name of the optimizer, one of ""adadelta"", ""adagrad"", ""adam"",\'\n    \'""ftrl"", ""momentum"", ""sgd"" or ""rmsprop"".\')\n\ntf.app.flags.DEFINE_float(\n    \'adadelta_rho\', 0.95,\n    \'The decay rate for adadelta.\')\n\ntf.app.flags.DEFINE_float(\n    \'adagrad_initial_accumulator_value\', 0.1,\n    \'Starting value for the AdaGrad accumulators.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta1\', 0.9,\n    \'The exponential decay rate for the 1st moment estimates.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta2\', 0.999,\n    \'The exponential decay rate for the 2nd moment estimates.\')\n\ntf.app.flags.DEFINE_float(\'opt_epsilon\', 1.0e-8, \'Epsilon term for the optimizer.\')\n\ntf.app.flags.DEFINE_float(\'ftrl_learning_rate_power\', -0.5,\n                          \'The learning rate power.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_initial_accumulator_value\', 0.1,\n    \'Starting value for the FTRL accumulators.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_l1\', 0.0, \'The FTRL l1 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_l2\', 0.0, \'The FTRL l2 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_momentum\', 0.9, \'Momentum.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_decay\', 0.9, \'Decay term for RMSProp.\')\n\ntf.app.flags.DEFINE_float(\n    \'clip_gradients\', 0,\n    \'Clip gradients by this value.\')\n\n#######################\n# Learning Rate Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'learning_rate_decay_type\',\n    \'exponential\',\n    \'Specifies how the learning rate is decayed. One of ""fixed"", ""exponential"",\'\n    \' or ""polynomial""\')\n\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.0001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'label_smoothing\', 0.0, \'The amount of label smoothing.\')\n\ntf.app.flags.DEFINE_float(\n    \'learning_rate_decay_factor\',\n    # 0.94,\n    0.1,\n    \'Learning rate decay factor.\')\n\n# tf.app.flags.DEFINE_float(\n#     \'num_epochs_per_decay\', 2.0,\n#     \'Number of epochs after which learning rate decays.\')\ntf.app.flags.DEFINE_integer(\n    \'num_steps_per_decay\', 10000,\n    \'Number of steps after which learning rate decays.\')\n\ntf.app.flags.DEFINE_bool(\n    \'sync_replicas\', False,\n    \'Whether or not to synchronize the replicas during training.\')\n\ntf.app.flags.DEFINE_integer(\n    \'replicas_to_aggregate\', 1,\n    \'The Number of gradients to collect before updating params.\')\n\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None,\n    \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\n#######################\n# Dataset Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'imagenet\', \'The name of the dataset to load.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_split_name\', \'train\', \'The name of the train/test split.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', None, \'The directory where the dataset files are stored.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_list_dir\', \'/home/rgirdhar/Work/Data/018_VideoVLAD/raw/UCF101/Lists/\',\n    \'The directory where the dataset list files are stored.\')\n\ntf.app.flags.DEFINE_integer(\n    \'labels_offset\', 0,\n    \'An offset for the labels in the dataset. This flag is primarily used to \'\n    \'evaluate the VGG and ResNet architectures which do not use a background \'\n    \'class for the ImageNet dataset.\')\n\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'inception_v3\', \'The name of the architecture to train.\')\n\ntf.app.flags.DEFINE_float(\n    \'dropout\', 0.8, \'Dropout on last layers.\')\n\ntf.app.flags.DEFINE_float(\n    \'pooled_dropout\', 0.5, \'Dropout on conv-pooled/netvlad output.\')\n\ntf.app.flags.DEFINE_string(\n    \'preprocessing_name\', None, \'The name of the preprocessing to use. If left \'\n    \'as `None`, then the model_name flag is used.\')\n\ntf.app.flags.DEFINE_string(\n    \'bgr_flip\', None,\n    (\'Set true or false to force either, for each stream. As none (default) it will do\'\n      \'whatever is default for that preprocessor\'))\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 32, \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n    \'frames_per_video\', 1, \'The number of frames in each batch element.\')\n\ntf.app.flags.DEFINE_integer(\n    \'iter_size\', 1, \'Number of forward iterations before a back.\')\n\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', None, \'Train image size\')\n\ntf.app.flags.DEFINE_integer(\'max_number_of_steps\', None,\n                            \'The maximum number of training steps.\')\n\ntf.app.flags.DEFINE_string(\'modality\', \'rgb\',\n                           \'Modality of training data.\')\n\ntf.app.flags.DEFINE_string(\'scale_ratios\', \'1,0.875,0.75,0.66\',\n                           \'Ratios to scale the frames by for augmentation.\')\n\ntf.app.flags.DEFINE_float(\'out_dim_scale\', 1.0,\n                          \'Resize the output image by this scale. Eg, 224x \'\n                          \'with 2 would be 448x images.\')\n\ntf.app.flags.DEFINE_integer(\'split_id\', 1,\n                            \'Dataset split to use.\')\n\n#####################\n# Fine-Tuning Flags #\n#####################\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The path to a checkpoint from which to fine-tune.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_style\', None,\n    \'Comma separated list of type of each checkpoint. [v1/v2_withStream]. \'\n    \'Default all are v1.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring \'\n    \'from a checkpoint.\')\n\ntf.app.flags.DEFINE_string(\n    \'trainable_scopes\', None,\n    \'Comma-separated list of scopes to filter the set of variables to train.\'\n    \'By default, None would train all the variables.\')\n\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', False,\n    \'When restoring a checkpoint would ignore missing variables.\')\n\ntf.app.flags.DEFINE_boolean(\n    \'debug\', False,\n    \'Running some debugging print ops.\')\n\ntf.app.flags.DEFINE_string(\n    \'special_assign_vars\', None,\n    \'Specify some variables to assigned using specific files. \'\n    \'Use format: <var_name1>,<file_name1>,<var_name2>,<file_name2>...\')\n\n###########\n# NetVLAD #\n###########\n\ntf.app.flags.DEFINE_string(\n    \'pooling\', None,\n    \'Set =[netvlad/avg-conv] to train with that.\')\n\ntf.app.flags.DEFINE_string(\n    \'classifier_type\', \'linear\',\n    \'Classifier to use with netvlad/avg-conv. Use linear/two-layer.\')\n\ntf.app.flags.DEFINE_boolean(\n    \'netvlad_batch_norm\', False,\n    \'Apply a batch norm to the netvlad features.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_streams\', 1,\n    \'Number of [flow/rgb etc] streams.\')\n\ntf.app.flags.DEFINE_string(\'netvlad_initCenters\', \'\',\n                           \'Path to PKL with the initial centers.\')\n\ntf.app.flags.DEFINE_string(\'stream_pool_type\', None,\n                           \'Pool streams [concat-netvlad].\')\n\ntf.app.flags.DEFINE_string(\'conv_endpoint\', None,\n                           \'Set a non-default conv endpoint for netvlad.\'\n                           \'Default for vgg16: conv5. Can set fc7.\'\n                           \'Default for inceptionV2TSN is inception_5a.\')\n\n#########\n# Other #\n#########\n\ntf.app.flags.DEFINE_boolean(\n    \'profile_iterations\', False,\n    \'Write timeline profile of each iteration.\')\n\n\ndef _configure_learning_rate(num_samples_per_epoch, global_step):\n  """"""Configures the learning rate.\n\n  Args:\n    num_samples_per_epoch: The number of samples in each epoch of training.\n    global_step: The global_step tensor.\n\n  Returns:\n    A `Tensor` representing the learning rate.\n\n  Raises:\n    ValueError: if\n  """"""\n  # decay_steps = int(num_samples_per_epoch / FLAGS.batch_size *\n  #                   FLAGS.num_epochs_per_decay)\n  decay_steps = FLAGS.num_steps_per_decay\n  if FLAGS.sync_replicas:\n    decay_steps /= FLAGS.replicas_to_aggregate\n\n  if FLAGS.learning_rate_decay_type == \'exponential\':\n    return tf.train.exponential_decay(FLAGS.learning_rate,\n                                      global_step,\n                                      decay_steps,\n                                      FLAGS.learning_rate_decay_factor,\n                                      staircase=True,\n                                      name=\'exponential_decay_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'fixed\':\n    return tf.constant(FLAGS.learning_rate, name=\'fixed_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'polynomial\':\n    return tf.train.polynomial_decay(FLAGS.learning_rate,\n                                     global_step,\n                                     decay_steps,\n                                     FLAGS.end_learning_rate,\n                                     power=1.0,\n                                     cycle=False,\n                                     name=\'polynomial_decay_learning_rate\')\n  else:\n    raise ValueError(\'learning_rate_decay_type [%s] was not recognized\',\n                     FLAGS.learning_rate_decay_type)\n\n\ndef _configure_optimizer(learning_rate):\n  """"""Configures the optimizer used for training.\n\n  Args:\n    learning_rate: A scalar or `Tensor` learning rate.\n\n  Returns:\n    An instance of an optimizer.\n\n  Raises:\n    ValueError: if FLAGS.optimizer is not recognized.\n  """"""\n  if FLAGS.optimizer == \'adadelta\':\n    optimizer = tf.train.AdadeltaOptimizer(\n        learning_rate,\n        rho=FLAGS.adadelta_rho,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'adagrad\':\n    optimizer = tf.train.AdagradOptimizer(\n        learning_rate,\n        initial_accumulator_value=FLAGS.adagrad_initial_accumulator_value)\n  elif FLAGS.optimizer == \'adam\':\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate,\n        beta1=FLAGS.adam_beta1,\n        beta2=FLAGS.adam_beta2,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'ftrl\':\n    optimizer = tf.train.FtrlOptimizer(\n        learning_rate,\n        learning_rate_power=FLAGS.ftrl_learning_rate_power,\n        initial_accumulator_value=FLAGS.ftrl_initial_accumulator_value,\n        l1_regularization_strength=FLAGS.ftrl_l1,\n        l2_regularization_strength=FLAGS.ftrl_l2)\n  elif FLAGS.optimizer == \'momentum\':\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate,\n        momentum=FLAGS.momentum,\n        name=\'Momentum\')\n  elif FLAGS.optimizer == \'rmsprop\':\n    optimizer = tf.train.RMSPropOptimizer(\n        learning_rate,\n        decay=FLAGS.rmsprop_decay,\n        momentum=FLAGS.rmsprop_momentum,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'sgd\':\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  else:\n    raise ValueError(\'Optimizer [%s] was not recognized\', FLAGS.optimizer)\n  return optimizer\n\n\ndef _add_variables_summaries(learning_rate):\n  summaries = []\n  for variable in slim.get_model_variables():\n    summaries.append(tf.histogram_summary(variable.op.name, variable))\n  summaries.append(tf.scalar_summary(\'training/Learning Rate\', learning_rate))\n  return summaries\n\n\ndef _get_init_fn():\n  """"""Returns a function run by the chief worker to warm-start the training.\n\n  Note that the init_fn is only run when initializing the model during the very\n  first global step.\n\n  Returns:\n    An init function run by the supervisor.\n  """"""\n  if FLAGS.checkpoint_path is None:\n    return None\n\n  # Warn the user if a checkpoint exists in the train_dir. Then we\'ll be\n  # ignoring the checkpoint anyway.\n  if tf.train.latest_checkpoint(FLAGS.train_dir):\n    tf.logging.info(\n        \'Ignoring --checkpoint_path because a checkpoint already exists in %s\'\n        % FLAGS.train_dir)\n    return None\n\n  exclusions = []\n  if FLAGS.checkpoint_exclude_scopes:\n    exclusions = [scope.strip()\n                  for scope in FLAGS.checkpoint_exclude_scopes.split(\',\')]\n\n  # TODO(sguada) variables.filter_variables()\n  variables_to_restore = []\n  for var in slim.get_model_variables():\n    excluded = False\n    for exclusion in exclusions:\n      if var.op.name.startswith(exclusion):\n        excluded = True\n        break\n    if not excluded:\n      variables_to_restore.append(var)\n\n  checkpoint_paths = FLAGS.checkpoint_path.split(\',\')\n  for cid in range(len(checkpoint_paths)):\n    if tf.gfile.IsDirectory(checkpoint_paths[cid]):\n      checkpoint_paths[cid] = tf.train.latest_checkpoint(checkpoint_paths[cid])\n\n  tf.logging.info(\'Fine-tuning from %s\' % FLAGS.checkpoint_path)\n\n  return model_restorer.restore_model(\n      checkpoint_paths,\n      variables_to_restore,\n      ignore_missing_vars=FLAGS.ignore_missing_vars,\n      num_streams=FLAGS.num_streams,\n      checkpoint_style=FLAGS.checkpoint_style,\n      special_assign_vars=FLAGS.special_assign_vars)\n\n\ndef _get_variables_to_train():\n  """"""Returns a list of variables to train.\n\n  Returns:\n    A list of variables to train by the optimizer.\n  """"""\n  if FLAGS.trainable_scopes is None:\n    return tf.trainable_variables()\n  else:\n    scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(\',\')]\n\n  variables_to_train = []\n  for scope in scopes:\n    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n    variables_to_train.extend(variables)\n  return variables_to_train\n\n\nend_points_debug = []\ndef train_step(sess, train_op, global_step, train_step_kwargs):\n  """"""Function that takes a gradient step and specifies whether to stop.\n  Args:\n    sess: The current session.\n    train_op: A dictionary of `Operation` that evaluates the gradients and returns the\n      total loss (for first) in case of iter_size > 1.\n    global_step: A `Tensor` representing the global training step.\n    train_step_kwargs: A dictionary of keyword arguments.\n  Returns:\n    The total loss and a boolean indicating whether or not to stop training.\n  """"""\n  start_time = time.time()\n  if FLAGS.iter_size == 1:\n    # for debugging specific endpoint values,\n    # set the train file to one image and use\n    # pdb here\n    # import pdb\n    # pdb.set_trace()\n    if FLAGS.profile_iterations:\n      run_options = tf.RunOptions(\n          trace_level=tf.RunOptions.FULL_TRACE)\n      run_metadata = tf.RunMetadata()\n      total_loss, np_global_step = sess.run([train_op, global_step],\n          options=run_options,\n          run_metadata=run_metadata)\n      tl = timeline.Timeline(run_metadata.step_stats)\n      ctf = tl.generate_chrome_trace_format()\n      with open(os.path.join(FLAGS.train_dir,\n                             \'timeline_%08d.json\' % np_global_step), \'w\') as f:\n        f.write(ctf)\n    else:\n      total_loss, np_global_step = sess.run([train_op, global_step])\n  else:\n    for j in range(FLAGS.iter_size-1):\n      sess.run([train_op[j]])\n    total_loss, np_global_step = sess.run(\n        [train_op[FLAGS.iter_size-1], global_step])\n  time_elapsed = time.time() - start_time\n\n  if \'should_log\' in train_step_kwargs:\n    if sess.run(train_step_kwargs[\'should_log\']):\n      logging.info(\'%s: global step %d: loss = %.4f (%.2f sec)\',\n                   datetime.now(), np_global_step, total_loss, time_elapsed)\n\n  if \'should_stop\' in train_step_kwargs:\n    should_stop = sess.run(train_step_kwargs[\'should_stop\'])\n  else:\n    should_stop = False\n\n  return total_loss, should_stop\n\n\ndef summarize_images(images, num_channels_stream):\n  from nets.nets_factory import split_images\n  images_list = split_images(images, num_channels_stream)\n  for sid,im_st in enumerate(images_list):\n    ndims = im_st.get_shape().ndims\n    if im_st.get_shape().as_list()[-1] != 3:\n      # im_st = tf.expand_dims(tf.transpose(im_st, [ndims-1] + range(ndims-1)),\n      #                       ndims)\n      im_st = tf.reduce_mean(im_st, reduction_indices=ndims-1,\n                             keep_dims=True)\n    if ndims > 4:\n      im_st = tf.reshape(im_st, [-1,] + im_st.get_shape().as_list()[-3:])\n    tf.image_summary(\'stream%d\' % sid, im_st / 128)\n\n\ndef main(_):\n  if not FLAGS.dataset_dir:\n    raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n  os.environ[\'CUDA_VISIBLE_DEVICES\'] = FLAGS.gpus\n  if FLAGS.num_clones == -1:\n    FLAGS.num_clones = len(FLAGS.gpus.split(\',\'))\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n  with tf.Graph().as_default():\n    # tf.set_random_seed(42)\n    tf.set_random_seed(0)\n    ######################\n    # Config model_deploy#\n    ######################\n    deploy_config = model_deploy.DeploymentConfig(\n        num_clones=FLAGS.num_clones,\n        clone_on_cpu=FLAGS.clone_on_cpu,\n        replica_id=FLAGS.task,\n        num_replicas=FLAGS.worker_replicas,\n        num_ps_tasks=FLAGS.num_ps_tasks)\n\n    # Create global_step\n    with tf.device(deploy_config.variables_device()):\n      global_step = slim.create_global_step()\n\n    ######################\n    # Select the dataset #\n    ######################\n    dataset = dataset_factory.get_dataset(\n        FLAGS.dataset_name, FLAGS.dataset_split_name,\n        FLAGS.dataset_dir.split(\',\'),\n        dataset_list_dir=FLAGS.dataset_list_dir,\n        num_samples=FLAGS.frames_per_video,\n        modality=FLAGS.modality,\n        split_id=FLAGS.split_id)\n\n    ######################\n    # Select the network #\n    ######################\n    network_fn = nets_factory.get_network_fn(\n        FLAGS.model_name,\n        num_classes=(dataset.num_classes - FLAGS.labels_offset),\n        batch_size=FLAGS.batch_size,\n        weight_decay=FLAGS.weight_decay,\n        is_training=True,\n        dropout_keep_prob=(1.0-FLAGS.dropout),\n        pooled_dropout_keep_prob=(1.0-FLAGS.pooled_dropout),\n        batch_norm=FLAGS.netvlad_batch_norm)\n\n    #####################################\n    # Select the preprocessing function #\n    #####################################\n    preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n        preprocessing_name,\n        is_training=True)  # in case of pooling images,\n                           # now preprocessing is done video-level\n\n    ##############################################################\n    # Create a dataset provider that loads data from the dataset #\n    ##############################################################\n    with tf.device(deploy_config.inputs_device()):\n      provider = dataset_data_provider.DatasetDataProvider(\n        dataset,\n        num_readers=FLAGS.num_readers,\n        common_queue_capacity=20 * FLAGS.batch_size,\n        common_queue_min=10 * FLAGS.batch_size,\n        bgr_flips=FLAGS.bgr_flip)\n      [image, label] = provider.get([\'image\', \'label\'])\n      # now note that the above image might be a 23 channel image if you have\n      # both RGB and flow streams. It will need to split later, but all the\n      # preprocessing will be done consistently for all frames over all streams\n      label = tf.string_to_number(label, tf.int32)\n      label.set_shape(())\n      label -= FLAGS.labels_offset\n\n      train_image_size = FLAGS.train_image_size or network_fn.default_image_size\n\n      scale_ratios=[float(el) for el in FLAGS.scale_ratios.split(\',\')],\n      image = image_preprocessing_fn(image, train_image_size,\n                                     train_image_size,\n                                     scale_ratios=scale_ratios,\n                                     out_dim_scale=FLAGS.out_dim_scale,\n                                     model_name=FLAGS.model_name)\n\n      images, labels = tf.train.batch(\n          [image, label],\n          batch_size=FLAGS.batch_size,\n          num_threads=FLAGS.num_preprocessing_threads,\n          capacity=5 * FLAGS.batch_size)\n      if FLAGS.debug:\n        images = tf.Print(images, [labels], \'Read batch\')\n      labels = slim.one_hot_encoding(\n          labels, dataset.num_classes - FLAGS.labels_offset)\n      batch_queue = slim.prefetch_queue.prefetch_queue(\n          [images, labels], capacity=2 * deploy_config.num_clones)\n      summarize_images(images, provider.num_channels_stream)\n\n    ####################\n    # Define the model #\n    ####################\n    kwargs = {}\n    if FLAGS.conv_endpoint is not None:\n      kwargs[\'conv_endpoint\'] = FLAGS.conv_endpoint\n    def clone_fn(batch_queue):\n      """"""Allows data parallelism by creating multiple clones of network_fn.""""""\n      images, labels = batch_queue.dequeue()\n      logits, end_points = network_fn(\n          images, pool_type=FLAGS.pooling,\n          classifier_type=FLAGS.classifier_type,\n          num_channels_stream=provider.num_channels_stream,\n          netvlad_centers=FLAGS.netvlad_initCenters.split(\',\'),\n          stream_pool_type=FLAGS.stream_pool_type,\n          **kwargs)\n\n      #############################\n      # Specify the loss function #\n      #############################\n      if \'AuxLogits\' in end_points:\n        slim.losses.softmax_cross_entropy(\n            end_points[\'AuxLogits\'], labels,\n            label_smoothing=FLAGS.label_smoothing, weight=0.4, scope=\'aux_loss\')\n      slim.losses.softmax_cross_entropy(\n          logits, labels, label_smoothing=FLAGS.label_smoothing, weight=1.0)\n      return end_points\n\n    # Gather initial summaries.\n    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n    first_clone_scope = deploy_config.clone_scope(0)\n    # Gather update_ops from the first clone. These contain, for example,\n    # the updates for the batch_norm variables created by network_fn.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n\n    # Add summaries for end_points.\n    global end_points_debug\n    end_points = clones[0].outputs\n    end_points_debug = dict(end_points)\n    end_points_debug[\'images\'] = images\n    end_points_debug[\'labels\'] = labels\n    for end_point in end_points:\n      x = end_points[end_point]\n      summaries.add(tf.histogram_summary(\'activations/\' + end_point, x))\n      summaries.add(tf.scalar_summary(\'sparsity/\' + end_point,\n                                      tf.nn.zero_fraction(x)))\n\n    # Add summaries for losses.\n    for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n      summaries.add(tf.scalar_summary(\'losses/%s\' % loss.op.name, loss))\n\n    # Add summaries for variables.\n    for variable in slim.get_model_variables():\n      summaries.add(tf.histogram_summary(variable.op.name, variable))\n\n    #################################\n    # Configure the moving averages #\n    #################################\n    if FLAGS.moving_average_decay:\n      moving_average_variables = slim.get_model_variables()\n      variable_averages = tf.train.ExponentialMovingAverage(\n          FLAGS.moving_average_decay, global_step)\n    else:\n      moving_average_variables, variable_averages = None, None\n\n    #########################################\n    # Configure the optimization procedure. #\n    #########################################\n    with tf.device(deploy_config.optimizer_device()):\n      learning_rate = _configure_learning_rate(dataset.num_samples, global_step)\n      optimizer = _configure_optimizer(learning_rate)\n      summaries.add(tf.scalar_summary(\'learning_rate\', learning_rate,\n                                      name=\'learning_rate\'))\n\n    if FLAGS.sync_replicas:\n      # If sync_replicas is enabled, the averaging will be done in the chief\n      # queue runner.\n      optimizer = tf.train.SyncReplicasOptimizer(\n          opt=optimizer,\n          replicas_to_aggregate=FLAGS.replicas_to_aggregate,\n          variable_averages=variable_averages,\n          variables_to_average=moving_average_variables,\n          replica_id=tf.constant(FLAGS.task, tf.int32, shape=()),\n          total_num_replicas=FLAGS.worker_replicas)\n    elif FLAGS.moving_average_decay:\n      # Update ops executed locally by trainer.\n      update_ops.append(variable_averages.apply(moving_average_variables))\n\n    # Variables to train.\n    variables_to_train = _get_variables_to_train()\n    logging.info(\'Training the following variables: %s\' % (\n      \' \'.join([el.name for el in variables_to_train])))\n\n    #  and returns a train_tensor and summary_op\n    total_loss, clones_gradients = model_deploy.optimize_clones(\n        clones,\n        optimizer,\n        var_list=variables_to_train)\n\n    # clip the gradients if needed\n    if FLAGS.clip_gradients > 0:\n      logging.info(\'Clipping gradients by %f\' % FLAGS.clip_gradients)\n      with tf.name_scope(\'clip_gradients\'):\n        clones_gradients = slim.learning.clip_gradient_norms(\n            clones_gradients,\n            FLAGS.clip_gradients)\n\n    # Add total_loss to summary.\n    summaries.add(tf.scalar_summary(\'total_loss\', total_loss,\n                                    name=\'total_loss\'))\n\n    # Create gradient updates.\n    train_ops = {}\n    if FLAGS.iter_size == 1:\n      grad_updates = optimizer.apply_gradients(clones_gradients,\n                                               global_step=global_step)\n      update_ops.append(grad_updates)\n\n      update_op = tf.group(*update_ops)\n      train_tensor = control_flow_ops.with_dependencies([update_op], total_loss,\n                                                        name=\'train_op\')\n      train_ops = train_tensor\n    else:\n      gvs = [(grad, var) for grad, var in clones_gradients]\n      varnames = [var.name for grad, var in gvs]\n      varname_to_var = {var.name: var for grad, var in gvs}\n      varname_to_grad = {var.name: grad for grad, var in gvs}\n      varname_to_ref_grad = {}\n      for vn in varnames:\n        grad = varname_to_grad[vn]\n        print(""accumulating ... "", (vn, grad.get_shape()))\n        with tf.variable_scope(""ref_grad""):\n          with tf.device(deploy_config.variables_device()):\n            ref_var = slim.local_variable(\n                np.zeros(grad.get_shape(),dtype=np.float32),\n                name=vn[:-2])\n            varname_to_ref_grad[vn] = ref_var\n\n      all_assign_ref_op = [ref.assign(varname_to_grad[vn]) for vn, ref in varname_to_ref_grad.items()]\n      all_assign_add_ref_op = [ref.assign_add(varname_to_grad[vn]) for vn, ref in varname_to_ref_grad.items()]\n      assign_gradients_ref_op = tf.group(*all_assign_ref_op)\n      accmulate_gradients_op = tf.group(*all_assign_add_ref_op)\n      with tf.control_dependencies([accmulate_gradients_op]):\n        final_gvs = [(varname_to_ref_grad[var.name] / float(FLAGS.iter_size), var) for grad, var in gvs]\n        apply_gradients_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        update_ops.append(apply_gradients_op)\n        update_op = tf.group(*update_ops)\n        train_tensor = control_flow_ops.with_dependencies([update_op],\n            total_loss, name=\'train_op\')\n      for i in range(FLAGS.iter_size):\n        if i == 0:\n          train_ops[i] = assign_gradients_ref_op\n        elif i < FLAGS.iter_size - 1:  # because apply_gradients also computes\n                                       # (see control_dependency), so\n                                       # no need of running an extra iteration\n          train_ops[i] = accmulate_gradients_op\n        else:\n          train_ops[i] = train_tensor\n\n\n    # Add the summaries from the first clone. These contain the summaries\n    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                       first_clone_scope))\n\n    # Merge all summaries together.\n    summary_op = tf.merge_summary(list(summaries), name=\'summary_op\')\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.intra_op_parallelism_threads = FLAGS.cpu_threads\n    # config.allow_soft_placement = True\n    # config.gpu_options.per_process_gpu_memory_fraction=0.7\n\n    ###########################\n    # Kicks off the training. #\n    ###########################\n    logging.info(\'RUNNING ON SPLIT %d\' % FLAGS.split_id)\n    slim.learning.train(\n        train_ops,\n        train_step_fn=train_step,\n        logdir=FLAGS.train_dir,\n        master=FLAGS.master,\n        is_chief=(FLAGS.task == 0),\n        init_fn=_get_init_fn(),\n        summary_op=summary_op,\n        number_of_steps=FLAGS.max_number_of_steps,\n        log_every_n_steps=FLAGS.log_every_n_steps,\n        save_summaries_secs=FLAGS.save_summaries_secs,\n        save_interval_secs=FLAGS.save_interval_secs,\n        sync_optimizer=optimizer if FLAGS.sync_replicas else None,\n        session_config=config)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
datasets/__init__.py,0,b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n\n'
datasets/charades.py,0,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n""""""Provides data for the Charades dataset.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datasets.video_data_utils import gen_dataset\n\n\ndef get_split(split_name, dataset_dir, dataset_list_dir=\'\', file_pattern=None,\n              reader=None, modality=\'rgb\', num_samples=1,\n              split_id=1):\n\n  _NUM_CLASSES = 157\n  _LIST_FN = lambda split, id: \\\n            \'%s/%s_split%d.txt\' % (dataset_list_dir, split, id)\n\n  return gen_dataset(split_name, dataset_dir, file_pattern,\n                     reader, modality, num_samples, split_id,\n                     _NUM_CLASSES, _LIST_FN)\n'"
datasets/cifar10.py,6,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the Cifar10 dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/slim/data/create_cifar10_dataset.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'cifar10_%s.tfrecord\'\n\nSPLITS_TO_SIZES = {\'train\': 50000, \'test\': 10000}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A [32 x 32 x 3] color image.\',\n    \'label\': \'A single integer between 0 and 9\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading cifar10.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if not reader:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(shape=[32, 32, 3]),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
datasets/convert_video_frames_stacked.py,0,"b""# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\nimport os\nimport cv2\nimport math\nimport numpy as np\n\nif 1:\n  vidlist = '/nfs.yoda/rgirdhar/Work/Data2/018_VideoVLAD/raw/HMDB51/lists/train_test_lists2/All.txt'\n  if 1:\n    modality = 'flow'\n    imgdir = '/scratch/rgirdhar/Work/Data/018_VideoVLAD/raw/HMDB51/flow2'\n    outdir = '/scratch/rgirdhar/Work/Data/018_VideoVLAD/raw/HMDB51/flow3_stacked'\nelif 0:\n  vidlist = '/nfs.yoda/rgirdhar/Work/Data2/018_VideoVLAD/raw/HMDB51/lists/train_test_lists2/All.txt'\n  if 1:\n    modality = 'rgb'\n    imgdir = '/scratch/rgirdhar/Work/Data/018_VideoVLAD/raw/HMDB51/frames2'\n    outdir = '/scratch/rgirdhar/Work/Data/018_VideoVLAD/raw/HMDB51/frames3_stacked'\n\n\ndef get_img_rgb(vpath, nframes, num_samples=25):\n  duration = nframes\n  step = int(math.floor((duration-1)/(num_samples-1)))\n  imgs = []\n  for i in range(num_samples):\n    impath = os.path.join(imgdir, vpath, 'image_%05d.jpg' % (i*step+1))\n    imgs.append(cv2.resize(cv2.imread(impath), (340, 256)))\n  return [np.vstack(imgs)]\n\n\ndef get_img_flow(vpath, nframes, num_samples=25, optical_flow_frames=10):\n  duration = nframes\n  step = int(math.floor((duration-optical_flow_frames)/(num_samples)))\n  imgs = []\n  for i in range(num_samples):\n    subimg = []\n    for j in range(optical_flow_frames):\n      for d in ['x', 'y']:\n        impath = os.path.join(imgdir, vpath, 'flow_%c_%05d.jpg' % (d, i*step+j+1))\n        I = cv2.imread(impath, 0)\n        if I is None:\n          print 'couldnt read %s' % impath\n          I = np.ones((340, 256)) * 128\n        subimg.append(cv2.resize(I, (340, 256)))\n    imgs.append(np.vstack(subimg))\n  return imgs\n\n\nwith open(vidlist, 'r') as fin:\n  for lno,line in enumerate(fin):\n    vpath, nframes, _ = line.split()\n    nframes = int(nframes)\n    outfpath = os.path.join(outdir, vpath, 'image.jpg')\n    # if not locker.lock(outfpath):\n    #   continue\n    if modality == 'flow':\n      img = get_img_flow(vpath, nframes, 25, optical_flow_frames=10)\n    else:\n      img = get_img_rgb(vpath, nframes, 25)\n    try:\n      os.makedirs(os.path.join(outdir, vpath))\n    except:\n      pass\n    if len(img) == 1:\n      cv2.imwrite(outfpath, img[0])\n    else:\n      for imid,im in enumerate(img):\n        cv2.imwrite(os.path.join(outdir, vpath, 'image%d.jpg' % imid), im)\n    print('Done %d' % (lno))\n    # locker.unlock(outfpath)\n"""
datasets/dataset_data_provider.py,3,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A DataProvider that provides data from a Dataset.\n\nDatasetDataProviders provide data from datasets. The provide can be configured\nto use multiple readers simultaneously or read via a single reader.\nAdditionally, the data being read can be optionally shuffled.\n\nFor example, to read data using a single thread without shuffling:\n\n  pascal_voc_data_provider = DatasetDataProvider(\n      slim.datasets.pascal_voc.get_split(\'train\'),\n      shuffle=False)\n  images, labels = pascal_voc_data_provider.Get([\'images\', \'labels\'])\n\nTo read data using multiple readers simultaneous with shuffling:\n\n  pascal_voc_data_provider = DatasetDataProvider(\n      slim.datasets.pascal_voc.Dataset(),\n      num_readers=10,\n      shuffle=True)\n  images, labels = pascal_voc_data_provider.Get([\'images\', \'labels\'])\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.contrib.slim.python.slim.data import data_provider\nfrom tensorflow.contrib.slim.python.slim.data import parallel_reader\nfrom tensorflow.python.platform import tf_logging as logging\nimport tensorflow as tf\n\n\nclass DatasetDataProvider(data_provider.DataProvider):\n\n  def __init__(self, dataset, num_readers=1, shuffle=True, num_epochs=None,\n               common_queue_capacity=256, common_queue_min=128,\n               bgr_flips=None):\n    """"""Creates a DatasetDataProvider.\n\n    Args:\n      dataset: An instance of the Dataset class.\n      num_readers: The number of parallel readers to use.\n      shuffle: Whether to shuffle the data sources and common queue when\n        reading.\n      num_epochs: The number of times each data source is read. If left as None,\n        the data will be cycled through indefinitely.\n      common_queue_capacity: The capacity of the common queue.\n      common_queue_min: The minimum number of elements in the common queue after\n        a dequeue.\n    """"""\n    self.num_channels_stream = []\n    if bgr_flips is not None:\n      bgr_flips = bgr_flips.split(\',\')\n    img_str, label = parallel_reader.parallel_read(\n        dataset.data_sources,\n        reader_class=dataset.reader,\n        num_epochs=num_epochs,\n        num_readers=num_readers,\n        shuffle=shuffle,\n        capacity=common_queue_capacity,\n        min_after_dequeue=common_queue_min)\n\n    items = dataset.decoder.list_items()\n    imgs = dataset.decoder.decode(img_str, items)\n    num_streams = len(imgs[0])\n    final_imgs = []\n    for sid in range(num_streams):\n      self.num_channels_stream.append(imgs[0][sid].get_shape().as_list()[-1])\n      img_stream = []\n      for bid in range(len(imgs)):\n        img_stream.append(imgs[bid][sid])\n      img = tf.pack(img_stream)\n      if bgr_flips[sid] == \'True\':\n        logging.info(\'BGR flipping stream %d\' % sid)\n        img = tf.reverse(img, [False, False, False, True])\n      final_imgs.append(img)\n\n    img = tf.concat(3, final_imgs)\n    tensors = [img, label]\n\n    super(DatasetDataProvider, self).__init__(\n        items_to_tensors=dict(zip(items, tensors)),\n        num_samples=dataset.num_samples)\n'"
datasets/dataset_factory.py,1,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A factory-pattern class which returns classification image/label pairs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datasets import cifar10\nfrom datasets import flowers\nfrom datasets import imagenet\nfrom datasets import mnist\nfrom datasets import ucf101\nfrom datasets import charades\nfrom datasets import places365\nfrom datasets import hmdb51\n\ndatasets_map = {\n    \'cifar10\': cifar10,\n    \'flowers\': flowers,\n    \'imagenet\': imagenet,\n    \'mnist\': mnist,\n    \'ucf101\': ucf101,\n    \'charades\': charades,\n    \'places365\': places365,\n    \'hmdb51\': hmdb51,\n}\n\n\ndef get_dataset(name, split_name, dataset_dir, dataset_list_dir=\'\',\n                file_pattern=None, reader=None, modality=\'rgb\', num_samples=1,\n                split_id=1):\n  """"""Given a dataset name and a split_name returns a Dataset.\n\n  Args:\n    name: String, the name of the dataset.\n    split_name: A train/test split name.\n    dataset_dir: The directory where the dataset files are stored.\n    dataset_list_dir: The directory where train/test splits are stored.\n    file_pattern: The file pattern to use for matching the dataset source files.\n    reader: The subclass of tf.ReaderBase. If left as `None`, then the default\n      reader defined by each dataset is used.\n    modality: In case of video datasets, you could read RGB or Flow frames\n    num_samples: In case of videos, number of frames per video\n\n  Returns:\n    A `Dataset` class.\n\n  Raises:\n    ValueError: If the dataset `name` is unknown.\n  """"""\n  if name not in datasets_map:\n    raise ValueError(\'Name of dataset unknown %s\' % name)\n  return datasets_map[name].get_split(\n      split_name,\n      dataset_dir,\n      dataset_list_dir,\n      file_pattern,\n      reader,\n      modality,\n      num_samples,\n      split_id=split_id)\n'"
datasets/dataset_utils.py,6,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains utilities for downloading and converting datasets.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\nLABELS_FILENAME = \'labels.txt\'\n\n\ndef int64_feature(values):\n  """"""Returns a TF-Feature of int64s.\n\n  Args:\n    values: A scalar or list of values.\n\n  Returns:\n    a TF-Feature.\n  """"""\n  if not isinstance(values, (tuple, list)):\n    values = [values]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\n\ndef bytes_feature(values):\n  """"""Returns a TF-Feature of bytes.\n\n  Args:\n    values: A string.\n\n  Returns:\n    a TF-Feature.\n  """"""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n\n\ndef image_to_tfexample(image_data, image_format, height, width, class_id):\n  return tf.train.Example(features=tf.train.Features(feature={\n      \'image/encoded\': bytes_feature(image_data),\n      \'image/format\': bytes_feature(image_format),\n      \'image/class/label\': int64_feature(class_id),\n      \'image/height\': int64_feature(height),\n      \'image/width\': int64_feature(width),\n  }))\n\n\ndef download_and_uncompress_tarball(tarball_url, dataset_dir):\n  """"""Downloads the `tarball_url` and uncompresses it locally.\n\n  Args:\n    tarball_url: The URL of a tarball file.\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = tarball_url.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n\n  def _progress(count, block_size, total_size):\n    sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n        filename, float(count * block_size) / float(total_size) * 100.0))\n    sys.stdout.flush()\n  filepath, _ = urllib.request.urlretrieve(tarball_url, filepath, _progress)\n  print()\n  statinfo = os.stat(filepath)\n  print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  tarfile.open(filepath, \'r:gz\').extractall(dataset_dir)\n\n\ndef write_label_file(labels_to_class_names, dataset_dir,\n                     filename=LABELS_FILENAME):\n  """"""Writes a file with the list of class names.\n\n  Args:\n    labels_to_class_names: A map of (integer) labels to class names.\n    dataset_dir: The directory in which the labels file should be written.\n    filename: The filename where the class names are written.\n  """"""\n  labels_filename = os.path.join(dataset_dir, filename)\n  with tf.gfile.Open(labels_filename, \'w\') as f:\n    for label in labels_to_class_names:\n      class_name = labels_to_class_names[label]\n      f.write(\'%d:%s\\n\' % (label, class_name))\n\n\ndef has_labels(dataset_dir, filename=LABELS_FILENAME):\n  """"""Specifies whether or not the dataset directory contains a label map file.\n\n  Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n  Returns:\n    `True` if the labels file exists and `False` otherwise.\n  """"""\n  return tf.gfile.Exists(os.path.join(dataset_dir, filename))\n\n\ndef read_label_file(dataset_dir, filename=LABELS_FILENAME):\n  """"""Reads the labels file and returns a mapping from ID to class name.\n\n  Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n  Returns:\n    A map from a label (integer) to class name.\n  """"""\n  labels_filename = os.path.join(dataset_dir, filename)\n  with tf.gfile.Open(labels_filename, \'r\') as f:\n    lines = f.read().decode()\n  lines = lines.split(\'\\n\')\n  lines = filter(None, lines)\n\n  labels_to_class_names = {}\n  for line in lines:\n    index = line.index(\':\')\n    labels_to_class_names[int(line[:index])] = line[index+1:]\n  return labels_to_class_names\n'"
datasets/download_and_convert_cifar10.py,12,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts cifar10 data to TFRecords of TF-Example protos.\n\nThis module downloads the cifar10 data, uncompresses it, reads the files\nthat make up the cifar10 data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take several minutes to run.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cPickle\nimport os\nimport sys\nimport tarfile\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\n# The URL where the CIFAR data can be downloaded.\n_DATA_URL = \'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'\n\n# The number of training files.\n_NUM_TRAIN_FILES = 5\n\n# The height and width of each image.\n_IMAGE_SIZE = 32\n\n# The names of the classes.\n_CLASS_NAMES = [\n    \'airplane\',\n    \'automobile\',\n    \'bird\',\n    \'cat\',\n    \'deer\',\n    \'dog\',\n    \'frog\',\n    \'horse\',\n    \'ship\',\n    \'truck\',\n]\n\n\ndef _add_to_tfrecord(filename, tfrecord_writer, offset=0):\n  """"""Loads data from the cifar10 pickle files and writes files to a TFRecord.\n\n  Args:\n    filename: The filename of the cifar10 pickle file.\n    tfrecord_writer: The TFRecord writer to use for writing.\n    offset: An offset into the absolute number of images previously written.\n\n  Returns:\n    The new offset.\n  """"""\n  with tf.gfile.Open(filename, \'r\') as f:\n    data = cPickle.load(f)\n\n  images = data[\'data\']\n  num_images = images.shape[0]\n\n  images = images.reshape((num_images, 3, 32, 32))\n  labels = data[\'labels\']\n\n  with tf.Graph().as_default():\n    image_placeholder = tf.placeholder(dtype=tf.uint8)\n    encoded_image = tf.image.encode_png(image_placeholder)\n\n    with tf.Session(\'\') as sess:\n\n      for j in range(num_images):\n        sys.stdout.write(\'\\r>> Reading file [%s] image %d/%d\' % (\n            filename, offset + j + 1, offset + num_images))\n        sys.stdout.flush()\n\n        image = np.squeeze(images[j]).transpose((1, 2, 0))\n        label = labels[j]\n\n        png_string = sess.run(encoded_image,\n                              feed_dict={image_placeholder: image})\n\n        example = dataset_utils.image_to_tfexample(\n            png_string, \'png\', _IMAGE_SIZE, _IMAGE_SIZE, label)\n        tfrecord_writer.write(example.SerializeToString())\n\n  return offset + num_images\n\n\ndef _get_output_filename(dataset_dir, split_name):\n  """"""Creates the output filename.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n    split_name: The name of the train/test split.\n\n  Returns:\n    An absolute file path.\n  """"""\n  return \'%s/cifar10_%s.tfrecord\' % (dataset_dir, split_name)\n\n\ndef _download_and_uncompress_dataset(dataset_dir):\n  """"""Downloads cifar10 and uncompresses it locally.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n\n  if not os.path.exists(filepath):\n    def _progress(count, block_size, total_size):\n      sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n          filename, float(count * block_size) / float(total_size) * 100.0))\n      sys.stdout.flush()\n    filepath, _ = urllib.request.urlretrieve(_DATA_URL, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n    tarfile.open(filepath, \'r:gz\').extractall(dataset_dir)\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n  tf.gfile.Remove(filepath)\n\n  tmp_dir = os.path.join(dataset_dir, \'cifar-10-batches-py\')\n  tf.gfile.DeleteRecursively(tmp_dir)\n\n\ndef run(dataset_dir):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  training_filename = _get_output_filename(dataset_dir, \'train\')\n  testing_filename = _get_output_filename(dataset_dir, \'test\')\n\n  if tf.gfile.Exists(training_filename) and tf.gfile.Exists(testing_filename):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n\n  # First, process the training data:\n  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:\n    offset = 0\n    for i in range(_NUM_TRAIN_FILES):\n      filename = os.path.join(dataset_dir,\n                              \'cifar-10-batches-py\',\n                              \'data_batch_%d\' % (i + 1))  # 1-indexed.\n      offset = _add_to_tfrecord(filename, tfrecord_writer, offset)\n\n  # Next, process the testing data:\n  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:\n    filename = os.path.join(dataset_dir,\n                            \'cifar-10-batches-py\',\n                            \'test_batch\')\n    _add_to_tfrecord(filename, tfrecord_writer)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the Cifar10 dataset!\')\n'"
datasets/download_and_convert_flowers.py,11,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts Flowers data to TFRecords of TF-Example protos.\n\nThis module downloads the Flowers data, uncompresses it, reads the files\nthat make up the Flowers data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take about a minute to run.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\nimport random\nimport sys\n\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\n# The URL where the Flowers data can be downloaded.\n_DATA_URL = \'http://download.tensorflow.org/example_images/flower_photos.tgz\'\n\n# The number of images in the validation set.\n_NUM_VALIDATION = 350\n\n# Seed for repeatability.\n_RANDOM_SEED = 0\n\n# The number of shards per dataset split.\n_NUM_SHARDS = 5\n\n\nclass ImageReader(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def read_image_dims(self, sess, image_data):\n    image = self.decode_jpeg(sess, image_data)\n    return image.shape[0], image.shape[1]\n\n  def decode_jpeg(self, sess, image_data):\n    image = sess.run(self._decode_jpeg,\n                     feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\ndef _get_filenames_and_classes(dataset_dir):\n  """"""Returns a list of filenames and inferred class names.\n\n  Args:\n    dataset_dir: A directory containing a set of subdirectories representing\n      class names. Each subdirectory should contain PNG or JPG encoded images.\n\n  Returns:\n    A list of image file paths, relative to `dataset_dir` and the list of\n    subdirectories, representing class names.\n  """"""\n  flower_root = os.path.join(dataset_dir, \'flower_photos\')\n  directories = []\n  class_names = []\n  for filename in os.listdir(flower_root):\n    path = os.path.join(flower_root, filename)\n    if os.path.isdir(path):\n      directories.append(path)\n      class_names.append(filename)\n\n  photo_filenames = []\n  for directory in directories:\n    for filename in os.listdir(directory):\n      path = os.path.join(directory, filename)\n      photo_filenames.append(path)\n\n  return photo_filenames, sorted(class_names)\n\n\ndef _get_dataset_filename(dataset_dir, split_name, shard_id):\n  output_filename = \'flowers_%s_%05d-of-%05d.tfrecord\' % (\n      split_name, shard_id, _NUM_SHARDS)\n  return os.path.join(dataset_dir, output_filename)\n\n\ndef _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir):\n  """"""Converts the given filenames to a TFRecord dataset.\n\n  Args:\n    split_name: The name of the dataset, either \'train\' or \'validation\'.\n    filenames: A list of absolute paths to png or jpg images.\n    class_names_to_ids: A dictionary from class names (strings) to ids\n      (integers).\n    dataset_dir: The directory where the converted datasets are stored.\n  """"""\n  assert split_name in [\'train\', \'validation\']\n\n  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n\n  with tf.Graph().as_default():\n    image_reader = ImageReader()\n\n    with tf.Session(\'\') as sess:\n\n      for shard_id in range(_NUM_SHARDS):\n        output_filename = _get_dataset_filename(\n            dataset_dir, split_name, shard_id)\n\n        with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n          start_ndx = shard_id * num_per_shard\n          end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n          for i in range(start_ndx, end_ndx):\n            sys.stdout.write(\'\\r>> Converting image %d/%d shard %d\' % (\n                i+1, len(filenames), shard_id))\n            sys.stdout.flush()\n\n            # Read the filename:\n            image_data = tf.gfile.FastGFile(filenames[i], \'r\').read()\n            height, width = image_reader.read_image_dims(sess, image_data)\n\n            class_name = os.path.basename(os.path.dirname(filenames[i]))\n            class_id = class_names_to_ids[class_name]\n\n            example = dataset_utils.image_to_tfexample(\n                image_data, \'jpg\', height, width, class_id)\n            tfrecord_writer.write(example.SerializeToString())\n\n  sys.stdout.write(\'\\n\')\n  sys.stdout.flush()\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n  tf.gfile.Remove(filepath)\n\n  tmp_dir = os.path.join(dataset_dir, \'flower_photos\')\n  tf.gfile.DeleteRecursively(tmp_dir)\n\n\ndef _dataset_exists(dataset_dir):\n  for split_name in [\'train\', \'validation\']:\n    for shard_id in range(_NUM_SHARDS):\n      output_filename = _get_dataset_filename(\n          dataset_dir, split_name, shard_id)\n      if not tf.gfile.Exists(output_filename):\n        return False\n  return True\n\n\ndef run(dataset_dir):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  if _dataset_exists(dataset_dir):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n  photo_filenames, class_names = _get_filenames_and_classes(dataset_dir)\n  class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n\n  # Divide into train and test:\n  random.seed(_RANDOM_SEED)\n  random.shuffle(photo_filenames)\n  training_filenames = photo_filenames[_NUM_VALIDATION:]\n  validation_filenames = photo_filenames[:_NUM_VALIDATION]\n\n  # First, convert the training and validation sets.\n  _convert_dataset(\'train\', training_filenames, class_names_to_ids,\n                   dataset_dir)\n  _convert_dataset(\'validation\', validation_filenames, class_names_to_ids,\n                   dataset_dir)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the Flowers dataset!\')\n\n'"
datasets/download_and_convert_mnist.py,11,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts MNIST data to TFRecords of TF-Example protos.\n\nThis module downloads the MNIST data, uncompresses it, reads the files\nthat make up the MNIST data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take about a minute to run.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport sys\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\n# The URLs where the MNIST data can be downloaded.\n_DATA_URL = \'http://yann.lecun.com/exdb/mnist/\'\n_TRAIN_DATA_FILENAME = \'train-images-idx3-ubyte.gz\'\n_TRAIN_LABELS_FILENAME = \'train-labels-idx1-ubyte.gz\'\n_TEST_DATA_FILENAME = \'t10k-images-idx3-ubyte.gz\'\n_TEST_LABELS_FILENAME = \'t10k-labels-idx1-ubyte.gz\'\n\n_IMAGE_SIZE = 28\n_NUM_CHANNELS = 1\n\n# The names of the classes.\n_CLASS_NAMES = [\n    \'zero\',\n    \'one\',\n    \'two\',\n    \'three\',\n    \'four\',\n    \'five\',\n    \'size\',\n    \'seven\',\n    \'eight\',\n    \'nine\',\n]\n\n\ndef _extract_images(filename, num_images):\n  """"""Extract the images into a numpy array.\n\n  Args:\n    filename: The path to an MNIST images file.\n    num_images: The number of images in the file.\n\n  Returns:\n    A numpy array of shape [number_of_images, height, width, channels].\n  """"""\n  print(\'Extracting images from: \', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(16)\n    buf = bytestream.read(\n        _IMAGE_SIZE * _IMAGE_SIZE * num_images * _NUM_CHANNELS)\n    data = np.frombuffer(buf, dtype=np.uint8)\n    data = data.reshape(num_images, _IMAGE_SIZE, _IMAGE_SIZE, _NUM_CHANNELS)\n  return data\n\n\ndef _extract_labels(filename, num_labels):\n  """"""Extract the labels into a vector of int64 label IDs.\n\n  Args:\n    filename: The path to an MNIST labels file.\n    num_labels: The number of labels in the file.\n\n  Returns:\n    A numpy array of shape [number_of_labels]\n  """"""\n  print(\'Extracting labels from: \', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(8)\n    buf = bytestream.read(1 * num_labels)\n    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n  return labels\n\n\ndef _add_to_tfrecord(data_filename, labels_filename, num_images,\n                     tfrecord_writer):\n  """"""Loads data from the binary MNIST files and writes files to a TFRecord.\n\n  Args:\n    data_filename: The filename of the MNIST images.\n    labels_filename: The filename of the MNIST labels.\n    num_images: The number of images in the dataset.\n    tfrecord_writer: The TFRecord writer to use for writing.\n  """"""\n  images = _extract_images(data_filename, num_images)\n  labels = _extract_labels(labels_filename, num_images)\n\n  shape = (_IMAGE_SIZE, _IMAGE_SIZE, _NUM_CHANNELS)\n  with tf.Graph().as_default():\n    image = tf.placeholder(dtype=tf.uint8, shape=shape)\n    encoded_png = tf.image.encode_png(image)\n\n    with tf.Session(\'\') as sess:\n      for j in range(num_images):\n        sys.stdout.write(\'\\r>> Converting image %d/%d\' % (j + 1, num_images))\n        sys.stdout.flush()\n\n        png_string = sess.run(encoded_png, feed_dict={image: images[j]})\n\n        example = dataset_utils.image_to_tfexample(\n            png_string, \'png\', _IMAGE_SIZE, _IMAGE_SIZE, labels[j])\n        tfrecord_writer.write(example.SerializeToString())\n\n\ndef _get_output_filename(dataset_dir, split_name):\n  """"""Creates the output filename.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n    split_name: The name of the train/test split.\n\n  Returns:\n    An absolute file path.\n  """"""\n  return \'%s/mnist_%s.tfrecord\' % (dataset_dir, split_name)\n\n\ndef _download_dataset(dataset_dir):\n  """"""Downloads MNIST locally.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  for filename in [_TRAIN_DATA_FILENAME,\n                   _TRAIN_LABELS_FILENAME,\n                   _TEST_DATA_FILENAME,\n                   _TEST_LABELS_FILENAME]:\n    filepath = os.path.join(dataset_dir, filename)\n\n    if not os.path.exists(filepath):\n      print(\'Downloading file %s...\' % filename)\n      def _progress(count, block_size, total_size):\n        sys.stdout.write(\'\\r>> Downloading %.1f%%\' % (\n            float(count * block_size) / float(total_size) * 100.0))\n        sys.stdout.flush()\n      filepath, _ = urllib.request.urlretrieve(_DATA_URL + filename,\n                                               filepath,\n                                               _progress)\n      print()\n      with tf.gfile.GFile(filepath) as f:\n        size = f.Size()\n      print(\'Successfully downloaded\', filename, size, \'bytes.\')\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  for filename in [_TRAIN_DATA_FILENAME,\n                   _TRAIN_LABELS_FILENAME,\n                   _TEST_DATA_FILENAME,\n                   _TEST_LABELS_FILENAME]:\n    filepath = os.path.join(dataset_dir, filename)\n    tf.gfile.Remove(filepath)\n\n\ndef run(dataset_dir):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  training_filename = _get_output_filename(dataset_dir, \'train\')\n  testing_filename = _get_output_filename(dataset_dir, \'test\')\n\n  if tf.gfile.Exists(training_filename) and tf.gfile.Exists(testing_filename):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  _download_dataset(dataset_dir)\n\n  # First, process the training data:\n  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:\n    data_filename = os.path.join(dataset_dir, _TRAIN_DATA_FILENAME)\n    labels_filename = os.path.join(dataset_dir, _TRAIN_LABELS_FILENAME)\n    _add_to_tfrecord(data_filename, labels_filename, 60000, tfrecord_writer)\n\n  # Next, process the testing data:\n  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:\n    data_filename = os.path.join(dataset_dir, _TEST_DATA_FILENAME)\n    labels_filename = os.path.join(dataset_dir, _TEST_LABELS_FILENAME)\n    _add_to_tfrecord(data_filename, labels_filename, 10000, tfrecord_writer)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the MNIST dataset!\')\n'"
datasets/flowers.py,6,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the Cifar10 dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/slim/data/create_cifar10_dataset.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'flowers_%s_*.tfrecord\'\n\nSPLITS_TO_SIZES = {\'train\': 3320, \'validation\': 350}\n\n_NUM_CLASSES = 5\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying size.\',\n    \'label\': \'A single integer between 0 and 4\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading cifar10.\n\n  Args:\n    split_name: A train/validation split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/validation split.\n  """"""\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
datasets/hmdb51.py,0,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n""""""Provides data for the HMDB51 dataset.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datasets.video_data_utils import gen_dataset\n\n\ndef get_split(split_name, dataset_dir, dataset_list_dir=\'\', file_pattern=None,\n              reader=None, modality=\'rgb\', num_samples=1,\n              split_id=1):\n  \n  _NUM_CLASSES = 51\n  _LIST_FN = lambda split, id: \\\n      \'%s/%s_split%d.txt\' % (dataset_list_dir, split, id)\n\n  return gen_dataset(split_name, dataset_dir, file_pattern,\n                     reader, modality, num_samples, split_id,\n                     _NUM_CLASSES, _LIST_FN)\n'"
datasets/image_read_utils.py,33,"b""# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\nimport tensorflow as tf\n\nIM_HT = 256\nIM_WD = 340\n\ndef _read_from_disk_spatial(fpath, nframes, num_samples=25, start_frame=0,\n                            file_prefix='', file_zero_padding=4, file_index=1,\n                            dataset_dir='', step=None):\n    duration = nframes\n    if step is None:\n      if num_samples == 1:\n          step = tf.random_uniform([1], 0, nframes, dtype='int32')[0]\n      else:\n          step = tf.cast((duration-tf.constant(1)) /\n                         (tf.constant(num_samples-1)), 'int32')\n    allimgs = []\n    with tf.variable_scope('read_rgb_video'):\n        for i in range(num_samples):\n            if num_samples == 1:\n                i = 1  # so that the random step value can be used\n            with tf.variable_scope('read_rgb_image'):\n                prefix = file_prefix + '_' if file_prefix else ''\n                impath = tf.string_join([\n                    tf.constant(dataset_dir + '/'),\n                    fpath, tf.constant('/'),\n                    prefix,\n                    tf.as_string(start_frame + i * step + file_index,\n                      width=file_zero_padding, fill='0'),\n                    tf.constant('.jpg')])\n                img_str = tf.read_file(impath)\n            allimgs.append(img_str)\n    return allimgs\n\n\ndef _read_from_disk_temporal(\n    fpath, nframes, num_samples=25,\n    optical_flow_frames=10, start_frame=0,\n    file_prefix='', file_zero_padding=4, file_index=1,\n    dataset_dir='', step=None):\n    duration = nframes\n    if step is None:\n      if num_samples == 1:\n          step = tf.random_uniform([1], 0, nframes-optical_flow_frames-1, dtype='int32')[0]\n      else:\n          step = tf.cast((duration-tf.constant(optical_flow_frames)) /\n                         (tf.constant(num_samples)), 'int32')\n    allimgs = []\n    with tf.variable_scope('read_flow_video'):\n        for i in range(num_samples):\n            if num_samples == 1:\n                i = 1  # so that the random step value can be used\n            with tf.variable_scope('read_flow_image'):\n              flow_img = []\n              for j in range(optical_flow_frames):\n                with tf.variable_scope('read_flow_channels'):\n                  for dr in ['x', 'y']:\n                    prefix = file_prefix + '_' if file_prefix else ''\n                    impath = tf.string_join([\n                        tf.constant(dataset_dir + '/'),\n                        fpath, tf.constant('/'),\n                        prefix, '%s_' % dr,\n                        tf.as_string(start_frame + i * step + file_index + j,\n                          width=file_zero_padding, fill='0'),\n                        tf.constant('.jpg')])\n                    img_str = tf.read_file(impath)\n                    flow_img.append(img_str)\n              allimgs.append(flow_img)\n    return allimgs\n\n\ndef decode_rgb(img_str):\n  with tf.variable_scope('decode_rgb_frame'):\n    img = tf.image.decode_jpeg(img_str, channels=3)\n    # Always convert before resize, this is a bug in TF\n    # https://github.com/tensorflow/tensorflow/issues/1763\n    # IMPORTANT NOTE: The original netvlad model was trained with the convert\n    # happening after the resize, and hence it's trained with the large values.\n    # It still works if I do that, but I'm training a new netvlad RGB model\n    # with the current setup.\n    img = tf.image.convert_image_dtype(img, dtype=tf.float32)\n    img = tf.image.resize_images(img, [IM_HT, IM_WD])\n  return [img]\n\n\ndef decode_flow(img_str):\n  # IMPORTANT NOTE: I am now resizing the flow frames before running through\n  # the preprocessing. I was not doing that earlier (in the master). This leads\n  # to the 66 number to drop to 63 on HMDB. But it should be fixable by\n  # re-training with this setup\n  with tf.variable_scope('decode_flow_frame'):\n    img = tf.concat(2, [tf.image.decode_jpeg(el, channels=1)\n      for el in tf.unpack(img_str)])\n    # Always convert before resize, this is a bug in TF\n    # https://github.com/tensorflow/tensorflow/issues/1763\n    img = tf.image.convert_image_dtype(img, dtype=tf.float32)\n    img = tf.image.resize_images(img, [IM_HT, IM_WD])\n  return [img]\n\n\ndef _decode_from_string(img_str, modality):\n  if modality == 'rgb':\n    return decode_rgb(img_str)\n  elif modality.startswith('flow'):\n    return decode_flow(img_str)\n  elif modality.startswith('rgb+flow'):\n    with tf.name_scope('decode_rgbNflow'):\n      img_rgb = decode_rgb(img_str[..., 0])\n      img_flow = decode_flow(img_str[..., 1:])\n      return [img_rgb[0], img_flow[0]]\n"""
datasets/imagenet.py,20,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the ImageNet ILSVRC 2012 Dataset plus some bounding boxes.\n\nSome images have one or more bounding boxes associated with the label of the\nimage. See details here: http://image-net.org/download-bboxes\n\nImageNet is based upon WordNet 3.0. To uniquely identify a synset, we use\n""WordNet ID"" (wnid), which is a concatenation of POS ( i.e. part of speech )\nand SYNSET OFFSET of WordNet. For more information, please refer to the\nWordNet documentation[http://wordnet.princeton.edu/wordnet/documentation/].\n\n""There are bounding boxes for over 3000 popular synsets available.\nFor each synset, there are on average 150 images with bounding boxes.""\n\nWARNING: Don\'t use for object detection, in this case all the bounding boxes\nof the image belong to just one class.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n# TODO(nsilberman): Add tfrecord file type once the script is updated.\n_FILE_PATTERN = \'%s-*\'\n\n_SPLITS_TO_SIZES = {\n    \'train\': 1281167,\n    \'validation\': 50000,\n}\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying height and width.\',\n    \'label\': \'The label id of the image, integer between 0 and 999\',\n    \'label_text\': \'The text of the label.\',\n    \'object/bbox\': \'A list of bounding boxes.\',\n    \'object/label\': \'A list of labels, one per each object.\',\n}\n\n_NUM_CLASSES = 1001\n\n\ndef create_readable_names_for_imagenet_labels():\n  """"""Create a dict mapping label id to human readable string.\n\n  Returns:\n      labels_to_names: dictionary where keys are integers from to 1000\n      and values are human-readable names.\n\n  We retrieve a synset file, which contains a list of valid synset labels used\n  by ILSVRC competition. There is one synset one per line, eg.\n          #   n01440764\n          #   n01443537\n  We also retrieve a synset_to_human_file, which contains a mapping from synsets\n  to human-readable names for every synset in Imagenet. These are stored in a\n  tsv format, as follows:\n          #   n02119247    black fox\n          #   n02119359    silver fox\n  We assign each synset (in alphabetical order) an integer, starting from 1\n  (since 0 is reserved for the background class).\n\n  Code is based on\n  https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py#L463\n  """"""\n\n  # pylint: disable=g-line-too-long\n  base_url = \'https://raw.githubusercontent.com/tensorflow/models/master/inception/inception/data/\'\n  synset_url = \'{}/imagenet_lsvrc_2015_synsets.txt\'.format(base_url)\n  synset_to_human_url = \'{}/imagenet_metadata.txt\'.format(base_url)\n\n  filename, _ = urllib.request.urlretrieve(synset_url)\n  synset_list = [s.strip() for s in open(filename).readlines()]\n  num_synsets_in_ilsvrc = len(synset_list)\n  assert num_synsets_in_ilsvrc == 1000\n\n  filename, _ = urllib.request.urlretrieve(synset_to_human_url)\n  synset_to_human_list = open(filename).readlines()\n  num_synsets_in_all_imagenet = len(synset_to_human_list)\n  assert num_synsets_in_all_imagenet == 21842\n\n  synset_to_human = {}\n  for s in synset_to_human_list:\n    parts = s.strip().split(\'\\t\')\n    assert len(parts) == 2\n    synset = parts[0]\n    human = parts[1]\n    synset_to_human[synset] = human\n\n  label_index = 1\n  labels_to_names = {0: \'background\'}\n  for synset in synset_list:\n    name = synset_to_human[synset]\n    labels_to_names[label_index] = name\n    label_index += 1\n\n  return labels_to_names\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading ImageNet.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature(\n          (), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature(\n          (), tf.string, default_value=\'jpeg\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], dtype=tf.int64, default_value=-1),\n      \'image/class/text\': tf.FixedLenFeature(\n          [], dtype=tf.string, default_value=\'\'),\n      \'image/object/bbox/xmin\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/ymin\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/xmax\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/ymax\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/class/label\': tf.VarLenFeature(\n          dtype=tf.int64),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n      \'label_text\': slim.tfexample_decoder.Tensor(\'image/class/text\'),\n      \'object/bbox\': slim.tfexample_decoder.BoundingBox(\n          [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\'),\n      \'object/label\': slim.tfexample_decoder.Tensor(\'image/object/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n  else:\n    labels_to_names = create_readable_names_for_imagenet_labels()\n    dataset_utils.write_label_file(labels_to_names, dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=_SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
datasets/mnist.py,6,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the MNIST dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/slim/data/create_mnist_dataset.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'mnist_%s.tfrecord\'\n\n_SPLITS_TO_SIZES = {\'train\': 60000, \'test\': 10000}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A [28 x 28 x 1] grayscale image.\',\n    \'label\': \'A single integer between 0 and 9\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading MNIST.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'raw\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [1], tf.int64, default_value=tf.zeros([1], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(shape=[28, 28, 1], channels=1),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\', shape=[]),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=_SPLITS_TO_SIZES[split_name],\n      num_classes=_NUM_CLASSES,\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      labels_to_names=labels_to_names)\n'"
datasets/places365.py,7,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n""""""Provides data for the UCF101 dataset.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\nfrom datasets.image_read_utils import _decode_from_string\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_TRAIN_LIST = \'/home/rgirdhar/Work/Data/020_Places365/places365_train_standard.txt\'\n_VAL_LIST = \'/home/rgirdhar/Work/Data/020_Places365/places365_val.txt\'\n\nFLAGS = tf.app.flags.FLAGS\n\nSPLITS_TO_SIZES = {\'train\': 1803460, \'val\': 36500}\n\n_NUM_CLASSES = 365\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A [? x ? x 3] color image.\',\n    \'label\': \'A single integer between 0 and 364\',\n}\n\n\ndef readerFn(num_samples=1):\n  class reader_func(tf.ReaderBase):\n    @staticmethod\n    def read(filename_queue):\n      value = filename_queue.dequeue()\n      fpath, label = tf.decode_csv(\n          value, record_defaults=[[\'\'], [\'\']],\n          field_delim=\' \')\n      image_buffer = tf.read_file(fpath)\n      return [image_buffer, label]\n  return reader_func\n\n\ndef decoderFn(num_samples=1):\n  class decoder_func(slim.data_decoder.DataDecoder):\n    @staticmethod\n    def list_items():\n      return [\'image\', \'label\']\n\n\n    @staticmethod\n    def decode(data, items):\n      image_buffer = _decode_from_string(data)\n      # if num_samples == 1:\n        # tf.Assert(tf.shape(image_buffer)[0] == 1, image_buffer)\n        # image_buffer = image_buffer[0]\n      # else:\n      image_buffer = tf.pack(image_buffer)\n      return image_buffer\n  return decoder_func\n\n\ndef get_split(split_name, dataset_dir, dataset_list_dir=\'\', file_pattern=None, reader=None, modality=\'rgb\', num_samples=1):\n  """"""Gets a dataset tuple with instructions for reading cifar10.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  # if not file_pattern:\n  #   file_pattern = _FILE_PATTERN\n  # file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n  if split_name == \'train\':\n    _LIST = _TRAIN_LIST\n  else:\n    _LIST = _VAL_LIST\n  with open(_LIST, \'r\') as fin:\n    data_sources = [\n      \' \'.join([os.path.join(dataset_dir, el.split()[0]),] + el.split()[1:])\n      for el in fin.read().splitlines()\n    ]\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if not reader:\n    reader = readerFn\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=data_sources,\n      reader=reader,\n      decoder=decoderFn(num_samples),\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
datasets/ucf101.py,0,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n""""""Provides data for the UCF101 dataset.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datasets.video_data_utils import gen_dataset\n\n\ndef get_split(split_name, dataset_dir, dataset_list_dir=\'\', file_pattern=None,\n              reader=None, modality=\'rgb\', num_samples=1,\n              split_id=1):\n  \n  _NUM_CLASSES = 101\n  _LIST_FN = lambda split, id: \\\n      \'%s/%s_split%d.txt\' % (dataset_list_dir, split, id)\n\n  return gen_dataset(split_name, dataset_dir, file_pattern,\n                     reader, modality, num_samples, split_id,\n                     _NUM_CLASSES, _LIST_FN)\n'"
datasets/video_data_utils.py,10,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n""""""Provides data for the UCF101 dataset.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport sys\n\nfrom datasets import dataset_utils\nfrom datasets.image_read_utils import _read_from_disk_spatial, \\\n       _decode_from_string, _read_from_disk_temporal\nfrom tensorflow.python.platform import tf_logging as logging\n\nslim = tf.contrib.slim\n\n\ndef getReaderFn(num_samples, modality=\'rgb\', dataset_dir=\'\'):\n  def readerFn():\n    class reader_func(tf.ReaderBase):\n      @staticmethod\n      def read(filename_queue):\n        value = filename_queue.dequeue()\n        fpath, nframes, label = tf.decode_csv(\n            value, record_defaults=[[\'\'], [-1], [\'\']],\n            field_delim=\' \')\n        # TODO(rgirdhar): Release the file_prefix=\'\', file_zero_padding=4,\n        # file_index=1 options to the bash script\n        if modality == \'rgb\':\n          assert(len(dataset_dir) >= 1)\n          image_buffer = _read_from_disk_spatial(\n              fpath, nframes, num_samples=num_samples,\n              file_prefix=\'image\', file_zero_padding=5, file_index=1,\n              dataset_dir=dataset_dir[0])\n        elif modality.startswith(\'flow\'):\n          assert(len(dataset_dir) >= 1)\n          optical_flow_frames = int(modality[4:])\n          image_buffer = _read_from_disk_temporal(\n              fpath, nframes, num_samples=num_samples,\n              optical_flow_frames=optical_flow_frames,\n              file_prefix=\'flow\', file_zero_padding=5, file_index=1,\n              dataset_dir=dataset_dir[0])\n        elif modality.startswith(\'rgb+flow\'):\n          assert(len(dataset_dir) >= 2)\n          # in this case, fix the step for both the streams to ensure correspondence\n          optical_flow_frames = int(modality[-2:])\n          duration = nframes\n          step = None\n          if num_samples == 1:\n            step = tf.random_uniform([1], 0, nframes-optical_flow_frames-1, dtype=\'int32\')[0]\n          else:\n            step = tf.cast((duration-tf.constant(optical_flow_frames)) /\n                           (tf.constant(num_samples)), \'int32\')\n\n          rgb_image_buffer = _read_from_disk_spatial(\n              fpath, nframes, num_samples=num_samples,\n              file_prefix=\'image\', file_zero_padding=5, file_index=1,\n              dataset_dir=dataset_dir[0],\n              step=step)\n          flow_image_buffer = _read_from_disk_temporal(\n              fpath, nframes, num_samples=num_samples,\n              optical_flow_frames=optical_flow_frames,\n              file_prefix=\'flow\', file_zero_padding=5, file_index=1,\n              dataset_dir=dataset_dir[1],\n              step=step)\n          image_buffer = zip(rgb_image_buffer, flow_image_buffer)\n          image_buffer = [[el[0]] + el[1] for el in image_buffer]\n        else:\n          logging.error(\'Unknown modality %s\\n\' % modality)\n          raise ValueError()\n        return [tf.pack(image_buffer), label]\n    return reader_func\n  return readerFn\n\n\ndef decoderFn(num_samples=1, modality=\'rgb\'):\n  class decoder_func(slim.data_decoder.DataDecoder):\n    @staticmethod\n    def list_items():\n      return [\'image\', \'label\']\n\n    @staticmethod\n    def decode(data, items):\n      with tf.name_scope(\'decode_video\'):\n        if modality == \'rgb\':\n          data.set_shape((num_samples,))\n        elif modality.startswith(\'flow\'):\n          optical_flow_frames = int(modality[4:])\n          data.set_shape((num_samples, 2 * optical_flow_frames))\n        elif modality.startswith(\'rgb+flow\'):\n          optical_flow_frames = int(modality[-2:])\n          data.set_shape((num_samples, 1 + 2 * optical_flow_frames))\n        else:\n          logging.error(\'Unknown modality %s\\n\' % modality)\n        image_buffer = [_decode_from_string(el, modality) for\n                        el in tf.unpack(data)]\n        # image_buffer = tf.pack(image_buffer)\n        return image_buffer\n  return decoder_func\n\n\ndef count_frames_file(fpath, frameLevel=True):\n  res = 0\n  with open(fpath, \'r\') as fin:\n    for line in fin:\n      if frameLevel:\n        res += int(line.split()[1])\n      else:\n        res += 1\n  return res\n\n\ndef gen_dataset(split_name, dataset_dir, file_pattern=None,\n                reader=None, modality=\'rgb\', num_samples=1,\n                split_id=1, num_classes=0, list_fn=None):\n  SPLITS_TO_SIZES = {\n    \'train\': count_frames_file(list_fn(\'train\', split_id), frameLevel=(num_samples==1)),\n    \'test\': count_frames_file(list_fn(\'test\', split_id), frameLevel=(num_samples==1)),\n  }\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  _ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A [? x ? x 3] color image.\',\n    \'label\': \'A single integer between 0 and %d\' % num_classes,\n  }\n  LIST_FILE = list_fn(split_name, split_id)\n  logging.info(\'Using file %s\' % LIST_FILE)\n  with open(LIST_FILE, \'r\') as fin:\n    data_sources = fin.read().splitlines()\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if not reader:\n    reader = getReaderFn(num_samples, modality, dataset_dir)\n\n  labels_to_names = None\n  # if dataset_utils.has_labels(dataset_dir):\n  #   labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=data_sources,\n      reader=reader,\n      decoder=decoderFn(num_samples, modality),\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=num_classes,\n      labels_to_names=labels_to_names)\n'"
demo/get_class.py,0,"b""import h5py\nimport sys\n\n# argv[1] : H5 file path\n# argv[2] : path to actions list\n\nwith h5py.File(sys.argv[1], 'r') as fin, open(sys.argv[2], 'r') as fin2:\n  act_names = fin2.read().splitlines()\n  act = act_names[fin['stream0/logits'].value.argmax()]\n  print('Detected action: {}'.format(act))\n"""
deployment/__init__.py,0,b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n\n'
deployment/model_deploy.py,52,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Deploy Slim models across multiple clones and replicas.\n\n# TODO(sguada) docstring paragraph by (a) motivating the need for the file and\n# (b) defining clones.\n\n# TODO(sguada) describe the high-level components of model deployment.\n# E.g. ""each model deployment is composed of several parts: a DeploymentConfig,\n# which captures A, B and C, an input_fn which loads data.. etc\n\nTo easily train a model on multiple GPUs or across multiple machines this\nmodule provides a set of helper functions: `create_clones`,\n`optimize_clones` and `deploy`.\n\nUsage:\n\n  g = tf.Graph()\n\n  # Set up DeploymentConfig\n  config = model_deploy.DeploymentConfig(num_clones=2, clone_on_cpu=True)\n\n  # Create the global step on the device storing the variables.\n  with tf.device(config.variables_device()):\n    global_step = slim.create_global_step()\n\n  # Define the inputs\n  with tf.device(config.inputs_device()):\n    images, labels = LoadData(...)\n    inputs_queue = slim.data.prefetch_queue((images, labels))\n\n  # Define the optimizer.\n  with tf.device(config.optimizer_device()):\n    optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate, FLAGS.momentum)\n\n  # Define the model including the loss.\n  def model_fn(inputs_queue):\n    images, labels = inputs_queue.dequeue()\n    predictions = CreateNetwork(images)\n    slim.losses.log_loss(predictions, labels)\n\n  model_dp = model_deploy.deploy(config, model_fn, [inputs_queue],\n                                 optimizer=optimizer)\n\n  # Run training.\n  slim.learning.train(model_dp.train_op, my_log_dir,\n                      summary_op=model_dp.summary_op)\n\nThe Clone namedtuple holds together the values associated with each call to\nmodel_fn:\n  * outputs: The return values of the calls to `model_fn()`.\n  * scope: The scope used to create the clone.\n  * device: The device used to create the clone.\n\nDeployedModel namedtuple, holds together the values needed to train multiple\nclones:\n  * train_op: An operation that run the optimizer training op and include\n    all the update ops created by `model_fn`. Present only if an optimizer\n    was specified.\n  * summary_op: An operation that run the summaries created by `model_fn`\n    and process_gradients.\n  * total_loss: A `Tensor` that contains the sum of all losses created by\n    `model_fn` plus the regularization losses.\n  * clones: List of `Clone` tuples returned by `create_clones()`.\n\nDeploymentConfig parameters:\n  * num_clones: Number of model clones to deploy in each replica.\n  * clone_on_cpu: True if clones should be placed on CPU.\n  * replica_id: Integer.  Index of the replica for which the model is\n      deployed.  Usually 0 for the chief replica.\n  * num_replicas: Number of replicas to use.\n  * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n  * worker_job_name: A name for the worker job.\n  * ps_job_name: A name for the parameter server job.\n\nTODO(sguada):\n  - describe side effect to the graph.\n  - what happens to summaries and update_ops.\n  - which graph collections are altered.\n  - write a tutorial on how to use this.\n  - analyze the possibility of calling deploy more than once.\n\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\nslim = tf.contrib.slim\n\n\n__all__ = [\'create_clones\',\n           \'deploy\',\n           \'optimize_clones\',\n           \'DeployedModel\',\n           \'DeploymentConfig\',\n           \'Clone\',\n          ]\n\n\n# Namedtuple used to represent a clone during deployment.\nClone = collections.namedtuple(\'Clone\',\n                               [\'outputs\',  # Whatever model_fn() returned.\n                                \'scope\',  # The scope used to create it.\n                                \'device\',  # The device used to create.\n                               ])\n\n# Namedtuple used to represent a DeployedModel, returned by deploy().\nDeployedModel = collections.namedtuple(\'DeployedModel\',\n                                       [\'train_op\',  # The `train_op`\n                                        \'summary_op\',  # The `summary_op`\n                                        \'total_loss\',  # The loss `Tensor`\n                                        \'clones\',  # A list of `Clones` tuples.\n                                       ])\n\n# Default parameters for DeploymentConfig\n_deployment_params = {\'num_clones\': 1,\n                      \'clone_on_cpu\': False,\n                      \'replica_id\': 0,\n                      \'num_replicas\': 1,\n                      \'num_ps_tasks\': 0,\n                      \'worker_job_name\': \'worker\',\n                      \'ps_job_name\': \'ps\'}\n\n\ndef create_clones(config, model_fn, args=None, kwargs=None):\n  """"""Creates multiple clones according to config using a `model_fn`.\n\n  The returned values of `model_fn(*args, **kwargs)` are collected along with\n  the scope and device used to created it in a namedtuple\n  `Clone(outputs, scope, device)`\n\n  Note: it is assumed that any loss created by `model_fn` is collected at\n  the tf.GraphKeys.LOSSES collection.\n\n  To recover the losses, summaries or update_ops created by the clone use:\n  ```python\n    losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n    summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, clone.scope)\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, clone.scope)\n  ```\n\n  The deployment options are specified by the config object and support\n  deploying one or several clones on different GPUs and one or several replicas\n  of such clones.\n\n  The argument `model_fn` is called `config.num_clones` times to create the\n  model clones as `model_fn(*args, **kwargs)`.\n\n  If `config` specifies deployment on multiple replicas then the default\n  tensorflow device is set appropriatly for each call to `model_fn` and for the\n  slim variable creation functions: model and global variables will be created\n  on the `ps` device, the clone operations will be on the `worker` device.\n\n  Args:\n    config: A DeploymentConfig object.\n    model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n    args: Optional list of arguments to pass to `model_fn`.\n    kwargs: Optional list of keyword arguments to pass to `model_fn`.\n\n  Returns:\n    A list of namedtuples `Clone`.\n  """"""\n  clones = []\n  args = args or []\n  kwargs = kwargs or {}\n  with slim.arg_scope([slim.model_variable, slim.variable],\n                      device=config.variables_device()):\n    # Create clones.\n    for i in range(0, config.num_clones):\n      with tf.name_scope(config.clone_scope(i)) as clone_scope:\n        clone_device = config.clone_device(i)\n        with tf.device(clone_device):\n          with tf.variable_scope(tf.get_variable_scope(),\n                                 reuse=True if i > 0 else None):\n            outputs = model_fn(*args, **kwargs)\n          clones.append(Clone(outputs, clone_scope, clone_device))\n  return clones\n\n\ndef _gather_clone_loss(clone, num_clones, regularization_losses):\n  """"""Gather the loss for a single clone.\n\n  Args:\n    clone: A Clone namedtuple.\n    num_clones: The number of clones being deployed.\n    regularization_losses: Possibly empty list of regularization_losses\n      to add to the clone losses.\n\n  Returns:\n    A tensor for the total loss for the clone.  Can be None.\n  """"""\n  # The return value.\n  sum_loss = None\n  # Individual components of the loss that will need summaries.\n  clone_loss = None\n  regularization_loss = None\n  # Compute and aggregate losses on the clone device.\n  with tf.device(clone.device):\n    all_losses = []\n    clone_losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n    if clone_losses:\n      clone_loss = tf.add_n(clone_losses, name=\'clone_loss\')\n      if num_clones > 1:\n        clone_loss = tf.div(clone_loss, 1.0 * num_clones,\n                            name=\'scaled_clone_loss\')\n      all_losses.append(clone_loss)\n    if regularization_losses:\n      regularization_loss = tf.add_n(regularization_losses,\n                                     name=\'regularization_loss\')\n      all_losses.append(regularization_loss)\n    if all_losses:\n      sum_loss = tf.add_n(all_losses)\n  # Add the summaries out of the clone device block.\n  if clone_loss is not None:\n    tf.scalar_summary(clone.scope + \'/clone_loss\', clone_loss,\n                      name=\'clone_loss\')\n  if regularization_loss is not None:\n    tf.scalar_summary(\'regularization_loss\', regularization_loss,\n                      name=\'regularization_loss\')\n  return sum_loss\n\n\ndef _optimize_clone(optimizer, clone, num_clones, regularization_losses,\n                    **kwargs):\n  """"""Compute losses and gradients for a single clone.\n\n  Args:\n    optimizer: A tf.Optimizer  object.\n    clone: A Clone namedtuple.\n    num_clones: The number of clones being deployed.\n    regularization_losses: Possibly empty list of regularization_losses\n      to add to the clone losses.\n    **kwargs: Dict of kwarg to pass to compute_gradients().\n\n  Returns:\n    A tuple (clone_loss, clone_grads_and_vars).\n      - clone_loss: A tensor for the total loss for the clone.  Can be None.\n      - clone_grads_and_vars: List of (gradient, variable) for the clone.\n        Can be empty.\n  """"""\n  sum_loss = _gather_clone_loss(clone, num_clones, regularization_losses)\n  clone_grad = None\n  if sum_loss is not None:\n    with tf.device(clone.device):\n      clone_grad = optimizer.compute_gradients(sum_loss, **kwargs)\n  return sum_loss, clone_grad\n\n\ndef optimize_clones(clones, optimizer,\n                    regularization_losses=None,\n                    **kwargs):\n  """"""Compute clone losses and gradients for the given list of `Clones`.\n\n  Note: The regularization_losses are added to the first clone losses.\n\n  Args:\n   clones: List of `Clones` created by `create_clones()`.\n   optimizer: An `Optimizer` object.\n   regularization_losses: Optional list of regularization losses. If None it\n     will gather them from tf.GraphKeys.REGULARIZATION_LOSSES. Pass `[]` to\n     exclude them.\n   **kwargs: Optional list of keyword arguments to pass to `compute_gradients`.\n\n  Returns:\n   A tuple (total_loss, grads_and_vars).\n     - total_loss: A Tensor containing the average of the clone losses including\n       the regularization loss.\n     - grads_and_vars: A List of tuples (gradient, variable) containing the sum\n       of the gradients for each variable.\n\n  """"""\n  grads_and_vars = []\n  clones_losses = []\n  num_clones = len(clones)\n  if regularization_losses is None:\n    regularization_losses = tf.get_collection(\n        tf.GraphKeys.REGULARIZATION_LOSSES)\n  for clone in clones:\n    with tf.name_scope(clone.scope):\n      clone_loss, clone_grad = _optimize_clone(\n          optimizer, clone, num_clones,\n          regularization_losses, **kwargs)\n      if clone_loss is not None:\n        clones_losses.append(clone_loss)\n        grads_and_vars.append(clone_grad)\n      # Only use regularization_losses for the first clone\n      regularization_losses = None\n  # Compute the total_loss summing all the clones_losses.\n  total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n  # Sum the gradients accross clones.\n  grads_and_vars = _sum_clones_gradients(grads_and_vars)\n  return total_loss, grads_and_vars\n\n\ndef deploy(config,\n           model_fn,\n           args=None,\n           kwargs=None,\n           optimizer=None,\n           summarize_gradients=False):\n  """"""Deploys a Slim-constructed model across multiple clones.\n\n  The deployment options are specified by the config object and support\n  deploying one or several clones on different GPUs and one or several replicas\n  of such clones.\n\n  The argument `model_fn` is called `config.num_clones` times to create the\n  model clones as `model_fn(*args, **kwargs)`.\n\n  The optional argument `optimizer` is an `Optimizer` object.  If not `None`,\n  the deployed model is configured for training with that optimizer.\n\n  If `config` specifies deployment on multiple replicas then the default\n  tensorflow device is set appropriatly for each call to `model_fn` and for the\n  slim variable creation functions: model and global variables will be created\n  on the `ps` device, the clone operations will be on the `worker` device.\n\n  Args:\n    config: A `DeploymentConfig` object.\n    model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n    args: Optional list of arguments to pass to `model_fn`.\n    kwargs: Optional list of keyword arguments to pass to `model_fn`.\n    optimizer: Optional `Optimizer` object.  If passed the model is deployed\n      for training with that optimizer.\n    summarize_gradients: Whether or not add summaries to the gradients.\n\n  Returns:\n    A `DeployedModel` namedtuple.\n\n  """"""\n  # Gather initial summaries.\n  summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n  # Create Clones.\n  clones = create_clones(config, model_fn, args, kwargs)\n  first_clone = clones[0]\n\n  # Gather update_ops from the first clone. These contain, for example,\n  # the updates for the batch_norm variables created by model_fn.\n  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone.scope)\n\n  train_op = None\n  total_loss = None\n  with tf.device(config.optimizer_device()):\n    if optimizer:\n      # Place the global step on the device storing the variables.\n      with tf.device(config.variables_device()):\n        global_step = slim.get_or_create_global_step()\n\n      # Compute the gradients for the clones.\n      total_loss, clones_gradients = optimize_clones(clones, optimizer)\n\n      if clones_gradients:\n        if summarize_gradients:\n          # Add summaries to the gradients.\n          summaries |= set(_add_gradients_summaries(clones_gradients))\n\n        # Create gradient updates.\n        grad_updates = optimizer.apply_gradients(clones_gradients,\n                                                 global_step=global_step)\n        update_ops.append(grad_updates)\n\n        update_op = tf.group(*update_ops)\n        train_op = control_flow_ops.with_dependencies([update_op], total_loss,\n                                                      name=\'train_op\')\n    else:\n      clones_losses = []\n      regularization_losses = tf.get_collection(\n          tf.GraphKeys.REGULARIZATION_LOSSES)\n      for clone in clones:\n        with tf.name_scope(clone.scope):\n          clone_loss = _gather_clone_loss(clone, len(clones),\n                                          regularization_losses)\n          if clone_loss is not None:\n            clones_losses.append(clone_loss)\n          # Only use regularization_losses for the first clone\n          regularization_losses = None\n      if clones_losses:\n        total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n\n    # Add the summaries from the first clone. These contain the summaries\n    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                       first_clone.scope))\n\n    if total_loss is not None:\n      # Add total_loss to summary.\n      summaries.add(tf.scalar_summary(\'total_loss\', total_loss,\n                                      name=\'total_loss\'))\n\n    if summaries:\n      # Merge all summaries together.\n      summary_op = tf.merge_summary(list(summaries), name=\'summary_op\')\n    else:\n      summary_op = None\n\n  return DeployedModel(train_op, summary_op, total_loss, clones)\n\n\ndef _sum_clones_gradients(clone_grads):\n  """"""Calculate the sum gradient for each shared variable across all clones.\n\n  This function assumes that the clone_grads has been scaled appropriately by\n  1 / num_clones.\n\n  Args:\n    clone_grads: A List of List of tuples (gradient, variable), one list per\n    `Clone`.\n\n  Returns:\n     List of tuples of (gradient, variable) where the gradient has been summed\n     across all clones.\n  """"""\n  sum_grads = []\n  for grad_and_vars in zip(*clone_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))\n    grads = []\n    var = grad_and_vars[0][1]\n    for g, v in grad_and_vars:\n      assert v == var\n      if g is not None:\n        grads.append(g)\n    if grads:\n      if len(grads) > 1:\n        sum_grad = tf.add_n(grads, name=var.op.name + \'/sum_grads\')\n      else:\n        sum_grad = grads[0]\n      sum_grads.append((sum_grad, var))\n  return sum_grads\n\n\ndef _add_gradients_summaries(grads_and_vars):\n  """"""Add histogram summaries to gradients.\n\n  Note: The summaries are also added to the SUMMARIES collection.\n\n  Args:\n    grads_and_vars: A list of gradient to variable pairs (tuples).\n\n  Returns:\n    The _list_ of the added summaries for grads_and_vars.\n  """"""\n  summaries = []\n  for grad, var in grads_and_vars:\n    if grad is not None:\n      if isinstance(grad, tf.IndexedSlices):\n        grad_values = grad.values\n      else:\n        grad_values = grad\n      summaries.append(tf.histogram_summary(var.op.name + \':gradient\',\n                                            grad_values))\n      summaries.append(tf.histogram_summary(var.op.name + \':gradient_norm\',\n                                            tf.global_norm([grad_values])))\n    else:\n      tf.logging.info(\'Var %s has no gradient\', var.op.name)\n  return summaries\n\n\nclass DeploymentConfig(object):\n  """"""Configuration for deploying a model with `deploy()`.\n\n  You can pass an instance of this class to `deploy()` to specify exactly\n  how to deploy the model to build.  If you do not pass one, an instance built\n  from the default deployment_hparams will be used.\n  """"""\n\n  def __init__(self,\n               num_clones=1,\n               clone_on_cpu=False,\n               replica_id=0,\n               num_replicas=1,\n               num_ps_tasks=0,\n               worker_job_name=\'worker\',\n               ps_job_name=\'ps\'):\n    """"""Create a DeploymentConfig.\n\n    The config describes how to deploy a model across multiple clones and\n    replicas.  The model will be replicated `num_clones` times in each replica.\n    If `clone_on_cpu` is True, each clone will placed on CPU.\n\n    If `num_replicas` is 1, the model is deployed via a single process.  In that\n    case `worker_device`, `num_ps_tasks`, and `ps_device` are ignored.\n\n    If `num_replicas` is greater than 1, then `worker_device` and `ps_device`\n    must specify TensorFlow devices for the `worker` and `ps` jobs and\n    `num_ps_tasks` must be positive.\n\n    Args:\n      num_clones: Number of model clones to deploy in each replica.\n      clone_on_cpu: If True clones would be placed on CPU.\n      replica_id: Integer.  Index of the replica for which the model is\n        deployed.  Usually 0 for the chief replica.\n      num_replicas: Number of replicas to use.\n      num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n      worker_job_name: A name for the worker job.\n      ps_job_name: A name for the parameter server job.\n\n    Raises:\n      ValueError: If the arguments are invalid.\n    """"""\n    if num_replicas > 1:\n      if num_ps_tasks < 1:\n        raise ValueError(\'When using replicas num_ps_tasks must be positive\')\n    if num_replicas > 1 or num_ps_tasks > 0:\n      if not worker_job_name:\n        raise ValueError(\'Must specify worker_job_name when using replicas\')\n      if not ps_job_name:\n        raise ValueError(\'Must specify ps_job_name when using parameter server\')\n    if replica_id >= num_replicas:\n      raise ValueError(\'replica_id must be less than num_replicas\')\n    self._num_clones = num_clones\n    self._clone_on_cpu = clone_on_cpu\n    self._replica_id = replica_id\n    self._num_replicas = num_replicas\n    self._num_ps_tasks = num_ps_tasks\n    self._ps_device = \'/job:\' + ps_job_name if num_ps_tasks > 0 else \'\'\n    self._worker_device = \'/job:\' + worker_job_name if num_ps_tasks > 0 else \'\'\n\n  @property\n  def num_clones(self):\n    return self._num_clones\n\n  @property\n  def clone_on_cpu(self):\n    return self._clone_on_cpu\n\n  @property\n  def replica_id(self):\n    return self._replica_id\n\n  @property\n  def num_replicas(self):\n    return self._num_replicas\n\n  @property\n  def num_ps_tasks(self):\n    return self._num_ps_tasks\n\n  @property\n  def ps_device(self):\n    return self._ps_device\n\n  @property\n  def worker_device(self):\n    return self._worker_device\n\n  def caching_device(self):\n    """"""Returns the device to use for caching variables.\n\n    Variables are cached on the worker CPU when using replicas.\n\n    Returns:\n      A device string or None if the variables do not need to be cached.\n    """"""\n    if self._num_ps_tasks > 0:\n      return lambda op: op.device\n    else:\n      return None\n\n  def clone_device(self, clone_index):\n    """"""Device used to create the clone and all the ops inside the clone.\n\n    Args:\n      clone_index: Int, representing the clone_index.\n\n    Returns:\n      A value suitable for `tf.device()`.\n\n    Raises:\n      ValueError: if `clone_index` is greater or equal to the number of clones"".\n    """"""\n    if clone_index >= self._num_clones:\n      raise ValueError(\'clone_index must be less than num_clones\')\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._worker_device\n    if self._clone_on_cpu:\n      device += \'/device:CPU:0\'\n    else:\n      if self._num_clones > 1:\n        device += \'/device:GPU:%d\' % clone_index\n    return device\n\n  def clone_scope(self, clone_index):\n    """"""Name scope to create the clone.\n\n    Args:\n      clone_index: Int, representing the clone_index.\n\n    Returns:\n      A name_scope suitable for `tf.name_scope()`.\n\n    Raises:\n      ValueError: if `clone_index` is greater or equal to the number of clones"".\n    """"""\n    if clone_index >= self._num_clones:\n      raise ValueError(\'clone_index must be less than num_clones\')\n    scope = \'\'\n    if self._num_clones > 1:\n      scope = \'clone_%d\' % clone_index\n    return scope\n\n  def optimizer_device(self):\n    """"""Device to use with the optimizer.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    if self._num_ps_tasks > 0 or self._num_clones > 0:\n      return self._worker_device + \'/device:CPU:0\'\n    else:\n      return \'\'\n\n  def inputs_device(self):\n    """"""Device to use to build the inputs.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._worker_device\n    device += \'/device:CPU:0\'\n    # rgirdhar: changing to gpu as per xinlei suggestion\n    # but then there is no fifo queue implementation for GPU\n    # device += \'/device:GPU:0\'\n    return device\n\n  def variables_device(self):\n    """"""Returns the device to use for variables created inside the clone.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._ps_device\n    device += \'/device:CPU:0\'\n\n    class _PSDeviceChooser(object):\n      """"""Slim device chooser for variables when using PS.""""""\n\n      def __init__(self, device, tasks):\n        self._device = device\n        self._tasks = tasks\n        self._task = 0\n\n      def choose(self, op):\n        if op.device:\n          return op.device\n        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n        if node_def.op == \'Variable\':\n          t = self._task\n          self._task = (self._task + 1) % self._tasks\n          d = \'%s/task:%d\' % (self._device, t)\n          return d\n        else:\n          return op.device\n\n    if not self._num_ps_tasks:\n      return device\n    else:\n      chooser = _PSDeviceChooser(device, self._num_ps_tasks)\n      return chooser.choose\n'"
deployment/model_deploy_test.py,96,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for model_deploy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom deployment import model_deploy\n\nslim = tf.contrib.slim\n\n\nclass DeploymentConfigTest(tf.test.TestCase):\n\n  def testDefaults(self):\n    deploy_config = model_deploy.DeploymentConfig()\n\n    self.assertEqual(slim.get_variables(), [])\n    self.assertEqual(deploy_config.caching_device(), None)\n    self.assertDeviceEqual(deploy_config.clone_device(0), \'\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.variables_device(), \'CPU:0\')\n\n  def testCPUonly(self):\n    deploy_config = model_deploy.DeploymentConfig(clone_on_cpu=True)\n\n    self.assertEqual(deploy_config.caching_device(), None)\n    self.assertDeviceEqual(deploy_config.clone_device(0), \'CPU:0\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.variables_device(), \'CPU:0\')\n\n  def testMultiGPU(self):\n    deploy_config = model_deploy.DeploymentConfig(num_clones=2)\n\n    self.assertEqual(deploy_config.caching_device(), None)\n    self.assertDeviceEqual(deploy_config.clone_device(0), \'GPU:0\')\n    self.assertDeviceEqual(deploy_config.clone_device(1), \'GPU:1\')\n    self.assertEqual(deploy_config.clone_scope(0), \'clone_0\')\n    self.assertEqual(deploy_config.clone_scope(1), \'clone_1\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.variables_device(), \'CPU:0\')\n\n  def testPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_clones=1, num_ps_tasks=1)\n\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n    with tf.device(deploy_config.variables_device()):\n      a = tf.Variable(0)\n      b = tf.Variable(0)\n      c = tf.no_op()\n      d = slim.variable(\'a\', [],\n                        caching_device=deploy_config.caching_device())\n    self.assertDeviceEqual(a.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(a.device, a.value().device)\n    self.assertDeviceEqual(b.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(b.device, b.value().device)\n    self.assertDeviceEqual(c.device, \'\')\n    self.assertDeviceEqual(d.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(d.value().device, \'\')\n\n  def testMultiGPUPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_clones=2, num_ps_tasks=1)\n\n    self.assertEqual(deploy_config.caching_device()(tf.no_op()), \'\')\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker/device:GPU:0\')\n    self.assertDeviceEqual(deploy_config.clone_device(1),\n                           \'/job:worker/device:GPU:1\')\n    self.assertEqual(deploy_config.clone_scope(0), \'clone_0\')\n    self.assertEqual(deploy_config.clone_scope(1), \'clone_1\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n\n  def testReplicasPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_replicas=2,\n                                                  num_ps_tasks=2)\n\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n\n  def testReplicasMultiGPUPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_replicas=2,\n                                                  num_clones=2,\n                                                  num_ps_tasks=2)\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker/device:GPU:0\')\n    self.assertDeviceEqual(deploy_config.clone_device(1),\n                           \'/job:worker/device:GPU:1\')\n    self.assertEqual(deploy_config.clone_scope(0), \'clone_0\')\n    self.assertEqual(deploy_config.clone_scope(1), \'clone_1\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n\n  def testVariablesPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_ps_tasks=2)\n\n    with tf.device(deploy_config.variables_device()):\n      a = tf.Variable(0)\n      b = tf.Variable(0)\n      c = tf.no_op()\n      d = slim.variable(\'a\', [],\n                        caching_device=deploy_config.caching_device())\n\n    self.assertDeviceEqual(a.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(a.device, a.value().device)\n    self.assertDeviceEqual(b.device, \'/job:ps/task:1/device:CPU:0\')\n    self.assertDeviceEqual(b.device, b.value().device)\n    self.assertDeviceEqual(c.device, \'\')\n    self.assertDeviceEqual(d.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(d.value().device, \'\')\n\n\ndef LogisticClassifier(inputs, labels, scope=None, reuse=None):\n  with tf.variable_scope(scope, \'LogisticClassifier\', [inputs, labels],\n                         reuse=reuse):\n    predictions = slim.fully_connected(inputs, 1, activation_fn=tf.sigmoid,\n                                       scope=\'fully_connected\')\n    slim.losses.log_loss(predictions, labels)\n    return predictions\n\n\ndef BatchNormClassifier(inputs, labels, scope=None, reuse=None):\n  with tf.variable_scope(scope, \'BatchNormClassifier\', [inputs, labels],\n                         reuse=reuse):\n    inputs = slim.batch_norm(inputs, decay=0.1)\n    predictions = slim.fully_connected(inputs, 1,\n                                       activation_fn=tf.sigmoid,\n                                       scope=\'fully_connected\')\n    slim.losses.log_loss(predictions, labels)\n    return predictions\n\n\nclass CreatecloneTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create an easy training set:\n    np.random.seed(0)\n\n    self._inputs = np.zeros((16, 4))\n    self._labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32)\n    self._logdir = self.get_temp_dir()\n\n    for i in range(16):\n      j = int(2 * self._labels[i] + np.random.randint(0, 2))\n      self._inputs[i, j] = 1\n\n  def testCreateLogisticClassifier(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = LogisticClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      clone = clones[0]\n      self.assertEqual(len(slim.get_variables()), 2)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n        self.assertDeviceEqual(v.value().device, \'CPU:0\')\n      self.assertEqual(clone.outputs.op.name,\n                       \'LogisticClassifier/fully_connected/Sigmoid\')\n      self.assertEqual(clone.scope, \'\')\n      self.assertDeviceEqual(clone.device, \'\')\n      self.assertEqual(len(slim.losses.get_losses()), 1)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(update_ops, [])\n\n  def testCreateSingleclone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      clone = clones[0]\n      self.assertEqual(len(slim.get_variables()), 5)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n        self.assertDeviceEqual(v.value().device, \'CPU:0\')\n      self.assertEqual(clone.outputs.op.name,\n                       \'BatchNormClassifier/fully_connected/Sigmoid\')\n      self.assertEqual(clone.scope, \'\')\n      self.assertDeviceEqual(clone.device, \'\')\n      self.assertEqual(len(slim.losses.get_losses()), 1)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 2)\n\n  def testCreateMulticlone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      num_clones = 4\n      deploy_config = model_deploy.DeploymentConfig(num_clones=num_clones)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n        self.assertDeviceEqual(v.value().device, \'CPU:0\')\n      self.assertEqual(len(clones), num_clones)\n      for i, clone in enumerate(clones):\n        self.assertEqual(\n            clone.outputs.op.name,\n            \'clone_%d/BatchNormClassifier/fully_connected/Sigmoid\' % i)\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, clone.scope)\n        self.assertEqual(len(update_ops), 2)\n        self.assertEqual(clone.scope, \'clone_%d/\' % i)\n        self.assertDeviceEqual(clone.device, \'GPU:%d\' % i)\n\n  def testCreateOnecloneWithPS(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1,\n                                                    num_ps_tasks=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(clones), 1)\n      clone = clones[0]\n      self.assertEqual(clone.outputs.op.name,\n                       \'BatchNormClassifier/fully_connected/Sigmoid\')\n      self.assertDeviceEqual(clone.device, \'/job:worker\')\n      self.assertEqual(clone.scope, \'\')\n      self.assertEqual(len(slim.get_variables()), 5)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'/job:ps/task:0/CPU:0\')\n        self.assertDeviceEqual(v.device, v.value().device)\n\n  def testCreateMulticloneWithPS(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2,\n                                                    num_ps_tasks=2)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      for i, v in enumerate(slim.get_variables()):\n        t = i % 2\n        self.assertDeviceEqual(v.device, \'/job:ps/task:%d/device:CPU:0\' % t)\n        self.assertDeviceEqual(v.device, v.value().device)\n      self.assertEqual(len(clones), 2)\n      for i, clone in enumerate(clones):\n        self.assertEqual(\n            clone.outputs.op.name,\n            \'clone_%d/BatchNormClassifier/fully_connected/Sigmoid\' % i)\n        self.assertEqual(clone.scope, \'clone_%d/\' % i)\n        self.assertDeviceEqual(clone.device, \'/job:worker/device:GPU:%d\' % i)\n\n\nclass OptimizeclonesTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create an easy training set:\n    np.random.seed(0)\n\n    self._inputs = np.zeros((16, 4))\n    self._labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32)\n    self._logdir = self.get_temp_dir()\n\n    for i in range(16):\n      j = int(2 * self._labels[i] + np.random.randint(0, 2))\n      self._inputs[i, j] = 1\n\n  def testCreateLogisticClassifier(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = LogisticClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 2)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(update_ops, [])\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateSingleclone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateMulticlone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      num_clones = 4\n      deploy_config = model_deploy.DeploymentConfig(num_clones=num_clones)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), num_clones * 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateMulticloneCPU(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      model_args = (tf_inputs, tf_labels)\n      num_clones = 4\n      deploy_config = model_deploy.DeploymentConfig(num_clones=num_clones,\n                                                    clone_on_cpu=True)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, model_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), num_clones * 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateOnecloneWithPS(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      model_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1,\n                                                    num_ps_tasks=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, model_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'/job:worker\')\n        self.assertDeviceEqual(v.device, \'/job:ps/task:0/CPU:0\')\n\n\nclass DeployTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create an easy training set:\n    np.random.seed(0)\n\n    self._inputs = np.zeros((16, 4))\n    self._labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32)\n    self._logdir = self.get_temp_dir()\n\n    for i in range(16):\n      j = int(2 * self._labels[i] + np.random.randint(0, 2))\n      self._inputs[i, j] = 1\n\n  def testLocalTrainOp(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      model_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2,\n                                                    clone_on_cpu=True)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n\n      self.assertEqual(slim.get_variables(), [])\n      model = model_deploy.deploy(deploy_config, model_fn, model_args,\n                                  optimizer=optimizer)\n\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 4)\n      self.assertEqual(len(model.clones), 2)\n      self.assertEqual(model.total_loss.op.name, \'total_loss\')\n      self.assertEqual(model.summary_op.op.name, \'summary_op/summary_op\')\n      self.assertEqual(model.train_op.op.name, \'train_op\')\n\n      with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n        moving_mean = tf.contrib.framework.get_variables_by_name(\n            \'moving_mean\')[0]\n        moving_variance = tf.contrib.framework.get_variables_by_name(\n            \'moving_variance\')[0]\n        initial_loss = sess.run(model.total_loss)\n        initial_mean, initial_variance = sess.run([moving_mean,\n                                                   moving_variance])\n        self.assertAllClose(initial_mean, [0.0, 0.0, 0.0, 0.0])\n        self.assertAllClose(initial_variance, [1.0, 1.0, 1.0, 1.0])\n        for _ in range(10):\n          sess.run(model.train_op)\n        final_loss = sess.run(model.total_loss)\n        self.assertLess(final_loss, initial_loss / 10.0)\n\n        final_mean, final_variance = sess.run([moving_mean,\n                                               moving_variance])\n        self.assertAllClose(final_mean, [0.125, 0.25, 0.375, 0.25])\n        self.assertAllClose(final_variance, [0.109375, 0.1875,\n                                             0.234375, 0.1875])\n\n  def testNoSummariesOnGPU(self):\n    with tf.Graph().as_default():\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2)\n\n      # clone function creates a fully_connected layer with a regularizer loss.\n      def ModelFn():\n        inputs = tf.constant(1.0, shape=(10, 20), dtype=tf.float32)\n        reg = tf.contrib.layers.l2_regularizer(0.001)\n        tf.contrib.layers.fully_connected(inputs, 30, weights_regularizer=reg)\n\n      model = model_deploy.deploy(\n          deploy_config, ModelFn,\n          optimizer=tf.train.GradientDescentOptimizer(1.0))\n      # The model summary op should have a few summary inputs and all of them\n      # should be on the CPU.\n      self.assertTrue(model.summary_op.op.inputs)\n      for inp in  model.summary_op.op.inputs:\n        self.assertEqual(\'/device:CPU:0\', inp.device)\n\n  def testNoSummariesOnGPUForEvals(self):\n    with tf.Graph().as_default():\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2)\n\n      # clone function creates a fully_connected layer with a regularizer loss.\n      def ModelFn():\n        inputs = tf.constant(1.0, shape=(10, 20), dtype=tf.float32)\n        reg = tf.contrib.layers.l2_regularizer(0.001)\n        tf.contrib.layers.fully_connected(inputs, 30, weights_regularizer=reg)\n\n      # No optimizer here, it\'s an eval.\n      model = model_deploy.deploy(deploy_config, ModelFn)\n      # The model summary op should have a few summary inputs and all of them\n      # should be on the CPU.\n      self.assertTrue(model.summary_op.op.inputs)\n      for inp in  model.summary_op.op.inputs:\n        self.assertEqual(\'/device:CPU:0\', inp.device)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
eval/charades_convert_scores_format.py,0,"b'import h5py\nimport argparse\nimport numpy as np\nimport sklearn.preprocessing\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--scores\', help=""H5 file with the scores"")\nparser.add_argument(\'--test\', help=""file with test file ids"")\nparser.add_argument(\'--idt\', default=None, help=""IDT scores file, shared by Gunnar"")\nparser.add_argument(\'--idt_wt\', default=0.5, type=float, help=""Weight to use for iDT."")\nparser.add_argument(\'--outfpath\', help=""output file"")\nargs = parser.parse_args()\n\nwith open(args.test, \'r\') as fin:\n  vid_ids = [line.split()[0][2:-4] for line in fin.read().splitlines()]\nwith h5py.File(args.scores) as fin:\n  scores = fin[\'stream0/logits\'].value\nif args.idt is not None and len(args.idt) > 0:\n  with open(args.idt, \'r\') as fin:\n    print(\'Combining with iDT scores\')\n    lines = fin.read().splitlines()\n    idt_vid_ids = [line.split()[0] for line in lines]\n    idt_scores = np.array([[float(el) for el in line.split()[1:]] for line in\n                           lines])\n    # add 0 scores for videos for which there is no iDT score\n    missing_vids = [el for el in vid_ids if el not in idt_vid_ids]\n    print(\'%d missing videos scores in iDT. Using 0s for those.\' % len(missing_vids))\n    idt_scores = np.vstack((idt_scores, np.zeros((len(missing_vids),\n                                                  idt_scores.shape[1]))))\n    idt_vid_ids = idt_vid_ids + missing_vids\n    order = [idt_vid_ids.index(el) for el in vid_ids]\n    idt_scores = idt_scores[np.array(order), :]\n    scores = args.idt_wt * sklearn.preprocessing.normalize(idt_scores) + \\\n             (1 - args.idt_wt) * sklearn.preprocessing.normalize(scores)\n\nwith open(args.outfpath, \'w\') as fout:\n  for i in range(len(vid_ids)):\n    fout.write(\'%s %s\\n\' % (vid_ids[i], \' \'.join([str(el) for el in scores[i, :]])))\n'"
nets/__init__.py,0,b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n\n'
nets/frame_pooling.py,37,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\nimport numpy as np\nimport cPickle as pickle\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom tensorflow.python.platform import tf_logging as logging\n\nFLAGS = tf.app.flags.FLAGS\n# NetVLAD Parameters\ntf.app.flags.DEFINE_float(\'netvlad_alpha\', 1000.0,\n                          """"""Alpha to use for netVLAD."""""")\n\n\ndef softmax(target, axis, name=None):\n    with tf.name_scope(name, \'softmax\', [target]):\n        max_axis = tf.reduce_max(target, axis, keep_dims=True)\n        target_exp = tf.exp(target-max_axis)\n        normalize = tf.reduce_sum(target_exp, axis, keep_dims=True)\n        softmax = target_exp / normalize\n        return softmax\n\n\ndef netvlad(net, videos_per_batch, weight_decay, netvlad_initCenters):\n    end_points = {}\n    # VLAD pooling\n    try:\n      netvlad_initCenters = int(netvlad_initCenters)\n      # initialize the cluster centers randomly\n      cluster_centers = np.random.normal(size=(\n        netvlad_initCenters, net.get_shape().as_list()[-1]))\n      logging.info(\'Randomly initializing the {} netvlad cluster \'\n                   \'centers\'.format(cluster_centers.shape))\n    except ValueError:\n      with open(netvlad_initCenters, \'rb\') as fin:\n        kmeans = pickle.load(fin)\n        cluster_centers = kmeans.cluster_centers_\n    with tf.variable_scope(\'NetVLAD\'):\n        # normalize features\n        net_normed = tf.nn.l2_normalize(net, 3, name=\'FeatureNorm\')\n        end_points[tf.get_variable_scope().name + \'/net_normed\'] = net_normed\n        vlad_centers = slim.model_variable(\n            \'centers\',\n            shape=cluster_centers.shape,\n            initializer=tf.constant_initializer(cluster_centers),\n            regularizer=slim.l2_regularizer(weight_decay))\n        end_points[tf.get_variable_scope().name + \'/vlad_centers\'] = vlad_centers\n        vlad_W = slim.model_variable(\n            \'vlad_W\',\n            shape=(1, 1, ) + cluster_centers.transpose().shape,\n            initializer=tf.constant_initializer(\n                cluster_centers.transpose()[np.newaxis, np.newaxis, ...] *\n                2 * FLAGS.netvlad_alpha),\n            regularizer=slim.l2_regularizer(weight_decay))\n        end_points[tf.get_variable_scope().name + \'/vlad_W\'] = vlad_W\n        vlad_B = slim.model_variable(\n            \'vlad_B\',\n            shape=cluster_centers.shape[0],\n            initializer=tf.constant_initializer(\n                -FLAGS.netvlad_alpha *\n                np.sum(np.square(cluster_centers), axis=1)),\n            regularizer=slim.l2_regularizer(weight_decay))\n        end_points[tf.get_variable_scope().name + \'/vlad_B\'] = vlad_B\n        conv_output = tf.nn.conv2d(net_normed, vlad_W, [1, 1, 1, 1], \'VALID\')\n        dists = tf.nn.bias_add(conv_output, vlad_B)\n        assgn = softmax(dists, axis=3)\n        end_points[tf.get_variable_scope().name + \'/assgn\'] = assgn\n\n        vid_splits = tf.split(0, videos_per_batch, net_normed)\n        assgn_splits = tf.split(0, videos_per_batch, assgn)\n        num_vlad_centers = vlad_centers.get_shape()[0]\n        vlad_centers_split = tf.split(0, num_vlad_centers, vlad_centers)\n        final_vlad = []\n        for feats, assgn in zip(vid_splits, assgn_splits):\n            vlad_vectors = []\n            assgn_split_byCluster = tf.split(3, num_vlad_centers, assgn)\n            for k in range(num_vlad_centers):\n                res = tf.reduce_sum(\n                    tf.mul(tf.sub(\n                    feats,\n                    vlad_centers_split[k]), assgn_split_byCluster[k]),\n                    [0, 1, 2])\n                vlad_vectors.append(res)\n            vlad_vectors_frame = tf.pack(vlad_vectors, axis=0)\n            final_vlad.append(vlad_vectors_frame)\n        vlad_rep = tf.pack(final_vlad, axis=0, name=\'unnormed-vlad\')\n        end_points[tf.get_variable_scope().name + \'/unnormed_vlad\'] = vlad_rep\n        with tf.name_scope(\'intranorm\'):\n            intranormed = tf.nn.l2_normalize(vlad_rep, dim=2)\n        end_points[tf.get_variable_scope().name + \'/intranormed_vlad\'] = intranormed\n        with tf.name_scope(\'finalnorm\'):\n            vlad_rep = tf.nn.l2_normalize(tf.reshape(\n                intranormed,\n                [intranormed.get_shape().as_list()[0], -1]),\n                dim=1)\n    return vlad_rep, end_points\n\n\ndef pool_conv(net, videos_per_batch, type=\'avg\'):\n    """"""\n    Pool all the features across the frame and across all the frames\n    for the video to get a single representation.\n    Useful as a way to debug NetVLAD, as this should be worse than \n    NetVLAD with k = 1.\n    """"""\n    if type == \'avg\':\n      method = tf.reduce_mean\n    elif type == \'max\':\n      method = tf.reduce_max\n    else:\n      raise ValueError(\'Not Found\')\n    with tf.name_scope(\'%s-conv\' % type):\n        vid_splits = tf.split(0, videos_per_batch, net);\n        vids_pooled = [method(vid, [0, 1, 2]) for vid in vid_splits]\n        return tf.pack(vids_pooled, axis=0)\n'"
nets/inception.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Brings all inception models under one namespace.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint: disable=unused-import\nfrom nets.inception_v2_tsn import inception_v2_tsn\nfrom nets.inception_v2_tsn import inception_v2_tsn_arg_scope\nfrom nets.inception_v2_tsn import inception_v2_tsn_base\n# pylint: enable=unused-import\n'"
nets/inception_v2_tsn.py,57,"b'""""""Contains the definition for inception v2 (TSN) classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.platform import tf_logging as logging\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\nrandom_normal = lambda stddev: tf.random_normal_initializer(0.0, stddev)\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n  \'inception_concat_layers\', \'\', \'List of layers to combine with logits.\')\n\ndef conv_set(net, num_outputs, filter_size, stride=1, weight_std=0.001,\n             padding=0):\n  if padding > 0:\n    net = tf.pad(net, [[0, 0], [padding, padding], [padding, padding], [0, 0]])\n  net = slim.conv2d(\n    net, num_outputs, filter_size,\n    stride=stride,\n    padding=\'VALID\')\n  net = slim.batch_norm(net,\n                        updates_collections=tf.GraphKeys.UPDATE_OPS,\n                        epsilon=1e-5,\n                        decay=0.9,\n                        scale=True)\n  net = tf.nn.relu(net)\n  return net\n\n\ndef pool(net, pool_type=\'avg\', kernel=3, stride=1, padding=0):\n  if pool_type == \'avg\':\n    fn = slim.avg_pool2d\n  elif pool_type == \'max\':\n    fn = slim.max_pool2d\n  else:\n    raise ValueError(\'Unknown pool type\')\n  with tf.name_scope(\'%s_pool\' % pool_type):\n    net = fn(net, [kernel, kernel], stride=stride,\n             padding=\'VALID\' if padding==0 else \'SAME\')\n  return net\n\n\ndef inception_module(net, small_module=False,\n                     num_outputs=[64,64,64,32,64,96,96],\n                     force_max_pool=False):\n  all_nets = []\n  if not small_module:\n    with tf.variable_scope(\'1x1\'):\n      net_1 = conv_set(net, num_outputs[0], [1, 1])\n    all_nets.append(net_1)\n\n  with tf.variable_scope(\'3x3_reduce\'):\n    net_2 = conv_set(net, num_outputs[1], [1, 1])\n  with tf.variable_scope(\'3x3\'):\n    net_2 = conv_set(net_2, num_outputs[2], [3, 3],\n                     padding=1,\n                     stride=2 if small_module else 1)\n  all_nets.append(net_2)\n\n  with tf.variable_scope(\'double_3x3_reduce\'):\n    net_3 = conv_set(net, num_outputs[4], [1, 1])\n  with tf.variable_scope(\'double_3x3_1\'):\n    net_3 = conv_set(net_3, num_outputs[5], [3, 3], padding=1)\n  with tf.variable_scope(\'double_3x3_2\'):\n    net_3 = conv_set(net_3, num_outputs[6], [3, 3], padding=1,\n                     stride=2 if small_module else 1)\n  all_nets.append(net_3)\n\n  with tf.variable_scope(\'pool\'):\n    if small_module:\n      net_4 = pool(net, \'max\', 3, 2, 1)\n    elif force_max_pool:\n      net_4 = pool(net, \'max\', 3, 1, 1)\n    else:\n      net_4 = pool(net, \'avg\', 3, 1, 1)\n  if not small_module:\n    with tf.variable_scope(\'pool_proj\'):\n      net_4 = conv_set(net_4, num_outputs[3], [1, 1])\n  all_nets.append(net_4)\n\n  net = tf.concat(3, all_nets)\n  return net\n\n\ndef inception_v2_tsn_base(inputs,\n                          final_endpoint=\'Mixed_5c\',\n                          min_depth=16,\n                          depth_multiplier=1.0,\n                          scope=None,\n                          is_training=False):\n  """"""Inception v2 (TSN code).\n\n  """"""\n\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  with tf.variable_scope(scope, \'InceptionV2_TSN\', [inputs]):\n      # 224 x 224 x 3\n      end_point = \'conv1/7x7_s2\'\n      with tf.variable_scope(end_point):\n        with slim.arg_scope([slim.batch_norm],\n                            is_training=False,\n                            trainable=False):\n          net = conv_set(inputs, 64, [7, 7],\n                         stride=2,\n                         padding=3)\n      end_points[tf.get_variable_scope().name + \'/\' + end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 112 x 112 x 64\n      end_point = \'pool1/3x3_s2\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point,\n                            stride=2, padding=\'SAME\')\n      # net = pool(net, \'max\', 3, 2, 1)\n      end_points[tf.get_variable_scope().name + \'/\' + end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'conv2/3x3_reduce\'\n      with tf.variable_scope(end_point):\n        net = conv_set(net, 64, [1, 1], weight_std=0.1,\n                       padding=0)\n      # net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2,\n      #                       padding=\'SAME\')\n      end_points[tf.get_variable_scope().name + \'/\' + end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      end_point = \'conv2/3x3\'\n      with tf.variable_scope(end_point):\n        net = conv_set(net, 192, [3, 3], weight_std=0.1, padding=1)\n      end_points[tf.get_variable_scope().name + \'/\' + end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      end_point = \'pool2/3x3_s2\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2,\n                            padding=\'SAME\')\n      # net = pool(net, \'max\', 3, 2, 1)\n      end_points[tf.get_variable_scope().name + \'/\' + end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # Inception module.\n      end_point = \'inception_3a\'\n      with tf.variable_scope(end_point):\n        net = inception_module(net)\n      end_points[tf.get_variable_scope().name + \'/\' + end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      end_point = \'inception_3b\'\n      with tf.variable_scope(end_point):\n        net = inception_module(net, num_outputs=[64,64,96,64,64,96,96])\n      end_points[tf.get_variable_scope().name + \'/\' + end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      end_point = \'inception_3c\'\n      with tf.variable_scope(end_point):\n        net = inception_module(net, small_module=True,\n                               num_outputs=[-1,128,160,-1,64,96,96])\n      end_points[tf.get_variable_scope().name + \'/\' + end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      end_point = \'inception_4a\'\n      with tf.variable_scope(end_point):\n        net = inception_module(net, num_outputs=[224,64,96,128,96,128,128])\n      end_points[tf.get_variable_scope().name + \'/\' + end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      end_point = \'inception_4b\'\n      with tf.variable_scope(end_point):\n        net = inception_module(net, num_outputs=[192,96,128,128,96,128,128])\n      end_points[tf.get_variable_scope().name + \'/\' + end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      end_point = \'inception_4c\'\n      with tf.variable_scope(end_point):\n        net = inception_module(net, num_outputs=[160,128,160,128,128,160,160])\n      end_points[tf.get_variable_scope().name + \'/\' + end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      end_point = \'inception_4d\'\n      with tf.variable_scope(end_point):\n        net = inception_module(net, num_outputs=[96,128,192,128,160,192,192])\n      end_points[tf.get_variable_scope().name + \'/\' + end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      end_point = \'inception_4e\'\n      with tf.variable_scope(end_point):\n        net = inception_module(net, small_module=True,\n                               num_outputs=[-1,128,192,-1,192,256,256])\n      end_points[tf.get_variable_scope().name + \'/\' + end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      end_point = \'inception_5a\'\n      with tf.variable_scope(end_point):\n        net = inception_module(net, num_outputs=[352,192,320,128,160,224,224])\n      end_points[tf.get_variable_scope().name + \'/\' + end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      end_point = \'inception_5b\'\n      with tf.variable_scope(end_point):\n        net = inception_module(net, num_outputs=[352,192,320,128,192,224,224],\n                              force_max_pool=True)\n      end_points[tf.get_variable_scope().name + \'/\' + end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n  return net, end_points\n\n\ndef inception_v2_tsn(inputs,\n                     num_classes=1000,\n                     is_training=True,\n                     dropout_keep_prob=0.2,\n                     min_depth=16,\n                     depth_multiplier=1.0,\n                     prediction_fn=slim.softmax,\n                     spatial_squeeze=True,\n                     reuse=None,\n                     conv_only=None,\n                     # conv_endpoint=\'inception_5b\',\n                     conv_endpoint=\'inception_5a\',  # testing for now\n                     scope=\'InceptionV2_TSN\'):\n  """"""Inception v2 model for video classification.\n\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV2_TSN\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.dropout],\n                        is_training=is_training):\n      with slim.arg_scope([slim.batch_norm],\n                          is_training=False,\n                          trainable=False):\n        net, end_points = inception_v2_tsn_base(\n            inputs, scope=scope, min_depth=min_depth,\n            depth_multiplier=depth_multiplier,\n            final_endpoint=conv_endpoint if conv_only else None,\n            is_training=is_training)\n        if conv_only:\n          return net, end_points\n        with tf.variable_scope(\'Logits\'):\n          kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n          net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\', stride=1,\n                                scope=\'AvgPool_Logits_{}x{}\'.format(*kernel_size))\n          if FLAGS.inception_concat_layers:\n            with tf.variable_scope(\'SkipConn\'):\n              concat_end_points = [net]\n              for lname in FLAGS.inception_concat_layers.split(\',\'):\n                ept = end_points[lname]\n                concat_end_points.append(\n                  slim.avg_pool2d(\n                    ept,\n                    _reduced_kernel_size_for_small_input(ept, [14, 14]),\n                    padding=\'VALID\', stride=1))\n              net = tf.concat(-1, concat_end_points)\n          # 1 x 1 x 1024\n          logging.info(\'Using dropout %f\' % (1-dropout_keep_prob))\n          net = slim.dropout(net, keep_prob=dropout_keep_prob,\n                             scope=\'Dropout_Logits\')\n          logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                               normalizer_fn=None,\n                               weights_initializer=random_normal(0.001),\n                               biases_initializer=init_ops.zeros_initializer())\n          if spatial_squeeze:\n            logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v2_tsn.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ndef inception_v2_tsn_arg_scope(weight_decay=0.00004):\n  """"""Defines the default InceptionV2 arg scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': 0.9997,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': 0.001,\n      # collection containing update_ops.\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n      # Allow a gamma variable\n      \'scale\': True,\n  }\n\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=tf.contrib.layers.xavier_initializer(),\n        activation_fn=None,  # manually added later, as I need to add BN after\n                             # the convolution\n        biases_initializer=init_ops.constant_initializer(value=0.2),\n        normalizer_fn=None) as sc:\n      return sc\n'"
nets/lenet.py,6,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the LeNet model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef lenet(images, num_classes=10, is_training=False,\n          dropout_keep_prob=0.5,\n          prediction_fn=slim.softmax,\n          scope=\'LeNet\'):\n  """"""Creates a variant of the LeNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = lenet.lenet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'LeNet\', [images, num_classes]):\n    net = slim.conv2d(images, 32, [5, 5], scope=\'conv1\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n\n    net = slim.fully_connected(net, 1024, scope=\'fc3\')\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                  scope=\'fc4\')\n\n  end_points[\'Logits\'] = logits\n  end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\nlenet.default_image_size = 28\n\n\ndef lenet_arg_scope(weight_decay=0.0):\n  """"""Defines the default lenet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n      activation_fn=tf.nn.relu) as sc:\n    return sc\n'"
nets/nets_factory.py,24,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport tensorflow as tf\nfrom tensorflow.python.platform import tf_logging as logging\n\nfrom nets import vgg\nfrom nets import frame_pooling as pooling\nfrom nets import inception\n\nslim = tf.contrib.slim\n\nnetworks_map = {\'vgg_a\': vgg.vgg_a,\n                \'vgg_16\': vgg.vgg_16,\n                \'vgg_19\': vgg.vgg_19,\n                \'inception_v2_tsn\': inception.inception_v2_tsn,\n               }\n\narg_scopes_map = {\'vgg_a\': vgg.vgg_arg_scope,\n                  \'vgg_16\': vgg.vgg_arg_scope,\n                  \'vgg_19\': vgg.vgg_arg_scope,\n                  \'inception_v2_tsn\': inception.inception_v2_tsn_arg_scope,\n                 }\n\ndef split_images(images, num_channels_stream):\n  if num_channels_stream is None:\n    return [images]\n  images_splits = []\n  cur_pos = 0\n  for pos in num_channels_stream:\n    images_splits.append(images[..., cur_pos : cur_pos+pos])\n    cur_pos += pos\n  return images_splits\n\n\ndef get_network_fn(\n    name, num_classes, batch_size,\n    weight_decay=0.0, is_training=False,\n    dropout_keep_prob=0.2,\n    pooled_dropout_keep_prob=0.5,\n    batch_norm=False):\n  """"""Returns a network_fn such as `logits, end_points = network_fn(images)`.\n\n  Args:\n    name: The name of the network.\n    num_classes: The number of classes to use for classification.\n    weight_decay: The l2 coefficient for the model weights.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    network_fn: A function that applies the model to a batch of images. It has\n      the following signature:\n        logits, end_points = network_fn(images)\n  Raises:\n    ValueError: If network `name` is not recognized.\n  """"""\n  if name not in networks_map:\n    raise ValueError(\'Name of network unknown %s\' % name)\n  arg_scope = arg_scopes_map[name](weight_decay=weight_decay)\n  func = networks_map[name]\n  @functools.wraps(func)\n  def network_fn(images, pool_type=None,\n                 classifier_type=None,\n                 num_channels_stream=None,\n                 netvlad_centers=[],\n                 stream_pool_type=None,\n                 **kwargs):\n    num_image_sets = 1\n    if len(images.get_shape()) == 5:\n      num_image_sets = images.get_shape().as_list()[0]\n      images = tf.reshape(images, [-1, ] + images.get_shape().as_list()[2:])\n    images_sets = split_images(images, num_channels_stream)\n    all_end_points = []\n    all_out_nets = []\n    with slim.arg_scope(arg_scope):\n      for sid,images in enumerate(images_sets):\n        with tf.variable_scope(\'stream%d\' % sid):\n          net, end_points = func(images,\n                                 num_classes,\n                                 is_training=is_training,\n                                 dropout_keep_prob=dropout_keep_prob,\n                                 conv_only=(pool_type == \'netvlad\' or\n                                  pool_type == \'avg-conv\' or\n                                  pool_type == \'max-conv\' or\n                                  stream_pool_type ==\n                                            \'concat-last-conv-and-netvlad\' or\n                                  stream_pool_type == \'one-bag-and-netvlad\'),\n                                 **kwargs)\n          all_out_nets.append(net)\n          if pool_type in [\'netvlad\', \'avg-conv\', \'max-conv\']:\n            # last_conv = end_points[tf.get_variable_scope().name + \'/\' +\n            #                       conv_endpoint_map[name]]\n            last_conv = net  # both VGG and resnet have conv_only implemented\n            if pool_type == \'netvlad\':\n              net, netvlad_end_points = \\\n                  pooling.netvlad(last_conv, batch_size, 0.0,\n                                  netvlad_initCenters=netvlad_centers[sid])\n              end_points[tf.get_variable_scope().name + \'/netvlad\'] = net\n              end_points.update(netvlad_end_points)\n            elif pool_type == \'avg-conv\':\n              net = pooling.pool_conv(last_conv, batch_size, \'avg\')\n              end_points[tf.get_variable_scope().name + \'/avg-conv\'] = net\n            elif pool_type == \'max-conv\':\n              net = pooling.pool_conv(last_conv, batch_size, \'max\')\n              end_points[tf.get_variable_scope().name + \'/max-conv\'] = net\n            if batch_norm:\n              with tf.variable_scope(\'pooled-batch-norm\'):\n                net = slim.batch_norm(net, is_training=is_training)\n\n            if classifier_type is not None and classifier_type != \'None\':\n              print(\'Dropout is not being applied to the model to be consistent with original release of the code. \'\n                    \'Due to an issue it was not enabled in the original release. \'\n                    \'Please uncomment lines in nets/nets_factory.py to enable the dropout.\')\n              # net = slim.dropout(net, pooled_dropout_keep_prob, scope=\'pooled-dropout\',\n              #                    is_training=is_training)\n            if classifier_type == \'linear\':\n              with tf.variable_scope(\'classifier\'):\n                net = slim.fully_connected(\n                    net, num_classes,\n                    weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n                    activation_fn=None,\n                    normalizer_fn=None,\n                    scope=\'logits\')\n            elif classifier_type == \'two-layer\':\n              with tf.variable_scope(\'classifier\'):\n                net = slim.fully_connected(\n                    net, 4096,\n                    scope=\'logits-1\')\n                net = slim.fully_connected(\n                    net, num_classes,\n                    activation_fn=None,\n                    normalizer_fn=None,\n                    scope=\'logits-2\')\n            end_points[tf.get_variable_scope().name + \'/logits\'] = net\n          elif pool_type in [\'avg\', \'avg-after-softmax\']:\n            if pool_type == \'avg-after-softmax\':\n              net = tf.nn.softmax(net)\n            video_frames = tf.split(0, num_image_sets, net)\n            net = tf.concat(0, [tf.reduce_mean(el, 0, keep_dims=True)\n                                   for el in video_frames])\n            end_points[tf.get_variable_scope().name + \'/logits\'] = net\n          all_end_points.append(end_points)\n      all_end_points.append({})  # for the stream concat ops\n      if stream_pool_type == \'concat-netvlad\':\n        assert(len(all_end_points) == 2+1) # TODO: fix this\n        net = tf.concat(1,\n                        (all_end_points[0][\'stream0/netvlad\'],\n                         all_end_points[1][\'stream1/netvlad\']))\n        all_end_points[-1][\'concat-netvlad\'] = net\n      elif stream_pool_type == \'wtd-avg-pool-logits\':\n        assert(len(all_end_points) == 2+1) # TODO: fix this\n        net = 0.667 * all_end_points[1][\'stream1/logits\'] + \\\n              0.333 * all_end_points[0][\'stream0/logits\']\n        all_end_points[-1][\'wtd-avg-pool-logits\'] = net\n      elif stream_pool_type == \'concat-last-conv-and-netvlad\' or \\\n           stream_pool_type == \'one-bag-and-netvlad\':\n        with tf.variable_scope(stream_pool_type):\n          if stream_pool_type == \'one-bag-and-netvlad\':\n            net = tf.concat(1, all_out_nets)\n          else:\n            net = tf.concat(3, all_out_nets)\n          end_points[tf.get_variable_scope().name + \'/concat-last-conv\'] = net\n          net, netvlad_end_points = pooling.netvlad(\n            net, batch_size, 0.0, netvlad_initCenters=netvlad_centers[0])\n          end_points[tf.get_variable_scope().name + \'/concat-last-conv-netvlad\'] = net\n          end_points.update(netvlad_end_points)\n      elif stream_pool_type is not None:\n        raise ValueError(\'Unknown stream pool type %s\' % stream_pool_type)\n      if stream_pool_type in [\'concat-netvlad\', \'concat-last-conv-and-netvlad\',\n                              \'one-bag-and-netvlad\']:\n        net = slim.dropout(net, pooled_dropout_keep_prob, scope=\'pooled-dropout\',\n                           is_training=is_training)\n        with tf.variable_scope(\'classifier\'):\n          net = slim.fully_connected(\n            net, num_classes,\n            weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n            activation_fn=None,\n            normalizer_fn=None,\n            scope=\'stream-pool-logits\')\n        all_end_points[-1][\'stream_pool_type\'] = net\n      logits = net\n\n    final_end_points = {}\n    for el in all_end_points:\n      final_end_points.update(el)\n\n    return logits, final_end_points\n  if hasattr(func, \'default_image_size\'):\n    network_fn.default_image_size = func.default_image_size\n\n  return network_fn\n'"
nets/nets_factory_test.py,4,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for slim.inception.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nfrom nets import nets_factory\n\n\nclass NetworksTest(tf.test.TestCase):\n\n  def testGetNetworkFn(self):\n    batch_size = 5\n    num_classes = 1000\n    for net in nets_factory.networks_map:\n      with self.test_session():\n        net_fn = nets_factory.get_network_fn(net, num_classes)\n        # Most networks use 224 as their default_image_size\n        image_size = getattr(net_fn, \'default_image_size\', 224)\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        logits, end_points = net_fn(inputs)\n        self.assertTrue(isinstance(logits, tf.Tensor))\n        self.assertTrue(isinstance(end_points, dict))\n        self.assertEqual(logits.get_shape().as_list()[0], batch_size)\n        self.assertEqual(logits.get_shape().as_list()[-1], num_classes)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
nets/vgg.py,14,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains model definitions for versions of the Oxford VGG network.\n\nThese model definitions were introduced in the following technical report:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n\nUsage:\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_a(inputs)\n\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_16(inputs)\n\n@@vgg_a\n@@vgg_16\n@@vgg_19\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef vgg_arg_scope(weight_decay=0.0005):\n  """"""Defines the VGG arg scope.\n\n  Args:\n    weight_decay: The l2 regularization coefficient.\n\n  Returns:\n    An arg_scope.\n  """"""\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\') as arg_sc:\n      return arg_sc\n\n\ndef vgg_a(inputs,\n          num_classes=1000,\n          is_training=True,\n          dropout_keep_prob=0.5,\n          spatial_squeeze=True,\n          scope=\'vgg_a\'):\n  """"""Oxford Net VGG 11-Layers version A Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_a\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=\'VALID\', scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      #avg_fc7 = tf.reduce_mean(net, axis=0) # Added by brussell\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      #end_points.update({\'avg_fc7\': avg_fc7}) # Added by brussell\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_a.default_image_size = 224\n\n\ndef vgg_16(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_16\',\n           conv_only=False,\n           conv_endpoint=\'conv5\'):\n  """"""Oxford Net VGG 16-Layers version D Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_16\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # addl_endpoints = {}\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      # net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 512, [3, 3],\n          activation_fn=None, scope=\'conv4/conv4_3\')\n      if conv_only and conv_endpoint == \'conv4\':\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        return net, end_points\n      net = tf.nn.relu(net)\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      # net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      # Splitting it so as to read before ReLU\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.conv2d(net, 512, [3, 3],\n          activation_fn=None, scope=\'conv5/conv5_3\')\n      # addl_endpoints[\'conv5_3_beforeReLU\'] = net\n      if conv_only and conv_endpoint == \'conv5\':\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        return net, end_points\n      # netvlad is done over output now, and vgg was returning relu-ed\n      # outupt, leading to low performance. So conv_only output now is before\n      # relu. This is the default now.\n      net = tf.nn.relu(net)\n      # addl_endpoints[\'conv5_3_afterReLU\'] = net\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=\'VALID\', scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\', activation_fn=None)\n      if conv_only and conv_endpoint == \'fc7\': \n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        return net, end_points\n      \'\'\'\n      note that you\'re averaging over the whole batch here, so run the testing/eval.py with batch_size of 1 (else it\'ll average features across all 25 frames of each video in the batch). The ""right"" way of doing this -- that\'ll work with any batch size -- would be a bit more involved.\n      \'\'\'\n      avg_fc7 = tf.reduce_mean(net, axis=0) # Added by brussell \n      net = tf.nn.relu(net)\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      end_points.update({\'avg_fc7\': avg_fc7}) # Added by brussell\n      # end_points.update(addl_endpoints)\n      if not conv_only and spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_16.default_image_size = 224\n\n\ndef vgg_19(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_19\'):\n  """"""Oxford Net VGG 19-Layers version E Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_19\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=\'VALID\', scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_19.default_image_size = 224\n\n# Alias\nvgg_d = vgg_16\nvgg_e = vgg_19\n'"
nets/vgg_test.py,44,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.vgg.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\n\nclass VGGATest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1\',\n                        \'vgg_a/pool1\',\n                        \'vgg_a/conv2/conv2_1\',\n                        \'vgg_a/pool2\',\n                        \'vgg_a/conv3/conv3_1\',\n                        \'vgg_a/conv3/conv3_2\',\n                        \'vgg_a/pool3\',\n                        \'vgg_a/conv4/conv4_1\',\n                        \'vgg_a/conv4/conv4_2\',\n                        \'vgg_a/pool4\',\n                        \'vgg_a/conv5/conv5_1\',\n                        \'vgg_a/conv5/conv5_2\',\n                        \'vgg_a/pool5\',\n                        \'vgg_a/fc6\',\n                        \'vgg_a/fc7\',\n                        \'vgg_a/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1/weights\',\n                        \'vgg_a/conv1/conv1_1/biases\',\n                        \'vgg_a/conv2/conv2_1/weights\',\n                        \'vgg_a/conv2/conv2_1/biases\',\n                        \'vgg_a/conv3/conv3_1/weights\',\n                        \'vgg_a/conv3/conv3_1/biases\',\n                        \'vgg_a/conv3/conv3_2/weights\',\n                        \'vgg_a/conv3/conv3_2/biases\',\n                        \'vgg_a/conv4/conv4_1/weights\',\n                        \'vgg_a/conv4/conv4_1/biases\',\n                        \'vgg_a/conv4/conv4_2/weights\',\n                        \'vgg_a/conv4/conv4_2/biases\',\n                        \'vgg_a/conv5/conv5_1/weights\',\n                        \'vgg_a/conv5/conv5_1/biases\',\n                        \'vgg_a/conv5/conv5_2/weights\',\n                        \'vgg_a/conv5/conv5_2/biases\',\n                        \'vgg_a/fc6/weights\',\n                        \'vgg_a/fc6/biases\',\n                        \'vgg_a/fc7/weights\',\n                        \'vgg_a/fc7/biases\',\n                        \'vgg_a/fc8/weights\',\n                        \'vgg_a/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_a(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False,\n                            spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs)\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG16Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1\',\n                        \'vgg_16/conv1/conv1_2\',\n                        \'vgg_16/pool1\',\n                        \'vgg_16/conv2/conv2_1\',\n                        \'vgg_16/conv2/conv2_2\',\n                        \'vgg_16/pool2\',\n                        \'vgg_16/conv3/conv3_1\',\n                        \'vgg_16/conv3/conv3_2\',\n                        \'vgg_16/conv3/conv3_3\',\n                        \'vgg_16/pool3\',\n                        \'vgg_16/conv4/conv4_1\',\n                        \'vgg_16/conv4/conv4_2\',\n                        \'vgg_16/conv4/conv4_3\',\n                        \'vgg_16/pool4\',\n                        \'vgg_16/conv5/conv5_1\',\n                        \'vgg_16/conv5/conv5_2\',\n                        \'vgg_16/conv5/conv5_3\',\n                        \'vgg_16/pool5\',\n                        \'vgg_16/fc6\',\n                        \'vgg_16/fc7\',\n                        \'vgg_16/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1/weights\',\n                        \'vgg_16/conv1/conv1_1/biases\',\n                        \'vgg_16/conv1/conv1_2/weights\',\n                        \'vgg_16/conv1/conv1_2/biases\',\n                        \'vgg_16/conv2/conv2_1/weights\',\n                        \'vgg_16/conv2/conv2_1/biases\',\n                        \'vgg_16/conv2/conv2_2/weights\',\n                        \'vgg_16/conv2/conv2_2/biases\',\n                        \'vgg_16/conv3/conv3_1/weights\',\n                        \'vgg_16/conv3/conv3_1/biases\',\n                        \'vgg_16/conv3/conv3_2/weights\',\n                        \'vgg_16/conv3/conv3_2/biases\',\n                        \'vgg_16/conv3/conv3_3/weights\',\n                        \'vgg_16/conv3/conv3_3/biases\',\n                        \'vgg_16/conv4/conv4_1/weights\',\n                        \'vgg_16/conv4/conv4_1/biases\',\n                        \'vgg_16/conv4/conv4_2/weights\',\n                        \'vgg_16/conv4/conv4_2/biases\',\n                        \'vgg_16/conv4/conv4_3/weights\',\n                        \'vgg_16/conv4/conv4_3/biases\',\n                        \'vgg_16/conv5/conv5_1/weights\',\n                        \'vgg_16/conv5/conv5_1/biases\',\n                        \'vgg_16/conv5/conv5_2/weights\',\n                        \'vgg_16/conv5/conv5_2/biases\',\n                        \'vgg_16/conv5/conv5_3/weights\',\n                        \'vgg_16/conv5/conv5_3/biases\',\n                        \'vgg_16/fc6/weights\',\n                        \'vgg_16/fc6/biases\',\n                        \'vgg_16/fc7/weights\',\n                        \'vgg_16/fc7/biases\',\n                        \'vgg_16/fc8/weights\',\n                        \'vgg_16/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_16(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs)\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG19Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1\',\n          \'vgg_19/conv1/conv1_2\',\n          \'vgg_19/pool1\',\n          \'vgg_19/conv2/conv2_1\',\n          \'vgg_19/conv2/conv2_2\',\n          \'vgg_19/pool2\',\n          \'vgg_19/conv3/conv3_1\',\n          \'vgg_19/conv3/conv3_2\',\n          \'vgg_19/conv3/conv3_3\',\n          \'vgg_19/conv3/conv3_4\',\n          \'vgg_19/pool3\',\n          \'vgg_19/conv4/conv4_1\',\n          \'vgg_19/conv4/conv4_2\',\n          \'vgg_19/conv4/conv4_3\',\n          \'vgg_19/conv4/conv4_4\',\n          \'vgg_19/pool4\',\n          \'vgg_19/conv5/conv5_1\',\n          \'vgg_19/conv5/conv5_2\',\n          \'vgg_19/conv5/conv5_3\',\n          \'vgg_19/conv5/conv5_4\',\n          \'vgg_19/pool5\',\n          \'vgg_19/fc6\',\n          \'vgg_19/fc7\',\n          \'vgg_19/fc8\'\n      ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1/weights\',\n          \'vgg_19/conv1/conv1_1/biases\',\n          \'vgg_19/conv1/conv1_2/weights\',\n          \'vgg_19/conv1/conv1_2/biases\',\n          \'vgg_19/conv2/conv2_1/weights\',\n          \'vgg_19/conv2/conv2_1/biases\',\n          \'vgg_19/conv2/conv2_2/weights\',\n          \'vgg_19/conv2/conv2_2/biases\',\n          \'vgg_19/conv3/conv3_1/weights\',\n          \'vgg_19/conv3/conv3_1/biases\',\n          \'vgg_19/conv3/conv3_2/weights\',\n          \'vgg_19/conv3/conv3_2/biases\',\n          \'vgg_19/conv3/conv3_3/weights\',\n          \'vgg_19/conv3/conv3_3/biases\',\n          \'vgg_19/conv3/conv3_4/weights\',\n          \'vgg_19/conv3/conv3_4/biases\',\n          \'vgg_19/conv4/conv4_1/weights\',\n          \'vgg_19/conv4/conv4_1/biases\',\n          \'vgg_19/conv4/conv4_2/weights\',\n          \'vgg_19/conv4/conv4_2/biases\',\n          \'vgg_19/conv4/conv4_3/weights\',\n          \'vgg_19/conv4/conv4_3/biases\',\n          \'vgg_19/conv4/conv4_4/weights\',\n          \'vgg_19/conv4/conv4_4/biases\',\n          \'vgg_19/conv5/conv5_1/weights\',\n          \'vgg_19/conv5/conv5_1/biases\',\n          \'vgg_19/conv5/conv5_2/weights\',\n          \'vgg_19/conv5/conv5_2/biases\',\n          \'vgg_19/conv5/conv5_3/weights\',\n          \'vgg_19/conv5/conv5_3/biases\',\n          \'vgg_19/conv5/conv5_4/weights\',\n          \'vgg_19/conv5/conv5_4/biases\',\n          \'vgg_19/fc6/weights\',\n          \'vgg_19/fc6/biases\',\n          \'vgg_19/fc7/weights\',\n          \'vgg_19/fc7/biases\',\n          \'vgg_19/fc8/weights\',\n          \'vgg_19/fc8/biases\',\n      ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_19(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs)\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
preprocessing/__init__.py,0,b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n\n'
preprocessing/preprocessing_factory.py,4,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom preprocessing import vgg_ucf_preprocessing\n\nslim = tf.contrib.slim\n\n\ndef get_preprocessing(name, is_training=False):\n  """"""Returns preprocessing_fn(image, height, width, **kwargs).\n\n  Args:\n    name: The name of the preprocessing function.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    preprocessing_fn: A function that preprocessing a single image (pre-batch).\n      It has the following signature:\n        image = preprocessing_fn(image, output_height, output_width, ...).\n\n  Raises:\n    ValueError: If Preprocessing `name` is not recognized.\n  """"""\n  preprocessing_fn_map = {\n      \'vgg_ucf\': vgg_ucf_preprocessing,\n  }\n\n  if name not in preprocessing_fn_map:\n    raise ValueError(\'Preprocessing name [%s] was not recognized\' % name)\n\n  def preprocessing_fn(image, output_height, output_width, **kwargs):\n    with tf.variable_scope(\'preprocess_image\'):\n      if len(image.get_shape()) == 3:\n        return preprocessing_fn_map[name].preprocess_image(\n            image, output_height, output_width, is_training=is_training, **kwargs)\n      elif len(image.get_shape()) == 4:\n        # preprocess all the images in one set in the same way by concat-ing\n        # them in channels\n        nImgs = image.get_shape().as_list()[0]\n        final_img_concat = preprocessing_fn_map[name].preprocess_image(\n            tf.concat(2, tf.unpack(image)),\n            output_height, output_width, is_training=is_training, **kwargs)\n        return tf.concat(0, tf.split(3, nImgs, final_img_concat))\n      else:\n        print(\'Incorrect dims image!\')\n\n  return preprocessing_fn\n'"
preprocessing/utils.py,2,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\nimport tensorflow as tf\n\ndef _mean_image_subtraction(image, means):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector (or a factor of C) \n           of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, or if it does\n    not have dim=3. Also, if it is not a multiple of the means\n    passed in.\n  """"""\n  if image.get_shape().ndims % 3 != 0:\n    raise ValueError(\'Input must be of size [height, width, C>0], C multiple of 3.\')\n  num_channels = image.get_shape().as_list()[-1]\n  if num_channels % len(means) != 0:\n    raise ValueError(\'len(means) must be a factor the number of channels.\')\n  means = means * int(num_channels / len(means))\n\n  channels = tf.split(2, num_channels, image)\n  for i in range(num_channels):\n    channels[i] -= means[i]\n  return tf.concat(2, channels)\n'"
preprocessing/vgg_preprocessing.py,52,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nThe preprocessing steps for VGG were introduced in the following technical\nreport:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\nfrom preprocessing.utils import _mean_image_subtraction\n\nslim = tf.contrib.slim\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n  """"""Crops the given image using the provided offsets and sizes.\n\n  Note that the method doesn\'t assume we know the input image size but it does\n  assume we know the input image rank.\n\n  Args:\n    image: an image of shape [height, width, channels].\n    offset_height: a scalar tensor indicating the height offset.\n    offset_width: a scalar tensor indicating the width offset.\n    crop_height: the height of the cropped image.\n    crop_width: the width of the cropped image.\n\n  Returns:\n    the cropped (and resized) image.\n\n  Raises:\n    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n      less than the crop size.\n  """"""\n  original_shape = tf.shape(image)\n\n  rank_assertion = tf.Assert(\n      tf.equal(tf.rank(image), 3),\n      [\'Rank of image must be equal to 3.\'])\n  cropped_shape = control_flow_ops.with_dependencies(\n      [rank_assertion],\n      tf.pack([crop_height, crop_width, original_shape[2]]))\n\n  size_assertion = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(original_shape[0], crop_height),\n          tf.greater_equal(original_shape[1], crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  offsets = tf.to_int32(tf.pack([offset_height, offset_width, 0]))\n\n  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n  # define the crop size.\n  image = control_flow_ops.with_dependencies(\n      [size_assertion],\n      tf.slice(image, offsets, cropped_shape))\n  return tf.reshape(image, cropped_shape)\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n  """"""Crops the given list of images.\n\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n\n  Returns:\n    the image_list with cropped images.\n\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  """"""\n  if not image_list:\n    raise ValueError(\'Empty image_list.\')\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n        tf.equal(image_rank, 3),\n        [\'Wrong rank for tensor  %s [expected] [actual]\',\n         image_list[i].name, 3, image_rank])\n    rank_assertions.append(rank_assert)\n\n  image_shape = control_flow_ops.with_dependencies(\n      [rank_assertions[0]],\n      tf.shape(image_list[0]))\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(image_height, crop_height),\n          tf.greater_equal(image_width, crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    shape = control_flow_ops.with_dependencies([rank_assertions[i]],\n                                               tf.shape(image))\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n        tf.equal(height, image_height),\n        [\'Wrong height for tensor %s [expected][actual]\',\n         image.name, height, image_height])\n    width_assert = tf.Assert(\n        tf.equal(width, image_width),\n        [\'Wrong width for tensor %s [expected][actual]\',\n         image.name, width, image_width])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  max_offset_height = control_flow_ops.with_dependencies(\n      asserts, tf.reshape(image_height - crop_height + 1, []))\n  max_offset_width = control_flow_ops.with_dependencies(\n      asserts, tf.reshape(image_width - crop_width + 1, []))\n  offset_height = tf.random_uniform(\n      [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random_uniform(\n      [], maxval=max_offset_width, dtype=tf.int32)\n\n  return [_crop(image, offset_height, offset_width,\n                crop_height, crop_width) for image in image_list]\n\n\ndef _central_crop(image_list, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    the list of cropped images.\n  """"""\n  outputs = []\n  for image in image_list:\n    image_height = tf.shape(image)[0]\n    image_width = tf.shape(image)[1]\n\n    offset_height = (image_height - crop_height) / 2\n    offset_width = (image_width - crop_width) / 2\n\n    outputs.append(_crop(image, offset_height, offset_width,\n                         crop_height, crop_width))\n  return outputs\n\n\ndef _smallest_size_at_least(height, width, smallest_side):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: and int32 scalar tensor indicating the new width.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  height = tf.to_float(height)\n  width = tf.to_float(width)\n  smallest_side = tf.to_float(smallest_side)\n\n  scale = tf.cond(tf.greater(height, width),\n                  lambda: smallest_side / width,\n                  lambda: smallest_side / height)\n  new_height = tf.to_int32(height * scale)\n  new_width = tf.to_int32(width * scale)\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, smallest_side):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  shape = tf.shape(image)\n  height = shape[0]\n  width = shape[1]\n  new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n  image = tf.expand_dims(image, 0)\n  resized_image = tf.image.resize_bilinear(image, [new_height, new_width],\n                                           align_corners=False)\n  resized_image = tf.squeeze(resized_image)\n  resized_image.set_shape([None, None, 3])\n  return resized_image\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         resize_side_min=_RESIZE_SIDE_MIN,\n                         resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  resize_side = tf.random_uniform(\n      [], minval=resize_side_min, maxval=resize_side_max+1, dtype=tf.int32)\n\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _random_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  image = tf.image.random_flip_left_right(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_for_eval(image, output_height, output_width, resize_side):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side: The smallest side of the image for aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _central_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_image(image, output_height, output_width, is_training=False,\n                     resize_side_min=_RESIZE_SIDE_MIN,\n                     resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, then this value\n      is used for rescaling.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, this value is\n      ignored. Otherwise, the resize side is sampled from\n        [resize_size_min, resize_size_max].\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    return preprocess_for_train(image, output_height, output_width,\n                                resize_side_min, resize_side_max)\n  else:\n    return preprocess_for_eval(image, output_height, output_width,\n                               resize_side_min)\n'"
preprocessing/vgg_ucf_preprocessing.py,46,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\nfrom preprocessing.utils import _mean_image_subtraction\nfrom preprocessing.vgg_preprocessing import _crop, _central_crop\nfrom tensorflow.python.platform import tf_logging as logging\n\nslim = tf.contrib.slim\n\n_R_MEAN = 123.0\n_G_MEAN = 117.0\n_B_MEAN = 104.0\n\n_RESIZE_HT = 256\n_RESIZE_WD = 340  # This used to be 340 in the previous code\n_SCALE_RATIOS = [1,.875,.75,.66]\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n  """"""Crops the given list of images.\n\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n\n  Returns:\n    the image_list with cropped images.\n\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  """"""\n  if not image_list:\n    raise ValueError(\'Empty image_list.\')\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n        tf.equal(image_rank, 3),\n        [\'Wrong rank for tensor  %s [expected] [actual]\',\n         image_list[i].name, 3, image_rank])\n    rank_assertions.append(rank_assert)\n\n  image_shape = control_flow_ops.with_dependencies(\n      [rank_assertions[0]],\n      tf.shape(image_list[0]))\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(image_height, crop_height),\n          tf.greater_equal(image_width, crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    shape = control_flow_ops.with_dependencies([rank_assertions[i]],\n                                               tf.shape(image))\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n        tf.equal(height, image_height),\n        [\'Wrong height for tensor %s [expected][actual]\',\n         image.name, height, image_height])\n    width_assert = tf.Assert(\n        tf.equal(width, image_width),\n        [\'Wrong width for tensor %s [expected][actual]\',\n         image.name, width, image_width])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  # TODO (rgirdhar): Force corner crops, right now going with random crops\n  max_offset_height = control_flow_ops.with_dependencies(\n      asserts, tf.reshape(image_height - crop_height + 1, []))\n  max_offset_width = control_flow_ops.with_dependencies(\n      asserts, tf.reshape(image_width - crop_width + 1, []))\n  offset_height = tf.random_uniform(\n      [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random_uniform(\n      [], maxval=max_offset_width, dtype=tf.int32)\n\n  return [_crop(image, offset_height, offset_width,\n                crop_height, crop_width) for image in image_list]\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         mean_vals,\n                         out_dim_scale=1.0):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  num_channels = image.get_shape().as_list()[-1]\n  image = tf.image.resize_images(image, [_RESIZE_HT, _RESIZE_WD])\n  # compute the crop size\n  base_size = float(min(_RESIZE_HT, _RESIZE_WD))\n  scale_ratio_h = tf.random_shuffle(tf.constant(_SCALE_RATIOS))[0]\n  scale_ratio_w = tf.random_shuffle(tf.constant(_SCALE_RATIOS))[0]\n  image = _random_crop([image],\n      tf.cast(output_height * scale_ratio_h, tf.int32),\n      tf.cast(output_width * scale_ratio_w, tf.int32))[0]\n  image = tf.image.resize_images(\n    image, [int(output_height * out_dim_scale),\n            int(output_width * out_dim_scale)])\n  image = tf.to_float(image)\n  image = tf.image.random_flip_left_right(image)\n  image.set_shape([int(output_height * out_dim_scale),\n                   int(output_width * out_dim_scale), num_channels])\n  image = _mean_image_subtraction(image, mean_vals)\n  image = tf.expand_dims(image, 0) # 1x... image, to be consistent with eval\n  # Gets logged multiple times with NetVLAD, so gives an error.\n  # I\'m anyway logging from the train code, so removing it here.\n  # tf.image_summary(\'final_distorted_image\',\n  #     tf.expand_dims(image / 128.0, 0))\n  return image\n\n\ndef preprocess_for_eval(image, output_height, output_width,\n                        mean_vals, out_dim_scale=1.0, ncrops=1):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  image = tf.image.resize_images(image, [_RESIZE_HT, _RESIZE_WD])\n  if ncrops == 1:\n    images = tf.image.crop_to_bounding_box(image, 16, 60, output_height, output_width)\n    if abs(out_dim_scale - 1) >= 0.1:\n      images = tf.image.resize_images(\n        images, [int(output_height * out_dim_scale),\n                int(output_width * out_dim_scale)])\n    images = _mean_image_subtraction(tf.to_float(images), mean_vals)\n    images = tf.expand_dims(images, 0)\n  elif ncrops == 5:\n    collect = []\n    collect.append(tf.image.crop_to_bounding_box(\n      image, 0, 0, output_height, output_width))\n    collect.append(tf.image.crop_to_bounding_box(\n      image, 0, 115, output_height, output_width))\n    collect.append(tf.image.crop_to_bounding_box(\n      image, 31, 0, output_height, output_width))\n    collect.append(tf.image.crop_to_bounding_box(\n      image, 31, 115, output_height, output_width))\n    collect.append(tf.image.crop_to_bounding_box(\n      image, 16, 60, output_height, output_width))\n    # for i in range(5):\n    #   collect.append(tf.image.flip_left_right(collect[i]))\n    for i in range(len(collect)):\n      collect[i] = _mean_image_subtraction(tf.to_float(collect[i]), mean_vals)\n    images = tf.pack(collect)\n  # image.set_shape([output_height, output_width, 3])\n  # images = tf.to_float(images)\n  return images\n\n\ndef preprocess_image(image, output_height, output_width, is_training=False,\n                     scale_ratios=_SCALE_RATIOS,\n                     ncrops=1,\n                     out_dim_scale=1.0,  # scale the output image dimensions by\n                                         # this ratio\n                     model_name=\'vgg_16\'):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  global _SCALE_RATIOS\n  _SCALE_RATIOS = scale_ratios[0]\n  # my image_read_utils normalizes the image to 0-1, so restore that\n  IMG_SCALER = 255.0\n  FLOW_MEAN = 128\n  FINAL_SCALER = 1.0\n  global _B_MEAN, _G_MEAN, _R_MEAN\n  if model_name.startswith(\'inception\') and model_name != \'inception_v2_tsn\':\n    logging.info(\'Using inception parameters for preprocessing\')\n    IMG_SCALER = 1.0\n    _B_MEAN = 0.5\n    _R_MEAN = 0.5\n    _G_MEAN = 0.5\n    FLOW_MEAN = 0.5\n    FINAL_SCALER = 2.0\n  image = image * IMG_SCALER\n  num_channels = image.get_shape().as_list()[-1]\n  if num_channels % 3 == 0:\n    # For RGB, anyway it\'s always BGR flipped\n    # if bgr_flip:\n    logging.info(\'Assuming the batch is full of RGB images, and BGR flipped\')\n    mean_vals = [_B_MEAN, _G_MEAN, _R_MEAN]\n    # else:\n    #   mean_vals = [_R_MEAN, _G_MEAN, _B_MEAN]\n  elif num_channels % 23 == 0:\n    logging.info(\'Assuming the batch is full of RGB+Flow images, and first \'\n                 \'part is BGR flipped\')\n    mean_vals = [_B_MEAN, _G_MEAN, _R_MEAN] + [FLOW_MEAN] * 20\n  else:\n    logging.info(\'Assuming the batch is full of Flow images.\')\n    mean_vals = [FLOW_MEAN] * num_channels\n  if is_training:\n    res = preprocess_for_train(image, output_height, output_width, mean_vals,\n                               out_dim_scale)\n  else:\n    logging.info(\'Performing eval pre-processing\')\n    res = preprocess_for_eval(image, output_height, output_width, mean_vals,\n                              out_dim_scale, ncrops)\n  return res * FINAL_SCALER\n'"
restore/__init__.py,0,b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n'
restore/model_restorer.py,1,"b""# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\nimport numpy as np\nimport h5py\n\nfrom tensorflow.contrib import slim\nfrom tensorflow.python.platform import tf_logging as logging\nimport tensorflow as tf\nimport var_name_mapper\n\n\ndef restore_model(checkpoint_paths,\n                  variables_to_restore,\n                  ignore_missing_vars=False,\n                  num_streams=1,\n                  checkpoint_style=None,\n                  special_assign_vars=None):\n    all_ops = []\n    if len(checkpoint_paths) == 1 and num_streams > 1:\n      logging.info('Provided one checkpoint for multi-stream '\n                   'network. Will use this as a saved model '\n                   'with this exact multi stream network.')\n      all_ops.append(slim.assign_from_checkpoint_fn(\n        checkpoint_paths[0],\n        variables_to_restore,\n        ignore_missing_vars=ignore_missing_vars))\n    else:\n      for sid in range(num_streams):\n        this_checkpoint_style = checkpoint_style.split(',')[sid] if \\\n                                checkpoint_style is not None else None\n        checkpoint_path = checkpoint_paths[sid]\n        # assert tf.gfile.Exists(checkpoint_path)\n        this_stream_name = 'stream%d/' % sid\n        this_checkpoint_variables = [var for var in variables_to_restore\n                                     if var in\n                                     slim.get_model_variables(this_stream_name)]\n        if checkpoint_path.endswith('.npy'):\n          vars_to_restore_names = [\n              el.name for el in this_checkpoint_variables]\n          key_name_mapper = var_name_mapper.map()\n          init_weights = np.load(checkpoint_path).item()\n          init_weights_final = {}\n          vars_restored = []\n          for key in init_weights.keys():\n            for subkey in init_weights[key].keys():\n              prefix = this_stream_name\n              if this_checkpoint_style == 'v2_withStream':\n                prefix = 'stream0/'  # because any model trained with stream\n                                     # will have that stream as 0\n              final_key_name = prefix + key_name_mapper(\n                  key + '/' + subkey)\n              if final_key_name not in vars_to_restore_names:\n                logging.error('Not using %s from npy' % final_key_name)\n                continue\n              \n              target_shape = slim.get_model_variables(\n                final_key_name)[0].get_shape().as_list()\n              pretrained_wts = init_weights[key][subkey]\n              target_shape_squeezed = np.delete(\n                target_shape, np.where(np.array(target_shape) == 1))\n              pretrained_shape_squeezed = np.delete(\n                pretrained_wts.shape, np.where(np.array(pretrained_wts.shape) == 1))\n              if np.all(target_shape_squeezed !=\n                        pretrained_shape_squeezed):\n                logging.error('Shape mismatch var: %s from npy [%s vs %s]' \n                              % (final_key_name, target_shape,\n                                 pretrained_wts.shape))\n\n              init_weights_final[final_key_name] = \\\n                  pretrained_wts\n              vars_restored.append(final_key_name)\n          init_weights = init_weights_final\n          for v in vars_to_restore_names:\n            if v not in vars_restored:\n              logging.fatal('No weights found for %s' % v)\n          all_ops.append(slim.assign_from_values_fn(\n              init_weights))\n        else:\n          if this_checkpoint_style != 'v2_withStream':\n            all_ops.append(slim.assign_from_checkpoint_fn(\n                checkpoint_path,\n                # stripping the stream name to map variables\n                dict(\n                  [('/'.join(el.name.split('/')[1:]).split(':')[0], el) for\n                      el in this_checkpoint_variables]),\n                ignore_missing_vars=ignore_missing_vars))\n          else:\n            all_ops.append(slim.assign_from_checkpoint_fn(\n                checkpoint_path,\n                # stripping the stream name to map variables, to stream0,\n                # as the model is v2_withStream, hence must be trained with\n                # stream0/ prefix\n                dict(\n                  [('/'.join(['stream0'] + el.name.split('/')[1:]).split(':')[0], el) for\n                      el in this_checkpoint_variables]),\n                ignore_missing_vars=ignore_missing_vars))\n    if special_assign_vars is not None:\n      all_ops.append(get_special_assigns(special_assign_vars))\n    def combined(sess):\n      for op in all_ops:\n        op(sess)\n    return combined\n\n\ndef get_special_assigns(special_assign_vars):\n  init_wts = {}\n  special_assign_vars = special_assign_vars.split(',')\n  for i in range(len(special_assign_vars) / 2):\n    var_name = special_assign_vars[2*i]\n    file_path = special_assign_vars[2*i+1]\n    with h5py.File(file_path, 'r') as fin:\n      init_wts[var_name] = fin['feat'].value\n    logging.info('Special Assign: %s with a %s array' % (\n      var_name, init_wts[var_name].shape))\n  return slim.assign_from_values_fn(init_wts)\n"""
restore/var_name_mapper.py,2,"b""# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\nimport tensorflow as tf\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n  'var_name_mapping', 'none',\n  'Map the variable names while loading caffemodels.')\n\n\ndef map():\n  map_fn = lambda x: x\n  if FLAGS.var_name_mapping == 'placenet365-vgg':\n    map_fn = placenet365_vgg_fn\n  elif FLAGS.var_name_mapping == 'cuhk-action-vgg':\n    map_fn = cuhk_action_vgg\n  elif FLAGS.var_name_mapping == 'cuhk-action-tsn':\n    map_fn = cuhk_action_tsn\n  elif FLAGS.var_name_mapping == 'xiaolonw_action_vgg_hmdb':\n    map_fn = xiaolonw_action_vgg_hmdb\n  return map_fn\n\n\ndef placenet365_vgg_fn(var_name):\n  final_name = var_name\n  if final_name.split('/')[0].startswith('conv'):\n    final_name = \\\n      final_name.split('/')[0].split('_')[0] + '/' + final_name\n  elif final_name.split('/')[0] == 'fc8a':\n    final_name = final_name.replace('fc8a', 'fc8')\n  return 'vgg_16/' + final_name + ':0'\n\n\ndef cuhk_action_vgg(var_name):\n  final_name = var_name\n  if final_name.split('/')[0].startswith('conv'):\n    final_name = \\\n      final_name.split('/')[0].split('_')[0] + '/' + final_name\n  elif final_name.split('/')[0].startswith('fc8'):\n    final_name = final_name.replace(final_name.split('/')[0], 'fc8')\n  return 'vgg_16/' + final_name + ':0'\n\n\ndef xiaolonw_action_vgg_hmdb(var_name):\n  final_name = var_name\n  if final_name.split('/')[0].startswith('conv'):\n    final_name = \\\n      final_name.split('/')[0].split('_')[0] + '/' + final_name\n  elif final_name.split('/')[0] == 'fc8_hmdb':\n    final_name = final_name.replace('fc8_hmdb', 'fc8')\n  return 'vgg_16/' + final_name + ':0'\n\n\ndef cuhk_action_tsn(var_name):\n  final_name = var_name\n  var_name = final_name.split('/')[-1]\n  if final_name.split('/')[0].endswith('_bn'):\n    if var_name == 'scale':\n      var_name = 'gamma'\n    elif var_name == 'shift':\n      var_name = 'beta'\n    elif var_name == 'mean':\n      var_name = 'moving_mean'\n    elif var_name == 'variance':\n      var_name = 'moving_variance'\n    final_name = \\\n      final_name.split('/')[0][:-3] + '/BatchNorm/' + var_name\n  elif final_name.split('/')[0] == 'fc-action':\n    final_name = 'Logits/Conv2d_1c_1x1/' + var_name\n  else:\n    final_name = final_name.split('/')[0] + '/Conv/' + var_name\n  block_name = final_name.split('/')[0]\n  pos = None\n  if block_name.startswith('inception'):\n    pos = len('inception_xx')\n  elif block_name.startswith('conv'):\n    pos = len('convx')\n  if pos is not None:\n    final_name = final_name[:pos] + '/' + final_name[pos+1:]\n  return 'InceptionV2_TSN/' + final_name + ':0'\n"""
vlad_utils/cluster_feats.py,0,"b""# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\nimport numpy as np\nimport cPickle as pickle\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import normalize\nimport h5py\nimport argparse\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Cluster Features')\n    parser.add_argument('-k', '--nclusters',\n            type=int, default=32,\n            help='Number of clusters.')\n    parser.add_argument('-j', '--njobs',\n            type=int, default=8,\n            help='Number of jobs to run.')\n    parser.add_argument('-o', '--outfpath',\n            type=str, required=True,\n            help='Path to pkl file to store the clusters.')\n    parser.add_argument('-i', '--inputfeatpath',\n            type=str, required=True,\n            help='Path to h5 file with features.')\n    parser.add_argument('-n', '--feat_name',\n            type=str, required=True,\n            help='Layer name whose features to use.') \n    parser.add_argument('--nfeats', type=int,\n            default=-1, help='Set to use subset of feats for clustering.')\n    parser.add_argument('--relu', type=bool,\n            default=False, help='Set true to relu the features after reading.')\n    args = vars(parser.parse_args())\n\n    with h5py.File(args['inputfeatpath'], 'r') as fin:\n      allfeats = fin[args['feat_name']].value\n    allfeats = allfeats[:args['nfeats'], ...]\n    if args['relu']:\n      print('ReLU-ing all features')\n      allfeats[allfeats < 0] = 0\n    allfeats = np.reshape(allfeats, (-1, allfeats.shape[-1]))\n    print('Clustering %d feats of %d dim into %d clusters' % (\n      allfeats.shape[0], allfeats.shape[1], args['nclusters']))\n    # V.IMP to normalize (since the network sees normalized conv output)\n    # Previous resnet code was not doing it, which might be a bug\n    allfeats = normalize(allfeats)\n    kmeans = KMeans(args['nclusters'], n_jobs=args['njobs'])\n    kmeans.fit(allfeats)\n    with open(args['outfpath'], 'wb') as fout:\n        pickle.dump(kmeans, fout)\n\n\nif __name__ == '__main__':\n    main()\n"""
vlad_utils/train_test_svm.py,0,"b""# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\nimport sklearn\nimport sklearn.linear_model\nimport numpy as np\nimport h5py\nimport subprocess\n\ntrain_path = '/nfs.yoda/rgirdhar/Work/Data2/018_VideoVLAD/processed/002_Snapshots/002_NetTFSlimNetVLAD/007_VGG_Places365_pretrained/Features/v2/netvlad_beforeReLU_k32.h5'\nval_path = '/nfs.yoda/rgirdhar/Work/Data2/018_VideoVLAD/processed/002_Snapshots/002_NetTFSlimNetVLAD/007_VGG_Places365_pretrained/Features/v2/netvlad_beforeReLU_k32_VAL.h5'\nMAX_TRAIN_SAMPLES = 30000\n\nwith h5py.File(train_path, 'r') as fin:\n  train_feats = fin['feats'].value[:MAX_TRAIN_SAMPLES, ...]\n  train_labels = fin['labels'].value[:MAX_TRAIN_SAMPLES, ...]\n\nclf = sklearn.svm.LinearSVC(C=1)\n# clf = learn.LinearClassifier(n_classes=101)\n# clf = sklearn.svm.SVC(C=1, kernel='linear', probability=False, decision_function_shape='ovr')\n# clf = sklearn.linear_model.LogisticRegression()\nclf.fit(train_feats, train_labels)\ndel train_feats\n# clf.fit(train_feats, train_labels, steps=20000, batch_size=32)\n\nwith h5py.File(val_path, 'r') as fin:\n  val_feats = fin['feats'].value\n  val_labels = fin['labels'].value\nres = clf.predict(val_feats)\n\nprint 'acc: %f' % (np.mean(res == np.array(val_labels)))\n\nsubprocess.call('mkdir -p /tmp/clf_stor/', shell=True)\nclf.save('/tmp/clf_stor')\n\n"""
vlad_utils/train_test_svm_ucf.py,0,"b""# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\nimport sklearn\nimport sklearn.linear_model\nimport numpy as np\nimport h5py\nimport subprocess\nfrom sklearn.preprocessing import StandardScaler\n\n# train_path = '/nfs.yoda/rgirdhar/Work/Data2/018_VideoVLAD/processed/002_Snapshots/002_NetTFSlimNetVLAD/028_VGG_UCF101_pretrained_netvlad/Features/train.h5'\ntrain_path = '/nfs.yoda/rgirdhar/Work/Data2/018_VideoVLAD/processed/002_Snapshots/002_NetTFSlimNetVLAD/030_Debug/Features/train.h5'\ntrain_list = '/home/rgirdhar/Work/Data/018_VideoVLAD/raw/UCF101/Lists/trainlist01_withCounts.txt'\n# test_path = '/nfs.yoda/rgirdhar/Work/Data2/018_VideoVLAD/processed/002_Snapshots/002_NetTFSlimNetVLAD/028_VGG_UCF101_pretrained_netvlad/Features/test.h5'\ntest_path = '/nfs.yoda/rgirdhar/Work/Data2/018_VideoVLAD/processed/002_Snapshots/002_NetTFSlimNetVLAD/030_Debug/Features/test.h5'\ntest_list = '/home/rgirdhar/Work/Data/018_VideoVLAD/raw/UCF101/Lists/testlist01_withCounts.txt'\n\ndef readData(h5_path, txt_path, feature='netvlad'):\n  with h5py.File(h5_path, 'r') as fin:\n    feats = fin[feature].value\n  with open(txt_path, 'r') as fin:\n    labels = [int(el.split()[-1]) for el in fin.read().splitlines()]\n  return feats, labels\n\ntrain_feats, train_labels = readData(train_path, train_list)\ntest_feats, test_labels = readData(test_path, test_list)\n\nscaler = StandardScaler()\ntrain_feats = scaler.fit_transform(train_feats)\ntest_feats = scaler.transform(test_feats)\n\nclf = sklearn.svm.LinearSVC(C=1)\n# clf = learn.LinearClassifier(n_classes=101)\n# clf = sklearn.svm.SVC(C=1, kernel='linear', probability=False, decision_function_shape='ovr')\n# clf = sklearn.linear_model.LogisticRegression()\nclf.fit(train_feats, train_labels)\ndel train_feats\n# clf.fit(train_feats, train_labels, steps=20000, batch_size=32)\n\nres = clf.predict(test_feats)\n\nprint 'acc: %f' % (np.mean(res == np.array(test_labels)))\n\n"""
datasets/utils/rename_ucf_flow.py,0,"b'# ------------------------------------------------------------------------------\n# ActionVLAD: Learning spatio-temporal aggregation for action classification\n# Copyright (c) 2017 Carnegie Mellon University and Adobe Systems Incorporated\n# Please see LICENSE on https://github.com/rohitgirdhar/ActionVLAD/ for details\n# ------------------------------------------------------------------------------\n## Utility to convert downloaded Limin Wang flow into the same format as I use\n## with rgb images\n## http://mmlab.siat.ac.cn/very_deep_two_stream_model/ucf101_flow_img_tvl1_gpu.zip\n\nimport subprocess\nimport shutil\nimport os\n\nlwang_flow_dir = \\\n  \'/scratch/rgirdhar/Datasets/Video/001_UCF101/processed/Flow/ucf101_flow_img_tvl1_gpu/\'\nout_flow_dir = \\\n  \'/scratch/rgirdhar/Datasets/Video/001_UCF101/processed/Flow/renamed/\'\n\nvidlist = \\\n  \'/data/rgirdhar/Data2/Projects/2016/001_NetVLADVideo/raw/UCF101/Lists/AllVideos_withLabel.txt\'\n\nbuggy = [\'Rafting/v_Rafting_g01_c02.avi\']\n\nwith open(vidlist, \'r\') as fin:\n  for line in fin:\n    fname, count, _ = line.split()\n    count = int(count)\n    if fname in buggy:\n      continue\n    outdir = \'%s/%s\' % (out_flow_dir, fname)\n    if os.path.exists(outdir):\n      continue\n    subprocess.call(\'mkdir -p %s\' % (outdir), shell=True)\n    for i in range(1, count):\n      for d in [\'x\', \'y\']:\n        shutil.copyfile(\'%s/%s/flow_%c_%04d.jpg\' % (\n          lwang_flow_dir, fname[:-4], d, i),\n          \'%s/%s/flow_%c_%05d.jpg\' % (\n            out_flow_dir, fname, d, i))\n\n# and rename the buggy one in bash\n# for i in {0..211}; do cur=`printf ""%04d_y.jpg"" $i`; target=`printf ""flow_y_%05d.jpg"" $(($i+1))`; mv $cur $target; done\n'"
