file_path,api_count,code
configure.py,0,"b'import argparse\nimport sys\n\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser()\n\n    # Data loading params\n    parser.add_argument(""--train_path"", default=""SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT"",\n                        type=str, help=""Path of train data"")\n    parser.add_argument(""--test_path"", default=""SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT"",\n                        type=str, help=""Path of test data"")\n    parser.add_argument(""--max_sentence_length"", default=90,\n                        type=int, help=""Max sentence length in data"")\n    parser.add_argument(""--dev_sample_percentage"", default=0.1,\n                        type=float, help=""Percentage of the training data to use for validation"")\n\n    # Model Hyper-parameters\n    # Embeddings\n    parser.add_argument(""--embedding_path"", default=None,\n                        type=str, help=""Path of pre-trained word embeddings (word2vec)"")\n    parser.add_argument(""--text_embedding_dim"", default=300,\n                        type=int, help=""Dimensionality of word embedding (default: 300)"")\n    parser.add_argument(""--pos_embedding_dim"", default=50,\n                        type=int, help=""Dimensionality of relative position embedding (default: 50)"")\n    # CNN\n    parser.add_argument(""--filter_sizes"", default=""2,3,4,5"",\n                        type=str, help=""Comma-separated filter sizes (Default: 2,3,4,5)"")\n    parser.add_argument(""--num_filters"", default=128,\n                        type=int, help=""Number of filters per filter size (Default: 128)"")\n\n    # Misc\n    parser.add_argument(""--desc"", default="""",\n                        type=str, help=""Description for model"")\n    parser.add_argument(""--dropout_keep_prob"", default=0.5,\n                        type=float, help=""Dropout keep probability of output layer (default: 0.5)"")\n    parser.add_argument(""--l2_reg_lambda"", default=1e-5,\n                        type=float, help=""L2 regularization lambda (default: 1e-5)"")\n\n    # Training parameters\n    parser.add_argument(""--batch_size"", default=20,\n                        type=int, help=""Batch Size (default: 20)"")\n    parser.add_argument(""--num_epochs"", default=100,\n                        type=int, help=""Number of training epochs (Default: 100)"")\n    parser.add_argument(""--display_every"", default=10,\n                        type=int, help=""Number of iterations to display training information"")\n    parser.add_argument(""--evaluate_every"", default=100,\n                        type=int, help=""Evaluate model on dev set after this many steps (default: 100)"")\n    parser.add_argument(""--num_checkpoints"", default=5,\n                        type=int, help=""Number of checkpoints to store (default: 5)"")\n    parser.add_argument(""--learning_rate"", default=1.0,\n                        type=float, help=""Which learning rate to start with (Default: 1.0)"")\n    parser.add_argument(""--decay_rate"", default=0.9,\n                        type=float, help=""Decay rate for learning rate (Default: 0.9)"")\n\n    # Testing parameters\n    parser.add_argument(""--checkpoint_dir"", default="""",\n                        type=str, help=""Checkpoint directory from training run"")\n\n    # Misc Parameters\n    parser.add_argument(""--allow_soft_placement"", default=True,\n                        type=bool, help=""Allow device soft device placement"")\n    parser.add_argument(""--log_device_placement"", default=False,\n                        type=bool, help=""Log placement of ops on devices"")\n    parser.add_argument(""--gpu_allow_growth"", default=True,\n                        type=bool, help=""Allow gpu memory growth"")\n\n    if len(sys.argv) == 0:\n        parser.print_help()\n        sys.exit(1)\n\n    print("""")\n    args = parser.parse_args()\n    for arg in vars(args):\n        print(""{}={}"".format(arg.upper(), getattr(args, arg)))\n    print("""")\n\n    return args\n\n\nFLAGS = parse_args()\n'"
data_helpers.py,0,"b'import numpy as np\nimport pandas as pd\nimport nltk\nimport re\n\nimport utils\nfrom configure import FLAGS\n\n\ndef clean_str(text):\n    text = text.lower()\n    # Clean the text\n    text = re.sub(r""[^A-Za-z0-9^,!.\\/\'+-=]"", "" "", text)\n    text = re.sub(r""what\'s"", ""what is "", text)\n    text = re.sub(r""that\'s"", ""that is "", text)\n    text = re.sub(r""there\'s"", ""there is "", text)\n    text = re.sub(r""it\'s"", ""it is "", text)\n    text = re.sub(r""\\\'s"", "" "", text)\n    text = re.sub(r""\\\'ve"", "" have "", text)\n    text = re.sub(r""can\'t"", ""can not "", text)\n    text = re.sub(r""n\'t"", "" not "", text)\n    text = re.sub(r""i\'m"", ""i am "", text)\n    text = re.sub(r""\\\'re"", "" are "", text)\n    text = re.sub(r""\\\'d"", "" would "", text)\n    text = re.sub(r""\\\'ll"", "" will "", text)\n    text = re.sub(r"","", "" "", text)\n    text = re.sub(r""\\."", "" "", text)\n    text = re.sub(r""!"", "" ! "", text)\n    text = re.sub(r""\\/"", "" "", text)\n    text = re.sub(r""\\^"", "" ^ "", text)\n    text = re.sub(r""\\+"", "" + "", text)\n    text = re.sub(r""\\-"", "" - "", text)\n    text = re.sub(r""\\="", "" = "", text)\n    text = re.sub(r""\'"", "" "", text)\n    text = re.sub(r""(\\d+)(k)"", r""\\g<1>000"", text)\n    text = re.sub(r"":"", "" : "", text)\n    text = re.sub(r"" e g "", "" eg "", text)\n    text = re.sub(r"" b g "", "" bg "", text)\n    text = re.sub(r"" u s "", "" american "", text)\n    text = re.sub(r""\\0s"", ""0"", text)\n    text = re.sub(r"" 9 11 "", ""911"", text)\n    text = re.sub(r""e - mail"", ""email"", text)\n    text = re.sub(r""j k"", ""jk"", text)\n    text = re.sub(r""\\s{2,}"", "" "", text)\n\n    return text.strip()\n\n\ndef load_data_and_labels(path):\n    data = []\n    lines = [line.strip() for line in open(path)]\n    max_sentence_length = 0\n    for idx in range(0, len(lines), 4):\n        id = lines[idx].split(""\\t"")[0]\n        relation = lines[idx + 1]\n\n        sentence = lines[idx].split(""\\t"")[1][1:-1]\n        sentence = sentence.replace(\'<e1>\', \' _e11_ \')\n        sentence = sentence.replace(\'</e1>\', \' _e12_ \')\n        sentence = sentence.replace(\'<e2>\', \' _e21_ \')\n        sentence = sentence.replace(\'</e2>\', \' _e22_ \')\n\n        sentence = clean_str(sentence)\n        tokens = nltk.word_tokenize(sentence)\n        if max_sentence_length < len(tokens):\n            max_sentence_length = len(tokens)\n        e1 = tokens.index(""e12"") - 1\n        e2 = tokens.index(""e22"") - 1\n        sentence = "" "".join(tokens)\n\n        data.append([id, sentence, e1, e2, relation])\n\n    print(path)\n    print(""max sentence length = {}\\n"".format(max_sentence_length))\n\n    df = pd.DataFrame(data=data, columns=[""id"", ""sentence"", ""e1"", ""e2"", ""relation""])\n\n    pos1, pos2 = get_relative_position(df, FLAGS.max_sentence_length)\n\n    df[\'label\'] = [utils.class2label[r] for r in df[\'relation\']]\n\n    # Text Data\n    x_text = df[\'sentence\'].tolist()\n\n    # Label Data\n    y = df[\'label\']\n    labels_flat = y.values.ravel()\n    labels_count = np.unique(labels_flat).shape[0]\n\n    # convert class labels from scalars to one-hot vectors\n    # 0  => [1 0 0 0 0 ... 0 0 0 0 0]\n    # 1  => [0 1 0 0 0 ... 0 0 0 0 0]\n    # ...\n    # 18 => [0 0 0 0 0 ... 0 0 0 0 1]\n    def dense_to_one_hot(labels_dense, num_classes):\n        num_labels = labels_dense.shape[0]\n        index_offset = np.arange(num_labels) * num_classes\n        labels_one_hot = np.zeros((num_labels, num_classes))\n        labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n        return labels_one_hot\n\n    labels = dense_to_one_hot(labels_flat, labels_count)\n    labels = labels.astype(np.uint8)\n\n    return x_text, labels, pos1, pos2\n\n\ndef get_relative_position(df, max_sentence_length):\n    # Position data\n    pos1 = []\n    pos2 = []\n    for df_idx in range(len(df)):\n        sentence = df.iloc[df_idx][\'sentence\']\n        tokens = nltk.word_tokenize(sentence)\n        e1 = df.iloc[df_idx][\'e1\']\n        e2 = df.iloc[df_idx][\'e2\']\n\n        p1 = """"\n        p2 = """"\n        for word_idx in range(len(tokens)):\n            p1 += str((max_sentence_length - 1) + word_idx - e1) + "" ""\n            p2 += str((max_sentence_length - 1) + word_idx - e2) + "" ""\n        pos1.append(p1)\n        pos2.append(p2)\n\n    return pos1, pos2\n\n\ndef batch_iter(data, batch_size, num_epochs, shuffle=True):\n    """"""\n    Generates a batch iterator for a dataset.\n    """"""\n    data = np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n    for epoch in range(num_epochs):\n        # Shuffle the data at each epoch\n        if shuffle:\n            shuffle_indices = np.random.permutation(np.arange(data_size))\n            shuffled_data = data[shuffle_indices]\n        else:\n            shuffled_data = data\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]\n\n\nif __name__ == ""__main__"":\n    trainFile = \'SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT\'\n    testFile = \'SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT\'\n\n    load_data_and_labels(testFile)\n'"
eval.py,9,"b'import tensorflow as tf\nimport numpy as np\nimport os\nimport subprocess\n\nimport data_helpers\nimport utils\nfrom configure import FLAGS\n\n\ndef eval():\n    with tf.device(\'/cpu:0\'):\n        x_text, y, pos1, pos2 = data_helpers.load_data_and_labels(FLAGS.test_path)\n\n    # Map data into vocabulary\n    text_path = os.path.join(FLAGS.checkpoint_dir, "".."", ""text_vocab"")\n    text_vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(text_path)\n    x = np.array(list(text_vocab_processor.transform(x_text)))\n\n    # Map data into position\n    position_path = os.path.join(FLAGS.checkpoint_dir, "".."", ""pos_vocab"")\n    position_vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(position_path)\n    p1 = np.array(list(position_vocab_processor.transform(pos1)))\n    p2 = np.array(list(position_vocab_processor.transform(pos2)))\n\n    checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=FLAGS.allow_soft_placement,\n            log_device_placement=FLAGS.log_device_placement)\n        session_conf.gpu_options.allow_growth = FLAGS.gpu_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_text = graph.get_operation_by_name(""input_text"").outputs[0]\n            input_p1 = graph.get_operation_by_name(""input_p1"").outputs[0]\n            input_p2 = graph.get_operation_by_name(""input_p2"").outputs[0]\n            # input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n\n            # Tensors we want to evaluate\n            predictions = graph.get_operation_by_name(""output/predictions"").outputs[0]\n\n            # Generate batches for one epoch\n            batches = data_helpers.batch_iter(list(zip(x, p1, p2)), FLAGS.batch_size, 1, shuffle=False)\n\n            # Collect the predictions here\n            preds = []\n            for batch in batches:\n                x_batch, p1_batch, p2_batch = zip(*batch)\n                pred = sess.run(predictions, {input_text: x_batch,\n                                              input_p1: p1_batch,\n                                              input_p2: p2_batch,\n                                              dropout_keep_prob: 1.0})\n                preds.append(pred)\n            preds = np.concatenate(preds)\n            truths = np.argmax(y, axis=1)\n\n            prediction_path = os.path.join(FLAGS.checkpoint_dir, "".."", ""predictions.txt"")\n            truth_path = os.path.join(FLAGS.checkpoint_dir, "".."", ""ground_truths.txt"")\n            prediction_file = open(prediction_path, \'w\')\n            truth_file = open(truth_path, \'w\')\n            for i in range(len(preds)):\n                prediction_file.write(""{}\\t{}\\n"".format(i, utils.label2class[preds[i]]))\n                truth_file.write(""{}\\t{}\\n"".format(i, utils.label2class[truths[i]]))\n            prediction_file.close()\n            truth_file.close()\n\n            perl_path = os.path.join(os.path.curdir,\n                                     ""SemEval2010_task8_all_data"",\n                                     ""SemEval2010_task8_scorer-v1.2"",\n                                     ""semeval2010_task8_scorer-v1.2.pl"")\n            process = subprocess.Popen([""perl"", perl_path, prediction_path, truth_path], stdout=subprocess.PIPE)\n            for line in str(process.communicate()[0].decode(""utf-8"")).split(""\\\\n""):\n                print(line)\n\n\ndef main(_):\n    eval()\n\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
text_cnn.py,35,"b'import tensorflow as tf\n\n\nclass TextCNN:\n    def __init__(self, sequence_length, num_classes,\n                 text_vocab_size, text_embedding_size, pos_vocab_size, pos_embedding_size,\n                 filter_sizes, num_filters, l2_reg_lambda=0.0):\n\n        # Placeholders for input, output and dropout\n        self.input_text = tf.placeholder(tf.int32, shape=[None, sequence_length], name=\'input_text\')\n        self.input_p1 = tf.placeholder(tf.int32, shape=[None, sequence_length], name=\'input_p1\')\n        self.input_p2 = tf.placeholder(tf.int32, shape=[None, sequence_length], name=\'input_p2\')\n        self.input_y = tf.placeholder(tf.float32, shape=[None, num_classes], name=\'input_y\')\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\'dropout_keep_prob\')\n\n        initializer = tf.keras.initializers.glorot_normal\n\n        # Embedding layer\n        with tf.device(\'/cpu:0\'), tf.variable_scope(""text-embedding""):\n            self.W_text = tf.Variable(tf.random_uniform([text_vocab_size, text_embedding_size], -0.25, 0.25), name=""W_text"")\n            self.text_embedded_chars = tf.nn.embedding_lookup(self.W_text, self.input_text)\n            self.text_embedded_chars_expanded = tf.expand_dims(self.text_embedded_chars, -1)\n\n        with tf.device(\'/cpu:0\'), tf.variable_scope(""position-embedding""):\n            self.W_pos = tf.get_variable(""W_pos"", [pos_vocab_size, pos_embedding_size], initializer=initializer())\n            self.p1_embedded_chars = tf.nn.embedding_lookup(self.W_pos, self.input_p1)\n            self.p2_embedded_chars = tf.nn.embedding_lookup(self.W_pos, self.input_p2)\n            self.p1_embedded_chars_expanded = tf.expand_dims(self.p1_embedded_chars, -1)\n            self.p2_embedded_chars_expanded = tf.expand_dims(self.p2_embedded_chars, -1)\n\n        self.embedded_chars_expanded = tf.concat([self.text_embedded_chars_expanded,\n                                                  self.p1_embedded_chars_expanded,\n                                                  self.p2_embedded_chars_expanded], 2)\n        _embedding_size = text_embedding_size + 2*pos_embedding_size\n\n        # Create a convolution + maxpool layer for each filter size\n        pooled_outputs = []\n        for i, filter_size in enumerate(filter_sizes):\n            with tf.variable_scope(""conv-maxpool-%s"" % filter_size):\n                # Convolution Layer\n                conv = tf.layers.conv2d(self.embedded_chars_expanded, num_filters, [filter_size, _embedding_size],\n                                        kernel_initializer=initializer(), activation=tf.nn.relu, name=""conv"")\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(conv, ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                                        strides=[1, 1, 1, 1], padding=\'VALID\', name=""pool"")\n                pooled_outputs.append(pooled)\n\n        # Combine all the pooled features\n        num_filters_total = num_filters * len(filter_sizes)\n        self.h_pool = tf.concat(pooled_outputs, 3)\n        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n\n        # Add dropout\n        with tf.variable_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n\n        # Final scores and predictions\n        with tf.variable_scope(""output""):\n            self.logits = tf.layers.dense(self.h_drop, num_classes, kernel_initializer=initializer())\n            self.predictions = tf.argmax(self.logits, 1, name=""predictions"")\n\n        # Calculate mean cross-entropy loss\n        with tf.variable_scope(""loss""):\n            losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.input_y)\n            self.l2 = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * self.l2\n\n        # Accuracy\n        with tf.name_scope(""accuracy""):\n            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name=""accuracy"")\n'"
train.py,18,"b'import tensorflow as tf\nimport numpy as np\nimport os\nimport datetime\nimport time\n\nfrom text_cnn import TextCNN\nimport data_helpers\nimport utils\nfrom configure import FLAGS\n\nfrom sklearn.metrics import f1_score\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings(""ignore"", category=sklearn.exceptions.UndefinedMetricWarning)\n\n\ndef train():\n    with tf.device(\'/cpu:0\'):\n        x_text, y, pos1, pos2 = data_helpers.load_data_and_labels(FLAGS.train_path)\n\n    # Build vocabulary\n    # Example: x_text[3] = ""A misty <e1>ridge</e1> uprises from the <e2>surge</e2>.""\n    # [\'a misty ridge uprises from the surge <UNK> <UNK> ... <UNK>\']\n    # =>\n    # [27 39 40 41 42  1 43  0  0 ... 0]\n    # dimension = FLAGS.max_sentence_length\n    text_vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(FLAGS.max_sentence_length)\n    x = np.array(list(text_vocab_processor.fit_transform(x_text)))\n    print(""Text Vocabulary Size: {:d}"".format(len(text_vocab_processor.vocabulary_)))\n    print(""x = {0}"".format(x.shape))\n    print(""y = {0}"".format(y.shape))\n    print("""")\n\n    # Example: pos1[3] = [-2 -1  0  1  2   3   4 999 999 999 ... 999]\n    # [95 96 97 98 99 100 101 999 999 999 ... 999]\n    # =>\n    # [11 12 13 14 15  16  21  17  17  17 ...  17]\n    # dimension = MAX_SENTENCE_LENGTH\n    pos_vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(FLAGS.max_sentence_length)\n    pos_vocab_processor.fit(pos1 + pos2)\n    p1 = np.array(list(pos_vocab_processor.transform(pos1)))\n    p2 = np.array(list(pos_vocab_processor.transform(pos2)))\n    print(""Position Vocabulary Size: {:d}"".format(len(pos_vocab_processor.vocabulary_)))\n    print(""position_1 = {0}"".format(p1.shape))\n    print(""position_2 = {0}"".format(p2.shape))\n    print("""")\n\n    # Randomly shuffle data to split into train and test(dev)\n    np.random.seed(10)\n    shuffle_indices = np.random.permutation(np.arange(len(y)))\n    x_shuffled = x[shuffle_indices]\n    p1_shuffled = p1[shuffle_indices]\n    p2_shuffled = p2[shuffle_indices]\n    y_shuffled = y[shuffle_indices]\n\n    # Split train/test set\n    # TODO: This is very crude, should use cross-validation\n    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n    p1_train, p1_dev = p1_shuffled[:dev_sample_index], p1_shuffled[dev_sample_index:]\n    p2_train, p2_dev = p2_shuffled[:dev_sample_index], p2_shuffled[dev_sample_index:]\n    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n    print(""Train/Dev split: {:d}/{:d}\\n"".format(len(y_train), len(y_dev)))\n\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=FLAGS.allow_soft_placement,\n            log_device_placement=FLAGS.log_device_placement)\n        session_conf.gpu_options.allow_growth = FLAGS.gpu_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            cnn = TextCNN(\n                sequence_length=x_train.shape[1],\n                num_classes=y_train.shape[1],\n                text_vocab_size=len(text_vocab_processor.vocabulary_),\n                text_embedding_size=FLAGS.text_embedding_dim,\n                pos_vocab_size=len(pos_vocab_processor.vocabulary_),\n                pos_embedding_size=FLAGS.pos_embedding_dim,\n                filter_sizes=list(map(int, FLAGS.filter_sizes.split("",""))),\n                num_filters=FLAGS.num_filters,\n                l2_reg_lambda=FLAGS.l2_reg_lambda)\n\n            # Define Training procedure\n            global_step = tf.Variable(0, name=""global_step"", trainable=False)\n            optimizer = tf.train.AdadeltaOptimizer(FLAGS.learning_rate, FLAGS.decay_rate, 1e-6)\n            gvs = optimizer.compute_gradients(cnn.loss)\n            capped_gvs = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in gvs]\n            train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n\n            # Output directory for models and summaries\n            timestamp = str(int(time.time()))\n            out_dir = os.path.abspath(os.path.join(os.path.curdir, ""runs"", timestamp))\n            print(""Writing to {}\\n"".format(out_dir))\n\n            # Summaries for loss and accuracy\n            loss_summary = tf.summary.scalar(""loss"", cnn.loss)\n            acc_summary = tf.summary.scalar(""accuracy"", cnn.accuracy)\n\n            # Train Summaries\n            train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Dev summaries\n            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n            dev_summary_dir = os.path.join(out_dir, ""summaries"", ""dev"")\n            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n\n            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n            if not os.path.exists(checkpoint_dir):\n                os.makedirs(checkpoint_dir)\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n\n            # Write vocabulary\n            text_vocab_processor.save(os.path.join(out_dir, ""text_vocab""))\n            pos_vocab_processor.save(os.path.join(out_dir, ""pos_vocab""))\n\n            # Initialize all variables\n            sess.run(tf.global_variables_initializer())\n\n            # Pre-trained word2vec\n            if FLAGS.embedding_path:\n                pretrain_W = utils.load_word2vec(FLAGS.embedding_path, FLAGS.text_embedding_dim, text_vocab_processor)\n                sess.run(cnn.W_text.assign(pretrain_W))\n                print(""Success to load pre-trained word2vec model!\\n"")\n\n            # Generate batches\n            batches = data_helpers.batch_iter(list(zip(x_train, p1_train, p2_train, y_train)),\n                                              FLAGS.batch_size, FLAGS.num_epochs)\n            # Training loop. For each batch...\n            best_f1 = 0.0  # For save checkpoint(model)\n            for batch in batches:\n                x_batch, p1_batch, p2_batch, y_batch = zip(*batch)\n                # Train\n                feed_dict = {\n                    cnn.input_text: x_batch,\n                    cnn.input_p1: p1_batch,\n                    cnn.input_p2: p2_batch,\n                    cnn.input_y: y_batch,\n                    cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n                }\n                _, step, summaries, loss, accuracy = sess.run(\n                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n                train_summary_writer.add_summary(summaries, step)\n\n                # Training log display\n                if step % FLAGS.display_every == 0:\n                    time_str = datetime.datetime.now().isoformat()\n                    print(""{}: step {}, loss {:g}, acc {:g}"".format(time_str, step, loss, accuracy))\n\n                # Evaluation\n                if step % FLAGS.evaluate_every == 0:\n                    print(""\\nEvaluation:"")\n                    feed_dict = {\n                        cnn.input_text: x_dev,\n                        cnn.input_p1: p1_dev,\n                        cnn.input_p2: p2_dev,\n                        cnn.input_y: y_dev,\n                        cnn.dropout_keep_prob: 1.0\n                    }\n                    summaries, loss, accuracy, predictions = sess.run(\n                        [dev_summary_op, cnn.loss, cnn.accuracy, cnn.predictions], feed_dict)\n                    dev_summary_writer.add_summary(summaries, step)\n\n                    time_str = datetime.datetime.now().isoformat()\n                    f1 = f1_score(np.argmax(y_dev, axis=1), predictions, labels=np.array(range(1, 19)), average=""macro"")\n                    print(""{}: step {}, loss {:g}, acc {:g}"".format(time_str, step, loss, accuracy))\n                    print(""[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): {:g}\\n"".format(f1))\n\n                    # Model checkpoint\n                    if best_f1 < f1:\n                        best_f1 = f1\n                        path = saver.save(sess, checkpoint_prefix + ""-{:.3g}"".format(best_f1), global_step=step)\n                        print(""Saved model checkpoint to {}\\n"".format(path))\n\n\ndef main(_):\n    train()\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
utils.py,0,"b'import numpy as np\n\n\nclass2label = {\'Other\': 0,\n               \'Message-Topic(e1,e2)\': 1, \'Message-Topic(e2,e1)\': 2,\n               \'Product-Producer(e1,e2)\': 3, \'Product-Producer(e2,e1)\': 4,\n               \'Instrument-Agency(e1,e2)\': 5, \'Instrument-Agency(e2,e1)\': 6,\n               \'Entity-Destination(e1,e2)\': 7, \'Entity-Destination(e2,e1)\': 8,\n               \'Cause-Effect(e1,e2)\': 9, \'Cause-Effect(e2,e1)\': 10,\n               \'Component-Whole(e1,e2)\': 11, \'Component-Whole(e2,e1)\': 12,\n               \'Entity-Origin(e1,e2)\': 13, \'Entity-Origin(e2,e1)\': 14,\n               \'Member-Collection(e1,e2)\': 15, \'Member-Collection(e2,e1)\': 16,\n               \'Content-Container(e1,e2)\': 17, \'Content-Container(e2,e1)\': 18}\n\nlabel2class = {0: \'Other\',\n               1: \'Message-Topic(e1,e2)\', 2: \'Message-Topic(e2,e1)\',\n               3: \'Product-Producer(e1,e2)\', 4: \'Product-Producer(e2,e1)\',\n               5: \'Instrument-Agency(e1,e2)\', 6: \'Instrument-Agency(e2,e1)\',\n               7: \'Entity-Destination(e1,e2)\', 8: \'Entity-Destination(e2,e1)\',\n               9: \'Cause-Effect(e1,e2)\', 10: \'Cause-Effect(e2,e1)\',\n               11: \'Component-Whole(e1,e2)\', 12: \'Component-Whole(e2,e1)\',\n               13: \'Entity-Origin(e1,e2)\', 14: \'Entity-Origin(e2,e1)\',\n               15: \'Member-Collection(e1,e2)\', 16: \'Member-Collection(e2,e1)\',\n               17: \'Content-Container(e1,e2)\', 18: \'Content-Container(e2,e1)\'}\n\n\ndef load_word2vec(embedding_path, embedding_dim, vocab):\n    # initial matrix with random uniform\n    initW = np.random.randn(len(vocab.vocabulary_), embedding_dim).astype(np.float32) / np.sqrt(len(vocab.vocabulary_))\n    # load any vectors from the word2vec\n    print(""Load word2vec file {0}"".format(embedding_path))\n    with open(embedding_path, ""rb"") as f:\n        header = f.readline()\n        vocab_size, layer_size = map(int, header.split())\n        binary_len = np.dtype(\'float32\').itemsize * layer_size\n        for line in range(vocab_size):\n            word = []\n            while True:\n                ch = f.read(1).decode(\'latin-1\')\n                if ch == \' \':\n                    word = \'\'.join(word)\n                    break\n                if ch != \'\\n\':\n                    word.append(ch)\n            idx = vocab.vocabulary_.get(word)\n            if idx != 0:\n                initW[idx] = np.fromstring(f.read(binary_len), dtype=\'float32\')\n            else:\n                f.read(binary_len)\n    return initW\n'"
