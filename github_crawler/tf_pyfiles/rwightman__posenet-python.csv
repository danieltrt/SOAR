file_path,api_count,code
benchmark.py,1,"b'import tensorflow as tf\nimport time\nimport argparse\nimport os\n\nimport posenet\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model\', type=int, default=101)\nparser.add_argument(\'--image_dir\', type=str, default=\'./images\')\nparser.add_argument(\'--num_images\', type=int, default=1000)\nargs = parser.parse_args()\n\n\ndef main():\n\n    with tf.Session() as sess:\n        model_cfg, model_outputs = posenet.load_model(args.model, sess)\n        output_stride = model_cfg[\'output_stride\']\n        num_images = args.num_images\n\n        filenames = [\n            f.path for f in os.scandir(args.image_dir) if f.is_file() and f.path.endswith((\'.png\', \'.jpg\'))]\n        if len(filenames) > num_images:\n            filenames = filenames[:num_images]\n\n        images = {f: posenet.read_imgfile(f, 1.0, output_stride)[0] for f in filenames}\n\n        start = time.time()\n        for i in range(num_images):\n            heatmaps_result, offsets_result, displacement_fwd_result, displacement_bwd_result = sess.run(\n                model_outputs,\n                feed_dict={\'image:0\': images[filenames[i % len(filenames)]]}\n            )\n\n            output = posenet.decode_multiple_poses(\n                heatmaps_result.squeeze(axis=0),\n                offsets_result.squeeze(axis=0),\n                displacement_fwd_result.squeeze(axis=0),\n                displacement_bwd_result.squeeze(axis=0),\n                output_stride=output_stride,\n                max_pose_detections=10,\n                min_pose_score=0.25)\n\n        print(\'Average FPS:\', num_images / (time.time() - start))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
get_test_images.py,0,"b'import urllib.request\nimport os\nimport argparse\n\nGOOGLE_CLOUD_IMAGE_BUCKET = \'https://storage.googleapis.com/tfjs-models/assets/posenet/\'\n\nTEST_IMAGES = [\n  \'frisbee.jpg\',\n  \'frisbee_2.jpg\',\n  \'backpackman.jpg\',\n  \'boy_doughnut.jpg\',\n  \'soccer.png\',\n  \'with_computer.jpg\',\n  \'snowboard.jpg\',\n  \'person_bench.jpg\',\n  \'skiing.jpg\',\n  \'fire_hydrant.jpg\',\n  \'kyte.jpg\',\n  \'looking_at_computer.jpg\',\n  \'tennis.jpg\',\n  \'tennis_standing.jpg\',\n  \'truck.jpg\',\n  \'on_bus.jpg\',\n  \'tie_with_beer.jpg\',\n  \'baseball.jpg\',\n  \'multi_skiing.jpg\',\n  \'riding_elephant.jpg\',\n  \'skate_park_venice.jpg\',\n  \'skate_park.jpg\',\n  \'tennis_in_crowd.jpg\',\n  \'two_on_bench.jpg\',\n]\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--image_dir\', type=str, default=\'./images\')\nargs = parser.parse_args()\n\n\ndef main():\n    if not os.path.exists(args.image_dir):\n        os.makedirs(args.image_dir)\n\n    for f in TEST_IMAGES:\n        url = os.path.join(GOOGLE_CLOUD_IMAGE_BUCKET, f)\n        print(\'Downloading %s\' % f)\n        urllib.request.urlretrieve(url, os.path.join(args.image_dir, f))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
image_demo.py,1,"b'import tensorflow as tf\nimport cv2\nimport time\nimport argparse\nimport os\n\nimport posenet\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model\', type=int, default=101)\nparser.add_argument(\'--scale_factor\', type=float, default=1.0)\nparser.add_argument(\'--notxt\', action=\'store_true\')\nparser.add_argument(\'--image_dir\', type=str, default=\'./images\')\nparser.add_argument(\'--output_dir\', type=str, default=\'./output\')\nargs = parser.parse_args()\n\n\ndef main():\n\n    with tf.Session() as sess:\n        model_cfg, model_outputs = posenet.load_model(args.model, sess)\n        output_stride = model_cfg[\'output_stride\']\n\n        if args.output_dir:\n            if not os.path.exists(args.output_dir):\n                os.makedirs(args.output_dir)\n\n        filenames = [\n            f.path for f in os.scandir(args.image_dir) if f.is_file() and f.path.endswith((\'.png\', \'.jpg\'))]\n\n        start = time.time()\n        for f in filenames:\n            input_image, draw_image, output_scale = posenet.read_imgfile(\n                f, scale_factor=args.scale_factor, output_stride=output_stride)\n\n            heatmaps_result, offsets_result, displacement_fwd_result, displacement_bwd_result = sess.run(\n                model_outputs,\n                feed_dict={\'image:0\': input_image}\n            )\n\n            pose_scores, keypoint_scores, keypoint_coords = posenet.decode_multiple_poses(\n                heatmaps_result.squeeze(axis=0),\n                offsets_result.squeeze(axis=0),\n                displacement_fwd_result.squeeze(axis=0),\n                displacement_bwd_result.squeeze(axis=0),\n                output_stride=output_stride,\n                max_pose_detections=10,\n                min_pose_score=0.25)\n\n            keypoint_coords *= output_scale\n\n            if args.output_dir:\n                draw_image = posenet.draw_skel_and_kp(\n                    draw_image, pose_scores, keypoint_scores, keypoint_coords,\n                    min_pose_score=0.25, min_part_score=0.25)\n\n                cv2.imwrite(os.path.join(args.output_dir, os.path.relpath(f, args.image_dir)), draw_image)\n\n            if not args.notxt:\n                print()\n                print(""Results for image: %s"" % f)\n                for pi in range(len(pose_scores)):\n                    if pose_scores[pi] == 0.:\n                        break\n                    print(\'Pose #%d, score = %f\' % (pi, pose_scores[pi]))\n                    for ki, (s, c) in enumerate(zip(keypoint_scores[pi, :], keypoint_coords[pi, :, :])):\n                        print(\'Keypoint %s, score = %f, coord = %s\' % (posenet.PART_NAMES[ki], s, c))\n\n        print(\'Average FPS:\', len(filenames) / (time.time() - start))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
webcam_demo.py,1,"b'import tensorflow as tf\nimport cv2\nimport time\nimport argparse\n\nimport posenet\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model\', type=int, default=101)\nparser.add_argument(\'--cam_id\', type=int, default=0)\nparser.add_argument(\'--cam_width\', type=int, default=1280)\nparser.add_argument(\'--cam_height\', type=int, default=720)\nparser.add_argument(\'--scale_factor\', type=float, default=0.7125)\nparser.add_argument(\'--file\', type=str, default=None, help=""Optionally use a video file instead of a live camera"")\nargs = parser.parse_args()\n\n\ndef main():\n    with tf.Session() as sess:\n        model_cfg, model_outputs = posenet.load_model(args.model, sess)\n        output_stride = model_cfg[\'output_stride\']\n\n        if args.file is not None:\n            cap = cv2.VideoCapture(args.file)\n        else:\n            cap = cv2.VideoCapture(args.cam_id)\n        cap.set(3, args.cam_width)\n        cap.set(4, args.cam_height)\n\n        start = time.time()\n        frame_count = 0\n        while True:\n            input_image, display_image, output_scale = posenet.read_cap(\n                cap, scale_factor=args.scale_factor, output_stride=output_stride)\n\n            heatmaps_result, offsets_result, displacement_fwd_result, displacement_bwd_result = sess.run(\n                model_outputs,\n                feed_dict={\'image:0\': input_image}\n            )\n\n            pose_scores, keypoint_scores, keypoint_coords = posenet.decode_multi.decode_multiple_poses(\n                heatmaps_result.squeeze(axis=0),\n                offsets_result.squeeze(axis=0),\n                displacement_fwd_result.squeeze(axis=0),\n                displacement_bwd_result.squeeze(axis=0),\n                output_stride=output_stride,\n                max_pose_detections=10,\n                min_pose_score=0.15)\n\n            keypoint_coords *= output_scale\n\n            # TODO this isn\'t particularly fast, use GL for drawing and display someday...\n            overlay_image = posenet.draw_skel_and_kp(\n                display_image, pose_scores, keypoint_scores, keypoint_coords,\n                min_pose_score=0.15, min_part_score=0.1)\n\n            cv2.imshow(\'posenet\', overlay_image)\n            frame_count += 1\n            if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                break\n\n        print(\'Average FPS: \', frame_count / (time.time() - start))\n\n\nif __name__ == ""__main__"":\n    main()'"
posenet/__init__.py,0,b'from posenet.constants import *\nfrom posenet.decode_multi import decode_multiple_poses\nfrom posenet.model import load_model\nfrom posenet.utils import *\n'
posenet/constants.py,0,"b'\nPART_NAMES = [\n    ""nose"", ""leftEye"", ""rightEye"", ""leftEar"", ""rightEar"", ""leftShoulder"",\n    ""rightShoulder"", ""leftElbow"", ""rightElbow"", ""leftWrist"", ""rightWrist"",\n    ""leftHip"", ""rightHip"", ""leftKnee"", ""rightKnee"", ""leftAnkle"", ""rightAnkle""\n]\n\nNUM_KEYPOINTS = len(PART_NAMES)\n\nPART_IDS = {pn: pid for pid, pn in enumerate(PART_NAMES)}\n\nCONNECTED_PART_NAMES = [\n    (""leftHip"", ""leftShoulder""), (""leftElbow"", ""leftShoulder""),\n    (""leftElbow"", ""leftWrist""), (""leftHip"", ""leftKnee""),\n    (""leftKnee"", ""leftAnkle""), (""rightHip"", ""rightShoulder""),\n    (""rightElbow"", ""rightShoulder""), (""rightElbow"", ""rightWrist""),\n    (""rightHip"", ""rightKnee""), (""rightKnee"", ""rightAnkle""),\n    (""leftShoulder"", ""rightShoulder""), (""leftHip"", ""rightHip"")\n]\n\nCONNECTED_PART_INDICES = [(PART_IDS[a], PART_IDS[b]) for a, b in CONNECTED_PART_NAMES]\n\nLOCAL_MAXIMUM_RADIUS = 1\n\nPOSE_CHAIN = [\n    (""nose"", ""leftEye""), (""leftEye"", ""leftEar""), (""nose"", ""rightEye""),\n    (""rightEye"", ""rightEar""), (""nose"", ""leftShoulder""),\n    (""leftShoulder"", ""leftElbow""), (""leftElbow"", ""leftWrist""),\n    (""leftShoulder"", ""leftHip""), (""leftHip"", ""leftKnee""),\n    (""leftKnee"", ""leftAnkle""), (""nose"", ""rightShoulder""),\n    (""rightShoulder"", ""rightElbow""), (""rightElbow"", ""rightWrist""),\n    (""rightShoulder"", ""rightHip""), (""rightHip"", ""rightKnee""),\n    (""rightKnee"", ""rightAnkle"")\n]\n\nPARENT_CHILD_TUPLES = [(PART_IDS[parent], PART_IDS[child]) for parent, child in POSE_CHAIN]\n\nPART_CHANNELS = [\n  \'left_face\',\n  \'right_face\',\n  \'right_upper_leg_front\',\n  \'right_lower_leg_back\',\n  \'right_upper_leg_back\',\n  \'left_lower_leg_front\',\n  \'left_upper_leg_front\',\n  \'left_upper_leg_back\',\n  \'left_lower_leg_back\',\n  \'right_feet\',\n  \'right_lower_leg_front\',\n  \'left_feet\',\n  \'torso_front\',\n  \'torso_back\',\n  \'right_upper_arm_front\',\n  \'right_upper_arm_back\',\n  \'right_lower_arm_back\',\n  \'left_lower_arm_front\',\n  \'left_upper_arm_front\',\n  \'left_upper_arm_back\',\n  \'left_lower_arm_back\',\n  \'right_hand\',\n  \'right_lower_arm_front\',\n  \'left_hand\'\n]'"
posenet/decode.py,0,"b'import numpy as np\n\nfrom posenet.constants import *\n\n\ndef traverse_to_targ_keypoint(\n        edge_id, source_keypoint, target_keypoint_id, scores, offsets, output_stride, displacements\n):\n    height = scores.shape[0]\n    width = scores.shape[1]\n\n    source_keypoint_indices = np.clip(\n        np.round(source_keypoint / output_stride), a_min=0, a_max=[height - 1, width - 1]).astype(np.int32)\n\n    displaced_point = source_keypoint + displacements[\n        source_keypoint_indices[0], source_keypoint_indices[1], edge_id]\n\n    displaced_point_indices = np.clip(\n        np.round(displaced_point / output_stride), a_min=0, a_max=[height - 1, width - 1]).astype(np.int32)\n\n    score = scores[displaced_point_indices[0], displaced_point_indices[1], target_keypoint_id]\n\n    image_coord = displaced_point_indices * output_stride + offsets[\n        displaced_point_indices[0], displaced_point_indices[1], target_keypoint_id]\n\n    return score, image_coord\n\n\ndef decode_pose(\n        root_score, root_id, root_image_coord,\n        scores,\n        offsets,\n        output_stride,\n        displacements_fwd,\n        displacements_bwd\n):\n    num_parts = scores.shape[2]\n    num_edges = len(PARENT_CHILD_TUPLES)\n\n    instance_keypoint_scores = np.zeros(num_parts)\n    instance_keypoint_coords = np.zeros((num_parts, 2))\n    instance_keypoint_scores[root_id] = root_score\n    instance_keypoint_coords[root_id] = root_image_coord\n\n    for edge in reversed(range(num_edges)):\n        target_keypoint_id, source_keypoint_id = PARENT_CHILD_TUPLES[edge]\n        if (instance_keypoint_scores[source_keypoint_id] > 0.0 and\n                instance_keypoint_scores[target_keypoint_id] == 0.0):\n            score, coords = traverse_to_targ_keypoint(\n                edge,\n                instance_keypoint_coords[source_keypoint_id],\n                target_keypoint_id,\n                scores, offsets, output_stride, displacements_bwd)\n            instance_keypoint_scores[target_keypoint_id] = score\n            instance_keypoint_coords[target_keypoint_id] = coords\n\n    for edge in range(num_edges):\n        source_keypoint_id, target_keypoint_id = PARENT_CHILD_TUPLES[edge]\n        if (instance_keypoint_scores[source_keypoint_id] > 0.0 and\n                instance_keypoint_scores[target_keypoint_id] == 0.0):\n            score, coords = traverse_to_targ_keypoint(\n                edge,\n                instance_keypoint_coords[source_keypoint_id],\n                target_keypoint_id,\n                scores, offsets, output_stride, displacements_fwd)\n            instance_keypoint_scores[target_keypoint_id] = score\n            instance_keypoint_coords[target_keypoint_id] = coords\n\n    return instance_keypoint_scores, instance_keypoint_coords\n'"
posenet/decode_multi.py,0,"b""from posenet.decode import *\nfrom posenet.constants import *\nimport time\nimport scipy.ndimage as ndi\n\n\ndef within_nms_radius(poses, squared_nms_radius, point, keypoint_id):\n    for _, _, pose_coord in poses:\n        if np.sum((pose_coord[keypoint_id] - point) ** 2) <= squared_nms_radius:\n            return True\n    return False\n\n\ndef within_nms_radius_fast(pose_coords, squared_nms_radius, point):\n    if not pose_coords.shape[0]:\n        return False\n    return np.any(np.sum((pose_coords - point) ** 2, axis=1) <= squared_nms_radius)\n\n\ndef get_instance_score(\n        existing_poses, squared_nms_radius,\n        keypoint_scores, keypoint_coords):\n    not_overlapped_scores = 0.\n    for keypoint_id in range(len(keypoint_scores)):\n        if not within_nms_radius(\n                existing_poses, squared_nms_radius,\n                keypoint_coords[keypoint_id], keypoint_id):\n            not_overlapped_scores += keypoint_scores[keypoint_id]\n    return not_overlapped_scores / len(keypoint_scores)\n\n\ndef get_instance_score_fast(\n        exist_pose_coords,\n        squared_nms_radius,\n        keypoint_scores, keypoint_coords):\n\n    if exist_pose_coords.shape[0]:\n        s = np.sum((exist_pose_coords - keypoint_coords) ** 2, axis=2) > squared_nms_radius\n        not_overlapped_scores = np.sum(keypoint_scores[np.all(s, axis=0)])\n    else:\n        not_overlapped_scores = np.sum(keypoint_scores)\n    return not_overlapped_scores / len(keypoint_scores)\n\n\ndef score_is_max_in_local_window(keypoint_id, score, hmy, hmx, local_max_radius, scores):\n    height = scores.shape[0]\n    width = scores.shape[1]\n\n    y_start = max(hmy - local_max_radius, 0)\n    y_end = min(hmy + local_max_radius + 1, height)\n    x_start = max(hmx - local_max_radius, 0)\n    x_end = min(hmx + local_max_radius + 1, width)\n\n    for y in range(y_start, y_end):\n        for x in range(x_start, x_end):\n            if scores[y, x, keypoint_id] > score:\n                return False\n    return True\n\n\ndef build_part_with_score(score_threshold, local_max_radius, scores):\n    parts = []\n    height = scores.shape[0]\n    width = scores.shape[1]\n    num_keypoints = scores.shape[2]\n\n    for hmy in range(height):\n        for hmx in range(width):\n            for keypoint_id in range(num_keypoints):\n                score = scores[hmy, hmx, keypoint_id]\n                if score < score_threshold:\n                    continue\n                if score_is_max_in_local_window(keypoint_id, score, hmy, hmx,\n                                                local_max_radius, scores):\n                    parts.append((\n                        score, keypoint_id, np.array((hmy, hmx))\n                    ))\n    return parts\n\n\ndef build_part_with_score_fast(score_threshold, local_max_radius, scores):\n    parts = []\n    num_keypoints = scores.shape[2]\n    lmd = 2 * local_max_radius + 1\n\n    # NOTE it seems faster to iterate over the keypoints and perform maximum_filter\n    # on each subarray vs doing the op on the full score array with size=(lmd, lmd, 1)\n    for keypoint_id in range(num_keypoints):\n        kp_scores = scores[:, :, keypoint_id].copy()\n        kp_scores[kp_scores < score_threshold] = 0.\n        max_vals = ndi.maximum_filter(kp_scores, size=lmd, mode='constant')\n        max_loc = np.logical_and(kp_scores == max_vals, kp_scores > 0)\n        max_loc_idx = max_loc.nonzero()\n        for y, x in zip(*max_loc_idx):\n            parts.append((\n                scores[y, x, keypoint_id],\n                keypoint_id,\n                np.array((y, x))\n            ))\n\n    return parts\n\n\ndef decode_multiple_poses(\n        scores, offsets, displacements_fwd, displacements_bwd, output_stride,\n        max_pose_detections=10, score_threshold=0.5, nms_radius=20, min_pose_score=0.5):\n\n    pose_count = 0\n    pose_scores = np.zeros(max_pose_detections)\n    pose_keypoint_scores = np.zeros((max_pose_detections, NUM_KEYPOINTS))\n    pose_keypoint_coords = np.zeros((max_pose_detections, NUM_KEYPOINTS, 2))\n\n    squared_nms_radius = nms_radius ** 2\n\n    scored_parts = build_part_with_score_fast(score_threshold, LOCAL_MAXIMUM_RADIUS, scores)\n    scored_parts = sorted(scored_parts, key=lambda x: x[0], reverse=True)\n\n    # change dimensions from (h, w, x) to (h, w, x//2, 2) to allow return of complete coord array\n    height = scores.shape[0]\n    width = scores.shape[1]\n    offsets = offsets.reshape(height, width, 2, -1).swapaxes(2, 3)\n    displacements_fwd = displacements_fwd.reshape(height, width, 2, -1).swapaxes(2, 3)\n    displacements_bwd = displacements_bwd.reshape(height, width, 2, -1).swapaxes(2, 3)\n\n    for root_score, root_id, root_coord in scored_parts:\n        root_image_coords = root_coord * output_stride + offsets[\n            root_coord[0], root_coord[1], root_id]\n\n        if within_nms_radius_fast(\n                pose_keypoint_coords[:pose_count, root_id, :], squared_nms_radius, root_image_coords):\n            continue\n\n        keypoint_scores, keypoint_coords = decode_pose(\n            root_score, root_id, root_image_coords,\n            scores, offsets, output_stride,\n            displacements_fwd, displacements_bwd)\n\n        pose_score = get_instance_score_fast(\n            pose_keypoint_coords[:pose_count, :, :], squared_nms_radius, keypoint_scores, keypoint_coords)\n\n        # NOTE this isn't in the original implementation, but it appears that by initially ordering by\n        # part scores, and having a max # of detections, we can end up populating the returned poses with\n        # lower scored poses than if we discard 'bad' ones and continue (higher pose scores can still come later).\n        # Set min_pose_score to 0. to revert to original behaviour\n        if min_pose_score == 0. or pose_score >= min_pose_score:\n            pose_scores[pose_count] = pose_score\n            pose_keypoint_scores[pose_count, :] = keypoint_scores\n            pose_keypoint_coords[pose_count, :, :] = keypoint_coords\n            pose_count += 1\n\n        if pose_count >= max_pose_detections:\n            break\n\n    return pose_scores, pose_keypoint_scores, pose_keypoint_coords\n"""
posenet/model.py,3,"b""import tensorflow as tf\nimport os\nimport posenet.converter.config\n\nMODEL_DIR = './_models'\nDEBUG_OUTPUT = False\n\n\ndef model_id_to_ord(model_id):\n    if 0 <= model_id < 4:\n        return model_id  # id is already ordinal\n    elif model_id == 50:\n        return 0\n    elif model_id == 75:\n        return 1\n    elif model_id == 100:\n        return 2\n    else:  # 101\n        return 3\n\n\ndef load_config(model_ord):\n    converter_cfg = posenet.converter.config.load_config()\n    checkpoints = converter_cfg['checkpoints']\n    output_stride = converter_cfg['outputStride']\n    checkpoint_name = checkpoints[model_ord]\n\n    model_cfg = {\n        'output_stride': output_stride,\n        'checkpoint_name': checkpoint_name,\n    }\n    return model_cfg\n\n\ndef load_model(model_id, sess, model_dir=MODEL_DIR):\n    model_ord = model_id_to_ord(model_id)\n    model_cfg = load_config(model_ord)\n    model_path = os.path.join(model_dir, 'model-%s.pb' % model_cfg['checkpoint_name'])\n    if not os.path.exists(model_path):\n        print('Cannot find model file %s, converting from tfjs...' % model_path)\n        from posenet.converter.tfjs2python import convert\n        convert(model_ord, model_dir, check=False)\n        assert os.path.exists(model_path)\n\n    with tf.gfile.GFile(model_path, 'rb') as f:\n        graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n    sess.graph.as_default()\n    tf.import_graph_def(graph_def, name='')\n\n    if DEBUG_OUTPUT:\n        graph_nodes = [n for n in graph_def.node]\n        names = []\n        for t in graph_nodes:\n            names.append(t.name)\n            print('Loaded graph node:', t.name)\n\n    offsets = sess.graph.get_tensor_by_name('offset_2:0')\n    displacement_fwd = sess.graph.get_tensor_by_name('displacement_fwd_2:0')\n    displacement_bwd = sess.graph.get_tensor_by_name('displacement_bwd_2:0')\n    heatmaps = sess.graph.get_tensor_by_name('heatmap:0')\n\n    return model_cfg, [heatmaps, offsets, displacement_fwd, displacement_bwd]\n"""
posenet/utils.py,0,"b'import cv2\nimport numpy as np\n\nimport posenet.constants\n\n\ndef valid_resolution(width, height, output_stride=16):\n    target_width = (int(width) // output_stride) * output_stride + 1\n    target_height = (int(height) // output_stride) * output_stride + 1\n    return target_width, target_height\n\n\ndef _process_input(source_img, scale_factor=1.0, output_stride=16):\n    target_width, target_height = valid_resolution(\n        source_img.shape[1] * scale_factor, source_img.shape[0] * scale_factor, output_stride=output_stride)\n    scale = np.array([source_img.shape[0] / target_height, source_img.shape[1] / target_width])\n\n    input_img = cv2.resize(source_img, (target_width, target_height), interpolation=cv2.INTER_LINEAR)\n    input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB).astype(np.float32)\n    input_img = input_img * (2.0 / 255.0) - 1.0\n    input_img = input_img.reshape(1, target_height, target_width, 3)\n    return input_img, source_img, scale\n\n\ndef read_cap(cap, scale_factor=1.0, output_stride=16):\n    res, img = cap.read()\n    if not res:\n        raise IOError(""webcam failure"")\n    return _process_input(img, scale_factor, output_stride)\n\n\ndef read_imgfile(path, scale_factor=1.0, output_stride=16):\n    img = cv2.imread(path)\n    return _process_input(img, scale_factor, output_stride)\n\n\ndef draw_keypoints(\n        img, instance_scores, keypoint_scores, keypoint_coords,\n        min_pose_confidence=0.5, min_part_confidence=0.5):\n    cv_keypoints = []\n    for ii, score in enumerate(instance_scores):\n        if score < min_pose_confidence:\n            continue\n        for ks, kc in zip(keypoint_scores[ii, :], keypoint_coords[ii, :, :]):\n            if ks < min_part_confidence:\n                continue\n            cv_keypoints.append(cv2.KeyPoint(kc[1], kc[0], 10. * ks))\n    out_img = cv2.drawKeypoints(img, cv_keypoints, outImage=np.array([]))\n    return out_img\n\n\ndef get_adjacent_keypoints(keypoint_scores, keypoint_coords, min_confidence=0.1):\n    results = []\n    for left, right in posenet.CONNECTED_PART_INDICES:\n        if keypoint_scores[left] < min_confidence or keypoint_scores[right] < min_confidence:\n            continue\n        results.append(\n            np.array([keypoint_coords[left][::-1], keypoint_coords[right][::-1]]).astype(np.int32),\n        )\n    return results\n\n\ndef draw_skeleton(\n        img, instance_scores, keypoint_scores, keypoint_coords,\n        min_pose_confidence=0.5, min_part_confidence=0.5):\n    out_img = img\n    adjacent_keypoints = []\n    for ii, score in enumerate(instance_scores):\n        if score < min_pose_confidence:\n            continue\n        new_keypoints = get_adjacent_keypoints(\n            keypoint_scores[ii, :], keypoint_coords[ii, :, :], min_part_confidence)\n        adjacent_keypoints.extend(new_keypoints)\n    out_img = cv2.polylines(out_img, adjacent_keypoints, isClosed=False, color=(255, 255, 0))\n    return out_img\n\n\ndef draw_skel_and_kp(\n        img, instance_scores, keypoint_scores, keypoint_coords,\n        min_pose_score=0.5, min_part_score=0.5):\n    out_img = img\n    adjacent_keypoints = []\n    cv_keypoints = []\n    for ii, score in enumerate(instance_scores):\n        if score < min_pose_score:\n            continue\n\n        new_keypoints = get_adjacent_keypoints(\n            keypoint_scores[ii, :], keypoint_coords[ii, :, :], min_part_score)\n        adjacent_keypoints.extend(new_keypoints)\n\n        for ks, kc in zip(keypoint_scores[ii, :], keypoint_coords[ii, :, :]):\n            if ks < min_part_score:\n                continue\n            cv_keypoints.append(cv2.KeyPoint(kc[1], kc[0], 10. * ks))\n\n    out_img = cv2.drawKeypoints(\n        out_img, cv_keypoints, outImage=np.array([]), color=(255, 255, 0),\n        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n    out_img = cv2.polylines(out_img, adjacent_keypoints, isClosed=False, color=(255, 255, 0))\n    return out_img\n'"
posenet/converter/config.py,0,"b'import yaml\nimport os\n\nBASE_DIR = os.path.dirname(__file__)\n\n\ndef load_config(config_name=\'config.yaml\'):\n    cfg_f = open(os.path.join(BASE_DIR, config_name), ""r+"")\n    cfg = yaml.load(cfg_f)\n    return cfg\n'"
posenet/converter/tfjs2python.py,22,"b'import json\nimport struct\nimport tensorflow as tf\nfrom tensorflow.python.tools.freeze_graph import freeze_graph\nimport cv2\nimport numpy as np\nimport os\nimport tempfile\n\nfrom posenet.converter.config import load_config\n\nBASE_DIR = os.path.join(tempfile.gettempdir(), \'_posenet_weights\')\n\n\ndef to_output_strided_layers(convolution_def, output_stride):\n    current_stride = 1\n    rate = 1\n    block_id = 0\n    buff = []\n    for _a in convolution_def:\n        conv_type = _a[0]\n        stride = _a[1]\n        \n        if current_stride == output_stride:\n            layer_stride = 1\n            layer_rate = rate\n            rate *= stride\n        else:\n            layer_stride = stride\n            layer_rate = 1\n            current_stride *= stride\n        \n        buff.append({\n            \'blockId\': block_id,\n            \'convType\': conv_type,\n            \'stride\': layer_stride,\n            \'rate\': layer_rate,\n            \'outputStride\': current_stride\n        })\n        block_id += 1\n\n    return buff\n\n\ndef load_variables(chkpoint, base_dir=BASE_DIR):\n    manifest_path = os.path.join(base_dir, chkpoint, ""manifest.json"")\n    if not os.path.exists(manifest_path):\n        print(\'Weights for checkpoint %s are not downloaded. Downloading to %s ...\' % (chkpoint, base_dir))\n        from posenet.converter.wget import download\n        download(chkpoint, base_dir)\n        assert os.path.exists(manifest_path)\n\n    with open(manifest_path) as f:\n        variables = json.load(f)\n\n    # with tf.variable_scope(None, \'MobilenetV1\'):\n    for x in variables:\n        filename = variables[x][""filename""]\n        byte = open(os.path.join(base_dir, chkpoint, filename), \'rb\').read()\n        fmt = str(int(len(byte) / struct.calcsize(\'f\'))) + \'f\'\n        d = struct.unpack(fmt, byte)\n        d = tf.cast(d, tf.float32)\n        d = tf.reshape(d, variables[x][""shape""])\n        variables[x][""x""] = tf.Variable(d, name=x)\n\n    return variables\n\n\ndef _read_imgfile(path, width, height):\n    img = cv2.imread(path)\n    img = cv2.resize(img, (width, height))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(float)\n    img = img * (2.0 / 255.0) - 1.0\n    return img\n\n\ndef build_network(image, layers, variables):\n\n    def _weights(layer_name):\n        return variables[""MobilenetV1/"" + layer_name + ""/weights""][\'x\']\n\n    def _biases(layer_name):\n        return variables[""MobilenetV1/"" + layer_name + ""/biases""][\'x\']\n\n    def _depthwise_weights(layer_name):\n        return variables[""MobilenetV1/"" + layer_name + ""/depthwise_weights""][\'x\']\n\n    def _conv_to_output(mobile_net_output, output_layer_name):\n        w = tf.nn.conv2d(mobile_net_output, _weights(output_layer_name), [1, 1, 1, 1], padding=\'SAME\')\n        w = tf.nn.bias_add(w, _biases(output_layer_name), name=output_layer_name)\n        return w\n\n    def _conv(inputs, stride, block_id):\n        return tf.nn.relu6(\n            tf.nn.conv2d(inputs, _weights(""Conv2d_"" + str(block_id)), stride, padding=\'SAME\')\n            + _biases(""Conv2d_"" + str(block_id)))\n\n    def _separable_conv(inputs, stride, block_id, dilations):\n        if dilations is None:\n            dilations = [1, 1]\n\n        dw_layer = ""Conv2d_"" + str(block_id) + ""_depthwise""\n        pw_layer = ""Conv2d_"" + str(block_id) + ""_pointwise""\n\n        w = tf.nn.depthwise_conv2d(\n            inputs, _depthwise_weights(dw_layer), stride, \'SAME\', rate=dilations, data_format=\'NHWC\')\n        w = tf.nn.bias_add(w, _biases(dw_layer))\n        w = tf.nn.relu6(w)\n\n        w = tf.nn.conv2d(w, _weights(pw_layer), [1, 1, 1, 1], padding=\'SAME\')\n        w = tf.nn.bias_add(w, _biases(pw_layer))\n        w = tf.nn.relu6(w)\n\n        return w\n\n    x = image\n    buff = []\n    with tf.variable_scope(None, \'MobilenetV1\'):\n\n        for m in layers:\n            stride = [1, m[\'stride\'], m[\'stride\'], 1]\n            rate = [m[\'rate\'], m[\'rate\']]\n            if m[\'convType\'] == ""conv2d"":\n                x = _conv(x, stride, m[\'blockId\'])\n                buff.append(x)\n            elif m[\'convType\'] == ""separableConv"":\n                x = _separable_conv(x, stride, m[\'blockId\'], rate)\n                buff.append(x)\n\n    heatmaps = _conv_to_output(x, \'heatmap_2\')\n    offsets = _conv_to_output(x, \'offset_2\')\n    displacement_fwd = _conv_to_output(x, \'displacement_fwd_2\')\n    displacement_bwd = _conv_to_output(x, \'displacement_bwd_2\')\n    heatmaps = tf.sigmoid(heatmaps, \'heatmap\')\n\n    return heatmaps, offsets, displacement_fwd, displacement_bwd\n\n\ndef convert(model_id, model_dir, check=False):\n    cfg = load_config()\n    checkpoints = cfg[\'checkpoints\']\n    image_size = cfg[\'imageSize\']\n    output_stride = cfg[\'outputStride\']\n    chkpoint = checkpoints[model_id]\n\n    if chkpoint == \'mobilenet_v1_050\':\n        mobile_net_arch = cfg[\'mobileNet50Architecture\']\n    elif chkpoint == \'mobilenet_v1_075\':\n        mobile_net_arch = cfg[\'mobileNet75Architecture\']\n    else:\n        mobile_net_arch = cfg[\'mobileNet100Architecture\']\n\n    width = image_size\n    height = image_size\n\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    cg = tf.Graph()\n    with cg.as_default():\n        layers = to_output_strided_layers(mobile_net_arch, output_stride)\n        variables = load_variables(chkpoint)\n\n        init = tf.global_variables_initializer()\n        with tf.Session() as sess:\n            sess.run(init)\n            saver = tf.train.Saver()\n\n            image_ph = tf.placeholder(tf.float32, shape=[1, None, None, 3], name=\'image\')\n            outputs = build_network(image_ph, layers, variables)\n\n            sess.run(\n                [outputs],\n                feed_dict={\n                    image_ph: [np.ndarray(shape=(height, width, 3), dtype=np.float32)]\n                }\n            )\n\n            save_path = os.path.join(model_dir, \'checkpoints\', \'model-%s.ckpt\' % chkpoint)\n            if not os.path.exists(os.path.dirname(save_path)):\n                os.makedirs(os.path.dirname(save_path))\n            checkpoint_path = saver.save(sess, save_path, write_state=False)\n\n            tf.train.write_graph(cg, model_dir, ""model-%s.pbtxt"" % chkpoint)\n\n            # Freeze graph and write our final model file\n            freeze_graph(\n                input_graph=os.path.join(model_dir, ""model-%s.pbtxt"" % chkpoint),\n                input_saver="""",\n                input_binary=False,\n                input_checkpoint=checkpoint_path,\n                output_node_names=\'heatmap,offset_2,displacement_fwd_2,displacement_bwd_2\',\n                restore_op_name=""save/restore_all"",\n                filename_tensor_name=""save/Const:0"",\n                output_graph=os.path.join(model_dir, ""model-%s.pb"" % chkpoint),\n                clear_devices=True,\n                initializer_nodes="""")\n\n            if check and os.path.exists(""./images/tennis_in_crowd.jpg""):\n                # Result\n                input_image = _read_imgfile(""./images/tennis_in_crowd.jpg"", width, height)\n                input_image = np.array(input_image, dtype=np.float32)\n                input_image = input_image.reshape(1, height, width, 3)\n\n                heatmaps_result, offsets_result, displacement_fwd_result, displacement_bwd_result = sess.run(\n                    outputs,\n                    feed_dict={image_ph: input_image}\n                )\n\n                print(""Test image stats"")\n                print(input_image)\n                print(input_image.shape)\n                print(np.mean(input_image))\n\n                heatmaps_result = heatmaps_result[0]\n\n                print(""Heatmaps"")\n                print(heatmaps_result[0:1, 0:1, :])\n                print(heatmaps_result.shape)\n                print(np.mean(heatmaps_result))\n'"
posenet/converter/wget.py,0,"b'import urllib.request\nimport posixpath\nimport json\nimport zlib\nimport os\n\nfrom posenet.converter.config import load_config\n\nCFG = load_config()\nGOOGLE_CLOUD_STORAGE_DIR = CFG[\'GOOGLE_CLOUD_STORAGE_DIR\']\nCHECKPOINTS = CFG[\'checkpoints\']\nCHK = CFG[\'chk\']\n\n\ndef download_file(checkpoint, filename, base_dir):\n    output_path = os.path.join(base_dir, checkpoint, filename)\n    url = posixpath.join(GOOGLE_CLOUD_STORAGE_DIR, checkpoint, filename)\n    req = urllib.request.Request(url)\n    response = urllib.request.urlopen(req)\n    if response.info().get(\'Content-Encoding\') == \'gzip\':\n        data = zlib.decompress(response.read(), zlib.MAX_WBITS | 32)\n    else:\n        # this path not tested since gzip encoding default on google server\n        # may need additional encoding/text handling if hit in the future\n        data = response.read()\n    with open(output_path, \'wb\') as f:\n        f.write(data)\n\n\ndef download(checkpoint, base_dir=\'./weights/\'):\n    save_dir = os.path.join(base_dir, checkpoint)\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    download_file(checkpoint, \'manifest.json\', base_dir)\n    with open(os.path.join(save_dir, \'manifest.json\'), \'r\') as f:\n        json_dict = json.load(f)\n\n    for x in json_dict:\n        filename = json_dict[x][\'filename\']\n        print(\'Downloading\', filename)\n        download_file(checkpoint, filename, base_dir)\n\n\ndef main():\n    checkpoint = CHECKPOINTS[CHK]\n    download(checkpoint)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
