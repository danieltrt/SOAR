file_path,api_count,code
coco_json_to_sagemaker_json.py,0,"b""import json\nimport os\nimport random\nimport shutil\n\nimages_path = '/Users/ryo/Desktop/test_annotations/raccoon_dataset-master/images';\ncoco_json_path = '/Users/ryo/Desktop/test_annotations/raccoon_dataset-master/annotations.json'\n\nsagemaker_json_path = '/Users/ryo/Desktop/test_annotations/raccoon_dataset-master/sagemaker_tmp';\nto_be_uploaded_to_s3_path = '/Users/ryo/Desktop/test_annotations/raccoon_dataset-master/sagemaker';\nsagemaker_images_path_train = to_be_uploaded_to_s3_path + '/train';\nsagemaker_images_path_val = to_be_uploaded_to_s3_path + '/validation';\nsagemaker_json_path_train = to_be_uploaded_to_s3_path + '/train_annotation';\nsagemaker_json_path_val = to_be_uploaded_to_s3_path + '/validation_annotation';\n\ndef makedir(path):\n    if not os.path.isdir(path):\n        os.mkdir(path)\n\nmakedir(sagemaker_json_path)\nmakedir(to_be_uploaded_to_s3_path)\nmakedir(sagemaker_images_path_train)\nmakedir(sagemaker_images_path_val)\nmakedir(sagemaker_json_path_train)\nmakedir(sagemaker_json_path_val)\n\ndef fixCategoryId(category_id):\n    return category_id - 1;\n\nwith open(coco_json_path) as f:\n    js = json.load(f)\n    images = js['images']\n    categories = js['categories']\n    annotations = js['annotations']\n    for i in images:\n        jsonFile = i['file_name']\n        jsonFile = jsonFile.split('.')[0] + '.json'\n\n        line = {}\n        line['file'] = i['file_name']\n        line['image_size'] = [{\n            'width': int(i['width']),\n            'height': int(i['height']),\n            'depth': 3\n        }]\n        line['annotations'] = []\n        line['categories'] = []\n        for j in annotations:\n            if j['image_id'] == i['id'] and len(j['bbox']) > 0:\n                line['annotations'].append({\n                    'class_id': fixCategoryId(int(j['category_id'])),\n                    'top': int(j['bbox'][1]),\n                    'left': int(j['bbox'][0]),\n                    'width': int(j['bbox'][2]),\n                    'height': int(j['bbox'][3])\n                })\n                class_name = ''\n                for k in categories:\n                    if int(j['category_id']) == k['id']:\n                        class_name = str(k['name'])\n                assert class_name is not ''\n                line['categories'].append({\n                    'class_id': fixCategoryId(int(j['category_id'])),\n                    'name': class_name\n                })\n        if line['annotations']:\n            with open(os.path.join(sagemaker_json_path, jsonFile), 'w') as p:\n                json.dump(line, p)\n\njsons = os.listdir(sagemaker_json_path)\nnum_annotated_files = len(jsons)\ntrain_split_pct = 0.90\nnum_train_jsons = int(num_annotated_files * train_split_pct)\nrandom.seed(0)\nrandom.shuffle(jsons)\ntrain_jsons = jsons[:num_train_jsons]\nval_jsons = jsons[num_train_jsons:]\n\ncount_train = 0\nfor i in train_jsons:\n    file_name = i.split('.')[0]\n    if len(file_name) == 0:\n        continue\n    image_file_path = images_path + '/' + file_name + '.jpg'\n    shutil.copy(image_file_path, sagemaker_images_path_train)\n    shutil.copy(sagemaker_json_path + '/'+i, sagemaker_json_path_train)\n    count_train += 1\n\ncount_val = 0\nfor i in val_jsons:\n    file_name = i.split('.')[0]\n    if len(file_name) == 0:\n        continue\n    image_file_path = images_path + '/' + file_name + '.jpg'\n    shutil.copy(image_file_path, sagemaker_images_path_val)\n    shutil.copy(sagemaker_json_path + '/'+i, sagemaker_json_path_val)\n    count_val += 1\n\nprint ('train_jsons {}'.format(count_train))\nprint ('val_jsons {}'.format(count_val))\n"""
dicom_to_images.py,0,"b""import numpy as np\nimport os, pydicom\nimport PIL.Image\nimport sys\n\ndcm_file = sys.argv[1]\noutput_folder = sys.argv[2]\n\ndef pixelArrayToImage(dcm_filename, frame_idx, pixel_array):\n    shape = pixel_array.shape\n    image_array = pixel_array.astype(float)\n    image_array = (np.maximum(image_array, 0) / image_array.max()) * 255.0\n    image_array = np.uint8(image_array)\n    path_jpg = output_folder + '/' + dcm_filename + '_' + str(frame_idx) + '.jpg'\n    im = PIL.Image.fromarray(image_array)\n    im.save(path_jpg)\n\nds = pydicom.dcmread(dcm_file)\ndcm_filename = os.path.splitext(os.path.basename(dcm_file))[0]\nprint(ds)\nprint(ds.Rows)\nprint(ds.Columns)\nprint(ds.SamplesPerPixel)\nif 'NumberOfFrames' in ds:\n    print(ds.NumberOfFrames)\n\nframe_idx = 0\nif not 'NumberOfFrames' in ds:\n    pixelArrayToImage(dcm_filename, frame_idx, ds.pixel_array)\nelif ds.NumberOfFrames == 1:\n    pixelArrayToImage(dcm_filename, frame_idx, ds.pixel_array)\nelse:\n    for frame_idx, pixel_array in enumerate(ds.pixel_array):\n        pixelArrayToImage(dcm_filename, frame_idx, pixel_array)\n    \n\n\n"""
rectlabel_coco_matterport.py,0,"b'""""""\n\n# Train a new model starting from pre-trained COCO weights\npython matterport_coco_rectlabel.py train --dataset=/Users/ryo/rcam/Mask_RCNN-master/datasets/fashion --annotations=annotations_rle.json --weights=coco\n\n""""""\n\nimport os\nimport sys\nimport time\nimport numpy as np\nimport PIL.Image\n\nfrom pycocotools.coco import COCO\nfrom pycocotools import mask as maskUtils\n\n# Root directory of the project\nROOT_DIR = os.path.abspath(""../../"")\n\n# Import Mask RCNN\nsys.path.append(ROOT_DIR)  # To find local version of the library\nfrom mrcnn.config import Config\nfrom mrcnn import model as modellib, utils\n\n# Path to trained weights file\nCOCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, ""mask_rcnn_coco.h5"")\n\n# Directory to save logs and model checkpoints, if not provided\n# through the command line argument --logs\nDEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, ""logs"")\n\n# Dump mask images for debug\nDUMP_MASK_IMAGES = False\n\n############################################################\n#  Configurations\n############################################################\n\nclass CocoConfig(Config):\n    """"""Configuration for training on MS COCO.\n    Derives from the base Config class and overrides values specific\n    to the COCO dataset.\n    """"""\n    # Give the configuration a recognizable name\n    NAME = ""coco""\n\n    # We use a GPU with 12GB memory, which can fit two images.\n    # Adjust down if you use a smaller GPU.\n    IMAGES_PER_GPU = 2\n\n    # Number of classes (including background)\n    NUM_CLASSES = 1 + 1  # Background + 1 class\n\n    # Number of training steps per epoch\n    STEPS_PER_EPOCH = 100\n\n    # Skip detections with < 90% confidence\n    DETECTION_MIN_CONFIDENCE = 0.5\n\n############################################################\n#  Dataset\n############################################################\n\nclass CocoDataset(utils.Dataset):\n    def load_coco(self, dataset_dir, annotations_file, subset):\n        """"""Load the COCO dataset.\n        dataset_dir: The root directory of the COCO dataset.\n        annotations_file: The COCO JSON file.\n        subset: What to load (train, val)\n        """"""\n        image_dir = ""{}/{}"".format(dataset_dir, subset)\n        coco = COCO(""{}/{}"".format(image_dir, annotations_file))\n        \n        # Load all classes\n        class_ids = sorted(coco.getCatIds())\n        image_ids = []\n        for id in class_ids:\n            image_ids.extend(list(coco.getImgIds(catIds=[id])))\n        # Remove duplicates\n        image_ids = list(set(image_ids))\n\n        # Add classes\n        for i in class_ids:\n            self.add_class(""coco"", i, coco.loadCats(i)[0][""name""])\n\n        # Add images\n        for i in image_ids:\n            self.add_image(\n                ""coco"", image_id=i,\n                path=os.path.join(image_dir, coco.imgs[i][\'file_name\']),\n                width=coco.imgs[i][""width""],\n                height=coco.imgs[i][""height""],\n                annotations=coco.loadAnns(coco.getAnnIds(\n                    imgIds=[i], catIds=class_ids, iscrowd=None)))\n\n    def load_mask(self, image_id):\n        """"""Load instance masks for the given image.\n\n        Different datasets use different ways to store masks. This\n        function converts the different mask format to one format\n        in the form of a bitmap [height, width, instances].\n\n        Returns:\n        masks: A bool array of shape [height, width, instance count] with\n            one mask per instance.\n        class_ids: a 1D array of class IDs of the instance masks.\n        """"""\n        # If not a COCO image, delegate to parent class.\n        image_info = self.image_info[image_id]\n        if image_info[""source""] != ""coco"":\n            return super(CocoDataset, self).load_mask(image_id)\n\n        instance_masks = []\n        class_ids = []\n        annotations = self.image_info[image_id][""annotations""]\n        # Build mask of shape [height, width, instance_count] and list\n        # of class IDs that correspond to each channel of the mask.\n        for idx, annotation in enumerate(annotations):\n            class_id = self.map_source_class_id(\n                ""coco.{}"".format(annotation[\'category_id\']))\n            if class_id:\n                m = self.annToMask(annotation, image_info, idx)\n                instance_masks.append(m)\n                class_ids.append(class_id)\n\n        # Pack instance masks into an array\n        if class_ids:\n            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n            class_ids = np.array(class_ids, dtype=np.int32)\n            return mask, class_ids\n        else:\n            # Call super class to return an empty mask\n            return super(CocoDataset, self).load_mask(image_id)\n\n    def image_reference(self, image_id):\n        """"""Return the path of the image.""""""\n        info = self.image_info[image_id]\n        if info[""source""] == ""coco"":\n            return info[""path""]\n        else:\n            super(CocoDataset, self).image_reference(image_id)\n\n    def annToMask(self, ann, image_info, idx):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        segm = ann[\'segmentation\']\n        if isinstance(segm, list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            height = image_info[""height""]\n            width = image_info[""width""]\n            rles = maskUtils.frPyObjects(segm, height, width)\n            rle = maskUtils.merge(rles)\n            m = maskUtils.decode(rle)\n        else:\n            # rle\n            m = maskUtils.decode(segm)   \n        if DUMP_MASK_IMAGES:\n            m[m > 0] = 255\n            pil_image = PIL.Image.fromarray(m)\n            save_path = image_info[""path""].split(\'.\')[0] + ""_""  + str(idx)+ "".png""\n            pil_image.save(save_path)\n        return m\n\ndef train(model):\n    """"""Train the model.""""""\n    # Training dataset.\n    dataset_train = CocoDataset()\n    dataset_train.load_coco(args.dataset, args.annotations, ""train"")\n    dataset_train.prepare()\n\n    # Validation dataset\n    dataset_val = CocoDataset()\n    dataset_val.load_coco(args.dataset, args.annotations, ""val"")\n    dataset_val.prepare()\n\n    # *** This training schedule is an example. Update to your needs ***\n    # Since we\'re using a very small dataset, and starting from\n    # COCO trained weights, we don\'t need to train too long. Also,\n    # no need to train all layers, just the heads should do it.\n    print(""Training network heads"")\n    model.train(dataset_train, dataset_val,\n                learning_rate=config.LEARNING_RATE,\n                epochs=30,\n                layers=\'heads\')\n\ndef color_splash(image, mask):\n    """"""Apply color splash effect.\n    image: RGB image [height, width, 3]\n    mask: instance segmentation mask [height, width, instance count]\n\n    Returns result image.\n    """"""\n    # Make a grayscale copy of the image. The grayscale copy still\n    # has 3 RGB channels, though.\n    gray = skimage.color.gray2rgb(skimage.color.rgb2gray(image)) * 255\n    # Copy color pixels from the original color image where mask is set\n    if mask.shape[-1] > 0:\n        # We\'re treating all instances as one, so collapse the mask into one layer\n        mask = (np.sum(mask, -1, keepdims=True) >= 1)\n        splash = np.where(mask, image, gray).astype(np.uint8)\n    else:\n        splash = gray.astype(np.uint8)\n    return splash\n    \ndef detect_and_color_splash(model, image_path):\n    assert image_path\n\n    # Run model detection and generate the color splash effect\n    print(""Running on {}"".format(args.image))\n    # Read image\n    image = skimage.io.imread(args.image)\n    # Detect objects\n    r = model.detect([image], verbose=1)[0]\n    # Color splash\n    splash = color_splash(image, r[\'masks\'])\n    # Save output\n    file_name = ""splash_{:%Y%m%dT%H%M%S}.png"".format(datetime.datetime.now())\n    skimage.io.imsave(file_name, splash)\n    print(""Saved to "", file_name)\n\n############################################################\n#  Training\n############################################################\n\nif __name__ == \'__main__\':\n    import argparse\n\n    # Parse command line arguments\n    parser = argparse.ArgumentParser(\n        description=\'Train Mask R-CNN.\')\n    parser.add_argument(""command"",\n                        metavar=""<command>"",\n                        help=""\'train\' or \'inference\'"")\n    parser.add_argument(\'--dataset\', required=True,\n                        metavar=""/path/to/dataset/"",\n                        help=\'Directory of the dataset\')\n    parser.add_argument(\'--annotations\', required=True,\n                        metavar=""annotations.json"",\n                        help=\'The COCO JSON file\')\n    parser.add_argument(\'--weights\', required=True,\n                        metavar=""/path/to/weights.h5"",\n                        help=""Path to weights .h5 file or \'coco\'"")\n    parser.add_argument(\'--logs\', required=False,\n                        default=DEFAULT_LOGS_DIR,\n                        metavar=""/path/to/logs/"",\n                        help=\'Logs and checkpoints directory (default=logs/)\')\n    parser.add_argument(\'--image\', required=False,\n                        metavar=""path or URL to image"",\n                        help=\'Image to apply the inference\')\n    args = parser.parse_args()\n    print(""Command: "", args.command)\n    print(""Weights: "", args.weights)\n    print(""Dataset: "", args.dataset)\n    print(""Annotations: "", args.annotations)\n    print(""Logs: "", args.logs)\n    print(""Image: "", args.image)\n\n    # Configurations\n    if args.command == ""train"":\n        config = CocoConfig()\n    else:\n        class InferenceConfig(CocoConfig):\n            # Set batch size to 1 since we\'ll be running inference on\n            # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n            GPU_COUNT = 1\n            IMAGES_PER_GPU = 1\n        config = InferenceConfig()\n    config.display()\n\n    # Create model\n    if args.command == ""train"":\n        model = modellib.MaskRCNN(mode=""training"", config=config,\n                                  model_dir=args.logs)\n    else:\n        model = modellib.MaskRCNN(mode=""inference"", config=config,\n                                  model_dir=args.logs)\n\n    # Select weights file to load\n    if args.weights.lower() == ""coco"":\n        weights_path = COCO_WEIGHTS_PATH\n        if not os.path.exists(weights_path):\n            utils.download_trained_weights(weights_path)\n    elif args.weights.lower() == ""last"":\n        weights_path = model.find_last()\n    elif args.weights.lower() == ""imagenet"":\n        weights_path = model.get_imagenet_weights()\n    else:\n        weights_path = args.weights\n\n    # Load weights\n    print(""Loading weights "", weights_path)\n    if args.weights.lower() == ""coco"":\n        # Exclude the last layers because they require a matching\n        # number of classes\n        model.load_weights(weights_path, by_name=True, exclude=[\n            ""mrcnn_class_logits"", ""mrcnn_bbox_fc"",\n            ""mrcnn_bbox"", ""mrcnn_mask""])\n    else:\n        model.load_weights(weights_path, by_name=True)\n\n    # Train or inference\n    if args.command == ""train"":\n        train(model)\n    elif args.command == ""inference"":\n        detect_and_color_splash(model, image_path=args.image)\n    else:\n        print(""\'{}\' is not recognized. ""\n              ""Use \'train\' or \'inference\'"".format(args.command))\n'"
rectlabel_create_coco_tf_record.py,8,"b'import hashlib\nimport io\nimport json\nimport os\nimport contextlib2\nimport numpy as np\nimport PIL.Image\n\nfrom pycocotools import mask\nimport tensorflow as tf\n\nfrom object_detection.dataset_tools import tf_record_creation_util\nfrom object_detection.utils import dataset_util\nfrom object_detection.utils import label_map_util\n\nflags = tf.app.flags\nflags.DEFINE_string(\'images_dir\', \'\', \'Full path to images folder.\')\nflags.DEFINE_string(\'train_txt_path\', \'\', \'Full path to train.txt.\')\nflags.DEFINE_string(\'val_txt_path\', \'\', \'Full path to val.txt.\')\nflags.DEFINE_string(\'annotations_file\', \'\', \'Full path to annotations JSON file.\')\nflags.DEFINE_string(\'output_dir\', \'\', \'Full path to output directory.\')\nflags.DEFINE_boolean(\'include_masks\', False, \'To train Mask-RCNN, add --include_masks.\')\nFLAGS = flags.FLAGS\n\nDUMP_MASK_IMAGES = False\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\ndef create_tf_example(image_path,\n                      image,\n                      annotations_list,\n                      category_index,\n                      include_masks=False):\n    image_height = image[\'height\']\n    image_width = image[\'width\']\n    filename = image[\'file_name\']\n    image_id = image[\'id\']\n    with tf.gfile.GFile(image_path, \'rb\') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = PIL.Image.open(encoded_jpg_io)\n    key = hashlib.sha256(encoded_jpg).hexdigest()\n    xmin = []\n    xmax = []\n    ymin = []\n    ymax = []\n    is_crowd = []\n    category_names = []\n    category_ids = []\n    area = []\n    encoded_mask_png = []\n    num_annotations_skipped = 0\n    for idx, object_annotations in enumerate(annotations_list):\n        (x, y, width, height) = tuple(object_annotations[\'bbox\'])\n        if width <= 0 or height <= 0:\n            num_annotations_skipped += 1\n            continue\n        if x + width > image_width or y + height > image_height:\n            num_annotations_skipped += 1\n            continue\n        xmin.append(float(x) / image_width)\n        xmax.append(float(x + width) / image_width)\n        ymin.append(float(y) / image_height)\n        ymax.append(float(y + height) / image_height)\n        is_crowd.append(object_annotations[\'iscrowd\'])\n        category_id = int(object_annotations[\'category_id\'])\n        category_ids.append(category_id)\n        category_names.append(category_index[category_id][\'name\'].encode(\'utf8\'))\n        area.append(object_annotations[\'area\'])\n        if include_masks:\n            segm = object_annotations[\'segmentation\']\n            if isinstance(segm, list):\n                rles = mask.frPyObjects(segm, image_height, image_width)\n                rle = mask.merge(rles)\n                m = mask.decode(rle)\n            else:\n                m = mask.decode(segm) \n            pil_image = PIL.Image.fromarray(m)\n            output_io = io.BytesIO()\n            pil_image.save(output_io, format=\'PNG\')\n            encoded_mask_png.append(output_io.getvalue())            \n            if DUMP_MASK_IMAGES:\n                m[m > 0] = 255\n                pil_image = PIL.Image.fromarray(m)\n                save_path = filename.split(\'.\')[0] + ""_""  + str(idx)+ "".png""\n                save_path = FLAGS.output_dir + \'/\' + filename.split(\'.\')[0] + \'_mask_\' + str(idx)+ \'.png\'\n                pil_image.save(save_path)\n    feature_dict = {\n        \'image/height\': dataset_util.int64_feature(image_height),\n        \'image/width\': dataset_util.int64_feature(image_width),\n        \'image/filename\': dataset_util.bytes_feature(filename.encode(\'utf8\')),\n        \'image/source_id\': dataset_util.bytes_feature(str(image_id).encode(\'utf8\')),\n        \'image/key/sha256\': dataset_util.bytes_feature(key.encode(\'utf8\')),\n        \'image/encoded\': dataset_util.bytes_feature(encoded_jpg),\n        \'image/format\': dataset_util.bytes_feature(\'jpeg\'.encode(\'utf8\')),\n        \'image/object/bbox/xmin\': dataset_util.float_list_feature(xmin),\n        \'image/object/bbox/xmax\': dataset_util.float_list_feature(xmax),\n        \'image/object/bbox/ymin\': dataset_util.float_list_feature(ymin),\n        \'image/object/bbox/ymax\': dataset_util.float_list_feature(ymax),\n        \'image/object/class/label\': dataset_util.int64_list_feature(category_ids),\n        \'image/object/is_crowd\': dataset_util.int64_list_feature(is_crowd),\n        \'image/object/area\': dataset_util.float_list_feature(area),\n    }\n    if include_masks:\n        feature_dict[\'image/object/mask\'] = (dataset_util.bytes_list_feature(encoded_mask_png))\n    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n    return example\n\ndef _create_tf_record_from_coco_annotations(tf_record_close_stack, category_index, images, images_id_list, image_files, annotations_index, output_path, include_masks, num_shards):\n    print(\'# Started \' + output_path)\n    output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(tf_record_close_stack, output_path, num_shards)\n    for idx, image_id in enumerate(images_id_list):\n        image_file = image_files[idx]\n        image_path = os.path.join(FLAGS.images_dir, image_file)\n        print(idx, image_path)\n        if image_id not in annotations_index:\n            print(image_path + \' does not have annotations\')\n            continue\n        annotations_list = annotations_index[image_id]\n        tf_example = create_tf_example(image_path, images[image_id - 1], annotations_list, category_index, include_masks)\n        shard_idx = idx % num_shards\n        output_tfrecords[shard_idx].write(tf_example.SerializeToString())   \n\ndef get_image_filename_list(images):\n    image_filename_list = []\n    for image in images:\n        image_filename_list.append(image[\'file_name\'])\n    return image_filename_list;\n\ndef get_images_id_list(image_filename_list, image_files):\n    images_id_list = []\n    for image_file in image_files:\n        idx = image_filename_list.index(image_file)\n        images_id_list.append(idx + 1)\n    return images_id_list\n\ndef get_annotations_indx(groundtruth_data, images_id_list):\n    annotations_index = {}\n    if \'annotations\' not in groundtruth_data:\n        return annotations_index\n    for annotation in groundtruth_data[\'annotations\']:\n        image_id = annotation[\'image_id\']\n        if image_id not in images_id_list:\n            continue\n        if image_id not in annotations_index:\n            annotations_index[image_id] = []\n        annotations_index[image_id].append(annotation)\n    return annotations_index\n\ndef main(_):\n    if not tf.gfile.IsDirectory(FLAGS.output_dir):\n        tf.gfile.MakeDirs(FLAGS.output_dir)\n    train_images = dataset_util.read_examples_list(FLAGS.train_txt_path)\n    val_images = dataset_util.read_examples_list(FLAGS.val_txt_path)\n    annotations_file = FLAGS.annotations_file\n    train_output_path = os.path.join(FLAGS.output_dir, \'train.record\')\n    val_output_path = os.path.join(FLAGS.output_dir, \'val.record\')\n    with contextlib2.ExitStack() as tf_record_close_stack, tf.gfile.GFile(annotations_file, \'r\') as fid:\n        groundtruth_data = json.load(fid)\n        category_index = label_map_util.create_category_index(groundtruth_data[\'categories\'])\n        images = groundtruth_data[\'images\']\n        image_filename_list = get_image_filename_list(images)\n        train_images_id_list = get_images_id_list(image_filename_list, train_images)\n        val_images_id_list = get_images_id_list(image_filename_list, val_images)\n        train_annotations_index = get_annotations_indx(groundtruth_data, train_images_id_list)\n        val_annotations_index = get_annotations_indx(groundtruth_data, val_images_id_list)\n        _create_tf_record_from_coco_annotations(\n            tf_record_close_stack,\n            category_index,\n            images,\n            train_images_id_list,\n            train_images,\n            train_annotations_index,\n            train_output_path,\n            FLAGS.include_masks,\n            num_shards=1)\n        _create_tf_record_from_coco_annotations(\n            tf_record_close_stack,\n            category_index,\n            images,\n            val_images_id_list,\n            val_images,\n            val_annotations_index,\n            val_output_path,\n            FLAGS.include_masks,\n            num_shards=1)\n        print(\'# Finished.\')\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
rectlabel_create_pascal_tf_record.py,7,"b""import hashlib\nimport io\nimport os\nimport glob\nimport random\nfrom pprint import pprint\n\nfrom lxml import etree\nimport numpy as np\nimport PIL.Image\nimport tensorflow as tf\n\nfrom object_detection.utils import dataset_util\nfrom object_detection.utils import label_map_util\n\nflags = tf.app.flags\nflags.DEFINE_string('images_dir', '', 'Full path to images folder.')\nflags.DEFINE_string('train_txt_path', '', 'Full path to train.txt.')\nflags.DEFINE_string('val_txt_path', '', 'Full path to val.txt.')\nflags.DEFINE_string('annotations_dir', '', 'Full path to annotations directory.')\nflags.DEFINE_string('masks_dir', '', 'Full path to exported mask images folder.')\nflags.DEFINE_string('label_map_path', 'label_map.pbtxt', 'Full path to label map file.')\nflags.DEFINE_string('output_dir', '', 'Full path to output directory.')\nFLAGS = flags.FLAGS\n\ndef getClassId(name, label_map_dict):\n    if name in label_map_dict:\n        return label_map_dict[name]\n    name_split = name.split('-')\n    name = name_split[0]\n    for object_name, object_id in label_map_dict.items():\n        object_name_escape = object_name.replace('-', '_')\n        if object_name_escape == name:\n            return object_id\n    return -1\n\ndef dict_to_tf_example(data, image_path, masks_dir, label_map_dict):\n    with tf.gfile.GFile(image_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = PIL.Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n    key = hashlib.sha256(encoded_jpg).hexdigest()\n    width = int(data['size']['width'])\n    height = int(data['size']['height'])\n    xmin = []\n    ymin = []\n    xmax = []\n    ymax = []\n    classes = []\n    classes_text = []\n    masks = []\n    if 'object' in data:\n        for idx, obj in enumerate(data['object']):\n            name = obj['name']\n            if name is None:\n                name = ''\n            class_id = getClassId(name, label_map_dict)\n            if class_id < 0:\n                print(name + ' is not in the label map.')\n                continue\n            xmin.append(float(obj['bndbox']['xmin']) / width)\n            ymin.append(float(obj['bndbox']['ymin']) / height)\n            xmax.append(float(obj['bndbox']['xmax']) / width)\n            ymax.append(float(obj['bndbox']['ymax']) / height)\n            classes_text.append(name.encode('utf8'))\n            classes.append(class_id)\n            if masks_dir:\n                mask_path = os.path.join(masks_dir, os.path.splitext(data['filename'])[0] + '_object' + str(idx) + '.png')\n                with tf.gfile.GFile(mask_path, 'rb') as fid:\n                    encoded_mask_png = fid.read()\n                encoded_png_io = io.BytesIO(encoded_mask_png)\n                mask = PIL.Image.open(encoded_png_io)\n                if mask.format != 'PNG':\n                    raise ValueError('Mask format not PNG')\n                mask_np = np.asarray(mask)\n                mask_remapped = (mask_np == 255).astype(np.uint8)\n                masks.append(mask_remapped)\n    feature_dict = {\n        'image/height': dataset_util.int64_feature(height),\n        'image/width': dataset_util.int64_feature(width),\n        'image/filename': dataset_util.bytes_feature(data['filename'].encode('utf8')),\n        'image/source_id': dataset_util.bytes_feature(data['filename'].encode('utf8')),\n        'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n        'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n        'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),\n        'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),\n        'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),\n        'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),\n        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n        'image/object/class/label': dataset_util.int64_list_feature(classes),\n    }\n    if masks_dir:\n        encoded_mask_png_list = []\n        for mask in masks:\n            img = PIL.Image.fromarray(mask)\n            output = io.BytesIO()\n            img.save(output, format='PNG')\n            encoded_mask_png_list.append(output.getvalue())\n        feature_dict['image/object/mask'] = (dataset_util.bytes_list_feature(encoded_mask_png_list))\n    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n    return example\n\ndef process_images(image_files, output_path):\n    print('# Started ' + output_path)\n    annotations_dir = FLAGS.annotations_dir\n    label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n    writer = tf.python_io.TFRecordWriter(output_path)\n    for idx, image_file in enumerate(image_files):\n        image_path = os.path.join(FLAGS.images_dir, image_file)\n        print(idx, image_path)\n        annotation_path = os.path.join(annotations_dir, os.path.splitext(image_file)[0] + '.xml')\n        if not os.path.exists(annotation_path):\n            print(annotation_path + ' not exists')\n            continue;\n        with tf.gfile.GFile(annotation_path, 'r') as fid:\n            xml_str = fid.read()\n        xml = etree.fromstring(xml_str)\n        data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n        tf_example = dict_to_tf_example(data, image_path, FLAGS.masks_dir, label_map_dict)\n        writer.write(tf_example.SerializeToString())\n    writer.close()\n\ndef main(_):\n    train_images = dataset_util.read_examples_list(FLAGS.train_txt_path)\n    val_images = dataset_util.read_examples_list(FLAGS.val_txt_path)\n    train_output_path = os.path.join(FLAGS.output_dir, 'train.record')\n    val_output_path = os.path.join(FLAGS.output_dir, 'val.record')\n    process_images(train_images, train_output_path)\n    process_images(val_images, val_output_path)\n    print('# Finished.')\n\nif __name__ == '__main__':\n    tf.app.run()\n"""
train_turicreate.py,0,"b""import turicreate as tc\nimport sys\nimport os\n\ncsv_file = sys.argv[1]\ncsv = tc.SFrame(csv_file)\n\nimage_path = csv['path'][0]\nimage_folder = str(os.path.split(image_path)[0])\n\ndata = tc.image_analysis.load_images(image_folder, recursive=False)\ndata = data.join(csv)\n\nmodel = tc.object_detector.create(data, max_iterations=3)\nmodel.save('mymodel.model')\n\nmodel.export_coreml('mymodel.mlmodel')"""
