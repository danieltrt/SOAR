file_path,api_count,code
corr.py,6,"b""import tensorflow as tf\nimport numpy as np\nimport math\n\nBATCH_SIZE = 8\nHEIGHT = 30\nWIDTH = 60\nCHANNELS = 3\n\nNEIGHBORHOOD_SIZE = 41\nMAX_DISPLACEMENT = int(math.ceil(NEIGHBORHOOD_SIZE / 2.0))\nSTRIDE_2 = 2\n\nassert(STRIDE_2 <= NEIGHBORHOOD_SIZE)\n\n# Define two feature maps\nfmA = tf.ones((BATCH_SIZE, HEIGHT, WIDTH, CHANNELS), dtype=tf.int32)\nfmB = tf.convert_to_tensor(np.random.randint(5, size=(BATCH_SIZE, HEIGHT, WIDTH, CHANNELS)), dtype=tf.int32)\n\ndepth = int(math.floor((2.0 * MAX_DISPLACEMENT + 1) / STRIDE_2) ** 2)\n\nprint 'Output should be size:', (BATCH_SIZE, HEIGHT, WIDTH, depth)\nprint 'Striding at values: ', [e for e in range(-MAX_DISPLACEMENT + 1, MAX_DISPLACEMENT, STRIDE_2)]\n\ndef main():\n    out = []\n    for i in range(-MAX_DISPLACEMENT + 1, MAX_DISPLACEMENT, STRIDE_2): # height\n        for j in range(-MAX_DISPLACEMENT + 1, MAX_DISPLACEMENT, STRIDE_2): # width\n            padded_a = tf.pad(fmA, [[0,0], [0, abs(i)], [0, abs(j)], [0, 0]])\n            padded_b = tf.pad(fmB, [[0, 0], [abs(i), 0], [abs(j), 0], [0, 0]])\n            m = padded_a * padded_b\n\n            height_start_idx = 0 if i <= 0 else i\n            height_end_idx = height_start_idx + HEIGHT\n            width_start_idx = 0 if j <= 0 else j\n            width_end_idx = width_start_idx + WIDTH\n            cut = m[:, height_start_idx:height_end_idx, width_start_idx:width_end_idx, :]\n\n            final = tf.reduce_sum(cut, 3)\n            out.append(final)\n    corr = tf.stack(out, 3)\n    print 'Output size: ', corr.shape\n\nmain()\n"""
test.py,7,"b'import os\nimport tensorflow as tf\nimport numpy as np\nfrom scipy.misc import imread\nimport matplotlib\nfrom src.flowlib import read_flow, flow_to_image\nmatplotlib.use(\'TKAgg\')\nimport matplotlib.pyplot as plt\n\n_preprocessing_ops = tf.load_op_library(\n    tf.resource_loader.get_path_to_datafile(""./src/ops/build/preprocessing.so""))\n\n\ndef display(img, c):\n    plt.subplot(int(\'22\' + str(c + 1)))\n    plt.imshow(img[0, :, :, :])\n\n\ndef main():\n    """"""\n.Input(""image_a: float32"")\n.Input(""image_b: float32"")\n.Attr(""crop: list(int) >= 2"")\n.Attr(""params_a_name: list(string)"")\n.Attr(""params_a_rand_type: list(string)"")\n.Attr(""params_a_exp: list(bool)"")\n.Attr(""params_a_mean: list(float32)"")\n.Attr(""params_a_spread: list(float32)"")\n.Attr(""params_a_prob: list(float32)"")\n.Attr(""params_b_name: list(string)"")\n.Attr(""params_b_rand_type: list(string)"")\n.Attr(""params_b_exp: list(bool)"")\n.Attr(""params_b_mean: list(float32)"")\n.Attr(""params_b_spread: list(float32)"")\n.Attr(""params_b_prob: list(float32)"")\n.Output(""aug_image_a: float32"")\n.Output(""aug_image_b: float32"")\n.Output(""spatial_transform_a: float32"")\n.Output(""inv_spatial_transform_b: float32"")\n    """"""\n\n    crop = [364, 492]\n    params_a_name = [\'translate_x\', \'translate_y\']\n    params_a_rand_type = [\'uniform_bernoulli\', \'uniform_bernoulli\']\n    params_a_exp = [False, False]\n    params_a_mean = [0.0, 0.0]\n    params_a_spread = [0.4, 0.4]\n    params_a_prob = [1.0, 1.0]\n    params_b_name = []\n    params_b_rand_type = []\n    params_b_exp = []\n    params_b_mean = []\n    params_b_spread = []\n    params_b_prob = []\n\n    with tf.Session() as sess:\n        with tf.device(\'/gpu:0\'):\n            image_a = imread(\'./img0.ppm\') / 255.0\n            image_b = imread(\'./img1.ppm\') / 255.0\n            flow = read_flow(\'./flow.flo\')\n\n            image_a_tf = tf.expand_dims(tf.to_float(tf.constant(image_a, dtype=tf.float64)), 0)\n            image_b_tf = tf.expand_dims(tf.to_float(tf.constant(image_b, dtype=tf.float64)), 0)\n\n            preprocess = _preprocessing_ops.data_augmentation(image_a_tf,\n                                                              image_b_tf,\n                                                              crop,\n                                                              params_a_name,\n                                                              params_a_rand_type,\n                                                              params_a_exp,\n                                                              params_a_mean,\n                                                              params_a_spread,\n                                                              params_a_prob,\n                                                              params_b_name,\n                                                              params_b_rand_type,\n                                                              params_b_exp,\n                                                              params_b_mean,\n                                                              params_b_spread,\n                                                              params_b_prob)\n\n            out = sess.run(preprocess)\n            trans = out.spatial_transform_a\n            inv_trans = out.inv_spatial_transform_b\n\n            print trans.shape\n            print inv_trans.shape\n\n            flow_tf = tf.expand_dims(tf.to_float(tf.constant(flow)), 0)\n            aug_flow_tf = _preprocessing_ops.flow_augmentation(flow_tf, trans, inv_trans, crop)\n\n            aug_flow = sess.run(aug_flow_tf)[0, :, :, :]\n\n            # Plot img0, img0aug\n            plt.subplot(321)\n            plt.imshow(image_a)\n            plt.subplot(322)\n            plt.imshow(out.aug_image_a[0, :, :, :])\n\n            # Plot img1, img1aug\n            plt.subplot(323)\n            plt.imshow(image_b)\n            plt.subplot(324)\n            plt.imshow(out.aug_image_b[0, :, :, :])\n\n            # Plot flow, flowaug\n            plt.subplot(325)\n            plt.imshow(flow_to_image(flow))\n            plt.subplot(326)\n            plt.imshow(flow_to_image(aug_flow))\n\n            plt.show()\n\n            # image_b_aug = sess.run(image_b_tf)\n            #\n            # display(np.expand_dims(image_a, 0), 0)\n            # display(np.expand_dims(image_b, 0), 1)\n            # display(image_a_aug, 2)\n            # display(image_b_aug, 3)\n            # plt.show()\n\n            # o = _preprocessing_ops.flow_augmentation(flow, trans, inv_t, [4, 8])\n            # print n[:, :, :]\n            # print n[0, 0, 1], n[0, 0, 0]\n            # print n[1, 0, 1], n[1, 0, 0]\n            # print n[2, 0, 1], n[2, 0, 0]\n            # print \'---\'\n            # print sess.run(o)\n\n            """"""# Goes along width first!!\n            // Caffe, NKHW: ((n * K + k) * H + h) * W + w at point (n, k, h, w)\n            // TF, NHWK: ((n * H + h) * W + w) * K + k at point (n, h, w, k)\n\n            H=5, W=10, K=2\n            n=0, h=1, w=5, k=0\n\n            (2 * 10)                + c\n\n            30      49                  n[0, 1, 5, 0]""""""\n\n\nprint os.getpid()\nraw_input(""Press Enter to continue..."")\nmain()\n\n# Last index is channel!!\n\n#   K\n\n# value 13 should be at [0, 2, 7, 1] aka batch=0, height=1, width=0, channel=0. it is at index=20.\n#\n# items = {\n#     \'N\': [0, 0],\n#     \'H\': [5, 2],\n#     \'W\': [10, 7],\n#     \'K\': [2, 1],\n# }\n#\n# for (i1, v1) in items.iteritems():\n#     for (i2, v2) in items.iteritems():\n#         for (i3, v3) in items.iteritems():\n#             for (i4, v4) in items.iteritems():\n#                 if ((v1[1] * v2[0] + v2[1]) * v3[0] + v3[1]) * v4[0] + v4[1] == 55:\n#                     print \'found it: \', i1, i2, i3, i4\n'"
scripts/convert_fc_to_tfrecords.py,4,"b""import argparse\nimport os\nimport sys\nimport numpy as np\nfrom progressbar import ProgressBar, Percentage, Bar\nfrom scipy.misc import imread\nimport tensorflow as tf\n\nFLAGS = None\n\n# Values defined here: https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html#flyingchairs\nTRAIN = 1\nVAL = 2\n\n\n# https://stackoverflow.com/questions/28013200/reading-middlebury-flow-files-with-python-bytes-array-numpy\ndef open_flo_file(filename):\n    with open(filename, 'rb') as f:\n        magic = np.fromfile(f, np.float32, count=1)\n        if 202021.25 != magic:\n            print 'Magic number incorrect. Invalid .flo file'\n        else:\n            w = np.fromfile(f, np.int32, count=1)\n            h = np.fromfile(f, np.int32, count=1)\n            data = np.fromfile(f, np.float32, count=2*w*h)\n            # Reshape data into 3D array (columns, rows, bands)\n            return np.resize(data, (w[0], h[0], 2))\n\n\n# https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/how_tos/reading_data/convert_to_records.py\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef convert_dataset(indices, name):\n    # Open a TFRRecordWriter\n    filename = os.path.join(FLAGS.out, name + '.tfrecords')\n    writeOpts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n    writer = tf.python_io.TFRecordWriter(filename, options=writeOpts)\n\n    # Load each data sample (image_a, image_b, flow) and write it to the TFRecord\n    count = 0\n    pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(indices)).start()\n    for i in indices:\n        image_a_path = os.path.join(FLAGS.data_dir, '%05d_img1.ppm' % (i + 1))\n        image_b_path = os.path.join(FLAGS.data_dir, '%05d_img2.ppm' % (i + 1))\n        flow_path = os.path.join(FLAGS.data_dir, '%05d_flow.flo' % (i + 1))\n\n        image_a = imread(image_a_path)\n        image_b = imread(image_b_path)\n\n        # Convert from RGB -> BGR\n        image_a = image_a[..., [2, 1, 0]]\n        image_b = image_b[..., [2, 1, 0]]\n\n        # Scale from [0, 255] -> [0.0, 1.0]\n        image_a = image_a / 255.0\n        image_b = image_b / 255.0\n\n        image_a_raw = image_a.tostring()\n        image_b_raw = image_b.tostring()\n        flow_raw = open_flo_file(flow_path).tostring()\n\n        example = tf.train.Example(features=tf.train.Features(feature={\n            'image_a': _bytes_feature(image_a_raw),\n            'image_b': _bytes_feature(image_b_raw),\n            'flow': _bytes_feature(flow_raw)}))\n        writer.write(example.SerializeToString())\n        pbar.update(count + 1)\n        count += 1\n    writer.close()\n\n\ndef main():\n    # Load train/val split into arrays\n    train_val_split = np.loadtxt(FLAGS.train_val_split)\n    train_idxs = np.flatnonzero(train_val_split == TRAIN)\n    val_idxs = np.flatnonzero(train_val_split == VAL)\n\n    # Convert the train and val datasets into .tfrecords format\n    convert_dataset(train_idxs, 'fc_train')\n    convert_dataset(val_idxs, 'fc_val')\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--data_dir',\n        type=str,\n        required=True,\n        help='Directory that includes all .ppm and .flo files in the dataset'\n    )\n    parser.add_argument(\n        '--train_val_split',\n        type=str,\n        required=True,\n        help='Path to text file with train-validation split (1-train, 2-validation)'\n    )\n    parser.add_argument(\n        '--out',\n        type=str,\n        required=True,\n        help='Directory for output .tfrecords files'\n    )\n    FLAGS = parser.parse_args()\n\n    # Verify arguments are valid\n    if not os.path.isdir(FLAGS.data_dir):\n        raise ValueError('data_dir must exist and be a directory')\n    if not os.path.isdir(FLAGS.out):\n        raise ValueError('out must exist and be a directory')\n    if not os.path.exists(FLAGS.train_val_split):\n        raise ValueError('train_val_split must exist')\n    main()\n"""
src/__init__.py,0,b''
src/correlation.py,3,"b'import tensorflow as tf\n\n_correlation_ops = tf.load_op_library(\n    tf.resource_loader.get_path_to_datafile(""./ops/build/correlation.so""))\n\n\ndef correlation(input_a, input_b, kernel_size, max_displacement, stride_1, stride_2, padding):\n    return _correlation_ops.correlation(input_a,\n                                        input_b,\n                                        kernel_size,\n                                        max_displacement,\n                                        stride_1,\n                                        stride_2,\n                                        padding)\n\n\n@tf.RegisterGradient(""Correlation"")\ndef _correlation_grad(corr_op, gradients):\n    kernel_size = corr_op.get_attr(""kernel_size"")\n    max_displacement = corr_op.get_attr(""max_displacement"")\n    stride_1 = corr_op.get_attr(""stride_1"")\n    stride_2 = corr_op.get_attr(""stride_2"")\n    pad = corr_op.get_attr(""pad"")\n\n    corr_grads = _correlation_ops.correlation_grad(gradients,\n                                                   corr_op.inputs[0],\n                                                   corr_op.inputs[1],\n                                                   kernel_size,\n                                                   max_displacement,\n                                                   stride_1,\n                                                   stride_2,\n                                                   pad)\n\n    # Return the gradients with respect to input_a and input_b\n    return corr_grads.backprops_a, corr_grads.backprops_b\n'"
src/dataloader.py,48,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\nimport copy\nslim = tf.contrib.slim\n\n_preprocessing_ops = tf.load_op_library(\n    tf.resource_loader.get_path_to_datafile(""./ops/build/preprocessing.so""))\n\n\n# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py\nclass Image(slim.tfexample_decoder.ItemHandler):\n    """"""An ItemHandler that decodes a parsed Tensor as an image.""""""\n\n    def __init__(self,\n                 image_key=None,\n                 format_key=None,\n                 shape=None,\n                 channels=3,\n                 dtype=tf.uint8,\n                 repeated=False):\n        """"""Initializes the image.\n        Args:\n          image_key: the name of the TF-Example feature in which the encoded image\n            is stored.\n          shape: the output shape of the image as 1-D `Tensor`\n            [height, width, channels]. If provided, the image is reshaped\n            accordingly. If left as None, no reshaping is done. A shape should\n            be supplied only if all the stored images have the same shape.\n          channels: the number of channels in the image.\n          dtype: images will be decoded at this bit depth. Different formats\n            support different bit depths.\n              See tf.image.decode_image,\n                  tf.decode_raw,\n          repeated: if False, decodes a single image. If True, decodes a\n            variable number of image strings from a 1D tensor of strings.\n        """"""\n        if not image_key:\n            image_key = \'image/encoded\'\n\n        super(Image, self).__init__([image_key])\n        self._image_key = image_key\n        self._shape = shape\n        self._channels = channels\n        self._dtype = dtype\n        self._repeated = repeated\n\n    def tensors_to_item(self, keys_to_tensors):\n        """"""See base class.""""""\n        image_buffer = keys_to_tensors[self._image_key]\n\n        if self._repeated:\n            return functional_ops.map_fn(lambda x: self._decode(x),\n                                         image_buffer, dtype=self._dtype)\n        else:\n            return self._decode(image_buffer)\n\n    def _decode(self, image_buffer):\n        """"""Decodes the image buffer.\n        Args:\n          image_buffer: The tensor representing the encoded image tensor.\n        Returns:\n          A tensor that represents decoded image of self._shape, or\n          (?, ?, self._channels) if self._shape is not specified.\n        """"""\n        def decode_raw():\n            """"""Decodes a raw image.""""""\n            return tf.decode_raw(image_buffer, out_type=self._dtype)\n\n        image = decode_raw()\n        # image.set_shape([None, None, self._channels])\n        if self._shape is not None:\n            image = tf.reshape(image, self._shape)\n\n        return image\n\n\ndef __get_dataset(dataset_config, split_name):\n    """"""\n    dataset_config: A dataset_config defined in datasets.py\n    split_name: \'train\'/\'validate\'\n    """"""\n    with tf.name_scope(\'__get_dataset\'):\n        if split_name not in dataset_config[\'SIZES\']:\n            raise ValueError(\'split name %s not recognized\' % split_name)\n\n        IMAGE_HEIGHT, IMAGE_WIDTH = dataset_config[\'IMAGE_HEIGHT\'], dataset_config[\'IMAGE_WIDTH\']\n        reader = tf.TFRecordReader\n        keys_to_features = {\n            \'image_a\': tf.FixedLenFeature((), tf.string),\n            \'image_b\': tf.FixedLenFeature((), tf.string),\n            \'flow\': tf.FixedLenFeature((), tf.string),\n        }\n        items_to_handlers = {\n            \'image_a\': Image(\n                image_key=\'image_a\',\n                dtype=tf.float64,\n                shape=[IMAGE_HEIGHT, IMAGE_WIDTH, 3],\n                channels=3),\n            \'image_b\': Image(\n                image_key=\'image_b\',\n                dtype=tf.float64,\n                shape=[IMAGE_HEIGHT, IMAGE_WIDTH, 3],\n                channels=3),\n            \'flow\': Image(\n                image_key=\'flow\',\n                dtype=tf.float32,\n                shape=[IMAGE_HEIGHT, IMAGE_WIDTH, 2],\n                channels=2),\n        }\n        decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n        return slim.dataset.Dataset(\n            data_sources=dataset_config[\'PATHS\'][split_name],\n            reader=reader,\n            decoder=decoder,\n            num_samples=dataset_config[\'SIZES\'][split_name],\n            items_to_descriptions=dataset_config[\'ITEMS_TO_DESCRIPTIONS\'])\n\n\ndef config_to_arrays(dataset_config):\n    output = {\n        \'name\': [],\n        \'rand_type\': [],\n        \'exp\': [],\n        \'mean\': [],\n        \'spread\': [],\n        \'prob\': [],\n        \'coeff_schedule\': [],\n    }\n    config = copy.deepcopy(dataset_config)\n\n    if \'coeff_schedule_param\' in config:\n        del config[\'coeff_schedule_param\']\n\n    # Get all attributes\n    for (name, value) in config.iteritems():\n        if name == \'coeff_schedule_param\':\n            output[\'coeff_schedule\'] = [value[\'half_life\'],\n                                        value[\'initial_coeff\'],\n                                        value[\'final_coeff\']]\n        else:\n            output[\'name\'].append(name)\n            output[\'rand_type\'].append(value[\'rand_type\'])\n            output[\'exp\'].append(value[\'exp\'])\n            output[\'mean\'].append(value[\'mean\'])\n            output[\'spread\'].append(value[\'spread\'])\n            output[\'prob\'].append(value[\'prob\'])\n\n    return output\n\n\n# https://github.com/tgebru/transform/blob/master/src/caffe/layers/data_augmentation_layer.cpp#L34\ndef _generate_coeff(param, discount_coeff=tf.constant(1.0), default_value=tf.constant(0.0)):\n    if not all(name in param for name in [\'rand_type\', \'exp\', \'mean\', \'spread\', \'prob\']):\n        raise RuntimeError(\'Expected rand_type, exp, mean, spread, prob in `param`\')\n\n    rand_type = param[\'rand_type\']\n    exp = float(param[\'exp\'])\n    mean = tf.convert_to_tensor(param[\'mean\'], dtype=tf.float32)\n    spread = float(param[\'spread\'])  # AKA standard deviation\n    prob = float(param[\'prob\'])\n\n    # Multiply spread by our discount_coeff so it changes over time\n    spread = spread * discount_coeff\n\n    if rand_type == \'uniform\':\n        value = tf.cond(spread > 0.0,\n                        lambda: tf.random_uniform([], mean - spread, mean + spread),\n                        lambda: mean)\n        if exp:\n            value = tf.exp(value)\n    elif rand_type == \'gaussian\':\n        value = tf.cond(spread > 0.0,\n                        lambda: tf.random_normal([], mean, spread),\n                        lambda: mean)\n        if exp:\n            value = tf.exp(value)\n    elif rand_type == \'bernoulli\':\n        if prob > 0.0:\n            value = tf.contrib.distributions.Bernoulli(probs=prob).sample([])\n        else:\n            value = 0.0\n    elif rand_type == \'uniform_bernoulli\':\n        tmp1 = 0.0\n        tmp2 = 0\n        if prob > 0.0:\n            tmp2 = tf.contrib.distributions.Bernoulli(probs=prob).sample([])\n        else:\n            tmp2 = 0\n\n        if tmp2 == 0:\n            if default_value is not None:\n                return default_value\n        else:\n            tmp1 = tf.cond(spread > 0.0,\n                           lambda: tf.random_uniform([], mean - spread, mean + spread),\n                           lambda: mean)\n        if exp:\n            tmp1 = tf.exp(tmp1)\n        value = tmp1\n    elif rand_type == \'gaussian_bernoulli\':\n        tmp1 = 0.0\n        tmp2 = 0\n        if prob > 0.0:\n            tmp2 = tf.contrib.distributions.Bernoulli(probs=prob).sample([])\n        else:\n            tmp2 = 0\n\n        if tmp2 == 0:\n            if default_value is not None:\n                return default_value\n        else:\n            tmp1 = tf.cond(spread > 0.0,\n                           lambda: tf.random_normal([], mean, spread),\n                           lambda: mean)\n        if exp:\n            tmp1 = tf.exp(tmp1)\n        value = tmp1\n    else:\n        raise ValueError(\'Unknown distribution type %s.\' % rand_type)\n    return value\n\n\ndef load_batch(dataset_config, split_name, global_step):\n    num_threads = 32\n    reader_kwargs = {\'options\': tf.python_io.TFRecordOptions(\n        tf.python_io.TFRecordCompressionType.ZLIB)}\n\n    with tf.name_scope(\'load_batch\'):\n        dataset = __get_dataset(dataset_config, split_name)\n        data_provider = slim.dataset_data_provider.DatasetDataProvider(\n            dataset,\n            num_readers=num_threads,\n            common_queue_capacity=2048,\n            common_queue_min=1024,\n            reader_kwargs=reader_kwargs)\n        image_a, image_b, flow = data_provider.get([\'image_a\', \'image_b\', \'flow\'])\n        image_a, image_b, flow = map(tf.to_float, [image_a, image_b, flow])\n\n        if dataset_config[\'PREPROCESS\'][\'scale\']:\n            image_a = image_a / 255.0\n            image_b = image_b / 255.0\n\n        crop = [dataset_config[\'PREPROCESS\'][\'crop_height\'],\n                dataset_config[\'PREPROCESS\'][\'crop_width\']]\n        config_a = config_to_arrays(dataset_config[\'PREPROCESS\'][\'image_a\'])\n        config_b = config_to_arrays(dataset_config[\'PREPROCESS\'][\'image_b\'])\n\n        image_as, image_bs, flows = map(lambda x: tf.expand_dims(x, 0), [image_a, image_b, flow])\n\n        # Perform data augmentation on GPU\n        with tf.device(\'/cpu:0\'):\n            image_as, image_bs, transforms_from_a, transforms_from_b = \\\n                _preprocessing_ops.data_augmentation(image_as,\n                                                     image_bs,\n                                                     global_step,\n                                                     crop,\n                                                     config_a[\'name\'],\n                                                     config_a[\'rand_type\'],\n                                                     config_a[\'exp\'],\n                                                     config_a[\'mean\'],\n                                                     config_a[\'spread\'],\n                                                     config_a[\'prob\'],\n                                                     config_a[\'coeff_schedule\'],\n                                                     config_b[\'name\'],\n                                                     config_b[\'rand_type\'],\n                                                     config_b[\'exp\'],\n                                                     config_b[\'mean\'],\n                                                     config_b[\'spread\'],\n                                                     config_b[\'prob\'],\n                                                     config_b[\'coeff_schedule\'])\n\n            noise_coeff_a = None\n            noise_coeff_b = None\n\n            # Generate and apply noise coeff for A if defined in A params\n            if \'noise\' in dataset_config[\'PREPROCESS\'][\'image_a\']:\n                discount_coeff = tf.constant(1.0)\n                if \'coeff_schedule_param\' in dataset_config[\'PREPROCESS\'][\'image_a\']:\n                    initial_coeff = dataset_config[\'PREPROCESS\'][\'image_a\'][\'coeff_schedule_param\'][\'initial_coeff\']\n                    final_coeff = dataset_config[\'PREPROCESS\'][\'image_a\'][\'coeff_schedule_param\'][\'final_coeff\']\n                    half_life = dataset_config[\'PREPROCESS\'][\'image_a\'][\'coeff_schedule_param\'][\'half_life\']\n                    discount_coeff = initial_coeff + \\\n                        (final_coeff - initial_coeff) * \\\n                        (2.0 / (1.0 + exp(-1.0986 * global_step / half_life)) - 1.0)\n\n                noise_coeff_a = _generate_coeff(\n                    dataset_config[\'PREPROCESS\'][\'image_a\'][\'noise\'], discount_coeff)\n                noise_a = tf.random_normal(shape=tf.shape(image_as),\n                                           mean=0.0, stddev=noise_coeff_a,\n                                           dtype=tf.float32)\n                image_as = tf.clip_by_value(image_as + noise_a, 0.0, 1.0)\n\n            # Generate noise coeff for B if defined in B params\n            if \'noise\' in dataset_config[\'PREPROCESS\'][\'image_b\']:\n                discount_coeff = tf.constant(1.0)\n                if \'coeff_schedule_param\' in dataset_config[\'PREPROCESS\'][\'image_b\']:\n                    initial_coeff = dataset_config[\'PREPROCESS\'][\'image_b\'][\'coeff_schedule_param\'][\'initial_coeff\']\n                    final_coeff = dataset_config[\'PREPROCESS\'][\'image_b\'][\'coeff_schedule_param\'][\'final_coeff\']\n                    half_life = dataset_config[\'PREPROCESS\'][\'image_b\'][\'coeff_schedule_param\'][\'half_life\']\n                    discount_coeff = initial_coeff + \\\n                        (final_coeff - initial_coeff) * \\\n                        (2.0 / (1.0 + exp(-1.0986 * global_step / half_life)) - 1.0)\n                noise_coeff_b = _generate_coeff(\n                    dataset_config[\'PREPROCESS\'][\'image_b\'][\'noise\'], discount_coeff)\n\n            # Combine coeff from a with coeff from b\n            if noise_coeff_a is not None:\n                if noise_coeff_b is not None:\n                    noise_coeff_b = noise_coeff_a * noise_coeff_b\n                else:\n                    noise_coeff_b = noise_coeff_a\n\n            # Add noise to B if needed\n            if noise_coeff_b is not None:\n                noise_b = tf.random_normal(shape=tf.shape(image_bs),\n                                           mean=0.0, stddev=noise_coeff_b,\n                                           dtype=tf.float32)\n                image_bs = tf.clip_by_value(image_bs + noise_b, 0.0, 1.0)\n\n                # Perform flow augmentation using spatial parameters from data augmentation\n            flows = _preprocessing_ops.flow_augmentation(\n                flows, transforms_from_a, transforms_from_b, crop)\n\n            return tf.train.batch([image_as, image_bs, flows],\n                                  enqueue_many=True,\n                                  batch_size=dataset_config[\'BATCH_SIZE\'],\n                                  capacity=dataset_config[\'BATCH_SIZE\'] * 4,\n                                  num_threads=num_threads,\n                                  allow_smaller_final_batch=False)\n'"
src/dataset_configs.py,0,"b'""""""\nAdd dataset configurations here. Each dataset must have the following structure:\n\nNAME = {\n    IMAGE_HEIGHT: int,\n    IMAGE_WIDTH: int,\n    ITEMS_TO_DESCRIPTIONS: {\n        \'image_a\': \'A 3-channel image.\',\n        \'image_b\': \'A 3-channel image.\',\n        \'flow\': \'A 2-channel optical flow field\',\n    },\n    SIZES: {\n        \'train\': int,\n        \'validate\': int,    (optional)\n        ...\n    },\n    BATCH_SIZE: int,\n    PATHS: {\n        \'train\': \'\',\n        \'validate\': \'\', (optional)\n        ...\n    }\n}\n""""""\n\n""""""\nnote that one step = one batch of data processed, ~not~ an entire epoch\n\'coeff_schedule_param\': {\n    \'half_life\': 50000,         after this many steps, the value will be i + (f - i)/2\n    \'initial_coeff\': 0.5,       initial value\n    \'final_coeff\': 1,           final value\n},\n""""""\n\nFLYING_CHAIRS_DATASET_CONFIG = {\n    \'IMAGE_HEIGHT\': 384,\n    \'IMAGE_WIDTH\': 512,\n    \'ITEMS_TO_DESCRIPTIONS\': {\n        \'image_a\': \'A 3-channel image.\',\n        \'image_b\': \'A 3-channel image.\',\n        \'flow\': \'A 2-channel optical flow field\',\n    },\n    \'SIZES\': {\n        \'train\': 22232,\n        \'validate\': 640,\n        \'sample\': 8,\n    },\n    \'BATCH_SIZE\': 8,\n    \'PATHS\': {\n        \'train\': \'./data/tfrecords/fc_train.tfrecords\',\n        \'validate\': \'./data/tfrecords/fc_val.tfrecords\',\n        \'sample\': \'./data/tfrecords/fc_sample.tfrecords\',\n    },\n    \'PREPROCESS\': {\n        \'scale\': False,\n        \'crop_height\': 320,\n        \'crop_width\': 448,\n        \'image_a\': {\n            \'translate\': {\n                \'rand_type\': ""uniform_bernoulli"",\n                \'exp\': False,\n                \'mean\': 0,\n                \'spread\': 0.4,\n                \'prob\': 1.0,\n            },\n            \'rotate\': {\n                \'rand_type\': ""uniform_bernoulli"",\n                \'exp\': False,\n                \'mean\': 0,\n                \'spread\': 0.4,\n                \'prob\': 1.0,\n            },\n            \'zoom\': {\n                \'rand_type\': ""uniform_bernoulli"",\n                \'exp\': True,\n                \'mean\': 0.2,\n                \'spread\': 0.4,\n                \'prob\': 1.0,\n            },\n            \'squeeze\': {\n                \'rand_type\': ""uniform_bernoulli"",\n                \'exp\': True,\n                \'mean\': 0,\n                \'spread\': 0.3,\n                \'prob\': 1.0,\n            },\n            \'noise\': {\n                \'rand_type\': ""uniform_bernoulli"",\n                \'exp\': False,\n                \'mean\': 0.03,\n                \'spread\': 0.03,\n                \'prob\': 1.0,\n            },\n        },\n        # All preprocessing to image A will be applied to image B in addition to the following.\n        \'image_b\': {\n            \'translate\': {\n                \'rand_type\': ""gaussian_bernoulli"",\n                \'exp\': False,\n                \'mean\': 0,\n                \'spread\': 0.03,\n                \'prob\': 1.0,\n            },\n            \'rotate\': {\n                \'rand_type\': ""gaussian_bernoulli"",\n                \'exp\': False,\n                \'mean\': 0,\n                \'spread\': 0.03,\n                \'prob\': 1.0,\n            },\n            \'zoom\': {\n                \'rand_type\': ""gaussian_bernoulli"",\n                \'exp\': True,\n                \'mean\': 0,\n                \'spread\': 0.03,\n                \'prob\': 1.0,\n            },\n            \'gamma\': {\n                \'rand_type\': ""gaussian_bernoulli"",\n                \'exp\': True,\n                \'mean\': 0,\n                \'spread\': 0.02,\n                \'prob\': 1.0,\n            },\n            \'brightness\': {\n                \'rand_type\': ""gaussian_bernoulli"",\n                \'exp\': False,\n                \'mean\': 0,\n                \'spread\': 0.02,\n                \'prob\': 1.0,\n            },\n            \'contrast\': {\n                \'rand_type\': ""gaussian_bernoulli"",\n                \'exp\': True,\n                \'mean\': 0,\n                \'spread\': 0.02,\n                \'prob\': 1.0,\n            },\n            \'color\': {\n                \'rand_type\': ""gaussian_bernoulli"",\n                \'exp\': True,\n                \'mean\': 0,\n                \'spread\': 0.02,\n                \'prob\': 1.0,\n            },\n            \'coeff_schedule_param\': {\n                \'half_life\': 50000,\n                \'initial_coeff\': 0.5,\n                \'final_coeff\': 1,\n            },\n        }\n    },\n}\n'"
src/downsample.py,2,"b'import tensorflow as tf\n\n_downsample = tf.load_op_library(\n    tf.resource_loader.get_path_to_datafile(""./ops/build/downsample.so""))\n\n\ndef downsample(tensor, size):\n    return _downsample.downsample(tensor, size)\n'"
src/flow_warp.py,3,"b'import tensorflow as tf\n\n_flow_warp_ops = tf.load_op_library(\n    tf.resource_loader.get_path_to_datafile(""./ops/build/flow_warp.so""))\n\n\ndef flow_warp(image, flow):\n    return _flow_warp_ops.flow_warp(image, flow)\n\n\n@tf.RegisterGradient(""FlowWarp"")\ndef _flow_warp_grad(flow_warp_op, gradients):\n    return _flow_warp_ops.flow_warp_grad(flow_warp_op.inputs[0],\n                                         flow_warp_op.inputs[1],\n                                         gradients)\n'"
src/flowlib.py,0,"b'#!/usr/bin/python\n""""""\n# ==============================\n# flowlib.py\n# library for optical flow processing\n# Author: Ruoteng Li\n# Date: 6th Aug 2016\n# ==============================\n""""""\nimport png\nimport numpy as np\nimport matplotlib.colors as cl\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n\nUNKNOWN_FLOW_THRESH = 1e7\nSMALLFLOW = 0.0\nLARGEFLOW = 1e8\n\n""""""\n=============\nFlow Section\n=============\n""""""\n\n\ndef show_flow(filename):\n    """"""\n    visualize optical flow map using matplotlib\n    :param filename: optical flow file\n    :return: None\n    """"""\n    flow = read_flow(filename)\n    img = flow_to_image(flow)\n    plt.imshow(img)\n    plt.show()\n\n\ndef visualize_flow(flow, mode=\'Y\'):\n    """"""\n    this function visualize the input flow\n    :param flow: input flow in array\n    :param mode: choose which color mode to visualize the flow (Y: Ccbcr, RGB: RGB color)\n    :return: None\n    """"""\n    if mode == \'Y\':\n        # Ccbcr color wheel\n        img = flow_to_image(flow)\n        plt.imshow(img)\n        plt.show()\n    elif mode == \'RGB\':\n        (h, w) = flow.shape[0:2]\n        du = flow[:, :, 0]\n        dv = flow[:, :, 1]\n        valid = flow[:, :, 2]\n        max_flow = max(np.max(du), np.max(dv))\n        img = np.zeros((h, w, 3), dtype=np.float64)\n        # angle layer\n        img[:, :, 0] = np.arctan2(dv, du) / (2 * np.pi)\n        # magnitude layer, normalized to 1\n        img[:, :, 1] = np.sqrt(du * du + dv * dv) * 8 / max_flow\n        # phase layer\n        img[:, :, 2] = 8 - img[:, :, 1]\n        # clip to [0,1]\n        small_idx = img[:, :, 0:3] < 0\n        large_idx = img[:, :, 0:3] > 1\n        img[small_idx] = 0\n        img[large_idx] = 1\n        # convert to rgb\n        img = cl.hsv_to_rgb(img)\n        # remove invalid point\n        img[:, :, 0] = img[:, :, 0] * valid\n        img[:, :, 1] = img[:, :, 1] * valid\n        img[:, :, 2] = img[:, :, 2] * valid\n        # show\n        plt.imshow(img)\n        plt.show()\n\n    return None\n\n\ndef read_flow(filename):\n    """"""\n    read optical flow from Middlebury .flo file\n    :param filename: name of the flow file\n    :return: optical flow data in matrix\n    """"""\n    f = open(filename, \'rb\')\n    magic = np.fromfile(f, np.float32, count=1)\n    data2d = None\n\n    if 202021.25 != magic:\n        print \'Magic number incorrect. Invalid .flo file\'\n    else:\n        w = np.fromfile(f, np.int32, count=1)\n        h = np.fromfile(f, np.int32, count=1)\n        print ""Reading %d x %d flo file"" % (h, w)\n        data2d = np.fromfile(f, np.float32, count=2 * w * h)\n        # reshape data into 3D array (columns, rows, channels)\n        data2d = np.resize(data2d, (h[0], w[0], 2))\n    f.close()\n    return data2d\n\n\ndef read_flow_png(flow_file):\n    """"""\n    Read optical flow from KITTI .png file\n    :param flow_file: name of the flow file\n    :return: optical flow data in matrix\n    """"""\n    flow_object = png.Reader(filename=flow_file)\n    flow_direct = flow_object.asDirect()\n    flow_data = list(flow_direct[2])\n    (w, h) = flow_direct[3][\'size\']\n    flow = np.zeros((h, w, 3), dtype=np.float64)\n    for i in range(len(flow_data)):\n        flow[i, :, 0] = flow_data[i][0::3]\n        flow[i, :, 1] = flow_data[i][1::3]\n        flow[i, :, 2] = flow_data[i][2::3]\n\n    invalid_idx = (flow[:, :, 2] == 0)\n    flow[:, :, 0:2] = (flow[:, :, 0:2] - 2 ** 15) / 64.0\n    flow[invalid_idx, 0] = 0\n    flow[invalid_idx, 1] = 0\n    return flow\n\n\ndef write_flow(flow, filename):\n    """"""\n    write optical flow in Middlebury .flo format\n    :param flow: optical flow map\n    :param filename: optical flow file path to be saved\n    :return: None\n    """"""\n    f = open(filename, \'wb\')\n    magic = np.array([202021.25], dtype=np.float32)\n    (height, width) = flow.shape[0:2]\n    w = np.array([width], dtype=np.int32)\n    h = np.array([height], dtype=np.int32)\n    magic.tofile(f)\n    w.tofile(f)\n    h.tofile(f)\n    flow.tofile(f)\n    f.close()\n\n\ndef segment_flow(flow):\n    h = flow.shape[0]\n    w = flow.shape[1]\n    u = flow[:, :, 0]\n    v = flow[:, :, 1]\n\n    idx = ((abs(u) > LARGEFLOW) | (abs(v) > LARGEFLOW))\n    idx2 = (abs(u) == SMALLFLOW)\n    class0 = (v == 0) & (u == 0)\n    u[idx2] = 0.00001\n    tan_value = v / u\n\n    class1 = (tan_value < 1) & (tan_value >= 0) & (u > 0) & (v >= 0)\n    class2 = (tan_value >= 1) & (u >= 0) & (v >= 0)\n    class3 = (tan_value < -1) & (u <= 0) & (v >= 0)\n    class4 = (tan_value < 0) & (tan_value >= -1) & (u < 0) & (v >= 0)\n    class8 = (tan_value >= -1) & (tan_value < 0) & (u > 0) & (v <= 0)\n    class7 = (tan_value < -1) & (u >= 0) & (v <= 0)\n    class6 = (tan_value >= 1) & (u <= 0) & (v <= 0)\n    class5 = (tan_value >= 0) & (tan_value < 1) & (u < 0) & (v <= 0)\n\n    seg = np.zeros((h, w))\n\n    seg[class1] = 1\n    seg[class2] = 2\n    seg[class3] = 3\n    seg[class4] = 4\n    seg[class5] = 5\n    seg[class6] = 6\n    seg[class7] = 7\n    seg[class8] = 8\n    seg[class0] = 0\n    seg[idx] = 0\n\n    return seg\n\n\ndef flow_error(tu, tv, u, v):\n    """"""\n    Calculate average end point error\n    :param tu: ground-truth horizontal flow map\n    :param tv: ground-truth vertical flow map\n    :param u:  estimated horizontal flow map\n    :param v:  estimated vertical flow map\n    :return: End point error of the estimated flow\n    """"""\n    smallflow = 0.0\n    \'\'\'\n    stu = tu[bord+1:end-bord,bord+1:end-bord]\n    stv = tv[bord+1:end-bord,bord+1:end-bord]\n    su = u[bord+1:end-bord,bord+1:end-bord]\n    sv = v[bord+1:end-bord,bord+1:end-bord]\n    \'\'\'\n    stu = tu[:]\n    stv = tv[:]\n    su = u[:]\n    sv = v[:]\n\n    idxUnknow = (abs(stu) > UNKNOWN_FLOW_THRESH) | (abs(stv) > UNKNOWN_FLOW_THRESH)\n    stu[idxUnknow] = 0\n    stv[idxUnknow] = 0\n    su[idxUnknow] = 0\n    sv[idxUnknow] = 0\n\n    ind2 = [(np.absolute(stu) > smallflow) | (np.absolute(stv) > smallflow)]\n    index_su = su[ind2]\n    index_sv = sv[ind2]\n    an = 1.0 / np.sqrt(index_su ** 2 + index_sv ** 2 + 1)\n    un = index_su * an\n    vn = index_sv * an\n\n    index_stu = stu[ind2]\n    index_stv = stv[ind2]\n    tn = 1.0 / np.sqrt(index_stu ** 2 + index_stv ** 2 + 1)\n    tun = index_stu * tn\n    tvn = index_stv * tn\n\n    \'\'\'\n    angle = un * tun + vn * tvn + (an * tn)\n    index = [angle == 1.0]\n    angle[index] = 0.999\n    ang = np.arccos(angle)\n    mang = np.mean(ang)\n    mang = mang * 180 / np.pi\n    \'\'\'\n\n    epe = np.sqrt((stu - su) ** 2 + (stv - sv) ** 2)\n    epe = epe[ind2]\n    mepe = np.mean(epe)\n    return mepe\n\n\ndef flow_to_image(flow):\n    """"""\n    Convert flow into middlebury color code image\n    :param flow: optical flow map\n    :return: optical flow image in middlebury color\n    """"""\n    u = flow[:, :, 0]\n    v = flow[:, :, 1]\n\n    maxu = -999.\n    maxv = -999.\n    minu = 999.\n    minv = 999.\n\n    idxUnknow = (abs(u) > UNKNOWN_FLOW_THRESH) | (abs(v) > UNKNOWN_FLOW_THRESH)\n    u[idxUnknow] = 0\n    v[idxUnknow] = 0\n\n    maxu = max(maxu, np.max(u))\n    minu = min(minu, np.min(u))\n\n    maxv = max(maxv, np.max(v))\n    minv = min(minv, np.min(v))\n\n    rad = np.sqrt(u ** 2 + v ** 2)\n    maxrad = max(-1, np.max(rad))\n\n    print ""max flow: %.4f\\nflow range:\\nu = %.3f .. %.3f\\nv = %.3f .. %.3f"" % (maxrad, minu,maxu, minv, maxv)\n\n    u = u/(maxrad + np.finfo(float).eps)\n    v = v/(maxrad + np.finfo(float).eps)\n\n    img = compute_color(u, v)\n\n    idx = np.repeat(idxUnknow[:, :, np.newaxis], 3, axis=2)\n    img[idx] = 0\n\n    return np.uint8(img)\n\n\ndef evaluate_flow_file(gt, pred):\n    """"""\n    evaluate the estimated optical flow end point error according to ground truth provided\n    :param gt: ground truth file path\n    :param pred: estimated optical flow file path\n    :return: end point error, float32\n    """"""\n    # Read flow files and calculate the errors\n    gt_flow = read_flow(gt)        # ground truth flow\n    eva_flow = read_flow(pred)     # predicted flow\n    # Calculate errors\n    average_pe = flow_error(gt_flow[:, :, 0], gt_flow[:, :, 1], eva_flow[:, :, 0], eva_flow[:, :, 1])\n    return average_pe\n\n\ndef evaluate_flow(gt_flow, pred_flow):\n    """"""\n    gt: ground-truth flow\n    pred: estimated flow\n    """"""\n    average_pe = flow_error(gt_flow[:, :, 0], gt_flow[:, :, 1], pred_flow[:, :, 0], pred_flow[:, :, 1])\n    return average_pe\n\n\n""""""\n==============\nDisparity Section\n==============\n""""""\n\n\ndef read_disp_png(file_name):\n    """"""\n    Read optical flow from KITTI .png file\n    :param file_name: name of the flow file\n    :return: optical flow data in matrix\n    """"""\n    image_object = png.Reader(filename=file_name)\n    image_direct = image_object.asDirect()\n    image_data = list(image_direct[2])\n    (w, h) = image_direct[3][\'size\']\n    channel = len(image_data[0]) / w\n    flow = np.zeros((h, w, channel), dtype=np.uint16)\n    for i in range(len(image_data)):\n        for j in range(channel):\n            flow[i, :, j] = image_data[i][j::channel]\n    return flow[:, :, 0] / 256\n\n\ndef disp_to_flowfile(disp, filename):\n    """"""\n    Read KITTI disparity file in png format\n    :param disp: disparity matrix\n    :param filename: the flow file name to save\n    :return: None\n    """"""\n    f = open(filename, \'wb\')\n    magic = np.array([202021.25], dtype=np.float32)\n    (height, width) = disp.shape[0:2]\n    w = np.array([width], dtype=np.int32)\n    h = np.array([height], dtype=np.int32)\n    empty_map = np.zeros((height, width), dtype=np.float32)\n    data = np.dstack((disp, empty_map))\n    magic.tofile(f)\n    w.tofile(f)\n    h.tofile(f)\n    data.tofile(f)\n    f.close()\n\n\n""""""\n==============\nImage Section\n==============\n""""""\n\n\ndef read_image(filename):\n    """"""\n    Read normal image of any format\n    :param filename: name of the image file\n    :return: image data in matrix uint8 type\n    """"""\n    img = Image.open(filename)\n    im = np.array(img)\n    return im\n\n\ndef warp_image(im, flow):\n    """"""\n    Use optical flow to warp image to the next\n    :param im: image to warp\n    :param flow: optical flow\n    :return: warped image\n    """"""\n    from scipy import interpolate\n    image_height = im.shape[0]\n    image_width = im.shape[1]\n    flow_height = flow.shape[0]\n    flow_width = flow.shape[1]\n    n = image_height * image_width\n    (iy, ix) = np.mgrid[0:image_height, 0:image_width]\n    (fy, fx) = np.mgrid[0:flow_height, 0:flow_width]\n    fx += flow[:,:,0]\n    fy += flow[:,:,1]\n    mask = np.logical_or(fx <0 , fx > flow_width)\n    mask = np.logical_or(mask, fy < 0)\n    mask = np.logical_or(mask, fy > flow_height)\n    fx = np.minimum(np.maximum(fx, 0), flow_width)\n    fy = np.minimum(np.maximum(fy, 0), flow_height)\n    points = np.concatenate((ix.reshape(n,1), iy.reshape(n,1)), axis=1)\n    xi = np.concatenate((fx.reshape(n, 1), fy.reshape(n,1)), axis=1)\n    warp = np.zeros((image_height, image_width, im.shape[2]))\n    for i in range(im.shape[2]):\n        channel = im[:, :, i]\n        plt.imshow(channel, cmap=\'gray\')\n        values = channel.reshape(n, 1)\n        new_channel = interpolate.griddata(points, values, xi, method=\'cubic\')\n        new_channel = np.reshape(new_channel, [flow_height, flow_width])\n        new_channel[mask] = 1\n        warp[:, :, i] = new_channel.astype(np.uint8)\n\n    return warp.astype(np.uint8)\n\n\n""""""\n==============\nOthers\n==============\n""""""\n\ndef scale_image(image, new_range):\n    """"""\n    Linearly scale the image into desired range\n    :param image: input image\n    :param new_range: the new range to be aligned\n    :return: image normalized in new range\n    """"""\n    min_val = np.min(image).astype(np.float32)\n    max_val = np.max(image).astype(np.float32)\n    min_val_new = np.array(min(new_range), dtype=np.float32)\n    max_val_new = np.array(max(new_range), dtype=np.float32)\n    scaled_image = (image - min_val) / (max_val - min_val) * (max_val_new - min_val_new) + min_val_new\n    return scaled_image.astype(np.uint8)\n\n\ndef compute_color(u, v):\n    """"""\n    compute optical flow color map\n    :param u: optical flow horizontal map\n    :param v: optical flow vertical map\n    :return: optical flow in color code\n    """"""\n    [h, w] = u.shape\n    img = np.zeros([h, w, 3])\n    nanIdx = np.isnan(u) | np.isnan(v)\n    u[nanIdx] = 0\n    v[nanIdx] = 0\n\n    colorwheel = make_color_wheel()\n    ncols = np.size(colorwheel, 0)\n\n    rad = np.sqrt(u**2+v**2)\n\n    a = np.arctan2(-v, -u) / np.pi\n\n    fk = (a+1) / 2 * (ncols - 1) + 1\n\n    k0 = np.floor(fk).astype(int)\n\n    k1 = k0 + 1\n    k1[k1 == ncols+1] = 1\n    f = fk - k0\n\n    for i in range(0, np.size(colorwheel,1)):\n        tmp = colorwheel[:, i]\n        col0 = tmp[k0-1] / 255\n        col1 = tmp[k1-1] / 255\n        col = (1-f) * col0 + f * col1\n\n        idx = rad <= 1\n        col[idx] = 1-rad[idx]*(1-col[idx])\n        notidx = np.logical_not(idx)\n\n        col[notidx] *= 0.75\n        img[:, :, i] = np.uint8(np.floor(255 * col*(1-nanIdx)))\n\n    return img\n\n\ndef make_color_wheel():\n    """"""\n    Generate color wheel according Middlebury color code\n    :return: Color wheel\n    """"""\n    RY = 15\n    YG = 6\n    GC = 4\n    CB = 11\n    BM = 13\n    MR = 6\n\n    ncols = RY + YG + GC + CB + BM + MR\n\n    colorwheel = np.zeros([ncols, 3])\n\n    col = 0\n\n    # RY\n    colorwheel[0:RY, 0] = 255\n    colorwheel[0:RY, 1] = np.transpose(np.floor(255*np.arange(0, RY) / RY))\n    col += RY\n\n    # YG\n    colorwheel[col:col+YG, 0] = 255 - np.transpose(np.floor(255*np.arange(0, YG) / YG))\n    colorwheel[col:col+YG, 1] = 255\n    col += YG\n\n    # GC\n    colorwheel[col:col+GC, 1] = 255\n    colorwheel[col:col+GC, 2] = np.transpose(np.floor(255*np.arange(0, GC) / GC))\n    col += GC\n\n    # CB\n    colorwheel[col:col+CB, 1] = 255 - np.transpose(np.floor(255*np.arange(0, CB) / CB))\n    colorwheel[col:col+CB, 2] = 255\n    col += CB\n\n    # BM\n    colorwheel[col:col+BM, 2] = 255\n    colorwheel[col:col+BM, 0] = np.transpose(np.floor(255*np.arange(0, BM) / BM))\n    col += + BM\n\n    # MR\n    colorwheel[col:col+MR, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, MR) / MR))\n    colorwheel[col:col+MR, 0] = 255\n\n    return colorwheel\n'"
src/net.py,27,"b'import abc\nfrom enum import Enum\nimport os\nimport tensorflow as tf\nfrom .flowlib import flow_to_image, write_flow\nimport numpy as np\nfrom scipy.misc import imread, imsave\nimport uuid\nfrom .training_schedules import LONG_SCHEDULE\nslim = tf.contrib.slim\n\n\nclass Mode(Enum):\n    TRAIN = 1\n    TEST = 2\n\n\nclass Net(object):\n    __metaclass__ = abc.ABCMeta\n\n    def __init__(self, mode=Mode.TRAIN, debug=False):\n        self.global_step = slim.get_or_create_global_step()\n        self.mode = mode\n        self.debug = debug\n\n    @abc.abstractmethod\n    def model(self, inputs, training_schedule, trainable=True):\n        """"""\n        Defines the model and returns a tuple of Tensors needed for calculating the loss.\n        """"""\n        return\n\n    @abc.abstractmethod\n    def loss(self, **kwargs):\n        """"""\n        Accepts prediction Tensors from the output of `model`.\n        Returns a single Tensor representing the total loss of the model.\n        """"""\n        return\n\n    def test(self, checkpoint, input_a_path, input_b_path, out_path, save_image=True, save_flo=False):\n        input_a = imread(input_a_path)\n        input_b = imread(input_b_path)\n\n        # Convert from RGB -> BGR\n        input_a = input_a[..., [2, 1, 0]]\n        input_b = input_b[..., [2, 1, 0]]\n\n        # Scale from [0, 255] -> [0.0, 1.0] if needed\n        if input_a.max() > 1.0:\n            input_a = input_a / 255.0\n        if input_b.max() > 1.0:\n            input_b = input_b / 255.0\n\n        # TODO: This is a hack, we should get rid of this\n        training_schedule = LONG_SCHEDULE\n\n        inputs = {\n            \'input_a\': tf.expand_dims(tf.constant(input_a, dtype=tf.float32), 0),\n            \'input_b\': tf.expand_dims(tf.constant(input_b, dtype=tf.float32), 0),\n        }\n        predictions = self.model(inputs, training_schedule)\n        pred_flow = predictions[\'flow\']\n\n        saver = tf.train.Saver()\n\n        with tf.Session() as sess:\n            saver.restore(sess, checkpoint)\n            pred_flow = sess.run(pred_flow)[0, :, :, :]\n\n            unique_name = \'flow-\' + str(uuid.uuid4())\n            if save_image:\n                flow_img = flow_to_image(pred_flow)\n                full_out_path = os.path.join(out_path, unique_name + \'.png\')\n                imsave(full_out_path, flow_img)\n\n            if save_flo:\n                full_out_path = os.path.join(out_path, unique_name + \'.flo\')\n                write_flow(pred_flow, full_out_path)\n\n    def train(self, log_dir, training_schedule, input_a, input_b, flow, checkpoints=None):\n        tf.summary.image(""image_a"", input_a, max_outputs=2)\n        tf.summary.image(""image_b"", input_b, max_outputs=2)\n\n        self.learning_rate = tf.train.piecewise_constant(\n            self.global_step,\n            [tf.cast(v, tf.int64) for v in training_schedule[\'step_values\']],\n            training_schedule[\'learning_rates\'])\n\n        optimizer = tf.train.AdamOptimizer(\n            self.learning_rate,\n            training_schedule[\'momentum\'],\n            training_schedule[\'momentum2\'])\n\n        inputs = {\n            \'input_a\': input_a,\n            \'input_b\': input_b,\n        }\n        predictions = self.model(inputs, training_schedule)\n        total_loss = self.loss(flow, predictions)\n        tf.summary.scalar(\'loss\', total_loss)\n\n        if checkpoints:\n            for (checkpoint_path, (scope, new_scope)) in checkpoints.iteritems():\n                variables_to_restore = slim.get_variables(scope=scope)\n                renamed_variables = {\n                    var.op.name.split(new_scope + \'/\')[1]: var\n                    for var in variables_to_restore\n                }\n                restorer = tf.train.Saver(renamed_variables)\n                with tf.Session() as sess:\n                    restorer.restore(sess, checkpoint_path)\n\n        # Show the generated flow in TensorBoard\n        if \'flow\' in predictions:\n            pred_flow_0 = predictions[\'flow\'][0, :, :, :]\n            pred_flow_0 = tf.py_func(flow_to_image, [pred_flow_0], tf.uint8)\n            pred_flow_1 = predictions[\'flow\'][1, :, :, :]\n            pred_flow_1 = tf.py_func(flow_to_image, [pred_flow_1], tf.uint8)\n            pred_flow_img = tf.stack([pred_flow_0, pred_flow_1], 0)\n            tf.summary.image(\'pred_flow\', pred_flow_img, max_outputs=2)\n\n        true_flow_0 = flow[0, :, :, :]\n        true_flow_0 = tf.py_func(flow_to_image, [true_flow_0], tf.uint8)\n        true_flow_1 = flow[1, :, :, :]\n        true_flow_1 = tf.py_func(flow_to_image, [true_flow_1], tf.uint8)\n        true_flow_img = tf.stack([true_flow_0, true_flow_1], 0)\n        tf.summary.image(\'true_flow\', true_flow_img, max_outputs=2)\n\n        train_op = slim.learning.create_train_op(\n            total_loss,\n            optimizer,\n            summarize_gradients=True)\n\n        if self.debug:\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                tf.train.start_queue_runners(sess)\n                slim.learning.train_step(\n                    sess,\n                    train_op,\n                    self.global_step,\n                    {\n                        \'should_trace\': tf.constant(1),\n                        \'should_log\': tf.constant(1),\n                        \'logdir\': log_dir + \'/debug\',\n                    }\n                )\n        else:\n            slim.learning.train(\n                train_op,\n                log_dir,\n                # session_config=tf.ConfigProto(allow_soft_placement=True),\n                global_step=self.global_step,\n                save_summaries_secs=60,\n                number_of_steps=training_schedule[\'max_iter\']\n            )\n'"
src/training_schedules.py,0,"b""LONG_SCHEDULE = {\n    'step_values': [400000, 600000, 800000, 1000000],\n    'learning_rates': [0.0001, 0.00005, 0.000025, 0.0000125, 0.00000625],\n    'momentum': 0.9,\n    'momentum2': 0.999,\n    'weight_decay': 0.0004,\n    'max_iter': 1200000,\n}\n\nFINETUNE_SCHEDULE = {\n    # TODO: Finetune schedule\n}\n"""
src/utils.py,10,"b'import tensorflow as tf\n\n\n# Thanks, https://github.com/tensorflow/tensorflow/issues/4079\ndef LeakyReLU(x, leak=0.1, name=""lrelu""):\n    with tf.variable_scope(name):\n        f1 = 0.5 * (1.0 + leak)\n        f2 = 0.5 * (1.0 - leak)\n        return f1 * x + f2 * abs(x)\n\n\ndef average_endpoint_error(labels, predictions):\n    """"""\n    Given labels and predictions of size (N, H, W, 2), calculates average endpoint error:\n        sqrt[sum_across_channels{(X - Y)^2}]\n    """"""\n    num_samples = predictions.shape.as_list()[0]\n    with tf.name_scope(None, ""average_endpoint_error"", (predictions, labels)) as scope:\n        predictions = tf.to_float(predictions)\n        labels = tf.to_float(labels)\n        predictions.get_shape().assert_is_compatible_with(labels.get_shape())\n\n        squared_difference = tf.square(tf.subtract(predictions, labels))\n        # sum across channels: sum[(X - Y)^2] -> N, H, W, 1\n        loss = tf.reduce_sum(squared_difference, 3, keep_dims=True)\n        loss = tf.sqrt(loss)\n        return tf.reduce_sum(loss) / num_samples\n\n\ndef pad(tensor, num=1):\n    """"""\n    Pads the given tensor along the height and width dimensions with `num` 0s on each side\n    """"""\n    return tf.pad(tensor, [[0, 0], [num, num], [num, num], [0, 0]], ""CONSTANT"")\n\n\ndef antipad(tensor, num=1):\n    """"""\n    Performs a crop. ""padding"" for a deconvolutional layer (conv2d tranpose) removes\n    padding from the output rather than adding it to the input.\n    """"""\n    batch, h, w, c = tensor.shape.as_list()\n    return tf.slice(tensor, begin=[0, num, num, 0], size=[batch, h - 2 * num, w - 2 * num, c])\n'"
scripts/caffe/convert_caffe_weights_to_npy.py,0,"b'""""""\nPlease read README.md for usage instructions.\n\nExtracts Caffe parameters from a given caffemodel/prototxt to a dictionary of numpy arrays,\nready for conversion to TensorFlow variables. Writes the dictionary to a .npy file.\n""""""\nimport argparse\nimport caffe\nimport numpy as np\nimport os\nimport tempfile\n\nFLAGS = None\nARCHS = {\n    \'C\': {\n        \'CAFFEMODEL\': \'../models/FlowNet2-C/FlowNet2-C_weights.caffemodel\',\n        \'DEPLOY_PROTOTXT\': \'../models/FlowNet2-C/FlowNet2-C_deploy.prototxt.template\',\n        # Mappings between Caffe parameter names and TensorFlow variable names\n        \'PARAMS\': {\n            \'conv1\': \'FlowNetC/conv1\',\n            \'conv2\': \'FlowNetC/conv2\',\n            \'conv3\': \'FlowNetC/conv3\',\n            \'conv_redir\': \'FlowNetC/conv_redir\',\n            \'conv3_1\': \'FlowNetC/conv3_1\',\n            \'conv4\': \'FlowNetC/conv4\',\n            \'conv4_1\': \'FlowNetC/conv4_1\',\n            \'conv5\': \'FlowNetC/conv5\',\n            \'conv5_1\': \'FlowNetC/conv5_1\',\n            \'conv6\': \'FlowNetC/conv6\',\n            \'conv6_1\': \'FlowNetC/conv6_1\',\n            \'Convolution1\': \'FlowNetC/predict_flow6\',\n            \'deconv5\': \'FlowNetC/deconv5\',\n            \'upsample_flow6to5\': \'FlowNetC/upsample_flow6to5\',\n            \'Convolution2\': \'FlowNetC/predict_flow5\',\n            \'deconv4\': \'FlowNetC/deconv4\',\n            \'upsample_flow5to4\': \'FlowNetC/upsample_flow5to4\',\n            \'Convolution3\': \'FlowNetC/predict_flow4\',\n            \'deconv3\': \'FlowNetC/deconv3\',\n            \'upsample_flow4to3\': \'FlowNetC/upsample_flow4to3\',\n            \'Convolution4\': \'FlowNetC/predict_flow3\',\n            \'deconv2\': \'FlowNetC/deconv2\',\n            \'upsample_flow3to2\': \'FlowNetC/upsample_flow3to2\',\n            \'Convolution5\': \'FlowNetC/predict_flow2\',\n        }\n    },\n    \'S\': {\n        \'CAFFEMODEL\': \'../models/FlowNet2-S/FlowNet2-S_weights.caffemodel.h5\',\n        \'DEPLOY_PROTOTXT\': \'../models/FlowNet2-S/FlowNet2-S_deploy.prototxt.template\',\n        # Mappings between Caffe parameter names and TensorFlow variable names\n        \'PARAMS\': {\n            \'conv1\': \'FlowNetS/conv1\',\n            \'conv2\': \'FlowNetS/conv2\',\n            \'conv3\': \'FlowNetS/conv3\',\n            \'conv3_1\': \'FlowNetS/conv3_1\',\n            \'conv4\': \'FlowNetS/conv4\',\n            \'conv4_1\': \'FlowNetS/conv4_1\',\n            \'conv5\': \'FlowNetS/conv5\',\n            \'conv5_1\': \'FlowNetS/conv5_1\',\n            \'conv6\': \'FlowNetS/conv6\',\n            \'conv6_1\': \'FlowNetS/conv6_1\',\n            \'Convolution1\': \'FlowNetS/predict_flow6\',\n            \'deconv5\': \'FlowNetS/deconv5\',\n            \'upsample_flow6to5\': \'FlowNetS/upsample_flow6to5\',\n            \'Convolution2\': \'FlowNetS/predict_flow5\',\n            \'deconv4\': \'FlowNetS/deconv4\',\n            \'upsample_flow5to4\': \'FlowNetS/upsample_flow5to4\',\n            \'Convolution3\': \'FlowNetS/predict_flow4\',\n            \'deconv3\': \'FlowNetS/deconv3\',\n            \'upsample_flow4to3\': \'FlowNetS/upsample_flow4to3\',\n            \'Convolution4\': \'FlowNetS/predict_flow3\',\n            \'deconv2\': \'FlowNetS/deconv2\',\n            \'upsample_flow3to2\': \'FlowNetS/upsample_flow3to2\',\n            \'Convolution5\': \'FlowNetS/predict_flow2\',\n        }\n    },\n    \'CS\': {\n        \'CAFFEMODEL\': \'../models/FlowNet2-CS/FlowNet2-CS_weights.caffemodel\',\n        \'DEPLOY_PROTOTXT\': \'../models/FlowNet2-CS/FlowNet2-CS_deploy.prototxt.template\',\n        # Mappings between Caffe parameter names and TensorFlow variable names\n        \'PARAMS\': {\n            # Net C\n            \'conv1\': \'FlowNetCS/FlowNetC/conv1\',\n            \'conv2\': \'FlowNetCS/FlowNetC/conv2\',\n            \'conv3\': \'FlowNetCS/FlowNetC/conv3\',\n            \'conv_redir\': \'FlowNetCS/FlowNetC/conv_redir\',\n            \'conv3_1\': \'FlowNetCS/FlowNetC/conv3_1\',\n            \'conv4\': \'FlowNetCS/FlowNetC/conv4\',\n            \'conv4_1\': \'FlowNetCS/FlowNetC/conv4_1\',\n            \'conv5\': \'FlowNetCS/FlowNetC/conv5\',\n            \'conv5_1\': \'FlowNetCS/FlowNetC/conv5_1\',\n            \'conv6\': \'FlowNetCS/FlowNetC/conv6\',\n            \'conv6_1\': \'FlowNetCS/FlowNetC/conv6_1\',\n            \'Convolution1\': \'FlowNetCS/FlowNetC/predict_flow6\',\n            \'deconv5\': \'FlowNetCS/FlowNetC/deconv5\',\n            \'upsample_flow6to5\': \'FlowNetCS/FlowNetC/upsample_flow6to5\',\n            \'Convolution2\': \'FlowNetCS/FlowNetC/predict_flow5\',\n            \'deconv4\': \'FlowNetCS/FlowNetC/deconv4\',\n            \'upsample_flow5to4\': \'FlowNetCS/FlowNetC/upsample_flow5to4\',\n            \'Convolution3\': \'FlowNetCS/FlowNetC/predict_flow4\',\n            \'deconv3\': \'FlowNetCS/FlowNetC/deconv3\',\n            \'upsample_flow4to3\': \'FlowNetCS/FlowNetC/upsample_flow4to3\',\n            \'Convolution4\': \'FlowNetCS/FlowNetC/predict_flow3\',\n            \'deconv2\': \'FlowNetCS/FlowNetC/deconv2\',\n            \'upsample_flow3to2\': \'FlowNetCS/FlowNetC/upsample_flow3to2\',\n            \'Convolution5\': \'FlowNetCS/FlowNetC/predict_flow2\',\n\n            # Net S\n            \'net2_conv1\': \'FlowNetCS/FlowNetS/conv1\',\n            \'net2_conv2\': \'FlowNetCS/FlowNetS/conv2\',\n            \'net2_conv3\': \'FlowNetCS/FlowNetS/conv3\',\n            \'net2_conv3_1\': \'FlowNetCS/FlowNetS/conv3_1\',\n            \'net2_conv4\': \'FlowNetCS/FlowNetS/conv4\',\n            \'net2_conv4_1\': \'FlowNetCS/FlowNetS/conv4_1\',\n            \'net2_conv5\': \'FlowNetCS/FlowNetS/conv5\',\n            \'net2_conv5_1\': \'FlowNetCS/FlowNetS/conv5_1\',\n            \'net2_conv6\': \'FlowNetCS/FlowNetS/conv6\',\n            \'net2_conv6_1\': \'FlowNetCS/FlowNetS/conv6_1\',\n            \'net2_predict_conv6\': \'FlowNetCS/FlowNetS/predict_flow6\',\n            \'net2_deconv5\': \'FlowNetCS/FlowNetS/deconv5\',\n            \'net2_net2_upsample_flow6to5\': \'FlowNetCS/FlowNetS/upsample_flow6to5\',\n            \'net2_predict_conv5\': \'FlowNetCS/FlowNetS/predict_flow5\',\n            \'net2_deconv4\': \'FlowNetCS/FlowNetS/deconv4\',\n            \'net2_net2_upsample_flow5to4\': \'FlowNetCS/FlowNetS/upsample_flow5to4\',\n            \'net2_predict_conv4\': \'FlowNetCS/FlowNetS/predict_flow4\',\n            \'net2_deconv3\': \'FlowNetCS/FlowNetS/deconv3\',\n            \'net2_net2_upsample_flow4to3\': \'FlowNetCS/FlowNetS/upsample_flow4to3\',\n            \'net2_predict_conv3\': \'FlowNetCS/FlowNetS/predict_flow3\',\n            \'net2_deconv2\': \'FlowNetCS/FlowNetS/deconv2\',\n            \'net2_net2_upsample_flow3to2\': \'FlowNetCS/FlowNetS/upsample_flow3to2\',\n            \'net2_predict_conv2\': \'FlowNetCS/FlowNetS/predict_flow2\',\n        }\n    },\n    \'CSS\': {\n        \'CAFFEMODEL\': \'../models/FlowNet2-CSS/FlowNet2-CSS_weights.caffemodel.h5\',\n        \'DEPLOY_PROTOTXT\': \'../models/FlowNet2-CSS/FlowNet2-CSS_deploy.prototxt.template\',\n        # Mappings between Caffe parameter names and TensorFlow variable names\n        \'PARAMS\': {\n            # Net C\n            \'conv1\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv1\',\n            \'conv2\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv2\',\n            \'conv3\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv3\',\n            \'conv_redir\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv_redir\',\n            \'conv3_1\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv3_1\',\n            \'conv4\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv4\',\n            \'conv4_1\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv4_1\',\n            \'conv5\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv5\',\n            \'conv5_1\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv5_1\',\n            \'conv6\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv6\',\n            \'conv6_1\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv6_1\',\n            \'Convolution1\': \'FlowNetCSS/FlowNetCS/FlowNetC/predict_flow6\',\n            \'deconv5\': \'FlowNetCSS/FlowNetCS/FlowNetC/deconv5\',\n            \'upsample_flow6to5\': \'FlowNetCSS/FlowNetCS/FlowNetC/upsample_flow6to5\',\n            \'Convolution2\': \'FlowNetCSS/FlowNetCS/FlowNetC/predict_flow5\',\n            \'deconv4\': \'FlowNetCSS/FlowNetCS/FlowNetC/deconv4\',\n            \'upsample_flow5to4\': \'FlowNetCSS/FlowNetCS/FlowNetC/upsample_flow5to4\',\n            \'Convolution3\': \'FlowNetCSS/FlowNetCS/FlowNetC/predict_flow4\',\n            \'deconv3\': \'FlowNetCSS/FlowNetCS/FlowNetC/deconv3\',\n            \'upsample_flow4to3\': \'FlowNetCSS/FlowNetCS/FlowNetC/upsample_flow4to3\',\n            \'Convolution4\': \'FlowNetCSS/FlowNetCS/FlowNetC/predict_flow3\',\n            \'deconv2\': \'FlowNetCSS/FlowNetCS/FlowNetC/deconv2\',\n            \'upsample_flow3to2\': \'FlowNetCSS/FlowNetCS/FlowNetC/upsample_flow3to2\',\n            \'Convolution5\': \'FlowNetCSS/FlowNetCS/FlowNetC/predict_flow2\',\n\n            # Net S 1\n            \'net2_conv1\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv1\',\n            \'net2_conv2\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv2\',\n            \'net2_conv3\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv3\',\n            \'net2_conv3_1\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv3_1\',\n            \'net2_conv4\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv4\',\n            \'net2_conv4_1\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv4_1\',\n            \'net2_conv5\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv5\',\n            \'net2_conv5_1\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv5_1\',\n            \'net2_conv6\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv6\',\n            \'net2_conv6_1\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv6_1\',\n            \'net2_predict_conv6\': \'FlowNetCSS/FlowNetCS/FlowNetS/predict_flow6\',\n            \'net2_deconv5\': \'FlowNetCSS/FlowNetCS/FlowNetS/deconv5\',\n            \'net2_net2_upsample_flow6to5\': \'FlowNetCSS/FlowNetCS/FlowNetS/upsample_flow6to5\',\n            \'net2_predict_conv5\': \'FlowNetCSS/FlowNetCS/FlowNetS/predict_flow5\',\n            \'net2_deconv4\': \'FlowNetCSS/FlowNetCS/FlowNetS/deconv4\',\n            \'net2_net2_upsample_flow5to4\': \'FlowNetCSS/FlowNetCS/FlowNetS/upsample_flow5to4\',\n            \'net2_predict_conv4\': \'FlowNetCSS/FlowNetCS/FlowNetS/predict_flow4\',\n            \'net2_deconv3\': \'FlowNetCSS/FlowNetCS/FlowNetS/deconv3\',\n            \'net2_net2_upsample_flow4to3\': \'FlowNetCSS/FlowNetCS/FlowNetS/upsample_flow4to3\',\n            \'net2_predict_conv3\': \'FlowNetCSS/FlowNetCS/FlowNetS/predict_flow3\',\n            \'net2_deconv2\': \'FlowNetCSS/FlowNetCS/FlowNetS/deconv2\',\n            \'net2_net2_upsample_flow3to2\': \'FlowNetCSS/FlowNetCS/FlowNetS/upsample_flow3to2\',\n            \'net2_predict_conv2\': \'FlowNetCSS/FlowNetCS/FlowNetS/predict_flow2\',\n\n            # Net S 2\n            \'net3_conv1\': \'FlowNetCSS/FlowNetS/conv1\',\n            \'net3_conv2\': \'FlowNetCSS/FlowNetS/conv2\',\n            \'net3_conv3\': \'FlowNetCSS/FlowNetS/conv3\',\n            \'net3_conv3_1\': \'FlowNetCSS/FlowNetS/conv3_1\',\n            \'net3_conv4\': \'FlowNetCSS/FlowNetS/conv4\',\n            \'net3_conv4_1\': \'FlowNetCSS/FlowNetS/conv4_1\',\n            \'net3_conv5\': \'FlowNetCSS/FlowNetS/conv5\',\n            \'net3_conv5_1\': \'FlowNetCSS/FlowNetS/conv5_1\',\n            \'net3_conv6\': \'FlowNetCSS/FlowNetS/conv6\',\n            \'net3_conv6_1\': \'FlowNetCSS/FlowNetS/conv6_1\',\n            \'net3_predict_conv6\': \'FlowNetCSS/FlowNetS/predict_flow6\',\n            \'net3_deconv5\': \'FlowNetCSS/FlowNetS/deconv5\',\n            \'net3_net3_upsample_flow6to5\': \'FlowNetCSS/FlowNetS/upsample_flow6to5\',\n            \'net3_predict_conv5\': \'FlowNetCSS/FlowNetS/predict_flow5\',\n            \'net3_deconv4\': \'FlowNetCSS/FlowNetS/deconv4\',\n            \'net3_net3_upsample_flow5to4\': \'FlowNetCSS/FlowNetS/upsample_flow5to4\',\n            \'net3_predict_conv4\': \'FlowNetCSS/FlowNetS/predict_flow4\',\n            \'net3_deconv3\': \'FlowNetCSS/FlowNetS/deconv3\',\n            \'net3_net3_upsample_flow4to3\': \'FlowNetCSS/FlowNetS/upsample_flow4to3\',\n            \'net3_predict_conv3\': \'FlowNetCSS/FlowNetS/predict_flow3\',\n            \'net3_deconv2\': \'FlowNetCSS/FlowNetS/deconv2\',\n            \'net3_net3_upsample_flow3to2\': \'FlowNetCSS/FlowNetS/upsample_flow3to2\',\n            \'net3_predict_conv2\': \'FlowNetCSS/FlowNetS/predict_flow2\',\n        },\n    },\n    \'CSS-ft-sd\': {\n        \'CAFFEMODEL\': \'../models/FlowNet2-CSS-ft-sd/FlowNet2-CSS-ft-sd_weights.caffemodel.h5\',\n        \'DEPLOY_PROTOTXT\': \'../models/FlowNet2-CSS-ft-sd/FlowNet2-CSS-ft-sd_deploy.prototxt.template\',\n        # Mappings between Caffe parameter names and TensorFlow variable names\n        \'PARAMS\': {\n            # Net C\n            \'conv1\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv1\',\n            \'conv2\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv2\',\n            \'conv3\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv3\',\n            \'conv_redir\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv_redir\',\n            \'conv3_1\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv3_1\',\n            \'conv4\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv4\',\n            \'conv4_1\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv4_1\',\n            \'conv5\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv5\',\n            \'conv5_1\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv5_1\',\n            \'conv6\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv6\',\n            \'conv6_1\': \'FlowNetCSS/FlowNetCS/FlowNetC/conv6_1\',\n            \'Convolution1\': \'FlowNetCSS/FlowNetCS/FlowNetC/predict_flow6\',\n            \'deconv5\': \'FlowNetCSS/FlowNetCS/FlowNetC/deconv5\',\n            \'upsample_flow6to5\': \'FlowNetCSS/FlowNetCS/FlowNetC/upsample_flow6to5\',\n            \'Convolution2\': \'FlowNetCSS/FlowNetCS/FlowNetC/predict_flow5\',\n            \'deconv4\': \'FlowNetCSS/FlowNetCS/FlowNetC/deconv4\',\n            \'upsample_flow5to4\': \'FlowNetCSS/FlowNetCS/FlowNetC/upsample_flow5to4\',\n            \'Convolution3\': \'FlowNetCSS/FlowNetCS/FlowNetC/predict_flow4\',\n            \'deconv3\': \'FlowNetCSS/FlowNetCS/FlowNetC/deconv3\',\n            \'upsample_flow4to3\': \'FlowNetCSS/FlowNetCS/FlowNetC/upsample_flow4to3\',\n            \'Convolution4\': \'FlowNetCSS/FlowNetCS/FlowNetC/predict_flow3\',\n            \'deconv2\': \'FlowNetCSS/FlowNetCS/FlowNetC/deconv2\',\n            \'upsample_flow3to2\': \'FlowNetCSS/FlowNetCS/FlowNetC/upsample_flow3to2\',\n            \'Convolution5\': \'FlowNetCSS/FlowNetCS/FlowNetC/predict_flow2\',\n\n            # Net S 1\n            \'net2_conv1\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv1\',\n            \'net2_conv2\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv2\',\n            \'net2_conv3\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv3\',\n            \'net2_conv3_1\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv3_1\',\n            \'net2_conv4\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv4\',\n            \'net2_conv4_1\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv4_1\',\n            \'net2_conv5\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv5\',\n            \'net2_conv5_1\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv5_1\',\n            \'net2_conv6\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv6\',\n            \'net2_conv6_1\': \'FlowNetCSS/FlowNetCS/FlowNetS/conv6_1\',\n            \'net2_predict_conv6\': \'FlowNetCSS/FlowNetCS/FlowNetS/predict_flow6\',\n            \'net2_deconv5\': \'FlowNetCSS/FlowNetCS/FlowNetS/deconv5\',\n            \'net2_net2_upsample_flow6to5\': \'FlowNetCSS/FlowNetCS/FlowNetS/upsample_flow6to5\',\n            \'net2_predict_conv5\': \'FlowNetCSS/FlowNetCS/FlowNetS/predict_flow5\',\n            \'net2_deconv4\': \'FlowNetCSS/FlowNetCS/FlowNetS/deconv4\',\n            \'net2_net2_upsample_flow5to4\': \'FlowNetCSS/FlowNetCS/FlowNetS/upsample_flow5to4\',\n            \'net2_predict_conv4\': \'FlowNetCSS/FlowNetCS/FlowNetS/predict_flow4\',\n            \'net2_deconv3\': \'FlowNetCSS/FlowNetCS/FlowNetS/deconv3\',\n            \'net2_net2_upsample_flow4to3\': \'FlowNetCSS/FlowNetCS/FlowNetS/upsample_flow4to3\',\n            \'net2_predict_conv3\': \'FlowNetCSS/FlowNetCS/FlowNetS/predict_flow3\',\n            \'net2_deconv2\': \'FlowNetCSS/FlowNetCS/FlowNetS/deconv2\',\n            \'net2_net2_upsample_flow3to2\': \'FlowNetCSS/FlowNetCS/FlowNetS/upsample_flow3to2\',\n            \'net2_predict_conv2\': \'FlowNetCSS/FlowNetCS/FlowNetS/predict_flow2\',\n\n            # Net S 2\n            \'net3_conv1\': \'FlowNetCSS/FlowNetS/conv1\',\n            \'net3_conv2\': \'FlowNetCSS/FlowNetS/conv2\',\n            \'net3_conv3\': \'FlowNetCSS/FlowNetS/conv3\',\n            \'net3_conv3_1\': \'FlowNetCSS/FlowNetS/conv3_1\',\n            \'net3_conv4\': \'FlowNetCSS/FlowNetS/conv4\',\n            \'net3_conv4_1\': \'FlowNetCSS/FlowNetS/conv4_1\',\n            \'net3_conv5\': \'FlowNetCSS/FlowNetS/conv5\',\n            \'net3_conv5_1\': \'FlowNetCSS/FlowNetS/conv5_1\',\n            \'net3_conv6\': \'FlowNetCSS/FlowNetS/conv6\',\n            \'net3_conv6_1\': \'FlowNetCSS/FlowNetS/conv6_1\',\n            \'net3_predict_conv6\': \'FlowNetCSS/FlowNetS/predict_flow6\',\n            \'net3_deconv5\': \'FlowNetCSS/FlowNetS/deconv5\',\n            \'net3_net3_upsample_flow6to5\': \'FlowNetCSS/FlowNetS/upsample_flow6to5\',\n            \'net3_predict_conv5\': \'FlowNetCSS/FlowNetS/predict_flow5\',\n            \'net3_deconv4\': \'FlowNetCSS/FlowNetS/deconv4\',\n            \'net3_net3_upsample_flow5to4\': \'FlowNetCSS/FlowNetS/upsample_flow5to4\',\n            \'net3_predict_conv4\': \'FlowNetCSS/FlowNetS/predict_flow4\',\n            \'net3_deconv3\': \'FlowNetCSS/FlowNetS/deconv3\',\n            \'net3_net3_upsample_flow4to3\': \'FlowNetCSS/FlowNetS/upsample_flow4to3\',\n            \'net3_predict_conv3\': \'FlowNetCSS/FlowNetS/predict_flow3\',\n            \'net3_deconv2\': \'FlowNetCSS/FlowNetS/deconv2\',\n            \'net3_net3_upsample_flow3to2\': \'FlowNetCSS/FlowNetS/upsample_flow3to2\',\n            \'net3_predict_conv2\': \'FlowNetCSS/FlowNetS/predict_flow2\',\n        },\n    },\n    \'SD\': {\n        \'CAFFEMODEL\': \'../models/FlowNet2-SD/FlowNet2-SD_weights.caffemodel.h5\',\n        \'DEPLOY_PROTOTXT\': \'../models/FlowNet2-SD/FlowNet2-SD_deploy.prototxt.template\',\n        # Mappings between Caffe parameter names and TensorFlow variable names\n        \'PARAMS\': {\n            \'conv0\': \'FlowNetSD/conv0\',\n            \'conv1\': \'FlowNetSD/conv1\',\n            \'conv1_1\': \'FlowNetSD/conv1_1\',\n            \'conv2\': \'FlowNetSD/conv2\',\n            \'conv2_1\': \'FlowNetSD/conv2_1\',\n            \'conv3\': \'FlowNetSD/conv3\',\n            \'conv3_1\': \'FlowNetSD/conv3_1\',\n            \'conv4\': \'FlowNetSD/conv4\',\n            \'conv4_1\': \'FlowNetSD/conv4_1\',\n            \'conv5\': \'FlowNetSD/conv5\',\n            \'conv5_1\': \'FlowNetSD/conv5_1\',\n            \'conv6\': \'FlowNetSD/conv6\',\n            \'conv6_1\': \'FlowNetSD/conv6_1\',\n            \'Convolution1\': \'FlowNetSD/predict_flow6\',\n            \'deconv5\': \'FlowNetSD/deconv5\',\n            \'upsample_flow6to5\': \'FlowNetSD/upsample_flow6to5\',\n            \'interconv5\': \'FlowNetSD/interconv5\',\n            \'Convolution2\': \'FlowNetSD/predict_flow5\',\n            \'deconv4\': \'FlowNetSD/deconv4\',\n            \'upsample_flow5to4\': \'FlowNetSD/upsample_flow5to4\',\n            \'interconv4\': \'FlowNetSD/interconv4\',\n            \'Convolution3\': \'FlowNetSD/predict_flow4\',\n            \'deconv3\': \'FlowNetSD/deconv3\',\n            \'upsample_flow4to3\': \'FlowNetSD/upsample_flow4to3\',\n            \'interconv3\': \'FlowNetSD/interconv3\',\n            \'Convolution4\': \'FlowNetSD/predict_flow3\',\n            \'deconv2\': \'FlowNetSD/deconv2\',\n            \'upsample_flow3to2\': \'FlowNetSD/upsample_flow3to2\',\n            \'interconv2\': \'FlowNetSD/interconv2\',\n            \'Convolution5\': \'FlowNetSD/predict_flow2\',\n        },\n    },\n    \'2\': {\n        \'CAFFEMODEL\': \'../models/FlowNet2/FlowNet2_weights.caffemodel.h5\',\n        \'DEPLOY_PROTOTXT\': \'../models/FlowNet2/FlowNet2_deploy.prototxt.template\',\n        # Mappings between Caffe parameter names and TensorFlow variable names\n        \'PARAMS\': {\n            # Net C\n            \'conv1\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/conv1\',\n            \'conv2\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/conv2\',\n            \'conv3\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/conv3\',\n            \'conv_redir\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/conv_redir\',\n            \'conv3_1\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/conv3_1\',\n            \'conv4\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/conv4\',\n            \'conv4_1\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/conv4_1\',\n            \'conv5\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/conv5\',\n            \'conv5_1\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/conv5_1\',\n            \'conv6\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/conv6\',\n            \'conv6_1\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/conv6_1\',\n            \'Convolution1\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/predict_flow6\',\n            \'deconv5\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/deconv5\',\n            \'upsample_flow6to5\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/upsample_flow6to5\',\n            \'Convolution2\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/predict_flow5\',\n            \'deconv4\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/deconv4\',\n            \'upsample_flow5to4\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/upsample_flow5to4\',\n            \'Convolution3\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/predict_flow4\',\n            \'deconv3\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/deconv3\',\n            \'upsample_flow4to3\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/upsample_flow4to3\',\n            \'Convolution4\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/predict_flow3\',\n            \'deconv2\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/deconv2\',\n            \'upsample_flow3to2\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/upsample_flow3to2\',\n            \'Convolution5\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetC/predict_flow2\',\n\n            # Net S 1\n            \'net2_conv1\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/conv1\',\n            \'net2_conv2\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/conv2\',\n            \'net2_conv3\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/conv3\',\n            \'net2_conv3_1\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/conv3_1\',\n            \'net2_conv4\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/conv4\',\n            \'net2_conv4_1\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/conv4_1\',\n            \'net2_conv5\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/conv5\',\n            \'net2_conv5_1\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/conv5_1\',\n            \'net2_conv6\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/conv6\',\n            \'net2_conv6_1\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/conv6_1\',\n            \'net2_predict_conv6\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/predict_flow6\',\n            \'net2_deconv5\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/deconv5\',\n            \'net2_net2_upsample_flow6to5\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/upsample_flow6to5\',\n            \'net2_predict_conv5\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/predict_flow5\',\n            \'net2_deconv4\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/deconv4\',\n            \'net2_net2_upsample_flow5to4\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/upsample_flow5to4\',\n            \'net2_predict_conv4\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/predict_flow4\',\n            \'net2_deconv3\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/deconv3\',\n            \'net2_net2_upsample_flow4to3\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/upsample_flow4to3\',\n            \'net2_predict_conv3\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/predict_flow3\',\n            \'net2_deconv2\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/deconv2\',\n            \'net2_net2_upsample_flow3to2\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/upsample_flow3to2\',\n            \'net2_predict_conv2\': \'FlowNet2/FlowNetCSS/FlowNetCS/FlowNetS/predict_flow2\',\n\n            # Net S 2\n            \'net3_conv1\': \'FlowNet2/FlowNetCSS/FlowNetS/conv1\',\n            \'net3_conv2\': \'FlowNet2/FlowNetCSS/FlowNetS/conv2\',\n            \'net3_conv3\': \'FlowNet2/FlowNetCSS/FlowNetS/conv3\',\n            \'net3_conv3_1\': \'FlowNet2/FlowNetCSS/FlowNetS/conv3_1\',\n            \'net3_conv4\': \'FlowNet2/FlowNetCSS/FlowNetS/conv4\',\n            \'net3_conv4_1\': \'FlowNet2/FlowNetCSS/FlowNetS/conv4_1\',\n            \'net3_conv5\': \'FlowNet2/FlowNetCSS/FlowNetS/conv5\',\n            \'net3_conv5_1\': \'FlowNet2/FlowNetCSS/FlowNetS/conv5_1\',\n            \'net3_conv6\': \'FlowNet2/FlowNetCSS/FlowNetS/conv6\',\n            \'net3_conv6_1\': \'FlowNet2/FlowNetCSS/FlowNetS/conv6_1\',\n            \'net3_predict_conv6\': \'FlowNet2/FlowNetCSS/FlowNetS/predict_flow6\',\n            \'net3_deconv5\': \'FlowNet2/FlowNetCSS/FlowNetS/deconv5\',\n            \'net3_net3_upsample_flow6to5\': \'FlowNet2/FlowNetCSS/FlowNetS/upsample_flow6to5\',\n            \'net3_predict_conv5\': \'FlowNet2/FlowNetCSS/FlowNetS/predict_flow5\',\n            \'net3_deconv4\': \'FlowNet2/FlowNetCSS/FlowNetS/deconv4\',\n            \'net3_net3_upsample_flow5to4\': \'FlowNet2/FlowNetCSS/FlowNetS/upsample_flow5to4\',\n            \'net3_predict_conv4\': \'FlowNet2/FlowNetCSS/FlowNetS/predict_flow4\',\n            \'net3_deconv3\': \'FlowNet2/FlowNetCSS/FlowNetS/deconv3\',\n            \'net3_net3_upsample_flow4to3\': \'FlowNet2/FlowNetCSS/FlowNetS/upsample_flow4to3\',\n            \'net3_predict_conv3\': \'FlowNet2/FlowNetCSS/FlowNetS/predict_flow3\',\n            \'net3_deconv2\': \'FlowNet2/FlowNetCSS/FlowNetS/deconv2\',\n            \'net3_net3_upsample_flow3to2\': \'FlowNet2/FlowNetCSS/FlowNetS/upsample_flow3to2\',\n            \'net3_predict_conv2\': \'FlowNet2/FlowNetCSS/FlowNetS/predict_flow2\',\n\n            # Net SD\n            \'netsd_conv0\': \'FlowNet2/FlowNetSD/conv0\',\n            \'netsd_conv1\': \'FlowNet2/FlowNetSD/conv1\',\n            \'netsd_conv1_1\': \'FlowNet2/FlowNetSD/conv1_1\',\n            \'netsd_conv2\': \'FlowNet2/FlowNetSD/conv2\',\n            \'netsd_conv2_1\': \'FlowNet2/FlowNetSD/conv2_1\',\n            \'netsd_conv3\': \'FlowNet2/FlowNetSD/conv3\',\n            \'netsd_conv3_1\': \'FlowNet2/FlowNetSD/conv3_1\',\n            \'netsd_conv4\': \'FlowNet2/FlowNetSD/conv4\',\n            \'netsd_conv4_1\': \'FlowNet2/FlowNetSD/conv4_1\',\n            \'netsd_conv5\': \'FlowNet2/FlowNetSD/conv5\',\n            \'netsd_conv5_1\': \'FlowNet2/FlowNetSD/conv5_1\',\n            \'netsd_conv6\': \'FlowNet2/FlowNetSD/conv6\',\n            \'netsd_conv6_1\': \'FlowNet2/FlowNetSD/conv6_1\',\n            \'netsd_Convolution1\': \'FlowNet2/FlowNetSD/predict_flow6\',\n            \'netsd_deconv5\': \'FlowNet2/FlowNetSD/deconv5\',\n            \'netsd_upsample_flow6to5\': \'FlowNet2/FlowNetSD/upsample_flow6to5\',\n            \'netsd_interconv5\': \'FlowNet2/FlowNetSD/interconv5\',\n            \'netsd_Convolution2\': \'FlowNet2/FlowNetSD/predict_flow5\',\n            \'netsd_deconv4\': \'FlowNet2/FlowNetSD/deconv4\',\n            \'netsd_upsample_flow5to4\': \'FlowNet2/FlowNetSD/upsample_flow5to4\',\n            \'netsd_interconv4\': \'FlowNet2/FlowNetSD/interconv4\',\n            \'netsd_Convolution3\': \'FlowNet2/FlowNetSD/predict_flow4\',\n            \'netsd_deconv3\': \'FlowNet2/FlowNetSD/deconv3\',\n            \'netsd_upsample_flow4to3\': \'FlowNet2/FlowNetSD/upsample_flow4to3\',\n            \'netsd_interconv3\': \'FlowNet2/FlowNetSD/interconv3\',\n            \'netsd_Convolution4\': \'FlowNet2/FlowNetSD/predict_flow3\',\n            \'netsd_deconv2\': \'FlowNet2/FlowNetSD/deconv2\',\n            \'netsd_upsample_flow3to2\': \'FlowNet2/FlowNetSD/upsample_flow3to2\',\n            \'netsd_interconv2\': \'FlowNet2/FlowNetSD/interconv2\',\n            \'netsd_Convolution5\': \'FlowNet2/FlowNetSD/predict_flow2\',\n\n            # Fusion Net\n            \'fuse_conv0\': \'FlowNet2/fuse_conv0\',\n            \'fuse_conv1\': \'FlowNet2/fuse_conv1\',\n            \'fuse_conv1_1\': \'FlowNet2/fuse_conv1_1\',\n            \'fuse_conv2\': \'FlowNet2/fuse_conv2\',\n            \'fuse_conv2_1\': \'FlowNet2/fuse_conv2_1\',\n            \'fuse__Convolution5\': \'FlowNet2/predict_flow2\',\n            \'fuse_deconv1\': \'FlowNet2/fuse_deconv1\',\n            \'fuse_upsample_flow2to1\': \'FlowNet2/fuse_upsample_flow2to1\',\n            \'fuse_interconv1\': \'FlowNet2/fuse_interconv1\',\n            \'fuse__Convolution6\': \'FlowNet2/predict_flow1\',\n            \'fuse_deconv0\': \'FlowNet2/fuse_deconv0\',\n            \'fuse_upsample_flow1to0\': \'FlowNet2/fuse_upsample_flow1to0\',\n            \'fuse_interconv0\': \'FlowNet2/fuse_interconv0\',\n            \'fuse__Convolution7\': \'FlowNet2/predict_flow0\',\n        }\n    },\n}\narch = None\n\n# Setup variables to be injected into prototxt.template\n# For now, use the dimensions of the Flying Chair Dataset\nvars = {}\nvars[\'TARGET_WIDTH\'] = vars[\'ADAPTED_WIDTH\'] = 512\nvars[\'TARGET_HEIGHT\'] = vars[\'ADAPTED_HEIGHT\'] = 384\nvars[\'SCALE_WIDTH\'] = vars[\'SCALE_HEIGHT\'] = 1.0\n\n\ndef main():\n    # Create tempfile to hold prototxt\n    tmp = tempfile.NamedTemporaryFile(mode=\'w\', delete=True)\n\n    # Parse prototxt and inject `vars`\n    proto = open(arch[\'DEPLOY_PROTOTXT\']).readlines()\n    for line in proto:\n        for key, value in vars.items():\n            tag = ""$%s$"" % key\n            line = line.replace(tag, str(value))\n        tmp.write(line)\n    tmp.flush()\n\n    # Instantiate Caffe Model\n    net = caffe.Net(tmp.name, arch[\'CAFFEMODEL\'], caffe.TEST)\n\n    out = {}\n    for (caffe_param, tf_param) in arch[\'PARAMS\'].items():\n        # Caffe stores weights as (channels_out, channels_in, h, w)\n        # but TF expects (h, w, channels_in, channels_out)\n        out[tf_param + \'/weights\'] = net.params[caffe_param][0].data.transpose((2, 3, 1, 0))\n        out[tf_param + \'/biases\'] = net.params[caffe_param][1].data\n\n    np.save(FLAGS.out, out)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--out\',\n        type=str,\n        required=True,\n        help=\'Output file path, eg /foo/bar.npy\'\n    )\n    parser.add_argument(\n        \'--arch\',\n        type=str,\n        choices=[\'C\', \'S\', \'CS\', \'CSS\', \'CSS-ft-sd\', \'SD\', \'2\'],\n        required=True,\n        help=\'Name of the FlowNet arch: C, S, CS, CSS, CSS-ft-sd, SD or 2\'\n    )\n    FLAGS = parser.parse_args()\n    arch = ARCHS[FLAGS.arch]\n    main()\n'"
scripts/caffe/convert_npy_weights_to_tf.py,5,"b'""""""\nPlease read README.md for usage instructions.\n\nGive a path to a .npy file which contains a dictionary of model parameters.\nCreates a TensorFlow Variable for each parameter and saves the session in a .ckpt file to restore later.\n""""""\nimport argparse\nimport numpy as np\nimport os\nimport tensorflow as tf\nslim = tf.contrib.slim\n\n\ndef main():\n    parameters = np.load(FLAGS.input)\n    #  unpack the dictionary since serializing to .npy stored it in an array\n    parameters = parameters[()]\n\n    for (name, param) in parameters.iteritems():\n        tf.Variable(param, name=name)\n        print(""Saving variable `"" + name + ""` of shape "", param.shape)\n\n    global_step = slim.get_or_create_global_step()\n\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        input_name = os.path.splitext(FLAGS.input)[0]\n        save_path = saver.save(sess, input_name + \'.ckpt\', global_step=global_step)\n        print(""Model saved in file: %s"" % save_path)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--input\',\n        type=str,\n        required=True,\n        help=\'Path to a .npy file containing a dictionary of parameters\'\n    )\n    FLAGS = parser.parse_args()\n    main()\n'"
src/flownet2/__init__.py,0,b''
src/flownet2/flownet2.py,12,"b""from ..net import Net, Mode\nfrom ..flownet_css.flownet_css import FlowNetCSS\nfrom ..flownet_sd.flownet_sd import FlowNetSD\nfrom ..flow_warp import flow_warp\nfrom ..utils import LeakyReLU, average_endpoint_error, pad, antipad\nfrom ..downsample import downsample\nimport tensorflow as tf\nslim = tf.contrib.slim\n\n\nclass FlowNet2(Net):\n\n    def __init__(self, mode=Mode.TRAIN, debug=False):\n        self.net_css = FlowNetCSS(mode, debug)\n        self.net_sd = FlowNetSD(mode, debug)\n        super(FlowNet2, self).__init__(mode=mode, debug=debug)\n\n    def model(self, inputs, training_schedule, trainable=True):\n        _, height, width, _ = inputs['input_a'].shape.as_list()\n        with tf.variable_scope('FlowNet2'):\n            # Forward pass through FlowNetCSS and FlowNetSD with weights frozen\n            net_css_predictions = self.net_css.model(inputs, training_schedule, trainable=False)\n            net_sd_predictions = self.net_sd.model(inputs, training_schedule, trainable=False)\n\n            def ChannelNorm(tensor):\n                sq = tf.square(tensor)\n                r_sum = tf.reduce_sum(sq, keep_dims=True, axis=3)\n                return tf.sqrt(r_sum)\n\n            sd_flow_norm = ChannelNorm(net_sd_predictions['flow'])\n            css_flow_norm = ChannelNorm(net_css_predictions['flow'])\n\n            flow_warp_sd = flow_warp(inputs['input_b'], net_sd_predictions['flow'])\n            img_diff_sd = inputs['input_a'] - flow_warp_sd\n            img_diff_sd_norm = ChannelNorm(img_diff_sd)\n\n            flow_warp_css = flow_warp(inputs['input_b'], net_css_predictions['flow'])\n            img_diff_css = inputs['input_a'] - flow_warp_css\n            img_diff_css_norm = ChannelNorm(img_diff_css)\n\n            input_to_fusion = tf.concat([inputs['input_a'],\n                                         net_sd_predictions['flow'],\n                                         net_css_predictions['flow'],\n                                         sd_flow_norm,\n                                         css_flow_norm,\n                                         img_diff_sd_norm,\n                                         img_diff_css_norm], axis=3)\n\n            # Fusion Network\n            with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n                                # Only backprop this network if trainable\n                                trainable=trainable,\n                                # He (aka MSRA) weight initialization\n                                weights_initializer=slim.variance_scaling_initializer(),\n                                activation_fn=LeakyReLU,\n                                # We will do our own padding to match the original Caffe code\n                                padding='VALID'):\n\n                weights_regularizer = slim.l2_regularizer(training_schedule['weight_decay'])\n                with slim.arg_scope([slim.conv2d], weights_regularizer=weights_regularizer):\n                    fuse_conv0 = slim.conv2d(pad(input_to_fusion), 64, 3, scope='fuse_conv0')\n                    fuse_conv1 = slim.conv2d(pad(fuse_conv0), 64, 3, stride=2, scope='fuse_conv1')\n                    fuse_conv1_1 = slim.conv2d(pad(fuse_conv1), 128, 3, scope='fuse_conv1_1')\n                    fuse_conv2 = slim.conv2d(pad(fuse_conv1_1), 128, 3,\n                                             stride=2, scope='fuse_conv2')\n                    fuse_conv2_1 = slim.conv2d(pad(fuse_conv2), 128, 3, scope='fuse_conv2_1')\n\n                    predict_flow2 = slim.conv2d(pad(fuse_conv2_1), 2, 3,\n                                                scope='predict_flow2',\n                                                activation_fn=None)\n                    fuse_deconv1 = antipad(slim.conv2d_transpose(fuse_conv2_1, 32, 4,\n                                                                 stride=2,\n                                                                 scope='fuse_deconv1'))\n                    fuse_upsample_flow2to1 = antipad(slim.conv2d_transpose(predict_flow2, 2, 4,\n                                                                           stride=2,\n                                                                           scope='fuse_upsample_flow2to1',\n                                                                           activation_fn=None))\n                    concat1 = tf.concat([fuse_conv1_1, fuse_deconv1,\n                                         fuse_upsample_flow2to1], axis=3)\n                    fuse_interconv1 = slim.conv2d(pad(concat1), 32, 3,\n                                                  activation_fn=None, scope='fuse_interconv1')\n\n                    predict_flow1 = slim.conv2d(pad(fuse_interconv1), 2, 3,\n                                                scope='predict_flow1',\n                                                activation_fn=None)\n                    fuse_deconv0 = antipad(slim.conv2d_transpose(concat1, 16, 4,\n                                                                 stride=2,\n                                                                 scope='fuse_deconv0'))\n                    fuse_upsample_flow1to0 = antipad(slim.conv2d_transpose(predict_flow1, 2, 4,\n                                                                           stride=2,\n                                                                           scope='fuse_upsample_flow1to0',\n                                                                           activation_fn=None))\n                    concat0 = tf.concat([fuse_conv0, fuse_deconv0, fuse_upsample_flow1to0], axis=3)\n                    fuse_interconv0 = slim.conv2d(pad(concat0), 16, 3,\n                                                  activation_fn=None, scope='fuse_interconv0')\n\n                    predict_flow0 = slim.conv2d(pad(fuse_interconv0), 2,\n                                                3, activation_fn=None, scope='predict_flow0')\n\n                    flow = tf.image.resize_bilinear(\n                        predict_flow0, tf.stack([height, width]), align_corners=True)\n                    return {\n                        'predict_flow0': predict_flow0,\n                        'flow': flow,\n                    }\n\n    def loss(self, flow, predictions):\n        # L2 loss between predict_flow0, true flow (weighted w/ 0.005)\n        predict_flow0 = predictions['predict_flow0']\n        size = [predict_flow0.shape[1], predict_flow0.shape[2]]\n        downsampled_flow0 = downsample(flow, size)\n        loss = average_endpoint_error(downsampled_flow0, predict_flow0)\n        tf.losses.add_loss(loss)\n\n        # Return the 'total' loss: loss fns + regularization terms defined in the model\n        return tf.losses.get_total_loss()\n"""
src/flownet2/test.py,0,"b""import argparse\nimport os\nfrom ..net import Mode\nfrom .flownet2 import FlowNet2\n\nFLAGS = None\n\n\ndef main():\n    # Create a new network\n    net = FlowNet2(mode=Mode.TEST)\n\n    # Train on the data\n    net.test(\n        checkpoint='./checkpoints/FlowNet2/flownet-2.ckpt-0',\n        input_a_path=FLAGS.input_a,\n        input_b_path=FLAGS.input_b,\n        out_path=FLAGS.out,\n    )\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--input_a',\n        type=str,\n        required=True,\n        help='Path to first image'\n    )\n    parser.add_argument(\n        '--input_b',\n        type=str,\n        required=True,\n        help='Path to second image'\n    )\n    parser.add_argument(\n        '--out',\n        type=str,\n        required=True,\n        help='Path to output flow result'\n    )\n    FLAGS = parser.parse_args()\n\n    # Verify arguments are valid\n    if not os.path.exists(FLAGS.input_a):\n        raise ValueError('image_a path must exist')\n    if not os.path.exists(FLAGS.input_b):\n        raise ValueError('image_b path must exist')\n    if not os.path.isdir(FLAGS.out):\n        raise ValueError('out directory must exist')\n    main()\n"""
src/flownet2/train.py,0,"b""from ..dataloader import load_batch\nfrom ..dataset_configs import FLYING_CHAIRS_DATASET_CONFIG\nfrom ..training_schedules import LONG_SCHEDULE\nfrom .flownet2 import FlowNet2\n\n# Create a new network\nnet = FlowNet2()\n\n# Load a batch of data\ninput_a, input_b, flow = load_batch(FLYING_CHAIRS_DATASET_CONFIG, 'sample', net.global_step)\n\n# Train on the data\nnet.train(\n    log_dir='./logs/flownet_2',\n    training_schedule=LONG_SCHEDULE,\n    input_a=input_a,\n    input_b=input_b,\n    flow=flow,\n    # Load trained weights for CSS and SD parts of network\n    checkpoints={\n        './checkpoints/FlowNetCSS-ft-sd/flownet-CSS-ft-sd.ckpt-0': ('FlowNet2/FlowNetCSS', 'FlowNet2'),\n        './checkpoints/FlowNetSD/flownet-SD.ckpt-0': ('FlowNet2/FlowNetSD', 'FlowNet2')\n    }\n)\n"""
src/flownet_c/__init__.py,0,b''
src/flownet_c/flownet_c.py,11,"b'from ..net import Net, Mode\nfrom ..utils import LeakyReLU, average_endpoint_error, pad, antipad\nfrom ..correlation import correlation\nfrom ..downsample import downsample\nimport math\nimport tensorflow as tf\nslim = tf.contrib.slim\n\n\nclass FlowNetC(Net):\n\n    def __init__(self, mode=Mode.TRAIN, debug=False):\n        super(FlowNetC, self).__init__(mode=mode, debug=debug)\n\n    def model(self, inputs, training_schedule, trainable=True):\n        _, height, width, _ = inputs[\'input_a\'].shape.as_list()\n        with tf.variable_scope(\'FlowNetC\'):\n            with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n                                # Only backprop this network if trainable\n                                trainable=trainable,\n                                # He (aka MSRA) weight initialization\n                                weights_initializer=slim.variance_scaling_initializer(),\n                                activation_fn=LeakyReLU,\n                                # We will do our own padding to match the original Caffe code\n                                padding=\'VALID\'):\n\n                weights_regularizer = slim.l2_regularizer(training_schedule[\'weight_decay\'])\n                with slim.arg_scope([slim.conv2d], weights_regularizer=weights_regularizer):\n                    with slim.arg_scope([slim.conv2d], stride=2):\n                        conv_a_1 = slim.conv2d(pad(inputs[\'input_a\'], 3), 64, 7, scope=\'conv1\')\n                        conv_a_2 = slim.conv2d(pad(conv_a_1, 2), 128, 5, scope=\'conv2\')\n                        conv_a_3 = slim.conv2d(pad(conv_a_2, 2), 256, 5, scope=\'conv3\')\n\n                        conv_b_1 = slim.conv2d(pad(inputs[\'input_b\'], 3),\n                                               64, 7, scope=\'conv1\', reuse=True)\n                        conv_b_2 = slim.conv2d(pad(conv_b_1, 2), 128, 5, scope=\'conv2\', reuse=True)\n                        conv_b_3 = slim.conv2d(pad(conv_b_2, 2), 256, 5, scope=\'conv3\', reuse=True)\n\n                        # Compute cross correlation with leaky relu activation\n                        cc = correlation(conv_a_3, conv_b_3, 1, 20, 1, 2, 20)\n                        cc_relu = LeakyReLU(cc)\n\n                    # Combine cross correlation results with convolution of feature map A\n                    netA_conv = slim.conv2d(conv_a_3, 32, 1, scope=\'conv_redir\')\n                    # Concatenate along the channels axis\n                    net = tf.concat([netA_conv, cc_relu], axis=3)\n\n                    conv3_1 = slim.conv2d(pad(net), 256, 3, scope=\'conv3_1\')\n                    with slim.arg_scope([slim.conv2d], num_outputs=512, kernel_size=3):\n                        conv4 = slim.conv2d(pad(conv3_1), stride=2, scope=\'conv4\')\n                        conv4_1 = slim.conv2d(pad(conv4), scope=\'conv4_1\')\n                        conv5 = slim.conv2d(pad(conv4_1), stride=2, scope=\'conv5\')\n                        conv5_1 = slim.conv2d(pad(conv5), scope=\'conv5_1\')\n                    conv6 = slim.conv2d(pad(conv5_1), 1024, 3, stride=2, scope=\'conv6\')\n                    conv6_1 = slim.conv2d(pad(conv6), 1024, 3, scope=\'conv6_1\')\n\n                    """""" START: Refinement Network """"""\n                    with slim.arg_scope([slim.conv2d_transpose], biases_initializer=None):\n                        predict_flow6 = slim.conv2d(pad(conv6_1), 2, 3,\n                                                    scope=\'predict_flow6\',\n                                                    activation_fn=None)\n                        deconv5 = antipad(slim.conv2d_transpose(conv6_1, 512, 4,\n                                                                stride=2,\n                                                                scope=\'deconv5\'))\n                        upsample_flow6to5 = antipad(slim.conv2d_transpose(predict_flow6, 2, 4,\n                                                                          stride=2,\n                                                                          scope=\'upsample_flow6to5\',\n                                                                          activation_fn=None))\n                        concat5 = tf.concat([conv5_1, deconv5, upsample_flow6to5], axis=3)\n\n                        predict_flow5 = slim.conv2d(pad(concat5), 2, 3,\n                                                    scope=\'predict_flow5\',\n                                                    activation_fn=None)\n                        deconv4 = antipad(slim.conv2d_transpose(concat5, 256, 4,\n                                                                stride=2,\n                                                                scope=\'deconv4\'))\n                        upsample_flow5to4 = antipad(slim.conv2d_transpose(predict_flow5, 2, 4,\n                                                                          stride=2,\n                                                                          scope=\'upsample_flow5to4\',\n                                                                          activation_fn=None))\n                        concat4 = tf.concat([conv4_1, deconv4, upsample_flow5to4], axis=3)\n\n                        predict_flow4 = slim.conv2d(pad(concat4), 2, 3,\n                                                    scope=\'predict_flow4\',\n                                                    activation_fn=None)\n                        deconv3 = antipad(slim.conv2d_transpose(concat4, 128, 4,\n                                                                stride=2,\n                                                                scope=\'deconv3\'))\n                        upsample_flow4to3 = antipad(slim.conv2d_transpose(predict_flow4, 2, 4,\n                                                                          stride=2,\n                                                                          scope=\'upsample_flow4to3\',\n                                                                          activation_fn=None))\n                        concat3 = tf.concat([conv3_1, deconv3, upsample_flow4to3], axis=3)\n\n                        predict_flow3 = slim.conv2d(pad(concat3), 2, 3,\n                                                    scope=\'predict_flow3\',\n                                                    activation_fn=None)\n                        deconv2 = antipad(slim.conv2d_transpose(concat3, 64, 4,\n                                                                stride=2,\n                                                                scope=\'deconv2\'))\n                        upsample_flow3to2 = antipad(slim.conv2d_transpose(predict_flow3, 2, 4,\n                                                                          stride=2,\n                                                                          scope=\'upsample_flow3to2\',\n                                                                          activation_fn=None))\n                        concat2 = tf.concat([conv_a_2, deconv2, upsample_flow3to2], axis=3)\n\n                        predict_flow2 = slim.conv2d(pad(concat2), 2, 3,\n                                                    scope=\'predict_flow2\',\n                                                    activation_fn=None)\n                    """""" END: Refinement Network """"""\n\n                    flow = predict_flow2 * 20.0\n                    # TODO: Look at Accum (train) or Resample (deploy) to see if we need to do something different\n                    flow = tf.image.resize_bilinear(flow,\n                                                    tf.stack([height, width]),\n                                                    align_corners=True)\n\n                    return {\n                        \'predict_flow6\': predict_flow6,\n                        \'predict_flow5\': predict_flow5,\n                        \'predict_flow4\': predict_flow4,\n                        \'predict_flow3\': predict_flow3,\n                        \'predict_flow2\': predict_flow2,\n                        \'flow\': flow,\n                    }\n\n    def loss(self, flow, predictions):\n        flow = flow * 0.05\n\n        losses = []\n        INPUT_HEIGHT, INPUT_WIDTH = float(flow.shape[1].value), float(flow.shape[2].value)\n\n        # L2 loss between predict_flow6, blob23 (weighted w/ 0.32)\n        predict_flow6 = predictions[\'predict_flow6\']\n        size = [predict_flow6.shape[1], predict_flow6.shape[2]]\n        downsampled_flow6 = downsample(flow, size)\n        losses.append(average_endpoint_error(downsampled_flow6, predict_flow6))\n\n        # L2 loss between predict_flow5, blob28 (weighted w/ 0.08)\n        predict_flow5 = predictions[\'predict_flow5\']\n        size = [predict_flow5.shape[1], predict_flow5.shape[2]]\n        downsampled_flow5 = downsample(flow, size)\n        losses.append(average_endpoint_error(downsampled_flow5, predict_flow5))\n\n        # L2 loss between predict_flow4, blob33 (weighted w/ 0.02)\n        predict_flow4 = predictions[\'predict_flow4\']\n        size = [predict_flow4.shape[1], predict_flow4.shape[2]]\n        downsampled_flow4 = downsample(flow, size)\n        losses.append(average_endpoint_error(downsampled_flow4, predict_flow4))\n\n        # L2 loss between predict_flow3, blob38 (weighted w/ 0.01)\n        predict_flow3 = predictions[\'predict_flow3\']\n        size = [predict_flow3.shape[1], predict_flow3.shape[2]]\n        downsampled_flow3 = downsample(flow, size)\n        losses.append(average_endpoint_error(downsampled_flow3, predict_flow3))\n\n        # L2 loss between predict_flow2, blob43 (weighted w/ 0.005)\n        predict_flow2 = predictions[\'predict_flow2\']\n        size = [predict_flow2.shape[1], predict_flow2.shape[2]]\n        downsampled_flow2 = downsample(flow, size)\n        losses.append(average_endpoint_error(downsampled_flow2, predict_flow2))\n\n        loss = tf.losses.compute_weighted_loss(losses, [0.32, 0.08, 0.02, 0.01, 0.005])\n\n        # Return the \'total\' loss: loss fns + regularization terms defined in the model\n        return tf.losses.get_total_loss()\n'"
src/flownet_c/test.py,0,"b""import argparse\nimport os\nfrom ..net import Mode\nfrom .flownet_c import FlowNetC\n\nFLAGS = None\n\n\ndef main():\n    # Create a new network\n    net = FlowNetC(mode=Mode.TEST)\n\n    # Train on the data\n    net.test(\n        checkpoint='./checkpoints/FlowNetC/flownet-C.ckpt-0',\n        input_a_path=FLAGS.input_a,\n        input_b_path=FLAGS.input_b,\n        out_path=FLAGS.out,\n    )\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--input_a',\n        type=str,\n        required=True,\n        help='Path to first image'\n    )\n    parser.add_argument(\n        '--input_b',\n        type=str,\n        required=True,\n        help='Path to second image'\n    )\n    parser.add_argument(\n        '--out',\n        type=str,\n        required=True,\n        help='Path to output flow result'\n    )\n    FLAGS = parser.parse_args()\n\n    # Verify arguments are valid\n    if not os.path.exists(FLAGS.input_a):\n        raise ValueError('image_a path must exist')\n    if not os.path.exists(FLAGS.input_b):\n        raise ValueError('image_b path must exist')\n    if not os.path.isdir(FLAGS.out):\n        raise ValueError('out directory must exist')\n    main()\n"""
src/flownet_c/train.py,0,"b""from ..dataloader import load_batch\nfrom ..dataset_configs import FLYING_CHAIRS_DATASET_CONFIG\nfrom ..training_schedules import LONG_SCHEDULE\nfrom .flownet_c import FlowNetC\n\n# Create a new network\nnet = FlowNetC()\n\n# Load a batch of data\ninput_a, input_b, flow = load_batch(FLYING_CHAIRS_DATASET_CONFIG, 'sample', net.global_step)\n\n# Train on the data\nnet.train(\n    log_dir='./logs/flownet_c',\n    training_schedule=LONG_SCHEDULE,\n    input_a=input_a,\n    input_b=input_b,\n    flow=flow\n)\n"""
src/flownet_cs/__init__.py,0,b''
src/flownet_cs/flownet_cs.py,4,"b""from ..net import Net, Mode\nfrom ..flownet_c.flownet_c import FlowNetC\nfrom ..flownet_s.flownet_s import FlowNetS\nfrom ..flow_warp import flow_warp\nimport tensorflow as tf\n\n\nclass FlowNetCS(Net):\n\n    def __init__(self, mode=Mode.TRAIN, debug=False):\n        self.net_c = FlowNetC(mode, debug)\n        self.net_s = FlowNetS(mode, debug)\n        super(FlowNetCS, self).__init__(mode=mode, debug=debug)\n\n    def model(self, inputs, training_schedule, trainable=True):\n        with tf.variable_scope('FlowNetCS'):\n            # Forward pass through FlowNetC with weights frozen\n            net_c_predictions = self.net_c.model(inputs, training_schedule, trainable=False)\n\n            # Perform flow warping (to move image B closer to image A based on flow prediction)\n            warped = flow_warp(inputs['input_b'], net_c_predictions['flow'])\n\n            # Compute brightness error: sqrt(sum (input_a - warped)^2 over channels)\n            brightness_error = inputs['input_a'] - warped\n            brightness_error = tf.square(brightness_error)\n            brightness_error = tf.reduce_sum(brightness_error, keep_dims=True, axis=3)\n            brightness_error = tf.sqrt(brightness_error)\n\n            # Gather all inputs to FlowNetS\n            inputs_to_s = {\n                'input_a': inputs['input_a'],\n                'input_b': inputs['input_b'],\n                'warped': warped,\n                'flow': net_c_predictions['flow'] * 0.05,\n                'brightness_error': brightness_error,\n            }\n\n            return self.net_s.model(inputs_to_s, training_schedule, trainable=trainable)\n\n    def loss(self, flow, predictions):\n        return self.net_s.loss(flow, predictions)\n"""
src/flownet_cs/test.py,0,"b""import argparse\nimport os\nfrom ..net import Mode\nfrom .flownet_cs import FlowNetCS\n\nFLAGS = None\n\n\ndef main():\n    # Create a new network\n    net = FlowNetCS(mode=Mode.TEST)\n\n    # Train on the data\n    net.test(\n        checkpoint='./checkpoints/FlowNetCS/flownet-CS.ckpt-0',\n        input_a_path=FLAGS.input_a,\n        input_b_path=FLAGS.input_b,\n        out_path=FLAGS.out,\n    )\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--input_a',\n        type=str,\n        required=True,\n        help='Path to first image'\n    )\n    parser.add_argument(\n        '--input_b',\n        type=str,\n        required=True,\n        help='Path to second image'\n    )\n    parser.add_argument(\n        '--out',\n        type=str,\n        required=True,\n        help='Path to output flow result'\n    )\n    FLAGS = parser.parse_args()\n\n    # Verify arguments are valid\n    if not os.path.exists(FLAGS.input_a):\n        raise ValueError('image_a path must exist')\n    if not os.path.exists(FLAGS.input_b):\n        raise ValueError('image_b path must exist')\n    if not os.path.isdir(FLAGS.out):\n        raise ValueError('out directory must exist')\n    main()\n"""
src/flownet_cs/train.py,0,"b""from ..dataloader import load_batch\nfrom ..dataset_configs import FLYING_CHAIRS_DATASET_CONFIG\nfrom ..training_schedules import LONG_SCHEDULE\nfrom .flownet_cs import FlowNetCS\n\n# Create a new network\nnet = FlowNetCS()\n\n# Load a batch of data\ninput_a, input_b, flow = load_batch(FLYING_CHAIRS_DATASET_CONFIG, 'sample', net.global_step)\n\n# Train on the data\nnet.train(\n    log_dir='./logs/flownet_cs',\n    training_schedule=LONG_SCHEDULE,\n    input_a=input_a,\n    input_b=input_b,\n    flow=flow,\n    # Load trained weights for C part of network\n    checkpoints={'./checkpoints/FlowNetC/flownet-C.ckpt-0': ('FlowNetCS/FlowNetC', 'FlowNetCS')}\n)\n"""
src/flownet_css/__init__.py,0,b''
src/flownet_css/flownet_css.py,4,"b""from ..net import Net, Mode\nfrom ..flownet_cs.flownet_cs import FlowNetCS\nfrom ..flownet_s.flownet_s import FlowNetS\nfrom ..flow_warp import flow_warp\nimport tensorflow as tf\n\n\nclass FlowNetCSS(Net):\n\n    def __init__(self, mode=Mode.TRAIN, debug=False):\n        self.net_cs = FlowNetCS(mode, debug)\n        self.net_s = FlowNetS(mode, debug)\n        super(FlowNetCSS, self).__init__(mode=mode, debug=debug)\n\n    def model(self, inputs, training_schedule, trainable=True):\n        with tf.variable_scope('FlowNetCSS'):\n            # Forward pass through FlowNetCS with weights frozen\n            net_cs_predictions = self.net_cs.model(inputs, training_schedule, trainable=False)\n\n            # Perform flow warping (to move image B closer to image A based on flow prediction)\n            warped = flow_warp(inputs['input_b'], net_cs_predictions['flow'])\n\n            # Compute brightness error: sqrt(sum (input_a - warped)^2 over channels)\n            brightness_error = inputs['input_a'] - warped\n            brightness_error = tf.square(brightness_error)\n            brightness_error = tf.reduce_sum(brightness_error, keep_dims=True, axis=3)\n            brightness_error = tf.sqrt(brightness_error)\n\n            # Gather all inputs to FlowNetS\n            inputs_to_s = {\n                'input_a': inputs['input_a'],\n                'input_b': inputs['input_b'],\n                'warped': warped,\n                'flow': net_cs_predictions['flow'] * 0.05,\n                'brightness_error': brightness_error,\n            }\n\n            return self.net_s.model(inputs_to_s, training_schedule, trainable=trainable)\n\n    def loss(self, flow, predictions):\n        return self.net_s.loss(flow, predictions)\n"""
src/flownet_css/test.py,0,"b""import argparse\nimport os\nfrom ..net import Mode\nfrom .flownet_css import FlowNetCSS\n\nFLAGS = None\n\n\ndef main():\n    # Create a new network\n    net = FlowNetCSS(mode=Mode.TEST)\n\n    # Train on the data\n    net.test(\n        checkpoint='./checkpoints/FlowNetCSS/flownet-CSS.ckpt-0',\n        input_a_path=FLAGS.input_a,\n        input_b_path=FLAGS.input_b,\n        out_path=FLAGS.out,\n    )\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--input_a',\n        type=str,\n        required=True,\n        help='Path to first image'\n    )\n    parser.add_argument(\n        '--input_b',\n        type=str,\n        required=True,\n        help='Path to second image'\n    )\n    parser.add_argument(\n        '--out',\n        type=str,\n        required=True,\n        help='Path to output flow result'\n    )\n    FLAGS = parser.parse_args()\n\n    # Verify arguments are valid\n    if not os.path.exists(FLAGS.input_a):\n        raise ValueError('image_a path must exist')\n    if not os.path.exists(FLAGS.input_b):\n        raise ValueError('image_b path must exist')\n    if not os.path.isdir(FLAGS.out):\n        raise ValueError('out directory must exist')\n    main()\n"""
src/flownet_css/train.py,0,"b""from ..dataloader import load_batch\nfrom ..dataset_configs import FLYING_CHAIRS_DATASET_CONFIG\nfrom ..training_schedules import LONG_SCHEDULE\nfrom .flownet_css import FlowNetCSS\n\n# Create a new network\nnet = FlowNetCSS()\n\n# Load a batch of data\ninput_a, input_b, flow = load_batch(FLYING_CHAIRS_DATASET_CONFIG, 'sample', net.global_step)\n\n# Train on the data\nnet.train(\n    log_dir='./logs/flownet_css',\n    training_schedule=LONG_SCHEDULE,\n    input_a=input_a,\n    input_b=input_b,\n    flow=flow,\n    # Load trained weights for CS part of network\n    checkpoints={\n        './checkpoints/FlowNetCS/flownet-CS.ckpt-0': ('FlowNetCSS/FlowNetCS', 'FlowNetCSS')}\n)\n"""
src/flownet_s/__init__.py,0,b''
src/flownet_s/flownet_s.py,12,"b'from ..net import Net, Mode\nfrom ..utils import LeakyReLU, average_endpoint_error, pad, antipad\nfrom ..downsample import downsample\nimport math\nimport tensorflow as tf\nslim = tf.contrib.slim\n\n\nclass FlowNetS(Net):\n\n    def __init__(self, mode=Mode.TRAIN, debug=False):\n        super(FlowNetS, self).__init__(mode=mode, debug=debug)\n\n    def model(self, inputs, training_schedule, trainable=True):\n        _, height, width, _ = inputs[\'input_a\'].shape.as_list()\n        stacked = False\n        with tf.variable_scope(\'FlowNetS\'):\n            if \'warped\' in inputs and \'flow\' in inputs and \'brightness_error\' in inputs:\n                stacked = True\n                concat_inputs = tf.concat([inputs[\'input_a\'],\n                                           inputs[\'input_b\'],\n                                           inputs[\'warped\'],\n                                           inputs[\'flow\'],\n                                           inputs[\'brightness_error\']], axis=3)\n            else:\n                concat_inputs = tf.concat([inputs[\'input_a\'], inputs[\'input_b\']], axis=3)\n            with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n                                # Only backprop this network if trainable\n                                trainable=trainable,\n                                # He (aka MSRA) weight initialization\n                                weights_initializer=slim.variance_scaling_initializer(),\n                                activation_fn=LeakyReLU,\n                                # We will do our own padding to match the original Caffe code\n                                padding=\'VALID\'):\n\n                weights_regularizer = slim.l2_regularizer(training_schedule[\'weight_decay\'])\n                with slim.arg_scope([slim.conv2d], weights_regularizer=weights_regularizer):\n                    with slim.arg_scope([slim.conv2d], stride=2):\n                        conv_1 = slim.conv2d(pad(concat_inputs, 3), 64, 7, scope=\'conv1\')\n                        conv_2 = slim.conv2d(pad(conv_1, 2), 128, 5, scope=\'conv2\')\n                        conv_3 = slim.conv2d(pad(conv_2, 2), 256, 5, scope=\'conv3\')\n\n                    conv3_1 = slim.conv2d(pad(conv_3), 256, 3, scope=\'conv3_1\')\n                    with slim.arg_scope([slim.conv2d], num_outputs=512, kernel_size=3):\n                        conv4 = slim.conv2d(pad(conv3_1), stride=2, scope=\'conv4\')\n                        conv4_1 = slim.conv2d(pad(conv4), scope=\'conv4_1\')\n                        conv5 = slim.conv2d(pad(conv4_1), stride=2, scope=\'conv5\')\n                        conv5_1 = slim.conv2d(pad(conv5), scope=\'conv5_1\')\n                    conv6 = slim.conv2d(pad(conv5_1), 1024, 3, stride=2, scope=\'conv6\')\n                    conv6_1 = slim.conv2d(pad(conv6), 1024, 3, scope=\'conv6_1\')\n\n                    """""" START: Refinement Network """"""\n                    with slim.arg_scope([slim.conv2d_transpose], biases_initializer=None):\n                        predict_flow6 = slim.conv2d(pad(conv6_1), 2, 3,\n                                                    scope=\'predict_flow6\',\n                                                    activation_fn=None)\n                        deconv5 = antipad(slim.conv2d_transpose(conv6_1, 512, 4,\n                                                                stride=2,\n                                                                scope=\'deconv5\'))\n                        upsample_flow6to5 = antipad(slim.conv2d_transpose(predict_flow6, 2, 4,\n                                                                          stride=2,\n                                                                          scope=\'upsample_flow6to5\',\n                                                                          activation_fn=None))\n                        concat5 = tf.concat([conv5_1, deconv5, upsample_flow6to5], axis=3)\n\n                        predict_flow5 = slim.conv2d(pad(concat5), 2, 3,\n                                                    scope=\'predict_flow5\',\n                                                    activation_fn=None)\n                        deconv4 = antipad(slim.conv2d_transpose(concat5, 256, 4,\n                                                                stride=2,\n                                                                scope=\'deconv4\'))\n                        upsample_flow5to4 = antipad(slim.conv2d_transpose(predict_flow5, 2, 4,\n                                                                          stride=2,\n                                                                          scope=\'upsample_flow5to4\',\n                                                                          activation_fn=None))\n                        concat4 = tf.concat([conv4_1, deconv4, upsample_flow5to4], axis=3)\n\n                        predict_flow4 = slim.conv2d(pad(concat4), 2, 3,\n                                                    scope=\'predict_flow4\',\n                                                    activation_fn=None)\n                        deconv3 = antipad(slim.conv2d_transpose(concat4, 128, 4,\n                                                                stride=2,\n                                                                scope=\'deconv3\'))\n                        upsample_flow4to3 = antipad(slim.conv2d_transpose(predict_flow4, 2, 4,\n                                                                          stride=2,\n                                                                          scope=\'upsample_flow4to3\',\n                                                                          activation_fn=None))\n                        concat3 = tf.concat([conv3_1, deconv3, upsample_flow4to3], axis=3)\n\n                        predict_flow3 = slim.conv2d(pad(concat3), 2, 3,\n                                                    scope=\'predict_flow3\',\n                                                    activation_fn=None)\n                        deconv2 = antipad(slim.conv2d_transpose(concat3, 64, 4,\n                                                                stride=2,\n                                                                scope=\'deconv2\'))\n                        upsample_flow3to2 = antipad(slim.conv2d_transpose(predict_flow3, 2, 4,\n                                                                          stride=2,\n                                                                          scope=\'upsample_flow3to2\',\n                                                                          activation_fn=None))\n                        concat2 = tf.concat([conv_2, deconv2, upsample_flow3to2], axis=3)\n\n                        predict_flow2 = slim.conv2d(pad(concat2), 2, 3,\n                                                    scope=\'predict_flow2\',\n                                                    activation_fn=None)\n                    """""" END: Refinement Network """"""\n\n                    flow = predict_flow2 * 20.0\n                    # TODO: Look at Accum (train) or Resample (deploy) to see if we need to do something different\n                    flow = tf.image.resize_bilinear(flow,\n                                                    tf.stack([height, width]),\n                                                    align_corners=True)\n\n                    return {\n                        \'predict_flow6\': predict_flow6,\n                        \'predict_flow5\': predict_flow5,\n                        \'predict_flow4\': predict_flow4,\n                        \'predict_flow3\': predict_flow3,\n                        \'predict_flow2\': predict_flow2,\n                        \'flow\': flow,\n                    }\n\n    def loss(self, flow, predictions):\n        flow = flow * 0.05\n\n        losses = []\n        INPUT_HEIGHT, INPUT_WIDTH = float(flow.shape[1].value), float(flow.shape[2].value)\n\n        # L2 loss between predict_flow6, blob23 (weighted w/ 0.32)\n        predict_flow6 = predictions[\'predict_flow6\']\n        size = [predict_flow6.shape[1], predict_flow6.shape[2]]\n        downsampled_flow6 = downsample(flow, size)\n        losses.append(average_endpoint_error(downsampled_flow6, predict_flow6))\n\n        # L2 loss between predict_flow5, blob28 (weighted w/ 0.08)\n        predict_flow5 = predictions[\'predict_flow5\']\n        size = [predict_flow5.shape[1], predict_flow5.shape[2]]\n        downsampled_flow5 = downsample(flow, size)\n        losses.append(average_endpoint_error(downsampled_flow5, predict_flow5))\n\n        # L2 loss between predict_flow4, blob33 (weighted w/ 0.02)\n        predict_flow4 = predictions[\'predict_flow4\']\n        size = [predict_flow4.shape[1], predict_flow4.shape[2]]\n        downsampled_flow4 = downsample(flow, size)\n        losses.append(average_endpoint_error(downsampled_flow4, predict_flow4))\n\n        # L2 loss between predict_flow3, blob38 (weighted w/ 0.01)\n        predict_flow3 = predictions[\'predict_flow3\']\n        size = [predict_flow3.shape[1], predict_flow3.shape[2]]\n        downsampled_flow3 = downsample(flow, size)\n        losses.append(average_endpoint_error(downsampled_flow3, predict_flow3))\n\n        # L2 loss between predict_flow2, blob43 (weighted w/ 0.005)\n        predict_flow2 = predictions[\'predict_flow2\']\n        size = [predict_flow2.shape[1], predict_flow2.shape[2]]\n        downsampled_flow2 = downsample(flow, size)\n        losses.append(average_endpoint_error(downsampled_flow2, predict_flow2))\n\n        loss = tf.losses.compute_weighted_loss(losses, [0.32, 0.08, 0.02, 0.01, 0.005])\n\n        # Return the \'total\' loss: loss fns + regularization terms defined in the model\n        return tf.losses.get_total_loss()\n'"
src/flownet_s/test.py,0,"b""import argparse\nimport os\nfrom ..net import Mode\nfrom .flownet_s import FlowNetS\n\nFLAGS = None\n\n\ndef main():\n    # Create a new network\n    net = FlowNetS(mode=Mode.TEST)\n\n    # Train on the data\n    net.test(\n        checkpoint='./checkpoints/FlowNetS/flownet-S.ckpt-0',\n        input_a_path=FLAGS.input_a,\n        input_b_path=FLAGS.input_b,\n        out_path=FLAGS.out,\n    )\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--input_a',\n        type=str,\n        required=True,\n        help='Path to first image'\n    )\n    parser.add_argument(\n        '--input_b',\n        type=str,\n        required=True,\n        help='Path to second image'\n    )\n    parser.add_argument(\n        '--out',\n        type=str,\n        required=True,\n        help='Path to output flow result'\n    )\n    FLAGS = parser.parse_args()\n\n    # Verify arguments are valid\n    if not os.path.exists(FLAGS.input_a):\n        raise ValueError('image_a path must exist')\n    if not os.path.exists(FLAGS.input_b):\n        raise ValueError('image_b path must exist')\n    if not os.path.isdir(FLAGS.out):\n        raise ValueError('out directory must exist')\n    main()\n"""
src/flownet_s/train.py,0,"b""from ..dataloader import load_batch\nfrom ..dataset_configs import FLYING_CHAIRS_DATASET_CONFIG\nfrom ..training_schedules import LONG_SCHEDULE\nfrom .flownet_s import FlowNetS\n\n# Create a new network\nnet = FlowNetS()\n\n# Load a batch of data\ninput_a, input_b, flow = load_batch(FLYING_CHAIRS_DATASET_CONFIG, 'sample', net.global_step)\n\n# Train on the data\nnet.train(\n    log_dir='./logs/flownet_s_sample',\n    training_schedule=LONG_SCHEDULE,\n    input_a=input_a,\n    input_b=input_b,\n    flow=flow\n)\n"""
src/flownet_sd/__init__.py,0,b''
src/flownet_sd/flownet_sd.py,11,"b'from ..net import Net, Mode\nfrom ..utils import LeakyReLU, average_endpoint_error, pad, antipad\nfrom ..downsample import downsample\nimport math\nimport tensorflow as tf\nslim = tf.contrib.slim\n\n\nclass FlowNetSD(Net):\n\n    def __init__(self, mode=Mode.TRAIN, debug=False):\n        super(FlowNetSD, self).__init__(mode=mode, debug=debug)\n\n    def model(self, inputs, training_schedule, trainable=True):\n        _, height, width, _ = inputs[\'input_a\'].shape.as_list()\n        with tf.variable_scope(\'FlowNetSD\'):\n            concat_inputs = tf.concat([inputs[\'input_a\'], inputs[\'input_b\']], axis=3)\n            with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n                                # Only backprop this network if trainable\n                                trainable=trainable,\n                                # He (aka MSRA) weight initialization\n                                weights_initializer=slim.variance_scaling_initializer(),\n                                activation_fn=LeakyReLU,\n                                # We will do our own padding to match the original Caffe code\n                                padding=\'VALID\'):\n\n                weights_regularizer = slim.l2_regularizer(training_schedule[\'weight_decay\'])\n                with slim.arg_scope([slim.conv2d], weights_regularizer=weights_regularizer):\n                    conv0 = slim.conv2d(pad(concat_inputs), 64, 3, scope=\'conv0\')\n                    conv1 = slim.conv2d(pad(conv0), 64, 3, stride=2, scope=\'conv1\')\n                    conv1_1 = slim.conv2d(pad(conv1), 128, 3, scope=\'conv1_1\')\n                    conv2 = slim.conv2d(pad(conv1_1), 128, 3, stride=2, scope=\'conv2\')\n                    conv2_1 = slim.conv2d(pad(conv2), 128, 3, scope=\'conv2_1\')\n                    conv3 = slim.conv2d(pad(conv2_1), 256, 3, stride=2, scope=\'conv3\')\n                    conv3_1 = slim.conv2d(pad(conv3), 256, 3, scope=\'conv3_1\')\n                    conv4 = slim.conv2d(pad(conv3_1), 512, 3, stride=2, scope=\'conv4\')\n                    conv4_1 = slim.conv2d(pad(conv4), 512, 3, scope=\'conv4_1\')\n                    conv5 = slim.conv2d(pad(conv4_1), 512, 3, stride=2, scope=\'conv5\')\n                    conv5_1 = slim.conv2d(pad(conv5), 512, 3, scope=\'conv5_1\')\n                    conv6 = slim.conv2d(pad(conv5_1), 1024, 3, stride=2, scope=\'conv6\')\n                    conv6_1 = slim.conv2d(pad(conv6), 1024, 3, scope=\'conv6_1\')\n\n                    """""" START: Refinement Network """"""\n                    with slim.arg_scope([slim.conv2d_transpose], biases_initializer=None):\n                        predict_flow6 = slim.conv2d(pad(conv6_1), 2, 3,\n                                                    scope=\'predict_flow6\',\n                                                    activation_fn=None)\n                        deconv5 = antipad(slim.conv2d_transpose(conv6_1, 512, 4,\n                                                                stride=2,\n                                                                scope=\'deconv5\'))\n                        upsample_flow6to5 = antipad(slim.conv2d_transpose(predict_flow6, 2, 4,\n                                                                          stride=2,\n                                                                          scope=\'upsample_flow6to5\',\n                                                                          activation_fn=None))\n                        concat5 = tf.concat([conv5_1, deconv5, upsample_flow6to5], axis=3)\n                        interconv5 = slim.conv2d(pad(concat5), 512, 3,\n                                                 activation_fn=None, scope=\'interconv5\')\n\n                        predict_flow5 = slim.conv2d(pad(interconv5), 2, 3,\n                                                    scope=\'predict_flow5\',\n                                                    activation_fn=None)\n                        deconv4 = antipad(slim.conv2d_transpose(concat5, 256, 4,\n                                                                stride=2,\n                                                                scope=\'deconv4\'))\n                        upsample_flow5to4 = antipad(slim.conv2d_transpose(predict_flow5, 2, 4,\n                                                                          stride=2,\n                                                                          scope=\'upsample_flow5to4\',\n                                                                          activation_fn=None))\n                        concat4 = tf.concat([conv4_1, deconv4, upsample_flow5to4], axis=3)\n                        interconv4 = slim.conv2d(pad(concat4), 256, 3,\n                                                 activation_fn=None, scope=\'interconv4\')\n\n                        predict_flow4 = slim.conv2d(pad(interconv4), 2, 3,\n                                                    scope=\'predict_flow4\',\n                                                    activation_fn=None)\n                        deconv3 = antipad(slim.conv2d_transpose(concat4, 128, 4,\n                                                                stride=2,\n                                                                scope=\'deconv3\'))\n                        upsample_flow4to3 = antipad(slim.conv2d_transpose(predict_flow4, 2, 4,\n                                                                          stride=2,\n                                                                          scope=\'upsample_flow4to3\',\n                                                                          activation_fn=None))\n                        concat3 = tf.concat([conv3_1, deconv3, upsample_flow4to3], axis=3)\n                        interconv3 = slim.conv2d(pad(concat3), 128, 3,\n                                                 activation_fn=None, scope=\'interconv3\')\n\n                        predict_flow3 = slim.conv2d(pad(interconv3), 2, 3,\n                                                    scope=\'predict_flow3\',\n                                                    activation_fn=None)\n                        deconv2 = antipad(slim.conv2d_transpose(concat3, 64, 4,\n                                                                stride=2,\n                                                                scope=\'deconv2\'))\n                        upsample_flow3to2 = antipad(slim.conv2d_transpose(predict_flow3, 2, 4,\n                                                                          stride=2,\n                                                                          scope=\'upsample_flow3to2\',\n                                                                          activation_fn=None))\n                        concat2 = tf.concat([conv2, deconv2, upsample_flow3to2], axis=3)\n                        interconv2 = slim.conv2d(pad(concat2), 64, 3,\n                                                 activation_fn=None, scope=\'interconv2\')\n\n                        predict_flow2 = slim.conv2d(pad(interconv2), 2, 3,\n                                                    scope=\'predict_flow2\',\n                                                    activation_fn=None)\n                    """""" END: Refinement Network """"""\n\n                    flow = predict_flow2 * 0.05\n                    # TODO: Look at Accum (train) or Resample (deploy) to see if we need to do something different\n                    flow = tf.image.resize_bilinear(flow,\n                                                    tf.stack([height, width]),\n                                                    align_corners=True)\n\n                    return {\n                        \'predict_flow6\': predict_flow6,\n                        \'predict_flow5\': predict_flow5,\n                        \'predict_flow4\': predict_flow4,\n                        \'predict_flow3\': predict_flow3,\n                        \'predict_flow2\': predict_flow2,\n                        \'flow\': flow,\n                    }\n\n    def loss(self, flow, predictions):\n        flow = flow * 20.0\n\n        losses = []\n        INPUT_HEIGHT, INPUT_WIDTH = float(flow.shape[1].value), float(flow.shape[2].value)\n\n        # L2 loss between predict_flow6, blob23 (weighted w/ 0.32)\n        predict_flow6 = predictions[\'predict_flow6\']\n        size = [predict_flow6.shape[1], predict_flow6.shape[2]]\n        downsampled_flow6 = downsample(flow, size)\n        losses.append(average_endpoint_error(downsampled_flow6, predict_flow6))\n\n        # L2 loss between predict_flow5, blob28 (weighted w/ 0.08)\n        predict_flow5 = predictions[\'predict_flow5\']\n        size = [predict_flow5.shape[1], predict_flow5.shape[2]]\n        downsampled_flow5 = downsample(flow, size)\n        losses.append(average_endpoint_error(downsampled_flow5, predict_flow5))\n\n        # L2 loss between predict_flow4, blob33 (weighted w/ 0.02)\n        predict_flow4 = predictions[\'predict_flow4\']\n        size = [predict_flow4.shape[1], predict_flow4.shape[2]]\n        downsampled_flow4 = downsample(flow, size)\n        losses.append(average_endpoint_error(downsampled_flow4, predict_flow4))\n\n        # L2 loss between predict_flow3, blob38 (weighted w/ 0.01)\n        predict_flow3 = predictions[\'predict_flow3\']\n        size = [predict_flow3.shape[1], predict_flow3.shape[2]]\n        downsampled_flow3 = downsample(flow, size)\n        losses.append(average_endpoint_error(downsampled_flow3, predict_flow3))\n\n        # L2 loss between predict_flow2, blob43 (weighted w/ 0.005)\n        predict_flow2 = predictions[\'predict_flow2\']\n        size = [predict_flow2.shape[1], predict_flow2.shape[2]]\n        downsampled_flow2 = downsample(flow, size)\n        losses.append(average_endpoint_error(downsampled_flow2, predict_flow2))\n\n        loss = tf.losses.compute_weighted_loss(losses, [0.32, 0.08, 0.02, 0.01, 0.005])\n\n        # Return the \'total\' loss: loss fns + regularization terms defined in the model\n        return tf.losses.get_total_loss()\n'"
src/flownet_sd/test.py,0,"b""import argparse\nimport os\nfrom ..net import Mode\nfrom .flownet_sd import FlowNetSD\n\nFLAGS = None\n\n\ndef main():\n    # Create a new network\n    net = FlowNetSD(mode=Mode.TEST)\n\n    # Train on the data\n    net.test(\n        checkpoint='./checkpoints/FlowNetSD/flownet-SD.ckpt-0',\n        input_a_path=FLAGS.input_a,\n        input_b_path=FLAGS.input_b,\n        out_path=FLAGS.out,\n    )\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--input_a',\n        type=str,\n        required=True,\n        help='Path to first image'\n    )\n    parser.add_argument(\n        '--input_b',\n        type=str,\n        required=True,\n        help='Path to second image'\n    )\n    parser.add_argument(\n        '--out',\n        type=str,\n        required=True,\n        help='Path to output flow result'\n    )\n    FLAGS = parser.parse_args()\n\n    # Verify arguments are valid\n    if not os.path.exists(FLAGS.input_a):\n        raise ValueError('image_a path must exist')\n    if not os.path.exists(FLAGS.input_b):\n        raise ValueError('image_b path must exist')\n    if not os.path.isdir(FLAGS.out):\n        raise ValueError('out directory must exist')\n    main()\n"""
src/flownet_sd/train.py,0,"b""from ..dataloader import load_batch\nfrom ..dataset_configs import FLYING_CHAIRS_DATASET_CONFIG\nfrom ..training_schedules import LONG_SCHEDULE\nfrom .flownet_sd import FlowNetSD\n\n# Create a new network\nnet = FlowNetSD()\n\n# Load a batch of data\ninput_a, input_b, flow = load_batch(FLYING_CHAIRS_DATASET_CONFIG, 'sample', net.global_step)\n\n# Train on the data\nnet.train(\n    log_dir='./logs/flownet_sd_sample',\n    training_schedule=LONG_SCHEDULE,\n    input_a=input_a,\n    input_b=input_b,\n    flow=flow\n)\n"""
