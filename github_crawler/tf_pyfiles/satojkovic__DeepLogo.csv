file_path,api_count,code
config.py,0,"b""import os\n\nTRAIN_DIR = 'flickr_logos_27_dataset'\nIMAGES_DIR = os.path.join(TRAIN_DIR, 'flickr_logos_27_dataset_images')\nCROPPED_IMAGES_DIR = os.path.join(TRAIN_DIR, 'flickr_logos_27_dataset_images_cropped')\nANNOT_FILE = os.path.join(TRAIN_DIR, 'flickr_logos_27_dataset_training_set_annotation.txt')\nCROPPED_ANNOT_FILE = os.path.join(TRAIN_DIR, 'flickr_logos_27_dataset_training_set_annotation_cropped.txt')\nCROPPED_ANNOT_FILE_TEST = os.path.join(TRAIN_DIR, 'flickr_logos_27_dataset_test_set_annotation_cropped.txt')\n\nCLASS_NAMES = [\n    'Adidas', 'Apple', 'BMW', 'Citroen', 'Cocacola', 'DHL', 'Fedex', 'Ferrari',\n    'Ford', 'Google', 'HP', 'Heineken', 'Intel', 'McDonalds', 'Mini', 'Nbc',\n    'Nike', 'Pepsi', 'Porsche', 'Puma', 'RedBull', 'Sprite', 'Starbucks',\n    'Texaco', 'Unicef', 'Vodafone', 'Yahoo'\n]\nCLASS_NAMES_FILE = os.path.join(TRAIN_DIR, 'flickr_logos_27_dataset_class_names.txt')\n"""
gen_tfrecord.py,3,"b'#!/usr/bin/env python\n# -*- coding=utf-8 -*-\n\nimport argparse\nimport os\nimport numpy as np\nimport io\nfrom PIL import Image\nimport config\n\nimport tensorflow as tf\nfrom object_detection.utils import dataset_util\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--train_or_test\', required=True, help=\'Generate tfrecord for train or test\')\n    parser.add_argument(\'--csv_input\', required=True, help=\'Path to the csv input\')\n    parser.add_argument(\'--img_dir\', required=True, help=\'Path to the image directory\')\n    parser.add_argument(\'--output_path\', required=True, help=\'Path to output tfrecord\')\n    return parser.parse_args()\n\n\ndef create_tf_example(csv, img_dir):\n    img_fname = csv[0]\n    x1, y1, x2, y2 = list(map(int, csv[1:-1]))\n    cls_idx = int(csv[-1])\n    cls_text = config.CLASS_NAMES[cls_idx].encode(\'utf8\')\n    with tf.gfile.GFile(os.path.join(img_dir, img_fname), \'rb\') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    width, height = image.size\n\n    xmin = [x1 / width]\n    xmax = [x2 / width]\n    ymin = [y1 / height]\n    ymax = [y2 / height]\n    cls_text = [cls_text]\n    cls_idx = [cls_idx]\n\n    filename = img_fname.encode(\'utf8\')\n    image_format = b\'jpg\'\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/height\': dataset_util.int64_feature(height),\n        \'image/width\': dataset_util.int64_feature(width),\n        \'image/filename\': dataset_util.bytes_feature(filename),\n        \'image/source_id\': dataset_util.bytes_feature(filename),\n        \'image/encoded\': dataset_util.bytes_feature(encoded_jpg),\n        \'image/format\': dataset_util.bytes_feature(image_format),\n        \'image/object/bbox/xmin\': dataset_util.float_list_feature(xmin),\n        \'image/object/bbox/xmax\': dataset_util.float_list_feature(xmax),\n        \'image/object/bbox/ymin\': dataset_util.float_list_feature(ymin),\n        \'image/object/bbox/ymax\': dataset_util.float_list_feature(ymax),\n        \'image/object/class/text\': dataset_util.bytes_list_feature(cls_text),\n        \'image/object/class/label\': dataset_util.int64_list_feature(cls_idx),        \n    }))\n\n    return tf_example\n\n\nif __name__ == ""__main__"":\n    args = parse_arguments()\n    train_or_test = args.train_or_test.lower()\n\n    writer = tf.python_io.TFRecordWriter(args.output_path)\n    csvs = np.loadtxt(args.csv_input, dtype=str, delimiter=\',\')\n    img_fnames = set()\n    num_data = 0\n    for csv in csvs:\n        img_fname = csv[0]\n        tf_example = create_tf_example(csv, args.img_dir)\n        if train_or_test == \'train\' or (train_or_test == \'test\' and not img_fname in img_fnames):\n            writer.write(tf_example.SerializeToString())\n            num_data += 1\n        img_fnames.add(img_fname)\n\n    writer.close()\n    print(\'Generated ({} imgs): {}\'.format(num_data, args.output_path))'"
gen_tfrecord_logos32plus.py,4,"b'#!/usr/bin/env python\n# -*- coding=utf-8 -*-\n\nimport argparse\nimport os\nimport numpy as np\nimport io\nfrom PIL import Image\nimport config\nimport scipy.io as sio\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nfrom object_detection.utils import dataset_util\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gt_mat\', required=True, help=\'Path to grountruth.mat\')\n    parser.add_argument(\'--img_dir\', required=True, help=\'Path to image directory\')\n    return parser.parse_args()\n\n\ndef create_tf_example(img_fname, logo_name, bbox, img_dir, logo_names):\n    x1, y1, w, h = list(map(int, bbox))\n    x2, y2 = x1 + w, y1 + h\n    cls_idx = logo_names[logo_name]\n    cls_text = logo_name.encode(\'utf8\')\n    with tf.gfile.GFile(os.path.join(img_dir, img_fname), \'rb\') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    width, height = image.size\n\n    xmin = [x1 / width]\n    xmax = [x2 / width]\n    ymin = [y1 / height]\n    ymax = [y2 / height]\n    cls_text = [cls_text]\n    cls_idx = [cls_idx]\n\n    filename = img_fname.encode(\'utf8\')\n    image_format = b\'jpg\'\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/height\': dataset_util.int64_feature(height),\n        \'image/width\': dataset_util.int64_feature(width),\n        \'image/filename\': dataset_util.bytes_feature(filename),\n        \'image/source_id\': dataset_util.bytes_feature(filename),\n        \'image/encoded\': dataset_util.bytes_feature(encoded_jpg),\n        \'image/format\': dataset_util.bytes_feature(image_format),\n        \'image/object/bbox/xmin\': dataset_util.float_list_feature(xmin),\n        \'image/object/bbox/xmax\': dataset_util.float_list_feature(xmax),\n        \'image/object/bbox/ymin\': dataset_util.float_list_feature(ymin),\n        \'image/object/bbox/ymax\': dataset_util.float_list_feature(ymax),\n        \'image/object/class/text\': dataset_util.bytes_list_feature(cls_text),\n        \'image/object/class/label\': dataset_util.int64_list_feature(cls_idx),        \n    }))\n\n    return tf_example\n\n\nif __name__ == ""__main__"":\n    args = parse_arguments()\n\n    gts = sio.loadmat(args.gt_mat)\n    logo_names = {gt[2][0] for gt in gts[\'groundtruth\'][0]}\n    logo_names = sorted(list(logo_names))\n    logo_names = {name: i for i, name in enumerate(logo_names)}\n    gt_train, gt_test = train_test_split(gts[\'groundtruth\'][0])\n\n    train_writer = tf.python_io.TFRecordWriter(\'train_logos32plus.tfrecord\')\n    for gt in tqdm(gt_train):\n        img_fname = gt[0][0].replace(\'\\\\\', \'/\')\n        logo_name = gt[2][0]\n        for bbox in gt[1]:\n            tf_example = create_tf_example(img_fname, logo_name, bbox, args.img_dir, logo_names)\n            train_writer.write(tf_example.SerializeToString())\n    train_writer.close()\n\n    test_writer = tf.python_io.TFRecordWriter(\'test_logos32plus.tfrecord\')\n    num_data = 0\n    for gt in tqdm(gt_test):\n        img_fname = gt[0][0].replace(\'\\\\\', \'/\')\n        logo_name = gt[2][0]\n        for bbox in gt[1]:\n            tf_example = create_tf_example(img_fname, logo_name, bbox, args.img_dir, logo_names)\n            test_writer.write(tf_example.SerializeToString())\n            break\n        num_data += 1\n    test_writer.close()\n    print(\'Test ({} imgs)\'.format(num_data))\n'"
logo_detection.py,17,"b'#!/usr/bin/env python\n# coding: utf-8\n\nimport numpy as np\nimport os\nimport six.moves.urllib as urllib\nimport sys\nimport tarfile\nimport tensorflow as tf\nimport zipfile\nimport cv2\n\nfrom distutils.version import StrictVersion\nfrom collections import defaultdict\nfrom io import StringIO\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nfrom object_detection.utils import ops as utils_ops\n\nif StrictVersion(tf.__version__) < StrictVersion(\'1.12.0\'):\n  raise ImportError(\'Please upgrade your TensorFlow installation to v1.12.*.\')\n\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import visualization_utils as vis_util\n\nimport argparse\n\ndef parse_arguments():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--model_name\', required=True, help=\'Path to model\')\n  parser.add_argument(\'--label_map\', required=True, help=\'Path to label map text\')\n  parser.add_argument(\'--test_annot_text\', required=True, help=\'Path to test annotation text\')\n  parser.add_argument(\'--test_image_dir\', required=True, help=\'Path to test image directory\')\n  parser.add_argument(\'--output_dir\', required=True, help=\'Path to output directory\')\n  return parser.parse_args()\n\ndef load_image_into_numpy_array(image):\n  (im_width, im_height) = image.size\n  return np.array(image.getdata()).reshape(\n      (im_height, im_width, 3)).astype(np.uint8)\n\ndef run_inference_for_single_image(image, graph):\n  with graph.as_default():\n    with tf.Session() as sess:\n      # Get handles to input and output tensors\n      ops = tf.get_default_graph().get_operations()\n      all_tensor_names = {output.name for op in ops for output in op.outputs}\n      tensor_dict = {}\n      for key in [\n          \'num_detections\', \'detection_boxes\', \'detection_scores\',\n          \'detection_classes\', \'detection_masks\'\n      ]:\n        tensor_name = key + \':0\'\n        if tensor_name in all_tensor_names:\n          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n              tensor_name)\n      if \'detection_masks\' in tensor_dict:\n        # The following processing is only for single image\n        detection_boxes = tf.squeeze(tensor_dict[\'detection_boxes\'], [0])\n        detection_masks = tf.squeeze(tensor_dict[\'detection_masks\'], [0])\n        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n        real_num_detection = tf.cast(tensor_dict[\'num_detections\'][0], tf.int32)\n        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n            detection_masks, detection_boxes, image.shape[1], image.shape[2])\n        detection_masks_reframed = tf.cast(\n            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n        # Follow the convention by adding back the batch dimension\n        tensor_dict[\'detection_masks\'] = tf.expand_dims(\n            detection_masks_reframed, 0)\n      image_tensor = tf.get_default_graph().get_tensor_by_name(\'image_tensor:0\')\n\n      # Run inference\n      output_dict = sess.run(tensor_dict,\n                             feed_dict={image_tensor: image})\n\n      # all outputs are float32 numpy arrays, so convert types as appropriate\n      output_dict[\'num_detections\'] = int(output_dict[\'num_detections\'][0])\n      output_dict[\'detection_classes\'] = output_dict[\n          \'detection_classes\'][0].astype(np.int64)\n      output_dict[\'detection_boxes\'] = output_dict[\'detection_boxes\'][0]\n      output_dict[\'detection_scores\'] = output_dict[\'detection_scores\'][0]\n      if \'detection_masks\' in output_dict:\n        output_dict[\'detection_masks\'] = output_dict[\'detection_masks\'][0]\n  return output_dict\n\nif __name__ == ""__main__"":\n  args = parse_arguments()\n  MODEL_NAME = args.model_name\n\n  # Path to frozen detection graph. This is the actual model that is used for the object detection.\n  PATH_TO_FROZEN_GRAPH = MODEL_NAME + \'/frozen_inference_graph.pb\'\n\n  # List of the strings that is used to add correct label for each box.\n  PATH_TO_LABELS = args.label_map\n\n  detection_graph = tf.Graph()\n  with detection_graph.as_default():\n    od_graph_def = tf.GraphDef()\n    with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, \'rb\') as fid:\n      serialized_graph = fid.read()\n      od_graph_def.ParseFromString(serialized_graph)\n      tf.import_graph_def(od_graph_def, name=\'\')\n\n  category_index = label_map_util.create_category_index_from_labelmap(\n    PATH_TO_LABELS, use_display_name=True)\n\n  data = np.loadtxt(args.test_annot_text, delimiter=\',\', dtype=str)\n  test_set_images = data[:, 0]\n  print(len(test_set_images))\n\n  PATH_TO_TEST_IMAGES_DIR = args.test_image_dir\n  TEST_IMAGE_PATHS = [os.path.join(PATH_TO_TEST_IMAGES_DIR, img) for img in test_set_images]\n\n  # Size, in inches, of the output images.\n  IMAGE_SIZE = (12, 8)\n\n  if not os.path.exists(os.path.abspath(args.output_dir)):\n    os.makedirs(args.output_dir)\n\n  for i, image_path in enumerate(TEST_IMAGE_PATHS):\n    image = Image.open(image_path)\n    # the array based representation of the image will be used later in order to prepare the\n    # result image with boxes and labels on it.\n    image_np = load_image_into_numpy_array(image)\n    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n    image_np_expanded = np.expand_dims(image_np, axis=0)\n    # Actual detection.\n    output_dict = run_inference_for_single_image(image_np_expanded, detection_graph)\n    # Visualization of the results of a detection.\n    vis_util.visualize_boxes_and_labels_on_image_array(\n        image_np,\n        output_dict[\'detection_boxes\'],\n        output_dict[\'detection_classes\'],\n        output_dict[\'detection_scores\'],\n        category_index,\n        instance_masks=output_dict.get(\'detection_masks\'),\n        use_normalized_coordinates=True,\n        line_thickness=8)\n    plt.figure(figsize=IMAGE_SIZE)\n    plt.imshow(image_np)\n    plt.savefig(os.path.join(args.output_dir, \'detect_results_\' + str(i).zfill(3) + \'.png\'))\n\n'"
preproc_annot.py,0,"b'#!/usr/bin/env python\n# -*- coding=utf-8 -*-\n\nimport pandas as pd\nfrom skimage import io\nimport config\nimport os\nimport warnings\nimport numpy as np\n\ndef main():\n    # Load annot file\n    logos_frame = pd.read_csv(config.ANNOT_FILE, header=None, delim_whitespace=True)\n    print(\'Num. of annots:\', len(logos_frame))\n\n    if not os.path.exists(config.CROPPED_IMAGES_DIR):\n        os.makedirs(config.CROPPED_IMAGES_DIR)\n\n    num_cropped_images = 0\n    annots = []\n    for i, row in logos_frame.iterrows():\n        img_name = row[0]\n        cls_name = row[1]\n        cls_idx = config.CLASS_NAMES.index(cls_name)\n        subset = row[2]\n        x1, y1, x2, y2 = row[3:]\n        w, h = (x2 - x1), (y2 - y1)\n        if w == 0 or h == 0:\n            print(\'Skip:\', img_name)\n            continue\n        img = io.imread(os.path.join(config.IMAGES_DIR, img_name))\n        img_height, img_width, _ = img.shape\n        x = (x1 + x2) / 2\n        y = (y1 + y1) / 2\n        annot = \',\'.join([img_name, str(x1), str(y1), str(x2), str(y2), str(cls_idx)])\n        annots.append(annot)\n        num_cropped_images += 1\n\n    np.random.shuffle(annots)\n    num_train = int(num_cropped_images * 0.8)\n    with open(config.CROPPED_ANNOT_FILE, \'w\') as f:\n        for annot in annots[:num_train]:\n            f.writelines(annot)\n            f.writelines(""\\n"")\n\n    seen = set()\n    num_test = 0\n    with open(config.CROPPED_ANNOT_FILE_TEST, \'w\') as f:\n        for annot in annots[num_train:]:\n            img_fn = annot.split(\',\')[0]\n            if img_fn in seen:\n                continue\n            f.writelines(annot)\n            f.writelines(""\\n"")\n            seen.add(img_fn)\n            num_test += 1\n    print(\'Num. of annotations: {}(train) {}(test)\'.format(num_train, num_test))\n    print(\'Created: {}\'.format(config.CROPPED_ANNOT_FILE))\n    print(\'Created: {}\'.format(config.CROPPED_ANNOT_FILE_TEST))\n\nif __name__ == ""__main__"":\n    with warnings.catch_warnings():\n        # Supress low contrast warnings\n        warnings.simplefilter(""ignore"")\n\n        # Crop logo images\n        main()'"
