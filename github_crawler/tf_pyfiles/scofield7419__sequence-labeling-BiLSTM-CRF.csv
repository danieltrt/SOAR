file_path,api_count,code
main.py,0,"b'# -*- coding: utf-8 -*-\n# @Time : 2019/6/2 \xe4\xb8\x8a\xe5\x8d\x8810:55\n# @Author : Scofield Phil\n# @FileName: main.py\n# @Project: sequence-lableing-vex\n\nfrom __future__ import print_function\nimport argparse\nimport random\nimport numpy as np\nimport os\nfrom engines.BiLSTM_CRFs import BiLSTM_CRFs\nfrom engines.DataManager import DataManager\nfrom engines.Configer import Configer\nfrom engines.utils import get_logger\n\n\ndef set_env(configs):\n    random.seed(configs.seed)\n    np.random.seed(configs.seed)\n\n\ndef fold_check(configs):\n    datasets_fold = \'datasets_fold\'\n    assert hasattr(configs, datasets_fold), ""item datasets_fold not configured""\n\n    if not os.path.exists(configs.datasets_fold):\n        print(""datasets fold not found"")\n        exit(1)\n\n    checkpoints_dir = \'checkpoints_dir\'\n    if not os.path.exists(configs.checkpoints_dir) or \\\n            not hasattr(configs, checkpoints_dir):\n        print(""checkpoints fold not found, creating..."")\n        cides = configs.checkpoints_dir.split(\'/\')\n        if len(cides) == 2 and os.path.exists(cides[0]) \\\n                and not os.path.exists(configs.checkpoints_dir):\n            os.mkdir(configs.checkpoints_dir)\n        else:\n            os.mkdir(""checkpoints"")\n\n    vocabs_dir = \'vocabs_dir\'\n    if not os.path.exists(configs.vocabs_dir):\n        print(""vocabs fold not found, creating..."")\n        if hasattr(configs, vocabs_dir):\n            os.mkdir(configs.vocabs_dir)\n        else:\n            os.mkdir(configs.datasets_fold + ""/vocabs"")\n\n    log_dir = \'log_dir\'\n    if not os.path.exists(configs.log_dir):\n        print(""log fold not found, creating..."")\n        if hasattr(configs, log_dir):\n            os.mkdir(configs.log_dir)\n        else:\n            os.mkdir(configs.datasets_fold + ""/vocabs"")\n\n\ndef predict_single(model, sentence):\n    sentence_tokens, entities, entities_type, entities_index = model.predict_single(sentence)\n    if configs.label_level == 1:\n        logger.info(\n            ""\\nExtracted entities:\\n %s\\n\\n"" % (""\\n"".join(entities)))\n    elif configs.label_level == 2:\n        logger.info(\n            ""\\nExtracted entities:\\n %s\\n\\n"" % (""\\n"".join([a + ""\\t(%s)"" % b for a, b in zip(entities, entities_type)])))\n\n\ndef start_api():\n    cmd_new = r\'cd demo_webapp; python manage.py runserver %s:%s\' % (configs.ip, configs.port)\n    res = os.system(cmd_new)\n    try:\n        logger.info(res)\n    except:\n        logger.info(res)\n\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Tuning with BiLSTM+CRF\')\n    parser.add_argument(\'--config_file\', default=\'system.config\', help=\'Configuration File\')\n    args = parser.parse_args()\n    configs = Configer(config_file=args.config_file)\n\n    fold_check(configs)\n    logger = get_logger(configs.log_dir)\n    configs.show_data_summary(logger)\n    set_env(configs)\n\n    mode = configs.mode.lower()\n\n    if mode == \'api_service\':\n        logger.info(""mode: api service"")\n        start_api()\n    else:\n        dataManager = DataManager(configs, logger)\n        model = BiLSTM_CRFs(configs, logger, dataManager)\n        if mode == \'train\':\n            logger.info(""mode: train"")\n            model.train()\n        elif mode == \'test\':\n            logger.info(""mode: test"")\n            model.test()\n        elif mode == \'interactive_predict\':\n            logger.info(""mode: predict_one"")\n            model.soft_load()\n            while True:\n                logger.info(""please input a sentence (enter `exit\' to exit.)\\n"")\n                sentence = input()\n                if sentence == \'exit\':\n                    break\n                logger.info(sentence)\n                predict_single(model, sentence)\n'"
demo_webapp/manage.py,0,"b'#!/usr/bin/env python\nimport os\nimport sys\n\nif __name__ == ""__main__"":\n    os.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""demo_webapp.settings"")\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError:\n        # The above import may fail for some other reason. Ensure that the\n        # issue is really that Django is missing to avoid masking other\n        # exceptions on Python 2.\n        try:\n            import django\n        except ImportError:\n            raise ImportError(\n                ""Couldn\'t import Django. Are you sure it\'s installed and ""\n                ""available on your PYTHONPATH environment variable? Did you ""\n                ""forget to activate a virtual environment?""\n            )\n        raise\n    execute_from_command_line(sys.argv)\n'"
engines/BiLSTM_CRFs.py,75,"b'# -*- coding: utf-8 -*-\r\n# @Time : 2019/6/2 \xe4\xb8\x8a\xe5\x8d\x8810:55\r\n# @Author : Scofield Phil\r\n# @FileName: BiLSTM_CRFs.py\r\n# @Project: sequence-lableing-vex\r\n\r\nimport math, os\r\nfrom engines.utils import metrics, save_csv_, extractEntity\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport time\r\n\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\r\n\r\n\r\nclass BiLSTM_CRFs(object):\r\n    def __init__(self, configs, logger, dataManager):\r\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = configs.CUDA_VISIBLE_DEVICES\r\n\r\n        self.configs = configs\r\n        self.logger = logger\r\n        self.logdir = configs.log_dir\r\n        self.measuring_metrics = configs.measuring_metrics\r\n        self.dataManager = dataManager\r\n\r\n        if configs.mode == ""train"":\r\n            self.is_training = True\r\n        else:\r\n            self.is_training = False\r\n\r\n        self.checkpoint_name = configs.checkpoint_name\r\n        self.checkpoints_dir = configs.checkpoints_dir\r\n        self.output_test_file = configs.datasets_fold + ""/"" + configs.output_test_file\r\n        self.is_output_sentence_entity = configs.is_output_sentence_entity\r\n        self.output_sentence_entity_file = configs.datasets_fold + ""/"" + configs.output_sentence_entity_file\r\n\r\n        self.biderectional = configs.biderectional\r\n        self.cell_type = configs.cell_type\r\n        self.num_layers = configs.encoder_layers\r\n\r\n        self.is_crf = configs.use_crf\r\n\r\n        self.learning_rate = configs.learning_rate\r\n        self.dropout_rate = configs.dropout\r\n        self.batch_size = configs.batch_size\r\n\r\n        self.emb_dim = configs.embedding_dim\r\n        self.hidden_dim = configs.hidden_dim\r\n\r\n        if configs.cell_type == \'LSTM\':\r\n            if self.biderectional:\r\n                self.cell = tf.nn.rnn_cell.LSTMCell(self.hidden_dim)\r\n            else:\r\n                self.cell = tf.nn.rnn_cell.LSTMCell(2 * self.hidden_dim)\r\n        else:\r\n            if self.biderectional:\r\n                self.cell = tf.nn.rnn_cell.GRUCell(self.hidden_dim)\r\n            else:\r\n                self.cell = tf.nn.rnn_cell.GRUCell(2 * self.hidden_dim)\r\n\r\n        self.is_attention = configs.use_self_attention\r\n        self.attention_dim = configs.attention_dim\r\n\r\n        self.num_epochs = configs.epoch\r\n        self.max_time_steps = configs.max_sequence_length\r\n\r\n        self.num_tokens = dataManager.max_token_number\r\n        self.num_classes = dataManager.max_label_number\r\n\r\n        self.is_early_stop = configs.is_early_stop\r\n        self.patient = configs.patient\r\n\r\n        self.max_to_keep = configs.checkpoints_max_to_keep\r\n        self.print_per_batch = configs.print_per_batch\r\n        self.best_f1_val = 0\r\n\r\n        if configs.optimizer == \'Adagrad\':\r\n            self.optimizer = tf.train.AdagradOptimizer(self.learning_rate)\r\n        elif configs.optimizer == \'Adadelta\':\r\n            self.optimizer = tf.train.AdadeltaOptimizer(self.learning_rate)\r\n        elif configs.optimizer == \'RMSprop\':\r\n            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\r\n        elif configs.optimizer == \'GD\':\r\n            self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\r\n        else:\r\n            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\r\n\r\n        self.initializer = tf.contrib.layers.xavier_initializer()\r\n        self.global_step = tf.Variable(0, trainable=False, name=""global_step"", dtype=tf.int32)\r\n\r\n        if configs.use_pretrained_embedding:\r\n            embedding_matrix = dataManager.getEmbedding(configs.token_emb_dir)\r\n            self.embedding = tf.Variable(embedding_matrix, trainable=False, name=""emb"", dtype=tf.float32)\r\n        else:\r\n            self.embedding = tf.get_variable(""emb"", [self.num_tokens, self.emb_dim], trainable=True,\r\n                                             initializer=self.initializer)\r\n\r\n        self.build()\r\n        self.logger.info(""model initialed...\\n"")\r\n\r\n        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\r\n\r\n    def build(self):\r\n        self.inputs = tf.placeholder(tf.int32, [None, self.max_time_steps])\r\n        self.targets = tf.placeholder(tf.int32, [None, self.max_time_steps])\r\n\r\n        self.inputs_emb = tf.nn.embedding_lookup(self.embedding, self.inputs)\r\n        self.inputs_emb = tf.transpose(self.inputs_emb, [1, 0, 2])\r\n        self.inputs_emb = tf.reshape(self.inputs_emb, [-1, self.emb_dim])\r\n        self.inputs_emb = tf.split(self.inputs_emb, self.max_time_steps, 0)\r\n\r\n        # lstm cell\r\n        if self.biderectional:\r\n            lstm_cell_fw = self.cell\r\n            lstm_cell_bw = self.cell\r\n\r\n            # dropout\r\n            if self.is_training:\r\n                lstm_cell_fw = tf.nn.rnn_cell.DropoutWrapper(lstm_cell_fw, output_keep_prob=(1 - self.dropout_rate))\r\n                lstm_cell_bw = tf.nn.rnn_cell.DropoutWrapper(lstm_cell_bw, output_keep_prob=(1 - self.dropout_rate))\r\n\r\n            lstm_cell_fw = tf.nn.rnn_cell.MultiRNNCell([lstm_cell_fw] * self.num_layers)\r\n            lstm_cell_bw = tf.nn.rnn_cell.MultiRNNCell([lstm_cell_bw] * self.num_layers)\r\n\r\n            # get the length of each sample\r\n            self.length = tf.reduce_sum(tf.sign(self.inputs), reduction_indices=1)\r\n            self.length = tf.cast(self.length, tf.int32)\r\n\r\n            # forward and backward\r\n            outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(\r\n                lstm_cell_fw,\r\n                lstm_cell_bw,\r\n                self.inputs_emb,\r\n                dtype=tf.float32,\r\n                sequence_length=self.length\r\n            )\r\n\r\n        else:\r\n            lstm_cell = self.cell\r\n            if self.is_training:\r\n                lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=(1 - self.dropout_rate))\r\n            lstm_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * self.num_layers)\r\n            self.length = tf.reduce_sum(tf.sign(self.inputs), reduction_indices=1)\r\n            self.length = tf.cast(self.length, tf.int32)\r\n\r\n            outputs, _ = tf.contrib.rnn.static_rnn(\r\n                lstm_cell,\r\n                self.inputs_emb,\r\n                dtype=tf.float32,\r\n                sequence_length=self.length\r\n            )\r\n        # outputs: list_steps[batch, 2*dim]\r\n        outputs = tf.concat(outputs, 1)\r\n        outputs = tf.reshape(outputs, [self.batch_size, self.max_time_steps, self.hidden_dim * 2])\r\n\r\n        # self attention module\r\n        if self.is_attention:\r\n            H1 = tf.reshape(outputs, [-1, self.hidden_dim * 2])\r\n            W_a1 = tf.get_variable(""W_a1"", shape=[self.hidden_dim * 2, self.attention_dim],\r\n                                   initializer=self.initializer, trainable=True)\r\n            u1 = tf.matmul(H1, W_a1)\r\n\r\n            H2 = tf.reshape(tf.identity(outputs), [-1, self.hidden_dim * 2])\r\n            W_a2 = tf.get_variable(""W_a2"", shape=[self.hidden_dim * 2, self.attention_dim],\r\n                                   initializer=self.initializer, trainable=True)\r\n            u2 = tf.matmul(H2, W_a2)\r\n\r\n            u1 = tf.reshape(u1, [self.batch_size, self.max_time_steps, self.hidden_dim * 2])\r\n            u2 = tf.reshape(u2, [self.batch_size, self.max_time_steps, self.hidden_dim * 2])\r\n            u = tf.matmul(u1, u2, transpose_b=True)\r\n\r\n            # Array of weights for each time step\r\n            A = tf.nn.softmax(u, name=""attention"")\r\n            outputs = tf.matmul(A, tf.reshape(tf.identity(outputs),\r\n                                              [self.batch_size, self.max_time_steps, self.hidden_dim * 2]))\r\n\r\n        # linear\r\n        self.outputs = tf.reshape(outputs, [-1, self.hidden_dim * 2])\r\n        self.softmax_w = tf.get_variable(""softmax_w"", [self.hidden_dim * 2, self.num_classes],\r\n                                         initializer=self.initializer)\r\n        self.softmax_b = tf.get_variable(""softmax_b"", [self.num_classes], initializer=self.initializer)\r\n        self.logits = tf.matmul(self.outputs, self.softmax_w) + self.softmax_b\r\n\r\n        self.logits = tf.reshape(self.logits, [self.batch_size, self.max_time_steps, self.num_classes])\r\n        # print(self.logits.get_shape().as_list())\r\n        if not self.is_crf:\r\n            # softmax\r\n            softmax_out = tf.nn.softmax(self.logits, axis=-1)\r\n\r\n            self.batch_pred_sequence = tf.cast(tf.argmax(softmax_out, -1), tf.int32)\r\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.targets)\r\n            mask = tf.sequence_mask(self.length)\r\n\r\n            self.losses = tf.boolean_mask(losses, mask)\r\n\r\n            self.loss = tf.reduce_mean(losses)\r\n        else:\r\n            # crf\r\n            self.log_likelihood, self.transition_params = tf.contrib.crf.crf_log_likelihood(\r\n                self.logits, self.targets, self.length)\r\n            self.batch_pred_sequence, self.batch_viterbi_score = tf.contrib.crf.crf_decode(self.logits,\r\n                                                                                           self.transition_params,\r\n                                                                                           self.length)\r\n\r\n            self.loss = tf.reduce_mean(-self.log_likelihood)\r\n\r\n        self.train_summary = tf.summary.scalar(""loss"", self.loss)\r\n        self.dev_summary = tf.summary.scalar(""loss"", self.loss)\r\n\r\n        self.opt_op = self.optimizer.minimize(self.loss, global_step=self.global_step)\r\n\r\n    def train(self):\r\n        X_train, y_train, X_val, y_val = self.dataManager.getTrainingSet()\r\n        tf.initialize_all_variables().run(session=self.sess)\r\n\r\n        saver = tf.train.Saver(max_to_keep=self.max_to_keep)\r\n        tf.summary.merge_all()\r\n        train_writer = tf.summary.FileWriter(self.logdir + ""/training_loss"", self.sess.graph)\r\n        dev_writer = tf.summary.FileWriter(self.logdir + ""/validating_loss"", self.sess.graph)\r\n\r\n        num_iterations = int(math.ceil(1.0 * len(X_train) / self.batch_size))\r\n        num_val_iterations = int(math.ceil(1.0 * len(X_val) / self.batch_size))\r\n\r\n        cnt = 0\r\n        cnt_dev = 0\r\n        unprogressed = 0\r\n        very_start_time = time.time()\r\n        best_at_epoch = 0\r\n        self.logger.info(""\\ntraining starting"" + (""+"" * 20))\r\n        for epoch in range(self.num_epochs):\r\n            start_time = time.time()\r\n            # shuffle train at each epoch\r\n            sh_index = np.arange(len(X_train))\r\n            np.random.shuffle(sh_index)\r\n            X_train = X_train[sh_index]\r\n            y_train = y_train[sh_index]\r\n\r\n            self.logger.info(""\\ncurrent epoch: %d"" % (epoch))\r\n            for iteration in range(num_iterations):\r\n                X_train_batch, y_train_batch = self.dataManager.nextBatch(X_train, y_train,\r\n                                                                          start_index=iteration * self.batch_size)\r\n                _, loss_train, train_batch_viterbi_sequence, train_summary = \\\r\n                    self.sess.run([\r\n                        self.opt_op,\r\n                        self.loss,\r\n                        self.batch_pred_sequence,\r\n                        self.train_summary\r\n                    ],\r\n                        feed_dict={\r\n                            self.inputs: X_train_batch,\r\n                            self.targets: y_train_batch,\r\n                        })\r\n\r\n                if iteration % self.print_per_batch == 0:\r\n                    cnt += 1\r\n                    train_writer.add_summary(train_summary, cnt)\r\n\r\n                    measures = metrics(X_train_batch, y_train_batch,\r\n                                       train_batch_viterbi_sequence,\r\n                                       self.measuring_metrics, self.dataManager)\r\n\r\n                    res_str = \'\'\r\n                    for k, v in measures.items():\r\n                        res_str += (k + "": %.3f "" % v)\r\n                    self.logger.info(""training batch: %5d, loss: %.5f, %s"" % (iteration, loss_train, res_str))\r\n\r\n            # validation\r\n            loss_vals = list()\r\n            val_results = dict()\r\n            for measu in self.measuring_metrics:\r\n                val_results[measu] = 0\r\n\r\n            for iterr in range(num_val_iterations):\r\n                cnt_dev += 1\r\n                X_val_batch, y_val_batch = self.dataManager.nextBatch(X_val, y_val, start_index=iterr * self.batch_size)\r\n\r\n                loss_val, val_batch_viterbi_sequence, dev_summary = \\\r\n                    self.sess.run([\r\n                        self.loss,\r\n                        self.batch_pred_sequence,\r\n                        self.dev_summary\r\n                    ],\r\n                        feed_dict={\r\n                            self.inputs: X_val_batch,\r\n                            self.targets: y_val_batch,\r\n                        })\r\n\r\n                measures = metrics(X_val_batch, y_val_batch, val_batch_viterbi_sequence,\r\n                                   self.measuring_metrics, self.dataManager)\r\n                dev_writer.add_summary(dev_summary, cnt_dev)\r\n\r\n                for k, v in measures.items():\r\n                    val_results[k] += v\r\n                loss_vals.append(loss_val)\r\n\r\n            time_span = (time.time() - start_time) / 60\r\n            val_res_str = \'\'\r\n            dev_f1_avg = 0\r\n            for k, v in val_results.items():\r\n                val_results[k] /= num_val_iterations\r\n                val_res_str += (k + "": %.3f "" % val_results[k])\r\n                if k == \'f1\': dev_f1_avg = val_results[k]\r\n\r\n            self.logger.info(""time consumption:%.2f(min),  validation loss: %.5f, %s"" %\r\n                             (time_span, np.array(loss_vals).mean(), val_res_str))\r\n            if np.array(dev_f1_avg).mean() > self.best_f1_val:\r\n                unprogressed = 0\r\n                self.best_f1_val = np.array(dev_f1_avg).mean()\r\n                best_at_epoch = epoch\r\n                saver.save(self.sess, self.checkpoints_dir + ""/"" + self.checkpoint_name, global_step=self.global_step)\r\n                self.logger.info(""saved the new best model with f1: %.3f"" % (self.best_f1_val))\r\n            else:\r\n                unprogressed += 1\r\n\r\n            if self.is_early_stop:\r\n                if unprogressed >= self.patient:\r\n                    self.logger.info(""early stopped, no progress obtained within %d epochs"" % self.patient)\r\n                    self.logger.info(""overall best f1 is %f at %d epoch"" % (self.best_f1_val, best_at_epoch))\r\n                    self.logger.info(\r\n                        ""total training time consumption: %.3f(min)"" % ((time.time() - very_start_time) / 60))\r\n                    self.sess.close()\r\n                    return\r\n        self.logger.info(""overall best f1 is %f at %d epoch"" % (self.best_f1_val, best_at_epoch))\r\n        self.logger.info(""total training time consumption: %.3f(min)"" % ((time.time() - very_start_time) / 60))\r\n        self.sess.close()\r\n\r\n    def test(self):\r\n        X_test, y_test_psyduo_label, X_test_str = self.dataManager.getTestingSet()\r\n\r\n        num_iterations = int(math.ceil(1.0 * len(X_test) / self.batch_size))\r\n        self.logger.info(""total number of testing iterations: "" + str(num_iterations))\r\n\r\n        self.logger.info(""loading model parameter\\n"")\r\n        tf.initialize_all_variables().run(session=self.sess)\r\n        saver = tf.train.Saver()\r\n        saver.restore(self.sess, tf.train.latest_checkpoint(self.checkpoints_dir))\r\n\r\n        tokens = []\r\n        labels = []\r\n        entities = []\r\n        entities_types = []\r\n        self.logger.info(""\\ntesting starting"" + (""+"" * 20))\r\n        for i in range(num_iterations):\r\n            self.logger.info(""batch: "" + str(i + 1))\r\n            X_test_batch = X_test[i * self.batch_size: (i + 1) * self.batch_size]\r\n            X_test_str_batch = X_test_str[i * self.batch_size: (i + 1) * self.batch_size]\r\n            y_test_psyduo_label_batch = y_test_psyduo_label[i * self.batch_size: (i + 1) * self.batch_size]\r\n\r\n            if i == num_iterations - 1 and len(X_test_batch) < self.batch_size:\r\n                X_test_batch = list(X_test_batch)\r\n                X_test_str_batch = list(X_test_str_batch)\r\n                y_test_psyduo_label_batch = list(y_test_psyduo_label_batch)\r\n                gap = self.batch_size - len(X_test_batch)\r\n\r\n                X_test_batch += [[0 for j in range(self.max_time_steps)] for i in range(gap)]\r\n                X_test_str_batch += [[\'x\' for j in range(self.max_time_steps)] for i in\r\n                                     range(gap)]\r\n                y_test_psyduo_label_batch += [[self.dataManager.label2id[\'O\'] for j in range(self.max_time_steps)] for i\r\n                                              in range(gap)]\r\n                X_test_batch = np.array(X_test_batch)\r\n                X_test_str_batch = np.array(X_test_str_batch)\r\n                y_test_psyduo_label_batch = np.array(y_test_psyduo_label_batch)\r\n                results, token, entity, entities_type, _ = self.predictBatch(self.sess, X_test_batch,\r\n                                                                             y_test_psyduo_label_batch,\r\n                                                                             X_test_str_batch)\r\n                results = results[:len(X_test_batch)]\r\n                token = token[:len(X_test_batch)]\r\n                entity = entity[:len(X_test_batch)]\r\n                entities_type = entities_type[:len(X_test_batch)]\r\n            else:\r\n                results, token, entity, entities_type, _ = self.predictBatch(self.sess, X_test_batch,\r\n                                                                             y_test_psyduo_label_batch,\r\n                                                                             X_test_str_batch)\r\n\r\n            labels.extend(results)\r\n            tokens.extend(token)\r\n            entities.extend(entity)\r\n            entities_types.extend(entities_type)\r\n\r\n        def save_test_out(tokens, labels):\r\n            # transform format\r\n            newtokens, newlabels = [], []\r\n            for to, la in zip(tokens, labels):\r\n                newtokens.extend(to)\r\n                newtokens.append("""")\r\n                newlabels.extend(la)\r\n                newlabels.append("""")\r\n            # save\r\n            save_csv_(pd.DataFrame({""token"": newtokens, ""label"": newlabels}), self.output_test_file, [""token"", ""label""],\r\n                      delimiter=self.configs.delimiter)\r\n\r\n        save_test_out(tokens, labels)\r\n        self.logger.info(""testing results saved.\\n"")\r\n\r\n        if self.is_output_sentence_entity:\r\n            with open(self.output_sentence_entity_file, ""w"", encoding=\'utf-8\') as outfile:\r\n                for i in range(len(entities)):\r\n                    if self.configs.label_level == 1:\r\n                        outfile.write(\' \'.join(tokens[i]) + ""\\n"" + ""\\n"".join(entities[i]) + ""\\n\\n"")\r\n                    elif self.configs.label_level == 2:\r\n                        outfile.write(\' \'.join(tokens[i]) + ""\\n"" + ""\\n"".join(\r\n                            [a + ""\\t(%s)"" % b for a, b in zip(entities[i], entities_types[i])]) + ""\\n\\n"")\r\n\r\n            self.logger.info(""testing results with sentences&entities saved.\\n"")\r\n\r\n        self.sess.close()\r\n\r\n    def predict_single(self, sentence):\r\n        X, Sentence, Y = self.dataManager.prepare_single_sentence(sentence)\r\n        _, tokens, entitys, predicts_labels_entitylevel, indexs = self.predictBatch(self.sess, X, Y, Sentence)\r\n        return tokens[0], entitys[0], predicts_labels_entitylevel[0], indexs[0]\r\n\r\n    def predictBatch(self, sess, X, y_psydo_label, X_test_str_batch):\r\n        entity_list = []\r\n        tokens = []\r\n        predicts_labels_entitylevel = []\r\n        indexs = []\r\n        predicts_labels_tokenlevel = []\r\n\r\n        predicts_label_id, lengths = \\\r\n            sess.run([\r\n                self.batch_pred_sequence,\r\n                self.length\r\n            ],\r\n                feed_dict={\r\n                    self.inputs: X,\r\n                    self.targets: y_psydo_label,\r\n                })\r\n\r\n        for i in range(len(lengths)):\r\n            x_ = [val for val in X_test_str_batch[i, 0:lengths[i]]]\r\n            tokens.append(x_)\r\n\r\n            y_pred = [str(self.dataManager.id2label[val]) for val in predicts_label_id[i, 0:lengths[i]]]\r\n            predicts_labels_tokenlevel.append(y_pred)\r\n\r\n            entitys, entity_labels, labled_indexs = extractEntity(x_, y_pred, self.dataManager)\r\n            entity_list.append(entitys)\r\n            predicts_labels_entitylevel.append(entity_labels)\r\n            indexs.append(labled_indexs)\r\n\r\n        return predicts_labels_tokenlevel, tokens, entity_list, predicts_labels_entitylevel, indexs\r\n\r\n    def soft_load(self):\r\n        self.logger.info(""loading model parameter"")\r\n        tf.initialize_all_variables().run(session=self.sess)\r\n        saver = tf.train.Saver()\r\n        saver.restore(self.sess, tf.train.latest_checkpoint(self.checkpoints_dir))\r\n        self.logger.info(""loading model successfully"")\r\n'"
engines/Configer.py,0,"b'# -*- coding: utf-8 -*-\n# @Time : 2019/6/2 \xe4\xb8\x8a\xe5\x8d\x8810:52\n# @Author : Scofield Phil\n# @FileName: Parser.py\n# @Project: sequence-lableing-vex\nimport sys\n\n\nclass Configer:\n    def __init__(self, config_file=\'system.config\'):\n        config = self.config_file_to_dict(config_file)\n\n        ## Status:\n        the_item = \'mode\'\n        if the_item in config:\n            self.mode = config[the_item]\n\n        ## Datasets(Input/Output):\n        the_item = \'datasets_fold\'\n        if the_item in config:\n            self.datasets_fold = config[the_item]\n        the_item = \'train_file\'\n        if the_item in config:\n            self.train_file = config[the_item]\n            the_item = \'dev_file\'\n        if the_item in config:\n            self.dev_file = config[the_item]\n        the_item = \'test_file\'\n        if the_item in config:\n            self.test_file = config[the_item]\n        the_item = \'delimiter\'\n        if the_item in config:\n            self.delimiter = config[the_item]\n\n        the_item = \'use_pretrained_embedding\'\n        if the_item in config:\n            self.use_pretrained_embedding = self.str2bool(config[the_item])\n        the_item = \'token_emb_dir\'\n        if the_item in config:\n            self.token_emb_dir = config[the_item]\n\n        the_item = \'vocabs_dir\'\n        if the_item in config:\n            self.vocabs_dir = config[the_item]\n\n        the_item = \'checkpoints_dir\'\n        if the_item in config:\n            self.checkpoints_dir = config[the_item]\n\n        the_item = \'log_dir\'\n        if the_item in config:\n            self.log_dir = config[the_item]\n\n        ## Labeling Scheme\n        the_item = \'label_scheme\'\n        if the_item in config:\n            self.label_scheme = config[the_item]\n\n        the_item = \'label_level\'\n        if the_item in config:\n            self.label_level = int(config[the_item])\n\n        the_item = \'hyphen\'\n        if the_item in config:\n            self.hyphen = config[the_item]\n\n        the_item = \'suffix\'\n        if the_item in config:\n            self.suffix = config[the_item]\n\n        the_item = \'labeling_level\'\n        if the_item in config:\n            self.labeling_level = config[the_item]\n\n        the_item = \'measuring_metrics\'\n        if the_item in config:\n            self.measuring_metrics = config[the_item]\n\n        ## ModelConfiguration\n        the_item = \'use_crf\'\n        if the_item in config:\n            self.use_crf = self.str2bool(config[the_item])\n        the_item = \'use_self_attention\'\n        if the_item in config:\n            self.use_self_attention = self.str2bool(config[the_item])\n\n        the_item = \'cell_type\'\n        if the_item in config:\n            self.cell_type = config[the_item]\n        the_item = \'biderectional\'\n        if the_item in config:\n            self.biderectional = self.str2bool(config[the_item])\n        the_item = \'encoder_layers\'\n        if the_item in config:\n            self.encoder_layers = int(config[the_item])\n\n        the_item = \'embedding_dim\'\n        if the_item in config:\n            self.embedding_dim = int(config[the_item])\n\n        the_item = \'max_sequence_length\'\n        if the_item in config:\n            self.max_sequence_length = int(config[the_item])\n\n        the_item = \'attention_dim\'\n        if the_item in config:\n            self.attention_dim = int(config[the_item])\n\n        the_item = \'hidden_dim\'\n        if the_item in config:\n            self.hidden_dim = int(config[the_item])\n\n        the_item = \'CUDA_VISIBLE_DEVICES\'\n        if the_item in config:\n            self.CUDA_VISIBLE_DEVICES = config[the_item]\n\n        the_item = \'seed\'\n        if the_item in config:\n            self.seed = int(config[the_item])\n\n        ## Training Settings:\n        the_item = \'is_early_stop\'\n        if the_item in config:\n            self.is_early_stop = self.str2bool(config[the_item])\n        the_item = \'patient\'\n        if the_item in config:\n            self.patient = int(config[the_item])\n\n        the_item = \'epoch\'\n        if the_item in config:\n            self.epoch = int(config[the_item])\n        the_item = \'batch_size\'\n        if the_item in config:\n            self.batch_size = int(config[the_item])\n\n        the_item = \'dropout\'\n        if the_item in config:\n            self.dropout = float(config[the_item])\n        the_item = \'learning_rate\'\n        if the_item in config:\n            self.learning_rate = float(config[the_item])\n\n        the_item = \'optimizer\'\n        if the_item in config:\n            self.optimizer = config[the_item]\n\n        the_item = \'checkpoint_name\'\n        if the_item in config:\n            self.checkpoint_name = config[the_item]\n\n        the_item = \'checkpoints_max_to_keep\'\n        if the_item in config:\n            self.checkpoints_max_to_keep = int(config[the_item])\n        the_item = \'print_per_batch\'\n        if the_item in config:\n            self.print_per_batch = int(config[the_item])\n\n        ## Testing Settings\n\n        the_item = \'output_test_file\'\n        if the_item in config:\n            self.output_test_file = config[the_item]\n        the_item = \'is_output_sentence_entity\'\n        if the_item in config:\n            self.is_output_sentence_entity = self.str2bool(config[the_item])\n        the_item = \'output_sentence_entity_file\'\n        if the_item in config:\n            self.output_sentence_entity_file = config[the_item]\n\n        ## Api service Settings\n        the_item = \'ip\'\n        if the_item in config:\n            self.ip = config[the_item]\n        the_item = \'port\'\n        if the_item in config:\n            self.port = config[the_item]\n\n    def config_file_to_dict(self, input_file):\n        config = {}\n        fins = open(input_file, \'r\').readlines()\n        for line in fins:\n            if len(line) > 0 and line[0] == ""#"":\n                continue\n            if ""="" in line:\n                pair = line.strip().split(\'#\', 1)[0].split(\'=\', 1)\n                item = pair[0]\n                value = pair[1]\n                try:\n                    if item in config:\n                        print(""Warning: duplicated config item found: %s, updated."" % (pair[0]))\n                    if value[0] == \'[\' and value[-1] == \']\':\n                        value_itmes = list(value[1:-1].split(\',\'))\n                        config[item] = value_itmes\n                    else:\n                        config[item] = value\n                except:\n                    print(""configuration parsing error, please check correctness of the config file."")\n                    exit(1)\n        return config\n\n    def str2bool(self, string):\n        if string == ""True"" or string == ""true"" or string == ""TRUE"":\n            return True\n        else:\n            return False\n\n    def show_data_summary(self, logger):\n        logger.info(""\\n"")\n        logger.info(""++"" * 20 + ""CONFIGURATION SUMMARY"" + ""++"" * 20)\n        logger.info("" Status:"")\n        logger.info(""     mode               : %s"" % (self.mode))\n        logger.info("" "" + ""++"" * 20)\n        logger.info("" Datasets:"")\n        logger.info(""     datasets       fold: %s"" % (self.datasets_fold))\n        logger.info(""     train          file: %s"" % (self.train_file))\n        logger.info(""     developing     file: %s"" % (self.dev_file))\n        logger.info(""     test           file: %s"" % (self.test_file))\n        logger.info(""     pre trained embedin: %s"" % (self.use_pretrained_embedding))\n        logger.info(""     embedding      file: %s"" % (self.token_emb_dir))\n        logger.info(""     vocab           dir: %s"" % (self.vocabs_dir))\n        logger.info(""     delimiter          : %s"" % (self.delimiter))\n        logger.info(""     checkpoints     dir: %s"" % (self.checkpoints_dir))\n        logger.info(""     log             dir: %s"" % (self.log_dir))\n        logger.info("" "" + ""++"" * 20)\n        logger.info(""Labeling Scheme:"")\n        logger.info(""     label        scheme: %s"" % (self.label_scheme))\n        logger.info(""     label         level: %s"" % (self.label_level))\n        logger.info(""     suffixs           : %s"" % (self.suffix))\n        logger.info(""     labeling_level     : %s"" % (self.labeling_level))\n        logger.info(""     measuring   metrics: %s"" % (self.measuring_metrics))\n        logger.info("" "" + ""++"" * 20)\n        logger.info(""Model Configuration:"")\n        logger.info(""     use             crf: %s"" % (self.use_crf))\n        logger.info(""     use  self attention: %s"" % (self.use_self_attention))\n        logger.info(""     cell           type: %s"" % (self.cell_type))\n        logger.info(""     biderectional      : %s"" % (self.biderectional))\n        logger.info(""     encoder      layers: %s"" % (self.encoder_layers))\n        logger.info(""     embedding       dim: %s"" % (self.embedding_dim))\n\n        logger.info(""     max sequence length: %s"" % (self.max_sequence_length))\n        logger.info(""     attention       dim: %s"" % (self.attention_dim))\n        logger.info(""     hidden          dim: %s"" % (self.hidden_dim))\n        logger.info(""     CUDA VISIBLE DEVICE: %s"" % (self.CUDA_VISIBLE_DEVICES))\n        logger.info(""     seed               : %s"" % (self.seed))\n        logger.info("" "" + ""++"" * 20)\n        logger.info("" Training Settings:"")\n        logger.info(""     epoch              : %s"" % (self.epoch))\n        logger.info(""     batch          size: %s"" % (self.batch_size))\n        logger.info(""     dropout            : %s"" % (self.dropout))\n        logger.info(""     learning       rate: %s"" % (self.learning_rate))\n\n        logger.info(""     optimizer          : %s"" % (self.optimizer))\n        logger.info(""     checkpoint     name: %s"" % (self.checkpoint_name))\n\n        logger.info(""     max     checkpoints: %s"" % (self.checkpoints_max_to_keep))\n        logger.info(""     print   per   batch: %s"" % (self.print_per_batch))\n\n        logger.info(""     is   early     stop: %s"" % (self.is_early_stop))\n        logger.info(""     patient            : %s"" % (self.patient))\n        logger.info("" "" + ""++"" * 20)\n        logger.info("" Training Settings:"")\n        logger.info(""     output   test  file: %s"" % (self.output_test_file))\n        logger.info(""     output sent and ent: %s"" % (self.is_output_sentence_entity))\n        logger.info(""     output sen&ent file: %s"" % (self.output_sentence_entity_file))\n        logger.info("" "" + ""++"" * 20)\n        logger.info("" Api service Settings:"")\n        logger.info(""     ip                 : %s"" % (self.ip))\n        logger.info(""     port               : %s"" % (self.port))\n\n        logger.info(""++"" * 20 + ""CONFIGURATION SUMMARY END"" + ""++"" * 20)\n        logger.info(\'\\n\\n\')\n        sys.stdout.flush()\n'"
engines/DataManager.py,0,"b'# -*- coding: utf-8 -*-\r\n# @Time : 2019/6/2 \xe4\xb8\x8a\xe5\x8d\x8810:55\r\n# @Author : Scofield Phil\r\n# @FileName: DataManager.py\r\n# @Project: sequence-lableing-vex\r\n\r\nimport os, logging\r\nimport numpy as np\r\nfrom engines.utils import read_csv_\r\nimport jieba\r\njieba.setLogLevel(logging.INFO)\r\n\r\n\r\nclass DataManager:\r\n    def __init__(self, configs, logger):\r\n        self.configs=configs\r\n        self.train_file = configs.train_file\r\n        self.logger = logger\r\n        self.hyphen = configs.hyphen\r\n\r\n        self.UNKNOWN = ""<UNK>""\r\n        self.PADDING = ""<PAD>""\r\n\r\n        self.train_file = configs.datasets_fold + ""/"" + configs.train_file\r\n        self.dev_file = configs.datasets_fold + ""/"" + configs.dev_file\r\n        self.test_file = configs.datasets_fold + ""/"" + configs.test_file\r\n\r\n        self.output_test_file = configs.datasets_fold + ""/"" + configs.output_test_file\r\n        self.is_output_sentence_entity = configs.is_output_sentence_entity\r\n        self.output_sentence_entity_file = configs.datasets_fold + ""/"" + configs.output_sentence_entity_file\r\n\r\n        self.label_scheme = configs.label_scheme\r\n        self.label_level = configs.label_level\r\n        self.suffix = configs.suffix\r\n        self.labeling_level = configs.labeling_level\r\n\r\n        self.batch_size = configs.batch_size\r\n        self.max_sequence_length = configs.max_sequence_length\r\n        self.embedding_dim = configs.embedding_dim\r\n\r\n        self.vocabs_dir = configs.vocabs_dir\r\n        self.token2id_file = self.vocabs_dir + ""/token2id""\r\n        self.label2id_file = self.vocabs_dir + ""/label2id""\r\n\r\n        self.token2id, self.id2token, self.label2id, self.id2label = self.loadVocab()\r\n\r\n        self.max_token_number = len(self.token2id)\r\n        self.max_label_number = len(self.label2id)\r\n\r\n        jieba.load_userdict(self.token2id.keys())\r\n\r\n        self.logger.info(""dataManager initialed...\\n"")\r\n\r\n    def loadVocab(self):\r\n        if not os.path.isfile(self.token2id_file):\r\n            self.logger.info(""vocab files not exist, building vocab..."")\r\n            return self.buildVocab(self.train_file)\r\n\r\n        self.logger.info(""loading vocab..."")\r\n        token2id = {}\r\n        id2token = {}\r\n        with open(self.token2id_file, \'r\', encoding=\'utf-8\') as infile:\r\n            for row in infile:\r\n                row = row.rstrip()\r\n                token = row.split(\'\\t\')[0]\r\n                token_id = int(row.split(\'\\t\')[1])\r\n                token2id[token] = token_id\r\n                id2token[token_id] = token\r\n\r\n        label2id = {}\r\n        id2label = {}\r\n        with open(self.label2id_file, \'r\', encoding=\'utf-8\') as infile:\r\n            for row in infile:\r\n                row = row.rstrip()\r\n                label = row.split(\'\\t\')[0]\r\n                label_id = int(row.split(\'\\t\')[1])\r\n                label2id[label] = label_id\r\n                id2label[label_id] = label\r\n\r\n        return token2id, id2token, label2id, id2label\r\n\r\n    def buildVocab(self, train_path):\r\n        df_train = read_csv_(train_path, names=[""token"", ""label""],delimiter=self.configs.delimiter)\r\n        tokens = list(set(df_train[""token""][df_train[""token""].notnull()]))\r\n        labels = list(set(df_train[""label""][df_train[""label""].notnull()]))\r\n        token2id = dict(zip(tokens, range(1, len(tokens) + 1)))\r\n        label2id = dict(zip(labels, range(1, len(labels) + 1)))\r\n        id2token = dict(zip(range(1, len(tokens) + 1), tokens))\r\n        id2label = dict(zip(range(1, len(labels) + 1), labels))\r\n        id2token[0] = self.PADDING\r\n        id2label[0] = self.PADDING\r\n        token2id[self.PADDING] = 0\r\n        label2id[self.PADDING] = 0\r\n        id2token[len(tokens) + 1] = self.UNKNOWN\r\n        token2id[self.UNKNOWN] = len(tokens) + 1\r\n\r\n        self.saveVocab(id2token, id2label)\r\n\r\n        return token2id, id2token, label2id, id2label\r\n\r\n    def saveVocab(self, id2token, id2label):\r\n        with open(self.token2id_file, ""w"", encoding=\'utf-8\') as outfile:\r\n            for idx in id2token:\r\n                outfile.write(id2token[idx] + ""\\t"" + str(idx) + ""\\n"")\r\n        with open(self.label2id_file, ""w"", encoding=\'utf-8\') as outfile:\r\n            for idx in id2label:\r\n                outfile.write(id2label[idx] + ""\\t"" + str(idx) + ""\\n"")\r\n\r\n    def getEmbedding(self, embed_file):\r\n        emb_matrix = np.random.normal(loc=0.0, scale=0.08, size=(len(self.token2id.keys()), self.embedding_dim))\r\n        emb_matrix[self.token2id[self.PADDING], :] = np.zeros(shape=(self.embedding_dim))\r\n\r\n        with open(embed_file, ""r"", encoding=""utf-8"") as infile:\r\n            for row in infile:\r\n                row = row.rstrip()\r\n                items = row.split()\r\n                token = items[0]\r\n                assert self.embedding_dim == len(\r\n                    items[1:]), ""embedding dim must be consistent with the one in `token_emb_dir\'.""\r\n                emb_vec = np.array([float(val) for val in items[1:]])\r\n                if token in self.token2id.keys():\r\n                    emb_matrix[self.token2id[token], :] = emb_vec\r\n\r\n        return emb_matrix\r\n\r\n    def nextBatch(self, X, y, start_index):\r\n        last_index = start_index + self.batch_size\r\n        X_batch = list(X[start_index:min(last_index, len(X))])\r\n        y_batch = list(y[start_index:min(last_index, len(X))])\r\n        if last_index > len(X):\r\n            left_size = last_index - (len(X))\r\n            for i in range(left_size):\r\n                index = np.random.randint(len(X))\r\n                X_batch.append(X[index])\r\n                y_batch.append(y[index])\r\n        X_batch = np.array(X_batch)\r\n        y_batch = np.array(y_batch)\r\n        return X_batch, y_batch\r\n\r\n    def nextRandomBatch(self, X, y):\r\n        X_batch = []\r\n        y_batch = []\r\n        for i in range(self.batch_size):\r\n            index = np.random.randint(len(X))\r\n            X_batch.append(X[index])\r\n            y_batch.append(y[index])\r\n        X_batch = np.array(X_batch)\r\n        y_batch = np.array(y_batch)\r\n        return X_batch, y_batch\r\n\r\n    def padding(self, sample):\r\n        for i in range(len(sample)):\r\n            if len(sample[i]) < self.max_sequence_length:\r\n                sample[i] += [self.token2id[self.PADDING] for _ in range(self.max_sequence_length - len(sample[i]))]\r\n        return sample\r\n\r\n    def prepare(self, tokens, labels, is_padding=True, return_psyduo_label=False):\r\n        X = []\r\n        y = []\r\n        y_psyduo = []\r\n        tmp_x = []\r\n        tmp_y = []\r\n        tmp_y_psyduo = []\r\n\r\n        for record in zip(tokens, labels):\r\n            c = record[0]\r\n            l = record[1]\r\n            if c == -1:  # empty line\r\n                if len(tmp_x) <= self.max_sequence_length:\r\n                    X.append(tmp_x)\r\n                    y.append(tmp_y)\r\n                    if return_psyduo_label: y_psyduo.append(tmp_y_psyduo)\r\n                tmp_x = []\r\n                tmp_y = []\r\n                if return_psyduo_label: tmp_y_psyduo = []\r\n            else:\r\n                tmp_x.append(c)\r\n                tmp_y.append(l)\r\n                if return_psyduo_label: tmp_y_psyduo.append(self.label2id[""O""])\r\n        if is_padding:\r\n            X = np.array(self.padding(X))\r\n        else:\r\n            X = np.array(X)\r\n        y = np.array(self.padding(y))\r\n        if return_psyduo_label:\r\n            y_psyduo = np.array(self.padding(y_psyduo))\r\n            return X, y_psyduo\r\n\r\n        return X, y\r\n\r\n    def getTrainingSet(self, train_val_ratio=0.9):\r\n        df_train = read_csv_(self.train_file, names=[""token"", ""label""],delimiter=self.configs.delimiter)\r\n\r\n        # map the token and label into id\r\n        df_train[""token_id""] = df_train.token.map(lambda x: -1 if str(x) == str(np.nan) else self.token2id[x])\r\n        df_train[""label_id""] = df_train.label.map(lambda x: -1 if str(x) == str(np.nan) else self.label2id[x])\r\n\r\n        # convert the data in maxtrix\r\n        X, y = self.prepare(df_train[""token_id""], df_train[""label_id""])\r\n\r\n        # shuffle the samples\r\n        num_samples = len(X)\r\n        indexs = np.arange(num_samples)\r\n        np.random.shuffle(indexs)\r\n        X = X[indexs]\r\n        y = y[indexs]\r\n\r\n        if self.dev_file != None:\r\n            X_train = X\r\n            y_train = y\r\n            X_val, y_val = self.getValidingSet()\r\n        else:\r\n            # split the data into train and validation set\r\n            X_train = X[:int(num_samples * train_val_ratio)]\r\n            y_train = y[:int(num_samples * train_val_ratio)]\r\n            X_val = X[int(num_samples * train_val_ratio):]\r\n            y_val = y[int(num_samples * train_val_ratio):]\r\n\r\n        self.logger.info(""\\ntraining set size: %d, validating set size: %d\\n"" % (len(X_train), len(y_val)))\r\n\r\n        return X_train, y_train, X_val, y_val\r\n\r\n    def getValidingSet(self):\r\n        df_val = read_csv_(self.dev_file, names=[""token"", ""label""],delimiter=self.configs.delimiter)\r\n\r\n        df_val[""token_id""] = df_val.token.map(lambda x: self.mapFunc(x, self.token2id))\r\n        df_val[""label_id""] = df_val.label.map(lambda x: -1 if str(x) == str(np.nan) else self.label2id[x])\r\n\r\n        X_val, y_val = self.prepare(df_val[""token_id""], df_val[""label_id""])\r\n        return X_val, y_val\r\n\r\n    def getTestingSet(self):\r\n        df_test = read_csv_(self.test_file, names=None,delimiter=self.configs.delimiter)\r\n\r\n        if len(list(df_test.columns)) == 2:\r\n            df_test.columns = [""token"", ""label""]\r\n            df_test = df_test[[""token""]]\r\n        elif len(list(df_test.columns)) == 1:\r\n            df_test.columns = [""token""]\r\n\r\n        df_test[""token_id""] = df_test.token.map(lambda x: self.mapFunc(x, self.token2id))\r\n        df_test[""token""] = df_test.token.map(lambda x: -1 if str(x) == str(np.nan) else x)\r\n\r\n        X_test_id, y_test_psyduo_label = self.prepare(df_test[""token_id""], df_test[""token_id""],\r\n                                                      return_psyduo_label=True)\r\n        X_test_token, _ = self.prepare(df_test[""token""], df_test[""token""])\r\n\r\n        self.logger.info(""\\ntesting set size: %d\\n"" % (len(X_test_id)))\r\n        return X_test_id, y_test_psyduo_label, X_test_token\r\n\r\n    def mapFunc(self, x, token2id):\r\n        if str(x) == str(np.nan):\r\n            return -1\r\n        elif x not in token2id:\r\n            return token2id[self.UNKNOWN]\r\n        else:\r\n            return token2id[x]\r\n\r\n    def prepare_single_sentence(self, sentence):\r\n        if self.labeling_level == \'word\':\r\n            if self.check_contain_chinese(sentence):\r\n                sentence = list(jieba.cut(sentence))\r\n            else:\r\n                sentence = list(sentence.split())\r\n        elif self.labeling_level == \'char\':\r\n            sentence = list(sentence)\r\n\r\n        gap = self.batch_size - 1\r\n\r\n        x_ = []\r\n        y_ = []\r\n\r\n        for token in sentence:\r\n            try:\r\n                x_.append(self.token2id[token])\r\n            except:\r\n                x_.append(self.token2id[self.UNKNOWN])\r\n            y_.append(self.label2id[""O""])\r\n\r\n        if len(x_) < self.max_sequence_length:\r\n            sentence += [\'x\' for _ in range(self.max_sequence_length - len(sentence))]\r\n            x_ += [self.token2id[self.PADDING] for _ in range(self.max_sequence_length - len(x_))]\r\n            y_ += [self.label2id[""O""] for _ in range(self.max_sequence_length - len(y_))]\r\n        elif len(x_) > self.max_sequence_length:\r\n            sentence = sentence[:self.max_sequence_length]\r\n            x_ = x_[:self.max_sequence_length]\r\n            y_ = y_[:self.max_sequence_length]\r\n\r\n        X = [x_]\r\n        Sentence = [sentence]\r\n        Y = [y_]\r\n        X += [[0 for j in range(self.max_sequence_length)] for i in range(gap)]\r\n        Sentence += [[\'x\' for j in range(self.max_sequence_length)] for i in range(gap)]\r\n        Y += [[self.label2id[\'O\'] for j in range(self.max_sequence_length)] for i in range(gap)]\r\n        X = np.array(X)\r\n        Sentence = np.array(Sentence)\r\n        Y = np.array(Y)\r\n\r\n        return X, Sentence, Y\r\n\r\n    def check_contain_chinese(self, check_str):\r\n        for ch in list(check_str):\r\n            if u\'\\u4e00\' <= ch <= u\'\\u9fff\':\r\n                return True\r\n        return False\r\n'"
engines/utils.py,0,"b'# -*- coding: utf-8 -*-\r\n# @Time : 2019/6/2 \xe4\xb8\x8a\xe5\x8d\x8810:55\r\n# @Author : Scofield Phil\r\n# @FileName: utils.py\r\n# @Project: sequence-lableing-vex\r\n\r\nimport re, logging, datetime, csv\r\nimport pandas as pd\r\n\r\n\r\ndef get_logger(log_dir):\r\n    log_file = log_dir + ""/"" + (datetime.datetime.now().strftime(\'%Y-%m-%d-%H-%M-%S.log\'))\r\n    logger = logging.getLogger(__name__)\r\n    logger.setLevel(level=logging.INFO)\r\n\r\n    formatter = logging.Formatter(\'%(message)s\')\r\n\r\n    # log into file\r\n    handler = logging.FileHandler(log_file)\r\n    handler.setLevel(logging.INFO)\r\n    handler.setFormatter(formatter)\r\n    logger.addHandler(handler)\r\n\r\n    # log into terminal\r\n    console = logging.StreamHandler()\r\n    console.setFormatter(formatter)\r\n    console.setLevel(logging.INFO)\r\n    logger.addHandler(console)\r\n    logger.info(datetime.datetime.now().strftime(\'%Y-%m-%d: %H %M %S\'))\r\n\r\n    return logger\r\n\r\n\r\ndef extractEntity_(sentence, labels_, reg_str, label_level):\r\n    entitys = []\r\n    labled_labels = []\r\n    labled_indexs = []\r\n    labels__ = [(\'%03d\' % (ind)) + lb for lb, ind in zip(labels_, range(len(labels_)))]\r\n    labels = "" "".join(labels__)\r\n\r\n    re_entity = re.compile(reg_str)\r\n\r\n    m = re_entity.search(labels)\r\n    while m:\r\n        entity_labels = m.group()\r\n        if label_level == 1:\r\n            labled_labels.append(""_"")\r\n        elif label_level == 2:\r\n            labled_labels.append(entity_labels.split()[0][5:])\r\n\r\n        start_index = int(entity_labels.split()[0][:3])\r\n        if len(entity_labels.split()) != 1:\r\n            end_index = int(entity_labels.split()[-1][:3]) + 1\r\n        else:\r\n            end_index = start_index + 1\r\n        entity = \' \'.join(sentence[start_index:end_index])\r\n        labels = labels__[end_index:]\r\n        labels = "" "".join(labels)\r\n        entitys.append(entity)\r\n        labled_indexs.append((start_index, end_index))\r\n        m = re_entity.search(labels)\r\n\r\n    return entitys, labled_labels, labled_indexs\r\n\r\n\r\ndef extractEntity(x, y, dataManager):\r\n    label_scheme = dataManager.label_scheme\r\n    label_level = dataManager.label_level\r\n    label_hyphen = dataManager.hyphen\r\n\r\n    if label_scheme == ""BIO"":\r\n        if label_level == 1:\r\n            reg_str = r\'([0-9][0-9][0-9]B\' + r\' )([0-9][0-9][0-9]I\' + r\' )*\'\r\n\r\n        elif label_level == 2:\r\n            tag_bodys = [""("" + tag + "")"" for tag in dataManager.suffix]\r\n            tag_str = ""("" + (\'|\'.join(tag_bodys)) + "")""\r\n            reg_str = r\'([0-9][0-9][0-9]B\'+label_hyphen + tag_str + r\' )([0-9][0-9][0-9]I\'+label_hyphen + tag_str + r\' )*\'\r\n\r\n    elif label_scheme == ""BIESO"":\r\n        if label_level == 1:\r\n            reg_str = r\'([0-9][0-9][0-9]B\' + r\' )([0-9][0-9][0-9]I\' + r\' )*([0-9][0-9][0-9]E\' + r\' )|([0-9][0-9][0-9]S\' + r\' )\'\r\n\r\n        elif label_level == 2:\r\n            tag_bodys = [""("" + tag + "")"" for tag in dataManager.suffix]\r\n            tag_str = ""("" + (\'|\'.join(tag_bodys)) + "")""\r\n            reg_str = r\'([0-9][0-9][0-9]B\'+label_hyphen + tag_str + r\' )([0-9][0-9][0-9]I\'+label_hyphen + tag_str + r\' )*([0-9][0-9][0-9]E\'+label_hyphen + tag_str + r\' )|([0-9][0-9][0-9]S\'+label_hyphen + tag_str + r\' )\'\r\n\r\n    return extractEntity_(x, y, reg_str, label_level)\r\n\r\n\r\ndef metrics(X, y_true, y_pred, measuring_metrics, dataManager):\r\n    precision = -1.0\r\n    recall = -1.0\r\n    f1 = -1.0\r\n\r\n    hit_num = 0\r\n    pred_num = 0\r\n    true_num = 0\r\n\r\n    correct_label_num = 0\r\n    total_label_num = 0\r\n    for i in range(len(y_true)):\r\n        x = [str(dataManager.id2token[val]) for val in X[i] if val != dataManager.token2id[dataManager.PADDING]]\r\n        y = [str(dataManager.id2label[val]) for val in y_true[i] if val != dataManager.label2id[dataManager.PADDING]]\r\n        y_hat = [str(dataManager.id2label[val]) for val in y_pred[i] if\r\n                 val != dataManager.label2id[dataManager.PADDING]]  # if val != 5\r\n\r\n        correct_label_num += len([1 for a, b in zip(y, y_hat) if a == b])\r\n        total_label_num += len(y)\r\n\r\n        true_labels, labled_labels, _ = extractEntity(x, y, dataManager)\r\n        pred_labels, labled_labels, _ = extractEntity(x, y_hat, dataManager)\r\n\r\n        hit_num += len(set(true_labels) & set(pred_labels))\r\n        pred_num += len(set(pred_labels))\r\n        true_num += len(set(true_labels))\r\n\r\n    if total_label_num != 0:\r\n        accuracy = 1.0 * correct_label_num / total_label_num\r\n\r\n    if pred_num != 0:\r\n        precision = 1.0 * hit_num / pred_num\r\n    if true_num != 0:\r\n        recall = 1.0 * hit_num / true_num\r\n    if precision > 0 and recall > 0:\r\n        f1 = 2.0 * (precision * recall) / (precision + recall)\r\n\r\n    results = {}\r\n    for measu in measuring_metrics:\r\n        results[measu] = vars()[measu]\r\n    return results\r\n\r\n\r\ndef read_csv_(file_name, names, delimiter=\'t\'):\r\n    if delimiter==\'t\':\r\n        sep = ""\\t""\r\n    elif delimiter==\'b\':\r\n        sep = "" ""\r\n    else:\r\n        sep = delimiter\r\n\r\n    return pd.read_csv(file_name, sep=sep, quoting=csv.QUOTE_NONE, skip_blank_lines=False, header=None,\r\n                       names=names)\r\n\r\n\r\ndef save_csv_(df_, file_name, names, delimiter=\'t\'):\r\n    if delimiter == \'t\':\r\n        sep = ""\\t""\r\n    elif delimiter == \'b\':\r\n        sep = "" ""\r\n    else:\r\n        sep = delimiter\r\n\r\n    df_.to_csv(file_name, quoting=csv.QUOTE_NONE,\r\n               columns=names, sep=sep, header=False,\r\n               index=False)\r\n'"
tools/calcu_measure_testout.py,0,"b'# -*- coding: utf-8 -*-\nimport csv\nimport numpy as np\nimport pandas as pd\nfrom engines.utils import metrics\n\ndef calcu_measure_testout(data_dir=\'\', dataset_files=[], delimeter=\'\\t\'):\n    \'\'\'\n    unfinished\n    :return:\n    \'\'\'\n    print(data_dir)\n    names = [""token"", ""label""]\n\n    \n\ncalcu_measure_testout(data_dir=\'../data/example_datasets4\',\n       dataset_files=[\'test.out\', \'test.csv\'], delimeter=\'\\t\')\n'"
tools/statis.py,0,"b'# -*- coding: utf-8 -*-\nimport csv\nimport numpy as np\nimport pandas as pd\n\n\ndef statis(data_dir=\'\', dataset_files=[], delimeter=\'\\t\'):\n    \'\'\'\n    make statistics for train, dev, test sets.\n    :return:\n    \'\'\'\n    print(data_dir)\n    names = [""token"", ""label""]\n\n    # df_all = pd.DataFrame(columns=names)\n    df_all_token = []\n    df_all_label = []\n    for dataset in dataset_files:\n        df_set = pd.read_csv(data_dir + ""/"" + dataset, sep=delimeter, quoting=csv.QUOTE_NONE, skip_blank_lines=False,\n                             header=None)\n        columns = list(df_set.columns)\n        tmp_x = []\n        ind = 0\n        max_lengs = []\n\n        if len(columns) == 2:\n            df_set.columns = names\n            tmp_y = []\n\n            ### max sequence length\n            df_set[""token""] = df_set.token.map(lambda x: -1111 if str(x) == str(np.nan)else x)\n            df_set[""label""] = df_set.label.map(lambda x: -1111 if str(x) == str(np.nan)else x)\n            for record in zip(df_set[""token""], df_set[""label""]):\n                c = record[0]\n                l = record[1]\n                if c == -1111:\n                    ind += 1\n                    # if len(tmp_x) > max_leng:\n                    #     max_leng = len(tmp_x)\n                    max_lengs.append(len(tmp_x))\n                    tmp_x = []\n                    tmp_y = []\n                else:\n                    tmp_x.append(c)\n                    tmp_y.append(l)\n            # df_all.append(df_set, ignore_index=True)\n            df_set[""label""] = df_set.label.map(lambda x: """" if x == -1111 else x)\n            df_all_label.extend(list(df_set[""label""]))\n\n        else:\n            df_set[""token""] = df_set.token.map(lambda x: -1111 if str(x) == str(np.nan)else x)\n\n            for c in df_set[""token""]:\n                if c == -1111:\n                    ind += 1\n                    # if len(tmp_x) > max_leng:\n                    #     max_leng = len(tmp_x)\n                    max_lengs.append(len(tmp_x))\n                    tmp_x = []\n                else:\n                    tmp_x.append(c)\n\n        df_all_token.extend(list(df_set[""token""]))\n\n        print(""total sentence number in %s : %d"" % (dataset, ind))\n        print(""max sequence length in %s : %d"" % (dataset, max(max_lengs)))\n        print(""avg sequence length in %s : %.3f"" % (dataset, np.array(max_lengs).mean()))\n        print(""median sequence length in %s : %.3f"" % (dataset, np.median(np.array(max_lengs))) )\n        print(""95 percentile sequence length in %s : %.3f"" % (dataset, np.percentile(np.array(max_lengs), 95)) )\n\n        print(""length of token line in %s is %d"" % (dataset, len(df_set)))\n        print()\n\n    tokens_dict = list(set(df_all_token))\n    print(""length of token vocab is %d"" % (len(tokens_dict)))\n\n    labels_dict = list(set(df_all_label))\n    labels_dict.remove("""")\n    print(""length of labels set is %d"" % (len(labels_dict)))\n    print(""labels set: [%s]"" % ("","".join(labels_dict)))\n\n\nstatis(data_dir=\'../data/example_datasets4\',\n       dataset_files=[\'train.csv\', \'dev.csv\', \'test.csv\'], delimeter=\'\\t\')\n'"
demo_webapp/demo_webapp/__init__.py,0,b''
demo_webapp/demo_webapp/settings.py,0,"b'""""""\nDjango settings for demo_webapp project.\n\nGenerated by \'django-admin startproject\' using Django 1.11.8.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.11/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/1.11/ref/settings/\n""""""\n\nimport os\n# from interface import projectPath\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n# print(BASE_DIR)\n\n# print(projectPath)\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/1.11/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = \'(b+4gldcws^yp8xg!j16((xyjr3n)8wi_bqu=!c$=r%&o0)92l\'\n\n# SECURITY WARNING: don\'t run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = [\n    \'*\',\n    \'0.0.0.0\',\n    \'127.0.0.1\',\n    \'localhost\',\n]\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    \'django.contrib.admin\',\n    \'django.contrib.auth\',\n    \'django.contrib.contenttypes\',\n    \'django.contrib.sessions\',\n    \'django.contrib.messages\',\n    \'django.contrib.staticfiles\',\n    \'interface\'\n]\n\nMIDDLEWARE = [\n    \'django.middleware.security.SecurityMiddleware\',\n    \'django.contrib.sessions.middleware.SessionMiddleware\',\n    \'django.middleware.common.CommonMiddleware\',\n    \'django.middleware.csrf.CsrfViewMiddleware\',\n    \'django.contrib.auth.middleware.AuthenticationMiddleware\',\n    \'django.contrib.messages.middleware.MessageMiddleware\',\n    \'django.middleware.clickjacking.XFrameOptionsMiddleware\',\n]\n\nROOT_URLCONF = \'demo_webapp.urls\'\n\nTEMPLATES = [\n    {\n        \'BACKEND\': \'django.template.backends.django.DjangoTemplates\',\n        \'DIRS\': [BASE_DIR + ""/interface/templates"", ],\n        \'APP_DIRS\': True,\n        \'OPTIONS\': {\n            \'context_processors\': [\n                \'django.template.context_processors.debug\',\n                \'django.template.context_processors.request\',\n                \'django.contrib.auth.context_processors.auth\',\n                \'django.contrib.messages.context_processors.messages\',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = \'demo_webapp.wsgi.application\'\n\n\n# Database\n# https://docs.djangoproject.com/en/1.11/ref/settings/#databases\n\nDATABASES = {\n    \'default\': {\n        \'ENGINE\': \'django.db.backends.sqlite3\',\n        \'NAME\': os.path.join(BASE_DIR, \'db.sqlite3\'),\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/1.11/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.UserAttributeSimilarityValidator\',\n    },\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.MinimumLengthValidator\',\n    },\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.CommonPasswordValidator\',\n    },\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.NumericPasswordValidator\',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/1.11/topics/i18n/\n\nLANGUAGE_CODE = \'en-us\'\n\nTIME_ZONE = \'UTC\'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/1.11/howto/static-files/\n\nSTATIC_URL = \'/static/\'\n\n\nSTATICFILES_FINDERS = (\n    ""django.contrib.staticfiles.finders.FileSystemFinder"",\n    ""django.contrib.staticfiles.finders.AppDirectoriesFinder""\n)'"
demo_webapp/demo_webapp/urls.py,0,"b'""""""demo_webapp URL Configuration\n\nThe `urlpatterns` list routes URLs to views. For more information please see:\n    https://docs.djangoproject.com/en/1.11/topics/http/urls/\nExamples:\nFunction views\n    1. Add an import:  from my_app import views\n    2. Add a URL to urlpatterns:  url(r\'^$\', views.home, name=\'home\')\nClass-based views\n    1. Add an import:  from other_app.views import Home\n    2. Add a URL to urlpatterns:  url(r\'^$\', Home.as_view(), name=\'home\')\nIncluding another URLconf\n    1. Import the include() function: from django.conf.urls import url, include\n    2. Add a URL to urlpatterns:  url(r\'^blog/\', include(\'blog.urls\'))\n""""""\nfrom django.conf.urls import url\nfrom django.contrib import admin\nfrom interface import views as interface_views\n\nurlpatterns = [\n    url(r\'^admin/\', admin.site.urls),\n\n    url(r\'^$\', interface_views.index, name=\'index\'),\n    url(r\'api/\', interface_views.api, name=\'api\'),\n    url(r\'predict/\', interface_views.predict, name=\'predict\'),\n]\n'"
demo_webapp/demo_webapp/wsgi.py,0,"b'""""""\nWSGI config for demo_webapp project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.11/howto/deployment/wsgi/\n""""""\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""demo_webapp.settings"")\n\napplication = get_wsgi_application()\n'"
demo_webapp/interface/__init__.py,0,"b'from django.conf import settings\nimport sys\n\nprojectPath = ""/"".join(settings.BASE_DIR.split(\'/\')[:-1])\nsys.path.append(projectPath)\n# print(chatbotPath)\n\nfrom engines.BiLSTM_CRFs import BiLSTM_CRFs\nfrom engines.Configer import Configer\nfrom engines.DataManager import DataManager\nfrom engines.utils import get_logger\n\nconfigs = Configer(config_file=projectPath + ""/system.config"")\nconfigs.datasets_fold = (projectPath + ""/"" + configs.datasets_fold)\nconfigs.vocabs_dir = (projectPath + ""/"" + configs.vocabs_dir)\nconfigs.token_emb_dir = (projectPath + ""/"" + configs.token_emb_dir)\nconfigs.checkpoints_dir = (projectPath + ""/"" + configs.checkpoints_dir)\nconfigs.log_dir = (projectPath + ""/"" + configs.log_dir)\n\nlogger = get_logger(configs.log_dir)\ndataManager = DataManager(configs, logger)\nmodel = BiLSTM_CRFs(configs, logger, dataManager)\nmodel.soft_load()\nmodel.predict_single("" "") #warm start\ncolor_list = [\'19AC8D\', \'FFF000\', \'6892B9\', \'59AC6F\', \'E87F78\', \'335ABF\', \'DEC778\',\n              \'B581E4\', \'B93326\', \'B1B99C\']\ncolor_list = color_list * 5\n\nif configs.label_level ==2:\n    color_dict = {typ: color_list[i] for i, typ in enumerate(configs.suffix)}\n'"
demo_webapp/interface/admin.py,0,b'from django.contrib import admin\n\n# Register your models here.\n'
demo_webapp/interface/apps.py,0,"b""from django.apps import AppConfig\n\n\nclass InterfaceConfig(AppConfig):\n    name = 'interface'\n"""
demo_webapp/interface/models.py,0,b'from django.db import models\n\n# Create your models here.\n\n\n\n'
demo_webapp/interface/tests.py,0,b'from django.test import TestCase\n\n# Create your tests here.\n'
demo_webapp/interface/views.py,0,"b'from django.shortcuts import render\nfrom django.views.decorators.csrf import csrf_protect\nfrom django.http import HttpResponse\nimport json, random\nfrom interface import model, logger, configs\n\nif configs.label_level == 1:\n    from interface import color_list\nelif configs.label_level == 2:\n    from interface import color_dict\n\n\ndef index(request):\n    return render(request, \'index.html\')\n\n\ndef api(request):\n    sentence = request.GET[""sentence""]\n    sentence_tokens, entities, entities_type, entities_index = model.predict_single(sentence)\n    if len(entities) != 0:\n        if configs.label_level ==1:\n            respo = {\n                ""sentence"": \' \'.join(sentence_tokens),\n                ""entities"": entities,\n            }\n        elif configs.label_level ==2:\n            respo = {\n                ""sentence"": \' \'.join(sentence_tokens),\n                ""entities"": [{ent: typ} for ent, typ in zip(entities, entities_type)],\n            }\n    else:\n        respo = {\n            ""sentence"": sentence,\n            ""entities"": \'\',\n        }\n    return HttpResponse(json.dumps(respo), content_type=""application/json"")\n\n\n@csrf_protect\ndef predict(request):\n    sentence = request.POST[""sentence""].strip()\n    logger.info(sentence)\n\n    if sentence.strip() != """":\n        sentence_tokens, entities, entities_type, entities_index = model.predict_single(sentence)\n\n        logger.info(""\\nSentence tokens:\\n %s\\n"" % ("" "".join(sentence_tokens)))\n        if configs.label_level == 1:\n            logger.info(""Extracted entities:\\n %s\\n"" % (""\\n"".join(\n            [ent + ""\\t([%d-%d])"" % (inda, indb) for ent, (inda, indb) in\n             zip(entities, entities_index)])))\n        elif configs.label_level == 2:\n            logger.info(""Extracted entities:\\n %s\\n"" % (""\\n"".join(\n            [ent + ""\\t(%s;[%d-%d])"" % (typ, inda, indb) for ent, typ, (inda, indb) in\n             zip(entities, entities_type, entities_index)])))\n\n\n        html_str_ = r\'<h3 style=""font-weight: bold;line-height: 42px;text-indent:3em;margin-top: 20px"" class=""resh2"">%s</h3>\'\n        html_str_inner_level1 = r\'<mark style=""background-color:#%s"" >%s</mark>\'\n        html_str_inner_level2 = r\'<mark style=""background-color:#%s"" >%s<sub style=""font-weight: normal;font-size: small""> (%s)</sub></mark>\'\n\n        text_ = \'\'\n        for i, token in enumerate(sentence_tokens):\n            if len(entities_index) > 0:\n                if entities_index[0][0] <= i and i < entities_index[0][1]:\n                    continue\n\n            if len(entities_index) > 0 and i == entities_index[0][1]:\n                if configs.label_level == 1:\n                    text_ += ("" "" + html_str_inner_level1 % (random.choice(color_list), entities[0]))\n                elif configs.label_level == 2:\n                    text_ += ("" "" + html_str_inner_level2 % (color_dict[entities_type[0]], entities[0], entities_type[0]))\n\n                entities_index = entities_index[1:]\n                entities_type = entities_type[1:]\n                entities = entities[1:]\n            text_ += ("" "" + token)\n\n        json_data = {\'info\': html_str_ % text_}\n    else:\n        json_data = {\'info\': """"}\n\n    logger.info(json_data)\n\n    return HttpResponse(json.dumps(json_data), content_type=""application/json"")\n'"
demo_webapp/interface/migrations/__init__.py,0,b''
