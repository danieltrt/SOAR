file_path,api_count,code
config/__init__.py,0,b''
config/basemodel.py,0,"b'\'\'\' Config Proto \'\'\'\n\nimport sys\nimport os\n\n\n####### INPUT OUTPUT #######\n\n# The name of the current model for output\nname = \'faceres_ms\'\n\n# The folder to save log and model\nlog_base_dir = \'./log/\'\n\n# The interval between writing summary\nsummary_interval = 100\n\n# Training dataset path\ntrain_dataset_path = \'/path/to/msceleb1m/dataset/folder\'\n\n# Testing dataset path\ntest_dataset_path = \'/path/to/lfw/dataset/folder\'\n\n# LFW standard protocol file\nlfw_pairs_file = \'./proto/lfw_pairs.txt\'\n\n# Target image size for the input of network\nimage_size = [96, 112]\n\n# 3 channels means RGB, 1 channel for grayscale\nchannels = 3\n\n# Preprocess for training\npreprocess_train = [\n    # [\'resize\', (96,112)],\n    [\'random_flip\'],\n    # (\'random_crop\', (96,112)],\n    # [\'random_downsample\', 0.5],\n    [\'standardize\', \'mean_scale\'],\n]\n\n# Preprocess for testing\npreprocess_test = [\n    # [\'resize\', (96,112)],\n    # [\'center_crop\', (96, 112)],\n    [\'standardize\', \'mean_scale\'],\n]\n\n# Number of GPUs\nnum_gpus = 1\n\n\n####### NETWORK #######\n\n# Use sibling network\nuse_sibling = False\n\n# The network architecture\nnetwork = ""nets/face_resnet.py""\n\n# Model version, only for some networks\nmodel_version = None\n\n# Number of dimensions in the embedding space\nembedding_size = 512\n\n\n####### TRAINING STRATEGY #######\n\n# Optimizer\noptimizer = ""MOM""\n\n# Number of samples per batch\nbatch_size = 256\n\n# Structure of batch, use one of following:\n# random_sample, random_pair, random_AB_pair\nbatch_format = \'random_sample\'\n\n# Number of batches per epoch\nepoch_size = 1000\n\n# Number of epochs\nnum_epochs = 320\n\n# learning rate strategy\nlearning_rate_strategy = \'step\'\n\n# learning rate schedule\nlr = 0.1\nlearning_rate_schedule = {\n    0:      1 * lr,\n    160000:  0.1 * lr,\n    240000:  0.01 * lr,\n    280000:  0.001 * lr,\n}\n\n# Multiply the learning rate for variables that contain certain keywords\nlearning_rate_multipliers = {\n}\n\n# The model folder from which to retore the parameters\nrestore_model = None\n\n# Keywords to filter restore variables, set None for all\nrestore_scopes = [\'FaceResNet\']\n\n# Weight decay for model variables\nweight_decay = 5e-4\n\n# Keep probability for dropouts\nkeep_prob = 1.0\n\n\n\n####### LOSS FUNCTION #######\n\n# Loss functions and their parameters\nlosses = {\n    # \'softmax\': {},\n    # \'cosine\': {\'scale\': \'auto\'},\n    # \'angular\': {\'m\': 4, \'lamb_min\':5.0, \'lamb_max\':1500.0},\n    \'am\': {\'scale\': \'auto\', \'m\':5.0}\n}\n\n'"
config/finetune.py,0,"b'\'\'\' Config Proto \'\'\'\n\nimport sys\nimport os\n\n\n####### INPUT OUTPUT #######\n\n# The name of the current model for output\nname = \'faceres_finetuned\'\n\n# The folder to save log and model\nlog_base_dir = \'./log/\'\n\n# The interval between writing summary\nsummary_interval = 10\n\n# Training dataset path\ntrain_dataset_path = \'/path/to/training/dataset/folder\'\n\n# Testing dataset path\ntest_dataset_path = \'/path/to/testing/dataset/folder\'\n\n# LFW standard protocol file\nlfw_pairs_file = \'./proto/lfw_pairs.txt\'\n\n# Target image size for the input of network\nimage_size = [96, 112]\n\n# 3 channels means RGB, 1 channel for grayscale\nchannels = 3\n\n# Preprocess for training\npreprocess_train = [\n    # [\'resize\', (48,56)],\n    # [\'resize\', (96,112)],\n    [\'random_flip\'],\n    # (\'random_crop\', (96,112)],\n    # [\'random_downsample\', 0.5],\n    [\'standardize\', \'mean_scale\'],\n]\n\n# Preprocess for testing\npreprocess_test = [\n    # [\'resize\', (96,112)],\n    # [\'center_crop\', (96, 112)],\n    [\'standardize\', \'mean_scale\'],\n]\n\n# Number of GPUs\nnum_gpus = 1\n\n\n####### NETWORK #######\n\n# Use sibling network\nuse_sibling = True\n\n# The network architecture\nnetwork = ""nets/sibling_shared_res.py""\n\n# Model version, only for some networks\nmodel_version = \'fc\'\n\n# Number of dimensions in the embedding space\nembedding_size = 512\n\n\n####### TRAINING STRATEGY #######\n\n# Optimizer\noptimizer = ""MOM""\n\n# Number of samples per batch\nbatch_size = 248\n\n# Structure of batch, use one of following:\n# random_sample, random_pair, random_AB_pair\nbatch_format = \'random_AB_pair\'\n\n# Number of batches per epoch\nepoch_size = 100\n\n# Number of epochs\nnum_epochs = 40\n\n# learning rate strategy\nlearning_rate_strategy = \'step\'\n\n# learning rate schedule\nlr = 0.01\nlearning_rate_schedule = {\n    0:      1 * lr,\n    3200:    0.1 * lr,\n}\n\n# Multiply the learning rate for variables that contain certain keywords\nlearning_rate_multipliers = {\n}\n\n# The model folder from which to retore the parameters\nrestore_model = \'/path/to/the/pretrained/model/folder\'\n\n# Keywords to filter restore variables, set None for all\nrestore_scopes = [\'FaceResNet\']\n\n# For sibling networks, a dictionary needs to be built to map their variables\n# to the variables in the base network (they have different variable names).\n# This argument decides the mapping between the sibling network and target network.\nreplace_scopes = {\n    ""FaceResNet/NetA"": ""FaceResNet"",\n    ""FaceResNet/NetB"": ""FaceResNet"",\n    ""FaceResNet/SharedNet"": ""FaceResNet"",\n}\n\n# Weight decay for model variables\nweight_decay = 5e-4\n\n# Keep probability for dropouts\nkeep_prob = 1.0\n\n\n\n####### LOSS FUNCTION #######\n\n# Loss functions and their parameters\nlosses = {\n    # \'softmax\': {},\n    # \'cosine\': {\'scale\': \'auto\'},\n    # \'angular\': {\'m\': 4, \'lamb_min\':5.0, \'lamb_max\':1500.0},\n    # \'am\': {\'scale\': \'auto\', \'m\': 5.0}\n    \'diam\': {\'scale\': \'auto\', \'m\': 5.0, \'alpha\':1.0}\n    # \'pair\': {\'m\': 0.5},\n}\n\n'"
nets/__init__.py,0,b''
nets/face_resnet.py,18,"b'""""""Tensorflow implementation of Face-ResNet:\nA. Hasnat, J. Bohne, J. Milgram, S. Gentric, and L. Chen. Deepvisage: Making face \nrecognition simple yet with powerful generalization skills. arXiv:1703.08388, 2017.\n""""""\n# MIT License\n# \n# Copyright (c) 2018 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\ndef parametric_relu(x):\n    num_channels = x.shape[-1].value\n    with tf.variable_scope(\'p_re_lu\'):\n        alpha = tf.get_variable(\'alpha\', (1,1,num_channels),\n                        initializer=tf.constant_initializer(0.0),\n                        dtype=tf.float32)\n        return tf.nn.relu(x) + alpha * tf.minimum(0.0, x)\n\n# activation = lambda x: tf.keras.layers.PReLU(shared_axes=[1,2]).apply(x)\nactivation = parametric_relu\n\ndef se_module(input_net, ratio=16, reuse = None, scope = None):\n    with tf.variable_scope(scope, \'SE\', [input_net], reuse=reuse):\n        h,w,c = tuple([dim.value for dim in input_net.shape[1:4]])\n        assert c % ratio == 0\n        hidden_units = int(c / ratio)\n        squeeze = slim.avg_pool2d(input_net, [h,w], padding=\'VALID\')\n        excitation = slim.flatten(squeeze)\n        excitation = slim.fully_connected(excitation, hidden_units, scope=\'se_fc1\',\n                                weights_regularizer=None,\n                                # weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n                                weights_initializer=slim.xavier_initializer(), \n                                activation_fn=tf.nn.relu)\n        excitation = slim.fully_connected(excitation, c, scope=\'se_fc2\',\n                                weights_regularizer=None,\n                                # weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n                                weights_initializer=slim.xavier_initializer(), \n                                activation_fn=tf.nn.sigmoid)        \n        excitation = tf.reshape(excitation, [-1,1,1,c])\n        output_net = input_net * excitation\n\n        return output_net\n        \n\ndef conv_module(net, num_res_layers, num_kernels, reuse = None, scope = None):\n    with tf.variable_scope(scope, \'conv\', [net], reuse=reuse):\n        # Every 2 conv layers constitute a residual block\n        if scope == \'conv1\':\n            for i in range(len(num_kernels)):\n                with tf.variable_scope(\'layer_%d\'%i, reuse=reuse):\n                    net = slim.conv2d(net, num_kernels[i], kernel_size=3, stride=1, padding=\'VALID\',\n                                    weights_initializer=slim.xavier_initializer())\n                    print(\'| ---- layer_%d\' % i)\n            net = slim.max_pool2d(net, 2, stride=2, padding=\'VALID\')\n        else:\n            shortcut = net\n            for i in range(num_res_layers):\n                with tf.variable_scope(\'layer_%d\'%i, reuse=reuse):\n                    net = slim.conv2d(net, num_kernels[0], kernel_size=3, stride=1, padding=\'SAME\',\n                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n                                    biases_initializer=None)\n                    print(\'| ---- layer_%d\' % i)\n                if i % 2 == 1:\n                    net = se_module(net)\n                    net = net + shortcut\n                    shortcut = net\n                    print(\'| shortcut\')\n            # Pooling for conv2 - conv4\n            if len(num_kernels) > 1:\n                with tf.variable_scope(\'expand\', reuse=reuse):\n                    net = slim.conv2d(net, num_kernels[1], kernel_size=3, stride=1, padding=\'VALID\',\n                                    weights_initializer=slim.xavier_initializer())\n                    net = slim.max_pool2d(net, 2, stride=2, padding=\'VALID\')\n                    print(\'- expand\')\n\n    return net\n\ndef inference(images, keep_probability, phase_train=True, bottleneck_layer_size=512, \n            weight_decay=0.0, reuse=None, model_version=None):\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        weights_regularizer=slim.l2_regularizer(weight_decay),\n                        activation_fn=activation,\n                        normalizer_fn=None,\n                        normalizer_params=None):\n        with tf.variable_scope(\'FaceResNet\', [images], reuse=reuse):\n            with slim.arg_scope([slim.batch_norm, slim.dropout],\n                                is_training=phase_train):\n                print(\'input shape:\', [dim.value for dim in images.shape])\n                \n                net = conv_module(images, 0, [32, 64], scope=\'conv1\')\n                print(\'module_1 shape:\', [dim.value for dim in net.shape])\n\n                net = conv_module(net, 2, [64, 128], scope=\'conv2\')\n                print(\'module_2 shape:\', [dim.value for dim in net.shape])\n\n                net = conv_module(net, 4, [128, 256], scope=\'conv3\')\n                print(\'module_3 shape:\', [dim.value for dim in net.shape])\n\n                net = conv_module(net, 10, [256, 512], scope=\'conv4\')\n                print(\'module_4 shape:\', [dim.value for dim in net.shape])\n\n                net = conv_module(net, 6, [512], scope=\'conv5\')\n                print(\'module_5 shape:\', [dim.value for dim in net.shape])\n                \n                \n                net = slim.flatten(net)\n                net = slim.fully_connected(net, bottleneck_layer_size, scope=\'Bottleneck\',\n                                        weights_initializer=slim.xavier_initializer(), \n                                        activation_fn=None)\n\n    return net\n'"
nets/sibling_shared_res.py,33,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n\nmodel_params = {\n    \'conv1\': [\'conv1\'],\n    \'conv12\': [\'conv1\', \'conv2\'],\n    \'conv13\': [\'conv1\', \'conv2\', \'conv3\'],\n    \'conv14\': [\'conv1\', \'conv2\', \'conv3\', \'conv4\'],\n    \'conv15\': [\'conv1\', \'conv2\', \'conv3\', \'conv4\', \'conv5\'],\n    \'all\': [\'conv1\', \'conv2\', \'conv3\', \'conv4\', \'conv5\', \'fc\'],\n    \'conv45+fc\': [\'conv4\', \'conv5\', \'fc\'],\n    \'conv5+fc\': [\'conv5\', \'fc\'],\n    \'fc\': [\'fc\'],\n}\n\nbatch_norm_params = {\n    # Decay for the moving averages.\n    \'decay\': 0.995,\n    # epsilon to prevent 0s in variance.\n    \'epsilon\': 0.001,\n    # force in-place updates of mean and variance estimates\n    \'updates_collections\': None,\n    # Moving averages ends up in the trainable variables collection\n    \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n}\n\nbatch_norm_params_last = {\n    # Decay for the moving averages.\n    \'decay\': 0.995,\n    # epsilon to prevent 0s in variance.\n    \'epsilon\': 10e-8,\n    # force in-place updates of mean and variance estimates\n    \'center\': False,\n    # not use beta\n    \'scale\': False,\n    # not use gamma\n    \'updates_collections\': None,\n    # Moving averages ends up in the trainable variables collection\n    \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n}\n\ndef parametric_relu(x):\n    num_channels = x.shape[-1].value\n    with tf.variable_scope(\'p_re_lu\'):\n        alpha = tf.get_variable(\'alpha\', (1,1,num_channels),\n                        initializer=tf.constant_initializer(0.0),\n                        dtype=tf.float32)\n        return tf.nn.relu(x) + alpha * tf.minimum(0.0, x)\n\n# activation = lambda x: tf.keras.layers.PReLU(shared_axes=[1,2]).apply(x)\nactivation = parametric_relu\n\ndef se_module(input_net, ratio=16, reuse = None, scope = None):\n    with tf.variable_scope(scope, \'SE\', [input_net], reuse=reuse):\n        h,w,c = tuple([dim.value for dim in input_net.shape[1:4]])\n        assert c % ratio == 0\n        hidden_units = int(c / ratio)\n        squeeze = slim.avg_pool2d(input_net, [h,w], padding=\'VALID\')\n        excitation = slim.flatten(squeeze)\n        excitation = slim.fully_connected(excitation, hidden_units, scope=\'se_fc1\',\n                                weights_regularizer=None,\n                                # weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n                                weights_initializer=slim.xavier_initializer(), \n                                activation_fn=tf.nn.relu)\n        excitation = slim.fully_connected(excitation, c, scope=\'se_fc2\',\n                                weights_regularizer=None,\n                                # weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n                                weights_initializer=slim.xavier_initializer(), \n                                activation_fn=tf.nn.sigmoid)        \n        excitation = tf.reshape(excitation, [-1,1,1,c])\n        output_net = input_net * excitation\n\n        return output_net\n    \n        \n\ndef conv_module(net, num_res_layers, num_kernels, reuse = None, scope = None):\n    with tf.variable_scope(scope, \'conv\', [net], reuse=reuse):\n        # Every 2 conv layers constitute a residual block\n        if scope == \'conv1\':\n            for i in range(len(num_kernels)):\n                with tf.variable_scope(\'layer_%d\'%i, reuse=reuse):\n                    net = slim.conv2d(net, num_kernels[i], kernel_size=3, stride=1, padding=\'VALID\',\n                                    weights_initializer=slim.xavier_initializer())\n                    # net = activation(net)\n                    print(\'| ---- layer_%d\' % i)\n            net = slim.max_pool2d(net, 2, stride=2, padding=\'VALID\')\n        else:\n            shortcut = net\n            for i in range(num_res_layers):\n                with tf.variable_scope(\'layer_%d\'%i, reuse=reuse):\n                    net = slim.conv2d(net, num_kernels[0], kernel_size=3, stride=1, padding=\'SAME\',\n                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n                                    biases_initializer=None)\n                    # net = activation(net)\n                    print(\'| ---- layer_%d\' % i)\n                if i % 2 == 1:\n                    net = se_module(net)\n                    net = net + shortcut\n                    shortcut = net\n                    print(\'| shortcut\')\n            # Pooling for conv2 - conv4\n            if len(num_kernels) > 1:\n                with tf.variable_scope(\'expand\', reuse=reuse):\n                    # net = slim.batch_norm(net, **batch_norm_params)\n                    net = slim.conv2d(net, num_kernels[1], kernel_size=3, stride=1, padding=\'VALID\',\n                                    weights_initializer=slim.xavier_initializer())\n                    # net = activation(net)\n                    net = slim.max_pool2d(net, 2, stride=2, padding=\'VALID\')\n                    print(\'- expand\')\n\n    return net\n\ndef build_scope(images, bottleneck_layer_size, shared_modules, scope_name, shared_scope_name, reuse=tf.AUTO_REUSE):\n    get_scope = lambda x: shared_scope_name if x in shared_modules else scope_name\n    with tf.variable_scope(get_scope(\'conv1\'), reuse=reuse):\n        print(tf.get_variable_scope().name)\n        net = conv_module(images, 0, [32, 64], scope=\'conv1\')\n        print(\'module_1 shape:\', [dim.value for dim in net.shape])\n    with tf.variable_scope(get_scope(\'conv2\'), reuse=reuse):\n        print(tf.get_variable_scope().name)\n        net = conv_module(net, 2, [64, 128], scope=\'conv2\')\n        print(\'module_2 shape:\', [dim.value for dim in net.shape])\n    with tf.variable_scope(get_scope(\'conv3\'), reuse=reuse):\n        print(tf.get_variable_scope().name)\n        net = conv_module(net, 4, [128, 256], scope=\'conv3\')\n        print(\'module_3 shape:\', [dim.value for dim in net.shape])\n    with tf.variable_scope(get_scope(\'conv4\'), reuse=reuse):\n        print(tf.get_variable_scope().name)\n        net = conv_module(net, 10, [256, 512], scope=\'conv4\')\n        print(\'module_4 shape:\', [dim.value for dim in net.shape])\n    with tf.variable_scope(get_scope(\'conv5\'), reuse=reuse):\n        print(tf.get_variable_scope().name)\n        net = conv_module(net, 6, [512], scope=\'conv5\')\n        print(\'module_5 shape:\', [dim.value for dim in net.shape])\n    with tf.variable_scope(get_scope(\'fc\'), reuse=reuse):\n        print(tf.get_variable_scope().name)\n        net = slim.flatten(net)\n        prelogits = slim.fully_connected(net, bottleneck_layer_size, scope=\'Bottleneck\',\n                                weights_initializer=slim.xavier_initializer(), \n                                activation_fn=None)\n    return prelogits\n\ndef inference(images_A, images_B, keep_probability=1.0, phase_train=True, bottleneck_layer_size=512, \n            weight_decay=0.0, reuse=None, model_version=None):\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        weights_regularizer=slim.l2_regularizer(weight_decay),\n                        activation_fn=activation,\n                        normalizer_fn=None,\n                        normalizer_params=None):\n        with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=phase_train):\n            with tf.variable_scope(\'FaceResNet\', [images_A, images_B], reuse=reuse):\n                \n                shared_modules = model_params[model_version]\n\n                print(\'input shape:\', [dim.value for dim in images_A.shape])\n\n                prelogits_A = build_scope(images_A, bottleneck_layer_size, \n                    shared_modules, ""NetA"", ""SharedNet"")\n                prelogits_B = build_scope(images_B, bottleneck_layer_size, \n                    shared_modules, ""NetB"", ""SharedNet"")\n\n        return prelogits_A, prelogits_B\n'"
src/basenet.py,38,"b'""""""Basic Network for face recognition\n""""""\n# MIT License\n# \n# Copyright (c) 2017 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport sys\nimport time\nimport imp\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nimport tflib\n\nclass BaseNetwork:\n    def __init__(self):\n        self.graph = tf.Graph()\n        gpu_options = tf.GPUOptions(allow_growth=True)\n        tf_config = tf.ConfigProto(gpu_options=gpu_options,\n                allow_soft_placement=True, log_device_placement=False)\n        self.sess = tf.Session(graph=self.graph, config=tf_config)\n            \n    def initialize(self, config, num_classes):\n        \'\'\'\n            Initialize the graph from scratch according config.\n        \'\'\'\n        with self.graph.as_default():\n            with self.sess.as_default():\n                # Set up placeholders\n                w, h = config.image_size\n                channels = config.channels\n                image_batch_placeholder = tf.placeholder(tf.float32, shape=[None, h, w, channels], name=\'image_batch\')\n                label_batch_placeholder = tf.placeholder(tf.int32, shape=[None], name=\'label_batch\')\n                learning_rate_placeholder = tf.placeholder(tf.float32, name=\'learning_rate\')\n                keep_prob_placeholder = tf.placeholder(tf.float32, name=\'keep_prob\')\n                phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n                global_step = tf.Variable(0, trainable=False, dtype=tf.int32, name=\'global_step\')\n\n                image_splits = tf.split(image_batch_placeholder, config.num_gpus)\n                label_splits = tf.split(label_batch_placeholder, config.num_gpus)\n                grads_splits = []\n                split_dict = {}\n                def insert_dict(k,v):\n                    if k in split_dict: split_dict[k].append(v)\n                    else: split_dict[k] = [v]\n                        \n                for i in range(config.num_gpus):\n                    scope_name = \'\' if i==0 else \'gpu_%d\' % i\n                    with tf.name_scope(scope_name):\n                        with tf.variable_scope(\'\', reuse=i>0):\n                            with tf.device(\'/gpu:%d\' % i):\n                                images = tf.identity(image_splits[i], name=\'inputs\')\n                                labels = tf.identity(label_splits[i], name=\'labels\')\n                                # Save the first channel for testing\n                                if i == 0:\n                                    self.inputs = images\n                                \n                                # Build networks\n                                network = imp.load_source(\'network\', config.network)\n                                prelogits = network.inference(images, keep_prob_placeholder, phase_train_placeholder,\n                                                        bottleneck_layer_size = config.embedding_size, \n                                                        weight_decay = config.weight_decay, \n                                                        model_version = config.model_version)\n                                prelogits = tf.identity(prelogits, name=\'prelogits\')\n                                embeddings = tf.nn.l2_normalize(prelogits, dim=1, name=\'embeddings\')\n                                if i == 0:\n                                    self.outputs = tf.identity(embeddings, name=\'outputs\')\n\n                                # Build all losses\n                                losses = []\n\n                                # Orignal Softmax\n                                if \'softmax\' in config.losses.keys():\n                                    logits = slim.fully_connected(prelogits, num_classes, \n                                                                    weights_regularizer=slim.l2_regularizer(config.weight_decay),\n                                                                    weights_initializer=slim.xavier_initializer(),\n                                                                    biases_initializer=tf.constant_initializer(0.0),\n                                                                    activation_fn=None, scope=\'Logits\')\n                                    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n                                                    labels=labels, logits=logits), name=\'cross_entropy\')\n                                    losses.append(cross_entropy)\n                                    insert_dict(\'sloss\', cross_entropy)\n                                # L2-Softmax\n                                if \'cosine\' in config.losses.keys():\n                                    logits, cosine_loss = tflib.cosine_softmax(prelogits, labels, num_classes, \n                                                            weight_decay=config.weight_decay,\n                                                            **config.losses[\'cosine\']) \n                                    losses.append(cosine_loss)\n                                    insert_dict(\'closs\', cosine_loss)\n                                # A-Softmax\n                                if \'angular\' in config.losses.keys():\n                                    angular_loss = tflib.angular_softmax(prelogits, labels, num_classes, \n                                                            global_step, weight_decay=config.weight_decay,\n                                                            **config.losses[\'angular\'])  \n                                    losses.append(angular_loss)\n                                    insert_dict(\'aloss\', angular_loss)\n                                # AM-Softmax\n                                if \'am\' in config.losses.keys():\n                                    am_loss = tflib.am_softmax(prelogits, labels, num_classes, \n                                                            weight_decay=config.weight_decay,\n                                                            **config.losses[\'am\'])\n                                    losses.append(am_loss)\n                                    insert_dict(\'loss\', am_loss)\n                                # Max-margin Pairwise Score (MPS)\n                                if \'pair\' in config.losses.keys():\n                                    pair_loss = tflib.pair_loss(prelogits, labels, num_classes, \n                                                            **config.losses[\'pair\'])  \n                                    losses.append(pair_loss)\n                                    insert_dict(\'loss\', pair_loss)\n                                # DIAM-Softmax\n                                if \'diam\' in config.losses.keys():\n                                    diam_loss = tflib.diam_softmax(prelogits, labels, num_classes, \n                                                            **config.losses[\'diam\'])  \n                                    diam_loss = tf.identity(diam_loss, name=\'diam_loss\')\n                                    losses.append(diam_loss)\n                                    insert_dict(\'amloss\', diam_loss)\n\n                               # Collect all losses\n                                reg_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES), name=\'reg_loss\')\n                                losses.append(reg_loss)\n                                insert_dict(\'reg_loss\', reg_loss)\n\n                                total_loss = tf.add_n(losses, name=\'total_loss\')\n                                grads_split = tf.gradients(total_loss, tf.trainable_variables())\n                                grads_splits.append(grads_split)\n\n\n\n                # Merge the splits\n                self.watchlist = {}\n                grads = tflib.average_grads(grads_splits)\n                for k,v in split_dict.items():\n                    v = tflib.average_tensors(v)\n                    self.watchlist[k] = v\n                    if \'loss\' in k:\n                        tf.summary.scalar(\'losses/\' + k, v)\n                    else:\n                        tf.summary.scalar(k, v)\n\n\n                # Training Operaters\n                apply_gradient_op = tflib.apply_gradient(tf.trainable_variables(), grads, config.optimizer,\n                                        learning_rate_placeholder, config.learning_rate_multipliers)\n\n                update_global_step_op = tf.assign_add(global_step, 1)\n\n                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n                train_ops = [apply_gradient_op, update_global_step_op] + update_ops\n                train_op = tf.group(*train_ops)\n\n                tf.summary.scalar(\'learning_rate\', learning_rate_placeholder)\n                summary_op = tf.summary.merge_all()\n\n                # Initialize variables\n                self.sess.run(tf.local_variables_initializer())\n                self.sess.run(tf.global_variables_initializer())\n                self.saver = tf.train.Saver(tf.trainable_variables())\n\n                # Keep useful tensors\n                self.image_batch_placeholder = image_batch_placeholder\n                self.label_batch_placeholder = label_batch_placeholder \n                self.learning_rate_placeholder = learning_rate_placeholder \n                self.keep_prob_placeholder = keep_prob_placeholder \n                self.phase_train_placeholder = phase_train_placeholder \n                self.global_step = global_step\n                self.train_op = train_op\n                self.summary_op = summary_op\n                \n\n\n    def train(self, image_batch, label_batch, learning_rate, keep_prob):\n        feed_dict = {self.image_batch_placeholder: image_batch,\n                    self.label_batch_placeholder: label_batch,\n                    self.learning_rate_placeholder: learning_rate,\n                    self.keep_prob_placeholder: keep_prob,\n                    self.phase_train_placeholder: True,}\n        _, wl, sm = self.sess.run([self.train_op, self.watchlist, self.summary_op], feed_dict = feed_dict)\n        step = self.sess.run(self.global_step)\n\n        return wl, sm, step\n    \n    def restore_model(self, *args, **kwargs):\n        trainable_variables = self.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n        tflib.restore_model(self.sess, trainable_variables, *args, **kwargs)\n\n    def save_model(self, model_dir, global_step):\n        tflib.save_model(self.sess, self.saver, model_dir, global_step)\n        \n\n    def load_model(self, *args, **kwargs):\n        tflib.load_model(self.sess, *args, **kwargs)\n        self.phase_train_placeholder = self.graph.get_tensor_by_name(\'phase_train:0\')\n        self.keep_prob_placeholder = self.graph.get_tensor_by_name(\'keep_prob:0\')\n        self.inputs = self.graph.get_tensor_by_name(\'inputs:0\')\n        self.outputs = self.graph.get_tensor_by_name(\'outputs:0\')\n\n    def extract_feature(self, images, batch_size, verbose=False):\n        num_images = images.shape[0] if type(images)==np.ndarray else len(images)\n        num_features = self.outputs.shape[1]\n        result = np.ndarray((num_images, num_features), dtype=np.float32)\n        start_time = time.time()\n        for start_idx in range(0, num_images, batch_size):\n            if verbose:\n                elapsed_time = time.strftime(\'%H:%M:%S\', time.gmtime(time.time()-start_time))\n                sys.stdout.write(\'# of images: %d Current image: %d Elapsed time: %s \\t\\r\' \n                    % (num_images, start_idx, elapsed_time))\n            end_idx = min(num_images, start_idx + batch_size)\n            inputs = images[start_idx:end_idx]\n            feed_dict = {self.inputs: inputs,\n                        self.phase_train_placeholder: False,\n                    self.keep_prob_placeholder: 1.0}\n            result[start_idx:end_idx] = self.sess.run(self.outputs, feed_dict=feed_dict)\n        if verbose:\n            print(\'\')\n        return result\n\n        \n'"
src/extract_features.py,0,"b'""""""Extract features using pre-trained model\n""""""\n# MIT License\n# \n# Copyright (c) 2018 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport os\nimport sys\nimport time\nimport math\nimport argparse\nimport numpy as np\nimport tensorflow as tf\n\nfrom basenet import BaseNetwork\nfrom sibling_net import SiblingNetwork\nimport utils\nimport tflib\n\n\ndef main(args):\n    # Get the configuration file\n    config = utils.import_file(os.path.join(args.model_dir, \'config.py\'), \'config\')\n    \n    # Get the paths of the aligned images\n    with open(args.image_list) as f:\n        paths = [line.strip() for line in f]\n    print(\'%d images to load.\' % len(paths))\n    assert(len(paths)>0)\n    \n\n    # Pre-process the images\n    images = utils.preprocess(paths, config, False)\n    switch = np.array([utils.is_typeB(p) for p in paths])\n    print(\'%d type A images and %d type B images.\' % (np.sum(switch), np.sum(~switch)))\n\n\n    # Load model files and config file\n    if config.use_sibling:\n        network = SiblingNetwork()\n    else:\n        network = BaseNetwork()\n    network.load_model(args.model_dir)\n\n\n    # Run forward pass to calculate embeddings\n    if config.use_sibling:\n        embeddings = network.extract_feature(images, switch, args.batch_size, verbose=True)\n    else:\n        embeddings = network.extract_feature(images, args.batch_size, verbose=True)\n\n\n    # Output the extracted features\n    np.save(args.output, embeddings)\n\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--model_dir"", help=""The path to the pre-trained model directory"",\n                        type=str, required=True)\n    parser.add_argument(""--image_list"", help=""The list file to aligned face images to extract features"",\n                        type=str, required=True)\n    parser.add_argument(""--output"", help=""The output numpy file to store the extracted features"",\n                        type=str, required=True)\n    parser.add_argument(""--batch_size"", help=""Number of images per mini batch"",\n                        type=int, default=128)\n    args = parser.parse_args()\n    main(args)\n'"
src/lfw.py,0,"b'""""""Test protocols on LFW dataset\n""""""\n# MIT License\n# \n# Copyright (c) 2017 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport os\nimport numpy as np\nfrom collections import namedtuple\n\nimport utils\n\nStandardFold = namedtuple(\'StandardFold\', [\'indices1\', \'indices2\', \'labels\'])\n\nclass LFWTest:\n    def __init__(self, image_paths):\n        self.image_paths = image_paths\n        self.images = None\n        self.standard_folds = None\n        self.queue_idx = None\n\n    def init_standard_proto(self, lfw_pairs_file):\n        index_dict = {}\n        for i, image_path in enumerate(self.image_paths):\n            image_name, image_ext = os.path.splitext(os.path.basename(image_path))\n            index_dict[image_name] = i\n\n        pairs = []\n        with open(lfw_pairs_file, \'r\') as f:\n            for line in f.readlines()[1:]:\n                pair = line.strip().split()\n                pairs.append(pair)\n\n        # 10 folds\n        self.standard_folds = []\n        for i in range(10):\n            indices1 = np.zeros(600, dtype=np.int32)\n            indices2 = np.zeros(600, dtype=np.int32)\n            labels = np.array([True]*300+[False]*300, dtype=np.bool)\n            # 300 positive pairs, 300 negative pairs in order\n            for j in range(600):\n                pair = pairs[600*i+j]\n                if j < 300:\n                    assert len(pair) == 3\n                    img1 = pair[0] + \'_\' + \'%04d\' % int(pair[1])\n                    img2 = pair[0] + \'_\' + \'%04d\' % int(pair[2])\n                else:\n                    assert len(pair) == 4\n                    img1 = pair[0] + \'_\' + \'%04d\' % int(pair[1])\n                    img2 = pair[2] + \'_\' + \'%04d\' % int(pair[3])                \n                indices1[j] = index_dict[img1]\n                indices2[j] = index_dict[img2]\n            fold = StandardFold(indices1, indices2, labels)\n            self.standard_folds.append(fold)\n\n    def test_standard_proto(self, features):\n\n        assert self.standard_folds is not None\n        \n        accuracies = np.zeros(10, dtype=np.float32)\n        thresholds = np.zeros(10, dtype=np.float32)\n\n        features1 = []\n        features2 = []\n\n        for i in range(10):\n            # Training\n            train_indices1 = np.concatenate([self.standard_folds[j].indices1 for j in range(10) if j!=i])\n            train_indices2 = np.concatenate([self.standard_folds[j].indices2 for j in range(10) if j!=i])\n            train_labels = np.concatenate([self.standard_folds[j].labels for j in range(10) if j!=i])\n\n            train_features1 = features[train_indices1,:]\n            train_features2 = features[train_indices2,:]\n            \n            train_score =  - np.sum(np.square(train_features1 - train_features2), axis=1)\n            # train_score = np.sum(train_features1 * train_features2, axis=1)\n            _, thresholds[i] = utils.accuracy(train_score, train_labels)\n\n            # Testing\n            fold = self.standard_folds[i]\n            test_features1 = features[fold.indices1,:]\n            test_features2 = features[fold.indices2,:]\n            \n            test_score = - np.sum(np.square(test_features1 - test_features2), axis=1)\n            # test_score = np.sum(test_features1 * test_features2, axis=1)\n            accuracies[i], _ = utils.accuracy(test_score, fold.labels, np.array([thresholds[i]]))\n\n        accuracy = np.mean(accuracies)\n        threshold = - np.mean(thresholds)\n        return accuracy, threshold\n\n\n'"
src/sibling_net.py,50,"b'""""""Sibling Network for heterogeneous face recognition\n""""""\n# MIT License\n# \n# Copyright (c) 2018 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport sys\nimport time\nimport imp\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nimport tflib\n\nclass SiblingNetwork:\n    def __init__(self):\n        self.graph = tf.Graph()\n        gpu_options = tf.GPUOptions(allow_growth=True)\n        tf_config = tf.ConfigProto(gpu_options=gpu_options,\n                allow_soft_placement=True, log_device_placement=False)\n        self.sess = tf.Session(graph=self.graph, config=tf_config)\n            \n    def initialize(self, config, num_classes):\n        \'\'\'\n            Initialize the graph from scratch according config.\n        \'\'\'\n        with self.graph.as_default():\n            with self.sess.as_default():\n                # Set up placeholders\n                w, h = config.image_size\n                channels = config.channels\n                image_batch_placeholder = tf.placeholder(tf.float32, shape=[None, h, w, channels], name=\'image_batch\')\n                label_batch_placeholder = tf.placeholder(tf.int32, shape=[None], name=\'label_batch\')\n                learning_rate_placeholder = tf.placeholder(tf.float32, name=\'learning_rate\')\n                keep_prob_placeholder = tf.placeholder(tf.float32, name=\'keep_prob\')\n                phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n                switch_place_holder = tf.placeholder(tf.bool, shape=[None], name=""switch_all"")\n                global_step = tf.Variable(0, trainable=False, dtype=tf.int32, name=\'global_step\')\n                network = imp.load_source(\'network\', config.network)\n\n                image_splits = tf.split(image_batch_placeholder, config.num_gpus)\n                switch_splits = tf.split(switch_place_holder, config.num_gpus)\n                label_splits = tf.split(label_batch_placeholder, config.num_gpus)\n                grads_splits = []\n                split_dict = {}\n                def insert_dict(k,v):\n                    if k in split_dict: split_dict[k].append(v)\n                    else: split_dict[k] = [v]\n                        \n                for i in range(config.num_gpus):\n                    scope_name = \'\' if i==0 else \'gpu_%d\' % i\n                    with tf.name_scope(scope_name):\n                        with tf.variable_scope(\'\', reuse=i>0):\n                            with tf.device(\'/gpu:%d\' % i):\n                                images = tf.identity(image_splits[i], name=\'inputs\')\n                                switch = tf.identity(switch_splits[i], name=\'switch\')\n                                labels = tf.identity(label_splits[i], name=\'labels\')\n                                # Save the first channel for testing\n                                if i == 0:\n                                    self.inputs = images\n                                    self.switch = switch\n                                \n                                images_A = tf.boolean_mask(images, tf.logical_not(switch), name=""images_A"")\n                                images_B =  tf.boolean_mask(images, switch, name=""images_B"")\n                                labels_A = tf.boolean_mask(labels, tf.logical_not(switch), name=""labels_B"")\n                                labels_B =  tf.boolean_mask(labels, switch, name=""labels_B"")\n\n                                prelogits_A, prelogits_B = network.inference(images_A, images_B,\n                                        keep_prob_placeholder, phase_train_placeholder,bottleneck_layer_size = config.embedding_size, \n                                        weight_decay = config.weight_decay, model_version = config.model_version)\n                                prelogits_A = tf.identity(prelogits_A, name=\'prelogits_A\')\n                                prelogits_B = tf.identity(prelogits_B, name=\'prelogits_B\')\n                                embeddings_A = tf.nn.l2_normalize(prelogits_A, dim=1, name=\'embeddings_A\')\n                                embeddings_B = tf.nn.l2_normalize(prelogits_B, dim=1, name=\'embeddings_B\')\n                                if i == 0:\n                                    self.outputs_A = tf.identity(embeddings_A, name=\'outputs_A\')\n                                    self.outputs_B = tf.identity(embeddings_B, name=\'outputs_B\')\n\n\n\n                                # Build all losses\n                                losses = []\n                                prelogits_all = tf.concat([prelogits_A, prelogits_B], axis=0)\n                                labels_all = tf.concat([labels_A, labels_B], axis=0)\n                                # L2-Softmax\n                                if \'cosine\' in config.losses.keys():\n                                    logits, cosine_loss = tflib.cosine_softmax(prelogits_all, labels_all, num_classes, \n                                                            weight_decay=config.weight_decay,\n                                                            **config.losses[\'cosine\']) \n                                    cosine_loss = tf.identity(cosine_loss, name=\'cosine_loss\')\n                                    losses.append(cosine_loss)\n                                    insert_dict(\'closs\', cosine_loss)\n                                # AM-Softmax\n                                if \'am\' in config.losses.keys():\n                                    am_loss = tflib.am_softmax(prelogits_all, labels_all, num_classes, \n                                                            weight_decay=config.weight_decay,\n                                                            **config.losses[\'am\']) \n                                    am_loss = tf.identity(am_loss, name=\'am_loss\')\n                                    losses.append(am_loss)\n                                    insert_dict(\'amloss\', am_loss)\n                                # Max-margin Pairwise Score (MPS)\n                                if \'pair\' in config.losses.keys():\n                                    pair_loss = tflib.pair_loss_sibling(prelogits_A, prelogits_B, labels_A, labels_B, num_classes, \n                                                            **config.losses[\'pair\'])  \n                                    losses.append(pair_loss)\n                                    insert_dict(\'loss\', pair_loss)\n                                # DIAM-Softmax\n                                if \'diam\' in config.losses.keys():\n                                    diam_loss = tflib.diam_softmax(prelogits_all, labels_all, num_classes, \n                                                            **config.losses[\'diam\'])  \n                                    diam_loss = tf.identity(diam_loss, name=\'diam_loss\')\n                                    losses.append(diam_loss)\n                                    insert_dict(\'amloss\', diam_loss)\n\n\n                               # Collect all losses\n                                reg_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES), name=\'reg_loss\')\n                                losses.append(reg_loss)\n                                insert_dict(\'reg_loss\', reg_loss)\n\n                                total_loss = tf.add_n(losses, name=\'total_loss\')\n                                grads_split = tf.gradients(total_loss, tf.trainable_variables())\n                                grads_splits.append(grads_split)\n\n\n\n                # Merge the splits\n                self.watchlist = {}\n                grads = tflib.average_grads(grads_splits)\n                for k,v in split_dict.items():\n                    v = tflib.average_tensors(v)\n                    self.watchlist[k] = v\n                    if \'loss\' in k:\n                        tf.summary.scalar(\'losses/\' + k, v)\n                    else:\n                        tf.summary.scalar(k, v)\n\n\n                # Training Operaters\n                apply_gradient_op = tflib.apply_gradient(tf.trainable_variables(), grads, config.optimizer,\n                                        learning_rate_placeholder, config.learning_rate_multipliers)\n\n                update_global_step_op = tf.assign_add(global_step, 1)\n\n                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n                train_ops = [apply_gradient_op, update_global_step_op] + update_ops\n                train_op = tf.group(*train_ops)\n\n                tf.summary.scalar(\'learning_rate\', learning_rate_placeholder)\n                summary_op = tf.summary.merge_all()\n\n                # Initialize variables\n                self.sess.run(tf.local_variables_initializer())\n                self.sess.run(tf.global_variables_initializer())\n                self.saver = tf.train.Saver(tf.trainable_variables())\n\n                # Keep useful tensors\n                self.image_batch_placeholder = image_batch_placeholder\n                self.label_batch_placeholder = label_batch_placeholder \n                self.switch_place_holder = switch_place_holder\n                self.learning_rate_placeholder = learning_rate_placeholder \n                self.keep_prob_placeholder = keep_prob_placeholder \n                self.phase_train_placeholder = phase_train_placeholder \n                self.global_step = global_step\n                self.train_op = train_op\n                self.summary_op = summary_op\n                \n\n\n    def train(self, image_batch, label_batch, switch_batch, learning_rate, keep_prob):\n\n        assert np.all(label_batch[switch_batch] == label_batch[~switch_batch]), print(label_batch)\n\n        feed_dict = {self.image_batch_placeholder: image_batch,\n                    self.label_batch_placeholder: label_batch,\n                    self.switch_place_holder: switch_batch,\n                    self.learning_rate_placeholder: learning_rate,\n                    self.keep_prob_placeholder: keep_prob,\n                    self.phase_train_placeholder: True,}\n        _, wl, sm = self.sess.run([self.train_op, self.watchlist, self.summary_op], feed_dict = feed_dict)\n        step = self.sess.run(self.global_step)\n\n        return wl, sm, step\n    \n    def restore_model(self, replace_scopes, *args, **kwargs):\n        # Scopes are restored one by one to avoid name collision\n        for dst_scope, src_scope in replace_scopes.items():\n            trainable_variables = self.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=dst_scope)\n            replace_rules = {dst_scope: src_scope}\n            tflib.restore_model(self.sess, trainable_variables, *args, replace_rules=replace_rules, **kwargs)\n\n    def save_model(self, model_dir, global_step):\n        tflib.save_model(self.sess, self.saver, model_dir, global_step)\n        \n\n    def load_model(self, *args, **kwargs):\n        tflib.load_model(self.sess, *args, **kwargs)\n        self.phase_train_placeholder = self.graph.get_tensor_by_name(\'phase_train:0\')\n        self.keep_prob_placeholder = self.graph.get_tensor_by_name(\'keep_prob:0\')\n        self.inputs = self.graph.get_tensor_by_name(\'inputs:0\')\n        self.switch = self.graph.get_tensor_by_name(\'switch:0\')\n        self.outputs_B = self.graph.get_tensor_by_name(\'outputs_B:0\')\n        self.outputs_A = self.graph.get_tensor_by_name(\'outputs_A:0\')\n\n    def extract_feature(self, images, switch, batch_size, verbose=False):\n        num_images = images.shape[0] if type(images)==np.ndarray else len(images)\n        num_features = self.outputs_B.shape[1]\n        result = np.ndarray((num_images, num_features), dtype=np.float32)\n        start_time = time.time()\n        for start_idx in range(0, num_images, batch_size):\n            end_idx = min(num_images, start_idx + batch_size)\n            inputs = images[start_idx:end_idx]\n            switch_batch = switch[start_idx:end_idx]\n            if verbose:\n                elapsed_time = time.strftime(\'%H:%M:%S\', time.gmtime(time.time()-start_time))\n                sys.stdout.write(\'# of images: %d Current image: %d Elapsed time: %s \\t\\r\' \n                    % (num_images, start_idx, elapsed_time))\n            feed_dict = {\n                    self.inputs: inputs,\n                    self.switch: switch_batch,\n                    self.phase_train_placeholder: False,\n                    self.keep_prob_placeholder: 1.0}\n            if np.any(switch_batch) and np.any(~switch_batch):\n                result_A, result_B = self.sess.run([self.outputs_A, self.outputs_B], feed_dict=feed_dict)\n                result_temp = np.ndarray((end_idx-start_idx, num_features), dtype=np.float32)\n                result_temp[~switch_batch,:] = result_A\n                result_temp[switch_batch,:] = result_B\n                result[start_idx:end_idx] = result_temp\n            elif np.all(switch_batch):\n                result_B = self.sess.run(self.outputs_B, feed_dict=feed_dict)\n                result[start_idx:end_idx] = result_B\n            else:\n                result_A = self.sess.run(self.outputs_A, feed_dict=feed_dict)\n                result[start_idx:end_idx] = result_A               \n        if verbose:\n            print(\'\')\n        return result\n\n        \n'"
src/tflib.py,159,"b'""""""Functions for building tensorflow graphs.\n""""""\n# MIT License\n# \n# Copyright (c) 2018 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n\ndef average_tensors(tensors, name=None):\n    if len(tensors) == 1:\n        return tf.identity(tensors[0], name=name)\n    else:\n        # Each tensor in the list should be of the same size\n        expanded_tensors = []\n\n        for t in tensors:\n            expanded_t = tf.expand_dims(t, 0)\n            expanded_tensors.append(expanded_t)\n\n        average_tensor = tf.concat(axis=0, values=expanded_tensors)\n        average_tensor = tf.reduce_mean(average_tensor, 0, name=name)\n\n        return average_tensor\n\ndef average_grads(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n        tower_grads: List of lists of gradients. The outer list is over different \n        towers. The inner list is over the gradient calculation in each tower.\n    Returns:\n        List of gradients where the gradient has been averaged across all towers.\n    """"""\n    if len(tower_grads) == 1:\n        return tower_grads[0]\n    else:\n        average_grads = []\n        for grad_ in zip(*tower_grads):\n            # Note that each grad looks like the following:\n            #   (grad0_gpu0, ... , grad0_gpuN)\n            average_grad = None if grad_[0]==None else average_tensors(grad_)\n            average_grads.append(average_grad)\n\n        return average_grads\n\ndef apply_gradient(update_gradient_vars, grads, optimizer, learning_rate, learning_rate_multipliers=None):\n    assert(len(grads)==len(update_gradient_vars))\n    if learning_rate_multipliers is None: learning_rate_multipliers = {}\n    # Build a dictionary to save multiplier config\n    # format -> {scope_name: ((grads, vars), lr_multi)}\n    learning_rate_dict = {}\n    learning_rate_dict[\'__default__\'] = ([], 1.0)\n    for scope, multiplier in learning_rate_multipliers.items():\n        assert scope != \'__default__\'\n        learning_rate_dict[scope] = ([], multiplier)\n\n    # Scan all the variables, insert into dict\n    scopes = learning_rate_dict.keys()\n    for var, grad in zip(update_gradient_vars, grads):\n        count = 0\n        scope_temp = \'\'\n        for scope in scopes:\n            if scope in var.name:\n                scope_temp = scope\n                count += 1\n        assert count <= 1, ""More than one multiplier scopes appear in variable: %s"" % var.name\n        if count == 0: scope_temp = \'__default__\'\n        if grad is not None:\n            learning_rate_dict[scope_temp][0].append((grad, var))\n     \n    # Build a optimizer for each multiplier scope\n    apply_gradient_ops = []\n    print(\'\\nLearning rate multipliers:\')\n    for scope, scope_content in learning_rate_dict.items():\n        scope_grads_vars, multiplier = scope_content\n        print(\'%s:\\n  # variables: %d\\n  lr_multi: %f\' % (scope, len(scope_grads_vars), multiplier))\n        if len(scope_grads_vars) == 0:\n            continue\n        scope_learning_rate = multiplier * learning_rate\n        if optimizer==\'ADAGRAD\':\n            opt = tf.train.AdagradOptimizer(scope_learning_rate)\n        elif optimizer==\'ADADELTA\':\n            opt = tf.train.AdadeltaOptimizer(scope_learning_rate, rho=0.9, epsilon=1e-6)\n        elif optimizer==\'ADAM\':\n            opt = tf.train.AdamOptimizer(scope_learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n        elif optimizer==\'RMSPROP\':\n            opt = tf.train.RMSPropOptimizer(scope_learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n        elif optimizer==\'MOM\':\n            opt = tf.train.MomentumOptimizer(scope_learning_rate, 0.9, use_nesterov=False)\n        elif optimizer==\'SGD\':\n            opt = tf.train.GradientDescentOptimizer(scope_learning_rate)\n        else:\n            raise ValueError(\'Invalid optimization algorithm\')\n        apply_gradient_ops.append(opt.apply_gradients(scope_grads_vars))\n    print(\'\')\n    apply_gradient_op = tf.group(*apply_gradient_ops)\n\n    return apply_gradient_op\n\n\ndef save_model(sess, saver, model_dir, global_step):\n    with sess.graph.as_default():\n        checkpoint_path = os.path.join(model_dir, \'ckpt\')\n        metagraph_path = os.path.join(model_dir, \'graph.meta\')\n\n        print(\'Saving variables...\')\n        saver.save(sess, checkpoint_path, global_step=global_step, write_meta_graph=False)\n        if not os.path.exists(metagraph_path):\n            print(\'Saving metagraph...\')\n            saver.export_meta_graph(metagraph_path)\n\ndef restore_model(sess, var_list, model_dir, restore_scopes=None, replace_rules=None):\n    \'\'\' Load the variable values from a checkpoint file into pre-defined graph.\n    Filter the variables so that they contain at least one of the given keywords.\'\'\'\n    with sess.graph.as_default():\n        if restore_scopes is not None:\n            var_list = [var for var in var_list if any([scope in var.name for scope in restore_scopes])]\n        if replace_rules is not None:\n            var_dict = {}\n            for var in var_list:\n                name_new = var.name\n                for k,v in replace_rules.items(): name_new=name_new.replace(k,v)\n                name_new = name_new[:-2] # When using dict, numbers should be removed\n                var_dict[name_new] = var\n            var_list = var_dict\n        model_dir = os.path.expanduser(model_dir)\n        ckpt_file = tf.train.latest_checkpoint(model_dir)\n\n        print(\'Restoring {} variables from {} ...\'.format(len(var_list), ckpt_file))\n        saver = tf.train.Saver(var_list)\n        saver.restore(sess, ckpt_file)\n\ndef load_model(sess, model_path, scope=None):\n    \'\'\' Load the the graph and variables values from a model path.\n    Model path is either a a frozen graph or a directory with both\n    a .meta file and checkpoint files.\'\'\'\n    with sess.graph.as_default():\n        model_path = os.path.expanduser(model_path)\n        if (os.path.isfile(model_path)):\n            # Frozen grpah\n            print(\'Model filename: %s\' % model_path)\n            with gfile.FastGFile(model_path,\'rb\') as f:\n                graph_def = tf.GraphDef()\n                graph_def.ParseFromString(f.read())\n                tf.import_graph_def(graph_def, name=\'\')\n        else:\n            # Load grapha and variables separatedly.\n            meta_files = [file for file in os.listdir(model_path) if file.endswith(\'.meta\')]\n            assert len(meta_files) == 1\n            meta_file = os.path.join(model_path, meta_files[0])\n            ckpt_file = tf.train.latest_checkpoint(model_path)\n            \n            print(\'Metagraph file: %s\' % meta_file)\n            print(\'Checkpoint file: %s\' % ckpt_file)\n            saver = tf.train.import_meta_graph(meta_file, clear_devices=True, import_scope=scope)\n            saver.restore(sess, ckpt_file)\n\n\ndef euclidean_distance(X, Y, sqrt=False):\n    \'\'\'Compute the distance between each X and Y.\n\n    Args: \n        X: a (m x d) tensor\n        Y: a (d x n) tensor\n    \n    Returns: \n        diffs: an m x n distance matrix.\n    \'\'\'\n    with tf.name_scope(\'EuclideanDistance\'):\n        XX = tf.reduce_sum(tf.square(X), 1, keep_dims=True)\n        YY = tf.reduce_sum(tf.square(Y), 0, keep_dims=True)\n        XY = tf.matmul(X, Y)\n        diffs = XX + YY - 2*XY\n        diffs = tf.maximum(0.0, diffs)\n        if sqrt == True:\n            diffs = tf.sqrt(diffs)\n    return diffs\n\ndef cosine_softmax(prelogits, label, num_classes, weight_decay, scale=16.0, reuse=None):\n    \'\'\' Tensorflow implementation of L2-Sofmax, proposed in:\n        R. Ranjan, C. D. Castillo, and R. Chellappa. L2 constrained softmax loss for \n        discriminativeface veri\xef\xac\x81cation. arXiv:1703.09507, 2017. \n    \'\'\'\n    \n    nrof_features = prelogits.shape[1].value\n    \n    with tf.variable_scope(\'Logits\', reuse=reuse):\n        weights = tf.get_variable(\'weights\', shape=(nrof_features, num_classes),\n                regularizer=slim.l2_regularizer(weight_decay),\n                initializer=slim.xavier_initializer(),\n                # initializer=tf.truncated_normal_initializer(stddev=0.1),\n                dtype=tf.float32)\n        _scale = tf.get_variable(\'scale\', shape=(),\n                regularizer=slim.l2_regularizer(1e-2),\n                initializer=tf.constant_initializer(1.00),\n                trainable=True,\n                dtype=tf.float32)\n\n        weights_normed = tf.nn.l2_normalize(weights, dim=0)\n        prelogits_normed = tf.nn.l2_normalize(prelogits, dim=1)\n\n        if scale == \'auto\':\n            scale = tf.nn.softplus(_scale)\n        else:\n            assert type(scale) == float\n            scale = tf.constant(scale)\n\n        logits = scale * tf.matmul(prelogits_normed, weights_normed)\n\n\n    cross_entropy =  tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\\\n            labels=label, logits=logits), name=\'cross_entropy\')\n\n    return logits, cross_entropy\n\n\n\ndef angular_softmax(prelogits, label, num_classes, global_step, weight_decay,\n            m, lamb_min, lamb_max, reuse=None):\n    \'\'\' Tensorflow implementation of Angular-Sofmax, proposed in:\n        W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song. \n        Sphereface: Deep hypersphere embedding for face recognition. In CVPR, 2017.\n    \'\'\'\n    num_features = prelogits.shape[1].value\n    batch_size = tf.shape(prelogits)[0]\n    lamb_min = lamb_min\n    lamb_max = lamb_max\n    lambda_m_theta = [\n        lambda x: x**0,\n        lambda x: x**1,\n        lambda x: 2.0*(x**2) - 1.0,\n        lambda x: 4.0*(x**3) - 3.0*x,\n        lambda x: 8.0*(x**4) - 8.0*(x**2) + 1.0,\n        lambda x: 16.0*(x**5) - 20.0*(x**3) + 5.0*x\n    ]\n\n    with tf.variable_scope(\'AngularSoftmax\', reuse=reuse):\n        weights = tf.get_variable(\'weights\', shape=(num_features, num_classes),\n                regularizer=slim.l2_regularizer(1e-4),\n                initializer=slim.xavier_initializer(),\n                # initializer=tf.truncated_normal_initializer(stddev=0.1),\n                trainable=True,\n                dtype=tf.float32)\n        lamb = tf.get_variable(\'lambda\', shape=(),\n                initializer=tf.constant_initializer(lamb_max),\n                trainable=False,\n                dtype=tf.float32)\n        prelogits_norm  = tf.sqrt(tf.reduce_sum(tf.square(prelogits), axis=1, keep_dims=True))\n        weights_normed = tf.nn.l2_normalize(weights, dim=0)\n        prelogits_normed = tf.nn.l2_normalize(prelogits, dim=1)\n\n        # Compute cosine and phi\n        cos_theta = tf.matmul(prelogits_normed, weights_normed)\n        cos_theta = tf.minimum(1.0, tf.maximum(-1.0, cos_theta))\n        theta = tf.acos(cos_theta)\n        cos_m_theta = lambda_m_theta[m](cos_theta)\n        k = tf.floor(m*theta / 3.14159265)\n        phi_theta = tf.pow(-1.0, k) * cos_m_theta - 2.0 * k\n\n        cos_theta = cos_theta * prelogits_norm\n        phi_theta = phi_theta * prelogits_norm\n\n        lamb_new = tf.maximum(lamb_min, lamb_max/(1.0+0.1*tf.cast(global_step, tf.float32)))\n        update_lamb = tf.assign(lamb, lamb_new)\n        \n        # Compute loss\n        with tf.control_dependencies([update_lamb]):\n            label_dense = tf.one_hot(label, num_classes, dtype=tf.float32)\n\n            logits = cos_theta\n            logits -= label_dense * cos_theta * 1.0 / (1.0+lamb)\n            logits += label_dense * phi_theta * 1.0 / (1.0+lamb)\n            \n            cross_entropy =  tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\\\n                labels=label, logits=logits), name=\'cross_entropy\')\n\n    return cross_entropy\n\n\ndef am_softmax(prelogits, label, num_classes, \n                weight_decay, scale=\'auto\', m=1.0, reuse=None):\n    \'\'\' Tensorflow implementation of AM-Sofmax, proposed in:\n        F. Wang, W. Liu, H. Liu, and J. Cheng. Additive margin softmax for face veri\xef\xac\x81cation. arXiv:1801.05599, 2018.\n    \'\'\'\n    num_features = prelogits.shape[1].value\n    batch_size = tf.shape(prelogits)[0]\n    with tf.variable_scope(\'SplitSoftmax\', reuse=reuse):\n        weights = tf.get_variable(\'weights\', shape=(num_classes, num_features),\n                regularizer=slim.l2_regularizer(weight_decay),\n                initializer=slim.xavier_initializer(),\n                # initializer=tf.truncated_normal_initializer(stddev=0.0),\n                # initializer=tf.constant_initializer(0),\n                trainable=True,\n                dtype=tf.float32)\n        _scale = tf.get_variable(\'scale\', shape=(),\n                regularizer=slim.l2_regularizer(1e-2),\n                initializer=tf.constant_initializer(1.00),\n                trainable=True,\n                dtype=tf.float32)\n\n        # Normalizing the vecotors\n        weights_normed = tf.nn.l2_normalize(weights, dim=1)\n        prelogits_normed = tf.nn.l2_normalize(prelogits, dim=1)\n\n        # Label and logits between batch and examplars\n        label_mat_glob = tf.one_hot(label, num_classes, dtype=tf.float32)\n        label_mask_pos_glob = tf.cast(label_mat_glob, tf.bool)\n        label_mask_neg_glob = tf.logical_not(label_mask_pos_glob)\n\n        dist_mat_glob = tf.matmul(prelogits_normed, tf.transpose(weights_normed))\n        dist_pos_glob = tf.boolean_mask(dist_mat_glob, label_mask_pos_glob)\n        dist_neg_glob = tf.boolean_mask(dist_mat_glob, label_mask_neg_glob)\n\n        logits_glob = dist_mat_glob\n        logits_pos_glob = tf.boolean_mask(logits_glob, label_mask_pos_glob)\n        logits_neg_glob = tf.boolean_mask(logits_glob, label_mask_neg_glob)\n\n        logits_pos = logits_pos_glob\n        logits_neg = logits_neg_glob\n\n        if scale == \'auto\':\n            # Automatic learned scale\n            scale = tf.log(tf.exp(0.0) + tf.exp(_scale))\n        else:\n            # Assigned scale value\n            assert type(scale) == float\n            scale = tf.constant(scale)\n\n        # Losses\n        _logits_pos = tf.reshape(logits_pos, [batch_size, -1])\n        _logits_neg = tf.reshape(logits_neg, [batch_size, -1])\n\n        _logits_pos = _logits_pos * scale\n        _logits_neg = _logits_neg * scale\n        _logits_neg = tf.reduce_logsumexp(_logits_neg, axis=1)[:,None]\n\n        loss_ = tf.nn.softplus(m + _logits_neg - _logits_pos)\n        loss = tf.reduce_mean(loss_, name=\'am_softmax\')\n\n\n    return loss\n\ndef diam_softmax(prelogits, label, num_classes,\n                    scale=\'auto\', m=1.0, alpha=0.5, reuse=None):\n    \'\'\' Implementation of DIAM-Softmax, AM-Softmax with Dynamic Weight Imprinting (DWI), proposed in:\n            Y. Shi and A. K. Jain. DocFace+: ID Document to Selfie Matching. arXiv:1809.05620, 2018.\n        The weights in the DIAM-Softmax are dynamically updated using the mean features of training samples.\n    \'\'\'\n    num_features = prelogits.shape[1].value\n    batch_size = tf.shape(prelogits)[0]\n    with tf.variable_scope(\'AM-Softmax\', reuse=reuse):\n        weights = tf.get_variable(\'weights\', shape=(num_classes, num_features),\n                initializer=slim.xavier_initializer(),\n                trainable=False,\n                dtype=tf.float32)\n        _scale = tf.get_variable(\'_scale\', shape=(),\n                regularizer=slim.l2_regularizer(1e-2),\n                initializer=tf.constant_initializer(0.0),\n                trainable=True,\n                dtype=tf.float32)\n\n        # Normalizing the vecotors\n        prelogits_normed = tf.nn.l2_normalize(prelogits, dim=1)\n        weights_normed = tf.nn.l2_normalize(weights, dim=1)\n\n        # Label and logits between batch and examplars\n        label_mat_glob = tf.one_hot(label, num_classes, dtype=tf.float32)\n        label_mask_pos_glob = tf.cast(label_mat_glob, tf.bool)\n        label_mask_neg_glob = tf.logical_not(label_mask_pos_glob)\n\n        logits_glob = tf.matmul(prelogits_normed, tf.transpose(weights_normed))\n        # logits_glob = -0.5 * euclidean_distance(prelogits_normed, tf.transpose(weights_normed))\n        logits_pos_glob = tf.boolean_mask(logits_glob, label_mask_pos_glob)\n        logits_neg_glob = tf.boolean_mask(logits_glob, label_mask_neg_glob)\n\n        logits_pos = logits_pos_glob\n        logits_neg = logits_neg_glob\n\n        if scale == \'auto\':\n            # Automatic learned scale\n            scale = tf.log(tf.exp(0.0) + tf.exp(_scale))\n        else:\n            # Assigned scale value\n            assert type(scale) == float\n\n        # Losses\n        _logits_pos = tf.reshape(logits_pos, [batch_size, -1])\n        _logits_neg = tf.reshape(logits_neg, [batch_size, -1])\n\n        _logits_pos = _logits_pos * scale\n        _logits_neg = _logits_neg * scale\n        _logits_neg = tf.reduce_logsumexp(_logits_neg, axis=1)[:,None]\n\n        loss_ = tf.nn.softplus(m + _logits_neg - _logits_pos)\n        loss = tf.reduce_mean(loss_, name=\'diam_softmax\')\n\n        # Dynamic weight imprinting\n        # We follow the CenterLoss to update the weights, which is equivalent to\n        # imprinting the mean features\n        weights_batch = tf.gather(weights, label)\n        diff_weights = weights_batch - prelogits_normed\n        unique_label, unique_idx, unique_count = tf.unique_with_counts(label)\n        appear_times = tf.gather(unique_count, unique_idx)\n        appear_times = tf.reshape(appear_times, [-1, 1])\n        diff_weights = diff_weights / tf.cast(appear_times, tf.float32)\n        diff_weights = alpha * diff_weights\n        weights_update_op = tf.scatter_sub(weights, label, diff_weights)\n        with tf.control_dependencies([weights_update_op]):\n            weights_update_op = tf.assign(weights, tf.nn.l2_normalize(weights,dim=1))\n        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, weights_update_op)\n        \n        return loss\n\n\n\ndef pair_loss(prelogits, label, num_classes, m=1.0, reuse=None):\n    \'\'\' Max-margin Pair Score (MPS) loss function proposed in:\n        Y. Shi and A. K. Jain. DocFace: Matching ID Document Photos to Selfies. arXiv:1703.08388, 2017.\n    \'\'\'\n    nrof_features = prelogits.shape[1].value\n    batch_size = tf.shape(prelogits)[0]\n    with tf.variable_scope(\'MaxmarginPairLoss\', reuse=reuse):\n\n        # Normalizing the vecotors\n        prelogits_normed = tf.nn.l2_normalize(prelogits, dim=1)\n\n        prelogits_reshape = tf.reshape(prelogits_normed, [-1,2,tf.shape(prelogits_normed)[1]])\n        prelogits_tmp = prelogits_reshape[:,0,:]\n        prelogits_pro = prelogits_reshape[:,1,:]\n    \n        dist_mat_batch = -0.5 * euclidean_distance(prelogits_tmp, tf.transpose(prelogits_pro), True)\n        # dist_mat_batch = tf.matmul(prelogits_tmp, tf.transpose(prelogits_pro))\n        \n        logits_mat_batch = dist_mat_batch\n\n        num_pairs = tf.shape(prelogits_reshape)[0]\n        label_mask_pos_batch = tf.cast(tf.eye(num_pairs), tf.bool)\n        label_mask_neg_batch = tf.logical_not(label_mask_pos_batch)\n        dist_pos_batch = tf.boolean_mask(dist_mat_batch, label_mask_pos_batch)\n        dist_neg_batch = tf.boolean_mask(dist_mat_batch, label_mask_neg_batch)\n\n        logits_pos_batch = tf.boolean_mask(logits_mat_batch, label_mask_pos_batch)\n        logits_neg_batch = tf.boolean_mask(logits_mat_batch, label_mask_neg_batch)\n\n        logits_pos = logits_pos_batch\n        logits_neg = logits_neg_batch\n    \n        dist_pos = dist_pos_batch\n        dist_neg = dist_neg_batch\n\n\n        # Losses\n\n        _logits_pos = tf.reshape(logits_pos, [num_pairs, -1])\n        _logits_neg_1 = tf.reshape(logits_neg, [num_pairs, -1])\n        _logits_neg_2 = tf.reshape(logits_neg, [-1, num_pairs])\n\n        _logits_pos = _logits_pos\n        _logits_neg_1 = tf.reduce_max(_logits_neg_1, axis=1)[:,None]\n        _logits_neg_2 = tf.reduce_max(_logits_neg_2, axis=0)[:,None]\n        _logits_neg = tf.maximum(_logits_neg_1, _logits_neg_2)\n        # _logits_neg = tf.reduce_logsumexp(_logits_neg, axis=1)[:,None]\n\n        loss_pos = tf.nn.relu(m + _logits_neg - _logits_pos) * 0.5\n        loss_neg = tf.nn.relu(m + _logits_neg - _logits_pos) * 0.5\n        loss = tf.reduce_mean(loss_pos + loss_neg, name=\'pair_loss\')\n\n    return loss\n\n\n\ndef pair_loss_sibling(prelogits_tmp, prelogits_pro, label_tmp, label_pro, \n                    num_classes, m=1.0, reuse=None):\n    \'\'\' Max-margin Pair Score (MPS) loss function proposed in:\n        Y. Shi and A. K. Jain. DocFace: Matching ID Document Photos to Selfies. arXiv:1703.08388, 2017.\n    \'\'\'\n    num_features = prelogits_tmp.shape[1].value\n    batch_size = tf.shape(prelogits_tmp)[0] + tf.shape(prelogits_pro)[0]\n    with tf.variable_scope(\'MaxmarginPairLoss\', reuse=reuse):\n\n        # Normalizing the vecotors\n        prelogits_tmp = tf.nn.l2_normalize(prelogits_tmp, dim=1)\n        prelogits_pro = tf.nn.l2_normalize(prelogits_pro, dim=1)\n\n        dist_mat_batch = -0.5 * euclidean_distance(prelogits_tmp, tf.transpose(prelogits_pro), True)   \n        # dist_mat_batch = tf.matmul(prelogits_tmp, tf.transpose(prelogits_pro))\n        \n        logits_mat_batch = dist_mat_batch\n\n        num_pairs = tf.shape(prelogits_tmp)[0]\n\n        label_mask_pos_batch = tf.cast(tf.eye(num_pairs), tf.bool)\n        label_mask_neg_batch = tf.logical_not(label_mask_pos_batch)\n        dist_pos_batch = tf.boolean_mask(dist_mat_batch, label_mask_pos_batch)\n        dist_neg_batch = tf.boolean_mask(dist_mat_batch, label_mask_neg_batch)\n\n        logits_pos_batch = tf.boolean_mask(logits_mat_batch, label_mask_pos_batch)\n        logits_neg_batch = tf.boolean_mask(logits_mat_batch, label_mask_neg_batch)\n\n        logits_pos = logits_pos_batch\n        logits_neg = logits_neg_batch\n\n        # Losses\n        _logits_pos = tf.reshape(logits_pos, [num_pairs, -1])\n        _logits_neg_1 = tf.reshape(logits_neg, [num_pairs, -1])\n        _logits_neg_2 = tf.reshape(logits_neg, [-1, num_pairs])\n\n        _logits_neg_1 = tf.reduce_max(_logits_neg_1, axis=1)[:,None]\n        _logits_neg_2 = tf.reduce_max(_logits_neg_2, axis=0)[:,None]\n        _logits_neg = tf.maximum(_logits_neg_1, _logits_neg_2)\n\n        loss_pos = tf.nn.relu(m + _logits_neg - _logits_pos) * 0.5\n        loss_neg = tf.nn.relu(m + _logits_neg - _logits_pos) * 0.5\n        loss = tf.reduce_mean(loss_pos + loss_neg, name=\'pair_loss\')\n\n    return loss\n'"
src/train_base.py,2,"b'""""""Main training file for face recognition\n""""""\n# MIT License\n# \n# Copyright (c) 2018 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport os\nimport sys\nimport time\nimport argparse\nimport tensorflow as tf\nimport numpy as np\n\nimport utils\nimport tflib\nfrom lfw import LFWTest\nfrom basenet import BaseNetwork \n\n\ndef main(args):\n    # I/O\n    config_file = args.config_file\n    config = utils.import_file(config_file, \'config\')\n\n    trainset = utils.Dataset(config.train_dataset_path)\n    testset = utils.Dataset(config.test_dataset_path)\n\n    network = BaseNetwork()\n    network.initialize(config, trainset.num_classes)\n\n\n    # Initalization for running\n    log_dir = utils.create_log_dir(config, config_file)\n    summary_writer = tf.summary.FileWriter(log_dir, network.graph)\n    if config.restore_model is not None:\n        network.restore_model(config.restore_model, config.restore_scopes)\n\n    # Set up LFW test protocol and load images\n    print(\'Loading images...\')\n    lfwtest = LFWTest(testset.images)\n    lfwtest.init_standard_proto(config.lfw_pairs_file)\n    lfwtest.images = utils.preprocess(lfwtest.image_paths, config, is_training=False)\n\n\n    trainset.start_batch_queue(config, True)\n\n\n    #\n    # Main Loop\n    #\n    print(\'\\nStart Training\\nname: %s\\n# epochs: %d\\nepoch_size: %d\\nbatch_size: %d\\n\'\\\n        % (config.name, config.num_epochs, config.epoch_size, config.batch_size))\n    global_step = 0\n    start_time = time.time()\n\n    for epoch in range(config.num_epochs):\n\n        # Training\n        for step in range(config.epoch_size):\n            # Prepare input\n            learning_rate = utils.get_updated_learning_rate(global_step, config)\n            batch = trainset.pop_batch_queue()\n        \n            wl, sm, global_step = network.train(batch[\'images\'], batch[\'labels\'], learning_rate, config.keep_prob)\n\n            # Display\n            if step % config.summary_interval == 0:\n                duration = time.time() - start_time\n                start_time = time.time()\n                utils.display_info(epoch, step, duration, wl)\n                summary_writer.add_summary(sm, global_step=global_step)\n\n        # Testing on LFW\n        print(\'Testing on standard LFW protocol...\')\n        embeddings = network.extract_feature(lfwtest.images, config.batch_size)\n        accuracy_embeddings, threshold_embeddings = lfwtest.test_standard_proto(embeddings)\n        print(\'Embeddings Accuracy: %2.4f Threshold %2.3f\' % (accuracy_embeddings, threshold_embeddings))\n\n        with open(os.path.join(log_dir,\'lfw_result.txt\'),\'at\') as f:\n            f.write(\'%d\\t%.5f\\n\' % (global_step,accuracy_embeddings))\n        summary = tf.Summary()\n        summary.value.add(tag=\'lfw/accuracy\', simple_value=accuracy_embeddings)\n        summary_writer.add_summary(summary, global_step)\n\n        # Save the model\n        network.save_model(log_dir, global_step)\n\nif __name__==""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""config_file"", help=""The path to the training configuration file"",\n                        type=str)\n    args = parser.parse_args()\n    main(args)\n\n'"
src/train_sibling.py,3,"b'""""""Training Sibling Network for heterogeneous face recognition\r\n""""""\r\n# MIT License\r\n# \r\n# Copyright (c) 2018 Yichun Shi\r\n# \r\n# Permission is hereby granted, free of charge, to any person obtaining a copy\r\n# of this software and associated documentation files (the ""Software""), to deal\r\n# in the Software without restriction, including without limitation the rights\r\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n# copies of the Software, and to permit persons to whom the Software is\r\n# furnished to do so, subject to the following conditions:\r\n# \r\n# The above copyright notice and this permission notice shall be included in all\r\n# copies or substantial portions of the Software.\r\n# \r\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\n# SOFTWARE.\r\n\r\nimport os\r\nimport sys\r\nimport time\r\nimport argparse\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nimport utils\r\nimport tflib\r\nfrom sibling_net import SiblingNetwork \r\n\r\ndef main(args):\r\n    # I/O\r\n    config_file = args.config_file\r\n    config = utils.import_file(config_file, \'config\')\r\n\r\n    trainset = utils.Dataset(config.train_dataset_path)\r\n    testset = utils.Dataset(config.test_dataset_path)\r\n\r\n    network = SiblingNetwork()\r\n    network.initialize(config, trainset.num_classes)\r\n\r\n\r\n    # Initalization for running\r\n    log_dir = utils.create_log_dir(config, config_file)\r\n    summary_writer = tf.summary.FileWriter(log_dir, network.graph)\r\n    if config.restore_model is not None:\r\n        network.restore_model(config.replace_scopes, config.restore_model, config.restore_scopes)\r\n\r\n    # Separate type A and type B images for building batch and switch\r\n    trainset.separate_AB()\r\n    testset.separate_AB()\r\n\r\n    # Set up test protocol and load images\r\n    print(\'Loading images...\')\r\n    testset.images = utils.preprocess(testset.images, config, is_training=False)\r\n\r\n\r\n    trainset.start_batch_queue(config, True)\r\n\r\n\r\n    #\r\n    # Main Loop\r\n    #\r\n    print(\'\\nStart Training\\nname: %s\\n# epochs: %d\\nepoch_size: %d\\nbatch_size: %d\\n\'\\\r\n        % (config.name, config.num_epochs, config.epoch_size, config.batch_size))\r\n    global_step = 0\r\n    start_time = time.time()\r\n\r\n    for epoch in range(config.num_epochs):\r\n\r\n        if epoch == 0:\r\n            print(\'Testing...\')\r\n            embeddings = network.extract_feature(testset.images, testset.is_typeB, config.batch_size)\r\n            tars, fars, _ = utils.test_roc(embeddings, testset.labels, testset.is_typeB, FARs=[1e-5, 1e-4, 1e-3])\r\n            with open(os.path.join(log_dir,\'result.txt\'),\'at\') as f:\r\n                for i in range(len(tars)):\r\n                    print(\'[%d] TAR: %2.4f FAR %2.5f\' % (epoch+1, tars[i], fars[i]))\r\n                    f.write(\'[%d] TAR: %2.4f FAR %2.5f\\n\' % (epoch+1, tars[i], fars[i]))\r\n                    summary = tf.Summary()\r\n                    summary.value.add(tag=\'test/tar_%d\'%i, simple_value=tars[i])\r\n                    summary_writer.add_summary(summary, global_step)\r\n\r\n        # Training\r\n        for step in range(config.epoch_size):\r\n            # Prepare input\r\n            learning_rate = utils.get_updated_learning_rate(global_step, config)\r\n            batch = trainset.pop_batch_queue()\r\n        \r\n            wl, sm, global_step = network.train(batch[\'images\'], batch[\'labels\'], batch[\'is_typeB\'], learning_rate, config.keep_prob)\r\n\r\n            # Display\r\n            if step % config.summary_interval == 0:\r\n                duration = time.time() - start_time\r\n                start_time = time.time()\r\n                utils.display_info(epoch, step, duration, wl)\r\n                summary_writer.add_summary(sm, global_step=global_step)\r\n\r\n        # Testing\r\n        print(\'Testing...\')\r\n        embeddings = network.extract_feature(testset.images, testset.is_typeB, config.batch_size)\r\n        tars, fars, _ = utils.test_roc(embeddings, testset.labels, testset.is_typeB, FARs=[1e-5, 1e-4, 1e-3])\r\n        with open(os.path.join(log_dir,\'result.txt\'),\'at\') as f:\r\n            for i in range(len(tars)):\r\n                print(\'[%d] TAR: %2.4f FAR %2.3f\' % (epoch+1, tars[i], fars[i]))\r\n                f.write(\'[%d] TAR: %2.4f FAR %2.3f\\n\' % (epoch+1, tars[i], fars[i]))\r\n                summary = tf.Summary()\r\n                summary.value.add(tag=\'test/tar_%d\'%i, simple_value=tars[i])\r\n                summary_writer.add_summary(summary, global_step)\r\n\r\n        # Save the model\r\n        network.save_model(log_dir, global_step)\r\n\r\nif __name__==""__main__"":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(""config_file"", help=""The path to the training configuration file"",\r\n                        type=str)\r\n    args = parser.parse_args()\r\n    main(args)\r\n'"
src/utils.py,0,"b'""""""Utilities for training and testing\n""""""\n# MIT License\n# \n# Copyright (c) 2018 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport sys\nimport os\nimport numpy as np\nfrom scipy import misc\nimport imp\nimport time\nimport math\nimport random\nfrom datetime import datetime\nimport shutil\nfrom multiprocessing import Process, Queue\n\n\n# The images are separated into two types, A and B, for different branches in \n# the sibling networks. For example, in the ID-selfie problem, type A represents\n# ID images, and type B stands for selfie images.\nis_typeB = lambda x : os.path.basename(x).startswith(\'B\')\n\n\ndef import_file(full_path_to_module, name=\'module.name\'):\n    \n    module_obj = imp.load_source(name, full_path_to_module)\n    \n    return module_obj\n\ndef create_log_dir(config, config_file):\n    subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n    log_dir = os.path.join(os.path.expanduser(config.log_base_dir), config.name, subdir)\n    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\'t exist\n        os.makedirs(log_dir)\n    shutil.copyfile(config_file, os.path.join(log_dir,\'config.py\'))\n\n    return log_dir\n\ndef create_sub_dir(log_dir, name):\n    sub_dir = os.path.join(log_dir, name)\n    if not os.path.isdir(sub_dir):\n        os.makedirs(sub_dir)\n    return sub_dir\n\n\n\n\n\nclass DataClass():\n    def __init__(self, class_name, indices, label):\n        self.class_name = class_name\n        self.indices = list(indices)\n        self.label = label\n        self.indices_B = None\n        self.indices_A = None\n        return\n\n    def random_pair(self):\n        return np.random.permutation(self.indices_A)[:2]\n\n    def random_AB_pair(self):\n        assert self.indices_A is not None and self.indices_B is not None\n        indices_A = np.random.permutation(self.indices_A)[0]\n        indices_B = np.random.permutation(self.indices_B)[0]\n        return [indices_A, indices_B]\n\n\nclass Dataset():\n\n    def __init__(self, path=None):\n        self.num_classes = None\n        self.classes = None\n        self.images = None\n        self.labels = None\n        self.features = None\n        self.index_queue = None\n        self.queue_idx = None\n        self.batch_queue = None\n        self.is_typeB =  None\n\n        if path is not None:\n            self.init_from_path(path)\n\n    def init_from_path(self, path):\n        path = os.path.expanduser(path)\n        _, ext = os.path.splitext(path)\n        if os.path.isdir(path):\n            self.init_from_folder(path)\n        elif ext == \'.txt\':\n            self.init_from_list(path)\n        else:\n            raise ValueError(\'Cannot initialize dataset from path: %s\\n\\\n                It should be either a folder or a .txt list file\' % path)\n        print(\'%d images of %d classes loaded\' % (len(self.images), self.num_classes))\n\n    def init_from_folder(self, folder):\n        folder = os.path.expanduser(folder)\n        class_names = os.listdir(folder)\n        class_names.sort()\n        classes = []\n        images = []\n        labels = []\n        for label, class_name in enumerate(class_names):\n            classdir = os.path.join(folder, class_name)\n            if os.path.isdir(classdir):\n                images_class = os.listdir(classdir)\n                images_class.sort()\n                images_class = [os.path.join(classdir,img) for img in images_class]\n                indices_class = np.arange(len(images), len(images) + len(images_class))\n                classes.append(DataClass(class_name, indices_class, label))\n                images.extend(images_class)\n                labels.extend(len(images_class) * [label])\n        self.classes = np.array(classes, dtype=np.object)\n        self.images = np.array(images, dtype=np.object)\n        self.labels = np.array(labels, dtype=np.int32)\n        self.num_classes = len(classes)\n\n    def init_from_list(self, filename):\n        with open(filename, \'r\') as f:\n            lines = f.readlines()\n        lines = [line.strip().split(\' \') for line in lines]\n        assert len(lines)>0, \\\n            \'List file must be in format: ""fullpath(str) label(int)""\'\n        images = [line[0] for line in lines]\n        if len(lines[0]) > 1:\n            labels = [int(line[1]) for line in lines]\n        else:\n            labels = [os.path.dirname(img) for img in images]\n            _, labels = np.unique(labels, return_inverse=True)\n        self.images = np.array(images, dtype=np.object)\n        self.labels = np.array(labels, dtype=np.int32)\n        self.init_classes()\n\n    def init_classes(self):\n        dict_classes = {}\n        classes = []\n        for i, label in enumerate(self.labels):\n            if not label in dict_classes:\n                dict_classes[label] = [i]\n            else:\n                dict_classes[label].append(i)\n        for label, indices in dict_classes.items():\n            classes.append(DataClass(str(label), indices, label))\n        self.classes = np.array(classes, dtype=np.object)\n        self.num_classes = len(classes)\n\n    def separate_AB(self):\n        assert type(self.images[0]) is str\n        \n        self.is_typeB = np.zeros(len(self.images), dtype=np.bool)\n\n        for c in self.classes:\n            # Find the index of type A file\n            c.indices_A = [i for i in c.indices if not is_typeB(self.images[i])]\n            assert len(c.indices_A) >= 1, str(self.images[c.indices])\n            \n            # Find the index of type B file\n            c.indices_B = [i for i in c.indices if is_typeB(self.images[i])]\n            assert len(c.indices_B) >= 1, str(self.images[c.indices])\n            \n            self.is_typeB[c.indices_B] = True\n\n        print(\'type A images: %d   type B images: %d\'%(np.sum(~self.is_typeB), np.sum(self.is_typeB)))\n\n\n    # Data Loading\n    def init_index_queue(self):\n        if self.index_queue is None:\n            self.index_queue = Queue()\n\n        index_queue = np.random.permutation(len(self.images))[:,None]\n        for idx in list(index_queue):\n            self.index_queue.put(idx)\n\n\n    def get_batch(self, batch_size, batch_format):\n        \'\'\' Get the indices from index queue and fetch the data with indices.\'\'\'\n        indices_batch = []\n\n        if batch_format == \'random_sample\':\n            while len(indices_batch) < batch_size:\n                indices_batch.extend(self.index_queue.get(block=True, timeout=30))\n            assert len(indices_batch) == batch_size\n        elif batch_format == \'random_pair\':\n            assert batch_size%2 == 0\n            classes = np.random.permutation(self.classes)[:batch_size//2]\n            indices_batch = np.concatenate([c.random_pair() for c in classes], axis=0)\n        elif batch_format == \'random_AB_pair\':\n            assert batch_size%2 == 0\n            classes = np.random.permutation(self.classes)[:batch_size//2]\n            indices_batch = np.concatenate([c.random_AB_pair() for c in classes], axis=0)\n        else:\n            raise ValueError(\'get_batch: Unknown batch_format: {}!\'.format(batch_format))\n\n        batch = {}\n        if len(indices_batch) > 0:\n            batch[\'images\'] = self.images[indices_batch]\n            batch[\'labels\'] = self.labels[indices_batch]\n            if self.is_typeB is not None:\n                batch[\'is_typeB\'] = self.is_typeB[indices_batch]\n        return batch\n\n\n    # Multithreading preprocessing images\n    def start_index_queue(self):\n        self.index_queue = Queue()\n        def index_queue_worker():\n            while True:\n                if self.index_queue.empty():\n                    self.init_index_queue()\n                time.sleep(0.01)\n        self.index_worker = Process(target=index_queue_worker)\n        self.index_worker.daemon = True\n        self.index_worker.start()\n\n    def start_batch_queue(self, config, is_training, maxsize=1, num_threads=4):\n\n        if self.index_queue is None:\n            self.start_index_queue()\n\n        self.batch_queue = Queue(maxsize=maxsize)\n        def batch_queue_worker(seed):\n            np.random.seed(seed)\n            while True:\n                batch = self.get_batch(config.batch_size, config.batch_format)\n                if batch is not None:\n                    batch[\'image_paths\'] = batch[\'images\']\n                    batch[\'images\'] = preprocess(batch[\'image_paths\'], config, is_training)\n                self.batch_queue.put(batch)\n\n        self.batch_workers = []\n        for i in range(num_threads):\n            worker = Process(target=batch_queue_worker, args=(i,))\n            worker.daemon = True\n            worker.start()\n            self.batch_workers.append(worker)\n\n\n    def pop_batch_queue(self):\n        batch = self.batch_queue.get(block=True, timeout=60)\n        return batch\n\n    def release_queue(self):\n        if self.index_queue is not None:\n            self.index_queue.close()\n        if self.batch_queue is not None:\n            self.batch_queue.close()\n        if self.index_worker is not None:\n            self.index_worker.terminate()\n            del self.index_worker\n            self.index_worker = None\n        if self.batch_workers is not None:\n            for w in self.batch_workers:\n                w.terminate()\n                del w\n            self.batch_workers = None\n\n\n\n# Calulate the shape for creating new array given (w,h)\ndef get_new_shape(images, size):\n    w, h = tuple(size)\n    shape = list(images.shape)\n    shape[1] = h\n    shape[2] = w\n    shape = tuple(shape)\n    return shape\n\ndef random_crop(images, size):\n    n, _h, _w = images.shape[:3]\n    w, h = tuple(size)\n    shape_new = get_new_shape(images, size)\n    assert (_h>=h and _w>=w)\n\n    images_new = np.ndarray(shape_new, dtype=images.dtype)\n\n    y = np.random.randint(low=0, high=_h-h+1, size=(n))\n    x = np.random.randint(low=0, high=_w-w+1, size=(n))\n\n    for i in range(n):\n        images_new[i] = images[i, y[i]:y[i]+h, x[i]:x[i]+w]\n\n    return images_new\n\ndef center_crop(images, size):\n    n, _h, _w = images.shape[:3]\n    w, h = tuple(size)\n    assert (_h>=h and _w>=w)\n\n    y = int(round(0.5 * (_h - h)))\n    x = int(round(0.5 * (_w - w)))\n\n    images_new = images[:, y:y+h, x:x+w]\n\n    return images_new\n\ndef random_flip(images):\n    images_new = images\n    flips = np.random.rand(images_new.shape[0])>=0.5\n    \n    for i in range(images_new.shape[0]):\n        if flips[i]:\n            images_new[i] = np.fliplr(images[i])\n\n    return images_new\n\ndef resize(images, size):\n    n, _h, _w = images.shape[:3]\n    w, h = tuple(size)\n    shape_new = get_new_shape(images, size)\n\n    images_new = np.ndarray(shape_new, dtype=images.dtype)\n\n    for i in range(n):\n        images_new[i] = misc.imresize(images[i], (h,w))\n\n    return images_new\n\ndef standardize_images(images, standard):\n    if standard==\'mean_scale\':\n        mean = 127.5\n        std = 128.0\n    elif standard==\'scale\':\n        mean = 0.0\n        std = 255.0\n    images_new = images.astype(np.float32)\n    images_new = (images_new - mean) / std\n    return images_new\n\ndef random_downsample(images, min_ratio):\n    n, _h, _w = images.shape[:3]\n    images_new = images\n    ratios = min_ratio + (1-min_ratio) * np.random.rand(images_new.shape[0])\n\n    for i in range(images_new.shape[0]):\n        w = int(round(ratios[i] * _w))\n        h = int(round(ratios[i] * _h))\n        images_new[i,:h,:w] = misc.imresize(images[i], (h,w))\n        images_new[i] = misc.imresize(images_new[i,:h,:w], (_h,_w))\n        \n    return images_new\n\n\ndef preprocess(images, config, is_training=False):\n    # Load images first if they are file paths\n    if type(images[0]) == str:\n        image_paths = images\n        images = []\n        assert (config.channels==1 or config.channels==3)\n        mode = \'RGB\' if config.channels==3 else \'I\'\n        for image_path in image_paths:\n            images.append(misc.imread(image_path, mode=mode))\n        images = np.stack(images, axis=0)\n\n    # Process images\n    f = {\n        \'resize\': resize,\n        \'random_crop\': random_crop,\n        \'center_crop\': center_crop,\n        \'random_flip\': random_flip,\n        \'standardize\': standardize_images,\n        \'random_downsample\': random_downsample,\n    }\n    proc_funcs = config.preprocess_train if is_training else config.preprocess_test\n\n    for proc in proc_funcs:\n        proc_name, proc_args = proc[0], proc[1:]\n        images = f[proc_name](images, *proc_args)\n    if len(images.shape) == 3:\n        images = images[:,:,:,None]\n    return images\n        \n\n\ndef get_updated_learning_rate(global_step, config):\n    if config.learning_rate_strategy == \'step\':\n        max_step = -1\n        learning_rate = 0.0\n        for step, lr in config.learning_rate_schedule.items():\n            if global_step >= step and step > max_step:\n                learning_rate = lr\n                max_step = step\n        if max_step == -1:\n            raise ValueError(\'cannot find learning rate for step %d\' % global_step)\n    elif config.learning_rate_strategy == \'cosine\':\n        initial = config.learning_rate_schedule[\'initial\']\n        interval = config.learning_rate_schedule[\'interval\']\n        end_step = config.learning_rate_schedule[\'end_step\']\n        step = math.floor(float(global_step) / interval) * interval\n        assert step <= end_step\n        learning_rate = initial * 0.5 * (math.cos(math.pi * step / end_step) + 1)\n    return learning_rate\n\ndef display_info(epoch, step, duration, watch_list):\n    sys.stdout.write(\'[%d][%d] time: %2.2f\' % (epoch+1, step+1, duration))\n    for item in watch_list.items():\n        if type(item[1]) in [np.float32, np.float64]:\n            sys.stdout.write(\'   %s: %2.3f\' % (item[0], item[1]))\n        elif type(item[1]) in [np.int32, np.int64, np.bool]:\n            sys.stdout.write(\'   %s: %d\' % (item[0], item[1]))\n    sys.stdout.write(\'\\n\')\n\n\n\ndef get_pairwise_score_label(score_mat, label):\n    n = label.size\n    assert score_mat.shape[0]==score_mat.shape[1]==n\n    triu_indices = np.triu_indices(n, 1)\n    if len(label.shape)==1:\n        label = label[:, None]\n    label_mat = label==label.T\n    score_vec = score_mat[triu_indices]\n    label_vec = label_mat[triu_indices]\n    return score_vec, label_vec\n\ndef test_roc(features, labels, is_typeB, FARs, return_index=False, score_mat=None):\n    n = features.shape[0]\n    assert labels.shape[0] == is_typeB.shape[0] == n\n    if score_mat is None:\n        score_mat = -euclidean(features, features)\n    label_mat = labels.reshape([-1,1]) == labels.reshape([1,-1])\n    is_pair = is_typeB.reshape([-1,1]) != is_typeB.reshape([1,-1])\n    is_pair = np.triu(is_pair, 1)\n    \n    score_vec = score_mat[is_pair]\n    label_vec = label_mat[is_pair]\n    if return_index:\n        rc = np.mgrid[0:n,0:n].transpose([1,2,0])\n        rc_vec = rc[is_pair,:]\n        TARs, FARs, thresholds, fai, fri = ROC(score_vec, label_vec,\n            FARs=FARs, get_false_indices=True)\n        for i in range(len(FARs)):\n            fai[i], fri[i] = rc_vec[fai[i]], rc_vec[fri[i]]\n        return TARs, FARs, thresholds, fai, fri\n    else:\n        TARs, FARs, thresholds = ROC(score_vec, label_vec,\n            FARs=FARs, get_false_indices=False)\n        return TARs, FARs, thresholds\n\ndef zero_one_switch(length):\n    \'\'\' Build a switch vector of the given length.\'\'\'\n    assert length % 2 == 0\n    zeros = np.zeros((length // 2, 1), dtype=np.bool)\n    ones = np.ones((length // 2, 1), dtype=np.bool)\n    switch = np.concatenate([zeros,ones], axis=1).reshape([-1])\n    return switch\n\ndef euclidean(x1,x2):\n    \'\'\' Compute a distance matrix between every row of x1 and x2.\'\'\'\n    assert x1.shape[1]==x2.shape[1]\n    x2 = x2.transpose()\n    x1_norm = np.sum(np.square(x1), axis=1, keepdims=True)\n    x2_norm = np.sum(np.square(x2), axis=0, keepdims=True)\n    dist = x1_norm + x2_norm - 2*np.dot(x1,x2)\n    return dist\n\ndef normalize(x, ord=None, axis=None, epsilon=10e-12):\n    \'\'\' Devide the vectors in x by their norms.\'\'\'\n    if axis is None:\n        axis = len(x.shape) - 1\n    norm = np.linalg.norm(x, ord=None, axis=axis, keepdims=True)\n    x = x / (norm + epsilon)\n    return x\n\n\ndef find_thresholds_by_FAR(score_vec, label_vec, FARs=None, epsilon=1e-8):\n    assert len(score_vec.shape)==1\n    assert score_vec.shape == label_vec.shape\n    assert label_vec.dtype == np.bool\n    score_neg = score_vec[~label_vec]\n    score_neg[::-1].sort()\n    # score_neg = np.sort(score_neg)[::-1] # score from high to low\n    num_neg = len(score_neg)\n\n    assert num_neg >= 1\n\n    if FARs is None:\n        thresholds = np.unique(score_neg)\n        thresholds = np.insert(thresholds, 0, thresholds[0]+epsilon)\n        thresholds = np.insert(thresholds, thresholds.size, thresholds[-1]-epsilon)\n    else:\n        FARs = np.array(FARs)\n        num_false_alarms = np.round(num_neg * FARs).astype(np.int32)\n\n        thresholds = []\n        for num_false_alarm in num_false_alarms:\n            if num_false_alarm==0:\n                threshold = score_neg[0] + epsilon\n            else:\n                threshold = score_neg[num_false_alarm-1]\n            thresholds.append(threshold)\n        thresholds = np.array(thresholds)\n\n    return thresholds\n\ndef ROC(score_vec, label_vec, thresholds=None, FARs=None, get_false_indices=False):\n    \'\'\' Compute Receiver operating characteristic (ROC) with a score and label vector.\'\'\'\n    assert score_vec.ndim == 1\n    assert score_vec.shape == label_vec.shape\n    assert label_vec.dtype == np.bool\n    \n    if thresholds is None:\n        thresholds = find_thresholds_by_FAR(score_vec, label_vec, FARs=FARs)\n\n    assert len(thresholds.shape)==1 \n    if np.size(thresholds) > 10000:\n        print(\'number of thresholds (%d) very large, computation may take a long time!\' % np.size(thresholds))\n\n    # FARs would be check again\n    TARs = np.zeros(thresholds.shape[0])\n    FARs = np.zeros(thresholds.shape[0])\n    false_accept_indices = []\n    false_reject_indices = []\n    for i,threshold in enumerate(thresholds):\n        accept = score_vec >= threshold\n        TARs[i] = np.mean(accept[label_vec])\n        FARs[i] = np.mean(accept[~label_vec])\n        if get_false_indices:\n            false_accept_indices.append(np.argwhere(accept & (~label_vec)).flatten())\n            false_reject_indices.append(np.argwhere((~accept) & label_vec).flatten())\n\n    if get_false_indices:\n        return TARs, FARs, thresholds, false_accept_indices, false_reject_indices\n    else:\n        return TARs, FARs, thresholds\n\ndef accuracy(score_vec, label_vec, thresholds=None):\n    \'\'\' Compute the accuracy given a binary label vector and a score vector.\'\'\'\n    assert len(score_vec.shape)==1\n    assert len(label_vec.shape)==1\n    assert score_vec.shape == label_vec.shape\n    assert label_vec.dtype==np.bool\n    # find thresholds by TAR\n    if thresholds is None:\n        score_pos = score_vec[label_vec==True]\n        thresholds = np.sort(score_pos)[::1]    \n\n    assert len(thresholds.shape)==1\n    if np.size(thresholds) > 10000:\n        print(\'number of thresholds (%d) very large, computation may take a long time!\' % np.size(thresholds))\n    \n    # Loop Computation\n    accuracies = np.zeros(np.size(thresholds))\n    for i, threshold in enumerate(thresholds):\n        pred_vec = score_vec>=threshold\n        accuracies[i] = np.mean(pred_vec==label_vec)\n\n    argmax = np.argmax(accuracies)\n    accuracy = accuracies[argmax]\n    threshold = np.mean(thresholds[accuracies==accuracy])\n\n    return accuracy, threshold\n'"
