file_path,api_count,code
network.py,31,"b'""""""Main implementation class of PFE\n""""""\n# MIT License\n# \n# Copyright (c) 2019 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport os\nimport sys\nimport imp\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nfrom utils.tflib import mutual_likelihood_score_loss\n\nclass Network:\n    def __init__(self):\n        self.graph = tf.Graph()\n        gpu_options = tf.GPUOptions(allow_growth=True)\n        tf_config = tf.ConfigProto(gpu_options=gpu_options,\n                allow_soft_placement=True, log_device_placement=False)\n        self.sess = tf.Session(graph=self.graph, config=tf_config)\n            \n    def initialize(self, config, num_classes=None):\n        \'\'\'\n            Initialize the graph from scratch according to config.\n        \'\'\'\n        with self.graph.as_default():\n            with self.sess.as_default():\n                # Set up placeholders\n                h, w = config.image_size\n                channels = config.channels\n                self.images = tf.placeholder(tf.float32, shape=[None, h, w, channels], name=\'images\')\n                self.labels = tf.placeholder(tf.int32, shape=[None], name=\'labels\')\n\n                self.learning_rate = tf.placeholder(tf.float32, name=\'learning_rate\')\n                self.keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n                self.phase_train = tf.placeholder(tf.bool, name=\'phase_train\')\n                self.global_step = tf.Variable(0, trainable=False, dtype=tf.int32, name=\'global_step\')\n\n                # Initialialize the backbone network\n                network = imp.load_source(\'embedding_network\', config.embedding_network)\n                mu, conv_final = network.inference(self.images, config.embedding_size)\n\n                # Initialize the uncertainty module\n                uncertainty_module = imp.load_source(\'uncertainty_module\', config.uncertainty_module)\n                log_sigma_sq = uncertainty_module.inference(conv_final, config.embedding_size, \n                                        phase_train = self.phase_train, weight_decay = config.weight_decay,\n                                        scope=\'UncertaintyModule\')\n\n                self.mu = tf.identity(mu, name=\'mu\')\n                self.sigma_sq = tf.identity(tf.exp(log_sigma_sq), name=\'sigma_sq\')\n\n                # Build all losses\n                loss_list = []\n                self.watch_list = {}\n\n               \n                MLS_loss = mutual_likelihood_score_loss(self.labels, mu, log_sigma_sq)\n                loss_list.append(MLS_loss)\n                self.watch_list[\'loss\'] = MLS_loss\n\n\n                # Collect all losses\n                reg_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES), name=\'reg_loss\')\n                loss_list.append(reg_loss)\n                self.watch_list[\'reg_loss\'] = reg_loss\n\n\n                total_loss = tf.add_n(loss_list, name=\'total_loss\')\n                grads = tf.gradients(total_loss, self.trainable_variables)\n\n\n                # Training Operaters\n                train_ops = []\n\n                opt = tf.train.MomentumOptimizer(self.learning_rate, momentum=0.9)\n                apply_gradient_op = opt.apply_gradients(list(zip(grads, self.trainable_variables)))\n\n                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n                train_ops.extend([apply_gradient_op] + update_ops)\n\n                train_ops.append(tf.assign_add(self.global_step, 1))\n                self.train_op = tf.group(*train_ops)\n\n                # Collect TF summary\n                for k,v in self.watch_list.items():\n                    tf.summary.scalar(\'losses/\' + k, v)\n                tf.summary.scalar(\'learning_rate\', self.learning_rate)\n                self.summary_op = tf.summary.merge_all()\n\n                # Initialize variables\n                self.sess.run(tf.local_variables_initializer())\n                self.sess.run(tf.global_variables_initializer())\n                self.saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=99)\n \n        return\n\n    @property\n    def trainable_variables(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'UncertaintyModule\')\n\n    def save_model(self, model_dir, global_step):\n        with self.sess.graph.as_default():\n            checkpoint_path = os.path.join(model_dir, \'ckpt\')\n            metagraph_path = os.path.join(model_dir, \'graph.meta\')\n\n            print(\'Saving variables...\')\n            self.saver.save(self.sess, checkpoint_path, global_step=global_step, write_meta_graph=False)\n            if not os.path.exists(metagraph_path):\n                print(\'Saving metagraph...\')\n                self.saver.export_meta_graph(metagraph_path)\n\n    def restore_model(self, model_dir, restore_scopes=None):\n        var_list = self.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n        with self.sess.graph.as_default():\n            if restore_scopes is not None:\n                var_list = [var for var in var_list if any([scope in var.name for scope in restore_scopes])]\n            model_dir = os.path.expanduser(model_dir)\n            ckpt_file = tf.train.latest_checkpoint(model_dir)\n\n            print(\'Restoring {} variables from {} ...\'.format(len(var_list), ckpt_file))\n            saver = tf.train.Saver(var_list)\n            saver.restore(self.sess, ckpt_file)\n\n    def load_model(self, model_path, scope=None):\n        with self.sess.graph.as_default():\n            model_path = os.path.expanduser(model_path)\n\n            # Load grapha and variables separatedly.\n            meta_files = [file for file in os.listdir(model_path) if file.endswith(\'.meta\')]\n            assert len(meta_files) == 1\n            meta_file = os.path.join(model_path, meta_files[0])\n            ckpt_file = tf.train.latest_checkpoint(model_path)\n            \n            print(\'Metagraph file: %s\' % meta_file)\n            print(\'Checkpoint file: %s\' % ckpt_file)\n            saver = tf.train.import_meta_graph(meta_file, clear_devices=True, import_scope=scope)\n            saver.restore(self.sess, ckpt_file)\n\n            # Setup the I/O Tensors\n            self.images = self.graph.get_tensor_by_name(\'images:0\')\n            self.phase_train = self.graph.get_tensor_by_name(\'phase_train:0\')\n            self.keep_prob = self.graph.get_tensor_by_name(\'keep_prob:0\')\n            self.mu = self.graph.get_tensor_by_name(\'mu:0\')\n            self.sigma_sq = self.graph.get_tensor_by_name(\'sigma_sq:0\')\n            self.config = imp.load_source(\'network_config\', os.path.join(model_path, \'config.py\'))\n\n\n\n    def train(self, images_batch, labels_batch, learning_rate, keep_prob):\n        feed_dict = {   self.images: images_batch,\n                        self.labels: labels_batch,\n                        self.learning_rate: learning_rate,\n                        self.keep_prob: keep_prob,\n                        self.phase_train: True,}\n        _, wl, sm = self.sess.run([self.train_op, self.watch_list, self.summary_op], feed_dict = feed_dict)\n\n        step = self.sess.run(self.global_step)\n\n        return wl, sm, step\n\n    def extract_feature(self, images, batch_size, proc_func=None, verbose=False):\n        num_images = len(images)\n        num_features = self.mu.shape[1]\n        mu = np.ndarray((num_images, num_features), dtype=np.float32)\n        sigma_sq = np.ndarray((num_images, num_features), dtype=np.float32)\n        start_time = time.time()\n        for start_idx in range(0, num_images, batch_size):\n            if verbose:\n                elapsed_time = time.strftime(\'%H:%M:%S\', time.gmtime(time.time()-start_time))\n                sys.stdout.write(\'# of images: %d Current image: %d Elapsed time: %s \\t\\r\' \n                    % (num_images, start_idx, elapsed_time))\n            end_idx = min(num_images, start_idx + batch_size)\n            images_batch = images[start_idx:end_idx]\n            if proc_func:\n                images_batch = proc_func(images_batch)\n            feed_dict = {self.images: images_batch,\n                        self.phase_train: False,\n                    self.keep_prob: 1.0}\n            mu[start_idx:end_idx], sigma_sq[start_idx:end_idx] = self.sess.run([self.mu, self.sigma_sq], feed_dict=feed_dict)\n        if verbose:\n            print(\'\')\n        return mu, sigma_sq\n\n\n'"
train.py,1,"b'""""""Main training file for PFE\n""""""\n# MIT License\n# \n# Copyright (c) 2019 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport os\nimport sys\nimport time\nimport imp\nimport argparse\nimport tensorflow as tf\nimport numpy as np\n\nfrom utils import utils\nfrom utils.imageprocessing import preprocess\nfrom utils.dataset import Dataset\nfrom network import Network\n\n\ndef main(args):\n\n    # I/O\n    config_file = args.config_file\n    config = imp.load_source(\'config\', config_file)\n    if args.name:\n        config.name = args.name\n\n    trainset = Dataset(config.train_dataset_path)\n\n    network = Network()\n    network.initialize(config, trainset.num_classes)\n\n    # Initalization for running\n    log_dir = utils.create_log_dir(config, config_file)\n    summary_writer = tf.summary.FileWriter(log_dir, network.graph)\n    if config.restore_model:\n        network.restore_model(config.restore_model, config.restore_scopes)\n\n    proc_func = lambda images: preprocess(images, config, True)\n    trainset.start_batch_queue(config.batch_format, proc_func=proc_func)\n\n\n    # Main Loop\n    print(\'\\nStart Training\\nname: {}\\n# epochs: {}\\nepoch_size: {}\\nbatch_size: {}\\n\'.format(\n            config.name, config.num_epochs, config.epoch_size, config.batch_format[\'size\']))\n    global_step = 0\n    start_time = time.time()\n    for epoch in range(config.num_epochs):\n\n        # Training\n        for step in range(config.epoch_size):\n            # Prepare input\n            learning_rate = utils.get_updated_learning_rate(global_step, config)\n            batch = trainset.pop_batch_queue()\n\n            wl, sm, global_step = network.train(batch[\'image\'], batch[\'label\'], learning_rate, config.keep_prob)\n\n            wl[\'lr\'] = learning_rate\n\n            # Display\n            if step % config.summary_interval == 0:\n                duration = time.time() - start_time\n                start_time = time.time()\n                utils.display_info(epoch, step, duration, wl)\n                summary_writer.add_summary(sm, global_step=global_step)\n\n        # Save the model\n        network.save_model(log_dir, global_step)\n\n\nif __name__==""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""config_file"", help=""The path to the training configuration file"",\n                        type=str)\n    parser.add_argument(""--name"", help=""Rename the log dir"",\n                        type=str, default=None)\n    args = parser.parse_args()\n    main(args)\n'"
align/align_dataset.py,0,"b'""""""Align face images given landmarks.""""""\n\n# MIT License\n# \n# Copyright (c) 2019 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom align.matlab_cp2tform import get_similarity_transform_for_cv2\n\nimport numpy as np\nfrom scipy import misc\nimport sys\nimport os\nimport argparse\nimport random\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef align(src_img, src_pts, ref_pts, image_size, scale=1.0, transpose_input=False):\n    w, h = image_size = tuple(image_size)\n\n    # Actual offset = new center - old center (scaled)\n    scale_ = max(w,h) * scale\n    cx_ref = cy_ref = 0.\n    offset_x = 0.5 * w - cx_ref * scale_\n    offset_y = 0.5 * h - cy_ref * scale_\n\n    s = np.array(src_pts).astype(np.float32).reshape([-1,2])\n    r = np.array(ref_pts).astype(np.float32) * scale_ + np.array([[offset_x, offset_y]])\n    if transpose_input: \n        s = s.reshape([2,-1]).T\n\n    tfm = get_similarity_transform_for_cv2(s, r)\n    dst_img = cv2.warpAffine(src_img, tfm, image_size)\n\n    s_new = np.concatenate([s.reshape([2,-1]), np.ones((1, s.shape[0]))])\n    s_new = np.matmul(tfm, s_new)\n    s_new = s_new.reshape([-1]) if transpose_input else s_new.T.reshape([-1]) \n    tfm = tfm.reshape([-1])\n    return dst_img, s_new, tfm\n\n\ndef main(args):\n    with open(args.input_file, \'r\') as f:\n        lines = f.readlines()\n\n    ref_pts = np.array( [[ -1.58083929e-01, -3.84258929e-02],\n                         [  1.56533929e-01, -4.01660714e-02],\n                         [  2.25000000e-04,  1.40505357e-01],\n                         [ -1.29024107e-01,  3.24691964e-01],\n                         [  1.31516964e-01,  3.23250893e-01]])\n\n    for i,line in enumerate(lines):\n        line = line.strip()\n        items = line.split()\n        img_path = items[0]\n        src_pts = [float(item) for item in items[1:]]\n\n        # Transform\n        if args.prefix:\n            img_path = os.path.join(args.prefix, img_path)\n        img = misc.imread(img_path)\n        img_new, new_pts, tfm = align(img, src_pts, ref_pts, args.image_size, args.scale, args.transpose_input)\n\n        # Visulize\n        if args.visualize:\n            plt.imshow(img_new)\n            plt.show()\n               \n\n        # Output\n        if args.output_dir:\n            file_name = os.path.basename(img_path)\n            sub_dir = [d for d in img_path.split(\'/\') if d!=\'\']\n            sub_dir = \'/\'.join(sub_dir[-args.dir_depth:-1])\n            dir_path = os.path.join(args.output_dir, sub_dir)\n                \n            if not os.path.isdir(dir_path):\n                os.makedirs(dir_path)\n            img_path_new = os.path.join(dir_path, file_name)\n            misc.imsave(img_path_new, img_new)\n            if i % 100==0:\n                print(img_path_new)\n\n\n    return\n\n\n        \n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'input_file\', type=str, help=\'A list file of image paths and landmarks.\')\n    parser.add_argument(\'output_dir\', type=str, help=\'Directory with aligned face thumbnails.\', default=None)\n    parser.add_argument(\'--prefix\', type=str, help=\'The prefix of the image files in the input_file.\', default=None)\n    parser.add_argument(\'--image_size\', type=int, nargs=2,\n        help=\'Image size (height, width) in pixels.\', default=[112, 112])\n    parser.add_argument(\'--scale\', type=float,\n        help=\'Scale the face size in the target image.\', default=1.0)\n    parser.add_argument(\'--dir_depth\', type=int,\n        help=\'When writing into new directory, how many layers of the dir tree should be kept.\', default=2)\n    parser.add_argument(\'--transpose_input\', action=\'store_true\',\n        help=\'Set true if the input landmarks is in the format x1 x2 ... y1 y2 ...\')\n    parser.add_argument(\'--visualize\', action=\'store_true\',\n        help=\'Visualize the aligned images.\')\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
align/crop_ijba.py,0,"b""''' Crop IJB-A images\nGenerate the dataset folder for all images used in verification.\nCrop all the face images using the bounding boxes in the protocol file.\nThe structure of the output folder: save_prefix/subject_id/image_name\n'''\n\nimport os\nimport sys\nimport argparse\nimport numpy as np\nfrom scipy import misc\nimport cv2 # Some images can not be read by misc, use opencv instead\n\nsquare_crop = True          # Take the max of (w,h) for a square bounding box\npadding_ratio = 0.0         # Add padding to bounding boxes by a ratio\ntarget_size = (256, 256)    # If not None, resize image after processing\n\ndef square_bbox(bbox):\n    '''Output a square-like bounding box. But because all the numbers are float, \n    it is not guaranteed to really be a square.'''\n    x, y, w, h = tuple(bbox)\n    cx = x + 0.5 * w\n    cy = y + 0.5 * h\n    _w = _h = max(w, h)\n    _x = cx - 0.5*_w\n    _y = cy - 0.5*_h\n    return (_x, _y, _w, _h)\n    \ndef pad_bbox(bbox, padding_ratio):\n    x, y, w, h = tuple(bbox)\n    pad_x = padding_ratio * w\n    pad_y = padding_ratio * h\n    return (x-pad_x, y-pad_y, w+2*pad_x, h+2*pad_y)\n\ndef crop(image, bbox):\n    rint = lambda a: int(round(a))\n    x, y, w, h = tuple(map(rint, bbox))\n    safe_pad = max(0, -x ,-y, x+w-image.shape[1], y+h-image.shape[0])\n    img = np.zeros((image.shape[0]+2*safe_pad, image.shape[1]+2*safe_pad, image.shape[2]))\n    img[safe_pad:safe_pad+image.shape[0], safe_pad:safe_pad+image.shape[1], :] = image\n    img = img[safe_pad+y : safe_pad+y+h, safe_pad+x : safe_pad+x+w, :]\n    return img\n\n\ndef main(args):\n    with open(args.meta_file, 'r') as fr:\n        lines = fr.readlines()\n\n    # Some files have different extensions in the meta file,\n    # record their oroginal name for reading\n    files_img = os.listdir(args.prefix+'/img/')\n    files_frames = os.listdir(args.prefix+'/frame/')\n    dict_path= {}\n    for img in files_img:\n        basename = os.path.splitext(img)[0]\n        dict_path['img/' + basename] = args.prefix + '/img/' + img\n    for img in files_frames:\n        basename = os.path.splitext(img)[0]\n        dict_path['frame/' + basename] = args.prefix + '/frame/' + img\n    \n    count_success = 0\n    count_fail = 0\n    dict_name = {}\n    for i,line in enumerate(lines):\n        if i > 0:\n            parts = line.split(',')\n            label = parts[0]\n            impath = os.path.join(args.prefix,parts[2])\n            imname = os.path.join(label, parts[2].replace('/','_'))\n            \n            # Check name duplication\n            if imname in dict_name:\n                print('image %s at line %d collision with  line %d' % (imname, i, dict_name[imname]))\n            dict_name[imname] = i\n            \n            # Check extention difference\n            if not os.path.isfile(impath):\n                basename = os.path.splitext(parts[2])[0]\n                if basename in dict_path:\n                    impath = dict_path[basename]\n                else:\n                    print('%s not found in the input directory, skipped' % (impath))\n                    continue\n\n            img = cv2.imread(impath, flags=1)\n\n            if img.ndim == 0:\n                print('Invalid image: %s' % impath)\n                count_fail += 1\n            else:\n                bbox = tuple(map(float,parts[6:10]))\n                if square_crop:\n                    bbox = square_bbox(bbox)\n                bbox = pad_bbox(bbox, padding_ratio)\n                img = crop(img, bbox)\n\n                impath_new = os.path.join(args.save_prefix, imname)\n                if os.path.isdir(os.path.dirname(impath_new)) == False:\n                    os.makedirs(os.path.dirname(impath_new))\n                if target_size:\n                    img = cv2.resize(img, target_size)\n                cv2.imwrite(impath_new, img)\n                count_success += 1\n        if i % 100 == 0:\n            print('cropping %dth image' % (i+1))\n\n    print('%d images cropped, %d images failed' % (count_success, count_fail))\n    print('%d image names created' % len(dict_name))\n\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('meta_file', type=str, help='Path to metadata file.')\n    parser.add_argument('prefix', type=str, help='Path to the folder containing the original images of IJB-A.')\n    parser.add_argument('save_prefix', type=str, help='Directory for output images.')\n    return parser.parse_args(argv)\n\nif __name__ == '__main__':\n    main(parse_arguments(sys.argv[1:]))\n"""
align/matlab_cp2tform.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Tue Jul 11 06:54:28 2017\n\n@author: zhaoyafei\n""""""\n\nimport numpy as np\nfrom numpy.linalg import inv, norm, lstsq\nfrom numpy.linalg import matrix_rank as rank\n\n\n""""""\nIntroduction:\n----------\nnumpy implemetation form matlab function CP2TFORM(...)\nwith \'transformtype\':\n    1) \'nonreflective similarity\'\n    2) \'similarity\'\n\n\nMATLAB code:\n----------\n%--------------------------------------\n% Function  findNonreflectiveSimilarity\n%\nfunction [trans, output] = findNonreflectiveSimilarity(uv,xy,options)\n%\n% For a nonreflective similarity:\n%\n% let sc = s*cos(theta)\n% let ss = s*sin(theta)\n%\n%                   [ sc -ss\n% [u v] = [x y 1] *   ss  sc\n%                     tx  ty]\n%\n% There are 4 unknowns: sc,ss,tx,ty.\n%\n% Another way to write this is:\n%\n% u = [x y 1 0] * [sc\n%                  ss\n%                  tx\n%                  ty]\n%\n% v = [y -x 0 1] * [sc\n%                   ss\n%                   tx\n%                   ty]\n%\n% With 2 or more correspondence points we can combine the u equations and\n% the v equations for one linear system to solve for sc,ss,tx,ty.\n%\n% [ u1  ] = [ x1  y1  1  0 ] * [sc]\n% [ u2  ]   [ x2  y2  1  0 ]   [ss]\n% [ ... ]   [ ...          ]   [tx]\n% [ un  ]   [ xn  yn  1  0 ]   [ty]\n% [ v1  ]   [ y1 -x1  0  1 ]\n% [ v2  ]   [ y2 -x2  0  1 ]\n% [ ... ]   [ ...          ]\n% [ vn  ]   [ yn -xn  0  1 ]\n%\n% Or rewriting the above matrix equation:\n% U = X * r, where r = [sc ss tx ty]\'\n% so r = X\\ U.\n%\n\nK = options.K;\nM = size(xy,1);\nx = xy(:,1);\ny = xy(:,2);\nX = [x   y  ones(M,1)   zeros(M,1);\n     y  -x  zeros(M,1)  ones(M,1)  ];\n\nu = uv(:,1);\nv = uv(:,2);\nU = [u; v];\n\n% We know that X * r = U\nif rank(X) >= 2*K\n    r = X \\ U;\nelse\n    error(message(\'images:cp2tform:twoUniquePointsReq\'))\nend\n\nsc = r(1);\nss = r(2);\ntx = r(3);\nty = r(4);\n\nTinv = [sc -ss 0;\n        ss  sc 0;\n        tx  ty 1];\n\nT = inv(Tinv);\nT(:,3) = [0 0 1]\';\n\ntrans = maketform(\'affine\', T);\noutput = [];\n\n%-------------------------\n% Function  findSimilarity\n%\nfunction [trans, output] = findSimilarity(uv,xy,options)\n%\n% The similarities are a superset of the nonreflective similarities as they may\n% also include reflection.\n%\n% let sc = s*cos(theta)\n% let ss = s*sin(theta)\n%\n%                   [ sc -ss\n% [u v] = [x y 1] *   ss  sc\n%                     tx  ty]\n%\n%          OR\n%\n%                   [ sc  ss\n% [u v] = [x y 1] *   ss -sc\n%                     tx  ty]\n%\n% Algorithm:\n% 1) Solve for trans1, a nonreflective similarity.\n% 2) Reflect the xy data across the Y-axis,\n%    and solve for trans2r, also a nonreflective similarity.\n% 3) Transform trans2r to trans2, undoing the reflection done in step 2.\n% 4) Use TFORMFWD to transform uv using both trans1 and trans2,\n%    and compare the results, Returnsing the transformation corresponding\n%    to the smaller L2 norm.\n\n% Need to reset options.K to prepare for calls to findNonreflectiveSimilarity.\n% This is safe because we already checked that there are enough point pairs.\noptions.K = 2;\n\n% Solve for trans1\n[trans1, output] = findNonreflectiveSimilarity(uv,xy,options);\n\n\n% Solve for trans2\n\n% manually reflect the xy data across the Y-axis\nxyR = xy;\nxyR(:,1) = -1*xyR(:,1);\n\ntrans2r  = findNonreflectiveSimilarity(uv,xyR,options);\n\n% manually reflect the tform to undo the reflection done on xyR\nTreflectY = [-1  0  0;\n              0  1  0;\n              0  0  1];\ntrans2 = maketform(\'affine\', trans2r.tdata.T * TreflectY);\n\n\n% Figure out if trans1 or trans2 is better\nxy1 = tformfwd(trans1,uv);\nnorm1 = norm(xy1-xy);\n\nxy2 = tformfwd(trans2,uv);\nnorm2 = norm(xy2-xy);\n\nif norm1 <= norm2\n    trans = trans1;\nelse\n    trans = trans2;\nend\n""""""\n\nclass MatlabCp2tormException(Exception):\n    def __str__(self):\n        return \'In File {}:{}\'.format(\n                __file__, super.__str__(self))\n\ndef tformfwd(trans, uv):\n    """"""\n    Function:\n    ----------\n        apply affine transform \'trans\' to uv\n\n    Parameters:\n    ----------\n        @trans: 3x3 np.array\n            transform matrix\n        @uv: Kx2 np.array\n            each row is a pair of coordinates (x, y)\n\n    Returns:\n    ----------\n        @xy: Kx2 np.array\n            each row is a pair of transformed coordinates (x, y)\n    """"""\n    uv = np.hstack((\n        uv, np.ones((uv.shape[0], 1))\n    ))\n    xy = np.dot(uv, trans)\n    xy = xy[:, 0:-1]\n    return xy\n\n\ndef tforminv(trans, uv):\n    """"""\n    Function:\n    ----------\n        apply the inverse of affine transform \'trans\' to uv\n\n    Parameters:\n    ----------\n        @trans: 3x3 np.array\n            transform matrix\n        @uv: Kx2 np.array\n            each row is a pair of coordinates (x, y)\n\n    Returns:\n    ----------\n        @xy: Kx2 np.array\n            each row is a pair of inverse-transformed coordinates (x, y)\n    """"""\n    Tinv = inv(trans)\n    xy = tformfwd(Tinv, uv)\n    return xy\n\n\ndef findNonreflectiveSimilarity(uv, xy, options=None):\n    """"""\n    Function:\n    ----------\n        Find Non-reflective Similarity Transform Matrix \'trans\':\n            u = uv[:, 0]\n            v = uv[:, 1]\n            x = xy[:, 0]\n            y = xy[:, 1]\n            [x, y, 1] = [u, v, 1] * trans\n\n    Parameters:\n    ----------\n        @uv: Kx2 np.array\n            source points each row is a pair of coordinates (x, y)\n        @xy: Kx2 np.array\n            each row is a pair of inverse-transformed\n        @option: not used, keep it as None\n\n    Returns:\n        @trans: 3x3 np.array\n            transform matrix from uv to xy\n        @trans_inv: 3x3 np.array\n            inverse of trans, transform matrix from xy to uv\n\n    Matlab:\n    ----------\n    % For a nonreflective similarity:\n    %\n    % let sc = s*cos(theta)\n    % let ss = s*sin(theta)\n    %\n    %                   [ sc -ss\n    % [u v] = [x y 1] *   ss  sc\n    %                     tx  ty]\n    %\n    % There are 4 unknowns: sc,ss,tx,ty.\n    %\n    % Another way to write this is:\n    %\n    % u = [x y 1 0] * [sc\n    %                  ss\n    %                  tx\n    %                  ty]\n    %\n    % v = [y -x 0 1] * [sc\n    %                   ss\n    %                   tx\n    %                   ty]\n    %\n    % With 2 or more correspondence points we can combine the u equations and\n    % the v equations for one linear system to solve for sc,ss,tx,ty.\n    %\n    % [ u1  ] = [ x1  y1  1  0 ] * [sc]\n    % [ u2  ]   [ x2  y2  1  0 ]   [ss]\n    % [ ... ]   [ ...          ]   [tx]\n    % [ un  ]   [ xn  yn  1  0 ]   [ty]\n    % [ v1  ]   [ y1 -x1  0  1 ]\n    % [ v2  ]   [ y2 -x2  0  1 ]\n    % [ ... ]   [ ...          ]\n    % [ vn  ]   [ yn -xn  0  1 ]\n    %\n    % Or rewriting the above matrix equation:\n    % U = X * r, where r = [sc ss tx ty]\'\n    % so r = X\\ U.\n    %\n    """"""\n    options = {\'K\': 2}\n\n    K = options[\'K\']\n    M = xy.shape[0]\n    x = xy[:, 0].reshape((-1, 1))  # use reshape to keep a column vector\n    y = xy[:, 1].reshape((-1, 1))  # use reshape to keep a column vector\n    # print \'--->x, y:\\n\', x, y\n\n    tmp1 = np.hstack((x, y, np.ones((M, 1)), np.zeros((M, 1))))\n    tmp2 = np.hstack((y, -x, np.zeros((M, 1)), np.ones((M, 1))))\n    X = np.vstack((tmp1, tmp2))\n    # print \'--->X.shape: \', X.shape\n    # print \'X:\\n\', X\n\n    u = uv[:, 0].reshape((-1, 1))  # use reshape to keep a column vector\n    v = uv[:, 1].reshape((-1, 1))  # use reshape to keep a column vector\n    U = np.vstack((u, v))\n    # print \'--->U.shape: \', U.shape\n    # print \'U:\\n\', U\n\n    # We know that X * r = U\n    if rank(X) >= 2 * K:\n        r, _, _, _ = lstsq(X, U)\n        r = np.squeeze(r)\n    else:\n        raise Exception(\'cp2tform:twoUniquePointsReq\')\n\n    # print \'--->r:\\n\', r\n\n    sc = r[0]\n    ss = r[1]\n    tx = r[2]\n    ty = r[3]\n\n    Tinv = np.array([\n        [sc, -ss, 0],\n        [ss,  sc, 0],\n        [tx,  ty, 1]\n    ])\n\n    # print \'--->Tinv:\\n\', Tinv\n\n    T = inv(Tinv)\n    # print \'--->T:\\n\', T\n\n    T[:, 2] = np.array([0, 0, 1])\n\n    return T, Tinv\n\n\ndef findSimilarity(uv, xy, options=None):\n    """"""\n    Function:\n    ----------\n        Find Reflective Similarity Transform Matrix \'trans\':\n            u = uv[:, 0]\n            v = uv[:, 1]\n            x = xy[:, 0]\n            y = xy[:, 1]\n            [x, y, 1] = [u, v, 1] * trans\n\n    Parameters:\n    ----------\n        @uv: Kx2 np.array\n            source points each row is a pair of coordinates (x, y)\n        @xy: Kx2 np.array\n            each row is a pair of inverse-transformed\n        @option: not used, keep it as None\n\n    Returns:\n    ----------\n        @trans: 3x3 np.array\n            transform matrix from uv to xy\n        @trans_inv: 3x3 np.array\n            inverse of trans, transform matrix from xy to uv\n\n    Matlab:\n    ----------\n    % The similarities are a superset of the nonreflective similarities as they may\n    % also include reflection.\n    %\n    % let sc = s*cos(theta)\n    % let ss = s*sin(theta)\n    %\n    %                   [ sc -ss\n    % [u v] = [x y 1] *   ss  sc\n    %                     tx  ty]\n    %\n    %          OR\n    %\n    %                   [ sc  ss\n    % [u v] = [x y 1] *   ss -sc\n    %                     tx  ty]\n    %\n    % Algorithm:\n    % 1) Solve for trans1, a nonreflective similarity.\n    % 2) Reflect the xy data across the Y-axis,\n    %    and solve for trans2r, also a nonreflective similarity.\n    % 3) Transform trans2r to trans2, undoing the reflection done in step 2.\n    % 4) Use TFORMFWD to transform uv using both trans1 and trans2,\n    %    and compare the results, Returnsing the transformation corresponding\n    %    to the smaller L2 norm.\n\n    % Need to reset options.K to prepare for calls to findNonreflectiveSimilarity.\n    % This is safe because we already checked that there are enough point pairs.\n    """"""\n    options = {\'K\': 2}\n\n#    uv = np.array(uv)\n#    xy = np.array(xy)\n\n    # Solve for trans1\n    trans1, trans1_inv = findNonreflectiveSimilarity(uv, xy, options)\n\n    # Solve for trans2\n\n    # manually reflect the xy data across the Y-axis\n    xyR = xy\n    xyR[:, 0] = -1 * xyR[:, 0]\n\n    trans2r, trans2r_inv = findNonreflectiveSimilarity(uv, xyR, options)\n\n    # manually reflect the tform to undo the reflection done on xyR\n    TreflectY = np.array([\n        [-1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ])\n\n    trans2 = np.dot(trans2r, TreflectY)\n\n    # Figure out if trans1 or trans2 is better\n    xy1 = tformfwd(trans1, uv)\n    norm1 = norm(xy1 - xy)\n\n    xy2 = tformfwd(trans2, uv)\n    norm2 = norm(xy2 - xy)\n\n    if norm1 <= norm2:\n        return trans1, trans1_inv\n    else:\n        trans2_inv = inv(trans2)\n        return trans2, trans2_inv\n\n\ndef get_similarity_transform(src_pts, dst_pts, reflective=True):\n    """"""\n    Function:\n    ----------\n        Find Similarity Transform Matrix \'trans\':\n            u = src_pts[:, 0]\n            v = src_pts[:, 1]\n            x = dst_pts[:, 0]\n            y = dst_pts[:, 1]\n            [x, y, 1] = [u, v, 1] * trans\n\n    Parameters:\n    ----------\n        @src_pts: Kx2 np.array\n            source points, each row is a pair of coordinates (x, y)\n        @dst_pts: Kx2 np.array\n            destination points, each row is a pair of transformed\n            coordinates (x, y)\n        @reflective: True or False\n            if True:\n                use reflective similarity transform\n            else:\n                use non-reflective similarity transform\n\n    Returns:\n    ----------\n       @trans: 3x3 np.array\n            transform matrix from uv to xy\n        trans_inv: 3x3 np.array\n            inverse of trans, transform matrix from xy to uv\n    """"""\n\n    if reflective:\n        trans, trans_inv = findSimilarity(src_pts, dst_pts)\n    else:\n        trans, trans_inv = findNonreflectiveSimilarity(src_pts, dst_pts)\n\n    return trans, trans_inv\n\n\ndef cvt_tform_mat_for_cv2(trans):\n    """"""\n    Function:\n    ----------\n        Convert Transform Matrix \'trans\' into \'cv2_trans\' which could be\n        directly used by cv2.warpAffine():\n            u = src_pts[:, 0]\n            v = src_pts[:, 1]\n            x = dst_pts[:, 0]\n            y = dst_pts[:, 1]\n            [x, y].T = cv_trans * [u, v, 1].T\n\n    Parameters:\n    ----------\n        @trans: 3x3 np.array\n            transform matrix from uv to xy\n\n    Returns:\n    ----------\n        @cv2_trans: 2x3 np.array\n            transform matrix from src_pts to dst_pts, could be directly used\n            for cv2.warpAffine()\n    """"""\n    cv2_trans = trans[:, 0:2].T\n\n    return cv2_trans\n\n\ndef get_similarity_transform_for_cv2(src_pts, dst_pts, reflective=True):\n    """"""\n    Function:\n    ----------\n        Find Similarity Transform Matrix \'cv2_trans\' which could be\n        directly used by cv2.warpAffine():\n            u = src_pts[:, 0]\n            v = src_pts[:, 1]\n            x = dst_pts[:, 0]\n            y = dst_pts[:, 1]\n            [x, y].T = cv_trans * [u, v, 1].T\n\n    Parameters:\n    ----------\n        @src_pts: Kx2 np.array\n            source points, each row is a pair of coordinates (x, y)\n        @dst_pts: Kx2 np.array\n            destination points, each row is a pair of transformed\n            coordinates (x, y)\n        reflective: True or False\n            if True:\n                use reflective similarity transform\n            else:\n                use non-reflective similarity transform\n\n    Returns:\n    ----------\n        @cv2_trans: 2x3 np.array\n            transform matrix from src_pts to dst_pts, could be directly used\n            for cv2.warpAffine()\n    """"""\n    trans, trans_inv = get_similarity_transform(src_pts, dst_pts, reflective)\n    cv2_trans = cvt_tform_mat_for_cv2(trans)\n\n    return cv2_trans\n\n\nif __name__ == \'__main__\':\n    """"""\n    u = [0, 6, -2]\n    v = [0, 3, 5]\n    x = [-1, 0, 4]\n    y = [-1, -10, 4]\n\n    # In Matlab, run:\n    #\n    #   uv = [u\'; v\'];\n    #   xy = [x\'; y\'];\n    #   tform_sim=cp2tform(uv,xy,\'similarity\');\n    #\n    #   trans = tform_sim.tdata.T\n    #   ans =\n    #       -0.0764   -1.6190         0\n    #        1.6190   -0.0764         0\n    #       -3.2156    0.0290    1.0000\n    #   trans_inv = tform_sim.tdata.Tinv\n    #    ans =\n    #\n    #       -0.0291    0.6163         0\n    #       -0.6163   -0.0291         0\n    #       -0.0756    1.9826    1.0000\n    #    xy_m=tformfwd(tform_sim, u,v)\n    #\n    #    xy_m =\n    #\n    #       -3.2156    0.0290\n    #        1.1833   -9.9143\n    #        5.0323    2.8853\n    #    uv_m=tforminv(tform_sim, x,y)\n    #\n    #    uv_m =\n    #\n    #        0.5698    1.3953\n    #        6.0872    2.2733\n    #       -2.6570    4.3314\n    """"""\n    u = [0, 6, -2]\n    v = [0, 3, 5]\n    x = [-1, 0, 4]\n    y = [-1, -10, 4]\n\n    uv = np.array((u, v)).T\n    xy = np.array((x, y)).T\n\n    print(\'\\n--->uv:\')\n    print(uv)\n    print(\'\\n--->xy:\')\n    print(xy)\n\n    trans, trans_inv = get_similarity_transform(uv, xy)\n\n    print(\'\\n--->trans matrix:\')\n    print(trans)\n\n    print(\'\\n--->trans_inv matrix:\')\n    print(trans_inv)\n\n    print(\'\\n---> apply transform to uv\')\n    print(\'\\nxy_m = uv_augmented * trans\')\n    uv_aug = np.hstack((\n        uv, np.ones((uv.shape[0], 1))\n    ))\n    xy_m = np.dot(uv_aug, trans)\n    print(xy_m)\n\n    print(\'\\nxy_m = tformfwd(trans, uv)\')\n    xy_m = tformfwd(trans, uv)\n    print(xy_m)\n\n    print(\'\\n---> apply inverse transform to xy\')\n    print(\'\\nuv_m = xy_augmented * trans_inv\')\n    xy_aug = np.hstack((\n        xy, np.ones((xy.shape[0], 1))\n    ))\n    uv_m = np.dot(xy_aug, trans_inv)\n    print(uv_m)\n\n    print(\'\\nuv_m = tformfwd(trans_inv, xy)\')\n    uv_m = tformfwd(trans_inv, xy)\n    print(uv_m)\n\n    uv_m = tforminv(trans, xy)\n    print(\'\\nuv_m = tforminv(trans, xy)\')\n    print(uv_m)\n'"
clib/__init__.py,0,b'from .mls import *\n'
clib/setup.py,0,"b'from distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Build import cythonize\n\next_modules = [\n    Extension(\n        ""mls"",\n        [""mls.pyx""],\n        extra_compile_args=[\'-fopenmp\'],\n        extra_link_args=[\'-fopenmp\'],\n    )\n]\n\nsetup(\n    name=\'mls\',\n    ext_modules = cythonize(ext_modules)\n)\n\n'"
config/sphere64_casia.py,0,"b'\'\'\' Config Proto \'\'\'\n\nimport sys\nimport os\n\n\n####### INPUT OUTPUT #######\n\n# The name of the current model for output\nname = \'sphere64_casia_am_PFE\'\n\n# The folder to save log and model\nlog_base_dir = \'./log/\'\n\n# The interval between writing summary\nsummary_interval = 100\n\n# Training dataset path\ntrain_dataset_path = ""./data/list_casia_mtcnncaffe_aligned_nooverlap.txt""\n\n# Target image size for the input of network\nimage_size = [112, 96]\n\n# 3 channels means RGB, 1 channel for grayscale\nchannels = 3\n\n# Preprocess for training\npreprocess_train = [\n    [\'center_crop\', (112, 96)],\n    [\'random_flip\'],\n    [\'standardize\', \'mean_scale\'],\n]\n\n# Preprocess for testing\npreprocess_test = [\n    [\'center_crop\', (112, 96)],\n    [\'standardize\', \'mean_scale\'],\n]\n\n# Number of GPUs\nnum_gpus = 1\n\n####### NETWORK #######\n\n# The network architecture\nembedding_network = ""models/sphere_net_PFE.py""\n\n# The network architecture\nuncertainty_module = ""models/uncertainty_module.py""\n\n# Number of dimensions in the embedding space\nembedding_size = 512\n\n\n####### TRAINING STRATEGY #######\n\n# Base Random Seed\nbase_random_seed = 0\n\n# Number of samples per batch\nbatch_format = {\n    \'size\': 256,\n    \'num_classes\': 64,\n}\n\n# Number of batches per epoch\nepoch_size = 1000\n\n# Number of epochs\nnum_epochs = 3\n\n# learning rate strategy\nlearning_rate_strategy = \'step\'\n\n# learning rate schedule\nlr = 1e-3\nlearning_rate_schedule = {\n    0:      1 * lr,\n    2000:   0.1 * lr,\n}\n\n# Restore model\nrestore_model = \'./pretrained/sphere64_casia_am\'\n\n# Keywords to filter restore variables, set None for all\nrestore_scopes = [\'SphereNet/conv\', \'SphereNet/Bot\']\n\n# Weight decay for model variables\nweight_decay = 5e-4\n\n# Keep probability for dropouts\nkeep_prob = 1.0\n\n\n'"
config/sphere64_msarcface.py,0,"b'\'\'\' Config Proto \'\'\'\n\nimport sys\nimport os\n\n\n####### INPUT OUTPUT #######\n\n# The name of the current model for output\nname = \'sphere64_msarcface_am_PFE\'\n\n# The folder to save log and model\nlog_base_dir = \'./log/\'\n\n# The interval between writing summary\nsummary_interval = 100\n\n# Training dataset path\ntrain_dataset_path = ""./data/ms_arcface""\n\n# Target image size for the input of network\nimage_size = [112, 96]\n\n# 3 channels means RGB, 1 channel for grayscale\nchannels = 3\n\n# Preprocess for training\npreprocess_train = [\n    [\'center_crop\', (112, 96)],\n    [\'random_flip\'],\n    [\'standardize\', \'mean_scale\'],\n]\n\n# Preprocess for testing\npreprocess_test = [\n    [\'center_crop\', (112, 96)],\n    [\'standardize\', \'mean_scale\'],\n]\n\n# Number of GPUs\nnum_gpus = 1\n\n####### NETWORK #######\n\n# The network architecture\nembedding_network = ""models/sphere_net_PFE.py""\n\n# The network architecture\nuncertainty_module = ""models/uncertainty_module.py""\n\n# Number of dimensions in the embedding space\nembedding_size = 512\n\n\n####### TRAINING STRATEGY #######\n\n# Base Random Seed\nbase_random_seed = 0\n\n# Number of samples per batch\nbatch_format = {\n    \'size\': 256,\n    \'num_classes\': 64,\n}\n\n# Number of batches per epoch\nepoch_size = 1000\n\n# Number of epochs\nnum_epochs = 12\n\n# learning rate strategy\nlearning_rate_strategy = \'step\'\n\n# learning rate schedule\nlr = 1e-3\nlearning_rate_schedule = {\n    0:      1 * lr,\n    8000:   0.1 * lr,\n}\n\n# Restore model\nrestore_model = \'./pretrained/sphere64_msarcface_am\'\n\n# Keywords to filter restore variables, set None for all\nrestore_scopes = [\'SphereNet/conv\', \'SphereNet/Bot\']\n\n# Weight decay for model variables\nweight_decay = 5e-4\n\n# Keep probability for dropouts\nkeep_prob = 1.0\n\n\n'"
evaluation/__init__.py,0,b''
evaluation/eval_ijb.py,0,"b'""""""Test PFE on IJB-A.\n""""""\n# MIT License\n# \n# Copyright (c) 2019 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport os\nimport sys\nimport time\nimport math\nimport argparse\nimport numpy as np\n\nfrom utils import utils\nfrom utils.dataset import Dataset\nfrom utils.imageprocessing import preprocess\nfrom network import Network\n\n\nfrom evaluation.ijba import IJBATest\nfrom evaluation.ijbc import IJBCTest\n\n\ndef aggregate_templates(templates, features, method):\n    for i,t in enumerate(templates):\n        if len(t.indices) > 0:\n            if method == \'mean\':\n                t.feature = utils.l2_normalize(np.mean(features[t.indices], axis=0))\n            if method == \'PFE_fuse\':\n                t.mu, t.sigma_sq = utils.aggregate_PFE(features[t.indices], normalize=True, concatenate=False)\n                t.feature = t.mu\n            if method == \'PFE_fuse_match\':\n                if not hasattr(t, \'mu\'):\n                    t.mu, t.sigma_sq = utils.aggregate_PFE(features[t.indices], normalize=True, concatenate=False)\n                t.feature = np.concatenate([t.mu, t.sigma_sq])\n        else:\n            t.feature = None\n        if i % 1000 == 0:\n            sys.stdout.write(\'Fusing templates {}/{}...\\t\\r\'.format(i, len(templates)))\n    print(\'\')\n\n\ndef force_compare(compare_func, verbose=False):\n    def compare(t1, t2):\n        score_vec = np.zeros(len(t1))\n        for i in range(len(t1)):\n            if t1[i] is None or t2[i] is None:\n                score_vec[i] = -9999\n            else:\n                score_vec[i] = compare_func(t1[i][None], t2[i][None])\n            if verbose and i % 1000 == 0:\n                sys.stdout.write(\'Matching pair {}/{}...\\t\\r\'.format(i, len(t1)))\n        if verbose:\n            print(\'\')\n        return score_vec\n    return compare\n\n\ndef main(args):\n\n    network = Network()\n    network.load_model(args.model_dir)\n    proc_func = lambda x: preprocess(x, network.config, False)\n\n    testset = Dataset(args.dataset_path)\n    if args.protocol == \'ijba\':\n        tester = IJBATest(testset[\'abspath\'].values)\n        tester.init_proto(args.protocol_path)\n    elif args.protocol == \'ijbc\':\n        tester = IJBCTest(testset[\'abspath\'].values)\n        tester.init_proto(args.protocol_path)\n    else:\n        raise ValueError(\'Unkown protocol. Only accept ""ijba"" or ""ijbc"".\')\n\n\n    mu, sigma_sq = network.extract_feature(tester.image_paths, args.batch_size, proc_func=proc_func, verbose=True)\n    features = np.concatenate([mu, sigma_sq], axis=1)\n\n    print(\'---- Average pooling\')\n    aggregate_templates(tester.verification_templates, features, \'mean\')\n    TARs, std, FARs = tester.test_verification(force_compare(utils.pair_euc_score))\n    for i in range(len(TARs)):\n        print(\'TAR: {:.5} +- {:.5} FAR: {:.5}\'.format(TARs[i], std[i], FARs[i]))\n\n    print(\'---- Uncertainty pooling\')\n    aggregate_templates(tester.verification_templates, features, \'PFE_fuse\')\n    TARs, std, FARs = tester.test_verification(force_compare(utils.pair_euc_score))\n    for i in range(len(TARs)):\n        print(\'TAR: {:.5} +- {:.5} FAR: {:.5}\'.format(TARs[i], std[i], FARs[i]))\n\n\n    print(\'---- MLS comparison\')\n    aggregate_templates(tester.verification_templates, features, \'PFE_fuse_match\')\n    TARs, std, FARs = tester.test_verification(force_compare(utils.pair_MLS_score))\n    for i in range(len(TARs)):\n        print(\'TAR: {:.5} +- {:.5} FAR: {:.5}\'.format(TARs[i], std[i], FARs[i]))\n\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--model_dir"", help=""The path to the pre-trained model directory"",\n                        type=str, default=None)\n    parser.add_argument(""--protocol"", help=""The dataset to test"",\n                        type=str, default=\'ijba\')\n    parser.add_argument(""--dataset_path"", help=""The path to the IJB-A dataset directory"",\n                        type=str, default=\'data/ijba_mtcnncaffe_aligned\')\n    parser.add_argument(""--protocol_path"", help=""The path to the IJB-A protocol directory"",\n                        type=str, default=\'proto/IJB-A\')\n    parser.add_argument(""--batch_size"", help=""Number of images per mini batch"",\n                        type=int, default=256)\n    args = parser.parse_args()\n    main(args)\n'"
evaluation/eval_lfw.py,0,"b'""""""Test PFE on LFW.\n""""""\n# MIT License\n# \n# Copyright (c) 2019 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport os\nimport sys\nimport imp\nimport argparse\nimport time\nimport math\nimport numpy as np\n\nfrom utils import utils\nfrom utils.imageprocessing import preprocess\nfrom utils.dataset import Dataset\nfrom network import Network\nfrom evaluation.lfw import LFWTest\n\n\ndef main(args):\n\n\n    paths = Dataset(args.dataset_path)[\'abspath\']\n    print(\'%d images to load.\' % len(paths))\n    assert(len(paths)>0)\n\n    # Load model files and config file\n    network = Network()\n    network.load_model(args.model_dir) \n    images = preprocess(paths, network.config, False)\n\n    # Run forward pass to calculate embeddings\n    mu, sigma_sq = network.extract_feature(images, args.batch_size, verbose=True)\n    feat_pfe = np.concatenate([mu, sigma_sq], axis=1)\n    \n    lfwtest = LFWTest(paths)\n    lfwtest.init_standard_proto(args.protocol_path)\n\n    accuracy, threshold = lfwtest.test_standard_proto(mu, utils.pair_euc_score)\n    print(\'Euclidean (cosine) accuracy: %.5f threshold: %.5f\' % (accuracy, threshold))\n    accuracy, threshold = lfwtest.test_standard_proto(feat_pfe, utils.pair_MLS_score)\n    print(\'MLS accuracy: %.5f threshold: %.5f\' % (accuracy, threshold))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--model_dir"", help=""The path to the pre-trained model directory"",\n                        type=str)\n    parser.add_argument(""--dataset_path"", help=""The path to the LFW dataset directory"",\n                        type=str, default=\'data/lfw_mtcnncaffe_aligned\')\n    parser.add_argument(""--protocol_path"", help=""The path to the LFW protocol file"",\n                        type=str, default=\'./proto/lfw_pairs.txt\')\n    parser.add_argument(""--batch_size"", help=""Number of images per mini batch"",\n                        type=int, default=128)\n    args = parser.parse_args()\n    main(args)\n'"
evaluation/ijba.py,0,"b'""""""Test protocol for IJB-A.\n""""""\n# MIT License\n# \n# Copyright (c) 2019 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport sys\nimport os\nimport numpy as np\nimport utils\n\nfrom collections import namedtuple\n\nfrom evaluation import metrics\n\nVerificationFold = namedtuple(\'VerificationFold\', [\'train_indices\', \'test_indices\', \'train_templates\', \'templates1\',\'templates2\'])\n\nclass Template:\n    def __init__(self, subject_id, label, indices, medias):\n        self.subject_id = subject_id\n        self.label = label\n        self.indices = np.array(indices)\n        self.medias = np.array(medias)\n        \n\ndef build_subject_dict(image_list):\n    subject_dict = {}\n    for i, line in enumerate(image_list):\n        subject_id, image = tuple(line.split(\'/\')[-2:])\n        subject_id = int(subject_id)\n        image, _ = os.path.splitext(image)\n        image = image.replace(\'_\',\'/\',1) # Recover filenames \n        if not subject_id in subject_dict:\n            subject_dict[subject_id] = {}\n        subject_dict[subject_id][image] = i\n    return subject_dict\n\ndef build_templates(subject_dict, meta_file):\n    with open(meta_file, \'r\') as f:\n        meta_list = f.readlines()\n        meta_list = [x.split(\'\\n\')[0] for x in meta_list]\n        meta_list = meta_list[1:]\n\n    templates = []\n    template_id = None\n    template_label = None\n    template_indices = None \n    template_medias = None\n    count = 0\n    for line in meta_list:\n        temp_id, subject_id, image, media = tuple(line.split(\',\')[0:4])\n        temp_id = int(temp_id)\n        subject_id = int(subject_id)\n        image, _ = os.path.splitext(image)\n        if subject_id in subject_dict and image in subject_dict[subject_id]:\n            index = subject_dict[subject_id][image]\n            count += 1\n        else:\n            index = None\n\n        if temp_id != template_id:\n            if template_id is not None:\n                templates.append(Template(template_id, template_label, template_indices, template_medias))\n            template_id = temp_id\n            template_label = subject_id\n            template_indices = []\n            template_medias = []\n        if index is not None:\n            template_indices.append(index)   \n            template_medias.append(media)      \n\n    # last template\n    templates.append(Template(template_id, template_label, template_indices, template_medias))\n    return templates\n\ndef read_pairs(pair_file):\n    with open(pair_file, \'r\') as f:\n        pairs = f.readlines()\n        pairs = [x.split(\'\\n\')[0] for x in pairs]\n        pairs = [pair.split(\',\') for pair in pairs]\n        pairs = [(int(pair[0]), int(pair[1])) for pair in pairs]\n    return pairs\n\nclass IJBATest:\n\n    def __init__(self, image_paths):\n        self.image_paths = image_paths\n        self.subject_dict = build_subject_dict(image_paths)\n        self.verification_folds = None\n        self.verification_templates = None\n\n    def init_verification_proto(self, protofolder):\n        self.verification_folds = []\n        self.verification_templates = []\n        for split in range(10):\n            splitfolder = os.path.join(protofolder,\'split%d\'%(split+1))\n            train_file = os.path.join(splitfolder,\'train_%d.csv\'%(split+1))\n            meta_file = os.path.join(splitfolder,\'verify_metadata_%d.csv\'%(split+1))\n            pair_file = os.path.join(splitfolder,\'verify_comparisons_%d.csv\'%(split+1))\n\n            train_templates = build_templates(self.subject_dict, train_file)\n            train_indices = list(np.unique(np.concatenate([t.indices for t in train_templates])).astype(int))\n\n            test_templates = build_templates(self.subject_dict, meta_file)\n            test_indices = list(np.unique(np.concatenate([t.indices for t in test_templates])).astype(int))\n            template_dict = {}\n            for t in test_templates:\n                template_dict[t.subject_id] = t\n            pairs = read_pairs(pair_file)\n            templates1 = []\n            templates2 = []\n            for p in pairs:\n                templates1.append(template_dict[p[0]])\n                templates2.append(template_dict[p[1]])\n\n            train_templates = np.array(train_templates, dtype=np.object)\n            templates1 = np.array(templates1, dtype=np.object)\n            templates2 = np.array(templates2, dtype=np.object)\n\n            self.verification_folds.append(VerificationFold(\\\n                train_indices=train_indices, test_indices=test_indices,\n                train_templates=train_templates, templates1=templates1, templates2=templates2))\n\n            self.verification_templates.extend(train_templates)\n            self.verification_templates.extend(templates1)\n            self.verification_templates.extend(templates2)\n\n    def init_proto(self, protofolder):\n        self.init_verification_proto(os.path.join(protofolder, \'IJB-A_11_sets\'))\n\n    def test_verification_fold(self, compare_func, fold_idx, FARs=None, get_false_indices=False):\n\n        FARs = [0.001, 0.01] if FARs is None else FARs\n\n        fold = self.verification_folds[fold_idx]\n\n        templates1 = fold.templates1\n        templates2 = fold.templates2\n\n        features1 = [t.feature for t in templates1]\n        features2 = [t.feature for t in templates2]\n        labels1 = np.array([t.label for t in templates1])\n        labels2 = np.array([t.label for t in templates2])\n\n        score_vec = compare_func(features1, features2)\n        label_vec = labels1 == labels2\n\n        score_neg = score_vec[~label_vec]     \n\n        return metrics.ROC(score_vec, label_vec, \n                FARs=FARs, get_false_indices=get_false_indices)\n\n    def test_verification(self, compare_func, FARs=None):\n        \n        TARs_all = []\n        FARs_all = []\n        for i in range(10):\n            TARs, FARs, thresholds = self.test_verification_fold(compare_func, i, FARs=FARs)\n            TARs_all.append(TARs)\n            FARs_all.append(FARs)\n\n        TARs_all = np.stack(TARs_all)\n        FARs_all = np.stack(FARs_all)\n\n\n        return np.mean(TARs_all, axis=0), np.std(TARs_all, axis=0), np.mean(FARs_all, axis=0)\n'"
evaluation/ijbc.py,0,"b'""""""\nMain file for evaluation on IJB-A and IJB-B protocols.\nMore instructions can be found in README.md file.\n2017 Yichun Shi\n""""""\n\nimport sys\nimport os\nimport numpy as np\nimport utils\n\nfrom evaluation import metrics\nfrom collections import namedtuple\n\n\n# Configuration\nVerificationFold = namedtuple(\'VerificationFold\', [\'train_indices\', \'test_indices\', \'train_templates\', \'templates1\',\'templates2\'])\n\nclass Template:\n    def __init__(self, template_id, label, indices, medias):\n        self.template_id = template_id\n        self.label = label\n        self.indices = np.array(indices)\n        self.medias = np.array(medias)\n        \n\ndef build_subject_dict(image_list):\n    subject_dict = {}\n    for i, line in enumerate(image_list):\n        subject_id, image = tuple(line.split(\'/\')[-2:])\n        if subject_id == \'NaN\': continue\n        subject_id = int(subject_id)\n        image, _ = os.path.splitext(image)\n        image = image.replace(\'_\',\'/\',1) # Recover filenames \n        if not subject_id in subject_dict:\n            subject_dict[subject_id] = {}\n        subject_dict[subject_id][image] = i\n    return subject_dict\n\ndef build_templates(subject_dict, meta_file):\n    with open(meta_file, \'r\') as f:\n        meta_list = f.readlines()\n        meta_list = [x.split(\'\\n\')[0] for x in meta_list]\n        meta_list = meta_list[1:]\n\n    templates = []\n    template_id = None\n    template_label = None\n    template_indices = None\n    template_medias = None\n    count = 0\n    for line in meta_list:\n        temp_id, subject_id, image, media = tuple(line.split(\',\')[0:4])\n        temp_id = int(temp_id)\n        subject_id = int(subject_id)\n        image, _ = os.path.splitext(image)\n        if subject_id in subject_dict and image in subject_dict[subject_id]:\n            index = subject_dict[subject_id][image]\n            count += 1\n        else:\n            index = None\n\n        if temp_id != template_id:\n            if template_id is not None:\n                templates.append(Template(template_id, template_label, template_indices, template_medias))\n            template_id = temp_id\n            template_label = subject_id\n            template_indices = []\n            template_medias = []\n\n        if index is not None:\n            template_indices.append(index)        \n            template_medias.append(media)        \n\n    # last template\n    templates.append(Template(template_id, template_label, template_indices, template_medias))\n    return templates\n\ndef read_pairs(pair_file):\n    with open(pair_file, \'r\') as f:\n        pairs = f.readlines()\n        pairs = [x.split(\'\\n\')[0] for x in pairs]\n        pairs = [pair.split(\',\') for pair in pairs]\n        pairs = [(int(pair[0]), int(pair[1])) for pair in pairs]\n    return pairs\n\nclass IJBCTest:\n\n    def __init__(self, image_paths):\n        self.image_paths = image_paths\n        self.subject_dict = build_subject_dict(image_paths)\n        self.verification_folds = None\n        self.verification_templates = None\n        self.verification_G1_templates = None\n        self.verification_G2_templates = None\n\n    def init_verification_proto(self, protofolder):\n        self.verification_folds = []\n        self.verification_templates = []\n\n        meta_gallery1 = os.path.join(protofolder,\'ijbc_1N_gallery_G1.csv\')\n        meta_gallery2 = os.path.join(protofolder,\'ijbc_1N_gallery_G2.csv\')\n        meta_probe = os.path.join(protofolder,\'ijbc_1N_probe_mixed.csv\')\n        pair_file = os.path.join(protofolder,\'ijbc_11_G1_G2_matches.csv\')\n\n        gallery_templates = build_templates(self.subject_dict, meta_gallery1)\n        gallery_templates.extend(build_templates(self.subject_dict, meta_gallery2))\n        gallery_templates.extend(build_templates(self.subject_dict, meta_probe))\n\n        # Build pairs\n        template_dict = {}\n        for t in gallery_templates:\n            template_dict[t.template_id] = t\n        pairs = read_pairs(pair_file)\n        self.verification_G1_templates = []\n        self.verification_G2_templates = []\n        for p in pairs:\n            self.verification_G1_templates.append(template_dict[p[0]])\n            self.verification_G2_templates.append(template_dict[p[1]])\n\n        self.verification_G1_templates = np.array(self.verification_G1_templates, dtype=np.object)\n        self.verification_G2_templates = np.array(self.verification_G2_templates, dtype=np.object)\n    \n        self.verification_templates = np.concatenate([\n            self.verification_G1_templates, self.verification_G2_templates])\n        print(\'{} templates are initialized.\'.format(len(self.verification_templates)))\n\n\n    def init_proto(self, protofolder):\n        self.init_verification_proto(protofolder)\n\n    def test_verification(self, compare_func, FARs=None):\n\n        FARs = [1e-5, 1e-4, 1e-3, 1e-2] if FARs is None else FARs\n\n        templates1 = self.verification_G1_templates\n        templates2 = self.verification_G2_templates\n\n        features1 = [t.feature for t in templates1]\n        features2 = [t.feature for t in templates2]\n        labels1 = np.array([t.label for t in templates1])\n        labels2 = np.array([t.label for t in templates2])\n\n        score_vec = compare_func(features1, features2)\n        label_vec = labels1 == labels2\n\n        tars, fars, thresholds = metrics.ROC(score_vec, label_vec, FARs=FARs)\n        \n        # There is no std for IJB-C\n        std = [0. for t in tars]\n\n        return tars, std, fars\n\n'"
evaluation/lfw.py,0,"b'""""""Test protocols on LFW dataset\n""""""\n# MIT License\n# \n# Copyright (c) 2017 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport os\nimport numpy as np\nimport scipy.io as sio\nfrom evaluation import metrics\nfrom collections import namedtuple\n\nStandardFold = namedtuple(\'StandardFold\', [\'indices1\', \'indices2\', \'labels\'])\n\nclass LFWTest:\n    def __init__(self, image_paths):\n        self.image_paths = np.array(image_paths).astype(np.object).flatten()\n        self.images = None\n        self.labels = None\n        self.standard_folds = None\n        self.blufr_folds = None\n        self.queue_idx = None\n\n    def init_standard_proto(self, lfw_pairs_file):\n        index_dict = {}\n        for i, image_path in enumerate(self.image_paths):\n            image_name, image_ext = os.path.splitext(os.path.basename(image_path))\n            index_dict[image_name] = i\n\n        pairs = []\n        with open(lfw_pairs_file, \'r\') as f:\n            for line in f.readlines()[1:]:\n                pair = line.strip().split()\n                pairs.append(pair)\n\n        # 10 folds\n        self.standard_folds = []\n        for i in range(10):\n            indices1 = np.zeros(600, dtype=np.int32)\n            indices2 = np.zeros(600, dtype=np.int32)\n            labels = np.array([True]*300+[False]*300, dtype=np.bool)\n            # 300 positive pairs, 300 negative pairs in order\n            for j in range(600):\n                pair = pairs[600*i+j]\n                if j < 300:\n                    assert len(pair) == 3\n                    img1 = pair[0] + \'_\' + \'%04d\' % int(pair[1])\n                    img2 = pair[0] + \'_\' + \'%04d\' % int(pair[2])\n                else:\n                    assert len(pair) == 4\n                    img1 = pair[0] + \'_\' + \'%04d\' % int(pair[1])\n                    img2 = pair[2] + \'_\' + \'%04d\' % int(pair[3])                \n                indices1[j] = index_dict[img1]\n                indices2[j] = index_dict[img2]\n            fold = StandardFold(indices1, indices2, labels)\n            self.standard_folds.append(fold)\n\n    def test_standard_proto(self, features, compare_func):\n\n        assert self.standard_folds is not None\n        \n        accuracies = np.zeros(10, dtype=np.float32)\n        thresholds = np.zeros(10, dtype=np.float32)\n\n        features1 = []\n        features2 = []\n\n        for i in range(10):\n            # Training\n            train_indices1 = np.concatenate([self.standard_folds[j].indices1 for j in range(10) if j!=i])\n            train_indices2 = np.concatenate([self.standard_folds[j].indices2 for j in range(10) if j!=i])\n            train_labels = np.concatenate([self.standard_folds[j].labels for j in range(10) if j!=i])\n\n            train_features1 = features[train_indices1,:]\n            train_features2 = features[train_indices2,:]\n            \n            train_score = compare_func(train_features1, train_features2)\n            _, thresholds[i] = metrics.accuracy(train_score, train_labels)\n\n            # Testing\n            fold = self.standard_folds[i]\n            test_features1 = features[fold.indices1,:]\n            test_features2 = features[fold.indices2,:]\n            \n            test_score = compare_func(test_features1, test_features2)\n            accuracies[i], _ = metrics.accuracy(test_score, fold.labels, np.array([thresholds[i]]))\n\n        accuracy = np.mean(accuracies)\n        threshold = - np.mean(thresholds)\n        return accuracy, threshold\n\n'"
evaluation/metrics.py,0,"b'""""""Common metrics used for evaluation\n""""""\n# MIT License\n# \n# Copyright (c) 2019 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport numpy as np\nfrom warnings import warn\n\n# Find thresholds given FARs\n# but the real FARs using these thresholds could be different\n# the exact FARs need to recomputed using calcROC\ndef find_thresholds_by_FAR(score_vec, label_vec, FARs=None, epsilon=1e-8):\n    assert len(score_vec.shape)==1\n    assert score_vec.shape == label_vec.shape\n    assert label_vec.dtype == np.bool\n    score_neg = score_vec[~label_vec]\n    score_neg[::-1].sort()\n    # score_neg = np.sort(score_neg)[::-1] # score from high to low\n    num_neg = len(score_neg)\n\n    assert num_neg >= 1\n\n    if FARs is None:\n        thresholds = np.unique(score_neg)\n        thresholds = np.insert(thresholds, 0, thresholds[0]+epsilon)\n        thresholds = np.insert(thresholds, thresholds.size, thresholds[-1]-epsilon)\n    else:\n        FARs = np.array(FARs)\n        num_false_alarms = np.round(num_neg * FARs).astype(np.int32)\n\n        thresholds = []\n        for num_false_alarm in num_false_alarms:\n            if num_false_alarm==0:\n                threshold = score_neg[0] + epsilon\n            else:\n                threshold = score_neg[num_false_alarm-1]\n            thresholds.append(threshold)\n        thresholds = np.array(thresholds)\n\n    return thresholds\n\n\n\ndef ROC(score_vec, label_vec, thresholds=None, FARs=None, get_false_indices=False):\n    \'\'\' Compute Receiver operating characteristic (ROC) with a score and label vector.\n    \'\'\'\n    assert score_vec.ndim == 1\n    assert score_vec.shape == label_vec.shape\n    assert label_vec.dtype == np.bool\n    \n    if thresholds is None:\n        thresholds = find_thresholds_by_FAR(score_vec, label_vec, FARs=FARs)\n\n    assert len(thresholds.shape)==1 \n    if np.size(thresholds) > 10000:\n        warn(\'number of thresholds (%d) very large, computation may take a long time!\' % np.size(thresholds))\n\n    # FARs would be check again\n    TARs = np.zeros(thresholds.shape[0])\n    FARs = np.zeros(thresholds.shape[0])\n    false_accept_indices = []\n    false_reject_indices = []\n    for i,threshold in enumerate(thresholds):\n        accept = score_vec >= threshold\n        TARs[i] = np.mean(accept[label_vec])\n        FARs[i] = np.mean(accept[~label_vec])\n        if get_false_indices:\n            false_accept_indices.append(np.argwhere(accept & (~label_vec)).flatten())\n            false_reject_indices.append(np.argwhere((~accept) & label_vec).flatten())\n\n    if get_false_indices:\n        return TARs, FARs, thresholds, false_accept_indices, false_reject_indices\n    else:\n        return TARs, FARs, thresholds\n\ndef ROC_by_mat(score_mat, label_mat, thresholds=None, FARs=None, get_false_indices=False, triu_k=None):\n    \'\'\' Compute ROC using a pairwise score matrix and a corresponding label matrix.\n        A wapper of ROC function.\n    \'\'\'\n    assert score_mat.ndim == 2\n    assert score_mat.shape == label_mat.shape\n    assert label_mat.dtype == np.bool\n    \n    # Convert into vectors\n    m,n  = score_mat.shape\n    if triu_k is not None:\n        assert m==n, ""If using triu for ROC, the score matrix must be a sqaure matrix!""\n        triu_indices = np.triu_indices(m, triu_k)\n        score_vec = score_mat[triu_indices]\n        label_vec = label_mat[triu_indices]\n    else:\n        score_vec = score_mat.flatten()\n        label_vec = label_mat.flatten()\n\n    # Compute ROC\n    if get_false_indices:\n        TARs, FARs, thresholds, false_accept_indices, false_reject_indices = \\\n                    ROC(score_vec, label_vec, thresholds, FARs, True)\n    else:\n        TARs, FARs, thresholds = ROC(score_vec, label_vec, thresholds, FARs, False)\n\n    # Convert false accept/reject indices into [row, col] indices\n    if get_false_indices:\n        rows, cols = np.meshgrid(np.arange(m), np.arange(n), indexing=\'ij\')\n        rc = np.stack([rows, cols], axis=2)\n        if triu_k is not None:\n            rc = rc[triu_indices,:]\n        else:\n            rc = rc.reshape([-1,2])\n\n        for i in range(len(FARs)):\n            false_accept_indices[i] = rc[false_accept_indices[i]]\n            false_reject_indices[i] = rc[false_reject_indices[i]]\n        return TARs, FARs, thresholds, false_accept_indices, false_reject_indices\n    else:\n        return TARs, FARs, thresholds\n\n\n\n\ndef DIR_FAR(score_mat, label_mat, ranks=[1], FARs=[1.0], get_false_indices=False):\n    \'\'\' Closed/Open-set Identification. \n        A general case of Cummulative Match Characteristic (CMC) \n        where thresholding is allowed for open-set identification.\n    args:\n        score_mat:            a P x G matrix, P is number of probes, G is size of gallery\n        label_mat:            a P x G matrix, bool\n        ranks:                a list of integers\n        FARs:                 false alarm rates, if 1.0, closed-set identification (CMC)\n        get_false_indices:    not implemented yet\n    return:\n        DIRs:                 an F x R matrix, F is the number of FARs, R is the number of ranks, \n                              flatten into a vector if F=1 or R=1.\n        FARs:                 an vector of length = F.\n        thredholds:           an vector of length = F.\n    \'\'\'\n    assert score_mat.shape==label_mat.shape\n    assert np.all(label_mat.astype(np.float32).sum(axis=1) <=1 )\n    # Split the matrix for match probes and non-match probes\n    # subfix _m: match, _nm: non-match\n    # For closed set, we only use the match probes\n    match_indices = label_mat.astype(np.bool).any(axis=1)\n    score_mat_m = score_mat[match_indices,:]\n    label_mat_m = label_mat[match_indices,:]\n    score_mat_nm = score_mat[np.logical_not(match_indices),:]\n    label_mat_nm = label_mat[np.logical_not(match_indices),:]\n\n    print(\'mate probes: %d, non mate probes: %d\' % (score_mat_m.shape[0], score_mat_nm.shape[0]))\n\n    # Find the thresholds for different FARs\n    max_score_nm = np.max(score_mat_nm, axis=1)\n    label_temp = np.zeros(max_score_nm.shape, dtype=np.bool)\n    if len(FARs) == 1 and FARs[0] >= 1.0:\n        # If only testing closed-set identification, use the minimum score as threshold\n        # in case there is no non-mate probes\n        thresholds = [np.min(score_mat) - 1e-10]\n    else:\n        # If there is open-set identification, find the thresholds by FARs.\n        assert score_mat_nm.shape[0] > 0, ""For open-set identification (FAR<1.0), there should be at least one non-mate probe!""\n        thresholds = find_thresholds_by_FAR(max_score_nm, label_temp, FARs=FARs)\n\n    # Sort the labels row by row according to scores\n    sort_idx_mat_m = np.argsort(score_mat_m, axis=1)\n    sorted_label_mat_m = np.ndarray(label_mat_m.shape, dtype=np.bool)\n    for row in range(label_mat_m.shape[0]):\n        sort_idx = (sort_idx_mat_m[row, :])[::-1]\n        sorted_label_mat_m[row,:] = label_mat_m[row, sort_idx]\n        \n    # Calculate DIRs for different FARs and ranks\n    gt_score_m = score_mat_m[label_mat_m]\n    assert gt_score_m.size == score_mat_m.shape[0]\n\n    DIRs = np.zeros([len(FARs), len(ranks)], dtype=np.float32)\n    FARs = np.zeros([len(FARs)], dtype=np.float32)\n    for i, threshold in enumerate(thresholds):\n        for j, rank  in enumerate(ranks):\n            score_rank = gt_score_m >= threshold\n            retrieval_rank = sorted_label_mat_m[:,0:rank].any(axis=1)\n            DIRs[i,j] = (score_rank & retrieval_rank).astype(np.float32).mean()\n        if score_mat_nm.shape[0] > 0:\n            FARs[i] = (max_score_nm >= threshold).astype(np.float32).mean()\n\n    if DIRs.shape[0] == 1 or DIRs.shape[1] == 1:\n        DIRs = DIRs.flatten()\n\n    return DIRs, FARs, thresholds\n\ndef accuracy(score_vec, label_vec, thresholds=None):\n    assert len(score_vec.shape)==1\n    assert len(label_vec.shape)==1\n    assert score_vec.shape == label_vec.shape\n    assert label_vec.dtype==np.bool\n    # find thresholds by TAR\n    if thresholds is None:\n        score_pos = score_vec[label_vec==True]\n        thresholds = np.sort(score_pos)[::1]    \n\n    assert len(thresholds.shape)==1\n    if np.size(thresholds) > 10000:\n        warn(\'number of thresholds (%d) very large, computation may take a long time!\' % np.size(thresholds))\n    \n    # Loop Computation\n    accuracies = np.zeros(np.size(thresholds))\n    for i, threshold in enumerate(thresholds):\n        pred_vec = score_vec>=threshold\n        accuracies[i] = np.mean(pred_vec==label_vec)\n\n    # Matrix Computation, Each column is a threshold\n    # predictions = score_vec[:,None] >= thresholds[None,:]\n    # accuracies = np.mean(predictions==label_vec[:,None], axis=0)\n\n    argmax = np.argmax(accuracies)\n    accuracy = accuracies[argmax]\n    threshold = np.mean(thresholds[accuracies==accuracy])\n\n    return accuracy, threshold\n'"
models/__init__.py,0,b''
models/sphere_net_PFE.py,15,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nmodel_params = {\n    '4': ([0, 0, 0, 0], [64, 128, 256, 512]),\n    '10': ([0, 1, 2, 0], [64, 128, 256, 512]),\n    '20': ([1, 2, 4, 1], [64, 128, 256, 512]),\n    '36': ([2, 4, 8, 2], [64, 128, 256, 512]),\n    '64': ([3, 8, 16, 3], [64, 128, 256, 512]),\n}\n\nbatch_norm_params_last = {\n    'decay': 0.995,\n    'epsilon': 0.001,\n    'center': True,\n    'scale': False,\n    'updates_collections': None,\n    'variables_collections': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n}\n\n\ndef parametric_relu(x):\n    num_channels = x.shape[-1].value\n    with tf.variable_scope('p_re_lu'):\n        alpha = tf.get_variable('alpha', (1,1,num_channels),\n                        initializer=tf.constant_initializer(0.0),\n                        dtype=tf.float32)\n        return tf.nn.relu(x) + alpha * tf.minimum(0.0, x)\n\ndef se_module(input_net, ratio=16, reuse = None, scope = None):\n    with tf.variable_scope(scope, 'SE', [input_net], reuse=reuse):\n        h,w,c = tuple([dim.value for dim in input_net.shape[1:4]])\n        assert c % ratio == 0\n        hidden_units = int(c / ratio)\n        squeeze = slim.avg_pool2d(input_net, [h,w], padding='VALID')\n        excitation = slim.flatten(squeeze)\n        excitation = slim.fully_connected(excitation, hidden_units, scope='se_fc1',\n                                weights_regularizer=None,\n                                weights_initializer=slim.xavier_initializer(), \n                                activation_fn=tf.nn.relu)\n        excitation = slim.fully_connected(excitation, c, scope='se_fc2',\n                                weights_regularizer=None,\n                                weights_initializer=slim.xavier_initializer(), \n                                activation_fn=tf.nn.sigmoid)        \n        excitation = tf.reshape(excitation, [-1,1,1,c])\n        output_net = input_net * excitation\n\n        return output_net\n\ndef conv_module(net, num_res_layers, num_kernels, trans_kernel_size=3, trans_stride=2,\n                     use_se=False, reuse=None, scope=None):\n    with tf.variable_scope(scope, 'conv', [net], reuse=reuse):\n        net = slim.conv2d(net, num_kernels, kernel_size=trans_kernel_size, stride=trans_stride, padding='SAME',\n                weights_initializer=slim.xavier_initializer()) \n        shortcut = net\n        for i in range(num_res_layers):\n            net = slim.conv2d(net, num_kernels, kernel_size=3, stride=1, padding='SAME',\n                weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n                biases_initializer=None)\n            net = slim.conv2d(net, num_kernels, kernel_size=3, stride=1, padding='SAME',\n                weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n                biases_initializer=None)\n            print('| ---- block_%d' % i)\n            if use_se:\n                net = se_module(net)\n            net = net + shortcut\n            shortcut = net\n    return net\n\ndef inference(images, embedding_size=512, reuse=None, scope='SphereNet'):\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        weights_regularizer=slim.l2_regularizer(0.0),\n                        normalizer_fn=None, \n                        normalizer_params=None, \n                        activation_fn=parametric_relu):\n        with tf.variable_scope('SphereNet', [images], reuse=reuse):\n            # Fix the moving mean and std when training PFE \n            with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=False): \n\n                print('SphereNet input shape:', [dim.value for dim in images.shape])\n                \n                model_version = '64' \n                num_layers, num_kernels = model_params[model_version]\n\n\n                net = conv_module(images, num_layers[0], num_kernels[0], scope='conv1')\n                print('module_1 shape:', [dim.value for dim in net.shape])\n\n                net = conv_module(net, num_layers[1], num_kernels[1], scope='conv2')\n                print('module_2 shape:', [dim.value for dim in net.shape])\n                \n                net = conv_module(net, num_layers[2], num_kernels[2], scope='conv3')\n                print('module_3 shape:', [dim.value for dim in net.shape])\n\n                net = conv_module(net, num_layers[3], num_kernels[3], scope='conv4')\n                print('module_4 shape:', [dim.value for dim in net.shape])\n\n                net_ = net\n                net = slim.flatten(net)\n\n                mu = slim.fully_connected(net, embedding_size, scope='Bottleneck',\n                                        weights_initializer=slim.xavier_initializer(),\n                                        normalizer_fn=slim.batch_norm, normalizer_params=batch_norm_params_last,\n                                        activation_fn=None)\n                \n                # Output used for PFE\n                mu = tf.nn.l2_normalize(mu, axis=1)\n                conv_final = net\n            \n    return mu, conv_final\n"""
models/uncertainty_module.py,13,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n\nbatch_norm_params = {\n    'decay': 0.995,\n    'epsilon': 0.001,\n    'center': True,\n    'scale': True,\n    'updates_collections': None,\n    'variables_collections': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n}\n\nbatch_norm_params_sigma = {\n    'decay': 0.995,\n    'epsilon': 0.001,\n    'center': False,\n    'scale': False,\n    'updates_collections': None,\n    'variables_collections': [ tf.GraphKeys.TRAINABLE_VARIABLES ],}\n\ndef scale_and_shift(x, gamma_init=1.0, beta_init=0.0):\n    num_channels = x.shape[-1].value\n    with tf.variable_scope('scale_and_shift'):\n        gamma = tf.get_variable('alpha', (),\n                        initializer=tf.constant_initializer(gamma_init),\n                        regularizer=slim.l2_regularizer(0.0),\n                        dtype=tf.float32)\n        beta = tf.get_variable('gamma', (),\n                        initializer=tf.constant_initializer(beta_init),\n                        dtype=tf.float32)\n        x = gamma * x +  beta\n\n        return x   \n    \n\ndef inference(inputs, embedding_size, phase_train, \n        weight_decay=5e-4, reuse=None, scope='UncertaintyModule'):\n    with slim.arg_scope([slim.fully_connected],\n                        weights_regularizer=slim.l2_regularizer(weight_decay),\n                        activation_fn=tf.nn.relu):\n        with tf.variable_scope(scope, [inputs], reuse=reuse):\n            with slim.arg_scope([slim.batch_norm, slim.dropout],\n                                is_training=phase_train):\n                print('UncertaintyModule input shape:', [dim.value for dim in inputs.shape])\n\n                net = slim.flatten(inputs)\n\n                net = slim.fully_connected(net, embedding_size, scope='fc1',\n                    normalizer_fn=slim.batch_norm, normalizer_params=batch_norm_params, \n                    activation_fn=tf.nn.relu)\n\n\n                log_sigma_sq = slim.fully_connected(net, embedding_size, scope='fc_log_sigma_sq',\n                    normalizer_fn=slim.batch_norm, normalizer_params=batch_norm_params_sigma,\n                    activation_fn=None)\n          \n                # Share the gamma and beta for all dimensions\n                log_sigma_sq = scale_and_shift(log_sigma_sq, 1e-4, -7.0)\n\n                # Add epsilon for sigma_sq for numerical stableness                \n                log_sigma_sq = tf.log(1e-6 + tf.exp(log_sigma_sq))\n\n    return log_sigma_sq\n"""
utils/__init__.py,0,b'from . import *\n'
utils/dataset.py,0,"b'""""""Data fetching with pandas\n""""""\n# MIT License\n# \n# Copyright (c) 2018 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport sys\nimport os\nimport time\nimport math\nimport random\nimport shutil\nfrom multiprocessing import Process, Queue\n\nimport numpy as np\nimport pandas as pd\n\nqueue_timeout = 600\n\nclass Dataset(object):\n\n    def __init__(self, path=None, prefix=None):\n\n        if path is not None:\n            self.init_from_path(path)\n        else:\n            self.data = pd.DataFrame([], columns=[\'path\', \'abspath\', \'label\', \'name\'])\n\n        self.prefix = prefix\n        self.base_seed = 0\n        self.batch_queue = None\n        self.batch_workers = None\n       \n\n    def __getitem__(self, key):\n        return self.data[key]\n\n    def __setitem__(self, key, value):\n        self.data[key] = value\n        return self.data[key]\n\n    def _delitem(self, key):\n        self.data.__delitem__(key)\n\n    @property\n    def num_classes(self):\n        return len(self.data[\'label\'].unique())\n\n    @property\n    def classes(self):\n        return self.data[\'label\'].unique()\n\n    @property\n    def size(self):\n        return self.data.shape[0]\n\n    @property\n    def loc(self):\n        return self.data.loc       \n\n    @property\n    def iloc(self):\n        return self.data.iloc\n\n    def init_from_path(self, path):\n        path = os.path.expanduser(path)\n        _, ext = os.path.splitext(path)\n        if os.path.isdir(path):\n            self.init_from_folder(path)\n        elif ext == \'.txt\':\n            self.init_from_list(path)\n        else:\n            raise ValueError(\'Cannot initialize dataset from path: %s\\n\\\n                It should be either a folder, .txt or .hdf5 file\' % path)\n        # print(\'%d images of %d classes loaded\' % (len(self.images), self.num_classes))\n\n    def init_from_folder(self, folder):\n        folder = os.path.abspath(os.path.expanduser(folder))\n        class_names = os.listdir(folder)\n        class_names.sort()\n        paths = []\n        labels = []\n        names = []\n        for label, class_name in enumerate(class_names):\n            classdir = os.path.join(folder, class_name)\n            if os.path.isdir(classdir):\n                images_class = os.listdir(classdir)\n                images_class.sort()\n                images_class = [os.path.join(class_name,img) for img in images_class]\n                paths.extend(images_class)\n                labels.extend(len(images_class) * [label])\n                names.extend(len(images_class) * [class_name])\n        abspaths = [os.path.join(folder,p) for p in paths]\n        self.data = pd.DataFrame({\'path\': paths, \'abspath\': abspaths, \n                                            \'label\': labels, \'name\': names})\n        self.prefix = folder\n\n    \n\n    def init_from_list(self, filename, folder_depth=2):\n        with open(filename, \'r\') as f:\n            lines = f.readlines()\n        lines = [line.strip().split(\' \') for line in lines]\n        abspaths = [os.path.abspath(line[0]) for line in lines]\n        paths = [\'/\'.join(p.split(\'/\')[-folder_depth:]) for p in abspaths]\n        if len(lines[0]) == 2:\n            labels = [int(line[1]) for line in lines]\n            names = [str(lb) for lb in labels]\n        elif len(lines[0]) == 1:\n            names = [p.split(\'/\')[-folder_depth] for p in abspaths]\n            _, labels = np.unique(names, return_inverse=True)\n        else:\n            raise ValueError(\'List file must be in format: ""fullpath(str) \\\n                                        label(int)"" or just ""fullpath(str)""\')\n\n        self.data = pd.DataFrame({\'path\': paths, \'abspath\': abspaths, \n                                            \'label\': labels, \'name\': names})\n        self.prefix = abspaths[0].split(\'/\')[:-folder_depth]\n\n\n    #\n    # Data Loading\n    #\n\n    def set_base_seed(self, base_seed=0):\n        self.base_seed = base_seed\n\n    def random_samples_from_class(self, label, num_samples, exception=None):\n        # indices_temp = self.class_indices[label]\n        indices_temp = list(np.where(self.data[\'label\'].values == label)[0])\n        \n        if exception is not None:\n            indices_temp.remove(exception)\n            assert len(indices_temp) > 0\n        # Sample indices multiple times when more samples are required than present.\n        indices = []\n        iterations = int(np.ceil(1.0*num_samples / len(indices_temp)))\n        for i in range(iterations):\n            sample_indices = np.random.permutation(indices_temp)\n            indices.append(sample_indices)\n        indices = list(np.concatenate(indices, axis=0)[:num_samples])\n        return indices\n\n    def get_batch_indices(self, batch_format):\n        \'\'\' Get the indices from index queue and fetch the data with indices.\'\'\'\n        indices_batch = []\n        batch_size = batch_format[\'size\']\n\n        num_classes = batch_format[\'num_classes\']\n        assert batch_size % num_classes == 0\n        num_samples_per_class = batch_size // num_classes\n        idx_classes = np.random.permutation(self.classes)[:num_classes]\n        indices_batch = []\n        for c in idx_classes:\n            indices_batch.extend(self.random_samples_from_class(c, num_samples_per_class))\n\n        return indices_batch\n\n    def get_batch(self, batch_format):\n\n        indices = self.get_batch_indices(batch_format)\n        batch = {}\n        for column in self.data.columns:\n            batch[column] = self.data[column].values[indices]\n\n        return batch\n\n    # Multithreading preprocessing images\n\n    def start_batch_queue(self, batch_format, proc_func=None, maxsize=1, num_threads=3):\n\n        self.batch_queue = Queue(maxsize=maxsize)\n        def batch_queue_worker(seed):\n            np.random.seed(seed+self.base_seed)\n            while True:\n                batch = self.get_batch(batch_format)\n                if proc_func is not None:\n                    batch[\'image\'] = proc_func(batch[\'abspath\'])\n                self.batch_queue.put(batch)\n\n        self.batch_workers = []\n        for i in range(num_threads):\n            worker = Process(target=batch_queue_worker, args=(i,))\n            worker.daemon = True\n            worker.start()\n            self.batch_workers.append(worker)\n    \n    def pop_batch_queue(self, timeout=queue_timeout):\n        return self.batch_queue.get(block=True, timeout=timeout)\n      \n    def release_queue(self):\n        if self.index_queue is not None:\n            self.index_queue.close()\n        if self.batch_queue is not None:\n            self.batch_queue.close()\n        if self.index_worker is not None:\n            self.index_worker.terminate()   \n            del self.index_worker\n            self.index_worker = None\n        if self.batch_workers is not None:\n            for w in self.batch_workers:\n                w.terminate()\n                del w\n            self.batch_workers = None\n\n'"
utils/imageprocessing.py,0,"b'""""""Functions for image processing\n""""""\n# MIT License\n# \n# Copyright (c) 2017 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport sys\nimport os\nimport math\nimport random\nimport numpy as np\nfrom scipy import misc\n\n\n# Calulate the shape for creating new array given (h,w)\ndef get_new_shape(images, size=None, n=None):\n    shape = list(images.shape)\n    if size is not None:\n        h, w = tuple(size)\n        shape[1] = h\n        shape[2] = w\n    if n is not None:\n        shape[0] = n\n    shape = tuple(shape)\n    return shape\n\ndef random_crop(images, size):\n    n, _h, _w = images.shape[:3]\n    h, w = tuple(size)\n    shape_new = get_new_shape(images, size)\n    assert (_h>=h and _w>=w)\n\n    images_new = np.ndarray(shape_new, dtype=images.dtype)\n\n    y = np.random.randint(low=0, high=_h-h+1, size=(n))\n    x = np.random.randint(low=0, high=_w-w+1, size=(n))\n\n    for i in range(n):\n        images_new[i] = images[i, y[i]:y[i]+h, x[i]:x[i]+w]\n\n    return images_new\n\ndef center_crop(images, size):\n    n, _h, _w = images.shape[:3]\n    h, w = tuple(size)\n    assert (_h>=h and _w>=w)\n\n    y = int(round(0.5 * (_h - h)))\n    x = int(round(0.5 * (_w - w)))\n\n    images_new = images[:, y:y+h, x:x+w]\n\n    return images_new\n\ndef random_flip(images):\n    images_new = images.copy()\n    flips = np.random.rand(images_new.shape[0])>=0.5\n    \n    for i in range(images_new.shape[0]):\n        if flips[i]:\n            images_new[i] = np.fliplr(images[i])\n\n    return images_new\n\ndef flip(images):\n    images_new = images.copy()\n    for i in range(images_new.shape[0]):\n        images_new[i] = np.fliplr(images[i])\n\n    return images_new\n\ndef resize(images, size):\n    n, _h, _w = images.shape[:3]\n    h, w = tuple(size)\n    shape_new = get_new_shape(images, size)\n\n    images_new = np.ndarray(shape_new, dtype=images.dtype)\n\n    for i in range(n):\n        images_new[i] = misc.imresize(images[i], (h,w))\n\n    return images_new\n\ndef padding(images, padding):\n    n, _h, _w = images.shape[:3]\n    if len(padding) == 2:\n        pad_t = pad_b = padding[0]\n        pad_l = pad_r = padding[1]\n    else:\n        pad_t, pad_b, pad_l, pad_r = tuple(padding)\n       \n    size_new = (_h + pad_t + pad_b, _w + pad_l + pad_b)\n    shape_new = get_new_shape(images, size_new)\n    images_new = np.zeros(shape_new, dtype=images.dtype)\n    images_new[:, pad_t:pad_t+_h, pad_l:pad_l+_w] = images\n\n    return images_new\n\ndef standardize_images(images, standard):\n    if standard==\'mean_scale\':\n        mean = 127.5\n        std = 128.0\n    elif standard==\'scale\':\n        mean = 0.0\n        std = 255.0\n    images_new = images.astype(np.float32)\n    images_new = (images_new - mean) / std\n    return images_new\n\n\n\ndef random_shift(images, max_ratio):\n    n, _h, _w = images.shape[:3]\n    pad_x = int(_w * max_ratio) + 1\n    pad_y = int(_h * max_ratio) + 1\n    images_temp = padding(images, (pad_x, pad_y))\n    images_new = images.copy()\n\n    shift_x = (_w * max_ratio * np.random.rand(n)).astype(np.int32)\n    shift_y = (_h * max_ratio * np.random.rand(n)).astype(np.int32)\n\n    for i in range(n):\n        images_new[i] = images_temp[i, pad_y+shift_y[i]:pad_y+shift_y[i]+_h, \n                            pad_x+shift_x[i]:pad_x+shift_x[i]+_w]\n\n    return images_new    \n    \n\ndef random_downsample(images, min_ratio):\n    n, _h, _w = images.shape[:3]\n    images_new = images.copy()\n    ratios = min_ratio + (1-min_ratio) * np.random.rand(n)\n\n    for i in range(n):\n        w = int(round(ratios[i] * _w))\n        h = int(round(ratios[i] * _h))\n        images_new[i,:h,:w] = misc.imresize(images[i], (h,w))\n        images_new[i] = misc.imresize(images_new[i,:h,:w], (_h,_w))\n        \n    return images_new\n\ndef random_interpolate(images):\n    _n, _h, _w = images.shape[:3]\n    nd = images.ndim - 1\n    assert _n % 2 == 0\n    n = int(_n / 2)\n\n    ratios = np.random.rand(n,*([1]*nd))\n    images_left, images_right = (images[np.arange(n)*2], images[np.arange(n)*2+1])\n    images_new = ratios * images_left + (1-ratios) * images_right\n    images_new = images_new.astype(np.uint8)\n\n    return images_new\n    \ndef expand_flip(images):\n    \'\'\'Flip each image in the array and insert it after the original image.\'\'\'\n    _n, _h, _w = images.shape[:3]\n    shape_new = get_new_shape(images, n=2*_n)\n    images_new = np.stack([images, flip(images)], axis=1)\n    images_new = images_new.reshape(shape_new)\n    return images_new\n\ndef five_crop(images, size):\n    _n, _h, _w = images.shape[:3]\n    h, w = tuple(size)\n    assert h <= _h and w <= _w\n\n    shape_new = get_new_shape(images, size, n=5*_n)\n    images_new = []\n    images_new.append(images[:,:h,:w])\n    images_new.append(images[:,:h,-w:])\n    images_new.append(images[:,-h:,:w])\n    images_new.append(images[:,-h:,-w:])\n    images_new.append(center_crop(images, size))\n    images_new = np.stack(images_new, axis=1).reshape(shape_new)\n    return images_new\n\ndef ten_crop(images, size):\n    _n, _h, _w = images.shape[:3]\n    shape_new = get_new_shape(images, size, n=10*_n)\n    images_ = five_crop(images, size)\n    images_flip_ = five_crop(flip(images), size)\n    images_new = np.stack([images_, images_flip_], axis=1)\n    images_new = images_new.reshape(shape_new)\n    return images_new\n    \n    \n\nregister = {\n    \'resize\': resize,\n    \'padding\': padding,\n    \'random_crop\': random_crop,\n    \'center_crop\': center_crop,\n    \'random_flip\': random_flip,\n    \'standardize\': standardize_images,\n    \'random_shift\': random_shift,\n    \'random_interpolate\': random_interpolate,\n    \'random_downsample\': random_downsample,\n    \'expand_flip\': expand_flip,\n    \'five_crop\': five_crop,\n    \'ten_crop\': ten_crop,\n}\n\ndef preprocess(images, config, is_training=False):\n    # Load images first if they are file paths\n    if type(images[0]) == str:\n        image_paths = images\n        images = []\n        assert (config.channels==1 or config.channels==3)\n        mode = \'RGB\' if config.channels==3 else \'I\'\n        for image_path in image_paths:\n            images.append(misc.imread(image_path, mode=mode))\n        images = np.stack(images, axis=0)\n    else:\n        assert type(images) == np.ndarray\n        assert images.ndim == 4\n\n    # Process images\n    proc_funcs = config.preprocess_train if is_training else config.preprocess_test\n\n    for proc in proc_funcs:\n        proc_name, proc_args = proc[0], proc[1:]\n        assert proc_name in register, \\\n            ""Not a registered preprocessing function: {}"".format(proc_name)\n        images = register[proc_name](images, *proc_args)\n    if len(images.shape) == 3:\n        images = images[:,:,:,None]\n    return images\n        \n\n'"
utils/tflib.py,24,"b'\'\'\' Functions for tensorflow \'\'\'\n# MIT License\n# \n# Copyright (c) 2019 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport tensorflow as tf\n\ndef negative_MLS(X, Y, sigma_sq_X, sigma_sq_Y, mean=False):\n    with tf.name_scope(\'negative_MLS\'):\n        if mean:\n            D = X.shape[1].value\n\n            Y = tf.transpose(Y)\n            XX = tf.reduce_sum(tf.square(X), 1, keep_dims=True)\n            YY = tf.reduce_sum(tf.square(Y), 0, keep_dims=True)\n            XY = tf.matmul(X, Y)\n            diffs = XX + YY - 2*XY\n\n            sigma_sq_Y = tf.transpose(sigma_sq_Y)\n            sigma_sq_X = tf.reduce_mean(sigma_sq_X, axis=1, keep_dims=True)\n            sigma_sq_Y = tf.reduce_mean(sigma_sq_Y, axis=0, keep_dims=True)\n            sigma_sq_fuse = sigma_sq_X + sigma_sq_Y\n\n            diffs = diffs / (1e-8 + sigma_sq_fuse) + D * tf.log(sigma_sq_fuse)\n\n            return diffs\n        else:\n            D = X.shape[1].value\n            X = tf.reshape(X, [-1, 1, D])\n            Y = tf.reshape(Y, [1, -1, D])\n            sigma_sq_X = tf.reshape(sigma_sq_X, [-1, 1, D])\n            sigma_sq_Y = tf.reshape(sigma_sq_Y, [1, -1, D])\n            sigma_sq_fuse = sigma_sq_X + sigma_sq_Y\n            diffs = tf.square(X-Y) / (1e-10 + sigma_sq_fuse) + tf.log(sigma_sq_fuse)\n            return tf.reduce_sum(diffs, axis=2)\n\ndef mutual_likelihood_score_loss(labels, mu, log_sigma_sq):\n\n    with tf.name_scope(\'MLS_Loss\'):\n\n        batch_size = tf.shape(mu)[0]\n\n        diag_mask = tf.eye(batch_size, dtype=tf.bool)\n        non_diag_mask = tf.logical_not(diag_mask)\n\n        sigma_sq = tf.exp(log_sigma_sq)\n        loss_mat = negative_MLS(mu, mu, sigma_sq, sigma_sq)\n        \n        label_mat = tf.equal(labels[:,None], labels[None,:])\n        label_mask_pos = tf.logical_and(non_diag_mask, label_mat)\n\n        loss_pos = tf.boolean_mask(loss_mat, label_mask_pos) \n    \n        return tf.reduce_mean(loss_pos)\n'"
utils/utils.py,0,"b'""""""Utilities for training and testing\n""""""\n# MIT License\n# \n# Copyright (c) 2019 Yichun Shi\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport sys\nimport os\nimport numpy as np\nfrom scipy import misc\nimport time\nimport math\nimport random\nfrom datetime import datetime\nimport shutil\n\ndef create_log_dir(config, config_file):\n    subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n    log_dir = os.path.join(os.path.expanduser(config.log_base_dir), config.name, subdir)\n    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\'t exist\n        os.makedirs(log_dir)\n    shutil.copyfile(config_file, os.path.join(log_dir,\'config.py\'))\n\n    return log_dir\n\n\ndef get_updated_learning_rate(global_step, config):\n    if config.learning_rate_strategy == \'step\':\n        max_step = -1\n        learning_rate = 0.0\n        for step, lr in config.learning_rate_schedule.items():\n            if global_step >= step and step > max_step:\n                learning_rate = lr\n                max_step = step\n        if max_step == -1:\n            raise ValueError(\'cannot find learning rate for step %d\' % global_step)\n    elif config.learning_rate_strategy == \'cosine\':\n        initial = config.learning_rate_schedule[\'initial\']\n        interval = config.learning_rate_schedule[\'interval\']\n        end_step = config.learning_rate_schedule[\'end_step\']\n        step = math.floor(float(global_step) / interval) * interval\n        assert step <= end_step\n        learning_rate = initial * 0.5 * (math.cos(math.pi * step / end_step) + 1)\n    elif config.learning_rate_strategy == \'linear\':\n        initial = config.learning_rate_schedule[\'initial\']\n        start = config.learning_rate_schedule[\'start\']\n        end_step = config.learning_rate_schedule[\'end_step\']\n        assert global_step <= end_step\n        assert start < end_step\n        if global_step < start:\n            learning_rate = initial\n        else:\n            learning_rate = 1.0 * initial * (end_step - global_step) / (end_step - start)\n    else:\n        raise ValueError(""Unkown learning rate strategy!"")\n\n    return learning_rate\n\ndef display_info(epoch, step, duration, watch_list):\n    sys.stdout.write(\'[%d][%d] time: %2.2f\' % (epoch+1, step+1, duration))\n    for item in watch_list.items():\n        if type(item[1]) in [float, np.float32, np.float64]:\n            sys.stdout.write(\'   %s: %2.3f\' % (item[0], item[1]))\n        elif type(item[1]) in [int, bool, np.int32, np.int64, np.bool]:\n            sys.stdout.write(\'   %s: %d\' % (item[0], item[1]))\n    sys.stdout.write(\'\\n\')\n\ndef l2_normalize(x, axis=None, eps=1e-8):\n    x = x / (eps + np.linalg.norm(x, axis=axis))\n    return x\n\ndef pair_euc_score(x1, x2):\n    x1, x2 = np.array(x1), np.array(x2)\n    dist = np.sum(np.square(x1 - x2), axis=1)\n    return -dist\n\ndef pair_MLS_score(x1, x2, sigma_sq1=None, sigma_sq2=None):\n    if sigma_sq1 is None:\n        x1, x2 = np.array(x1), np.array(x2)\n        assert sigma_sq2 is None, \'either pass in concated features, or mu, sigma_sq for both!\'\n        D = int(x1.shape[1] / 2)\n        mu1, sigma_sq1 = x1[:,:D], x1[:,D:]\n        mu2, sigma_sq2 = x2[:,:D], x2[:,D:]\n    else:\n        x1, x2 = np.array(x1), np.array(x2)\n        sigma_sq1, sigma_sq2 = np.array(sigma_sq1), np.array(sigma_sq2)\n        mu1, mu2 = x1, x2\n    sigma_sq_mutual = sigma_sq1 + sigma_sq2\n    dist = np.sum(np.square(mu1 - mu2) / sigma_sq_mutual + np.log(sigma_sq_mutual), axis=1)\n    return -dist\n\n\ndef aggregate_PFE(x, sigma_sq=None, normalize=True, concatenate=False):\n    if sigma_sq is None:\n        D = int(x.shape[1] / 2)\n        mu, sigma_sq = x[:,:D], x[:,D:]\n    else:\n        mu = x\n    attention = 1. / sigma_sq\n    attention = attention / np.sum(attention, axis=0, keepdims=True)\n    \n    mu_new  = np.sum(mu * attention, axis=0)\n    sigma_sq_new = np.min(sigma_sq, axis=0)\n\n    if normalize:\n        mu_new = l2_normalize(mu_new)\n\n    if concatenate:\n        return np.concatenate([mu_new, sigma_sq_new])\n    else:\n        return mu_new, sigma_sq_new\n    \n'"
