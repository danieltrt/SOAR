file_path,api_count,code
keras_centernet/__init__.py,0,b''
tests/decode_test.py,0,"b""import pytest\nimport numpy as np\nfrom keras_centernet.models.decode import _ctdet_decode, _hpdet_decode\nfrom keras import backend as K\nimport os\nimport pickle\n\n\ndef test_ctdet_decode():\n  np.random.seed(32)\n  hm = np.random.randn(2, 64, 64, 80)\n  reg = np.random.randn(2, 64, 64, 2) * 10.0\n  wh = np.random.randn(2, 64, 64, 2) * 20.0\n\n  keras_hm = K.constant(hm)\n  keras_reg = K.constant(reg)\n  keras_wh = K.constant(wh)\n\n  keras_detections = K.eval(_ctdet_decode(keras_hm, keras_reg, keras_wh, output_stride=1))\n\n  gold_fn = 'tests/data/ctdet_decode_gold.p'\n  if not os.path.exists(gold_fn):\n    import torch as th\n    import sys\n    sys.path.append(os.path.expanduser('~/Pytorch/CenterNet/src'))\n    from lib.models.decode import ctdet_decode  # noqa\n    py_hm = th.from_numpy(hm.transpose(0, 3, 1, 2)).float()\n    py_hm.sigmoid_()\n    py_reg = th.from_numpy(reg.transpose(0, 3, 1, 2)).float()\n    py_wh = th.from_numpy(wh.transpose(0, 3, 1, 2)).float()\n    py_detections = ctdet_decode(py_hm, py_reg, py_wh).detach().numpy()\n    with open(gold_fn, 'wb') as f:\n      pickle.dump(py_detections, f)\n  else:\n    with open(gold_fn, 'rb') as f:\n      py_detections = pickle.load(f)\n  assert np.allclose(keras_detections, py_detections)\n\n\ndef test_hpdet_decode():\n  np.random.seed(32)\n  hm = np.random.randn(2, 64, 64, 1)\n  hm_hp = np.random.randn(2, 64, 64, 17)\n  hp_offset = np.random.randn(2, 64, 64, 2) * 10.0\n  kps = np.random.randn(2, 64, 64, 34) * 5.0\n  reg = np.random.randn(2, 64, 64, 2) * 10.0\n  wh = np.random.randn(2, 64, 64, 2) * 20.0\n\n  keras_hm = K.constant(hm)\n  keras_hm_hp = K.constant(hm_hp)\n  keras_hp_offset = K.constant(hp_offset)\n  keras_kps = K.constant(kps)\n  keras_reg = K.constant(reg)\n  keras_wh = K.constant(wh)\n\n  keras_detections = K.eval(_hpdet_decode(\n    keras_hm, keras_wh, keras_kps, keras_reg, keras_hm_hp, keras_hp_offset, output_stride=1))\n  gold_fn = 'tests/data/hpdet_decode_gold.p'\n  if not os.path.exists(gold_fn):\n    import torch as th\n    import sys\n    sys.path.append(os.path.expanduser('~/Pytorch/CenterNet/src'))\n    from lib.models.decode import multi_pose_decode as hpdet_decode  # noqa\n    py_hm = th.from_numpy(hm.transpose(0, 3, 1, 2)).float()\n    py_hm.sigmoid_()\n    py_hm_hp = th.from_numpy(hm_hp.transpose(0, 3, 1, 2)).float()\n    py_hm_hp.sigmoid_()\n    py_kps = th.from_numpy(kps.transpose(0, 3, 1, 2)).float()\n    py_reg = th.from_numpy(reg.transpose(0, 3, 1, 2)).float()\n    py_wh = th.from_numpy(wh.transpose(0, 3, 1, 2)).float()\n    py_hp_offset = th.from_numpy(hp_offset.transpose(0, 3, 1, 2)).float()\n    py_detections = hpdet_decode(py_hm, py_wh, py_kps, py_reg, py_hm_hp, py_hp_offset).detach().numpy()\n    py_kps = py_detections[..., 5:-1].copy()\n    py_kps_x = py_kps[..., ::2]\n    py_kps_y = py_kps[..., 1::2]\n    py_kps = np.concatenate([py_kps_x, py_kps_y], 2)\n    py_detections[..., 5:-1] = py_kps\n    with open(gold_fn, 'wb') as f:\n      pickle.dump(py_detections, f)\n  else:\n    with open(gold_fn, 'rb') as f:\n      py_detections = pickle.load(f)\n\n  assert np.allclose(keras_detections, py_detections)\n\n\nif __name__ == '__main__':\n  pytest.main([__file__])\n"""
tests/hourglass_test.py,0,"b'import pytest\nimport numpy as np\nimport pickle\nimport cv2\nfrom keras_centernet.models.networks.hourglass import HourglassNetwork, normalize_image\nimport sys\nsys.path.append(\'/home/steffen/Pytorch/CenterNet/src\')\nfrom lib.models.networks.large_hourglass import HourglassNet\nfrom lib.models.model import load_model\nimport torch as th\n\n\ndef load_input_gold():\n  inp = cv2.imread(\'tests/data/gold.png\')\n  with open(\'tests/data/output.p\', \'rb\') as f:\n    gold = pickle.load(f)\n  return inp, gold[\'hm\'], gold[\'reg\'], gold[\'wh\']\n\n\ndef test_hourglass_predict():\n  img, hm_gold, reg_gold, wh_gold = load_input_gold()\n  kwargs = {\n    \'num_stacks\': 2,\n    \'cnv_dim\': 256,\n    \'weights\': \'ctdet_coco\',\n    \'inres\': (256, 256),\n  }\n  heads = {\n    \'hm\': 80,\n    \'reg\': 2,\n    \'wh\': 2\n  }\n  model = HourglassNetwork(heads=heads, **kwargs)\n  pimg = np.expand_dims(normalize_image(img), 0)\n  output = model.predict(pimg)\n  hm, reg, wh = output[3:]\n  from IPython import embed; embed()\n  assert hm.shape == hm_gold.shape\n  assert reg.shape == reg_gold.shape\n  assert wh.shape == wh_gold.shape\n\n\ndef test_hourglass_output():\n  kwargs = {\n    \'num_stacks\': 2,\n    \'cnv_dim\': 256,\n    \'weights\': \'ctdet_coco\',\n    \'inres\': (256, 256),\n  }\n  heads = {\n    \'hm\': 80,\n    \'reg\': 2,\n    \'wh\': 2\n  }\n  py_model = HourglassNet(heads)\n  state_dict = th.load(\'/home/steffen/Pytorch/CenterNet/src/hg_weights.pth\')\n  py_model.load_state_dict(state_dict)\n  py_model = load_model(py_model, ""/home/steffen/Pytorch/CenterNet/models/ctdet_coco_hg.pth"")\n  py_model.eval()\n  device = \'cpu\'\n  py_model = py_model.to(device)\n  img = cv2.imread(\'tests/data/letterbox_gold.png\')\n  pimg = np.float32(img) / 127.5 - 1.0\n  pimg = np.expand_dims(pimg, 0)\n  py_pimg = th.from_numpy(pimg.transpose(0, 3, 1, 2))\n  py_pimg = py_pimg.to(device)\n  py_output = py_model(py_pimg)[-1]\n  py_hm = py_output[\'hm\'].detach().cpu().numpy().transpose(0, 2, 3, 1)\n  py_reg = py_output[\'reg\'].detach().cpu().numpy().transpose(0, 2, 3, 1)\n  py_wh = py_output[\'wh\'].detach().cpu().numpy().transpose(0, 2, 3, 1)\n\n  model = HourglassNetwork(heads=heads, **kwargs)\n  output = model.predict(pimg)\n  hm = output[3]\n  reg = output[4]\n  wh = output[5]\n\n  print(""HM: "", py_hm.mean().round(3), py_hm.std().round(3).round(3), py_hm.min().round(3), py_hm.max().round(3), "" | "", hm.mean().round(3), hm.std().round(3).round(3), hm.min().round(3), hm.max().round(3))  # noqa\n  print(""REG: "", py_reg.mean().round(3), py_reg.std().round(3).round(3), py_reg.min().round(3), py_reg.max().round(3), "" | "", reg.mean().round(3), reg.std().round(3).round(3), reg.min().round(3), reg.max().round(3))  # noqa\n  print(""WH: "", py_wh.mean().round(3), py_wh.std().round(3).round(3), py_wh.min().round(3), py_wh.max().round(3), "" | "", wh.mean().round(3), wh.std().round(3).round(3), wh.min().round(3), wh.max().round(3))  # noqa\n\n  from IPython import embed; embed()\n\n\nif __name__ == \'__main__\':\n  test_hourglass_output()\n  pytest.main([__file__])\n'"
tests/letterbox_test.py,0,"b'import pytest\nimport numpy as np\nimport cv2\nfrom keras_centernet.utils import letterbox as lb\n\n\ndef load_input_gold():\n  inp = cv2.imread(\'tests/data/input.png\')\n  gold = cv2.imread(\'tests/data/letterbox_gold.png\')\n  return inp, gold\n\n\ndef test_letterbox_shape_training():\n  lt = lb.LetterboxTransformer(256, 256)\n  inp, gold = load_input_gold()\n  letterbox = lt(inp)\n  assert gold.shape == letterbox.shape\n\n\ndef test_letterbox_data_training():\n  lt = lb.LetterboxTransformer(256, 256)\n  inp, gold = load_input_gold()\n  letterbox = lt(inp)\n  assert np.all(gold == letterbox)\n\n\ndef test_letterbox_shape_testing():\n  lt = lb.LetterboxTransformer(mode=\'testing\', max_stride=128)\n  test_shapes = {\n    (427, 640, 3): (512, 768, 3),\n    (230, 352, 3): (256, 384, 3),\n    (428, 640, 3): (512, 768, 3),\n  }\n  for in_shape, out_shape in test_shapes.items():\n    inp = np.zeros(in_shape, dtype=\'float32\')\n    output = lt(inp)\n    assert output.shape == out_shape, ""Expected %s got %s"" % (out_shape, output.shape)\n    inp_recovered = cv2.warpAffine(output, lt.M_inv, in_shape[:2][::-1])\n    assert inp_recovered.shape == in_shape, ""Expected %s got %s"" % (in_shape, inp_recovered.shape)\n\n\ndef test_invert_transform():\n  M = np.float32([[1.2, 0.0, 32.0], [0.0, 0.8, -4.0]])\n  assert np.all(M == lb.invert_transform(lb.invert_transform(M)))\n\n\ndef test_affine_transform():\n  np.random.seed(32)\n  M = np.float32([[1.2, 0.0, 32.0], [0.0, 0.8, -4.0]])\n  M_inv = lb.invert_transform(M)\n  coords = np.random.randn(2, 12) + 5.0 * 20.0\n  coords_transformed = coords.copy()\n  coords_transformed = lb.affine_transform_coords(coords_transformed, M)\n  coords_transformed = lb.affine_transform_coords(coords_transformed, M_inv)\n  assert np.allclose(coords, coords_transformed)\n\n\nif __name__ == \'__main__\':\n  pytest.main([__file__])\n'"
keras_centernet/bin/ctdet_coco.py,0,"b'#!/usr/bin/env python3\nimport argparse\nimport cv2\nfrom glob import glob\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport json\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nfrom keras_centernet.models.networks.hourglass import HourglassNetwork, normalize_image\nfrom keras_centernet.models.decode import CtDetDecode\nfrom keras_centernet.utils.letterbox import LetterboxTransformer\n\n# https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/datasets/coco.py\nCOCO_IDS = [0,\n            1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17,\n            18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36,\n            37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53,\n            54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, 72, 73,\n            74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--output\', default=\'output\', type=str)\n  parser.add_argument(\'--data\', default=\'val2017\', type=str)\n  parser.add_argument(\'--annotations\', default=\'annotations\', type=str)\n  parser.add_argument(\'--inres\', default=\'512,512\', type=str)\n  parser.add_argument(\'--no-full-resolution\', action=\'store_true\')\n  args, _ = parser.parse_known_args()\n  args.inres = tuple(int(x) for x in args.inres.split(\',\'))\n  if not args.no_full_resolution:\n    args.inres = (None, None)\n\n  os.makedirs(args.output, exist_ok=True)\n  kwargs = {\n    \'num_stacks\': 2,\n    \'cnv_dim\': 256,\n    \'weights\': \'ctdet_coco\',\n    \'inres\': args.inres,\n  }\n  heads = {\n    \'hm\': 80,\n    \'reg\': 2,\n    \'wh\': 2\n  }\n  out_fn_box = os.path.join(args.output, args.data + \'_bbox_results_%s_%s.json\' % (args.inres[0], args.inres[1]))\n  model = HourglassNetwork(heads=heads, **kwargs)\n  model = CtDetDecode(model)\n  if args.no_full_resolution:\n    letterbox_transformer = LetterboxTransformer(args.inres[0], args.inres[1])\n  else:\n    letterbox_transformer = LetterboxTransformer(mode=\'testing\', max_stride=128)\n\n  fns = sorted(glob(os.path.join(args.data, \'*.jpg\')))\n  results = []\n  for fn in tqdm(fns):\n    img = cv2.imread(fn)\n    image_id = int(os.path.splitext(os.path.basename(fn))[0])\n    pimg = letterbox_transformer(img)\n    pimg = normalize_image(pimg)\n    pimg = np.expand_dims(pimg, 0)\n    detections = model.predict(pimg)[0]\n    for d in detections:\n      x1, y1, x2, y2, score, cl = d\n      # if score < 0.001:\n      #   break\n      x1, y1, x2, y2 = letterbox_transformer.correct_box(x1, y1, x2, y2)\n      cl = int(cl)\n      x1, y1, x2, y2 = float(x1), float(y1), float(x2), float(y2)\n      image_result = {\n        \'image_id\': image_id,\n        \'category_id\': COCO_IDS[cl + 1],\n        \'score\': float(score),\n        \'bbox\': [x1, y1, (x2 - x1), (y2 - y1)],\n      }\n      results.append(image_result)\n\n  if not len(results):\n    print(""No predictions were generated."")\n    return\n\n  # write output\n  with open(out_fn_box, \'w\') as f:\n    json.dump(results, f, indent=2)\n  print(""Predictions saved to: %s"" % out_fn_box)\n  # load results in COCO evaluation tool\n  gt_fn = os.path.join(args.annotations, \'instances_%s.json\' % args.data)\n  print(""Loading GT: %s"" % gt_fn)\n  coco_true = COCO(gt_fn)\n  coco_pred = coco_true.loadRes(out_fn_box)\n\n  # run COCO evaluation\n  coco_eval = COCOeval(coco_true, coco_pred, \'bbox\')\n  coco_eval.evaluate()\n  coco_eval.accumulate()\n  coco_eval.summarize()\n  return coco_eval.stats\n\n\nif __name__ == \'__main__\':\n  main()\n'"
keras_centernet/bin/ctdet_image.py,0,"b'#!/usr/bin/env python3\nimport argparse\nimport cv2\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nfrom glob import glob\n\nfrom keras_centernet.models.networks.hourglass import HourglassNetwork, normalize_image\nfrom keras_centernet.models.decode import CtDetDecode\nfrom keras_centernet.utils.utils import COCODrawer\nfrom keras_centernet.utils.letterbox import LetterboxTransformer\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--fn\', default=\'assets/demo.jpg\', type=str)\n  parser.add_argument(\'--output\', default=\'output\', type=str)\n  parser.add_argument(\'--inres\', default=\'512,512\', type=str)\n  args, _ = parser.parse_known_args()\n  args.inres = tuple(int(x) for x in args.inres.split(\',\'))\n  os.makedirs(args.output, exist_ok=True)\n  kwargs = {\n    \'num_stacks\': 2,\n    \'cnv_dim\': 256,\n    \'weights\': \'ctdet_coco\',\n    \'inres\': args.inres,\n  }\n  heads = {\n    \'hm\': 80,  # 3\n    \'reg\': 2,  # 4\n    \'wh\': 2  # 5\n  }\n  model = HourglassNetwork(heads=heads, **kwargs)\n  model = CtDetDecode(model)\n  drawer = COCODrawer()\n  fns = sorted(glob(args.fn))\n  for fn in tqdm(fns):\n    img = cv2.imread(fn)\n    letterbox_transformer = LetterboxTransformer(args.inres[0], args.inres[1])\n    pimg = letterbox_transformer(img)\n    pimg = normalize_image(pimg)\n    pimg = np.expand_dims(pimg, 0)\n    detections = model.predict(pimg)[0]\n    for d in detections:\n      x1, y1, x2, y2, score, cl = d\n      if score < 0.3:\n        break\n      x1, y1, x2, y2 = letterbox_transformer.correct_box(x1, y1, x2, y2)\n      img = drawer.draw_box(img, x1, y1, x2, y2, cl)\n\n    out_fn = os.path.join(args.output, \'ctdet.\' + os.path.basename(fn))\n    cv2.imwrite(out_fn, img)\n    print(""Image saved to: %s"" % out_fn)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
keras_centernet/bin/ctdet_video.py,0,"b'#!/usr/bin/env python3\nimport argparse\nimport cv2\nimport numpy as np\nimport os\nimport time\n\nfrom keras_centernet.models.networks.hourglass import HourglassNetwork, normalize_image\nfrom keras_centernet.models.decode import CtDetDecode\nfrom keras_centernet.utils.utils import COCODrawer\nfrom keras_centernet.utils.letterbox import LetterboxTransformer\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--video\', default=\'webcam\', type=str)\n  parser.add_argument(\'--output\', default=\'output\', type=str)\n  parser.add_argument(\'--inres\', default=\'512,512\', type=str)\n  parser.add_argument(\'--outres\', default=\'1080,1920\', type=str)\n  parser.add_argument(\'--max-frames\', default=1000000, type=int)\n  parser.add_argument(\'--fps\', default=25.0 * 1.0, type=float)\n  args, _ = parser.parse_known_args()\n  args.inres = tuple(int(x) for x in args.inres.split(\',\'))\n  args.outres = tuple(int(x) for x in args.outres.split(\',\'))\n  os.makedirs(args.output, exist_ok=True)\n  kwargs = {\n    \'num_stacks\': 2,\n    \'cnv_dim\': 256,\n    \'weights\': \'ctdet_coco\',\n    \'inres\': args.inres,\n  }\n  heads = {\n    \'hm\': 80,  # 3\n    \'reg\': 2,  # 4\n    \'wh\': 2  # 5\n  }\n  model = HourglassNetwork(heads=heads, **kwargs)\n  model = CtDetDecode(model)\n  drawer = COCODrawer()\n  letterbox_transformer = LetterboxTransformer(args.inres[0], args.inres[1])\n  cap = cv2.VideoCapture(0 if args.video == \'webcam\' else args.video)\n  out_fn = os.path.join(args.output, \'ctdet.\' + os.path.basename(args.video)).replace(\'.mp4\', \'.avi\')\n  fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n  out = cv2.VideoWriter(out_fn, fourcc, args.fps, args.outres[::-1])\n  k = 0\n  tic = time.time()\n  while cap.isOpened():\n    if k > args.max_frames:\n      print(""Bye"")\n      break\n    if k > 0 and k % 100 == 0:\n      toc = time.time()\n      duration = toc - tic\n      print(""[%05d]: %.3f seconds / 100 iterations"" % (k, duration))\n      tic = toc\n\n    k += 1\n    ret, img = cap.read()\n    if not ret:\n      print(""Done"")\n      break\n    pimg = letterbox_transformer(img)\n    pimg = normalize_image(pimg)\n    pimg = np.expand_dims(pimg, 0)\n    detections = model.predict(pimg)[0]\n    for d in detections:\n      x1, y1, x2, y2, score, cl = d\n      if score < 0.3:\n        break\n      x1, y1, x2, y2 = letterbox_transformer.correct_box(x1, y1, x2, y2)\n      img = drawer.draw_box(img, x1, y1, x2, y2, cl)\n\n    out.write(img)\n  print(""Video saved to: %s"" % out_fn)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
keras_centernet/bin/hpdet_coco.py,0,"b'#!/usr/bin/env python3\nimport argparse\nimport cv2\nfrom glob import glob\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport json\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nfrom keras_centernet.models.networks.hourglass import HourglassNetwork, normalize_image\nfrom keras_centernet.models.decode import HpDetDecode\nfrom keras_centernet.utils.letterbox import LetterboxTransformer\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--output\', default=\'output\', type=str)\n  parser.add_argument(\'--data\', default=\'val2017\', type=str)\n  parser.add_argument(\'--annotations\', default=\'annotations\', type=str)\n  parser.add_argument(\'--inres\', default=\'512,512\', type=str)\n  parser.add_argument(\'--no-full-resolution\', action=\'store_true\')\n  args, _ = parser.parse_known_args()\n  args.inres = tuple(int(x) for x in args.inres.split(\',\'))\n  if not args.no_full_resolution:\n    args.inres = (None, None)\n\n  os.makedirs(args.output, exist_ok=True)\n  kwargs = {\n    \'num_stacks\': 2,\n    \'cnv_dim\': 256,\n    \'weights\': \'hpdet_coco\',\n    \'inres\': args.inres,\n  }\n  heads = {\n    \'hm\': 1,  # 6\n    \'hm_hp\': 17,  # 7\n    \'hp_offset\': 2,  # 8\n    \'hps\': 34,  # 9\n    \'reg\': 2,  # 10\n    \'wh\': 2,  # 11\n  }\n  out_fn_keypoints = os.path.join(args.output, args.data + \'_keypoints_results_%s_%s.json\' % (\n      args.inres[0], args.inres[1]))\n  model = HourglassNetwork(heads=heads, **kwargs)\n  model = HpDetDecode(model)\n  if args.no_full_resolution:\n    letterbox_transformer = LetterboxTransformer(args.inres[0], args.inres[1])\n  else:\n    letterbox_transformer = LetterboxTransformer(mode=\'testing\', max_stride=128)\n\n  fns = sorted(glob(os.path.join(args.data, \'*.jpg\')))\n  results = []\n  for fn in tqdm(fns):\n    img = cv2.imread(fn)\n    image_id = int(os.path.splitext(os.path.basename(fn))[0])\n    pimg = letterbox_transformer(img)\n    pimg = normalize_image(pimg)\n    pimg = np.expand_dims(pimg, 0)\n    detections = model.predict(pimg)[0]\n    for d in detections:\n      score = d[4]\n      x1, y1, x2, y2 = d[:4]\n      x1, y1, x2, y2 = letterbox_transformer.correct_box(x1, y1, x2, y2)\n      x1, y1, x2, y2 = float(x1), float(y1), float(x2), float(y2)\n\n      kps = d[5:-1]\n      kps_x = kps[:17]\n      kps_y = kps[17:]\n      kps = letterbox_transformer.correct_coords(np.vstack([kps_x, kps_y])).T\n      # add z = 1\n      kps = np.concatenate([kps, np.ones((17, 1), dtype=\'float32\')], -1)\n      kps = list(map(float, kps.flatten()))\n\n      image_result = {\n        \'image_id\': image_id,\n        \'category_id\': 1,\n        \'score\': float(score),\n        \'bbox\': [x1, y1, (x2 - x1), (y2 - y1)],\n        \'keypoints\': kps,\n      }\n      results.append(image_result)\n\n  if not len(results):\n    print(""No predictions were generated."")\n    return\n\n  # write output\n  with open(out_fn_keypoints, \'w\') as f:\n    json.dump(results, f, indent=2)\n  print(""Predictions saved to: %s"" % out_fn_keypoints)\n  # load results in COCO evaluation tool\n  gt_fn = os.path.join(args.annotations, \'person_keypoints_%s.json\' % args.data)\n  print(""Loading GT: %s"" % gt_fn)\n  coco_true = COCO(gt_fn)\n  coco_pred = coco_true.loadRes(out_fn_keypoints)\n  coco_eval = COCOeval(coco_true, coco_pred, \'keypoints\')\n  coco_eval.evaluate()\n  coco_eval.accumulate()\n  coco_eval.summarize()\n\n  return coco_eval.stats\n\n\nif __name__ == \'__main__\':\n  main()\n'"
keras_centernet/bin/hpdet_image.py,0,"b'#!/usr/bin/env python3\nimport argparse\nimport cv2\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nfrom glob import glob\n\nfrom keras_centernet.models.networks.hourglass import HourglassNetwork, normalize_image\nfrom keras_centernet.models.decode import HpDetDecode\nfrom keras_centernet.utils.utils import COCODrawer\nfrom keras_centernet.utils.letterbox import LetterboxTransformer\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--fn\', default=\'assets/demo.jpg\', type=str)\n  parser.add_argument(\'--output\', default=\'output\', type=str)\n  parser.add_argument(\'--inres\', default=\'512,512\', type=str)\n  args, _ = parser.parse_known_args()\n  args.inres = tuple(int(x) for x in args.inres.split(\',\'))\n  os.makedirs(args.output, exist_ok=True)\n  kwargs = {\n    \'num_stacks\': 2,\n    \'cnv_dim\': 256,\n    \'weights\': \'hpdet_coco\',\n    \'inres\': args.inres,\n  }\n  heads = {\n    \'hm\': 1,  # 6\n    \'hm_hp\': 17,  # 7\n    \'hp_offset\': 2,  # 8\n    \'hps\': 34,  # 9\n    \'reg\': 2,  # 10\n    \'wh\': 2,  # 11\n  }\n  model = HourglassNetwork(heads=heads, **kwargs)\n  model = HpDetDecode(model)\n  drawer = COCODrawer()\n  fns = sorted(glob(args.fn))\n  for fn in tqdm(fns):\n    img = cv2.imread(fn)\n    letterbox_transformer = LetterboxTransformer(args.inres[0], args.inres[1])\n    pimg = letterbox_transformer(img)\n    pimg = normalize_image(pimg)\n    pimg = np.expand_dims(pimg, 0)\n    detections = model.predict(pimg)[0]\n    for d in detections:\n      score, cl = d[4], d[-1]\n      if score < 0.3:\n        break\n      x1, y1, x2, y2 = d[:4]\n      kps = d[5:-1]\n      kps_x = kps[:17]\n      kps_y = kps[17:]\n      kps = letterbox_transformer.correct_coords(np.vstack([kps_x, kps_y])).T\n      x1, y1, x2, y2 = letterbox_transformer.correct_box(x1, y1, x2, y2)\n      img = drawer.draw_pose(img, kps)\n      img = drawer.draw_box(img, x1, y1, x2, y2, cl)\n\n    out_fn = os.path.join(args.output, \'hpdet.\' + os.path.basename(fn))\n    cv2.imwrite(out_fn, img)\n    print(""Image saved to: %s"" % out_fn)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
keras_centernet/bin/hpdet_video.py,0,"b'#!/usr/bin/env python3\nimport argparse\nimport cv2\nimport numpy as np\nimport os\nimport time\n\nfrom keras_centernet.models.networks.hourglass import HourglassNetwork, normalize_image\nfrom keras_centernet.models.decode import HpDetDecode\nfrom keras_centernet.utils.utils import COCODrawer\nfrom keras_centernet.utils.letterbox import LetterboxTransformer\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--video\', default=\'webcam\', type=str)\n  parser.add_argument(\'--output\', default=\'output\', type=str)\n  parser.add_argument(\'--inres\', default=\'512,512\', type=str)\n  parser.add_argument(\'--outres\', default=\'1080,1920\', type=str)\n  parser.add_argument(\'--max-frames\', default=1000000, type=int)\n  parser.add_argument(\'--fps\', default=25.0 * 1.0, type=float)\n  args, _ = parser.parse_known_args()\n  args.inres = tuple(int(x) for x in args.inres.split(\',\'))\n  args.outres = tuple(int(x) for x in args.outres.split(\',\'))\n  os.makedirs(args.output, exist_ok=True)\n  kwargs = {\n    \'num_stacks\': 2,\n    \'cnv_dim\': 256,\n    \'weights\': \'hpdet_coco\',\n    \'inres\': args.inres,\n  }\n  heads = {\n    \'hm\': 1,  # 6\n    \'hm_hp\': 17,  # 7\n    \'hp_offset\': 2,  # 8\n    \'hps\': 34,  # 9\n    \'reg\': 2,  # 10\n    \'wh\': 2,  # 11\n  }\n  model = HourglassNetwork(heads=heads, **kwargs)\n  model = HpDetDecode(model)\n  drawer = COCODrawer()\n  letterbox_transformer = LetterboxTransformer(args.inres[0], args.inres[1])\n  cap = cv2.VideoCapture(0 if args.video == \'webcam\' else args.video)\n  out_fn = os.path.join(args.output, \'hpdet.\' + os.path.basename(args.video)).replace(\'.mp4\', \'.avi\')\n  fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n  out = cv2.VideoWriter(out_fn, fourcc, args.fps, args.outres[::-1])\n  k = 0\n  tic = time.time()\n  while cap.isOpened():\n    if k > args.max_frames:\n      print(""Bye"")\n      break\n    if k > 0 and k % 100 == 0:\n      toc = time.time()\n      duration = toc - tic\n      print(""[%05d]: %.3f seconds / 100 iterations"" % (k, duration))\n      tic = toc\n\n    k += 1\n    ret, img = cap.read()\n    if not ret:\n      print(""Done"")\n      break\n    pimg = letterbox_transformer(img)\n    pimg = normalize_image(pimg)\n    pimg = np.expand_dims(pimg, 0)\n    detections = model.predict(pimg)[0]\n    for d in detections:\n      score, cl = d[4], d[-1]\n      if score < 0.3:\n        break\n      x1, y1, x2, y2 = d[:4]\n      kps = d[5:-1]\n      kps_x = kps[:17]\n      kps_y = kps[17:]\n      kps = letterbox_transformer.correct_coords(np.vstack([kps_x, kps_y])).T\n      x1, y1, x2, y2 = letterbox_transformer.correct_box(x1, y1, x2, y2)\n      img = drawer.draw_pose(img, kps)\n      img = drawer.draw_box(img, x1, y1, x2, y2, cl)\n\n    out.write(img)\n  print(""Video saved to: %s"" % out_fn)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
keras_centernet/models/decode.py,3,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.layers import Lambda\nimport tensorflow as tf\n\n\ndef _nms(heat, kernel=3):\n  hmax = K.pool2d(heat, (kernel, kernel), padding='same', pool_mode='max')\n  keep = K.cast(K.equal(hmax, heat), K.floatx())\n  return heat * keep\n\n\ndef _ctdet_decode(hm, reg, wh, k=100, output_stride=4):\n  hm = K.sigmoid(hm)\n  hm = _nms(hm)\n  hm_shape = K.shape(hm)\n  reg_shape = K.shape(reg)\n  wh_shape = K.shape(wh)\n  batch, width, cat = hm_shape[0], hm_shape[2], hm_shape[3]\n\n  hm_flat = K.reshape(hm, (batch, -1))\n  reg_flat = K.reshape(reg, (reg_shape[0], -1, reg_shape[-1]))\n  wh_flat = K.reshape(wh, (wh_shape[0], -1, wh_shape[-1]))\n\n  def _process_sample(args):\n    _hm, _reg, _wh = args\n    _scores, _inds = tf.math.top_k(_hm, k=k, sorted=True)\n    _classes = K.cast(_inds % cat, 'float32')\n    _inds = K.cast(_inds / cat, 'int32')\n    _xs = K.cast(_inds % width, 'float32')\n    _ys = K.cast(K.cast(_inds / width, 'int32'), 'float32')\n    _wh = K.gather(_wh, _inds)\n    _reg = K.gather(_reg, _inds)\n\n    _xs = _xs + _reg[..., 0]\n    _ys = _ys + _reg[..., 1]\n\n    _x1 = _xs - _wh[..., 0] / 2\n    _y1 = _ys - _wh[..., 1] / 2\n    _x2 = _xs + _wh[..., 0] / 2\n    _y2 = _ys + _wh[..., 1] / 2\n\n    # rescale to image coordinates\n    _x1 = output_stride * _x1\n    _y1 = output_stride * _y1\n    _x2 = output_stride * _x2\n    _y2 = output_stride * _y2\n\n    _detection = K.stack([_x1, _y1, _x2, _y2, _scores, _classes], -1)\n    return _detection\n\n  detections = K.map_fn(_process_sample, [hm_flat, reg_flat, wh_flat], dtype=K.floatx())\n  return detections\n\n\ndef CtDetDecode(model, hm_index=3, reg_index=4, wh_index=5, k=100, output_stride=4):\n  def _decode(args):\n    hm, reg, wh = args\n    return _ctdet_decode(hm, reg, wh, k=k, output_stride=output_stride)\n  output = Lambda(_decode)([model.outputs[i] for i in [hm_index, reg_index, wh_index]])\n  model = Model(model.input, output)\n  return model\n\n\ndef _hpdet_decode(hm, wh, kps, reg, hm_hp, hp_offset, k=100, output_stride=4):\n  hm = K.sigmoid(hm)\n  hm = _nms(hm)\n  hm_shape = K.shape(hm)\n  reg_shape = K.shape(reg)\n  wh_shape = K.shape(wh)\n  kps_shape = K.shape(kps)\n  batch, width, cat = hm_shape[0], hm_shape[2], hm_shape[3]\n\n  hm_flat = K.reshape(hm, (batch, -1))\n  reg_flat = K.reshape(reg, (reg_shape[0], -1, reg_shape[-1]))\n  wh_flat = K.reshape(wh, (wh_shape[0], -1, wh_shape[-1]))\n  kps_flat = K.reshape(kps, (kps_shape[0], -1, kps_shape[-1]))\n\n  hm_hp = K.sigmoid(hm_hp)\n  hm_hp = _nms(hm_hp)\n  hm_hp_shape = K.shape(hm_hp)\n  hp_offset_shape = K.shape(hp_offset)\n\n  hm_hp_flat = K.reshape(hm_hp, (hm_hp_shape[0], -1, hm_hp_shape[-1]))\n  hp_offset_flat = K.reshape(hp_offset, (hp_offset_shape[0], -1, hp_offset_shape[-1]))\n\n  def _process_sample(args):\n    _hm, _reg, _wh, _kps, _hm_hp, _hp_offset = args\n    _scores, _inds = tf.math.top_k(_hm, k=k, sorted=True)\n    _classes = K.cast(_inds % cat, 'float32')\n    _inds = K.cast(_inds / cat, 'int32')\n    _xs = K.cast(_inds % width, 'float32')\n    _ys = K.cast(K.cast(_inds / width, 'int32'), 'float32')\n    _wh = K.gather(_wh, _inds)\n    _reg = K.gather(_reg, _inds)\n    _kps = K.gather(_kps, _inds)\n\n    # shift keypoints by their center\n    _kps_x = _kps[:, ::2]\n    _kps_y = _kps[:, 1::2]\n    _kps_x = _kps_x + K.expand_dims(_xs, -1)  # k x J\n    _kps_y = _kps_y + K.expand_dims(_ys, -1)  # k x J\n    _kps = K.stack([_kps_x, _kps_y], -1)  # k x J x 2\n\n    _xs = _xs + _reg[..., 0]\n    _ys = _ys + _reg[..., 1]\n\n    _x1 = _xs - _wh[..., 0] / 2\n    _y1 = _ys - _wh[..., 1] / 2\n    _x2 = _xs + _wh[..., 0] / 2\n    _y2 = _ys + _wh[..., 1] / 2\n\n    # snap center keypoints to the closest heatmap keypoint\n    def _process_channel(args):\n      __kps, __hm_hp = args\n      thresh = 0.1\n      __hm_scores, __hm_inds = tf.math.top_k(__hm_hp, k=k, sorted=True)\n      __hm_xs = K.cast(__hm_inds % width, 'float32')\n      __hm_ys = K.cast(K.cast(__hm_inds / width, 'int32'), 'float32')\n      __hp_offset = K.gather(_hp_offset, __hm_inds)\n      __hm_xs = __hm_xs + __hp_offset[..., 0]\n      __hm_ys = __hm_ys + __hp_offset[..., 1]\n      mask = K.cast(__hm_scores > thresh, 'float32')\n      __hm_scores = (1. - mask) * -1. + mask * __hm_scores\n      __hm_xs = (1. - mask) * -10000. + mask * __hm_xs\n      __hm_ys = (1. - mask) * -10000. + mask * __hm_ys\n      __hm_kps = K.stack([__hm_xs, __hm_ys], -1)  # k x 2\n      __broadcast_hm_kps = K.expand_dims(__hm_kps, 1)  # k x 1 x 2\n      __broadcast_kps = K.expand_dims(__kps, 0)  # 1 x k x 2\n      dist = K.sqrt(K.sum(K.pow(__broadcast_kps - __broadcast_hm_kps, 2), 2))  # k, k\n      min_dist = K.min(dist, 0)\n      min_ind = K.argmin(dist, 0)\n      __hm_scores = K.gather(__hm_scores, min_ind)\n      __hm_kps = K.gather(__hm_kps, min_ind)\n      mask = (K.cast(__hm_kps[..., 0] < _x1, 'float32') + K.cast(__hm_kps[..., 0] > _x2, 'float32') +\n              K.cast(__hm_kps[..., 1] < _y1, 'float32') + K.cast(__hm_kps[..., 1] > _y2, 'float32') +\n              K.cast(__hm_scores < thresh, 'float32') +\n              K.cast(min_dist > 0.3 * (K.maximum(_wh[..., 0], _wh[..., 1])), 'float32'))\n      mask = K.expand_dims(mask, -1)\n      mask = K.cast(mask > 0, 'float32')\n      __kps = (1. - mask) * __hm_kps + mask * __kps\n      return __kps\n\n    _kps = K.permute_dimensions(_kps, (1, 0, 2))  # J x k x 2\n    _hm_hp = K.permute_dimensions(_hm_hp, (1, 0))  # J x -1\n    _kps = K.map_fn(_process_channel, [_kps, _hm_hp], dtype='float32')\n    _kps = K.reshape(K.permute_dimensions(_kps, (1, 2, 0)), (k, -1))  # k x J * 2\n\n    # rescale to image coordinates\n    _x1 = output_stride * _x1\n    _y1 = output_stride * _y1\n    _x2 = output_stride * _x2\n    _y2 = output_stride * _y2\n    _kps = output_stride * _kps\n\n    _boxes = K.stack([_x1, _y1, _x2, _y2], -1)\n    _scores = K.expand_dims(_scores, -1)\n    _classes = K.expand_dims(_classes, -1)\n    _detection = K.concatenate([_boxes, _scores, _kps, _classes], -1)\n    return _detection\n\n  detections = K.map_fn(_process_sample,\n                        [hm_flat, reg_flat, wh_flat, kps_flat, hm_hp_flat, hp_offset_flat], dtype='float32')\n  return detections\n\n\ndef HpDetDecode(model, hm_index=6, wh_index=11, kps_index=9, reg_index=10, hm_hp_index=7, hp_offset_index=8,\n                k=100, output_stride=4):\n  def _decode(args):\n    hm, wh, kps, reg, hm_hp, hp_offset = args\n    return _hpdet_decode(hm, wh, kps, reg, hm_hp, hp_offset, k=k, output_stride=output_stride)\n\n  output = Lambda(_decode)(\n    [model.outputs[i] for i in [hm_index, wh_index, kps_index, reg_index, hm_hp_index, hp_offset_index]])\n  model = Model(model.input, output)\n  return model\n"""
keras_centernet/utils/letterbox.py,0,"b'import cv2\nimport numpy as np\n\n\ndef training_transform(height, width, output_height, output_width):\n  # https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/warp_affine/warp_affine.html\n  height_scale, width_scale = output_height / height, output_width / width\n  scale = min(height_scale, width_scale)\n  resize_height, resize_width = round(height * scale), round(width * scale)\n  pad_top = (output_height - resize_height) // 2\n  pad_left = (output_width - resize_width) // 2\n  A = np.float32([[scale, 0.0], [0.0, scale]])\n  B = np.float32([[pad_left], [pad_top]])\n  M = np.hstack([A, B])\n  return M, output_height, output_width\n\n\ndef testing_transform(height, width, max_stride):\n  h_pad, w_pad = round(height / max_stride + 0.51) * max_stride, round(width / max_stride + 0.51) * max_stride\n  pad_left = (w_pad - width) // 2\n  pad_top = (h_pad - height) // 2\n  A = np.eye(2, dtype=\'float32\')\n  B = np.float32([[pad_left], [pad_top]])\n  M = np.hstack([A, B])\n  return M, h_pad, w_pad\n\n\ndef invert_transform(M):\n  # T = A @ x + B => x = A_inv @ (T - B) = A_inv @ T + (-A_inv @ B)\n  A_inv = np.float32([[1. / M[0, 0], 0.0], [0.0, 1. / M[1, 1]]])\n  B_inv = -A_inv @ M[:, 2:3]\n  M_inv = np.hstack([A_inv, B_inv])\n  return M_inv\n\n\ndef affine_transform_coords(coords, M):\n  A, B = M[:2, :2], M[:2, 2:3]\n  transformed_coords = A @ coords + B\n  return transformed_coords\n\n\nclass LetterboxTransformer:\n  def __init__(self, height=None, width=None, mode=\'training\', max_stride=128):\n    """"""Resize the input images. For `mode=\'training\'` the resolution is fixed to `height` x `width`.\n       The resolution is changed but the aspect ratio is kept.\n       In `mode=\'testing\'` the input is padded to the next bigger multiple of `max_stride` of the network.\n       The orginal resolutions is thus kept.""""""\n    self.height = height\n    self.width = width\n    self.mode = mode\n    self.max_stride = max_stride\n    self.M = None\n    self.M_inv = None\n\n  def __call__(self, image):\n    h, w = image.shape[:2]\n    if self.mode == \'training\':\n      M, h_out, w_out = training_transform(h, w, self.height, self.width)\n    elif self.mode == \'testing\':\n      M, h_out, w_out = testing_transform(h, w, self.max_stride)\n\n    # https://answers.opencv.org/question/33516/cv2warpaffine-results-in-an-image-shifted-by-05-pixel\n    # This is different from `cv2.resize(image, (resize_width, resize_height))` & pad\n    letterbox = cv2.warpAffine(image, M, (w_out, h_out))\n    self.M = M\n    self.M_inv = invert_transform(M)\n    return letterbox\n\n  def correct_box(self, x1, y1, x2, y2):\n    coords = np.float32([[x1, x2], [y1, y2]])\n    coords = affine_transform_coords(coords, self.M_inv)\n    x1, y1, x2, y2 = coords[0, 0], coords[1, 0], coords[0, 1], coords[1, 1]\n    return x1, y1, x2, y2\n\n  def correct_coords(self, coords):\n    coords = affine_transform_coords(coords, self.M_inv)\n    return coords\n'"
keras_centernet/utils/utils.py,0,"b'import math\nimport cv2\nfrom PIL import ImageFont, ImageDraw, Image\nimport numpy as np\n\ncoco_names = [\'person\', \'bicycle\', \'car\', \'motorbike\', \'aeroplane\', \'bus\', \'train\', \'truck\', \'boat\', \'traffic light\', \'fire hydrant\', \'stop sign\', \'parking meter\', \'bench\', \'bird\', \'cat\', \'dog\', \'horse\', \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\', \'backpack\', \'umbrella\', \'handbag\', \'tie\', \'suitcase\', \'frisbee\', \'skis\', \'snowboard\', \'sports ball\', \'kite\', \'baseball bat\', \'baseball glove\', \'skateboard\', \'surfboard\', \'tennis racket\', \'bottle\', \'wine glass\', \'cup\', \'fork\', \'knife\', \'spoon\', \'bowl\', \'banana\', \'apple\', \'sandwich\', \'orange\', \'broccoli\', \'carrot\', \'hot dog\', \'pizza\', \'donut\', \'cake\', \'chair\', \'sofa\', \'pottedplant\', \'bed\', \'diningtable\', \'toilet\', \'tvmonitor\', \'laptop\', \'mouse\', \'remote\', \'keyboard\', \'cell phone\', \'microwave\', \'oven\', \'toaster\', \'sink\', \'refrigerator\', \'book\', \'clock\', \'vase\', \'scissors\', \'teddy bear\', \'hair drier\', \'toothbrush\',]  # noqa\n\n\ndef get_color(c, x, max_value, colors=[[1, 0, 1], [0, 0, 1], [0, 1, 1], [0, 1, 0], [1, 1, 0], [1, 0, 0]]):\n  # https://github.com/pjreddie/darknet/blob/master/src/image.c\n  ratio = (x / max_value) * 5\n  i = math.floor(ratio)\n  j = math.ceil(ratio)\n  ratio -= i\n  r = (1. - ratio) * colors[i][c] + ratio * colors[j][c]\n  return r\n\n\ndef get_rgb_color(cls, clses):\n  offset = cls * 123457 % clses\n  red = get_color(2, offset, clses)\n  green = get_color(1, offset, clses)\n  blue = get_color(0, offset, clses)\n  return int(red * 255), int(green * 255), int(blue * 255)\n\n\nclass COCODrawer:\n  def __init__(self, font_size=24, font=""assets/Roboto-Regular.ttf"", char_width=14):\n    self.coco_names = coco_names\n    self.font_size = font_size\n    self.font = ImageFont.truetype(font, font_size)\n    self.char_width = char_width\n\n    self.num_joints = 17\n    self.edges = [[0, 1], [0, 2], [1, 3], [2, 4],\n                  [3, 5], [4, 6], [5, 6],\n                  [5, 7], [7, 9], [6, 8], [8, 10],\n                  [5, 11], [6, 12], [11, 12],\n                  [11, 13], [13, 15], [12, 14], [14, 16]]\n    self.ec = [(255, 0, 0), (0, 0, 255), (255, 0, 0), (0, 0, 255),\n               (255, 0, 0), (0, 0, 255), (255, 0, 255),\n               (255, 0, 0), (255, 0, 0), (0, 0, 255), (0, 0, 255),\n               (255, 0, 0), (0, 0, 255), (255, 0, 255),\n               (255, 0, 0), (255, 0, 0), (0, 0, 255), (0, 0, 255)]\n    self.colors_hp = [(255, 0, 255), (255, 0, 0), (0, 0, 255),\n                      (255, 0, 0), (0, 0, 255), (255, 0, 0), (0, 0, 255),\n                      (255, 0, 0), (0, 0, 255), (255, 0, 0), (0, 0, 255),\n                      (255, 0, 0), (0, 0, 255), (255, 0, 0), (0, 0, 255),\n                      (255, 0, 0), (0, 0, 255)]\n\n  def draw_pose(self, img, kps):\n    """"""Draw the pose like https://github.com/xingyizhou/CenterNet/blob/master/src/lib/utils/debugger.py#L191\n    Arguments\n      img: uint8 BGR\n      kps: (17, 2) keypoint [[x, y]] coordinates\n    """"""\n    kps = np.array(kps, dtype=np.int32).reshape(self.num_joints, 2)\n    for j in range(self.num_joints):\n      cv2.circle(img, (kps[j, 0], kps[j, 1]), 3, self.colors_hp[j], -1)\n    for j, e in enumerate(self.edges):\n      if kps[e].min() > 0:\n        cv2.line(img, (kps[e[0], 0], kps[e[0], 1]), (kps[e[1], 0], kps[e[1], 1]), self.ec[j], 2,\n                 lineType=cv2.LINE_AA)\n    return img\n\n  def draw_box(self, img, x1, y1, x2, y2, cl):\n    cl = int(cl)\n    x1, y1, x2, y2 = int(round(float(x1))), int(round(float(y1))), int(round(float(x2))), int(round(float(y2)))\n    h = img.shape[0]\n    width = max(1, int(h * 0.006))\n    name = self.coco_names[cl].split()[-1]\n    bgr_color = get_rgb_color(cl, len(self.coco_names))[::-1]\n    # bounding box\n    cv2.rectangle(img, (x1, y1), (x2, y2), bgr_color, width)\n    # font background\n    font_width = len(name) * self.char_width\n    cv2.rectangle(img, (x1 - math.ceil(width / 2), y1 - self.font_size), (x1 + font_width, y1), bgr_color, -1)\n    # text\n    pil_img = Image.fromarray(img[..., ::-1])\n    draw = ImageDraw.Draw(pil_img)\n    draw.text((x1 + width, y1 - self.font_size), name, font=self.font, fill=(0, 0, 0, 255))\n    img = np.array(pil_img)[..., ::-1].copy()\n    return img\n'"
keras_centernet/models/networks/hourglass.py,0,"b'""""""Hourglass Network for Keras.\n# Reference paper\n- [Objects as Points]\n  (https://arxiv.org/pdf/1904.07850.pdf)\n# Reference implementation\n- [PyTorch CenterNet]\n  (https://github.com/xingyizhou/CenterNet/blob/master/src/lib/models/networks/large_hourglass.py)\n- [Keras Stacked_Hourglass_Network_Keras]\n  (https://github.com/yuanyuanli85/Stacked_Hourglass_Network_Keras/blob/master/src/net/hourglass.py)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom keras.models import Model\nfrom keras.layers import Conv2D, Input, Activation, BatchNormalization, Add, UpSampling2D, ZeroPadding2D\nfrom keras.utils import get_file\nimport keras.backend as K\nimport numpy as np\n\n\nCTDET_COCO_WEIGHTS_PATH = (\n  \'https://github.com/see--/keras-centernet/\'\n  \'releases/download/0.1.0/ctdet_coco_hg.hdf5\')\n\nHPDET_COCO_WEIGHTS_PATH = (\n  \'https://github.com/see--/keras-centernet/\'\n  \'releases/download/0.1.0/hpdet_coco_hg.hdf5\')\n\n\ndef normalize_image(image):\n  """"""Normalize the image for the Hourglass network.\n  # Arguments\n    image: BGR uint8\n  # Returns\n    float32 image with the same shape as the input\n  """"""\n  mean = [0.40789655, 0.44719303, 0.47026116]\n  std = [0.2886383, 0.27408165, 0.27809834]\n  return ((np.float32(image) / 255.) - mean) / std\n\n\ndef HourglassNetwork(heads, num_stacks, cnv_dim=256, inres=(512, 512), weights=\'ctdet_coco\',\n                     dims=[256, 384, 384, 384, 512]):\n  """"""Instantiates the Hourglass architecture.\n  Optionally loads weights pre-trained on COCO.\n  Note that the data format convention used by the model is\n  the one specified in your Keras config at `~/.keras/keras.json`.\n  # Arguments\n      num_stacks: number of hourglass modules.\n      cnv_dim: number of filters after the resolution is decreased.\n      inres: network input shape, should be a multiple of 128.\n      weights: one of `None` (random initialization),\n            \'ctdet_coco\' (pre-training on COCO for 2D object detection),\n            \'hpdet_coco\' (pre-training on COCO for human pose detection),\n            or the path to the weights file to be loaded.\n      dims: numbers of channels in the hourglass blocks.\n  # Returns\n      A Keras model instance.\n  # Raises\n      ValueError: in case of invalid argument for `weights`,\n          or invalid input shape.\n  """"""\n  if not (weights in {\'ctdet_coco\', \'hpdet_coco\', None} or os.path.exists(weights)):\n    raise ValueError(\'The `weights` argument should be either \'\n                     \'`None` (random initialization), `ctdet_coco` \'\n                     \'(pre-trained on COCO), `hpdet_coco` (pre-trained on COCO) \'\n                     \'or the path to the weights file to be loaded.\')\n  input_layer = Input(shape=(inres[0], inres[1], 3), name=\'HGInput\')\n  inter = pre(input_layer, cnv_dim)\n  prev_inter = None\n  outputs = []\n  for i in range(num_stacks):\n    prev_inter = inter\n    _heads, inter = hourglass_module(heads, inter, cnv_dim, i, dims)\n    outputs.extend(_heads)\n    if i < num_stacks - 1:\n      inter_ = Conv2D(cnv_dim, 1, use_bias=False, name=\'inter_.%d.0\' % i)(prev_inter)\n      inter_ = BatchNormalization(epsilon=1e-5, name=\'inter_.%d.1\' % i)(inter_)\n\n      cnv_ = Conv2D(cnv_dim, 1, use_bias=False, name=\'cnv_.%d.0\' % i)(inter)\n      cnv_ = BatchNormalization(epsilon=1e-5, name=\'cnv_.%d.1\' % i)(cnv_)\n\n      inter = Add(name=\'inters.%d.inters.add\' % i)([inter_, cnv_])\n      inter = Activation(\'relu\', name=\'inters.%d.inters.relu\' % i)(inter)\n      inter = residual(inter, cnv_dim, \'inters.%d\' % i)\n\n  model = Model(inputs=input_layer, outputs=outputs)\n  if weights == \'ctdet_coco\':\n    weights_path = get_file(\n      \'%s_hg.hdf5\' % weights,\n      CTDET_COCO_WEIGHTS_PATH,\n      cache_subdir=\'models\',\n      file_hash=\'ce01e92f75b533e3ff8e396c76d55d97ff3ec27e99b1bdac1d7b0d6dcf5d90eb\')\n    model.load_weights(weights_path)\n  elif weights == \'hpdet_coco\':\n    weights_path = get_file(\n      \'%s_hg.hdf5\' % weights,\n      HPDET_COCO_WEIGHTS_PATH,\n      cache_subdir=\'models\',\n      file_hash=\'5c562ee22dc383080629dae975f269d62de3a41da6fd0c821085fbee183d555d\')\n    model.load_weights(weights_path)\n  elif weights is not None:\n    model.load_weights(weights)\n\n  return model\n\n\ndef hourglass_module(heads, bottom, cnv_dim, hgid, dims):\n  # create left features , f1, f2, f4, f8, f16 and f32\n  lfs = left_features(bottom, hgid, dims)\n\n  # create right features, connect with left features\n  rf1 = right_features(lfs, hgid, dims)\n  rf1 = convolution(rf1, 3, cnv_dim, name=\'cnvs.%d\' % hgid)\n\n  # add 1x1 conv with two heads, inter is sent to next stage\n  # head_parts is used for intermediate supervision\n  heads = create_heads(heads, rf1, hgid)\n  return heads, rf1\n\n\ndef convolution(_x, k, out_dim, name, stride=1):\n  padding = (k - 1) // 2\n  _x = ZeroPadding2D(padding=padding, name=name + \'.pad\')(_x)\n  _x = Conv2D(out_dim, k, strides=stride, use_bias=False, name=name + \'.conv\')(_x)\n  _x = BatchNormalization(epsilon=1e-5, name=name + \'.bn\')(_x)\n  _x = Activation(\'relu\', name=name + \'.relu\')(_x)\n  return _x\n\n\ndef residual(_x, out_dim, name, stride=1):\n  shortcut = _x\n  num_channels = K.int_shape(shortcut)[-1]\n  _x = ZeroPadding2D(padding=1, name=name + \'.pad1\')(_x)\n  _x = Conv2D(out_dim, 3, strides=stride, use_bias=False, name=name + \'.conv1\')(_x)\n  _x = BatchNormalization(epsilon=1e-5, name=name + \'.bn1\')(_x)\n  _x = Activation(\'relu\', name=name + \'.relu1\')(_x)\n\n  _x = Conv2D(out_dim, 3, padding=\'same\', use_bias=False, name=name + \'.conv2\')(_x)\n  _x = BatchNormalization(epsilon=1e-5, name=name + \'.bn2\')(_x)\n\n  if num_channels != out_dim or stride != 1:\n    shortcut = Conv2D(out_dim, 1, strides=stride, use_bias=False, name=name + \'.shortcut.0\')(\n        shortcut)\n    shortcut = BatchNormalization(epsilon=1e-5, name=name + \'.shortcut.1\')(shortcut)\n\n  _x = Add(name=name + \'.add\')([_x, shortcut])\n  _x = Activation(\'relu\', name=name + \'.relu\')(_x)\n  return _x\n\n\ndef pre(_x, num_channels):\n  # front module, input to 1/4 resolution\n  _x = convolution(_x, 7, 128, name=\'pre.0\', stride=2)\n  _x = residual(_x, num_channels, name=\'pre.1\', stride=2)\n  return _x\n\n\ndef left_features(bottom, hgid, dims):\n  # create left half blocks for hourglass module\n  # f1, f2, f4 , f8, f16, f32 : 1, 1/2, 1/4 1/8, 1/16, 1/32 resolution\n  # 5 times reduce/increase: (256, 384, 384, 384, 512)\n  features = [bottom]\n  for kk, nh in enumerate(dims):\n    pow_str = \'\'\n    for _ in range(kk):\n      pow_str += \'.center\'\n    _x = residual(features[-1], nh, name=\'kps.%d%s.down.0\' % (hgid, pow_str), stride=2)\n    _x = residual(_x, nh, name=\'kps.%d%s.down.1\' % (hgid, pow_str))\n    features.append(_x)\n  return features\n\n\ndef connect_left_right(left, right, num_channels, num_channels_next, name):\n  # left: 2 residual modules\n  left = residual(left, num_channels_next, name=name + \'skip.0\')\n  left = residual(left, num_channels_next, name=name + \'skip.1\')\n\n  # up: 2 times residual & nearest neighbour\n  out = residual(right, num_channels, name=name + \'out.0\')\n  out = residual(out, num_channels_next, name=name + \'out.1\')\n  out = UpSampling2D(name=name + \'out.upsampleNN\')(out)\n  out = Add(name=name + \'out.add\')([left, out])\n  return out\n\n\ndef bottleneck_layer(_x, num_channels, hgid):\n  # 4 residual blocks with 512 channels in the middle\n  pow_str = \'center.\' * 5\n  _x = residual(_x, num_channels, name=\'kps.%d.%s0\' % (hgid, pow_str))\n  _x = residual(_x, num_channels, name=\'kps.%d.%s1\' % (hgid, pow_str))\n  _x = residual(_x, num_channels, name=\'kps.%d.%s2\' % (hgid, pow_str))\n  _x = residual(_x, num_channels, name=\'kps.%d.%s3\' % (hgid, pow_str))\n  return _x\n\n\ndef right_features(leftfeatures, hgid, dims):\n  rf = bottleneck_layer(leftfeatures[-1], dims[-1], hgid)\n  for kk in reversed(range(len(dims))):\n    pow_str = \'\'\n    for _ in range(kk):\n      pow_str += \'center.\'\n    rf = connect_left_right(leftfeatures[kk], rf, dims[kk], dims[max(kk - 1, 0)], name=\'kps.%d.%s\' % (hgid, pow_str))\n  return rf\n\n\ndef create_heads(heads, rf1, hgid):\n  _heads = []\n  for head in sorted(heads):\n    num_channels = heads[head]\n    _x = Conv2D(256, 3, use_bias=True, padding=\'same\', name=head + \'.%d.0.conv\' % hgid)(rf1)\n    _x = Activation(\'relu\', name=head + \'.%d.0.relu\' % hgid)(_x)\n    _x = Conv2D(num_channels, 1, use_bias=True, name=head + \'.%d.1\' % hgid)(_x)\n    _heads.append(_x)\n  return _heads\n\n\nif __name__ == \'__main__\':\n  kwargs = {\n    \'num_stacks\': 2,\n    \'cnv_dim\': 256,\n    \'inres\': (512, 512),\n  }\n  heads = {\n    \'hm\': 80,\n    \'reg\': 2,\n    \'wh\': 2\n  }\n  model = HourglassNetwork(heads=heads, **kwargs)\n  print(model.summary(line_length=200))\n  # from IPython import embed; embed()\n'"
