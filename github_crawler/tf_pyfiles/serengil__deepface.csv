file_path,api_count,code
setup.py,0,"b'import setuptools\n\nwith open(""README.md"", ""r"", encoding=""utf-8"") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=""deepface"",  \n    version=""0.0.27"",\n    author=""Sefik Ilkin Serengil"",\n    author_email=""serengil@gmail.com"",\n    description=""Deep Face Analysis Framework for Face Recognition and Demography (Age, Gender, Emotion, Race)"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/serengil/deepface"",\n    packages=setuptools.find_packages(),\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n    ],\n    python_requires=\'>=3.5.5\',\n    install_requires=[""numpy>=1.14.0"", ""pandas>=0.23.4"", ""tqdm>=4.30.0"", ""gdown>=3.10.1"", ""Pillow>=5.2.0"", ""opencv-python>=3.4.4"", ""tensorflow>=1.9.0"", ""keras>=2.2.0"", ""Flask>=1.1.2""]\n)\n'"
api/api.py,1,"b'from flask import Flask, jsonify, request, make_response\n\nimport argparse\nimport uuid\nimport json\nimport time\nfrom tqdm import tqdm\n\nimport tensorflow as tf\n\nfrom deepface import DeepFace\nfrom deepface.basemodels import VGGFace, OpenFace, Facenet, FbDeepFace\nfrom deepface.extendedmodels import Age, Gender, Race, Emotion\n\n#import DeepFace\n#from basemodels import VGGFace, OpenFace, Facenet, FbDeepFace\n#from extendedmodels import Age, Gender, Race, Emotion\n\n#------------------------------\n\napp = Flask(__name__)\n\n#------------------------------\n\ntic = time.time()\n\nprint(""Loading Face Recognition Models..."")\n\npbar = tqdm(range(0,4), desc=\'Loading Face Recognition Models...\')\n\nfor index in pbar:\n\tif index == 0:\n\t\tpbar.set_description(""Loading VGG-Face"")\n\t\tvggface_model = VGGFace.loadModel()\n\telif index == 1:\n\t\tpbar.set_description(""Loading OpenFace"")\n\t\topenface_model = OpenFace.loadModel()\n\telif index == 2:\n\t\tpbar.set_description(""Loading Google FaceNet"")\n\t\tfacenet_model = Facenet.loadModel()\n\telif index == 3:\n\t\tpbar.set_description(""Loading Facebook DeepFace"")\n\t\tdeepface_model = FbDeepFace.loadModel()\n\ntoc = time.time()\n\nprint(""Face recognition models are built in "", toc-tic,"" seconds"")\n\n#------------------------------\n\ntic = time.time()\n\nprint(""Loading Facial Attribute Analysis Models..."")\n\npbar = tqdm(range(0,4), desc=\'Loading Facial Attribute Analysis Models...\')\n\nfor index in pbar:\n\tif index == 0:\n\t\tpbar.set_description(""Loading emotion analysis model"")\n\t\temotion_model = Emotion.loadModel()\n\telif index == 1:\n\t\tpbar.set_description(""Loading age prediction model"")\n\t\tage_model = Age.loadModel()\n\telif index == 2:\n\t\tpbar.set_description(""Loading gender prediction model"")\n\t\tgender_model = Gender.loadModel()\n\telif index == 3:\n\t\tpbar.set_description(""Loading race prediction model"")\n\t\trace_model = Race.loadModel()\n\ntoc = time.time()\n\nfacial_attribute_models = {}\nfacial_attribute_models[""emotion""] = emotion_model\nfacial_attribute_models[""age""] = age_model\nfacial_attribute_models[""gender""] = gender_model\nfacial_attribute_models[""race""] = race_model\n\nprint(""Facial attribute analysis models are built in "", toc-tic,"" seconds"")\n\n#------------------------------\n\ngraph = tf.get_default_graph()\n\n#------------------------------\n#Service API Interface\n\n@app.route(\'/\')\ndef index():\n\treturn \'<h1>Hello, world!</h1>\'\n\n@app.route(\'/analyze\', methods=[\'POST\'])\ndef analyze():\n\t\n\tglobal graph\n\t\n\ttic = time.time()\n\treq = request.get_json()\n\ttrx_id = uuid.uuid4()\n\n\t#---------------------------\n\t\n\tresp_obj = jsonify({\'success\': False})\n\twith graph.as_default():\n\t\tinstances = []\n\t\tif ""img"" in list(req.keys()):\n\t\t\traw_content = req[""img""] #list\n\n\t\t\tfor item in raw_content: #item is in type of dict\n\t\t\t\tinstances.append(item)\n\t\t\n\t\tif len(instances) == 0:\n\t\t\treturn jsonify({\'success\': False, \'error\': \'you must pass at least one img object in your request\'}), 205\n\t\t\n\t\tprint(""Analyzing "", len(instances),"" instances"")\n\n\t\t#---------------------------\n\n\t\tactions= [\'emotion\', \'age\', \'gender\', \'race\']\n\t\tif ""actions"" in list(req.keys()):\n\t\t\tactions = req[""actions""]\n\t\t\n\t\t#---------------------------\n\n\t\t#resp_obj = DeepFace.analyze(instances, actions=actions)\n\t\tresp_obj = DeepFace.analyze(instances, actions=actions, models=facial_attribute_models)\n\t\t\n\t\t#---------------------------\n\n\ttoc = time.time()\n\n\tresp_obj[""trx_id""] = trx_id\n\tresp_obj[""seconds""] = toc-tic\n\n\treturn resp_obj, 200\n\n@app.route(\'/verify\', methods=[\'POST\'])\n\ndef verify():\n\t\n\tglobal graph\n\t\n\ttic = time.time()\n\treq = request.get_json()\n\ttrx_id = uuid.uuid4()\n\t\n\tresp_obj = jsonify({\'success\': False})\n\t\n\twith graph.as_default():\n\t\t\n\t\tmodel_name = ""VGG-Face""; distance_metric = ""cosine""\n\t\tif ""model_name"" in list(req.keys()):\n\t\t\tmodel_name = req[""model_name""]\n\t\tif ""distance_metric"" in list(req.keys()):\n\t\t\tdistance_metric = req[""distance_metric""]\n\t\t\n\t\t#----------------------\n\t\t\n\t\tinstances = []\n\t\tif ""img"" in list(req.keys()):\n\t\t\traw_content = req[""img""] #list\n\n\t\t\tfor item in raw_content: #item is in type of dict\n\t\t\t\tinstance = []\n\t\t\t\timg1 = item[""img1""]; img2 = item[""img2""]\n\n\t\t\t\tvalidate_img1 = False\n\t\t\t\tif len(img1) > 11 and img1[0:11] == ""data:image/"":\n\t\t\t\t\tvalidate_img1 = True\n\t\t\t\t\n\t\t\t\tvalidate_img2 = False\n\t\t\t\tif len(img2) > 11 and img2[0:11] == ""data:image/"":\n\t\t\t\t\tvalidate_img2 = True\n\n\t\t\t\tif validate_img1 != True or validate_img2 != True:\n\t\t\t\t\treturn jsonify({\'success\': False, \'error\': \'you must pass both img1 and img2 as base64 encoded string\'}), 205\n\n\t\t\t\tinstance.append(img1); instance.append(img2)\n\t\t\t\tinstances.append(instance)\n\t\t\t\n\t\t#--------------------------\n\n\t\tif len(instances) == 0:\n\t\t\treturn jsonify({\'success\': False, \'error\': \'you must pass at least one img object in your request\'}), 205\n\t\t\n\t\tprint(""Input request of "", trx_id, "" has "",len(instances),"" pairs to verify"")\n\t\t\n\t\t#--------------------------\n\t\t\n\t\tif model_name == ""VGG-Face"":\n\t\t\tresp_obj = DeepFace.verify(instances, model_name = model_name, distance_metric = distance_metric, model = vggface_model)\n\t\telif model_name == ""Facenet"":\n\t\t\tresp_obj = DeepFace.verify(instances, model_name = model_name, distance_metric = distance_metric, model = facenet_model)\n\t\telif model_name == ""OpenFace"":\n\t\t\tresp_obj = DeepFace.verify(instances, model_name = model_name, distance_metric = distance_metric, model = openface_model)\n\t\telif model_name == ""DeepFace"":\n\t\t\tresp_obj = DeepFace.verify(instances, model_name = model_name, distance_metric = distance_metric, model = deepface_model)\n\t\telif model_name == ""Ensemble"":\n\t\t\tmodels =  {}\n\t\t\tmodels[""VGG-Face""] = vggface_model\n\t\t\tmodels[""Facenet""] = facenet_model\n\t\t\tmodels[""OpenFace""] = openface_model\n\t\t\tmodels[""DeepFace""] = deepface_model\n\t\t\t\n\t\t\tresp_obj = DeepFace.verify(instances, model_name = model_name, model = models)\n\t\t\t\n\t\telse:\n\t\t\treturn jsonify({\'success\': False, \'error\': \'You must pass a valid model name. Available models are VGG-Face, Facenet, OpenFace, DeepFace but you passed %s\' % (model_name)}), 205\n\t\t\n\t#--------------------------\n\t\n\ttoc =  time.time()\n\t\n\tresp_obj[""trx_id""] = trx_id\n\tresp_obj[""seconds""] = toc-tic\n\t\n\treturn resp_obj, 200\n\n\nif __name__ == \'__main__\':\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n\t\t\'-p\', \'--port\',\n\t\ttype=int,\n\t\tdefault=5000,\n\t\thelp=\'Port of serving api\')\n\targs = parser.parse_args()\n\tapp.run(host=\'0.0.0.0\', port=args.port)\n'"
deepface/DeepFace.py,1,"b'from keras.preprocessing import image\nimport warnings\nwarnings.filterwarnings(""ignore"")\nimport time\nimport os\nfrom os import path\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport json\nimport cv2\nfrom keras import backend as K\nimport keras\nimport tensorflow as tf\nimport pickle\n\nfrom deepface import DeepFace\nfrom deepface.basemodels import VGGFace, OpenFace, Facenet, FbDeepFace\nfrom deepface.extendedmodels import Age, Gender, Race, Emotion\nfrom deepface.commons import functions, realtime, distance as dst\n\ndef verify(img1_path, img2_path=\'\'\n\t, model_name =\'VGG-Face\', distance_metric = \'cosine\', model = None, enforce_detection = True):\n\n\ttic = time.time()\n\n\tif type(img1_path) == list:\n\t\tbulkProcess = True\n\t\timg_list = img1_path.copy()\n\telse:\n\t\tbulkProcess = False\n\t\timg_list = [[img1_path, img2_path]]\n\n\t#------------------------------\n\t\n\tresp_objects = []\n\t\n\tif model_name == \'Ensemble\':\n\t\tprint(""Ensemble learning enabled"")\n\t\t\n\t\timport lightgbm as lgb #lightgbm==2.3.1\n\t\t\n\t\tif model == None:\n\t\t\tmodel = {}\n\t\t\t\n\t\t\tmodel_pbar = tqdm(range(0, 4), desc=\'Face recognition models\')\n\t\t\t\n\t\t\tfor index in model_pbar:\n\t\t\t\t\n\t\t\t\tif index == 0:\n\t\t\t\t\tmodel_pbar.set_description(""Loading VGG-Face"")\n\t\t\t\t\tmodel[""VGG-Face""] = VGGFace.loadModel()\n\t\t\t\telif index == 1:\n\t\t\t\t\tmodel_pbar.set_description(""Loading Google FaceNet"")\n\t\t\t\t\tmodel[""Facenet""] = Facenet.loadModel()\n\t\t\t\telif index == 2:\n\t\t\t\t\tmodel_pbar.set_description(""Loading OpenFace"")\n\t\t\t\t\tmodel[""OpenFace""] = OpenFace.loadModel()\n\t\t\t\telif index == 3:\n\t\t\t\t\tmodel_pbar.set_description(""Loading Facebook DeepFace"")\n\t\t\t\t\tmodel[""DeepFace""] = FbDeepFace.loadModel()\n\t\t\t\t\t\n\t\t#--------------------------\n\t\t#validate model dictionary because it might be passed from input as pre-trained\n\t\t\n\t\tfound_models = []\n\t\tfor key, value in model.items():\n\t\t\tfound_models.append(key)\n\t\t\n\t\tif (\'VGG-Face\' in found_models) and (\'Facenet\' in found_models) and (\'OpenFace\' in found_models) and (\'DeepFace\' in found_models):\n\t\t\tprint(""Ensemble learning will be applied for "", found_models,"" models"")\n\t\telse:\n\t\t\traise ValueError(""You would like to apply ensemble learning and pass pre-built models but models must contain [VGG-Face, Facenet, OpenFace, DeepFace] but you passed ""+found_models)\n\t\t\t\n\t\t#--------------------------\n\t\t\n\t\tmodel_names = [""VGG-Face"", ""Facenet"", ""OpenFace"", ""DeepFace""]\n\t\tmetrics = [""cosine"", ""euclidean"", ""euclidean_l2""]\n\t\t\n\t\tpbar = tqdm(range(0,len(img_list)), desc=\'Verification\')\n\t\t\n\t\t#for instance in img_list:\n\t\tfor index in pbar:\n\t\t\tinstance = img_list[index]\n\t\t\t\n\t\t\tif type(instance) == list and len(instance) >= 2:\n\t\t\t\timg1_path = instance[0]\n\t\t\t\timg2_path = instance[1]\n\t\t\t\t\n\t\t\t\tensemble_features = []; ensemble_features_string = ""[""\n\t\t\t\t\n\t\t\t\tfor i in  model_names:\n\t\t\t\t\tcustom_model = model[i]\n\t\t\t\t\tinput_shape = custom_model.layers[0].input_shape[1:3]\n\t\t\t\t\t\n\t\t\t\t\timg1 = functions.detectFace(img1_path, input_shape, enforce_detection = enforce_detection)\n\t\t\t\t\timg2 = functions.detectFace(img2_path, input_shape, enforce_detection = enforce_detection)\n\t\t\t\t\t\n\t\t\t\t\timg1_representation = custom_model.predict(img1)[0,:]\n\t\t\t\t\timg2_representation = custom_model.predict(img2)[0,:]\n\t\t\t\t\t\n\t\t\t\t\tfor j in metrics:\n\t\t\t\t\t\tif j == \'cosine\':\n\t\t\t\t\t\t\tdistance = dst.findCosineDistance(img1_representation, img2_representation)\n\t\t\t\t\t\telif j == \'euclidean\':\n\t\t\t\t\t\t\tdistance = dst.findEuclideanDistance(img1_representation, img2_representation)\n\t\t\t\t\t\telif j == \'euclidean_l2\':\n\t\t\t\t\t\t\tdistance = dst.findEuclideanDistance(dst.l2_normalize(img1_representation), dst.l2_normalize(img2_representation))\n\t\t\t\t\t\t\n\t\t\t\t\t\tif i == \'OpenFace\' and j == \'euclidean\': #this returns same with OpenFace - euclidean_l2\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tensemble_features.append(distance)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif len(ensemble_features) > 1:\n\t\t\t\t\t\t\t\tensemble_features_string += "", ""\n\t\t\t\t\t\t\tensemble_features_string += str(distance)\n\t\t\t\t\t\t\t\n\t\t\t\t#print(""ensemble_features: "", ensemble_features)\n\t\t\t\tensemble_features_string += ""]""\n\t\t\t\t\n\t\t\t\t#-------------------------------\n\t\t\t\t#find deepface path\n\t\t\t\tdeepface_path = DeepFace.__file__\n\t\t\t\tdeepface_path = deepface_path.replace(""\\\\"", ""/"").replace(""/deepface/DeepFace.py"", """")\n\t\t\t\tensemble_model_path = deepface_path+""/models/face-recognition-ensemble-model.txt""\n\t\t\t\t#print(ensemble_model_path)\n\t\t\t\t\n\t\t\t\tdeepface_ensemble = lgb.Booster(model_file = ensemble_model_path)\n\t\t\t\t\n\t\t\t\tprediction = deepface_ensemble.predict(np.expand_dims(np.array(ensemble_features), axis=0))[0]\n\t\t\t\t\n\t\t\t\tverified = np.argmax(prediction) == 1\n\t\t\t\tif verified: identified = ""true""\n\t\t\t\telse: identified = ""false""\n\t\t\t\t\n\t\t\t\tscore = prediction[np.argmax(prediction)]\n\t\t\t\t\n\t\t\t\t#print(""verified: "", verified,"", score: "", score)\n\t\t\t\t\n\t\t\t\tresp_obj = ""{""\n\t\t\t\tresp_obj += ""\\""verified\\"": ""+identified\n\t\t\t\tresp_obj += "", \\""score\\"": ""+str(score)\n\t\t\t\tresp_obj += "", \\""distance\\"": ""+ensemble_features_string\n\t\t\t\tresp_obj += "", \\""model\\"": [\\""VGG-Face\\"", \\""Facenet\\"", \\""OpenFace\\"", \\""DeepFace\\""]""\n\t\t\t\tresp_obj += "", \\""similarity_metric\\"": [\\""cosine\\"", \\""euclidean\\"", \\""euclidean_l2\\""]""\n\t\t\t\tresp_obj += ""}""\n\t\t\t\t\n\t\t\t\t#print(resp_obj)\n\t\t\t\t\n\t\t\t\tresp_obj = json.loads(resp_obj) #string to json\n\t\t\t\t\n\t\t\t\tif bulkProcess == True:\n\t\t\t\t\tresp_objects.append(resp_obj)\n\t\t\t\telse:\n\t\t\t\t\treturn resp_obj\n\t\t\t\t\n\t\t\t\t#-------------------------------\n\t\t\n\t\tif bulkProcess == True:\n\t\t\tresp_obj = ""{""\n\n\t\t\tfor i in range(0, len(resp_objects)):\n\t\t\t\tresp_item = json.dumps(resp_objects[i])\n\n\t\t\t\tif i > 0:\n\t\t\t\t\tresp_obj += "", ""\n\n\t\t\t\tresp_obj += ""\\""pair_""+str(i+1)+""\\"": ""+resp_item\n\t\t\tresp_obj += ""}""\n\t\t\tresp_obj = json.loads(resp_obj)\n\t\t\treturn resp_obj\n\t\t\n\t\treturn None\n\t\t\n\t#ensemble learning block end\n\t#--------------------------------\n\t#ensemble learning disabled\n\t\n\tif model == None:\n\t\tif model_name == \'VGG-Face\':\n\t\t\tprint(""Using VGG-Face model backend and"", distance_metric,""distance."")\n\t\t\tmodel = VGGFace.loadModel()\n\n\t\telif model_name == \'OpenFace\':\n\t\t\tprint(""Using OpenFace model backend"", distance_metric,""distance."")\n\t\t\tmodel = OpenFace.loadModel()\n\n\t\telif model_name == \'Facenet\':\n\t\t\tprint(""Using Facenet model backend"", distance_metric,""distance."")\n\t\t\tmodel = Facenet.loadModel()\n\n\t\telif model_name == \'DeepFace\':\n\t\t\tprint(""Using FB DeepFace model backend"", distance_metric,""distance."")\n\t\t\tmodel = FbDeepFace.loadModel()\n\n\t\telse:\n\t\t\traise ValueError(""Invalid model_name passed - "", model_name)\n\telse: #model != None\n\t\tprint(""Already built model is passed"")\n\n\t#------------------------------\n\t#face recognition models have different size of inputs\n\tinput_shape = model.layers[0].input_shape[1:3]\n\n\t#------------------------------\n\n\t#tuned thresholds for model and metric pair\n\tthreshold = functions.findThreshold(model_name, distance_metric)\n\n\t#------------------------------\n\tpbar = tqdm(range(0,len(img_list)), desc=\'Verification\')\n\t\n\t#for instance in img_list:\n\tfor index in pbar:\n\t\n\t\tinstance = img_list[index]\n\t\t\n\t\tif type(instance) == list and len(instance) >= 2:\n\t\t\timg1_path = instance[0]\n\t\t\timg2_path = instance[1]\n\n\t\t\t#----------------------\n\t\t\t#crop and align faces\n\n\t\t\timg1 = functions.detectFace(img1_path, input_shape, enforce_detection = enforce_detection)\n\t\t\timg2 = functions.detectFace(img2_path, input_shape, enforce_detection = enforce_detection)\n\n\t\t\t#----------------------\n\t\t\t#find embeddings\n\n\t\t\timg1_representation = model.predict(img1)[0,:]\n\t\t\timg2_representation = model.predict(img2)[0,:]\n\n\t\t\t#----------------------\n\t\t\t#find distances between embeddings\n\n\t\t\tif distance_metric == \'cosine\':\n\t\t\t\tdistance = dst.findCosineDistance(img1_representation, img2_representation)\n\t\t\telif distance_metric == \'euclidean\':\n\t\t\t\tdistance = dst.findEuclideanDistance(img1_representation, img2_representation)\n\t\t\telif distance_metric == \'euclidean_l2\':\n\t\t\t\tdistance = dst.findEuclideanDistance(dst.l2_normalize(img1_representation), dst.l2_normalize(img2_representation))\n\t\t\telse:\n\t\t\t\traise ValueError(""Invalid distance_metric passed - "", distance_metric)\n\n\t\t\t#----------------------\n\t\t\t#decision\n\n\t\t\tif distance <= threshold:\n\t\t\t\tidentified =  ""true""\n\t\t\telse:\n\t\t\t\tidentified =  ""false""\n\n\t\t\t#----------------------\n\t\t\t#response object\n\n\t\t\tresp_obj = ""{""\n\t\t\tresp_obj += ""\\""verified\\"": ""+identified\n\t\t\tresp_obj += "", \\""distance\\"": ""+str(distance)\n\t\t\tresp_obj += "", \\""max_threshold_to_verify\\"": ""+str(threshold)\n\t\t\tresp_obj += "", \\""model\\"": \\""""+model_name+""\\""""\n\t\t\tresp_obj += "", \\""similarity_metric\\"": \\""""+distance_metric+""\\""""\n\t\t\tresp_obj += ""}""\n\n\t\t\tresp_obj = json.loads(resp_obj) #string to json\n\n\t\t\tif bulkProcess == True:\n\t\t\t\tresp_objects.append(resp_obj)\n\t\t\telse:\n\t\t\t\t#K.clear_session()\n\t\t\t\treturn resp_obj\n\t\t\t#----------------------\n\n\t\telse:\n\t\t\traise ValueError(""Invalid arguments passed to verify function: "", instance)\n\n\t#-------------------------\n\n\ttoc = time.time()\n\n\t#print(""identification lasts "",toc-tic,"" seconds"")\n\n\tif bulkProcess == True:\n\t\tresp_obj = ""{""\n\n\t\tfor i in range(0, len(resp_objects)):\n\t\t\tresp_item = json.dumps(resp_objects[i])\n\n\t\t\tif i > 0:\n\t\t\t\tresp_obj += "", ""\n\n\t\t\tresp_obj += ""\\""pair_""+str(i+1)+""\\"": ""+resp_item\n\t\tresp_obj += ""}""\n\t\tresp_obj = json.loads(resp_obj)\n\t\treturn resp_obj\n\t\t#return resp_objects\n\n\ndef analyze(img_path, actions = [], models = {}, enforce_detection = True):\n\n\tif type(img_path) == list:\n\t\timg_paths = img_path.copy()\n\t\tbulkProcess = True\n\telse:\n\t\timg_paths = [img_path]\n\t\tbulkProcess = False\n\n\t#---------------------------------\n\n\t#if a specific target is not passed, then find them all\n\tif len(actions) == 0:\n\t\tactions= [\'emotion\', \'age\', \'gender\', \'race\']\n\n\tprint(""Actions to do: "", actions)\n\n\t#---------------------------------\n\n\tif \'emotion\' in actions:\n\t\tif \'emotion\' in models:\n\t\t\tprint(""already built emotion model is passed"")\n\t\t\temotion_model = models[\'emotion\']\n\t\telse:\n\t\t\temotion_model = Emotion.loadModel()\n\n\tif \'age\' in actions:\n\t\tif \'age\' in models:\n\t\t\tprint(""already built age model is passed"")\n\t\t\tage_model = models[\'age\']\n\t\telse:\n\t\t\tage_model = Age.loadModel()\n\n\tif \'gender\' in actions:\n\t\tif \'gender\' in models:\n\t\t\tprint(""already built gender model is passed"")\n\t\t\tgender_model = models[\'gender\']\n\t\telse:\n\t\t\tgender_model = Gender.loadModel()\n\n\tif \'race\' in actions:\n\t\tif \'race\' in models:\n\t\t\tprint(""already built race model is passed"")\n\t\t\trace_model = models[\'race\']\n\t\telse:\n\t\t\trace_model = Race.loadModel()\n\t#---------------------------------\n\n\tresp_objects = []\n\t\n\tglobal_pbar = tqdm(range(0,len(img_paths)), desc=\'Analyzing\')\n\t\n\t#for img_path in img_paths:\n\tfor j in global_pbar:\n\t\timg_path = img_paths[j]\n\n\t\tresp_obj = ""{""\n\n\t\t#TO-DO: do this in parallel\n\n\t\tpbar = tqdm(range(0,len(actions)), desc=\'Finding actions\')\n\n\t\taction_idx = 0\n\t\timg_224 = None # Set to prevent re-detection\n\t\t#for action in actions:\n\t\tfor index in pbar:\n\t\t\taction = actions[index]\n\t\t\tpbar.set_description(""Action: %s"" % (action))\n\n\t\t\tif action_idx > 0:\n\t\t\t\tresp_obj += "", ""\n\n\t\t\tif action == \'emotion\':\n\t\t\t\temotion_labels = [\'angry\', \'disgust\', \'fear\', \'happy\', \'sad\', \'surprise\', \'neutral\']\n\t\t\t\timg = functions.detectFace(img_path, target_size = (48, 48), grayscale = True, enforce_detection = enforce_detection)\n\n\t\t\t\temotion_predictions = emotion_model.predict(img)[0,:]\n\n\t\t\t\tsum_of_predictions = emotion_predictions.sum()\n\n\t\t\t\temotion_obj = ""\\""emotion\\"": {""\n\t\t\t\tfor i in range(0, len(emotion_labels)):\n\t\t\t\t\temotion_label = emotion_labels[i]\n\t\t\t\t\temotion_prediction = 100 * emotion_predictions[i] / sum_of_predictions\n\n\t\t\t\t\tif i > 0: emotion_obj += "", ""\n\n\t\t\t\t\temotion_obj += ""\\""%s\\"": %s"" % (emotion_label, emotion_prediction)\n\n\t\t\t\temotion_obj += ""}""\n\n\t\t\t\temotion_obj += "", \\""dominant_emotion\\"": \\""%s\\"""" % (emotion_labels[np.argmax(emotion_predictions)])\n\n\t\t\t\tresp_obj += emotion_obj\n\n\t\t\telif action == \'age\':\n\t\t\t\tif img_224 is None:\n\t\t\t\t\timg_224 = functions.detectFace(img_path, target_size = (224, 224), grayscale = False, enforce_detection = enforce_detection) #just emotion model expects grayscale images\n\t\t\t\t#print(""age prediction"")\n\t\t\t\tage_predictions = age_model.predict(img_224)[0,:]\n\t\t\t\tapparent_age = Age.findApparentAge(age_predictions)\n\n\t\t\t\tresp_obj += ""\\""age\\"": %s"" % (apparent_age)\n\n\t\t\telif action == \'gender\':\n\t\t\t\tif img_224 is None:\n\t\t\t\t\timg_224 = functions.detectFace(img_path, target_size = (224, 224), grayscale = False, enforce_detection = enforce_detection) #just emotion model expects grayscale images\n\t\t\t\t#print(""gender prediction"")\n\n\t\t\t\tgender_prediction = gender_model.predict(img_224)[0,:]\n\n\t\t\t\tif np.argmax(gender_prediction) == 0:\n\t\t\t\t\tgender = ""Woman""\n\t\t\t\telif np.argmax(gender_prediction) == 1:\n\t\t\t\t\tgender = ""Man""\n\n\t\t\t\tresp_obj += ""\\""gender\\"": \\""%s\\"""" % (gender)\n\n\t\t\telif action == \'race\':\n\t\t\t\tif img_224 is None:\n\t\t\t\t\timg_224 = functions.detectFace(img_path, target_size = (224, 224), grayscale = False, enforce_detection = enforce_detection) #just emotion model expects grayscale images\n\t\t\t\trace_predictions = race_model.predict(img_224)[0,:]\n\t\t\t\trace_labels = [\'asian\', \'indian\', \'black\', \'white\', \'middle eastern\', \'latino hispanic\']\n\n\t\t\t\tsum_of_predictions = race_predictions.sum()\n\n\t\t\t\trace_obj = ""\\""race\\"": {""\n\t\t\t\tfor i in range(0, len(race_labels)):\n\t\t\t\t\trace_label = race_labels[i]\n\t\t\t\t\trace_prediction = 100 * race_predictions[i] / sum_of_predictions\n\n\t\t\t\t\tif i > 0: race_obj += "", ""\n\n\t\t\t\t\trace_obj += ""\\""%s\\"": %s"" % (race_label, race_prediction)\n\n\t\t\t\trace_obj += ""}""\n\t\t\t\trace_obj += "", \\""dominant_race\\"": \\""%s\\"""" % (race_labels[np.argmax(race_predictions)])\n\n\t\t\t\tresp_obj += race_obj\n\n\t\t\taction_idx = action_idx + 1\n\n\t\tresp_obj += ""}""\n\n\t\tresp_obj = json.loads(resp_obj)\n\n\t\tif bulkProcess == True:\n\t\t\tresp_objects.append(resp_obj)\n\t\telse:\n\t\t\treturn resp_obj\n\n\tif bulkProcess == True:\n\t\tresp_obj = ""{""\n\n\t\tfor i in range(0, len(resp_objects)):\n\t\t\tresp_item = json.dumps(resp_objects[i])\n\n\t\t\tif i > 0:\n\t\t\t\tresp_obj += "", ""\n\n\t\t\tresp_obj += ""\\""instance_""+str(i+1)+""\\"": ""+resp_item\n\t\tresp_obj += ""}""\n\t\tresp_obj = json.loads(resp_obj)\n\t\treturn resp_obj\n\t\t#return resp_objects\n\n\ndef detectFace(img_path):\n\timg = functions.detectFace(img_path)[0] #detectFace returns (1, 224, 224, 3)\n\treturn img[:, :, ::-1] #bgr to rgb\n\ndef find(img_path, db_path\n\t, model_name =\'VGG-Face\', distance_metric = \'cosine\', model = None, enforce_detection = True):\n\t\t\n\ttic = time.time()\n\t\n\tif type(img_path) == list:\n\t\tbulkProcess = True\n\t\timg_paths = img_path.copy()\n\telse:\n\t\tbulkProcess = False\n\t\timg_paths = [img_path]\n\t\n\tif os.path.isdir(db_path) == True:\n\t\t\n\t\t#---------------------------------------\n\t\t\n\t\tif model == None:\n\t\t\tif model_name == \'VGG-Face\':\n\t\t\t\tprint(""Using VGG-Face model backend and"", distance_metric,""distance."")\n\t\t\t\tmodel = VGGFace.loadModel()\n\t\t\telif model_name == \'OpenFace\':\n\t\t\t\tprint(""Using OpenFace model backend"", distance_metric,""distance."")\n\t\t\t\tmodel = OpenFace.loadModel()\n\t\t\telif model_name == \'Facenet\':\n\t\t\t\tprint(""Using Facenet model backend"", distance_metric,""distance."")\n\t\t\t\tmodel = Facenet.loadModel()\n\t\t\telif model_name == \'DeepFace\':\n\t\t\t\tprint(""Using FB DeepFace model backend"", distance_metric,""distance."")\n\t\t\t\tmodel = FbDeepFace.loadModel()\n\t\t\telif model_name == \'Ensemble\':\n\t\t\t\t\n\t\t\t\tprint(""Ensemble learning enabled"")\n\t\t\t\t\n\t\t\t\timport lightgbm as lgb #lightgbm==2.3.1\n\t\t\t\t\n\t\t\t\tmodel_names = [\'VGG-Face\', \'Facenet\', \'OpenFace\', \'DeepFace\']\n\t\t\t\tmetric_names = [\'cosine\', \'euclidean\', \'euclidean_l2\']\n\t\t\t\tmodels = {}\n\t\t\t\t\n\t\t\t\tpbar = tqdm(range(0, len(model_names)), desc=\'Face recognition models\')\n\t\t\t\t\n\t\t\t\tfor index in pbar:\n\t\t\t\t\tif index == 0:\n\t\t\t\t\t\tpbar.set_description(""Loading VGG-Face"")\n\t\t\t\t\t\tmodels[\'VGG-Face\'] = VGGFace.loadModel()\n\t\t\t\t\telif index == 1:\n\t\t\t\t\t\tpbar.set_description(""Loading FaceNet"")\n\t\t\t\t\t\tmodels[\'Facenet\'] = Facenet.loadModel()\n\t\t\t\t\telif index == 2:\n\t\t\t\t\t\tpbar.set_description(""Loading OpenFace"")\n\t\t\t\t\t\tmodels[\'OpenFace\'] = OpenFace.loadModel()\n\t\t\t\t\telif index == 3:\n\t\t\t\t\t\tpbar.set_description(""Loading DeepFace"")\n\t\t\t\t\t\tmodels[\'DeepFace\'] = FbDeepFace.loadModel()\n\t\t\t\t\t\t\n\t\t\telse:\n\t\t\t\traise ValueError(""Invalid model_name passed - "", model_name)\t\n\t\telse: #model != None\n\t\t\tprint(""Already built model is passed"")\n\t\t\t\n\t\t\tif model_name == \'Ensemble\':\n\t\t\t\t\n\t\t\t\t#validate model dictionary because it might be passed from input as pre-trained\n\t\t\t\t\n\t\t\t\tfound_models = []\n\t\t\t\tfor key, value in model.items():\n\t\t\t\t\tfound_models.append(key)\n\t\t\t\t\n\t\t\t\tif (\'VGG-Face\' in found_models) and (\'Facenet\' in found_models) and (\'OpenFace\' in found_models) and (\'DeepFace\' in found_models):\n\t\t\t\t\tprint(""Ensemble learning will be applied for "", found_models,"" models"")\n\t\t\t\telse:\n\t\t\t\t\traise ValueError(""You would like to apply ensemble learning and pass pre-built models but models must contain [VGG-Face, Facenet, OpenFace, DeepFace] but you passed ""+found_models)\n\t\t\n\t\t#threshold = functions.findThreshold(model_name, distance_metric)\n\t\t\n\t\t#---------------------------------------\n\t\t\n\t\tfile_name = ""representations_%s.pkl"" % (model_name)\n\t\tfile_name = file_name.replace(""-"", ""_"").lower()\n\t\t\n\t\tif path.exists(db_path+""/""+file_name):\n\t\t\t\n\t\t\tprint(""WARNING: Representations for images in "",db_path,"" folder were previously stored in "", file_name, "". If you added new instances after this file creation, then please delete this file and call find function again. It will create it again."")\n\t\t\t\n\t\t\tf = open(db_path+\'/\'+file_name, \'rb\')\n\t\t\trepresentations = pickle.load(f)\n\t\t\t\n\t\t\tprint(""There are "", len(representations),"" representations found in "",file_name)\n\t\t\t\n\t\telse:\n\t\t\temployees = []\n\t\t\t\n\t\t\tfor r, d, f in os.walk(db_path): # r=root, d=directories, f = files\n\t\t\t\tfor file in f:\n\t\t\t\t\tif (\'.jpg\' in file):\n\t\t\t\t\t\texact_path = r + ""/"" + file\n\t\t\t\t\t\temployees.append(exact_path)\n\t\t\t\n\t\t\tif len(employees) == 0:\n\t\t\t\traise ValueError(""There is no image in "", db_path,"" folder!"")\n\t\t\t\n\t\t\t#------------------------\n\t\t\t#find representations for db images\n\t\t\t\n\t\t\trepresentations = []\n\t\t\t\n\t\t\tpbar = tqdm(range(0,len(employees)), desc=\'Finding representations\')\n\t\t\t\n\t\t\t#for employee in employees:\n\t\t\tfor index in pbar:\n\t\t\t\temployee = employees[index]\n\t\t\t\t\n\t\t\t\tif model_name != \'Ensemble\':\n\t\t\t\t\n\t\t\t\t\tinput_shape = model.layers[0].input_shape[1:3]\n\t\t\t\t\timg = functions.detectFace(employee, input_shape, enforce_detection = enforce_detection)\n\t\t\t\t\trepresentation = model.predict(img)[0,:]\n\t\t\t\t\t\n\t\t\t\t\tinstance = []\n\t\t\t\t\tinstance.append(employee)\n\t\t\t\t\tinstance.append(representation)\n\t\t\t\t\t\n\t\t\t\telse: #ensemble learning\n\t\t\t\t\t\n\t\t\t\t\tinstance = []\n\t\t\t\t\tinstance.append(employee)\n\t\t\t\t\t\n\t\t\t\t\tfor j in model_names:\n\t\t\t\t\t\tmodel = models[j]\n\t\t\t\t\t\tinput_shape = model.layers[0].input_shape[1:3]\n\t\t\t\t\t\timg = functions.detectFace(employee, input_shape, enforce_detection = enforce_detection)\n\t\t\t\t\t\trepresentation = model.predict(img)[0,:]\n\t\t\t\t\t\tinstance.append(representation)\n\t\t\t\t\n\t\t\t\t#-------------------------------\n\t\t\t\t\n\t\t\t\trepresentations.append(instance)\n\t\t\t\n\t\t\tf = open(db_path+\'/\'+file_name, ""wb"")\n\t\t\tpickle.dump(representations, f)\n\t\t\tf.close()\n\t\t\t\n\t\t\tprint(""Representations stored in "",db_path,""/"",file_name,"" file. Please delete this file when you add new identities in your database."")\n\t\t\n\t\t#----------------------------\n\t\t#we got representations for database\n\t\t\n\t\tif model_name != \'Ensemble\':\n\t\t\tdf = pd.DataFrame(representations, columns = [""identity"", ""representation""])\n\t\telse: #ensemble learning\n\t\t\tdf = pd.DataFrame(representations, columns = [""identity"", ""VGG-Face_representation"", ""Facenet_representation"", ""OpenFace_representation"", ""DeepFace_representation""])\n\t\t\t\n\t\tdf_base = df.copy()\n\t\t\n\t\tresp_obj = []\n\t\t\n\t\tglobal_pbar = tqdm(range(0,len(img_paths)), desc=\'Analyzing\')\n\t\tfor j in global_pbar:\n\t\t\timg_path = img_paths[j]\n\t\t\n\t\t\t#find representation for passed image\n\t\t\t\n\t\t\tif model_name == \'Ensemble\':\n\t\t\t\tfor j in model_names:\n\t\t\t\t\tmodel = models[j]\n\t\t\t\t\tinput_shape = model.layers[0].input_shape[1:3]\n\t\t\t\t\timg = functions.detectFace(img_path, input_shape, enforce_detection = enforce_detection)\n\t\t\t\t\ttarget_representation = model.predict(img)[0,:]\n\t\t\t\t\t\n\t\t\t\t\tfor k in metric_names:\n\t\t\t\t\t\tdistances = []\n\t\t\t\t\t\tfor index, instance in df.iterrows():\n\t\t\t\t\t\t\tsource_representation = instance[""%s_representation"" % (j)]\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif k == \'cosine\':\n\t\t\t\t\t\t\t\tdistance = dst.findCosineDistance(source_representation, target_representation)\n\t\t\t\t\t\t\telif k == \'euclidean\':\n\t\t\t\t\t\t\t\tdistance = dst.findEuclideanDistance(source_representation, target_representation)\n\t\t\t\t\t\t\telif k == \'euclidean_l2\':\n\t\t\t\t\t\t\t\tdistance = dst.findEuclideanDistance(dst.l2_normalize(source_representation), dst.l2_normalize(target_representation))\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tdistances.append(distance)\n\t\t\t\t\t\t\n\t\t\t\t\t\tif j == \'OpenFace\' and k == \'euclidean\':\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tdf[""%s_%s"" % (j, k)] = distances\n\t\t\t\t\n\t\t\t\t#----------------------------------\n\t\t\t\t\n\t\t\t\tfeature_names = []\n\t\t\t\tfor j in model_names:\n\t\t\t\t\tfor k in metric_names:\n\t\t\t\t\t\tif j == \'OpenFace\' and k == \'euclidean\':\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tfeature = \'%s_%s\' % (j, k)\n\t\t\t\t\t\t\tfeature_names.append(feature)\n\t\t\t\t\n\t\t\t\t#print(df[feature_names].head())\n\t\t\t\t\n\t\t\t\tx = df[feature_names].values\n\t\t\t\t\n\t\t\t\t#----------------------------------\n\t\t\t\t#lightgbm model\n\t\t\t\tdeepface_path = DeepFace.__file__\n\t\t\t\tdeepface_path = deepface_path.replace(""\\\\"", ""/"").replace(""/deepface/DeepFace.py"", """")\n\t\t\t\tensemble_model_path = deepface_path+""/models/face-recognition-ensemble-model.txt""\n\t\t\t\tdeepface_ensemble = lgb.Booster(model_file = ensemble_model_path)\n\t\t\t\t\n\t\t\t\ty = deepface_ensemble.predict(x)\n\t\t\t\t\n\t\t\t\tverified_labels = []; scores = []\n\t\t\t\tfor i in y:\n\t\t\t\t\tverified = np.argmax(i) == 1\n\t\t\t\t\tscore = i[np.argmax(i)]\n\t\t\t\t\t\n\t\t\t\t\tverified_labels.append(verified)\n\t\t\t\t\tscores.append(score)\n\t\t\t\t\n\t\t\t\tdf[\'verified\'] = verified_labels\n\t\t\t\tdf[\'score\'] = scores\n\t\t\t\t\n\t\t\t\tdf = df[df.verified == True]\n\t\t\t\t#df = df[df.score > 0.99] #confidence score\n\t\t\t\tdf = df.sort_values(by = [""score""], ascending=False).reset_index(drop=True)\n\t\t\t\tdf = df[[\'identity\', \'verified\', \'score\']]\n\t\t\t\t\n\t\t\t\tresp_obj.append(df)\n\t\t\t\tdf = df_base.copy() #restore df for the next iteration\n\t\t\t\t\n\t\t\t\t#----------------------------------\n\t\t\t\n\t\t\tif model_name != \'Ensemble\':\n\t\t\t\tinput_shape = model.layers[0].input_shape[1:3]\n\t\t\t\timg = functions.detectFace(img_path, input_shape, enforce_detection = enforce_detection)\n\t\t\t\ttarget_representation = model.predict(img)[0,:]\n\t\t\n\t\t\t\tdistances = []\n\t\t\t\tfor index, instance in df.iterrows():\n\t\t\t\t\tsource_representation = instance[""representation""]\n\t\t\t\t\t\n\t\t\t\t\tif distance_metric == \'cosine\':\n\t\t\t\t\t\tdistance = dst.findCosineDistance(source_representation, target_representation)\n\t\t\t\t\telif distance_metric == \'euclidean\':\n\t\t\t\t\t\tdistance = dst.findEuclideanDistance(source_representation, target_representation)\n\t\t\t\t\telif distance_metric == \'euclidean_l2\':\n\t\t\t\t\t\tdistance = dst.findEuclideanDistance(dst.l2_normalize(source_representation), dst.l2_normalize(target_representation))\n\t\t\t\t\telse:\n\t\t\t\t\t\traise ValueError(""Invalid distance_metric passed - "", distance_metric)\n\t\t\t\t\t\n\t\t\t\t\tdistances.append(distance)\n\t\t\t\t\n\t\t\t\tthreshold = functions.findThreshold(model_name, distance_metric)\n\t\t\t\t\n\t\t\t\tdf[""distance""] = distances\n\t\t\t\tdf = df.drop(columns = [""representation""])\n\t\t\t\tdf = df[df.distance <= threshold]\n\t\t\t\n\t\t\t\tdf = df.sort_values(by = [""distance""], ascending=True).reset_index(drop=True)\n\t\t\t\tresp_obj.append(df)\n\t\t\t\tdf = df_base.copy() #restore df for the next iteration\n\t\t\t\n\t\ttoc = time.time()\n\t\t\n\t\tprint(""find function lasts "",toc-tic,"" seconds"")\n\t\t\n\t\tif len(resp_obj) == 1:\n\t\t\treturn resp_obj[0]\n\t\t\n\t\treturn resp_obj\n\t\t\n\telse:\n\t\traise ValueError(""Passed db_path does not exist!"")\n\t\t\n\treturn None\n\t\ndef stream(db_path = \'\', model_name =\'VGG-Face\', distance_metric = \'cosine\', enable_face_analysis = True):\n\trealtime.analysis(db_path, model_name, distance_metric, enable_face_analysis)\n\ndef allocateMemory():\n\tprint(""Analyzing your system..."")\n\tfunctions.allocateMemory()\n\nfunctions.initializeFolder()\n\n#---------------------------\n\n'"
deepface/__init__.py,0,b''
tests/Ensemble-Face-Recognition.py,0,"b'import pandas as pd\nimport numpy as np\nimport itertools\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\ntqdm.pandas()\n\n#--------------------------\n#Data set\n\n# Ref: https://github.com/serengil/deepface/tree/master/tests/dataset\nidendities = {\n    ""Angelina"": [""img1.jpg"", ""img2.jpg"", ""img4.jpg"", ""img5.jpg"", ""img6.jpg"", ""img7.jpg"", ""img10.jpg"", ""img11.jpg""],\n    ""Scarlett"": [""img8.jpg"", ""img9.jpg"", ""img47.jpg"", ""img48.jpg"", ""img49.jpg"", ""img50.jpg"", ""img51.jpg""],\n    ""Jennifer"": [""img3.jpg"", ""img12.jpg"", ""img53.jpg"", ""img54.jpg"", ""img55.jpg"", ""img56.jpg""],\n    ""Mark"": [""img13.jpg"", ""img14.jpg"", ""img15.jpg"", ""img57.jpg"", ""img58.jpg""],\n    ""Jack"": [""img16.jpg"", ""img17.jpg"", ""img59.jpg"", ""img61.jpg"", ""img62.jpg""],\n    ""Elon"": [""img18.jpg"", ""img19.jpg"", ""img67.jpg""],\n    ""Jeff"": [""img20.jpg"", ""img21.jpg""],\n    ""Marissa"": [""img22.jpg"", ""img23.jpg""],\n    ""Sundar"": [""img24.jpg"", ""img25.jpg""],\n    ""Katy"": [""img26.jpg"", ""img27.jpg"", ""img28.jpg"", ""img42.jpg"", ""img43.jpg"", ""img44.jpg"", ""img45.jpg"", ""img46.jpg""],\n    ""Matt"": [""img29.jpg"", ""img30.jpg"", ""img31.jpg"", ""img32.jpg"", ""img33.jpg""],\n    ""Leonardo"": [""img34.jpg"", ""img35.jpg"", ""img36.jpg"", ""img37.jpg""],\n    ""George"": [""img38.jpg"", ""img39.jpg"", ""img40.jpg"", ""img41.jpg""]\n    \n}\n#--------------------------\n#Positives\n\npositives = []\n\nfor key, values in idendities.items():\n    \n    #print(key)\n    for i in range(0, len(values)-1):\n        for j in range(i+1, len(values)):\n            #print(values[i], "" and "", values[j])\n            positive = []\n            positive.append(values[i])\n            positive.append(values[j])\n            positives.append(positive)\n\npositives = pd.DataFrame(positives, columns = [""file_x"", ""file_y""])\npositives[""decision""] = ""Yes""\nprint(positives.shape)\n#--------------------------\n#Negatives\n\nsamples_list = list(idendities.values())\n\nnegatives = []\n\nfor i in range(0, len(idendities) - 1):\n    for j in range(i+1, len(idendities)):\n        #print(samples_list[i], "" vs "",samples_list[j]) \n        cross_product = itertools.product(samples_list[i], samples_list[j])\n        cross_product = list(cross_product)\n        #print(cross_product)\n        \n        for cross_sample in cross_product:\n            #print(cross_sample[0], "" vs "", cross_sample[1])\n            negative = []\n            negative.append(cross_sample[0])\n            negative.append(cross_sample[1])\n            negatives.append(negative)\n\t\t\t\nnegatives = pd.DataFrame(negatives, columns = [""file_x"", ""file_y""])\nnegatives[""decision""] = ""No""\n\nnegatives = negatives.sample(positives.shape[0])\n\nprint(negatives.shape)\n#--------------------------\n#Merge positive and negative ones\n\ndf = pd.concat([positives, negatives]).reset_index(drop = True)\n\nprint(df.decision.value_counts())\n\ndf.file_x = ""deepface/tests/dataset/""+df.file_x\ndf.file_y = ""deepface/tests/dataset/""+df.file_y\n#--------------------------\n#DeepFace\n\nfrom deepface import DeepFace\nfrom deepface.basemodels import VGGFace, OpenFace, Facenet, FbDeepFace\n\npretrained_models = {}\n\npretrained_models[""VGG-Face""] = VGGFace.loadModel()\nprint(""VGG-Face loaded"")\npretrained_models[""Facenet""] = Facenet.loadModel()\nprint(""Facenet loaded"")\npretrained_models[""OpenFace""] = OpenFace.loadModel() \nprint(""OpenFace loaded"")\npretrained_models[""DeepFace""] = FbDeepFace.loadModel()\nprint(""FbDeepFace loaded"")\n\ninstances = df[[""file_x"", ""file_y""]].values.tolist()\n\nmodels = [\'VGG-Face\', \'Facenet\', \'OpenFace\', \'DeepFace\']\nmetrics = [\'cosine\', \'euclidean_l2\']\n\nif True:\n    for model in models:\n        for metric in metrics:\n\n            resp_obj = DeepFace.verify(instances\n                                       , model_name = model\n                                       , model = pretrained_models[model]\n                                       , distance_metric = metric)\n\n            distances = []\n\n            for i in range(0, len(instances)):\n                distance = round(resp_obj[""pair_%s"" % (i+1)][""distance""], 4)\n                distances.append(distance)\n\n            df[\'%s_%s\' % (model, metric)] = distances\n    \n    df.to_csv(""face-recognition-pivot.csv"", index = False)\nelse:\n    df = pd.read_csv(""face-recognition-pivot.csv"")\n\ndf_raw = df.copy()\n\n#--------------------------\n#Distribution\n\nfig = plt.figure(figsize=(15, 15))\n\nfigure_idx = 1\nfor model in models:\n    for metric in metrics:\n        \n        feature = \'%s_%s\' % (model, metric)\n        \n        ax1 = fig.add_subplot(4, 2, figure_idx)\n        \n        df[df.decision == ""Yes""][feature].plot(kind=\'kde\', title = feature, label = \'Yes\', legend = True)\n        df[df.decision == ""No""][feature].plot(kind=\'kde\', title = feature, label = \'No\', legend = True)\n        \n        figure_idx = figure_idx + 1\n\nplt.show()\n#--------------------------\n#Pre-processing for modelling\n\ncolumns = []\nfor model in models:\n    for metric in metrics:\n        feature = \'%s_%s\' % (model, metric)\n        columns.append(feature)\n\ncolumns.append(""decision"")\n\ndf = df[columns]\n\ndf.loc[df[df.decision == \'Yes\'].index, \'decision\'] = 1\ndf.loc[df[df.decision == \'No\'].index, \'decision\'] = 0\n\nprint(df.head())\n#--------------------------\n#Train test split\n\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, test_size=0.30, random_state=17)\n\ntarget_name = ""decision""\n\ny_train = df_train[target_name].values\nx_train = df_train.drop(columns=[target_name]).values\n\ny_test = df_test[target_name].values\nx_test = df_test.drop(columns=[target_name]).values\n\n#--------------------------\n#LightGBM\n\nimport lightgbm as lgb\n\nfeatures = df.drop(columns=[target_name]).columns.tolist()\nlgb_train = lgb.Dataset(x_train, y_train, feature_name = features)\nlgb_test = lgb.Dataset(x_test, y_test, feature_name = features)\n\nparams = {\n    \'task\': \'train\'\n    , \'boosting_type\': \'gbdt\'\n    , \'objective\': \'multiclass\'\n    , \'num_class\': 2\n    , \'metric\': \'multi_logloss\'\n}\n\ngbm = lgb.train(params, lgb_train, num_boost_round=250, early_stopping_rounds = 15 , valid_sets=lgb_test)\n\ngbm.save_model(""face-recognition-ensemble-model.txt"")\n\n#--------------------------\n#Evaluation\n\npredictions = gbm.predict(x_test)\n\ncm = confusion_matrix(y_test, prediction_classes)\nprint(cm)\n\ntn, fp, fn, tp = cm.ravel()\n\nrecall = tp / (tp + fn)\nprecision = tp / (tp + fp)\naccuracy = (tp + tn)/(tn + fp +  fn + tp)\nf1 = 2 * (precision * recall) / (precision + recall)\n\nprint(""Precision: "", 100*precision,""%"")\nprint(""Recall: "", 100*recall,""%"")\nprint(""F1 score "",100*f1, ""%"")\nprint(""Accuracy: "", 100*accuracy,""%"")\n#--------------------------\n#Interpretability\n\nax = lgb.plot_importance(gbm, max_num_features=20)\nplt.show()\n\nimport os\nos.environ[""PATH""] += os.pathsep + \'C:/Program Files (x86)/Graphviz2.38/bin\'\n\nplt.rcParams[""figure.figsize""] = [20, 20]\n\nfor i in range(0, gbm.num_trees()):\n    ax = lgb.plot_tree(gbm, tree_index = i)\n    plt.show()\n    \n    if i == 2:\n        break\n#--------------------------\n#ROC Curve\n\ny_pred_proba = predictions[::,1]\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\n\nplt.figure(figsize=(7,3))\nplt.plot(fpr,tpr,label=""data 1, auc=""+str(auc))\n\n#--------------------------'"
tests/face-recognition-how.py,0,"b'#!pip install deepface\r\nfrom deepface.basemodels import VGGFace, OpenFace, Facenet, FbDeepFace\r\nfrom deepface.commons import functions\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\n#----------------------------------------------\r\n#build face recognition model\r\n\r\nmodel = VGGFace.loadModel()\r\n#model = Facenet.loadModel()\r\n#model = OpenFace.loadModel()\r\n#model = FbDeepFace.loadModel()\r\n\r\ninput_shape = model.layers[0].input_shape[1:3]\r\n\r\nprint(""model input shape: "", model.layers[0].input_shape[1:])\r\nprint(""model output shape: "", model.layers[-1].input_shape[-1])\r\n\r\n#----------------------------------------------\r\n#load images and find embeddings\r\n\r\nimg1 = functions.detectFace(""dataset/img1.jpg"", input_shape)\r\nimg1_representation = model.predict(img1)[0,:]\r\n\r\nimg2 = functions.detectFace(""dataset/img3.jpg"", input_shape)\r\nimg2_representation = model.predict(img2)[0,:]\r\n\r\n#----------------------------------------------\r\n#distance between two images\r\n\r\ndistance_vector = np.square(img1_representation - img2_representation)\r\n#print(distance_vector)\r\n\r\ndistance = np.sqrt(distance_vector.sum())\r\nprint(""Euclidean distance: "",distance)\r\n\r\n#----------------------------------------------\r\n#expand vectors to be shown better in graph\r\n\r\nimg1_graph = []; img2_graph = []; distance_graph = []\r\n\r\nfor i in range(0, 200):\r\n\timg1_graph.append(img1_representation)\r\n\timg2_graph.append(img2_representation)\r\n\tdistance_graph.append(distance_vector)\r\n\r\nimg1_graph = np.array(img1_graph)\r\nimg2_graph = np.array(img2_graph)\r\ndistance_graph = np.array(distance_graph)\r\n\r\n#----------------------------------------------\r\n#plotting\r\n\r\nfig = plt.figure()\r\n\r\nax1 = fig.add_subplot(3,2,1)\r\nplt.imshow(img1[0][:,:,::-1])\r\nplt.axis(\'off\')\r\n\r\nax2 = fig.add_subplot(3,2,2)\r\nim = plt.imshow(img1_graph, interpolation=\'nearest\', cmap=plt.cm.ocean)\r\nplt.colorbar()\r\n\r\nax3 = fig.add_subplot(3,2,3)\r\nplt.imshow(img2[0][:,:,::-1])\r\nplt.axis(\'off\')\r\n\r\nax4 = fig.add_subplot(3,2,4)\r\nim = plt.imshow(img2_graph, interpolation=\'nearest\', cmap=plt.cm.ocean)\r\nplt.colorbar()\r\n\r\nax5 = fig.add_subplot(3,2,5)\r\nplt.text(0.35, 0, ""Distance: %s"" % (distance))\r\nplt.axis(\'off\')\r\n\r\nax6 = fig.add_subplot(3,2,6)\r\nim = plt.imshow(distance_graph, interpolation=\'nearest\', cmap=plt.cm.ocean)\r\nplt.colorbar()\r\n\r\nplt.show()\r\n\r\n#----------------------------------------------'"
tests/unit_tests.py,0,"b'from deepface import DeepFace\nimport json\n\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n#-----------------------------------------\n\nprint(""Bulk tests"")\n\nprint(""-----------------------------------------"")\n\nprint(""Large scale face recognition"")\n\ndf = DeepFace.find(img_path = ""dataset/img1.jpg"", db_path = ""dataset"")\nprint(df.head())\n\nprint(""-----------------------------------------"")\n\nprint(""Ensemble for find function"")\ndf = DeepFace.find(img_path = ""dataset/img1.jpg"", db_path = ""dataset"", model_name = ""Ensemble"")\nprint(df.head())\n\nprint(""-----------------------------------------"")\n\nprint(""Bulk face recognition tests"")\n\ndataset = [\n\t[\'dataset/img1.jpg\', \'dataset/img2.jpg\', True],\n\t[\'dataset/img5.jpg\', \'dataset/img6.jpg\', True]\n]\n\nresp_obj = DeepFace.verify(dataset)\nprint(resp_obj[""pair_1""][""verified""] == True)\nprint(resp_obj[""pair_2""][""verified""] == True)\n\nprint(""-----------------------------------------"")\n\nprint(""Ensemble learning bulk"")\nresp_obj = DeepFace.verify(dataset, model_name = ""Ensemble"")\n\nfor i in range(0, len(dataset)):\n\titem = resp_obj[\'pair_%s\' % (i+1)]\n\tverified = item[""verified""]\n\tscore = item[""score""]\n\tprint(verified)\n\nprint(""-----------------------------------------"")\n\nprint(""Bulk facial analysis tests"")\n\ndataset = [\n\t\'dataset/img1.jpg\',\n\t\'dataset/img2.jpg\',\n\t\'dataset/img5.jpg\',\n\t\'dataset/img6.jpg\'\n]\n\nresp_obj = DeepFace.analyze(dataset)\nprint(resp_obj[""instance_1""][""age""],"" years old "", resp_obj[""instance_1""][""dominant_emotion""], "" "",resp_obj[""instance_1""][""gender""])\nprint(resp_obj[""instance_2""][""age""],"" years old "", resp_obj[""instance_2""][""dominant_emotion""], "" "",resp_obj[""instance_2""][""gender""])\nprint(resp_obj[""instance_3""][""age""],"" years old "", resp_obj[""instance_3""][""dominant_emotion""], "" "",resp_obj[""instance_3""][""gender""])\nprint(resp_obj[""instance_4""][""age""],"" years old "", resp_obj[""instance_4""][""dominant_emotion""], "" "",resp_obj[""instance_4""][""gender""])\n\n\nprint(""-----------------------------------------"")\n\n#-----------------------------------------\n\nprint(""Facial analysis test. Passing nothing as an action"")\n\nimg = ""dataset/img4.jpg""\ndemography = DeepFace.analyze(img)\nprint(demography)\n\nprint(""-----------------------------------------"")\n\nprint(""Facial analysis test. Passing all to the action"")\ndemography = DeepFace.analyze(img, [\'age\', \'gender\', \'race\', \'emotion\'])\n\nprint(""Demography:"")\nprint(demography)\n\n#check response is a valid json\nprint(""Age: "", demography[""age""])\nprint(""Gender: "", demography[""gender""])\nprint(""Race: "", demography[""dominant_race""])\nprint(""Emotion: "", demography[""dominant_emotion""])\n\nprint(""-----------------------------------------"")\n\nprint(""Face recognition tests"")\n\ndataset = [\n\t[\'dataset/img1.jpg\', \'dataset/img2.jpg\', True],\n\t[\'dataset/img5.jpg\', \'dataset/img6.jpg\', True],\n\t[\'dataset/img6.jpg\', \'dataset/img7.jpg\', True],\n\t[\'dataset/img8.jpg\', \'dataset/img9.jpg\', True],\n\t\n\t[\'dataset/img1.jpg\', \'dataset/img11.jpg\', True],\n\t[\'dataset/img2.jpg\', \'dataset/img11.jpg\', True],\n\t\n\t[\'dataset/img1.jpg\', \'dataset/img3.jpg\', False],\n\t[\'dataset/img2.jpg\', \'dataset/img3.jpg\', False],\n\t[\'dataset/img6.jpg\', \'dataset/img8.jpg\', False],\n\t[\'dataset/img6.jpg\', \'dataset/img9.jpg\', False],\n]\n\nmodels = [\'VGG-Face\', \'Facenet\', \'OpenFace\', \'DeepFace\']\nmetrics = [\'cosine\', \'euclidean\', \'euclidean_l2\']\n\npassed_tests = 0; test_cases = 0\n\nfor model in models:\n\tfor metric in metrics:\n\t\tfor instance in dataset:\n\t\t\timg1 = instance[0]\n\t\t\timg2 = instance[1]\n\t\t\tresult = instance[2]\n\t\t\t\n\t\t\tresp_obj = DeepFace.verify(img1, img2, model_name = model, distance_metric = metric)\n\t\t\tprediction = resp_obj[""verified""]\n\t\t\tdistance = round(resp_obj[""distance""], 2)\n\t\t\trequired_threshold = resp_obj[""max_threshold_to_verify""]\n\t\t\t\n\t\t\ttest_result_label = ""failed""\n\t\t\tif prediction == result:\n\t\t\t\tpassed_tests = passed_tests + 1\n\t\t\t\ttest_result_label = ""passed""\n\t\t\t\n\t\t\tif prediction == True:\n\t\t\t\tclassified_label = ""verified""\n\t\t\telse:\n\t\t\t\tclassified_label = ""unverified""\n\t\t\t\n\t\t\ttest_cases = test_cases + 1\n\t\t\t\n\t\t\tprint(img1, "" and "", img2,"" are "", classified_label, "" as same person based on "", model,"" model and "",metric,"" distance metric. Distance: "",distance,"", Required Threshold: "", required_threshold,"" ("",test_result_label,"")"")\n\t\t\n\t\tprint(""--------------------------"")\n\n#-----------------------------------------\n\nprint(""Passed unit tests: "",passed_tests,"" / "",test_cases)\n\naccuracy = 100 * passed_tests / test_cases\naccuracy = round(accuracy, 2)\n\nif accuracy > 75:\n\tprint(""Unit tests are completed successfully. Score: "",accuracy,""%"")\nelse:\n\traise ValueError(""Unit test score does not satisfy the minimum required accuracy. Minimum expected score is 80% but this got "",accuracy,""%"")\n\n#-----------------------------------\n\n# api tests - already built models will be passed to the functions\n\nfrom deepface.basemodels import VGGFace, OpenFace, Facenet, FbDeepFace\n\n#-----------------------------------\n\nvggface_model = VGGFace.loadModel()\nresp_obj = DeepFace.verify(""dataset/img1.jpg"", ""dataset/img2.jpg"", model_name = ""VGG-Face"", model = vggface_model)\nprint(resp_obj)\n\n#-----------------------------------\n\nfrom deepface.extendedmodels import Age, Gender, Race, Emotion\n\nemotion_model = Emotion.loadModel()\nage_model = Age.loadModel()\ngender_model = Gender.loadModel()\nrace_model = Race.loadModel()\n\nfacial_attribute_models = {}\nfacial_attribute_models[""emotion""] = emotion_model\nfacial_attribute_models[""age""] = age_model\nfacial_attribute_models[""gender""] = gender_model\nfacial_attribute_models[""race""] = race_model\n\nresp_obj = DeepFace.analyze(""dataset/img1.jpg"", models=facial_attribute_models)\n'"
deepface/basemodels/Facenet.py,0,"b'import os\nfrom pathlib import Path\nimport gdown\nfrom functools import partial\n\nfrom keras.models import Model\nfrom keras.layers import Activation\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Concatenate\nfrom keras.layers import Conv2D\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import Input\nfrom keras.layers import Lambda\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import add\nfrom keras import backend as K\n\ndef scaling(x, scale):\n\treturn x * scale\n\ndef InceptionResNetV2():\n\t\n\tinputs = Input(shape=(160, 160, 3))\n\tx = Conv2D(32, 3, strides=2, padding=\'valid\', use_bias=False, name= \'Conv2d_1a_3x3\') (inputs)\n\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Conv2d_1a_3x3_BatchNorm\')(x)\n\tx = Activation(\'relu\', name=\'Conv2d_1a_3x3_Activation\')(x)\n\tx = Conv2D(32, 3, strides=1, padding=\'valid\', use_bias=False, name= \'Conv2d_2a_3x3\') (x)\n\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Conv2d_2a_3x3_BatchNorm\')(x)\n\tx = Activation(\'relu\', name=\'Conv2d_2a_3x3_Activation\')(x)\n\tx = Conv2D(64, 3, strides=1, padding=\'same\', use_bias=False, name= \'Conv2d_2b_3x3\') (x)\n\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Conv2d_2b_3x3_BatchNorm\')(x)\n\tx = Activation(\'relu\', name=\'Conv2d_2b_3x3_Activation\')(x)\n\tx = MaxPooling2D(3, strides=2, name=\'MaxPool_3a_3x3\')(x)\n\tx = Conv2D(80, 1, strides=1, padding=\'valid\', use_bias=False, name= \'Conv2d_3b_1x1\') (x)\n\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Conv2d_3b_1x1_BatchNorm\')(x)\n\tx = Activation(\'relu\', name=\'Conv2d_3b_1x1_Activation\')(x)\n\tx = Conv2D(192, 3, strides=1, padding=\'valid\', use_bias=False, name= \'Conv2d_4a_3x3\') (x)\n\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Conv2d_4a_3x3_BatchNorm\')(x)\n\tx = Activation(\'relu\', name=\'Conv2d_4a_3x3_Activation\')(x)\n\tx = Conv2D(256, 3, strides=2, padding=\'valid\', use_bias=False, name= \'Conv2d_4b_3x3\') (x)\n\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Conv2d_4b_3x3_BatchNorm\')(x)\n\tx = Activation(\'relu\', name=\'Conv2d_4b_3x3_Activation\')(x)\n\t\n\t# 5x Block35 (Inception-ResNet-A block):\n\tbranch_0 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_1_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_1_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block35_1_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_1_Branch_1_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_1_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block35_1_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_1_Branch_1_Conv2d_0b_3x3\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_1_Branch_1_Conv2d_0b_3x3_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block35_1_Branch_1_Conv2d_0b_3x3_Activation\')(branch_1)\n\tbranch_2 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_1_Branch_2_Conv2d_0a_1x1\') (x)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_1_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Block35_1_Branch_2_Conv2d_0a_1x1_Activation\')(branch_2)\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_1_Branch_2_Conv2d_0b_3x3\') (branch_2)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_1_Branch_2_Conv2d_0b_3x3_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Block35_1_Branch_2_Conv2d_0b_3x3_Activation\')(branch_2)\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_1_Branch_2_Conv2d_0c_3x3\') (branch_2)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_1_Branch_2_Conv2d_0c_3x3_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Block35_1_Branch_2_Conv2d_0c_3x3_Activation\')(branch_2)\n\tbranches = [branch_0, branch_1, branch_2]\n\tmixed = Concatenate(axis=3, name=\'Block35_1_Concatenate\')(branches)\n\tup = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block35_1_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.17})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block35_1_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_2_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_2_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block35_2_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_2_Branch_1_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_2_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block35_2_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_2_Branch_1_Conv2d_0b_3x3\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_2_Branch_1_Conv2d_0b_3x3_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block35_2_Branch_1_Conv2d_0b_3x3_Activation\')(branch_1)\n\tbranch_2 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_2_Branch_2_Conv2d_0a_1x1\') (x)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_2_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Block35_2_Branch_2_Conv2d_0a_1x1_Activation\')(branch_2)\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_2_Branch_2_Conv2d_0b_3x3\') (branch_2)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_2_Branch_2_Conv2d_0b_3x3_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Block35_2_Branch_2_Conv2d_0b_3x3_Activation\')(branch_2)\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_2_Branch_2_Conv2d_0c_3x3\') (branch_2)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_2_Branch_2_Conv2d_0c_3x3_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Block35_2_Branch_2_Conv2d_0c_3x3_Activation\')(branch_2)\n\tbranches = [branch_0, branch_1, branch_2]\n\tmixed = Concatenate(axis=3, name=\'Block35_2_Concatenate\')(branches)\n\tup = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block35_2_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.17})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block35_2_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_3_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_3_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block35_3_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_3_Branch_1_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_3_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block35_3_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_3_Branch_1_Conv2d_0b_3x3\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_3_Branch_1_Conv2d_0b_3x3_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block35_3_Branch_1_Conv2d_0b_3x3_Activation\')(branch_1)\n\tbranch_2 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_3_Branch_2_Conv2d_0a_1x1\') (x)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_3_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Block35_3_Branch_2_Conv2d_0a_1x1_Activation\')(branch_2)\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_3_Branch_2_Conv2d_0b_3x3\') (branch_2)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_3_Branch_2_Conv2d_0b_3x3_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Block35_3_Branch_2_Conv2d_0b_3x3_Activation\')(branch_2)\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_3_Branch_2_Conv2d_0c_3x3\') (branch_2)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_3_Branch_2_Conv2d_0c_3x3_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Block35_3_Branch_2_Conv2d_0c_3x3_Activation\')(branch_2)\n\tbranches = [branch_0, branch_1, branch_2]\n\tmixed = Concatenate(axis=3, name=\'Block35_3_Concatenate\')(branches)\n\tup = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block35_3_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.17})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block35_3_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_4_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_4_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block35_4_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_4_Branch_1_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_4_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block35_4_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_4_Branch_1_Conv2d_0b_3x3\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_4_Branch_1_Conv2d_0b_3x3_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block35_4_Branch_1_Conv2d_0b_3x3_Activation\')(branch_1)\n\tbranch_2 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_4_Branch_2_Conv2d_0a_1x1\') (x)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_4_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Block35_4_Branch_2_Conv2d_0a_1x1_Activation\')(branch_2)\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_4_Branch_2_Conv2d_0b_3x3\') (branch_2)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_4_Branch_2_Conv2d_0b_3x3_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Block35_4_Branch_2_Conv2d_0b_3x3_Activation\')(branch_2)\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_4_Branch_2_Conv2d_0c_3x3\') (branch_2)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_4_Branch_2_Conv2d_0c_3x3_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Block35_4_Branch_2_Conv2d_0c_3x3_Activation\')(branch_2)\n\tbranches = [branch_0, branch_1, branch_2]\n\tmixed = Concatenate(axis=3, name=\'Block35_4_Concatenate\')(branches)\n\tup = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block35_4_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.17})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block35_4_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_5_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_5_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block35_5_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_5_Branch_1_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_5_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block35_5_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_5_Branch_1_Conv2d_0b_3x3\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_5_Branch_1_Conv2d_0b_3x3_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block35_5_Branch_1_Conv2d_0b_3x3_Activation\')(branch_1)\n\tbranch_2 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_5_Branch_2_Conv2d_0a_1x1\') (x)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_5_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Block35_5_Branch_2_Conv2d_0a_1x1_Activation\')(branch_2)\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_5_Branch_2_Conv2d_0b_3x3\') (branch_2)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_5_Branch_2_Conv2d_0b_3x3_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Block35_5_Branch_2_Conv2d_0b_3x3_Activation\')(branch_2)\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_5_Branch_2_Conv2d_0c_3x3\') (branch_2)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_5_Branch_2_Conv2d_0c_3x3_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Block35_5_Branch_2_Conv2d_0c_3x3_Activation\')(branch_2)\n\tbranches = [branch_0, branch_1, branch_2]\n\tmixed = Concatenate(axis=3, name=\'Block35_5_Concatenate\')(branches)\n\tup = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block35_5_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.17})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block35_5_Activation\')(x)\n\n\t# Mixed 6a (Reduction-A block):\n\tbranch_0 = Conv2D(384, 3, strides=2, padding=\'valid\', use_bias=False, name= \'Mixed_6a_Branch_0_Conv2d_1a_3x3\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_6a_Branch_0_Conv2d_1a_3x3_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Mixed_6a_Branch_0_Conv2d_1a_3x3_Activation\')(branch_0)\n\tbranch_1 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Mixed_6a_Branch_1_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_6a_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Mixed_6a_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(192, 3, strides=1, padding=\'same\', use_bias=False, name= \'Mixed_6a_Branch_1_Conv2d_0b_3x3\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_6a_Branch_1_Conv2d_0b_3x3_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Mixed_6a_Branch_1_Conv2d_0b_3x3_Activation\')(branch_1)\n\tbranch_1 = Conv2D(256, 3, strides=2, padding=\'valid\', use_bias=False, name= \'Mixed_6a_Branch_1_Conv2d_1a_3x3\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_6a_Branch_1_Conv2d_1a_3x3_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Mixed_6a_Branch_1_Conv2d_1a_3x3_Activation\')(branch_1)\n\tbranch_pool = MaxPooling2D(3, strides=2, padding=\'valid\', name=\'Mixed_6a_Branch_2_MaxPool_1a_3x3\')(x)\n\tbranches = [branch_0, branch_1, branch_pool]\n\tx = Concatenate(axis=3, name=\'Mixed_6a\')(branches)\n\n\t# 10x Block17 (Inception-ResNet-B block):\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_1_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_1_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block17_1_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_1_Branch_1_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_1_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_1_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_1_Branch_1_Conv2d_0b_1x7\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_1_Branch_1_Conv2d_0b_1x7_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_1_Branch_1_Conv2d_0b_1x7_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_1_Branch_1_Conv2d_0c_7x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_1_Branch_1_Conv2d_0c_7x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_1_Branch_1_Conv2d_0c_7x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block17_1_Concatenate\')(branches)\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_1_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block17_1_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_2_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_2_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block17_2_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_2_Branch_2_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_2_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_2_Branch_2_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_2_Branch_2_Conv2d_0b_1x7\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_2_Branch_2_Conv2d_0b_1x7_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_2_Branch_2_Conv2d_0b_1x7_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_2_Branch_2_Conv2d_0c_7x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_2_Branch_2_Conv2d_0c_7x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_2_Branch_2_Conv2d_0c_7x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block17_2_Concatenate\')(branches)\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_2_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block17_2_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_3_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_3_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block17_3_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_3_Branch_3_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_3_Branch_3_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_3_Branch_3_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_3_Branch_3_Conv2d_0b_1x7\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_3_Branch_3_Conv2d_0b_1x7_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_3_Branch_3_Conv2d_0b_1x7_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_3_Branch_3_Conv2d_0c_7x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_3_Branch_3_Conv2d_0c_7x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_3_Branch_3_Conv2d_0c_7x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block17_3_Concatenate\')(branches)\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_3_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block17_3_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_4_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_4_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block17_4_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_4_Branch_4_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_4_Branch_4_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_4_Branch_4_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_4_Branch_4_Conv2d_0b_1x7\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_4_Branch_4_Conv2d_0b_1x7_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_4_Branch_4_Conv2d_0b_1x7_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_4_Branch_4_Conv2d_0c_7x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_4_Branch_4_Conv2d_0c_7x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_4_Branch_4_Conv2d_0c_7x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block17_4_Concatenate\')(branches)\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_4_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block17_4_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_5_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_5_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block17_5_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_5_Branch_5_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_5_Branch_5_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_5_Branch_5_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_5_Branch_5_Conv2d_0b_1x7\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_5_Branch_5_Conv2d_0b_1x7_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_5_Branch_5_Conv2d_0b_1x7_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_5_Branch_5_Conv2d_0c_7x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_5_Branch_5_Conv2d_0c_7x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_5_Branch_5_Conv2d_0c_7x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block17_5_Concatenate\')(branches)\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_5_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block17_5_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_6_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_6_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block17_6_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_6_Branch_6_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_6_Branch_6_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_6_Branch_6_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_6_Branch_6_Conv2d_0b_1x7\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_6_Branch_6_Conv2d_0b_1x7_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_6_Branch_6_Conv2d_0b_1x7_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_6_Branch_6_Conv2d_0c_7x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_6_Branch_6_Conv2d_0c_7x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_6_Branch_6_Conv2d_0c_7x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block17_6_Concatenate\')(branches)\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_6_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block17_6_Activation\')(x)\t\n\t\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_7_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_7_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block17_7_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_7_Branch_7_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_7_Branch_7_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_7_Branch_7_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_7_Branch_7_Conv2d_0b_1x7\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_7_Branch_7_Conv2d_0b_1x7_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_7_Branch_7_Conv2d_0b_1x7_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_7_Branch_7_Conv2d_0c_7x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_7_Branch_7_Conv2d_0c_7x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_7_Branch_7_Conv2d_0c_7x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block17_7_Concatenate\')(branches)\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_7_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block17_7_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_8_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_8_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block17_8_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_8_Branch_8_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_8_Branch_8_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_8_Branch_8_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_8_Branch_8_Conv2d_0b_1x7\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_8_Branch_8_Conv2d_0b_1x7_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_8_Branch_8_Conv2d_0b_1x7_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_8_Branch_8_Conv2d_0c_7x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_8_Branch_8_Conv2d_0c_7x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_8_Branch_8_Conv2d_0c_7x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block17_8_Concatenate\')(branches)\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_8_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block17_8_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_9_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_9_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block17_9_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_9_Branch_9_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_9_Branch_9_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_9_Branch_9_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_9_Branch_9_Conv2d_0b_1x7\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_9_Branch_9_Conv2d_0b_1x7_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_9_Branch_9_Conv2d_0b_1x7_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_9_Branch_9_Conv2d_0c_7x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_9_Branch_9_Conv2d_0c_7x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_9_Branch_9_Conv2d_0c_7x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block17_9_Concatenate\')(branches)\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_9_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block17_9_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_10_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_10_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block17_10_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_10_Branch_10_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_10_Branch_10_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_10_Branch_10_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_10_Branch_10_Conv2d_0b_1x7\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_10_Branch_10_Conv2d_0b_1x7_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_10_Branch_10_Conv2d_0b_1x7_Activation\')(branch_1)\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_10_Branch_10_Conv2d_0c_7x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_10_Branch_10_Conv2d_0c_7x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block17_10_Branch_10_Conv2d_0c_7x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block17_10_Concatenate\')(branches)\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_10_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block17_10_Activation\')(x)\n\n\t# Mixed 7a (Reduction-B block): 8 x 8 x 2080\t\n\tbranch_0 = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=False, name= \'Mixed_7a_Branch_0_Conv2d_0a_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_7a_Branch_0_Conv2d_0a_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Mixed_7a_Branch_0_Conv2d_0a_1x1_Activation\')(branch_0)\n\tbranch_0 = Conv2D(384, 3, strides=2, padding=\'valid\', use_bias=False, name= \'Mixed_7a_Branch_0_Conv2d_1a_3x3\') (branch_0)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_7a_Branch_0_Conv2d_1a_3x3_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Mixed_7a_Branch_0_Conv2d_1a_3x3_Activation\')(branch_0)\n\tbranch_1 = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=False, name= \'Mixed_7a_Branch_1_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_7a_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Mixed_7a_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(256, 3, strides=2, padding=\'valid\', use_bias=False, name= \'Mixed_7a_Branch_1_Conv2d_1a_3x3\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_7a_Branch_1_Conv2d_1a_3x3_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Mixed_7a_Branch_1_Conv2d_1a_3x3_Activation\')(branch_1)\n\tbranch_2 = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=False, name= \'Mixed_7a_Branch_2_Conv2d_0a_1x1\') (x)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_7a_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Mixed_7a_Branch_2_Conv2d_0a_1x1_Activation\')(branch_2)\n\tbranch_2 = Conv2D(256, 3, strides=1, padding=\'same\', use_bias=False, name= \'Mixed_7a_Branch_2_Conv2d_0b_3x3\') (branch_2)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_7a_Branch_2_Conv2d_0b_3x3_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Mixed_7a_Branch_2_Conv2d_0b_3x3_Activation\')(branch_2)\n\tbranch_2 = Conv2D(256, 3, strides=2, padding=\'valid\', use_bias=False, name= \'Mixed_7a_Branch_2_Conv2d_1a_3x3\') (branch_2)\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_7a_Branch_2_Conv2d_1a_3x3_BatchNorm\')(branch_2)\n\tbranch_2 = Activation(\'relu\', name=\'Mixed_7a_Branch_2_Conv2d_1a_3x3_Activation\')(branch_2)\n\tbranch_pool = MaxPooling2D(3, strides=2, padding=\'valid\', name=\'Mixed_7a_Branch_3_MaxPool_1a_3x3\')(x)\n\tbranches = [branch_0, branch_1, branch_2, branch_pool]\n\tx = Concatenate(axis=3, name=\'Mixed_7a\')(branches)\n\n\t# 5x Block8 (Inception-ResNet-C block):\n\t\n\tbranch_0 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_1_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_1_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block8_1_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_1_Branch_1_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_1_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_1_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding=\'same\', use_bias=False, name= \'Block8_1_Branch_1_Conv2d_0b_1x3\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_1_Branch_1_Conv2d_0b_1x3_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_1_Branch_1_Conv2d_0b_1x3_Activation\')(branch_1)\n\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block8_1_Branch_1_Conv2d_0c_3x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_1_Branch_1_Conv2d_0c_3x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_1_Branch_1_Conv2d_0c_3x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block8_1_Concatenate\')(branches)\n\tup = Conv2D(1792, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block8_1_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.2})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block8_1_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_2_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_2_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block8_2_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_2_Branch_2_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_2_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_2_Branch_2_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding=\'same\', use_bias=False, name= \'Block8_2_Branch_2_Conv2d_0b_1x3\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_2_Branch_2_Conv2d_0b_1x3_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_2_Branch_2_Conv2d_0b_1x3_Activation\')(branch_1)\n\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block8_2_Branch_2_Conv2d_0c_3x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_2_Branch_2_Conv2d_0c_3x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_2_Branch_2_Conv2d_0c_3x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block8_2_Concatenate\')(branches)\n\tup = Conv2D(1792, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block8_2_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.2})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block8_2_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_3_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_3_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block8_3_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_3_Branch_3_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_3_Branch_3_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_3_Branch_3_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding=\'same\', use_bias=False, name= \'Block8_3_Branch_3_Conv2d_0b_1x3\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_3_Branch_3_Conv2d_0b_1x3_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_3_Branch_3_Conv2d_0b_1x3_Activation\')(branch_1)\n\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block8_3_Branch_3_Conv2d_0c_3x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_3_Branch_3_Conv2d_0c_3x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_3_Branch_3_Conv2d_0c_3x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block8_3_Concatenate\')(branches)\n\tup = Conv2D(1792, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block8_3_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.2})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block8_3_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_4_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_4_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block8_4_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_4_Branch_4_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_4_Branch_4_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_4_Branch_4_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding=\'same\', use_bias=False, name= \'Block8_4_Branch_4_Conv2d_0b_1x3\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_4_Branch_4_Conv2d_0b_1x3_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_4_Branch_4_Conv2d_0b_1x3_Activation\')(branch_1)\n\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block8_4_Branch_4_Conv2d_0c_3x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_4_Branch_4_Conv2d_0c_3x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_4_Branch_4_Conv2d_0c_3x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block8_4_Concatenate\')(branches)\n\tup = Conv2D(1792, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block8_4_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.2})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block8_4_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_5_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_5_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block8_5_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_5_Branch_5_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_5_Branch_5_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_5_Branch_5_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding=\'same\', use_bias=False, name= \'Block8_5_Branch_5_Conv2d_0b_1x3\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_5_Branch_5_Conv2d_0b_1x3_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_5_Branch_5_Conv2d_0b_1x3_Activation\')(branch_1)\n\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block8_5_Branch_5_Conv2d_0c_3x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_5_Branch_5_Conv2d_0c_3x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_5_Branch_5_Conv2d_0c_3x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block8_5_Concatenate\')(branches)\n\tup = Conv2D(1792, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block8_5_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.2})(up)\n\tx = add([x, up])\n\tx = Activation(\'relu\', name=\'Block8_5_Activation\')(x)\n\t\n\tbranch_0 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_6_Branch_0_Conv2d_1x1\') (x)\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_6_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\n\tbranch_0 = Activation(\'relu\', name=\'Block8_6_Branch_0_Conv2d_1x1_Activation\')(branch_0)\n\tbranch_1 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_6_Branch_1_Conv2d_0a_1x1\') (x)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_6_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_6_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\n\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding=\'same\', use_bias=False, name= \'Block8_6_Branch_1_Conv2d_0b_1x3\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_6_Branch_1_Conv2d_0b_1x3_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_6_Branch_1_Conv2d_0b_1x3_Activation\')(branch_1)\n\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block8_6_Branch_1_Conv2d_0c_3x1\') (branch_1)\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_6_Branch_1_Conv2d_0c_3x1_BatchNorm\')(branch_1)\n\tbranch_1 = Activation(\'relu\', name=\'Block8_6_Branch_1_Conv2d_0c_3x1_Activation\')(branch_1)\n\tbranches = [branch_0, branch_1]\n\tmixed = Concatenate(axis=3, name=\'Block8_6_Concatenate\')(branches)\n\tup = Conv2D(1792, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block8_6_Conv2d_1x1\') (mixed)\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 1})(up)\n\tx = add([x, up])\n\t\n\t# Classification block\n\tx = GlobalAveragePooling2D(name=\'AvgPool\')(x)\n\tx = Dropout(1.0 - 0.8, name=\'Dropout\')(x)\n\t# Bottleneck\n\tx = Dense(128, use_bias=False, name=\'Bottleneck\')(x)\n\tx = BatchNormalization(momentum=0.995, epsilon=0.001, scale=False, name=\'Bottleneck_BatchNorm\')(x)\n\n\t# Create model\n\tmodel = Model(inputs, x, name=\'inception_resnet_v1\')\n\n\treturn model\n\ndef loadModel():\n\tmodel = InceptionResNetV2()\n\t\n\t#-----------------------------------\n\t\n\thome = str(Path.home())\n\t\n\tif os.path.isfile(home+\'/.deepface/weights/facenet_weights.h5\') != True:\n\t\tprint(""facenet_weights.h5 will be downloaded..."")\n\t\t\n\t\turl = \'https://drive.google.com/uc?id=1971Xk5RwedbudGgTIrGAL4F7Aifu7id1\'\n\t\toutput = home+\'/.deepface/weights/facenet_weights.h5\'\n\t\tgdown.download(url, output, quiet=False)\n\t\n\t#-----------------------------------\n\t\n\tmodel.load_weights(home+\'/.deepface/weights/facenet_weights.h5\')\n\t\n\t#-----------------------------------\n\t\n\treturn model'"
deepface/basemodels/FbDeepFace.py,0,"b'import os\nfrom pathlib import Path\nimport gdown\nimport keras\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution2D, LocallyConnected2D, MaxPooling2D, Flatten, Dense, Dropout\nimport zipfile\n\n#-------------------------------------\n\ndef loadModel():\n\tbase_model = Sequential()\n\tbase_model.add(Convolution2D(32, (11, 11), activation=\'relu\', name=\'C1\', input_shape=(152, 152, 3)))\n\tbase_model.add(MaxPooling2D(pool_size=3, strides=2, padding=\'same\', name=\'M2\'))\n\tbase_model.add(Convolution2D(16, (9, 9), activation=\'relu\', name=\'C3\'))\n\tbase_model.add(LocallyConnected2D(16, (9, 9), activation=\'relu\', name=\'L4\'))\n\tbase_model.add(LocallyConnected2D(16, (7, 7), strides=2, activation=\'relu\', name=\'L5\') )\n\tbase_model.add(LocallyConnected2D(16, (5, 5), activation=\'relu\', name=\'L6\'))\n\tbase_model.add(Flatten(name=\'F0\'))\n\tbase_model.add(Dense(4096, activation=\'relu\', name=\'F7\'))\n\tbase_model.add(Dropout(rate=0.5, name=\'D0\'))\n\tbase_model.add(Dense(8631, activation=\'softmax\', name=\'F8\'))\n\t\n\t#---------------------------------\n\t\n\thome = str(Path.home())\n\t\n\tif os.path.isfile(home+\'/.deepface/weights/VGGFace2_DeepFace_weights_val-0.9034.h5\') != True:\n\t\tprint(""VGGFace2_DeepFace_weights_val-0.9034.h5 will be downloaded..."")\n\t\t\n\t\turl = \'https://github.com/swghosh/DeepFace/releases/download/weights-vggface2-2d-aligned/VGGFace2_DeepFace_weights_val-0.9034.h5.zip\'\n\t\t\n\t\toutput = home+\'/.deepface/weights/VGGFace2_DeepFace_weights_val-0.9034.h5.zip\'\n\t\t\n\t\tgdown.download(url, output, quiet=False)\n\t\t\n\t\t#unzip VGGFace2_DeepFace_weights_val-0.9034.h5.zip\n\t\twith zipfile.ZipFile(output, \'r\') as zip_ref:\n\t\t\tzip_ref.extractall(home+\'/.deepface/weights/\')\n\t\t\n\tbase_model.load_weights(home+\'/.deepface/weights/VGGFace2_DeepFace_weights_val-0.9034.h5\')\t\n\t\n\t#drop F8 and D0. F7 is the representation layer.\n\tdeepface_model = Model(inputs=base_model.layers[0].input, outputs=base_model.layers[-3].output)\n\t\t\n\treturn deepface_model'"
deepface/basemodels/OpenFace.py,2,"b'import os\nfrom pathlib import Path\nimport gdown\n\nimport tensorflow as tf\nimport keras\nfrom keras.models import Model, Sequential\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\nfrom keras.layers.core import Dense, Activation, Lambda, Flatten\nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\nfrom keras.layers.merge import Concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import load_model\nfrom keras import backend as K\n\n#---------------------------------------\n\ndef loadModel():\n\tmyInput = Input(shape=(96, 96, 3))\n\n\tx = ZeroPadding2D(padding=(3, 3), input_shape=(96, 96, 3))(myInput)\n\tx = Conv2D(64, (7, 7), strides=(2, 2), name=\'conv1\')(x)\n\tx = BatchNormalization(axis=3, epsilon=0.00001, name=\'bn1\')(x)\n\tx = Activation(\'relu\')(x)\n\tx = ZeroPadding2D(padding=(1, 1))(x)\n\tx = MaxPooling2D(pool_size=3, strides=2)(x)\n\tx = Lambda(lambda x: tf.nn.lrn(x, alpha=1e-4, beta=0.75), name=\'lrn_1\')(x)\n\tx = Conv2D(64, (1, 1), name=\'conv2\')(x)\n\tx = BatchNormalization(axis=3, epsilon=0.00001, name=\'bn2\')(x)\n\tx = Activation(\'relu\')(x)\n\tx = ZeroPadding2D(padding=(1, 1))(x)\n\tx = Conv2D(192, (3, 3), name=\'conv3\')(x)\n\tx = BatchNormalization(axis=3, epsilon=0.00001, name=\'bn3\')(x)\n\tx = Activation(\'relu\')(x)\n\tx = Lambda(lambda x: tf.nn.lrn(x, alpha=1e-4, beta=0.75), name=\'lrn_2\')(x) #x is equal added\n\tx = ZeroPadding2D(padding=(1, 1))(x)\n\tx = MaxPooling2D(pool_size=3, strides=2)(x)\n\n\t# Inception3a\n\tinception_3a_3x3 = Conv2D(96, (1, 1), name=\'inception_3a_3x3_conv1\')(x)\n\tinception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3a_3x3_bn1\')(inception_3a_3x3)\n\tinception_3a_3x3 = Activation(\'relu\')(inception_3a_3x3)\n\tinception_3a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3)\n\tinception_3a_3x3 = Conv2D(128, (3, 3), name=\'inception_3a_3x3_conv2\')(inception_3a_3x3)\n\tinception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3a_3x3_bn2\')(inception_3a_3x3)\n\tinception_3a_3x3 = Activation(\'relu\')(inception_3a_3x3)\n\n\tinception_3a_5x5 = Conv2D(16, (1, 1), name=\'inception_3a_5x5_conv1\')(x)\n\tinception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3a_5x5_bn1\')(inception_3a_5x5)\n\tinception_3a_5x5 = Activation(\'relu\')(inception_3a_5x5)\n\tinception_3a_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5)\n\tinception_3a_5x5 = Conv2D(32, (5, 5), name=\'inception_3a_5x5_conv2\')(inception_3a_5x5)\n\tinception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3a_5x5_bn2\')(inception_3a_5x5)\n\tinception_3a_5x5 = Activation(\'relu\')(inception_3a_5x5)\n\n\tinception_3a_pool = MaxPooling2D(pool_size=3, strides=2)(x)\n\tinception_3a_pool = Conv2D(32, (1, 1), name=\'inception_3a_pool_conv\')(inception_3a_pool)\n\tinception_3a_pool = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3a_pool_bn\')(inception_3a_pool)\n\tinception_3a_pool = Activation(\'relu\')(inception_3a_pool)\n\tinception_3a_pool = ZeroPadding2D(padding=((3, 4), (3, 4)))(inception_3a_pool)\n\n\tinception_3a_1x1 = Conv2D(64, (1, 1), name=\'inception_3a_1x1_conv\')(x)\n\tinception_3a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3a_1x1_bn\')(inception_3a_1x1)\n\tinception_3a_1x1 = Activation(\'relu\')(inception_3a_1x1)\n\n\tinception_3a = concatenate([inception_3a_3x3, inception_3a_5x5, inception_3a_pool, inception_3a_1x1], axis=3)\n\n\t# Inception3b\n\tinception_3b_3x3 = Conv2D(96, (1, 1), name=\'inception_3b_3x3_conv1\')(inception_3a)\n\tinception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3b_3x3_bn1\')(inception_3b_3x3)\n\tinception_3b_3x3 = Activation(\'relu\')(inception_3b_3x3)\n\tinception_3b_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3)\n\tinception_3b_3x3 = Conv2D(128, (3, 3), name=\'inception_3b_3x3_conv2\')(inception_3b_3x3)\n\tinception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3b_3x3_bn2\')(inception_3b_3x3)\n\tinception_3b_3x3 = Activation(\'relu\')(inception_3b_3x3)\n\n\tinception_3b_5x5 = Conv2D(32, (1, 1), name=\'inception_3b_5x5_conv1\')(inception_3a)\n\tinception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3b_5x5_bn1\')(inception_3b_5x5)\n\tinception_3b_5x5 = Activation(\'relu\')(inception_3b_5x5)\n\tinception_3b_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5)\n\tinception_3b_5x5 = Conv2D(64, (5, 5), name=\'inception_3b_5x5_conv2\')(inception_3b_5x5)\n\tinception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3b_5x5_bn2\')(inception_3b_5x5)\n\tinception_3b_5x5 = Activation(\'relu\')(inception_3b_5x5)\n\n\tinception_3b_pool = Lambda(lambda x: x**2, name=\'power2_3b\')(inception_3a)\n\tinception_3b_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_3b_pool)\n\tinception_3b_pool = Lambda(lambda x: x*9, name=\'mult9_3b\')(inception_3b_pool)\n\tinception_3b_pool = Lambda(lambda x: K.sqrt(x), name=\'sqrt_3b\')(inception_3b_pool)\n\tinception_3b_pool = Conv2D(64, (1, 1), name=\'inception_3b_pool_conv\')(inception_3b_pool)\n\tinception_3b_pool = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3b_pool_bn\')(inception_3b_pool)\n\tinception_3b_pool = Activation(\'relu\')(inception_3b_pool)\n\tinception_3b_pool = ZeroPadding2D(padding=(4, 4))(inception_3b_pool)\n\n\tinception_3b_1x1 = Conv2D(64, (1, 1), name=\'inception_3b_1x1_conv\')(inception_3a)\n\tinception_3b_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3b_1x1_bn\')(inception_3b_1x1)\n\tinception_3b_1x1 = Activation(\'relu\')(inception_3b_1x1)\n\n\tinception_3b = concatenate([inception_3b_3x3, inception_3b_5x5, inception_3b_pool, inception_3b_1x1], axis=3)\n\n\t# Inception3c\n\tinception_3c_3x3 = Conv2D(128, (1, 1), strides=(1, 1), name=\'inception_3c_3x3_conv1\')(inception_3b)\n\tinception_3c_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3c_3x3_bn1\')(inception_3c_3x3)\n\tinception_3c_3x3 = Activation(\'relu\')(inception_3c_3x3)\n\tinception_3c_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3c_3x3)\n\tinception_3c_3x3 = Conv2D(256, (3, 3), strides=(2, 2), name=\'inception_3c_3x3_conv\'+\'2\')(inception_3c_3x3)\n\tinception_3c_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3c_3x3_bn\'+\'2\')(inception_3c_3x3)\n\tinception_3c_3x3 = Activation(\'relu\')(inception_3c_3x3)\n\n\tinception_3c_5x5 = Conv2D(32, (1, 1), strides=(1, 1), name=\'inception_3c_5x5_conv1\')(inception_3b)\n\tinception_3c_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3c_5x5_bn1\')(inception_3c_5x5)\n\tinception_3c_5x5 = Activation(\'relu\')(inception_3c_5x5)\n\tinception_3c_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3c_5x5)\n\tinception_3c_5x5 = Conv2D(64, (5, 5), strides=(2, 2), name=\'inception_3c_5x5_conv\'+\'2\')(inception_3c_5x5)\n\tinception_3c_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3c_5x5_bn\'+\'2\')(inception_3c_5x5)\n\tinception_3c_5x5 = Activation(\'relu\')(inception_3c_5x5)\n\n\tinception_3c_pool = MaxPooling2D(pool_size=3, strides=2)(inception_3b)\n\tinception_3c_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_3c_pool)\n\n\tinception_3c = concatenate([inception_3c_3x3, inception_3c_5x5, inception_3c_pool], axis=3)\n\n\t#inception 4a\n\tinception_4a_3x3 = Conv2D(96, (1, 1), strides=(1, 1), name=\'inception_4a_3x3_conv\'+\'1\')(inception_3c)\n\tinception_4a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4a_3x3_bn\'+\'1\')(inception_4a_3x3)\n\tinception_4a_3x3 = Activation(\'relu\')(inception_4a_3x3)\n\tinception_4a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_4a_3x3)\n\tinception_4a_3x3 = Conv2D(192, (3, 3), strides=(1, 1), name=\'inception_4a_3x3_conv\'+\'2\')(inception_4a_3x3)\n\tinception_4a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4a_3x3_bn\'+\'2\')(inception_4a_3x3)\n\tinception_4a_3x3 = Activation(\'relu\')(inception_4a_3x3)\n\n\tinception_4a_5x5 = Conv2D(32, (1,1), strides=(1,1), name=\'inception_4a_5x5_conv1\')(inception_3c)\n\tinception_4a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4a_5x5_bn1\')(inception_4a_5x5)\n\tinception_4a_5x5 = Activation(\'relu\')(inception_4a_5x5)\n\tinception_4a_5x5 = ZeroPadding2D(padding=(2,2))(inception_4a_5x5)\n\tinception_4a_5x5 = Conv2D(64, (5,5), strides=(1,1), name=\'inception_4a_5x5_conv\'+\'2\')(inception_4a_5x5)\n\tinception_4a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4a_5x5_bn\'+\'2\')(inception_4a_5x5)\n\tinception_4a_5x5 = Activation(\'relu\')(inception_4a_5x5)\n\n\tinception_4a_pool = Lambda(lambda x: x**2, name=\'power2_4a\')(inception_3c)\n\tinception_4a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_4a_pool)\n\tinception_4a_pool = Lambda(lambda x: x*9, name=\'mult9_4a\')(inception_4a_pool)\n\tinception_4a_pool = Lambda(lambda x: K.sqrt(x), name=\'sqrt_4a\')(inception_4a_pool)\n\n\tinception_4a_pool = Conv2D(128, (1,1), strides=(1,1), name=\'inception_4a_pool_conv\'+\'\')(inception_4a_pool)\n\tinception_4a_pool = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4a_pool_bn\'+\'\')(inception_4a_pool)\n\tinception_4a_pool = Activation(\'relu\')(inception_4a_pool)\n\tinception_4a_pool = ZeroPadding2D(padding=(2, 2))(inception_4a_pool)\n\n\tinception_4a_1x1 = Conv2D(256, (1, 1), strides=(1, 1), name=\'inception_4a_1x1_conv\'+\'\')(inception_3c)\n\tinception_4a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4a_1x1_bn\'+\'\')(inception_4a_1x1)\n\tinception_4a_1x1 = Activation(\'relu\')(inception_4a_1x1)\n\n\tinception_4a = concatenate([inception_4a_3x3, inception_4a_5x5, inception_4a_pool, inception_4a_1x1], axis=3)\n\n\t#inception4e\n\tinception_4e_3x3 = Conv2D(160, (1,1), strides=(1,1), name=\'inception_4e_3x3_conv\'+\'1\')(inception_4a)\n\tinception_4e_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4e_3x3_bn\'+\'1\')(inception_4e_3x3)\n\tinception_4e_3x3 = Activation(\'relu\')(inception_4e_3x3)\n\tinception_4e_3x3 = ZeroPadding2D(padding=(1, 1))(inception_4e_3x3)\n\tinception_4e_3x3 = Conv2D(256, (3,3), strides=(2,2), name=\'inception_4e_3x3_conv\'+\'2\')(inception_4e_3x3)\n\tinception_4e_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4e_3x3_bn\'+\'2\')(inception_4e_3x3)\n\tinception_4e_3x3 = Activation(\'relu\')(inception_4e_3x3)\n\n\tinception_4e_5x5 = Conv2D(64, (1,1), strides=(1,1), name=\'inception_4e_5x5_conv\'+\'1\')(inception_4a)\n\tinception_4e_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4e_5x5_bn\'+\'1\')(inception_4e_5x5)\n\tinception_4e_5x5 = Activation(\'relu\')(inception_4e_5x5)\n\tinception_4e_5x5 = ZeroPadding2D(padding=(2, 2))(inception_4e_5x5)\n\tinception_4e_5x5 = Conv2D(128, (5,5), strides=(2,2), name=\'inception_4e_5x5_conv\'+\'2\')(inception_4e_5x5)\n\tinception_4e_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4e_5x5_bn\'+\'2\')(inception_4e_5x5)\n\tinception_4e_5x5 = Activation(\'relu\')(inception_4e_5x5)\n\n\tinception_4e_pool = MaxPooling2D(pool_size=3, strides=2)(inception_4a)\n\tinception_4e_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_4e_pool)\n\n\tinception_4e = concatenate([inception_4e_3x3, inception_4e_5x5, inception_4e_pool], axis=3)\n\n\t#inception5a\n\tinception_5a_3x3 = Conv2D(96, (1,1), strides=(1,1), name=\'inception_5a_3x3_conv\'+\'1\')(inception_4e)\n\tinception_5a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5a_3x3_bn\'+\'1\')(inception_5a_3x3)\n\tinception_5a_3x3 = Activation(\'relu\')(inception_5a_3x3)\n\tinception_5a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_5a_3x3)\n\tinception_5a_3x3 = Conv2D(384, (3,3), strides=(1,1), name=\'inception_5a_3x3_conv\'+\'2\')(inception_5a_3x3)\n\tinception_5a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5a_3x3_bn\'+\'2\')(inception_5a_3x3)\n\tinception_5a_3x3 = Activation(\'relu\')(inception_5a_3x3)\n\n\tinception_5a_pool = Lambda(lambda x: x**2, name=\'power2_5a\')(inception_4e)\n\tinception_5a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_5a_pool)\n\tinception_5a_pool = Lambda(lambda x: x*9, name=\'mult9_5a\')(inception_5a_pool)\n\tinception_5a_pool = Lambda(lambda x: K.sqrt(x), name=\'sqrt_5a\')(inception_5a_pool)\n\n\tinception_5a_pool = Conv2D(96, (1,1), strides=(1,1), name=\'inception_5a_pool_conv\'+\'\')(inception_5a_pool)\n\tinception_5a_pool = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5a_pool_bn\'+\'\')(inception_5a_pool)\n\tinception_5a_pool = Activation(\'relu\')(inception_5a_pool)\n\tinception_5a_pool = ZeroPadding2D(padding=(1,1))(inception_5a_pool)\n\n\tinception_5a_1x1 = Conv2D(256, (1,1), strides=(1,1), name=\'inception_5a_1x1_conv\'+\'\')(inception_4e)\n\tinception_5a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5a_1x1_bn\'+\'\')(inception_5a_1x1)\n\tinception_5a_1x1 = Activation(\'relu\')(inception_5a_1x1)\n\n\tinception_5a = concatenate([inception_5a_3x3, inception_5a_pool, inception_5a_1x1], axis=3)\n\n\t#inception_5b\n\tinception_5b_3x3 = Conv2D(96, (1,1), strides=(1,1), name=\'inception_5b_3x3_conv\'+\'1\')(inception_5a)\n\tinception_5b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5b_3x3_bn\'+\'1\')(inception_5b_3x3)\n\tinception_5b_3x3 = Activation(\'relu\')(inception_5b_3x3)\n\tinception_5b_3x3 = ZeroPadding2D(padding=(1,1))(inception_5b_3x3)\n\tinception_5b_3x3 = Conv2D(384, (3,3), strides=(1,1), name=\'inception_5b_3x3_conv\'+\'2\')(inception_5b_3x3)\n\tinception_5b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5b_3x3_bn\'+\'2\')(inception_5b_3x3)\n\tinception_5b_3x3 = Activation(\'relu\')(inception_5b_3x3)\n\n\tinception_5b_pool = MaxPooling2D(pool_size=3, strides=2)(inception_5a)\n\n\tinception_5b_pool = Conv2D(96, (1,1), strides=(1,1), name=\'inception_5b_pool_conv\'+\'\')(inception_5b_pool)\n\tinception_5b_pool = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5b_pool_bn\'+\'\')(inception_5b_pool)\n\tinception_5b_pool = Activation(\'relu\')(inception_5b_pool)\n\n\tinception_5b_pool = ZeroPadding2D(padding=(1, 1))(inception_5b_pool)\n\n\tinception_5b_1x1 = Conv2D(256, (1,1), strides=(1,1), name=\'inception_5b_1x1_conv\'+\'\')(inception_5a)\n\tinception_5b_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5b_1x1_bn\'+\'\')(inception_5b_1x1)\n\tinception_5b_1x1 = Activation(\'relu\')(inception_5b_1x1)\n\n\tinception_5b = concatenate([inception_5b_3x3, inception_5b_pool, inception_5b_1x1], axis=3)\n\n\tav_pool = AveragePooling2D(pool_size=(3, 3), strides=(1, 1))(inception_5b)\n\treshape_layer = Flatten()(av_pool)\n\tdense_layer = Dense(128, name=\'dense_layer\')(reshape_layer)\n\tnorm_layer = Lambda(lambda  x: K.l2_normalize(x, axis=1), name=\'norm_layer\')(dense_layer)\n\n\t# Final Model\n\tmodel = Model(inputs=[myInput], outputs=norm_layer)\n\t\n\t#-----------------------------------\n\t\n\thome = str(Path.home())\n\t\n\tif os.path.isfile(home+\'/.deepface/weights/openface_weights.h5\') != True:\n\t\tprint(""openface_weights.h5 will be downloaded..."")\n\t\t\n\t\turl = \'https://drive.google.com/uc?id=1LSe1YCV1x-BfNnfb7DFZTNpv_Q9jITxn\'\n\t\toutput = home+\'/.deepface/weights/openface_weights.h5\'\n\t\tgdown.download(url, output, quiet=False)\n\t\n\t#-----------------------------------\n\t\n\tmodel.load_weights(home+\'/.deepface/weights/openface_weights.h5\')\n\t\n\t#-----------------------------------\n\t\n\treturn model'"
deepface/basemodels/VGGFace.py,0,"b'import os\nfrom pathlib import Path\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\nimport gdown\n\n#---------------------------------------\n\ndef baseModel():\n\tmodel = Sequential()\n\tmodel.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n\tmodel.add(Convolution2D(64, (3, 3), activation=\'relu\'))\n\tmodel.add(ZeroPadding2D((1,1)))\n\tmodel.add(Convolution2D(64, (3, 3), activation=\'relu\'))\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n\n\tmodel.add(ZeroPadding2D((1,1)))\n\tmodel.add(Convolution2D(128, (3, 3), activation=\'relu\'))\n\tmodel.add(ZeroPadding2D((1,1)))\n\tmodel.add(Convolution2D(128, (3, 3), activation=\'relu\'))\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n\n\tmodel.add(ZeroPadding2D((1,1)))\n\tmodel.add(Convolution2D(256, (3, 3), activation=\'relu\'))\n\tmodel.add(ZeroPadding2D((1,1)))\n\tmodel.add(Convolution2D(256, (3, 3), activation=\'relu\'))\n\tmodel.add(ZeroPadding2D((1,1)))\n\tmodel.add(Convolution2D(256, (3, 3), activation=\'relu\'))\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n\n\tmodel.add(ZeroPadding2D((1,1)))\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\n\tmodel.add(ZeroPadding2D((1,1)))\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\n\tmodel.add(ZeroPadding2D((1,1)))\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n\n\tmodel.add(ZeroPadding2D((1,1)))\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\n\tmodel.add(ZeroPadding2D((1,1)))\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\n\tmodel.add(ZeroPadding2D((1,1)))\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n\n\tmodel.add(Convolution2D(4096, (7, 7), activation=\'relu\'))\n\tmodel.add(Dropout(0.5))\n\tmodel.add(Convolution2D(4096, (1, 1), activation=\'relu\'))\n\tmodel.add(Dropout(0.5))\n\tmodel.add(Convolution2D(2622, (1, 1)))\n\tmodel.add(Flatten())\n\tmodel.add(Activation(\'softmax\'))\n\t\n\treturn model\n\ndef loadModel():\n\t\n\tmodel = baseModel()\n\t\n\t#-----------------------------------\n\t\n\thome = str(Path.home())\n\t\n\tif os.path.isfile(home+\'/.deepface/weights/vgg_face_weights.h5\') != True:\n\t\tprint(""vgg_face_weights.h5 will be downloaded..."")\n\t\t\n\t\turl = \'https://drive.google.com/uc?id=1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo\'\n\t\toutput = home+\'/.deepface/weights/vgg_face_weights.h5\'\n\t\tgdown.download(url, output, quiet=False)\n\t\n\t#-----------------------------------\n\t\n\tmodel.load_weights(home+\'/.deepface/weights/vgg_face_weights.h5\')\n\t\n\t#-----------------------------------\n\t\n\t#TO-DO: why?\n\tvgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)\n\t\n\treturn vgg_face_descriptor'"
deepface/basemodels/__init__.py,0,b''
deepface/commons/__init__.py,0,b''
deepface/commons/distance.py,0,"b'import numpy as np\n\ndef findCosineDistance(source_representation, test_representation):\n    a = np.matmul(np.transpose(source_representation), test_representation)\n    b = np.sum(np.multiply(source_representation, source_representation))\n    c = np.sum(np.multiply(test_representation, test_representation))\n    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n\ndef findEuclideanDistance(source_representation, test_representation):\n    euclidean_distance = source_representation - test_representation\n    euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\n    euclidean_distance = np.sqrt(euclidean_distance)\n    return euclidean_distance\n\ndef l2_normalize(x):\n    return x / np.sqrt(np.sum(np.multiply(x, x)))\t\n\n""""""def l2_normalize(x, axis=-1, epsilon=1e-10):\n    output = x / np.sqrt(np.maximum(np.sum(np.square(x), axis=axis, keepdims=True), epsilon))\n    return output""""""'"
deepface/commons/functions.py,2,"b'import os\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing.image import load_img, save_img, img_to_array\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom keras.preprocessing import image\nimport cv2\nfrom pathlib import Path\nimport gdown\nimport hashlib\nimport math\nfrom PIL import Image\nimport copy\nimport base64\nimport multiprocessing\nimport subprocess\nimport tensorflow as tf\nimport keras\n\ndef loadBase64Img(uri):\n   encoded_data = uri.split(\',\')[1]\n   nparr = np.fromstring(base64.b64decode(encoded_data), np.uint8)\n   img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n   return img\n\ndef distance(a, b):\n\tx1 = a[0]; y1 = a[1]\n\tx2 = b[0]; y2 = b[1]\n\t\n\treturn math.sqrt(((x2 - x1) * (x2 - x1)) + ((y2 - y1) * (y2 - y1)))\n\ndef findFileHash(file):\n\tBLOCK_SIZE = 65536 # The size of each read from the file\n\n\tfile_hash = hashlib.sha256() # Create the hash object, can use something other than `.sha256()` if you wish\n\twith open(file, \'rb\') as f: # Open the file to read it\'s bytes\n\t\tfb = f.read(BLOCK_SIZE) # Read from the file. Take in the amount declared above\n\t\twhile len(fb) > 0: # While there is still data being read from the file\n\t\t\tfile_hash.update(fb) # Update the hash\n\t\t\tfb = f.read(BLOCK_SIZE) # Read the next block from the file\n\t\n\treturn file_hash.hexdigest()\n\ndef initializeFolder():\n\t\n\thome = str(Path.home())\n\t\n\tif not os.path.exists(home+""/.deepface""):\n\t\tos.mkdir(home+""/.deepface"")\n\t\tprint(""Directory "",home,""/.deepface created"")\n\t\n\tif not os.path.exists(home+""/.deepface/weights""):\n\t\tos.mkdir(home+""/.deepface/weights"")\n\t\tprint(""Directory "",home,""/.deepface/weights created"")\n\t\n\t#----------------------------------\n\t""""""\n\t#avoid interrupted file download\n\t\n\tweight_hashes = [\n\t\t[\'age_model_weights.h5\', \'0aeff75734bfe794113756d2bfd0ac823d51e9422c8961125b570871d3c2b114\']\n\t\t, [\'facenet_weights.h5\', \'90659cc97bfda5999120f95d8e122f4d262cca11715a21e59ba024bcce816d5c\']\n\t\t, [\'facial_expression_model_weights.h5\', \'e8e8851d3fa05c001b1c27fd8841dfe08d7f82bb786a53ad8776725b7a1e824c\']\n\t\t, [\'gender_model_weights.h5\', \'45513ce5678549112d25ab85b1926fb65986507d49c674a3d04b2ba70dba2eb5\']\n\t\t, [\'openface_weights.h5\', \'5b41897ec6dd762cee20575eee54ed4d719a78cb982b2080a87dc14887d88a7a\']\n\t\t, [\'race_model_single_batch.h5\', \'eb22b28b1f6dfce65b64040af4e86003a5edccb169a1a338470dde270b6f5e54\']\n\t\t, [\'vgg_face_weights.h5\', \'759266b9614d0fd5d65b97bf716818b746cc77ab5944c7bffc937c6ba9455d8c\']\n\t]\n\t\n\tfor i in weight_hashes:\n\t\t\n\t\tweight_file = home+""/.deepface/weights/""+i[0]\n\t\texpected_hash = i[1]\n\t\t\n\t\t#check file exits\n\t\tif os.path.isfile(weight_file) == True:\n\t\t\tcurrent_hash = findFileHash(weight_file)\n\t\t\tif current_hash != expected_hash:\n\t\t\t\tprint(""hash violated for "", i[0],"". It\'s going to be removed."")\n\t\t\t\tos.remove(weight_file)\n\t""""""\n\t#----------------------------------\n\n""""""\nTODO: C4.5 tree finds the following split points for cosine, euclidean, euclidean_l2 respectively.\nCheck these thresholds in unit tests.\nvgg-face: 0.3147, 0.4764, 0.7933\nfacenet: 0.4062, 11.2632, 0.9014\nopenface: 0.1118, 0.4729, 0.4729\ndeepface: 0.1349, 42.2178, 0.5194\n""""""\n\n""""""\nTODO: create an ensemble method\n""""""\ndef findThreshold(model_name, distance_metric):\n\t\n\tthreshold = 0.40\n\t\n\tif model_name == \'VGG-Face\':\n\t\tif distance_metric == \'cosine\':\n\t\t\tthreshold = 0.40\n\t\telif distance_metric == \'euclidean\':\n\t\t\tthreshold = 0.55\n\t\telif distance_metric == \'euclidean_l2\':\n\t\t\tthreshold = 0.75\t\n\t\n\telif model_name == \'OpenFace\':\n\t\tif distance_metric == \'cosine\':\n\t\t\tthreshold = 0.10\n\t\telif distance_metric == \'euclidean\':\n\t\t\tthreshold = 0.55\n\t\telif distance_metric == \'euclidean_l2\':\n\t\t\tthreshold = 0.55\n\t\n\telif model_name == \'Facenet\':\n\t\tif distance_metric == \'cosine\':\n\t\t\tthreshold = 0.40\n\t\telif distance_metric == \'euclidean\':\n\t\t\tthreshold = 10\n\t\telif distance_metric == \'euclidean_l2\':\n\t\t\tthreshold = 0.80\n\t\n\telif model_name == \'DeepFace\':\n\t\tif distance_metric == \'cosine\':\n\t\t\tthreshold = 0.23\n\t\telif distance_metric == \'euclidean\':\n\t\t\tthreshold = 64\n\t\telif distance_metric == \'euclidean_l2\':\n\t\t\tthreshold = 0.64\n\t\n\treturn threshold\n\ndef get_opencv_path():\n\topencv_home = cv2.__file__\n\tfolders = opencv_home.split(os.path.sep)[0:-1]\n\t\n\tpath = folders[0]\n\tfor folder in folders[1:]:\n\t\tpath = path + ""/"" + folder\n\n\tface_detector_path = path+""/data/haarcascade_frontalface_default.xml""\n\teye_detector_path = path+""/data/haarcascade_eye.xml""\n\t\n\tif os.path.isfile(face_detector_path) != True:\n\t\traise ValueError(""Confirm that opencv is installed on your environment! Expected path "",face_detector_path,"" violated."")\n\t\n\treturn path+""/data/""\n\ndef detectFace(img, target_size=(224, 224), grayscale = False, enforce_detection = True):\n\t\n\timg_path = """"\n\t\n\t#-----------------------\n\t\n\texact_image = False\n\tif type(img).__module__ == np.__name__:\n\t\texact_image = True\n\t\n\tbase64_img = False\n\tif len(img) > 11 and img[0:11] == ""data:image/"":\n\t\tbase64_img = True\n\n\t#-----------------------\n\t\n\topencv_path = get_opencv_path()\n\tface_detector_path = opencv_path+""haarcascade_frontalface_default.xml""\n\teye_detector_path = opencv_path+""haarcascade_eye.xml""\n\t\n\tif os.path.isfile(face_detector_path) != True:\n\t\traise ValueError(""Confirm that opencv is installed on your environment! Expected path "",face_detector_path,"" violated."")\n\t\n\t#--------------------------------\n\t\n\tface_detector = cv2.CascadeClassifier(face_detector_path)\n\teye_detector = cv2.CascadeClassifier(eye_detector_path)\n\t\n\tif base64_img == True:\n\t\timg = loadBase64Img(img)\n\t\t\n\telif exact_image != True: #image path passed as input\n\t\t\n\t\tif os.path.isfile(img) != True:\n\t\t\traise ValueError(""Confirm that "",img,"" exists"")\n\t\t\n\t\timg = cv2.imread(img)\n\t\n\timg_raw = img.copy()\n\t\n\t#--------------------------------\n\t\n\tfaces = face_detector.detectMultiScale(img, 1.3, 5)\n\t\n\t#print(""found faces in "",image_path,"" is "",len(faces))\n\t\n\tif len(faces) > 0:\n\t\tx,y,w,h = faces[0]\n\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)]\n\t\tdetected_face_gray = cv2.cvtColor(detected_face, cv2.COLOR_BGR2GRAY)\n\t\t\n\t\t#---------------------------\n\t\t#face alignment\n\t\t\n\t\teyes = eye_detector.detectMultiScale(detected_face_gray)\n\t\t\n\t\tif len(eyes) >= 2:\n\t\t\t#find the largest 2 eye\n\t\t\tbase_eyes = eyes[:, 2]\n\t\t\t\n\t\t\titems = []\n\t\t\tfor i in range(0, len(base_eyes)):\n\t\t\t\titem = (base_eyes[i], i)\n\t\t\t\titems.append(item)\n\t\t\t\n\t\t\tdf = pd.DataFrame(items, columns = [""length"", ""idx""]).sort_values(by=[\'length\'], ascending=False)\n\t\t\t\n\t\t\teyes = eyes[df.idx.values[0:2]]\n\t\t\t\n\t\t\t#-----------------------\n\t\t\t#decide left and right eye\n\t\t\t\n\t\t\teye_1 = eyes[0]; eye_2 = eyes[1]\n\t\t\t\n\t\t\tif eye_1[0] < eye_2[0]:\n\t\t\t\tleft_eye = eye_1\n\t\t\t\tright_eye = eye_2\n\t\t\telse:\n\t\t\t\tleft_eye = eye_2\n\t\t\t\tright_eye = eye_1\n\t\t\t\n\t\t\t#-----------------------\n\t\t\t#find center of eyes\n\t\t\t\n\t\t\tleft_eye_center = (int(left_eye[0] + (left_eye[2] / 2)), int(left_eye[1] + (left_eye[3] / 2)))\n\t\t\tleft_eye_x = left_eye_center[0]; left_eye_y = left_eye_center[1]\n\t\t\t\n\t\t\tright_eye_center = (int(right_eye[0] + (right_eye[2]/2)), int(right_eye[1] + (right_eye[3]/2)))\n\t\t\tright_eye_x = right_eye_center[0]; right_eye_y = right_eye_center[1]\n\t\t\t\n\t\t\t#-----------------------\n\t\t\t#find rotation direction\n\t\t\t\t\n\t\t\tif left_eye_y > right_eye_y:\n\t\t\t\tpoint_3rd = (right_eye_x, left_eye_y)\n\t\t\t\tdirection = -1 #rotate same direction to clock\n\t\t\telse:\n\t\t\t\tpoint_3rd = (left_eye_x, right_eye_y)\n\t\t\t\tdirection = 1 #rotate inverse direction of clock\n\t\t\t\n\t\t\t#-----------------------\n\t\t\t#find length of triangle edges\n\t\t\t\n\t\t\ta = distance(left_eye_center, point_3rd)\n\t\t\tb = distance(right_eye_center, point_3rd)\n\t\t\tc = distance(right_eye_center, left_eye_center)\n\t\t\t\n\t\t\t#-----------------------\n\t\t\t#apply cosine rule\n\t\t\t\n\t\t\tcos_a = (b*b + c*c - a*a)/(2*b*c)\n\t\t\tangle = np.arccos(cos_a) #angle in radian\n\t\t\tangle = (angle * 180) / math.pi #radian to degree\n\t\t\t\n\t\t\t#-----------------------\n\t\t\t#rotate base image\n\t\t\t\n\t\t\tif direction == -1:\n\t\t\t\tangle = 90 - angle\n\t\t\t\n\t\t\timg = Image.fromarray(img_raw)\n\t\t\timg = np.array(img.rotate(direction * angle))\n\t\t\t\n\t\t\t#you recover the base image and face detection disappeared. apply again.\n\t\t\tfaces = face_detector.detectMultiScale(img, 1.3, 5)\n\t\t\tif len(faces) > 0:\n\t\t\t\tx,y,w,h = faces[0]\n\t\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)]\n\t\t\t\n\t\t\t#-----------------------\n\t\t\n\t\t#face alignment block end\n\t\t#---------------------------\n\t\t\n\t\t#face alignment block needs colorful images. that\'s why, converting to gray scale logic moved to here.\n\t\tif grayscale == True:\n\t\t\tdetected_face = cv2.cvtColor(detected_face, cv2.COLOR_BGR2GRAY)\n\t\t\n\t\tdetected_face = cv2.resize(detected_face, target_size)\n\t\t\n\t\timg_pixels = image.img_to_array(detected_face)\n\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\n\t\t\n\t\t#normalize input in [0, 1]\n\t\timg_pixels /= 255\n\t\t\n\t\treturn img_pixels\n\t\t\n\telse:\n\t\t\n\t\tif (exact_image == True) or (enforce_detection != True):\n\t\t\t\n\t\t\tif grayscale == True:\n\t\t\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\t\t\t\n\t\t\timg = cv2.resize(img, target_size)\n\t\t\timg_pixels = image.img_to_array(img)\n\t\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\n\t\t\timg_pixels /= 255\n\t\t\treturn img_pixels\n\t\telse:\n\t\t\traise ValueError(""Face could not be detected. Please confirm that the picture is a face photo or consider to set enforce_detection param to False."")\n\t\t\t\ndef allocateMemory():\n\t\n\t#find allocated memories\n\tgpu_indexes = []\n\tmemory_usage_percentages = []; available_memories = []; total_memories = []; utilizations = []\n\tpower_usages = []; power_capacities = []\n\t\n\ttry:\n\t\tresult = subprocess.check_output([\'nvidia-smi\'])\n\n\t\tdashboard = result.decode(""utf-8"").split(""=|"")\n\n\t\tdashboard = dashboard[1].split(""\\n"")\n\t\t\n\t\tgpu_idx = 0\n\t\tfor line in dashboard:\n\t\t\tif (""MiB"" in line):\n\t\t\t\tpower_info = line.split(""|"")[1]\n\t\t\t\tpower_capacity = int(power_info.split(""/"")[-1].replace(""W"", """"))\n\t\t\t\tpower_usage = int((power_info.split(""/"")[-2]).strip().split("" "")[-1].replace(""W"", """"))\n\t\t\t\t\n\t\t\t\tpower_usages.append(power_usage)\n\t\t\t\tpower_capacities.append(power_capacity)\n\t\t\t\t\n\t\t\t\t#----------------------------\n\t\t\t\t\n\t\t\t\tmemory_info = line.split(""|"")[2].replace(""MiB"","""").split(""/"")\n\t\t\t\tutilization_info = int(line.split(""|"")[3].split(""%"")[0])\n\t\t\t\t\n\t\t\t\tallocated = int(memory_info[0])\n\t\t\t\ttotal_memory = int(memory_info[1])\n\t\t\t\tavailable_memory = total_memory - allocated\n\t\t\t\t\n\t\t\t\ttotal_memories.append(total_memory)\n\t\t\t\tavailable_memories.append(available_memory)\n\t\t\t\tmemory_usage_percentages.append(round(100*int(allocated)/int(total_memory), 4))\n\t\t\t\tutilizations.append(utilization_info)\n\t\t\t\tgpu_indexes.append(gpu_idx)\n\t\t\t\t\n\t\t\t\tgpu_idx = gpu_idx + 1\n\t\t\n\t\tgpu_count = gpu_idx * 1\n\t\t\t\t\n\texcept Exception as err:\n\t\tgpu_count = 0\n\t\t#print(str(err))\n\t\t\n\t#------------------------------\n\t\n\tdf = pd.DataFrame(gpu_indexes, columns = [""gpu_index""])\n\tdf[""total_memories_in_mb""] = total_memories\n\tdf[""available_memories_in_mb""] = available_memories\n\tdf[""memory_usage_percentage""] = memory_usage_percentages\n\tdf[""utilizations""] = utilizations\n\tdf[""power_usages_in_watts""] = power_usages\n\tdf[""power_capacities_in_watts""] = power_capacities\n\t\n\tdf = df.sort_values(by = [""available_memories_in_mb""], ascending = False).reset_index(drop = True)\n\t\n\t#------------------------------\n\t\n\trequired_memory = 10000 #All deepface models require 9016 MiB\n\t\n\tif df.shape[0] > 0: #has gpu\n\t\tif df.iloc[0].available_memories_in_mb > required_memory:\n\t\t\tmy_gpu = str(int(df.iloc[0].gpu_index))\n\t\t\tos.environ[""CUDA_VISIBLE_DEVICES""] = my_gpu\n\t\t\t\n\t\t\t#------------------------------\n\t\t\t#tf allocates all memory by default\n\t\t\t#this block avoids greedy approach\n\t\t\t\n\t\t\tconfig = tf.ConfigProto()\n\t\t\tconfig.gpu_options.allow_growth = True\n\t\t\tsession = tf.Session(config=config)\n\t\t\tkeras.backend.set_session(session)\n\t\t\t\n\t\t\tprint(""DeepFace will run on GPU (gpu_"", my_gpu,"")"")\n\t\telse:\n\t\t\t#this case has gpu but no enough memory to allocate\n\t\t\tos.environ[""CUDA_VISIBLE_DEVICES""] = """" #run it on cpu\n\t\t\tprint(""Even though the system has GPUs, there is no enough space in memory to allocate."")\n\t\t\tprint(""DeepFace will run on CPU"")\n\telse:\n\t\tprint(""DeepFace will run on CPU"")\n\t\n\t#------------------------------\n'"
deepface/commons/realtime.py,0,"b'import os\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport time\nimport re\n\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\n#from basemodels import VGGFace, OpenFace, Facenet, FbDeepFace\n#from extendedmodels import Age, Gender, Race, Emotion\n#from commons import functions, realtime, distance as dst\n\nfrom deepface.basemodels import VGGFace, OpenFace, Facenet, FbDeepFace\nfrom deepface.extendedmodels import Age, Gender, Race, Emotion\nfrom deepface.commons import functions, realtime, distance as dst\n\ndef analysis(db_path, model_name, distance_metric, enable_face_analysis = True):\n\t\n\tinput_shape = (224, 224)\n\ttext_color = (255,255,255)\n\t\n\temployees = []\n\t#check passed db folder exists\n\tif os.path.isdir(db_path) == True:\n\t\tfor r, d, f in os.walk(db_path): # r=root, d=directories, f = files\n\t\t\tfor file in f:\n\t\t\t\tif (\'.jpg\' in file):\n\t\t\t\t\t#exact_path = os.path.join(r, file)\n\t\t\t\t\texact_path = r + ""/"" + file\n\t\t\t\t\t#print(exact_path)\n\t\t\t\t\temployees.append(exact_path)\n\t\t\t\t\t\n\t\n\t#------------------------\n\t\n\tif len(employees) > 0:\n\t\tif model_name == \'VGG-Face\':\n\t\t\tprint(""Using VGG-Face model backend and"", distance_metric,""distance."")\n\t\t\tmodel = VGGFace.loadModel()\n\t\t\tinput_shape = (224, 224)\t\n\t\t\n\t\telif model_name == \'OpenFace\':\n\t\t\tprint(""Using OpenFace model backend"", distance_metric,""distance."")\n\t\t\tmodel = OpenFace.loadModel()\n\t\t\tinput_shape = (96, 96)\n\t\t\n\t\telif model_name == \'Facenet\':\n\t\t\tprint(""Using Facenet model backend"", distance_metric,""distance."")\n\t\t\tmodel = Facenet.loadModel()\n\t\t\tinput_shape = (160, 160)\n\t\t\n\t\telif model_name == \'DeepFace\':\n\t\t\tprint(""Using FB DeepFace model backend"", distance_metric,""distance."")\n\t\t\tmodel = FbDeepFace.loadModel()\n\t\t\tinput_shape = (152, 152)\n\t\t\n\t\telse:\n\t\t\traise ValueError(""Invalid model_name passed - "", model_name)\n\t\t#------------------------\n\t\t\n\t\t#tuned thresholds for model and metric pair\n\t\tthreshold = functions.findThreshold(model_name, distance_metric)\n\t\t\n\t#------------------------\n\t#facial attribute analysis models\n\t\t\n\tif enable_face_analysis == True:\n\t\t\n\t\ttic = time.time()\n\t\t\n\t\temotion_model = Emotion.loadModel()\n\t\tprint(""Emotion model loaded"")\n\t\t\n\t\tage_model = Age.loadModel()\n\t\tprint(""Age model loaded"")\n\t\t\n\t\tgender_model = Gender.loadModel()\n\t\tprint(""Gender model loaded"")\n\t\t\n\t\ttoc = time.time()\n\t\t\n\t\tprint(""Facial attibute analysis models loaded in "",toc-tic,"" seconds"")\n\t\n\t#------------------------\n\t\n\t#find embeddings for employee list\n\t\n\ttic = time.time()\n\t\n\tpbar = tqdm(range(0, len(employees)), desc=\'Finding embeddings\')\n\t\n\tembeddings = []\n\t#for employee in employees:\n\tfor index in pbar:\n\t\temployee = employees[index]\n\t\tpbar.set_description(""Finding embedding for %s"" % (employee.split(""/"")[-1]))\n\t\tembedding = []\n\t\timg = functions.detectFace(employee, input_shape)\n\t\timg_representation = model.predict(img)[0,:]\n\t\t\n\t\tembedding.append(employee)\n\t\tembedding.append(img_representation)\n\t\tembeddings.append(embedding)\n\t\n\tdf = pd.DataFrame(embeddings, columns = [\'employee\', \'embedding\'])\n\tdf[\'distance_metric\'] = distance_metric\n\t\n\ttoc = time.time()\n\t\n\tprint(""Embeddings found for given data set in "", toc-tic,"" seconds"")\n\t\n\t#-----------------------\n\n\ttime_threshold = 5; frame_threshold = 5\n\tpivot_img_size = 112 #face recognition result image\n\n\t#-----------------------\n\t\n\topencv_path = functions.get_opencv_path()\n\tface_detector_path = opencv_path+""haarcascade_frontalface_default.xml""\n\tface_cascade = cv2.CascadeClassifier(face_detector_path)\n\t\n\t#-----------------------\n\n\tfreeze = False\n\tface_detected = False\n\tface_included_frames = 0 #freeze screen if face detected sequantially 5 frames\n\tfreezed_frame = 0\n\ttic = time.time()\n\n\tcap = cv2.VideoCapture(0) #webcam\n\t#cap = cv2.VideoCapture(""C:/Users/IS96273/Desktop/skype-video-1.mp4"") #video\n\n\twhile(True):\n\t\tret, img = cap.read()\n\t\t\n\t\t#cv2.namedWindow(\'img\', cv2.WINDOW_FREERATIO)\n\t\t#cv2.setWindowProperty(\'img\', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n\t\t\n\t\traw_img = img.copy()\n\t\tresolution = img.shape\n\t\t\n\t\tresolution_x = img.shape[1]; resolution_y = img.shape[0]\n\n\t\tif freeze == False: \n\t\t\tfaces = face_cascade.detectMultiScale(img, 1.3, 5)\n\t\t\t\n\t\t\tif len(faces) == 0:\n\t\t\t\tface_included_frames = 0\n\t\telse: \n\t\t\tfaces = []\n\t\t\n\t\tdetected_faces = []\n\t\tface_index = 0\n\t\tfor (x,y,w,h) in faces:\n\t\t\tif w > 130: #discard small detected faces\n\t\t\t\t\n\t\t\t\tface_detected = True\n\t\t\t\tif face_index == 0:\n\t\t\t\t\tface_included_frames = face_included_frames + 1 #increase frame for a single face\n\t\t\t\t\n\t\t\t\tcv2.rectangle(img, (x,y), (x+w,y+h), (67,67,67), 1) #draw rectangle to main image\n\t\t\t\t\n\t\t\t\tcv2.putText(img, str(frame_threshold - face_included_frames), (int(x+w/4),int(y+h/1.5)), cv2.FONT_HERSHEY_SIMPLEX, 4, (255, 255, 255), 2)\n\t\t\t\t\n\t\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n\t\t\t\t\n\t\t\t\t#-------------------------------------\n\t\t\t\t\n\t\t\t\tdetected_faces.append((x,y,w,h))\n\t\t\t\tface_index = face_index + 1\n\t\t\t\t\n\t\t\t\t#-------------------------------------\n\t\t\t\t\n\t\tif face_detected == True and face_included_frames == frame_threshold and freeze == False:\n\t\t\tfreeze = True\n\t\t\t#base_img = img.copy()\n\t\t\tbase_img = raw_img.copy()\n\t\t\tdetected_faces_final = detected_faces.copy()\n\t\t\ttic = time.time()\n\t\t\n\t\tif freeze == True:\n\n\t\t\ttoc = time.time()\n\t\t\tif (toc - tic) < time_threshold:\n\t\t\t\t\n\t\t\t\tif freezed_frame == 0:\n\t\t\t\t\tfreeze_img = base_img.copy()\n\t\t\t\t\t#freeze_img = np.zeros(resolution, np.uint8) #here, np.uint8 handles showing white area issue\n\t\t\t\t\t\n\t\t\t\t\tfor detected_face in detected_faces_final:\n\t\t\t\t\t\tx = detected_face[0]; y = detected_face[1]\n\t\t\t\t\t\tw = detected_face[2]; h = detected_face[3]\n\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\tcv2.rectangle(freeze_img, (x,y), (x+w,y+h), (67,67,67), 1) #draw rectangle to main image\n\t\t\t\t\t\t\n\t\t\t\t\t\t#-------------------------------\n\t\t\t\t\t\t\n\t\t\t\t\t\t#apply deep learning for custom_face\n\t\t\t\t\t\t\n\t\t\t\t\t\tcustom_face = base_img[y:y+h, x:x+w]\n\t\t\t\t\t\t\n\t\t\t\t\t\t#-------------------------------\n\t\t\t\t\t\t#facial attribute analysis\n\t\t\t\t\t\t\n\t\t\t\t\t\tif enable_face_analysis == True:\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tgray_img = functions.detectFace(custom_face, (48, 48), True)\n\t\t\t\t\t\t\temotion_labels = [\'Angry\', \'Disgust\', \'Fear\', \'Happy\', \'Sad\', \'Surprise\', \'Neutral\']\n\t\t\t\t\t\t\temotion_predictions = emotion_model.predict(gray_img)[0,:]\n\t\t\t\t\t\t\tsum_of_predictions = emotion_predictions.sum()\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tmood_items = []\n\t\t\t\t\t\t\tfor i in range(0, len(emotion_labels)):\n\t\t\t\t\t\t\t\tmood_item = []\n\t\t\t\t\t\t\t\temotion_label = emotion_labels[i]\n\t\t\t\t\t\t\t\temotion_prediction = 100 * emotion_predictions[i] / sum_of_predictions\n\t\t\t\t\t\t\t\tmood_item.append(emotion_label)\n\t\t\t\t\t\t\t\tmood_item.append(emotion_prediction)\n\t\t\t\t\t\t\t\tmood_items.append(mood_item)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\temotion_df = pd.DataFrame(mood_items, columns = [""emotion"", ""score""])\n\t\t\t\t\t\t\temotion_df = emotion_df.sort_values(by = [""score""], ascending=False).reset_index(drop=True)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t#background of mood box\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t#transparency\n\t\t\t\t\t\t\toverlay = freeze_img.copy()\n\t\t\t\t\t\t\topacity = 0.4\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif x+w+pivot_img_size < resolution_x:\n\t\t\t\t\t\t\t\t#right\n\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img\n\t\t\t\t\t\t\t\t\t#, (x+w,y+20)\n\t\t\t\t\t\t\t\t\t, (x+w,y)\n\t\t\t\t\t\t\t\t\t, (x+w+pivot_img_size, y+h)\n\t\t\t\t\t\t\t\t\t, (64,64,64),cv2.FILLED)\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tcv2.addWeighted(overlay, opacity, freeze_img, 1 - opacity, 0, freeze_img)\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\telif x-pivot_img_size > 0:\n\t\t\t\t\t\t\t\t#left\n\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img\n\t\t\t\t\t\t\t\t\t#, (x-pivot_img_size,y+20)\n\t\t\t\t\t\t\t\t\t, (x-pivot_img_size,y)\n\t\t\t\t\t\t\t\t\t, (x, y+h)\n\t\t\t\t\t\t\t\t\t, (64,64,64),cv2.FILLED)\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tcv2.addWeighted(overlay, opacity, freeze_img, 1 - opacity, 0, freeze_img)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tfor index, instance in emotion_df.iterrows():\n\t\t\t\t\t\t\t\temotion_label = ""%s "" % (instance[\'emotion\'])\n\t\t\t\t\t\t\t\temotion_score = instance[\'score\']/100\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tbar_x = 35 #this is the size if an emotion is 100%\n\t\t\t\t\t\t\t\tbar_x = int(bar_x * emotion_score)\n\n\t\t\t\t\t\t\t\tif x+w+pivot_img_size < resolution_x:\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\ttext_location_y = y + 20 + (index+1) * 20\n\t\t\t\t\t\t\t\t\ttext_location_x = x+w\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\tif text_location_y < y + h:\n\t\t\t\t\t\t\t\t\t\tcv2.putText(freeze_img, emotion_label, (text_location_x, text_location_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img\n\t\t\t\t\t\t\t\t\t\t\t, (x+w+70, y + 13 + (index+1) * 20)\n\t\t\t\t\t\t\t\t\t\t\t, (x+w+70+bar_x, y + 13 + (index+1) * 20 + 5)\n\t\t\t\t\t\t\t\t\t\t\t, (255,255,255), cv2.FILLED)\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\telif x-pivot_img_size > 0:\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\ttext_location_y = y + 20 + (index+1) * 20\n\t\t\t\t\t\t\t\t\ttext_location_x = x-pivot_img_size\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\tif text_location_y <= y+h:\n\t\t\t\t\t\t\t\t\t\tcv2.putText(freeze_img, emotion_label, (text_location_x, text_location_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img\n\t\t\t\t\t\t\t\t\t\t\t, (x-pivot_img_size+70, y + 13 + (index+1) * 20)\n\t\t\t\t\t\t\t\t\t\t\t, (x-pivot_img_size+70+bar_x, y + 13 + (index+1) * 20 + 5)\n\t\t\t\t\t\t\t\t\t\t\t, (255,255,255), cv2.FILLED)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t#-------------------------------\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tface_224 = functions.detectFace(custom_face, (224, 224), False)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tage_predictions = age_model.predict(face_224)[0,:]\n\t\t\t\t\t\t\tapparent_age = Age.findApparentAge(age_predictions)\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t#-------------------------------\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tgender_prediction = gender_model.predict(face_224)[0,:]\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif np.argmax(gender_prediction) == 0:\n\t\t\t\t\t\t\t\tgender = ""W""\n\t\t\t\t\t\t\telif np.argmax(gender_prediction) == 1:\n\t\t\t\t\t\t\t\tgender = ""M""\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t#print(str(int(apparent_age)),"" years old "", dominant_emotion, "" "", gender)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tanalysis_report = str(int(apparent_age))+"" ""+gender\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t#-------------------------------\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tinfo_box_color = (46,200,255)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t#top\n\t\t\t\t\t\t\tif y - pivot_img_size + int(pivot_img_size/5) > 0:\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\ttriangle_coordinates = np.array( [\n\t\t\t\t\t\t\t\t\t(x+int(w/2), y)\n\t\t\t\t\t\t\t\t\t, (x+int(w/2)-int(w/10), y-int(pivot_img_size/3))\n\t\t\t\t\t\t\t\t\t, (x+int(w/2)+int(w/10), y-int(pivot_img_size/3))\n\t\t\t\t\t\t\t\t] )\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tcv2.drawContours(freeze_img, [triangle_coordinates], 0, info_box_color, -1)\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img, (x+int(w/5), y-pivot_img_size+int(pivot_img_size/5)), (x+w-int(w/5), y-int(pivot_img_size/3)), info_box_color, cv2.FILLED)\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tcv2.putText(freeze_img, analysis_report, (x+int(w/3.5), y - int(pivot_img_size/2.1)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 111, 255), 2)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t#bottom\n\t\t\t\t\t\t\telif y + h + pivot_img_size - int(pivot_img_size/5) < resolution_y:\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\ttriangle_coordinates = np.array( [\n\t\t\t\t\t\t\t\t\t(x+int(w/2), y+h)\n\t\t\t\t\t\t\t\t\t, (x+int(w/2)-int(w/10), y+h+int(pivot_img_size/3))\n\t\t\t\t\t\t\t\t\t, (x+int(w/2)+int(w/10), y+h+int(pivot_img_size/3))\n\t\t\t\t\t\t\t\t] )\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tcv2.drawContours(freeze_img, [triangle_coordinates], 0, info_box_color, -1)\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img, (x+int(w/5), y + h + int(pivot_img_size/3)), (x+w-int(w/5), y+h+pivot_img_size-int(pivot_img_size/5)), info_box_color, cv2.FILLED)\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tcv2.putText(freeze_img, analysis_report, (x+int(w/3.5), y + h + int(pivot_img_size/1.5)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 111, 255), 2)\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t#-------------------------------\n\t\t\t\t\t\t#face recognition\n\t\t\t\t\t\t\n\t\t\t\t\t\tcustom_face = functions.detectFace(custom_face, input_shape)\n\t\t\t\t\t\t\n\t\t\t\t\t\t#check detectFace function handled\n\t\t\t\t\t\tif custom_face.shape[1:3] == input_shape:\n\t\t\t\t\t\t\tif df.shape[0] > 0: #if there are images to verify, apply face recognition\n\t\t\t\t\t\t\t\timg1_representation = model.predict(custom_face)[0,:]\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t#print(freezed_frame,"" - "",img1_representation[0:5])\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tdef findDistance(row):\n\t\t\t\t\t\t\t\t\tdistance_metric = row[\'distance_metric\']\n\t\t\t\t\t\t\t\t\timg2_representation = row[\'embedding\']\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\tdistance = 1000 #initialize very large value\n\t\t\t\t\t\t\t\t\tif distance_metric == \'cosine\':\n\t\t\t\t\t\t\t\t\t\tdistance = dst.findCosineDistance(img1_representation, img2_representation)\n\t\t\t\t\t\t\t\t\telif distance_metric == \'euclidean\':\n\t\t\t\t\t\t\t\t\t\tdistance = dst.findEuclideanDistance(img1_representation, img2_representation)\n\t\t\t\t\t\t\t\t\telif distance_metric == \'euclidean_l2\':\n\t\t\t\t\t\t\t\t\t\tdistance = dst.findEuclideanDistance(dst.l2_normalize(img1_representation), dst.l2_normalize(img2_representation))\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\treturn distance\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tdf[\'distance\'] = df.apply(findDistance, axis = 1)\n\t\t\t\t\t\t\t\tdf = df.sort_values(by = [""distance""])\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tcandidate = df.iloc[0]\n\t\t\t\t\t\t\t\temployee_name = candidate[\'employee\']\n\t\t\t\t\t\t\t\tbest_distance = candidate[\'distance\']\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tif best_distance <= threshold:\n\t\t\t\t\t\t\t\t\t#print(employee_name)\n\t\t\t\t\t\t\t\t\tdisplay_img = cv2.imread(employee_name)\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\tdisplay_img = cv2.resize(display_img, (pivot_img_size, pivot_img_size))\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\tlabel = employee_name.split(""/"")[-1].replace("".jpg"", """")\n\t\t\t\t\t\t\t\t\tlabel = re.sub(\'[0-9]\', \'\', label)\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\t\t\tif y - pivot_img_size > 0 and x + w + pivot_img_size < resolution_x:\n\t\t\t\t\t\t\t\t\t\t\t#top right\n\t\t\t\t\t\t\t\t\t\t\tfreeze_img[y - pivot_img_size:y, x+w:x+w+pivot_img_size] = display_img\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\toverlay = freeze_img.copy(); opacity = 0.4\n\t\t\t\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img,(x+w,y),(x+w+pivot_img_size, y+20),(46,200,255),cv2.FILLED)\n\t\t\t\t\t\t\t\t\t\t\tcv2.addWeighted(overlay, opacity, freeze_img, 1 - opacity, 0, freeze_img)\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcv2.putText(freeze_img, label, (x+w, y+10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1)\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t#connect face and text\n\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img,(x+int(w/2), y), (x+3*int(w/4), y-int(pivot_img_size/2)),(67,67,67),1)\n\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img, (x+3*int(w/4), y-int(pivot_img_size/2)), (x+w, y - int(pivot_img_size/2)), (67,67,67),1)\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\telif y + h + pivot_img_size < resolution_y and x - pivot_img_size > 0:\n\t\t\t\t\t\t\t\t\t\t\t#bottom left\n\t\t\t\t\t\t\t\t\t\t\tfreeze_img[y+h:y+h+pivot_img_size, x-pivot_img_size:x] = display_img\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\toverlay = freeze_img.copy(); opacity = 0.4\n\t\t\t\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img,(x-pivot_img_size,y+h-20),(x, y+h),(46,200,255),cv2.FILLED)\n\t\t\t\t\t\t\t\t\t\t\tcv2.addWeighted(overlay, opacity, freeze_img, 1 - opacity, 0, freeze_img)\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcv2.putText(freeze_img, label, (x - pivot_img_size, y+h-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1)\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t#connect face and text\n\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img,(x+int(w/2), y+h), (x+int(w/2)-int(w/4), y+h+int(pivot_img_size/2)),(67,67,67),1)\n\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img, (x+int(w/2)-int(w/4), y+h+int(pivot_img_size/2)), (x, y+h+int(pivot_img_size/2)), (67,67,67),1)\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\telif y - pivot_img_size > 0 and x - pivot_img_size > 0:\n\t\t\t\t\t\t\t\t\t\t\t#top left\n\t\t\t\t\t\t\t\t\t\t\tfreeze_img[y-pivot_img_size:y, x-pivot_img_size:x] = display_img\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\toverlay = freeze_img.copy(); opacity = 0.4\n\t\t\t\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img,(x- pivot_img_size,y),(x, y+20),(46,200,255),cv2.FILLED)\n\t\t\t\t\t\t\t\t\t\t\tcv2.addWeighted(overlay, opacity, freeze_img, 1 - opacity, 0, freeze_img)\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcv2.putText(freeze_img, label, (x - pivot_img_size, y+10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1)\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t#connect face and text\n\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img,(x+int(w/2), y), (x+int(w/2)-int(w/4), y-int(pivot_img_size/2)),(67,67,67),1)\n\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img, (x+int(w/2)-int(w/4), y-int(pivot_img_size/2)), (x, y - int(pivot_img_size/2)), (67,67,67),1)\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\telif x+w+pivot_img_size < resolution_x and y + h + pivot_img_size < resolution_y:\n\t\t\t\t\t\t\t\t\t\t\t#bottom righ\n\t\t\t\t\t\t\t\t\t\t\tfreeze_img[y+h:y+h+pivot_img_size, x+w:x+w+pivot_img_size] = display_img\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\toverlay = freeze_img.copy(); opacity = 0.4\n\t\t\t\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img,(x+w,y+h-20),(x+w+pivot_img_size, y+h),(46,200,255),cv2.FILLED)\n\t\t\t\t\t\t\t\t\t\t\tcv2.addWeighted(overlay, opacity, freeze_img, 1 - opacity, 0, freeze_img)\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcv2.putText(freeze_img, label, (x+w, y+h-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1)\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t#connect face and text\n\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img,(x+int(w/2), y+h), (x+int(w/2)+int(w/4), y+h+int(pivot_img_size/2)),(67,67,67),1)\n\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img, (x+int(w/2)+int(w/4), y+h+int(pivot_img_size/2)), (x+w, y+h+int(pivot_img_size/2)), (67,67,67),1)\n\t\t\t\t\t\t\t\t\texcept Exception as err:\n\t\t\t\t\t\t\t\t\t\tprint(str(err))\n\t\t\t\t\t\t\n\t\t\t\t\t\ttic = time.time() #in this way, freezed image can show 5 seconds\n\t\t\t\t\t\t\n\t\t\t\t\t\t#-------------------------------\n\t\t\t\t\n\t\t\t\ttime_left = int(time_threshold - (toc - tic) + 1)\n\t\t\t\t\n\t\t\t\tcv2.rectangle(freeze_img, (10, 10), (90, 50), (67,67,67), -10)\n\t\t\t\tcv2.putText(freeze_img, str(time_left), (40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 1)\n\t\t\t\t\n\t\t\t\tcv2.imshow(\'img\', freeze_img)\n\t\t\t\t\n\t\t\t\tfreezed_frame = freezed_frame + 1\n\t\t\telse:\n\t\t\t\tface_detected = False\n\t\t\t\tface_included_frames = 0\n\t\t\t\tfreeze = False\n\t\t\t\tfreezed_frame = 0\n\t\t\t\n\t\telse:\n\t\t\tcv2.imshow(\'img\',img)\n\t\t\n\t\tif cv2.waitKey(1) & 0xFF == ord(\'q\'): #press q to quit\n\t\t\tbreak\n\t\t\n\t#kill open cv things\t\t\n\tcap.release()\n\tcv2.destroyAllWindows()'"
deepface/extendedmodels/Age.py,0,"b'#from basemodels import VGGFace\nfrom deepface.basemodels import VGGFace\n\nimport os\nfrom pathlib import Path\nimport gdown\nimport numpy as np\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution2D, Flatten, Activation\n\ndef loadModel():\n\t\n\tmodel = VGGFace.baseModel()\n\t\n\t#--------------------------\n\t\n\tclasses = 101\n\tbase_model_output = Sequential()\n\tbase_model_output = Convolution2D(classes, (1, 1), name=\'predictions\')(model.layers[-4].output)\n\tbase_model_output = Flatten()(base_model_output)\n\tbase_model_output = Activation(\'softmax\')(base_model_output)\n\t\n\t#--------------------------\n\n\tage_model = Model(inputs=model.input, outputs=base_model_output)\n\t\n\t#--------------------------\n\t\n\t#load weights\n\t\n\thome = str(Path.home())\n\t\n\tif os.path.isfile(home+\'/.deepface/weights/age_model_weights.h5\') != True:\n\t\tprint(""age_model_weights.h5 will be downloaded..."")\n\t\t\n\t\turl = \'https://drive.google.com/uc?id=1YCox_4kJ-BYeXq27uUbasu--yz28zUMV\'\n\t\toutput = home+\'/.deepface/weights/age_model_weights.h5\'\n\t\tgdown.download(url, output, quiet=False)\n\t\n\tage_model.load_weights(home+\'/.deepface/weights/age_model_weights.h5\')\n\t\n\treturn age_model\n\t\n\t#--------------------------\n\ndef findApparentAge(age_predictions):\n\toutput_indexes = np.array([i for i in range(0, 101)])\n\tapparent_age = np.sum(age_predictions * output_indexes)\n\treturn apparent_age'"
deepface/extendedmodels/Emotion.py,0,"b'import os\nimport gdown\nfrom pathlib import Path\nfrom keras.models import Model, Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout\nimport zipfile\n\ndef loadModel():\n\t\n\tnum_classes = 7\n\t\n\tmodel = Sequential()\n\n\t#1st convolution layer\n\tmodel.add(Conv2D(64, (5, 5), activation=\'relu\', input_shape=(48,48,1)))\n\tmodel.add(MaxPooling2D(pool_size=(5,5), strides=(2, 2)))\n\n\t#2nd convolution layer\n\tmodel.add(Conv2D(64, (3, 3), activation=\'relu\'))\n\tmodel.add(Conv2D(64, (3, 3), activation=\'relu\'))\n\tmodel.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n\n\t#3rd convolution layer\n\tmodel.add(Conv2D(128, (3, 3), activation=\'relu\'))\n\tmodel.add(Conv2D(128, (3, 3), activation=\'relu\'))\n\tmodel.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n\n\tmodel.add(Flatten())\n\n\t#fully connected neural networks\n\tmodel.add(Dense(1024, activation=\'relu\'))\n\tmodel.add(Dropout(0.2))\n\tmodel.add(Dense(1024, activation=\'relu\'))\n\tmodel.add(Dropout(0.2))\n\n\tmodel.add(Dense(num_classes, activation=\'softmax\'))\n\t\n\t#----------------------------\n\t\n\thome = str(Path.home())\n\t\n\tif os.path.isfile(home+\'/.deepface/weights/facial_expression_model_weights.h5\') != True:\n\t\tprint(""facial_expression_model_weights.h5 will be downloaded..."")\n\t\t\n\t\t#TO-DO: upload weights to google drive\n\t\t\n\t\t#zip\n\t\turl = \'https://drive.google.com/uc?id=13iUHHP3SlNg53qSuQZDdHDSDNdBP9nwy\'\n\t\toutput = home+\'/.deepface/weights/facial_expression_model_weights.zip\'\n\t\tgdown.download(url, output, quiet=False)\n\t\t\n\t\t#unzip facial_expression_model_weights.zip\n\t\twith zipfile.ZipFile(output, \'r\') as zip_ref:\n\t\t\tzip_ref.extractall(home+\'/.deepface/weights/\')\n\t\t\n\tmodel.load_weights(home+\'/.deepface/weights/facial_expression_model_weights.h5\')\n\t\n\treturn model\n\t\n\t#----------------------------\n\t\n\treturn 0'"
deepface/extendedmodels/Gender.py,0,"b'#from basemodels import VGGFace\nfrom deepface.basemodels import VGGFace\n\nimport os\nfrom pathlib import Path\nimport gdown\nimport numpy as np\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution2D, Flatten, Activation\n\ndef loadModel():\n\t\n\tmodel = VGGFace.baseModel()\n\t\n\t#--------------------------\n\t\n\tclasses = 2\n\tbase_model_output = Sequential()\n\tbase_model_output = Convolution2D(classes, (1, 1), name=\'predictions\')(model.layers[-4].output)\n\tbase_model_output = Flatten()(base_model_output)\n\tbase_model_output = Activation(\'softmax\')(base_model_output)\n\t\n\t#--------------------------\n\n\tgender_model = Model(inputs=model.input, outputs=base_model_output)\n\t\n\t#--------------------------\n\t\n\t#load weights\n\t\n\thome = str(Path.home())\n\t\n\tif os.path.isfile(home+\'/.deepface/weights/gender_model_weights.h5\') != True:\n\t\tprint(""gender_model_weights.h5 will be downloaded..."")\n\t\t\n\t\turl = \'https://drive.google.com/uc?id=1wUXRVlbsni2FN9-jkS_f4UTUrm1bRLyk\'\n\t\toutput = home+\'/.deepface/weights/gender_model_weights.h5\'\n\t\tgdown.download(url, output, quiet=False)\n\t\n\tgender_model.load_weights(home+\'/.deepface/weights/gender_model_weights.h5\')\n\t\n\treturn gender_model\n\t\n\t#--------------------------'"
deepface/extendedmodels/Race.py,0,"b'#from basemodels import VGGFace\nfrom deepface.basemodels import VGGFace\n\nimport os\nfrom pathlib import Path\nimport gdown\nimport numpy as np\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution2D, Flatten, Activation\nimport zipfile\n\ndef loadModel():\n\t\n\tmodel = VGGFace.baseModel()\n\t\n\t#--------------------------\n\t\n\tclasses = 6\n\tbase_model_output = Sequential()\n\tbase_model_output = Convolution2D(classes, (1, 1), name=\'predictions\')(model.layers[-4].output)\n\tbase_model_output = Flatten()(base_model_output)\n\tbase_model_output = Activation(\'softmax\')(base_model_output)\n\t\n\t#--------------------------\n\n\trace_model = Model(inputs=model.input, outputs=base_model_output)\n\t\n\t#--------------------------\n\t\n\t#load weights\n\t\n\thome = str(Path.home())\n\t\n\tif os.path.isfile(home+\'/.deepface/weights/race_model_single_batch.h5\') != True:\n\t\tprint(""race_model_single_batch.h5 will be downloaded..."")\n\t\t\n\t\t#zip\n\t\turl = \'https://drive.google.com/uc?id=1nz-WDhghGQBC4biwShQ9kYjvQMpO6smj\'\n\t\toutput = home+\'/.deepface/weights/race_model_single_batch.zip\'\n\t\tgdown.download(url, output, quiet=False)\n\t\t\n\t\t#unzip race_model_single_batch.zip\n\t\twith zipfile.ZipFile(output, \'r\') as zip_ref:\n\t\t\tzip_ref.extractall(home+\'/.deepface/weights/\')\n\t\n\trace_model.load_weights(home+\'/.deepface/weights/race_model_single_batch.h5\')\n\t\n\treturn race_model\n\t\n\t#--------------------------\n'"
deepface/extendedmodels/__init__.py,0,b''
