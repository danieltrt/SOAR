file_path,api_count,code
model/inception_resnet_v1.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""Inception-ResNet V1 model for Keras.\r\n# Reference\r\nhttp://arxiv.org/abs/1602.07261\r\nhttps://github.com/davidsandberg/facenet/blob/master/src/models/inception_resnet_v1.py\r\nhttps://github.com/myutwo150/keras-inception-resnet-v2/blob/master/inception_resnet_v2.py\r\n""""""\r\nimport os\r\nfrom pathlib import Path\r\nimport gdown\r\nfrom functools import partial\r\n\r\nfrom keras.models import Model\r\nfrom keras.layers import Activation\r\nfrom keras.layers import BatchNormalization\r\nfrom keras.layers import Concatenate\r\nfrom keras.layers import Conv2D\r\nfrom keras.layers import Dense\r\nfrom keras.layers import Dropout\r\nfrom keras.layers import GlobalAveragePooling2D\r\nfrom keras.layers import Input\r\nfrom keras.layers import Lambda\r\nfrom keras.layers import MaxPooling2D\r\nfrom keras.layers import add\r\nfrom keras import backend as K\r\n\r\ndef scaling(x, scale):\r\n\treturn x * scale\r\n\r\ndef InceptionResNetV1():\r\n\t\r\n\tinputs = Input(shape=(160, 160, 3))\r\n\tx = Conv2D(32, 3, strides=2, padding=\'valid\', use_bias=False, name= \'Conv2d_1a_3x3\') (inputs)\r\n\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Conv2d_1a_3x3_BatchNorm\')(x)\r\n\tx = Activation(\'relu\', name=\'Conv2d_1a_3x3_Activation\')(x)\r\n\tx = Conv2D(32, 3, strides=1, padding=\'valid\', use_bias=False, name= \'Conv2d_2a_3x3\') (x)\r\n\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Conv2d_2a_3x3_BatchNorm\')(x)\r\n\tx = Activation(\'relu\', name=\'Conv2d_2a_3x3_Activation\')(x)\r\n\tx = Conv2D(64, 3, strides=1, padding=\'same\', use_bias=False, name= \'Conv2d_2b_3x3\') (x)\r\n\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Conv2d_2b_3x3_BatchNorm\')(x)\r\n\tx = Activation(\'relu\', name=\'Conv2d_2b_3x3_Activation\')(x)\r\n\tx = MaxPooling2D(3, strides=2, name=\'MaxPool_3a_3x3\')(x)\r\n\tx = Conv2D(80, 1, strides=1, padding=\'valid\', use_bias=False, name= \'Conv2d_3b_1x1\') (x)\r\n\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Conv2d_3b_1x1_BatchNorm\')(x)\r\n\tx = Activation(\'relu\', name=\'Conv2d_3b_1x1_Activation\')(x)\r\n\tx = Conv2D(192, 3, strides=1, padding=\'valid\', use_bias=False, name= \'Conv2d_4a_3x3\') (x)\r\n\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Conv2d_4a_3x3_BatchNorm\')(x)\r\n\tx = Activation(\'relu\', name=\'Conv2d_4a_3x3_Activation\')(x)\r\n\tx = Conv2D(256, 3, strides=2, padding=\'valid\', use_bias=False, name= \'Conv2d_4b_3x3\') (x)\r\n\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Conv2d_4b_3x3_BatchNorm\')(x)\r\n\tx = Activation(\'relu\', name=\'Conv2d_4b_3x3_Activation\')(x)\r\n\t\r\n\t# 5x Block35 (Inception-ResNet-A block):\r\n\tbranch_0 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_1_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_1_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block35_1_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_1_Branch_1_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_1_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block35_1_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_1_Branch_1_Conv2d_0b_3x3\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_1_Branch_1_Conv2d_0b_3x3_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block35_1_Branch_1_Conv2d_0b_3x3_Activation\')(branch_1)\r\n\tbranch_2 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_1_Branch_2_Conv2d_0a_1x1\') (x)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_1_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Block35_1_Branch_2_Conv2d_0a_1x1_Activation\')(branch_2)\r\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_1_Branch_2_Conv2d_0b_3x3\') (branch_2)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_1_Branch_2_Conv2d_0b_3x3_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Block35_1_Branch_2_Conv2d_0b_3x3_Activation\')(branch_2)\r\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_1_Branch_2_Conv2d_0c_3x3\') (branch_2)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_1_Branch_2_Conv2d_0c_3x3_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Block35_1_Branch_2_Conv2d_0c_3x3_Activation\')(branch_2)\r\n\tbranches = [branch_0, branch_1, branch_2]\r\n\tmixed = Concatenate(axis=3, name=\'Block35_1_Concatenate\')(branches)\r\n\tup = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block35_1_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.17})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block35_1_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_2_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_2_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block35_2_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_2_Branch_1_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_2_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block35_2_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_2_Branch_1_Conv2d_0b_3x3\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_2_Branch_1_Conv2d_0b_3x3_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block35_2_Branch_1_Conv2d_0b_3x3_Activation\')(branch_1)\r\n\tbranch_2 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_2_Branch_2_Conv2d_0a_1x1\') (x)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_2_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Block35_2_Branch_2_Conv2d_0a_1x1_Activation\')(branch_2)\r\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_2_Branch_2_Conv2d_0b_3x3\') (branch_2)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_2_Branch_2_Conv2d_0b_3x3_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Block35_2_Branch_2_Conv2d_0b_3x3_Activation\')(branch_2)\r\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_2_Branch_2_Conv2d_0c_3x3\') (branch_2)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_2_Branch_2_Conv2d_0c_3x3_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Block35_2_Branch_2_Conv2d_0c_3x3_Activation\')(branch_2)\r\n\tbranches = [branch_0, branch_1, branch_2]\r\n\tmixed = Concatenate(axis=3, name=\'Block35_2_Concatenate\')(branches)\r\n\tup = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block35_2_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.17})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block35_2_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_3_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_3_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block35_3_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_3_Branch_1_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_3_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block35_3_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_3_Branch_1_Conv2d_0b_3x3\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_3_Branch_1_Conv2d_0b_3x3_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block35_3_Branch_1_Conv2d_0b_3x3_Activation\')(branch_1)\r\n\tbranch_2 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_3_Branch_2_Conv2d_0a_1x1\') (x)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_3_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Block35_3_Branch_2_Conv2d_0a_1x1_Activation\')(branch_2)\r\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_3_Branch_2_Conv2d_0b_3x3\') (branch_2)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_3_Branch_2_Conv2d_0b_3x3_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Block35_3_Branch_2_Conv2d_0b_3x3_Activation\')(branch_2)\r\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_3_Branch_2_Conv2d_0c_3x3\') (branch_2)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_3_Branch_2_Conv2d_0c_3x3_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Block35_3_Branch_2_Conv2d_0c_3x3_Activation\')(branch_2)\r\n\tbranches = [branch_0, branch_1, branch_2]\r\n\tmixed = Concatenate(axis=3, name=\'Block35_3_Concatenate\')(branches)\r\n\tup = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block35_3_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.17})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block35_3_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_4_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_4_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block35_4_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_4_Branch_1_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_4_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block35_4_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_4_Branch_1_Conv2d_0b_3x3\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_4_Branch_1_Conv2d_0b_3x3_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block35_4_Branch_1_Conv2d_0b_3x3_Activation\')(branch_1)\r\n\tbranch_2 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_4_Branch_2_Conv2d_0a_1x1\') (x)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_4_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Block35_4_Branch_2_Conv2d_0a_1x1_Activation\')(branch_2)\r\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_4_Branch_2_Conv2d_0b_3x3\') (branch_2)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_4_Branch_2_Conv2d_0b_3x3_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Block35_4_Branch_2_Conv2d_0b_3x3_Activation\')(branch_2)\r\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_4_Branch_2_Conv2d_0c_3x3\') (branch_2)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_4_Branch_2_Conv2d_0c_3x3_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Block35_4_Branch_2_Conv2d_0c_3x3_Activation\')(branch_2)\r\n\tbranches = [branch_0, branch_1, branch_2]\r\n\tmixed = Concatenate(axis=3, name=\'Block35_4_Concatenate\')(branches)\r\n\tup = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block35_4_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.17})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block35_4_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_5_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_5_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block35_5_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_5_Branch_1_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_5_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block35_5_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_5_Branch_1_Conv2d_0b_3x3\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_5_Branch_1_Conv2d_0b_3x3_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block35_5_Branch_1_Conv2d_0b_3x3_Activation\')(branch_1)\r\n\tbranch_2 = Conv2D(32, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block35_5_Branch_2_Conv2d_0a_1x1\') (x)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_5_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Block35_5_Branch_2_Conv2d_0a_1x1_Activation\')(branch_2)\r\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_5_Branch_2_Conv2d_0b_3x3\') (branch_2)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_5_Branch_2_Conv2d_0b_3x3_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Block35_5_Branch_2_Conv2d_0b_3x3_Activation\')(branch_2)\r\n\tbranch_2 = Conv2D(32, 3, strides=1, padding=\'same\', use_bias=False, name= \'Block35_5_Branch_2_Conv2d_0c_3x3\') (branch_2)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block35_5_Branch_2_Conv2d_0c_3x3_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Block35_5_Branch_2_Conv2d_0c_3x3_Activation\')(branch_2)\r\n\tbranches = [branch_0, branch_1, branch_2]\r\n\tmixed = Concatenate(axis=3, name=\'Block35_5_Concatenate\')(branches)\r\n\tup = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block35_5_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.17})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block35_5_Activation\')(x)\r\n\r\n\t# Mixed 6a (Reduction-A block):\r\n\tbranch_0 = Conv2D(384, 3, strides=2, padding=\'valid\', use_bias=False, name= \'Mixed_6a_Branch_0_Conv2d_1a_3x3\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_6a_Branch_0_Conv2d_1a_3x3_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Mixed_6a_Branch_0_Conv2d_1a_3x3_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Mixed_6a_Branch_1_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_6a_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Mixed_6a_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(192, 3, strides=1, padding=\'same\', use_bias=False, name= \'Mixed_6a_Branch_1_Conv2d_0b_3x3\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_6a_Branch_1_Conv2d_0b_3x3_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Mixed_6a_Branch_1_Conv2d_0b_3x3_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(256, 3, strides=2, padding=\'valid\', use_bias=False, name= \'Mixed_6a_Branch_1_Conv2d_1a_3x3\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_6a_Branch_1_Conv2d_1a_3x3_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Mixed_6a_Branch_1_Conv2d_1a_3x3_Activation\')(branch_1)\r\n\tbranch_pool = MaxPooling2D(3, strides=2, padding=\'valid\', name=\'Mixed_6a_Branch_2_MaxPool_1a_3x3\')(x)\r\n\tbranches = [branch_0, branch_1, branch_pool]\r\n\tx = Concatenate(axis=3, name=\'Mixed_6a\')(branches)\r\n\r\n\t# 10x Block17 (Inception-ResNet-B block):\r\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_1_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_1_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block17_1_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_1_Branch_1_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_1_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_1_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_1_Branch_1_Conv2d_0b_1x7\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_1_Branch_1_Conv2d_0b_1x7_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_1_Branch_1_Conv2d_0b_1x7_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_1_Branch_1_Conv2d_0c_7x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_1_Branch_1_Conv2d_0c_7x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_1_Branch_1_Conv2d_0c_7x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block17_1_Concatenate\')(branches)\r\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_1_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block17_1_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_2_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_2_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block17_2_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_2_Branch_2_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_2_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_2_Branch_2_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_2_Branch_2_Conv2d_0b_1x7\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_2_Branch_2_Conv2d_0b_1x7_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_2_Branch_2_Conv2d_0b_1x7_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_2_Branch_2_Conv2d_0c_7x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_2_Branch_2_Conv2d_0c_7x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_2_Branch_2_Conv2d_0c_7x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block17_2_Concatenate\')(branches)\r\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_2_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block17_2_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_3_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_3_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block17_3_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_3_Branch_3_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_3_Branch_3_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_3_Branch_3_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_3_Branch_3_Conv2d_0b_1x7\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_3_Branch_3_Conv2d_0b_1x7_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_3_Branch_3_Conv2d_0b_1x7_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_3_Branch_3_Conv2d_0c_7x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_3_Branch_3_Conv2d_0c_7x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_3_Branch_3_Conv2d_0c_7x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block17_3_Concatenate\')(branches)\r\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_3_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block17_3_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_4_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_4_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block17_4_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_4_Branch_4_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_4_Branch_4_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_4_Branch_4_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_4_Branch_4_Conv2d_0b_1x7\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_4_Branch_4_Conv2d_0b_1x7_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_4_Branch_4_Conv2d_0b_1x7_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_4_Branch_4_Conv2d_0c_7x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_4_Branch_4_Conv2d_0c_7x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_4_Branch_4_Conv2d_0c_7x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block17_4_Concatenate\')(branches)\r\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_4_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block17_4_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_5_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_5_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block17_5_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_5_Branch_5_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_5_Branch_5_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_5_Branch_5_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_5_Branch_5_Conv2d_0b_1x7\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_5_Branch_5_Conv2d_0b_1x7_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_5_Branch_5_Conv2d_0b_1x7_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_5_Branch_5_Conv2d_0c_7x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_5_Branch_5_Conv2d_0c_7x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_5_Branch_5_Conv2d_0c_7x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block17_5_Concatenate\')(branches)\r\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_5_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block17_5_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_6_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_6_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block17_6_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_6_Branch_6_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_6_Branch_6_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_6_Branch_6_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_6_Branch_6_Conv2d_0b_1x7\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_6_Branch_6_Conv2d_0b_1x7_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_6_Branch_6_Conv2d_0b_1x7_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_6_Branch_6_Conv2d_0c_7x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_6_Branch_6_Conv2d_0c_7x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_6_Branch_6_Conv2d_0c_7x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block17_6_Concatenate\')(branches)\r\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_6_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block17_6_Activation\')(x)\t\r\n\t\r\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_7_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_7_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block17_7_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_7_Branch_7_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_7_Branch_7_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_7_Branch_7_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_7_Branch_7_Conv2d_0b_1x7\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_7_Branch_7_Conv2d_0b_1x7_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_7_Branch_7_Conv2d_0b_1x7_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_7_Branch_7_Conv2d_0c_7x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_7_Branch_7_Conv2d_0c_7x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_7_Branch_7_Conv2d_0c_7x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block17_7_Concatenate\')(branches)\r\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_7_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block17_7_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_8_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_8_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block17_8_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_8_Branch_8_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_8_Branch_8_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_8_Branch_8_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_8_Branch_8_Conv2d_0b_1x7\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_8_Branch_8_Conv2d_0b_1x7_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_8_Branch_8_Conv2d_0b_1x7_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_8_Branch_8_Conv2d_0c_7x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_8_Branch_8_Conv2d_0c_7x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_8_Branch_8_Conv2d_0c_7x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block17_8_Concatenate\')(branches)\r\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_8_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block17_8_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_9_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_9_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block17_9_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_9_Branch_9_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_9_Branch_9_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_9_Branch_9_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_9_Branch_9_Conv2d_0b_1x7\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_9_Branch_9_Conv2d_0b_1x7_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_9_Branch_9_Conv2d_0b_1x7_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_9_Branch_9_Conv2d_0c_7x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_9_Branch_9_Conv2d_0c_7x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_9_Branch_9_Conv2d_0c_7x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block17_9_Concatenate\')(branches)\r\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_9_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block17_9_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_10_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_10_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block17_10_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(128, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block17_10_Branch_10_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_10_Branch_10_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_10_Branch_10_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding=\'same\', use_bias=False, name= \'Block17_10_Branch_10_Conv2d_0b_1x7\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_10_Branch_10_Conv2d_0b_1x7_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_10_Branch_10_Conv2d_0b_1x7_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block17_10_Branch_10_Conv2d_0c_7x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block17_10_Branch_10_Conv2d_0c_7x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block17_10_Branch_10_Conv2d_0c_7x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block17_10_Concatenate\')(branches)\r\n\tup = Conv2D(896, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block17_10_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.1})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block17_10_Activation\')(x)\r\n\r\n\t# Mixed 7a (Reduction-B block): 8 x 8 x 2080\t\r\n\tbranch_0 = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=False, name= \'Mixed_7a_Branch_0_Conv2d_0a_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_7a_Branch_0_Conv2d_0a_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Mixed_7a_Branch_0_Conv2d_0a_1x1_Activation\')(branch_0)\r\n\tbranch_0 = Conv2D(384, 3, strides=2, padding=\'valid\', use_bias=False, name= \'Mixed_7a_Branch_0_Conv2d_1a_3x3\') (branch_0)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_7a_Branch_0_Conv2d_1a_3x3_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Mixed_7a_Branch_0_Conv2d_1a_3x3_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=False, name= \'Mixed_7a_Branch_1_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_7a_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Mixed_7a_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(256, 3, strides=2, padding=\'valid\', use_bias=False, name= \'Mixed_7a_Branch_1_Conv2d_1a_3x3\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_7a_Branch_1_Conv2d_1a_3x3_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Mixed_7a_Branch_1_Conv2d_1a_3x3_Activation\')(branch_1)\r\n\tbranch_2 = Conv2D(256, 1, strides=1, padding=\'same\', use_bias=False, name= \'Mixed_7a_Branch_2_Conv2d_0a_1x1\') (x)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_7a_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Mixed_7a_Branch_2_Conv2d_0a_1x1_Activation\')(branch_2)\r\n\tbranch_2 = Conv2D(256, 3, strides=1, padding=\'same\', use_bias=False, name= \'Mixed_7a_Branch_2_Conv2d_0b_3x3\') (branch_2)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_7a_Branch_2_Conv2d_0b_3x3_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Mixed_7a_Branch_2_Conv2d_0b_3x3_Activation\')(branch_2)\r\n\tbranch_2 = Conv2D(256, 3, strides=2, padding=\'valid\', use_bias=False, name= \'Mixed_7a_Branch_2_Conv2d_1a_3x3\') (branch_2)\r\n\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Mixed_7a_Branch_2_Conv2d_1a_3x3_BatchNorm\')(branch_2)\r\n\tbranch_2 = Activation(\'relu\', name=\'Mixed_7a_Branch_2_Conv2d_1a_3x3_Activation\')(branch_2)\r\n\tbranch_pool = MaxPooling2D(3, strides=2, padding=\'valid\', name=\'Mixed_7a_Branch_3_MaxPool_1a_3x3\')(x)\r\n\tbranches = [branch_0, branch_1, branch_2, branch_pool]\r\n\tx = Concatenate(axis=3, name=\'Mixed_7a\')(branches)\r\n\r\n\t# 5x Block8 (Inception-ResNet-C block):\r\n\t\r\n\tbranch_0 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_1_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_1_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block8_1_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_1_Branch_1_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_1_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_1_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding=\'same\', use_bias=False, name= \'Block8_1_Branch_1_Conv2d_0b_1x3\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_1_Branch_1_Conv2d_0b_1x3_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_1_Branch_1_Conv2d_0b_1x3_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block8_1_Branch_1_Conv2d_0c_3x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_1_Branch_1_Conv2d_0c_3x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_1_Branch_1_Conv2d_0c_3x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block8_1_Concatenate\')(branches)\r\n\tup = Conv2D(1792, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block8_1_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.2})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block8_1_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_2_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_2_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block8_2_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_2_Branch_2_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_2_Branch_2_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_2_Branch_2_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding=\'same\', use_bias=False, name= \'Block8_2_Branch_2_Conv2d_0b_1x3\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_2_Branch_2_Conv2d_0b_1x3_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_2_Branch_2_Conv2d_0b_1x3_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block8_2_Branch_2_Conv2d_0c_3x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_2_Branch_2_Conv2d_0c_3x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_2_Branch_2_Conv2d_0c_3x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block8_2_Concatenate\')(branches)\r\n\tup = Conv2D(1792, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block8_2_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.2})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block8_2_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_3_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_3_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block8_3_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_3_Branch_3_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_3_Branch_3_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_3_Branch_3_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding=\'same\', use_bias=False, name= \'Block8_3_Branch_3_Conv2d_0b_1x3\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_3_Branch_3_Conv2d_0b_1x3_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_3_Branch_3_Conv2d_0b_1x3_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block8_3_Branch_3_Conv2d_0c_3x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_3_Branch_3_Conv2d_0c_3x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_3_Branch_3_Conv2d_0c_3x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block8_3_Concatenate\')(branches)\r\n\tup = Conv2D(1792, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block8_3_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.2})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block8_3_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_4_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_4_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block8_4_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_4_Branch_4_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_4_Branch_4_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_4_Branch_4_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding=\'same\', use_bias=False, name= \'Block8_4_Branch_4_Conv2d_0b_1x3\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_4_Branch_4_Conv2d_0b_1x3_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_4_Branch_4_Conv2d_0b_1x3_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block8_4_Branch_4_Conv2d_0c_3x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_4_Branch_4_Conv2d_0c_3x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_4_Branch_4_Conv2d_0c_3x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block8_4_Concatenate\')(branches)\r\n\tup = Conv2D(1792, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block8_4_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.2})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block8_4_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_5_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_5_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block8_5_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_5_Branch_5_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_5_Branch_5_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_5_Branch_5_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding=\'same\', use_bias=False, name= \'Block8_5_Branch_5_Conv2d_0b_1x3\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_5_Branch_5_Conv2d_0b_1x3_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_5_Branch_5_Conv2d_0b_1x3_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block8_5_Branch_5_Conv2d_0c_3x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_5_Branch_5_Conv2d_0c_3x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_5_Branch_5_Conv2d_0c_3x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block8_5_Concatenate\')(branches)\r\n\tup = Conv2D(1792, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block8_5_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 0.2})(up)\r\n\tx = add([x, up])\r\n\tx = Activation(\'relu\', name=\'Block8_5_Activation\')(x)\r\n\t\r\n\tbranch_0 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_6_Branch_0_Conv2d_1x1\') (x)\r\n\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_6_Branch_0_Conv2d_1x1_BatchNorm\')(branch_0)\r\n\tbranch_0 = Activation(\'relu\', name=\'Block8_6_Branch_0_Conv2d_1x1_Activation\')(branch_0)\r\n\tbranch_1 = Conv2D(192, 1, strides=1, padding=\'same\', use_bias=False, name= \'Block8_6_Branch_1_Conv2d_0a_1x1\') (x)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_6_Branch_1_Conv2d_0a_1x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_6_Branch_1_Conv2d_0a_1x1_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding=\'same\', use_bias=False, name= \'Block8_6_Branch_1_Conv2d_0b_1x3\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_6_Branch_1_Conv2d_0b_1x3_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_6_Branch_1_Conv2d_0b_1x3_Activation\')(branch_1)\r\n\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding=\'same\', use_bias=False, name= \'Block8_6_Branch_1_Conv2d_0c_3x1\') (branch_1)\r\n\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name=\'Block8_6_Branch_1_Conv2d_0c_3x1_BatchNorm\')(branch_1)\r\n\tbranch_1 = Activation(\'relu\', name=\'Block8_6_Branch_1_Conv2d_0c_3x1_Activation\')(branch_1)\r\n\tbranches = [branch_0, branch_1]\r\n\tmixed = Concatenate(axis=3, name=\'Block8_6_Concatenate\')(branches)\r\n\tup = Conv2D(1792, 1, strides=1, padding=\'same\', use_bias=True, name= \'Block8_6_Conv2d_1x1\') (mixed)\r\n\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={\'scale\': 1})(up)\r\n\tx = add([x, up])\r\n\t\r\n\t# Classification block\r\n\tx = GlobalAveragePooling2D(name=\'AvgPool\')(x)\r\n\tx = Dropout(1.0 - 0.8, name=\'Dropout\')(x)\r\n\t# Bottleneck\r\n\tx = Dense(128, use_bias=False, name=\'Bottleneck\')(x)\r\n\tx = BatchNormalization(momentum=0.995, epsilon=0.001, scale=False, name=\'Bottleneck_BatchNorm\')(x)\r\n\r\n\t# Create model\r\n\tmodel = Model(inputs, x, name=\'inception_resnet_v1\')\r\n\r\n\treturn model\r\n'"
model/openface_model.py,2,"b""import tensorflow as tf\r\nimport keras\r\nfrom keras.models import Sequential, Model\r\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\r\nfrom keras.layers.core import Dense, Activation, Lambda, Flatten\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras import backend as K\r\nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\r\nfrom keras.layers.merge import Concatenate\r\n\r\n#-------------------------------------\r\n\r\nmyInput = Input(shape=(96, 96, 3))\r\n\r\nx = ZeroPadding2D(padding=(3, 3), input_shape=(96, 96, 3))(myInput)\r\nx = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(x)\r\nx = BatchNormalization(axis=3, epsilon=0.00001, name='bn1')(x)\r\nx = Activation('relu')(x)\r\nx = ZeroPadding2D(padding=(1, 1))(x)\r\nx = MaxPooling2D(pool_size=3, strides=2)(x)\r\nx = Lambda(lambda x: tf.nn.lrn(x, alpha=1e-4, beta=0.75), name='lrn_1')(x)\r\nx = Conv2D(64, (1, 1), name='conv2')(x)\r\nx = BatchNormalization(axis=3, epsilon=0.00001, name='bn2')(x)\r\nx = Activation('relu')(x)\r\nx = ZeroPadding2D(padding=(1, 1))(x)\r\nx = Conv2D(192, (3, 3), name='conv3')(x)\r\nx = BatchNormalization(axis=3, epsilon=0.00001, name='bn3')(x)\r\nx = Activation('relu')(x)\r\nLambda(lambda x: tf.nn.lrn(x, alpha=1e-4, beta=0.75), name='lrn_2')(x)\r\nx = ZeroPadding2D(padding=(1, 1))(x)\r\nx = MaxPooling2D(pool_size=3, strides=2)(x)\r\n\r\n# Inception3a\r\ninception_3a_3x3 = Conv2D(96, (1, 1), name='inception_3a_3x3_conv1')(x)\r\ninception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn1')(inception_3a_3x3)\r\ninception_3a_3x3 = Activation('relu')(inception_3a_3x3)\r\ninception_3a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3)\r\ninception_3a_3x3 = Conv2D(128, (3, 3), name='inception_3a_3x3_conv2')(inception_3a_3x3)\r\ninception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn2')(inception_3a_3x3)\r\ninception_3a_3x3 = Activation('relu')(inception_3a_3x3)\r\n\r\ninception_3a_5x5 = Conv2D(16, (1, 1), name='inception_3a_5x5_conv1')(x)\r\ninception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn1')(inception_3a_5x5)\r\ninception_3a_5x5 = Activation('relu')(inception_3a_5x5)\r\ninception_3a_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5)\r\ninception_3a_5x5 = Conv2D(32, (5, 5), name='inception_3a_5x5_conv2')(inception_3a_5x5)\r\ninception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn2')(inception_3a_5x5)\r\ninception_3a_5x5 = Activation('relu')(inception_3a_5x5)\r\n\r\ninception_3a_pool = MaxPooling2D(pool_size=3, strides=2)(x)\r\ninception_3a_pool = Conv2D(32, (1, 1), name='inception_3a_pool_conv')(inception_3a_pool)\r\ninception_3a_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_pool_bn')(inception_3a_pool)\r\ninception_3a_pool = Activation('relu')(inception_3a_pool)\r\ninception_3a_pool = ZeroPadding2D(padding=((3, 4), (3, 4)))(inception_3a_pool)\r\n\r\ninception_3a_1x1 = Conv2D(64, (1, 1), name='inception_3a_1x1_conv')(x)\r\ninception_3a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_1x1_bn')(inception_3a_1x1)\r\ninception_3a_1x1 = Activation('relu')(inception_3a_1x1)\r\n\r\ninception_3a = concatenate([inception_3a_3x3, inception_3a_5x5, inception_3a_pool, inception_3a_1x1], axis=3)\r\n\r\n# Inception3b\r\ninception_3b_3x3 = Conv2D(96, (1, 1), name='inception_3b_3x3_conv1')(inception_3a)\r\ninception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn1')(inception_3b_3x3)\r\ninception_3b_3x3 = Activation('relu')(inception_3b_3x3)\r\ninception_3b_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3)\r\ninception_3b_3x3 = Conv2D(128, (3, 3), name='inception_3b_3x3_conv2')(inception_3b_3x3)\r\ninception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn2')(inception_3b_3x3)\r\ninception_3b_3x3 = Activation('relu')(inception_3b_3x3)\r\n\r\ninception_3b_5x5 = Conv2D(32, (1, 1), name='inception_3b_5x5_conv1')(inception_3a)\r\ninception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn1')(inception_3b_5x5)\r\ninception_3b_5x5 = Activation('relu')(inception_3b_5x5)\r\ninception_3b_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5)\r\ninception_3b_5x5 = Conv2D(64, (5, 5), name='inception_3b_5x5_conv2')(inception_3b_5x5)\r\ninception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn2')(inception_3b_5x5)\r\ninception_3b_5x5 = Activation('relu')(inception_3b_5x5)\r\n\r\ninception_3b_pool = Lambda(lambda x: x**2, name='power2_3b')(inception_3a)\r\ninception_3b_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_3b_pool)\r\ninception_3b_pool = Lambda(lambda x: x*9, name='mult9_3b')(inception_3b_pool)\r\ninception_3b_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_3b')(inception_3b_pool)\r\ninception_3b_pool = Conv2D(64, (1, 1), name='inception_3b_pool_conv')(inception_3b_pool)\r\ninception_3b_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_pool_bn')(inception_3b_pool)\r\ninception_3b_pool = Activation('relu')(inception_3b_pool)\r\ninception_3b_pool = ZeroPadding2D(padding=(4, 4))(inception_3b_pool)\r\n\r\ninception_3b_1x1 = Conv2D(64, (1, 1), name='inception_3b_1x1_conv')(inception_3a)\r\ninception_3b_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_1x1_bn')(inception_3b_1x1)\r\ninception_3b_1x1 = Activation('relu')(inception_3b_1x1)\r\n\r\ninception_3b = concatenate([inception_3b_3x3, inception_3b_5x5, inception_3b_pool, inception_3b_1x1], axis=3)\r\n\r\n# Inception3c\r\ninception_3c_3x3 = Conv2D(128, (1, 1), strides=(1, 1), name='inception_3c_3x3_conv1')(inception_3b)\r\ninception_3c_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3c_3x3_bn1')(inception_3c_3x3)\r\ninception_3c_3x3 = Activation('relu')(inception_3c_3x3)\r\ninception_3c_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3c_3x3)\r\ninception_3c_3x3 = Conv2D(256, (3, 3), strides=(2, 2), name='inception_3c_3x3_conv'+'2')(inception_3c_3x3)\r\ninception_3c_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3c_3x3_bn'+'2')(inception_3c_3x3)\r\ninception_3c_3x3 = Activation('relu')(inception_3c_3x3)\r\n\r\ninception_3c_5x5 = Conv2D(32, (1, 1), strides=(1, 1), name='inception_3c_5x5_conv1')(inception_3b)\r\ninception_3c_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3c_5x5_bn1')(inception_3c_5x5)\r\ninception_3c_5x5 = Activation('relu')(inception_3c_5x5)\r\ninception_3c_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3c_5x5)\r\ninception_3c_5x5 = Conv2D(64, (5, 5), strides=(2, 2), name='inception_3c_5x5_conv'+'2')(inception_3c_5x5)\r\ninception_3c_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3c_5x5_bn'+'2')(inception_3c_5x5)\r\ninception_3c_5x5 = Activation('relu')(inception_3c_5x5)\r\n\r\ninception_3c_pool = MaxPooling2D(pool_size=3, strides=2)(inception_3b)\r\ninception_3c_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_3c_pool)\r\n\r\ninception_3c = concatenate([inception_3c_3x3, inception_3c_5x5, inception_3c_pool], axis=3)\r\n\r\n#inception 4a\r\ninception_4a_3x3 = Conv2D(96, (1, 1), strides=(1, 1), name='inception_4a_3x3_conv'+'1')(inception_3c)\r\ninception_4a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4a_3x3_bn'+'1')(inception_4a_3x3)\r\ninception_4a_3x3 = Activation('relu')(inception_4a_3x3)\r\ninception_4a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_4a_3x3)\r\ninception_4a_3x3 = Conv2D(192, (3, 3), strides=(1, 1), name='inception_4a_3x3_conv'+'2')(inception_4a_3x3)\r\ninception_4a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4a_3x3_bn'+'2')(inception_4a_3x3)\r\ninception_4a_3x3 = Activation('relu')(inception_4a_3x3)\r\n\r\ninception_4a_5x5 = Conv2D(32, (1,1), strides=(1,1), name='inception_4a_5x5_conv1')(inception_3c)\r\ninception_4a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4a_5x5_bn1')(inception_4a_5x5)\r\ninception_4a_5x5 = Activation('relu')(inception_4a_5x5)\r\ninception_4a_5x5 = ZeroPadding2D(padding=(2,2))(inception_4a_5x5)\r\ninception_4a_5x5 = Conv2D(64, (5,5), strides=(1,1), name='inception_4a_5x5_conv'+'2')(inception_4a_5x5)\r\ninception_4a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4a_5x5_bn'+'2')(inception_4a_5x5)\r\ninception_4a_5x5 = Activation('relu')(inception_4a_5x5)\r\n\r\ninception_4a_pool = Lambda(lambda x: x**2, name='power2_4a')(inception_3c)\r\ninception_4a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_4a_pool)\r\ninception_4a_pool = Lambda(lambda x: x*9, name='mult9_4a')(inception_4a_pool)\r\ninception_4a_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_4a')(inception_4a_pool)\r\n\r\ninception_4a_pool = Conv2D(128, (1,1), strides=(1,1), name='inception_4a_pool_conv'+'')(inception_4a_pool)\r\ninception_4a_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4a_pool_bn'+'')(inception_4a_pool)\r\ninception_4a_pool = Activation('relu')(inception_4a_pool)\r\ninception_4a_pool = ZeroPadding2D(padding=(2, 2))(inception_4a_pool)\r\n\r\ninception_4a_1x1 = Conv2D(256, (1, 1), strides=(1, 1), name='inception_4a_1x1_conv'+'')(inception_3c)\r\ninception_4a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4a_1x1_bn'+'')(inception_4a_1x1)\r\ninception_4a_1x1 = Activation('relu')(inception_4a_1x1)\r\n\r\ninception_4a = concatenate([inception_4a_3x3, inception_4a_5x5, inception_4a_pool, inception_4a_1x1], axis=3)\r\n\r\n#inception4e\r\ninception_4e_3x3 = Conv2D(160, (1,1), strides=(1,1), name='inception_4e_3x3_conv'+'1')(inception_4a)\r\ninception_4e_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4e_3x3_bn'+'1')(inception_4e_3x3)\r\ninception_4e_3x3 = Activation('relu')(inception_4e_3x3)\r\ninception_4e_3x3 = ZeroPadding2D(padding=(1, 1))(inception_4e_3x3)\r\ninception_4e_3x3 = Conv2D(256, (3,3), strides=(2,2), name='inception_4e_3x3_conv'+'2')(inception_4e_3x3)\r\ninception_4e_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4e_3x3_bn'+'2')(inception_4e_3x3)\r\ninception_4e_3x3 = Activation('relu')(inception_4e_3x3)\r\n\r\ninception_4e_5x5 = Conv2D(64, (1,1), strides=(1,1), name='inception_4e_5x5_conv'+'1')(inception_4a)\r\ninception_4e_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4e_5x5_bn'+'1')(inception_4e_5x5)\r\ninception_4e_5x5 = Activation('relu')(inception_4e_5x5)\r\ninception_4e_5x5 = ZeroPadding2D(padding=(2, 2))(inception_4e_5x5)\r\ninception_4e_5x5 = Conv2D(128, (5,5), strides=(2,2), name='inception_4e_5x5_conv'+'2')(inception_4e_5x5)\r\ninception_4e_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4e_5x5_bn'+'2')(inception_4e_5x5)\r\ninception_4e_5x5 = Activation('relu')(inception_4e_5x5)\r\n\r\ninception_4e_pool = MaxPooling2D(pool_size=3, strides=2)(inception_4a)\r\ninception_4e_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_4e_pool)\r\n\r\ninception_4e = concatenate([inception_4e_3x3, inception_4e_5x5, inception_4e_pool], axis=3)\r\n\r\n#inception5a\r\ninception_5a_3x3 = Conv2D(96, (1,1), strides=(1,1), name='inception_5a_3x3_conv'+'1')(inception_4e)\r\ninception_5a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5a_3x3_bn'+'1')(inception_5a_3x3)\r\ninception_5a_3x3 = Activation('relu')(inception_5a_3x3)\r\ninception_5a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_5a_3x3)\r\ninception_5a_3x3 = Conv2D(384, (3,3), strides=(1,1), name='inception_5a_3x3_conv'+'2')(inception_5a_3x3)\r\ninception_5a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5a_3x3_bn'+'2')(inception_5a_3x3)\r\ninception_5a_3x3 = Activation('relu')(inception_5a_3x3)\r\n\r\ninception_5a_pool = Lambda(lambda x: x**2, name='power2_5a')(inception_4e)\r\ninception_5a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_5a_pool)\r\ninception_5a_pool = Lambda(lambda x: x*9, name='mult9_5a')(inception_5a_pool)\r\ninception_5a_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_5a')(inception_5a_pool)\r\n\r\ninception_5a_pool = Conv2D(96, (1,1), strides=(1,1), name='inception_5a_pool_conv'+'')(inception_5a_pool)\r\ninception_5a_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5a_pool_bn'+'')(inception_5a_pool)\r\ninception_5a_pool = Activation('relu')(inception_5a_pool)\r\ninception_5a_pool = ZeroPadding2D(padding=(1,1))(inception_5a_pool)\r\n\r\ninception_5a_1x1 = Conv2D(256, (1,1), strides=(1,1), name='inception_5a_1x1_conv'+'')(inception_4e)\r\ninception_5a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5a_1x1_bn'+'')(inception_5a_1x1)\r\ninception_5a_1x1 = Activation('relu')(inception_5a_1x1)\r\n\r\ninception_5a = concatenate([inception_5a_3x3, inception_5a_pool, inception_5a_1x1], axis=3)\r\n\r\n#inception_5b\r\ninception_5b_3x3 = Conv2D(96, (1,1), strides=(1,1), name='inception_5b_3x3_conv'+'1')(inception_5a)\r\ninception_5b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5b_3x3_bn'+'1')(inception_5b_3x3)\r\ninception_5b_3x3 = Activation('relu')(inception_5b_3x3)\r\ninception_5b_3x3 = ZeroPadding2D(padding=(1,1))(inception_5b_3x3)\r\ninception_5b_3x3 = Conv2D(384, (3,3), strides=(1,1), name='inception_5b_3x3_conv'+'2')(inception_5b_3x3)\r\ninception_5b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5b_3x3_bn'+'2')(inception_5b_3x3)\r\ninception_5b_3x3 = Activation('relu')(inception_5b_3x3)\r\n\r\ninception_5b_pool = MaxPooling2D(pool_size=3, strides=2)(inception_5a)\r\n\r\ninception_5b_pool = Conv2D(96, (1,1), strides=(1,1), name='inception_5b_pool_conv'+'')(inception_5b_pool)\r\ninception_5b_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5b_pool_bn'+'')(inception_5b_pool)\r\ninception_5b_pool = Activation('relu')(inception_5b_pool)\r\n\r\ninception_5b_pool = ZeroPadding2D(padding=(1, 1))(inception_5b_pool)\r\n\r\ninception_5b_1x1 = Conv2D(256, (1,1), strides=(1,1), name='inception_5b_1x1_conv'+'')(inception_5a)\r\ninception_5b_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5b_1x1_bn'+'')(inception_5b_1x1)\r\ninception_5b_1x1 = Activation('relu')(inception_5b_1x1)\r\n\r\ninception_5b = concatenate([inception_5b_3x3, inception_5b_pool, inception_5b_1x1], axis=3)\r\n\r\nav_pool = AveragePooling2D(pool_size=(3, 3), strides=(1, 1))(inception_5b)\r\nreshape_layer = Flatten()(av_pool)\r\ndense_layer = Dense(128, name='dense_layer')(reshape_layer)\r\nnorm_layer = Lambda(lambda  x: K.l2_normalize(x, axis=1), name='norm_layer')(dense_layer)\r\n\r\n# Final Model\r\nmodel = Model(inputs=[myInput], outputs=norm_layer)"""
python/ActivationFunctions.py,20,"b'import tensorflow as tf\r\nimport numpy as np\r\nimport logging\r\n\r\nfrom tensorflow.contrib.learn.python.learn.utils import input_fn_utils\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\natributes = [\r\n\t[0, 0]\r\n\t, [0, 1]\r\n\t, [1, 0]\r\n\t, [1, 1]\r\n]\r\n\r\nlabels = [\r\n\t0\r\n\t, 1\r\n\t, 1\r\n\t, 0\r\n]\r\n\r\ndata = np.array(atributes, \'int64\')\r\ntarget = np.array(labels, \'int64\')\r\n\r\nfeature_columns = [tf.contrib.layers.real_valued_column(""""\r\n\t\t\t\t\t\t\t, dimension=len(atributes[0]) #attributes consist of two columns: x1 and x2.\r\n\t\t\t\t\t\t\t, dtype=tf.float32)]\r\n\r\nlearningRate = 0.1\r\nepoch = 2000\r\n\r\nvalidation_monitor = tf.contrib.learn.monitors.ValidationMonitor(data, target, every_n_steps = 500)\r\n\r\nsigmoid_classifier = tf.contrib.learn.DNNClassifier(\r\n\tfeature_columns = feature_columns\r\n\t, hidden_units = [3]\r\n\t, activation_fn = tf.nn.sigmoid\r\n\t, optimizer = tf.train.GradientDescentOptimizer(learningRate)\r\n\t, model_dir = ""model/sigmoid""\r\n\t, config = tf.contrib.learn.RunConfig(save_checkpoints_secs = 10)\r\n)\r\n\r\ntanh_classifier = tf.contrib.learn.DNNClassifier(\r\n\tfeature_columns = feature_columns\r\n\t, hidden_units = [3]\r\n\t, activation_fn = tf.nn.tanh\r\n\t, optimizer = tf.train.GradientDescentOptimizer(learningRate)\r\n\t, model_dir = ""model/tanh""\r\n\t, config = tf.contrib.learn.RunConfig(save_checkpoints_secs = 10)\r\n)\r\n\r\nsoftplus_classifier = tf.contrib.learn.DNNClassifier(\r\n\tfeature_columns = feature_columns\r\n\t, hidden_units = [3]\r\n\t, activation_fn = tf.nn.softplus\r\n\t, optimizer = tf.train.GradientDescentOptimizer(learningRate)\r\n\t, model_dir = ""model/softplus""\r\n\t, config = tf.contrib.learn.RunConfig(save_checkpoints_secs = 10)\r\n)\r\n\r\nrelu_classifier = tf.contrib.learn.DNNClassifier(\r\n\tfeature_columns = feature_columns\r\n\t, hidden_units = [3]\r\n\t, activation_fn = tf.nn.relu\r\n\t, optimizer = tf.train.GradientDescentOptimizer(learningRate)\r\n\t, model_dir = ""model/relu""\r\n\t, config = tf.contrib.learn.RunConfig(save_checkpoints_secs = 10)\r\n)\r\n\r\nsigmoid_classifier.fit(data, target, steps = epoch, monitors = [validation_monitor])\r\ntanh_classifier.fit(data, target, steps = epoch, monitors = [validation_monitor])\r\nsoftplus_classifier.fit(data, target, steps = epoch, monitors = [validation_monitor])\r\nrelu_classifier.fit(data, target, steps = epoch, monitors = [validation_monitor])'"
python/DNNClassifier.py,23,"b'import tensorflow as tf\r\nimport numpy as np\r\nimport logging\r\n\r\nfrom tensorflow.contrib.learn.python.learn.utils import input_fn_utils\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\natributes = [\r\n\t[0, 0]\r\n\t, [0, 1]\r\n\t, [1, 0]\r\n\t, [1, 1]\r\n]\r\n\r\nlabels = [\r\n\t0\r\n\t, 1\r\n\t, 1\r\n\t, 0\r\n]\r\n\r\n#data = np.array(atributes, \'float32\') #data and target tranformed to int instead of float, because exception throws for metric operations\r\n#target = np.array(labels, \'float32\')\r\ndata = np.array(atributes, \'int64\')\r\ntarget = np.array(labels, \'int64\')\r\n\r\nfeature_columns = [tf.contrib.layers.real_valued_column(""""\r\n\t\t\t\t\t\t\t, dimension=len(atributes[0]) #attributes consist of two columns: x1 and x2.\r\n\t\t\t\t\t\t\t, dtype=tf.float32)]\r\n\r\nlearningRate = 0.1\r\nepoch = 10000\r\n\r\n#available metrics: https://www.tensorflow.org/api_docs/python/tf/metrics\r\nvalidation_metrics = {\r\n\t""accuracy"": tf.contrib.learn.MetricSpec(metric_fn = tf.contrib.metrics.streaming_accuracy\r\n\t\t, prediction_key = tf.contrib.learn.PredictionKey.CLASSES)\r\n\t, ""precision"": tf.contrib.learn.MetricSpec(metric_fn = tf.contrib.metrics.streaming_precision\r\n\t\t, prediction_key = tf.contrib.learn.PredictionKey.CLASSES)\r\n\t, ""recall"": tf.contrib.learn.MetricSpec(metric_fn = tf.contrib.metrics.streaming_recall\r\n\t\t, prediction_key = tf.contrib.learn.PredictionKey.CLASSES)\r\n\t, ""mean_absolute_error"": tf.contrib.learn.MetricSpec(metric_fn = tf.contrib.metrics.streaming_mean_absolute_error\r\n\t\t, prediction_key = tf.contrib.learn.PredictionKey.CLASSES)\r\n\t, ""false_negatives"": tf.contrib.learn.MetricSpec(metric_fn = tf.contrib.metrics.streaming_false_negatives\r\n\t\t, prediction_key = tf.contrib.learn.PredictionKey.CLASSES)\r\n\t, ""false_positives"": tf.contrib.learn.MetricSpec(metric_fn = tf.contrib.metrics.streaming_false_positives\r\n\t\t, prediction_key = tf.contrib.learn.PredictionKey.CLASSES)\r\n\t, ""true_positives"": tf.contrib.learn.MetricSpec(metric_fn = tf.contrib.metrics.streaming_true_positives\r\n\t\t, prediction_key = tf.contrib.learn.PredictionKey.CLASSES)\r\n}\r\n\r\nvalidation_monitor = tf.contrib.learn.monitors.ValidationMonitor(data, target, every_n_steps = 500\r\n\t, metrics = validation_metrics #normally tp, fp, fn are not traced. we can add these metrics by adding metrics param\r\n)\r\n\r\nclassifier = tf.contrib.learn.DNNClassifier(\r\n\tfeature_columns = feature_columns\r\n\t, hidden_units = [3]\r\n\t, activation_fn = tf.nn.sigmoid\r\n\t, optimizer = tf.train.GradientDescentOptimizer(learningRate)\r\n\t, model_dir = ""model""\r\n\t, config = tf.contrib.learn.RunConfig(save_checkpoints_secs = 1)\r\n)\r\n\r\nclassifier.fit(data, target, steps = epoch\r\n\t, monitors = [validation_monitor])\r\n\r\n#print(""params: "", classifier.get_variable_names())\r\nprint(""total epoch: "", classifier.get_variable_value(""global_step""))\r\nprint(""weights from input layer to hidden layer\\n"", classifier.get_variable_value(""dnn/hiddenlayer_0/weights""))\r\nprint(""weights from hidden layer to output layer\\n"", classifier.get_variable_value(""dnn/logits/weights""))\r\n\r\n""""""\r\n#this block is deactivated because I would not exported saved model in external system like Java anymore\r\nfeature_spec = tf.contrib.layers.create_feature_spec_for_parsing(feature_columns)\r\nserving_input_fn = input_fn_utils.build_parsing_serving_input_fn(feature_spec)\r\nclassifier.export_savedmodel(classifier.model_dir+""\\export"", serving_input_fn, as_text=True)\r\n""""""\r\n\r\ndef test_set():\r\n\treturn np.array(atributes, np.float32)\r\n\r\npredictions = classifier.predict_classes(input_fn = test_set)\r\n\r\n#dumping predictions\r\nindex = 0\r\nfor i in predictions:\r\n\t\r\n\tprint(data[index], "" -> actual: "", target[index], "", predict: "", i)\r\n\tindex  = index + 1\r\n\r\n#--------------------------------\r\n\r\n#dumping metrics\r\nsuccess_metrics = classifier.evaluate(data, target, metrics = validation_metrics)\r\nprint(""FN: "", success_metrics[""false_negatives""])\r\nprint(""FP: "", success_metrics[""false_positives""])\r\nprint(""TP: "", success_metrics[""true_positives""])\r\nprint(""-----------------"")\r\nprint(""precision: "", success_metrics[""precision""]) # TP / (FP + TP)\r\nprint(""recall: "", success_metrics[""recall""]) # TP / (FN + TP)\r\nprint(""accuracy: "", success_metrics[""accuracy""])\r\nprint(""mae: "", success_metrics[""mean_absolute_error""])\r\n'"
python/DNNRegressor.py,18,"b'import tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.contrib.learn.python.learn.utils import input_fn_utils #export_savedmodel\r\n\r\nimport matplotlib.pyplot as plt\r\nimport io\r\n\r\n#----------------------------\r\n\r\ninput = np.loadtxt(""sine.csv"", dtype=\'f\', delimiter=\',\')\r\n#print(input)\r\n\r\nrow = input.shape[0]\r\ncol = input.shape[1]\r\n\r\n#----------------------------\r\n#attributes and labels\r\nattributes = [[0 for i in range(col-1)] for j in range(row)]\r\nlabels = []\r\nfor i in range(row):\r\n\tlabels.append(0)\r\n\r\nfor i in range(0, row):\r\n\tfor j in range(0, col):\r\n\t\tif j < col -1:\r\n\t\t\tattributes[i][j] = input[i][j]\r\n\t\telse:\r\n\t\t\tlabels[i] = input[i][j]\r\n\r\ndata =np.array(attributes, \'float32\')\r\ntarget = np.array(labels, \'float32\')\r\n\r\n#----------------------------\r\n\r\n#nn learning parameters \r\nlearningRate = 0.1\r\nepoch = 10000\r\n\r\n#----------------------------\r\n\r\nfeature_columns = [tf.contrib.layers.real_valued_column("""", dimension = col-1)]\r\n\r\n#neural network model\r\nregressor = tf.contrib.learn.DNNRegressor(\r\n\tfeature_columns = feature_columns\r\n\t, hidden_units = [4] #a hidden layer consisting of 4 nodes\r\n\t, optimizer = tf.train.GradientDescentOptimizer(learningRate)\r\n\t, activation_fn = tf.nn.sigmoid\r\n\t, model_dir = ""model"" #model will be stored in this folder\r\n\t, config = tf.contrib.learn.RunConfig(save_checkpoints_secs = 1)\r\n)\r\n\r\nvalidation_monitor = tf.contrib.learn.monitors.ValidationMonitor(data, target, every_n_steps = 1000)\r\n\r\nregressor.fit(data, target, steps = epoch\r\n\t, monitors = [validation_monitor]\r\n)\r\n\r\n""""""\r\n#this block provides to export nn model language neutrally. in this way, same model can be used in high level systems such as Java\r\nfeature_spec = tf.contrib.layers.create_feature_spec_for_parsing(feature_columns)\r\nserving_input_fn = input_fn_utils.build_parsing_serving_input_fn(feature_spec)\r\nregressor.export_savedmodel(regressor.model_dir+""\\export"", serving_input_fn, as_text=True)\r\n""""""\r\n\r\ndef test_set():\r\n\treturn np.array(attributes, np.float32)\r\n\r\npredictions = regressor.predict_scores(input_fn = test_set)\r\n\r\n#--------------------------\r\n#dumping predictions and actual sets\r\n\r\nindex = 0\r\nfor i in predictions:\r\n\tprint(""actual: "", target[index],"", predic: "", i)\r\n\tindex = index + 1\r\n\r\n#--------------------------\r\n\r\n#model performance\r\neva = regressor.evaluate(data, target)\r\nprint(""MSE: "", eva[""loss""])\r\n\r\n#--------------------------\r\n\r\n#visualizing predictions and actuals in TensorBoard\r\n\r\n#predictions have to be restored for processing\r\npredictions = regressor.predict_scores(input_fn = test_set)\r\n\r\nactuals = labels\r\nforecasts = list(predictions)\r\n\r\nforecast_writer = tf.summary.FileWriter(\'model/forecast\')\r\nactual_writer = tf.summary.FileWriter(\'model/actual\')\r\n\r\nfor i in range(0, row):\r\n\tactual_summary = tf.Summary(\r\n\t\tvalue = [tf.Summary.Value(tag=""summary_tag"", simple_value= actuals[i])])\r\n\tforecast_summary = tf.Summary(\r\n\t\tvalue = [tf.Summary.Value(tag=""summary_tag"", simple_value= forecasts[i])])\r\n\t\t\r\n\tactual_writer.add_summary(actual_summary, i)\r\n\tforecast_writer.add_summary(forecast_summary, i)\r\n\r\n#--------------------------\r\n\r\n""""""\r\n#x-axis: time, y-axis:value. actual and forecast values are plotted as 2 line in same graph. \r\n#this graph cannot be readable for displaying too many point\r\nplt.plot(actuals)\r\nplt.plot(forecasts)\r\nplt.show()\r\n""""""\r\n\r\n""""""\r\n#x-axis represents predicted values whereas y-axis represents actual values. \r\nplt.scatter(actuals, forecasts)\r\nplt.xlabel(\'predicted\')\r\nplt.ylabel(\'actual\')\r\nplt.show()\r\n""""""\r\n\r\n#----------------------------------\r\n#embedding matplotlib graph in TensorBoard\r\n\r\ndef create_plot(actuals, forecasts):\r\n\tplt.figure()\r\n\tplt.scatter(actuals, forecasts)\r\n\tbuf = io.BytesIO()\r\n\tplt.savefig(buf, format=\'png\')\r\n\tbuf.seek(0)\r\n\treturn buf\r\n\r\nplot_buf = create_plot(actuals, forecasts)\r\nimage = tf.image.decode_png(plot_buf.getvalue(), channels=4)\r\nimage = tf.expand_dims(image, 0)\r\n\r\nsummary_img = tf.summary.image(""scatter_plot"", image)\r\n\r\nsess = tf.Session()\r\n\r\nsummary = sess.run(summary_img)\r\nwriter = tf.summary.FileWriter(\'model/logs\')\r\nwriter.add_summary(summary)'"
python/FocusingOnFacesWithOpenCv.py,0,"b""import cv2\r\nimport time\r\nimport numpy as np\r\n\r\n#-----------------------\r\n\r\ntime_threshold = 10\r\nframe_threshold = 15\r\nmargin = 10\r\n\r\n#-----------------------\r\n\r\nfreeze = False\r\nface_detected = False; face_included_frames = 0\r\ntic = time.time()\r\n\r\nface_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\r\n\r\ncap = cv2.VideoCapture(0) #webcam\r\n\r\nwhile(True):\r\n\tret, img = cap.read()\r\n\traw_img = img.copy()\r\n\tresolution = img.shape #(480, 640, 3)\r\n\r\n\tif freeze == False: \r\n\t\tfaces = face_cascade.detectMultiScale(img, 1.3, 5)\r\n\t\t\r\n\t\tif len(faces) == 0:\r\n\t\t\tface_included_frames = 0\r\n\telse: \r\n\t\tfaces = []\r\n\t\r\n\tdetected_faces = []\r\n\tface_index = 0\r\n\tfor (x,y,w,h) in faces:\r\n\t\tif w > 130: #discard small detected faces\r\n\t\t\t\r\n\t\t\tface_detected = True\r\n\t\t\tif face_index == 0:\r\n\t\t\t\tface_included_frames = face_included_frames + 1 #increase frame for a single face\r\n\t\t\t\r\n\t\t\tcv2.rectangle(img, (x,y), (x+w,y+h), (67,67,67), 1) #draw rectangle to main image\r\n\t\t\t\r\n\t\t\tcv2.putText(img, str(frame_threshold - face_included_frames), (int(x+w/4),int(y+h/1.5)), cv2.FONT_HERSHEY_SIMPLEX, 4, (255, 255, 255), 2)\r\n\t\t\t\r\n\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\r\n\t\t\t\r\n\t\t\t#add margin to detected face\r\n\t\t\ttry:\r\n\t\t\t\tmargin_x = int((w * margin)/100)\r\n\t\t\t\tmargin_y = int((h * margin)/100)\r\n\t\t\t\tdetected_face = img[int(y-margin_y):int(y+h+2*margin_y), int(x-margin_x):int(x+w+2*margin_x)]\r\n\t\t\t\t\r\n\t\t\t\tdetected_faces.append((x-margin_x,y-margin_y,w+2*margin_x,h+2*margin_y))\r\n\t\t\t\t\r\n\t\t\texcept:\r\n\t\t\t\tdetected_faces.append((x,y,w,h))\r\n\t\t\tface_index = face_index + 1\r\n\t\t\t#detected_faces.append((x,y,w,h))\r\n\t\t\t\r\n\tif face_detected == True and face_included_frames == frame_threshold and freeze == False:\r\n\t\tfreeze = True\r\n\t\t#base_img = img.copy()\r\n\t\tbase_img = raw_img.copy()\r\n\t\tdetected_faces_final = detected_faces.copy()\r\n\t\ttic = time.time()\r\n\t\r\n\tif freeze == True:\r\n\t\t\r\n\t\tnn_tic = time.time()\r\n\t\t#--------------------------------------\r\n\t\t#put face recognition code block here\r\n\t\t\r\n\t\t#--------------------------------------\r\n\t\tnn_toc = time.time()\r\n\t\t#nn_toc - nn_tic is time for face recognition, ignore it\r\n\t\t\r\n\t\ttoc = time.time()\r\n\t\tif (toc - tic) - (nn_toc - nn_tic) < time_threshold:\r\n\t\t\tfreeze_img = base_img.copy()\r\n\t\t\t#freeze_img = np.zeros(resolution, np.uint8) #here, np.uint8 handles showing white area issue\r\n\t\t\t\r\n\t\t\tfor detected_face in detected_faces_final:\r\n\t\t\t\tx_offset = detected_face[0]\r\n\t\t\t\ty_offset = detected_face[1]\r\n\t\t\t\tw_offset = detected_face[2]\r\n\t\t\t\th_offset = detected_face[3]\r\n\t\t\t\t\r\n\t\t\t\tcv2.rectangle(freeze_img, (x_offset,y_offset), (x_offset+w_offset,y_offset+h_offset), (67,67,67), 2) #draw rectangle to main image\r\n\t\t\t\r\n\t\t\t\tcustom_face = base_img[y_offset:y_offset+h_offset, x_offset:x_offset+w_offset]\r\n\t\t\t\tfreeze_img[y_offset:y_offset+h_offset, x_offset:x_offset+w_offset] = custom_face\r\n\t\t\t\r\n\t\t\ttime_left = int(time_threshold - (toc - tic) + 1)\r\n\t\t\t#print(time_left)\r\n\t\t\tcv2.putText(freeze_img, str(time_left), (40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\r\n\t\t\tcv2.imshow('img', freeze_img)\r\n\t\telse:\r\n\t\t\tface_detected = False\r\n\t\t\tface_included_frames = 0\r\n\t\t\tfreeze = False\r\n\t\t\r\n\telse:\r\n\t\tcv2.imshow('img',img)\r\n\t\r\n\tif cv2.waitKey(1) & 0xFF == ord('q'): #press q to quit\r\n\t\tbreak\r\n\t\r\n#kill open cv things\t\t\r\ncap.release()\r\ncv2.destroyAllWindows()"""
python/HandwrittenDigitRecognitionUsingCNNWithKeras.py,0,"b'#This program applies convolutional neural networks to handwritten digit dataset (MNIST)\r\n#It consumes Keras API, uses TensorFlow as backend\r\n#It produces 99.29% accuracy on test set\r\n\r\n#blog post: https://sefiks.com/2017/11/05/handwritten-digit-recognition-using-cnn-with-keras/\r\n\r\n#---------------------------------------\r\nimport keras\r\nfrom keras.datasets import mnist\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Activation, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling2D\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\n\r\nnum_classes = 10 #results can be from 0 to 9\r\n\r\n#you might change batch size and epoch to monitor the effect on system.\r\n#more successful results can be produced if optimum batch size and epoch values found\r\nbatch_size = 250\r\nepochs = 10\r\n\r\n# the data, shuffled and split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\n#---------------------------------------\r\n\r\nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1) #transform 2D 28x28 matrix to 3D (28x28x1) matrix\r\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\r\n\r\nx_train = x_train.astype(\'float32\')\r\nx_test = x_test.astype(\'float32\')\r\n\r\nx_train /= 255 #inputs have to be between [0, 1]\r\nx_test /= 255\r\n\r\nprint(\'x_train shape:\', x_train.shape)\r\nprint(x_train.shape[0], \'train samples\')\r\nprint(x_test.shape[0], \'test samples\')\r\n\r\n#---------------------------------------\r\n\r\n# convert labels to binary form\r\ny_train = keras.utils.to_categorical(y_train, num_classes) #e.g. label 2 would be represented as 0010000000\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\n#---------------------------------------\r\n#create neural networks structure\r\nmodel = Sequential()\r\n\r\n#1st convolution layer\r\nmodel.add(Conv2D(32, (3, 3) #32 is number of filters and (3, 3) is the size of the filter.\r\n\t, input_shape=(28,28,1)))\r\nmodel.add(Activation(\'relu\'))\r\nmodel.add(MaxPooling2D(pool_size=(2,2)))\r\n\r\nmodel.add(Conv2D(64,(3, 3))) # apply 64 filters sized of (3x3) on 2nd convolution layer\r\nmodel.add(Activation(\'relu\'))\r\nmodel.add(MaxPooling2D(pool_size=(2,2)))\r\n\r\nmodel.add(Flatten())\r\n\r\n# Fully connected layer. 1 hidden layer consisting of 512 nodes\r\nmodel.add(Dense(512))\r\nmodel.add(Activation(\'relu\'))\r\nmodel.add(Dense(num_classes, activation=\'softmax\'))\r\n\r\n#---------------------------------------\r\n\r\n#Train the model with small size instances. Thus, you can create a model with a single CPU in a short time.\r\ngen = ImageDataGenerator()\r\n\r\ntrain_generator = gen.flow(x_train, y_train, batch_size=batch_size)\r\n\r\n#---------------------------------------\r\n\r\nmodel.compile(loss=\'categorical_crossentropy\'\r\n\t, optimizer=keras.optimizers.Adam()\r\n\t, metrics=[\'accuracy\']\r\n)\r\n\r\nmodel.fit_generator(train_generator, steps_per_epoch=batch_size, epochs=epochs, \r\n\tvalidation_data=(x_test, y_test) #validate on all test set\r\n)\r\nmodel.save(""model.hdf5"")\r\n#---------------------------------------\r\n\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\nprint(\'Test loss:\', score[0])\r\nprint(\'Test accuracy:\', 100*score[1])\r\n\r\n#---------------------------------------\r\nmodel = load_model(""model.hdf5"")\r\n\r\npredictions = model.predict(x_test)\r\n\r\n#display wrongly classified instances\r\nindex = 0\r\nfor i in predictions:\r\n\tif index < 10000:\r\n\t\tactual = np.argmax(y_test[index])\r\n\t\tpred = np.argmax(i)\r\n\t\t\r\n\t\tif actual != pred:\r\n\t\t\tprint(""predict: "",pred,"" actual: "",actual)\r\n\t\t\tpicture = x_test[index]\r\n\t\t\tpicture = picture.reshape([28, 28]);\r\n\t\t\tplt.gray()\r\n\t\t\tplt.imshow(picture)\r\n\t\t\tplt.show()\r\n\t\t\r\n\tindex = index + 1\r\n'"
python/HandwrittenDigitsClassification.py,7,"b'#This code classifies handwritten digits \r\n#Also known as MNIST - Modified National Institute of Standards and Technology database\r\n\r\n#This configuration produced 98.01% accuracy for test set whereas it produced 99.77% accuracy for trainset. \r\n#Producing close accuracy rates is expected for re-run (random initialization causes to produce different results each time)\r\n\r\n#blog post: https://sefiks.com/2017/09/11/handwritten-digit-classification-with-tensorflow/\r\n\r\n#-----------------------------------------------\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport math\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n#-----------------------------------------------\r\n#variables\r\n\r\nepoch = 15000\r\nlearningRate = 0.1\r\nbatch_size = 120\r\n\r\nmnist_data = ""C:/tmp/MNIST_data""\r\n\r\ntrainForRandomSet = True\r\n\r\n#-----------------------------------------------\r\n#data process and transformation\r\n\r\nMNIST_DATASET = input_data.read_data_sets(mnist_data)\r\n\r\ntrain_data = np.array(MNIST_DATASET.train.images, \'float32\')\r\ntrain_target = np.array(MNIST_DATASET.train.labels, \'int64\')\r\nprint(""training set consists of "", len(MNIST_DATASET.train.images), "" instances"")\r\n\r\ntest_data = np.array(MNIST_DATASET.test.images, \'float32\')\r\ntest_target = np.array(MNIST_DATASET.test.labels, \'int64\')\r\nprint(""test set consists of "", len(MNIST_DATASET.test.images), "" instances"")\r\n\r\n#-----------------------------------------------\r\n#visualization\r\nprint(""input layer consists of "", len(MNIST_DATASET.train.images[1]), "" features (""\r\n\t,math.sqrt(len(MNIST_DATASET.train.images[1])), ""x"", math.sqrt(len(MNIST_DATASET.train.images[1])),"" pixel images)"") #28x28 = 784 input feature\r\n""""""\r\nprint(""features: "", MNIST_DATASET.train.images[1])\r\nprint(""labels: "", MNIST_DATASET.train.labels[1])\r\n""""""\r\n\r\n""""""\r\n#to display a sample\r\nsample = 2\r\n#print(MNIST_DATASET.train.images[sample])\r\nprint(MNIST_DATASET.train.labels[sample])\r\n\r\nX = MNIST_DATASET.train.images[sample]\r\nX = X.reshape([28, 28]);\r\n#X = X.reshape([math.sqrt(len(MNIST_DATASET.train.images[1])), math.sqrt(len(MNIST_DATASET.train.images[1]))]);\r\nplt.gray()\r\nplt.imshow(X)\r\nplt.show()\r\n""""""\r\n#-----------------------------------------------\r\nfeature_columns = [tf.contrib.layers.real_valued_column("""", dimension=len(MNIST_DATASET.train.images[1]))]\r\n\r\nclassifier = tf.contrib.learn.DNNClassifier(\r\n\tfeature_columns=feature_columns\r\n\t, n_classes=10 #0 to 9 - 10 classes\r\n\t, hidden_units=[128, 32]  #2 hidden layers consisting of 128 and 32 units respectively\r\n\t, optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=learningRate)\r\n\t, activation_fn = tf.nn.relu\r\n\t#, activation_fn = tf.nn.softmax\r\n\t, model_dir=""model""\r\n)\r\n\r\n#----------------------------------------\r\n#training\r\n\r\nif trainForRandomSet == False:\r\n\t#train on all trainset\r\n\tclassifier.fit(train_data, train_target, steps=epoch)\r\nelse:\r\n\tdef generate_input_fn(data, label):\t\r\n\t\timage_batch, label_batch = tf.train.shuffle_batch(\r\n\t\t\t[data, label]\r\n\t\t\t, batch_size=batch_size\r\n\t\t\t, capacity=8*batch_size\r\n\t\t\t, min_after_dequeue=4*batch_size\r\n\t\t\t, enqueue_many=True\r\n\t\t)\r\n\t\treturn image_batch, label_batch\r\n\t\r\n\tdef input_fn_for_train():\r\n\t\treturn generate_input_fn(train_data, train_target)\r\n\t\r\n\t#train on small random selected dataset\r\n\tclassifier.fit(input_fn=input_fn_for_train, steps=epoch)\r\n\r\nprint(""\\n---training is over..."")\r\n\r\n#----------------------------------------\r\n#apply to make predictions\r\n\r\npredictions = classifier.predict_classes(test_data)\r\nindex = 0\r\nfor i in predictions:\r\n\tif index < 10: #visualize first 10 items on test set\r\n\t\tprint(""actual: "", test_target[index], "", prediction: "", i)\r\n\t\t\r\n\t\tpred = MNIST_DATASET.test.images[index]\r\n\t\tpred = pred.reshape([28, 28]);\r\n\t\tplt.gray()\r\n\t\tplt.imshow(pred)\r\n\t\tplt.show()\r\n\t\t\r\n\tindex  = index + 1\r\n\r\n#----------------------------------------\r\n#calculationg overall accuracy\r\n\r\nprint(""\\n---evaluation..."")\r\naccuracy_score = classifier.evaluate(test_data, test_target, steps=epoch)[\'accuracy\']\r\nprint(""accuracy: "", 100*accuracy_score,""%"")\r\n\r\n'"
python/HelloKeras.py,0,"b""import tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom keras.models import Sequential\r\nfrom keras.layers.core import Dense, Activation\r\nfrom keras.utils import np_utils\r\n\r\n#----------------------------\r\n#preparing data for Exclusive OR (XOR)\r\n\r\nattributes = [\r\n\t#x1, x2\r\n\t[0 ,0]\r\n\t, [0, 1]\r\n\t, [1, 0]\r\n\t, [1, 1]\r\n]\r\n\r\nlabels = [\r\n\t#is_0, is_1 -> only a column can be 1 in labels variable\r\n\t[1, 0] \r\n\t, [0, 1]\r\n\t, [0, 1]\r\n\t, [1, 0]\r\n]\r\n\r\n#transforming attributes and labels matrixes to numpy\r\ndata = np.array(attributes, 'int64')\r\ntarget = np.array(labels, 'int64')\r\n\r\n#----------------------------\r\n#creating model\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(3 #num of hidden units\r\n\t, input_shape=(len(attributes[0]),))) #num of features in input layer\r\nmodel.add(Activation('sigmoid')) #activation function from input layer to 1st hidden layer\r\nmodel.add(Dense(len(labels[0]))) #num of classes in output layer\r\nmodel.add(Activation('softmax')) #activation function from 1st hidden layer to output layer\r\n\r\n#compile\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\r\n\r\n#training\r\nscore = model.fit(data, target, epochs=100, verbose=0)\r\n\r\nprint(score.history)"""
python/KMeansClustering.py,3,"b'import tensorflow as tf\r\nimport numpy as np\r\nimport pylab as pl\r\n\r\nfrom tensorflow.contrib.factorization.python.ops import clustering_ops\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\n\r\n#-----------------------------------------\r\n#variables\r\nclasses = 3    # define number of clusters\r\ndisplay3D = True\r\n\r\n#-----------------------------------------\r\n\r\n#dataset\r\n#monthly expenses, net assets\r\natributes = [\r\n\t\t[1100, 1200]\r\n\t\t, [2200, 2500]\r\n\t\t, [3300, 3600]\r\n\t\t, [2400, 2700]\r\n\t\t, [14100, 3200]\r\n\t\t, [4120, 15200]\r\n\t\t, [3125, 3600]\r\n\t\t, [2400, 13700]\r\n\t\t, [3100, 3200]\r\n\t\t, [4100, 4200]\r\n\t\t, [13100, 13200]\r\n\t\t, [4110, 14200]\r\n\t\t, [5100, 15200]\r\n\t ]\r\n\r\nrow = len(atributes)\r\ncol = len(atributes[0])\r\n\r\nprint(""["", row,""x"",col,""] sized input"")\r\n\r\nif display3D == False:\r\n\tfor i in range(row):\r\n\t\tpl.scatter(atributes[i][0], atributes[i][1], c=\'black\')\r\n\tpl.show()\t\r\n\r\n#-----------------------------------------\r\n\r\nmodel = tf.contrib.learn.KMeansClustering(\r\n\t\tclasses\r\n\t\t, distance_metric = clustering_ops.SQUARED_EUCLIDEAN_DISTANCE #SQUARED_EUCLIDEAN_DISTANCE, COSINE_DISTANCE\r\n\t\t, initial_clusters=tf.contrib.learn.KMeansClustering.RANDOM_INIT\r\n\t)\r\n\r\n#-----------------------------------------\r\n\r\ndef train_input_fn():\r\n    data = tf.constant(atributes, tf.float32)\r\n    return (data, None)\r\n\r\nmodel.fit(input_fn=train_input_fn, steps=5000)\r\n\r\nprint(""--------------------"")\r\nprint(""kmeans model: "",model)\r\n\r\ndef predict_input_fn():\r\n\treturn np.array(atributes, np.float32)\r\n\r\npredictions = model.predict(input_fn=predict_input_fn, as_iterable=True)\r\n\r\ncolors = [\'orange\', \'red\', \'blue\']\r\n\r\nprint(""--------------------"")\r\n\r\nif display3D == True:\r\n\tfig = pl.figure()\r\n\tax = fig.add_subplot(111, projection=\'3d\')\r\n\r\nindex = 0\r\nfor i in predictions:\t\r\n\tprint(""["", atributes[index],""] -> cluster_"",i[\'cluster_idx\'])\r\n\t\r\n\tif display3D == False:\r\n\t\tpl.scatter(atributes[index][0], atributes[index][1], c=colors[i[\'cluster_idx\']]) #2d graph\r\n\tif display3D == True:\r\n\t\tax.scatter(atributes[index][0], atributes[index][1], c=colors[i[\'cluster_idx\']]) #3d graph\r\n\t\r\n\tindex  = index + 1\r\n\r\npl.show()\r\n\r\n#-----------------------------------------\r\n\r\n""""""\r\n#to predict the cluster of new instances\r\ntestset = [[1.3, 1.2]\r\n\t\t, [2.1, 2.3]\r\n\t ]\r\ndef newinstances_input_fn():\r\n\treturn np.array(testset, np.float32)\r\npredictions = model.predict(input_fn=newinstances_input_fn, as_iterable=True)\r\nfor i in predictions:\r\n\tprint(""cluster_"",i[\'cluster_idx\'])\r\n""""""'"
python/KerasModelRestoration.py,0,"b'import tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom keras.models import Sequential\r\nfrom keras.models import load_model\r\nfrom keras.models import model_from_json\r\nfrom keras.layers.core import Dense, Activation\r\nfrom keras.utils import np_utils\r\n\r\n#----------------------------\r\n\r\ntrain = False\r\nload_all_model = True #if train is False\r\n\r\n#----------------------------\r\n#preparing data for Exclusive OR (XOR)\r\n\r\nattributes = [\r\n\t#x1, x2\r\n\t[0 ,0]\r\n\t, [0, 1]\r\n\t, [1, 0]\r\n\t, [1, 1]\r\n]\r\n\r\nlabels = [\r\n\t#is_0, is_1 -> only a column can be 1 in labels variable\r\n\t[1, 0] \r\n\t, [0, 1]\r\n\t, [0, 1]\r\n\t, [1, 0]\r\n]\r\n\r\n#transforming attributes and labels matrixes to numpy\r\ndata = np.array(attributes, \'int64\')\r\ntarget = np.array(labels, \'int64\')\r\n\r\n#----------------------------\r\n#creating model\r\n\r\nif train == True:\t\r\n\tmodel = Sequential()\r\n\tmodel.add(Dense(3 #num of hidden units\r\n\t\t, input_shape=(len(attributes[0]),))) #num of features in input layer\r\n\tmodel.add(Activation(\'sigmoid\')) #activation function from input layer to 1st hidden layer\r\n\tmodel.add(Dense(len(labels[0]))) #num of classes in output layer\r\n\tmodel.add(Activation(\'softmax\')) #activation function from 1st hidden layer to output layer\r\n\t\r\n\tmodel_config = model.to_json()\r\n\topen(""model_structure.json"", ""w"").write(model_config)\r\n\t\r\n\t#compile\r\n\tmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\')\r\n\t\r\n\t#training\r\n\tmodel.fit(data, target, epochs=2000, verbose=0)\r\n\t\r\n\tmodel.save(""model.hdf5"")\r\n\tmodel.save_weights(\'model_weights.h5\')\r\n\t\r\nelse:\r\n\tif load_all_model == True:\r\n\t\tmodel = load_model(""model.hdf5"") #model structure, weights\r\n\t\tprint(""network structure and weights loaded"")\r\n\telse:\r\n\t\tmodel = model_from_json(open(""model_structure.json"", ""r"").read()) #load structure\r\n\t\tprint(""network structure loaded"")\r\n\t\tmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\')\r\n\t\tmodel.load_weights(\'model_weights.h5\') #load weights\r\n\t\tprint(""weights loaded"")\r\n\r\nscore = model.evaluate(data, target)\r\n\r\nprint(score)'"
python/OptimizationAlgorithms.py,20,"b'import tensorflow as tf\r\nimport numpy as np\r\nimport logging\r\n\r\nfrom tensorflow.contrib.learn.python.learn.utils import input_fn_utils\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\natributes = [\r\n\t[0, 0]\r\n\t, [0, 1]\r\n\t, [1, 0]\r\n\t, [1, 1]\r\n]\r\n\r\nlabels = [\r\n\t0\r\n\t, 1\r\n\t, 1\r\n\t, 0\r\n]\r\n\r\ndata = np.array(atributes, \'int64\')\r\ntarget = np.array(labels, \'int64\')\r\n\r\nfeature_columns = [tf.contrib.layers.real_valued_column(""""\r\n\t\t\t\t\t\t\t, dimension=len(atributes[0]) #attributes consist of two columns: x1 and x2.\r\n\t\t\t\t\t\t\t, dtype=tf.float32)]\r\n\r\nlearningRate = 0.1\r\nepoch = 2000\r\n\r\nvalidation_monitor = tf.contrib.learn.monitors.ValidationMonitor(data, target, every_n_steps = 500)\r\n\r\ngradiendescent_classifier = tf.contrib.learn.DNNClassifier(\r\n\tfeature_columns = feature_columns\r\n\t, hidden_units = [3]\r\n\t, activation_fn = tf.nn.sigmoid\r\n\t, optimizer = tf.train.GradientDescentOptimizer(learningRate)\r\n\t, model_dir = ""model/gradientdescent""\r\n\t, config = tf.contrib.learn.RunConfig(save_checkpoints_secs = 1)\r\n)\r\n\r\nadaptive_classifier = tf.contrib.learn.DNNClassifier(\r\n\tfeature_columns = feature_columns\r\n\t, hidden_units = [3]\r\n\t, activation_fn = tf.nn.sigmoid\r\n\t, optimizer = tf.train.AdagradOptimizer(learningRate)\r\n\t, model_dir = ""model/adaptivelearning""\r\n\t, config = tf.contrib.learn.RunConfig(save_checkpoints_secs = 1)\r\n)\r\n\r\nmomentum_classifier = tf.contrib.learn.DNNClassifier(\r\n\tfeature_columns = feature_columns\r\n\t, hidden_units = [3]\r\n\t, activation_fn = tf.nn.sigmoid\r\n\t, optimizer = tf.train.MomentumOptimizer(learningRate, momentum = 0.3)\r\n\t, model_dir = ""model/momentum""\r\n\t, config = tf.contrib.learn.RunConfig(save_checkpoints_secs = 1)\r\n)\r\n\r\nadam_classifier = tf.contrib.learn.DNNClassifier(\r\n\tfeature_columns = feature_columns\r\n\t, hidden_units = [3]\r\n\t, activation_fn = tf.nn.sigmoid\r\n\t, optimizer = tf.train.AdamOptimizer(learningRate)\r\n\t, model_dir = ""model/adam""\r\n\t, config = tf.contrib.learn.RunConfig(save_checkpoints_secs = 1)\r\n)\r\n\r\ngradiendescent_classifier.fit(data, target, steps = epoch, monitors = [validation_monitor])\r\nadaptive_classifier.fit(data, target, steps = epoch, monitors = [validation_monitor])\r\nmomentum_classifier.fit(data, target, steps = epoch, monitors = [validation_monitor])\r\nadam_classifier.fit(data, target, steps = epoch, monitors = [validation_monitor])\r\n\r\n'"
python/age-gender-prediction-real-time.py,0,"b'#Documentation: https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/\r\n\r\nimport numpy as np\r\nimport cv2\r\nfrom keras.models import Model, Sequential\r\nfrom keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\r\nfrom PIL import Image\r\nfrom keras.preprocessing.image import load_img, save_img, img_to_array\r\nfrom keras.applications.imagenet_utils import preprocess_input\r\nfrom keras.preprocessing import image\r\nfrom keras.models import model_from_json\r\nimport matplotlib.pyplot as plt\r\nfrom os import listdir\r\n\r\n#-----------------------\r\n#you can find male and female icons here: https://github.com/serengil/tensorflow-101/tree/master/dataset\r\n\r\nenableGenderIcons = True\r\n\r\nmale_icon = cv2.imread(""male.jpg"")\r\nmale_icon = cv2.resize(male_icon, (40, 40))\r\n\r\nfemale_icon = cv2.imread(""female.jpg"")\r\nfemale_icon = cv2.resize(female_icon, (40, 40))\r\n#-----------------------\r\n\r\nface_cascade = cv2.CascadeClassifier(\'haarcascade_frontalface_default.xml\')\r\n\r\ndef preprocess_image(image_path):\r\n    img = load_img(image_path, target_size=(224, 224))\r\n    img = img_to_array(img)\r\n    img = np.expand_dims(img, axis=0)\r\n    img = preprocess_input(img)\r\n    return img\r\n\r\ndef loadVggFaceModel():\r\n\tmodel = Sequential()\r\n\tmodel.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\r\n\tmodel.add(Convolution2D(64, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(64, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(128, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(128, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(256, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(256, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(256, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(Convolution2D(4096, (7, 7), activation=\'relu\'))\r\n\tmodel.add(Dropout(0.5))\r\n\tmodel.add(Convolution2D(4096, (1, 1), activation=\'relu\'))\r\n\tmodel.add(Dropout(0.5))\r\n\tmodel.add(Convolution2D(2622, (1, 1)))\r\n\tmodel.add(Flatten())\r\n\tmodel.add(Activation(\'softmax\'))\r\n\t\r\n\treturn model\r\n\r\ndef ageModel():\r\n\tmodel = loadVggFaceModel()\r\n\t\r\n\tbase_model_output = Sequential()\r\n\tbase_model_output = Convolution2D(101, (1, 1), name=\'predictions\')(model.layers[-4].output)\r\n\tbase_model_output = Flatten()(base_model_output)\r\n\tbase_model_output = Activation(\'softmax\')(base_model_output)\r\n\t\r\n\tage_model = Model(inputs=model.input, outputs=base_model_output)\r\n\t\r\n\t#you can find the pre-trained weights for age prediction here: https://drive.google.com/file/d/1YCox_4kJ-BYeXq27uUbasu--yz28zUMV/view?usp=sharing\r\n\tage_model.load_weights(""age_model_weights.h5"")\r\n\t\r\n\treturn age_model\r\n\r\ndef genderModel():\r\n\tmodel = loadVggFaceModel()\r\n\t\r\n\tbase_model_output = Sequential()\r\n\tbase_model_output = Convolution2D(2, (1, 1), name=\'predictions\')(model.layers[-4].output)\r\n\tbase_model_output = Flatten()(base_model_output)\r\n\tbase_model_output = Activation(\'softmax\')(base_model_output)\r\n\r\n\tgender_model = Model(inputs=model.input, outputs=base_model_output)\r\n\t\r\n\t#you can find the pre-trained weights for gender prediction here: https://drive.google.com/file/d/1wUXRVlbsni2FN9-jkS_f4UTUrm1bRLyk/view?usp=sharing\r\n\tgender_model.load_weights(""gender_model_weights.h5"")\r\n\t\r\n\treturn gender_model\r\n\t\r\nage_model = ageModel()\r\ngender_model = genderModel()\r\n\r\n#age model has 101 outputs and its outputs will be multiplied by its index label. sum will be apparent age\r\noutput_indexes = np.array([i for i in range(0, 101)])\r\n\r\n#------------------------\r\n\r\ncap = cv2.VideoCapture(0) #capture webcam\r\n\r\nwhile(True):\r\n\tret, img = cap.read()\r\n\t#img = cv2.resize(img, (640, 360))\r\n\t\r\n\tfaces = face_cascade.detectMultiScale(img, 1.3, 5)\r\n\t\r\n\tfor (x,y,w,h) in faces:\r\n\t\tif w > 130: #ignore small faces\r\n\t\t\t\r\n\t\t\t#mention detected face\r\n\t\t\t""""""overlay = img.copy(); output = img.copy(); opacity = 0.6\r\n\t\t\tcv2.rectangle(img,(x,y),(x+w,y+h),(128,128,128),cv2.FILLED) #draw rectangle to main image\r\n\t\t\tcv2.addWeighted(overlay, opacity, img, 1 - opacity, 0, img)""""""\r\n\t\t\tcv2.rectangle(img,(x,y),(x+w,y+h),(128,128,128),1) #draw rectangle to main image\r\n\t\t\t\r\n\t\t\t#extract detected face\r\n\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\r\n\t\t\t\r\n\t\t\ttry:\r\n\t\t\t\t#age gender data set has 40% margin around the face. expand detected face.\r\n\t\t\t\tmargin = 30\r\n\t\t\t\tmargin_x = int((w * margin)/100); margin_y = int((h * margin)/100)\r\n\t\t\t\tdetected_face = img[int(y-margin_y):int(y+h+margin_y), int(x-margin_x):int(x+w+margin_x)]\r\n\t\t\texcept:\r\n\t\t\t\tprint(""detected face has no margin"")\r\n\t\t\t\r\n\t\t\ttry:\r\n\t\t\t\t#vgg-face expects inputs (224, 224, 3)\r\n\t\t\t\tdetected_face = cv2.resize(detected_face, (224, 224))\r\n\t\t\t\t\r\n\t\t\t\timg_pixels = image.img_to_array(detected_face)\r\n\t\t\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\r\n\t\t\t\timg_pixels /= 255\r\n\t\t\t\t\r\n\t\t\t\t#find out age and gender\r\n\t\t\t\tage_distributions = age_model.predict(img_pixels)\r\n\t\t\t\tapparent_age = str(int(np.floor(np.sum(age_distributions * output_indexes, axis = 1))[0]))\r\n\t\t\t\t\r\n\t\t\t\tgender_distribution = gender_model.predict(img_pixels)[0]\r\n\t\t\t\tgender_index = np.argmax(gender_distribution)\r\n\t\t\t\t\r\n\t\t\t\tif gender_index == 0: gender = ""F""\r\n\t\t\t\telse: gender = ""M""\r\n\t\t\t\r\n\t\t\t\t#background for age gender declaration\r\n\t\t\t\tinfo_box_color = (46,200,255)\r\n\t\t\t\t#triangle_cnt = np.array( [(x+int(w/2), y+10), (x+int(w/2)-25, y-20), (x+int(w/2)+25, y-20)] )\r\n\t\t\t\ttriangle_cnt = np.array( [(x+int(w/2), y), (x+int(w/2)-20, y-20), (x+int(w/2)+20, y-20)] )\r\n\t\t\t\tcv2.drawContours(img, [triangle_cnt], 0, info_box_color, -1)\r\n\t\t\t\tcv2.rectangle(img,(x+int(w/2)-50,y-20),(x+int(w/2)+50,y-90),info_box_color,cv2.FILLED)\r\n\t\t\t\t\r\n\t\t\t\t#labels for age and gender\r\n\t\t\t\tcv2.putText(img, apparent_age, (x+int(w/2), y - 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 111, 255), 2)\r\n\t\t\t\t\r\n\t\t\t\tif enableGenderIcons:\r\n\t\t\t\t\tif gender == \'M\': gender_icon = male_icon\r\n\t\t\t\t\telse: gender_icon = female_icon\r\n\t\t\t\t\t\r\n\t\t\t\t\timg[y-75:y-75+male_icon.shape[0], x+int(w/2)-45:x+int(w/2)-45+male_icon.shape[1]] = gender_icon\r\n\t\t\t\telse:\r\n\t\t\t\t\tcv2.putText(img, gender, (x+int(w/2)-42, y - 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 111, 255), 2)\r\n\t\t\t\t\r\n\t\t\texcept Exception as e:\r\n\t\t\t\tprint(""exception"",str(e))\r\n\t\t\t\r\n\tcv2.imshow(\'img\',img)\r\n\t\r\n\tif cv2.waitKey(1) & 0xFF == ord(\'q\'): #press q to quit\r\n\t\tbreak\r\n\t\r\n#kill open cv things\t\t\r\ncap.release()\r\ncv2.destroyAllWindows()\r\n'"
python/celebrity-look-alike-real-time.py,0,"b'import os\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport scipy.io\r\n\r\nimport time\r\n\r\nfrom PIL import Image\r\n\r\nimport cv2\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom keras.models import Model, Sequential\r\nfrom keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\r\nfrom keras.preprocessing.image import load_img, save_img, img_to_array\r\nfrom keras.applications.imagenet_utils import preprocess_input\r\nfrom keras.preprocessing import image\r\n#-----------------------\r\nface_cascade = cv2.CascadeClassifier(\'haarcascade_frontalface_default.xml\')\r\n\r\ndef preprocess_image(image_path):\r\n    img = load_img(image_path, target_size=(224, 224))\r\n    img = img_to_array(img)\r\n    img = np.expand_dims(img, axis=0)\r\n    \r\n    #preprocess_input normalizes input in scale of [-1, +1]. You must apply same normalization in prediction.\r\n    #Ref: https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py (Line 45)\r\n    img = preprocess_input(img)\r\n    return img\r\n\r\ndef loadVggFaceModel():\r\n\tmodel = Sequential()\r\n\tmodel.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\r\n\tmodel.add(Convolution2D(64, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(64, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(128, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(128, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(256, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(256, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(256, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(Convolution2D(4096, (7, 7), activation=\'relu\'))\r\n\tmodel.add(Dropout(0.5))\r\n\tmodel.add(Convolution2D(4096, (1, 1), activation=\'relu\'))\r\n\tmodel.add(Dropout(0.5))\r\n\tmodel.add(Convolution2D(2622, (1, 1)))\r\n\tmodel.add(Flatten())\r\n\tmodel.add(Activation(\'softmax\'))\r\n\t\r\n\t#you can download pretrained weights from https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing\r\n\tfrom keras.models import model_from_json\r\n\tmodel.load_weights(\'vgg_face_weights.h5\')\r\n\t\r\n\tvgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)\r\n\t\r\n\treturn vgg_face_descriptor\r\n\r\nmodel = loadVggFaceModel()\r\nprint(""vgg face model loaded"")\r\n\r\n#------------------------\r\nexists = os.path.isfile(\'representations.pkl\')\r\n\r\nif exists != True: #initializations lasts almost 1 hour. but it can be run once.\r\n\t\r\n\t#download imdb data set here: https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/ . Faces only version (7 GB)\r\n\tmat = scipy.io.loadmat(\'imdb_data_set/imdb.mat\')\r\n\tprint(""imdb.mat meta data file loaded"")\r\n\r\n\tcolumns = [""dob"", ""photo_taken"", ""full_path"", ""gender"", ""name"", ""face_location"", ""face_score"", ""second_face_score"", ""celeb_names"", ""celeb_id""]\r\n\r\n\tinstances = mat[\'imdb\'][0][0][0].shape[1]\r\n\r\n\tdf = pd.DataFrame(index = range(0,instances), columns = columns)\r\n\r\n\tfor i in mat:\r\n\t\tif i == ""imdb"":\r\n\t\t\tcurrent_array = mat[i][0][0]\r\n\t\t\tfor j in range(len(current_array)):\r\n\t\t\t\t#print(j,"". "",columns[j],"": "",current_array[j][0])\r\n\t\t\t\tdf[columns[j]] = pd.DataFrame(current_array[j][0])\r\n\r\n\tprint(""data frame loaded ("",df.shape,"")"")\r\n\r\n\t#-------------------------------\r\n\r\n\t#remove pictures does not include any face\r\n\tdf = df[df[\'face_score\'] != -np.inf]\r\n\r\n\t#some pictures include more than one face, remove them\r\n\tdf = df[df[\'second_face_score\'].isna()]\r\n\r\n\t#discard inclear ones\r\n\tdf = df[df[\'face_score\'] >= 5]\r\n\r\n\t#-------------------------------\r\n\t#some speed up tricks. this is not a must.\r\n\r\n\t#discard old photos\r\n\tdf = df[df[\'photo_taken\'] >= 2000]\r\n\r\n\tprint(""some instances ignored ("",df.shape,"")"")\r\n\t#-------------------------------\r\n\r\n\tdef extractNames(name):\r\n\t\treturn name[0]\r\n\r\n\tdf[\'celebrity_name\'] = df[\'name\'].apply(extractNames)\r\n\r\n\tdef getImagePixels(image_path):\r\n\t\treturn cv2.imread(""imdb_data_set/%s"" % image_path[0]) #pixel values in scale of 0-255\r\n\r\n\ttic = time.time()\r\n\tdf[\'pixels\'] = df[\'full_path\'].apply(getImagePixels)\r\n\ttoc = time.time()\r\n\r\n\tprint(""reading pixels completed in "",toc-tic,"" seconds..."") #3.4 seconds\r\n\r\n\tdef findFaceRepresentation(img):\r\n\t\tdetected_face = img\r\n\t\t\r\n\t\ttry: \r\n\t\t\tdetected_face = cv2.resize(detected_face, (224, 224))\r\n\t\t\t#plt.imshow(cv2.cvtColor(detected_face, cv2.COLOR_BGR2RGB))\r\n\t\t\t\r\n\t\t\t#normalize detected face in scale of -1, +1\r\n\r\n\t\t\timg_pixels = image.img_to_array(detected_face)\r\n\t\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\r\n\t\t\timg_pixels /= 127.5\r\n\t\t\timg_pixels -= 1\r\n\t\t\t\r\n\t\t\trepresentation = model.predict(img_pixels)[0,:]\r\n\t\texcept:\r\n\t\t\trepresentation = None\r\n\t\t\t\r\n\t\treturn representation\r\n\r\n\ttic = time.time()\r\n\tdf[\'face_vector_raw\'] = df[\'pixels\'].apply(findFaceRepresentation) #vector for raw image\r\n\ttoc = time.time()\r\n\tprint(""extracting face vectors completed in "",toc-tic,"" seconds..."")\r\n\r\n\ttic = time.time()\r\n\tdf.to_pickle(""representations.pkl"")\r\n\ttoc = time.time()\r\n\tprint(""storing representations completed in "",toc-tic,"" seconds..."")\r\n\r\nelse:\r\n\t#if you run to_pickle command once, then read pickle completed in seconds in your following runs\r\n\ttic = time.time()\r\n\tdf = pd.read_pickle(""representations.pkl"")\r\n\ttoc = time.time()\r\n\tprint(""reading pre-processed data frame completed in "",toc-tic,"" seconds..."")\r\n\r\n#-----------------------------------------\r\n\r\nprint(""data set: "",df.shape)\r\n\r\ncap = cv2.VideoCapture(0) #webcam\r\n\r\nwhile(True):\r\n\tret, img = cap.read()\r\n\tfaces = face_cascade.detectMultiScale(img, 1.3, 5)\r\n\t\r\n\tresolution_x = img.shape[1]; resolution_y = img.shape[0]\r\n\t\r\n\tfor (x,y,w,h) in faces:\r\n\t\tif w > 0:\r\n\t\t\t#cv2.rectangle(img,(x,y),(x+w,y+h),(128,128,128),1)\r\n\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\r\n\t\t\t\r\n\t\t\t#add 5% margin around the face\r\n\t\t\ttry:\r\n\t\t\t\tmargin = 0 #5\r\n\t\t\t\tmargin_x = int((w * margin)/100); margin_y = int((h * margin)/100)\r\n\t\t\t\tif y-margin_y > 0 and x-margin_x > 0 and y+h+margin_y < resolution_y and x+w+margin_x < resolution_x:\r\n\t\t\t\t\tdetected_face = img[int(y-margin_y):int(y+h+margin_y), int(x-margin_x):int(x+w+margin_x)]\r\n\t\t\texcept:\r\n\t\t\t\tprint(""detected face has no margin"")\r\n\t\t\t\r\n\t\t\tdetected_face = cv2.resize(detected_face, (224, 224)) #resize to 224x224\r\n\t\t\t\r\n\t\t\timg_pixels = image.img_to_array(detected_face)\r\n\t\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\r\n\t\t\t#normalize in scale of [-1, +1]\r\n\t\t\timg_pixels /= 127.5\r\n\t\t\timg_pixels -= 1\r\n\t\t\t\r\n\t\t\tcaptured_representation = model.predict(img_pixels)[0,:]\t\t\t\r\n\t\t\t#----------------------------------------------\r\n\t\t\t\r\n\t\t\tdef findCosineSimilarity(source_representation, test_representation=captured_representation):\r\n\t\t\t\ttry:\r\n\t\t\t\t\ta = np.matmul(np.transpose(source_representation), test_representation)\r\n\t\t\t\t\tb = np.sum(np.multiply(source_representation, source_representation))\r\n\t\t\t\t\tc = np.sum(np.multiply(test_representation, test_representation))\r\n\t\t\t\t\treturn 1 - (a / (np.sqrt(b) * np.sqrt(c)))\r\n\t\t\t\texcept:\r\n\t\t\t\t\treturn 10 #assign a large value. similar faces will have small value.\r\n\t\t\t\r\n\t\t\tdf[\'similarity\'] = df[\'face_vector_raw\'].apply(findCosineSimilarity)\r\n\t\t\t\r\n\t\t\t#look-alike celebrity\r\n\t\t\tmin_index = df[[\'similarity\']].idxmin()[0]\r\n\t\t\tinstance = df.ix[min_index]\r\n\t\t\t\r\n\t\t\tname = instance[\'celebrity_name\']\r\n\t\t\tsimilarity = instance[\'similarity\']\r\n\t\t\tsimilarity = (1 - similarity)*100\r\n\t\t\t\r\n\t\t\t#print(name,"" ("",similarity,""%)"")\r\n\t\t\t\r\n\t\t\tif similarity > 50:\r\n\t\t\t\tfull_path = instance[\'full_path\'][0]\r\n\t\t\t\tcelebrity_img = cv2.imread(""imdb_data_set/%s"" % full_path)\r\n\t\t\t\tcelebrity_img = cv2.resize(celebrity_img, (112, 112))\r\n\t\t\t\t\r\n\t\t\t\ttry:\t\r\n\t\t\t\t\timg[y-120:y-120+112, x+w:x+w+112] = celebrity_img\r\n\r\n\t\t\t\t\tlabel = name+"" (""+""{0:.2f}"".format(similarity)+""%)""\r\n\t\t\t\t\tcv2.putText(img, label, (x+w-10, y - 120 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\r\n\t\t\t\t\t\r\n\t\t\t\t\t#connect face and text\r\n\t\t\t\t\tcv2.line(img,(x+w, y-64),(x+w-25, y-64),(67,67,67),1)\r\n\t\t\t\t\tcv2.line(img,(int(x+w/2),y),(x+w-25,y-64),(67,67,67),1)\r\n\t\t\t\texcept Exception as e:\r\n\t\t\t\t\tprint(""exception occured: "", str(e))\r\n\t\r\n\tcv2.imshow(\'img\',img)\r\n\t\r\n\tif cv2.waitKey(1) & 0xFF == ord(\'q\'): #press q to quit\r\n\t\tbreak\r\n\t\r\n#kill open cv things\t\t\r\ncap.release()\r\ncv2.destroyAllWindows()'"
python/convert-one-hot-encoded.py,1,"b'import pandas as pd\r\nimport time\r\n#------------------------------------\r\n#loading dataset\r\nbegin = time.time()\r\ndf = pd.read_csv(""adult.data""\r\n\t, names=[""age"", ""workclass"", ""fnlwgt"", ""education"", ""education-num"", ""marital-status"", ""occupation"", ""relationship"", ""race"", ""sex"", ""capital-gain"", ""capital-loss"", ""hours-per-week"", ""native-country"", ""earning""])\r\nprint(""dataset loaded in "",time.time()-begin,"" seconds"")\r\n\r\n#------------------------------------\r\nrows = df.shape[0] - 1\r\ncolumns = df.shape[1]\r\n\r\n""""""\r\n#dataset summary\r\nfor i in range(0, columns):\t\t\t\r\n\tif df[df.columns[i]].dtypes != ""int64"":\r\n\t\tprint(df.columns[i],"": "",df[df.columns[i]].unique(),"" ("",len(df[df.columns[i]].unique()),"" classes)"")\r\n\telse:\r\n\t\tprint(df.columns[i])\r\n""""""\r\n\r\n#------------------------------------\r\n\r\nf = open(\'one-hot-encoded.txt\', \'w\')\r\n\r\n#dump header\r\nheader = """"\r\nfor i in range(0, columns):\r\n\t\r\n\tif i == 0:\r\n\t\tseperator = """"\r\n\telse:\r\n\t\tseperator = "",""\t\r\n\t\r\n\tif df[df.columns[i]].dtypes != ""int64"":\t\t\r\n\t\tfor k in range(0, len(df[df.columns[i]].unique())):\r\n\t\t\theader += seperator + df[df.columns[i]].unique()[k]\t\r\n\telse:\r\n\t\theader += seperator + df.columns[i]\r\n\r\n\r\nheader += ""\\n""\t\r\n#print(header)\r\nf.write(header)\r\n\r\n#------------------------------------\r\n\r\n#iterate on rows\r\nfor index, row in df.iterrows():\r\n\tnew_line = """"\r\n\t#iterate on columns\r\n\tfor i in range(0, columns):\r\n\t\r\n\t\tif i == 0:\r\n\t\t\tseperator = """"\r\n\t\telse:\r\n\t\t\tseperator = "",""\r\n\t\t\t\r\n\t\tcolumn_name = df.columns[i]\r\n\t\tif df[df.columns[i]].dtypes == ""int64"":\r\n\t\t\tnew_line = new_line + seperator + str(row[column_name])\r\n\t\telse: #class\r\n\t\t\tnum_hot_encoded_classes = len(df[df.columns[i]].unique())\r\n\t\t\tfor k in range(0, num_hot_encoded_classes):\r\n\t\t\t\tif df[df.columns[i]].unique()[k] == row[column_name]:\r\n\t\t\t\t\tnew_line = new_line + seperator + ""1""\r\n\t\t\t\telse:\r\n\t\t\t\t\tnew_line = new_line + seperator + ""0""\r\n\t\r\n\tnew_line += ""\\n""\r\n\t#print(new_line)\t\t\t\r\n\tf.write(new_line)\r\n\r\n#------------------------------------\r\n\r\nf.close()\r\nprint(""converted to one-hot-encoded dataset in "",time.time()-begin,"" seconds"")'"
python/deep-face-real-time.py,0,"b'#author Sefik Ilkin Serengil\r\n#you can find the documentation of this code from the following link: https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/\r\n\r\nimport numpy as np\r\nimport cv2\r\n\r\nfrom keras.models import Model, Sequential\r\nfrom keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\r\nfrom PIL import Image\r\nfrom keras.preprocessing.image import load_img, save_img, img_to_array\r\nfrom keras.applications.imagenet_utils import preprocess_input\r\nfrom keras.preprocessing import image\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom os import listdir\r\n#-----------------------\r\n\r\ncolor = (67,67,67)\r\n\r\nface_cascade = cv2.CascadeClassifier(\'C:/Users/IS96273/AppData\\Local/Continuum/anaconda3/pkgs/opencv-3.3.1-py35h20b85fd_1/Library/etc/haarcascades/haarcascade_frontalface_default.xml\')\r\n\r\ndef preprocess_image(image_path):\r\n    img = load_img(image_path, target_size=(224, 224))\r\n    img = img_to_array(img)\r\n    img = np.expand_dims(img, axis=0)\r\n    \r\n    #preprocess_input normalizes input in scale of [-1, +1]. You must apply same normalization in prediction.\r\n    #Ref: https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py (Line 45)\r\n    img = preprocess_input(img)\r\n    return img\r\n\r\ndef loadVggFaceModel():\r\n\tmodel = Sequential()\r\n\tmodel.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\r\n\tmodel.add(Convolution2D(64, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(64, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(128, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(128, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(256, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(256, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(256, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(Convolution2D(4096, (7, 7), activation=\'relu\'))\r\n\tmodel.add(Dropout(0.5))\r\n\tmodel.add(Convolution2D(4096, (1, 1), activation=\'relu\'))\r\n\tmodel.add(Dropout(0.5))\r\n\tmodel.add(Convolution2D(2622, (1, 1)))\r\n\tmodel.add(Flatten())\r\n\tmodel.add(Activation(\'softmax\'))\r\n\t\r\n\t#you can download pretrained weights from https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing\r\n\tfrom keras.models import model_from_json\r\n\tmodel.load_weights(\'C:/Users/IS96273/Desktop/vgg_face_weights.h5\')\r\n\t\r\n\tvgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)\r\n\t\r\n\treturn vgg_face_descriptor\r\n\r\nmodel = loadVggFaceModel()\r\n\r\n#------------------------\r\n\r\n#put your employee pictures in this path as name_of_employee.jpg\r\nemployee_pictures = ""C:/Users/IS96273/Desktop/database/""\r\n\r\nemployees = dict()\r\n\r\nfor file in listdir(employee_pictures):\r\n\temployee, extension = file.split(""."")\r\n\temployees[employee] = model.predict(preprocess_image(\'C:/Users/IS96273/Desktop/database/%s.jpg\' % (employee)))[0,:]\r\n\t\r\nprint(""employee representations retrieved successfully"")\r\n\r\ndef findCosineSimilarity(source_representation, test_representation):\r\n    a = np.matmul(np.transpose(source_representation), test_representation)\r\n    b = np.sum(np.multiply(source_representation, source_representation))\r\n    c = np.sum(np.multiply(test_representation, test_representation))\r\n    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\r\n\r\n#------------------------\r\n\r\ncap = cv2.VideoCapture(0) #webcam\r\n#cap = cv2.VideoCapture(\'C:/Users/IS96273/Desktop/zuckerberg.mp4\') #video\r\n\r\nwhile(True):\r\n\tret, img = cap.read()\r\n\t#img = cv2.resize(img, (640, 360))\r\n\tfaces = face_cascade.detectMultiScale(img, 1.3, 5)\r\n\t\r\n\tfor (x,y,w,h) in faces:\r\n\t\tif w > 130: \r\n\t\t\t#cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) #draw rectangle to main image\r\n\t\t\t\r\n\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\r\n\t\t\tdetected_face = cv2.resize(detected_face, (224, 224)) #resize to 224x224\r\n\t\t\t\r\n\t\t\timg_pixels = image.img_to_array(detected_face)\r\n\t\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\r\n\t\t\t#img_pixels /= 255\r\n\t\t\t#employee dictionary is using preprocess_image and it normalizes in scale of [-1, +1]\r\n\t\t\timg_pixels /= 127.5\r\n\t\t\timg_pixels -= 1\r\n\t\t\t\r\n\t\t\tcaptured_representation = model.predict(img_pixels)[0,:]\r\n\t\t\t\r\n\t\t\tfound = 0\r\n\t\t\tfor i in employees:\r\n\t\t\t\temployee_name = i\r\n\t\t\t\trepresentation = employees[i]\r\n\t\t\t\t\r\n\t\t\t\tsimilarity = findCosineSimilarity(representation, captured_representation)\r\n\t\t\t\tif(similarity < 0.30):\r\n\t\t\t\t\tcv2.putText(img, employee_name, (int(x+w+15), int(y-12)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\r\n\t\t\t\t\t\r\n\t\t\t\t\tfound = 1\r\n\t\t\t\t\tbreak\r\n\t\t\t\t\t\r\n\t\t\t#connect face and text\r\n\t\t\tcv2.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),color,1)\r\n\t\t\tcv2.line(img,(x+w,y-20),(x+w+10,y-20),color,1)\r\n\t\t\r\n\t\t\tif(found == 0): #if found image is not in employee database\r\n\t\t\t\tcv2.putText(img, \'unknown\', (int(x+w+15), int(y-12)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\r\n\t\r\n\tcv2.imshow(\'img\',img)\r\n\t\r\n\tif cv2.waitKey(1) & 0xFF == ord(\'q\'): #press q to quit\r\n\t\tbreak\r\n\t\r\n#kill open cv things\t\t\r\ncap.release()\r\ncv2.destroyAllWindows()\r\n'"
python/emotion-analysis-from-video.py,0,"b'import numpy as np\r\nimport cv2\r\nfrom keras.preprocessing import image\r\nimport time\r\n\r\n#-----------------------------\r\n#opencv initialization\r\n\r\nface_cascade = cv2.CascadeClassifier(\'C:/Users/IS96273/AppData/Local/Continuum/anaconda3/envs/tensorflow/Library/etc/haarcascades/haarcascade_frontalface_default.xml\')\r\n\r\n#-----------------------------\r\n#face expression recognizer initialization\r\nfrom keras.models import model_from_json\r\nmodel = model_from_json(open(""facial_expression_model_structure.json"", ""r"").read())\r\nmodel.load_weights(\'facial_expression_model_weights.h5\') #load weights\r\n#-----------------------------\r\n\r\nemotions = (\'angry\', \'disgust\', \'fear\', \'happy\', \'sad\', \'surprise\', \'neutral\')\r\n\r\n#cap = cv2.VideoCapture(\'zuckerberg.mp4\') #process videos\r\ncap = cv2.VideoCapture(0) #process real time web-cam\r\n\r\nframe = 0\r\n\r\nwhile(True):\r\n\tret, img = cap.read()\r\n\t\r\n\timg = cv2.resize(img, (640, 360))\r\n\timg = img[0:308,:]\r\n\r\n\tgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n\r\n\tfaces = face_cascade.detectMultiScale(gray, 1.3, 5)\r\n\r\n\tfor (x,y,w,h) in faces:\r\n\t\tif w > 130: #trick: ignore small faces\r\n\t\t\t#cv2.rectangle(img,(x,y),(x+w,y+h),(64,64,64),2) #highlight detected face\r\n\t\t\t\r\n\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\r\n\t\t\tdetected_face = cv2.cvtColor(detected_face, cv2.COLOR_BGR2GRAY) #transform to gray scale\r\n\t\t\tdetected_face = cv2.resize(detected_face, (48, 48)) #resize to 48x48\r\n\t\t\t\r\n\t\t\timg_pixels = image.img_to_array(detected_face)\r\n\t\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\r\n\t\t\t\r\n\t\t\timg_pixels /= 255 #pixels are in scale of [0, 255]. normalize all pixels in scale of [0, 1]\r\n\t\t\t\r\n\t\t\t#------------------------------\r\n\t\t\t\r\n\t\t\tpredictions = model.predict(img_pixels) #store probabilities of 7 expressions\r\n\t\t\tmax_index = np.argmax(predictions[0])\r\n\t\t\t\r\n\t\t\t#background of expression list\r\n\t\t\toverlay = img.copy()\r\n\t\t\topacity = 0.4\r\n\t\t\tcv2.rectangle(img,(x+w+10,y-25),(x+w+150,y+115),(64,64,64),cv2.FILLED)\r\n\t\t\tcv2.addWeighted(overlay, opacity, img, 1 - opacity, 0, img)\r\n\t\t\t\r\n\t\t\t#connect face and expressions\r\n\t\t\tcv2.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),(255,255,255),1)\r\n\t\t\tcv2.line(img,(x+w,y-20),(x+w+10,y-20),(255,255,255),1)\r\n\t\t\t\r\n\t\t\temotion = """"\r\n\t\t\tfor i in range(len(predictions[0])):\r\n\t\t\t\temotion = ""%s %s%s"" % (emotions[i], round(predictions[0][i]*100, 2), \'%\')\r\n\t\t\t\t\r\n\t\t\t\t""""""if i != max_index:\r\n\t\t\t\t\tcolor = (255,0,0)""""""\r\n\t\t\t\t\t\r\n\t\t\t\tcolor = (255,255,255)\r\n\t\t\t\t\r\n\t\t\t\tcv2.putText(img, emotion, (int(x+w+15), int(y-12+i*20)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\r\n\t\t\t\t\r\n\t\t\t#-------------------------\r\n\t\r\n\tcv2.imshow(\'img\',img)\r\n\t\r\n\tframe = frame + 1\r\n\t#print(frame)\r\n\t\r\n\t#---------------------------------\r\n\t\r\n\tif frame > 227:\r\n\t\tbreak\r\n\t\r\n\tif cv2.waitKey(1) & 0xFF == ord(\'q\'): #press q to quit\r\n\t\tbreak\r\n\r\n#kill open cv things\r\ncap.release()\r\ncv2.destroyAllWindows()\r\n'"
python/face-alignment.py,0,"b'import os\r\nimport cv2\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport pandas as pd\r\nimport math\r\nfrom PIL import Image\r\n\r\n#------------------------\r\n\r\ndef euclidean_distance(a, b):\r\n\tx1 = a[0]; y1 = a[1]\r\n\tx2 = b[0]; y2 = b[1]\r\n\t\r\n\treturn math.sqrt(((x2 - x1) * (x2 - x1)) + ((y2 - y1) * (y2 - y1)))\r\n\r\ndef detectFace(img):\r\n\tfaces = face_detector.detectMultiScale(img, 1.3, 5)\r\n\t#print(""found faces: "", len(faces))\r\n\r\n\tif len(faces) > 0:\r\n\t\tface = faces[0]\r\n\t\tface_x, face_y, face_w, face_h = face\r\n\t\timg = img[int(face_y):int(face_y+face_h), int(face_x):int(face_x+face_w)]\r\n\t\timg_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n\t\t\r\n\t\treturn img, img_gray\r\n\telse:\r\n\t\timg_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n\t\treturn img, img_gray\r\n\t\t#raise ValueError(""No face found in the passed image "")\r\n\r\ndef alignFace(img_path):\r\n\timg = cv2.imread(img_path)\r\n\tplt.imshow(img[:, :, ::-1])\r\n\tplt.show()\r\n\r\n\timg_raw = img.copy()\r\n\r\n\timg, gray_img = detectFace(img)\r\n\t\r\n\teyes = eye_detector.detectMultiScale(gray_img)\r\n\t\r\n\t#print(""found eyes: "",len(eyes))\r\n\t\r\n\tif len(eyes) >= 2:\r\n\t\t#find the largest 2 eye\r\n\t\t\r\n\t\tbase_eyes = eyes[:, 2]\r\n\t\t#print(base_eyes)\r\n\t\t\r\n\t\titems = []\r\n\t\tfor i in range(0, len(base_eyes)):\r\n\t\t\titem = (base_eyes[i], i)\r\n\t\t\titems.append(item)\r\n\t\t\r\n\t\tdf = pd.DataFrame(items, columns = [""length"", ""idx""]).sort_values(by=[\'length\'], ascending=False)\r\n\t\t\r\n\t\teyes = eyes[df.idx.values[0:2]]\r\n\t\t\r\n\t\t#--------------------\r\n\t\t#decide left and right eye\r\n\t\t\r\n\t\teye_1 = eyes[0]; eye_2 = eyes[1]\r\n\t\t\r\n\t\tif eye_1[0] < eye_2[0]:\r\n\t\t\tleft_eye = eye_1\r\n\t\t\tright_eye = eye_2\r\n\t\telse:\r\n\t\t\tleft_eye = eye_2\r\n\t\t\tright_eye = eye_1\r\n\t\t\r\n\t\t#--------------------\r\n\t\t#center of eyes\r\n\t\t\r\n\t\tleft_eye_center = (int(left_eye[0] + (left_eye[2] / 2)), int(left_eye[1] + (left_eye[3] / 2)))\r\n\t\tleft_eye_x = left_eye_center[0]; left_eye_y = left_eye_center[1]\r\n\t\t\r\n\t\tright_eye_center = (int(right_eye[0] + (right_eye[2]/2)), int(right_eye[1] + (right_eye[3]/2)))\r\n\t\tright_eye_x = right_eye_center[0]; right_eye_y = right_eye_center[1]\r\n\t\t\r\n\t\t#center_of_eyes = (int((left_eye_x+right_eye_x)/2), int((left_eye_y+right_eye_y)/2))\r\n\t\t\r\n\t\tcv2.circle(img, left_eye_center, 2, (255, 0, 0) , 2)\r\n\t\tcv2.circle(img, right_eye_center, 2, (255, 0, 0) , 2)\r\n\t\t#cv2.circle(img, center_of_eyes, 2, (255, 0, 0) , 2)\r\n\t\t\r\n\t\t#----------------------\r\n\t\t#find rotation direction\r\n\t\t\r\n\t\tif left_eye_y > right_eye_y:\r\n\t\t\tpoint_3rd = (right_eye_x, left_eye_y)\r\n\t\t\tdirection = -1 #rotate same direction to clock\r\n\t\t\tprint(""rotate to clock direction"")\r\n\t\telse:\r\n\t\t\tpoint_3rd = (left_eye_x, right_eye_y)\r\n\t\t\tdirection = 1 #rotate inverse direction of clock\r\n\t\t\tprint(""rotate to inverse clock direction"")\r\n\t\t\r\n\t\t#----------------------\r\n\t\t\r\n\t\tcv2.circle(img, point_3rd, 2, (255, 0, 0) , 2)\r\n\t\t\r\n\t\tcv2.line(img,right_eye_center, left_eye_center,(67,67,67),1)\r\n\t\tcv2.line(img,left_eye_center, point_3rd,(67,67,67),1)\r\n\t\tcv2.line(img,right_eye_center, point_3rd,(67,67,67),1)\r\n\t\t\r\n\t\ta = euclidean_distance(left_eye_center, point_3rd)\r\n\t\tb = euclidean_distance(right_eye_center, point_3rd)\r\n\t\tc = euclidean_distance(right_eye_center, left_eye_center)\r\n\t\t\r\n\t\t#print(""left eye: "", left_eye_center)\r\n\t\t#print(""right eye: "", right_eye_center)\r\n\t\t#print(""additional point: "", point_3rd)\r\n\t\t#print(""triangle lengths: "",a, b, c)\r\n\t\t\r\n\t\tcos_a = (b*b + c*c - a*a)/(2*b*c)\r\n\t\t#print(""cos(a) = "", cos_a)\r\n\t\tangle = np.arccos(cos_a)\r\n\t\t#print(""angle: "", angle,"" in radian"")\r\n\t\t\r\n\t\tangle = (angle * 180) / math.pi\r\n\t\tprint(""angle: "", angle,"" in degree"")\r\n\t\t\r\n\t\tif direction == -1:\r\n\t\t\tangle = 90 - angle\r\n\t\t\r\n\t\tprint(""angle: "", angle,"" in degree"")\r\n\t\t\r\n\t\t#--------------------\r\n\t\t#rotate image\r\n\t\t\r\n\t\tnew_img = Image.fromarray(img_raw)\r\n\t\tnew_img = np.array(new_img.rotate(direction * angle))\r\n\t\r\n\treturn new_img\r\n\t\r\n#------------------------\r\n\r\n#opencv path\r\n\r\nopencv_home = cv2.__file__\r\nfolders = opencv_home.split(os.path.sep)[0:-1]\r\n\r\npath = folders[0]\r\nfor folder in folders[1:]:\r\n\tpath = path + ""/"" + folder\r\n\r\nface_detector_path = path+""/data/haarcascade_frontalface_default.xml""\r\neye_detector_path = path+""/data/haarcascade_eye.xml""\r\nnose_detector_path = path+""/data/haarcascade_mcs_nose.xml""\r\n\r\nif os.path.isfile(face_detector_path) != True:\r\n\traise ValueError(""Confirm that opencv is installed on your environment! Expected path "",detector_path,"" violated."")\r\n\r\nface_detector = cv2.CascadeClassifier(face_detector_path)\r\neye_detector = cv2.CascadeClassifier(eye_detector_path) \r\nnose_detector = cv2.CascadeClassifier(nose_detector_path) \r\n\r\n#------------------------\r\n\r\n#test_set = [""angelina.jpg"", ""angelina2.jpg"", ""angelina3.jpg""]\r\ntest_set = [""angelina.jpg""]\r\n\r\nfor instance in test_set:\r\n\talignedFace = alignFace(instance)\r\n\tplt.imshow(alignedFace[:, :, ::-1])\r\n\tplt.show()\r\n\t\r\n\timg, gray_img = detectFace(alignedFace)\r\n\tplt.imshow(img[:, :, ::-1])\r\n\tplt.show()\r\n'"
python/facenet-real-time.py,0,"b'#Face Recognition with Google\'s Facenet Model\r\n#Author Sefik Ilkin Serengil (sefiks.com)\r\n\r\n#You can find the documentation of this code from the following link: \r\n#https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/\r\n\r\n#Tested for TensorFlow 1.9.0, Keras 2.2.0 and Python 3.5.5\r\n\r\n#-----------------------\r\n\r\nimport numpy as np\r\nimport cv2\r\nfrom keras.models import Model, Sequential\r\nfrom keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\r\nfrom PIL import Image\r\nfrom keras.preprocessing.image import load_img, save_img, img_to_array\r\nfrom keras.applications.imagenet_utils import preprocess_input\r\nfrom keras.preprocessing import image\r\nimport matplotlib.pyplot as plt\r\nfrom keras.models import model_from_json\r\nfrom os import listdir\r\n\r\n#-----------------------\r\n\r\nface_cascade = cv2.CascadeClassifier(\'haarcascade_frontalface_default.xml\')\r\n\r\ndef preprocess_image(image_path):\r\n    img = load_img(image_path, target_size=(160, 160))\r\n    img = img_to_array(img)\r\n    img = np.expand_dims(img, axis=0)\r\n    \r\n    #preprocess_input normalizes input in scale of [-1, +1]. You must apply same normalization in prediction.\r\n    #Ref: https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py (Line 45)\r\n    img = preprocess_input(img)\r\n    return img\r\n\r\n#------------------------\r\n\r\n#https://github.com/serengil/tensorflow-101/blob/master/model/facenet_model.json\r\nmodel = model_from_json(open(""facenet_model.json"", ""r"").read())\r\nprint(""model built"")\r\n\r\n#https://drive.google.com/file/d/1971Xk5RwedbudGgTIrGAL4F7Aifu7id1/view?usp=sharing\r\nmodel.load_weights(\'weights/facenet_weights.h5\')\r\nprint(""weights loaded"")\r\n\r\n#------------------------\r\n\r\ndef findEuclideanDistance(source_representation, test_representation):\r\n    euclidean_distance = source_representation - test_representation\r\n    euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\r\n    euclidean_distance = np.sqrt(euclidean_distance)\r\n    return euclidean_distance\r\n\r\n#------------------------\r\n\r\nthreshold = 21 #tuned threshold for l2 disabled euclidean distance\r\n\r\n#------------------------\t\r\n\r\n#put your employee pictures in this path as name_of_employee.jpg\r\nemployee_pictures = ""database/""\r\n\r\nemployees = dict()\r\n\r\nfor file in listdir(employee_pictures):\r\n\temployee, extension = file.split(""."")\r\n\timg = preprocess_image(\'database/%s.jpg\' % (employee))\r\n\trepresentation = model.predict(img)[0,:]\r\n\t\r\n\temployees[employee] = representation\r\n\t\r\nprint(""employee representations retrieved successfully"")\r\n\r\n#------------------------\r\n\r\ncap = cv2.VideoCapture(0) #webcam\r\n\r\nwhile(True):\r\n\tret, img = cap.read()\r\n\tfaces = face_cascade.detectMultiScale(img, 1.3, 5)\r\n\t\r\n\tfor (x,y,w,h) in faces:\r\n\t\tif w > 130: #discard small detected faces\r\n\t\t\tcv2.rectangle(img, (x,y), (x+w,y+h), (67, 67, 67), 1) #draw rectangle to main image\r\n\t\t\t\r\n\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\r\n\t\t\tdetected_face = cv2.resize(detected_face, (160, 160)) #resize to 224x224\r\n\t\t\t\r\n\t\t\timg_pixels = image.img_to_array(detected_face)\r\n\t\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\r\n\t\t\t#employee dictionary is using preprocess_image and it normalizes in scale of [-1, +1]\r\n\t\t\timg_pixels /= 127.5\r\n\t\t\timg_pixels -= 1\r\n\t\t\t\r\n\t\t\tcaptured_representation = model.predict(img_pixels)[0,:]\r\n\t\t\t\r\n\t\t\tdistances = []\r\n\t\t\t\r\n\t\t\tfor i in employees:\r\n\t\t\t\temployee_name = i\r\n\t\t\t\tsource_representation = employees[i]\r\n\t\t\t\t\r\n\t\t\t\tdistance = findEuclideanDistance(captured_representation, source_representation)\r\n\t\t\t\t\r\n\t\t\t\t#print(employee_name,"": "",distance)\r\n\t\t\t\tdistances.append(distance)\r\n\t\t\t\r\n\t\t\tlabel_name = \'unknown\'\r\n\t\t\tindex = 0\r\n\t\t\tfor i in employees:\r\n\t\t\t\temployee_name = i\r\n\t\t\t\tif index == np.argmin(distances):\r\n\t\t\t\t\tif distances[index] <= threshold:\r\n\t\t\t\t\t\t#print(""detected: "",employee_name)\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t#label_name = ""%s (distance: %s)"" % (employee_name, str(round(distance,2)))\r\n\t\t\t\t\t\tsimilarity = 100 + (20 - distance)\r\n\t\t\t\t\t\tif similarity > 99.99: similarity = 99.99\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\tlabel_name = ""%s (%s%s)"" % (employee_name, str(round(similarity,2)), \'%\')\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\tbreak\r\n\t\t\t\t\t\r\n\t\t\t\tindex = index + 1\r\n\t\t\t\r\n\t\t\tcv2.putText(img, label_name, (int(x+w+15), int(y-64)), cv2.FONT_HERSHEY_SIMPLEX, 1, (67,67,67), 2)\r\n\t\t\t\t\t\r\n\t\t\t#connect face and text\r\n\t\t\tcv2.line(img,(x+w, y-64),(x+w-25, y-64),(67,67,67),1)\r\n\t\t\tcv2.line(img,(int(x+w/2),y),(x+w-25,y-64),(67,67,67),1)\r\n\t\t\t\r\n\tcv2.imshow(\'img\',img)\r\n\t\r\n\tif cv2.waitKey(1) & 0xFF == ord(\'q\'): #press q to quit\r\n\t\tbreak\r\n\t\r\n#kill open cv things\t\t\r\ncap.release()\r\ncv2.destroyAllWindows()\r\n'"
python/facial-expression-recognition-from-stream.py,0,"b'import numpy as np\r\nimport cv2\r\nfrom keras.preprocessing import image\r\n\r\n#-----------------------------\r\n#opencv initialization\r\n\r\nface_cascade = cv2.CascadeClassifier(\'C:/ProgramData/Anaconda3/envs/tensorflow/Library/etc/haarcascades/haarcascade_frontalface_default.xml\')\r\n\r\ncap = cv2.VideoCapture(0)\r\n#-----------------------------\r\n#face expression recognizer initialization\r\nfrom keras.models import model_from_json\r\nmodel = model_from_json(open(""facial_expression_model_structure.json"", ""r"").read())\r\nmodel.load_weights(\'facial_expression_model_weights.h5\') #load weights\r\n\r\n#-----------------------------\r\n\r\nemotions = (\'angry\', \'disgust\', \'fear\', \'happy\', \'sad\', \'surprise\', \'neutral\')\r\n\r\nwhile(True):\r\n\tret, img = cap.read()\r\n\t#img = cv2.imread(\'C:/Users/IS96273/Desktop/hababam.jpg\')\r\n\r\n\tgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n\r\n\tfaces = face_cascade.detectMultiScale(gray, 1.3, 5)\r\n\r\n\t#print(faces) #locations of detected faces\r\n\r\n\tfor (x,y,w,h) in faces:\r\n\t\tcv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) #draw rectangle to main image\r\n\t\t\r\n\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\r\n\t\tdetected_face = cv2.cvtColor(detected_face, cv2.COLOR_BGR2GRAY) #transform to gray scale\r\n\t\tdetected_face = cv2.resize(detected_face, (48, 48)) #resize to 48x48\r\n\t\t\r\n\t\timg_pixels = image.img_to_array(detected_face)\r\n\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\r\n\t\t\r\n\t\timg_pixels /= 255 #pixels are in scale of [0, 255]. normalize all pixels in scale of [0, 1]\r\n\t\t\r\n\t\tpredictions = model.predict(img_pixels) #store probabilities of 7 expressions\r\n\t\t\r\n\t\t#find max indexed array 0: angry, 1:disgust, 2:fear, 3:happy, 4:sad, 5:surprise, 6:neutral\r\n\t\tmax_index = np.argmax(predictions[0])\r\n\t\t\r\n\t\temotion = emotions[max_index]\r\n\t\t\r\n\t\t#write emotion text above rectangle\r\n\t\tcv2.putText(img, emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\r\n\t\t\r\n\t\t#process on detected face end\r\n\t\t#-------------------------\r\n\r\n\tcv2.imshow(\'img\',img)\r\n\r\n\tif cv2.waitKey(1) & 0xFF == ord(\'q\'): #press q to quit\r\n\t\tbreak\r\n\r\n#kill open cv things\t\t\r\ncap.release()\r\ncv2.destroyAllWindows()'"
python/facial-expression-recognition.py,2,"b'import tensorflow as tf\r\n\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\r\nfrom keras.layers import Dense, Activation, Dropout, Flatten\r\n\r\nfrom keras.preprocessing import image\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\n#------------------------------\r\n#cpu - gpu configuration\r\nconfig = tf.ConfigProto( device_count = {\'GPU\': 0 , \'CPU\': 56} ) #max: 1 gpu, 56 cpu\r\nsess = tf.Session(config=config) \r\nkeras.backend.set_session(sess)\r\n#------------------------------\r\n#variables\r\nnum_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\r\nbatch_size = 256\r\nepochs = 5\r\n#------------------------------\r\n#read kaggle facial expression recognition challenge dataset (fer2013.csv)\r\n#https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge\r\n\r\nwith open(""/data/fer2013/fer2013.csv"") as f:\r\n    content = f.readlines()\r\n\r\nlines = np.array(content)\r\n\r\nnum_of_instances = lines.size\r\nprint(""number of instances: "",num_of_instances)\r\nprint(""instance length: "",len(lines[1].split("","")[1].split("" "")))\r\n\r\n#------------------------------\r\n#initialize trainset and test set\r\nx_train, y_train, x_test, y_test = [], [], [], []\r\n\r\n#------------------------------\r\n#transfer train and test set data\r\nfor i in range(1,num_of_instances):\r\n    try:\r\n        emotion, img, usage = lines[i].split("","")\r\n          \r\n        val = img.split("" "")\r\n            \r\n        pixels = np.array(val, \'float32\')\r\n        \r\n        emotion = keras.utils.to_categorical(emotion, num_classes)\r\n    \r\n        if \'Training\' in usage:\r\n            y_train.append(emotion)\r\n            x_train.append(pixels)\r\n        elif \'PublicTest\' in usage:\r\n            y_test.append(emotion)\r\n            x_test.append(pixels)\r\n    except:\r\n\tprint("""",end="""")\r\n\r\n#------------------------------\r\n#data transformation for train and test sets\r\nx_train = np.array(x_train, \'float32\')\r\ny_train = np.array(y_train, \'float32\')\r\nx_test = np.array(x_test, \'float32\')\r\ny_test = np.array(y_test, \'float32\')\r\n\r\nx_train /= 255 #normalize inputs between [0, 1]\r\nx_test /= 255\r\n\r\nx_train = x_train.reshape(x_train.shape[0], 48, 48, 1)\r\nx_train = x_train.astype(\'float32\')\r\nx_test = x_test.reshape(x_test.shape[0], 48, 48, 1)\r\nx_test = x_test.astype(\'float32\')\r\n\r\nprint(x_train.shape[0], \'train samples\')\r\nprint(x_test.shape[0], \'test samples\')\r\n#------------------------------\r\n#construct CNN structure\r\nmodel = Sequential()\r\n\r\n#1st convolution layer\r\nmodel.add(Conv2D(64, (5, 5), activation=\'relu\', input_shape=(48,48,1)))\r\nmodel.add(MaxPooling2D(pool_size=(5,5), strides=(2, 2)))\r\n\r\n#2nd convolution layer\r\nmodel.add(Conv2D(64, (3, 3), activation=\'relu\'))\r\nmodel.add(Conv2D(64, (3, 3), activation=\'relu\'))\r\nmodel.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\r\n\r\n#3rd convolution layer\r\nmodel.add(Conv2D(128, (3, 3), activation=\'relu\'))\r\nmodel.add(Conv2D(128, (3, 3), activation=\'relu\'))\r\nmodel.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\r\n\r\nmodel.add(Flatten())\r\n\r\n#fully connected neural networks\r\nmodel.add(Dense(1024, activation=\'relu\'))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(1024, activation=\'relu\'))\r\nmodel.add(Dropout(0.2))\r\n\r\nmodel.add(Dense(num_classes, activation=\'softmax\'))\r\n#------------------------------\r\n#batch process\r\ngen = ImageDataGenerator()\r\ntrain_generator = gen.flow(x_train, y_train, batch_size=batch_size)\r\n\r\n#------------------------------\r\n\r\nmodel.compile(loss=\'categorical_crossentropy\'\r\n    , optimizer=keras.optimizers.Adam()\r\n    , metrics=[\'accuracy\']\r\n)\r\n\r\n#------------------------------\r\n\r\nfit = True\r\n\r\nif fit == True:\r\n\t#model.fit_generator(x_train, y_train, epochs=epochs) #train for all trainset\r\n\tmodel.fit_generator(train_generator, steps_per_epoch=batch_size, epochs=epochs) #train for randomly selected one\r\nelse:\r\n\tmodel.load_weights(\'/data/facial_expression_model_weights.h5\') #load weights\r\n\t\r\n#------------------------------\r\n""""""\r\n#overall evaluation\r\nscore = model.evaluate(x_test, y_test)\r\nprint(\'Test loss:\', score[0])\r\nprint(\'Test accuracy:\', 100*score[1])\r\n""""""\r\n#------------------------------\r\n#function for drawing bar chart for emotion preditions\r\ndef emotion_analysis(emotions):\r\n    objects = (\'angry\', \'disgust\', \'fear\', \'happy\', \'sad\', \'surprise\', \'neutral\')\r\n    y_pos = np.arange(len(objects))\r\n    \r\n    plt.bar(y_pos, emotions, align=\'center\', alpha=0.5)\r\n    plt.xticks(y_pos, objects)\r\n    plt.ylabel(\'percentage\')\r\n    plt.title(\'emotion\')\r\n    \r\n    plt.show()\r\n#------------------------------\r\n\r\nmonitor_testset_results = False\r\n\r\nif monitor_testset_results == True:\r\n\t#make predictions for test set\r\n\tpredictions = model.predict(x_test)\r\n\r\n\tindex = 0\r\n\tfor i in predictions:\r\n\t\tif index < 30 and index >= 20:\r\n\t\t\t#print(i) #predicted scores\r\n\t\t\t#print(y_test[index]) #actual scores\r\n\t\t\t\r\n\t\t\ttesting_img = np.array(x_test[index], \'float32\')\r\n\t\t\ttesting_img = testing_img.reshape([48, 48]);\r\n\t\t\t\r\n\t\t\tplt.gray()\r\n\t\t\tplt.imshow(testing_img)\r\n\t\t\tplt.show()\r\n\t\t\t\r\n\t\t\tprint(i)\r\n\t\t\t\r\n\t\t\temotion_analysis(i)\r\n\t\t\tprint(""----------------------------------------------"")\r\n\t\tindex = index + 1\r\n\r\n#------------------------------\r\n#make prediction for custom image out of test set\r\n\r\nimg = image.load_img(""C:/Users/IS96273/Desktop/jackman.png"", grayscale=True, target_size=(48, 48))\r\n\r\nx = image.img_to_array(img)\r\nx = np.expand_dims(x, axis = 0)\r\n\r\nx /= 255\r\n\r\ncustom = model.predict(x)\r\nemotion_analysis(custom[0])\r\n\r\nx = np.array(x, \'float32\')\r\nx = x.reshape([48, 48]);\r\n\r\nplt.gray()\r\nplt.imshow(x)\r\nplt.show()\r\n#------------------------------\r\n\r\n'"
python/fb-deepface-real-time.py,0,"b'#Face Recognition with Facebook DeepFace Model\r\n#Author Sefik Ilkin Serengil (sefiks.com)\r\n\r\n#-----------------------\r\n\r\nimport os\r\nfrom os import listdir\r\nimport numpy as np\r\nimport cv2\r\nfrom keras.models import Model, Sequential\r\nfrom keras.layers import Input, Convolution2D, LocallyConnected2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\r\nfrom PIL import Image\r\nfrom keras.preprocessing.image import load_img, save_img, img_to_array\r\nfrom keras.applications.imagenet_utils import preprocess_input\r\nfrom keras.preprocessing import image\r\nimport matplotlib.pyplot as plt\r\nfrom keras.models import model_from_json\r\n\r\n#-----------------------\r\n\r\ntarget_size = (152, 152)\r\n\r\n#-----------------------\r\n\r\n#OpenCV haarcascade module\r\n\r\nopencv_home = cv2.__file__\r\nfolders = opencv_home.split(os.path.sep)[0:-1]\r\npath = folders[0]\r\nfor folder in folders[1:]:\r\n\tpath = path + ""/"" + folder\r\n\r\ndetector_path = path+""/data/haarcascade_frontalface_default.xml""\r\n\r\nif os.path.isfile(detector_path) != True:\r\n\traise ValueError(""Confirm that opencv is installed on your environment! Expected path "",detector_path,"" violated."")\r\nelse:\r\n\tface_cascade = cv2.CascadeClassifier(detector_path)\r\n\r\n#-------------------------\r\ndef detectFace(img_path, target_size=(152, 152)):\r\n\t\r\n\timg = cv2.imread(img_path)\r\n\t\r\n\tfaces = face_cascade.detectMultiScale(img, 1.3, 5)\r\n\t\r\n\tif len(faces) > 0:\r\n\t\tx,y,w,h = faces[0]\r\n\t\t\r\n\t\tmargin = 0\r\n\t\tx_margin = w * margin / 100\r\n\t\ty_margin = h * margin / 100\r\n\t\t\r\n\t\tif y - y_margin > 0 and y+h+y_margin < img.shape[1] and x-x_margin > 0 and x+w+x_margin < img.shape[0]:\r\n\t\t\tdetected_face = img[int(y-y_margin):int(y+h+y_margin), int(x-x_margin):int(x+w+x_margin)]\r\n\t\telse:\r\n\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)]\r\n\t\t\r\n\t\tdetected_face = cv2.resize(detected_face, target_size)\r\n\t\t\r\n\t\timg_pixels = image.img_to_array(detected_face)\r\n\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\r\n\t\t\r\n\t\t#normalize in [0, 1]\r\n\t\timg_pixels /= 255 \r\n\t\t\r\n\t\treturn img_pixels\r\n\telse:\r\n\t\traise ValueError(""Face could not be detected in "", img_path,"". Please confirm that the picture is a face photo."")\r\n\r\n#-------------------------\r\n\r\n#DeepFace model\r\nbase_model = Sequential()\r\nbase_model.add(Convolution2D(32, (11, 11), activation=\'relu\', name=\'C1\', input_shape=(152, 152, 3)))\r\nbase_model.add(MaxPooling2D(pool_size=3, strides=2, padding=\'same\', name=\'M2\'))\r\nbase_model.add(Convolution2D(16, (9, 9), activation=\'relu\', name=\'C3\'))\r\nbase_model.add(LocallyConnected2D(16, (9, 9), activation=\'relu\', name=\'L4\'))\r\nbase_model.add(LocallyConnected2D(16, (7, 7), strides=2, activation=\'relu\', name=\'L5\') )\r\nbase_model.add(LocallyConnected2D(16, (5, 5), activation=\'relu\', name=\'L6\'))\r\nbase_model.add(Flatten(name=\'F0\'))\r\nbase_model.add(Dense(4096, activation=\'relu\', name=\'F7\'))\r\nbase_model.add(Dropout(rate=0.5, name=\'D0\'))\r\nbase_model.add(Dense(8631, activation=\'softmax\', name=\'F8\'))\r\n\r\nbase_model.load_weights(""weights/VGGFace2_DeepFace_weights_val-0.9034.h5"")\r\n\r\n#Drop F8 and D0 layers. F7 is the representation layer.\r\nmodel = Model(inputs=base_model.layers[0].input, outputs=base_model.layers[-3].output)\r\n\r\n#------------------------\r\ndef l2_normalize(x):\r\n\treturn x / np.sqrt(np.sum(np.multiply(x, x)))\r\n\r\ndef findEuclideanDistance(source_representation, test_representation):\r\n\teuclidean_distance = source_representation - test_representation\r\n\teuclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\r\n\teuclidean_distance = np.sqrt(euclidean_distance)\r\n\treturn euclidean_distance\r\n\r\n#------------------------\t\r\n\r\n#put your employee pictures in this path as name_of_employee.jpg\r\nemployee_pictures = ""database/""\r\n\r\nemployees = dict()\r\n\r\nfor file in listdir(employee_pictures):\r\n\temployee, extension = file.split(""."")\r\n\timg_path = \'database/%s.jpg\' % (employee)\r\n\timg = detectFace(img_path)\r\n\t\r\n\trepresentation = model.predict(img)[0]\r\n\t\r\n\temployees[employee] = representation\r\n\t\r\nprint(""employee representations retrieved successfully"")\r\n\r\n#------------------------\r\n\r\ncap = cv2.VideoCapture(0) #webcam\r\n\r\nwhile(True):\r\n\tret, img = cap.read()\r\n\tfaces = face_cascade.detectMultiScale(img, 1.3, 5)\r\n\t\r\n\tfor (x,y,w,h) in faces:\r\n\t\tif w > 130: #discard small detected faces\r\n\t\t\tcv2.rectangle(img, (x,y), (x+w,y+h), (67, 67, 67), 1) #draw rectangle to main image\r\n\t\t\t\r\n\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\r\n\t\t\tdetected_face = cv2.resize(detected_face, target_size) #resize to 152x152\r\n\t\t\t\r\n\t\t\timg_pixels = image.img_to_array(detected_face)\r\n\t\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\r\n\t\t\timg_pixels /= 255\r\n\t\t\t\r\n\t\t\tcaptured_representation = model.predict(img_pixels)[0]\r\n\t\t\t\r\n\t\t\tdistances = []\r\n\t\t\t\r\n\t\t\tfor i in employees:\r\n\t\t\t\temployee_name = i\r\n\t\t\t\tsource_representation = employees[i]\r\n\t\t\t\t\r\n\t\t\t\tdistance = findEuclideanDistance(l2_normalize(captured_representation), l2_normalize(source_representation))\r\n\t\t\t\tdistances.append(distance)\r\n\t\t\t\r\n\t\t\tis_found = False; index = 0\r\n\t\t\tfor i in employees:\r\n\t\t\t\temployee_name = i\r\n\t\t\t\tif index == np.argmin(distances):\r\n\t\t\t\t\tif distances[index] <= 0.70:\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\tprint(""detected: "",employee_name, ""("",distances[index],"")"")\r\n\t\t\t\t\t\temployee_name = employee_name.replace(""_"", """")\r\n\t\t\t\t\t\tsimilarity = distances[index]\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\tis_found = True\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\tbreak\r\n\t\t\t\t\t\r\n\t\t\t\tindex = index + 1\r\n\t\t\t\r\n\t\t\tif is_found:\r\n\t\t\t\tdisplay_img = cv2.imread(""database/%s.jpg"" % employee_name)\r\n\t\t\t\tpivot_img_size = 112\r\n\t\t\t\tdisplay_img = cv2.resize(display_img, (pivot_img_size, pivot_img_size))\r\n\t\t\t\t\t\t\t\t\r\n\t\t\t\ttry:\r\n\t\t\t\t\tresolution_x = img.shape[1]; resolution_y = img.shape[0]\r\n\t\t\t\t\t\r\n\t\t\t\t\tlabel = employee_name+"" (""+""{0:.2f}"".format(similarity)+"")""\r\n\t\t\t\t\t\r\n\t\t\t\t\tif y - pivot_img_size > 0 and x + w + pivot_img_size < resolution_x:\r\n\t\t\t\t\t\t#top right\r\n\t\t\t\t\t\timg[y - pivot_img_size:y, x+w:x+w+pivot_img_size] = display_img\r\n\t\t\t\t\t\tcv2.putText(img, label, (x+w, y+10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (67,67,67), 1)\t\t\t\t\t\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t#connect face and text\r\n\t\t\t\t\t\tcv2.line(img,(x+int(w/2), y), (x+3*int(w/4), y-int(pivot_img_size/2)),(67,67,67),1)\r\n\t\t\t\t\t\tcv2.line(img, (x+3*int(w/4), y-int(pivot_img_size/2)), (x+w, y - int(pivot_img_size/2)), (67,67,67),1)\r\n\t\t\t\t\telif y + h + pivot_img_size < resolution_y and x - pivot_img_size > 0:\r\n\t\t\t\t\t\t#bottom left\r\n\t\t\t\t\t\timg[y+h:y+h+pivot_img_size, x-pivot_img_size:x] = display_img\r\n\t\t\t\t\t\tcv2.putText(img, label, (x - pivot_img_size, y+h-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (67,67,67), 1)\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t#connect face and text\r\n\t\t\t\t\t\tcv2.line(img,(x+int(w/2), y+h), (x+int(w/2)-int(w/4), y+h+int(pivot_img_size/2)),(67,67,67),1)\r\n\t\t\t\t\t\tcv2.line(img, (x+int(w/2)-int(w/4), y+h+int(pivot_img_size/2)), (x, y+h+int(pivot_img_size/2)), (67,67,67),1)\r\n\t\t\t\t\t\t\r\n\t\t\t\t\telif y - pivot_img_size > 0 and x - pivot_img_size > 0:\r\n\t\t\t\t\t\t#top left\r\n\t\t\t\t\t\timg[y-pivot_img_size:y, x-pivot_img_size:x] = display_img\r\n\t\t\t\t\t\tcv2.putText(img, label, (x - pivot_img_size, y+10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (67,67,67), 1)\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t#connect face and text\r\n\t\t\t\t\t\tcv2.line(img,(x+int(w/2), y), (x+int(w/2)-int(w/4), y-int(pivot_img_size/2)),(67,67,67),1)\r\n\t\t\t\t\t\tcv2.line(img, (x+int(w/2)-int(w/4), y-int(pivot_img_size/2)), (x, y - int(pivot_img_size/2)), (67,67,67),1)\r\n\t\t\t\t\t\t\r\n\t\t\t\t\telif x+w+pivot_img_size < resolution_x and y + h + pivot_img_size < resolution_y:\r\n\t\t\t\t\t\t#bottom righ\r\n\t\t\t\t\t\timg[y+h:y+h+pivot_img_size, x+w:x+w+pivot_img_size] = display_img\r\n\t\t\t\t\t\tcv2.putText(img, label, (x+w, y+h-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (67,67,67), 1)\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t#connect face and text\r\n\t\t\t\t\t\tcv2.line(img,(x+int(w/2), y+h), (x+int(w/2)+int(w/4), y+h+int(pivot_img_size/2)),(67,67,67),1)\r\n\t\t\t\t\t\tcv2.line(img, (x+int(w/2)+int(w/4), y+h+int(pivot_img_size/2)), (x+w, y+h+int(pivot_img_size/2)), (67,67,67),1)\r\n\t\t\t\t\t\r\n\t\t\t\texcept Exception as e:\r\n\t\t\t\t\tprint(""exception occured: "", str(e))\r\n\t\t\t\r\n\tcv2.imshow(\'img\',img)\r\n\t\r\n\tif cv2.waitKey(1) & 0xFF == ord(\'q\'): #press q to quit\r\n\t\tbreak\r\n\t\r\n#kill open cv things\t\t\r\ncap.release()\r\ncv2.destroyAllWindows()\r\n'"
python/gpu-enabled-multiprocessing.py,2,"b'import pandas as pd\r\nimport multiprocessing\r\nfrom multiprocessing import Pool\r\n\r\ndef train(index, df):\r\n\timport tensorflow as tf\r\n\timport keras\r\n\tfrom keras.models import Sequential\r\n\t\r\n\t#------------------------------\r\n\t#this block enables GPU enabled multiprocessing\r\n\tcore_config = tf.ConfigProto()\r\n\tcore_config.gpu_options.allow_growth = True\r\n\tsession = tf.Session(config=core_config)\r\n\tkeras.backend.set_session(session)\r\n\t#------------------------------\r\n\t#prepare input and output values\r\n\tdf = df.drop(columns=[\'index\'])\r\n\t\r\n\tdata = df.drop(columns=[\'target\']).values\r\n\ttarget = df[\'target\']\r\n\t#------------------------------\r\n\tmodel = Sequential()\r\n\tmodel.add(Dense(5 #num of hidden units\r\n\t, input_shape=(data.shape[1],))) #num of features in input layer\r\n\tmodel.add(Activation(\'sigmoid\'))\r\n\t\r\n\tmodel.add(Dense(1))#number of nodes in output layer\r\n\tmodel.add(Activation(\'sigmoid\'))\r\n\t\r\n\tmodel.compile(loss=\'mse\', optimizer=keras.optimizers.Adam())\r\n\t#------------------------------\r\n\tmodel.fit(data, target, epochs = 5000, verbose = 1)\r\n\tmodel.save(""model_for_%s.hdf5"" % index)\r\n\t#------------------------------\r\n\t#finally, close sessions\r\n\tsession.close()\r\n\tkeras.backend.clear_session() \r\n\r\n#-----------------------------\r\n#main program\r\n\r\nmultiprocessing.set_start_method(\'spawn\', force=True)\r\n\r\ndf = pd.read_csv(""dataset.csv"")\r\n\r\nmy_tuple = [(i, df[df[\'index\'] == i]) for i in range(0, 20)]\r\n\r\nwith Pool(10) as pool: \r\n\tpool.starmap(train, my_tuple)\r\n'"
python/gradient-vanishing.py,8,"b'import tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport math\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n#-----------------------------------------------\r\n#variables\r\n\r\nepoch = 2000\r\nlearningRate = 0.1\r\nbatch_size = 120\r\n\r\nmnist_data = ""C:/tmp/MNIST_data""\r\n\r\ntrainForRandomSet = True\r\n\r\n#-----------------------------------------------\r\n#data process and transformation\r\n\r\nMNIST_DATASET = input_data.read_data_sets(mnist_data)\r\n\r\ntrain_data = np.array(MNIST_DATASET.train.images, \'float32\')\r\ntrain_target = np.array(MNIST_DATASET.train.labels, \'int64\')\r\nprint(""training set consists of "", len(MNIST_DATASET.train.images), "" instances"")\r\n\r\ntest_data = np.array(MNIST_DATASET.test.images, \'float32\')\r\ntest_target = np.array(MNIST_DATASET.test.labels, \'int64\')\r\nprint(""test set consists of "", len(MNIST_DATASET.test.images), "" instances"")\r\n\r\n#-----------------------------------------------\r\n#visualization\r\nprint(""input layer consists of "", len(MNIST_DATASET.train.images[1]),"" features"")\r\n\r\n#-----------------------------------------------\r\nfeature_columns = [tf.contrib.layers.real_valued_column("""", dimension=len(MNIST_DATASET.train.images[1]))]\r\n\r\nclassifier = tf.contrib.learn.DNNClassifier(\r\n\tfeature_columns=feature_columns\r\n\t, n_classes=10 #0 to 9 - 10 classes\r\n\t, hidden_units=[128, 64, 32, 16]  #4 hidden layers consisting of 128, 64, 32, 16 units respectively\r\n\t#, optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=learningRate)\r\n\t, optimizer=tf.train.GradientDescentOptimizer(learning_rate=learningRate)\r\n\t, activation_fn = tf.nn.sigmoid #activate this to see vanishing gradient\r\n\t#, activation_fn = tf.nn.relu #activate this to solve gradient vanishing problem\r\n)\r\n\r\n#----------------------------------------\r\n#training\r\n\r\nif trainForRandomSet == False:\r\n\t#train on all trainset\r\n\tclassifier.fit(train_data, train_target, steps=epoch)\r\nelse:\r\n\tdef generate_input_fn(data, label):\t\r\n\t\timage_batch, label_batch = tf.train.shuffle_batch(\r\n\t\t\t[data, label]\r\n\t\t\t, batch_size=batch_size\r\n\t\t\t, capacity=8*batch_size\r\n\t\t\t, min_after_dequeue=4*batch_size\r\n\t\t\t, enqueue_many=True\r\n\t\t)\r\n\t\treturn image_batch, label_batch\r\n\t\r\n\tdef input_fn_for_train():\r\n\t\treturn generate_input_fn(train_data, train_target)\r\n\t\r\n\t#train on small random selected dataset\r\n\tclassifier.fit(input_fn=input_fn_for_train, steps=epoch)\r\n\r\nprint(""\\n---training is over..."")\r\n\r\n#----------------------------------------\r\n#calculationg overall accuracy\r\n\r\naccuracy_score = classifier.evaluate(test_data, test_target, steps=epoch)[\'accuracy\']\r\nprint(""\\n---evaluation..."")\r\nprint(""accuracy: "", 100*accuracy_score,""%"")\r\n'"
python/iris.py,0,"b'import tensorflow as tf\r\nimport numpy as np\r\n\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.layers.core import Dense, Activation\r\nfrom keras.utils import np_utils\r\n#----------------------------\r\nattributes = np.genfromtxt(""iris-attr.data"", delimiter="","")\r\nlabel = np.genfromtxt(""iris-labels.data"", dtype=""int64"")\r\n\r\nnum_classes = 3\r\nlabel = keras.utils.to_categorical(label, num_classes)\r\n#----------------------------\r\n#creating model\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(4 #num of hidden units\r\n\t, input_shape=(len(attributes[0]),))) #num of features in input layer\r\nmodel.add(Activation(\'sigmoid\')) #activation function from input layer to 1st hidden layer\r\nmodel.add(Dense(len(label[0]))) #num of classes in output layer\r\nmodel.add(Activation(\'sigmoid\')) #activation function from 1st hidden layer to output layer\r\n\r\n#----------------------------\r\n#compile\r\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\')\r\n\r\n#training\r\nmodel.fit(attributes, label, epochs=1000, verbose=0)\r\n\r\npredictions = model.predict(attributes)\r\n\r\nindex = 0\r\nfor i in predictions:\r\n\t#print(np.argmax(i),"" ("",i,"")"")\r\n\t\r\n\tpred = np.argmax(i)\r\n\tactual = np.argmax(label[index])\r\n\tprint("" prediction: "",pred,"" - actual: "",actual)\r\n\tindex = index + 1'"
python/opencv-images-to-vide.py,0,"b'import cv2\r\nimport numpy as np\r\n \r\nframe_width = 913\r\nframe_height = 684\r\n \r\n# Define the codec and create VideoWriter object.The output is stored in \'outpy.avi\' file.\r\nout = cv2.VideoWriter(\'outpy.avi\',cv2.VideoWriter_fourcc(\'M\',\'J\',\'P\',\'G\'), 10, (frame_width,frame_height))\r\n\r\ns_img = cv2.imread(""style-small.jpg"")\r\nx_offset=600; y_offset=25\r\n\r\nterminate = False\r\n#while(True):\r\nfor i in range(999, 1500):\r\n  \r\n  #putting generated_*.png files in a directory    \r\n  file = ""C:/python/ortakoy/generated_%d.png"" % (i)\r\n  print(file)\r\n  frame = cv2.imread(file)\r\n  #cv2.imshow(\'frame\',frame)\r\n  \r\n  frame = cv2.resize(frame, (913, 684))\r\n\r\n  fps = 5\r\n    \r\n  epoch = i-999\r\n  label = \'Frame %d\' % (epoch+1)\r\n  \r\n  for j in range(fps):\r\n    \r\n    cv2.putText(frame, label, (int(50), int(630)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\r\n    \r\n    # Write the frame into the file \'output.avi\'\r\n    out.write(frame)\r\n  \r\n    # Display the resulting frame    \r\n    cv2.imshow(\'frame\',frame)\r\n  \r\n    # Press Q on keyboard to stop recording\r\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\r\n      terminate = True\r\n      break\r\n  \r\n  if terminate == True:\r\n    break\r\n    \r\n# When everything done, release the video capture and video write objects\r\ncap.release()\r\nout.release()\r\n \r\n# Closes all the frames\r\ncv2.destroyAllWindows() '"
python/openface-real-time.py,2,"b'#Face Recognition with OpenFace\r\n#Author Sefik Ilkin Serengil (sefiks.com)\r\n\r\n#You can find the documentation of this code from the following link: \r\n#https://sefiks.com/\r\n\r\n#tested for TensorFlow 1.9.0, Keras 2.2.0 and Python 3.5.5\r\n\r\n#-----------------------\r\nimport tensorflow as tf\r\nfrom keras.models import Model, Sequential\r\nfrom keras.layers import Input, Conv2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, concatenate\r\nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\r\nfrom keras.layers.core import Dense, Activation, Lambda, Flatten\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.preprocessing.image import load_img, save_img, img_to_array\r\nfrom keras.applications.imagenet_utils import preprocess_input\r\nfrom keras.preprocessing import image\r\nfrom keras.models import model_from_json\r\nfrom keras.layers.merge import Concatenate\r\nfrom keras import backend as K\r\n\r\nfrom os import listdir\r\nimport numpy as np\r\nimport cv2\r\nfrom PIL import Image\r\nimport matplotlib.pyplot as plt\r\n#-----------------------\r\n\r\ndump = False\r\n\r\ncolor = (67,67,67)\r\n\r\nface_cascade = cv2.CascadeClassifier(\'haarcascade_frontalface_default.xml\')\r\n\r\ndef preprocess_image(image_path):\r\n    img = load_img(image_path, target_size=(96, 96))\r\n    img = img_to_array(img)\r\n    img = np.expand_dims(img, axis=0)\r\n    \r\n    #preprocess_input normalizes input in scale of [-1, +1]. You must apply same normalization in prediction.\r\n    #Ref: https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py (Line 45)\r\n    img = preprocess_input(img)\r\n    return img\r\n\r\n#------------------------\r\n\r\ndef builtModel():\r\n\tmyInput = Input(shape=(96, 96, 3))\r\n\t\r\n\tx = ZeroPadding2D(padding=(3, 3), input_shape=(96, 96, 3))(myInput)\r\n\tx = Conv2D(64, (7, 7), strides=(2, 2), name=\'conv1\')(x)\r\n\tx = BatchNormalization(axis=3, epsilon=0.00001, name=\'bn1\')(x)\r\n\tx = Activation(\'relu\')(x)\r\n\tx = ZeroPadding2D(padding=(1, 1))(x)\r\n\tx = MaxPooling2D(pool_size=3, strides=2)(x)\r\n\tx = Lambda(lambda x: tf.nn.lrn(x, alpha=1e-4, beta=0.75), name=\'lrn_1\')(x)\r\n\tx = Conv2D(64, (1, 1), name=\'conv2\')(x)\r\n\tx = BatchNormalization(axis=3, epsilon=0.00001, name=\'bn2\')(x)\r\n\tx = Activation(\'relu\')(x)\r\n\tx = ZeroPadding2D(padding=(1, 1))(x)\r\n\tx = Conv2D(192, (3, 3), name=\'conv3\')(x)\r\n\tx = BatchNormalization(axis=3, epsilon=0.00001, name=\'bn3\')(x)\r\n\tx = Activation(\'relu\')(x)\r\n\tLambda(lambda x: tf.nn.lrn(x, alpha=1e-4, beta=0.75), name=\'lrn_2\')(x)\r\n\tx = ZeroPadding2D(padding=(1, 1))(x)\r\n\tx = MaxPooling2D(pool_size=3, strides=2)(x)\r\n\t\r\n\t# Inception3a\r\n\tinception_3a_3x3 = Conv2D(96, (1, 1), name=\'inception_3a_3x3_conv1\')(x)\r\n\tinception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3a_3x3_bn1\')(inception_3a_3x3)\r\n\tinception_3a_3x3 = Activation(\'relu\')(inception_3a_3x3)\r\n\tinception_3a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3)\r\n\tinception_3a_3x3 = Conv2D(128, (3, 3), name=\'inception_3a_3x3_conv2\')(inception_3a_3x3)\r\n\tinception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3a_3x3_bn2\')(inception_3a_3x3)\r\n\tinception_3a_3x3 = Activation(\'relu\')(inception_3a_3x3)\r\n\t\r\n\tinception_3a_5x5 = Conv2D(16, (1, 1), name=\'inception_3a_5x5_conv1\')(x)\r\n\tinception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3a_5x5_bn1\')(inception_3a_5x5)\r\n\tinception_3a_5x5 = Activation(\'relu\')(inception_3a_5x5)\r\n\tinception_3a_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5)\r\n\tinception_3a_5x5 = Conv2D(32, (5, 5), name=\'inception_3a_5x5_conv2\')(inception_3a_5x5)\r\n\tinception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3a_5x5_bn2\')(inception_3a_5x5)\r\n\tinception_3a_5x5 = Activation(\'relu\')(inception_3a_5x5)\r\n\t\r\n\tinception_3a_pool = MaxPooling2D(pool_size=3, strides=2)(x)\r\n\tinception_3a_pool = Conv2D(32, (1, 1), name=\'inception_3a_pool_conv\')(inception_3a_pool)\r\n\tinception_3a_pool = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3a_pool_bn\')(inception_3a_pool)\r\n\tinception_3a_pool = Activation(\'relu\')(inception_3a_pool)\r\n\tinception_3a_pool = ZeroPadding2D(padding=((3, 4), (3, 4)))(inception_3a_pool)\r\n\t\r\n\tinception_3a_1x1 = Conv2D(64, (1, 1), name=\'inception_3a_1x1_conv\')(x)\r\n\tinception_3a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3a_1x1_bn\')(inception_3a_1x1)\r\n\tinception_3a_1x1 = Activation(\'relu\')(inception_3a_1x1)\r\n\t\r\n\tinception_3a = concatenate([inception_3a_3x3, inception_3a_5x5, inception_3a_pool, inception_3a_1x1], axis=3)\r\n\t\r\n\t# Inception3b\r\n\tinception_3b_3x3 = Conv2D(96, (1, 1), name=\'inception_3b_3x3_conv1\')(inception_3a)\r\n\tinception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3b_3x3_bn1\')(inception_3b_3x3)\r\n\tinception_3b_3x3 = Activation(\'relu\')(inception_3b_3x3)\r\n\tinception_3b_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3)\r\n\tinception_3b_3x3 = Conv2D(128, (3, 3), name=\'inception_3b_3x3_conv2\')(inception_3b_3x3)\r\n\tinception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3b_3x3_bn2\')(inception_3b_3x3)\r\n\tinception_3b_3x3 = Activation(\'relu\')(inception_3b_3x3)\r\n\t\r\n\tinception_3b_5x5 = Conv2D(32, (1, 1), name=\'inception_3b_5x5_conv1\')(inception_3a)\r\n\tinception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3b_5x5_bn1\')(inception_3b_5x5)\r\n\tinception_3b_5x5 = Activation(\'relu\')(inception_3b_5x5)\r\n\tinception_3b_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5)\r\n\tinception_3b_5x5 = Conv2D(64, (5, 5), name=\'inception_3b_5x5_conv2\')(inception_3b_5x5)\r\n\tinception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3b_5x5_bn2\')(inception_3b_5x5)\r\n\tinception_3b_5x5 = Activation(\'relu\')(inception_3b_5x5)\r\n\t\r\n\tinception_3b_pool = Lambda(lambda x: x**2, name=\'power2_3b\')(inception_3a)\r\n\tinception_3b_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_3b_pool)\r\n\tinception_3b_pool = Lambda(lambda x: x*9, name=\'mult9_3b\')(inception_3b_pool)\r\n\tinception_3b_pool = Lambda(lambda x: K.sqrt(x), name=\'sqrt_3b\')(inception_3b_pool)\r\n\tinception_3b_pool = Conv2D(64, (1, 1), name=\'inception_3b_pool_conv\')(inception_3b_pool)\r\n\tinception_3b_pool = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3b_pool_bn\')(inception_3b_pool)\r\n\tinception_3b_pool = Activation(\'relu\')(inception_3b_pool)\r\n\tinception_3b_pool = ZeroPadding2D(padding=(4, 4))(inception_3b_pool)\r\n\t\r\n\tinception_3b_1x1 = Conv2D(64, (1, 1), name=\'inception_3b_1x1_conv\')(inception_3a)\r\n\tinception_3b_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3b_1x1_bn\')(inception_3b_1x1)\r\n\tinception_3b_1x1 = Activation(\'relu\')(inception_3b_1x1)\r\n\t\r\n\tinception_3b = concatenate([inception_3b_3x3, inception_3b_5x5, inception_3b_pool, inception_3b_1x1], axis=3)\r\n\t\r\n\t# Inception3c\r\n\tinception_3c_3x3 = Conv2D(128, (1, 1), strides=(1, 1), name=\'inception_3c_3x3_conv1\')(inception_3b)\r\n\tinception_3c_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3c_3x3_bn1\')(inception_3c_3x3)\r\n\tinception_3c_3x3 = Activation(\'relu\')(inception_3c_3x3)\r\n\tinception_3c_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3c_3x3)\r\n\tinception_3c_3x3 = Conv2D(256, (3, 3), strides=(2, 2), name=\'inception_3c_3x3_conv\'+\'2\')(inception_3c_3x3)\r\n\tinception_3c_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3c_3x3_bn\'+\'2\')(inception_3c_3x3)\r\n\tinception_3c_3x3 = Activation(\'relu\')(inception_3c_3x3)\r\n\t\r\n\tinception_3c_5x5 = Conv2D(32, (1, 1), strides=(1, 1), name=\'inception_3c_5x5_conv1\')(inception_3b)\r\n\tinception_3c_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3c_5x5_bn1\')(inception_3c_5x5)\r\n\tinception_3c_5x5 = Activation(\'relu\')(inception_3c_5x5)\r\n\tinception_3c_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3c_5x5)\r\n\tinception_3c_5x5 = Conv2D(64, (5, 5), strides=(2, 2), name=\'inception_3c_5x5_conv\'+\'2\')(inception_3c_5x5)\r\n\tinception_3c_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_3c_5x5_bn\'+\'2\')(inception_3c_5x5)\r\n\tinception_3c_5x5 = Activation(\'relu\')(inception_3c_5x5)\r\n\t\r\n\tinception_3c_pool = MaxPooling2D(pool_size=3, strides=2)(inception_3b)\r\n\tinception_3c_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_3c_pool)\r\n\t\r\n\tinception_3c = concatenate([inception_3c_3x3, inception_3c_5x5, inception_3c_pool], axis=3)\r\n\t\r\n\t#inception 4a\r\n\tinception_4a_3x3 = Conv2D(96, (1, 1), strides=(1, 1), name=\'inception_4a_3x3_conv\'+\'1\')(inception_3c)\r\n\tinception_4a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4a_3x3_bn\'+\'1\')(inception_4a_3x3)\r\n\tinception_4a_3x3 = Activation(\'relu\')(inception_4a_3x3)\r\n\tinception_4a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_4a_3x3)\r\n\tinception_4a_3x3 = Conv2D(192, (3, 3), strides=(1, 1), name=\'inception_4a_3x3_conv\'+\'2\')(inception_4a_3x3)\r\n\tinception_4a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4a_3x3_bn\'+\'2\')(inception_4a_3x3)\r\n\tinception_4a_3x3 = Activation(\'relu\')(inception_4a_3x3)\r\n\t\r\n\tinception_4a_5x5 = Conv2D(32, (1,1), strides=(1,1), name=\'inception_4a_5x5_conv1\')(inception_3c)\r\n\tinception_4a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4a_5x5_bn1\')(inception_4a_5x5)\r\n\tinception_4a_5x5 = Activation(\'relu\')(inception_4a_5x5)\r\n\tinception_4a_5x5 = ZeroPadding2D(padding=(2,2))(inception_4a_5x5)\r\n\tinception_4a_5x5 = Conv2D(64, (5,5), strides=(1,1), name=\'inception_4a_5x5_conv\'+\'2\')(inception_4a_5x5)\r\n\tinception_4a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4a_5x5_bn\'+\'2\')(inception_4a_5x5)\r\n\tinception_4a_5x5 = Activation(\'relu\')(inception_4a_5x5)\r\n\t\r\n\tinception_4a_pool = Lambda(lambda x: x**2, name=\'power2_4a\')(inception_3c)\r\n\tinception_4a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_4a_pool)\r\n\tinception_4a_pool = Lambda(lambda x: x*9, name=\'mult9_4a\')(inception_4a_pool)\r\n\tinception_4a_pool = Lambda(lambda x: K.sqrt(x), name=\'sqrt_4a\')(inception_4a_pool)\r\n\t\r\n\tinception_4a_pool = Conv2D(128, (1,1), strides=(1,1), name=\'inception_4a_pool_conv\'+\'\')(inception_4a_pool)\r\n\tinception_4a_pool = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4a_pool_bn\'+\'\')(inception_4a_pool)\r\n\tinception_4a_pool = Activation(\'relu\')(inception_4a_pool)\r\n\tinception_4a_pool = ZeroPadding2D(padding=(2, 2))(inception_4a_pool)\r\n\t\r\n\tinception_4a_1x1 = Conv2D(256, (1, 1), strides=(1, 1), name=\'inception_4a_1x1_conv\'+\'\')(inception_3c)\r\n\tinception_4a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4a_1x1_bn\'+\'\')(inception_4a_1x1)\r\n\tinception_4a_1x1 = Activation(\'relu\')(inception_4a_1x1)\r\n\t\r\n\tinception_4a = concatenate([inception_4a_3x3, inception_4a_5x5, inception_4a_pool, inception_4a_1x1], axis=3)\r\n\t\r\n\t#inception4e\r\n\tinception_4e_3x3 = Conv2D(160, (1,1), strides=(1,1), name=\'inception_4e_3x3_conv\'+\'1\')(inception_4a)\r\n\tinception_4e_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4e_3x3_bn\'+\'1\')(inception_4e_3x3)\r\n\tinception_4e_3x3 = Activation(\'relu\')(inception_4e_3x3)\r\n\tinception_4e_3x3 = ZeroPadding2D(padding=(1, 1))(inception_4e_3x3)\r\n\tinception_4e_3x3 = Conv2D(256, (3,3), strides=(2,2), name=\'inception_4e_3x3_conv\'+\'2\')(inception_4e_3x3)\r\n\tinception_4e_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4e_3x3_bn\'+\'2\')(inception_4e_3x3)\r\n\tinception_4e_3x3 = Activation(\'relu\')(inception_4e_3x3)\r\n\t\r\n\tinception_4e_5x5 = Conv2D(64, (1,1), strides=(1,1), name=\'inception_4e_5x5_conv\'+\'1\')(inception_4a)\r\n\tinception_4e_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4e_5x5_bn\'+\'1\')(inception_4e_5x5)\r\n\tinception_4e_5x5 = Activation(\'relu\')(inception_4e_5x5)\r\n\tinception_4e_5x5 = ZeroPadding2D(padding=(2, 2))(inception_4e_5x5)\r\n\tinception_4e_5x5 = Conv2D(128, (5,5), strides=(2,2), name=\'inception_4e_5x5_conv\'+\'2\')(inception_4e_5x5)\r\n\tinception_4e_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_4e_5x5_bn\'+\'2\')(inception_4e_5x5)\r\n\tinception_4e_5x5 = Activation(\'relu\')(inception_4e_5x5)\r\n\t\r\n\tinception_4e_pool = MaxPooling2D(pool_size=3, strides=2)(inception_4a)\r\n\tinception_4e_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_4e_pool)\r\n\t\r\n\tinception_4e = concatenate([inception_4e_3x3, inception_4e_5x5, inception_4e_pool], axis=3)\r\n\t\r\n\t#inception5a\r\n\tinception_5a_3x3 = Conv2D(96, (1,1), strides=(1,1), name=\'inception_5a_3x3_conv\'+\'1\')(inception_4e)\r\n\tinception_5a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5a_3x3_bn\'+\'1\')(inception_5a_3x3)\r\n\tinception_5a_3x3 = Activation(\'relu\')(inception_5a_3x3)\r\n\tinception_5a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_5a_3x3)\r\n\tinception_5a_3x3 = Conv2D(384, (3,3), strides=(1,1), name=\'inception_5a_3x3_conv\'+\'2\')(inception_5a_3x3)\r\n\tinception_5a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5a_3x3_bn\'+\'2\')(inception_5a_3x3)\r\n\tinception_5a_3x3 = Activation(\'relu\')(inception_5a_3x3)\r\n\t\r\n\tinception_5a_pool = Lambda(lambda x: x**2, name=\'power2_5a\')(inception_4e)\r\n\tinception_5a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_5a_pool)\r\n\tinception_5a_pool = Lambda(lambda x: x*9, name=\'mult9_5a\')(inception_5a_pool)\r\n\tinception_5a_pool = Lambda(lambda x: K.sqrt(x), name=\'sqrt_5a\')(inception_5a_pool)\r\n\t\r\n\tinception_5a_pool = Conv2D(96, (1,1), strides=(1,1), name=\'inception_5a_pool_conv\'+\'\')(inception_5a_pool)\r\n\tinception_5a_pool = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5a_pool_bn\'+\'\')(inception_5a_pool)\r\n\tinception_5a_pool = Activation(\'relu\')(inception_5a_pool)\r\n\tinception_5a_pool = ZeroPadding2D(padding=(1,1))(inception_5a_pool)\r\n\t\r\n\tinception_5a_1x1 = Conv2D(256, (1,1), strides=(1,1), name=\'inception_5a_1x1_conv\'+\'\')(inception_4e)\r\n\tinception_5a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5a_1x1_bn\'+\'\')(inception_5a_1x1)\r\n\tinception_5a_1x1 = Activation(\'relu\')(inception_5a_1x1)\r\n\t\r\n\tinception_5a = concatenate([inception_5a_3x3, inception_5a_pool, inception_5a_1x1], axis=3)\r\n\t\r\n\t#inception_5b\r\n\tinception_5b_3x3 = Conv2D(96, (1,1), strides=(1,1), name=\'inception_5b_3x3_conv\'+\'1\')(inception_5a)\r\n\tinception_5b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5b_3x3_bn\'+\'1\')(inception_5b_3x3)\r\n\tinception_5b_3x3 = Activation(\'relu\')(inception_5b_3x3)\r\n\tinception_5b_3x3 = ZeroPadding2D(padding=(1,1))(inception_5b_3x3)\r\n\tinception_5b_3x3 = Conv2D(384, (3,3), strides=(1,1), name=\'inception_5b_3x3_conv\'+\'2\')(inception_5b_3x3)\r\n\tinception_5b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5b_3x3_bn\'+\'2\')(inception_5b_3x3)\r\n\tinception_5b_3x3 = Activation(\'relu\')(inception_5b_3x3)\r\n\t\r\n\tinception_5b_pool = MaxPooling2D(pool_size=3, strides=2)(inception_5a)\r\n\t\r\n\tinception_5b_pool = Conv2D(96, (1,1), strides=(1,1), name=\'inception_5b_pool_conv\'+\'\')(inception_5b_pool)\r\n\tinception_5b_pool = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5b_pool_bn\'+\'\')(inception_5b_pool)\r\n\tinception_5b_pool = Activation(\'relu\')(inception_5b_pool)\r\n\t\r\n\tinception_5b_pool = ZeroPadding2D(padding=(1, 1))(inception_5b_pool)\r\n\t\r\n\tinception_5b_1x1 = Conv2D(256, (1,1), strides=(1,1), name=\'inception_5b_1x1_conv\'+\'\')(inception_5a)\r\n\tinception_5b_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name=\'inception_5b_1x1_bn\'+\'\')(inception_5b_1x1)\r\n\tinception_5b_1x1 = Activation(\'relu\')(inception_5b_1x1)\r\n\t\r\n\tinception_5b = concatenate([inception_5b_3x3, inception_5b_pool, inception_5b_1x1], axis=3)\r\n\t\r\n\tav_pool = AveragePooling2D(pool_size=(3, 3), strides=(1, 1))(inception_5b)\r\n\treshape_layer = Flatten()(av_pool)\r\n\tdense_layer = Dense(128, name=\'dense_layer\')(reshape_layer)\r\n\tnorm_layer = Lambda(lambda  x: K.l2_normalize(x, axis=1), name=\'norm_layer\')(dense_layer)\r\n\t\r\n\t# Final Model\r\n\tmodel = Model(inputs=[myInput], outputs=norm_layer)\r\n\treturn model\r\n\r\nmodel = builtModel()\r\nprint(""model built"")\r\n\r\n#------------------------\r\n\r\n#https://drive.google.com/file/d/1LSe1YCV1x-BfNnfb7DFZTNpv_Q9jITxn/view\r\nmodel.load_weights(\'weights/openface_weights.h5\')\r\nprint(""weights loaded"")\r\n\r\n#------------------------\r\ndef findCosineDistance(source_representation, test_representation):\r\n    a = np.matmul(np.transpose(source_representation), test_representation)\r\n    b = np.sum(np.multiply(source_representation, source_representation))\r\n    c = np.sum(np.multiply(test_representation, test_representation))\r\n    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\r\n\r\ndef l2_normalize(x, axis=-1, epsilon=1e-10):\r\n    output = x / np.sqrt(np.maximum(np.sum(np.square(x), axis=axis, keepdims=True), epsilon))\r\n    return output\t\r\n    \r\ndef findEuclideanDistance(source_representation, test_representation):\r\n    euclidean_distance = source_representation - test_representation\r\n    euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\r\n    euclidean_distance = np.sqrt(euclidean_distance)\r\n    #euclidean_distance = l2_normalize(euclidean_distance)\r\n    return euclidean_distance\r\n\r\n#------------------------\r\n\r\nmetric = ""cosine""  #cosine, euclidean\r\n\r\nif metric == ""cosine"":\r\n\tthreshold = 0.45\r\nelse:\r\n\tthreshold = 0.95\r\n\r\n#------------------------\t\r\n\r\n#put your employee pictures in this path as name_of_employee.jpg\r\nemployee_pictures = ""database/""\r\n\r\nemployees = dict()\r\n\r\nfor file in listdir(employee_pictures):\r\n\temployee, extension = file.split(""."")\r\n\timg = preprocess_image(\'database/%s.jpg\' % (employee))\r\n\trepresentation = model.predict(img)[0,:]\r\n\t\r\n\temployees[employee] = representation\r\n\t\r\nprint(""employee representations retrieved successfully"")\r\n\r\n#------------------------\r\n\r\ncap = cv2.VideoCapture(0) #webcam\r\n\r\nwhile(True):\r\n\tret, img = cap.read()\r\n\tfaces = face_cascade.detectMultiScale(img, 1.3, 5)\r\n\t\r\n\tfor (x,y,w,h) in faces:\r\n\t\tif w > 130: #discard small detected faces\r\n\t\t\tcv2.rectangle(img, (x,y), (x+w,y+h), color, 1) #draw rectangle to main image\r\n\t\t\t\r\n\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\r\n\t\t\tdetected_face = cv2.resize(detected_face, (96, 96)) #resize to 96x96\r\n\t\t\t\r\n\t\t\timg_pixels = image.img_to_array(detected_face)\r\n\t\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\r\n\t\t\t#employee dictionary is using preprocess_image and it normalizes in scale of [-1, +1]\r\n\t\t\timg_pixels /= 127.5\r\n\t\t\timg_pixels -= 1\r\n\t\t\t\r\n\t\t\tcaptured_representation = model.predict(img_pixels)[0,:]\r\n\t\t\t\r\n\t\t\tdistances = []\r\n\t\t\t\r\n\t\t\tfor i in employees:\r\n\t\t\t\temployee_name = i\r\n\t\t\t\tsource_representation = employees[i]\r\n\t\t\t\t\r\n\t\t\t\tif metric == ""cosine"":\r\n\t\t\t\t\tdistance = findCosineDistance(captured_representation, source_representation)\r\n\t\t\t\telif metric == ""euclidean"":\r\n\t\t\t\t\tdistance = findEuclideanDistance(captured_representation, source_representation)\r\n\t\t\t\t\r\n\t\t\t\tif dump:\r\n\t\t\t\t\tprint(employee_name,"": "",distance)\r\n\t\t\t\tdistances.append(distance)\r\n\t\t\t\r\n\t\t\tlabel_name = \'unknown\'\r\n\t\t\tindex = 0\r\n\t\t\tfor i in employees:\r\n\t\t\t\temployee_name = i\r\n\t\t\t\tif index == np.argmin(distances):\r\n\t\t\t\t\tif distances[index] <= threshold:\r\n\t\t\t\t\t\t#print(""detected: "",employee_name)\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\tif metric == ""euclidean"":\r\n\t\t\t\t\t\t\tsimilarity = 100 + (90 - 100*distance)\r\n\t\t\t\t\t\telif metric == ""cosine"":\r\n\t\t\t\t\t\t\tsimilarity = 100 + (40 - 100*distance)\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\tif similarity > 99.99: similarity = 99.99\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\tlabel_name = ""%s (%s%s)"" % (employee_name, str(round(similarity,2)), \'%\')\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\tbreak\r\n\t\t\t\t\t\r\n\t\t\t\tindex = index + 1\r\n\t\t\t\r\n\t\t\tcv2.putText(img, label_name, (int(x+w+15), int(y-64)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\r\n\t\t\t\r\n\t\t\tif dump:\r\n\t\t\t\tprint(""----------------------"")\r\n\t\t\t\t\t\r\n\t\t\t#connect face and text\r\n\t\t\tcv2.line(img,(x+w, y-64),(x+w-25, y-64),color,1)\r\n\t\t\tcv2.line(img,(int(x+w/2),y),(x+w-25,y-64),color,1)\r\n\t\t\t\r\n\tcv2.imshow(\'img\',img)\r\n\t\r\n\tif cv2.waitKey(1) & 0xFF == ord(\'q\'): #press q to quit\r\n\t\tbreak\r\n\t\r\n#kill open cv things\t\t\r\ncap.release()\r\ncv2.destroyAllWindows()'"
python/pandas-for-deep-learning.py,0,"b'import pandas as pd\r\nimport numpy as np\r\n\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Activation, Dropout, Flatten\r\n#--------------------------------\r\n\r\nnum_classes = 3 #Iris-setosa,Iris-versicolor,Iris-virginica\r\n\r\n#--------------------------------\r\ndef createNetwork():\r\n\tmodel = Sequential()\r\n\r\n\tmodel.add(Dense(4 #num of hidden units\r\n\t\t, input_shape=(4,))) #num of features in input layer\r\n\tmodel.add(Activation(\'sigmoid\')) #activation function from input layer to 1st hidden layer\r\n\t\r\n\tmodel.add(Dense(num_classes)) #num of classes in output layer\r\n\tmodel.add(Activation(\'sigmoid\')) #activation function from 1st hidden layer to output layer\r\n\t\r\n\treturn model\r\n\r\nmodel = createNetwork()\r\nmodel.compile(loss=\'categorical_crossentropy\'\r\n    , optimizer=keras.optimizers.Adam(lr=0.007)\r\n    , metrics=[\'accuracy\']\r\n)\r\n#--------------------------------\r\nchunk_size = 30\r\nepochs = 1000\r\n\r\nfor epoch in range(0, epochs): #epoch should be handled here, not in fit command!\r\n\t\r\n\tif epoch % 100 == 0:\r\n\t\tprint(""epoch "",epoch)\r\n\t\r\n\tchunk_index = 0\r\n\tfor chunk in pd.read_csv(""iris.data"", chunksize=chunk_size\r\n\t\t, names = [""sepal_length"",""sepal_width"",""petal_length"",""petal_width"",""class""]):\r\n\t\t\r\n\t\t#print(""current chunk: "",chunk_index*chunk_size)\r\n\t\t\t\t\r\n\t\tcurrent_set = chunk.values #convert df to numpy array\r\n\t\t\t\t\r\n\t\tfeatures = current_set[:,0:4]\r\n\t\tlabels = current_set[:,4]\r\n\t\t\r\n\t\tfor i in range(0,labels.shape[0]):\r\n\t\t\tif labels[i] == \'Iris-setosa\':\r\n\t\t\t\tlabels[i] = 0\r\n\t\t\telif labels[i] == \'Iris-versicolor\':\r\n\t\t\t\tlabels[i] = 1\r\n\t\t\telif labels[i] == \'Iris-virginica\':\r\n\t\t\t\tlabels[i] = 2\r\n\t\t\r\n\t\tlabels = keras.utils.to_categorical(labels, num_classes)\r\n\t\t\r\n\t\t#------------------------------------\r\n\t\tmodel.fit(features, labels, epochs=1, verbose=0) #epochs handled in the for loop above\r\n\t\t\r\n\t\tchunk_index = chunk_index + 1\r\n#-------------------------------------------\r\ndf = pd.read_csv(""iris.data"", names = [""sepal_length"",""sepal_width"",""petal_length"",""petal_width"",""class""])\r\n\r\nfor index, row in df.iterrows():\r\n\tfeatures = row.values[0:4]\r\n\tactual_label = row.values[4]\r\n\t\t\r\n\tprediction = model.predict(np.array([features]))\r\n\tprediction = np.argmax(prediction)\r\n\t\r\n\tif prediction == 0:\r\n\t\tpredicted_class = ""Iris-setosa""\r\n\telif prediction == 1:\r\n\t\tpredicted_class = ""Iris-versicolor""\r\n\telif prediction == 2:\r\n\t\tpredicted_class = ""Iris-virginica""\r\n\t\r\n\tif predicted_class != actual_label:\r\n\t\tprint(""*"", end=\'\')\r\n\t\t\r\n\tprint("" prediction: "",predicted_class, "" - actual: "",actual_label)'"
python/pandas-training.py,0,"b'import pandas as pd\r\nimport time\r\nimport numpy as np\r\n\r\n#------------------------------\r\n\r\nbegin = time.time()\r\ndf = pd.read_csv(""train.csv"")\r\n#df = pd.read_csv(""prediction.csv"")\r\nprint(""dataset loaded in "",time.time()-begin,"" seconds"")\r\n\r\nprint(""top 10 rows of dataset: "")\r\nprint(df.head(10))\r\n\r\nprint(""dataset has following columns: "",df.columns)\r\n\r\nprint(""Campaign_ID column has "",df[\'Campaign_ID\'].dtypes,"" datatype. It can be following variables:"")\r\nprint(df[\'Campaign_ID\'].unique())\r\n\r\n\r\nfor index, row in df.iterrows():\r\n\tif index < 10:\r\n\t\tprint(row)\r\n\telse:\r\n\t\tbreak\r\n\r\n#------------------------------\r\n\r\n#filtering\r\nfilter = df[(df[\'Campaign_ID\'] == 293787647989)]\r\nprint(filter.info())\r\n\r\n#summary\r\nprint(df[\'Clicks\'].value_counts().head(10))\r\n\r\n#ordering\r\nprint(df.sort_values(\'Clicks\', ascending=False).head(10))\r\n\r\n#------------------------------\r\n\r\n#load massive dataset as small chunks\r\n\r\nc_size = 10\r\n\r\nfor gm_chunk in pd.read_csv(""train.csv"", chunksize=c_size):\r\n\t#print(gm_chunk)\r\n\tnumpy_chunk = gm_chunk.values #to numpy array\r\n\t#pd.DataFrame(numpy_chunk) #numpy to pandas restoration\r\n\t#print(numpy_chunk.shape,"" shaped "",type(numpy_chunk))\r\n\tprint(numpy_chunk)\r\n\tbreak\r\n\r\n'"
python/real-time-ethnicity-prediction.py,0,"b'import numpy as np\r\nimport cv2\r\nfrom keras.models import Model, Sequential\r\nfrom keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\r\nfrom PIL import Image\r\nfrom keras.preprocessing.image import load_img, save_img, img_to_array\r\nfrom keras.applications.imagenet_utils import preprocess_input\r\nfrom keras.preprocessing import image\r\nimport matplotlib.pyplot as plt\r\nfrom os import listdir\r\n#-----------------------\r\n\r\ncolor = (67,67,67) #gray\r\n\r\nface_cascade = cv2.CascadeClassifier(\'haarcascade_frontalface_default.xml\')\r\n\r\ndef loadVggFaceModel():\r\n\tmodel = Sequential()\r\n\tmodel.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\r\n\tmodel.add(Convolution2D(64, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(64, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(128, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(128, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(256, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(256, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(256, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(ZeroPadding2D((1,1)))\r\n\tmodel.add(Convolution2D(512, (3, 3), activation=\'relu\'))\r\n\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\r\n\r\n\tmodel.add(Convolution2D(4096, (7, 7), activation=\'relu\'))\r\n\tmodel.add(Dropout(0.5))\r\n\tmodel.add(Convolution2D(4096, (1, 1), activation=\'relu\'))\r\n\tmodel.add(Dropout(0.5))\r\n\tmodel.add(Convolution2D(2622, (1, 1)))\r\n\tmodel.add(Flatten())\r\n\tmodel.add(Activation(\'softmax\'))\r\n\t\r\n\t""""""\r\n\t#you can download pretrained weights from https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing\r\n\tfrom keras.models import model_from_json\r\n\tmodel.load_weights(\'C:/Users/IS96273/Desktop/vgg_face_weights.h5\')\r\n\t""""""\r\n\t\r\n\tvgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)\r\n\t\r\n\treturn vgg_face_descriptor\r\n\r\n#------------------------\r\n\r\nmodel = loadVggFaceModel()\r\n\r\nbase_model_output = Sequential()\r\nbase_model_output = Convolution2D(6, (1, 1), name=\'predictions\')(model.layers[-4].output)\r\nbase_model_output = Flatten()(base_model_output)\r\nbase_model_output = Activation(\'softmax\')(base_model_output)\r\n\r\nrace_model = Model(inputs=model.input, outputs=base_model_output)\r\n\r\n#pre-trained race and ethnicity prediction model weights can be found here: https://drive.google.com/file/d/1nz-WDhghGQBC4biwShQ9kYjvQMpO6smj/view?usp=sharing\r\nrace_model.load_weights(\'weights/race_model_single_batch.h5\')\r\n\r\n#------------------------\r\n\r\nraces = [\'Asian\', \'Indian\', \'Black\', \'White\', \'Middle Eastern\', \'Latino_Hispanic\']\r\n\r\n#------------------------\r\n\r\ncap = cv2.VideoCapture(0) #webcam\r\n\r\nwhile(True):\r\n\tret, img = cap.read()\r\n\t#img = cv2.resize(img, (640, 360))\r\n\tfaces = face_cascade.detectMultiScale(img, 1.3, 5)\r\n\t\r\n\tfor (x,y,w,h) in faces:\r\n\t\tif w > 130: \r\n\t\t\tcv2.rectangle(img,(x,y),(x+w,y+h),(67, 67, 67),1) #draw rectangle to main image\r\n\t\t\t\r\n\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\r\n\t\t\t\r\n\t\t\t#add margin\r\n\t\t\tmargin_rate = 30\r\n\t\t\ttry:\r\n\t\t\t\tmargin_x = int(w * margin_rate / 100)\r\n\t\t\t\tmargin_y = int(h * margin_rate / 100)\r\n\t\t\t\t\r\n\t\t\t\tdetected_face = img[int(y-margin_y):int(y+h+margin_y), int(x-margin_x):int(x+w+margin_x)]\r\n\t\t\t\tdetected_face = cv2.resize(detected_face, (224, 224)) #resize to 224x224\r\n\t\t\t\t\r\n\t\t\t\t#display margin added face\r\n\t\t\t\t#cv2.rectangle(img,(x-margin_x,y-margin_y),(x+w+margin_x,y+h+margin_y),(67, 67, 67),1)\r\n\t\t\t\t\r\n\t\t\texcept Exception as err:\r\n\t\t\t\t#print(""margin cannot be added ("",str(err),"")"")\r\n\t\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)]\r\n\t\t\t\tdetected_face = cv2.resize(detected_face, (224, 224))\r\n\t\t\t\r\n\t\t\t#print(""shape: "",detected_face.shape)\r\n\t\t\t\r\n\t\t\tif detected_face.shape[0] > 0 and detected_face.shape[1] > 0 and detected_face.shape[2] >0: #sometimes shape becomes (264, 0, 3)\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\timg_pixels = image.img_to_array(detected_face)\r\n\t\t\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\r\n\t\t\t\timg_pixels /= 255\r\n\t\t\t\t\r\n\t\t\t\tprediction_proba = race_model.predict(img_pixels)\r\n\t\t\t\tprediction = np.argmax(prediction_proba)\r\n\t\t\t\t\r\n\t\t\t\tif False: # activate to dump\r\n\t\t\t\t\tfor i in range(0, len(races)):\r\n\t\t\t\t\t\tif np.argmax(prediction_proba) == i:\r\n\t\t\t\t\t\t\tprint(""* "", end=\'\')\r\n\t\t\t\t\t\tprint(races[i], "": "", prediction_proba[0][i])\r\n\t\t\t\t\tprint(""----------------"")\r\n\t\t\t\t\r\n\t\t\t\trace = races[prediction]\r\n\t\t\t\t#--------------------------\r\n\t\t\t\t#background\r\n\t\t\t\toverlay = img.copy()\r\n\t\t\t\topacity = 0.4\r\n\t\t\t\tcv2.rectangle(img,(x+w+10,y-50),(x+w+170,y+15),(64,64,64),cv2.FILLED)\r\n\t\t\t\tcv2.addWeighted(overlay, opacity, img, 1 - opacity, 0, img)\r\n\t\t\t\t\r\n\t\t\t\tcolor = (255,255,255)\r\n\t\t\t\tproba = round(100*prediction_proba[0, prediction], 2)\r\n\t\t\t\t\r\n\t\t\t\tif proba >= 51:\r\n\t\t\t\t\tlabel = str(race+"" (""+str(proba)+""%)"")\r\n\t\t\t\t\tcv2.putText(img, label, (int(x+w+25), int(y-12)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t#connect face and text\r\n\t\t\t\t\tcv2.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),(67, 67, 67),1)\r\n\t\t\t\t\tcv2.line(img,(x+w,y-20),(x+w+10,y-20),(67, 67, 67),1)\r\n\t\r\n\tcv2.imshow(\'img\',img)\r\n\t\r\n\tif cv2.waitKey(1) & 0xFF == ord(\'q\'): #press q to quit\r\n\t\tbreak\r\n\t\r\n#kill open cv things\t\t\r\ncap.release()\r\ncv2.destroyAllWindows()'"
python/single-layer-perceptron.py,0,"b'import numpy as np\r\n\r\noperator = \'and\'\r\n\r\n#----------------\r\n\r\natributes = np.array([ [0, 0], [0, 1], [1, 0], [1, 1]])\r\n\r\nif operator == \'and\':\r\n\tlabels = np.array([0, 0, 0, 1])\r\nelif operator == \'or\':\r\n\tlabels = np.array([0, 1, 1, 1])\r\nelif operator == \'xor\':\r\n\tlabels = np.array([0, 1, 1, 0])\r\n\r\n#----------------\r\n\r\nw = [+9, +9] #initial random values for weights\r\n\r\nthreshold = 5\r\nalpha = 0.5 #learning rate\r\nepoch = 1000 #learning time\r\n#----------------\r\n\r\nprint(""learning rate: "", alpha,"", threshold: "", threshold)\r\n\r\nfor i in range(0, epoch):\r\n\tprint(""epoch "", i+1)\r\n\tglobal_delta = 0 #this variable is used to terminate the for loop if learning completed in early epoch\r\n\tfor j in range(len(atributes)):\r\n\t\t\r\n\t\tactual = labels[j]\r\n\t\t\r\n\t\tsum = atributes[j][0]*w[0] + atributes[j][1]*w[1]\r\n\t\t\r\n\t\tif sum > threshold: #then fire\r\n\t\t\tpredicted = 1\r\n\t\telse: #do not fire\r\n\t\t\tpredicted = 0\r\n\t\t\r\n\t\tdelta = actual - predicted\r\n\t\tglobal_delta = global_delta + abs(delta)\r\n\t\t\r\n\t\t#update weights with respect to the error\r\n\t\tfor k in range(0, 2):\r\n\t\t\tw[k] = w[k] + delta * alpha\r\n\t\t\t\t\r\n\t\tprint(atributes[j][0],"" "", operator, "" "", atributes[j][1], "" -> actual: "", actual, "", predicted: "", predicted, "" (w: "",w[0],"")"")\r\n\t\t\r\n\tif global_delta == 0:\r\n\t\tbreak\r\n\t\r\n\tprint(""------------------------------"")\r\n'"
python/transfer_learning.py,0,"b'from keras.applications.inception_v3 import InceptionV3\r\nfrom keras.applications.inception_v3 import preprocess_input\r\nfrom keras.applications.inception_v3 import decode_predictions\r\n\r\nfrom keras.preprocessing import image\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n#--------------------------------\r\n\r\nstore_model = False\r\n\r\n#--------------------------------\r\n\r\nif store_model == True:\r\n\tmodel = InceptionV3(weights=\'imagenet\', include_top=True)\r\n\t\r\n\t#save model and weights\r\n\tmodel_config = model.to_json()\r\n\topen(""inceptionv3_structure.json"", ""w"").write(model_config)\r\n\tmodel.save_weights(\'inceptionv3_weights.h5\')\r\nelse:\r\n\tfrom keras.models import model_from_json\r\n\tmodel = model_from_json(open(""inceptionv3_structure.json"", ""r"").read())\r\n\tmodel.load_weights(\'inceptionv3_weights.h5\')\r\n\tprint(""inception v3 model loaded"")\r\n\t\r\n#print(""model structure: "", model.summary())\r\n#print(""model weights: "", model.get_weights())\r\n\r\n#put images in testset folder, name images from 1.jpg to 16.jpg\r\nfor i in range(1, 17):\r\n\t\r\n\timg_path = \'testset/%s.jpg\' % (i)\r\n\t\r\n\timg = image.load_img(img_path, target_size=(299, 299))\r\n\tx = image.img_to_array(img)\r\n\tx = np.expand_dims(x, axis = 0)\r\n\tx = preprocess_input(x)\r\n\t\r\n\tfeatures = model.predict(x)\r\n\tprint(decode_predictions(features, top = 3))\r\n\t\r\n\tplt.imshow(image.load_img(img_path))\r\n\tplt.show()\r\n'"
