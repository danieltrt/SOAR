file_path,api_count,code
annotation-tool.py,0,"b""'''\t\n\tAnnotation tool\n\n\t\tTool used to annotate labels for LP detection. Main keys:\n\t\t\tC: creates a new shape\n\t\t\tA: creates a new vertex over mouse position\n\t\t\tD: delete last vertex\n\t\t\tS: change position of closest vertex (to mouse pos.)\n\t\t\tX: switch to closest shape\n\t\t\tN or enter: next picture\n\t\t\tP: previous picture\n\t\t\tFor more commands, take a look at the main part...\n\n\t\tUsage:\n\n\t\t\tpython annotation-tool.py max_width max_height file1 ... fileN\n\n\t\tObs. This tool is not fully tested and crashes under unexpetected situations.\n\t\tIf you find any and correct, feel free to send a pull request =)\n\n'''\n\nimport cv2\nimport sys\nimport numpy as np\nimport os\n\nfrom math import cos, sin\nfrom os.path import isfile, splitext, basename, isdir\nfrom os import makedirs\n\nfrom src.utils import image_files_from_folder,getWH\nfrom src.label import Label, Shape, readShapes, writeShapes\nfrom src.projection_utils import perspective_transform, find_T_matrix, getRectPts\n\n\nclass ShapeDisplay(Shape):\n\n\tdef appendSide(self,pt):\n\t\tif (self.max_sides == 0) or (self.pts.shape[1] < self.max_sides):\n\t\t\tself.pts = np.append(self.pts,pt,axis=1)\n\n\tdef removeLast(self):\n\t\tself.pts = self.pts[...,:-1]\n\n\tdef changeClosest(self,pt):\n\t\tidx = np.argmin(self.distanceTo(pt))\n\t\tself.pts[...,idx] = pt[...,0]\n\n\tdef distanceTo(self,pt):\n\t\treturn np.sqrt(np.power((self.pts - pt),2).sum(0))\n\n\tdef shiftPts(self):\n\t\tif self.pts.shape[1] > 1:\n\t\t\tidx = range(1,self.pts.shape[1]) + [0]\n\t\t\tself.pts = self.pts[...,idx]\n\n\tdef getSquare(self):\n\t\ttl,br = self.pts.min(1),self.pts.max(1)\n\t\treturn Label(-1,tl,br)\n\n\tdef draw(self,drawLineFunc,drawCircleFunc,drawTextFunc,color=(255,255,255),txtcolor=(255,255,255)):\n\t\tss = self.pts.shape[1]\n\t\tif ss:\n\t\t\ttext = self.text if len(self.text) else 'NO_LABEL'\n\t\t\tdrawTextFunc(text,self.pts.min(1),color=txtcolor)\n\t\tif ss > 1:\n\t\t\tfor i in range(ss):\n\t\t\t\tdrawLineFunc(self.pts[:,i],self.pts[:,(i+1)%ss],color=color)\n\t\t\t\tdrawCircleFunc(self.pts[:,0] ,color=(255-color[0],0,255-color[2]))\n\t\t\t\tdrawCircleFunc(self.pts[:,-1],color=(255-color[0],255-color[1],255-color[2]))\n\n\ndef rotation_transform(wh,angles=np.array([0.,0.,0.]),zcop=1000., dpp=1000.):\n\trads = np.deg2rad(angles)\n\n\ta = rads[0]; Rx = np.matrix([[1, 0, 0]\t\t\t\t, [0, cos(a), sin(a)]\t, [0, -sin(a), cos(a)]\t])\n\ta = rads[1]; Ry = np.matrix([[cos(a), 0, -sin(a)]\t, [0, 1, 0]\t\t\t\t, [sin(a), 0, cos(a)]\t])\n\ta = rads[2]; Rz = np.matrix([[cos(a), sin(a), 0]\t, [-sin(a), cos(a), 0]\t, [0, 0, 1]\t\t\t\t])\n\n\tR = Rx*Ry*Rz;\n\n\treturn R\n\nclass Display():\n\n\tdef __init__(self,I,width,height,wname='Display'):\n\n\t\tself.Iorig = I.copy()\n\t\tself.width = width\n\t\tself.height = height\n\t\tself.wname = wname\n\n\t\tself.Idisplay = self.Iorig.copy()\n\t\tself.IdisplayCopy = self.Idisplay.copy()\n\n\t\tself.reset_view()\n\t\tself._setPerspective()\n\n\t\tcv2.namedWindow(self.wname)\n\t\tcv2.moveWindow(self.wname, 0, 0)\n\t\tcv2.setMouseCallback(self.wname,self.mouse_callback)\n\n\tdef reset_view(self):\n\t\tself.cx, self.cy = .5,.5\n\t\twh = np.array([self.width,self.height],dtype=float)\n\t\tself.zoom_factor = (wh/getWH(self.Iorig.shape)).min()\n\t\tself.mouse_center = np.array([.5,.5])\n\t\tself.angles = np.array([0.,0.,0.])\n\t\tself._setPerspective()\n\n\tdef updatePerspectiveMatrix(self):\n\t\tzf  = self.zoom_factor\n\t\tw,h = getWH(self.Iorig.shape)\n\n\t\tself.dx = self.cx*w*zf - self.width/2.\n\t\tself.dy = self.cy*h*zf - self.height/2.\n\n\t\tR = np.eye(3)\n\t\tR = np.matmul(R,np.matrix([[zf,0,-self.dx],[0,zf,-self.dy],[0,0,1]],dtype=float))\n\t\tR = np.matmul(R,perspective_transform((w,h),angles=self.angles))\n\n\t\tself.R = R\n\t\tself.Rinv = np.linalg.inv(R)\n\n\tdef show(self):\n\t\tcv2.imshow(self.wname,self.Idisplay)\n\n\tdef setPerspectiveAngle(self,addx=0.,addy=0.,addz=0.):\n\t\tself.angles += np.array([addx,addy,addz])\n\t\tself._setPerspective()\n\n\tdef _setPerspective(self,update=True):\n\t\tif update:\n\t\t\tself.updatePerspectiveMatrix()\n\t\tself.IdisplayCopy = cv2.warpPerspective(self.Iorig,self.R,(self.width,self.height),borderValue=.0,flags=cv2.INTER_LINEAR)\n\n\tdef resetDisplay(self):\n\t\tself.Idisplay = self.IdisplayCopy.copy()\n\n\tdef getMouseCenterRelative(self):\n\t\treturn self.mouse_center.copy().reshape((2,1))\n\n\tdef waitKey(self,time=50):\n\t\treturn cv2.waitKey(50) & 0x0000000FF\n\n\tdef __pt2xy(self,pt):\n\t\tpt = np.squeeze(np.array(np.matmul(self.R,np.append(pt*getWH(self.Iorig.shape),1.))))\n\t\tpt = pt[:2]/pt[2]\n\t\treturn tuple(pt.astype(int).tolist())\n\n\tdef __pts2xys(self,pts):\n\t\tN = pts.shape[1]\n\t\tpts = pts*getWH(self.Iorig.shape).reshape((2,1))\n\t\tpts = np.concatenate((pts,np.ones((1,N))))\n\t\tpts = np.squeeze(np.array(np.matmul(self.R,pts)))\n\t\tpts = pts[:2]/pts[2]\n\t\treturn pts\n\n\tdef drawLine(self,pt1,pt2,color=(255,255,255),thickness=3):\n\t\tpt1 = self.__pt2xy(pt1)\n\t\tpt2 = self.__pt2xy(pt2)\n\t\tcv2.line(self.Idisplay,pt1,pt2,color=color,thickness=thickness)\n\n\tdef drawCircle(self,center,color=(255,255,255),radius=7):\n\t\tcenter = self.__pt2xy(center)\n\t\tcv2.circle(self.Idisplay,center,radius,color,thickness=-1)\n\n\tdef drawText(self,text,bottom_left_pt,color=(255,255,255),bgcolor=(0,0,0),font_size=1):\n\t\tbl_corner = self.__pt2xy(bottom_left_pt)\n\t\tfont = cv2.FONT_HERSHEY_SIMPLEX\n\n\t\twh_text,v = cv2.getTextSize(text, font, font_size, 3)\n\t\ttl_corner = (bl_corner[0],bl_corner[1]-wh_text[1])\n\t\tbr_corner = (bl_corner[0]+wh_text[0],bl_corner[1])\n\n\t\tcv2.rectangle(self.Idisplay, tl_corner, br_corner, bgcolor,-1)\t\n\t\tcv2.putText(self.Idisplay,text,bl_corner,font,font_size,color,3)\n\n\tdef zoom(self,ff):\n\t\tself.zoom_factor *= ff\n\t\tself.cx,self.cy = self.mouse_center.tolist()\n\t\tself._setPerspective()\n\n\tdef rectifyToPts(self,pts):\n\t\t\n\t\tif pts.shape[1] != 4:\n\t\t\treturn\n\n\t\tptsh = pts*getWH(self.Iorig.shape).reshape((2,1))\n\t\tptsh = np.concatenate((ptsh,np.ones((1,4))))\n\n\t\tto_pts = self.__pts2xys(pts)\n\t\twi,hi = (to_pts.min(1)[:2]).tolist()\n\t\twf,hf = (to_pts.max(1)[:2]).tolist()\n\t\tto_pts = np.matrix([[wi,wf,wf,wi],[hi,hi,hf,hf],[1,1,1,1]])\n\n\t\tself.R = find_T_matrix(ptsh,to_pts)\n\t\tself.Rinv = np.linalg.inv(self.R)\n\t\tself._setPerspective(update=False)\n\n\tdef mouse_callback(self,event,x,y,flags,param):\n\t\tmc = np.array([x,y],dtype=float)\n\t\tmc = np.matmul(self.Rinv,np.append(mc,1.))\n\t\tmc = np.squeeze(np.array(mc))\n\t\tself.mouse_center = (mc[:2]/mc[2])/getWH(self.Iorig.shape)\n\ndef selectClosest(shapes,pt):\n\tif len(shapes):\n\t\tmindist,selected = shapes[0].distanceTo(pt).min(),0\n\t\tfor i,shape in enumerate(shapes[1:]):\n\t\t\tshpdist = shape.distanceTo(pt).min()\n\t\t\tif mindist > shpdist:\n\t\t\t\tselected = i+1\n\t\t\t\tmindist = shpdist\n\t\treturn selected\n\telse:\n\t\treturn -1\n\ndef displayAllShapes(disp,shapes,selected,typing_mode):\n\tfor i,shape in enumerate(shapes):\n\t\tcolor = (255,255,255) if i != selected else (0,0,255)\n\t\ttxtcolor = (0,0,255) if (i == selected and typing_mode) else (255,255,255)\n\t\tshape.draw(disp.drawLine,disp.drawCircle,disp.drawText,color=color,txtcolor=txtcolor)\n\n\nif __name__ == '__main__':\n\n\tif len(sys.argv) < 4:\n\t\tprint __doc__\n\t\tsys.exit()\n\n\tmaxW = int(sys.argv[1])\n\tmaxH = int(sys.argv[2])\n\timg_files = sys.argv[3:]\n\n\tmaxwh = np.array([maxW,maxH],dtype=float)\n\twname = 'Display'\n\n\t# Key ids\n\tENTER \t\t\t= 10\n\tESC \t\t\t= 27\n\tBACKSPACE \t\t= 8\n\tARROW_UP\t\t= 82\n\tARROW_DOWN\t\t= 84\n\tARROW_LEFT\t\t= 81\n\tARROW_RIGHT\t\t= 83\n\tGREATER_THAN\t= 46\n\tLESS_THAN \t\t= 44\n\tHOME \t\t\t= 80\n\n\tkey_exit \t\t\t\t\t= ESC\n\tkey_next \t\t\t\t\t= [ord('n'),ENTER]\n\tkey_previous \t\t\t\t= ord('p')\n\tkey_zoom_in\t\t\t\t\t= ord('q')\n\tkey_zoom_out\t\t\t\t= ord('w')\n\tkey_append_vertex \t\t\t= ord('a')\n\tkey_remove_last_vertex  \t= ord('d')\n\tkey_change_closest_vertex\t= ord('s')\n\tkey_create_new_shape \t\t= ord('c')\n\tkey_select_closest_shape \t= ord('x')\n\tkey_shift_pts \t\t\t\t= ord('g')\n\tkey_typing_mode \t\t\t= ord(' ')\n\tkey_delete_selected_shape\t= [ord('r')]\n\n\tkey_pitch_increase \t\t\t= ARROW_DOWN\n\tkey_pitch_decrease \t\t\t= ARROW_UP\n\tkey_yaw_increase \t\t\t= ARROW_LEFT\n\tkey_yaw_decrease\t\t\t= ARROW_RIGHT\n\tkey_roll_increase \t\t\t= GREATER_THAN\n\tkey_roll_decrease \t\t\t= LESS_THAN\n\n\tkey_perspective_reset\t\t= HOME\n\tzoom_factor = 1.\n\n\taction_keys = [key_exit,key_previous] + key_next\n\n\tcurr_image = 0\n\twhile curr_image < len(img_files):\n\n\t\timg_file = img_files[curr_image]\n\n\t\tlab_file = splitext(img_file)[0] + '.txt'\n\n\t\tif isfile(lab_file):\n\t\t\tshapes = readShapes(lab_file,obj_type=ShapeDisplay)\n\t\t\tselected = len(shapes) - 1\n\t\telse:\n\t\t\tshapes,selected = [ShapeDisplay()],0\n\t\t\n\t\tzoom_factor = 1.\n\n\t\tdisp = Display(cv2.imread(img_file),maxW,maxH)\n\t\tdisp.show()\n\t\tkey = disp.waitKey()\n\t\ttyping_mode = False\n\n\t\twhile not key in action_keys:\n\t\t\tdisp.resetDisplay()\n\t\t\tdisplayAllShapes(disp,shapes,selected,typing_mode)\n\t\t\tdisp.show()\n\t\t\tkey = disp.waitKey(10)\n\n\t\t\tif typing_mode:\n\t\t\t\tif key == key_typing_mode:\n\t\t\t\t\ttyping_mode = False\n\t\t\t\telse:\n\t\t\t\t\tif key != 255:\n\t\t\t\t\t\tif key >= 176:\n\t\t\t\t\t\t\tkey = key - 176 + 48\n\t\t\t\t\t\tif key == BACKSPACE: # backspace\n\t\t\t\t\t\t\tshapes[selected].text = shapes[selected].text[:-1]\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tshapes[selected].text += str(chr(key)).upper()\n\t\t\t\tkey = 255\n\t\t\t\tcontinue\n\n\t\t\tif key == key_zoom_in:\n\t\t\t\tdisp.zoom(1.5)\n\t\t\tif key == key_zoom_out:\n\t\t\t\tdisp.zoom(.5)\n\t\t\tif key == key_yaw_increase:\n\t\t\t\tdisp.setPerspectiveAngle(addy=10.)\n\t\t\tif key == key_yaw_decrease:\n\t\t\t\tdisp.setPerspectiveAngle(addy=-10.)\n\t\t\tif key == key_pitch_increase:\n\t\t\t\tdisp.setPerspectiveAngle(addx=10.)\n\t\t\tif key == key_pitch_decrease:\n\t\t\t\tdisp.setPerspectiveAngle(addx=-10.)\n\t\t\tif key == key_roll_increase:\n\t\t\t\tdisp.setPerspectiveAngle(addz=10.)\n\t\t\tif key == key_roll_decrease:\n\t\t\t\tdisp.setPerspectiveAngle(addz=-10.)\n\t\t\tif key == key_perspective_reset:\n\t\t\t\tdisp.reset_view()\n\n\t\t\tif len(shapes):\n\n\t\t\t\tif key == ord('l'):\n\t\t\t\t\tdisp.rectifyToPts(shapes[selected].pts)\n\n\t\t\t\tif key == key_typing_mode:\n\t\t\t\t\ttyping_mode = True\n\n\t\t\t\tif key == key_append_vertex:\n\t\t\t\t\tprint 'Append vertex'\n\t\t\t\t\tshapes[selected].appendSide(disp.getMouseCenterRelative())\n\n\t\t\t\tif key == key_remove_last_vertex:\n\t\t\t\t\tprint 'Remove last vertex'\n\t\t\t\t\tshapes[selected].removeLast()\n\n\t\t\t\tif key == key_change_closest_vertex:\n\t\t\t\t\tprint 'Change closest vertex'\n\t\t\t\t\tshapes[selected].changeClosest(disp.getMouseCenterRelative())\n\n\t\t\t\tif key in key_delete_selected_shape:\n\t\t\t\t\tprint 'Delete closest vertex'\n\t\t\t\t\tdel shapes[selected]\n\t\t\t\t\tpt = disp.getMouseCenterRelative()\n\t\t\t\t\tselected = selectClosest(shapes,pt)\n\n\t\t\t\tif key == key_shift_pts:\n\t\t\t\t\tshapes[selected].shiftPts()\n\n\t\t\tif key == key_create_new_shape:\n\t\t\t\tprint 'Create new shape'\n\t\t\t\tshapes.append(ShapeDisplay())\n\t\t\t\tselected = len(shapes)-1\n\n\t\t\tif key == key_select_closest_shape:\n\t\t\t\tprint 'Select closest'\n\t\t\t\tpt = disp.getMouseCenterRelative()\n\t\t\t\tselected = selectClosest(shapes,pt)\n\t\t\t\t\t\n\t\tif key == key_exit:\n\t\t\tsys.exit()\n\n\t\tif key in ([key_previous] + key_next):\n\t\t\twriteShapes(lab_file,shapes)\n\t\t\tcurr_image += 1 if key in key_next else -1\n\t\t\tcurr_image = max(curr_image,0)\n\t\t\tcontinue\n"""
create-model.py,0,"b""\nimport sys\nimport keras\n\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Add, Activation, Concatenate, Input\nfrom keras.models import Model\nfrom keras.applications.mobilenet import MobileNet\n\nfrom src.keras_utils import save_model\n\n\ndef res_block(x,sz,filter_sz=3,in_conv_size=1):\n\txi  = x\n\tfor i in range(in_conv_size):\n\t\txi  = Conv2D(sz, filter_sz, activation='linear', padding='same')(xi)\n\t\txi  = BatchNormalization()(xi)\n\t\txi \t= Activation('relu')(xi)\n\txi  = Conv2D(sz, filter_sz, activation='linear', padding='same')(xi)\n\txi  = BatchNormalization()(xi)\n\txi \t= Add()([xi,x])\n\txi \t= Activation('relu')(xi)\n\treturn xi\n\ndef conv_batch(_input,fsz,csz,activation='relu',padding='same',strides=(1,1)):\n\toutput = Conv2D(fsz, csz, activation='linear', padding=padding, strides=strides)(_input)\n\toutput = BatchNormalization()(output)\n\toutput = Activation(activation)(output)\n\treturn output\n\ndef end_block(x):\n\txprobs    = Conv2D(2, 3, activation='softmax', padding='same')(x)\n\txbbox     = Conv2D(6, 3, activation='linear' , padding='same')(x)\n\treturn Concatenate(3)([xprobs,xbbox])\n\n\ndef create_model_eccv():\n\t\n\tinput_layer = Input(shape=(None,None,3),name='input')\n\n\tx = conv_batch(input_layer, 16, 3)\n\tx = conv_batch(x, 16, 3)\n\tx = MaxPooling2D(pool_size=(2,2))(x)\n\tx = conv_batch(x, 32, 3)\n\tx = res_block(x, 32)\n\tx = MaxPooling2D(pool_size=(2,2))(x)\n\tx = conv_batch(x, 64, 3)\n\tx = res_block(x,64)\n\tx = res_block(x,64)\n\tx = MaxPooling2D(pool_size=(2,2))(x)\n\tx = conv_batch(x, 64, 3)\n\tx = res_block(x,64)\n\tx = res_block(x,64)\n\tx = MaxPooling2D(pool_size=(2,2))(x)\n\tx = conv_batch(x, 128, 3)\n\tx = res_block(x,128)\n\tx = res_block(x,128)\n\tx = res_block(x,128)\n\tx = res_block(x,128)\n\n\tx = end_block(x)\n\n\treturn Model(inputs=input_layer,outputs=x)\n\n\n# Model not converging...\ndef create_model_mobnet():\n\n\tinput_layer = Input(shape=(None,None,3),name='input')\n\tx = input_layer\n\n\tmbnet = MobileNet(input_shape=(224,224,3),include_top=True)\n\t\n\tbackbone = keras.models.clone_model(mbnet)\n\tfor i,bblayer in enumerate(backbone.layers[1:74]):\n\t\tlayer = bblayer.__class__.from_config(bblayer.get_config())\n\t\tlayer.name = 'backbone_' + layer.name\n\t\tx = layer(x)\n\n\tx = end_block(x)\n\n\tmodel = Model(inputs=input_layer,outputs=x)\n\n\tbackbone_layers = {'backbone_' + layer.name: layer for layer in backbone.layers}\n\tfor layer in model.layers:\n\t\tif layer.name in backbone_layers:\n\t\t\tprint 'setting ' + layer.name\n\t\t\tlayer.set_weights(backbone_layers[layer.name].get_weights())\n\n\treturn model\n\n\nif __name__ == '__main__':\n\n\tmodules = [func.replace('create_model_','') for func in dir(sys.modules[__name__]) if 'create_model_' in func]\n\n\tassert sys.argv[1] in modules, \\\n\t\t'Model name must be on of the following: %s' % ', '.join(modules)\n\n\tmodelf = getattr(sys.modules[__name__],'create_model_' + sys.argv[1])\n\t\n\tprint 'Creating model %s' % sys.argv[1]\n\tmodel = modelf()\n\tprint 'Finished'\n\n\tprint 'Saving at %s' % sys.argv[2]\n\tsave_model(model,sys.argv[2])\n\n"""
gen-outputs.py,0,"b""import sys\nimport cv2\nimport numpy as np\n\nfrom glob\t\t\t\t\t\timport glob\nfrom os.path \t\t\t\t\timport splitext, basename, isfile\nfrom src.utils \t\t\t\t\timport crop_region, image_files_from_folder\nfrom src.drawing_utils\t\t\timport draw_label, draw_losangle, write2img\nfrom src.label \t\t\t\t\timport lread, Label, readShapes\n\nfrom pdb import set_trace as pause\n\n\nYELLOW = (  0,255,255)\nRED    = (  0,  0,255)\n\ninput_dir = sys.argv[1]\noutput_dir = sys.argv[2]\n\nimg_files = image_files_from_folder(input_dir)\n\nfor img_file in img_files:\n\n\tbname = splitext(basename(img_file))[0]\n\n\tI = cv2.imread(img_file)\n\n\tdetected_cars_labels = '%s/%s_cars.txt' % (output_dir,bname)\n\n\tLcar = lread(detected_cars_labels)\n\n\tsys.stdout.write('%s' % bname)\n\n\tif Lcar:\n\n\t\tfor i,lcar in enumerate(Lcar):\n\n\t\t\tdraw_label(I,lcar,color=YELLOW,thickness=3)\n\n\t\t\tlp_label \t\t= '%s/%s_%dcar_lp.txt'\t\t% (output_dir,bname,i)\n\t\t\tlp_label_str \t= '%s/%s_%dcar_lp_str.txt'\t% (output_dir,bname,i)\n\n\t\t\tif isfile(lp_label):\n\n\t\t\t\tLlp_shapes = readShapes(lp_label)\n\t\t\t\tpts = Llp_shapes[0].pts*lcar.wh().reshape(2,1) + lcar.tl().reshape(2,1)\n\t\t\t\tptspx = pts*np.array(I.shape[1::-1],dtype=float).reshape(2,1)\n\t\t\t\tdraw_losangle(I,ptspx,RED,3)\n\n\t\t\t\tif isfile(lp_label_str):\n\t\t\t\t\twith open(lp_label_str,'r') as f:\n\t\t\t\t\t\tlp_str = f.read().strip()\n\t\t\t\t\tllp = Label(0,tl=pts.min(1),br=pts.max(1))\n\t\t\t\t\twrite2img(I,llp,lp_str)\n\n\t\t\t\t\tsys.stdout.write(',%s' % lp_str)\n\n\tcv2.imwrite('%s/%s_output.png' % (output_dir,bname),I)\n\tsys.stdout.write('\\n')\n\n\n"""
license-plate-detection.py,0,"b'import sys, os\nimport keras\nimport cv2\nimport traceback\n\nfrom src.keras_utils \t\t\timport load_model\nfrom glob \t\t\t\t\t\timport glob\nfrom os.path \t\t\t\t\timport splitext, basename\nfrom src.utils \t\t\t\t\timport im2single\nfrom src.keras_utils \t\t\timport load_model, detect_lp\nfrom src.label \t\t\t\t\timport Shape, writeShapes\n\n\ndef adjust_pts(pts,lroi):\n\treturn pts*lroi.wh().reshape((2,1)) + lroi.tl().reshape((2,1))\n\n\nif __name__ == \'__main__\':\n\n\ttry:\n\t\t\n\t\tinput_dir  = sys.argv[1]\n\t\toutput_dir = input_dir\n\n\t\tlp_threshold = .5\n\n\t\twpod_net_path = sys.argv[2]\n\t\twpod_net = load_model(wpod_net_path)\n\n\t\timgs_paths = glob(\'%s/*car.png\' % input_dir)\n\n\t\tprint \'Searching for license plates using WPOD-NET\'\n\n\t\tfor i,img_path in enumerate(imgs_paths):\n\n\t\t\tprint \'\\t Processing %s\' % img_path\n\n\t\t\tbname = splitext(basename(img_path))[0]\n\t\t\tIvehicle = cv2.imread(img_path)\n\n\t\t\tratio = float(max(Ivehicle.shape[:2]))/min(Ivehicle.shape[:2])\n\t\t\tside  = int(ratio*288.)\n\t\t\tbound_dim = min(side + (side%(2**4)),608)\n\t\t\tprint ""\\t\\tBound dim: %d, ratio: %f"" % (bound_dim,ratio)\n\n\t\t\tLlp,LlpImgs,_ = detect_lp(wpod_net,im2single(Ivehicle),bound_dim,2**4,(240,80),lp_threshold)\n\n\t\t\tif len(LlpImgs):\n\t\t\t\tIlp = LlpImgs[0]\n\t\t\t\tIlp = cv2.cvtColor(Ilp, cv2.COLOR_BGR2GRAY)\n\t\t\t\tIlp = cv2.cvtColor(Ilp, cv2.COLOR_GRAY2BGR)\n\n\t\t\t\ts = Shape(Llp[0].pts)\n\n\t\t\t\tcv2.imwrite(\'%s/%s_lp.png\' % (output_dir,bname),Ilp*255.)\n\t\t\t\twriteShapes(\'%s/%s_lp.txt\' % (output_dir,bname),[s])\n\n\texcept:\n\t\ttraceback.print_exc()\n\t\tsys.exit(1)\n\n\tsys.exit(0)\n\n\n'"
license-plate-ocr.py,1,"b""import sys\nimport cv2\nimport numpy as np\nimport traceback\n\nimport darknet.python.darknet as dn\n\nfrom os.path \t\t\t\timport splitext, basename\nfrom glob\t\t\t\t\timport glob\nfrom darknet.python.darknet import detect\nfrom src.label\t\t\t\timport dknet_label_conversion\nfrom src.utils \t\t\t\timport nms\n\n\nif __name__ == '__main__':\n\n\ttry:\n\t\n\t\tinput_dir  = sys.argv[1]\n\t\toutput_dir = input_dir\n\n\t\tocr_threshold = .4\n\n\t\tocr_weights = 'data/ocr/ocr-net.weights'\n\t\tocr_netcfg  = 'data/ocr/ocr-net.cfg'\n\t\tocr_dataset = 'data/ocr/ocr-net.data'\n\n\t\tocr_net  = dn.load_net(ocr_netcfg, ocr_weights, 0)\n\t\tocr_meta = dn.load_meta(ocr_dataset)\n\n\t\timgs_paths = sorted(glob('%s/*lp.png' % output_dir))\n\n\t\tprint 'Performing OCR...'\n\n\t\tfor i,img_path in enumerate(imgs_paths):\n\n\t\t\tprint '\\tScanning %s' % img_path\n\n\t\t\tbname = basename(splitext(img_path)[0])\n\n\t\t\tR,(width,height) = detect(ocr_net, ocr_meta, img_path ,thresh=ocr_threshold, nms=None)\n\n\t\t\tif len(R):\n\n\t\t\t\tL = dknet_label_conversion(R,width,height)\n\t\t\t\tL = nms(L,.45)\n\n\t\t\t\tL.sort(key=lambda x: x.tl()[0])\n\t\t\t\tlp_str = ''.join([chr(l.cl()) for l in L])\n\n\t\t\t\twith open('%s/%s_str.txt' % (output_dir,bname),'w') as f:\n\t\t\t\t\tf.write(lp_str + '\\n')\n\n\t\t\t\tprint '\\t\\tLP: %s' % lp_str\n\n\t\t\telse:\n\n\t\t\t\tprint 'No characters found'\n\n\texcept:\n\t\ttraceback.print_exc()\n\t\tsys.exit(1)\n\n\tsys.exit(0)\n"""
train-detector.py,0,"b""\nimport sys\nimport numpy as np\nimport cv2\nimport argparse\nimport keras\n\nfrom random import choice\nfrom os.path import isfile, isdir, basename, splitext\nfrom os import makedirs\n\nfrom src.keras_utils import save_model, load_model\nfrom src.label import readShapes\nfrom src.loss import loss\nfrom src.utils import image_files_from_folder, show\nfrom src.sampler import augment_sample, labels2output_map\nfrom src.data_generator import DataGenerator\n\nfrom pdb import set_trace as pause\n\n\ndef load_network(modelpath,input_dim):\n\n\tmodel = load_model(modelpath)\n\tinput_shape = (input_dim,input_dim,3)\n\n\t# Fixed input size for training\n\tinputs  = keras.layers.Input(shape=(input_dim,input_dim,3))\n\toutputs = model(inputs)\n\n\toutput_shape = tuple([s.value for s in outputs.shape[1:]])\n\toutput_dim   = output_shape[1]\n\tmodel_stride = input_dim / output_dim\n\n\tassert input_dim % output_dim == 0, \\\n\t\t'The output resolution must be divisible by the input resolution'\n\n\tassert model_stride == 2**4, \\\n\t\t'Make sure your model generates a feature map with resolution ' \\\n\t\t'16x smaller than the input'\n\n\treturn model, model_stride, input_shape, output_shape\n\ndef process_data_item(data_item,dim,model_stride):\n\tXX,llp,pts = augment_sample(data_item[0],data_item[1].pts,dim)\n\tYY = labels2output_map(llp,pts,dim,model_stride)\n\treturn XX,YY\n\n\nif __name__ == '__main__':\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-m' \t\t,'--model'\t\t\t,type=str   , required=True\t\t,help='Path to previous model')\n\tparser.add_argument('-n' \t\t,'--name'\t\t\t,type=str   , required=True\t\t,help='Model name')\n\tparser.add_argument('-tr'\t\t,'--train-dir'\t\t,type=str   , required=True\t\t,help='Input data directory for training')\n\tparser.add_argument('-its'\t\t,'--iterations'\t\t,type=int   , default=300000\t,help='Number of mini-batch iterations (default = 300.000)')\n\tparser.add_argument('-bs'\t\t,'--batch-size'\t\t,type=int   , default=32\t\t,help='Mini-batch size (default = 32)')\n\tparser.add_argument('-od'\t\t,'--output-dir'\t\t,type=str   , default='./'\t\t,help='Output directory (default = ./)')\n\tparser.add_argument('-op'\t\t,'--optimizer'\t\t,type=str   , default='Adam'\t,help='Optmizer (default = Adam)')\n\tparser.add_argument('-lr'\t\t,'--learning-rate'\t,type=float , default=.01\t\t,help='Optmizer (default = 0.01)')\n\targs = parser.parse_args()\n\n\tnetname \t= basename(args.name)\n\ttrain_dir \t= args.train_dir\n\toutdir \t\t= args.output_dir\n\n\titerations \t= args.iterations\n\tbatch_size \t= args.batch_size\n\tdim \t\t= 208\n\n\tif not isdir(outdir):\n\t\tmakedirs(outdir)\n\n\tmodel,model_stride,xshape,yshape = load_network(args.model,dim)\n\n\topt = getattr(keras.optimizers,args.optimizer)(lr=args.learning_rate)\n\tmodel.compile(loss=loss, optimizer=opt)\n\n\tprint 'Checking input directory...'\n\tFiles = image_files_from_folder(train_dir)\n\n\tData = []\n\tfor file in Files:\n\t\tlabfile = splitext(file)[0] + '.txt'\n\t\tif isfile(labfile):\n\t\t\tL = readShapes(labfile)\n\t\t\tI = cv2.imread(file)\n\t\t\tData.append([I,L[0]])\n\n\tprint '%d images with labels found' % len(Data)\n\n\tdg = DataGenerator(\tdata=Data, \\\n\t\t\t\t\t\tprocess_data_item_func=lambda x: process_data_item(x,dim,model_stride),\\\n\t\t\t\t\t\txshape=xshape, \\\n\t\t\t\t\t\tyshape=(yshape[0],yshape[1],yshape[2]+1), \\\n\t\t\t\t\t\tnthreads=2, \\\n\t\t\t\t\t\tpool_size=1000, \\\n\t\t\t\t\t\tmin_nsamples=100 )\n\tdg.start()\n\n\tXtrain = np.empty((batch_size,dim,dim,3),dtype='single')\n\tYtrain = np.empty((batch_size,dim/model_stride,dim/model_stride,2*4+1))\n\n\tmodel_path_backup = '%s/%s_backup' % (outdir,netname)\n\tmodel_path_final  = '%s/%s_final'  % (outdir,netname)\n\n\tfor it in range(iterations):\n\n\t\tprint 'Iter. %d (of %d)' % (it+1,iterations)\n\n\t\tXtrain,Ytrain = dg.get_batch(batch_size)\n\t\ttrain_loss = model.train_on_batch(Xtrain,Ytrain)\n\n\t\tprint '\\tLoss: %f' % train_loss\n\n\t\t# Save model every 1000 iterations\n\t\tif (it+1) % 1000 == 0:\n\t\t\tprint 'Saving model (%s)' % model_path_backup\n\t\t\tsave_model(model,model_path_backup)\n\n\tprint 'Stopping data generator'\n\tdg.stop()\n\n\tprint 'Saving model (%s)' % model_path_final\n\tsave_model(model,model_path_final)\n"""
vehicle-detection.py,0,"b""import sys\nimport cv2\nimport numpy as np\nimport traceback\n\nimport darknet.python.darknet as dn\n\nfrom src.label \t\t\t\timport Label, lwrite\nfrom os.path \t\t\t\timport splitext, basename, isdir\nfrom os \t\t\t\t\timport makedirs\nfrom src.utils \t\t\t\timport crop_region, image_files_from_folder\nfrom darknet.python.darknet import detect\n\n\nif __name__ == '__main__':\n\n\ttry:\n\t\n\t\tinput_dir  = sys.argv[1]\n\t\toutput_dir = sys.argv[2]\n\n\t\tvehicle_threshold = .5\n\n\t\tvehicle_weights = 'data/vehicle-detector/yolo-voc.weights'\n\t\tvehicle_netcfg  = 'data/vehicle-detector/yolo-voc.cfg'\n\t\tvehicle_dataset = 'data/vehicle-detector/voc.data'\n\n\t\tvehicle_net  = dn.load_net(vehicle_netcfg, vehicle_weights, 0)\n\t\tvehicle_meta = dn.load_meta(vehicle_dataset)\n\n\t\timgs_paths = image_files_from_folder(input_dir)\n\t\timgs_paths.sort()\n\n\t\tif not isdir(output_dir):\n\t\t\tmakedirs(output_dir)\n\n\t\tprint 'Searching for vehicles using YOLO...'\n\n\t\tfor i,img_path in enumerate(imgs_paths):\n\n\t\t\tprint '\\tScanning %s' % img_path\n\n\t\t\tbname = basename(splitext(img_path)[0])\n\n\t\t\tR,_ = detect(vehicle_net, vehicle_meta, img_path ,thresh=vehicle_threshold)\n\n\t\t\tR = [r for r in R if r[0] in ['car','bus']]\n\n\t\t\tprint '\\t\\t%d cars found' % len(R)\n\n\t\t\tif len(R):\n\n\t\t\t\tIorig = cv2.imread(img_path)\n\t\t\t\tWH = np.array(Iorig.shape[1::-1],dtype=float)\n\t\t\t\tLcars = []\n\n\t\t\t\tfor i,r in enumerate(R):\n\n\t\t\t\t\tcx,cy,w,h = (np.array(r[2])/np.concatenate( (WH,WH) )).tolist()\n\t\t\t\t\ttl = np.array([cx - w/2., cy - h/2.])\n\t\t\t\t\tbr = np.array([cx + w/2., cy + h/2.])\n\t\t\t\t\tlabel = Label(0,tl,br)\n\t\t\t\t\tIcar = crop_region(Iorig,label)\n\n\t\t\t\t\tLcars.append(label)\n\n\t\t\t\t\tcv2.imwrite('%s/%s_%dcar.png' % (output_dir,bname,i),Icar)\n\n\t\t\t\tlwrite('%s/%s_cars.txt' % (output_dir,bname),Lcars)\n\n\texcept:\n\t\ttraceback.print_exc()\n\t\tsys.exit(1)\n\n\tsys.exit(0)\n\t"""
darknet/__init__.py,0,b''
src/__init__.py,0,b''
src/data_generator.py,0,"b""\nimport numpy as np\n\nfrom threading import Semaphore, Thread\nfrom time import sleep\nfrom random import choice, randint\nfrom pdb import set_trace as pause\n\nclass DataGenerator(object):\n\n\tdef __init__(\tself, data, process_data_item_func, xshape, yshape, \\\n\t\t\t\t\tdata_item_selector\t= choice, \t\\\n\t\t\t\t\tnthreads\t\t\t= 2,\t\t\\\n\t\t\t\t\tpool_size\t\t\t= 1000,\t\t\\\n\t\t\t\t\tmin_nsamples\t\t= 1,\t\t\\\n\t\t\t\t\tdtype \t\t\t\t= 'single' ):\n\n\t\tassert pool_size >= min_nsamples, \\\n\t\t\t'Min. samples must be equal or less than pool_size'\n\t\tassert min_nsamples > 0 and pool_size > 0, \\\n\t\t\t'Min. samples and pool size must be positive non-zero numbers'\n\n\t\tself._data = data\n\t\tself._process_data_item = process_data_item_func\n\t\tself._data_item_selector = data_item_selector\n\t\tself._xshape = xshape\n\t\tself._yshape = yshape\n\t\tself._nthreads = nthreads\n\t\tself._pool_size = pool_size\n\t\tself._min_nsamples = min_nsamples\n\t\tself._dtype = dtype\n\t\t\n\t\tself._count = 0\n\t\tself._stop = False\n\t\tself._threads = []\n\t\tself._sem = Semaphore()\n\n\t\tself._X, self._Y = self._get_buffers(self._pool_size)\n\n\n\tdef _get_buffers(self,N):\n\t\tX = np.empty((N,) + self._xshape, dtype=self._dtype)\n\t\tY = np.empty((N,) + self._yshape, dtype=self._dtype)\n\t\treturn X,Y\n\n\tdef _compute_sample(self):\n\t\td = self._data_item_selector(self._data)\n\t\treturn self._process_data_item(d)\n\n\tdef _insert_data(self,x,y):\n\n\t\tself._sem.acquire()\n\n\t\tif self._count < self._pool_size:\n\t\t\tself._X[self._count] = x\n\t\t\tself._Y[self._count] = y\n\t\t\tself._count += 1\n\t\telse:\n\t\t\tidx = randint(0,self._pool_size-1)\n\t\t\tself._X[idx] = x\n\t\t\tself._Y[idx] = y\n\n\t\tself._sem.release()\n\n\tdef _run(self):\n\t\twhile True:\n\t\t\tx,y = self._compute_sample()\n\t\t\tself._insert_data(x,y)\n\t\t\tif self._stop:\n\t\t\t\tbreak\n\n\tdef stop(self):\n\t\tself._stop = True\n\t\tfor thread in self._threads:\n\t\t\tthread.join()\n\n\tdef start(self):\n\t\tself._stop = False\n\t\tself._threads = [Thread(target=self._run) for n in range(self._nthreads)]\n\t\tfor thread in self._threads:\n\t\t\tthread.setDaemon(True)\n\t\t\tthread.start()\n\n\tdef get_batch(self,N):\n\n\t\t# Wait until the buffer was filled with the minimum\n\t\t# number of samples\n\t\twhile self._count < self._min_nsamples:\n\t\t\tsleep(.1)\n\n\t\tX,Y = self._get_buffers(N)\n\t\tself._sem.acquire()\n\t\tfor i in range(N):\n\t\t\tidx = randint(0,self._count-1)\n\t\t\tX[i] = self._X[idx]\n\t\t\tY[i] = self._Y[idx]\n\t\tself._sem.release()\n\t\treturn X,Y\n\n\n"""
src/drawing_utils.py,0,"b'\nimport numpy as np\nimport cv2\n\n\ndef draw_label(I,l,color=(255,0,0),thickness=1):\n\twh = np.array(I.shape[1::-1]).astype(float)\n\ttl = tuple((l.tl()*wh).astype(int).tolist())\n\tbr = tuple((l.br()*wh).astype(int).tolist())\n\tcv2.rectangle(I,tl,br,color,thickness=thickness)\n\n\ndef draw_losangle(I,pts,color=(1.,1.,1.),thickness=1):\n\tassert(pts.shape[0] == 2 and pts.shape[1] == 4)\n\n\tfor i in range(4):\n\t\tpt1 = tuple(pts[:,i].astype(int).tolist())\n\t\tpt2 = tuple(pts[:,(i+1)%4].astype(int).tolist())\n\t\tcv2.line(I,pt1,pt2,color,thickness)\n\n\ndef write2img(Img,label,strg,txt_color=(0,0,0),bg_color=(255,255,255),font_size=1):\n\twh_img = np.array(Img.shape[1::-1])\n\n\tfont = cv2.FONT_HERSHEY_SIMPLEX\n\n\twh_text,v = cv2.getTextSize(strg, font, font_size, 3)\n\tbl_corner = label.tl()*wh_img\n\n\ttl_corner = np.array([bl_corner[0],bl_corner[1]-wh_text[1]])/wh_img\n\tbr_corner = np.array([bl_corner[0]+wh_text[0],bl_corner[1]])/wh_img\n\tbl_corner /= wh_img\n\n\tif (tl_corner < 0.).any():\n\t\tdelta = 0. - np.minimum(tl_corner,0.)\n\telif (br_corner > 1.).any():\n\t\tdelta = 1. - np.maximum(br_corner,1.)\n\telse:\n\t\tdelta = 0.\n\n\ttl_corner += delta\n\tbr_corner += delta\n\tbl_corner += delta\n\n\ttpl = lambda x: tuple((x*wh_img).astype(int).tolist())\n\n\tcv2.rectangle(Img, tpl(tl_corner), tpl(br_corner), bg_color, -1)\t\n\tcv2.putText(Img,strg,tpl(bl_corner),font,font_size,txt_color,3)'"
src/keras_utils.py,0,"b""\nimport numpy as np\nimport cv2\nimport time\n\nfrom os.path import splitext\n\nfrom src.label import Label\nfrom src.utils import getWH, nms\nfrom src.projection_utils import getRectPts, find_T_matrix\n\n\nclass DLabel (Label):\n\n\tdef __init__(self,cl,pts,prob):\n\t\tself.pts = pts\n\t\ttl = np.amin(pts,1)\n\t\tbr = np.amax(pts,1)\n\t\tLabel.__init__(self,cl,tl,br,prob)\n\ndef save_model(model,path,verbose=0):\n\tpath = splitext(path)[0]\n\tmodel_json = model.to_json()\n\twith open('%s.json' % path,'w') as json_file:\n\t\tjson_file.write(model_json)\n\tmodel.save_weights('%s.h5' % path)\n\tif verbose: print 'Saved to %s' % path\n\ndef load_model(path,custom_objects={},verbose=0):\n\tfrom keras.models import model_from_json\n\n\tpath = splitext(path)[0]\n\twith open('%s.json' % path,'r') as json_file:\n\t\tmodel_json = json_file.read()\n\tmodel = model_from_json(model_json, custom_objects=custom_objects)\n\tmodel.load_weights('%s.h5' % path)\n\tif verbose: print 'Loaded from %s' % path\n\treturn model\n\n\ndef reconstruct(Iorig,I,Y,out_size,threshold=.9):\n\n\tnet_stride \t= 2**4\n\tside \t\t= ((208. + 40.)/2.)/net_stride # 7.75\n\n\tProbs = Y[...,0]\n\tAffines = Y[...,2:]\n\trx,ry = Y.shape[:2]\n\tywh = Y.shape[1::-1]\n\tiwh = np.array(I.shape[1::-1],dtype=float).reshape((2,1))\n\n\txx,yy = np.where(Probs>threshold)\n\n\tWH = getWH(I.shape)\n\tMN = WH/net_stride\n\n\tvxx = vyy = 0.5 #alpha\n\n\tbase = lambda vx,vy: np.matrix([[-vx,-vy,1.],[vx,-vy,1.],[vx,vy,1.],[-vx,vy,1.]]).T\n\tlabels = []\n\n\tfor i in range(len(xx)):\n\t\ty,x = xx[i],yy[i]\n\t\taffine = Affines[y,x]\n\t\tprob = Probs[y,x]\n\n\t\tmn = np.array([float(x) + .5,float(y) + .5])\n\n\t\tA = np.reshape(affine,(2,3))\n\t\tA[0,0] = max(A[0,0],0.)\n\t\tA[1,1] = max(A[1,1],0.)\n\n\t\tpts = np.array(A*base(vxx,vyy)) #*alpha\n\t\tpts_MN_center_mn = pts*side\n\t\tpts_MN = pts_MN_center_mn + mn.reshape((2,1))\n\n\t\tpts_prop = pts_MN/MN.reshape((2,1))\n\n\t\tlabels.append(DLabel(0,pts_prop,prob))\n\n\tfinal_labels = nms(labels,.1)\n\tTLps = []\n\n\tif len(final_labels):\n\t\tfinal_labels.sort(key=lambda x: x.prob(), reverse=True)\n\t\tfor i,label in enumerate(final_labels):\n\n\t\t\tt_ptsh \t= getRectPts(0,0,out_size[0],out_size[1])\n\t\t\tptsh \t= np.concatenate((label.pts*getWH(Iorig.shape).reshape((2,1)),np.ones((1,4))))\n\t\t\tH \t\t= find_T_matrix(ptsh,t_ptsh)\n\t\t\tIlp \t= cv2.warpPerspective(Iorig,H,out_size,borderValue=.0)\n\n\t\t\tTLps.append(Ilp)\n\n\treturn final_labels,TLps\n\t\n\ndef detect_lp(model,I,max_dim,net_step,out_size,threshold):\n\n\tmin_dim_img = min(I.shape[:2])\n\tfactor \t\t= float(max_dim)/min_dim_img\n\n\tw,h = (np.array(I.shape[1::-1],dtype=float)*factor).astype(int).tolist()\n\tw += (w%net_step!=0)*(net_step - w%net_step)\n\th += (h%net_step!=0)*(net_step - h%net_step)\n\tIresized = cv2.resize(I,(w,h))\n\n\tT = Iresized.copy()\n\tT = T.reshape((1,T.shape[0],T.shape[1],T.shape[2]))\n\n\tstart \t= time.time()\n\tYr \t\t= model.predict(T)\n\tYr \t\t= np.squeeze(Yr)\n\telapsed = time.time() - start\n\n\tL,TLps = reconstruct(I,Iresized,Yr,out_size,threshold)\n\n\treturn L,TLps,elapsed"""
src/label.py,0,"b""\nimport numpy as np\n\nfrom os.path import isfile\n\n\nclass Label:\n\n\tdef __init__(self,cl=-1,tl=np.array([0.,0.]),br=np.array([0.,0.]),prob=None):\n\t\tself.__tl \t= tl\n\t\tself.__br \t= br\n\t\tself.__cl \t= cl\n\t\tself.__prob = prob\n\n\tdef __str__(self):\n\t\treturn 'Class: %d, top_left(x:%f,y:%f), bottom_right(x:%f,y:%f)' % (self.__cl, self.__tl[0], self.__tl[1], self.__br[0], self.__br[1])\n\n\tdef copy(self):\n\t\treturn Label(self.__cl,self.__tl,self.__br)\n\n\tdef wh(self): return self.__br-self.__tl\n\n\tdef cc(self): return self.__tl + self.wh()/2\n\n\tdef tl(self): return self.__tl\n \n\tdef br(self): return self.__br\n\n\tdef tr(self): return np.array([self.__br[0],self.__tl[1]])\n\n\tdef bl(self): return np.array([self.__tl[0],self.__br[1]])\n\n\tdef cl(self): return self.__cl\n\n\tdef area(self): return np.prod(self.wh())\n\n\tdef prob(self): return self.__prob\n\n\tdef set_class(self,cl):\n\t\tself.__cl = cl\n\n\tdef set_tl(self,tl):\n\t\tself.__tl = tl\n\n\tdef set_br(self,br):\n\t\tself.__br = br\n\n\tdef set_wh(self,wh):\n\t\tcc = self.cc()\n\t\tself.__tl = cc - .5*wh\n\t\tself.__br = cc + .5*wh\n\n\tdef set_prob(self,prob):\n\t\tself.__prob = prob\n\n\ndef lread(file_path,label_type=Label):\n\n\tif not isfile(file_path):\n\t\treturn []\n\n\tobjs = []\n\twith open(file_path,'r') as fd:\n\t\tfor line in fd:\n\t\t\tv \t\t= line.strip().split()\n\t\t\tcl \t\t= int(v[0])\n\t\t\tccx,ccy = float(v[1]),float(v[2])\n\t\t\tw,h \t= float(v[3]),float(v[4])\n\t\t\tprob \t= float(v[5]) if len(v) == 6 else None\n\n\t\t\tcc \t= np.array([ccx,ccy])\n\t\t\twh \t= np.array([w,h])\n\n\t\t\tobjs.append(label_type(cl,cc-wh/2,cc+wh/2,prob=prob))\n\n\treturn objs\n\ndef lwrite(file_path,labels,write_probs=True):\n\twith open(file_path,'w') as fd:\n\t\tfor l in labels:\n\t\t\tcc,wh,cl,prob = (l.cc(),l.wh(),l.cl(),l.prob())\n\t\t\tif prob != None and write_probs:\n\t\t\t\tfd.write('%d %f %f %f %f %f\\n' % (cl,cc[0],cc[1],wh[0],wh[1],prob))\n\t\t\telse:\n\t\t\t\tfd.write('%d %f %f %f %f\\n' % (cl,cc[0],cc[1],wh[0],wh[1]))\n\n\ndef dknet_label_conversion(R,img_width,img_height):\n\tWH = np.array([img_width,img_height],dtype=float)\n\tL  = []\n\tfor r in R:\n\t\tcenter = np.array(r[2][:2])/WH\n\t\twh2 = (np.array(r[2][2:])/WH)*.5\n\t\tL.append(Label(ord(r[0]),tl=center-wh2,br=center+wh2,prob=r[1]))\n\treturn L\n\n\nclass Shape():\n\n\tdef __init__(self,pts=np.zeros((2,0)),max_sides=4,text=''):\n\t\tself.pts = pts\n\t\tself.max_sides = max_sides\n\t\tself.text = text\n\n\tdef isValid(self):\n\t\treturn self.pts.shape[1] > 2\n\n\tdef write(self,fp):\n\t\tfp.write('%d,' % self.pts.shape[1])\n\t\tptsarray = self.pts.flatten()\n\t\tfp.write(''.join([('%f,' % value) for value in ptsarray]))\n\t\tfp.write('%s,' % self.text)\n\t\tfp.write('\\n')\n\n\tdef read(self,line):\n\t\tdata \t\t= line.strip().split(',')\n\t\tss \t\t\t= int(data[0])\n\t\tvalues \t\t= data[1:(ss*2 + 1)]\n\t\ttext \t\t= data[(ss*2 + 1)] if len(data) >= (ss*2 + 2) else ''\n\t\tself.pts \t= np.array([float(value) for value in values]).reshape((2,ss))\n\t\tself.text   = text\n\ndef readShapes(path,obj_type=Shape):\n\tshapes = []\n\twith open(path) as fp:\n\t\tfor line in fp:\n\t\t\tshape = obj_type()\n\t\t\tshape.read(line)\n\t\t\tshapes.append(shape)\n\treturn shapes\n\ndef writeShapes(path,shapes):\n\tif len(shapes):\n\t\twith open(path,'w') as fp:\n\t\t\tfor shape in shapes:\n\t\t\t\tif shape.isValid():\n\t\t\t\t\tshape.write(fp)\n\n"""
src/loss.py,20,"b'\nimport tensorflow as tf\n\n\ndef logloss(Ptrue,Pred,szs,eps=10e-10):\n\tb,h,w,ch = szs\n\tPred = tf.clip_by_value(Pred,eps,1.)\n\tPred = -tf.log(Pred)\n\tPred = Pred*Ptrue\n\tPred = tf.reshape(Pred,(b,h*w*ch))\n\tPred = tf.reduce_sum(Pred,1)\n\treturn Pred\n\ndef l1(true,pred,szs):\n\tb,h,w,ch = szs\n\tres = tf.reshape(true-pred,(b,h*w*ch))\n\tres = tf.abs(res)\n\tres = tf.reduce_sum(res,1)\n\treturn res\n\ndef loss(Ytrue, Ypred):\n\n\tb = tf.shape(Ytrue)[0]\n\th = tf.shape(Ytrue)[1]\n\tw = tf.shape(Ytrue)[2]\n\n\tobj_probs_true = Ytrue[...,0]\n\tobj_probs_pred = Ypred[...,0]\n\n\tnon_obj_probs_true = 1. - Ytrue[...,0]\n\tnon_obj_probs_pred = Ypred[...,1]\n\n\taffine_pred\t= Ypred[...,2:]\n\tpts_true \t= Ytrue[...,1:]\n\n\taffinex = tf.stack([tf.maximum(affine_pred[...,0],0.),affine_pred[...,1],affine_pred[...,2]],3)\n\taffiney = tf.stack([affine_pred[...,3],tf.maximum(affine_pred[...,4],0.),affine_pred[...,5]],3)\n\n\tv = 0.5\n\tbase = tf.stack([[[[-v,-v,1., v,-v,1., v,v,1., -v,v,1.]]]])\n\tbase = tf.tile(base,tf.stack([b,h,w,1]))\n\n\tpts = tf.zeros((b,h,w,0))\n\n\tfor i in range(0,12,3):\n\t\trow = base[...,i:(i+3)]\n\t\tptsx = tf.reduce_sum(affinex*row,3)\n\t\tptsy = tf.reduce_sum(affiney*row,3)\n\n\t\tpts_xy = tf.stack([ptsx,ptsy],3)\n\t\tpts = (tf.concat([pts,pts_xy],3))\n\n\tflags = tf.reshape(obj_probs_true,(b,h,w,1))\n\tres   = 1.*l1(pts_true*flags,pts*flags,(b,h,w,4*2))\n\tres  += 1.*logloss(obj_probs_true,obj_probs_pred,(b,h,w,1))\n\tres  += 1.*logloss(non_obj_probs_true,non_obj_probs_pred,(b,h,w,1))\n\treturn res'"
src/projection_utils.py,0,"b'\nimport numpy as np\n\nfrom math import sin, cos\n\n\ndef find_T_matrix(pts,t_pts):\n\tA = np.zeros((8,9))\n\tfor i in range(0,4):\n\t\txi  = pts[:,i];\n\t\txil = t_pts[:,i];\n\t\txi  = xi.T\n\t\t\n\t\tA[i*2,   3:6] = -xil[2]*xi\n\t\tA[i*2,   6: ] =  xil[1]*xi\n\t\tA[i*2+1,  :3] =  xil[2]*xi\n\t\tA[i*2+1, 6: ] = -xil[0]*xi\n\n\t\n\t[U,S,V] = np.linalg.svd(A)\n\tH = V[-1,:].reshape((3,3))\n\n\treturn H\n\ndef getRectPts(tlx,tly,brx,bry):\n\treturn np.matrix([[tlx,brx,brx,tlx],[tly,tly,bry,bry],[1.,1.,1.,1.]],dtype=float)\n\ndef perspective_transform(wh,angles=np.array([0.,0.,0.]),zcop=1000., dpp=1000.):\n\trads = np.deg2rad(angles)\n\n\ta = rads[0]; Rx = np.matrix([[1, 0, 0]\t\t\t\t, [0, cos(a), sin(a)]\t, [0, -sin(a), cos(a)]\t])\n\ta = rads[1]; Ry = np.matrix([[cos(a), 0, -sin(a)]\t, [0, 1, 0]\t\t\t\t, [sin(a), 0, cos(a)]\t])\n\ta = rads[2]; Rz = np.matrix([[cos(a), sin(a), 0]\t, [-sin(a), cos(a), 0]\t, [0, 0, 1]\t\t\t\t])\n\n\tR = Rx*Ry*Rz;\n\n\t(w,h) = tuple(wh)\n\txyz = np.matrix([[0,0,w,w],[0,h,0,h],[0,0,0,0]])\n\thxy = np.matrix([[0,0,w,w],[0,h,0,h],[1,1,1,1]])\n\n\txyz = xyz - np.matrix([[w],[h],[0]])/2.\n\txyz = R*xyz\n\n\txyz = xyz - np.matrix([[0],[0],[zcop]])\n\thxyz = np.concatenate([xyz,np.ones((1,4))])\n\n\tP = np.matrix([[1,0,0,0],[0,1,0,0],[0,0,-1./dpp,0]])\n\t_hxy = P*hxyz\n\t_hxy = _hxy/_hxy[2,:]\n\t_hxy = _hxy + np.matrix([[w],[h],[0]])/2.\n\t\n\treturn find_T_matrix(hxy,_hxy)'"
src/sampler.py,0,"b""\nimport cv2\nimport numpy as np\nimport random\n\nfrom src.utils \timport im2single, getWH, hsv_transform, IOU_centre_and_dims\nfrom src.label\timport Label\nfrom src.projection_utils import perspective_transform, find_T_matrix, getRectPts\n\n\ndef labels2output_map(label,lppts,dim,stride):\n\n\tside = ((float(dim) + 40.)/2.)/stride # 7.75 when dim = 208 and stride = 16\n\n\toutsize = dim/stride\n\tY  = np.zeros((outsize,outsize,2*4+1),dtype='float32')\n\tMN = np.array([outsize,outsize])\n\tWH = np.array([dim,dim],dtype=float)\n\n\ttlx,tly = np.floor(np.maximum(label.tl(),0.)*MN).astype(int).tolist()\n\tbrx,bry = np.ceil (np.minimum(label.br(),1.)*MN).astype(int).tolist()\n\n\tfor x in range(tlx,brx):\n\t\tfor y in range(tly,bry):\n\n\t\t\tmn = np.array([float(x) + .5, float(y) + .5])\n\t\t\tiou = IOU_centre_and_dims(mn/MN,label.wh(),label.cc(),label.wh())\n\n\t\t\tif iou > .5:\n\n\t\t\t\tp_WH = lppts*WH.reshape((2,1))\n\t\t\t\tp_MN = p_WH/stride\n\n\t\t\t\tp_MN_center_mn = p_MN - mn.reshape((2,1))\n\n\t\t\t\tp_side = p_MN_center_mn/side\n\n\t\t\t\tY[y,x,0] = 1.\n\t\t\t\tY[y,x,1:] = p_side.T.flatten()\n\n\treturn Y\n\ndef pts2ptsh(pts):\n\treturn np.matrix(np.concatenate((pts,np.ones((1,pts.shape[1]))),0))\n\ndef project(I,T,pts,dim):\n\tptsh \t= np.matrix(np.concatenate((pts,np.ones((1,4))),0))\n\tptsh \t= np.matmul(T,ptsh)\n\tptsh \t= ptsh/ptsh[2]\n\tptsret  = ptsh[:2]\n\tptsret  = ptsret/dim\n\tIroi = cv2.warpPerspective(I,T,(dim,dim),borderValue=.0,flags=cv2.INTER_LINEAR)\n\treturn Iroi,ptsret\n\ndef flip_image_and_pts(I,pts):\n\tI = cv2.flip(I,1)\n\tpts[0] = 1. - pts[0]\n\tidx = [1,0,3,2]\n\tpts = pts[...,idx]\n\treturn I,pts\n\ndef augment_sample(I,pts,dim):\n\n\tmaxsum,maxangle = 120,np.array([80.,80.,45.])\n\tangles = np.random.rand(3)*maxangle\n\tif angles.sum() > maxsum:\n\t\tangles = (angles/angles.sum())*(maxangle/maxangle.sum())\n\n\tI = im2single(I)\n\tiwh = getWH(I.shape)\n\n\twhratio = random.uniform(2.,4.)\n\twsiz = random.uniform(dim*.2,dim*1.)\n\t\n\thsiz = wsiz/whratio\n\n\tdx = random.uniform(0.,dim - wsiz)\n\tdy = random.uniform(0.,dim - hsiz)\n\n\tpph = getRectPts(dx,dy,dx+wsiz,dy+hsiz)\n\tpts = pts*iwh.reshape((2,1))\n\tT = find_T_matrix(pts2ptsh(pts),pph)\n\n\tH = perspective_transform((dim,dim),angles=angles)\n\tH = np.matmul(H,T)\n\n\tIroi,pts = project(I,H,pts,dim)\n\t\n\thsv_mod = np.random.rand(3).astype('float32')\n\thsv_mod = (hsv_mod - .5)*.3\n\thsv_mod[0] *= 360\n\tIroi = hsv_transform(Iroi,hsv_mod)\n\tIroi = np.clip(Iroi,0.,1.)\n\n\tpts = np.array(pts)\n\n\tif random.random() > .5:\n\t\tIroi,pts = flip_image_and_pts(Iroi,pts)\n\n\ttl,br = pts.min(1),pts.max(1)\n\tllp = Label(0,tl,br)\n\n\treturn Iroi,llp,pts\n"""
src/utils.py,0,"b""\nimport numpy as np\nimport cv2\nimport sys\n\nfrom glob import glob\n\n\ndef im2single(I):\n\tassert(I.dtype == 'uint8')\n\treturn I.astype('float32')/255.\n\n\ndef getWH(shape):\n\treturn np.array(shape[1::-1]).astype(float)\n\n\ndef IOU(tl1,br1,tl2,br2):\n\twh1,wh2 = br1-tl1,br2-tl2\n\tassert((wh1>=.0).all() and (wh2>=.0).all())\n\t\n\tintersection_wh = np.maximum(np.minimum(br1,br2) - np.maximum(tl1,tl2),0.)\n\tintersection_area = np.prod(intersection_wh)\n\tarea1,area2 = (np.prod(wh1),np.prod(wh2))\n\tunion_area = area1 + area2 - intersection_area;\n\treturn intersection_area/union_area\n\n\ndef IOU_labels(l1,l2):\n\treturn IOU(l1.tl(),l1.br(),l2.tl(),l2.br())\n\n\ndef IOU_centre_and_dims(cc1,wh1,cc2,wh2):\n\treturn IOU(cc1-wh1/2.,cc1+wh1/2.,cc2-wh2/2.,cc2+wh2/2.)\n\n\ndef nms(Labels,iou_threshold=.5):\n\n\tSelectedLabels = []\n\tLabels.sort(key=lambda l: l.prob(),reverse=True)\n\t\n\tfor label in Labels:\n\n\t\tnon_overlap = True\n\t\tfor sel_label in SelectedLabels:\n\t\t\tif IOU_labels(label,sel_label) > iou_threshold:\n\t\t\t\tnon_overlap = False\n\t\t\t\tbreak\n\n\t\tif non_overlap:\n\t\t\tSelectedLabels.append(label)\n\n\treturn SelectedLabels\n\n\ndef image_files_from_folder(folder,upper=True):\n\textensions = ['jpg','jpeg','png']\n\timg_files  = []\n\tfor ext in extensions:\n\t\timg_files += glob('%s/*.%s' % (folder,ext))\n\t\tif upper:\n\t\t\timg_files += glob('%s/*.%s' % (folder,ext.upper()))\n\treturn img_files\n\n\ndef is_inside(ltest,lref):\n\treturn (ltest.tl() >= lref.tl()).all() and (ltest.br() <= lref.br()).all()\n\n\ndef crop_region(I,label,bg=0.5):\n\n\twh = np.array(I.shape[1::-1])\n\n\tch = I.shape[2] if len(I.shape) == 3 else 1\n\ttl = np.floor(label.tl()*wh).astype(int)\n\tbr = np.ceil (label.br()*wh).astype(int)\n\toutwh = br-tl\n\n\tif np.prod(outwh) == 0.:\n\t\treturn None\n\n\toutsize = (outwh[1],outwh[0],ch) if ch > 1 else (outwh[1],outwh[0])\n\tif (np.array(outsize) < 0).any():\n\t\tpause()\n\tIout  = np.zeros(outsize,dtype=I.dtype) + bg\n\n\toffset \t= np.minimum(tl,0)*(-1)\n\ttl \t\t= np.maximum(tl,0)\n\tbr \t\t= np.minimum(br,wh)\n\twh \t\t= br - tl\n\n\tIout[offset[1]:(offset[1] + wh[1]),offset[0]:(offset[0] + wh[0])] = I[tl[1]:br[1],tl[0]:br[0]]\n\n\treturn Iout\n\ndef hsv_transform(I,hsv_modifier):\n\tI = cv2.cvtColor(I,cv2.COLOR_BGR2HSV)\n\tI = I + hsv_modifier\n\treturn cv2.cvtColor(I,cv2.COLOR_HSV2BGR)\n\ndef IOU(tl1,br1,tl2,br2):\n\twh1,wh2 = br1-tl1,br2-tl2\n\tassert((wh1>=.0).all() and (wh2>=.0).all())\n\t\n\tintersection_wh = np.maximum(np.minimum(br1,br2) - np.maximum(tl1,tl2),0.)\n\tintersection_area = np.prod(intersection_wh)\n\tarea1,area2 = (np.prod(wh1),np.prod(wh2))\n\tunion_area = area1 + area2 - intersection_area;\n\treturn intersection_area/union_area\n\ndef IOU_centre_and_dims(cc1,wh1,cc2,wh2):\n\treturn IOU(cc1-wh1/2.,cc1+wh1/2.,cc2-wh2/2.,cc2+wh2/2.)\n\n\ndef show(I,wname='Display'):\n\tcv2.imshow(wname, I)\n\tcv2.moveWindow(wname,0,0)\n\tkey = cv2.waitKey(0) & 0xEFFFFF\n\tcv2.destroyWindow(wname)\n\tif key == 27:\n\t\tsys.exit()\n\telse:\n\t\treturn key"""
darknet/examples/__init__.py,0,b''
darknet/examples/detector-scipy-opencv.py,0,"b'# Stupid python path shit.\n# Instead just add darknet.py to somewhere in your python path\n# OK actually that might not be a great idea, idk, work in progress\n# Use at your own risk. or don\'t, i don\'t care\n\nfrom scipy.misc import imread\nimport cv2\n\ndef array_to_image(arr):\n    arr = arr.transpose(2,0,1)\n    c = arr.shape[0]\n    h = arr.shape[1]\n    w = arr.shape[2]\n    arr = (arr/255.0).flatten()\n    data = dn.c_array(dn.c_float, arr)\n    im = dn.IMAGE(w,h,c,data)\n    return im\n\ndef detect2(net, meta, image, thresh=.5, hier_thresh=.5, nms=.45):\n    boxes = dn.make_boxes(net)\n    probs = dn.make_probs(net)\n    num =   dn.num_boxes(net)\n    dn.network_detect(net, image, thresh, hier_thresh, nms, boxes, probs)\n    res = []\n    for j in range(num):\n        for i in range(meta.classes):\n            if probs[j][i] > 0:\n                res.append((meta.names[i], probs[j][i], (boxes[j].x, boxes[j].y, boxes[j].w, boxes[j].h)))\n    res = sorted(res, key=lambda x: -x[1])\n    dn.free_ptrs(dn.cast(probs, dn.POINTER(dn.c_void_p)), num)\n    return res\n\nimport sys, os\nsys.path.append(os.path.join(os.getcwd(),\'python/\'))\n\nimport darknet as dn\n\n# Darknet\nnet = dn.load_net(""cfg/tiny-yolo.cfg"", ""tiny-yolo.weights"", 0)\nmeta = dn.load_meta(""cfg/coco.data"")\nr = dn.detect(net, meta, ""data/dog.jpg"")\nprint r\n\n# scipy\narr= imread(\'data/dog.jpg\')\nim = array_to_image(arr)\nr = detect2(net, meta, im)\nprint r\n\n# OpenCV\narr = cv2.imread(\'data/dog.jpg\')\nim = array_to_image(arr)\ndn.rgbgr_image(im)\nr = detect2(net, meta, im)\nprint r\n\n'"
darknet/examples/detector.py,0,"b'# Stupid python path shit.\n# Instead just add darknet.py to somewhere in your python path\n# OK actually that might not be a great idea, idk, work in progress\n# Use at your own risk. or don\'t, i don\'t care\n\nimport sys, os\nsys.path.append(os.path.join(os.getcwd(),\'python/\'))\n\nimport darknet as dn\nimport pdb\n\ndn.set_gpu(0)\nnet = dn.load_net(""cfg/yolo-thor.cfg"", ""/home/pjreddie/backup/yolo-thor_final.weights"", 0)\nmeta = dn.load_meta(""cfg/thor.data"")\nr = dn.detect(net, meta, ""data/bedroom.jpg"")\nprint r\n\n# And then down here you could detect a lot more images like:\nr = dn.detect(net, meta, ""data/eagle.jpg"")\nprint r\nr = dn.detect(net, meta, ""data/giraffe.jpg"")\nprint r\nr = dn.detect(net, meta, ""data/horses.jpg"")\nprint r\nr = dn.detect(net, meta, ""data/person.jpg"")\nprint r\n\n'"
darknet/python/__init__.py,0,b''
darknet/python/darknet.py,0,"b'from ctypes import *\nimport math\nimport random\n\ndef sample(probs):\n    s = sum(probs)\n    probs = [a/s for a in probs]\n    r = random.uniform(0, 1)\n    for i in range(len(probs)):\n        r = r - probs[i]\n        if r <= 0:\n            return i\n    return len(probs)-1\n\ndef c_array(ctype, values):\n    arr = (ctype*len(values))()\n    arr[:] = values\n    return arr\n\nclass BOX(Structure):\n    _fields_ = [(""x"", c_float),\n                (""y"", c_float),\n                (""w"", c_float),\n                (""h"", c_float)]\n\nclass DETECTION(Structure):\n    _fields_ = [(""bbox"", BOX),\n                (""classes"", c_int),\n                (""prob"", POINTER(c_float)),\n                (""mask"", POINTER(c_float)),\n                (""objectness"", c_float),\n                (""sort_class"", c_int)]\n\n\nclass IMAGE(Structure):\n    _fields_ = [(""w"", c_int),\n                (""h"", c_int),\n                (""c"", c_int),\n                (""data"", POINTER(c_float))]\n\nclass METADATA(Structure):\n    _fields_ = [(""classes"", c_int),\n                (""names"", POINTER(c_char_p))]\n\n    \n\n#lib = CDLL(""/home/pjreddie/documents/darknet/libdarknet.so"", RTLD_GLOBAL)\nlib = CDLL(""darknet/libdarknet.so"", RTLD_GLOBAL)\nlib.network_width.argtypes = [c_void_p]\nlib.network_width.restype = c_int\nlib.network_height.argtypes = [c_void_p]\nlib.network_height.restype = c_int\n\npredict = lib.network_predict\npredict.argtypes = [c_void_p, POINTER(c_float)]\npredict.restype = POINTER(c_float)\n\nset_gpu = lib.cuda_set_device\nset_gpu.argtypes = [c_int]\n\nmake_image = lib.make_image\nmake_image.argtypes = [c_int, c_int, c_int]\nmake_image.restype = IMAGE\n\nget_network_boxes = lib.get_network_boxes\nget_network_boxes.argtypes = [c_void_p, c_int, c_int, c_float, c_float, POINTER(c_int), c_int, POINTER(c_int)]\nget_network_boxes.restype = POINTER(DETECTION)\n\nmake_network_boxes = lib.make_network_boxes\nmake_network_boxes.argtypes = [c_void_p]\nmake_network_boxes.restype = POINTER(DETECTION)\n\nfree_detections = lib.free_detections\nfree_detections.argtypes = [POINTER(DETECTION), c_int]\n\nfree_ptrs = lib.free_ptrs\nfree_ptrs.argtypes = [POINTER(c_void_p), c_int]\n\nnetwork_predict = lib.network_predict\nnetwork_predict.argtypes = [c_void_p, POINTER(c_float)]\n\nreset_rnn = lib.reset_rnn\nreset_rnn.argtypes = [c_void_p]\n\nload_net = lib.load_network\nload_net.argtypes = [c_char_p, c_char_p, c_int]\nload_net.restype = c_void_p\n\ndo_nms_obj = lib.do_nms_obj\ndo_nms_obj.argtypes = [POINTER(DETECTION), c_int, c_int, c_float]\n\ndo_nms_sort = lib.do_nms_sort\ndo_nms_sort.argtypes = [POINTER(DETECTION), c_int, c_int, c_float]\n\nfree_image = lib.free_image\nfree_image.argtypes = [IMAGE]\n\nletterbox_image = lib.letterbox_image\nletterbox_image.argtypes = [IMAGE, c_int, c_int]\nletterbox_image.restype = IMAGE\n\nload_meta = lib.get_metadata\nlib.get_metadata.argtypes = [c_char_p]\nlib.get_metadata.restype = METADATA\n\nload_image = lib.load_image_color\nload_image.argtypes = [c_char_p, c_int, c_int]\nload_image.restype = IMAGE\n\nrgbgr_image = lib.rgbgr_image\nrgbgr_image.argtypes = [IMAGE]\n\npredict_image = lib.network_predict_image\npredict_image.argtypes = [c_void_p, IMAGE]\npredict_image.restype = POINTER(c_float)\n\ndef classify(net, meta, im):\n    out = predict_image(net, im)\n    res = []\n    for i in range(meta.classes):\n        res.append((meta.names[i], out[i]))\n    res = sorted(res, key=lambda x: -x[1])\n    return res\n\ndef detect(net, meta, image, thresh=.5, hier_thresh=.5, nms=.45):\n    im = load_image(image, 0, 0)\n    num = c_int(0)\n    pnum = pointer(num)\n    predict_image(net, im)\n    dets = get_network_boxes(net, im.w, im.h, thresh, hier_thresh, None, 0, pnum)\n    num = pnum[0]\n    if (nms): do_nms_obj(dets, num, meta.classes, nms);\n\n    res = []\n    for j in range(num):\n        for i in range(meta.classes):\n            if dets[j].prob[i] > 0:\n                b = dets[j].bbox\n                res.append((meta.names[i], dets[j].prob[i], (b.x, b.y, b.w, b.h)))\n    res = sorted(res, key=lambda x: -x[1])\n    wh = (im.w,im.h)\n    free_image(im)\n    free_detections(dets, num)\n    return res,wh\n    \nif __name__ == ""__main__"":\n    #net = load_net(""cfg/densenet201.cfg"", ""/home/pjreddie/trained/densenet201.weights"", 0)\n    #im = load_image(""data/wolf.jpg"", 0, 0)\n    #meta = load_meta(""cfg/imagenet1k.data"")\n    #r = classify(net, meta, im)\n    #print r[:10]\n    net = load_net(""cfg/tiny-yolo.cfg"", ""tiny-yolo.weights"", 0)\n    meta = load_meta(""cfg/coco.data"")\n    r = detect(net, meta, ""data/dog.jpg"")\n    print r\n    \n\n'"
darknet/python/proverbot.py,0,"b'from darknet import *\n\ndef predict_tactic(net, s):\n    prob = 0\n    d = c_array(c_float, [0.0]*256)\n    tac = \'\'\n    if not len(s):\n        s = \'\\n\'\n    for c in s[:-1]:\n        d[ord(c)] = 1\n        pred = predict(net, d)\n        d[ord(c)] = 0\n    c = s[-1]\n    while 1:\n        d[ord(c)] = 1\n        pred = predict(net, d)\n        d[ord(c)] = 0\n        pred = [pred[i] for i in range(256)]\n        ind = sample(pred)\n        c = chr(ind)\n        prob += math.log(pred[ind])\n        if len(tac) and tac[-1] == \'.\':\n            break\n        tac = tac + c\n    return (tac, prob)\n\ndef predict_tactics(net, s, n):\n    tacs = []\n    for i in range(n):\n        reset_rnn(net)\n        tacs.append(predict_tactic(net, s))\n    tacs = sorted(tacs, key=lambda x: -x[1])\n    return tacs\n\nnet = load_net(""cfg/coq.test.cfg"", ""/home/pjreddie/backup/coq.backup"", 0)\nt = predict_tactics(net, ""+++++\\n"", 10)\nprint t\n'"
darknet/scripts/voc_label.py,0,"b'import xml.etree.ElementTree as ET\nimport pickle\nimport os\nfrom os import listdir, getcwd\nfrom os.path import join\n\nsets=[(\'2012\', \'train\'), (\'2012\', \'val\'), (\'2007\', \'train\'), (\'2007\', \'val\'), (\'2007\', \'test\')]\n\nclasses = [""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"", ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"", ""dog"", ""horse"", ""motorbike"", ""person"", ""pottedplant"", ""sheep"", ""sofa"", ""train"", ""tvmonitor""]\n\n\ndef convert(size, box):\n    dw = 1./(size[0])\n    dh = 1./(size[1])\n    x = (box[0] + box[1])/2.0 - 1\n    y = (box[2] + box[3])/2.0 - 1\n    w = box[1] - box[0]\n    h = box[3] - box[2]\n    x = x*dw\n    w = w*dw\n    y = y*dh\n    h = h*dh\n    return (x,y,w,h)\n\ndef convert_annotation(year, image_id):\n    in_file = open(\'VOCdevkit/VOC%s/Annotations/%s.xml\'%(year, image_id))\n    out_file = open(\'VOCdevkit/VOC%s/labels/%s.txt\'%(year, image_id), \'w\')\n    tree=ET.parse(in_file)\n    root = tree.getroot()\n    size = root.find(\'size\')\n    w = int(size.find(\'width\').text)\n    h = int(size.find(\'height\').text)\n\n    for obj in root.iter(\'object\'):\n        difficult = obj.find(\'difficult\').text\n        cls = obj.find(\'name\').text\n        if cls not in classes or int(difficult)==1:\n            continue\n        cls_id = classes.index(cls)\n        xmlbox = obj.find(\'bndbox\')\n        b = (float(xmlbox.find(\'xmin\').text), float(xmlbox.find(\'xmax\').text), float(xmlbox.find(\'ymin\').text), float(xmlbox.find(\'ymax\').text))\n        bb = convert((w,h), b)\n        out_file.write(str(cls_id) + "" "" + "" "".join([str(a) for a in bb]) + \'\\n\')\n\nwd = getcwd()\n\nfor year, image_set in sets:\n    if not os.path.exists(\'VOCdevkit/VOC%s/labels/\'%(year)):\n        os.makedirs(\'VOCdevkit/VOC%s/labels/\'%(year))\n    image_ids = open(\'VOCdevkit/VOC%s/ImageSets/Main/%s.txt\'%(year, image_set)).read().strip().split()\n    list_file = open(\'%s_%s.txt\'%(year, image_set), \'w\')\n    for image_id in image_ids:\n        list_file.write(\'%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg\\n\'%(wd, year, image_id))\n        convert_annotation(year, image_id)\n    list_file.close()\n\nos.system(""cat 2007_train.txt 2007_val.txt 2012_train.txt 2012_val.txt > train.txt"")\nos.system(""cat 2007_train.txt 2007_val.txt 2007_test.txt 2012_train.txt 2012_val.txt > train.all.txt"")\n\n'"
