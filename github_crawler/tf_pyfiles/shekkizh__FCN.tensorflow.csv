file_path,api_count,code
BatchDatsetReader.py,0,"b'""""""\nCode ideas from https://github.com/Newmu/dcgan and tensorflow mnist dataset reader\n""""""\nimport numpy as np\nimport scipy.misc as misc\n\n\nclass BatchDatset:\n    files = []\n    images = []\n    annotations = []\n    image_options = {}\n    batch_offset = 0\n    epochs_completed = 0\n\n    def __init__(self, records_list, image_options={}):\n        """"""\n        Intialize a generic file reader with batching for list of files\n        :param records_list: list of file records to read -\n        sample record: {\'image\': f, \'annotation\': annotation_file, \'filename\': filename}\n        :param image_options: A dictionary of options for modifying the output image\n        Available options:\n        resize = True/ False\n        resize_size = #size of output image - does bilinear resize\n        color=True/False\n        """"""\n        print(""Initializing Batch Dataset Reader..."")\n        print(image_options)\n        self.files = records_list\n        self.image_options = image_options\n        self._read_images()\n\n    def _read_images(self):\n        self.__channels = True\n        self.images = np.array([self._transform(filename[\'image\']) for filename in self.files])\n        self.__channels = False\n        self.annotations = np.array(\n            [np.expand_dims(self._transform(filename[\'annotation\']), axis=3) for filename in self.files])\n        print (self.images.shape)\n        print (self.annotations.shape)\n\n    def _transform(self, filename):\n        image = misc.imread(filename)\n        if self.__channels and len(image.shape) < 3:  # make sure images are of shape(h,w,3)\n            image = np.array([image for i in range(3)])\n\n        if self.image_options.get(""resize"", False) and self.image_options[""resize""]:\n            resize_size = int(self.image_options[""resize_size""])\n            resize_image = misc.imresize(image,\n                                         [resize_size, resize_size], interp=\'nearest\')\n        else:\n            resize_image = image\n\n        return np.array(resize_image)\n\n    def get_records(self):\n        return self.images, self.annotations\n\n    def reset_batch_offset(self, offset=0):\n        self.batch_offset = offset\n\n    def next_batch(self, batch_size):\n        start = self.batch_offset\n        self.batch_offset += batch_size\n        if self.batch_offset > self.images.shape[0]:\n            # Finished epoch\n            self.epochs_completed += 1\n            print(""****************** Epochs completed: "" + str(self.epochs_completed) + ""******************"")\n            # Shuffle the data\n            perm = np.arange(self.images.shape[0])\n            np.random.shuffle(perm)\n            self.images = self.images[perm]\n            self.annotations = self.annotations[perm]\n            # Start next epoch\n            start = 0\n            self.batch_offset = batch_size\n\n        end = self.batch_offset\n        return self.images[start:end], self.annotations[start:end]\n\n    def get_random_batch(self, batch_size):\n        indexes = np.random.randint(0, self.images.shape[0], size=[batch_size]).tolist()\n        return self.images[indexes], self.annotations[indexes]\n'"
FCN.py,42,"b'from __future__ import print_function\nimport tensorflow as tf\nimport numpy as np\n\nimport TensorflowUtils as utils\nimport read_MITSceneParsingData as scene_parsing\nimport datetime\nimport BatchDatsetReader as dataset\nfrom six.moves import xrange\n\nFLAGS = tf.flags.FLAGS\ntf.flags.DEFINE_integer(""batch_size"", ""2"", ""batch size for training"")\ntf.flags.DEFINE_string(""logs_dir"", ""logs/"", ""path to logs directory"")\ntf.flags.DEFINE_string(""data_dir"", ""Data_zoo/MIT_SceneParsing/"", ""path to dataset"")\ntf.flags.DEFINE_float(""learning_rate"", ""1e-4"", ""Learning rate for Adam Optimizer"")\ntf.flags.DEFINE_string(""model_dir"", ""Model_zoo/"", ""Path to vgg model mat"")\ntf.flags.DEFINE_bool(\'debug\', ""False"", ""Debug mode: True/ False"")\ntf.flags.DEFINE_string(\'mode\', ""train"", ""Mode train/ test/ visualize"")\n\nMODEL_URL = \'http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat\'\n\nMAX_ITERATION = int(1e5 + 1)\nNUM_OF_CLASSESS = 151\nIMAGE_SIZE = 224\n\n\ndef vgg_net(weights, image):\n    layers = (\n        \'conv1_1\', \'relu1_1\', \'conv1_2\', \'relu1_2\', \'pool1\',\n\n        \'conv2_1\', \'relu2_1\', \'conv2_2\', \'relu2_2\', \'pool2\',\n\n        \'conv3_1\', \'relu3_1\', \'conv3_2\', \'relu3_2\', \'conv3_3\',\n        \'relu3_3\', \'conv3_4\', \'relu3_4\', \'pool3\',\n\n        \'conv4_1\', \'relu4_1\', \'conv4_2\', \'relu4_2\', \'conv4_3\',\n        \'relu4_3\', \'conv4_4\', \'relu4_4\', \'pool4\',\n\n        \'conv5_1\', \'relu5_1\', \'conv5_2\', \'relu5_2\', \'conv5_3\',\n        \'relu5_3\', \'conv5_4\', \'relu5_4\'\n    )\n\n    net = {}\n    current = image\n    for i, name in enumerate(layers):\n        kind = name[:4]\n        if kind == \'conv\':\n            kernels, bias = weights[i][0][0][0][0]\n            # matconvnet: weights are [width, height, in_channels, out_channels]\n            # tensorflow: weights are [height, width, in_channels, out_channels]\n            kernels = utils.get_variable(np.transpose(kernels, (1, 0, 2, 3)), name=name + ""_w"")\n            bias = utils.get_variable(bias.reshape(-1), name=name + ""_b"")\n            current = utils.conv2d_basic(current, kernels, bias)\n        elif kind == \'relu\':\n            current = tf.nn.relu(current, name=name)\n            if FLAGS.debug:\n                utils.add_activation_summary(current)\n        elif kind == \'pool\':\n            current = utils.avg_pool_2x2(current)\n        net[name] = current\n\n    return net\n\n\ndef inference(image, keep_prob):\n    """"""\n    Semantic segmentation network definition\n    :param image: input image. Should have values in range 0-255\n    :param keep_prob:\n    :return:\n    """"""\n    print(""setting up vgg initialized conv layers ..."")\n    model_data = utils.get_model_data(FLAGS.model_dir, MODEL_URL)\n\n    mean = model_data[\'normalization\'][0][0][0]\n    mean_pixel = np.mean(mean, axis=(0, 1))\n\n    weights = np.squeeze(model_data[\'layers\'])\n\n    processed_image = utils.process_image(image, mean_pixel)\n\n    with tf.variable_scope(""inference""):\n        image_net = vgg_net(weights, processed_image)\n        conv_final_layer = image_net[""conv5_3""]\n\n        pool5 = utils.max_pool_2x2(conv_final_layer)\n\n        W6 = utils.weight_variable([7, 7, 512, 4096], name=""W6"")\n        b6 = utils.bias_variable([4096], name=""b6"")\n        conv6 = utils.conv2d_basic(pool5, W6, b6)\n        relu6 = tf.nn.relu(conv6, name=""relu6"")\n        if FLAGS.debug:\n            utils.add_activation_summary(relu6)\n        relu_dropout6 = tf.nn.dropout(relu6, keep_prob=keep_prob)\n\n        W7 = utils.weight_variable([1, 1, 4096, 4096], name=""W7"")\n        b7 = utils.bias_variable([4096], name=""b7"")\n        conv7 = utils.conv2d_basic(relu_dropout6, W7, b7)\n        relu7 = tf.nn.relu(conv7, name=""relu7"")\n        if FLAGS.debug:\n            utils.add_activation_summary(relu7)\n        relu_dropout7 = tf.nn.dropout(relu7, keep_prob=keep_prob)\n\n        W8 = utils.weight_variable([1, 1, 4096, NUM_OF_CLASSESS], name=""W8"")\n        b8 = utils.bias_variable([NUM_OF_CLASSESS], name=""b8"")\n        conv8 = utils.conv2d_basic(relu_dropout7, W8, b8)\n        # annotation_pred1 = tf.argmax(conv8, dimension=3, name=""prediction1"")\n\n        # now to upscale to actual image size\n        deconv_shape1 = image_net[""pool4""].get_shape()\n        W_t1 = utils.weight_variable([4, 4, deconv_shape1[3].value, NUM_OF_CLASSESS], name=""W_t1"")\n        b_t1 = utils.bias_variable([deconv_shape1[3].value], name=""b_t1"")\n        conv_t1 = utils.conv2d_transpose_strided(conv8, W_t1, b_t1, output_shape=tf.shape(image_net[""pool4""]))\n        fuse_1 = tf.add(conv_t1, image_net[""pool4""], name=""fuse_1"")\n\n        deconv_shape2 = image_net[""pool3""].get_shape()\n        W_t2 = utils.weight_variable([4, 4, deconv_shape2[3].value, deconv_shape1[3].value], name=""W_t2"")\n        b_t2 = utils.bias_variable([deconv_shape2[3].value], name=""b_t2"")\n        conv_t2 = utils.conv2d_transpose_strided(fuse_1, W_t2, b_t2, output_shape=tf.shape(image_net[""pool3""]))\n        fuse_2 = tf.add(conv_t2, image_net[""pool3""], name=""fuse_2"")\n\n        shape = tf.shape(image)\n        deconv_shape3 = tf.stack([shape[0], shape[1], shape[2], NUM_OF_CLASSESS])\n        W_t3 = utils.weight_variable([16, 16, NUM_OF_CLASSESS, deconv_shape2[3].value], name=""W_t3"")\n        b_t3 = utils.bias_variable([NUM_OF_CLASSESS], name=""b_t3"")\n        conv_t3 = utils.conv2d_transpose_strided(fuse_2, W_t3, b_t3, output_shape=deconv_shape3, stride=8)\n\n        annotation_pred = tf.argmax(conv_t3, dimension=3, name=""prediction"")\n\n    return tf.expand_dims(annotation_pred, dim=3), conv_t3\n\n\ndef train(loss_val, var_list):\n    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n    grads = optimizer.compute_gradients(loss_val, var_list=var_list)\n    if FLAGS.debug:\n        # print(len(var_list))\n        for grad, var in grads:\n            utils.add_gradient_summary(grad, var)\n    return optimizer.apply_gradients(grads)\n\n\ndef main(argv=None):\n    keep_probability = tf.placeholder(tf.float32, name=""keep_probabilty"")\n    image = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE, IMAGE_SIZE, 3], name=""input_image"")\n    annotation = tf.placeholder(tf.int32, shape=[None, IMAGE_SIZE, IMAGE_SIZE, 1], name=""annotation"")\n\n    pred_annotation, logits = inference(image, keep_probability)\n    tf.summary.image(""input_image"", image, max_outputs=2)\n    tf.summary.image(""ground_truth"", tf.cast(annotation, tf.uint8), max_outputs=2)\n    tf.summary.image(""pred_annotation"", tf.cast(pred_annotation, tf.uint8), max_outputs=2)\n    loss = tf.reduce_mean((tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n                                                                          labels=tf.squeeze(annotation, squeeze_dims=[3]),\n                                                                          name=""entropy"")))\n    loss_summary = tf.summary.scalar(""entropy"", loss)\n\n    trainable_var = tf.trainable_variables()\n    if FLAGS.debug:\n        for var in trainable_var:\n            utils.add_to_regularization_and_summary(var)\n    train_op = train(loss, trainable_var)\n\n    print(""Setting up summary op..."")\n    summary_op = tf.summary.merge_all()\n\n    print(""Setting up image reader..."")\n    train_records, valid_records = scene_parsing.read_dataset(FLAGS.data_dir)\n    print(len(train_records))\n    print(len(valid_records))\n\n    print(""Setting up dataset reader"")\n    image_options = {\'resize\': True, \'resize_size\': IMAGE_SIZE}\n    if FLAGS.mode == \'train\':\n        train_dataset_reader = dataset.BatchDatset(train_records, image_options)\n    validation_dataset_reader = dataset.BatchDatset(valid_records, image_options)\n\n    sess = tf.Session()\n\n    print(""Setting up Saver..."")\n    saver = tf.train.Saver()\n\n    # create two summary writers to show training loss and validation loss in the same graph\n    # need to create two folders \'train\' and \'validation\' inside FLAGS.logs_dir\n    train_writer = tf.summary.FileWriter(FLAGS.logs_dir + \'/train\', sess.graph)\n    validation_writer = tf.summary.FileWriter(FLAGS.logs_dir + \'/validation\')\n\n    sess.run(tf.global_variables_initializer())\n    ckpt = tf.train.get_checkpoint_state(FLAGS.logs_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n        saver.restore(sess, ckpt.model_checkpoint_path)\n        print(""Model restored..."")\n\n    if FLAGS.mode == ""train"":\n        for itr in xrange(MAX_ITERATION):\n            train_images, train_annotations = train_dataset_reader.next_batch(FLAGS.batch_size)\n            feed_dict = {image: train_images, annotation: train_annotations, keep_probability: 0.85}\n\n            sess.run(train_op, feed_dict=feed_dict)\n\n            if itr % 10 == 0:\n                train_loss, summary_str = sess.run([loss, loss_summary], feed_dict=feed_dict)\n                print(""Step: %d, Train_loss:%g"" % (itr, train_loss))\n                train_writer.add_summary(summary_str, itr)\n\n            if itr % 500 == 0:\n                valid_images, valid_annotations = validation_dataset_reader.next_batch(FLAGS.batch_size)\n                valid_loss, summary_sva = sess.run([loss, loss_summary], feed_dict={image: valid_images, annotation: valid_annotations,\n                                                       keep_probability: 1.0})\n                print(""%s ---> Validation_loss: %g"" % (datetime.datetime.now(), valid_loss))\n\n                # add validation loss to TensorBoard\n                validation_writer.add_summary(summary_sva, itr)\n                saver.save(sess, FLAGS.logs_dir + ""model.ckpt"", itr)\n\n    elif FLAGS.mode == ""visualize"":\n        valid_images, valid_annotations = validation_dataset_reader.get_random_batch(FLAGS.batch_size)\n        pred = sess.run(pred_annotation, feed_dict={image: valid_images, annotation: valid_annotations,\n                                                    keep_probability: 1.0})\n        valid_annotations = np.squeeze(valid_annotations, axis=3)\n        pred = np.squeeze(pred, axis=3)\n\n        for itr in range(FLAGS.batch_size):\n            utils.save_image(valid_images[itr].astype(np.uint8), FLAGS.logs_dir, name=""inp_"" + str(5+itr))\n            utils.save_image(valid_annotations[itr].astype(np.uint8), FLAGS.logs_dir, name=""gt_"" + str(5+itr))\n            utils.save_image(pred[itr].astype(np.uint8), FLAGS.logs_dir, name=""pred_"" + str(5+itr))\n            print(""Saved image: %d"" % itr)\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
TensorflowUtils.py,43,"b'__author__ = \'Charlie\'\n# Utils used with tensorflow implemetation\nimport tensorflow as tf\nimport numpy as np\nimport scipy.misc as misc\nimport os, sys\nfrom six.moves import urllib\nimport tarfile\nimport zipfile\nimport scipy.io\n\n\ndef get_model_data(dir_path, model_url):\n    maybe_download_and_extract(dir_path, model_url)\n    filename = model_url.split(""/"")[-1]\n    filepath = os.path.join(dir_path, filename)\n    if not os.path.exists(filepath):\n        raise IOError(""VGG Model not found!"")\n    data = scipy.io.loadmat(filepath)\n    return data\n\n\ndef maybe_download_and_extract(dir_path, url_name, is_tarfile=False, is_zipfile=False):\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n    filename = url_name.split(\'/\')[-1]\n    filepath = os.path.join(dir_path, filename)\n    if not os.path.exists(filepath):\n        def _progress(count, block_size, total_size):\n            sys.stdout.write(\n                \'\\r>> Downloading %s %.1f%%\' % (filename, float(count * block_size) / float(total_size) * 100.0))\n            sys.stdout.flush()\n\n        filepath, _ = urllib.request.urlretrieve(url_name, filepath, reporthook=_progress)\n        print()\n        statinfo = os.stat(filepath)\n        print(\'Succesfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n        if is_tarfile:\n            tarfile.open(filepath, \'r:gz\').extractall(dir_path)\n        elif is_zipfile:\n            with zipfile.ZipFile(filepath) as zf:\n                zip_dir = zf.namelist()[0]\n                zf.extractall(dir_path)\n\n\ndef save_image(image, save_dir, name, mean=None):\n    """"""\n    Save image by unprocessing if mean given else just save\n    :param mean:\n    :param image:\n    :param save_dir:\n    :param name:\n    :return:\n    """"""\n    if mean:\n        image = unprocess_image(image, mean)\n    misc.imsave(os.path.join(save_dir, name + "".png""), image)\n\n\ndef get_variable(weights, name):\n    init = tf.constant_initializer(weights, dtype=tf.float32)\n    var = tf.get_variable(name=name, initializer=init,  shape=weights.shape)\n    return var\n\n\ndef weight_variable(shape, stddev=0.02, name=None):\n    # print(shape)\n    initial = tf.truncated_normal(shape, stddev=stddev)\n    if name is None:\n        return tf.Variable(initial)\n    else:\n        return tf.get_variable(name, initializer=initial)\n\n\ndef bias_variable(shape, name=None):\n    initial = tf.constant(0.0, shape=shape)\n    if name is None:\n        return tf.Variable(initial)\n    else:\n        return tf.get_variable(name, initializer=initial)\n\n\ndef get_tensor_size(tensor):\n    from operator import mul\n    return reduce(mul, (d.value for d in tensor.get_shape()), 1)\n\n\ndef conv2d_basic(x, W, bias):\n    conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=""SAME"")\n    return tf.nn.bias_add(conv, bias)\n\n\ndef conv2d_strided(x, W, b):\n    conv = tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding=""SAME"")\n    return tf.nn.bias_add(conv, b)\n\n\ndef conv2d_transpose_strided(x, W, b, output_shape=None, stride = 2):\n    # print x.get_shape()\n    # print W.get_shape()\n    if output_shape is None:\n        output_shape = x.get_shape().as_list()\n        output_shape[1] *= 2\n        output_shape[2] *= 2\n        output_shape[3] = W.get_shape().as_list()[2]\n    # print output_shape\n    conv = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, stride, stride, 1], padding=""SAME"")\n    return tf.nn.bias_add(conv, b)\n\n\ndef leaky_relu(x, alpha=0.0, name=""""):\n    return tf.maximum(alpha * x, x, name)\n\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=""SAME"")\n\n\ndef avg_pool_2x2(x):\n    return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=""SAME"")\n\n\ndef local_response_norm(x):\n    return tf.nn.lrn(x, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)\n\n\ndef batch_norm(x, n_out, phase_train, scope=\'bn\', decay=0.9, eps=1e-5):\n    """"""\n    Code taken from http://stackoverflow.com/a/34634291/2267819\n    """"""\n    with tf.variable_scope(scope):\n        beta = tf.get_variable(name=\'beta\', shape=[n_out], initializer=tf.constant_initializer(0.0)\n                               , trainable=True)\n        gamma = tf.get_variable(name=\'gamma\', shape=[n_out], initializer=tf.random_normal_initializer(1.0, 0.02),\n                                trainable=True)\n        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name=\'moments\')\n        ema = tf.train.ExponentialMovingAverage(decay=decay)\n\n        def mean_var_with_update():\n            ema_apply_op = ema.apply([batch_mean, batch_var])\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n\n        mean, var = tf.cond(phase_train,\n                            mean_var_with_update,\n                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, eps)\n    return normed\n\n\ndef process_image(image, mean_pixel):\n    return image - mean_pixel\n\n\ndef unprocess_image(image, mean_pixel):\n    return image + mean_pixel\n\n\ndef bottleneck_unit(x, out_chan1, out_chan2, down_stride=False, up_stride=False, name=None):\n    """"""\n    Modified implementation from github ry?!\n    """"""\n\n    def conv_transpose(tensor, out_channel, shape, strides, name=None):\n        out_shape = tensor.get_shape().as_list()\n        in_channel = out_shape[-1]\n        kernel = weight_variable([shape, shape, out_channel, in_channel], name=name)\n        shape[-1] = out_channel\n        return tf.nn.conv2d_transpose(x, kernel, output_shape=out_shape, strides=[1, strides, strides, 1],\n                                      padding=\'SAME\', name=\'conv_transpose\')\n\n    def conv(tensor, out_chans, shape, strides, name=None):\n        in_channel = tensor.get_shape().as_list()[-1]\n        kernel = weight_variable([shape, shape, in_channel, out_chans], name=name)\n        return tf.nn.conv2d(x, kernel, strides=[1, strides, strides, 1], padding=\'SAME\', name=\'conv\')\n\n    def bn(tensor, name=None):\n        """"""\n        :param tensor: 4D tensor input\n        :param name: name of the operation\n        :return: local response normalized tensor - not using batch normalization :(\n        """"""\n        return tf.nn.lrn(tensor, depth_radius=5, bias=2, alpha=1e-4, beta=0.75, name=name)\n\n    in_chans = x.get_shape().as_list()[3]\n\n    if down_stride or up_stride:\n        first_stride = 2\n    else:\n        first_stride = 1\n\n    with tf.variable_scope(\'res%s\' % name):\n        if in_chans == out_chan2:\n            b1 = x\n        else:\n            with tf.variable_scope(\'branch1\'):\n                if up_stride:\n                    b1 = conv_transpose(x, out_chans=out_chan2, shape=1, strides=first_stride,\n                                        name=\'res%s_branch1\' % name)\n                else:\n                    b1 = conv(x, out_chans=out_chan2, shape=1, strides=first_stride, name=\'res%s_branch1\' % name)\n                b1 = bn(b1, \'bn%s_branch1\' % name, \'scale%s_branch1\' % name)\n\n        with tf.variable_scope(\'branch2a\'):\n            if up_stride:\n                b2 = conv_transpose(x, out_chans=out_chan1, shape=1, strides=first_stride, name=\'res%s_branch2a\' % name)\n            else:\n                b2 = conv(x, out_chans=out_chan1, shape=1, strides=first_stride, name=\'res%s_branch2a\' % name)\n            b2 = bn(b2, \'bn%s_branch2a\' % name, \'scale%s_branch2a\' % name)\n            b2 = tf.nn.relu(b2, name=\'relu\')\n\n        with tf.variable_scope(\'branch2b\'):\n            b2 = conv(b2, out_chans=out_chan1, shape=3, strides=1, name=\'res%s_branch2b\' % name)\n            b2 = bn(b2, \'bn%s_branch2b\' % name, \'scale%s_branch2b\' % name)\n            b2 = tf.nn.relu(b2, name=\'relu\')\n\n        with tf.variable_scope(\'branch2c\'):\n            b2 = conv(b2, out_chans=out_chan2, shape=1, strides=1, name=\'res%s_branch2c\' % name)\n            b2 = bn(b2, \'bn%s_branch2c\' % name, \'scale%s_branch2c\' % name)\n\n        x = b1 + b2\n        return tf.nn.relu(x, name=\'relu\')\n\n\ndef add_to_regularization_and_summary(var):\n    if var is not None:\n        tf.summary.histogram(var.op.name, var)\n        tf.add_to_collection(""reg_loss"", tf.nn.l2_loss(var))\n\n\ndef add_activation_summary(var):\n    if var is not None:\n        tf.summary.histogram(var.op.name + ""/activation"", var)\n        tf.summary.scalar(var.op.name + ""/sparsity"", tf.nn.zero_fraction(var))\n\n\ndef add_gradient_summary(grad, var):\n    if grad is not None:\n        tf.summary.histogram(var.op.name + ""/gradient"", grad)\n'"
__init__.py,0,b''
read_MITSceneParsingData.py,0,"b'__author__ = \'charlie\'\nimport numpy as np\nimport os\nimport random\nfrom six.moves import cPickle as pickle\nfrom tensorflow.python.platform import gfile\nimport glob\n\nimport TensorflowUtils as utils\n\n# DATA_URL = \'http://sceneparsing.csail.mit.edu/data/ADEChallengeData2016.zip\'\nDATA_URL = \'http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip\'\n\n\ndef read_dataset(data_dir):\n    pickle_filename = ""MITSceneParsing.pickle""\n    pickle_filepath = os.path.join(data_dir, pickle_filename)\n    if not os.path.exists(pickle_filepath):\n        utils.maybe_download_and_extract(data_dir, DATA_URL, is_zipfile=True)\n        SceneParsing_folder = os.path.splitext(DATA_URL.split(""/"")[-1])[0]\n        result = create_image_lists(os.path.join(data_dir, SceneParsing_folder))\n        print (""Pickling ..."")\n        with open(pickle_filepath, \'wb\') as f:\n            pickle.dump(result, f, pickle.HIGHEST_PROTOCOL)\n    else:\n        print (""Found pickle file!"")\n\n    with open(pickle_filepath, \'rb\') as f:\n        result = pickle.load(f)\n        training_records = result[\'training\']\n        validation_records = result[\'validation\']\n        del result\n\n    return training_records, validation_records\n\n\ndef create_image_lists(image_dir):\n    if not gfile.Exists(image_dir):\n        print(""Image directory \'"" + image_dir + ""\' not found."")\n        return None\n    directories = [\'training\', \'validation\']\n    image_list = {}\n\n    for directory in directories:\n        file_list = []\n        image_list[directory] = []\n        file_glob = os.path.join(image_dir, ""images"", directory, \'*.\' + \'jpg\')\n        file_list.extend(glob.glob(file_glob))\n\n        if not file_list:\n            print(\'No files found\')\n        else:\n            for f in file_list:\n                filename = os.path.splitext(f.split(""/"")[-1])[0]\n                annotation_file = os.path.join(image_dir, ""annotations"", directory, filename + \'.png\')\n                if os.path.exists(annotation_file):\n                    record = {\'image\': f, \'annotation\': annotation_file, \'filename\': filename}\n                    image_list[directory].append(record)\n                else:\n                    print(""Annotation file not found for %s - Skipping"" % filename)\n\n        random.shuffle(image_list[directory])\n        no_of_images = len(image_list[directory])\n        print (\'No. of %s files: %d\' % (directory, no_of_images))\n\n    return image_list\n'"
