file_path,api_count,code
amodel.py,0,"b'# from __future__ import print_function\nimport numpy as np\n\nnp.random.seed(1337)  # for reproducibility\nimport os\nfrom keras.regularizers import l2\nfrom keras.callbacks import *\n# from visualizer import *\nfrom keras.models import *\nfrom keras.optimizers import *\nfrom keras.utils.np_utils import to_categorical, accuracy\nfrom keras.layers.core import *\nfrom keras.layers import Input, Embedding, LSTM, Dense, merge, TimeDistributed\n# from keras.utils.visualize_util import plot  # THIS IS BAD\n# from data_reader import *\nfrom reader import *\nfrom myutils import *\nimport logging\nfrom datetime import datetime\n\n\n# from myconfig import DATAPATH,MYPATH\n\ndef get_params():\n    parser = argparse.ArgumentParser(description=\'Short sample app\')\n    parser.add_argument(\'-lstm\', action=""store"", default=150, dest=""lstm_units"", type=int)\n    parser.add_argument(\'-epochs\', action=""store"", default=20, dest=""epochs"", type=int)\n    parser.add_argument(\'-batch\', action=""store"", default=32, dest=""batch_size"", type=int)\n    parser.add_argument(\'-emb\', action=""store"", default=100, dest=""emb"", type=int)\n    parser.add_argument(\'-xmaxlen\', action=""store"", default=20, dest=""xmaxlen"", type=int)\n    parser.add_argument(\'-ymaxlen\', action=""store"", default=20, dest=""ymaxlen"", type=int)\n    parser.add_argument(\'-maxfeat\', action=""store"", default=35000, dest=""max_features"", type=int)\n    parser.add_argument(\'-classes\', action=""store"", default=351, dest=""num_classes"", type=int)\n    parser.add_argument(\'-sample\', action=""store"", default=1, dest=""samples"", type=int)\n    parser.add_argument(\'-nopad\', action=""store"", default=False, dest=""no_padding"", type=bool)\n    parser.add_argument(\'-lr\', action=""store"", default=0.001, dest=""lr"", type=float)\n    parser.add_argument(\'-load\', action=""store"", default=False, dest=""load_save"", type=bool)\n    parser.add_argument(\'-verbose\', action=""store"", default=False, dest=""verbose"", type=bool)\n    parser.add_argument(\'-train\', action=""store"", default=""train_all.txt"", dest=""train"")\n    parser.add_argument(\'-test\', action=""store"", default=""test_all.txt"", dest=""test"")\n    parser.add_argument(\'-dev\', action=""store"", default=""dev.txt"", dest=""dev"")\n    opts = parser.parse_args(sys.argv[1:])\n    print ""lstm_units"", opts.lstm_units\n    print ""epochs"", opts.epochs\n    print ""batch_size"", opts.batch_size\n    print ""emb"", opts.emb\n    print ""samples"", opts.samples\n    print ""xmaxlen"", opts.xmaxlen\n    print ""ymaxlen"", opts.ymaxlen\n    print ""max_features"", opts.max_features\n    print ""no_padding"", opts.no_padding\n    return opts\n\n\nclass AccCallBack(Callback):\n    def __init__(self, xtrain, ytrain, xdev, ydev, xtest, ytest, vocab, opts):\n        self.xtrain = xtrain\n        self.ytrain = ytrain\n        self.xdev = xdev\n        self.ydev = ydev\n        self.xtest = xtest\n        self.ytest = ytest\n        self.vocab = vocab\n        self.opts = opts\n\n    def on_epoch_end(self, epoch, logs={}):\n        train_acc = compute_acc(self.xtrain, self.ytrain, self.vocab, self.model, self.opts)\n        dev_acc = compute_acc(self.xdev, self.ydev, self.vocab, self.model, self.opts)\n        test_acc = compute_acc(self.xtest, self.ytest, self.vocab, self.model, self.opts)\n        logging.info(\'----------------------------------\')\n        logging.info(\'Epoch \' + str(epoch) + \' train loss:\' + str(logs.get(\'loss\')) + \' - Validation loss: \' + str(\n            logs.get(\'val_loss\')) + \' train acc: \' + str(train_acc[0]) + \'/\' + str(train_acc[1]) + \' dev acc: \' + str(\n            dev_acc[0]) + \'/\' + str(dev_acc[1]) + \' test acc: \' + str(test_acc[0]) + \'/\' + str(test_acc[1]))\n        logging.info(\'----------------------------------\')\n\n\ndef get_H_n(X):\n    ans = X[:, -1, :]  # get last element from time dim\n    return ans\n\n\ndef get_Y(X, xmaxlen):\n    return X[:, :xmaxlen, :]  # get first xmaxlen elem from time dim\n\n\ndef get_R(X):\n    Y, alpha = X[0], X[1]\n    ans = K.T.batched_dot(Y, alpha)\n    return ans\n\n\ndef build_model(opts, verbose=False):\n    k = 2 * opts.lstm_units  # 300\n    L = opts.xmaxlen  # 20\n    N = opts.xmaxlen + opts.ymaxlen + 1  # for delim\n    print ""x len"", L, ""total len"", N\n    print ""k"", k, ""L"", L\n\n    main_input = Input(shape=(N,), dtype=\'int32\', name=\'main_input\')\n    x = Embedding(output_dim=opts.emb, input_dim=opts.max_features, input_length=N, name=\'x\')(main_input)\n    drop_out = Dropout(0.1, name=\'dropout\')(x)\n    lstm_fwd = LSTM(opts.lstm_units, return_sequences=True, name=\'lstm_fwd\')(drop_out)\n    lstm_bwd = LSTM(opts.lstm_units, return_sequences=True, go_backwards=True, name=\'lstm_bwd\')(drop_out)\n    bilstm = merge([lstm_fwd, lstm_bwd], name=\'bilstm\', mode=\'concat\')\n    drop_out = Dropout(0.1)(bilstm)\n    h_n = Lambda(get_H_n, output_shape=(k,), name=""h_n"")(drop_out)\n    Y = Lambda(get_Y, arguments={""xmaxlen"": L}, name=""Y"", output_shape=(L, k))(drop_out)\n    Whn = Dense(k, W_regularizer=l2(0.01), name=""Wh_n"")(h_n)\n    Whn_x_e = RepeatVector(L, name=""Wh_n_x_e"")(Whn)\n    WY = TimeDistributed(Dense(k, W_regularizer=l2(0.01)), name=""WY"")(Y)\n    merged = merge([Whn_x_e, WY], name=""merged"", mode=\'sum\')\n    M = Activation(\'tanh\', name=""M"")(merged)\n\n    alpha_ = TimeDistributed(Dense(1, activation=\'linear\'), name=""alpha_"")(M)\n    flat_alpha = Flatten(name=""flat_alpha"")(alpha_)\n    alpha = Dense(L, activation=\'softmax\', name=""alpha"")(flat_alpha)\n\n    Y_trans = Permute((2, 1), name=""y_trans"")(Y)  # of shape (None,300,20)\n\n    r_ = merge([Y_trans, alpha], output_shape=(k, 1), name=""r_"", mode=get_R)\n\n    r = Reshape((k,), name=""r"")(r_)\n\n    Wr = Dense(k, W_regularizer=l2(0.01))(r)\n    Wh = Dense(k, W_regularizer=l2(0.01))(h_n)\n    merged = merge([Wr, Wh], mode=\'sum\')\n    h_star = Activation(\'tanh\')(merged)\n    out = Dense(3, activation=\'softmax\')(h_star)\n    output = out\n    model = Model(input=[main_input], output=output)\n    if verbose:\n        model.summary()\n    # plot(model, \'model.png\')\n    # # model.compile(loss={\'output\':\'binary_crossentropy\'}, optimizer=Adam())\n    # model.compile(loss={\'output\':\'categorical_crossentropy\'}, optimizer=Adam(options.lr))\n    model.compile(loss=\'categorical_crossentropy\',optimizer=Adam(options.lr))\n    return model\n\n\ndef compute_acc(X, Y, vocab, model, opts):\n    scores = model.predict(X, batch_size=options.batch_size)\n    prediction = np.zeros(scores.shape)\n    for i in range(scores.shape[0]):\n        l = np.argmax(scores[i])\n        prediction[i][l] = 1.0\n    assert np.array_equal(np.ones(prediction.shape[0]), np.sum(prediction, axis=1))\n    plabels = np.argmax(prediction, axis=1)\n    tlabels = np.argmax(Y, axis=1)\n    acc = accuracy(tlabels, plabels)\n    return acc, acc\n\n\ndef getConfig(opts):\n    conf = [opts.xmaxlen,\n            opts.ymaxlen,\n            opts.batch_size,\n            opts.emb,\n            opts.lr,\n            opts.samples,\n            opts.lstm_units,\n            opts.epochs]\n    if opts.no_padding:\n        conf.append(""no-pad"")\n    return ""_"".join(map(lambda x: str(x), conf))\n\n\ndef save_model(model, wtpath, archpath, mode=\'yaml\'):\n    if mode == \'yaml\':\n        yaml_string = model.to_yaml()\n        open(archpath, \'w\').write(yaml_string)\n    else:\n        with open(archpath, \'w\') as f:\n            f.write(model.to_json())\n    model.save_weights(wtpath)\n\n\ndef load_model(wtpath, archpath, mode=\'yaml\'):\n    if mode == \'yaml\':\n        model = model_from_yaml(open(archpath).read())  # ,custom_objects={""MyEmbedding"": MyEmbedding})\n    else:\n        with open(archpath) as f:\n            model = model_from_json(f.read())  # , custom_objects={""MyEmbedding"": MyEmbedding})\n    model.load_weights(wtpath)\n    return model\n\n\ndef concat_in_out(X, Y, vocab):\n    numex = X.shape[0]  # num examples\n    glue = vocab[""delimiter""] * np.ones(numex).reshape(numex, 1)\n    inp_train = np.concatenate((X, glue, Y), axis=1)\n    return inp_train\n\n\ndef setup_logger(config_str):\n    logging.basicConfig(level=logging.DEBUG,\n                        format=\'%(asctime)s %(name)-12s %(levelname)-8s %(message)s\',\n                        datefmt=\'%m-%d %H:%M\',\n                        filename=datetime.now().strftime(\'mylogfile_%H_%M_%d_%m_%Y.log\'),\n                        filemode=\'w\')\n\n\nif __name__ == ""__main__"":\n    options = get_params()\n    train = [l.strip().split(\'\\t\') for l in open(options.train)]\n    dev = [l.strip().split(\'\\t\') for l in open(options.dev)]\n    test = [l.strip().split(\'\\t\') for l in open(options.test)]\n    vocab = get_vocab(train)\n    print ""vocab (incr. maxfeatures accordingly):"",len(vocab)\n    X_train,Y_train,Z_train=load_data(train,vocab)\n    X_dev,Y_dev,Z_dev=load_data(dev,vocab)\n    X_test,Y_test,Z_test=load_data(test,vocab)\n    print \'Build model...\'\n    model = build_model(options)\n\n    config_str = getConfig(options)\n    MODEL_ARCH = ""arch_att"" + config_str + "".yaml""\n    MODEL_WGHT = ""weights_att"" + config_str + "".weights""\n\n    MAXLEN = options.xmaxlen\n    X_train = pad_sequences(X_train, maxlen=MAXLEN, value=vocab[""unk""], padding=\'pre\')\n    X_dev = pad_sequences(X_dev, maxlen=MAXLEN, value=vocab[""unk""], padding=\'pre\')\n    X_test = pad_sequences(X_test, maxlen=MAXLEN, value=vocab[""unk""], padding=\'pre\')\n    Y_train = pad_sequences(Y_train, maxlen=MAXLEN, value=vocab[""unk""], padding=\'post\')\n    Y_dev = pad_sequences(Y_dev, maxlen=MAXLEN, value=vocab[""unk""], padding=\'post\')\n    Y_test = pad_sequences(Y_test, maxlen=MAXLEN, value=vocab[""unk""], padding=\'post\')\n\n    net_train = concat_in_out(X_train, Y_train, vocab)\n    net_dev = concat_in_out(X_dev, Y_dev, vocab)\n    net_test = concat_in_out(X_test, Y_test, vocab)\n\n    Z_train = to_categorical(Z_train, nb_classes=3)\n    Z_dev = to_categorical(Z_dev, nb_classes=3)\n    Z_test = to_categorical(Z_test, nb_classes=3)\n\n    print X_train.shape, Y_train.shape, net_train.shape\n    print map_to_txt(net_train[0], vocab), Z_train[0]\n    print map_to_txt(net_train[1], vocab), Z_train[1]\n    setup_logger(config_str)\n\n    assert net_train[0][options.xmaxlen] == 1\n    train_dict = {\'input\': net_train, \'output\': Z_train}\n    dev_dict = {\'input\': net_dev, \'output\': Z_dev}\n    print \'Build model...\'\n    model = build_model(options)\n\n    logging.info(vars(options))\n    logging.info(\n        ""train size: "" + str(len(net_train)) + "" dev size: "" + str(len(net_dev)) + "" test size: "" + str(len(net_test)))\n    if options.load_save and os.path.exists(MODEL_ARCH) and os.path.exists(MODEL_WGHT):\n        print(""Loading pre-trained model from"", MODEL_WGHT)\n        load_model(MODEL_WGHT, MODEL_ARCH, \'json\')\n        train_acc = compute_acc(net_train, Z_train, vocab, model, options)\n        dev_acc = compute_acc(net_dev, Z_dev, vocab, model, options)\n        test_acc = compute_acc(net_test, Z_test, vocab, model, options)\n        print train_acc, dev_acc, test_acc\n    else:\n        # history = model.fit(train_dict,\n        #                     batch_size=options.batch_size,\n        #                     nb_epoch=options.epochs,\n        #                     validation_data=dev_dict,\n        #                     callbacks=[\n        #                         AccCallBack(net_train, Z_train, net_dev, Z_dev, net_test, Z_test, vocab, options)]\n        #                     )\n        history = model.fit(net_train,Z_train,\n                            batch_size=options.batch_size,\n                            nb_epoch=options.epochs,\n                            validation_data=(net_dev,Z_dev),\n                            callbacks=[\n                                AccCallBack(net_train, Z_train, net_dev, Z_dev, net_test, Z_test, vocab, options)]\n                            )\n        save_model(model, MODEL_WGHT, MODEL_ARCH)\n'"
myutils.py,0,"b'import sys\nimport re\nimport numpy as np\nimport argparse\nimport random\n# from keras import backend as K\ndef tokenize(sent):\n    \'\'\'\n    data_reader.tokenize(\'a#b\')\n    [\'a\', \'#\', \'b\']\n    \'\'\'\n    return [x.strip().lower() for x in re.split(\'(\\W+)?\', sent) if x.strip()]\n\ndef map_to_idx(x, vocab):\n    \'\'\'\n    x is a sequence of tokens\n    \'\'\'\n    # 0 is for UNK\n    return [ vocab[w] if w in vocab else 0 for w in x  ]\n\ndef map_to_txt(x,vocab):\n    textify=map_to_idx(x,inverse_map(vocab))\n    return \' \'.join(textify)\n    \ndef inverse_map(vocab):\n    return {v: k for k, v in vocab.items()}\n\ndef inverse_ids2txt(X_inp,Y_inp,vocabx,vocaby,outp=None):\n    \'\'\'\n    takes x,y int seqs and maps them back to strings\n    \'\'\'\n    inv_map_x = inverse_map(vocabx)\n    inv_map_y = inverse_map(vocaby)\n    if outp:\n        for x,y,z in zip(X_inp,Y_inp,outp):\n            print(\' \'.join(map_to_idx(x,inv_map_x)))\n            print(\' \'.join(map_to_idx(y,inv_map_y)))\n            print(z)\n    else:\n        for x,y in zip(X_inp,Y_inp):\n            print(\' \'.join(map_to_idx(x,inv_map_x)))\n            print(\' \'.join(map_to_idx(y,inv_map_y)))\n\n\ndef create_train_examples(X, Y, yspace, num=-1, balanced=True):\n    \'\'\'\n    :param X: X seq\n    :param Y: Y seq\n    :param yspace: from which to sample\n    :param num: how many negs, -1 means all of it\n    :return: x,y,z such that if x,y in X,Y then z=1 else 0\n    \'\'\'\n    X_inp = []\n    Y_inp = []\n    outp = []\n    for x, y in zip(X, Y):\n        neg_samples=yspace[:]  # copy\n        neg_samples.remove(y)\n        if num == -1:\n            pass\n        else:\n            neg_samples=[i for i in random.sample(neg_samples,num)]\n\n        if not balanced:\n            X_inp.append(x)\n            Y_inp.append(y)\n            outp.append([1.0, 0.0])\n\n        for yn in neg_samples:\n            if balanced:\n                X_inp.append(x)\n                Y_inp.append(y)\n                outp.append([1.0, 0.0])\n            X_inp.append(x)\n            Y_inp.append(yn)\n            outp.append([0.0, 1.0])\n\n    return X_inp, Y_inp, outp\n\n# def load_word2vec_embeddings(vocab_dim,index_dict,word_vectors,output_dim):\n#     vocab_dim = 300 # dimensionality of your word vectors\n#     n_symbols = len(index_dict) + 1 # adding 1 to account for 0th index (for masking)\n#     embedding_weights = np.zeros((n_symbols+1,vocab_dim))\n#     for word,index in index_dict.items():\n#         embedding_weights[index,:] = word_vectors[word]\n#\n#     return Embedding(output_dim=output_dim, input_dim=n_symbols + 1, mask_zero=True, weights=[embedding_weights]) # note you have to put embedding weights in a list by convention\n\ndef check_layer_output_shape(layer, input_data):\n    ndim = len(input_data.shape)\n    layer.input = K.placeholder(ndim=ndim)\n    layer.set_input_shape(input_data.shape)\n    expected_output_shape = layer.output_shape[1:]\n\n    function = K.function([layer.input], [layer.get_output()])\n    output = function([input_data])[0]\n    print(output.shape,expected_output_shape)\n    assert output.shape[1:] == expected_output_shape\n\ndef mytest():\n    input_data = np.random.random((10, 142, 200))\n    Y = np.random.random((10, 142, 200))\n    alpha = np.random.random((10, 142))\n    print(input_data.shape)\n    # layer = Reshape(dims=(2, 3))\n    # layer = Lambda(get_H_n, output_shape=(200,))\n    # Y = Layer()\n    # alpha= Layer()\n    # Y.set_input_shape((None,142,200))\n    # alpha.set_input_shape((None,142))\n    # ll=Merge([Y,alpha],mode=\'join\')\n    # layer=Lambda(get_R, output_shape=(200,1))\n    # layer.set_previous(ll)\n    # print(layer.input)\n    # func = K.function([layer.input], [layer.get_output()])\n    # layer=Lambda(get_Y, output_shape=(110, 200))\n    # check_layer_output_shape(layer, input_data)\n    sys.exit(0)\n\ndef show_weights(model,node_name,indices=[0]):\n    Wr=model.nodes[node_name].get_weights()\n    for i in indices:\n        print(Wr[i][0:5])\n\n\ndef show_output(model,node_name,input_data_dict):\n    lout= K.function([model.inputs[i].input for i in model.input_order],\n                     [model.nodes[node_name].get_output(train=False)])\n    output= lout([input_data_dict[i] for i in model.input_order])[0]\n    print(\'input\', input_data_dict[\'input\'][0][0:10])\n    print(node_name, output[0][0:5])\n    print(\'input\', input_data_dict[\'input\'][1][0:10])\n    print(node_name, output[1][0:5])\n\n\ndef categorize(ll):\n    new_y_train = []\n    for y in ll:\n        if y == 1:\n            new_y_train += [[0, 1]]\n        else:\n            new_y_train += [[1, 0]]\n    return np.asarray(new_y_train)\n\nif __name__==""__main__"":\n    pass\n'"
reader.py,0,"b'# coding: utf-8\nimport json\nfrom myutils import *\nfrom keras.preprocessing.sequence import pad_sequences\nfrom collections import Counter\n\ndef get_data(data,vocab):\n    for d in data:\n        prem=map_to_idx(tokenize(d[""sentence1""]),vocab)\n        hyp=map_to_idx(tokenize(d[""sentence2""]),vocab)\n        label=d[""gold_label""]\n        yield prem, hyp , label\n\ndef load_data(train,vocab,labels={\'neutral\':0,\'entailment\':1,\'contradiction\':2}):\n    X,Y,Z=[],[],[]\n    for p,h,l in train:\n        p=map_to_idx(tokenize(p),vocab)\n        h=map_to_idx(tokenize(h),vocab)\n        # print \'P:\',map_to_txt(p,vocab)\n        # print \'H:\',map_to_txt(h,vocab)\n        # print p+"" DELIMITER ""+h\n        # ph=map_to_idx(tokenize(p+"" delimiter ""+h),vocab)\n        # print \'PH:\',map_to_txt(ph,vocab)\n        # print \'L:\',l\n        if l in labels:         # get rid of \'-\'\n            X+=[p]\n            Y+=[h]\n            Z+=[labels[l]]\n    return X,Y,Z\n\n\ndef get_vocab(data):\n    vocab=Counter()\n    for ex in data:\n        tokens=tokenize(ex[0])\n        tokens+=tokenize(ex[1])\n        vocab.update(tokens)\n    lst = [""unk"", ""delimiter""] + [ x for x, y in vocab.iteritems() if y > 0]\n    vocab = dict([ (y,x) for x,y in enumerate(lst) ])\n    return vocab\n\ndef convert2simple(data,out):\n    \'\'\'\n    get the good stuff out of json into a tsv file\n    \'\'\'\n    for d in data:\n        print>>out, d[""sentence1""]+""\\t""+d[""sentence2""]+""\\t""+d[""gold_label""]\n    out.close()\n\n\nif __name__==""__main__"":\n\n    train=[l.strip().split(\'\\t\') for l in open(\'train.txt\')][:20000]\n    dev=[l.strip().split(\'\\t\') for l in open(\'dev.txt\')]\n    test=[l.strip().split(\'\\t\') for l in open(\'test.txt\')]\n    labels={\'contradiction\':-1,\'neutral\':0,\'entailment\':1}\n\n    vocab=get_vocab(train)\n    X_train,Y_train,Z_train=load_data(train,vocab)\n    X_dev,Y_dev,Z_dev=load_data(dev,vocab)\n    print len(X_train),X_train[0]\n    print len(X_dev),X_dev[0]\n'"
tf_model.py,62,"b'import os\nimport sys\nimport tensorflow as tf\nimport numpy as np\nfrom collections import Counter, defaultdict\nfrom itertools import count\nimport random\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical, accuracy\nimport time\n\nV = 10000\ndim = 20\nk = 100  # opts.lstm_units\n\n\ndef get_params():\n    parser = argparse.ArgumentParser(description=\'Short sample app\')\n    parser.add_argument(\'-lstm\', action=""store"", default=150, dest=""lstm_units"", type=int)\n    parser.add_argument(\'-epochs\', action=""store"", default=20, dest=""epochs"", type=int)\n    parser.add_argument(\'-batch\', action=""store"", default=32, dest=""batch_size"", type=int)\n    parser.add_argument(\'-emb\', action=""store"", default=100, dest=""emb"", type=int)\n    parser.add_argument(\'-xmaxlen\', action=""store"", default=120, dest=""xmaxlen"", type=int)\n    parser.add_argument(\'-ymaxlen\', action=""store"", default=70, dest=""ymaxlen"", type=int)\n    parser.add_argument(\'-maxfeat\', action=""store"", default=35000, dest=""max_features"", type=int)\n    parser.add_argument(\'-classes\', action=""store"", default=3, dest=""num_classes"", type=int)\n    parser.add_argument(\'-sample\', action=""store"", default=1, dest=""samples"", type=int)\n    parser.add_argument(\'-nopad\', action=""store"", default=False, dest=""no_padding"", type=bool)\n    parser.add_argument(\'-lr\', action=""store"", default=0.001, dest=""lr"", type=float)\n    parser.add_argument(\'-load\', action=""store"", default=False, dest=""load_save"", type=bool)\n    parser.add_argument(\'-verbose\', action=""store"", default=False, dest=""verbose"", type=bool)\n    parser.add_argument(\'-train\', action=""store"", default=""train_all.txt"", dest=""train"")\n    parser.add_argument(\'-test\', action=""store"", default=""test_all.txt"", dest=""test"")\n    parser.add_argument(\'-dev\', action=""store"", default=""dev.txt"", dest=""dev"")\n    opts = parser.parse_args(sys.argv[1:])\n    print (""lstm_units"", opts.lstm_units)\n    print (""epochs"", opts.epochs)\n    print (""batch_size"", opts.batch_size)\n    print (""emb"", opts.emb)\n    print (""samples"", opts.samples)\n    print (""xmaxlen"", opts.xmaxlen)\n    print (""ymaxlen"", opts.ymaxlen)\n    print (""max_features"", opts.max_features)\n    print (""no_padding"", opts.no_padding)\n    return opts\n\n\nclass CustomModel:\n    def __init__(self, opts, sess, XMAXLEN, YMAXLEN, vocab, batch_size=1000):\n        self.dim = 100\n        self.sess = sess\n        self.h_dim = opts.lstm_units\n        self.batch_size = batch_size\n        self.vocab_size = len(vocab)\n        self.XMAXLEN = XMAXLEN\n        self.YMAXLEN = YMAXLEN\n\n    # def last_relevant(output, length):\n    #     batch_size = tf.shape(output)[0]\n    #     max_length = tf.shape(output)[1]\n    #     out_size = int(output.get_shape()[2])\n    #     index = tf.range(0, batch_size) * max_length + (length - 1)\n    #     flat = tf.reshape(output, [-1, out_size])\n    #     relevant = tf.gather(flat, index)\n    #     return relevant\n\n    # def repeat(x, n):\n    #     \'\'\'Repeats a 2D tensor:\n    #     if x has shape (samples, dim) and n=2,\n    #     the output will have shape (samples, 2, dim)\n    #     \'\'\'\n    #     x = tf.expand_dims(x, 1)\n    #     pattern = tf.pack([1, n, 1])\n    #     return tf.tile(x, pattern)\n\n    def build_model(self):\n        self.x = tf.placeholder(tf.int32, [self.batch_size, self.XMAXLEN], name=""premise"")\n        self.x_length = tf.placeholder(tf.int32, [self.batch_size], name=""premise_len"")\n        self.y = tf.placeholder(tf.int32, [self.batch_size, self.YMAXLEN], name=""hypothesis"")\n        self.y_length = tf.placeholder(tf.int32, [self.batch_size], name=""hyp_len"")\n        self.target = tf.placeholder(tf.float32, [self.batch_size,3], name=""label"")  # change this to int32 and it breaks.\n\n        # DO NOT DO THIS\n        # self.batch_size = tf.shape(self.x)[0]  # batch size\n        # self.x_length = tf.shape(self.x)[1]  # batch size\n        # print self.batch_size,self.x_length\n\n        self.embed_matrix = tf.get_variable(""embeddings"", [self.vocab_size, self.dim])\n        self.x_emb = tf.nn.embedding_lookup(self.embed_matrix, self.x)\n        self.y_emb = tf.nn.embedding_lookup(self.embed_matrix, self.y)\n\n        print self.x_emb, self.y_emb\n        with tf.variable_scope(""encode_x""):\n            self.fwd_lstm = tf.nn.rnn_cell.BasicLSTMCell(self.h_dim, state_is_tuple=True)\n            self.x_output, self.x_state = tf.nn.dynamic_rnn(cell=self.fwd_lstm, inputs=self.x_emb, dtype=tf.float32)\n            # self.x_output, self.x_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=self.fwd_lstm,cell_bw=self.bwd_lstm,inputs=self.x_emb,dtype=tf.float32)\n            print self.x_output\n            # print self.x_state\n        # print tf.shape(self.x)\n        with tf.variable_scope(""encode_y""):\n            self.fwd_lstm = tf.nn.rnn_cell.BasicLSTMCell(self.h_dim, state_is_tuple=True)\n            self.y_output, self.y_state = tf.nn.dynamic_rnn(cell=self.fwd_lstm, inputs=self.y_emb,\n                                                            initial_state=self.x_state, dtype=tf.float32)\n            # print self.y_output\n            # print self.y_state\n\n        self.Y = self.x_output  # its length must be x_length\n\n        # self.h_n = self.last_relevant(self.y_output,self.x_length)   # TODO\n        tmp5= tf.transpose(self.y_output, [1, 0, 2])\n        self.h_n = tf.gather(tmp5, int(tmp5.get_shape()[0]) - 1)\n        print self.h_n\n\n        # self.h_n_repeat = self.repeat(self.h_n,self.x_length)   # TODO\n        self.h_n_repeat = tf.expand_dims(self.h_n, 1)\n        pattern = tf.pack([1, self.XMAXLEN, 1])\n        self.h_n_repeat = tf.tile(self.h_n_repeat, pattern)\n\n        self.W_Y = tf.get_variable(""W_Y"", shape=[self.h_dim, self.h_dim])\n        self.W_h = tf.get_variable(""W_h"", shape=[self.h_dim, self.h_dim])\n\n        # TODO compute M = tanh(W*Y + W*[h_n...])\n        tmp1 = tf.matmul(tf.reshape(self.Y, shape=[self.batch_size * self.XMAXLEN, self.h_dim]), self.W_Y,\n                         name=""Wy"")\n        self.Wy = tf.reshape(tmp1, shape=[self.batch_size, self.XMAXLEN, self.h_dim]);\n        tmp2 = tf.matmul(tf.reshape(self.h_n_repeat, shape=[self.batch_size * self.XMAXLEN, self.h_dim]), self.W_h)\n        self.Whn = tf.reshape(tmp2, shape=[self.batch_size, self.XMAXLEN, self.h_dim], name=""Whn"");\n        self.M = tf.tanh(tf.add(self.Wy, self.Whn), name=""M"")\n        # print ""M"",self.M\n\n        # use attention\n        self.W_att = tf.get_variable(""W_att"",shape=[self.h_dim,1]) # h x 1\n        tmp3 = tf.matmul(tf.reshape(self.M,shape=[self.batch_size*self.XMAXLEN,self.h_dim]),self.W_att)\n        # need 1 here so that later can do multiplication with h x L\n        self.att = tf.nn.softmax(tf.reshape(tmp3,shape=[self.batch_size,1, self.XMAXLEN],name=""att"")) # nb x 1 x Xmax\n        # print ""att"",self.att\n\n        # COMPUTE WEIGHTED\n        self.r = tf.reshape(tf.batch_matmul(self.att, self.Y, name=""r""),shape=[self.batch_size,self.h_dim])  # (nb,1,L) X (nb,L,k) = (nb,1,k)\n        # get last step of Y as r which is (batch,k)\n        # tmp4 = tf.transpose(self.Y, [1, 0, 2])\n        # self.r = tf.gather(tmp4, int(tmp4.get_shape()[0]) - 1)\n        # print ""r"",self.r\n\n        self.W_p, self.b_p= tf.get_variable(""W_p"", shape=[self.h_dim, self.h_dim]), tf.get_variable(""b_p"",shape=[self.h_dim],initializer=tf.constant_initializer())\n        self.W_x, self.b_x = tf.get_variable(""W_x"", shape=[self.h_dim, self.h_dim]), tf.get_variable(""b_x"",shape=[self.h_dim],initializer=tf.constant_initializer())\n        self.Wpr = tf.matmul(self.r, self.W_p, name=""Wy"") + self.b_p\n        self.Wxhn = tf.matmul(self.h_n, self.W_x, name=""Wxhn"") + self.b_x\n        self.hstar = tf.tanh(tf.add(self.Wpr, self.Wxhn), name=""hstar"")\n        # print ""Wpr"",self.Wpr\n        # print ""Wxhn"",self.Wxhn\n        # print ""hstar"",self.hstar\n\n        self.W_pred = tf.get_variable(""W_pred"", shape=[self.h_dim, 3])\n        self.pred = tf.nn.softmax(tf.matmul(self.hstar, self.W_pred), name=""pred_layer"")\n        # print ""pred"",self.pred,""target"",self.target\n        correct = tf.equal(tf.argmax(self.pred,1),tf.argmax(self.target,1))\n        self.acc = tf.reduce_mean(tf.cast(correct, ""float""), name=""accuracy"")\n        # self.H_n = self.last_relevant(self.en_output)\n        self.loss = -tf.reduce_sum(self.target * tf.log(self.pred), name=""loss"")\n        # print self.loss\n        self.optimizer = tf.train.AdamOptimizer()\n        self.optim = self.optimizer.minimize(self.loss, var_list=tf.trainable_variables())\n        _ = tf.scalar_summary(""loss"", self.loss)\n\n    def train(self,\\\n              xdata, ydata, zdata, x_lengths, y_lengths,\\\n              xxdata, yydata, zzdata, xx_lengths, yy_lengths,\\\n              MAXITER):\n        merged_sum = tf.merge_all_summaries()\n        # writer = tf.train.SummaryWriter(""./logs/%s"" % ""modeldir"", self.sess.graph_def)\n        tf.initialize_all_variables().run()\n        start_time = time.time()\n        for ITER in range(MAXITER):\n            # xdata, ydata, zdata, x_lengths, y_lengths = joint_shuffle(xdata, ydata, zdata, x_lengths, y_lengths)\n            for i in xrange(0, len(l), self.batch_size):\n                x,y,z,xlen,ylen=xdata[i:i + self.batch_size],\\\n                                ydata[i:i + self.batch_size],\\\n                                zdata[i:i + self.batch_size],\\\n                                x_lengths[i:i + self.batch_size],\\\n                                y_lengths[i:i + self.batch_size]\n                feed_dict = {self.x: x,\\\n                             self.y: y,\\\n                             self.target: z,\\\n                             self.x_length:xlen,\\\n                             self.y_length:ylen}\n                att, _ , loss, acc, summ = self.sess.run([self.att,self.optim, self.loss, self.acc, merged_sum],feed_dict=feed_dict)\n                # print ""att for 0th"",att[0]\n                print ""loss"",loss, ""acc on train"", acc\n            total_test_acc=[]\n            for i in xrange(0, len(l), self.batch_size):\n                x,y,z,xlen,ylen=xxdata[i:i + self.batch_size],\\\n                                yydata[i:i + self.batch_size],\\\n                                zzdata[i:i + self.batch_size],\\\n                                xx_lengths[i:i + self.batch_size],\\\n                                yy_lengths[i:i + self.batch_size]\n                tfeed_dict = {self.x: x,\\\n                              self.y: y,\\\n                              self.target: z,\\\n                              self.x_length:xlen,\\\n                              self.y_length:ylen}\n                att, _ , test_loss, test_acc, summ = self.sess.run([self.att,self.optim, self.loss, self.acc, merged_sum],feed_dict=tfeed_dict)\n                total_test_acc.append(test_acc)\n            print ""acc on test"",np.mean(total_test_acc)\n        # for x, y, z in zip(xdata, ydata, zdata):\n            # print x, y, z\n            # feeddict = {self.x: x, self.y: y, self.target: z, self.x_length:x_lengths, self.y_length:y_lengths}\n            # self.sess.run([self.optim, self.loss, merged_sum],feed_dict=feeddict);\n        elapsed_time = time.time() - start_time\n        print ""total time"",elapsed_time\n\ndef joint_shuffle(xdata, ydata, zdata, x_lengths, y_lengths):\n    tmp=list(zip(xdata, ydata, zdata, x_lengths, y_lengths))\n    random.shuffle(tmp)\n    xdata, ydata, zdata, x_lengths, y_lengths = zip(*tmp)\n    return xdata, ydata, zdata, x_lengths, y_lengths\nif __name__ == ""__main__"":\n    from reader import *\n    from myutils import *\n\n    options = get_params()\n    train = [l.strip().split(\'\\t\') for l in open(options.train)]\n    dev = [l.strip().split(\'\\t\') for l in open(options.dev)]\n    test = [l.strip().split(\'\\t\') for l in open(options.test)]\n    vocab = get_vocab(train)\n\n    X_train, Y_train, Z_train = load_data(train, vocab)\n    X_dev, Y_dev, Z_dev = load_data(dev, vocab)\n    X_test, Y_test, Z_test = load_data(test, vocab)\n    # print Z_train[1]\n    # sys.exit()\n\n    X_train_lengths = [len(x) for x in X_train]\n    X_dev_lengths = np.asarray([len(x) for x in X_dev]).reshape(len(X_dev))\n    X_test_lengths = np.asarray([len(x) for x in X_test]).reshape(len(X_test))\n    # print len(X_test_lengths)\n\n    Y_train_lengths = np.asarray([len(x) for x in Y_train]).reshape(len(Y_train))\n    Y_dev_lengths = np.asarray([len(x) for x in Y_dev]).reshape(len(Y_dev))\n    Y_test_lengths = np.asarray([len(x) for x in Y_test]).reshape(len(Y_test))\n    # print len(Y_test_lengths)\n\n    Z_train = to_categorical(Z_train, nb_classes=options.num_classes)\n    Z_dev = to_categorical(Z_dev, nb_classes=options.num_classes)\n    Z_test = to_categorical(Z_test, nb_classes=options.num_classes)\n    # print Z_train[0]\n\n    XMAXLEN = options.xmaxlen\n    YMAXLEN = options.ymaxlen\n    MAXITER = 1000\n    X_train = pad_sequences(X_train, maxlen=XMAXLEN, value=vocab[""unk""], padding=\'post\') ## NO NEED TO GO TO NUMPY , CAN GIVE LIST OF PADDED LIST\n    X_dev = pad_sequences(X_dev, maxlen=XMAXLEN, value=vocab[""unk""], padding=\'post\')\n    X_test = pad_sequences(X_test, maxlen=XMAXLEN, value=vocab[""unk""], padding=\'post\')\n    Y_train = pad_sequences(Y_train, maxlen=YMAXLEN, value=vocab[""unk""], padding=\'post\')\n    Y_dev = pad_sequences(Y_dev, maxlen=YMAXLEN, value=vocab[""unk""], padding=\'post\')\n    Y_test = pad_sequences(Y_test, maxlen=YMAXLEN, value=vocab[""unk""], padding=\'post\')\n    print X_test.shape, X_test_lengths.shape\n    vocab = get_vocab(train)\n    with tf.Session() as sess:\n        model = CustomModel(options, sess, XMAXLEN, YMAXLEN, vocab, batch_size=200)\n        model.build_model()\n        model.train(X_train,Y_train,Z_train,X_train_lengths,Y_train_lengths,\\\n                    X_test,Y_test,Z_test,X_test_lengths,Y_test_lengths,\\\n                    MAXITER)\n'"
