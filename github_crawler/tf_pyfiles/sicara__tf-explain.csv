file_path,api_count,code
setup.py,0,"b'from setuptools import setup\nfrom setuptools import find_packages\n\nwith open(""README.md"", ""r"") as f:\n    long_description = f.read()\n\n\nsetup(\n    name=""tf-explain"",\n    version=""0.2.1"",\n    description=""Interpretability Callbacks for Tensorflow 2.0"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    author=""Rapha\xc3\xabl Meudec"",\n    author_email=""raphaelm@sicara.com"",\n    url=""https://github.com/sicara/tf-explain"",\n    license=""MIT"",\n    install_requires=[""opencv-python>=4.1.0.25""],\n    extras_require={\n        ""tests"": [\n            ""black>=19.3b0"",\n            ""pylint>=2.3.1"",\n            ""pytest>=5.0.1"",\n            ""pytest-timeout>=1.3.3"",\n            ""pytest-mock>=1.10.4"",\n            ""pytest-cov>=2.7.1"",\n            ""tox>=3.13.2"",\n        ],\n        ""publish"": [""bumpversion>=0.5.3"", ""twine>=1.13.0""],\n        ""docs"": [""sphinx>=2.1.2"", ""sphinx-rtd-theme>=0.4.3""],\n    },\n    packages=find_packages(),\n    python_requires="">=3.6"",\n    classifiers=[\n        ""Development Status :: 3 - Alpha"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Programming Language :: Python"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Operating System :: OS Independent"",\n    ],\n)\n'"
examples/__init__.py,0,b''
tests/__init__.py,0,b''
tests/conftest.py,6,"b'import shutil\nfrom pathlib import Path\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\n\n@pytest.fixture(scope=""session"")\ndef tests_path():\n    return Path(__file__).parent.absolute()\n\n\n@pytest.fixture(scope=""session"")\ndef root_path(tests_path):\n    return tests_path / ""..""\n\n\n@pytest.fixture()\ndef output_dir(tests_path):\n    output_dir = tests_path / ""output""\n    yield output_dir\n    shutil.rmtree(output_dir)\n\n\n@pytest.fixture(scope=""session"")\ndef random_data():\n    batch_size = 4\n    x = np.random.random((batch_size, 28, 28, 3))\n    y = tf.keras.utils.to_categorical(\n        np.random.randint(2, size=batch_size), num_classes=2\n    ).astype(""uint8"")\n\n    return x, y\n\n\n@pytest.fixture()\ndef convolutional_model(random_data):\n    x, y = random_data\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Conv2D(\n                16,\n                (3, 3),\n                activation=None,\n                name=""conv_1"",\n                input_shape=list(x.shape[1:]),\n            ),\n            tf.keras.layers.ReLU(name=""activation_1""),\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(2, activation=""softmax""),\n        ]\n    )\n\n    model.compile(optimizer=""adam"", loss=""categorical_crossentropy"")\n\n    return model\n'"
tf_explain/__init__.py,0,"b'""""""\nTF-explain Library\n\nThe library implements interpretability methods as Tensorflow 2.0\ncallbacks to ease neural network\'s understanding.\n""""""\n\n__version__ = ""0.2.1""\n\nfrom . import core\nfrom . import callbacks\nfrom . import utils\n'"
docs/source/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(""..""))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = ""tf-explain""\ncopyright = ""2019, Sicara""\nauthor = ""Sicara""\n\nmaster_doc = ""index""\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [""sphinx.ext.autodoc"", ""sphinx.ext.coverage"", ""sphinx.ext.napoleon""]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [""_static""]\n'"
examples/callbacks/mnist.py,16,"b'import numpy as np\nimport tensorflow as tf\nimport tf_explain\n\nINPUT_SHAPE = (28, 28, 1)\nNUM_CLASSES = 10\n\nAVAILABLE_DATASETS = {\n    ""mnist"": tf.keras.datasets.mnist,\n    ""fashion_mnist"": tf.keras.datasets.fashion_mnist,\n}\nDATASET_NAME = ""fashion_mnist""  # Choose between ""mnist"" and ""fashion_mnist""\n\n# Load dataset\ndataset = AVAILABLE_DATASETS[DATASET_NAME]\n(train_images, train_labels), (test_images, test_labels) = dataset.load_data()\n\n# Convert from (28, 28) images to (28, 28, 1)\ntrain_images = train_images[..., tf.newaxis].astype(""float32"") / 255.0\ntest_images = test_images[..., tf.newaxis].astype(""float32"") / 255.0\n\n# One hot encore labels 0, 1, .., 9 to [0, 0, .., 1, 0, 0]\ntrain_labels = tf.keras.utils.to_categorical(train_labels, num_classes=NUM_CLASSES)\ntest_labels = tf.keras.utils.to_categorical(test_labels, num_classes=NUM_CLASSES)\n\n# Create model\nimg_input = tf.keras.Input(INPUT_SHAPE)\n\nx = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=""relu"")(img_input)\nx = tf.keras.layers.Conv2D(\n    filters=64, kernel_size=(3, 3), activation=""relu"", name=""target_layer""\n)(x)\nx = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)\n\nx = tf.keras.layers.Dropout(0.25)(x)\nx = tf.keras.layers.Flatten()(x)\n\nx = tf.keras.layers.Dense(128, activation=""relu"")(x)\nx = tf.keras.layers.Dropout(0.5)(x)\n\nx = tf.keras.layers.Dense(NUM_CLASSES, activation=""softmax"")(x)\n\nmodel = tf.keras.Model(img_input, x)\nmodel.compile(optimizer=""adam"", loss=""categorical_crossentropy"", metrics=[""accuracy""])\n\n# Select a subset of the validation data to examine\n# Here, we choose 5 elements with one hot encoded label ""0"" == [1, 0, 0, .., 0]\nvalidation_class_zero = (\n    np.array(\n        [\n            el\n            for el, label in zip(test_images, test_labels)\n            if np.all(np.argmax(label) == 0)\n        ][0:5]\n    ),\n    None,\n)\n# Select a subset of the validation data to examine\n# Here, we choose 5 elements with one hot encoded label ""4"" == [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\nvalidation_class_four = (\n    np.array(\n        [\n            el\n            for el, label in zip(test_images, test_labels)\n            if np.all(np.argmax(label) == 4)\n        ][0:5]\n    ),\n    None,\n)\n\n# Instantiate callbacks\n# class_index value should match the validation_data selected above\ncallbacks = [\n    tf_explain.callbacks.GradCAMCallback(\n        validation_class_zero, class_index=0, layer_name=""target_layer""\n    ),\n    tf_explain.callbacks.GradCAMCallback(\n        validation_class_four, class_index=4, layer_name=""target_layer""\n    ),\n    tf_explain.callbacks.ActivationsVisualizationCallback(\n        validation_class_zero, layers_name=[""target_layer""]\n    ),\n    tf_explain.callbacks.SmoothGradCallback(\n        validation_class_zero, class_index=0, num_samples=15, noise=1.0\n    ),\n    tf_explain.callbacks.IntegratedGradientsCallback(\n        validation_class_zero, class_index=0, n_steps=10\n    ),\n    tf_explain.callbacks.VanillaGradientsCallback(validation_class_zero, class_index=0),\n    tf_explain.callbacks.GradientsInputsCallback(validation_class_zero, class_index=0),\n]\n\n# Start training\nmodel.fit(train_images, train_labels, epochs=5, callbacks=callbacks)\n'"
examples/core/__init__.py,0,b''
examples/core/activations_visualization.py,3,"b'import numpy as np\nimport tensorflow as tf\n\nfrom tf_explain.core.activations import ExtractActivations\n\ntarget_layers = [\n    ""conv1_relu""\n]  # Could be either the output of a Conv2D, or an activation\nIMAGE_PATH = ""./cat.jpg""\n\nif __name__ == ""__main__"":\n    model = tf.keras.applications.resnet50.ResNet50(\n        weights=""imagenet"", include_top=True\n    )\n\n    img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    model.summary()\n    data = (np.array([img]), None)\n\n    explainer = ExtractActivations()\n    # Compute Activations of layer activation_1\n    grid = explainer.explain(data, model, target_layers)\n    explainer.save(grid, ""."", ""activations.png"")\n'"
examples/core/grad_cam.py,3,"b'import tensorflow as tf\n\nfrom tf_explain.core.grad_cam import GradCAM\n\nIMAGE_PATH = ""./cat.jpg""\n\nif __name__ == ""__main__"":\n    model = tf.keras.applications.vgg16.VGG16(weights=""imagenet"", include_top=True)\n\n    img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    model.summary()\n    data = ([img], None)\n\n    tabby_cat_class_index = 281\n    explainer = GradCAM()\n    # Compute GradCAM on VGG16\n    grid = explainer.explain(\n        data, model, class_index=tabby_cat_class_index, layer_name=""block5_conv3""\n    )\n    explainer.save(grid, ""."", ""grad_cam.png"")\n'"
examples/core/gradients_inputs.py,3,"b'import numpy as np\nimport tensorflow as tf\n\nfrom tf_explain.core.gradients_inputs import GradientsInputs\n\n\nIMAGE_PATH = ""examples/core/cat.jpg""\n\nif __name__ == ""__main__"":\n    model = tf.keras.applications.vgg16.VGG16(weights=""imagenet"", include_top=True)\n\n    img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    model.summary()\n    data = (np.array([img]), None)\n\n    tabby_cat_class_index = 281\n    explainer = GradientsInputs()\n    # Compute GradientsInputs on VGG16\n    grid = explainer.explain(data, model, tabby_cat_class_index)\n    explainer.save(grid, ""."", ""gradients_inputs.png"")\n'"
examples/core/integrated_gradients.py,3,"b'import tensorflow as tf\n\nfrom tf_explain.core.integrated_gradients import IntegratedGradients\n\nIMAGE_PATH = ""./cat.jpg""\n\nif __name__ == ""__main__"":\n    model = tf.keras.applications.vgg16.VGG16(weights=""imagenet"", include_top=True)\n\n    img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    model.summary()\n    data = ([img], None)\n\n    tabby_cat_class_index = 281\n    explainer = IntegratedGradients()\n    # Compute SmoothGrad on VGG16\n    grid = explainer.explain(data, model, tabby_cat_class_index, n_steps=15)\n    explainer.save(grid, ""."", ""integrated_gradients.png"")\n'"
examples/core/occlusion_sensitivity.py,3,"b'import tensorflow as tf\n\nfrom tf_explain.core.occlusion_sensitivity import OcclusionSensitivity\n\nIMAGE_PATH = ""./cat.jpg""\n\nif __name__ == ""__main__"":\n    model = tf.keras.applications.resnet50.ResNet50(\n        weights=""imagenet"", include_top=True\n    )\n\n    img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    model.summary()\n    data = ([img], None)\n\n    tabby_cat_class_index = 281\n    explainer = OcclusionSensitivity()\n    # Compute Occlusion Sensitivity for patch_size 20\n    grid = explainer.explain(data, model, tabby_cat_class_index, 20)\n    explainer.save(grid, ""."", ""occlusion_sensitivity_20.png"")\n    # Compute Occlusion Sensitivity for patch_size 10\n    grid = explainer.explain(data, model, tabby_cat_class_index, 10)\n    explainer.save(grid, ""."", ""occlusion_sensitivity_10.png"")\n'"
examples/core/smoothgrad.py,3,"b'import tensorflow as tf\n\nfrom tf_explain.core.smoothgrad import SmoothGrad\n\nIMAGE_PATH = ""./cat.jpg""\n\nif __name__ == ""__main__"":\n    model = tf.keras.applications.vgg16.VGG16(weights=""imagenet"", include_top=True)\n\n    img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    model.summary()\n    data = ([img], None)\n\n    tabby_cat_class_index = 281\n    explainer = SmoothGrad()\n    # Compute SmoothGrad on VGG16\n    grid = explainer.explain(data, model, tabby_cat_class_index, 20, 1.0)\n    explainer.save(grid, ""."", ""smoothgrad.png"")\n'"
examples/core/vanilla_gradients.py,3,"b'import numpy as np\nimport tensorflow as tf\n\nfrom tf_explain.core.vanilla_gradients import VanillaGradients\n\n\nIMAGE_PATH = ""./cat.jpg""\n\nif __name__ == ""__main__"":\n    model = tf.keras.applications.vgg16.VGG16(weights=""imagenet"", include_top=True)\n\n    img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    model.summary()\n    data = (np.array([img]), None)\n\n    tabby_cat_class_index = 281\n    explainer = VanillaGradients()\n    # Compute VanillaGradients on VGG16\n    grid = explainer.explain(data, model, tabby_cat_class_index)\n    explainer.save(grid, ""."", ""vanilla_gradients.png"")\n'"
tests/callbacks/__init__.py,0,b''
tests/callbacks/test_activations_visualization.py,0,"b'import numpy as np\nfrom tf_explain.callbacks.activations_visualization import (\n    ActivationsVisualizationCallback,\n)\n\n\ndef test_should_call_activations_visualization_callback(\n    random_data, convolutional_model, output_dir, mocker\n):\n    mock_explainer = mocker.MagicMock(\n        explain=mocker.MagicMock(return_value=np.zeros((28, 28)))\n    )\n    mocker.patch(\n        ""tf_explain.callbacks.activations_visualization.ExtractActivations"",\n        return_value=mock_explainer,\n    )\n\n    images, labels = random_data\n\n    callbacks = [\n        ActivationsVisualizationCallback(\n            validation_data=random_data,\n            layers_name=[""activation_1""],\n            output_dir=output_dir,\n        )\n    ]\n\n    convolutional_model.fit(images, labels, batch_size=2, epochs=1, callbacks=callbacks)\n\n    mock_explainer.explain.assert_called_once_with(\n        random_data, convolutional_model, [""activation_1""]\n    )\n    assert len([_ for _ in output_dir.iterdir()]) == 1\n'"
tests/callbacks/test_grad_cam.py,0,"b'import numpy as np\nfrom tf_explain.callbacks.grad_cam import GradCAMCallback\n\n\ndef test_should_call_grad_cam_callback(\n    random_data, convolutional_model, output_dir, mocker\n):\n    mock_explainer = mocker.MagicMock(\n        explain=mocker.MagicMock(return_value=np.zeros((28, 28, 3)))\n    )\n    mocker.patch(""tf_explain.callbacks.grad_cam.GradCAM"", return_value=mock_explainer)\n\n    images, labels = random_data\n\n    callbacks = [\n        GradCAMCallback(\n            validation_data=random_data,\n            class_index=0,\n            layer_name=""activation_1"",\n            output_dir=output_dir,\n            use_guided_grads=True,\n        )\n    ]\n\n    convolutional_model.fit(images, labels, batch_size=2, epochs=1, callbacks=callbacks)\n\n    mock_explainer.explain.assert_called_once_with(\n        random_data,\n        convolutional_model,\n        class_index=0,\n        layer_name=""activation_1"",\n        use_guided_grads=True,\n    )\n    assert len([_ for _ in output_dir.iterdir()]) == 1\n'"
tests/callbacks/test_integrated_gradients.py,0,"b'import numpy as np\nfrom tf_explain.callbacks.integrated_gradients import IntegratedGradientsCallback\n\n\ndef test_should_call_integrated_gradients_callback(\n    random_data, convolutional_model, output_dir, mocker\n):\n    mock_explainer = mocker.MagicMock(\n        explain=mocker.MagicMock(return_value=np.zeros((28, 28)))\n    )\n    mocker.patch(\n        ""tf_explain.callbacks.integrated_gradients.IntegratedGradients"",\n        return_value=mock_explainer,\n    )\n\n    images, labels = random_data\n\n    callbacks = [\n        IntegratedGradientsCallback(\n            validation_data=random_data, class_index=0, output_dir=output_dir, n_steps=3\n        )\n    ]\n\n    convolutional_model.fit(images, labels, batch_size=2, epochs=1, callbacks=callbacks)\n\n    mock_explainer.explain.assert_called_once_with(\n        random_data, convolutional_model, 0, 3\n    )\n    assert len([_ for _ in output_dir.iterdir()]) == 1\n'"
tests/callbacks/test_occlusion_sensitivity.py,0,"b'import numpy as np\nfrom tf_explain.callbacks.occlusion_sensitivity import OcclusionSensitivityCallback\n\n\ndef test_should_call_occlusion_sensitivity_callback(\n    random_data, convolutional_model, output_dir, mocker\n):\n    mock_explainer = mocker.MagicMock(\n        explain=mocker.MagicMock(return_value=np.zeros((28, 28, 3)))\n    )\n    mocker.patch(\n        ""tf_explain.callbacks.occlusion_sensitivity.OcclusionSensitivity"",\n        return_value=mock_explainer,\n    )\n\n    images, labels = random_data\n\n    callbacks = [\n        OcclusionSensitivityCallback(\n            validation_data=random_data,\n            class_index=0,\n            patch_size=10,\n            output_dir=output_dir,\n        )\n    ]\n\n    convolutional_model.fit(images, labels, batch_size=2, epochs=1, callbacks=callbacks)\n\n    mock_explainer.explain.assert_called_once_with(\n        random_data, convolutional_model, 0, 10\n    )\n    assert len([_ for _ in output_dir.iterdir()]) == 1\n'"
tests/callbacks/test_smoothgrad.py,0,"b'import numpy as np\nfrom tf_explain.callbacks.smoothgrad import SmoothGradCallback\n\n\ndef test_should_call_smoothgrad_callback(\n    random_data, convolutional_model, output_dir, mocker\n):\n    mock_explainer = mocker.MagicMock(\n        explain=mocker.MagicMock(return_value=np.zeros((28, 28)))\n    )\n    mocker.patch(\n        ""tf_explain.callbacks.smoothgrad.SmoothGrad"", return_value=mock_explainer\n    )\n\n    images, labels = random_data\n\n    callbacks = [\n        SmoothGradCallback(\n            validation_data=random_data,\n            class_index=0,\n            output_dir=output_dir,\n            num_samples=3,\n            noise=1.2,\n        )\n    ]\n\n    convolutional_model.fit(images, labels, batch_size=2, epochs=1, callbacks=callbacks)\n\n    mock_explainer.explain.assert_called_once_with(\n        random_data, convolutional_model, 0, 3, 1.2\n    )\n    assert len([_ for _ in output_dir.iterdir()]) == 1\n'"
tests/callbacks/test_vanilla_gradients.py,0,"b'import numpy as np\n\nfrom tf_explain.callbacks.vanilla_gradients import VanillaGradientsCallback\n\n\ndef test_should_call_vanilla_gradients_callback(\n    random_data, convolutional_model, output_dir, mocker\n):\n    images, labels = random_data\n\n    vanilla_gradient_callback = VanillaGradientsCallback(\n        validation_data=random_data, class_index=0, output_dir=output_dir\n    )\n\n    mock_explainer = mocker.MagicMock(\n        explain=mocker.MagicMock(return_value=np.zeros((28, 28)))\n    )\n\n    vanilla_gradient_callback.explainer = mock_explainer\n\n    callbacks = [vanilla_gradient_callback]\n\n    convolutional_model.fit(images, labels, batch_size=2, epochs=1, callbacks=callbacks)\n\n    mock_explainer.explain.assert_called_once_with(random_data, convolutional_model, 0)\n    assert len(list(output_dir.iterdir())) == 1\n'"
tests/core/__init__.py,0,b''
tests/core/test_activations.py,0,"b'import numpy as np\n\nfrom tf_explain.core.activations import ExtractActivations\n\n\ndef test_should_generate_subgraph(convolutional_model):\n    activations_model = ExtractActivations.generate_activations_graph(\n        convolutional_model, [""activation_1""]\n    )\n\n    assert activations_model.layers[-1].name == ""activation_1""\n\n\ndef test_should_extract_activations(random_data, convolutional_model, mocker):\n    non_normalized_grid = np.array([[1, 2], [1, 2]])\n    mocker.patch(\n        ""tf_explain.core.activations.filter_display"", return_value=non_normalized_grid\n    )\n    explainer = ExtractActivations()\n    grid = explainer.explain(random_data, convolutional_model, [""activation_1""])\n\n    expected_output = np.array([[0, 255], [0, 255]]).astype(""uint8"")\n\n    np.testing.assert_array_equal(grid, expected_output)\n\n\ndef test_should_save_output_grid(output_dir):\n    grid = np.random.random((208, 208))\n\n    explainer = ExtractActivations()\n    explainer.save(grid, output_dir, ""output.png"")\n\n    assert len(list(output_dir.glob(""output.png""))) == 1\n'"
tests/core/test_grad_cam.py,20,"b'import numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom tf_explain.core.grad_cam import GradCAM\n\n\ndef test_should_generate_ponderated_output(mocker):\n    mocker.patch(\n        ""tf_explain.core.grad_cam.GradCAM.ponderate_output"",\n        side_effect=[mocker.sentinel.ponderated_1, mocker.sentinel.ponderated_2],\n    )\n\n    expected_output = [mocker.sentinel.ponderated_1, mocker.sentinel.ponderated_2]\n\n    outputs = [mocker.sentinel.output_1, mocker.sentinel.output_2]\n    grads = [mocker.sentinel.grads_1, mocker.sentinel.grads_2]\n\n    output = GradCAM.generate_ponderated_output(outputs, grads)\n\n    for real, expected in zip(output, expected_output):\n        assert real == expected\n\n\ndef test_should_ponderate_output():\n    grad = np.concatenate(\n        [np.ones((3, 3, 1)), 2 * np.ones((3, 3, 1)), 3 * np.ones((3, 3, 1))], axis=-1\n    )\n\n    output = np.concatenate(\n        [np.ones((3, 3, 1)), 2 * np.ones((3, 3, 1)), 4 * np.ones((3, 3, 1))], axis=-1\n    )\n\n    ponderated_output = GradCAM.ponderate_output(output, grad)\n\n    ponderated_sum = 1 * 1 + 2 * 2 + 3 * 4\n    expected_output = ponderated_sum * np.ones((3, 3))\n\n    np.testing.assert_almost_equal(expected_output, ponderated_output)\n\n\ndef test_should_produce_gradients_and_filters(convolutional_model, random_data):\n    images, _ = random_data\n    layer_name = ""activation_1""\n    use_guided_grads = True\n    output, grads = GradCAM.get_gradients_and_filters(\n        convolutional_model, images, layer_name, 0, use_guided_grads\n    )\n\n    assert output.shape == [len(images)] + list(\n        convolutional_model.get_layer(layer_name).output.shape[1:]\n    )\n    assert grads.shape == output.shape\n\n\ndef test_should_explain_output(mocker):\n    mock_get_gradients = mocker.patch(\n        ""tf_explain.core.grad_cam.GradCAM.get_gradients_and_filters"",\n        return_value=(\n            [mocker.sentinel.conv_output_1, mocker.sentinel.conv_output_2],\n            [mocker.sentinel.guided_grads_1, mocker.sentinel.guided_grads_2],\n        ),\n    )\n    mocker.sentinel.cam_1.numpy = lambda: mocker.sentinel.cam_1\n    mocker.sentinel.cam_2.numpy = lambda: mocker.sentinel.cam_2\n    mock_generate_output = mocker.patch(\n        ""tf_explain.core.grad_cam.GradCAM.generate_ponderated_output"",\n        return_value=[mocker.sentinel.cam_1, mocker.sentinel.cam_2],\n    )\n    mocker.patch(\n        ""tf_explain.core.grad_cam.heatmap_display"",\n        side_effect=[mocker.sentinel.heatmap_1, mocker.sentinel.heatmap_2],\n    )\n    mocker.patch(""tf_explain.core.grad_cam.grid_display"", side_effect=lambda x: x)\n\n    explainer = GradCAM()\n    data = ([mocker.sentinel.image_1, mocker.sentinel.image_2], mocker.sentinel.labels)\n    grid = explainer.explain(\n        data,\n        mocker.sentinel.model,\n        mocker.sentinel.class_index,\n        mocker.sentinel.layer_name,\n        mocker.sentinel.use_guided_grads,\n    )\n\n    for heatmap, expected_heatmap in zip(\n        grid, [mocker.sentinel.heatmap_1, mocker.sentinel.heatmap_2]\n    ):\n        assert heatmap == expected_heatmap\n\n    mock_get_gradients.assert_called_once_with(\n        mocker.sentinel.model,\n        [mocker.sentinel.image_1, mocker.sentinel.image_2],\n        mocker.sentinel.layer_name,\n        mocker.sentinel.class_index,\n        mocker.sentinel.use_guided_grads,\n    )\n\n    mock_generate_output.assert_called_once_with(\n        [mocker.sentinel.conv_output_1, mocker.sentinel.conv_output_2],\n        [mocker.sentinel.guided_grads_1, mocker.sentinel.guided_grads_2],\n    )\n\n\n@pytest.mark.parametrize(\n    ""model,expected_layer_name"",\n    [\n        (\n            tf.keras.Sequential(\n                [\n                    tf.keras.layers.Conv2D(\n                        3, 3, input_shape=(28, 28, 1), name=""conv_1""\n                    ),\n                    tf.keras.layers.MaxPooling2D(name=""maxpool_1""),\n                    tf.keras.layers.Conv2D(3, 3, name=""conv_2""),\n                    tf.keras.layers.Flatten(name=""flatten""),\n                    tf.keras.layers.Dense(1, name=""dense""),\n                ]\n            ),\n            ""conv_2"",\n        ),\n        (\n            tf.keras.Sequential(\n                [\n                    tf.keras.layers.Conv2D(\n                        3, 3, input_shape=(28, 28, 1), name=""conv_1""\n                    ),\n                    tf.keras.layers.MaxPooling2D(name=""maxpool_1""),\n                    tf.keras.layers.Conv2D(3, 3, name=""conv_2""),\n                    tf.keras.layers.GlobalAveragePooling2D(name=""gap""),\n                    tf.keras.layers.Dense(1, name=""dense""),\n                ]\n            ),\n            ""conv_2"",\n        ),\n        (\n            tf.keras.Sequential(\n                [\n                    tf.keras.layers.Conv2D(\n                        3, 3, input_shape=(28, 28, 1), name=""conv_1""\n                    ),\n                    tf.keras.layers.MaxPooling2D(name=""maxpool_1""),\n                    tf.keras.layers.Flatten(name=""flatten""),\n                    tf.keras.layers.Dense(1, name=""dense""),\n                ]\n            ),\n            ""maxpool_1"",\n        ),\n    ],\n)\ndef test_should_infer_layer_name_for_grad_cam(model, expected_layer_name):\n    layer_name = GradCAM.infer_grad_cam_target_layer(model)\n\n    assert layer_name == expected_layer_name\n\n\ndef test_should_raise_error_if_grad_cam_layer_cannot_be_found():\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(10, input_shape=(10,), name=""dense_1""),\n            tf.keras.layers.Dense(1, name=""dense_2""),\n        ]\n    )\n\n    with pytest.raises(ValueError):\n        layer_name = GradCAM.infer_grad_cam_target_layer(model)\n'"
tests/core/test_gradients_inputs.py,0,"b'from tf_explain.core.gradients_inputs import GradientsInputs\n\n\ndef test_get_ponderated_gradients(random_data, convolutional_model):\n    images, _ = random_data\n    gradients = GradientsInputs.compute_gradients(images, convolutional_model, 0)\n\n    assert gradients.shape == images.shape\n'"
tests/core/test_integrated_gradients.py,0,"b'import numpy as np\n\nfrom tf_explain.core.integrated_gradients import IntegratedGradients\n\n\ndef test_should_explain_output(convolutional_model, random_data, mocker):\n    mocker.patch(\n        ""tf_explain.core.integrated_gradients.grid_display"", side_effect=lambda x: x\n    )\n    images, labels = random_data\n    explainer = IntegratedGradients()\n    grid = explainer.explain((images, labels), convolutional_model, 0)\n\n    # Outputs is in grayscale format\n    assert grid.shape == images.shape[:-1]\n\n\ndef test_generate_linear_path():\n    input_shape = (28, 28, 1)\n    target = np.ones(input_shape)\n    baseline = np.zeros(input_shape)\n    n_steps = 3\n\n    expected_output = [baseline, 1 / 2 * (target - baseline), target]\n\n    output = IntegratedGradients.generate_linear_path(baseline, target, n_steps)\n\n    np.testing.assert_almost_equal(output, expected_output)\n\n\ndef test_get_integrated_gradients(random_data, convolutional_model):\n    images, _ = random_data\n    n_steps = 4\n    gradients = IntegratedGradients.get_integrated_gradients(\n        images, convolutional_model, 0, n_steps=n_steps\n    )\n\n    expected_output_shape = (int(images.shape[0] / n_steps), *images.shape[1:])\n\n    assert gradients.shape == expected_output_shape\n'"
tests/core/test_occlusion_sensitivity.py,0,"b'import math\n\nimport numpy as np\n\nfrom tf_explain.core.occlusion_sensitivity import OcclusionSensitivity\n\n\ndef test_should_get_sensitivity_map(convolutional_model, random_data, mocker):\n    x, y = random_data\n    patch_size = 4\n\n    predict_return_value = np.ones(\n        (\n            math.ceil(x[0].shape[0] / patch_size)\n            * math.ceil(x[0].shape[1] / patch_size),\n            1,\n        )\n    ) * np.expand_dims([0.6, 0.4], axis=0)\n    convolutional_model.predict = mocker.MagicMock(return_value=predict_return_value)\n    mocker.patch(\n        ""tf_explain.core.occlusion_sensitivity.apply_grey_patch"",\n        return_value=np.random.randint(\n            low=0, high=255, size=convolutional_model.inputs[0].shape[1:]\n        ),\n    )\n    mocker.patch(\n        ""tf_explain.core.occlusion_sensitivity.cv2.resize"", side_effect=lambda x, _: x\n    )\n\n    output = OcclusionSensitivity().get_sensitivity_map(\n        model=convolutional_model, image=x[0], class_index=0, patch_size=patch_size\n    )\n\n    expected_output = 0.4 * np.ones(\n        (x[0].shape[0] // patch_size, x[0].shape[1] // patch_size)\n    )\n\n    np.testing.assert_almost_equal(output, expected_output)\n\n\ndef test_should_produce_heatmap(convolutional_model, random_data, mocker):\n    mocker.patch(\n        ""tf_explain.core.occlusion_sensitivity.grid_display"",\n        return_value=mocker.sentinel.grid,\n    )\n\n    explainer = OcclusionSensitivity()\n    grid = explainer.explain(random_data, convolutional_model, 0, 10)\n\n    assert grid == mocker.sentinel.grid\n'"
tests/core/test_smoothgrad.py,0,"b'import numpy as np\n\nfrom tf_explain.core.smoothgrad import SmoothGrad\n\n\ndef test_should_explain_output(convolutional_model, random_data, mocker):\n    mocker.patch(""tf_explain.core.smoothgrad.grid_display"", side_effect=lambda x: x)\n    images, labels = random_data\n    explainer = SmoothGrad()\n    grid = explainer.explain((images, labels), convolutional_model, 0)\n\n    # Outputs is in grayscale format\n    assert grid.shape == images.shape[:-1]\n\n\ndef test_generate_noisy_images(mocker):\n    input_shape = (10, 28, 28, 1)\n    num_samples = 3\n    mocker.patch(\n        ""tf_explain.core.smoothgrad.np.random.normal"",\n        return_value=np.ones((30, 28, 28, 1)),\n    )\n\n    images = np.ones(input_shape)\n    output = SmoothGrad.generate_noisy_images(images, num_samples=num_samples, noise=1)\n\n    np.testing.assert_array_equal(output, 2 * np.ones((30, 28, 28, 1)))\n\n\ndef test_get_averaged_gradients(random_data, convolutional_model):\n    images, _ = random_data\n    num_samples = 2\n    gradients = SmoothGrad.get_averaged_gradients(\n        images, convolutional_model, 0, num_samples=num_samples\n    )\n\n    expected_output_shape = (int(images.shape[0] / num_samples), *images.shape[1:])\n\n    assert gradients.shape == expected_output_shape\n'"
tests/core/test_vanilla_gradients.py,0,"b'from tf_explain.core.vanilla_gradients import VanillaGradients\n\n\ndef test_should_explain_output(convolutional_model, random_data, mocker):\n    mocker.patch(\n        ""tf_explain.core.vanilla_gradients.grid_display"", side_effect=lambda x: x\n    )\n    images, labels = random_data\n    explainer = VanillaGradients()\n    grid = explainer.explain((images, labels), convolutional_model, 0)\n\n    # Outputs is in grayscale format\n    assert grid.shape == images.shape[:-1]\n\n\ndef test_get_averaged_gradients(random_data, convolutional_model):\n    images, _ = random_data\n    gradients = VanillaGradients.compute_gradients(images, convolutional_model, 0)\n\n    assert gradients.shape == images.shape\n'"
tests/integration/__init__.py,0,b''
tests/integration/conftest.py,5,"b'import numpy as np\nimport pytest\nimport tensorflow as tf\n\n\n@pytest.fixture(scope=""session"")\ndef num_classes():\n    return 10\n\n\n@pytest.fixture(scope=""session"")\ndef mnist_dataset(num_classes):\n    # Load dataset\n    dataset = tf.keras.datasets.mnist\n    (train_images, train_labels), (test_images, test_labels) = dataset.load_data()\n\n    # Convert from (28, 28) images to (28, 28, 1)\n    train_images = train_images[..., tf.newaxis].astype(""float32"")\n    test_images = test_images[..., tf.newaxis].astype(""float32"")\n\n    # One hot encore labels 0, 1, .., 9 to [0, 0, .., 1, 0, 0]\n    train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=num_classes)\n    test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=num_classes)\n\n    return (\n        train_images[0:500],\n        train_labels[0:500],\n        test_images[0:300],\n        test_labels[0:300],\n    )\n\n\n@pytest.fixture(scope=""session"")\ndef validation_dataset(mnist_dataset, num_classes):\n    train_images, train_labels, test_images, test_labels = mnist_dataset\n    TARGET_CLASS = np.random.choice(num_classes, 1)[0]\n    ONE_HOT_TARGET_CLASS = np.array(\n        [int(el == TARGET_CLASS) for el in range(num_classes)]\n    )\n\n    validation_target_class = (\n        np.array(\n            [\n                el\n                for el, label in zip(test_images, test_labels)\n                if np.all(label == ONE_HOT_TARGET_CLASS)\n            ]\n        ),\n        None,\n    )\n\n    return validation_target_class, TARGET_CLASS\n'"
tests/integration/test_keras_api.py,33,"b'import numpy as np\nimport pytest\nimport tensorflow as tf\nimport tf_explain\n\n\nINPUT_SHAPE = (28, 28, 1)\n\n\ndef functional_api_model(num_classes):\n    img_input = tf.keras.Input(INPUT_SHAPE)\n\n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=""relu"")(\n        img_input\n    )\n    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=""relu"")(x)\n    x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)\n\n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=""relu"")(x)\n    x = tf.keras.layers.Conv2D(\n        filters=64, kernel_size=(3, 3), activation=""relu"", name=""grad_cam_target""\n    )(x)\n    x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)\n\n    x = tf.keras.layers.Flatten()(x)\n\n    x = tf.keras.layers.Dense(64, activation=""relu"")(x)\n\n    x = tf.keras.layers.Dense(num_classes, activation=""softmax"")(x)\n\n    model = tf.keras.Model(img_input, x)\n    model.compile(\n        optimizer=""adam"", loss=""categorical_crossentropy"", metrics=[""accuracy""]\n    )\n\n    return model\n\n\ndef sequential_api_model(num_classes):\n    model = tf.keras.models.Sequential(\n        [\n            tf.keras.layers.Conv2D(\n                filters=32,\n                kernel_size=(3, 3),\n                activation=""relu"",\n                input_shape=INPUT_SHAPE,\n            ),\n            tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=""relu""),\n            tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n            tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=""relu""),\n            tf.keras.layers.Conv2D(\n                filters=64,\n                kernel_size=(3, 3),\n                activation=""relu"",\n                name=""grad_cam_target"",\n            ),\n            tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(64, activation=""relu""),\n            tf.keras.layers.Dense(num_classes, activation=""softmax""),\n        ]\n    )\n    model.compile(\n        optimizer=""adam"", loss=""categorical_crossentropy"", metrics=[""accuracy""]\n    )\n\n    return model\n\n\ndef subclassing_api_model(num_classes):\n    class SubclassedModel(tf.keras.models.Model):\n        def __init__(self, name=""subclassed""):\n            super(SubclassedModel, self).__init__(name=name)\n            self.conv_1 = tf.keras.layers.Conv2D(\n                filters=32, kernel_size=(3, 3), activation=""relu""\n            )\n            self.conv_2 = tf.keras.layers.Conv2D(\n                filters=64, kernel_size=(3, 3), activation=""relu""\n            )\n            self.maxpool_1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n\n            self.conv_3 = tf.keras.layers.Conv2D(\n                filters=32, kernel_size=(3, 3), activation=""relu""\n            )\n            self.conv_4 = tf.keras.layers.Conv2D(\n                filters=64,\n                kernel_size=(3, 3),\n                activation=""relu"",\n                name=""grad_cam_target"",\n            )\n            self.maxpool_2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n\n            self.flatten = tf.keras.layers.Flatten()\n\n            self.dense_1 = tf.keras.layers.Dense(64, activation=""relu"")\n\n            self.dense_2 = tf.keras.layers.Dense(num_classes, activation=""softmax"")\n\n        def build(self, input_shape):\n            super(SubclassedModel, self).build(input_shape)\n\n        def call(self, inputs, **kwargs):\n            x = inputs\n            for layer in [\n                self.conv_1,\n                self.conv_2,\n                self.maxpool_1,\n                self.conv_3,\n                self.conv_4,\n                self.maxpool_2,\n                self.flatten,\n                self.dense_1,\n                self.dense_2,\n            ]:\n                x = layer(x)\n\n            return x\n\n        def compute_output_shape(self, input_shape):\n            shape = tf.TensorShape(input_shape).as_list()\n            return tf.TensorShape([shape[0], num_classes])\n\n    model = SubclassedModel()\n\n    model(\n        np.random.random([4, *INPUT_SHAPE]).astype(""float32"")\n    )  # Sample call to build the model\n\n    model.compile(\n        optimizer=""adam"", loss=""categorical_crossentropy"", metrics=[""accuracy""]\n    )\n\n    return model\n\n\n# TODO: Activate subclassing API model (issue #55)\n@pytest.mark.parametrize(""model_builder"", [functional_api_model, sequential_api_model])\ndef test_all_keras_api(\n    model_builder, mnist_dataset, validation_dataset, num_classes, output_dir\n):\n    train_images, train_labels, test_images, test_labels = mnist_dataset\n\n    model = model_builder(num_classes)\n    model.summary()\n\n    validation_data, target_class = validation_dataset\n\n    # Instantiate callbacks\n    callbacks = [\n        tf_explain.callbacks.GradCAMCallback(\n            validation_data,\n            layer_name=""grad_cam_target"",\n            class_index=target_class,\n            output_dir=output_dir,\n            use_guided_grads=True,\n        ),\n        tf_explain.callbacks.ActivationsVisualizationCallback(\n            validation_data, ""grad_cam_target"", output_dir=output_dir\n        ),\n        tf_explain.callbacks.SmoothGradCallback(\n            validation_data,\n            class_index=target_class,\n            num_samples=15,\n            noise=1.0,\n            output_dir=output_dir,\n        ),\n        tf_explain.callbacks.VanillaGradientsCallback(\n            validation_data, class_index=target_class, output_dir=output_dir\n        ),\n        tf_explain.callbacks.GradientsInputsCallback(\n            validation_data, class_index=target_class, output_dir=output_dir\n        ),\n    ]\n\n    # Start training\n    model.fit(train_images, train_labels, epochs=3, callbacks=callbacks)\n'"
tests/utils/__init__.py,0,b''
tests/utils/test_display.py,0,"b'import numpy as np\nimport pytest\n\nfrom tf_explain.utils.display import grid_display, heatmap_display, image_to_uint_255\n\n\n@pytest.fixture(scope=""session"")\ndef array_to_display():\n    return np.random.random((7, 28, 28, 3))\n\n\n@pytest.mark.parametrize(\n    ""grid_display_kwargs,expected_shape"",\n    [\n        ({}, (28 * 3, 28 * 3, 3)),\n        ({""num_columns"": 10, ""num_rows"": 10}, (28 * 10, 28 * 10, 3)),\n        ({""num_columns"": 7}, (28, 28 * 7, 3)),\n        ({""num_rows"": 7}, (28 * 7, 28, 3)),\n    ],\n)\ndef test_should_return_a_grid_square_by_default(\n    grid_display_kwargs, expected_shape, array_to_display\n):\n    grid = grid_display(array_to_display, **grid_display_kwargs)\n\n    assert grid.shape == expected_shape\n\n\ndef test_should_raise_warning_if_grid_size_is_too_small(array_to_display):\n    with pytest.warns(Warning) as w:\n        grid = grid_display(array_to_display, num_rows=1, num_columns=1)\n\n    assert (\n        w[0].message.args[0]\n        == ""Given values for num_rows and num_columns doesn\'t allow to display all images. Values have been overrided to respect at least num_columns""\n    )\n    assert grid.shape == (28 * 7, 28, 3)\n\n\ndef test_should_fill_with_zeros_if_missing_elements(array_to_display):\n    grid = grid_display(array_to_display)\n\n    assert grid.shape == (28 * 3, 28 * 3, 3)\n    np.testing.assert_equal(grid[56:, 28:, :], np.zeros((28, 56, 3)))\n\n\n@pytest.mark.parametrize(\n    ""input_image,expected_output"",\n    [\n        (np.ones((28, 28)) / 255.0, np.ones((28, 28)).astype(""uint8"")),\n        (-np.ones((28, 28)) / 255.0, 127 * np.ones((28, 28)).astype(""uint8"")),\n        (\n            10 * np.ones((28, 28)).astype(""uint8""),\n            10 * np.ones((28, 28)).astype(""uint8""),\n        ),\n    ],\n)\ndef test_should_transform_to_uint_255_image(input_image, expected_output):\n    output = image_to_uint_255(input_image)\n\n    np.testing.assert_almost_equal(output, expected_output)\n\n\ndef test_should_display_heatmap(mocker):\n    mock_add_weighted = mocker.patch(""tf_explain.utils.display.cv2.addWeighted"")\n    mock_apply_color_map = mocker.patch(""tf_explain.utils.display.cv2.applyColorMap"")\n    mock_cvt_color = mocker.patch(\n        ""tf_explain.utils.display.cv2.cvtColor"", return_value=mocker.sentinel.heatmap\n    )\n\n    heatmap = np.random.random((3, 3))\n    original_image = np.zeros((10, 10, 3))\n\n    output = heatmap_display(heatmap, original_image)\n\n    assert output == mocker.sentinel.heatmap\n    assert mock_add_weighted.call_count == 1\n    assert mock_apply_color_map.call_count == 1\n    assert mock_cvt_color.call_count == 3\n'"
tests/utils/test_image.py,1,"b'import numpy as np\nimport tensorflow as tf\n\nfrom tf_explain.utils.image import apply_grey_patch, transform_to_normalized_grayscale\n\n\ndef test_should_apply_grey_patch_on_image():\n    input_image = np.zeros((10, 20, 3))\n\n    output = apply_grey_patch(input_image, 0, 0, 10)\n\n    expected_output = np.concatenate(\n        [127.5 * np.ones((10, 10, 3)), np.zeros((10, 10, 3))], axis=1\n    )\n\n    np.testing.assert_almost_equal(output, expected_output)\n\n\ndef test_should_transform_gradients_to_grayscale():\n    gradients = tf.random.uniform((4, 28, 28, 3))\n\n    grayscale_gradients = transform_to_normalized_grayscale(gradients)\n    expected_output_shape = (4, 28, 28)\n\n    assert grayscale_gradients.shape == expected_output_shape\n'"
tests/utils/test_saver.py,0,"b'from pathlib import Path\n\nimport numpy as np\n\nfrom tf_explain.utils.saver import save_grayscale, save_rgb\n\n\ndef test_should_save_grayscale_image(output_dir):\n    input_image = np.ones((28, 28, 1), dtype=""uint8"")\n\n    save_grayscale(input_image, output_dir, ""grayscale.png"")\n\n    assert len(list(Path(output_dir).iterdir())) == 1\n\n\ndef test_should_save_rgb_image(output_dir):\n    input_image = np.ones((28, 28, 3), dtype=""uint8"")\n\n    save_rgb(input_image, output_dir, ""rgb.png"")\n\n    assert len(list(Path(output_dir).iterdir())) == 1\n'"
tf_explain/callbacks/__init__.py,1,"b'""""""\nCallbacks Module\n\nThis module regroups all the methods as tf.keras.Callback.\n""""""\nfrom .activations_visualization import ActivationsVisualizationCallback\nfrom .grad_cam import GradCAMCallback\nfrom .gradients_inputs import GradientsInputsCallback\nfrom .vanilla_gradients import VanillaGradientsCallback\nfrom .integrated_gradients import IntegratedGradientsCallback\nfrom .occlusion_sensitivity import OcclusionSensitivityCallback\nfrom .smoothgrad import SmoothGradCallback\n\n\n__all__ = [\n    ""ActivationsVisualizationCallback"",\n    ""GradCAMCallback"",\n    ""IntegratedGradientsCallback"",\n    ""OcclusionSensitivityCallback"",\n    ""SmoothGradCallback"",\n    ""VanillaGradientsCallback"",\n    ""GradientsInputsCallback"",\n]\n'"
tf_explain/callbacks/activations_visualization.py,2,"b'""""""\nCallback Module for Activations Visualization\n""""""\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import Callback\n\nfrom tf_explain.core.activations import ExtractActivations\n\n\nclass ActivationsVisualizationCallback(Callback):\n\n    """""" Draw activations of a specific layer for a given input """"""\n\n    def __init__(\n        self,\n        validation_data,\n        layers_name,\n        output_dir=Path(""./logs/activations_visualizations""),\n    ):\n        """"""\n        Constructor.\n\n        Args:\n            validation_data (Tuple[np.ndarray, Optional[np.ndarray]]): Validation data\n                to perform the method on. Tuple containing (x, y).\n            layers_name (List[str]): List of layer names to inspect\n            output_dir (str): Output directory path\n        """"""\n        super(ActivationsVisualizationCallback, self).__init__()\n        self.validation_data = validation_data\n        self.layers_name = layers_name\n        self.output_dir = Path(output_dir) / datetime.now().strftime(""%Y%m%d-%H%M%S.%f"")\n        Path.mkdir(Path(self.output_dir), parents=True, exist_ok=True)\n\n        self.file_writer = tf.summary.create_file_writer(str(self.output_dir))\n\n    def on_epoch_end(self, epoch, logs=None):\n        """"""\n        Draw activations outputs at each epoch end to Tensorboard.\n\n        Args:\n            epoch (int): Epoch index\n            logs (dict): Additional information on epoch\n        """"""\n        explainer = ExtractActivations()\n        grid = explainer.explain(self.validation_data, self.model, self.layers_name)\n\n        # Using the file writer, log the reshaped image.\n        with self.file_writer.as_default():\n            tf.summary.image(\n                ""Activations Visualization"",\n                np.array([np.expand_dims(grid, axis=-1)]),\n                step=epoch,\n            )\n'"
tf_explain/callbacks/grad_cam.py,2,"b'""""""\nCallback Module for Grad CAM\n""""""\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import Callback\n\nfrom tf_explain.core.grad_cam import GradCAM\n\n\nclass GradCAMCallback(Callback):\n\n    """"""\n    Perform Grad CAM algorithm for a given input\n\n    Paper: [Grad-CAM: Visual Explanations from Deep Networks\n            via Gradient-based Localization](https://arxiv.org/abs/1610.02391)\n    """"""\n\n    def __init__(\n        self,\n        validation_data,\n        class_index,\n        layer_name=None,\n        output_dir=Path(""./logs/grad_cam""),\n        use_guided_grads=True,\n    ):\n        """"""\n        Constructor.\n\n        Args:\n            validation_data (Tuple[np.ndarray, Optional[np.ndarray]]): Validation data\n                to perform the method on. Tuple containing (x, y).\n            class_index (int): Index of targeted class\n            layer_name (str): Targeted layer for GradCAM\n            output_dir (str): Output directory path\n        """"""\n        super(GradCAMCallback, self).__init__()\n        self.validation_data = validation_data\n        self.layer_name = layer_name\n        self.class_index = class_index\n        self.output_dir = Path(output_dir) / datetime.now().strftime(""%Y%m%d-%H%M%S.%f"")\n        self.use_guided_grads = use_guided_grads\n        Path.mkdir(Path(self.output_dir), parents=True, exist_ok=True)\n\n        self.file_writer = tf.summary.create_file_writer(str(self.output_dir))\n\n    def on_epoch_end(self, epoch, logs=None):\n        """"""\n        Draw GradCAM outputs at each epoch end to Tensorboard.\n\n        Args:\n            epoch (int): Epoch index\n            logs (dict): Additional information on epoch\n        """"""\n        explainer = GradCAM()\n        heatmap = explainer.explain(\n            self.validation_data,\n            self.model,\n            class_index=self.class_index,\n            layer_name=self.layer_name,\n            use_guided_grads=self.use_guided_grads,\n        )\n\n        # Using the file writer, log the reshaped image.\n        with self.file_writer.as_default():\n            tf.summary.image(""Grad CAM"", np.array([heatmap]), step=epoch)\n'"
tf_explain/callbacks/gradients_inputs.py,0,"b'""""""\nCallback Module for Gradients*Inputs algorithm\n""""""\nfrom tf_explain.callbacks.vanilla_gradients import VanillaGradientsCallback\nfrom tf_explain.core import GradientsInputs\n\n\nclass GradientsInputsCallback(VanillaGradientsCallback):\n\n    """"""\n    Tensorflow Callback performing Gradients*Inputs algorithm for given input and target class\n    """"""\n\n    explainer = GradientsInputs()\n    default_output_subdir = ""gradients_inputs""\n'"
tf_explain/callbacks/integrated_gradients.py,2,"b'""""""\nCallback Module for Integrated Gradients\n""""""\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import Callback\n\nfrom tf_explain.core.integrated_gradients import IntegratedGradients\n\n\nclass IntegratedGradientsCallback(Callback):\n\n    """"""\n    Perform Integrated Gradients algorithm for a given input\n\n    Paper: [Axiomatic Attribution for Deep Networks](https://arxiv.org/pdf/1703.01365.pdf)\n    """"""\n\n    def __init__(\n        self,\n        validation_data,\n        class_index,\n        n_steps=5,\n        output_dir=Path(""./logs/integrated_gradients""),\n    ):\n        """"""\n        Constructor.\n\n        Args:\n            validation_data (Tuple[np.ndarray, Optional[np.ndarray]]): Validation data\n                to perform the method on. Tuple containing (x, y).\n            class_index (int): Index of targeted class\n            n_steps (int): Number of steps in the path\n            output_dir (str): Output directory path\n        """"""\n        super(IntegratedGradientsCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_index = class_index\n        self.n_steps = n_steps\n        self.output_dir = Path(output_dir) / datetime.now().strftime(""%Y%m%d-%H%M%S.%f"")\n        Path.mkdir(Path(self.output_dir), parents=True, exist_ok=True)\n\n        self.file_writer = tf.summary.create_file_writer(str(self.output_dir))\n\n    def on_epoch_end(self, epoch, logs=None):\n        """"""\n        Draw Integrated Gradients outputs at each epoch end to Tensorboard.\n\n        Args:\n            epoch (int): Epoch index\n            logs (dict): Additional information on epoch\n        """"""\n        explainer = IntegratedGradients()\n        grid = explainer.explain(\n            self.validation_data, self.model, self.class_index, self.n_steps\n        )\n\n        # Using the file writer, log the reshaped image.\n        with self.file_writer.as_default():\n            tf.summary.image(\n                ""IntegratedGradients"", np.expand_dims([grid], axis=-1), step=epoch\n            )\n'"
tf_explain/callbacks/occlusion_sensitivity.py,2,"b'""""""\nCallback Module for Occlusion Sensitivity\n""""""\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import Callback\n\nfrom tf_explain.core.occlusion_sensitivity import OcclusionSensitivity\n\n\nclass OcclusionSensitivityCallback(Callback):\n\n    """"""\n    Perform Occlusion Sensitivity for a given input\n    """"""\n\n    def __init__(\n        self,\n        validation_data,\n        class_index,\n        patch_size,\n        output_dir=Path(""./logs/occlusion_sensitivity""),\n    ):\n        """"""\n        Constructor.\n\n        Args:\n            validation_data (Tuple[np.ndarray, Optional[np.ndarray]]): Validation data\n                to perform the method on. Tuple containing (x, y).\n            class_index (int): Index of targeted class\n            patch_size (int): Size of patch to apply on the image\n            output_dir (str): Output directory path\n        """"""\n        super(OcclusionSensitivityCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_index = class_index\n        self.patch_size = patch_size\n        self.output_dir = Path(output_dir) / datetime.now().strftime(""%Y%m%d-%H%M%S.%f"")\n        Path.mkdir(self.output_dir, parents=True, exist_ok=True)\n\n        self.file_writer = tf.summary.create_file_writer(str(self.output_dir))\n\n    def on_epoch_end(self, epoch, logs=None):\n        """"""\n        Draw Occlusion Sensitivity outputs at each epoch end to Tensorboard.\n\n        Args:\n            epoch (int): Epoch index\n            logs (dict): Additional information on epoch\n        """"""\n        explainer = OcclusionSensitivity()\n        grid = explainer.explain(\n            self.validation_data, self.model, self.class_index, self.patch_size\n        )\n\n        # Using the file writer, log the reshaped image.\n        with self.file_writer.as_default():\n            tf.summary.image(""Occlusion Sensitivity"", np.array([grid]), step=epoch)\n'"
tf_explain/callbacks/smoothgrad.py,2,"b'""""""\nCallback Module for SmoothGrad\n""""""\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import Callback\n\nfrom tf_explain.core.smoothgrad import SmoothGrad\n\n\nclass SmoothGradCallback(Callback):\n\n    """"""\n    Perform SmoothGrad algorithm for a given input\n\n    Paper: [SmoothGrad: removing noise by adding noise](https://arxiv.org/abs/1706.03825)\n    """"""\n\n    def __init__(\n        self,\n        validation_data,\n        class_index,\n        num_samples=5,\n        noise=1.0,\n        output_dir=Path(""./logs/smoothgrad""),\n    ):\n        """"""\n        Constructor.\n\n        Args:\n            validation_data (Tuple[np.ndarray, Optional[np.ndarray]]): Validation data\n                to perform the method on. Tuple containing (x, y).\n            class_index (int): Index of targeted class\n            num_samples (int): Number of noisy samples to generate for each input image\n            noise (float): Standard deviation for noise normal distribution\n            output_dir (str): Output directory path\n        """"""\n        super(SmoothGradCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_index = class_index\n        self.num_samples = num_samples\n        self.noise = noise\n        self.output_dir = Path(output_dir) / datetime.now().strftime(""%Y%m%d-%H%M%S.%f"")\n        Path.mkdir(Path(self.output_dir), parents=True, exist_ok=True)\n\n        self.file_writer = tf.summary.create_file_writer(str(self.output_dir))\n\n    def on_epoch_end(self, epoch, logs=None):\n        """"""\n        Draw SmoothGrad outputs at each epoch end to Tensorboard.\n\n        Args:\n            epoch (int): Epoch index\n            logs (dict): Additional information on epoch\n        """"""\n        explainer = SmoothGrad()\n        grid = explainer.explain(\n            self.validation_data,\n            self.model,\n            self.class_index,\n            self.num_samples,\n            self.noise,\n        )\n\n        # Using the file writer, log the reshaped image.\n        with self.file_writer.as_default():\n            tf.summary.image(""SmoothGrad"", np.expand_dims([grid], axis=-1), step=epoch)\n'"
tf_explain/callbacks/vanilla_gradients.py,2,"b'""""""\nCallback Module for Vanilla Gradients\n""""""\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import Callback\n\nfrom tf_explain.core.vanilla_gradients import VanillaGradients\n\n\nclass VanillaGradientsCallback(Callback):\n\n    """"""\n    Perform gradients backpropagation for a given input\n\n    Paper: [Deep Inside Convolutional Networks: Visualising Image Classification\n        Models and Saliency Maps](https://arxiv.org/abs/1312.6034)\n    """"""\n\n    explainer = VanillaGradients()\n    default_output_subdir = ""vanilla_gradients""\n\n    def __init__(self, validation_data, class_index, output_dir=None):\n        """"""\n        Constructor.\n\n        Args:\n            validation_data (Tuple[np.ndarray, Optional[np.ndarray]]): Validation data\n                to perform the method on. Tuple containing (x, y).\n            class_index (int): Index of targeted class\n            num_samples (int): Number of noisy samples to generate for each input image\n        """"""\n        super(VanillaGradientsCallback, self).__init__()\n        self.validation_data = validation_data\n        self.class_index = class_index\n        if not output_dir:\n            output_dir = Path(""./logs"") / self.default_output_subdir\n        self.output_dir = Path(output_dir) / datetime.now().strftime(""%Y%m%d-%H%M%S.%f"")\n        Path.mkdir(Path(self.output_dir), parents=True, exist_ok=True)\n\n        self.file_writer = tf.summary.create_file_writer(str(self.output_dir))\n\n    def on_epoch_end(self, epoch, logs=None):\n        """"""\n        Draw VanillaGradients outputs at each epoch end to Tensorboard.\n\n        Args:\n            epoch (int): Epoch index\n            logs (dict): Additional information on epoch\n        """"""\n        grid = self.explainer.explain(\n            self.validation_data, self.model, self.class_index\n        )\n\n        # Using the file writer, log the reshaped image.\n        with self.file_writer.as_default():\n            tf.summary.image(\n                self.explainer.__class__.__name__,\n                np.expand_dims([grid], axis=-1),\n                step=epoch,\n            )\n'"
tf_explain/core/__init__.py,0,"b'""""""\nCore Module\n\nThis module regroups all the interpretability methods under a common .explain() interface.\n""""""\nfrom .activations import ExtractActivations\nfrom .grad_cam import GradCAM\nfrom .gradients_inputs import GradientsInputs\nfrom .vanilla_gradients import VanillaGradients\nfrom .integrated_gradients import IntegratedGradients\nfrom .occlusion_sensitivity import OcclusionSensitivity\nfrom .smoothgrad import SmoothGrad\n\n\n__all__ = [\n    ""ExtractActivations"",\n    ""GradCAM"",\n    ""GradientsInputs"",\n    ""IntegratedGradients"",\n    ""OcclusionSensitivity"",\n    ""SmoothGrad"",\n    ""VanillaGradients"",\n]\n'"
tf_explain/core/activations.py,4,"b'""""""\nCore Module for Activations Visualizations\n""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_explain.utils.display import filter_display\nfrom tf_explain.utils.saver import save_grayscale\n\n\nclass ExtractActivations:\n\n    """""" Draw activations of a specific layer for a given input """"""\n\n    def __init__(self, batch_size=None):\n        self.batch_size = batch_size\n\n    def explain(self, validation_data, model, layers_name):\n        """"""\n        Compute activations at targeted layers.\n\n        Args:\n            validation_data (Tuple[np.ndarray, Optional[np.ndarray]]): Validation data\n                to perform the method on. Tuple containing (x, y).\n            model (tf.keras.Model): tf.keras model to inspect\n            layers_name (List[str]): List of layer names to inspect\n\n        Returns:\n            np.ndarray: Grid of all the activations\n        """"""\n        activations_model = self.generate_activations_graph(model, layers_name)\n\n        predictions = activations_model.predict(\n            validation_data[0], batch_size=self.batch_size\n        )\n        grid = filter_display(predictions)\n\n        grid = (grid - grid.min()) / (grid.max() - grid.min())\n\n        return (np.clip(grid, 0, 1) * 255).astype(""uint8"")\n\n    @staticmethod\n    def generate_activations_graph(model, layers_name):\n        """"""\n        Generate a graph between inputs and targeted layers.\n\n        Args:\n            model (tf.keras.Model): tf.keras model to inspect\n            layers_name (List[str]): List of layer names to inspect\n\n        Returns:\n            tf.keras.Model: Subgraph to the targeted layers\n        """"""\n        outputs = [layer.output for layer in model.layers if layer.name in layers_name]\n        activations_model = tf.keras.models.Model(model.inputs, outputs=outputs)\n        activations_model.compile(optimizer=""sgd"", loss=""categorical_crossentropy"")\n\n        return activations_model\n\n    def save(self, grid, output_dir, output_name):\n        """"""\n        Save the output to a specific dir.\n\n        Args:\n            grid (numpy.ndarray): Grid of all the activations\n            output_dir (str): Output directory path\n            output_name (str): Output name\n        """"""\n        save_grayscale(grid, output_dir, output_name)\n'"
tf_explain/core/grad_cam.py,18,"b'""""""\nCore Module for Grad CAM Algorithm\n""""""\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_explain.utils.display import grid_display, heatmap_display\nfrom tf_explain.utils.saver import save_rgb\n\n\nclass GradCAM:\n\n    """"""\n    Perform Grad CAM algorithm for a given input\n\n    Paper: [Grad-CAM: Visual Explanations from Deep Networks\n            via Gradient-based Localization](https://arxiv.org/abs/1610.02391)\n    """"""\n\n    def explain(\n        self,\n        validation_data,\n        model,\n        class_index,\n        layer_name=None,\n        use_guided_grads=True,\n        colormap=cv2.COLORMAP_VIRIDIS,\n        image_weight=0.7,\n    ):\n        """"""\n        Compute GradCAM for a specific class index.\n\n        Args:\n            validation_data (Tuple[np.ndarray, Optional[np.ndarray]]): Validation data\n                to perform the method on. Tuple containing (x, y).\n            model (tf.keras.Model): tf.keras model to inspect\n            class_index (int): Index of targeted class\n            layer_name (str): Targeted layer for GradCAM. If no layer is provided, it is\n                automatically infered from the model architecture.\n            colormap (int): OpenCV Colormap to use for heatmap visualization\n            image_weight (float): An optional `float` value in range [0,1] indicating the weight of\n                the input image to be overlaying the calculated attribution maps. Defaults to `0.7`.\n            use_guided_grads (boolean): Whether to use guided grads or raw gradients\n\n        Returns:\n            numpy.ndarray: Grid of all the GradCAM\n        """"""\n        images, _ = validation_data\n\n        if layer_name is None:\n            layer_name = self.infer_grad_cam_target_layer(model)\n\n        outputs, grads = GradCAM.get_gradients_and_filters(\n            model, images, layer_name, class_index, use_guided_grads\n        )\n\n        cams = GradCAM.generate_ponderated_output(outputs, grads)\n\n        heatmaps = np.array(\n            [\n                # not showing the actual image if image_weight=0\n                heatmap_display(cam.numpy(), image, colormap, image_weight)\n                for cam, image in zip(cams, images)\n            ]\n        )\n\n        grid = grid_display(heatmaps)\n\n        return grid\n\n    @staticmethod\n    def infer_grad_cam_target_layer(model):\n        """"""\n        Search for the last convolutional layer to perform Grad CAM, as stated\n        in the original paper.\n\n        Args:\n            model (tf.keras.Model): tf.keras model to inspect\n\n        Returns:\n            str: Name of the target layer\n        """"""\n        for layer in reversed(model.layers):\n            # Select closest 4D layer to the end of the network.\n            if len(layer.output_shape) == 4:\n                return layer.name\n\n        raise ValueError(\n            ""Model does not seem to contain 4D layer. Grad CAM cannot be applied.""\n        )\n\n    @staticmethod\n    @tf.function\n    def get_gradients_and_filters(\n        model, images, layer_name, class_index, use_guided_grads\n    ):\n        """"""\n        Generate guided gradients and convolutional outputs with an inference.\n\n        Args:\n            model (tf.keras.Model): tf.keras model to inspect\n            images (numpy.ndarray): 4D-Tensor with shape (batch_size, H, W, 3)\n            layer_name (str): Targeted layer for GradCAM\n            class_index (int): Index of targeted class\n            use_guided_grads (boolean): Whether to use guided grads or raw gradients\n\n        Returns:\n            Tuple[tf.Tensor, tf.Tensor]: (Target layer outputs, Guided gradients)\n        """"""\n        grad_model = tf.keras.models.Model(\n            [model.inputs], [model.get_layer(layer_name).output, model.output]\n        )\n\n        with tf.GradientTape() as tape:\n            inputs = tf.cast(images, tf.float32)\n            conv_outputs, predictions = grad_model(inputs)\n            loss = predictions[:, class_index]\n\n        grads = tape.gradient(loss, conv_outputs)\n\n        if use_guided_grads:\n            grads = (\n                tf.cast(conv_outputs > 0, ""float32"")\n                * tf.cast(grads > 0, ""float32"")\n                * grads\n            )\n\n        return conv_outputs, grads\n\n    @staticmethod\n    def generate_ponderated_output(outputs, grads):\n        """"""\n        Apply Grad CAM algorithm scheme.\n\n        Inputs are the convolutional outputs (shape WxHxN) and gradients (shape WxHxN).\n        From there:\n            - we compute the spatial average of the gradients\n            - we build a ponderated sum of the convolutional outputs based on those averaged weights\n\n        Args:\n            output (tf.Tensor): Target layer outputs, with shape (batch_size, Hl, Wl, Nf),\n                where Hl and Wl are the target layer output height and width, and Nf the\n                number of filters.\n            grads (tf.Tensor): Guided gradients with shape (batch_size, Hl, Wl, Nf)\n\n        Returns:\n            List[tf.Tensor]: List of ponderated output of shape (batch_size, Hl, Wl, 1)\n        """"""\n\n        maps = [\n            GradCAM.ponderate_output(output, grad)\n            for output, grad in zip(outputs, grads)\n        ]\n\n        return maps\n\n    @staticmethod\n    def ponderate_output(output, grad):\n        """"""\n        Perform the ponderation of filters output with respect to average of gradients values.\n\n        Args:\n            output (tf.Tensor): Target layer outputs, with shape (Hl, Wl, Nf),\n                where Hl and Wl are the target layer output height and width, and Nf the\n                number of filters.\n            grads (tf.Tensor): Guided gradients with shape (Hl, Wl, Nf)\n\n        Returns:\n            tf.Tensor: Ponderated output of shape (Hl, Wl, 1)\n        """"""\n        weights = tf.reduce_mean(grad, axis=(0, 1))\n\n        # Perform ponderated sum : w_i * output[:, :, i]\n        cam = tf.reduce_sum(tf.multiply(weights, output), axis=-1)\n\n        return cam\n\n    def save(self, grid, output_dir, output_name):\n        """"""\n        Save the output to a specific dir.\n\n        Args:\n            grid (numpy.ndarray): Grid of all the heatmaps\n            output_dir (str): Output directory path\n            output_name (str): Output name\n        """"""\n        save_rgb(grid, output_dir, output_name)\n'"
tf_explain/core/gradients_inputs.py,5,"b'""""""\nCore Module for Gradients*Inputs\n""""""\nimport tensorflow as tf\n\nfrom tf_explain.core.vanilla_gradients import VanillaGradients\n\n\nclass GradientsInputs(VanillaGradients):\n\n    """"""\n    Perform Gradients*Inputs algorithm (gradients ponderated by the input values).\n    """"""\n\n    @staticmethod\n    @tf.function\n    def compute_gradients(images, model, class_index):\n        """"""\n        Compute gradients ponderated by input values for target class.\n\n        Args:\n            images (numpy.ndarray): 4D-Tensor of images with shape (batch_size, H, W, 3)\n            model (tf.keras.Model): tf.keras model to inspect\n            class_index (int): Index of targeted class\n\n        Returns:\n            tf.Tensor: 4D-Tensor\n        """"""\n        gradients = VanillaGradients.compute_gradients(images, model, class_index)\n        inputs = tf.cast(images, tf.float32)\n\n        return tf.multiply(inputs, gradients)\n'"
tf_explain/core/integrated_gradients.py,9,"b'""""""\nCore Module for Integrated Gradients Algorithm\n""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_explain.utils.display import grid_display\nfrom tf_explain.utils.image import transform_to_normalized_grayscale\nfrom tf_explain.utils.saver import save_grayscale\n\n\nclass IntegratedGradients:\n\n    """"""\n    Perform Integrated Gradients algorithm for a given input\n\n    Paper: [Axiomatic Attribution for Deep Networks](https://arxiv.org/pdf/1703.01365.pdf)\n    """"""\n\n    def explain(self, validation_data, model, class_index, n_steps=10):\n        """"""\n        Compute Integrated Gradients for a specific class index\n\n        Args:\n            validation_data (Tuple[np.ndarray, Optional[np.ndarray]]): Validation data\n                to perform the method on. Tuple containing (x, y).\n            model (tf.keras.Model): tf.keras model to inspect\n            class_index (int): Index of targeted class\n            n_steps (int): Number of steps in the path\n\n        Returns:\n            np.ndarray: Grid of all the integrated gradients\n        """"""\n        images, _ = validation_data\n\n        interpolated_images = IntegratedGradients.generate_interpolations(\n            np.array(images), n_steps\n        )\n\n        integrated_gradients = IntegratedGradients.get_integrated_gradients(\n            interpolated_images, model, class_index, n_steps\n        )\n\n        grayscale_integrated_gradients = transform_to_normalized_grayscale(\n            tf.abs(integrated_gradients)\n        ).numpy()\n\n        grid = grid_display(grayscale_integrated_gradients)\n\n        return grid\n\n    @staticmethod\n    @tf.function\n    def get_integrated_gradients(interpolated_images, model, class_index, n_steps):\n        """"""\n        Perform backpropagation to compute integrated gradients.\n\n        Args:\n            interpolated_images (numpy.ndarray): 4D-Tensor of shape (N * n_steps, H, W, 3)\n            model (tf.keras.Model): tf.keras model to inspect\n            class_index (int): Index of targeted class\n            n_steps (int): Number of steps in the path\n\n        Returns:\n            tf.Tensor: 4D-Tensor of shape (N, H, W, 3) with integrated gradients\n        """"""\n        with tf.GradientTape() as tape:\n            inputs = tf.cast(interpolated_images, tf.float32)\n            tape.watch(inputs)\n            predictions = model(inputs)\n            loss = predictions[:, class_index]\n\n        grads = tape.gradient(loss, inputs)\n        grads_per_image = tf.reshape(grads, (-1, n_steps, *grads.shape[1:]))\n\n        integrated_gradients = tf.reduce_mean(grads_per_image, axis=1)\n\n        return integrated_gradients\n\n    @staticmethod\n    def generate_interpolations(images, n_steps):\n        """"""\n        Generate interpolation paths for batch of images.\n\n        Args:\n            images (numpy.ndarray): 4D-Tensor of images with shape (N, H, W, 3)\n            n_steps (int): Number of steps in the path\n\n        Returns:\n            numpy.ndarray: Interpolation paths for each image with shape (N * n_steps, H, W, 3)\n        """"""\n        baseline = np.zeros(images.shape[1:])\n\n        return np.concatenate(\n            [\n                IntegratedGradients.generate_linear_path(baseline, image, n_steps)\n                for image in images\n            ]\n        )\n\n    @staticmethod\n    def generate_linear_path(baseline, target, n_steps):\n        """"""\n        Generate the interpolation path between the baseline image and the target image.\n\n        Args:\n            baseline (numpy.ndarray): Reference image\n            target (numpy.ndarray): Target image\n            n_steps (int): Number of steps in the path\n\n        Returns:\n            List(np.ndarray): List of images for each step\n        """"""\n        return [\n            baseline + (target - baseline) * index / (n_steps - 1)\n            for index in range(n_steps)\n        ]\n\n    def save(self, grid, output_dir, output_name):\n        """"""\n        Save the output to a specific dir.\n\n        Args:\n            grid (numpy.ndarray): Grid of all the heatmaps\n            output_dir (str): Output directory path\n            output_name (str): Output name\n        """"""\n        save_grayscale(grid, output_dir, output_name)\n'"
tf_explain/core/occlusion_sensitivity.py,2,"b'""""""\nCore Module for Occlusion Sensitivity\n""""""\nimport math\n\nimport cv2\nimport numpy as np\n\nfrom tf_explain.utils.display import grid_display, heatmap_display\nfrom tf_explain.utils.image import apply_grey_patch\nfrom tf_explain.utils.saver import save_rgb\n\n\nclass OcclusionSensitivity:\n\n    """"""\n    Perform Occlusion Sensitivity for a given input\n    """"""\n\n    def __init__(self, batch_size=None):\n        self.batch_size = batch_size\n\n    def explain(\n        self,\n        validation_data,\n        model,\n        class_index,\n        patch_size,\n        colormap=cv2.COLORMAP_VIRIDIS,\n    ):\n        """"""\n        Compute Occlusion Sensitivity maps for a specific class index.\n\n        Args:\n            validation_data (Tuple[np.ndarray, Optional[np.ndarray]]): Validation data\n                to perform the method on. Tuple containing (x, y).\n            model (tf.keras.Model): tf.keras model to inspect\n            class_index (int): Index of targeted class\n            patch_size (int): Size of patch to apply on the image\n            colormap (int): OpenCV Colormap to use for heatmap visualization\n\n        Returns:\n            np.ndarray: Grid of all the sensitivity maps with shape (batch_size, H, W, 3)\n        """"""\n        images, _ = validation_data\n        sensitivity_maps = np.array(\n            [\n                self.get_sensitivity_map(model, image, class_index, patch_size)\n                for image in images\n            ]\n        )\n\n        heatmaps = np.array(\n            [\n                heatmap_display(heatmap, image, colormap)\n                for heatmap, image in zip(sensitivity_maps, images)\n            ]\n        )\n\n        grid = grid_display(heatmaps)\n\n        return grid\n\n    def get_sensitivity_map(self, model, image, class_index, patch_size):\n        """"""\n        Compute sensitivity map on a given image for a specific class index.\n\n        Args:\n            model (tf.keras.Model): tf.keras model to inspect\n            image:\n            class_index (int): Index of targeted class\n            patch_size (int): Size of patch to apply on the image\n\n        Returns:\n            np.ndarray: Sensitivity map with shape (H, W, 3)\n        """"""\n        sensitivity_map = np.zeros(\n            (\n                math.ceil(image.shape[0] / patch_size),\n                math.ceil(image.shape[1] / patch_size),\n            )\n        )\n\n        patches = [\n            apply_grey_patch(image, top_left_x, top_left_y, patch_size)\n            for index_x, top_left_x in enumerate(range(0, image.shape[0], patch_size))\n            for index_y, top_left_y in enumerate(range(0, image.shape[1], patch_size))\n        ]\n\n        coordinates = [\n            (index_y, index_x)\n            for index_x in range(\n                sensitivity_map.shape[1]  # pylint: disable=unsubscriptable-object\n            )\n            for index_y in range(\n                sensitivity_map.shape[0]  # pylint: disable=unsubscriptable-object\n            )\n        ]\n\n        predictions = model.predict(np.array(patches), batch_size=self.batch_size)\n        target_class_predictions = [\n            prediction[class_index] for prediction in predictions\n        ]\n\n        for (index_y, index_x), confidence in zip(\n            coordinates, target_class_predictions\n        ):\n            sensitivity_map[index_y, index_x] = 1 - confidence\n\n        return cv2.resize(sensitivity_map, image.shape[0:2])\n\n    def save(self, grid, output_dir, output_name):\n        """"""\n        Save the output to a specific dir.\n\n        Args:\n            grid (numpy.ndarray): Grid of all heatmaps\n            output_dir (str): Output directory path\n            output_name (str): Output name\n        """"""\n        save_rgb(grid, output_dir, output_name)\n'"
tf_explain/core/smoothgrad.py,12,"b'# pylint: disable=no-member\n""""""\nCore Module for SmoothGrad Algorithm\n""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_explain.utils.display import grid_display\nfrom tf_explain.utils.image import transform_to_normalized_grayscale\nfrom tf_explain.utils.saver import save_grayscale\n\n\nclass SmoothGrad:\n\n    """"""\n    Perform SmoothGrad algorithm for a given input\n\n    Paper: [SmoothGrad: removing noise by adding noise](https://arxiv.org/abs/1706.03825)\n    """"""\n\n    def explain(self, validation_data, model, class_index, num_samples=5, noise=1.0):\n        """"""\n        Compute SmoothGrad for a specific class index\n\n        Args:\n            validation_data (Tuple[np.ndarray, Optional[np.ndarray]]): Validation data\n                to perform the method on. Tuple containing (x, y).\n            model (tf.keras.Model): tf.keras model to inspect\n            class_index (int): Index of targeted class\n            num_samples (int): Number of noisy samples to generate for each input image\n            noise (float): Standard deviation for noise normal distribution\n\n        Returns:\n            np.ndarray: Grid of all the smoothed gradients\n        """"""\n        images, _ = validation_data\n\n        noisy_images = SmoothGrad.generate_noisy_images(images, num_samples, noise)\n\n        smoothed_gradients = SmoothGrad.get_averaged_gradients(\n            noisy_images, model, class_index, num_samples\n        )\n\n        grayscale_gradients = transform_to_normalized_grayscale(\n            tf.abs(smoothed_gradients)\n        ).numpy()\n\n        grid = grid_display(grayscale_gradients)\n\n        return grid\n\n    @staticmethod\n    def generate_noisy_images(images, num_samples, noise):\n        """"""\n        Generate num_samples noisy images with std noise for each image.\n\n        Args:\n            images (numpy.ndarray): 4D-Tensor with shape (batch_size, H, W, 3)\n            num_samples (int): Number of noisy samples to generate for each input image\n            noise (float): Standard deviation for noise normal distribution\n\n        Returns:\n            np.ndarray: 4D-Tensor of noisy images with shape (batch_size*num_samples, H, W, 3)\n        """"""\n        repeated_images = np.repeat(images, num_samples, axis=0)\n        noise = np.random.normal(0, noise, repeated_images.shape).astype(np.float32)\n\n        return repeated_images + noise\n\n    @staticmethod\n    @tf.function\n    def get_averaged_gradients(noisy_images, model, class_index, num_samples):\n        """"""\n        Compute average of gradients for target class.\n\n        Args:\n            noisy_images (tf.Tensor): 4D-Tensor of noisy images with shape\n                (batch_size*num_samples, H, W, 3)\n            model (tf.keras.Model): tf.keras model to inspect\n            class_index (int): Index of targeted class\n            num_samples (int): Number of noisy samples to generate for each input image\n\n        Returns:\n            tf.Tensor: 4D-Tensor with smoothed gradients, with shape (batch_size, H, W, 1)\n        """"""\n        num_classes = model.output.shape[1]\n\n        expected_output = tf.one_hot([class_index] * noisy_images.shape[0], num_classes)\n\n        with tf.GradientTape() as tape:\n            inputs = tf.cast(noisy_images, tf.float32)\n            tape.watch(inputs)\n            predictions = model(inputs)\n            loss = tf.keras.losses.categorical_crossentropy(\n                expected_output, predictions\n            )\n\n        grads = tape.gradient(loss, inputs)\n\n        grads_per_image = tf.reshape(grads, (-1, num_samples, *grads.shape[1:]))\n        averaged_grads = tf.reduce_mean(grads_per_image, axis=1)\n\n        return averaged_grads\n\n    def save(self, grid, output_dir, output_name):\n        """"""\n        Save the output to a specific dir.\n\n        Args:\n            grid (numpy.ndarray): Gtid of all the smoothed gradients\n            output_dir (str): Output directory path\n            output_name (str): Output name\n        """"""\n        save_grayscale(grid, output_dir, output_name)\n'"
tf_explain/core/vanilla_gradients.py,9,"b'""""""\nCore Module for Vanilla Gradients\n""""""\nimport tensorflow as tf\n\nfrom tf_explain.utils.display import grid_display\nfrom tf_explain.utils.image import transform_to_normalized_grayscale\nfrom tf_explain.utils.saver import save_grayscale\n\n\nclass VanillaGradients:\n\n    """"""\n    Perform gradients backpropagation for a given input\n\n    Paper: [Deep Inside Convolutional Networks: Visualising Image Classification\n        Models and Saliency Maps](https://arxiv.org/abs/1312.6034)\n    """"""\n\n    def explain(self, validation_data, model, class_index):\n        """"""\n        Perform gradients backpropagation for a given input\n\n        Args:\n            validation_data (Tuple[np.ndarray, Optional[np.ndarray]]): Validation data\n                to perform the method on. Tuple containing (x, y).\n            model (tf.keras.Model): tf.keras model to inspect\n            class_index (int): Index of targeted class\n\n        Returns:\n            numpy.ndarray: Grid of all the gradients\n        """"""\n        images, _ = validation_data\n\n        gradients = self.compute_gradients(images, model, class_index)\n\n        grayscale_gradients = transform_to_normalized_grayscale(\n            tf.abs(gradients)\n        ).numpy()\n\n        grid = grid_display(grayscale_gradients)\n\n        return grid\n\n    @staticmethod\n    @tf.function\n    def compute_gradients(images, model, class_index):\n        """"""\n        Compute gradients for target class.\n\n        Args:\n            images (numpy.ndarray): 4D-Tensor of images with shape (batch_size, H, W, 3)\n            model (tf.keras.Model): tf.keras model to inspect\n            class_index (int): Index of targeted class\n\n        Returns:\n            tf.Tensor: 4D-Tensor\n        """"""\n        num_classes = model.output.shape[1]\n\n        expected_output = tf.one_hot([class_index] * images.shape[0], num_classes)\n\n        with tf.GradientTape() as tape:\n            inputs = tf.cast(images, tf.float32)\n            tape.watch(inputs)\n            predictions = model(inputs)\n            loss = tf.keras.losses.categorical_crossentropy(\n                expected_output, predictions\n            )\n\n        return tape.gradient(loss, inputs)\n\n    def save(self, grid, output_dir, output_name):\n        """"""\n        Save the output to a specific dir.\n\n        Args:\n            grid (numpy.ndarray): Gtid of all the smoothed gradients\n            output_dir (str): Output directory path\n            output_name (str): Output name\n        """"""\n        save_grayscale(grid, output_dir, output_name)\n'"
tf_explain/utils/__init__.py,0,b''
tf_explain/utils/display.py,0,"b'"""""" Module for display related operations. """"""\nimport math\nimport warnings\n\nimport cv2\nimport numpy as np\n\n\ndef grid_display(array, num_rows=None, num_columns=None):\n    """"""\n    Display a list of images as a grid.\n\n    Args:\n        array (numpy.ndarray): 4D Tensor (batch_size, height, width, channels)\n\n    Returns:\n        numpy.ndarray: 3D Tensor as concatenation of input images on a grid\n    """"""\n    if num_rows is not None and num_columns is not None:\n        total_grid_size = num_rows * num_columns\n        if total_grid_size < len(array):\n            warnings.warn(\n                Warning(\n                    ""Given values for num_rows and num_columns doesn\'t allow to display ""\n                    ""all images. Values have been overrided to respect at least num_columns""\n                )\n            )\n            num_rows = math.ceil(len(array) / num_columns)\n    elif num_rows is not None:\n        num_columns = math.ceil(len(array) / num_rows)\n    elif num_columns is not None:\n        num_rows = math.ceil(len(array) / num_columns)\n    else:\n        num_rows = math.ceil(math.sqrt(len(array)))\n        num_columns = math.ceil(math.sqrt(len(array)))\n\n    number_of_missing_elements = num_columns * num_rows - len(array)\n    # We fill the array with np.zeros elements to obtain a perfect square\n    array = np.append(\n        array,\n        np.zeros((number_of_missing_elements, *array[0].shape)).astype(array.dtype),\n        axis=0,\n    )\n\n    grid = np.concatenate(\n        [\n            np.concatenate(\n                array[index * num_columns : (index + 1) * num_columns], axis=1\n            )\n            for index in range(num_rows)\n        ],\n        axis=0,\n    )\n\n    return grid\n\n\ndef filter_display(array, num_rows=None, num_columns=None):\n    """"""\n    Display a list of filter outputs as a greyscale images grid.\n\n    Args:\n        array (numpy.ndarray): 4D Tensor (batch_size, height, width, channels)\n\n    Returns:\n        numpy.ndarray: 3D Tensor as concatenation of input images on a grid\n    """"""\n    return grid_display(\n        np.concatenate(np.rollaxis(array, 3, 1), axis=0), num_rows, num_columns\n    )\n\n\ndef image_to_uint_255(image):\n    """"""\n    Convert float images to int 0-255 images.\n\n    Args:\n        image (numpy.ndarray): Input image. Can be either [0, 255], [0, 1], [-1, 1]\n\n    Returns:\n        numpy.ndarray:\n    """"""\n    if image.dtype == np.uint8:\n        return image\n\n    if image.min() < 0:\n        image = (image + 1.0) / 2.0\n\n    return (image * 255).astype(""uint8"")\n\n\ndef heatmap_display(\n    heatmap, original_image, colormap=cv2.COLORMAP_VIRIDIS, image_weight=0.7\n):\n    """"""\n    Apply a heatmap (as an np.ndarray) on top of an original image.\n\n    Args:\n        heatmap (numpy.ndarray): Array corresponding to the heatmap\n        original_image (numpy.ndarray): Image on which we apply the heatmap\n        colormap (int): OpenCV Colormap to use for heatmap visualization\n        image_weight (float): An optional `float` value in range [0,1] indicating the weight of\n            the input image to be overlaying the calculated attribution maps. Defaults to `0.7`\n\n    Returns:\n        np.ndarray: Original image with heatmap applied\n    """"""\n    heatmap = cv2.resize(heatmap, (original_image.shape[1], original_image.shape[0]))\n\n    image = image_to_uint_255(original_image)\n\n    heatmap = (heatmap - np.min(heatmap)) / (heatmap.max() - heatmap.min())\n\n    heatmap = cv2.applyColorMap(\n        cv2.cvtColor((heatmap * 255).astype(""uint8""), cv2.COLOR_GRAY2BGR), colormap\n    )\n\n    output = cv2.addWeighted(\n        cv2.cvtColor(image, cv2.COLOR_RGB2BGR), image_weight, heatmap, 1, 0\n    )\n\n    return cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n'"
tf_explain/utils/image.py,6,"b'"""""" Module for image operations """"""\nimport numpy as np\nimport tensorflow as tf\n\n\ndef apply_grey_patch(image, top_left_x, top_left_y, patch_size):\n    """"""\n    Replace a part of the image with a grey patch.\n\n    Args:\n        image (numpy.ndarray): Input image\n        top_left_x (int): Top Left X position of the applied box\n        top_left_y (int): Top Left Y position of the applied box\n        patch_size (int): Size of patch to apply\n\n    Returns:\n        numpy.ndarray: Patched image\n    """"""\n    patched_image = np.array(image, copy=True)\n    patched_image[\n        top_left_y : top_left_y + patch_size, top_left_x : top_left_x + patch_size, :\n    ] = 127.5\n\n    return patched_image\n\n\n@tf.function\ndef transform_to_normalized_grayscale(tensor):\n    """"""\n    Transform tensor over RGB axis to grayscale.\n\n    Args:\n        tensor (tf.Tensor): 4D-Tensor with shape (batch_size, H, W, 3)\n\n    Returns:\n        tf.Tensor: 4D-Tensor of grayscale tensor, with shape (batch_size, H, W, 1)\n    """"""\n    grayscale_tensor = tf.reduce_sum(tensor, axis=-1)\n\n    normalized_tensor = tf.cast(\n        255 * tf.image.per_image_standardization(grayscale_tensor), tf.uint8\n    )\n\n    return normalized_tensor\n'"
tf_explain/utils/saver.py,0,"b'""""""\nModule for Image Save\n""""""\nfrom pathlib import Path\n\nimport cv2\n\n\ndef save_grayscale(image, output_dir, output_name):\n    """"""\n    Save a 3D Numpy array (H, W, 1) as an image.\n\n    Args:\n        image (numpy.ndarray): Image to save\n        output_dir (str): Output directory\n        output_name (str): Output name\n    """"""\n    Path.mkdir(Path(output_dir), parents=True, exist_ok=True)\n\n    cv2.imwrite(str(Path(output_dir) / output_name), image)\n\n\ndef save_rgb(image, output_dir, output_name):\n    """"""\n    Save a 3D Numpy array (H, W, 3) as an image.\n\n    Args:\n        image (numpy.ndarray): Image to save\n        output_dir (str): Output directory\n        output_name (str): Output name\n    """"""\n    Path.mkdir(Path(output_dir), parents=True, exist_ok=True)\n\n    cv2.imwrite(\n        str(Path(output_dir) / output_name), cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    )\n'"
