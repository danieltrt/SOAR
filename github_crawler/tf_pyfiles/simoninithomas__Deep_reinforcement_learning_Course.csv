file_path,api_count,code
A2C with Sonic the Hedgehog/agent.py,2,"b'import tensorflow as tf\r\nimport numpy as np\r\nimport gym\r\nimport math\r\nimport os\r\n\r\nimport model\r\nimport architecture as policies\r\nimport sonic_env as env\r\n\r\n# SubprocVecEnv creates a vector of n environments to run them simultaneously.\r\nfrom baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\r\n\r\n\r\ndef main():\r\n    config = tf.ConfigProto()\r\n\r\n    # Avoid warning message errors\r\n    os.environ[""CUDA_VISIBLE_DEVICES""]=""0""\r\n\r\n    # Allowing GPU memory growth\r\n    config.gpu_options.allow_growth = True\r\n\r\n    with tf.Session(config=config):\r\n        model.learn(policy=policies.A2CPolicy,\r\n                            env=SubprocVecEnv([env.make_train_0, env.make_train_1, env.make_train_2, env.make_train_3, env.make_train_4, env.make_train_5,env.make_train_6,env.make_train_7,env.make_train_8,env.make_train_9,env.make_train_10,env.make_train_11,env.make_train_12 ]), \r\n                            nsteps=2048, # Steps per environment\r\n                            total_timesteps=10000000,\r\n                            gamma=0.99,\r\n                            lam = 0.95,\r\n                            vf_coef=0.5,\r\n                            ent_coef=0.01,\r\n                            lr = 2e-4,\r\n                            max_grad_norm = 0.5, \r\n                            log_interval = 10\r\n                            )\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n\r\n\r\n'"
A2C with Sonic the Hedgehog/architecture.py,10,"b'import numpy as np\r\nimport tensorflow as tf\r\n\r\n# This function selects the probability distribution over actions\r\nfrom baselines.common.distributions import make_pdtype\r\n\r\n# Convolution layer\r\ndef conv_layer(inputs, filters, kernel_size, strides, gain=1.0):\r\n    return tf.layers.conv2d(inputs=inputs,\r\n                            filters=filters,\r\n                            kernel_size=kernel_size,\r\n                            strides=(strides, strides),\r\n                            activation=tf.nn.relu,\r\n                            kernel_initializer=tf.orthogonal_initializer(gain=gain))\r\n\r\n\r\n# Fully connected layer\r\ndef fc_layer(inputs, units, activation_fn=tf.nn.relu, gain=1.0):\r\n    return tf.layers.dense(inputs=inputs,\r\n                           units=units,\r\n                           activation=activation_fn,\r\n                           kernel_initializer=tf.orthogonal_initializer(gain))\r\n\r\n\r\n""""""\r\nThis object creates the A2C Network architecture\r\n""""""\r\nclass A2CPolicy(object):\r\n    def __init__(self, sess, ob_space, action_space, nbatch, nsteps, reuse = False):\r\n        # This will use to initialize our kernels\r\n        gain = np.sqrt(2)\r\n\r\n        # Based on the action space, will select what probability distribution type\r\n        # we will use to distribute action in our stochastic policy (in our case DiagGaussianPdType\r\n        # aka Diagonal Gaussian, 3D normal distribution\r\n        self.pdtype = make_pdtype(action_space)\r\n\r\n        height, weight, channel = ob_space.shape\r\n        ob_shape = (height, weight, channel)\r\n\r\n        # Create the input placeholder\r\n        inputs_ = tf.placeholder(tf.float32, [None, *ob_shape], name=""input"")\r\n\r\n        # Normalize the images\r\n        scaled_images = tf.cast(inputs_, tf.float32) / 255.\r\n\r\n        """"""\r\n        Build the model\r\n        3 CNN for spatial dependencies\r\n        Temporal dependencies is handle by stacking frames\r\n        (Something funny nobody use LSTM in OpenAI Retro contest)\r\n        1 common FC\r\n        1 FC for policy\r\n        1 FC for value\r\n        """"""\r\n        with tf.variable_scope(""model"", reuse = reuse):\r\n            conv1 = conv_layer(scaled_images, 32, 8, 4, gain)\r\n            conv2 = conv_layer(conv1, 64, 4, 2, gain)\r\n            conv3 = conv_layer(conv2, 64, 3, 1, gain)\r\n            flatten1 = tf.layers.flatten(conv3)\r\n            fc_common = fc_layer(flatten1, 512, gain=gain)\r\n\r\n            # This build a fc connected layer that returns a probability distribution\r\n            # over actions (self.pd) and our pi logits (self.pi).\r\n            self.pd, self.pi = self.pdtype.pdfromlatent(fc_common, init_scale=0.01)\r\n\r\n            # Calculate the v(s)\r\n            vf = fc_layer(fc_common, 1, activation_fn=None)[:, 0]\r\n\r\n        self.initial_state = None\r\n\r\n        # Take an action in the action distribution (remember we are in a situation\r\n        # of stochastic policy so we don\'t always take the action with the highest probability\r\n        # for instance if we have 2 actions 0.7 and 0.3 we have 30% chance to take the second)\r\n        a0 = self.pd.sample()\r\n\r\n        # Function use to take a step returns action to take and V(s)\r\n        def step(state_in, *_args, **_kwargs):\r\n            action, value = sess.run([a0, vf], {inputs_: state_in})\r\n           \r\n            #print(""step"", action)\r\n            \r\n            return action, value\r\n\r\n        # Function that calculates only the V(s)\r\n        def value(state_in, *_args, **_kwargs):\r\n            return sess.run(vf, {inputs_: state_in})\r\n\r\n        # Function that output only the action to take\r\n        def select_action(state_in, *_args, **_kwargs):\r\n            return sess.run(a0, {inputs_: state_in})\r\n\r\n        self.inputs_ = inputs_\r\n        self.vf = vf\r\n        self.step = step\r\n        self.value = value\r\n        self.select_action = select_action\r\n'"
A2C with Sonic the Hedgehog/model.py,15,"b'import os\r\nimport time\r\nimport numpy as np\r\nimport os.path as osp\r\nimport tensorflow as tf\r\nfrom baselines import logger\r\nimport cv2\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\n# Calculate cross entropy\r\nfrom baselines.a2c.utils import cat_entropy, mse\r\nfrom utilities import make_path, find_trainable_variables, discount_with_dones\r\n\r\nfrom baselines.common import explained_variance\r\nfrom baselines.common.runners import AbstractEnvRunner\r\n\r\nclass Model(object):\r\n    """"""\r\n    We use this object to :\r\n    __init__:\r\n    - Creates the step_model\r\n    - Creates the train_model\r\n\r\n    train():\r\n    - Make the training part (feedforward and retropropagation of gradients)\r\n\r\n    save/load():\r\n    - Save load the model\r\n    """"""\r\n    def __init__(self,\r\n                 policy,\r\n                ob_space,\r\n                action_space,\r\n                nenvs,\r\n                nsteps,\r\n                ent_coef,\r\n                vf_coef,\r\n                max_grad_norm):\r\n\r\n        sess = tf.get_default_session()\r\n\r\n        # Here we create the placeholders\r\n        actions_ = tf.placeholder(tf.int32, [None], name=""actions_"")\r\n        advantages_ = tf.placeholder(tf.float32, [None], name=""advantages_"")\r\n        rewards_ = tf.placeholder(tf.float32, [None], name=""rewards_"")\r\n        lr_ = tf.placeholder(tf.float32, name=""learning_rate_"")\r\n\r\n        # Here we create our two models:\r\n        # Step_model that is used for sampling\r\n        step_model = policy(sess, ob_space, action_space, nenvs, 1, reuse=False)\r\n\r\n        # Train model for training\r\n        train_model = policy(sess, ob_space, action_space, nenvs*nsteps, nsteps, reuse=True)\r\n\r\n        """"""\r\n        Calculate the loss\r\n        Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss\r\n        """"""\r\n        # Policy loss\r\n        # Output -log(pi)\r\n        neglogpac = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=train_model.pi, labels=actions_)\r\n\r\n        # 1/n * sum A(si,ai) * -logpi(ai|si)\r\n        pg_loss = tf.reduce_mean(advantages_ * neglogpac)\r\n\r\n        # Value loss 1/2 SUM [R - V(s)]^2\r\n        vf_loss = tf.reduce_mean(mse(tf.squeeze(train_model.vf),rewards_))\r\n\r\n        # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.\r\n        entropy = tf.reduce_mean(train_model.pd.entropy())\r\n\r\n\r\n        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef\r\n\r\n        # Update parameters using loss\r\n        # 1. Get the model parameters\r\n        params = find_trainable_variables(""model"")\r\n\r\n        # 2. Calculate the gradients\r\n        grads = tf.gradients(loss, params)\r\n        if max_grad_norm is not None:\r\n            # Clip the gradients (normalize)\r\n            grads, grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\r\n        grads = list(zip(grads, params))\r\n        # zip aggregate each gradient with parameters associated\r\n        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da\r\n\r\n        # 3. Build our trainer\r\n        trainer = tf.train.RMSPropOptimizer(learning_rate=lr_, decay=0.99, epsilon=1e-5)\r\n\r\n        # 4. Backpropagation\r\n        _train = trainer.apply_gradients(grads)\r\n\r\n        def train(states_in, actions, returns, values, lr):\r\n            # Here we calculate advantage A(s,a) = R + yV(s\') - V(s)\r\n            # Returns = R + yV(s\')\r\n            advantages = returns - values\r\n\r\n            # We create the feed dictionary\r\n            td_map = {train_model.inputs_: states_in,\r\n                     actions_: actions,\r\n                     advantages_: advantages, # Use to calculate our policy loss\r\n                     rewards_: returns, # Use as a bootstrap for real value\r\n                     lr_: lr}\r\n\r\n            policy_loss, value_loss, policy_entropy, _= sess.run([pg_loss, vf_loss, entropy, _train], td_map)\r\n            \r\n            return policy_loss, value_loss, policy_entropy\r\n\r\n\r\n        def save(save_path):\r\n            """"""\r\n            Save the model\r\n            """"""\r\n            saver = tf.train.Saver()\r\n            saver.save(sess, save_path)\r\n\r\n        def load(load_path):\r\n            """"""\r\n            Load the model\r\n            """"""\r\n            saver = tf.train.Saver()\r\n            print(\'Loading \' + load_path)\r\n            saver.restore(sess, load_path)\r\n\r\n        self.train = train\r\n        self.train_model = train_model\r\n        self.step_model = step_model\r\n        self.step = step_model.step\r\n        self.value = step_model.value\r\n        self.initial_state = step_model.initial_state\r\n        self.save = save\r\n        self.load = load\r\n        tf.global_variables_initializer().run(session=sess)\r\n\r\nclass Runner(AbstractEnvRunner):\r\n    """"""\r\n    We use this object to make a mini batch of experiences\r\n    __init__:\r\n    - Initialize the runner\r\n\r\n    run():\r\n    - Make a mini batch\r\n    """"""\r\n    def __init__(self, env, model, nsteps, total_timesteps, gamma, lam):\r\n        super().__init__(env = env, model = model, nsteps = nsteps)\r\n\r\n        # Discount rate\r\n        self.gamma = gamma\r\n\r\n        # Lambda used in GAE (General Advantage Estimation)\r\n        self.lam = lam\r\n\r\n        # Total timesteps taken\r\n        self.total_timesteps = total_timesteps\r\n\r\n    def run(self):\r\n        # Here, we init the lists that will contain the mb of experiences\r\n        mb_obs, mb_actions, mb_rewards, mb_values, mb_dones = [],[],[],[],[]\r\n\r\n        # For n in range number of steps\r\n        for n in range(self.nsteps):\r\n            # Given observations, take action and value (V(s))\r\n            # We already have self.obs because AbstractEnvRunner run self.obs[:] = env.reset()\r\n            actions, values = self.model.step(self.obs, self.dones)\r\n\r\n            #print(""actions runner runner"", actions)\r\n\r\n            # Append the observations into the mb\r\n            mb_obs.append(np.copy(self.obs)) #obs len nenvs (1 step per env)\r\n\r\n            # Append the actions taken into the mb\r\n            mb_actions.append(actions)\r\n\r\n            # Append the values calculated into the mb\r\n            mb_values.append(values)\r\n\r\n            # Append the dones situations into the mb\r\n            mb_dones.append(self.dones)\r\n\r\n            # Take actions in env and look the results\r\n            self.obs[:], rewards, self.dones, _ = self.env.step(actions)\r\n\r\n            mb_rewards.append(rewards)\r\n\r\n        #batch of steps to batch of rollouts\r\n        mb_obs = np.asarray(mb_obs, dtype=np.uint8)\r\n        mb_rewards = np.asarray(mb_rewards, dtype=np.float32)\r\n        mb_actions = np.asarray(mb_actions, dtype=np.int32)\r\n        mb_values = np.asarray(mb_values, dtype=np.float32)\r\n        mb_dones = np.asarray(mb_dones, dtype=np.bool)\r\n        last_values = self.model.value(self.obs)\r\n          \r\n\r\n        ### GENERALIZED ADVANTAGE ESTIMATION\r\n        # discount/bootstrap off value fn\r\n        # We create mb_returns and mb_advantages\r\n        # mb_returns will contain Advantage + value\r\n        mb_returns = np.zeros_like(mb_rewards)\r\n        mb_advantages = np.zeros_like(mb_rewards)\r\n\r\n        lastgaelam = 0\r\n\r\n        # From last step to first step\r\n        for t in reversed(range(self.nsteps)):\r\n            # If t == before last step\r\n            if t == self.nsteps - 1:\r\n                # If a state is done, nextnonterminal = 0\r\n                # In fact nextnonterminal allows us to do that logic\r\n\r\n                #if done (so nextnonterminal = 0):\r\n                #    delta = R - V(s) (because self.gamma * nextvalues * nextnonterminal = 0) \r\n                # else (not done)\r\n                    #delta = R + gamma * V(st+1)\r\n                nextnonterminal = 1.0 - self.dones\r\n                \r\n                # V(t+1)\r\n                nextvalues = last_values\r\n            else:\r\n                nextnonterminal = 1.0 - mb_dones[t+1]\r\n                \r\n                nextvalues = mb_values[t+1]\r\n\r\n            # Delta = R(st) + gamma * V(t+1) * nextnonterminal  - V(st)\r\n            delta = mb_rewards[t] + self.gamma * nextvalues * nextnonterminal - mb_values[t]\r\n\r\n            # Advantage = delta + gamma *  \xce\xbb (lambda) * nextnonterminal  * lastgaelam\r\n            mb_advantages[t] = lastgaelam = delta + self.gamma * self.lam * nextnonterminal * lastgaelam\r\n\r\n        # Returns\r\n        mb_returns = mb_advantages + mb_values\r\n\r\n        return map(sf01, (mb_obs, mb_actions, mb_returns, mb_values))\r\n\r\n\r\ndef sf01(arr):\r\n    """"""\r\n    swap and then flatten axes 0 and 1\r\n    """"""\r\n    s = arr.shape\r\n    return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])\r\n\r\ndef learn(policy,\r\n            env,\r\n            nsteps,\r\n            total_timesteps,\r\n            gamma,\r\n            lam,\r\n            vf_coef,\r\n            ent_coef,\r\n            lr,\r\n            max_grad_norm,\r\n            log_interval):\r\n\r\n    noptepochs = 4\r\n    nminibatches = 8\r\n\r\n\r\n    # Get the nb of env\r\n    nenvs = env.num_envs\r\n\r\n    # Get state_space and action_space\r\n    ob_space = env.observation_space\r\n    ac_space = env.action_space\r\n\r\n    # Calculate the batch_size\r\n    batch_size = nenvs * nsteps # For instance if we take 5 steps and we have 5 environments batch_size = 25\r\n\r\n    batch_train_size = batch_size // nminibatches\r\n\r\n    assert batch_size % nminibatches == 0\r\n\r\n    # Instantiate the model object (that creates step_model and train_model)\r\n    model = Model(policy=policy,\r\n                ob_space=ob_space,\r\n                action_space=ac_space,\r\n                nenvs=nenvs,\r\n                nsteps=nsteps,\r\n                ent_coef=ent_coef,\r\n                vf_coef=vf_coef,\r\n                max_grad_norm=max_grad_norm)\r\n\r\n\r\n    # Load the model\r\n    # If you want to continue training\r\n    load_path = ""./models/260/model.ckpt""\r\n    model.load(load_path)\r\n\r\n    # Instantiate the runner object\r\n    runner = Runner(env, model, nsteps=nsteps, total_timesteps=total_timesteps, gamma=gamma, lam=lam)\r\n\r\n\r\n    # Start total timer\r\n    tfirststart = time.time()\r\n\r\n\r\n    for update in range(1, total_timesteps//batch_size+1):\r\n        # Start timer\r\n        tstart = time.time()\r\n\r\n        # Get minibatch\r\n        obs, actions, returns, values = runner.run()\r\n\r\n        # Here what we\'re going to do is for each minibatch calculate the loss and append it.\r\n        mb_losses = []\r\n        total_batches_train = 0\r\n\r\n        # Index of each element of batch_size\r\n        # Create the indices array\r\n        indices = np.arange(batch_size)\r\n\r\n        for _ in range(noptepochs):\r\n            # Randomize the indexes\r\n            np.random.shuffle(indices)\r\n\r\n            # 0 to batch_size with batch_train_size step\r\n            for start in range(0, batch_size, batch_train_size):\r\n                end = start + batch_train_size\r\n                mbinds = indices[start:end]\r\n                slices = (arr[mbinds] for arr in (obs, actions, returns, values))\r\n                mb_losses.append(model.train(*slices, lr))\r\n\r\n        # Feedforward --> get losses --> update\r\n        lossvalues = np.mean(mb_losses, axis=0)\r\n\r\n        # End timer\r\n        tnow = time.time()\r\n\r\n        # Calculate the fps (frame per second)\r\n        fps = int(batch_size / (tnow - tstart))\r\n\r\n        if update % log_interval == 0 or update == 1:\r\n            \r\n            """"""\r\n            Computes fraction of variance that ypred explains about y.\r\n            Returns 1 - Var[y-ypred] / Var[y]\r\n            interpretation:\r\n            ev=0  =>  might as well have predicted zero\r\n            ev=1  =>  perfect prediction\r\n            ev<0  =>  worse than just predicting zero\r\n            """"""\r\n            ev = explained_variance(values, returns)\r\n            logger.record_tabular(""nupdates"", update)\r\n            logger.record_tabular(""total_timesteps"", update*batch_size)\r\n            logger.record_tabular(""fps"", fps)\r\n            logger.record_tabular(""policy_loss"", float(lossvalues[0]))\r\n            logger.record_tabular(""policy_entropy"", float(lossvalues[2]))\r\n            logger.record_tabular(""value_loss"", float(lossvalues[1]))\r\n            logger.record_tabular(""explained_variance"", float(ev))\r\n            logger.record_tabular(""time elapsed"", float(tnow - tfirststart))\r\n            logger.dump_tabular()\r\n\r\n            savepath = ""./models/"" + str(update) + ""/model.ckpt""\r\n            model.save(savepath)\r\n            print(\'Saving to\', savepath)\r\n            \r\n    env.close()\r\n\r\n\r\ndef play(policy, env):\r\n\r\n    # Get state_space and action_space\r\n    ob_space = env.observation_space\r\n    ac_space = env.action_space\r\n\r\n    # Instantiate the model object (that creates step_model and train_model)\r\n    model = Model(policy=policy,\r\n                ob_space=ob_space,\r\n                action_space=ac_space,\r\n                nenvs=1,\r\n                nsteps=1,\r\n                ent_coef=0,\r\n                vf_coef=0,\r\n                max_grad_norm=0)\r\n    \r\n    # Load the model\r\n    load_path = ""./models/260/model.ckpt""\r\n    model.load(load_path)\r\n\r\n    obs = env.reset()\r\n\r\n    # Play\r\n    score = 0\r\n    boom = 0\r\n    done = False\r\n    while done == False:\r\n        boom +=1\r\n        \r\n        # Get the action\r\n        actions, values = model.step(obs)\r\n        \r\n        # Take actions in env and look the results\r\n        obs, rewards, done, _ = env.step(actions)\r\n        \r\n        score += rewards\r\n    \r\n        env.render()\r\n        \r\n    \r\n    print(""Score "", score)\r\n    env.close()\r\n    \r\n\r\n\r\n\r\n'"
A2C with Sonic the Hedgehog/play.py,2,"b'import tensorflow as tf\r\nimport numpy as np\r\nimport gym\r\nimport math\r\nimport os\r\n\r\nimport model\r\nimport architecture as policies\r\nimport sonic_env as env\r\n\r\n# SubprocVecEnv creates a vector of n environments to run them simultaneously.\r\nfrom baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\r\nfrom baselines.common.vec_env.dummy_vec_env import DummyVecEnv\r\n\r\ndef main():\r\n    config = tf.ConfigProto()\r\n\r\n    # Avoid warning message errors\r\n    os.environ[""CUDA_VISIBLE_DEVICES""]=""0""\r\n\r\n    # Allowing GPU memory growth\r\n    config.gpu_options.allow_growth = True\r\n\r\n\r\n    with tf.Session(config=config):\r\n        \r\n        model.play(policy=policies.A2CPolicy, \r\n            env= DummyVecEnv([env.make_train_3]))\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
A2C with Sonic the Hedgehog/sonic_env.py,0,"b'# Part taken from adborghi fantastic implementation\r\n# https://github.com/aborghi/retro_contest_agent/blob/master/fastlearner/ppo2ttifrutti_sonic_env.py\r\nimport numpy as np\r\nimport gym\r\n\r\n#import gym_remote.client as grc\r\n\r\nfrom retro_contest.local import make\r\nfrom retro import make as make_retro\r\n\r\n# This will be useful for stacking frames\r\nfrom baselines.common.atari_wrappers import FrameStack\r\n\r\n# Library used to modify frames (former times we used matplotlib)\r\nimport cv2\r\n\r\n# setUseOpenCL = False means that we will not use GPU (disable OpenCL acceleration)\r\ncv2.ocl.setUseOpenCL(False)\r\n\r\nclass PreprocessFrame(gym.ObservationWrapper):\r\n    """"""\r\n    Here we do the preprocessing part:\r\n    - Set frame to gray\r\n    - Resize the frame to 96x96x1\r\n    """"""\r\n    def __init__(self, env):\r\n        gym.ObservationWrapper.__init__(self, env)\r\n        self.width = 96\r\n        self.height = 96\r\n        self.observation_space = gym.spaces.Box(low=0, high=255,\r\n            shape=(self.height, self.width, 1), dtype=np.uint8)\r\n\r\n    def observation(self, frame):\r\n        # Set frame to gray\r\n        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\r\n\r\n        # Resize the frame to 96x96x1\r\n        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\r\n        frame = frame[:, :, None]\r\n\r\n        return frame\r\n\r\n\r\nclass ActionsDiscretizer(gym.ActionWrapper):\r\n    """"""\r\n    Wrap a gym-retro environment and make it use discrete\r\n    actions for the Sonic game.\r\n    """"""\r\n    def __init__(self, env):\r\n        super(ActionsDiscretizer, self).__init__(env)\r\n        buttons = [""B"", ""A"", ""MODE"", ""START"", ""UP"", ""DOWN"", ""LEFT"", ""RIGHT"", ""C"", ""Y"", ""X"", ""Z""]\r\n        actions = [[\'LEFT\'], [\'RIGHT\'], [\'LEFT\', \'DOWN\'], [\'RIGHT\', \'DOWN\'], [\'DOWN\'],\r\n                   [\'DOWN\', \'B\'], [\'B\']]\r\n        self._actions = []\r\n\r\n        """"""\r\n        What we do in this loop:\r\n        For each action in actions\r\n            - Create an array of 12 False (12 = nb of buttons)\r\n            For each button in action: (for instance [\'LEFT\']) we need to make that left button index = True\r\n                - Then the button index = LEFT = True\r\n\r\n            In fact at the end we will have an array where each array is an action and each elements True of this array\r\n            are the buttons clicked.\r\n        """"""\r\n        for action in actions:\r\n            arr = np.array([False] * 12)\r\n            for button in action:\r\n                arr[buttons.index(button)] = True\r\n            self._actions.append(arr)\r\n        self.action_space = gym.spaces.Discrete(len(self._actions))\r\n\r\n    def action(self, a): # pylint: disable=W0221\r\n        return self._actions[a].copy()\r\n\r\n\r\nclass RewardScaler(gym.RewardWrapper):\r\n    """"""\r\n    Bring rewards to a reasonable scale for PPO.\r\n    This is incredibly important and effects performance\r\n    drastically.\r\n    """"""\r\n    def reward(self, reward):\r\n\r\n        return reward * 0.01\r\n\r\nclass AllowBacktracking(gym.Wrapper):\r\n    """"""\r\n    Use deltas in max(X) as the reward, rather than deltas\r\n    in X. This way, agents are not discouraged too heavily\r\n    from exploring backwards if there is no way to advance\r\n    head-on in the level.\r\n    """"""\r\n    def __init__(self, env):\r\n        super(AllowBacktracking, self).__init__(env)\r\n        self._cur_x = 0\r\n        self._max_x = 0\r\n\r\n    def reset(self, **kwargs): # pylint: disable=E0202\r\n        self._cur_x = 0\r\n        self._max_x = 0\r\n        return self.env.reset(**kwargs)\r\n\r\n    def step(self, action): # pylint: disable=E0202\r\n        obs, rew, done, info = self.env.step(action)\r\n        self._cur_x += rew\r\n        rew = max(0, self._cur_x - self._max_x)\r\n        self._max_x = max(self._max_x, self._cur_x)\r\n        return obs, rew, done, info\r\n\r\ndef make_env(env_idx):\r\n    """"""\r\n    Create an environment with some standard wrappers.\r\n    """"""\r\n\r\n    dicts = [\r\n        {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'SpringYardZone.Act3\'},\r\n        {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'SpringYardZone.Act2\'},\r\n        {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'GreenHillZone.Act3\'},\r\n        {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'GreenHillZone.Act1\'},\r\n        {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'StarLightZone.Act2\'},\r\n        {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'StarLightZone.Act1\'},\r\n        {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'MarbleZone.Act2\'},\r\n        {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'MarbleZone.Act1\'},\r\n        {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'MarbleZone.Act3\'},\r\n        {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'ScrapBrainZone.Act2\'},\r\n        {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'LabyrinthZone.Act2\'},\r\n        {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'LabyrinthZone.Act1\'},\r\n        {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'LabyrinthZone.Act3\'}\r\n        \r\n\r\n    ]\r\n    # Make the environment\r\n    print(dicts[env_idx][\'game\'], dicts[env_idx][\'state\'], flush=True)\r\n    #record_path = ""./records/"" + dicts[env_idx][\'state\']\r\n    env = make(game=dicts[env_idx][\'game\'], state=dicts[env_idx][\'state\'], bk2dir=""./records"")#record=\'/tmp\')\r\n\r\n    # Build the actions array, \r\n    env = ActionsDiscretizer(env)\r\n\r\n    # Scale the rewards\r\n    env = RewardScaler(env)\r\n\r\n    # PreprocessFrame\r\n    env = PreprocessFrame(env)\r\n\r\n    # Stack 4 frames\r\n    env = FrameStack(env, 4)\r\n\r\n    # Allow back tracking that helps agents are not discouraged too heavily\r\n    # from exploring backwards if there is no way to advance\r\n    # head-on in the level.\r\n    env = AllowBacktracking(env)\r\n\r\n\r\n\r\n    return env\r\n\r\n\r\n\r\ndef make_train_0():\r\n    return make_env(0)\r\n\r\ndef make_train_1():\r\n    return make_env(1)\r\n\r\ndef make_train_2():\r\n    return make_env(2)\r\n\r\ndef make_train_3():\r\n    return make_env(3)\r\n\r\ndef make_train_4():\r\n    return make_env(4)\r\n\r\ndef make_train_5():\r\n    return make_env(5)\r\n\r\ndef make_train_6():\r\n    return make_env(6)\r\n\r\ndef make_train_7():\r\n    return make_env(7)\r\n\r\ndef make_train_8():\r\n    return make_env(8)\r\n\r\ndef make_train_9():\r\n    return make_env(9)\r\n\r\ndef make_train_10():\r\n    return make_env(10)\r\n\r\ndef make_train_11():\r\n    return make_env(11)\r\n\r\ndef make_train_12():\r\n    return make_env(12)\r\n\r\ndef make_test_level_Green():\r\n    return make_test()\r\n\r\n\r\ndef make_test():\r\n    """"""\r\n    Create an environment with some standard wrappers.\r\n    """"""\r\n\r\n\r\n    # Make the environment\r\n    env = make_retro(game=\'SonicTheHedgehog-Genesis\', state=\'GreenHillZone.Act2\', record=""./records"")\r\n\r\n    # Build the actions array\r\n    env = ActionsDiscretizer(env)\r\n\r\n    # Scale the rewards\r\n    env = RewardScaler(env)\r\n\r\n    # PreprocessFrame\r\n    env = PreprocessFrame(env)\r\n\r\n    # Stack 4 frames\r\n    env = FrameStack(env, 4)\r\n\r\n    # Allow back tracking that helps agents are not discouraged too heavily\r\n    # from exploring backwards if there is no way to advance\r\n    # head-on in the level.\r\n    env = AllowBacktracking(env)\r\n\r\n    return env\r\n\r\n'"
A2C with Sonic the Hedgehog/utilities.py,2,"b'import tensorflow as tf\r\n# Get the variables\r\ndef find_trainable_variables(key):\r\n    with tf.variable_scope(key):\r\n        return tf.trainable_variables()\r\n\r\n\r\n# Make directory\r\ndef make_path(f):\r\n    # exist_ok: if the folder already exist makes no exception error\r\n    return os.makedirs(f, exist_ok=True)\r\n\r\n\r\ndef discount_with_dones(rewards, dones, gamma):\r\n    discounted = []\r\n    r = 0\r\n    for reward, done in zip(rewards[::-1], dones[::-1]):\r\n        r = reward + gamma*r*(1.-done) # fixed off by one bug\r\n        discounted.append(r)\r\n    return discounted[::-1]\r\n'"
PPO with Sonic the Hedgehog/agent.py,2,"b'import tensorflow as tf\r\nimport numpy as np\r\nimport gym\r\nimport math\r\nimport os\r\n\r\nimport model\r\nimport architecture as policies\r\nimport sonic_env as env\r\n\r\n# SubprocVecEnv creates a vector of n environments to run them simultaneously.\r\nfrom baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\r\nfrom baselines.common.vec_env.dummy_vec_env import DummyVecEnv\r\n\r\ndef main():\r\n    config = tf.ConfigProto()\r\n\r\n    # Avoid warning message errors\r\n    os.environ[""CUDA_VISIBLE_DEVICES""]=""0""\r\n\r\n    # Allowing GPU memory growth\r\n    config.gpu_options.allow_growth = True\r\n\r\n    with tf.Session(config=config):\r\n        model.learn(policy=policies.PPOPolicy,\r\n                            env=SubprocVecEnv([env.make_train_0, \r\n                                                env.make_train_1, \r\n                                                env.make_train_2, \r\n                                                env.make_train_3, \r\n                                                env.make_train_4, \r\n                                                env.make_train_5,\r\n                                                env.make_train_6,\r\n                                                env.make_train_7,\r\n                                                env.make_train_8,\r\n                                                env.make_train_9,\r\n                                                env.make_train_10,\r\n                                                env.make_train_11,\r\n                                                env.make_train_12]), \r\n                            nsteps=2048, # Steps per environment\r\n                            total_timesteps=10000000,\r\n                            gamma=0.99,\r\n                            lam = 0.95,\r\n                            vf_coef=0.5,\r\n                            ent_coef=0.01,\r\n                            lr = lambda _: 2e-4,\r\n                            cliprange = lambda _: 0.1, # 0.1 * learning_rate\r\n                            max_grad_norm = 0.5, \r\n                            log_interval = 10\r\n                            )\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n\r\n\r\n'"
PPO with Sonic the Hedgehog/architecture.py,10,"b'import numpy as np\r\nimport tensorflow as tf\r\n\r\n# This function selects the probability distribution over actions\r\nfrom baselines.common.distributions import make_pdtype\r\n\r\n# Convolution layer\r\ndef conv_layer(inputs, filters, kernel_size, strides, gain=1.0):\r\n    return tf.layers.conv2d(inputs=inputs,\r\n                            filters=filters,\r\n                            kernel_size=kernel_size,\r\n                            strides=(strides, strides),\r\n                            activation=tf.nn.relu,\r\n                            kernel_initializer=tf.orthogonal_initializer(gain=gain))\r\n\r\n\r\n# Fully connected layer\r\ndef fc_layer(inputs, units, activation_fn=tf.nn.relu, gain=1.0):\r\n    return tf.layers.dense(inputs=inputs,\r\n                           units=units,\r\n                           activation=activation_fn,\r\n                           kernel_initializer=tf.orthogonal_initializer(gain))\r\n\r\n\r\n""""""\r\nThis object creates the PPO Network architecture\r\n""""""\r\nclass PPOPolicy(object):\r\n    def __init__(self, sess, ob_space, action_space, nbatch, nsteps, reuse = False):\r\n        # This will use to initialize our kernels\r\n        gain = np.sqrt(2)\r\n\r\n        # Based on the action space, will select what probability distribution type\r\n        # we will use to distribute action in our stochastic policy (in our case DiagGaussianPdType\r\n        # aka Diagonal Gaussian, 3D normal distribution\r\n        self.pdtype = make_pdtype(action_space)\r\n\r\n        height, weight, channel = ob_space.shape\r\n        ob_shape = (height, weight, channel)\r\n\r\n        # Create the input placeholder\r\n        inputs_ = tf.placeholder(tf.float32, [None, *ob_shape], name=""input"")\r\n\r\n        # Normalize the images\r\n        scaled_images = tf.cast(inputs_, tf.float32) / 255.\r\n\r\n        """"""\r\n        Build the model\r\n        3 CNN for spatial dependencies\r\n        Temporal dependencies is handle by stacking frames\r\n        (Something funny nobody use LSTM in OpenAI Retro contest)\r\n        1 common FC\r\n        1 FC for policy\r\n        1 FC for value\r\n        """"""\r\n        with tf.variable_scope(""model"", reuse = reuse):\r\n            conv1 = conv_layer(scaled_images, 32, 8, 4, gain)\r\n            conv2 = conv_layer(conv1, 64, 4, 2, gain)\r\n            conv3 = conv_layer(conv2, 64, 3, 1, gain)\r\n            flatten1 = tf.layers.flatten(conv3)\r\n            fc_common = fc_layer(flatten1, 512, gain=gain)\r\n\r\n            # This build a fc connected layer that returns a probability distribution\r\n            # over actions (self.pd) and our pi logits (self.pi).\r\n            self.pd, self.pi = self.pdtype.pdfromlatent(fc_common, init_scale=0.01)\r\n\r\n            # Calculate the v(s)\r\n            vf = fc_layer(fc_common, 1, activation_fn=None)[:, 0]\r\n\r\n        self.initial_state = None\r\n\r\n        # Take an action in the action distribution (remember we are in a situation\r\n        # of stochastic policy so we don\'t always take the action with the highest probability\r\n        # for instance if we have 2 actions 0.7 and 0.3 we have 30% chance to take the second)\r\n        a0 = self.pd.sample()\r\n\r\n        # Calculate the neg log of our probability\r\n        neglogp0 = self.pd.neglogp(a0)\r\n\r\n        # Function use to take a step returns action to take and V(s)\r\n        def step(state_in, *_args, **_kwargs):\r\n\r\n            # return a0, vf, neglogp0\r\n            return sess.run([a0, vf, neglogp0], {inputs_: state_in})\r\n\r\n        # Function that calculates only the V(s)\r\n        def value(state_in, *_args, **_kwargs):\r\n            return sess.run(vf, {inputs_: state_in})\r\n\r\n        # Function that output only the action to take\r\n        def select_action(state_in, *_args, **_kwargs):\r\n            return sess.run(a0, {inputs_: state_in})\r\n\r\n        self.inputs_ = inputs_\r\n        self.vf = vf\r\n        self.step = step\r\n        self.value = value\r\n        self.select_action = select_action\r\n\r\n\r\n\r\n'"
PPO with Sonic the Hedgehog/model.py,23,"b'import os\r\nimport time\r\nimport numpy as np\r\nimport os.path as osp\r\nimport tensorflow as tf\r\nfrom baselines import logger\r\n\r\nfrom collections import deque\r\n\r\nimport cv2\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\nimport sonic_env\r\nimport architecture as policies\r\n\r\n# Calculate cross entropy\r\nfrom baselines.a2c.utils import cat_entropy\r\nfrom utilities import make_path, find_trainable_variables, discount_with_dones\r\n\r\nfrom baselines.common.vec_env.dummy_vec_env import DummyVecEnv\r\n\r\nfrom baselines.common import explained_variance\r\nfrom baselines.common.runners import AbstractEnvRunner\r\n\r\n\r\nclass Model(object):\r\n    """"""\r\n    We use this object to :\r\n    __init__:\r\n    - Creates the step_model\r\n    - Creates the train_model\r\n\r\n    train():\r\n    - Make the training part (feedforward and retropropagation of gradients)\r\n\r\n    save/load():\r\n    - Save load the model\r\n    """"""\r\n    def __init__(self,\r\n                 policy,\r\n                ob_space,\r\n                action_space,\r\n                nenvs,\r\n                nsteps,\r\n                ent_coef,\r\n                vf_coef,\r\n                max_grad_norm):\r\n\r\n        sess = tf.get_default_session()\r\n\r\n\r\n        # CREATE THE PLACEHOLDERS\r\n        actions_ = tf.placeholder(tf.int32, [None], name=""actions_"")\r\n        advantages_ = tf.placeholder(tf.float32, [None], name=""advantages_"")\r\n        rewards_ = tf.placeholder(tf.float32, [None], name=""rewards_"")\r\n        lr_ = tf.placeholder(tf.float32, name=""learning_rate_"")\r\n        # Keep track of old actor\r\n        oldneglopac_ = tf.placeholder(tf.float32, [None], name=""oldneglopac_"")\r\n        # Keep track of old critic \r\n        oldvpred_ = tf.placeholder(tf.float32, [None], name=""oldvpred_"")\r\n        # Cliprange\r\n        cliprange_ = tf.placeholder(tf.float32, [])\r\n\r\n\r\n        # CREATE OUR TWO MODELS\r\n        # Step_model that is used for sampling\r\n        step_model = policy(sess, ob_space, action_space, nenvs, 1, reuse=False)\r\n\r\n        # Test model for testing our agent\r\n        #test_model = policy(sess, ob_space, action_space, 1, 1, reuse=False)\r\n\r\n        # Train model for training\r\n        train_model = policy(sess, ob_space, action_space, nenvs*nsteps, nsteps, reuse=True)\r\n\r\n\r\n\r\n        # CALCULATE THE LOSS\r\n        # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss\r\n       \r\n        # Clip the value\r\n        # Get the value predicted\r\n        value_prediction = train_model.vf\r\n\r\n        # Clip the value = Oldvalue + clip(value - oldvalue, min = - cliprange, max = cliprange)\r\n        value_prediction_clipped = oldvpred_ + tf.clip_by_value(train_model.vf - oldvpred_,  - cliprange_, cliprange_)\r\n\r\n        # Unclipped value\r\n        value_loss_unclipped = tf.square(value_prediction - rewards_)\r\n\r\n        # Clipped value\r\n        value_loss_clipped = tf.square(value_prediction_clipped - rewards_)\r\n\r\n        # Value loss 0.5 * SUM [max(unclipped, clipped)\r\n        vf_loss = 0.5 * tf.reduce_mean(tf.maximum(value_loss_unclipped,value_loss_clipped ))\r\n\r\n\r\n        # Clip the policy\r\n        # Output -log(pi) (new -log(pi))\r\n        neglogpac = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=train_model.pi, labels=actions_)\r\n        \r\n        # Remember we want ratio (pi current policy / pi old policy)\r\n        # But neglopac returns us -log(policy)\r\n        # So we want to transform it into ratio\r\n        # e^(-log old - (-log new)) == e^(log new - log old) == e^(log(new / old)) \r\n        # = new/old (since exponential function cancels log)\r\n        # Wish we can use latex in comments\r\n        ratio = tf.exp(oldneglopac_ - neglogpac) # ratio = pi new / pi old\r\n\r\n        # Remember also that we\'re doing gradient ascent, aka we want to MAXIMIZE the objective function which is equivalent to say\r\n        # Loss = - J\r\n        # To make objective function negative we can put a negation on the multiplication (pi new / pi old) * - Advantages\r\n        pg_loss_unclipped = -advantages_ * ratio \r\n\r\n        # value, min [1 - e] , max [1 + e]\r\n        pg_loss_clipped = -advantages_ * tf.clip_by_value(ratio, 1.0 - cliprange_, 1.0 + cliprange_)\r\n\r\n        # Final PG loss\r\n        # Why maximum, because pg_loss_unclipped and pg_loss_clipped are negative, getting the min of positive elements = getting\r\n        # the max of negative elements\r\n        pg_loss = tf.reduce_mean(tf.maximum(pg_loss_unclipped, pg_loss_clipped))\r\n\r\n        # Calculate the entropy\r\n        # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.\r\n        entropy = tf.reduce_mean(train_model.pd.entropy())\r\n\r\n        # Total loss (Remember that L = - J because it\'s the same thing than max J\r\n        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef\r\n\r\n\r\n        # UPDATE THE PARAMETERS USING LOSS\r\n        # 1. Get the model parameters\r\n        params = find_trainable_variables(""model"")\r\n\r\n        # 2. Calculate the gradients\r\n        grads = tf.gradients(loss, params)\r\n        if max_grad_norm is not None:\r\n            # Clip the gradients (normalize)\r\n            grads, grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\r\n        grads = list(zip(grads, params))\r\n        # zip aggregate each gradient with parameters associated\r\n        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da\r\n\r\n        # 3. Build our trainer\r\n        trainer = tf.train.RMSPropOptimizer(learning_rate=lr_, epsilon=1e-5)\r\n\r\n        # 4. Backpropagation\r\n        _train = trainer.apply_gradients(grads)\r\n\r\n\r\n        # Train function\r\n        def train(states_in, actions, returns, values, neglogpacs, lr, cliprange):\r\n            \r\n            # Here we calculate advantage A(s,a) = R + yV(s\') - V(s)\r\n            # Returns = R + yV(s\')\r\n            advantages = returns - values\r\n\r\n            # Normalize the advantages (taken from aborghi implementation)\r\n            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\r\n\r\n            # We create the feed dictionary\r\n            td_map = {train_model.inputs_: states_in,\r\n                     actions_: actions,\r\n                     advantages_: advantages, # Use to calculate our policy loss\r\n                     rewards_: returns, # Use as a bootstrap for real value\r\n                     lr_: lr,\r\n                     cliprange_: cliprange,\r\n                     oldneglopac_: neglogpacs,\r\n                     oldvpred_: values}\r\n\r\n            policy_loss, value_loss, policy_entropy, _= sess.run([pg_loss, vf_loss, entropy, _train], td_map)\r\n            \r\n            return policy_loss, value_loss, policy_entropy\r\n\r\n\r\n        def save(save_path):\r\n            """"""\r\n            Save the model\r\n            """"""\r\n            saver = tf.train.Saver()\r\n            saver.save(sess, save_path)\r\n\r\n        def load(load_path):\r\n            """"""\r\n            Load the model\r\n            """"""\r\n            saver = tf.train.Saver()\r\n            print(\'Loading \' + load_path)\r\n            saver.restore(sess, load_path)\r\n\r\n        self.train = train\r\n        self.train_model = train_model\r\n        self.step_model = step_model\r\n        self.step = step_model.step\r\n        self.value = step_model.value\r\n        self.initial_state = step_model.initial_state\r\n        self.save = save\r\n        self.load = load\r\n        tf.global_variables_initializer().run(session=sess)\r\n\r\n\r\nclass Runner(AbstractEnvRunner):\r\n    """"""\r\n    We use this object to make a mini batch of experiences\r\n    __init__:\r\n    - Initialize the runner\r\n\r\n    run():\r\n    - Make a mini batch\r\n    """"""\r\n    def __init__(self, env, model, nsteps, total_timesteps, gamma, lam):\r\n        super().__init__(env = env, model = model, nsteps = nsteps)\r\n\r\n        # Discount rate\r\n        self.gamma = gamma\r\n\r\n        # Lambda used in GAE (General Advantage Estimation)\r\n        self.lam = lam\r\n\r\n        # Total timesteps taken\r\n        self.total_timesteps = total_timesteps\r\n\r\n    def run(self):\r\n        # Here, we init the lists that will contain the mb of experiences\r\n        mb_obs, mb_actions, mb_rewards, mb_values, mb_neglopacs, mb_dones = [],[],[],[],[],[]\r\n\r\n        # For n in range number of steps\r\n        for n in range(self.nsteps):\r\n            # Given observations, get action value and neglopacs\r\n            # We already have self.obs because AbstractEnvRunner run self.obs[:] = env.reset()\r\n            actions, values, neglopacs = self.model.step(self.obs, self.dones)\r\n\r\n            # Append the observations into the mb\r\n            mb_obs.append(np.copy(self.obs)) #obs len nenvs (1 step per env)\r\n\r\n            # Append the actions taken into the mb\r\n            mb_actions.append(actions)\r\n\r\n            # Append the values calculated into the mb\r\n            mb_values.append(values)\r\n\r\n            # Append the negative log probability into the mb\r\n            mb_neglopacs.append(neglopacs)\r\n\r\n            # Append the dones situations into the mb\r\n            mb_dones.append(self.dones)\r\n\r\n            # Take actions in env and look the results\r\n            # Infos contains a ton of useful informations\r\n            # {\'level_end_bonus\': 0, \'rings\': 0, \'score\': 0, \'zone\': 1, \'act\': 0, \'screen_x_end\': 6591, \'screen_y\': 12, \'lives\': 3, \'x\': 96, \'y\': 108, \'screen_x\': 0}\r\n            self.obs[:], rewards, self.dones, infos = self.env.step(actions)\r\n\r\n            mb_rewards.append(rewards)\r\n\r\n        #batch of steps to batch of rollouts\r\n        mb_obs = np.asarray(mb_obs, dtype=np.uint8)\r\n        mb_rewards = np.asarray(mb_rewards, dtype=np.float32)\r\n        mb_actions = np.asarray(mb_actions, dtype=np.int32)\r\n        mb_values = np.asarray(mb_values, dtype=np.float32)\r\n        mb_neglopacs = np.asarray(mb_neglopacs, dtype=np.float32)\r\n        mb_dones = np.asarray(mb_dones, dtype=np.bool)\r\n        last_values = self.model.value(self.obs)\r\n          \r\n\r\n        ### GENERALIZED ADVANTAGE ESTIMATION\r\n        # discount/bootstrap off value fn\r\n        # We create mb_returns and mb_advantages\r\n        # mb_returns will contain Advantage + value\r\n        mb_returns = np.zeros_like(mb_rewards)\r\n        mb_advantages = np.zeros_like(mb_rewards)\r\n\r\n        lastgaelam = 0\r\n\r\n        # From last step to first step\r\n        for t in reversed(range(self.nsteps)):\r\n            # If t == before last step\r\n            if t == self.nsteps - 1:\r\n                # If a state is done, nextnonterminal = 0\r\n                # In fact nextnonterminal allows us to do that logic\r\n\r\n                #if done (so nextnonterminal = 0):\r\n                #    delta = R - V(s) (because self.gamma * nextvalues * nextnonterminal = 0) \r\n                # else (not done)\r\n                    #delta = R + gamma * V(st+1)\r\n                nextnonterminal = 1.0 - self.dones\r\n                \r\n                # V(t+1)\r\n                nextvalues = last_values\r\n            else:\r\n                nextnonterminal = 1.0 - mb_dones[t+1]\r\n                \r\n                nextvalues = mb_values[t+1]\r\n\r\n            # Delta = R(st) + gamma * V(t+1) * nextnonterminal  - V(st)\r\n            delta = mb_rewards[t] + self.gamma * nextvalues * nextnonterminal - mb_values[t]\r\n\r\n            # Advantage = delta + gamma *  \xce\xbb (lambda) * nextnonterminal  * lastgaelam\r\n            mb_advantages[t] = lastgaelam = delta + self.gamma * self.lam * nextnonterminal * lastgaelam\r\n\r\n        # Returns\r\n        mb_returns = mb_advantages + mb_values\r\n\r\n        return map(sf01, (mb_obs, mb_actions, mb_returns, mb_values, mb_neglopacs))\r\n\r\n\r\ndef sf01(arr):\r\n    """"""\r\n    swap and then flatten axes 0 and 1\r\n    """"""\r\n    s = arr.shape\r\n    return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])\r\n\r\n\r\ndef constfn(val):\r\n    def f(_):\r\n        return val\r\n    return f\r\n\r\n\r\ndef learn(policy,\r\n            env,\r\n            nsteps,\r\n            total_timesteps,\r\n            gamma,\r\n            lam,\r\n            vf_coef,\r\n            ent_coef,\r\n            lr,\r\n            cliprange,\r\n            max_grad_norm,\r\n            log_interval):\r\n\r\n    noptepochs = 4\r\n    nminibatches = 8\r\n\r\n    if isinstance(lr, float): lr = constfn(lr)\r\n    else: assert callable(lr)\r\n    if isinstance(cliprange, float): cliprange = constfn(cliprange)\r\n    else: assert callable(cliprange)\r\n\r\n    # Get the nb of env\r\n    nenvs = env.num_envs\r\n\r\n    # Get state_space and action_space\r\n    ob_space = env.observation_space\r\n    ac_space = env.action_space\r\n\r\n    # Calculate the batch_size\r\n    batch_size = nenvs * nsteps # For instance if we take 5 steps and we have 5 environments batch_size = 25\r\n\r\n    batch_train_size = batch_size // nminibatches\r\n\r\n    assert batch_size % nminibatches == 0\r\n\r\n    # Instantiate the model object (that creates step_model and train_model)\r\n    model = Model(policy=policy,\r\n                ob_space=ob_space,\r\n                action_space=ac_space,\r\n                nenvs=nenvs,\r\n                nsteps=nsteps,\r\n                ent_coef=ent_coef,\r\n                vf_coef=vf_coef,\r\n                max_grad_norm=max_grad_norm)\r\n\r\n    # Load the model\r\n    # If you want to continue training\r\n    # load_path = ""./models/40/model.ckpt""\r\n    # model.load(load_path)\r\n\r\n    # Instantiate the runner object\r\n    runner = Runner(env, model, nsteps=nsteps, total_timesteps=total_timesteps, gamma=gamma, lam=lam)\r\n\r\n    # Start total timer\r\n    tfirststart = time.time()\r\n\r\n    nupdates = total_timesteps//batch_size+1\r\n\r\n    for update in range(1, nupdates+1):\r\n        # Start timer\r\n        tstart = time.time()\r\n\r\n        frac = 1.0 - (update - 1.0) / nupdates\r\n\r\n        # Calculate the learning rate\r\n        lrnow = lr(frac)\r\n\r\n        # Calculate the cliprange\r\n        cliprangenow = cliprange(frac)\r\n\r\n        # Get minibatch\r\n        obs, actions, returns, values, neglogpacs = runner.run()\r\n\r\n        # Here what we\'re going to do is for each minibatch calculate the loss and append it.\r\n        mb_losses = []\r\n        total_batches_train = 0\r\n\r\n        # Index of each element of batch_size\r\n        # Create the indices array\r\n        indices = np.arange(batch_size)\r\n\r\n        for _ in range(noptepochs):\r\n            # Randomize the indexes\r\n            np.random.shuffle(indices)\r\n\r\n            # 0 to batch_size with batch_train_size step\r\n            for start in range(0, batch_size, batch_train_size):\r\n                end = start + batch_train_size\r\n                mbinds = indices[start:end]\r\n                slices = (arr[mbinds] for arr in (obs, actions, returns, values, neglogpacs))\r\n                mb_losses.append(model.train(*slices, lrnow, cliprangenow))\r\n            \r\n\r\n        # Feedforward --> get losses --> update\r\n        lossvalues = np.mean(mb_losses, axis=0)\r\n\r\n        # End timer\r\n        tnow = time.time()\r\n\r\n        # Calculate the fps (frame per second)\r\n        fps = int(batch_size / (tnow - tstart))\r\n\r\n        if update % log_interval == 0 or update == 1:\r\n            """"""\r\n            Computes fraction of variance that ypred explains about y.\r\n            Returns 1 - Var[y-ypred] / Var[y]\r\n            interpretation:\r\n            ev=0  =>  might as well have predicted zero\r\n            ev=1  =>  perfect prediction\r\n            ev<0  =>  worse than just predicting zero\r\n            """"""\r\n            ev = explained_variance(values, returns)\r\n            logger.record_tabular(""serial_timesteps"", update*nsteps)\r\n            logger.record_tabular(""nupdates"", update)\r\n            logger.record_tabular(""total_timesteps"", update*batch_size)\r\n            logger.record_tabular(""fps"", fps)\r\n            logger.record_tabular(""policy_loss"", float(lossvalues[0]))\r\n            logger.record_tabular(""policy_entropy"", float(lossvalues[2]))\r\n            logger.record_tabular(""value_loss"", float(lossvalues[1]))\r\n            logger.record_tabular(""explained_variance"", float(ev))\r\n            logger.record_tabular(""time elapsed"", float(tnow - tfirststart))\r\n            \r\n            savepath = ""./models/"" + str(update) + ""/model.ckpt""\r\n            model.save(savepath)\r\n            print(\'Saving to\', savepath)\r\n\r\n            # Test our agent with 3 trials and mean the score\r\n            # This will be useful to see if our agent is improving\r\n            test_score = testing(model)\r\n\r\n            logger.record_tabular(""Mean score test level"", test_score)\r\n            logger.dump_tabular()\r\n            \r\n    env.close()\r\n\r\n# Avoid error when calculate the mean (in our case if epinfo is empty returns np.nan, not return an error)\r\ndef safemean(xs):\r\n    return np.nan if len(xs) == 0 else np.mean(xs)\r\n\r\n\r\ndef testing(model):\r\n    """"""\r\n    We\'ll use this function to calculate the score on test levels for each saved model,\r\n    to generate the video version\r\n    to generate the map version\r\n    """"""\r\n\r\n    test_env = DummyVecEnv([sonic_env.make_test])\r\n\r\n    # Get state_space and action_space\r\n    ob_space = test_env.observation_space\r\n    ac_space = test_env.action_space\r\n \r\n    # Play\r\n    total_score = 0\r\n    trial = 0\r\n    \r\n    # We make 3 trials\r\n    for trial in range(3):\r\n        obs = test_env.reset()\r\n        done = False\r\n        score = 0\r\n\r\n        while done == False:\r\n            # Get the action\r\n            action, value, _ = model.step(obs)\r\n            \r\n            # Take action in env and look the results\r\n            obs, reward, done, info = test_env.step(action)\r\n\r\n            score += reward[0]\r\n        total_score += score\r\n        trial += 1\r\n    test_env.close()\r\n\r\n    # Divide the score by the number of trials\r\n    total_test_score = total_score / 3\r\n    return total_test_score\r\n    \r\n\r\ndef generate_output(policy, test_env):\r\n    """"""\r\n    We\'ll use this function to calculate the score on test levels for each saved model,\r\n    to generate the video version\r\n    to generate the map version\r\n    """"""\r\n\r\n    # Get state_space and action_space\r\n    ob_space = test_env.observation_space\r\n    ac_space = test_env.action_space\r\n\r\n    test_score = []\r\n\r\n    # Instantiate the model object (that creates step_model and train_model)\r\n    models_indexes = [1, 10, 20, 30, 40]\r\n\r\n    # Instantiate the model object (that creates step_model and train_model)\r\n    validation_model = Model(policy=policy,\r\n                ob_space=ob_space,\r\n                action_space=ac_space,\r\n                nenvs=1,\r\n                nsteps=1,\r\n                ent_coef=0,\r\n                vf_coef=0,\r\n                max_grad_norm=0)\r\n\r\n    for model_index in models_indexes:\r\n        # Load the model\r\n        load_path = ""./models/""+ str(model_index) + ""/model.ckpt""\r\n        validation_model.load(load_path)\r\n\r\n        # Play\r\n        score = 0\r\n        timesteps = 0\r\n\r\n        # Play during 5000 timesteps\r\n        while timesteps < 5000:\r\n            timesteps +=1\r\n            \r\n            # Get the actions\r\n            actions, values, _ = validation_model.step(obs)\r\n            \r\n            # Take actions in envs and look the results\r\n            obs, rewards, dones, infos = test_env.step(actions)\r\n            \r\n            score += rewards\r\n       \r\n        # Divide the score by the number of testing environment\r\n        total_score = score / test_env.num_envs\r\n\r\n        test_score.append(total_score)\r\n    \r\n    env.close()\r\n\r\n    return test_score\r\n\r\n\r\ndef play(policy, env, update):\r\n\r\n    # Get state_space and action_space\r\n    ob_space = env.observation_space\r\n    ac_space = env.action_space\r\n\r\n    # Instantiate the model object (that creates step_model and train_model)\r\n    model = Model(policy=policy,\r\n                ob_space=ob_space,\r\n                action_space=ac_space,\r\n                nenvs=1,\r\n                nsteps=1,\r\n                ent_coef=0,\r\n                vf_coef=0,\r\n                max_grad_norm=0)\r\n    \r\n    # Load the model\r\n    load_path = ""./models/""+ str(update) + ""/model.ckpt""\r\n    print(load_path)\r\n\r\n    obs = env.reset()\r\n\r\n    # Play\r\n    score = 0\r\n    done = False\r\n\r\n    while done == False:\r\n        # Get the action\r\n        actions, values, _ = model.step(obs)\r\n        \r\n        # Take actions in env and look the results\r\n        obs, rewards, done, info = env.step(actions)\r\n        \r\n        score += rewards\r\n    \r\n        env.render()\r\n        \r\n    print(""Score "", score)\r\n    env.close()\r\n    \r\n\r\n\r\n\r\n'"
PPO with Sonic the Hedgehog/play.py,2,"b'import tensorflow as tf\r\nimport numpy as np\r\nimport gym\r\nimport math\r\nimport os\r\n\r\nimport model\r\nimport architecture as policies\r\nimport sonic_env as env\r\n\r\n# SubprocVecEnv creates a vector of n environments to run them simultaneously.\r\nfrom baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\r\nfrom baselines.common.vec_env.dummy_vec_env import DummyVecEnv\r\n\r\ndef main():\r\n    config = tf.ConfigProto()\r\n\r\n    # Avoid warning message errors\r\n    os.environ[""CUDA_VISIBLE_DEVICES""]=""0""\r\n\r\n    # Allowing GPU memory growth\r\n    config.gpu_options.allow_growth = True\r\n\r\n\r\n    with tf.Session(config=config):\r\n\r\n    \t# 0: {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'SpringYardZone.Act1\'},\r\n        # 1: {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'MarbleZone.Act1\'},\r\n        # 2: {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'GreenHillZone.Act2\'},\r\n        # 3: {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'StarLightZone.Act3\'},\r\n        # 4: {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'ScrapBrainZone.Act1\'},\r\n        # 5: {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'ScrapBrainZone.Act2\'},\r\n        # 6: {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'MetropolisZone.Act3\'},\r\n        # 7: {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'HillTopZone.Act2\'},\r\n        # 8: {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'CasinoNightZone.Act2\'},\r\n        # 9: {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'LavaReefZone.Act1\'},\r\n        # 10: {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'FlyingBatteryZone.Act2\'},\r\n        # 11: {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'HydrocityZone.Act1\'},\r\n        # 12: {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'AngelIslandZone.Act2\'}\r\n\r\n        model.play(policy=policies.PPOPolicy, \r\n            env= DummyVecEnv([env.make_train_1]),\r\n            update = 120)\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
PPO with Sonic the Hedgehog/sonic_env.py,0,"b'# Part taken from adborghi fantastic implementation\r\n# https://github.com/aborghi/retro_contest_agent/blob/master/fastlearner/ppo2ttifrutti_sonic_env.py\r\nimport numpy as np\r\nimport gym\r\n\r\n#import gym_remote.client as grc\r\n\r\nfrom retro_contest.local import make\r\nfrom retro import make as make_retro\r\n\r\n# This will be useful for stacking frames\r\nfrom baselines.common.atari_wrappers import FrameStack\r\n\r\n# Library used to modify frames (former times we used matplotlib)\r\nimport cv2\r\n\r\n# setUseOpenCL = False means that we will not use GPU (disable OpenCL acceleration)\r\ncv2.ocl.setUseOpenCL(False)\r\n\r\nclass PreprocessFrame(gym.ObservationWrapper):\r\n    """"""\r\n    Here we do the preprocessing part:\r\n    - Set frame to gray\r\n    - Resize the frame to 96x96x1\r\n    """"""\r\n    def __init__(self, env):\r\n        gym.ObservationWrapper.__init__(self, env)\r\n        self.width = 96\r\n        self.height = 96\r\n        self.observation_space = gym.spaces.Box(low=0, high=255,\r\n            shape=(self.height, self.width, 1), dtype=np.uint8)\r\n\r\n    def observation(self, frame):\r\n        # Set frame to gray\r\n        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\r\n\r\n        # Resize the frame to 96x96x1\r\n        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\r\n        frame = frame[:, :, None]\r\n\r\n        return frame\r\n\r\n\r\nclass ActionsDiscretizer(gym.ActionWrapper):\r\n    """"""\r\n    Wrap a gym-retro environment and make it use discrete\r\n    actions for the Sonic game.\r\n    """"""\r\n    def __init__(self, env):\r\n        super(ActionsDiscretizer, self).__init__(env)\r\n        buttons = [""B"", ""A"", ""MODE"", ""START"", ""UP"", ""DOWN"", ""LEFT"", ""RIGHT"", ""C"", ""Y"", ""X"", ""Z""]\r\n        actions = [[\'LEFT\'], [\'RIGHT\'], [\'LEFT\', \'DOWN\'], [\'RIGHT\', \'DOWN\'], [\'DOWN\'],\r\n                   [\'DOWN\', \'B\'], [\'B\']]\r\n        self._actions = []\r\n\r\n        """"""\r\n        What we do in this loop:\r\n        For each action in actions\r\n            - Create an array of 12 False (12 = nb of buttons)\r\n            For each button in action: (for instance [\'LEFT\']) we need to make that left button index = True\r\n                - Then the button index = LEFT = True\r\n\r\n            In fact at the end we will have an array where each array is an action and each elements True of this array\r\n            are the buttons clicked.\r\n        """"""\r\n        for action in actions:\r\n            arr = np.array([False] * 12)\r\n            for button in action:\r\n                arr[buttons.index(button)] = True\r\n            self._actions.append(arr)\r\n        self.action_space = gym.spaces.Discrete(len(self._actions))\r\n\r\n    def action(self, a): # pylint: disable=W0221\r\n        return self._actions[a].copy()\r\n\r\n\r\nclass RewardScaler(gym.RewardWrapper):\r\n    """"""\r\n    Bring rewards to a reasonable scale for PPO.\r\n    This is incredibly important and effects performance\r\n    drastically.\r\n    """"""\r\n    def reward(self, reward):\r\n\r\n        return reward * 0.01\r\n\r\nclass AllowBacktracking(gym.Wrapper):\r\n    """"""\r\n    Use deltas in max(X) as the reward, rather than deltas\r\n    in X. This way, agents are not discouraged too heavily\r\n    from exploring backwards if there is no way to advance\r\n    head-on in the level.\r\n    """"""\r\n    def __init__(self, env):\r\n        super(AllowBacktracking, self).__init__(env)\r\n        self._cur_x = 0\r\n        self._max_x = 0\r\n\r\n    def reset(self, **kwargs): # pylint: disable=E0202\r\n        self._cur_x = 0\r\n        self._max_x = 0\r\n        return self.env.reset(**kwargs)\r\n\r\n    def step(self, action): # pylint: disable=E0202\r\n        obs, rew, done, info = self.env.step(action)\r\n        self._cur_x += rew\r\n        rew = max(0, self._cur_x - self._max_x)\r\n        self._max_x = max(self._max_x, self._cur_x)\r\n        return obs, rew, done, info\r\n\r\ndef make_env(env_idx):\r\n    """"""\r\n    Create an environment with some standard wrappers.\r\n    """"""\r\n\r\n    dicts = [\r\n        {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'EmeraldHillZone.Act1\'},\r\n        {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'ChemicalPlantZone.Act2\'},\r\n        {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'ChemicalPlantZone.Act1\'},\r\n        {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'MetropolisZone.Act1\'},\r\n        {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'MetropolisZone.Act2\'},\r\n        {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'OilOceanZone.Act1\'},\r\n        {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'OilOceanZone.Act2\'},\r\n        {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'LavaReefZone.Act2\'},\r\n        {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'CarnivalNightZone.Act2\'},\r\n        {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'CarnivalNightZone.Act1\'},\r\n        {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'MushroomHillZone.Act2\'},\r\n        {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'MushroomHillZone.Act1\'},\r\n        {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'AngelIslandZone.Act1\'}\r\n    ]\r\n    # Make the environment\r\n    print(dicts[env_idx][\'game\'], dicts[env_idx][\'state\'], flush=True)\r\n    #record_path = ""./records/"" + dicts[env_idx][\'state\']\r\n    env = make(game=dicts[env_idx][\'game\'], state=dicts[env_idx][\'state\'])#, bk2dir=""./records"")#record=\'/tmp\')\r\n\r\n    # Build the actions array, \r\n    env = ActionsDiscretizer(env)\r\n\r\n    # Scale the rewards\r\n    env = RewardScaler(env)\r\n\r\n    # PreprocessFrame\r\n    env = PreprocessFrame(env)\r\n\r\n    # Stack 4 frames\r\n    env = FrameStack(env, 4)\r\n\r\n    # Allow back tracking that helps agents are not discouraged too heavily\r\n    # from exploring backwards if there is no way to advance\r\n    # head-on in the level.\r\n    env = AllowBacktracking(env)\r\n\r\n    return env\r\n\r\n\r\ndef make_test():\r\n    """"""\r\n    Create an environment with some standard wrappers.\r\n    \r\n    dicts = [\r\n        {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'EmeraldHillZone.Act1\'},\r\n        {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'ChemicalPlantZone.Act2\'},\r\n        {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'ChemicalPlantZone.Act1\'},\r\n        {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'MetropolisZone.Act1\'},\r\n        {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'MetropolisZone.Act2\'},\r\n        {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'OilOceanZone.Act1\'},\r\n        {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'OilOceanZone.Act2\'},\r\n        {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'LavaReefZone.Act2\'},\r\n        {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'CarnivalNightZone.Act2\'},\r\n        {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'CarnivalNightZone.Act1\'},\r\n        {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'MushroomHillZone.Act2\'},\r\n        {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'MushroomHillZone.Act1\'},\r\n        {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'AngelIslandZone.Act1\'}\r\n    ]\r\n    """"""\r\n    # Here we add record because we want to output a video\r\n    env = make(game=""SonicAndKnuckles3-Genesis"", state=""AngelIslandZone.Act1"")\r\n\r\n    # Build the actions array, \r\n    env = ActionsDiscretizer(env)\r\n\r\n    # Scale the rewards\r\n    env = RewardScaler(env)\r\n\r\n    # PreprocessFrame\r\n    env = PreprocessFrame(env)\r\n\r\n    # Stack 4 frames\r\n    env = FrameStack(env, 4)\r\n\r\n    # Allow back tracking that helps agents are not discouraged too heavily\r\n    # from exploring backwards if there is no way to advance\r\n    # head-on in the level.\r\n    env = AllowBacktracking(env)\r\n\r\n    return env\r\n\r\n\r\ndef make_train_0():\r\n    return make_env(0)\r\n\r\ndef make_train_1():\r\n    return make_env(1)\r\n\r\ndef make_train_2():\r\n    return make_env(2)\r\n\r\ndef make_train_3():\r\n    return make_env(3)\r\n\r\ndef make_train_4():\r\n    return make_env(4)\r\n\r\ndef make_train_5():\r\n    return make_env(5)\r\n\r\ndef make_train_6():\r\n    return make_env(6)\r\n\r\ndef make_train_7():\r\n    return make_env(7)\r\n\r\ndef make_train_8():\r\n    return make_env(8)\r\n\r\ndef make_train_9():\r\n    return make_env(9)\r\n\r\ndef make_train_10():\r\n    return make_env(10)\r\n\r\ndef make_train_11():\r\n    return make_env(11)\r\n\r\ndef make_train_12():\r\n    return make_env(12)\r\n\r\n'"
PPO with Sonic the Hedgehog/test.py,2,"b'import tensorflow as tf\r\nimport numpy as np\r\nimport gym\r\nimport math\r\nimport os\r\n\r\nimport model\r\nimport architecture as policies\r\nimport sonic_env as env\r\n\r\n# SubprocVecEnv creates a vector of n environments to run them simultaneously.\r\nfrom baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\r\nfrom baselines.common.vec_env.dummy_vec_env import DummyVecEnv\r\n\r\ndef main():\r\n    config = tf.ConfigProto()\r\n\r\n    # Avoid warning message errors\r\n    os.environ[""CUDA_VISIBLE_DEVICES""]=""0""\r\n\r\n    # Allowing GPU memory growth\r\n    config.gpu_options.allow_growth = True\r\n\r\n   \r\n    with tf.Session(config=config):\r\n\r\n    \t# 0: {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'SpringYardZone.Act1\'},\r\n        # 1: {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'MarbleZone.Act1\'},\r\n        # 2: {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'GreenHillZone.Act2\'},\r\n        # 3: {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'StarLightZone.Act3\'},\r\n        # 4: {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'ScrapBrainZone.Act1\'},\r\n        # 5: {\'game\': \'SonicTheHedgehog-Genesis\', \'state\': \'ScrapBrainZone.Act2\'},\r\n        # 6: {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'MetropolisZone.Act3\'},\r\n        # 7: {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'HillTopZone.Act2\'},\r\n        # 8: {\'game\': \'SonicTheHedgehog2-Genesis\', \'state\': \'CasinoNightZone.Act2\'},\r\n        # 9: {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'LavaReefZone.Act1\'},\r\n        # 10: {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'FlyingBatteryZone.Act2\'},\r\n        # 11: {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'HydrocityZone.Act1\'},\r\n        # 12: {\'game\': \'SonicAndKnuckles3-Genesis\', \'state\': \'AngelIslandZone.Act2\'}\r\n        \r\n        update = 40\r\n        \r\n        model.testing(policy=policies.PPOPolicy, \r\n            test_env= DummyVecEnv([env.make_test]),\r\n            update = update)\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
PPO with Sonic the Hedgehog/utilities.py,2,"b'import tensorflow as tf\r\n# Get the variables\r\ndef find_trainable_variables(key):\r\n    with tf.variable_scope(key):\r\n        return tf.trainable_variables()\r\n\r\n\r\n# Make directory\r\ndef make_path(f):\r\n    # exist_ok: if the folder already exist makes no exception error\r\n    return os.makedirs(f, exist_ok=True)\r\n\r\n\r\ndef discount_with_dones(rewards, dones, gamma):\r\n    discounted = []\r\n    r = 0\r\n    for reward, done in zip(rewards[::-1], dones[::-1]):\r\n        r = reward + gamma*r*(1.-done) # fixed off by one bug\r\n        discounted.append(r)\r\n    return discounted[::-1]\r\n'"
RND Montezuma's revenge PyTorch/agents.py,0,"b'import numpy as np\r\n\r\nimport torch.nn.functional as F\r\nimport torch.nn as nn\r\nimport torch\r\nimport torch.optim as optim\r\n\r\nfrom torch.distributions.categorical import Categorical\r\n\r\nfrom model import PPOModel, RNDModel\r\nfrom utils import global_grad_norm_\r\n\r\n\r\nclass RNDAgent(object):\r\n    def __init__(\r\n            self,\r\n            input_size,\r\n            output_size,\r\n            num_env,\r\n            num_step,\r\n            gamma,\r\n            lam=0.95,\r\n            learning_rate=1e-4,\r\n            ent_coef=0.01,\r\n            clip_grad_norm=0.5,\r\n            epoch=3,\r\n            batch_size=128,\r\n            ppo_eps=0.1,\r\n            update_proportion=0.25,\r\n            use_gae=True,\r\n            use_cuda=False):\r\n\r\n        # Build the PPO Model\r\n        self.model = PPOModel(input_size, output_size)\r\n\r\n        self.num_env = num_env\r\n        self.output_size = output_size\r\n        self.input_size = input_size\r\n        self.num_step = num_step\r\n        self.gamma = gamma\r\n        self.lam = lam\r\n        self.epoch = epoch\r\n        self.batch_size = batch_size\r\n        self.use_gae = use_gae\r\n        self.ent_coef = ent_coef\r\n        self.ppo_eps = ppo_eps\r\n        self.clip_grad_norm = clip_grad_norm\r\n        self.update_proportion = update_proportion\r\n\r\n        self.device = torch.device(\'cuda\' if use_cuda else \'cpu\')\r\n        print(""DEVICE: "", self.device)\r\n\r\n        # Build the RND model\r\n        self.rnd = RNDModel(input_size, output_size)\r\n\r\n        # Define the optimizer (Adam)\r\n        self.optimizer = optim.Adam(list(self.model.parameters()) + list(self.rnd.predictor.parameters()),\r\n                                    lr=learning_rate)\r\n\r\n        # CPU/GPU\r\n        self.rnd = self.rnd.to(self.device)\r\n\r\n        self.model = self.model.to(self.device)\r\n\r\n    def get_action(self, state):\r\n        # Transform our state into a float32 tensor\r\n        state = torch.Tensor(state).to(self.device)\r\n        state = state.float()\r\n\r\n        # Get the action dist, ext_value, int_value\r\n        policy, value_ext, value_int = self.model(state)\r\n\r\n        # Get action probability distribution\r\n        action_prob = F.softmax(policy, dim=-1).data.cpu().numpy()\r\n\r\n        # Select action\r\n        action = self.random_choice_prob_index(action_prob)\r\n\r\n        return action, value_ext.data.cpu().numpy().squeeze(), value_int.data.cpu().numpy().squeeze(), policy.detach()\r\n\r\n\r\n    @staticmethod\r\n    def random_choice_prob_index(p, axis=1):\r\n        r = np.expand_dims(np.random.rand(p.shape[1 - axis]), axis=axis)\r\n        return (p.cumsum(axis=axis) > r).argmax(axis=axis)\r\n\r\n    # Calculate Intrinsic reward (prediction error)\r\n    def compute_intrinsic_reward(self, next_obs):\r\n        next_obs = torch.FloatTensor(next_obs).to(self.device)\r\n\r\n        # Get target feature\r\n        target_next_feature = self.rnd.target(next_obs)\r\n\r\n        # Get prediction feature\r\n        predict_next_feature = self.rnd.predictor(next_obs)\r\n\r\n        # Calculate intrinsic reward\r\n        intrinsic_reward = (target_next_feature - predict_next_feature).pow(2).sum(1) / 2\r\n\r\n        return intrinsic_reward.data.cpu().numpy()\r\n\r\n\r\n    def train_model(self, s_batch, target_ext_batch, target_int_batch, y_batch, adv_batch, next_obs_batch, old_policy):\r\n        s_batch = torch.FloatTensor(s_batch).to(self.device)\r\n        target_ext_batch = torch.FloatTensor(target_ext_batch).to(self.device)\r\n        target_int_batch = torch.FloatTensor(target_int_batch).to(self.device)\r\n        y_batch = torch.LongTensor(y_batch).to(self.device)\r\n        adv_batch = torch.FloatTensor(adv_batch).to(self.device)\r\n        next_obs_batch = torch.FloatTensor(next_obs_batch).to(self.device)\r\n\r\n        sample_range = np.arange(len(s_batch))\r\n        forward_mse = nn.MSELoss(reduction=\'none\')\r\n\r\n        # Get old policy\r\n        with torch.no_grad():\r\n            policy_old_list = torch.stack(old_policy).permute(1, 0, 2).contiguous().view(-1, self.output_size).to(\r\n                self.device)\r\n\r\n            m_old = Categorical(F.softmax(policy_old_list, dim=-1))\r\n            log_prob_old = m_old.log_prob(y_batch)\r\n            # ------------------------------------------------------------\r\n\r\n        for i in range(self.epoch):\r\n            # Here we\'ll do minibatches of training\r\n            np.random.shuffle(sample_range)\r\n            for j in range(int(len(s_batch) / self.batch_size)):\r\n                sample_idx = sample_range[self.batch_size * j:self.batch_size * (j + 1)]\r\n\r\n                # --------------------------------------------------------------------------------\r\n                # for Curiosity-driven(Random Network Distillation)\r\n                predict_next_state_feature, target_next_state_feature = self.rnd(next_obs_batch[sample_idx])\r\n\r\n                forward_loss = forward_mse(predict_next_state_feature, target_next_state_feature.detach()).mean(-1)\r\n                # Proportion of exp used for predictor update\r\n                mask = torch.rand(len(forward_loss)).to(self.device)\r\n                mask = (mask < self.update_proportion).type(torch.FloatTensor).to(self.device)\r\n                forward_loss = (forward_loss * mask).sum() / torch.max(mask.sum(), torch.Tensor([1]).to(self.device))\r\n                # ---------------------------------------------------------------------------------\r\n\r\n                policy, value_ext, value_int = self.model(s_batch[sample_idx])\r\n                m = Categorical(F.softmax(policy, dim=-1))\r\n                log_prob = m.log_prob(y_batch[sample_idx])\r\n\r\n                ratio = torch.exp(log_prob - log_prob_old[sample_idx])\r\n\r\n                surr1 = ratio * adv_batch[sample_idx]\r\n                surr2 = torch.clamp(\r\n                    ratio,\r\n                    1.0 - self.ppo_eps,\r\n                    1.0 + self.ppo_eps) * adv_batch[sample_idx]\r\n\r\n                # Calculate actor loss\r\n                # - J is equivalent to max J hence -torch\r\n                actor_loss = -torch.min(surr1, surr2).mean()\r\n\r\n                # Calculate critic loss\r\n                critic_ext_loss = F.mse_loss(value_ext.sum(1), target_ext_batch[sample_idx])\r\n                critic_int_loss = F.mse_loss(value_int.sum(1), target_int_batch[sample_idx])\r\n                \r\n                # Critic loss = critic E loss + critic I loss\r\n                critic_loss = critic_ext_loss + critic_int_loss\r\n\r\n                # Calculate the entropy\r\n                # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.\r\n                entropy = m.entropy().mean()\r\n\r\n                # Reset the gradients\r\n                self.optimizer.zero_grad()\r\n                \r\n                # CALCULATE THE LOSS\r\n                # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss + forward_loss\r\n                loss = actor_loss + 0.5 * critic_loss - self.ent_coef * entropy + forward_loss\r\n                \r\n                # Backpropagation\r\n                loss.backward()\r\n                global_grad_norm_(list(self.model.parameters())+list(self.rnd.predictor.parameters()))\r\n                self.optimizer.step()\r\n'"
RND Montezuma's revenge PyTorch/config.py,0,"b""import configparser\r\n\r\nconfig = configparser.ConfigParser()\r\nconfig.read('./config.conf')\r\n\r\n# ---------------------------------\r\ndefault = 'DEFAULT'\r\n# ---------------------------------\r\ndefault_config = config[default]\r\n"""
RND Montezuma's revenge PyTorch/envs.py,0,"b'import gym\r\nimport cv2\r\n\r\nimport numpy as np\r\n\r\nfrom abc import abstractmethod\r\nfrom collections import deque\r\nfrom copy import copy\r\n\r\nimport gym_super_mario_bros\r\nfrom nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\r\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\r\n\r\nfrom torch.multiprocessing import Pipe, Process\r\n\r\nfrom model import *\r\nfrom config import *\r\nfrom PIL import Image\r\n\r\ntrain_method = default_config[\'TrainMethod\']\r\nmax_step_per_episode = int(default_config[\'MaxStepPerEpisode\'])\r\n\r\n\r\nclass Environment(Process):\r\n    @abstractmethod\r\n    def run(self):\r\n        pass\r\n\r\n    @abstractmethod\r\n    def reset(self):\r\n        pass\r\n\r\n    @abstractmethod\r\n    def pre_proc(self, x):\r\n        pass\r\n\r\n    @abstractmethod\r\n    def get_init_state(self, x):\r\n        pass\r\n\r\n\r\ndef unwrap(env):\r\n    if hasattr(env, ""unwrapped""):\r\n        return env.unwrapped\r\n    elif hasattr(env, ""env""):\r\n        return unwrap(env.env)\r\n    elif hasattr(env, ""leg_env""):\r\n        return unwrap(env.leg_env)\r\n    else:\r\n        return env\r\n\r\n\r\nclass MaxAndSkipEnv(gym.Wrapper):\r\n    def __init__(self, env, is_render, skip=4):\r\n        """"""Return only every `skip`-th frame""""""\r\n        gym.Wrapper.__init__(self, env)\r\n        # most recent raw observations (for max pooling across time steps)\r\n        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\r\n        self._skip = skip\r\n        self.is_render = is_render\r\n\r\n    def step(self, action):\r\n        """"""Repeat action, sum reward, and max over last observations.""""""\r\n        total_reward = 0.0\r\n        done = None\r\n        for i in range(self._skip):\r\n            obs, reward, done, info = self.env.step(action)\r\n            if self.is_render:\r\n                self.env.render()\r\n            if i == self._skip - 2:\r\n                self._obs_buffer[0] = obs\r\n            if i == self._skip - 1:\r\n                self._obs_buffer[1] = obs\r\n            total_reward += reward\r\n            if done:\r\n                break\r\n        # Note that the observation on the done=True frame\r\n        # doesn\'t matter\r\n        max_frame = self._obs_buffer.max(axis=0)\r\n\r\n        return max_frame, total_reward, done, info\r\n\r\n    def reset(self, **kwargs):\r\n        return self.env.reset(**kwargs)\r\n\r\n\r\nclass MontezumaInfoWrapper(gym.Wrapper):\r\n    def __init__(self, env, room_address):\r\n        super(MontezumaInfoWrapper, self).__init__(env)\r\n        self.room_address = room_address\r\n        self.visited_rooms = set()\r\n\r\n    def get_current_room(self):\r\n        ram = unwrap(self.env).ale.getRAM()\r\n        assert len(ram) == 128\r\n        return int(ram[self.room_address])\r\n\r\n    def step(self, action):\r\n        obs, rew, done, info = self.env.step(action)\r\n        self.visited_rooms.add(self.get_current_room())\r\n        if done:\r\n            if \'episode\' not in info:\r\n                info[\'episode\'] = {}\r\n            info[\'episode\'].update(visited_rooms=copy(self.visited_rooms))\r\n            self.visited_rooms.clear()\r\n        return obs, rew, done, info\r\n\r\n    def reset(self):\r\n        return self.env.reset()\r\n\r\n\r\nclass AtariEnvironment(Environment):\r\n    def __init__(\r\n            self,\r\n            env_id,\r\n            is_render,\r\n            env_idx,\r\n            child_conn,\r\n            history_size=4,\r\n            h=84,\r\n            w=84,\r\n            life_done=True,\r\n            sticky_action=True,\r\n            p=0.25):\r\n        super(AtariEnvironment, self).__init__()\r\n        self.daemon = True\r\n        self.env = MaxAndSkipEnv(gym.make(env_id), is_render)\r\n        if \'Montezuma\' in env_id:\r\n            self.env = MontezumaInfoWrapper(self.env, room_address=3 if \'Montezuma\' in env_id else 1)\r\n        self.env_id = env_id\r\n        self.is_render = is_render\r\n        self.env_idx = env_idx\r\n        self.steps = 0\r\n        self.episode = 0\r\n        self.rall = 0\r\n        self.recent_rlist = deque(maxlen=100)\r\n        self.child_conn = child_conn\r\n\r\n        self.sticky_action = sticky_action\r\n        self.last_action = 0\r\n        self.p = p\r\n\r\n        self.history_size = history_size\r\n        self.history = np.zeros([history_size, h, w])\r\n        self.h = h\r\n        self.w = w\r\n\r\n        self.reset()\r\n\r\n    def run(self):\r\n        super(AtariEnvironment, self).run()\r\n        while True:\r\n            action = self.child_conn.recv()\r\n\r\n            if \'Breakout\' in self.env_id:\r\n                action += 1\r\n\r\n            # sticky action\r\n            if self.sticky_action:\r\n                if np.random.rand() <= self.p:\r\n                    action = self.last_action\r\n                self.last_action = action\r\n\r\n            s, reward, done, info = self.env.step(action)\r\n\r\n            if max_step_per_episode < self.steps:\r\n                done = True\r\n\r\n            log_reward = reward\r\n            force_done = done\r\n\r\n            self.history[:3, :, :] = self.history[1:, :, :]\r\n            self.history[3, :, :] = self.pre_proc(s)\r\n\r\n            self.rall += reward\r\n            self.steps += 1\r\n\r\n            if done:\r\n                self.recent_rlist.append(self.rall)\r\n                print(""[Episode {}({})] Step: {}  Reward: {}  Recent Reward: {}  Visited Room: [{}]"".format(\r\n                    self.episode, self.env_idx, self.steps, self.rall, np.mean(self.recent_rlist),\r\n                    info.get(\'episode\', {}).get(\'visited_rooms\', {})))\r\n\r\n                self.history = self.reset()\r\n\r\n            self.child_conn.send(\r\n                [self.history[:, :, :], reward, force_done, done, log_reward])\r\n\r\n    def reset(self):\r\n        self.last_action = 0\r\n        self.steps = 0\r\n        self.episode += 1\r\n        self.rall = 0\r\n        s = self.env.reset()\r\n        self.get_init_state(\r\n            self.pre_proc(s))\r\n        return self.history[:, :, :]\r\n\r\n    def pre_proc(self, X):\r\n        X = np.array(Image.fromarray(X).convert(\'L\')).astype(\'float32\')\r\n        x = cv2.resize(X, (self.h, self.w))\r\n        return x\r\n\r\n    def get_init_state(self, s):\r\n        for i in range(self.history_size):\r\n            self.history[i, :, :] = self.pre_proc(s)\r\n\r\n\r\nclass MarioEnvironment(Process):\r\n    def __init__(\r\n            self,\r\n            env_id,\r\n            is_render,\r\n            env_idx,\r\n            child_conn,\r\n            history_size=4,\r\n            life_done=True,\r\n            h=84,\r\n            w=84, movement=COMPLEX_MOVEMENT, sticky_action=True,\r\n            p=0.25):\r\n        super(MarioEnvironment, self).__init__()\r\n        self.daemon = True\r\n        self.env = BinarySpaceToDiscreteSpaceEnv(\r\n            gym_super_mario_bros.make(env_id), COMPLEX_MOVEMENT)\r\n\r\n        self.is_render = is_render\r\n        self.env_idx = env_idx\r\n        self.steps = 0\r\n        self.episode = 0\r\n        self.rall = 0\r\n        self.recent_rlist = deque(maxlen=100)\r\n        self.child_conn = child_conn\r\n\r\n        self.life_done = life_done\r\n        self.sticky_action = sticky_action\r\n        self.last_action = 0\r\n        self.p = p\r\n\r\n        self.history_size = history_size\r\n        self.history = np.zeros([history_size, h, w])\r\n        self.h = h\r\n        self.w = w\r\n\r\n        self.reset()\r\n\r\n    def run(self):\r\n        super(MarioEnvironment, self).run()\r\n        while True:\r\n            action = self.child_conn.recv()\r\n            if self.is_render:\r\n                self.env.render()\r\n\r\n            # sticky action\r\n            if self.sticky_action:\r\n                if np.random.rand() <= self.p:\r\n                    action = self.last_action\r\n                self.last_action = action\r\n\r\n            obs, reward, done, info = self.env.step(action)\r\n\r\n            # when Mario loses life, changes the state to the terminal\r\n            # state.\r\n            if self.life_done:\r\n                if self.lives > info[\'life\'] and info[\'life\'] > 0:\r\n                    force_done = True\r\n                    self.lives = info[\'life\']\r\n                else:\r\n                    force_done = done\r\n                    self.lives = info[\'life\']\r\n            else:\r\n                force_done = done\r\n\r\n            # reward range -15 ~ 15\r\n            log_reward = reward / 15\r\n            self.rall += log_reward\r\n\r\n            r = log_reward\r\n\r\n            self.history[:3, :, :] = self.history[1:, :, :]\r\n            self.history[3, :, :] = self.pre_proc(obs)\r\n\r\n            self.steps += 1\r\n\r\n            if done:\r\n                self.recent_rlist.append(self.rall)\r\n                print(\r\n                    ""[Episode {}({})] Step: {}  Reward: {}  Recent Reward: {}  Stage: {} current x:{}   max x:{}"".format(\r\n                        self.episode,\r\n                        self.env_idx,\r\n                        self.steps,\r\n                        self.rall,\r\n                        np.mean(\r\n                            self.recent_rlist),\r\n                        info[\'stage\'],\r\n                        info[\'x_pos\'],\r\n                        self.max_pos))\r\n\r\n                self.history = self.reset()\r\n\r\n            self.child_conn.send([self.history[:, :, :], r, force_done, done, log_reward])\r\n\r\n    def reset(self):\r\n        self.last_action = 0\r\n        self.steps = 0\r\n        self.episode += 1\r\n        self.rall = 0\r\n        self.lives = 3\r\n        self.stage = 1\r\n        self.max_pos = 0\r\n        self.get_init_state(self.env.reset())\r\n        return self.history[:, :, :]\r\n\r\n    def pre_proc(self, X):\r\n        # grayscaling\r\n        x = cv2.cvtColor(X, cv2.COLOR_RGB2GRAY)\r\n        # resize\r\n        x = cv2.resize(x, (self.h, self.w))\r\n\r\n        return x\r\n\r\n    def get_init_state(self, s):\r\n        for i in range(self.history_size):\r\n            self.history[i, :, :] = self.pre_proc(s)\r\n'"
RND Montezuma's revenge PyTorch/eval.py,0,"b""from agents import *\r\nfrom envs import *\r\nfrom utils import *\r\nfrom config import *\r\nfrom torch.multiprocessing import Pipe\r\n\r\nfrom tensorboardX import SummaryWriter\r\n\r\nimport numpy as np\r\nimport pickle\r\n\r\n\r\ndef main():\r\n    print({section: dict(config[section]) for section in config.sections()})\r\n    env_id = default_config['EnvID']\r\n    env_type = default_config['EnvType']\r\n\r\n    if env_type == 'mario':\r\n        env = BinarySpaceToDiscreteSpaceEnv(gym_super_mario_bros.make(env_id), COMPLEX_MOVEMENT)\r\n    elif env_type == 'atari':\r\n        env = gym.make(env_id)\r\n    else:\r\n        raise NotImplementedError\r\n    input_size = env.observation_space.shape  # 4\r\n    output_size = env.action_space.n  # 2\r\n\r\n    if 'Breakout' in env_id:\r\n        output_size -= 1\r\n\r\n    env.close()\r\n\r\n    is_render = True\r\n    model_path = 'models/{}.model'.format(env_id)\r\n    predictor_path = 'models/{}.pred'.format(env_id)\r\n    target_path = 'models/{}.target'.format(env_id)\r\n\r\n    use_cuda = False\r\n    use_gae = default_config.getboolean('UseGAE')\r\n    #use_noisy_net = default_config.getboolean('UseNoisyNet')\r\n\r\n    lam = float(default_config['Lambda'])\r\n    num_worker = 1\r\n\r\n    num_step = int(default_config['NumStep'])\r\n\r\n    ppo_eps = float(default_config['PPOEps'])\r\n    epoch = int(default_config['Epoch'])\r\n    mini_batch = int(default_config['MiniBatch'])\r\n    batch_size = int(num_step * num_worker / mini_batch)\r\n    learning_rate = float(default_config['LearningRate'])\r\n    entropy_coef = float(default_config['Entropy'])\r\n    gamma = float(default_config['Gamma'])\r\n    clip_grad_norm = float(default_config['ClipGradNorm'])\r\n\r\n    sticky_action = False\r\n    action_prob = float(default_config['ActionProb'])\r\n    life_done = default_config.getboolean('LifeDone')\r\n\r\n    agent = RNDAgent\r\n\r\n    if default_config['EnvType'] == 'atari':\r\n        env_type = AtariEnvironment\r\n    elif default_config['EnvType'] == 'mario':\r\n        env_type = MarioEnvironment\r\n    else:\r\n        raise NotImplementedError\r\n\r\n    agent = agent(\r\n        input_size,\r\n        output_size,\r\n        num_worker,\r\n        num_step,\r\n        gamma,\r\n        lam=lam,\r\n        learning_rate=learning_rate,\r\n        ent_coef=entropy_coef,\r\n        clip_grad_norm=clip_grad_norm,\r\n        epoch=epoch,\r\n        batch_size=batch_size,\r\n        ppo_eps=ppo_eps,\r\n        use_cuda=use_cuda,\r\n        use_gae=use_gae\r\n    )\r\n\r\n    print('Loading Pre-trained model....')\r\n    if use_cuda:\r\n        agent.model.load_state_dict(torch.load(model_path))\r\n        agent.rnd.predictor.load_state_dict(torch.load(predictor_path))\r\n        agent.rnd.target.load_state_dict(torch.load(target_path))\r\n    else:\r\n        agent.model.load_state_dict(torch.load(model_path, map_location='cpu'))\r\n        agent.rnd.predictor.load_state_dict(torch.load(predictor_path, map_location='cpu'))\r\n        agent.rnd.target.load_state_dict(torch.load(target_path, map_location='cpu'))\r\n    print('End load...')\r\n\r\n    works = []\r\n    parent_conns = []\r\n    child_conns = []\r\n    for idx in range(num_worker):\r\n        parent_conn, child_conn = Pipe()\r\n        work = env_type(env_id, is_render, idx, child_conn, sticky_action=sticky_action, p=action_prob,\r\n                        life_done=life_done)\r\n        work.start()\r\n        works.append(work)\r\n        parent_conns.append(parent_conn)\r\n        child_conns.append(child_conn)\r\n\r\n    states = np.zeros([num_worker, 4, 84, 84])\r\n\r\n    steps = 0\r\n    rall = 0\r\n    rd = False\r\n    intrinsic_reward_list = []\r\n    while not rd:\r\n        steps += 1\r\n        actions, value_ext, value_int, policy = agent.get_action(np.float32(states) / 255.)\r\n\r\n        for parent_conn, action in zip(parent_conns, actions):\r\n            parent_conn.send(action)\r\n\r\n        next_states, rewards, dones, real_dones, log_rewards, next_obs = [], [], [], [], [], []\r\n        for parent_conn in parent_conns:\r\n            s, r, d, rd, lr = parent_conn.recv()\r\n            rall += r\r\n            next_states = s.reshape([1, 4, 84, 84])\r\n            next_obs = s[3, :, :].reshape([1, 1, 84, 84])\r\n\r\n        # total reward = int reward + ext Reward\r\n        intrinsic_reward = agent.compute_intrinsic_reward(next_obs)\r\n        intrinsic_reward_list.append(intrinsic_reward)\r\n        states = next_states[:, :, :, :]\r\n\r\n        if rd:\r\n            intrinsic_reward_list = (intrinsic_reward_list - np.mean(intrinsic_reward_list)) / np.std(\r\n                intrinsic_reward_list)\r\n            with open('int_reward', 'wb') as f:\r\n                pickle.dump(intrinsic_reward_list, f)\r\n            steps = 0\r\n            rall = 0\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n"""
RND Montezuma's revenge PyTorch/make_animation.py,0,"b""from matplotlib.animation import FuncAnimation\r\nfrom matplotlib import animation\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib\r\nimport numpy as np\r\nimport pickle\r\n\r\nmatplotlib.use('TkAgg')\r\n\r\nwith open('int_reward', 'rb') as f:\r\n    pkl = pickle.load(f)\r\n\r\nfig, ax = plt.subplots()\r\nxdata, ydata = [], []\r\n\r\nline, = ax.plot([], [], lw=2)\r\n\r\n\r\ndef init():\r\n    ax.set_xlim(0, len(pkl) + 5)\r\n    ax.set_ylim(pkl.min(), 1)\r\n    return line,\r\n\r\n\r\ndef update(frame):\r\n    xdata.append(int(frame) - 1)\r\n\r\n    ydata.append(pkl[int(frame) - 1, 0])\r\n    line.set_data(xdata, ydata)\r\n    return line,\r\n\r\n\r\nani = FuncAnimation(fig, update, frames=np.linspace(0, len(pkl) - 1, len(pkl), endpoint=False),\r\n                    init_func=init, blit=True, interval=20, repeat=False)\r\nplt.show()\r\n"""
RND Montezuma's revenge PyTorch/model.py,0,"b'import torch.nn.functional as F\r\nimport torch.nn as nn\r\nimport torch\r\nimport torch.optim as optim\r\nimport numpy as np\r\nimport math\r\nfrom torch.nn import init\r\n\r\n\r\nclass Flatten(nn.Module):\r\n    def forward(self, input):\r\n        return input.view(input.size(0), -1)\r\n\r\n\r\n""""""\r\nYou can check the PPO Model schema in the readme section\r\n""""""\r\nclass PPOModel(nn.Module):\r\n    def __init__(self, input_size, output_size):\r\n        super(PPOModel, self).__init__()\r\n\r\n        linear = nn.Linear\r\n\r\n        # Shared network (CNN Part)\r\n        self.feature = nn.Sequential(\r\n            nn.Conv2d(\r\n                in_channels=4,\r\n                out_channels=32,\r\n                kernel_size=8,\r\n                stride=4),\r\n            nn.ELU(),\r\n            nn.Conv2d(\r\n                in_channels=32,\r\n                out_channels=64,\r\n                kernel_size=4,\r\n                stride=2),\r\n            nn.ELU(),\r\n            nn.Conv2d(\r\n                in_channels=64,\r\n                out_channels=64,\r\n                kernel_size=3,\r\n                stride=1),\r\n            nn.ELU(),\r\n            Flatten(),\r\n            linear(\r\n                7 * 7 * 64,\r\n                256),\r\n            nn.ELU(),\r\n            linear(\r\n                256,\r\n                448),\r\n            nn.ELU()\r\n        )\r\n\r\n        self.actor = nn.Sequential(\r\n            linear(448, 448),\r\n            nn.ELU(),\r\n            linear(448, output_size)\r\n        )\r\n\r\n        # The layer before having 2 value head\r\n        self.common_critic_layer = nn.Sequential(\r\n            linear(448, 448),\r\n            nn.ELU()\r\n        )\r\n\r\n        self.critic_ext = linear(448, 1)\r\n        self.critic_int = linear(448, 1)\r\n\r\n        # Initialize the weights\r\n        for p in self.modules():\r\n            # We need to do that in order to initialize the weights\r\n            # Otherwise it returns an error saying that ELU (activation function) does not have weights\r\n\r\n            # First initialize the nn.Conv2d and nn.Linear\r\n            if isinstance(p, nn.Conv2d):\r\n                init.orthogonal_(p.weight, np.sqrt(2))\r\n                p.bias.data.zero_()\r\n\r\n            if isinstance(p, nn.Linear):\r\n                init.orthogonal_(p.weight, np.sqrt(2))\r\n                p.bias.data.zero_()\r\n\r\n        # Initialize critics\r\n        init.orthogonal_(self.critic_ext.weight, 0.01)\r\n        self.critic_ext.bias.data.zero_()\r\n\r\n        init.orthogonal_(self.critic_int.weight, 0.01)\r\n        self.critic_int.bias.data.zero_()\r\n\r\n        # Intiailize actor\r\n        for i in range(len(self.actor)):\r\n            if type(self.actor[i]) == nn.Linear:\r\n                init.orthogonal_(self.actor[i].weight, 0.01)\r\n                self.actor[i].bias.data.zero_()\r\n\r\n        # Init value common layer\r\n        for i in range(len(self.common_critic_layer)):\r\n            if type(self.common_critic_layer[i]) == nn.Linear:\r\n                init.orthogonal_(self.common_critic_layer[i].weight, 0.1)\r\n                self.common_critic_layer[i].bias.data.zero_()\r\n\r\n    def forward(self, state):\r\n        x = self.feature(state)\r\n        policy = self.actor(x)\r\n        value_ext = self.critic_ext(self.common_critic_layer(x) + x)\r\n        value_int = self.critic_int(self.common_critic_layer(x) + x)\r\n        return policy, value_ext, value_int\r\n\r\n\r\n\r\n""""""\r\nIn RND there are 2 networks:\r\n- Target Network: generates a constant output for a given state\r\n- Prediction network: tries to predict the target network\'s output\r\n"""""" \r\nclass RNDModel(nn.Module):\r\n    def __init__(self, input_size, output_size):\r\n        super(RNDModel, self).__init__()\r\n\r\n        self.input_size = input_size\r\n        self.output_size = output_size\r\n\r\n        feature_output = 7 * 7 * 64\r\n\r\n        # Prediction network\r\n        self.predictor = nn.Sequential(\r\n            nn.Conv2d(\r\n                in_channels=1,\r\n                out_channels=32,\r\n                kernel_size=8,\r\n                stride=4),\r\n            nn.ELU(),\r\n            nn.Conv2d(\r\n                in_channels=32,\r\n                out_channels=64,\r\n                kernel_size=4,\r\n                stride=2),\r\n            nn.ELU(),\r\n            nn.Conv2d(\r\n                in_channels=64,\r\n                out_channels=64,\r\n                kernel_size=3,\r\n                stride=1),\r\n            nn.ELU(),\r\n            Flatten(),\r\n            nn.Linear(feature_output, 512),\r\n            nn.ELU(),\r\n            nn.Linear(512, 512),\r\n            nn.ELU(),\r\n            nn.Linear(512, 512)\r\n        )\r\n\r\n        # Target network\r\n        self.target = nn.Sequential(\r\n            nn.Conv2d(\r\n                in_channels=1,\r\n                out_channels=32,\r\n                kernel_size=8,\r\n                stride=4),\r\n            nn.ELU(),\r\n            nn.Conv2d(\r\n                in_channels=32,\r\n                out_channels=64,\r\n                kernel_size=4,\r\n                stride=2),\r\n            nn.ELU(),\r\n            nn.Conv2d(\r\n                in_channels=64,\r\n                out_channels=64,\r\n                kernel_size=3,\r\n                stride=1),\r\n            nn.ELU(),\r\n            Flatten(),\r\n            nn.Linear(feature_output, 512)\r\n        )\r\n\r\n        # Initialize the weights and biases\r\n        for p in self.modules():\r\n            if isinstance(p, nn.Conv2d):\r\n                init.orthogonal_(p.weight, np.sqrt(2))\r\n                p.bias.data.zero_()\r\n\r\n            if isinstance(p, nn.Linear):\r\n                init.orthogonal_(p.weight, np.sqrt(2))\r\n                p.bias.data.zero_()\r\n\r\n        # Set that target network is not trainable\r\n        for param in self.target.parameters():\r\n            param.requires_grad = False\r\n\r\n    def forward(self, next_obs):\r\n        target_feature = self.target(next_obs)\r\n        predict_feature = self.predictor(next_obs)\r\n\r\n        return predict_feature, target_feature'"
RND Montezuma's revenge PyTorch/train.py,0,"b'from agents import *\r\nfrom envs import *\r\nfrom utils import *\r\nfrom config import *\r\nfrom torch.multiprocessing import Pipe\r\n\r\nfrom tensorboardX import SummaryWriter\r\n\r\nimport numpy as np\r\n\r\n\r\ndef main():\r\n    # Print the config hyperparameters\r\n    print({section: dict(config[section]) for section in config.sections()})\r\n\r\n    # Select the training environement\r\n    env_id = default_config[\'EnvID\']\r\n\r\n    #  Select the env_type\r\n    env_type = default_config[\'EnvType\']\r\n\r\n    if env_type == \'mario\':\r\n        env = BinarySpaceToDiscreteSpaceEnv(gym_super_mario_bros.make(env_id), COMPLEX_MOVEMENT)\r\n    elif env_type == \'atari\':\r\n        env = gym.make(env_id)\r\n    else:\r\n        print(""ERROR: This environment is not implemented yet!"")\r\n        raise NotImplementedError\r\n\r\n\r\n    # Get the state size and action space\r\n    input_size = env.observation_space.shape \r\n    print(""INPUT SIZE"", input_size)\r\n    output_size = env.action_space.n \r\n    print(""ACTION SPACE"", output_size)\r\n\r\n    if \'Breakout\' in env_id:\r\n        output_size -= 1\r\n\r\n    env.close()\r\n\r\n    is_load_model = False\r\n    is_render = False\r\n\r\n    # Define the model path names\r\n    model_path = \'models/{}.model\'.format(env_id)\r\n    predictor_path = \'models/{}.pred\'.format(env_id)\r\n    target_path = \'models/{}.target\'.format(env_id)\r\n\r\n    # Set the writer (Tensorboard)\r\n    writer = SummaryWriter()\r\n\r\n    # Get the config hyperparameters\r\n    use_cuda = default_config.getboolean(\'UseGPU\')\r\n    \r\n    # GAE hyperparameters\r\n    use_gae = default_config.getboolean(\'UseGAE\')\r\n    lam = float(default_config[\'Lambda\'])\r\n    \r\n    # Number of different instances of environments we\'re going to run in parallel\r\n    num_worker = int(default_config[\'NumEnv\'])\r\n\r\n    num_step = int(default_config[\'NumStep\'])\r\n\r\n    # PPO epsilon (aka what will help us to define the cliprange )\r\n    ppo_eps = float(default_config[\'PPOEps\'])\r\n\r\n    epoch = int(default_config[\'Epoch\'])\r\n    \r\n    mini_batch = int(default_config[\'MiniBatch\'])\r\n    \r\n    batch_size = int(num_step * num_worker / mini_batch)\r\n    \r\n    learning_rate = float(default_config[\'LearningRate\'])\r\n    \r\n    entropy_coef = float(default_config[\'Entropy\'])\r\n    \r\n    # Extrinsic reward discount rate\r\n    gamma = float(default_config[\'Gamma\'])\r\n\r\n    # Intrinsic reward discount rate\r\n    int_gamma = float(default_config[\'IntGamma\'])\r\n\r\n    # Gradient normalization clip\r\n    clip_grad_norm = float(default_config[\'ClipGradNorm\'])\r\n\r\n    # Extrinsic reward coefficient\r\n    ext_coef = float(default_config[\'ExtCoef\'])\r\n\r\n    # Intrinsic reward coefficient\r\n    int_coef = float(default_config[\'IntCoef\'])\r\n\r\n    # Use sticky action\r\n    sticky_action = default_config.getboolean(\'StickyAction\')\r\n    action_prob = float(default_config[\'ActionProb\'])\r\n\r\n    life_done = default_config.getboolean(\'LifeDone\')\r\n\r\n    reward_rms = RunningMeanStd()\r\n    obs_rms = RunningMeanStd(shape=(1, 1, 84, 84))\r\n    pre_obs_norm_step = int(default_config[\'ObsNormStep\'])\r\n    discounted_reward = RewardForwardFilter(int_gamma)\r\n\r\n    agent = RNDAgent\r\n\r\n    if default_config[\'EnvType\'] == \'atari\':\r\n        env_type = AtariEnvironment\r\n    elif default_config[\'EnvType\'] == \'mario\':\r\n        env_type = MarioEnvironment\r\n    else:\r\n        raise NotImplementedError\r\n\r\n    # Instantiate the agent\r\n    agent = agent(\r\n        input_size,\r\n        output_size,\r\n        num_worker,\r\n        num_step,\r\n        gamma,\r\n        lam=lam,\r\n        learning_rate=learning_rate,\r\n        ent_coef=entropy_coef,\r\n        clip_grad_norm=clip_grad_norm,\r\n        epoch=epoch,\r\n        batch_size=batch_size,\r\n        ppo_eps=ppo_eps,\r\n        use_cuda=use_cuda,\r\n        use_gae=use_gae\r\n    )\r\n\r\n   # Loads models\r\n    if is_load_model:\r\n        if use_cuda:\r\n            print(""Loading PPO Saved Model using GPU"")\r\n            agent.model.load_state_dict(torch.load(model_path))\r\n        else:\r\n            print(""Loading PPO Saved Model using CPU"")\r\n            agent.model.load_state_dict(torch.load(model_path, map_location=\'cpu\'))\r\n\r\n    works = []\r\n    parent_conns = []\r\n    child_conns = []\r\n\r\n    # Generate the different environements\r\n    for idx in range(num_worker):\r\n        parent_conn, child_conn = Pipe()\r\n        work = env_type(env_id, is_render, idx, child_conn, sticky_action=sticky_action, p=action_prob,\r\n                        life_done=life_done)\r\n        work.start()\r\n        works.append(work)\r\n        parent_conns.append(parent_conn)\r\n        child_conns.append(child_conn)\r\n\r\n    states = np.zeros([num_worker, 4, 84, 84])\r\n\r\n    sample_episode = 0\r\n    sample_rall = 0\r\n    sample_step = 0\r\n    sample_env_idx = 0\r\n    sample_i_rall = 0\r\n    global_update = 0\r\n    global_step = 0\r\n\r\n    # normalize obs\r\n    print(\'Start to initailize observation normalization parameter.....\')\r\n    next_obs = []\r\n    for step in range(num_step * pre_obs_norm_step):\r\n        actions = np.random.randint(0, output_size, size=(num_worker,))\r\n\r\n        for parent_conn, action in zip(parent_conns, actions):\r\n            parent_conn.send(action)\r\n\r\n        for parent_conn in parent_conns:\r\n            s, r, d, rd, lr = parent_conn.recv()\r\n            next_obs.append(s[3, :, :].reshape([1, 84, 84]))\r\n\r\n        if len(next_obs) % (num_step * num_worker) == 0:\r\n            next_obs = np.stack(next_obs)\r\n            obs_rms.update(next_obs)\r\n            next_obs = []\r\n    print(\'End to initalize...\')\r\n\r\n    while True:\r\n        total_state, total_reward, total_done, total_next_state, total_action, total_int_reward, total_next_obs, total_ext_values, total_int_values, total_policy, total_policy_np = \\\r\n            [], [], [], [], [], [], [], [], [], [], []\r\n        global_step += (num_worker * num_step)\r\n        global_update += 1\r\n\r\n        # Step 1. n-step rollout\r\n        for _ in range(num_step):\r\n            actions, value_ext, value_int, policy = agent.get_action(np.float32(states) / 255.)\r\n\r\n            for parent_conn, action in zip(parent_conns, actions):\r\n                parent_conn.send(action)\r\n\r\n            next_states, rewards, dones, real_dones, log_rewards, next_obs = [], [], [], [], [], []\r\n            for parent_conn in parent_conns:\r\n                s, r, d, rd, lr = parent_conn.recv()\r\n                next_states.append(s)\r\n                rewards.append(r)\r\n                dones.append(d)\r\n                real_dones.append(rd)\r\n                log_rewards.append(lr)\r\n                next_obs.append(s[3, :, :].reshape([1, 84, 84]))\r\n\r\n            next_states = np.stack(next_states)\r\n            rewards = np.hstack(rewards)\r\n            dones = np.hstack(dones)\r\n            real_dones = np.hstack(real_dones)\r\n            next_obs = np.stack(next_obs)\r\n\r\n            # total reward = int reward + ext Reward\r\n            intrinsic_reward = agent.compute_intrinsic_reward(\r\n                ((next_obs - obs_rms.mean) / np.sqrt(obs_rms.var)).clip(-5, 5))\r\n            intrinsic_reward = np.hstack(intrinsic_reward)\r\n            sample_i_rall += intrinsic_reward[sample_env_idx]\r\n\r\n            total_next_obs.append(next_obs)\r\n            total_int_reward.append(intrinsic_reward)\r\n            total_state.append(states)\r\n            total_reward.append(rewards)\r\n            total_done.append(dones)\r\n            total_action.append(actions)\r\n            total_ext_values.append(value_ext)\r\n            total_int_values.append(value_int)\r\n            total_policy.append(policy)\r\n            total_policy_np.append(policy.cpu().numpy())\r\n\r\n            states = next_states[:, :, :, :]\r\n\r\n            sample_rall += log_rewards[sample_env_idx]\r\n\r\n            sample_step += 1\r\n            if real_dones[sample_env_idx]:\r\n                sample_episode += 1\r\n                writer.add_scalar(\'data/reward_per_epi\', sample_rall, sample_episode)\r\n                writer.add_scalar(\'data/reward_per_rollout\', sample_rall, global_update)\r\n                writer.add_scalar(\'data/step\', sample_step, sample_episode)\r\n                sample_rall = 0\r\n                sample_step = 0\r\n                sample_i_rall = 0\r\n\r\n        # calculate last next value\r\n        _, value_ext, value_int, _ = agent.get_action(np.float32(states) / 255.)\r\n        total_ext_values.append(value_ext)\r\n        total_int_values.append(value_int)\r\n        # --------------------------------------------------\r\n\r\n        total_state = np.stack(total_state).transpose([1, 0, 2, 3, 4]).reshape([-1, 4, 84, 84])\r\n        total_reward = np.stack(total_reward).transpose().clip(-1, 1)\r\n        total_action = np.stack(total_action).transpose().reshape([-1])\r\n        total_done = np.stack(total_done).transpose()\r\n        total_next_obs = np.stack(total_next_obs).transpose([1, 0, 2, 3, 4]).reshape([-1, 1, 84, 84])\r\n        total_ext_values = np.stack(total_ext_values).transpose()\r\n        total_int_values = np.stack(total_int_values).transpose()\r\n        total_logging_policy = np.vstack(total_policy_np)\r\n\r\n        # Step 2. calculate intrinsic reward\r\n        # running mean intrinsic reward\r\n        total_int_reward = np.stack(total_int_reward).transpose()\r\n        total_reward_per_env = np.array([discounted_reward.update(reward_per_step) for reward_per_step in\r\n                                         total_int_reward.T])\r\n        mean, std, count = np.mean(total_reward_per_env), np.std(total_reward_per_env), len(total_reward_per_env)\r\n        reward_rms.update_from_moments(mean, std ** 2, count)\r\n\r\n        # normalize intrinsic reward\r\n        total_int_reward /= np.sqrt(reward_rms.var)\r\n        writer.add_scalar(\'data/int_reward_per_epi\', np.sum(total_int_reward) / num_worker, sample_episode)\r\n        writer.add_scalar(\'data/int_reward_per_rollout\', np.sum(total_int_reward) / num_worker, global_update)\r\n        # -------------------------------------------------------------------------------------------\r\n\r\n        # logging Max action probability\r\n        writer.add_scalar(\'data/max_prob\', softmax(total_logging_policy).max(1).mean(), sample_episode)\r\n\r\n        # Step 3. make target and advantage\r\n        # extrinsic reward calculate\r\n        ext_target, ext_adv = make_train_data(total_reward,\r\n                                              total_done,\r\n                                              total_ext_values,\r\n                                              gamma,\r\n                                              num_step,\r\n                                              num_worker)\r\n\r\n        # intrinsic reward calculate\r\n        # None Episodic\r\n        int_target, int_adv = make_train_data(total_int_reward,\r\n                                              np.zeros_like(total_int_reward),\r\n                                              total_int_values,\r\n                                              int_gamma,\r\n                                              num_step,\r\n                                              num_worker)\r\n\r\n        # add ext adv and int adv\r\n        total_adv = int_adv * int_coef + ext_adv * ext_coef\r\n        # -----------------------------------------------\r\n\r\n        # Step 4. update obs normalize param\r\n        obs_rms.update(total_next_obs)\r\n        # -----------------------------------------------\r\n\r\n        # Step 5. Training\r\n        agent.train_model(np.float32(total_state) / 255., ext_target, int_target, total_action,\r\n                          total_adv, ((total_next_obs - obs_rms.mean) / np.sqrt(obs_rms.var)).clip(-5, 5),\r\n                          total_policy)\r\n\r\n        if global_step % (num_worker * num_step * 100) == 0:\r\n            print(""Num Step: "", num_step)\r\n            print(\'Now Global Step :{}\'.format(global_step))\r\n            torch.save(agent.model.state_dict(), model_path)\r\n            torch.save(agent.rnd.predictor.state_dict(), predictor_path)\r\n            torch.save(agent.rnd.target.state_dict(), target_path)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
RND Montezuma's revenge PyTorch/utils.py,0,"b'from config import *\r\nimport numpy as np\r\n\r\nimport torch\r\nfrom torch._six import inf\r\n\r\n# if default_config[\'TrainMethod\'] in [\'PPO\', \'ICM\', \'RND\']:\r\n#     num_step = int(ppo_config[\'NumStep\'])\r\n# else:\r\n#     num_step = int(default_config[\'NumStep\'])\r\n\r\nuse_gae = default_config.getboolean(\'UseGAE\')\r\nlam = float(default_config[\'Lambda\'])\r\n\r\n""""""\r\nDiscount return and General advantage estimation\r\n""""""\r\ndef make_train_data(reward, done, value, gamma, num_step, num_worker):\r\n    discounted_return = np.empty([num_worker, num_step])\r\n\r\n    # Discounted Return\r\n    if use_gae:\r\n        gae = np.zeros_like([num_worker, ])\r\n        for t in range(num_step - 1, -1, -1):\r\n            delta = reward[:, t] + gamma * value[:, t + 1] * (1 - done[:, t]) - value[:, t]\r\n            gae = delta + gamma * lam * (1 - done[:, t]) * gae\r\n\r\n            discounted_return[:, t] = gae + value[:, t]\r\n\r\n            # For Actor\r\n        adv = discounted_return - value[:, :-1]\r\n\r\n    else:\r\n        running_add = value[:, -1]\r\n        for t in range(num_step - 1, -1, -1):\r\n            running_add = reward[:, t] + gamma * running_add * (1 - done[:, t])\r\n            discounted_return[:, t] = running_add\r\n\r\n        # For Actor\r\n        adv = discounted_return - value[:, :-1]\r\n\r\n    return discounted_return.reshape([-1]), adv.reshape([-1])\r\n\r\n\r\nclass RunningMeanStd(object):\r\n    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\r\n    def __init__(self, epsilon=1e-4, shape=()):\r\n        self.mean = np.zeros(shape, \'float64\')\r\n        self.var = np.ones(shape, \'float64\')\r\n        self.count = epsilon\r\n\r\n    def update(self, x):\r\n        batch_mean = np.mean(x, axis=0)\r\n        batch_var = np.var(x, axis=0)\r\n        batch_count = x.shape[0]\r\n        self.update_from_moments(batch_mean, batch_var, batch_count)\r\n\r\n    def update_from_moments(self, batch_mean, batch_var, batch_count):\r\n        delta = batch_mean - self.mean\r\n        tot_count = self.count + batch_count\r\n\r\n        new_mean = self.mean + delta * batch_count / tot_count\r\n        m_a = self.var * (self.count)\r\n        m_b = batch_var * (batch_count)\r\n        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\r\n        new_var = M2 / (self.count + batch_count)\r\n\r\n        new_count = batch_count + self.count\r\n\r\n        self.mean = new_mean\r\n        self.var = new_var\r\n        self.count = new_count\r\n\r\n\r\nclass RewardForwardFilter(object):\r\n    def __init__(self, gamma):\r\n        self.rewems = None\r\n        self.gamma = gamma\r\n\r\n    def update(self, rews):\r\n        if self.rewems is None:\r\n            self.rewems = rews\r\n        else:\r\n            self.rewems = self.rewems * self.gamma + rews\r\n        return self.rewems\r\n\r\n\r\ndef softmax(z):\r\n    assert len(z.shape) == 2\r\n    s = np.max(z, axis=1)\r\n    s = s[:, np.newaxis]  # necessary step to do broadcasting\r\n    e_x = np.exp(z - s)\r\n    div = np.sum(e_x, axis=1)\r\n    div = div[:, np.newaxis]  # dito\r\n    return e_x / div\r\n\r\n\r\ndef global_grad_norm_(parameters, norm_type=2):\r\n    r""""""Clips gradient norm of an iterable of parameters.\r\n\r\n    The norm is computed over all gradients together, as if they were\r\n    concatenated into a single vector. Gradients are modified in-place.\r\n\r\n    Arguments:\r\n        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\r\n            single Tensor that will have gradients normalized\r\n        max_norm (float or int): max norm of the gradients\r\n        norm_type (float or int): type of the used p-norm. Can be ``\'inf\'`` for\r\n            infinity norm.\r\n\r\n    Returns:\r\n        Total norm of the parameters (viewed as a single vector).\r\n    """"""\r\n    if isinstance(parameters, torch.Tensor):\r\n        parameters = [parameters]\r\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\r\n    norm_type = float(norm_type)\r\n    if norm_type == inf:\r\n        total_norm = max(p.grad.data.abs().max() for p in parameters)\r\n    else:\r\n        total_norm = 0\r\n        for p in parameters:\r\n            param_norm = p.grad.data.norm(norm_type)\r\n            total_norm += param_norm.item() ** norm_type\r\n        total_norm = total_norm ** (1. / norm_type)\r\n\r\n    return total_norm\r\n'"
Q learning/FrozenLake/test.py,0,b't\n'
