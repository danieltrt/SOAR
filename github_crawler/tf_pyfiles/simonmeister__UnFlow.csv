file_path,api_count,code
src/eval_gui.py,33,"b'import os\nimport sys\nimport shutil\n\nimport tensorflow as tf\nimport numpy as np\nimport png\n\nfrom e2eflow.core.flow_util import flow_to_color, flow_error_avg, outlier_pct\nfrom e2eflow.core.flow_util import flow_error_image\nfrom e2eflow.util import config_dict\nfrom e2eflow.core.image_warp import image_warp\nfrom e2eflow.kitti.input import KITTIInput\nfrom e2eflow.kitti.data import KITTIData\nfrom e2eflow.chairs.data import ChairsData\nfrom e2eflow.chairs.input import ChairsInput\nfrom e2eflow.sintel.data import SintelData\nfrom e2eflow.sintel.input import SintelInput\nfrom e2eflow.middlebury.input import MiddleburyInput\nfrom e2eflow.middlebury.data import MiddleburyData\nfrom e2eflow.core.unsupervised import unsupervised_loss\nfrom e2eflow.core.input import resize_input, resize_output_crop, resize_output, resize_output_flow\nfrom e2eflow.core.train import restore_networks\nfrom e2eflow.ops import forward_warp\nfrom e2eflow.gui import display\nfrom e2eflow.core.losses import DISOCC_THRESH, occlusion, create_outgoing_mask\nfrom e2eflow.util import convert_input_strings\n\n\ntf.app.flags.DEFINE_string(\'dataset\', \'kitti\',\n                            \'Name of dataset to evaluate on. One of {kitti, sintel, chairs, mdb}.\')\ntf.app.flags.DEFINE_string(\'variant\', \'train_2012\',\n                           \'Name of variant to evaluate on.\'\n                           \'If dataset = kitti, one of {train_2012, train_2015, test_2012, test_2015}.\'\n                           \'If dataset = sintel, one of {train_clean, train_final}.\'\n                           \'If dataset = mdb, one of {train, test}.\')\ntf.app.flags.DEFINE_string(\'ex\', \'\',\n                           \'Experiment name(s) (can be comma separated list).\')\ntf.app.flags.DEFINE_integer(\'num\', 10,\n                            \'Number of examples to evaluate. Set to -1 to evaluate all.\')\ntf.app.flags.DEFINE_integer(\'num_vis\', 100,\n                            \'Number of evalutations to visualize. Set to -1 to visualize all.\')\ntf.app.flags.DEFINE_string(\'gpu\', \'0\',\n                           \'GPU device to evaluate on.\')\ntf.app.flags.DEFINE_boolean(\'output_benchmark\', False,\n                            \'Output raw flow files.\')\ntf.app.flags.DEFINE_boolean(\'output_visual\', False,\n                            \'Output flow visualization files.\')\ntf.app.flags.DEFINE_boolean(\'output_backward\', False,\n                            \'Output backward flow files.\')\ntf.app.flags.DEFINE_boolean(\'output_png\', True, # TODO finish .flo output\n                            \'Raw output format to use with output_benchmark.\'\n                            \'Outputs .png flow files if true, output .flo otherwise.\')\nFLAGS = tf.app.flags.FLAGS\n\n\nNUM_EXAMPLES_PER_PAGE = 4\n\n\ndef write_rgb_png(z, path, bitdepth=8):\n    z = z[0, :, :, :]\n    with open(path, \'wb\') as f:\n        writer = png.Writer(width=z.shape[1], height=z.shape[0], bitdepth=bitdepth)\n        z2list = z.reshape(-1, z.shape[1]*z.shape[2]).tolist()\n        writer.write(f, z2list)\n\n\ndef flow_to_int16(flow):\n    _, h, w, _ = tf.unstack(tf.shape(flow))\n    u, v = tf.unstack(flow, num=2, axis=3)\n    r = tf.cast(tf.maximum(0.0, tf.minimum(u * 64.0 + 32768.0, 65535.0)), tf.uint16)\n    g = tf.cast(tf.maximum(0.0, tf.minimum(v * 64.0 + 32768.0, 65535.0)), tf.uint16)\n    b = tf.ones([1, h, w], tf.uint16)\n    return tf.stack([r, g, b], axis=3)\n\n\ndef write_flo(flow, filename):\n    """"""\n    write optical flow in Middlebury .flo format\n    :param flow: optical flow map\n    :param filename: optical flow file path to be saved\n    :return: None\n    """"""\n    flow = flow[0, :, :, :]\n    f = open(filename, \'wb\')\n    magic = np.array([202021.25], dtype=np.float32)\n    height, width = flow.shape[:2]\n    magic.tofile(f)\n    np.int32(width).tofile(f)\n    np.int32(height).tofile(f)\n    data = np.float32(flow).flatten()\n    data.tofile(f)\n    f.close()\n\n\ndef _evaluate_experiment(name, input_fn, data_input):\n    normalize_fn = data_input._normalize_image\n    resized_h = data_input.dims[0]\n    resized_w = data_input.dims[1]\n\n    current_config = config_dict(\'../config.ini\')\n    exp_dir = os.path.join(current_config[\'dirs\'][\'log\'], \'ex\', name)\n    config_path = os.path.join(exp_dir, \'config.ini\')\n    if not os.path.isfile(config_path):\n        config_path = \'../config.ini\'\n    if not os.path.isdir(exp_dir) or not tf.train.get_checkpoint_state(exp_dir):\n        exp_dir = os.path.join(current_config[\'dirs\'][\'checkpoints\'], name)\n    config = config_dict(config_path)\n    params = config[\'train\']\n    convert_input_strings(params, config_dict(\'../config.ini\')[\'dirs\'])\n    dataset_params_name = \'train_\' + FLAGS.dataset\n    if dataset_params_name in config:\n        params.update(config[dataset_params_name])\n    ckpt = tf.train.get_checkpoint_state(exp_dir)\n    if not ckpt:\n        raise RuntimeError(""Error: experiment must contain a checkpoint"")\n    ckpt_path = exp_dir + ""/"" + os.path.basename(ckpt.model_checkpoint_path)\n\n    with tf.Graph().as_default(): #, tf.device(\'gpu:\' + FLAGS.gpu):\n        inputs = input_fn()\n        im1, im2, input_shape = inputs[:3]\n        truth = inputs[3:]\n\n        height, width, _ = tf.unstack(tf.squeeze(input_shape), num=3, axis=0)\n        im1 = resize_input(im1, height, width, resized_h, resized_w)\n        im2 = resize_input(im2, height, width, resized_h, resized_w) # TODO adapt train.py\n\n        _, flow, flow_bw = unsupervised_loss(\n            (im1, im2),\n            normalization=data_input.get_normalization(),\n            params=params, augment=False, return_flow=True)\n\n        im1 = resize_output(im1, height, width, 3)\n        im2 = resize_output(im2, height, width, 3)\n        flow = resize_output_flow(flow, height, width, 2)\n        flow_bw = resize_output_flow(flow_bw, height, width, 2)\n\n        flow_fw_int16 = flow_to_int16(flow)\n        flow_bw_int16 = flow_to_int16(flow_bw)\n\n        im1_pred = image_warp(im2, flow)\n        im1_diff = tf.abs(im1 - im1_pred)\n        #im2_diff = tf.abs(im1 - im2)\n\n        #flow_bw_warped = image_warp(flow_bw, flow)\n\n        if len(truth) == 4:\n            flow_occ, mask_occ, flow_noc, mask_noc = truth\n            flow_occ = resize_output_crop(flow_occ, height, width, 2)\n            flow_noc = resize_output_crop(flow_noc, height, width, 2)\n            mask_occ = resize_output_crop(mask_occ, height, width, 1)\n            mask_noc = resize_output_crop(mask_noc, height, width, 1)\n\n            #div = divergence(flow_occ)\n            #div_bw = divergence(flow_bw)\n            occ_pred = 1 - (1 - occlusion(flow, flow_bw)[0])\n            def_pred = 1 - (1 - occlusion(flow, flow_bw)[1])\n            disocc_pred = forward_warp(flow_bw) < DISOCC_THRESH\n            disocc_fw_pred = forward_warp(flow) < DISOCC_THRESH\n            image_slots = [((im1 * 0.5 + im2 * 0.5) / 255, \'overlay\'),\n                           (im1_diff / 255, \'brightness error\'),\n                           #(im1 / 255, \'first image\', 1, 0),\n                           #(im2 / 255, \'second image\', 1, 0),\n                           #(im2_diff / 255, \'|first - second|\', 1, 2),\n                           (flow_to_color(flow), \'flow\'),\n                           #(flow_to_color(flow_bw), \'flow bw prediction\'),\n                           #(tf.image.rgb_to_grayscale(im1_diff) > 20, \'diff\'),\n                           #(occ_pred, \'occ\'),\n                           #(def_pred, \'disocc\'),\n                           #(disocc_pred, \'reverse disocc\'),\n                           #(disocc_fw_pred, \'forward disocc prediction\'),\n                           #(div, \'div\'),\n                           #(div < -2, \'neg div\'),\n                           #(div > 5, \'pos div\'),\n                           #(flow_to_color(flow_occ, mask_occ), \'flow truth\'),\n                           (flow_error_image(flow, flow_occ, mask_occ, mask_noc),\n                            \'flow error\') #  (blue: correct, red: wrong, dark: occluded)\n            ]\n\n            # list of (scalar_op, title)\n            scalar_slots = [(flow_error_avg(flow_noc, flow, mask_noc), \'EPE_noc\'),\n                            (flow_error_avg(flow_occ, flow, mask_occ), \'EPE_all\'),\n                            (outlier_pct(flow_noc, flow, mask_noc), \'outliers_noc\'),\n                            (outlier_pct(flow_occ, flow, mask_occ), \'outliers_all\')]\n        elif len(truth) == 2:\n            flow_gt, mask = truth\n            flow_gt = resize_output_crop(flow_gt, height, width, 2)\n            mask = resize_output_crop(mask, height, width, 1)\n\n            image_slots = [((im1 * 0.5 + im2 * 0.5) / 255, \'overlay\'),\n                           (im1_diff / 255, \'brightness error\'),\n                           (flow_to_color(flow), \'flow\'),\n                           (flow_to_color(flow_gt, mask), \'gt\'),\n            ]\n\n            # list of (scalar_op, title)\n            scalar_slots = [(flow_error_avg(flow_gt, flow, mask), \'EPE_all\')]\n        else:\n            image_slots = [(im1 / 255, \'first image\'),\n                           #(im1_pred / 255, \'warped second image\', 0, 1),\n                           (im1_diff / 255, \'warp error\'),\n                           #(im2 / 255, \'second image\', 1, 0),\n                           #(im2_diff / 255, \'|first - second|\', 1, 2),\n                           (flow_to_color(flow), \'flow prediction\')]\n            scalar_slots = []\n\n        num_ims = len(image_slots)\n        image_ops = [t[0] for t in image_slots]\n        scalar_ops = [t[0] for t in scalar_slots]\n        image_names = [t[1] for t in image_slots]\n        scalar_names = [t[1] for t in scalar_slots]\n        all_ops = image_ops + scalar_ops\n\n        image_lists = []\n        averages = np.zeros(len(scalar_ops))\n        sess_config = tf.ConfigProto(allow_soft_placement=True)\n\n        exp_out_dir = os.path.join(\'../out\', name)\n        if FLAGS.output_visual or FLAGS.output_benchmark:\n            if os.path.isdir(exp_out_dir):\n                shutil.rmtree(exp_out_dir)\n            os.makedirs(exp_out_dir)\n            shutil.copyfile(config_path, os.path.join(exp_out_dir, \'config.ini\'))\n\n        with tf.Session(config=sess_config) as sess:\n            saver = tf.train.Saver(tf.global_variables())\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n\n            restore_networks(sess, params, ckpt, ckpt_path)\n\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(sess=sess,\n                                                   coord=coord)\n\n            # TODO adjust for batch_size > 1 (also need to change image_lists appending)\n            max_iter = FLAGS.num if FLAGS.num > 0 else None\n\n            try:\n                num_iters = 0\n                while not coord.should_stop() and (max_iter is None or num_iters != max_iter):\n                    all_results = sess.run([flow, flow_bw, flow_fw_int16, flow_bw_int16] + all_ops)\n                    flow_fw_res, flow_bw_res, flow_fw_int16_res, flow_bw_int16_res = all_results[:4]\n                    all_results = all_results[4:]\n                    image_results = all_results[:num_ims]\n                    scalar_results = all_results[num_ims:]\n                    iterstr = str(num_iters).zfill(6)\n                    if FLAGS.output_visual:\n                        path_col = os.path.join(exp_out_dir, iterstr + \'_flow.png\')\n                        path_overlay = os.path.join(exp_out_dir, iterstr + \'_img.png\')\n                        path_error = os.path.join(exp_out_dir, iterstr + \'_err.png\')\n                        write_rgb_png(image_results[0] * 255, path_overlay)\n                        write_rgb_png(image_results[1] * 255, path_col)\n                        write_rgb_png(image_results[2] * 255, path_error)\n                    if FLAGS.output_benchmark:\n                        path_fw = os.path.join(exp_out_dir, iterstr)\n                        if FLAGS.output_png:\n                            write_rgb_png(flow_fw_int16_res, path_fw  + \'_10.png\', bitdepth=16)\n                        else:\n                            write_flo(flow_fw_res, path_fw + \'_10.flo\')\n                        if FLAGS.output_backward:\n                            path_fw = os.path.join(exp_out_dir, iterstr + \'_01.png\')\n                            write_rgb_png(flow_bw_int16_res, path_bw, bitdepth=16)\n                    if num_iters < FLAGS.num_vis:\n                        image_lists.append(image_results)\n                    averages += scalar_results\n                    if num_iters > 0:\n                        sys.stdout.write(\'\\r\')\n                    num_iters += 1\n                    sys.stdout.write(""-- evaluating \'{}\': {}/{}""\n                                     .format(name, num_iters, max_iter))\n                    sys.stdout.flush()\n                    print()\n            except tf.errors.OutOfRangeError:\n                pass\n\n            averages /= num_iters\n\n            coord.request_stop()\n            coord.join(threads)\n\n    for t, avg in zip(scalar_slots, averages):\n        _, scalar_name = t\n        print(""({}) {} = {}"".format(name, scalar_name, avg))\n\n    return image_lists, image_names\n\n\ndef main(argv=None):\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = FLAGS.gpu\n\n    print(""-- evaluating: on {} pairs from {}/{}""\n          .format(FLAGS.num, FLAGS.dataset, FLAGS.variant))\n\n    default_config = config_dict()\n    dirs = default_config[\'dirs\']\n\n    if FLAGS.dataset == \'kitti\':\n        data = KITTIData(dirs[\'data\'], development=True)\n        data_input = KITTIInput(data, batch_size=1, normalize=False,\n                                 dims=(384,1280))\n        inputs = getattr(data_input, \'input_\' + FLAGS.variant)()\n    elif FLAGS.dataset == \'chairs\':\n        data = ChairsData(dirs[\'data\'], development=True)\n        data_input = ChairsInput(data, batch_size=1, normalize=False,\n                                 dims=(384,512))\n        if FLAGS.variant == \'test_2015\' and FLAGS.num == -1:\n            FLAGS.num = 200\n        elif FLAGS.variant == \'test_2012\' and FLAGS.num == -1:\n            FLAGS.num = 195\n    elif FLAGS.dataset == \'sintel\':\n        data = SintelData(dirs[\'data\'], development=True)\n        data_input = SintelInput(data, batch_size=1, normalize=False,\n                                 dims=(512,1024))\n    if FLAGS.variant in [\'test_clean\', \'test_final\'] and FLAGS.num == -1:\n        FLAGS.num = 552\n    elif FLAGS.dataset == \'mdb\':\n        data = MiddleburyData(dirs[\'data\'], development=True)\n        data_input = MiddleburyInput(data, batch_size=1, normalize=False,\n                                     dims=(512,640))\n        if FLAGS.variant == \'test\' and FLAGS.num == -1:\n            FLAGS.num = 12\n\n    input_fn = getattr(data_input, \'input_\' + FLAGS.variant)\n\n    results = []\n    for name in FLAGS.ex.split(\',\'):\n        result, image_names = _evaluate_experiment(name, input_fn, data_input)\n        results.append(result)\n\n    display(results, image_names)\n\n\n\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
src/run.py,5,"b'import os\nimport copy\n\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\n\nfrom e2eflow.core.train import Trainer\nfrom e2eflow.experiment import Experiment\nfrom e2eflow.util import convert_input_strings\n\nfrom e2eflow.kitti.input import KITTIInput\nfrom e2eflow.kitti.data import KITTIData\nfrom e2eflow.chairs.data import ChairsData\nfrom e2eflow.chairs.input import ChairsInput\nfrom e2eflow.sintel.data import SintelData\nfrom e2eflow.sintel.input import SintelInput\nfrom e2eflow.synthia.data import SynthiaData\nfrom e2eflow.cityscapes.data import CityscapesData\n\n\ntf.app.flags.DEFINE_string(\'ex\', \'default\',\n                           \'Name of the experiment.\'\n                           \'If the experiment folder already exists in the log dir, \'\n                           \'training will be continued from the latest checkpoint.\')\ntf.app.flags.DEFINE_boolean(\'debug\', False,\n                            \'Enable image summaries and disable checkpoint writing for debugging.\')\ntf.app.flags.DEFINE_boolean(\'ow\', False,\n                            \'Overwrites a previous experiment with the same name (if present)\'\n                            \'instead of attempting to continue from its latest checkpoint.\')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(argv=None):\n    experiment = Experiment(\n        name=FLAGS.ex,\n        overwrite=FLAGS.ow)\n    dirs = experiment.config[\'dirs\']\n    run_config = experiment.config[\'run\']\n\n    gpu_list_param = run_config[\'gpu_list\']\n\n    if isinstance(gpu_list_param, int):\n        gpu_list = [gpu_list_param]\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(gpu_list_param)\n    else:\n        gpu_list = list(range(len(gpu_list_param.split(\',\'))))\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = gpu_list_param\n    gpu_batch_size = int(run_config[\'batch_size\'] / max(len(gpu_list), 1))\n    devices = [\'/gpu:\' + str(gpu_num) for gpu_num in gpu_list]\n\n    train_dataset = run_config.get(\'dataset\', \'kitti\')\n\n    kdata = KITTIData(data_dir=dirs[\'data\'],\n                      fast_dir=dirs.get(\'fast\'),\n                      stat_log_dir=None,\n                      development=run_config[\'development\'])\n    einput = KITTIInput(data=kdata,\n                        batch_size=1,\n                        normalize=False,\n                        dims=(384, 1280))\n\n    if train_dataset == \'chairs\':\n        cconfig = copy.deepcopy(experiment.config[\'train\'])\n        cconfig.update(experiment.config[\'train_chairs\'])\n        convert_input_strings(cconfig, dirs)\n        citers = cconfig.get(\'num_iters\', 0)\n        cdata = ChairsData(data_dir=dirs[\'data\'],\n                           fast_dir=dirs.get(\'fast\'),\n                           stat_log_dir=None,\n                           development=run_config[\'development\'])\n        cinput = ChairsInput(data=cdata,\n                 batch_size=gpu_batch_size,\n                 normalize=False,\n                 dims=(cconfig[\'height\'], cconfig[\'width\']))\n        tr = Trainer(\n              lambda shift: cinput.input_raw(swap_images=False,\n                                             shift=shift * run_config[\'batch_size\']),\n              lambda: einput.input_train_2012(),\n              params=cconfig,\n              normalization=cinput.get_normalization(),\n              train_summaries_dir=experiment.train_dir,\n              eval_summaries_dir=experiment.eval_dir,\n              experiment=FLAGS.ex,\n              ckpt_dir=experiment.save_dir,\n              debug=FLAGS.debug,\n              interactive_plot=run_config.get(\'interactive_plot\'),\n              devices=devices)\n        tr.run(0, citers)\n\n    elif train_dataset == \'kitti\':\n        kconfig = copy.deepcopy(experiment.config[\'train\'])\n        kconfig.update(experiment.config[\'train_kitti\'])\n        convert_input_strings(kconfig, dirs)\n        kiters = kconfig.get(\'num_iters\', 0)\n        kinput = KITTIInput(data=kdata,\n                            batch_size=gpu_batch_size,\n                            normalize=False,\n                            skipped_frames=True,\n                            dims=(kconfig[\'height\'], kconfig[\'width\']))\n        tr = Trainer(\n              lambda shift: kinput.input_raw(swap_images=False,\n                                             center_crop=True,\n                                             shift=shift * run_config[\'batch_size\']),\n              lambda: einput.input_train_2012(),\n              params=kconfig,\n              normalization=kinput.get_normalization(),\n              train_summaries_dir=experiment.train_dir,\n              eval_summaries_dir=experiment.eval_dir,\n              experiment=FLAGS.ex,\n              ckpt_dir=experiment.save_dir,\n              debug=FLAGS.debug,\n              interactive_plot=run_config.get(\'interactive_plot\'),\n              devices=devices)\n        tr.run(0, kiters)\n\n    elif train_dataset == \'cityscapes\':\n        kconfig = copy.deepcopy(experiment.config[\'train\'])\n        kconfig.update(experiment.config[\'train_cityscapes\'])\n        convert_input_strings(kconfig, dirs)\n        kiters = kconfig.get(\'num_iters\', 0)\n        cdata = CityscapesData(data_dir=dirs[\'data\'],\n                    fast_dir=dirs.get(\'fast\'),\n                    stat_log_dir=None,\n                    development=run_config[\'development\'])\n        kinput = KITTIInput(data=cdata,\n                            batch_size=gpu_batch_size,\n                            normalize=False,\n                            skipped_frames=False,\n                            dims=(kconfig[\'height\'], kconfig[\'width\']))\n        tr = Trainer(\n              lambda shift: kinput.input_raw(swap_images=False,\n                                             center_crop=True,\n                                             skip=[0, 1],\n                                             shift=shift * run_config[\'batch_size\']),\n              lambda: einput.input_train_2012(),\n              params=kconfig,\n              normalization=kinput.get_normalization(),\n              train_summaries_dir=experiment.train_dir,\n              eval_summaries_dir=experiment.eval_dir,\n              experiment=FLAGS.ex,\n              ckpt_dir=experiment.save_dir,\n              debug=FLAGS.debug,\n              interactive_plot=run_config.get(\'interactive_plot\'),\n              devices=devices)\n        tr.run(0, kiters)\n\n    elif train_dataset == \'synthia\':\n        sconfig = copy.deepcopy(experiment.config[\'train\'])\n        sconfig.update(experiment.config[\'train_synthia\'])\n        convert_input_strings(sconfig, dirs)\n        siters = sconfig.get(\'num_iters\', 0)\n        sdata = SynthiaData(data_dir=dirs[\'data\'],\n                fast_dir=dirs.get(\'fast\'),\n                stat_log_dir=None,\n                development=run_config[\'development\'])\n        sinput = KITTIInput(data=sdata,\n                            batch_size=gpu_batch_size,\n                            normalize=False,\n                            dims=(sconfig[\'height\'], sconfig[\'width\']))\n        tr = Trainer(\n              lambda shift: sinput.input_raw(swap_images=False,\n                                             shift=shift * run_config[\'batch_size\']),\n              lambda: einput.input_train_2012(),\n              params=sconfig,\n              normalization=sinput.get_normalization(),\n              train_summaries_dir=experiment.train_dir,\n              eval_summaries_dir=experiment.eval_dir,\n              experiment=FLAGS.ex,\n              ckpt_dir=experiment.save_dir,\n              debug=FLAGS.debug,\n              interactive_plot=run_config.get(\'interactive_plot\'),\n              devices=devices)\n        tr.run(0, siters)\n\n    elif train_dataset == \'kitti_ft\':\n        ftconfig = copy.deepcopy(experiment.config[\'train\'])\n        ftconfig.update(experiment.config[\'train_kitti_ft\'])\n        convert_input_strings(ftconfig, dirs)\n        ftiters = ftconfig.get(\'num_iters\', 0)\n        ftinput = KITTIInput(data=kdata,\n                             batch_size=gpu_batch_size,\n                             normalize=False,\n                             dims=(ftconfig[\'height\'], ftconfig[\'width\']))\n        tr = Trainer(\n              lambda shift: ftinput.input_train_gt(40),\n              lambda: einput.input_train_2015(40),\n              supervised=True,\n              params=ftconfig,\n              normalization=ftinput.get_normalization(),\n              train_summaries_dir=experiment.train_dir,\n              eval_summaries_dir=experiment.eval_dir,\n              experiment=FLAGS.ex,\n              ckpt_dir=experiment.save_dir,\n              debug=FLAGS.debug,\n              interactive_plot=run_config.get(\'interactive_plot\'),\n              devices=devices)\n        tr.run(0, ftiters)\n\n    else:\n      raise ValueError(\n          ""Invalid dataset. Dataset must be one of ""\n          ""{synthia, kitti, kitti_ft, cityscapes, chairs}"")\n\n    if not FLAGS.debug:\n        experiment.conclude()\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
src/test.py,1,"b'import tensorflow as tf\n#from e2eflow.test.test_image_warp import ImageWarpTest\n#from e2eflow.test.test_losses import LossesTest\n\n#from e2eflow.test.ops.backward_warp import BackwardWarpTest\n#from e2eflow.test.ops.forward_warp import ForwardWarpTest\n#from e2eflow.test.ops.downsample import DownsampleTest\nfrom e2eflow.test.ops.correlation import CorrelationTest\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
src/e2eflow/__init__.py,0,b''
src/e2eflow/experiment.py,2,"b'import os\nimport subprocess\nimport datetime\nfrom shutil import copyfile, rmtree\n\nimport tensorflow as tf\n\nfrom .util import config_dict\n\n\nclass Experiment():\n    def __init__(self, name, overwrite=False):\n        global_config = config_dict()\n        log_dir = os.path.join(global_config[\'dirs\'][\'log\'], \'ex\', name)\n        dirs = global_config[\'dirs\']\n\n        train_dir = os.path.join(log_dir, \'train\')\n        eval_dir = os.path.join(log_dir, \'eval\')\n        save_dir = os.path.join(dirs[\'checkpoints\'], name)\n\n        def _init_dirs():\n            os.makedirs(log_dir)\n            os.makedirs(save_dir)\n            os.makedirs(train_dir)\n            os.makedirs(eval_dir)\n\n        # Experiment already exists\n        if os.path.isdir(log_dir):\n            if overwrite:\n                rmtree(log_dir)\n                if os.path.isdir(save_dir):\n                    rmtree(save_dir)\n                _init_dirs()\n\n            else:\n                if not os.path.isdir(save_dir):\n                    os.makedirs(save_dir)\n                    # Copy stored checkpoint in case intermediate checkpoints\n                    # were deleted\n                    ckpt = self._copy_latest_checkpoint(log_dir, save_dir)\n                    if not ckpt:\n                        raise RuntimeError(\'Failed to restore ""{}"".\'\n                                           \'Use --overwrite=True to clear.\'\n                                           .format(name))\n                    print(\'Warning: intermediate checkpoints could not be restored.\')\n        else:\n            _init_dirs()\n\n        config_path = os.path.join(log_dir, \'config.ini\')\n        if not os.path.isfile(config_path) or overwrite:\n            copyfile(\'../config.ini\', config_path)\n        config = config_dict(config_path)\n\n        self.train_dir = train_dir\n        self.eval_dir = eval_dir\n        self.save_dir = save_dir\n        self.log_dir = log_dir\n        self.name = name\n        self.config = config\n\n    def latest_checkpoint(self):\n        return tf.train.latest_checkpoint(self.save_dir)\n\n    def _copy_latest_checkpoint(self, src, dst, reset_global_step=False):\n        ckpt = tf.train.latest_checkpoint(src)\n        if ckpt:\n            ckpt_base = os.path.basename(ckpt)\n            new_base = \'model.ckpt-0\' if reset_global_step else ckpt_base\n            with open(os.path.join(dst, \'checkpoint\'), \'w\') as f:\n                f.write(\'model_checkpoint_path: ""\' + new_base + \'""\\n\')\n                f.write(\'all_model_checkpoint_paths: ""\' + new_base + \'""\\n\')\n            for filename in os.listdir(src):\n                if ckpt_base in filename:\n                    new_filename = filename.replace(ckpt_base, new_base)\n                    copyfile(os.path.join(src, filename),\n                             os.path.join(dst, new_filename))\n        return ckpt\n\n    def conclude(self):\n        """"""Move final checkpoint to the permanent log dir.""""""\n        ckpt = self._copy_latest_checkpoint(self.save_dir, self.log_dir)\n        if not ckpt:\n            print(\'Warning: no checkpoints written\')\n'"
src/e2eflow/gui.py,0,"b'import numpy as np\nimport matplotlib\nmatplotlib.use(\'TkAgg\')\nimport matplotlib.pyplot as plt\nfrom matplotlib.widgets import Slider, Button\n\n\ndef display(results, image_names, title=""Flow eval""):\n    image_grids = []\n    num_images = len(results[0])\n    num_rows = len(results[0][0])\n    num_cols = len(results)\n\n    image_grids = []\n    for i in range(num_images):\n        image_grid = []\n        for image_lists in results:\n            image_grid.append(image_lists[i])\n        image_grids.append(image_grid)\n\n    fig = plt.figure(facecolor=\'grey\')\n    fig.suptitle(title)\n    mng = plt.get_current_fig_manager()\n    mng.resize(*mng.window.maxsize())\n    imshow_images = []\n    plt.subplots_adjust(wspace=0, hspace=0.03)\n\n    imshow_image_lists = []\n    for j, image_col in enumerate(image_grids[0]):\n        imshow_images = []\n        for i, t in enumerate(zip(image_names, image_col)):\n            title, image = t\n            ax = fig.add_subplot(num_rows, num_cols, i * num_cols + j + 1)\n            if j == 0:\n                ax.set_ylabel(title)\n            ax.set_yticks([])\n            ax.set_xticks([])\n            if np.size(image, 3) == 1:\n                imshow_images.append(ax.imshow(image[0, :, :, 0], ""gray""))\n            else:\n                imshow_images.append(ax.imshow(image[0, :, :, :]))\n        imshow_image_lists.append(imshow_images)\n\n    def display_example(index):\n        for j, image_col in enumerate(image_grids[int(index)]):\n            imshow_images = imshow_image_lists[j]\n            for im, image in zip(imshow_images, image_col):\n                if np.size(image, 3) == 1:\n                    im.set_data(image[0, :, :, 0])\n                else:\n                    im.set_data(image[0, :, :, :])\n        plt.draw()\n\n    current_index = 0\n\n    next_button_ax = fig.add_axes([0.8, 0.025, 0.1, 0.04])\n    next_button = Button(next_button_ax, \'next\')\n    prev_button_ax = fig.add_axes([0.7, 0.025, 0.1, 0.04])\n    prev_button = Button(prev_button_ax, \'previous\')\n    slider_ax  = fig.add_axes([0.1, 0.025, 0.55, 0.04])\n    slider = Slider(slider_ax, \'Page\', 0, num_images - 1,\n                    valinit=1, valfmt=\'%0.0f\')\n\n    def next_button_on_clicked(mouse_event):\n        nonlocal current_index\n        if current_index < num_images - 1:\n            current_index += 1\n            slider.set_val(current_index)\n\n    def prev_button_on_clicked(mouse_event):\n        nonlocal current_index\n        if current_index > 0:\n            current_index -= 1\n            slider.set_val(current_index)\n\n    def sliders_on_changed(val):\n        nonlocal current_index\n        current_index = val\n        display_example(val)\n\n    slider.on_changed(sliders_on_changed)\n    prev_button.on_clicked(prev_button_on_clicked)\n    next_button.on_clicked(next_button_on_clicked)\n\n    plt.draw()\n    plt.show()\n'"
src/e2eflow/ops.py,4,"b'import os\nimport sys\nimport tensorflow as tf\nimport subprocess\nfrom tensorflow.python.framework import ops\n\nimport configparser\n\n\n# Register ops for compilation here\nOP_NAMES = [\'backward_warp\', \'downsample\', \'correlation\', \'forward_warp\']\n\n\ncwd = os.getcwd()\nos.chdir(os.path.dirname(os.path.realpath(__file__)))\nos.chdir(""../../ops"")\n\nconfig = configparser.ConfigParser()\nconfig.read(""../config.ini"")\n\ndef compile(op=None):\n    if op is not None:\n        to_compile = [op]\n    else:\n        to_compile = OP_NAMES\n\n    tf_inc = "" "".join(tf.sysconfig.get_compile_flags())\n    tf_lib = "" "".join(tf.sysconfig.get_link_flags())\n    for n in to_compile:\n        base = n + ""_op""\n        fn_cu_cc = base + "".cu.cc""\n        fn_cu_o = base + "".cu.o""\n        fn_cc = base + "".cc""\n        fn_o = base + "".o""\n        fn_so = base + "".so""\n\n        out, err = subprocess.Popen([\'which\', \'nvcc\'], stdout=subprocess.PIPE).communicate()\n        cuda_dir = out.decode().split(\'/cuda\')[0]\n\n        nvcc_cmd = ""nvcc -std=c++11 -c -o {} {} {} -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC -I "" + cuda_dir + "" --expt-relaxed-constexpr""\n        nvcc_cmd = nvcc_cmd.format("" "".join([fn_cu_o, fn_cu_cc]),\n                                tf_inc, tf_lib)\n        subprocess.check_output(nvcc_cmd, shell=True)\n        gcc_cmd = ""{} -std=c++11 -shared -o {} {} -fPIC -L "" + cuda_dir + ""/cuda/lib64 -lcudart {} -O2 -D GOOGLE_CUDA=1""\n        gcc_cmd = gcc_cmd.format(config[\'compile\'][\'g++\'],\t\n                                 "" "".join([fn_so, fn_cu_o, fn_cc]),\t\n                                 tf_inc, tf_lib)\n        subprocess.check_output(gcc_cmd, shell=True)\n\n\nif __name__ == ""__main__"":\n    compile()\n\n\nmodule = sys.modules[__name__]\nfor n in OP_NAMES:\n    lib_path = \'./{}_op.so\'.format(n)\n    try:\n        op_lib = tf.load_op_library(lib_path)\n    except:\n        compile(n)\n        op_lib = tf.load_op_library(lib_path)\n    setattr(module, \'_\' + n + \'_module\', op_lib)\n\n\nos.chdir(cwd)\n\n\ndef correlation(first, second, **kwargs):\n    return _correlation_module.correlation(first, second, **kwargs)[0]\n\n\nbackward_warp = _backward_warp_module.backward_warp\ndownsample = _downsample_module.downsample\nforward_warp = _forward_warp_module.forward_warp\n\n\n# Register op gradients\n\n@ops.RegisterGradient(""BackwardWarp"")\ndef _BackwardWarpGrad(op, grad):\n    grad0 = _backward_warp_module.backward_warp_grad(\n        grad, op.inputs[0], op.inputs[1])\n    return [None, grad0]\n\n\n@ops.RegisterGradient(""ForwardWarp"")\ndef _ForwardWarpGrad(op, grad):\n    grad0 = _forward_warp_module.forward_warp_grad(\n        grad, op.inputs[0])\n    return [grad0]\n\n\n@ops.RegisterGradient(""Correlation"")\ndef _CorrelationGrad(op, in_grad, in_grad1, in_grad2):\n    grad0, grad1 = _correlation_module.correlation_grad(\n        in_grad, op.inputs[0], op.inputs[1],\n        op.outputs[1], op.outputs[2],\n        kernel_size=op.get_attr(\'kernel_size\'),\n        max_displacement=op.get_attr(\'max_displacement\'),\n        pad=op.get_attr(\'pad\'),\n        stride_1=op.get_attr(\'stride_1\'),\n        stride_2=op.get_attr(\'stride_2\'))\n    return [grad0, grad1]\n\n\nops.NotDifferentiable(""Downsample"")\n'"
src/e2eflow/util.py,2,"b'import os\nimport subprocess\nimport configparser\nfrom shutil import rmtree\nimport tensorflow as tf\n\n\nCONFIG_PATH = \'../config.ini\'\n#TMP_DIR = \'/tmp/e2eflow\'\n\n\ndef upload_gdrive(upload_dir, gdrive_filename):\n    # search for file in gdrive and capture id if it already exists\n    lst_lines = subprocess.Popen([\'../scripts/gdrive\', \'list\'],\n                                 stdout=subprocess.PIPE)\n    existing_id = None\n    for line in lst_lines.stdout:\n        splits = line.split()\n        if str(splits[1], \'utf-8\') == gdrive_filename:\n            existing_id = str(splits[0], \'utf-8\')\n    tmp_path = os.path.join(\'/tmp\', gdrive_filename)\n    if os.path.isfile(tmp_path):\n        os.remove(tmp_path)\n    p = subprocess.Popen([\'/usr/bin/zip\', \'-r\', tmp_path, upload_dir])\n    p.wait()\n    if existing_id:\n        p = subprocess.Popen([\'../scripts/gdrive\', \'update\',\n                              existing_id, tmp_path])\n    else:\n        p = subprocess.Popen([\'../scripts/gdrive\', \'upload\',\n                              \'--name\', gdrive_filename,\n                               tmp_path])\n    p.wait()\n    os.remove(tmp_path)\n\n\ndef config_dict(config_path=CONFIG_PATH):\n    """"""Returns the config as dictionary,\n    where the elements have intuitively correct types.\n    """"""\n\n    config = configparser.ConfigParser()\n    config.read(config_path)\n\n    d = dict()\n    for section_key in config.sections():\n        sd = dict()\n        section = config[section_key]\n        for key in section:\n            val = section[key]\n            try:\n                sd[key] = int(val)\n            except ValueError:\n                try:\n                    sd[key] = float(val)\n                except ValueError:\n                    try:\n                        sd[key] = section.getboolean(key)\n                    except ValueError:\n                        sd[key] = val\n        d[section_key] = sd\n    return d\n\n\ndef convert_input_strings(config_dct, dirs):\n    if \'manual_decay_iters\' in config_dct and \'manual_decay_lrs\' in config_dct:\n        iters_lst = config_dct[\'manual_decay_iters\'].split(\',\')\n        lrs_lst = config_dct[\'manual_decay_lrs\'].split(\',\')\n        iters_lst = [int(i) for i in iters_lst]\n        lrs_lst = [float(l) for l in lrs_lst]\n        config_dct[\'manual_decay_iters\'] = iters_lst\n        config_dct[\'manual_decay_lrs\'] = lrs_lst\n        config_dct[\'num_iters\'] = sum(iters_lst)\n\n    if \'finetune\' in config_dct:\n        finetune = []\n        for name in config_dct[\'finetune\'].split("",""):\n            ckpt_dir = os.path.join(dirs[\'checkpoints\'], name)\n            ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n            if ckpt is None:\n              ckpt_dir = os.path.join(dirs[\'log\'], \'ex\', name)\n              ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n            assert ckpt, ""Could not load experiment "" + name\n            finetune.append(ckpt)\n        config_dct[\'finetune\'] = finetune\n\n\ndef tryremove(name, file=False):\n    try:\n        if file:\n            os.remove(name)\n        else:\n            rmtree(name)\n    except OSError:\n        pass\n'"
src/e2eflow/chairs/__init__.py,0,b''
src/e2eflow/chairs/data.py,0,"b""import os\nimport sys\nimport re\n\nimport numpy as np\nfrom PIL import Image\n\nfrom ..core.data import Data\nfrom ..util import tryremove\nfrom shutil import copyfile, rmtree\nfrom urllib.request import urlretrieve\n\n\nclass ChairsData(Data):\n    URL = 'http://lmb.informatik.uni-freiburg.de/data/FlyingChairs/FlyingChairs.zip'\n    TRAIN_VAL_URL = 'http://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs/FlyingChairs_train_val.txt'\n    dirs = ['flying_chairs']\n\n    def __init__(self, data_dir, stat_log_dir=None,\n                 development=True, fast_dir=None):\n        super().__init__(data_dir, stat_log_dir,\n                         development=development,\n                         fast_dir=fast_dir)\n\n    def _fetch_if_missing(self):\n        local_path = os.path.join(self.data_dir, 'flying_chairs')\n        train_val_path = os.path.join(local_path, 'FlyingChairs_train_val.txt')\n        if not os.path.isdir(local_path):\n            did_download = True\n            self._download_and_extract(self.URL, local_path)\n            urlretrieve(self.TRAIN_VAL_URL, train_val_path)\n        else:\n            did_download = False\n\n        data_path = os.path.join(local_path, 'FlyingChairs_release', 'data')\n        os.makedirs(os.path.join(local_path, 'image'), exist_ok=True)\n        os.makedirs(os.path.join(local_path, 'flow'), exist_ok=True)\n        os.makedirs(os.path.join(local_path, 'test_image'), exist_ok=True)\n\n        if os.path.isdir(data_path):\n            print('>> converting chairs data to .png')\n            train_val_repeated = []\n            train_val = []\n            with open(train_val_path) as f:\n                for line in f:\n                    training = int(line.strip()) == 1\n                    train_val_repeated.extend([training, training])\n                    train_val.extend([training])\n            # Convert .ppm to .png and split data into image and flow directory\n            im_files = [f for f in os.listdir(data_path) if\n                        re.match(r'[0-9]+.*\\.ppm', f)]\n            im_files.sort()\n            flow_files = [f for f in os.listdir(data_path) if\n                          re.match(r'[0-9]+.*\\.flo', f)]\n            flow_files.sort()\n            for t, f in zip(train_val_repeated, im_files):\n                name, ext = os.path.splitext(f)\n                path = os.path.join(data_path, f)\n\n                im = Image.open(path)\n                folder = 'image' if t else 'test_image'\n                im.save(os.path.join(local_path, folder, name + '.png'),\n                        'PNG')\n            for t, f in zip(train_val, flow_files):\n                path = os.path.join(data_path, f)\n                if not t:\n                    copyfile(path, os.path.join(local_path, 'flow', f))\n            if did_download:\n                rmtree(data_path)\n            print('>> processed chairs data')\n\n    def get_raw_dirs(self):\n       return [os.path.join(self.current_dir, 'flying_chairs', 'image')]\n"""
src/e2eflow/chairs/input.py,2,"b""import os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..core.input import Input\nfrom ..middlebury.input import _read_flow\n\n\nclass ChairsInput(Input):\n    def __init__(self, data, batch_size, dims, *,\n                 num_threads=1, normalize=True):\n        super().__init__(data, batch_size, dims, num_threads=num_threads,\n                         normalize=normalize)\n\n    def _preprocess_flow(self, t, channels):\n        height, width = self.dims\n        # Reshape to tell tensorflow we know the size statically\n        return tf.reshape(self._resize_crop_or_pad(t), [height, width, channels])\n\n    def _input_flow(self):\n        flow_dir = os.path.join(self.data.current_dir, 'flying_chairs/flow')\n        flow_files = [os.path.join(flow_dir, fn) for fn in sorted(os.listdir(flow_dir))]\n\n        flow, mask = _read_flow(flow_files, 1)\n        flow = self._preprocess_flow(flow, 2)\n        mask = self._preprocess_flow(mask, 1)\n        return flow, mask\n\n    def input_test(self):\n        input_shape, im1, im2 = self._input_images('flying_chairs/test_image')\n        flow, mask = self._input_flow()\n        return tf.train.batch(\n            [im1, im2, input_shape, flow, mask],\n            batch_size=self.batch_size,\n            num_threads=self.num_threads,\n            allow_smaller_final_batch=True)\n\n    def input_raw(self, swap_images=True, shift=0):\n        return super().input_raw(sequence=False,\n                                 swap_images=swap_images,\n                                 needs_crop=False,\n                                 shift=shift)\n"""
src/e2eflow/cityscapes/__init__.py,0,b''
src/e2eflow/cityscapes/data.py,0,"b'import os\nimport sys\n\nimport numpy as np\nimport matplotlib.image as mpimg\n\nfrom ..core.data import Data\nfrom ..util import tryremove\n\n\nclass CityscapesData(Data):\n    dirs = [\'cs\']\n\n    def __init__(self, data_dir, stat_log_dir=None,\n                 development=True, fast_dir=None):\n        super().__init__(data_dir, stat_log_dir,\n                         development=development,\n                         fast_dir=fast_dir)\n\n    def _fetch_if_missing(self):\n        pass\n\n    def get_raw_dirs(self):\n       top_dir = os.path.join(self.current_dir, \'cs\', \'leftImg8bit_sequence_trainvaltest\')\n       if not os.path.isdir(top_dir):\n         raise RuntimeError(\n             ""Cityscapes data missing.\\n""\n             ""Download \'leftImg8bit_sequence_trainvaltest.zip (324GB)\' ""\n             ""from https://www.cityscapes-dataset.com/ and store in <data_dir>/cs."")\n       dirs = []\n       splits = os.listdir(top_dir)\n       for split in splits:\n           split_path = os.path.join(top_dir, split)\n           cities = os.listdir(split_path)\n           for city in cities:\n               city_path = os.path.join(split_path, city)\n               dirs.append(city_path)\n       return dirs\n'"
src/e2eflow/core/__init__.py,0,b''
src/e2eflow/core/augment.py,38,"b'import numpy as np\nimport tensorflow as tf\n\nfrom .spatial_transformer import transformer\n\n\ndef random_affine(tensors, *,\n                  max_translation_x=0.0, max_translation_y=0.0,\n                  max_rotation=0.0, min_scale=1.0, max_scale=1.0,\n                  horizontal_flipping=False):\n    """"""Applies geometric augmentations to a list of tensors.\n\n    Each element in the list is augmented in the same way.\n    For all elements, num_batch must be equal while height, width and channels\n    may differ.\n    """"""\n    def _deg2rad(deg):\n        return (deg * np.pi) / 180.0\n\n    with tf.variable_scope(\'random_affine\'):\n        num_batch = tf.shape(tensors[0])[0]\n\n        zero = tf.zeros([num_batch])\n        one = tf.ones([num_batch])\n\n        tx = tf.random_uniform([num_batch], -max_translation_x, max_translation_x)\n        ty = tf.random_uniform([num_batch], -max_translation_y, max_translation_y)\n        rot = tf.random_uniform([num_batch], -max_rotation, max_rotation)\n        rad = _deg2rad(rot)\n        scale = tf.random_uniform([num_batch], min_scale, max_scale)\n\n        t1 = [[tf.cos(rad), -tf.sin(rad), tx],\n              [tf.sin(rad), tf.cos(rad), ty]]\n        t1 = tf.transpose(t1, [2, 0, 1])\n\n        scale_x = scale\n        if horizontal_flipping:\n            flip = tf.random_uniform([num_batch], 0, 1)\n            flip = tf.where(tf.greater(flip, 0.5), -one, one)\n            scale_x = scale_x * flip\n\n        t2 = [[scale_x, zero, zero],\n              [zero, scale, zero],\n              [zero, zero, one]]\n        t2 = tf.transpose(t2, [2, 0, 1])\n\n        t = tf.matmul(t1, t2)\n\n        out = []\n        for tensor in tensors:\n            shape = tf.shape(tensor)\n            tensor = transformer(tensor, t, (shape[1], shape[2]))\n            out.append(tf.stop_gradient(tensor))\n    return out\n\n\ndef random_photometric(ims, *,\n                       noise_stddev=0.0, min_contrast=0.0, max_contrast=0.0,\n                       brightness_stddev=0.0, min_colour=1.0, max_colour=1.0,\n                       min_gamma=1.0, max_gamma=1.0):\n    """"""Applies photometric augmentations to a list of image batches.\n\n    Each image in the list is augmented in the same way.\n    For all elements, num_batch must be equal while height and width may differ.\n\n    Args:\n        ims: list of 3-channel image batches normalized to [0, 1].\n        channel_mean: tensor of shape [3] which was used to normalize the pixel\n            values ranging from 0 ... 255.\n\n    Returns:\n        Batch of normalized images with photometric augmentations. Has the same\n        shape as the input batch.\n    """"""\n\n    with tf.variable_scope(\'random_photometric\'):\n        num_batch = tf.shape(ims[0])[0]\n\n        contrast = tf.random_uniform([num_batch, 1], min_contrast, max_contrast)\n        gamma = tf.random_uniform([num_batch, 1], min_gamma, max_gamma)\n        gamma_inv = 1.0 / gamma\n        colour = tf.random_uniform([num_batch, 3], min_colour, max_colour)\n        if noise_stddev > 0.0:\n            noise = tf.random_normal([num_batch, 1], stddev=noise_stddev)\n        else:\n            noise = tf.zeros([num_batch, 1])\n        if brightness_stddev > 0.0:\n            brightness = tf.random_normal([num_batch, 1],\n                                          stddev=brightness_stddev)\n        else:\n            brightness = tf.zeros([num_batch, 1])\n\n        out = []\n        for im in ims:\n            # Transpose to [height, width, num_batch, channels]\n            im_re = tf.transpose(im, [1, 2, 0, 3])\n            im_re = im_re\n            im_re = (im_re * (contrast + 1.0) + brightness) * colour\n            im_re = tf.maximum(0.0, tf.minimum(1.0, im_re))\n            im_re = tf.pow(im_re, gamma_inv)\n\n            im_re = im_re + noise\n\n            # Subtract the mean again after clamping\n            im_re = im_re\n\n            im = tf.transpose(im_re, [2, 0, 1, 3])\n            im = tf.stop_gradient(im)\n            out.append(im)\n        return out\n\n\ndef random_crop(tensors, size, seed=None, name=None):\n    """"""Randomly crops multiple tensors (of the same shape) to a given size.\n\n    Each tensor is cropped in the same way.""""""\n    with tf.name_scope(name, ""random_crop"", [size]) as name:\n        size = tf.convert_to_tensor(size, dtype=tf.int32, name=""size"")\n        if len(tensors) == 2:\n            shape = tf.minimum(tf.shape(tensors[0]), tf.shape(tensors[1]))\n        else:\n            shape = tf.shape(tensors[0])\n\n        limit = shape - size + 1\n        offset = tf.random_uniform(\n           tf.shape(shape),\n           dtype=size.dtype,\n           maxval=size.dtype.max,\n           seed=seed) % limit\n        results = []\n        for tensor in tensors:\n            result = tf.slice(tensor, offset, size)\n            results.append(result)\n        return results\n'"
src/e2eflow/core/data.py,0,"b'""""""Utility functions for providing data directories.""""""\nimport os\nimport sys\nimport zipfile\nimport rarfile\nfrom urllib.request import FancyURLopener\nimport shutil\n\nimport numpy as np\nimport matplotlib.image as mpimg\n\n\nclass Data():\n    # Should be a list containing all subdirectories of the main data dir which\n    # belong to this dataset\n    dirs = None\n\n    def __init__(self, data_dir, stat_log_dir,\n                 development=True, fast_dir=None):\n        self.development = development\n        self.data_dir = data_dir\n        self.stat_log_dir = stat_log_dir\n        if not os.path.isdir(data_dir):\n            os.makedirs(data_dir)\n\n        self._fetch_if_missing()\n\n        self.fast_dir = fast_dir\n        if fast_dir:\n            print("">> Copying files to {}"".format(fast_dir))\n            for d in self.dirs:\n                src = os.path.join(data_dir, d)\n                dst = os.path.join(fast_dir, d)\n                if not os.path.isdir(dst):\n                    shutil.copytree(src, dst)\n                    print("">> Copied {}"".format(d))\n            self.current_dir = fast_dir\n        else:\n            self.current_dir = data_dir\n\n        if stat_log_dir:\n            self.stat_log_file = os.path.join(stat_log_dir,\n                                              self.__class__.__name__ + "".txt"")\n            self._ensure_statistics()\n\n    def __del__(self):\n        pass\n        #if self.fast_dir:\n        #    print("">> Removing files from {}"".format(self.fast_dir))\n        #    for d in self.dirs:\n        #        shutil.rmtree(os.path.join(self.fast_dir, d))\n\n    def clear_statistics(self):\n        """"""Delete saved statistics file if present.""""""\n        if self.stat_log_dir and os.path.isfile(self.stat_log_file):\n            os.remove(self.stat_log_file)\n\n    def _ensure_statistics(self):\n        """"""Make sure we know the dataset statistics.""""""\n        if os.path.isfile(self.stat_log_file):\n            vals = np.loadtxt(self.stat_log_file)\n            self.mean = vals[0]\n            self.stddev = vals[1]\n        else:\n            print("">> Computing statistics (mean, variance) for {}""\n                  .format(self.__class__.__name__))\n            mean, stddev = self.compute_statistics(self.get_raw_files())\n            self.mean = mean\n            self.stddev = stddev\n            os.makedirs(self.stat_log_dir, exist_ok=True)\n            np.savetxt(self.stat_log_file, [mean, stddev])\n            print("">> Statistics complete"")\n\n    def get_raw_dirs(self):\n        """"""Should return a list of all dirs containing training images.\n\n        Note: self.current_dir should be used for loading input data.\n        """"""\n        raise NotImplementedError()\n\n    def get_raw_files(self):\n        files = []\n        for d in self.get_raw_dirs():\n            for path in os.listdir(d):\n                files.append(os.path.join(d, path))\n        return files\n\n    def _fetch_if_missing(self):\n        """"""A call to this must make subsequent calls to get_raw_files succeed.\n        All subdirs of data_dir listed in self.dirs must exist after this call.\n        """"""\n        raise NotImplementedError()\n\n    def _download_and_extract(self, url, extract_to, ext=\'zip\'):\n        def _progress(count, block_size, total_size):\n            if total_size > 0:\n                print(\'\\r>> Downloading %s %.1f%%\' % (url,\n                      float(count * block_size) / float(total_size) * 100.0), end=\' \')\n            else:\n                print(\'\\r>> Downloading %s\' % (url), end=\' \')\n            sys.stdout.flush()\n        urlretrieve = FancyURLopener().retrieve\n        local_zip_path = os.path.join(self.data_dir, \'tmp.\' + ext)\n        urlretrieve(url, local_zip_path, _progress)\n        sys.stdout.write(""\\n>> Finished downloading. Unzipping...\\n"")\n        if ext == \'zip\':\n            with zipfile.ZipFile(local_zip_path, ""r"") as zip_ref:\n                zip_ref.extractall(extract_to)\n        else:\n            with rarfile.RarFile(local_zip_path, ""r"") as zip_ref:\n                zip_ref.extractall(extract_to)\n\n        sys.stdout.write("">> Finished unzipping.\\n"")\n        os.remove(local_zip_path)\n\n        self.clear_statistics()\n\n    def compute_statistics(self, files):\n        """"""Use welford\'s method to compute mean and variance of the given\n        dataset.\n\n        See https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm.""""""\n\n        assert len(files) > 1\n\n        n = 0\n        mean = np.zeros(3)\n        M2 = np.zeros(3)\n        for j, filename in enumerate(files):\n            #TODO ensure the pixel values are 0..255\n            im = np.reshape(mpimg.imread(filename) * 255, [-1, 3])\n            for i in range(np.shape(im)[1]):\n                n = n + 1\n                delta = im[i] - mean\n                mean += delta / n\n                M2 += delta * (im[i] - mean)\n            sys.stdout.write(\'\\r>> Processed %.1f%%\' % (\n                float(j) / float(len(files)) * 100.0))\n            sys.stdout.flush()\n        var = M2 / (n - 1)\n        stddev = np.sqrt(var)\n        return np.float32(mean), np.float32(stddev)\n'"
src/e2eflow/core/flow_util.py,42,"b'import numpy as np\nimport tensorflow as tf\n\n\ndef atan2(y, x):\n    angle = tf.where(tf.greater(x,0.0), tf.atan(y/x), tf.zeros_like(x))\n    angle = tf.where(tf.logical_and(tf.less(x,0.0), tf.greater_equal(y,0.0)),\n                      tf.atan(y/x) + np.pi, angle)\n    angle = tf.where(tf.logical_and(tf.less(x,0.0), tf.less(y,0.0)),\n                      tf.atan(y/x) - np.pi, angle)\n    angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)),\n                      np.pi * tf.ones_like(x), angle)\n    angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)),\n                      -np.pi * tf.ones_like(x), angle)\n    angle = tf.where(tf.logical_and(tf.equal(x,0.0),tf.equal(y,0.0)),\n                      np.nan * tf.zeros_like(x), angle)\n    return angle\n\n\ndef flow_to_color(flow, mask=None, max_flow=None):\n    """"""Converts flow to 3-channel color image.\n\n    Args:\n        flow: tensor of shape [num_batch, height, width, 2].\n        mask: flow validity mask of shape [num_batch, height, width, 1].\n    """"""\n    n = 8\n    num_batch, height, width, _ = tf.unstack(tf.shape(flow))\n    mask = tf.ones([num_batch, height, width, 1]) if mask is None else mask\n    flow_u, flow_v = tf.unstack(flow, axis=3)\n    if max_flow is not None:\n        max_flow = tf.maximum(max_flow, 1)\n    else:\n        max_flow = tf.reduce_max(tf.abs(flow * mask))\n    mag = tf.sqrt(tf.reduce_sum(tf.square(flow), 3))\n    angle = atan2(flow_v, flow_u)\n\n    im_h = tf.mod(angle / (2 * np.pi) + 1.0, 1.0)\n    im_s = tf.clip_by_value(mag * n / max_flow, 0, 1)\n    im_v = tf.clip_by_value(n - im_s, 0, 1)\n    im_hsv = tf.stack([im_h, im_s, im_v], 3)\n    im = tf.image.hsv_to_rgb(im_hsv)\n    return im * mask\n\n\ndef flow_error_image(flow_1, flow_2, mask_occ, mask_noc=None, log_colors=True):\n    """"""Visualize the error between two flows as 3-channel color image.\n\n    Adapted from the KITTI C++ devkit.\n\n    Args:\n        flow_1: first flow of shape [num_batch, height, width, 2].\n        flow_2: second flow (ground truth)\n        mask_occ: flow validity mask of shape [num_batch, height, width, 1].\n            Equals 1 at (occluded and non-occluded) valid pixels.\n        mask_noc: Is 1 only at valid pixels which are not occluded.\n    """"""\n    mask_noc = tf.ones(tf.shape(mask_occ)) if mask_noc is None else mask_noc\n    diff_sq = (flow_1 - flow_2) ** 2\n    diff = tf.sqrt(tf.reduce_sum(diff_sq, [3], keepdims=True))\n    if log_colors:\n        num_batch, height, width, _ = tf.unstack(tf.shape(flow_1))\n        colormap = [\n            [0,0.0625,49,54,149],\n            [0.0625,0.125,69,117,180],\n            [0.125,0.25,116,173,209],\n            [0.25,0.5,171,217,233],\n            [0.5,1,224,243,248],\n            [1,2,254,224,144],\n            [2,4,253,174,97],\n            [4,8,244,109,67],\n            [8,16,215,48,39],\n            [16,1000000000.0,165,0,38]]\n        colormap = np.asarray(colormap, dtype=np.float32)\n        colormap[:, 2:5] = colormap[:, 2:5] / 255\n        mag = tf.sqrt(tf.reduce_sum(tf.square(flow_2), 3, keepdims=True))\n        error = tf.minimum(diff / 3, 20 * diff / mag)\n        im = tf.zeros([num_batch, height, width, 3])\n        for i in range(colormap.shape[0]):\n            colors = colormap[i, :]\n            cond = tf.logical_and(tf.greater_equal(error, colors[0]),\n                                  tf.less(error, colors[1]))\n            im = tf.where(tf.tile(cond, [1, 1, 1, 3]),\n                           tf.ones([num_batch, height, width, 1]) * colors[2:5],\n                           im)\n        im = tf.where(tf.tile(tf.cast(mask_noc, tf.bool), [1, 1, 1, 3]),\n                       im, im * 0.5)\n        im = im * mask_occ\n    else:\n        error = (tf.minimum(diff, 5) / 5) * mask_occ\n        im_r = error # errors in occluded areas will be red\n        im_g = error * mask_noc\n        im_b = error * mask_noc\n        im = tf.concat(axis=3, values=[im_r, im_g, im_b])\n    return im\n\n\ndef flow_error_avg(flow_1, flow_2, mask):\n    """"""Evaluates the average endpoint error between flow batches.""""""\n    with tf.variable_scope(\'flow_error_avg\'):\n        diff = euclidean(flow_1 - flow_2) * mask\n        error = tf.reduce_sum(diff) / tf.reduce_sum(mask)\n        return error\n\n\ndef outlier_ratio(gt_flow, flow, mask, threshold=3.0, relative=0.05):\n    diff = euclidean(gt_flow - flow) * mask\n    if relative is not None:\n        threshold = tf.maximum(threshold, euclidean(gt_flow) * relative)\n        outliers = tf.cast(tf.greater_equal(diff, threshold), tf.float32)\n    else:\n        outliers = tf.cast(tf.greater_equal(diff, threshold), tf.float32)\n    ratio = tf.reduce_sum(outliers) / tf.reduce_sum(mask)\n    return ratio\n\n\ndef outlier_pct(gt_flow, flow, mask, threshold=3.0, relative=0.05):\n    frac = outlier_ratio(gt_flow, flow, mask, threshold, relative) * 100\n    return frac\n\n\ndef euclidean(t):\n    return tf.sqrt(tf.reduce_sum(t ** 2, [3], keepdims=True))\n'"
src/e2eflow/core/flownet.py,25,"b'import tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport tensorflow.contrib.layers as layers\n\nfrom ..ops import correlation\nfrom .image_warp import image_warp\n\nfrom .flow_util import flow_to_color\n\n\nFLOW_SCALE = 5.0\n\n\ndef flownet(im1, im2, flownet_spec=\'S\', full_resolution=False, train_all=False,\n            backward_flow=False):\n    num_batch, height, width, _ = tf.unstack(tf.shape(im1))\n    flownet_num = len(flownet_spec)\n    assert flownet_num > 0\n    flows_fw = []\n    flows_bw = []\n    for i, name in enumerate(flownet_spec):\n        assert name in (\'C\', \'c\', \'S\', \'s\')\n        channel_mult = 1 if name in (\'C\', \'S\') else 3 / 8\n        full_res = full_resolution and i == flownet_num - 1\n\n        def scoped_block():\n            if name.lower() == \'c\':\n                assert i == 0, \'FlowNetS must be used for refinement networks\'\n\n                with tf.variable_scope(\'flownet_c_features\'):\n                    _, conv2_a, conv3_a = flownet_c_features(im1, channel_mult=channel_mult)\n                    _, conv2_b, conv3_b = flownet_c_features(im2, channel_mult=channel_mult, reuse=True)\n\n                with tf.variable_scope(\'flownet_c\') as scope:\n                    flow_fw = flownet_c(conv3_a, conv3_b, conv2_a,\n                                        full_res=full_res,\n                                        channel_mult=channel_mult)\n                    flows_fw.append(flow_fw)\n                    if backward_flow:\n                        scope.reuse_variables()\n                        flow_bw = flownet_c(conv3_b, conv3_a, conv2_b,\n                                            full_res=full_res,\n                                            channel_mult=channel_mult)\n                        flows_bw.append(flow_bw)\n            elif name.lower() == \'s\':\n                def _flownet_s(im1, im2, flow=None):\n                    if flow is not None:\n                        flow = tf.image.resize_bilinear(flow, [height, width]) * 4 * FLOW_SCALE\n                        warp = image_warp(im2, flow)\n                        diff = tf.abs(warp - im1)\n                        if not train_all:\n                            flow = tf.stop_gradient(flow)\n                            warp = tf.stop_gradient(warp)\n                            diff = tf.stop_gradient(diff)\n\n                        inputs = tf.concat([im1, im2, flow, warp, diff], axis=3)\n                        inputs = tf.reshape(inputs, [num_batch, height, width, 14])\n                    else:\n                        inputs = tf.concat([im1, im2], 3)\n                    return flownet_s(inputs,\n                                     full_res=full_res,\n                                     channel_mult=channel_mult)\n                stacked = len(flows_fw) > 0\n                with tf.variable_scope(\'flownet_s\') as scope:\n                    flow_fw = _flownet_s(im1, im2, flows_fw[-1][0] if stacked else None)\n                    flows_fw.append(flow_fw)\n                    if backward_flow:\n                        scope.reuse_variables()\n                        flow_bw = _flownet_s(im2, im1, flows_bw[-1][0]  if stacked else None)\n                        flows_bw.append(flow_bw)\n\n        if i > 0:\n            scope_name = ""stack_{}_flownet"".format(i)\n            with tf.variable_scope(scope_name):\n                scoped_block()\n        else:\n            scoped_block()\n\n    if backward_flow:\n        return flows_fw, flows_bw\n    return flows_fw\n\n\ndef _leaky_relu(x):\n    with tf.variable_scope(\'leaky_relu\'):\n        return tf.maximum(0.1 * x, x)\n\n\ndef _flownet_upconv(conv6_1, conv5_1, conv4_1, conv3_1, conv2, conv1=None, inputs=None,\n                    channel_mult=1, full_res=False, channels=2):\n    m = channel_mult\n\n    flow6 = slim.conv2d(conv6_1, channels, 3, scope=\'flow6\',\n                        activation_fn=None)\n    deconv5 = slim.conv2d_transpose(conv6_1, int(512 * m), 4, stride=2,\n                                   scope=\'deconv5\')\n    flow6_up5 = slim.conv2d_transpose(flow6, channels, 4, stride=2,\n                                     scope=\'flow6_up5\',\n                                     activation_fn=None)\n    concat5 = tf.concat([conv5_1, deconv5, flow6_up5], 1)\n    flow5 = slim.conv2d(concat5, channels, 3, scope=\'flow5\',\n                       activation_fn=None)\n\n    deconv4 = slim.conv2d_transpose(concat5, int(256 * m), 4, stride=2,\n                                   scope=\'deconv4\')\n    flow5_up4 = slim.conv2d_transpose(flow5, channels, 4, stride=2,\n                                     scope=\'flow5_up4\',\n                                     activation_fn=None)\n    concat4 = tf.concat([conv4_1, deconv4, flow5_up4], 1)\n    flow4 = slim.conv2d(concat4, channels, 3, scope=\'flow4\',\n                       activation_fn=None)\n\n    deconv3 = slim.conv2d_transpose(concat4, int(128 * m), 4, stride=2,\n                                   scope=\'deconv3\')\n    flow4_up3 = slim.conv2d_transpose(flow4, channels, 4, stride=2,\n                                     scope=\'flow4_up3\',\n                                     activation_fn=None)\n    concat3 = tf.concat([conv3_1, deconv3, flow4_up3], 1)\n    flow3 = slim.conv2d(concat3, channels, 3, scope=\'flow3\',\n                       activation_fn=None)\n\n    deconv2 = slim.conv2d_transpose(concat3, int(64 * m), 4, stride=2,\n                                   scope=\'deconv2\')\n    flow3_up2 = slim.conv2d_transpose(flow3, channels, 4, stride=2,\n                                     scope=\'flow3_up2\',\n                                     activation_fn=None)\n    concat2 = tf.concat([conv2, deconv2, flow3_up2], 1)\n    flow2 = slim.conv2d(concat2, channels, 3, scope=\'flow2\',\n                       activation_fn=None)\n\n    flows = [flow2, flow3, flow4, flow5, flow6]\n\n    if full_res:\n        with tf.variable_scope(\'full_res\'):\n            deconv1 = slim.conv2d_transpose(concat2, int(32 * m), 4, stride=2,\n                                           scope=\'deconv1\')\n            flow2_up1 = slim.conv2d_transpose(flow2, channels, 4, stride=2,\n                                             scope=\'flow2_up1\',\n                                             activation_fn=None)\n            concat1 = tf.concat([conv1, deconv1, flow2_up1], 1)\n            flow1 = slim.conv2d(concat1, channels, 3, scope=\'flow1\',\n                                activation_fn=None)\n\n            deconv0 = slim.conv2d_transpose(concat1, int(16 * m), 4, stride=2,\n                                           scope=\'deconv0\')\n            flow1_up0 = slim.conv2d_transpose(flow1, channels, 4, stride=2,\n                                             scope=\'flow1_up0\',\n                                             activation_fn=None)\n            concat0 = tf.concat([inputs, deconv0, flow1_up0], 1)\n            flow0 = slim.conv2d(concat0, channels, 3, scope=\'flow0\',\n                                activation_fn=None)\n\n            flows = [flow0, flow1] + flows\n\n    return flows\n\n\ndef nhwc_to_nchw(tensors):\n    return [tf.transpose(t, [0, 3, 1, 2]) for t in tensors]\n\n\ndef nchw_to_nhwc(tensors):\n    return [tf.transpose(t, [0, 2, 3, 1]) for t in tensors]\n\n\ndef flownet_s(inputs, channel_mult=1, full_res=False):\n    """"""Given stacked inputs, returns flow predictions in decreasing resolution.\n\n    Uses FlowNetSimple.\n    """"""\n    m = channel_mult\n    inputs = nhwc_to_nchw([inputs])[0]\n\n    with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n                        data_format=\'NCHW\',\n                        weights_regularizer=slim.l2_regularizer(0.0004),\n                        weights_initializer=layers.variance_scaling_initializer(),\n                        activation_fn=_leaky_relu):\n        conv1 = slim.conv2d(inputs, int(64 * m), 7, stride=2, scope=\'conv1\')\n        conv2 = slim.conv2d(conv1, int(128 * m), 5, stride=2, scope=\'conv2\')\n        conv3 = slim.conv2d(conv2, int(256 * m), 5, stride=2, scope=\'conv3\')\n        conv3_1 = slim.conv2d(conv3, int(256 * m), 3, stride=1, scope=\'conv3_1\')\n        conv4 = slim.conv2d(conv3_1, int(512 * m), 3, stride=2, scope=\'conv4\')\n        conv4_1 = slim.conv2d(conv4, int(512 * m), 3, stride=1, scope=\'conv4_1\')\n        conv5 = slim.conv2d(conv4_1, int(512 * m), 3, stride=2, scope=\'conv5\')\n        conv5_1 = slim.conv2d(conv5, int(512 * m), 3, stride=1, scope=\'conv5_1\')\n        conv6 = slim.conv2d(conv5_1, int(1024 * m), 3, stride=2, scope=\'conv6\')\n        conv6_1 = slim.conv2d(conv6, int(1024 * m), 3, stride=1, scope=\'conv6_1\')\n\n        res = _flownet_upconv(conv6_1, conv5_1, conv4_1, conv3_1, conv2, conv1, inputs,\n                              channel_mult=channel_mult, full_res=full_res)\n        return nchw_to_nhwc(res)\n\n\ndef flownet_c_features(im, channel_mult=1, reuse=None):\n    m = channel_mult\n    im = nhwc_to_nchw([im])[0]\n    with slim.arg_scope([slim.conv2d],\n                        data_format=\'NCHW\',\n                        weights_regularizer=slim.l2_regularizer(0.0004),\n                        weights_initializer=layers.variance_scaling_initializer(),\n                        activation_fn=_leaky_relu):\n        conv1 = slim.conv2d(im, int(64 * m), 7, stride=2, scope=\'conv1\', reuse=reuse)\n        conv2 = slim.conv2d(conv1, int(128 * m), 5, stride=2, scope=\'conv2\', reuse=reuse)\n        conv3 = slim.conv2d(conv2, int(256 * m), 5, stride=2, scope=\'conv3\', reuse=reuse)\n        return conv1, conv2, conv3\n\n\ndef flownet_c(conv3_a, conv3_b, conv2_a, channel_mult=1, full_res=False):\n    """"""Given two images, returns flow predictions in decreasing resolution.\n\n    Uses FlowNetCorr.\n    """"""\n    m = channel_mult\n\n    with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n                        data_format=\'NCHW\',\n                        weights_regularizer=slim.l2_regularizer(0.0004),\n                        weights_initializer=layers.variance_scaling_initializer(),\n                        activation_fn=_leaky_relu):\n        corr = correlation(conv3_a, conv3_b,\n                           pad=20, kernel_size=1, max_displacement=20, stride_1=1, stride_2=2)\n\n        conv_redir = slim.conv2d(conv3_a, int(32 * m), 1, stride=1, scope=\'conv_redir\')\n\n        conv3_1 = slim.conv2d(tf.concat([conv_redir, corr], 1), int(256 * m), 3,\n                              stride=1, scope=\'conv3_1\')\n        conv4 = slim.conv2d(conv3_1, int(512 * m), 3, stride=2, scope=\'conv4\')\n        conv4_1 = slim.conv2d(conv4, int(512 * m), 3, stride=1, scope=\'conv4_1\')\n        conv5 = slim.conv2d(conv4_1, int(512 * m), 3, stride=2, scope=\'conv5\')\n        conv5_1 = slim.conv2d(conv5, int(512 * m), 3, stride=1, scope=\'conv5_1\')\n        conv6 = slim.conv2d(conv5_1, int(1024 * m), 3, stride=2, scope=\'conv6\')\n        conv6_1 = slim.conv2d(conv6, int(1024 * m), 3, stride=1, scope=\'conv6_1\')\n\n        res = _flownet_upconv(conv6_1, conv5_1, conv4_1, conv3_1, conv2_a,\n                              channel_mult=channel_mult, full_res=full_res)\n        return nchw_to_nhwc(res)\n'"
src/e2eflow/core/image_warp.py,29,"b'import tensorflow as tf\n\n\ndef image_warp(im, flow):\n    """"""Performs a backward warp of an image using the predicted flow.\n\n    Args:\n        im: Batch of images. [num_batch, height, width, channels]\n        flow: Batch of flow vectors. [num_batch, height, width, 2]\n    Returns:\n        warped: transformed image of the same shape as the input image.\n    """"""\n    with tf.variable_scope(\'image_warp\'):\n\n        num_batch, height, width, channels = tf.unstack(tf.shape(im))\n        max_x = tf.cast(width - 1, \'int32\')\n        max_y = tf.cast(height - 1, \'int32\')\n        zero = tf.zeros([], dtype=\'int32\')\n\n        # We have to flatten our tensors to vectorize the interpolation\n        im_flat = tf.reshape(im, [-1, channels])\n        flow_flat = tf.reshape(flow, [-1, 2])\n\n        # Floor the flow, as the final indices are integers\n        # The fractional part is used to control the bilinear interpolation.\n        flow_floor = tf.to_int32(tf.floor(flow_flat))\n        bilinear_weights = flow_flat - tf.floor(flow_flat)\n\n        # Construct base indices which are displaced with the flow\n        pos_x = tf.tile(tf.range(width), [height * num_batch])\n        grid_y = tf.tile(tf.expand_dims(tf.range(height), 1), [1, width])\n        pos_y = tf.tile(tf.reshape(grid_y, [-1]), [num_batch])\n\n        x = flow_floor[:, 0]\n        y = flow_floor[:, 1]\n        xw = bilinear_weights[:, 0]\n        yw = bilinear_weights[:, 1]\n\n        # Compute interpolation weights for 4 adjacent pixels\n        # expand to num_batch * height * width x 1 for broadcasting in add_n below\n        wa = tf.expand_dims((1 - xw) * (1 - yw), 1) # top left pixel\n        wb = tf.expand_dims((1 - xw) * yw, 1) # bottom left pixel\n        wc = tf.expand_dims(xw * (1 - yw), 1) # top right pixel\n        wd = tf.expand_dims(xw * yw, 1) # bottom right pixel\n\n        x0 = pos_x + x\n        x1 = x0 + 1\n        y0 = pos_y + y\n        y1 = y0 + 1\n\n        x0 = tf.clip_by_value(x0, zero, max_x)\n        x1 = tf.clip_by_value(x1, zero, max_x)\n        y0 = tf.clip_by_value(y0, zero, max_y)\n        y1 = tf.clip_by_value(y1, zero, max_y)\n\n        dim1 = width * height\n        batch_offsets = tf.range(num_batch) * dim1\n        base_grid = tf.tile(tf.expand_dims(batch_offsets, 1), [1, dim1])\n        base = tf.reshape(base_grid, [-1])\n\n        base_y0 = base + y0 * width\n        base_y1 = base + y1 * width\n        idx_a = base_y0 + x0\n        idx_b = base_y1 + x0\n        idx_c = base_y0 + x1\n        idx_d = base_y1 + x1\n\n        Ia = tf.gather(im_flat, idx_a)\n        Ib = tf.gather(im_flat, idx_b)\n        Ic = tf.gather(im_flat, idx_c)\n        Id = tf.gather(im_flat, idx_d)\n\n        warped_flat = tf.add_n([wa * Ia, wb * Ib, wc * Ic, wd * Id])\n        warped = tf.reshape(warped_flat, [num_batch, height, width, channels])\n\n        return warped\n'"
src/e2eflow/core/input.py,29,"b'import os\nimport random\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom .augment import random_crop\n\n\ndef resize_input(t, height, width, resized_h, resized_w):\n    # Undo old resizing and apply bilinear\n    t = tf.reshape(t, [resized_h, resized_w, 3])\n    t = tf.expand_dims(tf.image.resize_image_with_crop_or_pad(t, height, width), 0)\n    return tf.image.resize_bilinear(t, [resized_h, resized_w])\n\n\ndef resize_output_crop(t, height, width, channels):\n    _, oldh, oldw, c = tf.unstack(tf.shape(t))\n    t = tf.reshape(t, [oldh, oldw, c])\n    t = tf.image.resize_image_with_crop_or_pad(t, height, width)\n    return tf.reshape(t, [1, height, width, channels])\n\n\ndef resize_output(t, height, width, channels):\n    return tf.image.resize_bilinear(t, [height, width])\n\n\ndef resize_output_flow(t, height, width, channels):\n    batch, old_height, old_width, _ = tf.unstack(tf.shape(t), num=4)\n    t = tf.image.resize_bilinear(t, [height, width])\n    u, v = tf.unstack(t, axis=3)\n    u *= tf.cast(width, tf.float32) / tf.cast(old_width, tf.float32)\n    v *= tf.cast(height, tf.float32) / tf.cast(old_height, tf.float32)\n    return tf.reshape(tf.stack([u, v], axis=3), [batch, height, width, 2])\n\n\ndef frame_name_to_num(name):\n    stripped = name.split(\'.\')[0].lstrip(\'0\')\n    if stripped == \'\':\n        return 0\n    return int(stripped)\n\n\nclass Input():\n    mean = [104.920005, 110.1753, 114.785955]\n    stddev = 1 / 0.0039216\n\n    def __init__(self, data, batch_size, dims, *,\n                 num_threads=1, normalize=True,\n                 skipped_frames=False):\n        assert len(dims) == 2\n        self.data = data\n        self.dims = dims\n        self.batch_size = batch_size\n        self.num_threads = num_threads\n        self.normalize = normalize\n        self.skipped_frames = skipped_frames\n\n    def _resize_crop_or_pad(self, tensor):\n        height, width = self.dims\n        # return tf.image.resize_bilinear(tf.expand_dims(tensor, 0), [height, width])\n        return tf.image.resize_image_with_crop_or_pad(tensor, height, width)\n\n    def _resize_image_fixed(self, image):\n        height, width = self.dims\n        return tf.reshape(self._resize_crop_or_pad(image), [height, width, 3])\n\n    def _normalize_image(self, image):\n        return (image - self.mean) / self.stddev\n\n    def _preprocess_image(self, image):\n        image = self._resize_image_fixed(image)\n        if self.normalize:\n            image = self._normalize_image(image)\n        return image\n\n    def _input_images(self, image_dir, hold_out_inv=None):\n        """"""Assumes that paired images are next to each other after ordering the\n        files.\n        """"""\n        image_dir = os.path.join(self.data.current_dir, image_dir)\n\n        filenames_1 = []\n        filenames_2 = []\n        image_files = os.listdir(image_dir)\n        image_files.sort()\n\n        assert len(image_files) % 2 == 0, \'expected pairs of images\'\n\n        for i in range(len(image_files) // 2):\n            filenames_1.append(os.path.join(image_dir, image_files[i * 2]))\n            filenames_2.append(os.path.join(image_dir, image_files[i * 2 + 1]))\n\n        if hold_out_inv is not None:\n            filenames = list(zip(filenames_1, filenames_2))\n            random.seed(0)\n            random.shuffle(filenames)\n            filenames = filenames[:hold_out_inv]\n\n            filenames_1, filenames_2 = zip(*filenames)\n            filenames_1 = list(filenames_1)\n            filenames_2 = list(filenames_2)\n\n        input_1 = read_png_image(filenames_1, 1)\n        input_2 = read_png_image(filenames_2, 1)\n        image_1 = self._preprocess_image(input_1)\n        image_2 = self._preprocess_image(input_2)\n        return tf.shape(input_1), image_1, image_2\n\n    def _input_test(self, image_dir, hold_out_inv=None):\n        input_shape, im1, im2 = self._input_images(image_dir, hold_out_inv)\n        return tf.train.batch(\n            [im1, im2, input_shape],\n            batch_size=self.batch_size,\n            num_threads=self.num_threads,\n            allow_smaller_final_batch=True)\n\n    def get_normalization(self):\n        return self.mean, self.stddev\n\n    def input_raw(self, swap_images=True, sequence=True,\n                  needs_crop=True, shift=0, seed=0,\n                  center_crop=False, skip=0):\n        """"""Constructs input of raw data.\n\n        Args:\n            sequence: Assumes that image file order in data_dirs corresponds to\n                temporal order, if True. Otherwise, assumes uncorrelated pairs of\n                images in lexicographical ordering.\n            shift: number of examples to shift the input queue by.\n                Useful to resume training.\n            swap_images: for each pair (im1, im2), also include (im2, im1)\n            seed: seed for filename shuffling.\n        Returns:\n            image_1: batch of first images\n            image_2: batch of second images\n        """"""\n        if not isinstance(skip, list):\n            skip = [skip]\n\n        data_dirs = self.data.get_raw_dirs()\n        height, width = self.dims\n        #assert batch_size % 2 == 0\n\n        filenames = []\n        for dir_path in data_dirs:\n            files = os.listdir(dir_path)\n            files.sort()\n            if sequence:\n                steps = [1 + s for s in skip]\n                stops = [len(files) - s for s in steps]\n            else:\n                steps = [2]\n                stops = [len(files)]\n                assert len(files) % 2 == 0\n            for step, stop in zip(steps, stops):\n                for i in range(0, stop, step):\n                    if self.skipped_frames and sequence:\n                        assert step == 1\n                        num_first = frame_name_to_num(files[i])\n                        num_second = frame_name_to_num(files[i+1])\n                        if num_first + 1 != num_second:\n                            continue\n                    fn1 = os.path.join(dir_path, files[i])\n                    fn2 = os.path.join(dir_path, files[i + 1])\n                    filenames.append((fn1, fn2))\n\n        random.seed(seed)\n        random.shuffle(filenames)\n        print(""Training on {} frame pairs."".format(len(filenames)))\n\n        filenames_extended = []\n        for fn1, fn2 in filenames:\n            filenames_extended.append((fn1, fn2))\n            if swap_images:\n                filenames_extended.append((fn2, fn1))\n\n        shift = shift % len(filenames_extended)\n        filenames_extended = list(np.roll(filenames_extended, shift))\n\n\n        filenames_1, filenames_2 = zip(*filenames_extended)\n        filenames_1 = list(filenames_1)\n        filenames_2 = list(filenames_2)\n\n        with tf.variable_scope(\'train_inputs\'):\n            image_1 = read_png_image(filenames_1)\n            image_2 = read_png_image(filenames_2)\n\n            if needs_crop:\n                #if center_crop:\n                #    image_1 = tf.image.resize_image_with_crop_or_pad(image_1, height, width)\n                #    image_2 = tf.image.resize_image_with_crop_or_pad(image_1, height, width)\n                #else:\n                image_1, image_2 = random_crop([image_1, image_2], [height, width, 3])\n            else:\n                image_1 = tf.reshape(image_1, [height, width, 3])\n                image_2 = tf.reshape(image_2, [height, width, 3])\n\n            if self.normalize:\n                image_1 = self._normalize_image(image_1)\n                image_2 = self._normalize_image(image_2)\n\n            return tf.train.batch(\n                [image_1, image_2],\n                batch_size=self.batch_size,\n                num_threads=self.num_threads)\n\n\ndef read_png_image(filenames, num_epochs=None):\n    """"""Given a list of filenames, constructs a reader op for images.""""""\n    filename_queue = tf.train.string_input_producer(filenames,\n        shuffle=False, capacity=len(filenames))\n    reader = tf.WholeFileReader()\n    _, value = reader.read(filename_queue)\n    image_uint8 = tf.image.decode_png(value, channels=3)\n    image = tf.cast(image_uint8, tf.float32)\n    return image\n'"
src/e2eflow/core/losses.py,75,"b'import tensorflow as tf\nimport numpy as np\nfrom tensorflow.contrib.distributions import Normal\n\nfrom ..ops import backward_warp, forward_warp\nfrom .image_warp import image_warp\n\n\nDISOCC_THRESH = 0.8\n\n\ndef length_sq(x):\n    return tf.reduce_sum(tf.square(x), 3, keepdims=True)\n\n\ndef compute_losses(im1, im2, flow_fw, flow_bw,\n                   border_mask=None,\n                   mask_occlusion=\'\',\n                   data_max_distance=1):\n    losses = {}\n\n    im2_warped = image_warp(im2, flow_fw)\n    im1_warped = image_warp(im1, flow_bw)\n\n    im_diff_fw = im1 - im2_warped\n    im_diff_bw = im2 - im1_warped\n\n    disocc_fw = tf.cast(forward_warp(flow_fw) < DISOCC_THRESH, tf.float32)\n    disocc_bw = tf.cast(forward_warp(flow_bw) < DISOCC_THRESH, tf.float32)\n\n    if border_mask is None:\n        mask_fw = create_outgoing_mask(flow_fw)\n        mask_bw = create_outgoing_mask(flow_bw)\n    else:\n        mask_fw = border_mask\n        mask_bw = border_mask\n\n    flow_bw_warped = image_warp(flow_bw, flow_fw)\n    flow_fw_warped = image_warp(flow_fw, flow_bw)\n    flow_diff_fw = flow_fw + flow_bw_warped\n    flow_diff_bw = flow_bw + flow_fw_warped\n    \n    mag_sq_fw = length_sq(flow_fw) + length_sq(flow_bw_warped) \n    mag_sq_bw = length_sq(flow_bw) + length_sq(flow_fw_warped)\n    occ_thresh_fw =  0.01 * mag_sq_fw + 0.5\n    occ_thresh_bw =  0.01 * mag_sq_bw + 0.5\n    \n    fb_occ_fw = tf.cast(length_sq(flow_diff_fw) > occ_thresh_fw, tf.float32)\n    fb_occ_bw = tf.cast(length_sq(flow_diff_bw) > occ_thresh_bw, tf.float32)\n\n    if mask_occlusion == \'fb\':\n        mask_fw *= (1 - fb_occ_fw)\n        mask_bw *= (1 - fb_occ_bw)\n    elif mask_occlusion == \'disocc\':\n        mask_fw *= (1 - disocc_bw)\n        mask_bw *= (1 - disocc_fw)\n\n    occ_fw = 1 - mask_fw\n    occ_bw = 1 - mask_bw\n\n    losses[\'sym\'] = (charbonnier_loss(occ_fw - disocc_bw) +\n                     charbonnier_loss(occ_bw - disocc_fw))\n\n    losses[\'occ\'] = (charbonnier_loss(occ_fw) +\n                     charbonnier_loss(occ_bw))\n\n    losses[\'photo\'] =  (photometric_loss(im_diff_fw, mask_fw) +\n                        photometric_loss(im_diff_bw, mask_bw))\n\n    losses[\'grad\'] = (gradient_loss(im1, im2_warped, mask_fw) +\n                      gradient_loss(im2, im1_warped, mask_bw))\n\n    losses[\'smooth_1st\'] = (smoothness_loss(flow_fw) +\n                            smoothness_loss(flow_bw))\n\n    losses[\'smooth_2nd\'] = (second_order_loss(flow_fw) +\n                            second_order_loss(flow_bw))\n\n    losses[\'fb\'] = (charbonnier_loss(flow_diff_fw, mask_fw) +\n                    charbonnier_loss(flow_diff_bw, mask_bw))\n\n    losses[\'ternary\'] = (ternary_loss(im1, im2_warped, mask_fw,\n                                      max_distance=data_max_distance) +\n                         ternary_loss(im2, im1_warped, mask_bw,\n                                      max_distance=data_max_distance))\n\n    return losses\n\n\ndef ternary_loss(im1, im2_warped, mask, max_distance=1):\n    patch_size = 2 * max_distance + 1\n    with tf.variable_scope(\'ternary_loss\'):\n        def _ternary_transform(image):\n            intensities = tf.image.rgb_to_grayscale(image) * 255\n            #patches = tf.extract_image_patches( # fix rows_in is None\n            #    intensities,\n            #    ksizes=[1, patch_size, patch_size, 1],\n            #    strides=[1, 1, 1, 1],\n            #    rates=[1, 1, 1, 1],\n            #    padding=\'SAME\')\n            out_channels = patch_size * patch_size\n            w = np.eye(out_channels).reshape((patch_size, patch_size, 1, out_channels))\n            weights =  tf.constant(w, dtype=tf.float32)\n            patches = tf.nn.conv2d(intensities, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n\n            transf = patches - intensities\n            transf_norm = transf / tf.sqrt(0.81 + tf.square(transf))\n            return transf_norm\n\n        def _hamming_distance(t1, t2):\n            dist = tf.square(t1 - t2)\n            dist_norm = dist / (0.1 + dist)\n            dist_sum = tf.reduce_sum(dist_norm, 3, keepdims=True)\n            return dist_sum\n\n        t1 = _ternary_transform(im1)\n        t2 = _ternary_transform(im2_warped)\n        dist = _hamming_distance(t1, t2)\n\n        transform_mask = create_mask(mask, [[max_distance, max_distance],\n                                            [max_distance, max_distance]])\n        return charbonnier_loss(dist, mask * transform_mask)\n\n\ndef occlusion(flow_fw, flow_bw):\n    mag_sq = length_sq(flow_fw) + length_sq(flow_bw)\n    flow_bw_warped = image_warp(flow_bw, flow_fw)\n    flow_fw_warped = image_warp(flow_fw, flow_bw)\n    flow_diff_fw = flow_fw + flow_bw_warped\n    flow_diff_bw = flow_bw + flow_fw_warped\n    occ_thresh =  0.01 * mag_sq + 0.5\n    occ_fw = tf.cast(length_sq(flow_diff_fw) > occ_thresh, tf.float32)\n    occ_bw = tf.cast(length_sq(flow_diff_bw) > occ_thresh, tf.float32)\n    return occ_fw, occ_bw\n\n\n#def disocclusion(div):\n#    """"""Creates binary disocclusion map based on flow divergence.""""""\n#    return tf.round(norm(tf.maximum(0.0, div), 0.3))\n\n\n#def occlusion(im_diff, div):\n#    """"""Creates occlusion map based on warping error & flow divergence.""""""\n#    gray_diff = tf.image.rgb_to_grayscale(im_diff)\n#    return 1 - norm(gray_diff, 20.0 / 255) * norm(tf.minimum(0.0, div), 0.3)\n\n\ndef divergence(flow):\n    with tf.variable_scope(\'divergence\'):\n        filter_x = [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]] # sobel filter\n        filter_y = np.transpose(filter_x)\n        weight_array_x = np.zeros([3, 3, 1, 1])\n        weight_array_x[:, :, 0, 0] = filter_x\n        weights_x = tf.constant(weight_array_x, dtype=tf.float32)\n        weight_array_y = np.zeros([3, 3, 1, 1])\n        weight_array_y[:, :, 0, 0] = filter_y\n        weights_y = tf.constant(weight_array_y, dtype=tf.float32)\n        flow_u, flow_v = tf.split(axis=3, num_or_size_splits=2, value=flow)\n        grad_x = conv2d(flow_u, weights_x)\n        grad_y = conv2d(flow_v, weights_y)\n        div = tf.reduce_sum(tf.concat(axis=3, values=[grad_x, grad_y]), 3, keepdims=True)\n        return div\n\n\ndef norm(x, sigma):\n    """"""Gaussian decay.\n    Result is 1.0 for x = 0 and decays towards 0 for |x > sigma.\n    """"""\n    dist = Normal(0.0, sigma)\n    return dist.pdf(x) / dist.pdf(0.0)\n\n\ndef diffusion_loss(flow, im, occ):\n    """"""Forces diffusion weighted by motion, intensity and occlusion label similarity.\n    Inspired by Bilateral Flow Filtering.\n    """"""\n    def neighbor_diff(x, num_in=1):\n        weights = np.zeros([3, 3, num_in, 8 * num_in])\n        out_channel = 0\n        for c in range(num_in): # over input channels\n            for n in [0, 1, 2, 3, 5, 6, 7, 8]: # over neighbors\n                weights[1, 1, c, out_channel] = 1\n                weights[n // 3, n % 3, c, out_channel] = -1\n                out_channel += 1\n        weights = tf.constant(weights, dtype=tf.float32)\n        return conv2d(x, weights)\n\n    # Create 8 channel (one per neighbor) differences\n    occ_diff = neighbor_diff(occ)\n    flow_diff_u, flow_diff_v = tf.split(axis=3, num_or_size_splits=2, value=neighbor_diff(flow, 2))\n    flow_diff = tf.sqrt(tf.square(flow_diff_u) + tf.square(flow_diff_v))\n    intensity_diff = tf.abs(neighbor_diff(tf.image.rgb_to_grayscale(im)))\n\n    diff = norm(intensity_diff, 7.5 / 255) * norm(flow_diff, 0.5) * occ_diff * flow_diff\n    return charbonnier_loss(diff)\n\n\ndef photometric_loss(im_diff, mask):\n    return charbonnier_loss(im_diff, mask, beta=255)\n\n\ndef conv2d(x, weights):\n    return tf.nn.conv2d(x, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n\n\ndef _smoothness_deltas(flow):\n    with tf.variable_scope(\'smoothness_delta\'):\n        mask_x = create_mask(flow, [[0, 0], [0, 1]])\n        mask_y = create_mask(flow, [[0, 1], [0, 0]])\n        mask = tf.concat(axis=3, values=[mask_x, mask_y])\n\n        filter_x = [[0, 0, 0], [0, 1, -1], [0, 0, 0]]\n        filter_y = [[0, 0, 0], [0, 1, 0], [0, -1, 0]]\n        weight_array = np.ones([3, 3, 1, 2])\n        weight_array[:, :, 0, 0] = filter_x\n        weight_array[:, :, 0, 1] = filter_y\n        weights = tf.constant(weight_array, dtype=tf.float32)\n\n        flow_u, flow_v = tf.split(axis=3, num_or_size_splits=2, value=flow)\n        delta_u = conv2d(flow_u, weights)\n        delta_v = conv2d(flow_v, weights)\n        return delta_u, delta_v, mask\n\n\ndef _gradient_delta(im1, im2_warped):\n    with tf.variable_scope(\'gradient_delta\'):\n        filter_x = [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]] # sobel filter\n        filter_y = np.transpose(filter_x)\n        weight_array = np.zeros([3, 3, 3, 6])\n        for c in range(3):\n            weight_array[:, :, c, 2 * c] = filter_x\n            weight_array[:, :, c, 2 * c + 1] = filter_y\n        weights = tf.constant(weight_array, dtype=tf.float32)\n\n        im1_grad = conv2d(im1, weights)\n        im2_warped_grad = conv2d(im2_warped, weights)\n        diff = im1_grad - im2_warped_grad\n        return diff\n\n\ndef gradient_loss(im1, im2_warped, mask):\n    with tf.variable_scope(\'gradient_loss\'):\n        mask_x = create_mask(im1, [[0, 0], [1, 1]])\n        mask_y = create_mask(im1, [[1, 1], [0, 0]])\n        gradient_mask = tf.tile(tf.concat(axis=3, values=[mask_x, mask_y]), [1, 1, 1, 3])\n        diff = _gradient_delta(im1, im2_warped)\n        return charbonnier_loss(diff, mask * gradient_mask)\n\n\ndef smoothness_loss(flow):\n    with tf.variable_scope(\'smoothness_loss\'):\n        delta_u, delta_v, mask = _smoothness_deltas(flow)\n        loss_u = charbonnier_loss(delta_u, mask)\n        loss_v = charbonnier_loss(delta_v, mask)\n        return loss_u + loss_v\n\n\ndef _second_order_deltas(flow):\n    with tf.variable_scope(\'_second_order_deltas\'):\n        mask_x = create_mask(flow, [[0, 0], [1, 1]])\n        mask_y = create_mask(flow, [[1, 1], [0, 0]])\n        mask_diag = create_mask(flow, [[1, 1], [1, 1]])\n        mask = tf.concat(axis=3, values=[mask_x, mask_y, mask_diag, mask_diag])\n\n        filter_x = [[0, 0, 0],\n                    [1, -2, 1],\n                    [0, 0, 0]]\n        filter_y = [[0, 1, 0],\n                    [0, -2, 0],\n                    [0, 1, 0]]\n        filter_diag1 = [[1, 0, 0],\n                        [0, -2, 0],\n                        [0, 0, 1]]\n        filter_diag2 = [[0, 0, 1],\n                        [0, -2, 0],\n                        [1, 0, 0]]\n        weight_array = np.ones([3, 3, 1, 4])\n        weight_array[:, :, 0, 0] = filter_x\n        weight_array[:, :, 0, 1] = filter_y\n        weight_array[:, :, 0, 2] = filter_diag1\n        weight_array[:, :, 0, 3] = filter_diag2\n        weights = tf.constant(weight_array, dtype=tf.float32)\n\n        flow_u, flow_v = tf.split(axis=3, num_or_size_splits=2, value=flow)\n        delta_u = conv2d(flow_u, weights)\n        delta_v = conv2d(flow_v, weights)\n        return delta_u, delta_v, mask\n\n\ndef second_order_loss(flow):\n    with tf.variable_scope(\'second_order_loss\'):\n        delta_u, delta_v, mask = _second_order_deltas(flow)\n        loss_u = charbonnier_loss(delta_u, mask)\n        loss_v = charbonnier_loss(delta_v, mask)\n        return loss_u + loss_v\n\n\ndef charbonnier_loss(x, mask=None, truncate=None, alpha=0.45, beta=1.0, epsilon=0.001):\n    """"""Compute the generalized charbonnier loss of the difference tensor x.\n    All positions where mask == 0 are not taken into account.\n\n    Args:\n        x: a tensor of shape [num_batch, height, width, channels].\n        mask: a mask of shape [num_batch, height, width, mask_channels],\n            where mask channels must be either 1 or the same number as\n            the number of channels of x. Entries should be 0 or 1.\n    Returns:\n        loss as tf.float32\n    """"""\n    with tf.variable_scope(\'charbonnier_loss\'):\n        batch, height, width, channels = tf.unstack(tf.shape(x))\n        normalization = tf.cast(batch * height * width * channels, tf.float32)\n\n        error = tf.pow(tf.square(x * beta) + tf.square(epsilon), alpha)\n\n        if mask is not None:\n            error = tf.multiply(mask, error)\n\n        if truncate is not None:\n            error = tf.minimum(error, truncate)\n\n        return tf.reduce_sum(error) / normalization\n\n\ndef create_mask(tensor, paddings):\n    with tf.variable_scope(\'create_mask\'):\n        shape = tf.shape(tensor)\n        inner_width = shape[1] - (paddings[0][0] + paddings[0][1])\n        inner_height = shape[2] - (paddings[1][0] + paddings[1][1])\n        inner = tf.ones([inner_width, inner_height])\n\n        mask2d = tf.pad(inner, paddings)\n        mask3d = tf.tile(tf.expand_dims(mask2d, 0), [shape[0], 1, 1])\n        mask4d = tf.expand_dims(mask3d, 3)\n        return tf.stop_gradient(mask4d)\n\n\ndef create_border_mask(tensor, border_ratio=0.1):\n    with tf.variable_scope(\'create_border_mask\'):\n        num_batch, height, width, _ = tf.unstack(tf.shape(tensor))\n        min_dim = tf.cast(tf.minimum(height, width), \'float32\')\n        sz = tf.cast(tf.ceil(min_dim * border_ratio), \'int32\')\n        border_mask = create_mask(tensor, [[sz, sz], [sz, sz]])\n        return tf.stop_gradient(border_mask)\n\n\ndef create_outgoing_mask(flow):\n    """"""Computes a mask that is zero at all positions where the flow\n    would carry a pixel over the image boundary.""""""\n    with tf.variable_scope(\'create_outgoing_mask\'):\n        num_batch, height, width, _ = tf.unstack(tf.shape(flow))\n\n        grid_x = tf.reshape(tf.range(width), [1, 1, width])\n        grid_x = tf.tile(grid_x, [num_batch, height, 1])\n        grid_y = tf.reshape(tf.range(height), [1, height, 1])\n        grid_y = tf.tile(grid_y, [num_batch, 1, width])\n\n        flow_u, flow_v = tf.unstack(flow, 2, 3)\n        pos_x = tf.cast(grid_x, dtype=tf.float32) + flow_u\n        pos_y = tf.cast(grid_y, dtype=tf.float32) + flow_v\n        inside_x = tf.logical_and(pos_x <= tf.cast(width - 1, tf.float32),\n                                  pos_x >=  0.0)\n        inside_y = tf.logical_and(pos_y <= tf.cast(height - 1, tf.float32),\n                                  pos_y >=  0.0)\n        inside = tf.logical_and(inside_x, inside_y)\n        return tf.expand_dims(tf.cast(inside, tf.float32), 3)\n'"
src/e2eflow/core/spatial_transformer.py,73,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport tensorflow as tf\n\n\ndef transformer(U, theta, out_size, name=\'SpatialTransformer\', **kwargs):\n    """"""Spatial Transformer Layer\n    Implements a spatial transformer layer as described in [1]_.\n    Based on [2]_ and edited by David Dao for Tensorflow.\n    Parameters\n    ----------\n    U : float\n        The output of a convolutional net should have the\n        shape [num_batch, height, width, num_channels].\n    theta: float\n        The output of the\n        localisation network should be [num_batch, 6].\n    out_size: tuple of two ints\n        The size of the output of the network (height, width)\n    References\n    ----------\n    .. [1]  Spatial Transformer Networks\n            Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu\n            Submitted on 5 Jun 2015\n    .. [2]  https://github.com/skaae/transformer_network/blob/master/transformerlayer.py\n    Notes\n    -----\n    To initialize the network to the identity transform init\n    ``theta`` to :\n        identity = np.array([[1., 0., 0.],\n                             [0., 1., 0.]])\n        identity = identity.flatten()\n        theta = tf.Variable(initial_value=identity)\n    """"""\n\n    def _repeat(x, n_repeats):\n        with tf.variable_scope(\'_repeat\'):\n            rep = tf.transpose(\n                tf.expand_dims(tf.ones(shape=tf.stack([n_repeats, ])), 1), [1, 0])\n            rep = tf.cast(rep, \'int32\')\n            x = tf.matmul(tf.reshape(x, (-1, 1)), rep)\n            return tf.reshape(x, [-1])\n\n    def _interpolate(im, x, y, out_size):\n        with tf.variable_scope(\'_interpolate\'):\n            # constants\n            num_batch = tf.shape(im)[0]\n            height = tf.shape(im)[1]\n            width = tf.shape(im)[2]\n            channels = tf.shape(im)[3]\n\n            x = tf.cast(x, \'float32\')\n            y = tf.cast(y, \'float32\')\n            height_f = tf.cast(height, \'float32\')\n            width_f = tf.cast(width, \'float32\')\n            out_height = out_size[0]\n            out_width = out_size[1]\n            zero = tf.zeros([], dtype=\'int32\')\n            max_y = tf.cast(tf.shape(im)[1] - 1, \'int32\')\n            max_x = tf.cast(tf.shape(im)[2] - 1, \'int32\')\n\n            # scale indices from [-1, 1] to [0, width/height]\n            x = (x + 1.0)*(width_f) / 2.0\n            y = (y + 1.0)*(height_f) / 2.0\n\n            # do sampling\n            x0 = tf.cast(tf.floor(x), \'int32\')\n            x1 = x0 + 1\n            y0 = tf.cast(tf.floor(y), \'int32\')\n            y1 = y0 + 1\n\n            x0 = tf.clip_by_value(x0, zero, max_x)\n            x1 = tf.clip_by_value(x1, zero, max_x)\n            y0 = tf.clip_by_value(y0, zero, max_y)\n            y1 = tf.clip_by_value(y1, zero, max_y)\n            dim2 = width\n            dim1 = width*height\n            base = _repeat(tf.range(num_batch)*dim1, out_height*out_width)\n            base_y0 = base + y0*dim2\n            base_y1 = base + y1*dim2\n            idx_a = base_y0 + x0\n            idx_b = base_y1 + x0\n            idx_c = base_y0 + x1\n            idx_d = base_y1 + x1\n\n            # use indices to lookup pixels in the flat image and restore\n            # channels dim\n            im_flat = tf.reshape(im, tf.stack([-1, channels]))\n            im_flat = tf.cast(im_flat, \'float32\')\n            Ia = tf.gather(im_flat, idx_a)\n            Ib = tf.gather(im_flat, idx_b)\n            Ic = tf.gather(im_flat, idx_c)\n            Id = tf.gather(im_flat, idx_d)\n\n            # and finally calculate interpolated values\n            x0_f = tf.cast(x0, \'float32\')\n            x1_f = tf.cast(x1, \'float32\')\n            y0_f = tf.cast(y0, \'float32\')\n            y1_f = tf.cast(y1, \'float32\')\n            wa = tf.expand_dims(((x1_f-x) * (y1_f-y)), 1)\n            wb = tf.expand_dims(((x1_f-x) * (y-y0_f)), 1)\n            wc = tf.expand_dims(((x-x0_f) * (y1_f-y)), 1)\n            wd = tf.expand_dims(((x-x0_f) * (y-y0_f)), 1)\n            output = tf.add_n([wa*Ia, wb*Ib, wc*Ic, wd*Id])\n            return output\n\n    def _meshgrid(height, width):\n        with tf.variable_scope(\'_meshgrid\'):\n            # This should be equivalent to:\n            #  x_t, y_t = np.meshgrid(np.linspace(-1, 1, width),\n            #                         np.linspace(-1, 1, height))\n            #  ones = np.ones(np.prod(x_t.shape))\n            #  grid = np.vstack([x_t.flatten(), y_t.flatten(), ones])\n            x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])),\n                            tf.transpose(tf.expand_dims(tf.linspace(-1.0, 1.0, width), 1), [1, 0]))\n            y_t = tf.matmul(tf.expand_dims(tf.linspace(-1.0, 1.0, height), 1),\n                            tf.ones(shape=tf.stack([1, width])))\n\n            x_t_flat = tf.reshape(x_t, (1, -1))\n            y_t_flat = tf.reshape(y_t, (1, -1))\n\n            ones = tf.ones_like(x_t_flat)\n            grid = tf.concat(axis=0, values=[x_t_flat, y_t_flat, ones])\n            return grid\n\n    def _transform(theta, input_dim, out_size):\n        with tf.variable_scope(\'_transform\'):\n            num_batch = tf.shape(input_dim)[0]\n            height = tf.shape(input_dim)[1]\n            width = tf.shape(input_dim)[2]\n            num_channels = tf.shape(input_dim)[3]\n            theta = tf.reshape(theta, (-1, 2, 3))\n            theta = tf.cast(theta, \'float32\')\n\n            # grid of (x_t, y_t, 1), eq (1) in ref [1]\n            height_f = tf.cast(height, \'float32\')\n            width_f = tf.cast(width, \'float32\')\n            out_height = out_size[0]\n            out_width = out_size[1]\n            grid = _meshgrid(out_height, out_width)\n            grid = tf.expand_dims(grid, 0)\n            grid = tf.reshape(grid, [-1])\n            grid = tf.tile(grid, tf.stack([num_batch]))\n            grid = tf.reshape(grid, tf.stack([num_batch, 3, -1]))\n\n            # Transform A x (x_t, y_t, 1)^T -> (x_s, y_s)\n            T_g = tf.matmul(theta, grid)\n            x_s = tf.slice(T_g, [0, 0, 0], [-1, 1, -1])\n            y_s = tf.slice(T_g, [0, 1, 0], [-1, 1, -1])\n            x_s_flat = tf.reshape(x_s, [-1])\n            y_s_flat = tf.reshape(y_s, [-1])\n\n            input_transformed = _interpolate(\n                input_dim, x_s_flat, y_s_flat,\n                out_size)\n\n            output = tf.reshape(\n                input_transformed, tf.stack([num_batch, out_height, out_width, num_channels]))\n            return output\n\n    with tf.variable_scope(name):\n        output = _transform(theta, U, out_size)\n        return output\n\n\ndef batch_transformer(U, thetas, out_size, name=\'BatchSpatialTransformer\'):\n    """"""Batch Spatial Transformer Layer\n    Parameters\n    ----------\n    U : float\n        tensor of inputs [num_batch,height,width,num_channels]\n    thetas : float\n        a set of transformations for each input [num_batch,num_transforms,6]\n    out_size : int\n        the size of the output [out_height,out_width]\n    Returns: float\n        Tensor of size [num_batch*num_transforms,out_height,out_width,num_channels]\n    """"""\n    with tf.variable_scope(name):\n        num_batch, num_transforms = map(int, thetas.get_shape().as_list()[:2])\n        indices = [[i]*num_transforms for i in xrange(num_batch)]\n        input_repeated = tf.gather(U, tf.reshape(indices, [-1]))\n        return transformer(input_repeated, thetas, out_size)\n'"
src/e2eflow/core/supervised.py,4,"b""import tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nfrom .augment import random_photometric\nfrom .flow_util import flow_to_color\nfrom .losses import charbonnier_loss\nfrom .flownet import flownet\nfrom .unsupervised import _track_image, _track_loss, FLOW_SCALE\n\n\ndef supervised_loss(batch, params, normalization=None):\n    channel_mean = tf.constant(normalization[0]) / 255.0\n    im1, im2, flow_gt, mask_gt = batch\n    im1 = im1 / 255.0\n    im2 = im2 / 255.0\n    im_shape = tf.shape(im1)[1:3]\n\n    # -------------------------------------------------------------------------\n\n    im1_photo, im2_photo = random_photometric(\n        [im1, im2],\n        noise_stddev=0.04, min_contrast=-0.3, max_contrast=0.3,\n        brightness_stddev=0.02, min_colour=0.9, max_colour=1.1,\n        min_gamma=0.7, max_gamma=1.5)\n\n    _track_image(im1_photo, 'im1_photo')\n    _track_image(im2_photo, 'im2_photo')\n    _track_image(flow_to_color(flow_gt), 'flow_gt')\n    _track_image(mask_gt, 'mask_gt')\n\n    # Images for neural network input with mean-zero values in [-1, 1]\n    im1_photo = im1_photo - channel_mean\n    im2_photo = im2_photo - channel_mean\n\n    flownet_spec = params.get('flownet', 'S')\n    full_resolution = params.get('full_res')\n    train_all = params.get('train_all')\n    # -------------------------------------------------------------------------\n    # FlowNet\n    flows_fw = flownet(im1_photo, im2_photo,\n                       flownet_spec=flownet_spec,\n                       full_resolution=full_resolution,\n                       train_all=train_all)\n    \n    if not train_all:\n        flows_fw = [flows_fw[-1]]\n    final_loss = 0.0\n    for i, net_flows in enumerate(reversed(flows_fw)):\n        flow_fw = net_flows[0]\n        if params.get('full_res'):\n            final_flow_fw = flow_fw * FLOW_SCALE * 4\n        else:\n            final_flow_fw = tf.image.resize_bilinear(flow_fw, im_shape) * FLOW_SCALE * 4\n        _track_image(flow_to_color(final_flow_fw), 'flow_pred_' + str(i))\n\n        net_loss = charbonnier_loss(final_flow_fw - flow_gt, mask_gt)\n        final_loss += net_loss / (2 ** i)\n\n    regularization_loss = tf.add_n(slim.losses.get_regularization_losses())\n    final_loss += regularization_loss\n    _track_loss(regularization_loss, 'loss/regularization')\n    _track_loss(final_loss, 'loss/combined')\n\n    return final_loss\n"""
src/e2eflow/core/train.py,50,"b'import os\nimport re\nimport numpy as np\nfrom multiprocessing import Process\n#from matplotlib.pyplot import plot, show\n\nimport tensorflow as tf\nfrom tensorflow.python.client import timeline\nimport tensorflow.contrib.slim as slim\n\nfrom . import util\nfrom ..ops import forward_warp\nfrom .image_warp import image_warp\nfrom .unsupervised import unsupervised_loss\nfrom .supervised import supervised_loss\nfrom .losses import occlusion, DISOCC_THRESH, create_outgoing_mask\nfrom .flow_util import flow_error_avg, flow_to_color, flow_error_image, outlier_pct\nfrom ..gui import display\nfrom .util import summarized_placeholder\nfrom .input import resize_input, resize_output_crop, resize_output, resize_output_flow\n\n\ndef restore_networks(sess, params, ckpt, ckpt_path=None):\n    finetune = params.get(\'finetune\', [])\n    train_all = params.get(\'train_all\', None)\n    spec = params.get(\'flownet\', \'S\')\n    flownet_num = len(spec)\n\n    net_names = [\'flownet_c\'] + [\'stack_{}_flownet\'.format(i+1) for i in range(flownet_num - 1)]\n    assert len(finetune) <= flownet_num\n    # Save all trained networks, restore all networks which are kept fixed\n    if train_all:\n        restore_external_nets = finetune if ckpt is None else []\n        variables_to_save = slim.get_variables_to_restore(include=net_names)\n    else:\n        restore_external_nets = finetune if ckpt is None else finetune[:flownet_num - 1]\n        variables_to_save = slim.get_variables_to_restore(include=[net_names[-1]])\n\n    saver = tf.train.Saver(variables_to_save, max_to_keep=1000)\n\n    sess.run(tf.global_variables_initializer())\n\n    if ckpt is not None:\n        # continue training\n        saver.restore(sess, ckpt.model_checkpoint_path)\n        saver.recover_last_checkpoints(ckpt.all_model_checkpoint_paths)\n\n    for i, ckpt in enumerate(restore_external_nets):\n        print(\'-- restore\', net_names[i], ckpt.model_checkpoint_path)\n        try:\n            nets_to_restore = [net_names[i]]\n            variables_to_restore = slim.get_variables_to_restore(\n                include=nets_to_restore)\n            restorer = tf.train.Saver(variables_to_restore)\n            restorer.restore(sess, ckpt.model_checkpoint_path)\n        except:\n            # load partial network (missing final 2 upconvolutions)\n            nets_to_restore = [net_names[i]]\n            variables_to_restore = slim.get_variables_to_restore(\n                include=nets_to_restore)\n            variables_to_restore = [v for v in variables_to_restore\n                                    if not \'full_res\' in v.name]\n            restorer = tf.train.Saver(variables_to_restore)\n            restorer.restore(sess, ckpt.model_checkpoint_path)\n    return saver\n\n\ndef _add_loss_summaries():\n    losses = tf.get_collection(\'losses\')\n    for l in losses:\n        tensor_name = re.sub(\'tower_[0-9]*/\', \'\', l.op.name)\n        tf.summary.scalar(tensor_name, l)\n\n\ndef _add_param_summaries():\n    params = tf.get_collection(\'params\')\n    for p in params:\n        tensor_name = re.sub(\'tower_[0-9]*/\', \'\', p.op.name)\n        tf.summary.scalar(tensor_name, p)\n\n\ndef _add_image_summaries():\n    images = tf.get_collection(\'train_images\')\n    for im in images:\n        tensor_name = re.sub(\'tower_[0-9]*/\', \'\', im.op.name)\n        tf.summary.image(tensor_name, im)\n\n\ndef _eval_plot(results, image_names, title):\n    import matplotlib.pyplot as plt\n    display(results, image_names, title)\n\n\nclass Trainer():\n    def __init__(self, train_batch_fn, eval_batch_fn, params,\n                 train_summaries_dir, eval_summaries_dir, ckpt_dir,\n                 normalization, debug=False, experiment="""", interactive_plot=False,\n                 supervised=False, devices=None):\n\n        self.train_summaries_dir = train_summaries_dir\n        self.eval_summaries_dir = eval_summaries_dir\n        self.ckpt_dir = ckpt_dir\n        self.params = params\n        self.debug = debug\n        self.train_batch_fn = train_batch_fn\n        self.eval_batch_fn = eval_batch_fn\n        self.normalization = normalization\n        self.experiment = experiment\n        self.interactive_plot = interactive_plot\n        self.plot_proc = None\n        self.supervised = supervised\n        self.loss_fn = supervised_loss if supervised else unsupervised_loss\n        self.devices = devices or \'/gpu:0\'\n        self.shared_device = devices[0] if len(devices) == 1 else \'/cpu:0\'\n\n    def run(self, min_iter, max_iter):\n        """"""Train (at most) from min_iter + 1 to max_iter.\n        If checkpoints are found in ckpt_dir,\n        they must be have a global_step within [min_iter, max_iter]. In this case,\n        training is continued from global_step + 1 until max_iter is reached.\n        """"""\n        save_interval = self.params[\'save_interval\']\n\n        ckpt = tf.train.get_checkpoint_state(self.ckpt_dir)\n        if ckpt is not None:\n            ckpt_path = ckpt.model_checkpoint_path\n            global_step = int(ckpt_path.split(\'/\')[-1].split(\'-\')[-1])\n            assert global_step >= min_iter, \'training stage not reached\'\n\n            start_iter = global_step + 1\n            if start_iter > max_iter:\n                print(\'-- train: max_iter reached\')\n                return\n        else:\n            start_iter = min_iter + 1\n\n        print(\'-- training from i = {} to {}\'.format(start_iter, max_iter))\n\n        assert (max_iter - start_iter + 1) % save_interval == 0\n        for i in range(start_iter, max_iter + 1, save_interval):\n            self.train(i, i + save_interval - 1, i - (min_iter + 1))\n            self.eval(1)\n\n        if self.plot_proc:\n            self.plot_proc.join()\n\n    def get_train_and_loss_ops(self, batch, learning_rate, global_step):\n        if self.params[\'flownet\'] == \'resnet\':\n            opt = tf.train.MomentumOptimizer(learning_rate, 0.9)\n        else:\n            opt = tf.train.AdamOptimizer(beta1=0.9, beta2=0.999,\n                                         learning_rate=learning_rate)\n        def _add_summaries():\n            _add_loss_summaries()\n            _add_param_summaries()\n            if self.debug:\n                _add_image_summaries()\n\n        if len(self.devices) == 1:\n            loss_ = self.loss_fn(batch, self.params, self.normalization)\n            train_op = opt.minimize(loss_)\n            _add_summaries()\n        else:\n            tower_grads = []\n            with tf.variable_scope(tf.get_variable_scope()):\n                for i, devid in enumerate(self.devices):\n                    with tf.device(devid):\n                        with tf.name_scope(\'tower_{}\'.format(i)) as scope:\n                            loss_ = self.loss_fn(batch, self.params, self.normalization)\n                            _add_summaries()\n\n                            # Reuse variables for the next tower.\n                            tf.get_variable_scope().reuse_variables()\n\n                            # Retain the summaries from the final tower.\n                            tower_summaries = tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                                                scope)\n                            grads = opt.compute_gradients(loss_)\n                            tower_grads.append(grads)\n\n            grads = average_gradients(tower_grads)\n            apply_gradient_op = opt.apply_gradients(grads)\n            train_op = apply_gradient_op\n\n        return train_op, loss_\n\n    def train(self, start_iter, max_iter, iter_offset):\n        ckpt = tf.train.get_checkpoint_state(self.ckpt_dir)\n\n        with tf.Graph().as_default(), tf.device(self.shared_device):\n            batch = self.train_batch_fn(iter_offset)\n\n            with tf.name_scope(\'params\') as scope:\n                learning_rate_ = util.summarized_placeholder(\'learning_rate\', \'train\')\n                summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n\n            global_step_ = tf.placeholder(tf.int32, name=""global_step"")\n\n            train_op, loss_ = self.get_train_and_loss_ops(batch, learning_rate_, global_step_)\n\n            summaries = tf.get_collection(tf.GraphKeys.SUMMARIES)\n            summary_ = tf.summary.merge(summaries)\n\n            sess_config = tf.ConfigProto(allow_soft_placement=True)\n\n            with tf.Session(config=sess_config) as sess:\n                if self.debug:\n                    summary_writer = tf.summary.FileWriter(self.train_summaries_dir,\n                                                            sess.graph)\n                    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n                    run_metadata = tf.RunMetadata()\n                else:\n                    summary_writer = tf.summary.FileWriter(self.train_summaries_dir)\n                    run_options = None\n                    run_metadata = None\n\n                saver = restore_networks(sess, self.params, ckpt)\n\n                coord = tf.train.Coordinator()\n                threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n                for local_i, i in enumerate(range(start_iter, max_iter + 1)):\n                    #if INTERACTIVE_PLOT:\n                    #    plt.title = ""{} ({})"".format(self.experiment, i)\n                    decay_iters = local_i + iter_offset\n                    if \'manual_decay_lrs\' in self.params \\\n                            and \'manual_decay_iters\' in self.params:\n                        decay_index = 0\n                        iter_counter = 0\n                        for decay_i, manual_decay_iter in enumerate(self.params[\'manual_decay_iters\']):\n                            iter_counter += manual_decay_iter\n                            if decay_iters <= iter_counter:\n                                decay_index = decay_i\n                                break\n                        learning_rate = self.params[\'manual_decay_lrs\'][decay_index]\n                    else:\n                        decay_interval = self.params[\'decay_interval\']\n                        decay_after = self.params.get(\'decay_after\', 0)\n                        if decay_iters >= decay_after:\n                            decay_minimum = decay_after / decay_interval\n                            decay = (decay_iters // decay_interval) - decay_minimum\n                            learning_rate = self.params[\'learning_rate\'] / (2 ** decay)\n                        else:\n                            learning_rate = self.params[\'learning_rate\']\n\n                    feed_dict = {learning_rate_: learning_rate, global_step_: i}\n                    _, loss = sess.run(\n                        [train_op, loss_],\n                        feed_dict=feed_dict,\n                        options=run_options,\n                        run_metadata=run_metadata)\n\n                    if i == 1 or i % self.params[\'display_interval\'] == 0:\n                        summary = sess.run(summary_, feed_dict=feed_dict)\n                        summary_writer.add_summary(summary, i)\n                        print(""-- train: i = {}, loss = {}"".format(i, loss))\n\n                save_path =  os.path.join(self.ckpt_dir, \'model.ckpt\')\n                saver.save(sess, save_path, global_step=max_iter)\n\n                summary_writer.close()\n                coord.request_stop()\n                coord.join(threads)\n\n    def eval(self, num):\n        assert num == 1 # TODO enable num > 1\n\n        with tf.Graph().as_default():\n            inputs = self.eval_batch_fn()\n            im1, im2, input_shape = inputs[:3]\n            truths = inputs[3:]\n\n            height, width, _ = tf.unstack(tf.squeeze(input_shape), num=3, axis=0)\n            im1 = resize_input(im1, height, width, 384, 1280)\n            im2 = resize_input(im2, height, width, 384, 1280)\n\n            _, flow, flow_bw = unsupervised_loss(\n                (im1, im2),\n                params=self.params,\n                normalization=self.normalization,\n                augment=False, return_flow=True)\n\n            im1 = resize_output(im1, height, width, 3)\n            im2 = resize_output(im2, height, width, 3)\n            flow = resize_output_flow(flow, height, width, 2)\n            flow_bw = resize_output_flow(flow_bw, height, width, 2)\n\n            variables_to_restore = tf.all_variables()\n\n            images_ = [image_warp(im1, flow) / 255,\n                       flow_to_color(flow),\n                       1 - (1 - occlusion(flow, flow_bw)[0]) * create_outgoing_mask(flow) ,\n                       forward_warp(flow_bw) < DISOCC_THRESH]\n            image_names = [\'warped image\', \'flow\', \'occ\', \'reverse disocc\']\n\n            values_ = []\n            averages_ = []\n            truth_tuples = []\n            if len(truths) == 4:\n                flow_occ, mask_occ, flow_noc, mask_noc = truths\n                flow_occ = resize_output_crop(flow_occ, height, width, 2)\n                flow_noc = resize_output_crop(flow_noc, height, width, 2)\n                mask_occ = resize_output_crop(mask_occ, height, width, 1)\n                mask_noc = resize_output_crop(mask_noc, height, width, 1)\n\n                truth_tuples.append((\'occluded\', flow_occ, mask_occ))\n                truth_tuples.append((\'non-occluded\', flow_noc, mask_noc))\n                images_ += [flow_error_image(flow, flow_occ, mask_occ, mask_noc)]\n                image_names += [\'flow error\']\n            else:\n                raise NotImplementedError()\n                truth_tuples.append((\'flow\', truths[0], truths[1]))\n\n            for name, gt_flow, mask in truth_tuples:\n                error_ = flow_error_avg(gt_flow, flow, mask)\n                error_avg_ = summarized_placeholder(\'AEE/\' + name, key=\'eval_avg\')\n                outliers_ = outlier_pct(gt_flow, flow, mask)\n                outliers_avg = summarized_placeholder(\'outliers/\' + name,\n                                                      key=\'eval_avg\')\n                values_.extend([error_, outliers_])\n                averages_.extend([error_avg_, outliers_avg])\n\n            losses = tf.get_collection(\'losses\')\n            for l in losses:\n                values_.append(l)\n                tensor_name = re.sub(\'tower_[0-9]*/\', \'\', l.op.name)\n                loss_avg_ = summarized_placeholder(tensor_name, key=\'eval_avg\')\n                averages_.append(loss_avg_)\n\n            ckpt = tf.train.get_checkpoint_state(self.ckpt_dir)\n            assert ckpt is not None, ""No checkpoints to evaluate""\n\n            # Correct path for ckpts from different machine\n            # ckpt_path = self.ckpt_dir + ""/"" + os.path.basename(ckpt.model_checkpoint_path)\n            ckpt_path = ckpt.model_checkpoint_path\n\n            with tf.Session() as sess:\n                summary_writer = tf.summary.FileWriter(self.eval_summaries_dir)\n                saver = tf.train.Saver(variables_to_restore)\n\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                restore_networks(sess, self.params, ckpt)\n                global_step = ckpt_path.split(\'/\')[-1].split(\'-\')[-1]\n\n                coord = tf.train.Coordinator()\n                threads = tf.train.start_queue_runners(sess=sess,\n                                                       coord=coord)\n                averages = np.zeros(len(averages_))\n                num_iters = 0\n\n                image_lists = []\n                try:\n                    while not coord.should_stop():\n                        results = sess.run(values_ + images_)\n                        values = results[:len(averages_)]\n                        images = results[len(averages_):]\n                        image_lists.append(images)\n                        averages += values\n                        num_iters += 1\n                except tf.errors.OutOfRangeError:\n                    pass\n\n                averages /= num_iters\n                feed = {k: v for (k, v) in zip(averages_, averages)}\n\n                summary_ = tf.summary.merge_all(\'eval_avg\')\n                summary = sess.run(summary_, feed_dict=feed)\n                summary_writer.add_summary(summary, global_step)\n\n                print(""-- eval: i = {}"".format(global_step))\n\n                coord.request_stop()\n                coord.join(threads)\n                summary_writer.close()\n\n                if self.interactive_plot:\n                    if self.plot_proc:\n                        self.plot_proc.terminate()\n                    self.plot_proc = Process(target=_eval_plot,\n                                             args=([image_lists], image_names,\n                                                   ""{} (i={})"".format(self.experiment,\n                                                                      global_step)))\n                    self.plot_proc.start()\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n      is over individual gradients. The inner list is over the gradient\n      calculation for each tower.\n    Returns:\n     List of pairs of (gradient, variable) where the gradient has been averaged\n     across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            if g is not None:\n                # Add 0 dimension to the gradients to represent the tower.\n                expanded_g = tf.expand_dims(g, 0)\n\n                # Append on a \'tower\' dimension which we will average over below.\n                grads.append(expanded_g)\n        if grads != []:\n            # Average over the \'tower\' dimension.\n            grad = tf.concat(grads, 0)\n            grad = tf.reduce_mean(grad, 0)\n\n            # Keep in mind that the Variables are redundant because they are shared\n            # across towers. So .. we will just return the first tower\'s pointer to\n            # the Variable.\n            v = grad_and_vars[0][1]\n            grad_and_var = (grad, v)\n            average_grads.append(grad_and_var)\n    return average_grads\n'"
src/e2eflow/core/unsupervised.py,10,"b'import tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nfrom .augment import random_affine, random_photometric\nfrom .flow_util import flow_to_color\nfrom .util import resize_area, resize_bilinear\nfrom .losses import compute_losses, create_border_mask\nfrom .util import downsample\nfrom .image_warp import image_warp\nfrom .flownet import flownet, FLOW_SCALE\n\n\n# REGISTER ALL POSSIBLE LOSS TERMS\nLOSSES = [\'occ\', \'sym\', \'fb\', \'grad\', \'ternary\', \'photo\', \'smooth_1st\', \'smooth_2nd\']\n\n\ndef _track_loss(op, name):\n    tf.add_to_collection(\'losses\', tf.identity(op, name=name))\n\n\ndef _track_image(op, name):\n    name = \'train/\' + name\n    tf.add_to_collection(\'train_images\', tf.identity(op, name=name))\n\n\ndef unsupervised_loss(batch, params, normalization=None, augment=True,\n                      return_flow=False):\n    channel_mean = tf.constant(normalization[0]) / 255.0\n    im1, im2 = batch\n    im1 = im1 / 255.0\n    im2 = im2 / 255.0\n    im_shape = tf.shape(im1)[1:3]\n\n    # -------------------------------------------------------------------------\n    # Data & mask augmentation\n    border_mask = create_border_mask(im1, 0.1)\n\n    if augment:\n        im1_geo, im2_geo, border_mask_global = random_affine(\n            [im1, im2, border_mask],\n            horizontal_flipping=True,\n            min_scale=0.9, max_scale=1.1\n            )\n\n        # augment locally\n        im2_geo, border_mask_local = random_affine(\n            [im2_geo, border_mask],\n            min_scale=0.9, max_scale=1.1\n            )\n        border_mask = border_mask_local * border_mask_global\n\n        im1_photo, im2_photo = random_photometric(\n            [im1_geo, im2_geo],\n            noise_stddev=0.04, min_contrast=-0.3, max_contrast=0.3,\n            brightness_stddev=0.02, min_colour=0.9, max_colour=1.1,\n            min_gamma=0.7, max_gamma=1.5)\n\n        _track_image(im1_photo, \'augmented1\')\n        _track_image(im2_photo, \'augmented2\')\n    else:\n        im1_geo, im2_geo = im1, im2\n        im1_photo, im2_photo = im1, im2\n\n    # Images for loss comparisons with values in [0, 1] (scale to original using * 255)\n    im1_norm = im1_geo\n    im2_norm = im2_geo\n    # Images for neural network input with mean-zero values in [-1, 1]\n    im1_photo = im1_photo - channel_mean\n    im2_photo = im2_photo - channel_mean\n\n    flownet_spec = params.get(\'flownet\', \'S\')\n    full_resolution = params.get(\'full_res\')\n    train_all = params.get(\'train_all\')\n\n    flows_fw, flows_bw = flownet(im1_photo, im2_photo,\n                                 flownet_spec=flownet_spec,\n                                 full_resolution=full_resolution,\n                                 backward_flow=True,\n                                 train_all=train_all)\n\n    flows_fw = flows_fw[-1]\n    flows_bw = flows_bw[-1]\n\n    # -------------------------------------------------------------------------\n    # Losses\n    layer_weights = [12.7, 4.35, 3.9, 3.4, 1.1]\n    layer_patch_distances = [3, 2, 2, 1, 1]\n    if full_resolution:\n        layer_weights = [12.7, 5.5, 5.0, 4.35, 3.9, 3.4, 1.1]\n        layer_patch_distances = [3, 3] + layer_patch_distances\n        im1_s = im1_norm\n        im2_s = im2_norm\n        mask_s = border_mask\n        final_flow_scale = FLOW_SCALE * 4\n        final_flow_fw = flows_fw[0] * final_flow_scale\n        final_flow_bw = flows_bw[0] * final_flow_scale\n    else:\n        im1_s = downsample(im1_norm, 4)\n        im2_s = downsample(im2_norm, 4)\n        mask_s = downsample(border_mask, 4)\n        final_flow_scale = FLOW_SCALE\n        final_flow_fw = tf.image.resize_bilinear(flows_fw[0], im_shape) * final_flow_scale * 4\n        final_flow_bw = tf.image.resize_bilinear(flows_bw[0], im_shape) * final_flow_scale * 4\n\n    combined_losses = dict()\n    combined_loss = 0.0\n    for loss in LOSSES:\n        combined_losses[loss] = 0.0\n\n    if params.get(\'pyramid_loss\'):\n        flow_enum = enumerate(zip(flows_fw, flows_bw))\n    else:\n        flow_enum = [(0, (flows_fw[0], flows_bw[0]))]\n\n    for i, flow_pair in flow_enum:\n        layer_name = ""loss"" + str(i + 2)\n\n        flow_scale = final_flow_scale / (2 ** i)\n\n        with tf.variable_scope(layer_name):\n            layer_weight = layer_weights[i]\n            flow_fw_s, flow_bw_s = flow_pair\n\n            mask_occlusion = params.get(\'mask_occlusion\', \'\')\n            assert mask_occlusion in [\'fb\', \'disocc\', \'\']\n\n            losses = compute_losses(im1_s, im2_s,\n                                    flow_fw_s * flow_scale, flow_bw_s * flow_scale,\n                                    border_mask=mask_s if params.get(\'border_mask\') else None,\n                                    mask_occlusion=mask_occlusion,\n                                    data_max_distance=layer_patch_distances[i])\n\n            layer_loss = 0.0\n\n            for loss in LOSSES:\n                weight_name = loss + \'_weight\'\n                if params.get(weight_name):\n                    _track_loss(losses[loss], loss)\n                    layer_loss += params[weight_name] * losses[loss]\n                    combined_losses[loss] += layer_weight * losses[loss]\n\n            combined_loss += layer_weight * layer_loss\n\n            im1_s = downsample(im1_s, 2)\n            im2_s = downsample(im2_s, 2)\n            mask_s = downsample(mask_s, 2)\n\n    regularization_loss = tf.losses.get_regularization_loss()\n    final_loss = combined_loss + regularization_loss\n\n    _track_loss(final_loss, \'loss/combined\')\n\n    for loss in LOSSES:\n        _track_loss(combined_losses[loss], \'loss/\' + loss)\n        weight_name = loss + \'_weight\'\n        if params.get(weight_name):\n            weight = tf.identity(params[weight_name], name=\'weight/\' + loss)\n            tf.add_to_collection(\'params\', weight)\n\n    if not return_flow:\n        return final_loss\n\n    return final_loss, final_flow_fw, final_flow_bw\n'"
src/e2eflow/core/util.py,8,"b""import tensorflow as tf\nfrom ..ops import downsample as downsample_ops\n\n\ndef summarized_placeholder(name, prefix=None, key=tf.GraphKeys.SUMMARIES):\n    prefix = '' if not prefix else prefix + '/'\n    p = tf.placeholder(tf.float32, name=name)\n    tf.summary.scalar(prefix + name, p, collections=[key])\n    return p\n\n\ndef resize_area(tensor, like):\n    _, h, w, _ = tf.unstack(tf.shape(like))\n    return tf.stop_gradient(tf.image.resize_area(tensor, [h, w]))\n\n\ndef resize_bilinear(tensor, like):\n    _, h, w, _ = tf.unstack(tf.shape(like))\n    return tf.stop_gradient(tf.image.resize_bilinear(tensor, [h, w]))\n\ndef downsample(tensor, num):\n    _,height, width,_ = tensor.shape.as_list()\n    if height%2==0 and width%2==0:\n        return downsample_ops(tensor, num)\n    else:\n        return tf.image.resize_area(tensor,tf.constant([int(height/num),int(width/num)]))\n\t\n"""
src/e2eflow/kitti/__init__.py,0,b''
src/e2eflow/kitti/data.py,0,"b'import os\nimport sys\n\nimport numpy as np\nimport matplotlib.image as mpimg\n\nfrom . import raw_records\nfrom ..core.data import Data\nfrom ..util import tryremove\nfrom ..core.input import frame_name_to_num\n\n\ndef exclude_test_and_train_images(kitti_dir, exclude_lists_dir, exclude_target_dir,\n                                  remove=False):\n    to_move = []\n\n    def exclude_from_seq(day_name, seq_str, image, view, distance=10):\n        # image is the first frame of each frame pair to exclude\n        seq_dir_rel = os.path.join(day_name, seq_str, view, \'data\')\n        seq_dir_abs = os.path.join(kitti_dir, seq_dir_rel)\n        target_dir_abs = os.path.join(exclude_target_dir, seq_dir_rel)\n        if not os.path.isdir(seq_dir_abs):\n            print(""Not found: {}"".format(seq_dir_abs))\n            return\n        try:\n            os.makedirs(target_dir_abs)\n        except:\n            pass\n        seq_files = sorted(os.listdir(seq_dir_abs))\n        image_num = frame_name_to_num(image)\n        try:\n            image_index = seq_files.index(image)\n        except ValueError:\n            return\n        # assume that some in-between files may be missing\n        start = max(0, image_index - distance)\n        stop = min(len(seq_files), image_index + distance + 2)\n        start_num = image_num - distance\n        stop_num = image_num + distance + 2\n        for i in range(start, stop):\n            filename = seq_files[i]\n            num = frame_name_to_num(filename)\n            if num < start_num or num >= stop_num:\n                continue\n            to_move.append((os.path.join(seq_dir_abs, filename),\n                            os.path.join(target_dir_abs, filename)))\n\n    for filename in os.listdir(exclude_lists_dir):\n        exclude_list_path = os.path.join(exclude_lists_dir, filename)\n        with open(exclude_list_path) as f:\n            for line in f:\n                line = line.rstrip(\'\\n\')\n                if line.split(\' \')[0].endswith(\'_10\'):\n                    splits = line.split(\' \')[-1].split(\'\\\\\')\n                    image = splits[-1]\n                    seq_str = splits[0]\n                    day_name, seq_name = seq_str.split(\'_drive_\')\n                    seq_name = seq_name.split(\'_\')[0] + \'_extract\'\n                    seq_str = day_name + \'_drive_\' + seq_name\n                    exclude_from_seq(day_name, seq_str, image, \'image_02\')\n                    exclude_from_seq(day_name, seq_str, image, \'image_03\')\n    if remove:\n        print(""Collected {} files. Deleting..."".format(len(to_move)))\n    else:\n        print(""Collected {} files. Moving..."".format(len(to_move)))\n\n    for i, data in enumerate(to_move):\n        try:\n            src, dst = data\n            print(""{} / {}: {}"".format(i, len(to_move) - 1, src))\n            if remove:\n                os.remove(src)\n            else:\n                os.rename(src, dst)\n        except: # Some ranges may overlap\n            pass\n\n    return len(to_move)\n\n\nclass KITTIData(Data):\n    KITTI_RAW_URL = \'https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/\'\n    KITTI_2012_URL = \'https://s3.eu-central-1.amazonaws.com/avg-kitti/data_stereo_flow.zip\'\n    KITTI_2015_URL = \'https://s3.eu-central-1.amazonaws.com/avg-kitti/data_scene_flow.zip\'\n\n    dirs = [\'data_stereo_flow\', \'data_scene_flow\', \'kitti_raw\']\n\n    def __init__(self, data_dir, stat_log_dir=None,\n                 development=True, fast_dir=None):\n        super().__init__(data_dir, stat_log_dir,\n                         development=development,\n                         fast_dir=fast_dir)\n\n    def _fetch_if_missing(self):\n        self._maybe_get_kitti_raw()\n        self._maybe_get_kitti_2012()\n        self._maybe_get_kitti_2015()\n\n    def get_raw_dirs(self):\n       top_dir = os.path.join(self.current_dir, \'kitti_raw\')\n       dirs = []\n       dates = os.listdir(top_dir)\n       for date in dates:\n           date_path = os.path.join(top_dir, date)\n           extracts = os.listdir(date_path)\n           for extract in extracts:\n               extract_path = os.path.join(date_path, extract)\n               image_02_folder = os.path.join(extract_path, \'image_02/data\')\n               image_03_folder = os.path.join(extract_path, \'image_03/data\')\n               dirs.extend([image_02_folder, image_03_folder])\n       return dirs\n\n    def _maybe_get_kitti_2012(self):\n        local_path = os.path.join(self.data_dir, \'data_stereo_flow\')\n        if not os.path.isdir(local_path):\n            self._download_and_extract(self.KITTI_2012_URL, local_path)\n\n    def _maybe_get_kitti_2015(self):\n        local_path = os.path.join(self.data_dir, \'data_scene_flow\')\n        if not os.path.isdir(local_path):\n            self._download_and_extract(self.KITTI_2015_URL, local_path)\n\n    def _maybe_get_kitti_raw(self):\n        base_url = self.KITTI_RAW_URL\n        local_dir = os.path.join(self.data_dir, \'kitti_raw\')\n        records = raw_records.get_kitti_records(self.development)\n        downloaded_records = False\n          \n        for i, record in enumerate(records):\n            date_str = record.split(""_drive_"")[0]\n            foldername = record + ""_extract""\n            date_folder = os.path.join(local_dir, date_str)\n            if not os.path.isdir(date_folder):\n                os.makedirs(date_folder)\n            local_path = os.path.join(date_folder, foldername)\n            if not os.path.isdir(local_path):\n                url = base_url + record + ""/"" + foldername + \'.zip\'\n                print(url)\n                self._download_and_extract(url, local_dir)\n                downloaded_records = True\n\n            # Remove unused directories\n            tryremove(os.path.join(local_path, \'velodyne_points\'))\n            tryremove(os.path.join(local_path, \'oxts\'))\n            tryremove(os.path.join(local_path, \'image_00\'))\n            tryremove(os.path.join(local_path, \'image_01\'))\n            \n        if downloaded_records:\n            print(""Downloaded all KITTI raw files."")\n            exclude_target_dir = os.path.join(self.data_dir, \'exclude_target_dir\')\n            exclude_lists_dir = \'../files/kitti_excludes\'\n            excluded = exclude_test_and_train_images(local_dir, exclude_lists_dir, exclude_target_dir,\n                                                     remove=True)\n'"
src/e2eflow/kitti/input.py,12,"b'import os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nimport random\n\nfrom ..core.input import read_png_image, Input\nfrom ..core.augment import random_crop\n\n\ndef _read_flow(filenames, num_epochs=None):\n    """"""Given a list of filenames, constructs a reader op for ground truth.""""""\n    filename_queue = tf.train.string_input_producer(filenames,\n        shuffle=False, capacity=len(filenames), num_epochs=num_epochs)\n    reader = tf.WholeFileReader()\n    _, value = reader.read(filename_queue)\n    gt_uint16 = tf.image.decode_png(value, dtype=tf.uint16)\n    gt = tf.cast(gt_uint16, tf.float32)\n    flow = (gt[:, :, 0:2] - 2 ** 15) / 64.0\n    mask = gt[:, :, 2:3]\n    return flow, mask\n\n\nclass KITTIInput(Input):\n    def __init__(self, data, batch_size, dims, *,\n                 num_threads=1, normalize=True,\n                 skipped_frames=False):\n        super().__init__(data, batch_size, dims, num_threads=num_threads,\n                         normalize=normalize, skipped_frames=skipped_frames)\n\n    def _preprocess_flow(self, gt):\n        flow, mask = gt\n        height, width = self.dims\n        # Reshape to tell tensorflow we know the size statically\n        flow = tf.reshape(self._resize_crop_or_pad(flow), [height, width, 2])\n        mask = tf.reshape(self._resize_crop_or_pad(mask), [height, width, 1])\n        return flow, mask\n\n    def _input_flow(self, flow_dir, hold_out_inv):\n        flow_dir_occ = os.path.join(self.data.current_dir, flow_dir, \'flow_occ\')\n        flow_dir_noc = os.path.join(self.data.current_dir, flow_dir, \'flow_noc\')\n        filenames_gt_occ = []\n        filenames_gt_noc = []\n        flow_files_occ = os.listdir(flow_dir_occ)\n        flow_files_occ.sort()\n        flow_files_noc = os.listdir(flow_dir_noc)\n        flow_files_noc.sort()\n\n        if hold_out_inv is not None:\n            random.seed(0)\n            random.shuffle(flow_files_noc)\n            flow_files_noc = flow_files_noc[:hold_out_inv]\n\n            random.seed(0)\n            random.shuffle(flow_files_occ)\n            flow_files_occ = flow_files_occ[:hold_out_inv]\n\n        assert len(flow_files_noc) == len(flow_files_occ)\n\n        for i in range(len(flow_files_occ)):\n            filenames_gt_occ.append(os.path.join(flow_dir_occ,\n                                                 flow_files_occ[i]))\n            filenames_gt_noc.append(os.path.join(flow_dir_noc,\n                                                 flow_files_noc[i]))\n\n        flow_occ, mask_occ = self._preprocess_flow(\n            _read_flow(filenames_gt_occ, 1))\n        flow_noc, mask_noc = self._preprocess_flow(\n            _read_flow(filenames_gt_noc, 1))\n        return flow_occ, mask_occ, flow_noc, mask_noc\n\n    def _input_train(self, image_dir, flow_dir, hold_out_inv=None):\n        input_shape, im1, im2 = self._input_images(image_dir, hold_out_inv)\n        flow_occ, mask_occ, flow_noc, mask_noc = self._input_flow(flow_dir, hold_out_inv)\n        return tf.train.batch(\n            [im1, im2, input_shape, flow_occ, mask_occ, flow_noc, mask_noc],\n            batch_size=self.batch_size,\n            num_threads=self.num_threads,\n            allow_smaller_final_batch=True)\n\n    def input_train_gt(self, hold_out):\n        img_dirs = [\'data_scene_flow/training/image_2\',\n                    \'data_stereo_flow/training/colored_0\']\n        gt_dirs = [\'data_scene_flow/training/flow_occ\',\n                   \'data_stereo_flow/training/flow_occ\']\n\n        height, width = self.dims\n\n        filenames = []\n        for img_dir, gt_dir in zip(img_dirs, gt_dirs):\n            dataset_filenames = []\n            img_dir = os.path.join(self.data.current_dir, img_dir)\n            gt_dir = os.path.join(self.data.current_dir, gt_dir)\n            img_files = os.listdir(img_dir)\n            gt_files = os.listdir(gt_dir)\n            img_files.sort()\n            gt_files.sort()\n            assert len(img_files) % 2 == 0 and len(img_files) / 2 == len(gt_files)\n\n            for i in range(len(gt_files)):\n                fn_im1 = os.path.join(img_dir, img_files[2 * i])\n                fn_im2 = os.path.join(img_dir, img_files[2 * i + 1])\n                fn_gt = os.path.join(gt_dir, gt_files[i])\n                dataset_filenames.append((fn_im1, fn_im2, fn_gt))\n\n            random.seed(0)\n            random.shuffle(dataset_filenames)\n            dataset_filenames = dataset_filenames[hold_out:]\n            filenames.extend(dataset_filenames)\n\n        random.seed(0)\n        random.shuffle(filenames)\n\n        #shift = shift % len(filenames)\n        #filenames_ = list(np.roll(filenames, shift))\n\n        fns_im1, fns_im2, fns_gt = zip(*filenames)\n        fns_im1 = list(fns_im1)\n        fns_im2 = list(fns_im2)\n        fns_gt = list(fns_gt)\n\n        im1 = read_png_image(fns_im1)\n        im2 = read_png_image(fns_im2)\n        flow_gt, mask_gt = _read_flow(fns_gt)\n\n        gt_queue = tf.train.string_input_producer(fns_gt,\n            shuffle=False, capacity=len(fns_gt), num_epochs=None)\n        reader = tf.WholeFileReader()\n        _, gt_value = reader.read(gt_queue)\n        gt_uint16 = tf.image.decode_png(gt_value, dtype=tf.uint16)\n        gt = tf.cast(gt_uint16, tf.float32)\n\n        im1, im2, gt = random_crop([im1, im2, gt],\n                                   [height, width, 3])\n        flow_gt = (gt[:, :, 0:2] - 2 ** 15) / 64.0\n        mask_gt = gt[:, :, 2:3]\n\n        if self.normalize:\n            im1 = self._normalize_image(im1)\n            im2 = self._normalize_image(im2)\n\n        return tf.train.batch(\n            [im1, im2, flow_gt, mask_gt],\n            batch_size=self.batch_size,\n            num_threads=self.num_threads)\n\n    def input_train_2015(self, hold_out_inv=None):\n        return self._input_train(\'data_scene_flow/training/image_2\',\n                                 \'data_scene_flow/training\',\n                                 hold_out_inv)\n\n    def input_test_2015(self, hold_out_inv=None):\n        return self._input_test(\'data_scene_flow/testing/image_2\', hold_out_inv)\n\n    def input_train_2012(self, hold_out_inv=None):\n        return self._input_train(\'data_stereo_flow/training/colored_0\',\n                                 \'data_stereo_flow/training\',\n                                 hold_out_inv)\n\n    def input_test_2012(self, hold_out_inv=None):\n        return self._input_test(\'data_stereo_flow/testing/colored_0\', hold_out_inv)\n'"
src/e2eflow/kitti/raw_records.py,0,"b'def get_kitti_records(development):\n        # All records from city, residential and road datasets\n        return dev_records if development else all_records\n\n\ndev_records = [\n    ""2011_09_26_drive_0001""\n]\n\n\nall_records = [\n    # City\n    ""2011_09_26_drive_0001"",\n    ""2011_09_26_drive_0002"",\n    ""2011_09_26_drive_0005"",\n    ""2011_09_26_drive_0009"",\n    ""2011_09_26_drive_0011"",\n    ""2011_09_26_drive_0013"",\n    ""2011_09_26_drive_0014"",\n    ""2011_09_26_drive_0017"",\n    ""2011_09_26_drive_0018"",\n    ""2011_09_26_drive_0048"",\n    ""2011_09_26_drive_0051"",\n    ""2011_09_26_drive_0056"",\n    ""2011_09_26_drive_0057"",\n    ""2011_09_26_drive_0059"",\n    ""2011_09_26_drive_0060"",\n    ""2011_09_26_drive_0084"",\n    ""2011_09_26_drive_0091"",\n    ""2011_09_26_drive_0093"",\n    ""2011_09_26_drive_0095"",\n    ""2011_09_26_drive_0096"",\n    ""2011_09_26_drive_0104"",\n    ""2011_09_26_drive_0106"",\n    ""2011_09_26_drive_0113"",\n    ""2011_09_26_drive_0117"",\n    ""2011_09_28_drive_0001"",\n    ""2011_09_28_drive_0002"",\n    ""2011_09_29_drive_0026"",\n    ""2011_09_29_drive_0071"",\n     # Residential\n    ""2011_09_26_drive_0019"",\n    ""2011_09_26_drive_0020"",\n    ""2011_09_26_drive_0022"",\n    ""2011_09_26_drive_0023"",\n    ""2011_09_26_drive_0035"",\n    ""2011_09_26_drive_0036"",\n    ""2011_09_26_drive_0039"",\n    ""2011_09_26_drive_0046"",\n    ""2011_09_26_drive_0061"",\n    ""2011_09_26_drive_0064"",\n    ""2011_09_26_drive_0079"",\n    ""2011_09_26_drive_0086"",\n    ""2011_09_26_drive_0087"",\n    ""2011_09_30_drive_0018"",\n    ""2011_09_30_drive_0020"",\n    ""2011_09_30_drive_0027"",\n    ""2011_09_30_drive_0028"",\n    ""2011_09_30_drive_0033"",\n    ""2011_09_30_drive_0034"",\n    ""2011_10_03_drive_0027"",\n    ""2011_10_03_drive_0034"",\n    # Road\n    ""2011_09_26_drive_0015"",\n    ""2011_09_26_drive_0027"",\n    ""2011_09_26_drive_0028"",\n    ""2011_09_26_drive_0029"",\n    ""2011_09_26_drive_0032"",\n    ""2011_09_26_drive_0052"",\n    ""2011_09_26_drive_0070"",\n    ""2011_09_26_drive_0101"",\n    ""2011_09_29_drive_0004"",\n    ""2011_09_30_drive_0016"",\n    ""2011_10_03_drive_0042"",\n    #""2011_10_03_drive_0047""\n]\n'"
src/e2eflow/middlebury/__init__.py,0,b''
src/e2eflow/middlebury/data.py,0,"b'import os\nimport sys\n\nimport numpy as np\nimport matplotlib.image as mpimg\n\nfrom ..core.data import Data\nfrom ..util import tryremove\n\n\nclass MiddleburyData(Data):\n    MDB_FLOW_URL = \'http://vision.middlebury.edu/flow/data/comp/zip/other-gt-flow.zip\'\n    MDB_COLOR_URL = \'http://vision.middlebury.edu/flow/data/comp/zip/other-color-twoframes.zip\'\n    MDB_EVAL_URL = \'http://vision.middlebury.edu/flow/data/comp/zip/eval-color-twoframes.zip\'\n\n    dirs = [\'middlebury\']\n\n    def __init__(self, data_dir, stat_log_dir=None,\n                 development=True, fast_dir=None):\n        super().__init__(data_dir, stat_log_dir,\n                         development=development,\n                         fast_dir=fast_dir)\n\n    def _fetch_if_missing(self):\n        local_path = os.path.join(self.data_dir, \'middlebury\')\n        if not os.path.isdir(local_path):\n            self._download_and_extract(self.MDB_FLOW_URL, local_path)\n            self._download_and_extract(self.MDB_COLOR_URL, local_path)\n            self._download_and_extract(self.MDB_EVAL_URL, local_path)\n\n        for name in [\'Beanbags\', \'DogDance\', \'MiniCooper\', \'Walking\']:\n          tryremove(os.path.join(local_path, \'other-data\', name))\n\n    def get_raw_dirs(self):\n        raise NotImplementedError(""Can not train on middlebury"")\n'"
src/e2eflow/middlebury/input.py,20,"b'import os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..core.input import read_png_image, Input\n\n\ndef _read_flow(filenames, num_epochs=None):\n    """"""Given a list of filenames, constructs a reader op for ground truth flow files.""""""\n    filename_queue = tf.train.string_input_producer(filenames,\n        shuffle=False, capacity=len(filenames), num_epochs=num_epochs)\n    reader = tf.WholeFileReader()\n    _, value = reader.read(filename_queue)\n    value = tf.reshape(value, [1])\n    value_width = tf.substr(value, 4, 4)\n    value_height = tf.substr(value, 8, 4)\n    width = tf.reshape(tf.decode_raw(value_width, out_type=tf.int32), [])\n    height = tf.reshape(tf.decode_raw(value_height, out_type=tf.int32), [])\n\n    value_flow = tf.substr(value, 12, 8 * width * height)\n    flow = tf.decode_raw(value_flow, out_type=tf.float32)\n    flow = tf.reshape(flow, [height, width, 2])\n    mask = tf.to_float(tf.logical_and(flow[:, :, 0] < 1e9, flow[:, :, 1] < 1e9))\n    mask = tf.reshape(mask, [height, width, 1])\n\n    return flow, mask\n\n\ndef _read_binary(filenames, num_epochs=None):\n    """"""Given a list of filenames, constructs a reader op for ground truth binary files.""""""\n    filename_queue = tf.train.string_input_producer(filenames,\n        shuffle=False, capacity=len(filenames), num_epochs=num_epochs)\n    reader = tf.WholeFileReader()\n    _, value = reader.read(filename_queue)\n    value_decoded = tf.image.decode_png(value, channels=1)\n    return tf.cast(value_decoded, tf.float32)\n\n\ndef _get_filenames(parent_dir, ignore_last=False):\n    filenames = []\n    for sub_name in sorted(os.listdir(parent_dir)):\n        sub_dir = os.path.join(parent_dir, sub_name)\n        sub_filenames = os.listdir(sub_dir)\n        sub_filenames.sort()\n        if ignore_last:\n            sub_filenames = sub_filenames[:-1]\n        for filename in sub_filenames:\n            filenames.append(os.path.join(sub_dir, filename))\n\n    return filenames\n\n\nclass MiddleburyInput(Input):\n    def __init__(self, data, batch_size, dims, *,\n                 num_threads=1, normalize=True):\n        super().__init__(data, batch_size, dims, num_threads=num_threads,\n                         normalize=normalize)\n\n    def _preprocess_flow(self, t, channels):\n        height, width = self.dims\n        # Reshape to tell tensorflow we know the size statically\n        return tf.reshape(self._resize_crop_or_pad(t), [height, width, channels])\n\n    def _input_images(self, image_dir):\n        """"""Assumes that paired images are next to each other after ordering the\n        files.\n        """"""\n        image_dir = os.path.join(self.data.current_dir, image_dir)\n\n        filenames_1 = []\n        filenames_2 = []\n\n        for sub_name in sorted(os.listdir(image_dir)):\n            sub_dir = os.path.join(image_dir, sub_name)\n            sub_filenames = os.listdir(sub_dir)\n            sub_filenames.sort()\n            for i in range(len(sub_filenames) - 1):\n                filenames_1.append(os.path.join(sub_dir, sub_filenames[i]))\n                filenames_2.append(os.path.join(sub_dir, sub_filenames[i + 1]))\n\n        input_1 = read_png_image(filenames_1, 1)\n        input_2 = read_png_image(filenames_2, 1)\n        image_1 = self._preprocess_image(input_1)\n        image_2 = self._preprocess_image(input_2)\n        return tf.shape(input_1), image_1, image_2\n\n    def _input_flow(self):\n        flow_dir = os.path.join(self.data.current_dir, \'middlebury/other-gt-flow\')\n        flow_files = _get_filenames(flow_dir)\n\n        flow, mask = _read_flow(flow_files, 1)\n        flow = self._preprocess_flow(flow, 2)\n        mask = self._preprocess_flow(mask, 1)\n        return flow, mask\n\n    def _input_train(self):\n        input_shape, im1, im2 = self._input_images(\'middlebury/other-data\')\n        flow, mask = self._input_flow()\n        return tf.train.batch(\n            [im1, im2, input_shape, flow, mask],\n            batch_size=self.batch_size,\n            num_threads=self.num_threads,\n            allow_smaller_final_batch=True)\n\n    def input_train(self):\n        return self._input_train()\n\n    def input_test(self):\n       input_shape, im1, im2 = self._input_images(\'middlebury/eval-data\')\n       return tf.train.batch(\n          [im1, im2, input_shape],\n          batch_size=self.batch_size,\n          num_threads=self.num_threads,\n          allow_smaller_final_batch=True)\n'"
src/e2eflow/sintel/__init__.py,0,b''
src/e2eflow/sintel/data.py,0,"b""import os\nimport sys\n\nimport numpy as np\nimport matplotlib.image as mpimg\n\nfrom ..core.data import Data\nfrom ..util import tryremove\n\n\nclass SintelData(Data):\n    SINTEL_URL = 'http://files.is.tue.mpg.de/sintel/MPI-Sintel-complete.zip'\n\n    dirs = ['sintel']\n\n    def __init__(self, data_dir, stat_log_dir=None,\n                 development=True, fast_dir=None):\n        super().__init__(data_dir, stat_log_dir,\n                         development=development,\n                         fast_dir=fast_dir)\n\n    def _fetch_if_missing(self):\n        local_path = os.path.join(self.data_dir, 'sintel')\n        if not os.path.isdir(local_path):\n            self._download_and_extract(self.SINTEL_URL, local_path)\n\n    def get_raw_dirs(self):\n        dirs = []\n        for folder in ['training/clean', 'training/final', 'test/clean', 'test/final']:\n            top_dir = os.path.join(self.current_dir, 'sintel/' + folder)\n            for sub_dir in os.listdir(top_dir):\n              dirs.append(os.path.join(top_dir, sub_dir))\n        return dirs\n"""
src/e2eflow/sintel/input.py,19,"b'import os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..core.input import read_png_image, Input\n\n\ndef _read_flow(filenames, num_epochs=None):\n    """"""Given a list of filenames, constructs a reader op for ground truth flow files.""""""\n    filename_queue = tf.train.string_input_producer(filenames,\n        shuffle=False, capacity=len(filenames), num_epochs=num_epochs)\n    reader = tf.WholeFileReader()\n    _, value = reader.read(filename_queue)\n    value = tf.reshape(value, [1])\n    value_width = tf.substr(value, 4, 4)\n    value_height = tf.substr(value, 8, 4)\n    width = tf.reshape(tf.decode_raw(value_width, out_type=tf.int32), [])\n    height = tf.reshape(tf.decode_raw(value_height, out_type=tf.int32), [])\n\n    value_flow = tf.substr(value, 12, 8 * 436 * 1024)\n    flow = tf.decode_raw(value_flow, out_type=tf.float32)\n\n    return tf.reshape(flow, [436, 1024, 2])\n\n\ndef _read_binary(filenames, num_epochs=None):\n    """"""Given a list of filenames, constructs a reader op for ground truth binary files.""""""\n    filename_queue = tf.train.string_input_producer(filenames,\n        shuffle=False, capacity=len(filenames), num_epochs=num_epochs)\n    reader = tf.WholeFileReader()\n    _, value = reader.read(filename_queue)\n    value_decoded = tf.image.decode_png(value, channels=1)\n    return tf.cast(value_decoded, tf.float32)\n\n\ndef _get_filenames(parent_dir, ignore_last=False):\n    filenames = []\n    for sub_name in sorted(os.listdir(parent_dir)):\n        sub_dir = os.path.join(parent_dir, sub_name)\n        sub_filenames = os.listdir(sub_dir)\n        sub_filenames.sort()\n        if ignore_last:\n            sub_filenames = sub_filenames[:-1]\n        for filename in sub_filenames:\n            filenames.append(os.path.join(sub_dir, filename))\n\n    return filenames\n\n\nclass SintelInput(Input):\n    def __init__(self, data, batch_size, dims, *,\n                 num_threads=1, normalize=True):\n        super().__init__(data, batch_size, dims, num_threads=num_threads,\n                         normalize=normalize)\n\n    def _preprocess_flow(self, t, channels):\n        height, width = self.dims\n        # Reshape to tell tensorflow we know the size statically\n        return tf.reshape(self._resize_crop_or_pad(t), [height, width, channels])\n\n    def _input_images(self, image_dir):\n        """"""Assumes that paired images are next to each other after ordering the\n        files.\n        """"""\n        image_dir = os.path.join(self.data.current_dir, image_dir)\n\n        filenames_1 = []\n        filenames_2 = []\n\n        for sub_name in sorted(os.listdir(image_dir)):\n            sub_dir = os.path.join(image_dir, sub_name)\n            sub_filenames = os.listdir(sub_dir)\n            sub_filenames.sort()\n            for i in range(len(sub_filenames) - 1):\n                filenames_1.append(os.path.join(sub_dir, sub_filenames[i]))\n                filenames_2.append(os.path.join(sub_dir, sub_filenames[i + 1]))\n\n        input_1 = read_png_image(filenames_1, 1)\n        input_2 = read_png_image(filenames_2, 1)\n        image_1 = self._preprocess_image(input_1)\n        image_2 = self._preprocess_image(input_2)\n        return tf.shape(input_1), image_1, image_2\n\n    def _input_flow(self):\n        flow_dir = os.path.join(self.data.current_dir, \'sintel/training/flow\')\n        invalid_dir = os.path.join(self.data.current_dir, \'sintel/training/invalid\')\n        occ_dir = os.path.join(self.data.current_dir, \'sintel/training/occlusions\')\n        flow_files = _get_filenames(flow_dir)\n        invalid_files = _get_filenames(invalid_dir, ignore_last=True)\n        occ_files = _get_filenames(occ_dir)\n\n        assert len(flow_files) == len(invalid_files) == len(occ_files)\n\n        flow = self._preprocess_flow(_read_flow(flow_files, 1), 2)\n        invalid = self._preprocess_flow(_read_binary(invalid_files), 1)\n        occ = self._preprocess_flow(_read_binary(occ_files), 1)\n\n        flow_occ = flow\n        flow_noc = flow * (1 - occ)\n        mask_occ = (1 - invalid)\n        mask_noc = mask_occ * (1 - occ)\n\n        return flow_occ, mask_occ, flow_noc, mask_noc\n\n    def _input_train(self, image_dir):\n        input_shape, im1, im2 = self._input_images(image_dir)\n        flow_occ, mask_occ, flow_noc, mask_noc = self._input_flow()\n        return tf.train.batch(\n            [im1, im2, input_shape, flow_occ, mask_occ, flow_noc, mask_noc],\n            batch_size=self.batch_size,\n            num_threads=self.num_threads,\n            allow_smaller_final_batch=True)\n\n    def input_train_clean(self):\n        return self._input_train(\'sintel/training/clean\')\n\n    def input_train_final(self):\n        return self._input_train(\'sintel/training/final\')\n\n    def input_test_clean(self):\n        input_shape, im1, im2 = self._input_images(\'sintel/test/clean\')\n        return tf.train.batch(\n           [im1, im2, input_shape],\n           batch_size=self.batch_size,\n           num_threads=self.num_threads,\n           allow_smaller_final_batch=True)\n\n    def input_test_final(self):\n        input_shape, im1, im2 = self._input_images(\'sintel/test/final\')\n        return tf.train.batch(\n           [im1, im2, input_shape],\n           batch_size=self.batch_size,\n           num_threads=self.num_threads,\n           allow_smaller_final_batch=True)\n'"
src/e2eflow/synthia/__init__.py,0,b''
src/e2eflow/synthia/data.py,0,"b""import os\nimport sys\n\nimport numpy as np\nimport matplotlib.image as mpimg\n\nfrom ..core.data import Data\nfrom ..util import tryremove\n\nURL = 'http://synthia-dataset.cvc.uab.cat/SYNTHIA_SEQS/'\nSEQS = [ # SUMMER and WINTER from sequences `1 - 6`\n    'SYNTHIA-SEQS-01-SUMMER',\n    'SYNTHIA-SEQS-01-WINTER',\n    'SYNTHIA-SEQS-02-SUMMER',\n    'SYNTHIA-SEQS-02-WINTER',\n    'SYNTHIA-SEQS-04-SUMMER',\n    'SYNTHIA-SEQS-04-WINTER',\n    'SYNTHIA-SEQS-05-SUMMER',\n    'SYNTHIA-SEQS-05-WINTER',\n    'SYNTHIA-SEQS-06-SUMMER',\n    'SYNTHIA-SEQS-06-WINTER'\n]\n\nDEV_SEQS = ['SYNTHIA-SEQS-01-SUMMER']\n\n\nclass SynthiaData(Data):\n    dirs = ['synthia']\n\n    def __init__(self, data_dir, stat_log_dir=None,\n                 development=True, fast_dir=None):\n        super().__init__(data_dir, stat_log_dir,\n                         development=development,\n                         fast_dir=fast_dir)\n\n    def _fetch_if_missing(self):\n        self._maybe_get_synthia()\n\n    def get_raw_dirs(self):\n        root_dir = os.path.join(self.current_dir, 'synthia')\n        dirs = []\n        seqs = os.listdir(root_dir)\n        for seq in seqs:\n            seq_dir = os.path.join(root_dir, seq, seq, 'RGB', 'Stereo_Left')\n            views = os.listdir(seq_dir)\n            for view in views:\n                view_dir = os.path.join(seq_dir, view)\n                dirs.extend([view_dir])\n        return dirs\n\n    def _maybe_get_synthia(self):\n        seqs = DEV_SEQS if self.development else SEQS\n        for seq in seqs:\n            root_dir = os.path.join(self.data_dir, 'synthia')\n            url = URL + seq + '.rar'\n            url_dir = os.path.join(root_dir, seq)\n            if not os.path.isdir(url_dir):\n                self._download_and_extract(url, url_dir, 'rar')\n\n            # Remove unused directories\n            tryremove(os.path.join(url_dir, seq, 'GT'))\n            tryremove(os.path.join(url_dir, seq, 'Depth'))\n            tryremove(os.path.join(url_dir, seq, 'CameraParams'))\n            tryremove(os.path.join(url_dir, 'RGB', 'Stereo_Right'))\n"""
src/e2eflow/test/__init__.py,0,b''
src/e2eflow/test/test_image_warp.py,4,"b""import numpy as np\nimport tensorflow as tf\n\nfrom ..core.image_warp import image_warp\n\n\nclass ImageWarpTest(tf.test.TestCase):\n    def _warp_test(self, first, second, flow, debug=False):\n        num_batch, height, width, channels = second.shape\n        second_ = tf.placeholder(tf.float32, shape=second.shape, name='im')\n        flow_ = tf.placeholder(tf.float32, shape=flow.shape, name='flow')\n        inv_warped_second = image_warp(second_, flow_)\n\n        sess = tf.Session()\n        pred = sess.run(inv_warped_second,\n            feed_dict={second_: second, flow_: flow})\n        if debug:\n            print('-- result channels')\n            for c in range(channels):\n                print(np.reshape(pred[:, :, :, c], [height, width]))\n        self.assertAllClose(first, pred)\n\n    def test_move(self):\n        first = [\n            [0, 0, 0, 0],\n            [0, 1, 0.5, 0],\n            [0, 0.3, 0.4, 0],\n            [0, 0, 0, 0]]\n        second = [\n            [0, 1, 0, 0],\n            [0, 0, 0, 0.5],\n            [0.3, 0, 0, 0],\n            [0, 0, 0.4, 0]]\n        zero = [0, 0]\n        flow = [\n            [zero, [-1, 0], zero, zero],\n            [zero, [0, -1], [1, 0], [0, -1]],\n            [[0, -1], [-1, 0], [0, 1], zero],\n            [zero, zero, [0, -1], zero]]\n\n        self._warp_test(\n            np.reshape(first, [1, 4, 4, 1]),\n            np.reshape(second, [1, 4, 4, 1]),\n            np.reshape(flow, [1, 4, 4, 2]))\n\n    def test_batches(self):\n        # Make sure that batches do not interfere with each other\n        first_1 = [\n            [0, 0, 0, 0],\n            [0, 1, 0.5, 0],\n            [0, 0.3, 0.4, 0],\n            [0, 0, 0, 0]]\n        second_1 = [\n            [0, 1, 0, 0],\n            [0, 0, 0, 0.5],\n            [0.3, 0, 0, 0],\n            [0, 0, 0.4, 0]]\n        zero = [0, 0]\n        flow_1 = [\n            [zero, [-1, 0], zero, zero],\n            [zero, [0, -1], [1, 0], [0, -1]],\n            [[0, -1], [-1, 0], [0, 1], zero],\n            [zero, zero, [0, -1], zero]]\n        first_2 = np.zeros([4, 4])\n        second_2 = np.zeros([4, 4])\n        flow_2 = np.ones([4, 4, 2])\n        first = np.concatenate([first_1, first_2, first_1])\n        second = np.concatenate([second_1, second_2, second_1])\n        flow = np.concatenate([flow_1, flow_2, flow_1])\n\n        self._warp_test(\n            np.reshape(first, [3, 4, 4, 1]),\n            np.reshape(second, [3, 4, 4, 1]),\n            np.reshape(flow, [3, 4, 4, 2]))\n\n    def test_interpolate(self):\n        first = [\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 2.1]]\n        second = [\n            [0, 0, 0, 0],\n            [0, 1, 2, 0],\n            [0, 3, 4, 0],\n            [0, 0, 0, 0]]\n        zero = [0, 0]\n        flow = [\n            [zero, zero, zero, zero],\n            [zero, [-2, -2], [-2, -2], zero],\n            [zero, [-2, -2], [-2, -2], zero],\n            [zero, zero, zero, [-1.7, -1.6]]]\n\n        self._warp_test(\n            np.reshape(first, [1, 4, 4, 1]),\n            np.reshape(second, [1, 4, 4, 1]),\n            np.reshape(flow, [1, 4, 4, 2]))\n"""
src/e2eflow/test/test_losses.py,16,"b""import numpy as np\nimport tensorflow as tf\n\nfrom ..core.losses import _smoothness_deltas, create_outgoing_mask, \\\n    gradient_loss, compute_losses, ternary_loss\n\nfrom ..core.input import read_png_image\n\n\nclass LossesTest(tf.test.TestCase):\n    def test_smoothness_deltas(self):\n        flow = np.ones([1,3,3,2], np.float32)\n        flow[0, :, :, 0] = [[0,0,0],\n                            [0,8,3],\n                            [0,1,0]]\n        flow[0, :, :, 1] = [[0,0,0],\n                            [0,8,3],\n                            [0,1,0]]\n        delta_u_, delta_v_, mask_ = _smoothness_deltas(flow)\n        delta_u_ = tf.multiply(delta_u_, mask_)\n        delta_v_ = tf.multiply(delta_v_, mask_)\n        sess = tf.Session()\n        delta_u, delta_v, mask = sess.run([delta_u_, delta_v_, mask_])\n        self.assertAllEqual(mask[0,:,:,0], [[1,1,0],\n                                            [1,1,0],\n                                            [1,1,0]])\n        self.assertAllEqual(mask[0,:,:,1], [[1,1,1],\n                                            [1,1,1],\n                                            [0,0,0]])\n        self.assertAllEqual(delta_u[0,:,:,0], [[0,0,0],\n                                               [-8,5,0],\n                                               [-1,1,0]])\n        self.assertAllEqual(delta_u[0,:,:,1], [[0,-8,-3],\n                                               [0,7,3],\n                                               [0,0,0]])\n        self.assertAllEqual(delta_v[0,:,:,0], [[0,0,0],\n                                               [-8,5,0],\n                                               [-1,1,0]])\n        self.assertAllEqual(delta_v[0,:,:,1], [[0,-8,-3],\n                                               [0,7,3],\n                                               [0,0,0]])\n\n    def test_create_outgoing_mask_all_directions(self):\n       flow = np.ones([1,3,3,2], np.float32)\n       flow[0, :, :, 0] = [[0,0,1],\n                           [-1,3,0],\n                           [0,1,0]]\n       flow[0, :, :, 1] = [[-1,0,0],\n                           [0,0,0],\n                           [1,-1,0]]\n       sess = tf.Session()\n       mask = sess.run(create_outgoing_mask(flow))\n       self.assertAllEqual(mask[0,:,:,0], [[0,1,0],\n                                           [0,0,1],\n                                           [0,1,1]])\n    def test_create_outgoing_mask_large_movement(self):\n       flow = np.ones([1,3,3,2], np.float32)\n       flow[0, :, :, 0] = [[3,2,1],\n                           [2,1,0],\n                           [0,-2,-1]]\n       flow[0, :, :, 1] = [[0,0,0],\n                           [0,0,0],\n                           [0,0,0]]\n       sess = tf.Session()\n       mask = sess.run(create_outgoing_mask(flow))\n       self.assertAllEqual(mask[0,:,:,0], [[0,0,0],\n                                           [1,1,1],\n                                           [1,0,1]])\n\n    # def test_forward_backward_loss(self):\n    #     im1 = np.ones([1,3,3,3], np.float32)\n    #     im2 = np.ones([1,3,3,3], np.float32)\n    #     mask = np.ones([1,3,3,1], np.float32)\n    #     mask[0, :, :, 0] = [[1,1,0],\n    #                         [1,1,0],\n    #                         [0,0,0]]\n    #\n    #     flow_fw = np.ones([1,3,3,2], np.float32)\n    #     flow_fw[0, :, :, 0] = [[1,1,1],\n    #                           [1,1,1],\n    #                           [1,1,1]]\n    #     flow_fw[0, :, :, 1] = [[1,1,1],\n    #                           [1,1,1],\n    #                           [1,1,1]]\n    #     flow_bw = np.ones([1,3,3,2], np.float32)\n    #     flow_bw[0, :, :, 0] = [[-1,-1,-1],\n    #                           [-1,-1,-1],\n    #                           [-1,-1,-1]]\n    #     flow_bw[0, :, :, 1] = [[-1,-1,-1],\n    #                           [-1,-1,-1],\n    #                           [-1,-1,-1]]\n    #\n    #     sess = tf.Session()\n    #     losses = sess.run(compute_losses(im1, im2, flow_fw, flow_bw, mask))\n    #     self.assertAllClose(losses['fb'], 0.0, atol=1e-2)\n\n    def test_gradient_loss(self):\n        im1 = np.ones([1,3,3,3], np.float32)\n        im2 = np.ones([1,3,3,3], np.float32)\n        mask = np.ones([1,3,3,1], np.float32)\n        im1[0, :, :, 0] = [[0,1,0],\n                            [0,2,0],\n                            [0,3,4]]\n        im1[0, :, :, 1] = [[0,1,0],\n                            [0,2,0],\n                            [0,3,4]]\n        im1[0, :, :, 2] = [[0,1,0],\n                            [0,2,0],\n                            [0,3,4]]\n        im2[0, :, :, 0] = [[1,2,1],\n                           [1,3,1],\n                           [1,4,5]]\n        im2[0, :, :, 1] = [[1,2,1],\n                           [1,3,1],\n                           [1,4,5]]\n        im2[0, :, :, 2] = [[1,2,1],\n                           [1,3,1],\n                           [1,4,5]]\n        sess = tf.Session()\n        loss = sess.run(gradient_loss(im1, im2, mask))\n        self.assertAllClose(loss, 0.0, atol=1e-2)\n\n    def test_ternary_reference(self):\n        def _ternary_reference_test(im1_name, im2_name, expected):\n            with self.test_session(use_gpu=True) as sess:\n                im1 = tf.expand_dims(read_png_image([im1_name]), 0)\n                im2 = tf.expand_dims(read_png_image([im2_name]), 0)\n                _, height, width, _ = tf.unstack(tf.shape(im1))\n                mask = tf.ones([1, height, width, 1])\n\n                sess.run(tf.global_variables_initializer())\n                coord = tf.train.Coordinator()\n                threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n                scale = tf.cast(height * width, tf.float32)\n                loss_ = ternary_loss(im1, im2, mask, max_distance=3, truncate=22) * scale\n                loss = sess.run(loss_)\n                print(loss)\n                #self.assertAllClose(loss, expected)\n\n        _ternary_reference_test('../test_data/frame_0011.png',\n                                '../test_data/frame_0012.png',\n                                8.86846e+06)\n        _ternary_reference_test('../test_data/frame_0016.png',\n                                '../test_data/frame_0017.png',\n                                6.75537e+06)\n        _ternary_reference_test('../test_data/frame_0018.png',\n                                '../test_data/frame_0019.png',\n                                8.22283e+06)\n        _ternary_reference_test('../test_data/frame_0028.png',\n                                '../test_data/frame_0029.png',\n                                8.05619e+06)\n"""
src/e2eflow/test/ops/backward_warp.py,4,"b'import tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.ops import gradient_checker\n\nfrom ... import ops\n\n\nclass BackwardWarpTest(tf.test.TestCase):\n    def _warp_test(self, first, second, flow, debug=False):\n        with self.test_session(use_gpu=True) as sess:\n            num_batch, height, width, channels = second.shape\n            #second_ = tf.placeholder(tf.float32, shape=second.shape, name=\'im\')\n            flow_ = tf.placeholder(tf.float32, shape=flow.shape, name=\'flow\')\n            inv_warped_second = ops.backward_warp(second, flow_)\n\n            pred = sess.run(inv_warped_second, feed_dict={flow_: flow})\n            if debug:\n                print(\'-- result channels\')\n                for c in range(channels):\n                    print(np.reshape(pred[0, :, :, c], [height, width]))\n            self.assertAllClose(first, pred)\n\n            jacob_t, jacob_n = gradient_checker.compute_gradient(flow_, flow.shape,\n                                                                 inv_warped_second, pred.shape)\n            self.assertAllClose(jacob_t, jacob_n, 1e-3, 1e-3)\n\n    def test_move(self):\n        first = [\n            [0, 0, 0, 0],\n            [0, 1, 0.5, 0],\n            [0, 0.3, 0.4, 0],\n            [0, 0, 0, 0]]\n        second = [\n            [0, 1, 0, 0],\n            [0, 0, 0, 0.5],\n            [0.3, 0, 0, 0],\n            [0, 0, 0.4, 0]]\n        zero = [0, 0]\n        flow = [\n            [zero, [-1, 0], zero, zero],\n            [zero, [0, -1], [1, 0], [0, -1]],\n            [[0, -1], [-1, 0], [0, 1], zero],\n            [zero, zero, [0, -1], zero]]\n\n        self._warp_test(\n            np.reshape(first, [1, 4, 4, 1]),\n            np.reshape(second, [1, 4, 4, 1]),\n            np.reshape(flow, [1, 4, 4, 2]))\n\n    def test_batches(self):\n        # Make sure that batches do not interfere with each other\n        first_1 = [\n            [0, 0, 0, 0],\n            [0, 1, 0.5, 0],\n            [0, 0.3, 0.4, 0],\n            [0, 0, 0, 0]]\n        second_1 = [\n            [0, 1, 0, 0],\n            [0, 0, 0, 0.5],\n            [0.3, 0, 0, 0],\n            [0, 0, 0.4, 0]]\n        zero = [0, 0]\n        flow_1 = [\n            [zero, [-1, 0], zero, zero],\n            [zero, [0, -1], [1, 0], [0, -1]],\n            [[0, -1], [-1, 0], [0, 1], zero],\n            [zero, zero, [0, -1], zero]]\n        first_2 = np.zeros([4, 4])\n        second_2 = np.zeros([4, 4])\n        flow_2 = np.zeros([4, 4, 2])\n        first = np.concatenate([first_1, first_2, first_1])\n        second = np.concatenate([second_1, second_2, second_1])\n        flow = np.concatenate([flow_1, flow_2, flow_1])\n\n        self._warp_test(\n            np.reshape(first, [3, 4, 4, 1]),\n            np.reshape(second, [3, 4, 4, 1]),\n            np.reshape(flow, [3, 4, 4, 2]))\n\n    def test_interpolate(self):\n        first = [\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 2.1]]\n        second = [\n            [0, 0, 0, 0],\n            [0, 1, 2, 0],\n            [0, 3, 4, 0],\n            [0, 0, 0, 0]]\n        zero = [0, 0]\n        flow = [\n            [zero, zero, zero, zero],\n            [zero, [-2, -2], [-2, -2], zero],\n            [zero, [-2, -2], [-2, -2], zero],\n            [zero, zero, zero, [-1.7, -1.6]]]\n\n        self._warp_test(\n            np.reshape(first, [1, 4, 4, 1]),\n            np.reshape(second, [1, 4, 4, 1]),\n            np.reshape(flow, [1, 4, 4, 2]))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
src/e2eflow/test/ops/correlation.py,4,"b'import tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.ops import gradient_checker\n\nfrom ... import ops\n\n\n# NOTE: the tests are not exhaustive, as we assume that the published correlation code is stable\n\nclass CorrelationTest(tf.test.TestCase):\n    def _test_correlation(self, in0, in1, out=None, **kwargs):\n        with self.test_session(use_gpu=True) as sess:\n            in0_op = tf.constant(in0, tf.float32)\n            in1_op = tf.constant(in1, tf.float32)\n            result_op = ops.correlation(in0_op, in1_op, **kwargs)\n            result = sess.run(result_op)\n\n            if out is not None:\n                self.assertAllClose(out, result)\n\n            jacob_t, jacob_n = gradient_checker.compute_gradient([in0_op, in1_op],\n                                                                 [in0.shape, in1.shape],\n                                                                 result_op, result.shape)\n            #print(""--------------- n"")\n            #print(jacob_n)\n            #print(""--------------- t"")\n            #print(jacob_t)\n            self.assertAllClose(jacob_t, jacob_n, 1e-3, 1e-3)\n\n    def test_correlation_trivial(self):\n        first = [\n            [1, 1, 2, 2],\n            [0, 0, 2, 2],\n            [3, 3, 4, 4],\n            [3, 3, 2, 2]]\n        second = [\n            [1, 1, 2, 2],\n            [0, 0, 2, 2],\n            [3, 3, 4, 4],\n            [3, 3, 2, 2]]\n\n        first = np.reshape(first, [1, 1, 4, 4])\n        second = np.reshape(second, [1, 1, 4, 4])\n        expected = np.square(first)\n        self._test_correlation(first, second, expected,\n                               kernel_size=1, stride_2=1, max_displacement=0,\n                               pad=0)\n\n    def test_correlation_batch(self):\n        first = [\n           [1, 1, 2, 2],\n           [0, 0, 2, 2],\n           [3, 3, 4, 4],\n           [3, 3, 2, 2]]\n        second = [\n           [1, 1, 2, 2],\n           [0, 0, 2, 2],\n           [3, 3, 4, 4],\n           [3, 3, 2, 2]]\n\n        first = np.reshape(first, [1, 1, 4, 4])\n        second = np.reshape(second, [1, 1, 4, 4])\n        expected = np.square(first)\n\n        self._test_correlation(np.concatenate([first, first], 0),\n                              np.concatenate([second, second], 0),\n                              np.concatenate([expected, expected], 0),\n                              kernel_size=1, stride_2=1, max_displacement=0,\n                              pad=0)\n\n    def test_correlation_channels(self):\n        pass\n\n    def test_correlation_3x3(self):\n        return\n        first = [\n          [1, 1, 3],\n          [0, 0, 1],\n          [2, 2, 0.2]]\n        second = [\n          [1, 2, 0.1],\n          [3, 4, 2.2],\n          [4, 5, 1.6]]\n\n        first = np.reshape(first, [1, 1, 3, 3])\n        second = np.reshape(second, [1, 1, 3, 3])\n        self._test_correlation(first, second, None,\n                             kernel_size=3, stride_2=1, max_displacement=1,\n                             pad=2)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
src/e2eflow/test/ops/downsample.py,3,"b'import tensorflow as tf\nimport numpy as np\n\nfrom ... import ops\n\n\nclass DownsampleTest(tf.test.TestCase):\n    def test_downsample(self):\n        first = [\n            [1, 1, 2, 2],\n            [0, 0, 2, 2],\n            [3, 3, 4, 4],\n            [3, 3, 2, 2]]\n        second = [[0.5, 2], [3, 3]]\n\n        first = np.reshape(first, [1, 4, 4, 1])\n        second = np.reshape(second, [1, 2, 2, 1])\n\n        sess = tf.Session()\n        result = ops.downsample(first, 2)\n        result = sess.run(result)\n        self.assertAllClose(second, result)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
src/e2eflow/test/ops/forward_warp.py,3,"b'import tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.ops import gradient_checker\n\nfrom ... import ops\n\n\nclass ForwardWarpTest(tf.test.TestCase):\n    def test_grad(self):\n        with self.test_session(use_gpu=True) as sess:\n            flow_shape = [1, 10, 10, 2]\n            warped_shape = [1, 10, 10, 1]\n\n            flow_ = tf.placeholder(tf.float32, shape=flow_shape, name=\'flow\')\n            warped_ = ops.forward_warp(flow_)\n\n            jacob_t, jacob_n = gradient_checker.compute_gradient(flow_, flow_shape,\n                                                                 warped_, warped_shape)\n            self.assertAllClose(jacob_t, jacob_n, 1e-3, 1e-3)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
