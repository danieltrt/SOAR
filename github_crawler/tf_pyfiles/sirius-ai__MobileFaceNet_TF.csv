file_path,api_count,code
test_nets.py,10,"b'# -*- coding: utf-8 -*-\n# /usr/bin/env/python3\n\n\'\'\'\ntest pretrained model.\nAuthor: aiboy.wei@outlook.com .\n\'\'\'\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom utils.data_process import load_data\nfrom verification import evaluate\nfrom scipy.optimize import brentq\nfrom scipy import interpolate\nfrom sklearn import metrics\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport time\nimport sys\nimport re\nimport os\n\n\ndef load_model(model):\n    # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n    #  or if it is a protobuf file with a frozen graph\n    model_exp = os.path.expanduser(model)\n    if (os.path.isfile(model_exp)):\n        print(\'Model filename: %s\' % model_exp)\n        with tf.gfile.FastGFile(model_exp, \'rb\') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n            tf.import_graph_def(graph_def, name=\'\')\n    else:\n        print(\'Model directory: %s\' % model_exp)\n        meta_file, ckpt_file = get_model_filenames(model_exp)\n\n        print(\'Metagraph file: %s\' % meta_file)\n        print(\'Checkpoint file: %s\' % ckpt_file)\n\n        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file))\n        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n\n\ndef get_model_filenames(model_dir):\n    files = os.listdir(model_dir)\n    meta_files = [s for s in files if s.endswith(\'.meta\')]\n    if len(meta_files) == 0:\n        raise ValueError(\'No meta file found in the model directory (%s)\' % model_dir)\n    elif len(meta_files) > 1:\n        raise ValueError(\'There should not be more than one meta file in the model directory (%s)\' % model_dir)\n    meta_file = meta_files[0]\n    ckpt = tf.train.get_checkpoint_state(model_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n        ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n        return meta_file, ckpt_file\n\n    meta_files = [s for s in files if \'.ckpt\' in s]\n    max_step = -1\n    for f in files:\n        step_str = re.match(r\'(^model-[\\w\\- ]+.ckpt-(\\d+))\', f)\n        if step_str is not None and len(step_str.groups()) >= 2:\n            step = int(step_str.groups()[1])\n            if step > max_step:\n                max_step = step\n                ckpt_file = step_str.groups()[0]\n    return meta_file, ckpt_file\n\ndef main(args):\n    with tf.Graph().as_default():\n        with tf.Session() as sess:\n            # prepare validate datasets\n            ver_list = []\n            ver_name_list = []\n            for db in args.eval_datasets:\n                print(\'begin db %s convert.\' % db)\n                data_set = load_data(db, args.image_size, args)\n                ver_list.append(data_set)\n                ver_name_list.append(db)\n\n            # Load the model\n            load_model(args.model)\n\n            # Get input and output tensors, ignore phase_train_placeholder for it have default value.\n            inputs_placeholder = tf.get_default_graph().get_tensor_by_name(""input:0"")\n            embeddings = tf.get_default_graph().get_tensor_by_name(""embeddings:0"")\n\n            # image_size = images_placeholder.get_shape()[1]  # For some reason this doesn\'t work for frozen graphs\n            embedding_size = embeddings.get_shape()[1]\n\n            for db_index in range(len(ver_list)):\n                # Run forward pass to calculate embeddings\n                print(\'\\nRunnning forward pass on {} images\'.format(ver_name_list[db_index]))\n                start_time = time.time()\n                data_sets, issame_list = ver_list[db_index]\n                nrof_batches = data_sets.shape[0] // args.test_batch_size\n                emb_array = np.zeros((data_sets.shape[0], embedding_size))\n\n                for index in range(nrof_batches):\n                    start_index = index * args.test_batch_size\n                    end_index = min((index + 1) * args.test_batch_size, data_sets.shape[0])\n\n                    feed_dict = {inputs_placeholder: data_sets[start_index:end_index, ...]}\n                    emb_array[start_index:end_index, :] = sess.run(embeddings, feed_dict=feed_dict)\n\n                tpr, fpr, accuracy, val, val_std, far = evaluate(emb_array, issame_list, nrof_folds=args.eval_nrof_folds)\n                duration = time.time() - start_time\n                print(""total time %.3fs to evaluate %d images of %s"" % (duration, data_sets.shape[0], ver_name_list[db_index]))\n                print(\'Accuracy: %1.3f+-%1.3f\' % (np.mean(accuracy), np.std(accuracy)))\n                print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n                print(\'fpr and tpr: %1.3f %1.3f\' % (np.mean(fpr, 0), np.mean(tpr, 0)))\n\n                auc = metrics.auc(fpr, tpr)\n                print(\'Area Under Curve (AUC): %1.3f\' % auc)\n                eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n                print(\'Equal Error Rate (EER): %1.3f\' % eer)\n\ndef parse_arguments(argv):\n    \'\'\'test parameters\'\'\'\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--model\', type=str,\n                        help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\',\n                        default=\'./output/ckpt\')\n    parser.add_argument(\'--image_size\', default=[112, 112], help=\'the image size\')\n    parser.add_argument(\'--test_batch_size\', type=int,\n                        help=\'Number of images to process in a batch in the test set.\', default=100)\n    # parser.add_argument(\'--eval_datasets\', default=[\'lfw\', \'cfp_ff\', \'cfp_fp\', \'agedb_30\'], help=\'evluation datasets\')\n    parser.add_argument(\'--eval_datasets\', default=[\'lfw\'], help=\'evluation datasets\')\n    parser.add_argument(\'--eval_db_path\', default=\'./datasets/faces_ms1m_112x112\', help=\'evluate datasets base path\')\n    parser.add_argument(\'--eval_nrof_folds\', type=int,\n                        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))'"
train_nets.py,38,"b'# -*- coding: utf-8 -*-\n# /usr/bin/env/python3\n\n\'\'\'\nTensorflow implementation for MobileFaceNet.\nAuthor: aiboy.wei@outlook.com .\n\'\'\'\n\nfrom losses.face_losses import insightface_loss, cosineface_loss, combine_loss\nfrom utils.data_process import parse_function, load_data\nfrom nets.MobileFaceNet import inference\n# from losses.face_losses import cos_loss\nfrom verification import evaluate\nfrom scipy.optimize import brentq\nfrom utils.common import train\nfrom scipy import interpolate\nfrom datetime import datetime\nfrom sklearn import metrics\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport time\nimport os\n\nslim = tf.contrib.slim\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description=\'parameters to train net\')\n    parser.add_argument(\'--max_epoch\', default=12, help=\'epoch to train the network\')\n    parser.add_argument(\'--image_size\', default=[112, 112], help=\'the image size\')\n    parser.add_argument(\'--class_number\', type=int, required=True,\n                        help=\'class number depend on your training datasets, MS1M-V1: 85164, MS1M-V2: 85742\')\n    parser.add_argument(\'--embedding_size\', type=int,\n                        help=\'Dimensionality of the embedding.\', default=128)\n    parser.add_argument(\'--weight_decay\', default=5e-5, help=\'L2 weight regularization.\')\n    parser.add_argument(\'--lr_schedule\', help=\'Number of epochs for learning rate piecewise.\', default=[4, 7, 9, 11])\n    parser.add_argument(\'--train_batch_size\', default=90, help=\'batch size to train network\')\n    parser.add_argument(\'--test_batch_size\', type=int,\n                        help=\'Number of images to process in a batch in the test set.\', default=100)\n    parser.add_argument(\'--eval_datasets\', default=[\'lfw\', \'cfp_ff\', \'cfp_fp\', \'agedb_30\'], help=\'evluation datasets\')\n    # parser.add_argument(\'--eval_datasets\', default=[\'lfw\'], help=\'evluation datasets\')\n    parser.add_argument(\'--eval_db_path\', default=\'./datasets/faces_ms1m_112x112\', help=\'evluate datasets base path\')\n    parser.add_argument(\'--eval_nrof_folds\', type=int,\n                        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n    parser.add_argument(\'--tfrecords_file_path\', default=\'./datasets/faces_ms1m_112x112/tfrecords\', type=str,\n                        help=\'path to the output of tfrecords file path\')\n    parser.add_argument(\'--summary_path\', default=\'./output/summary\', help=\'the summary file save path\')\n    parser.add_argument(\'--ckpt_path\', default=\'./output/ckpt\', help=\'the ckpt file save path\')\n    parser.add_argument(\'--ckpt_best_path\', default=\'./output/ckpt_best\', help=\'the best ckpt file save path\')\n    parser.add_argument(\'--log_file_path\', default=\'./output/logs\', help=\'the ckpt file save path\')\n    parser.add_argument(\'--saver_maxkeep\', default=50, help=\'tf.train.Saver max keep ckpt files\')\n    #parser.add_argument(\'--buffer_size\', default=10000, help=\'tf dataset api buffer size\')\n    parser.add_argument(\'--summary_interval\', default=400, help=\'interval to save summary\')\n    parser.add_argument(\'--ckpt_interval\', default=2000, help=\'intervals to save ckpt file\')\n    parser.add_argument(\'--validate_interval\', default=2000, help=\'intervals to save ckpt file\')\n    parser.add_argument(\'--show_info_interval\', default=50, help=\'intervals to save ckpt file\')\n    parser.add_argument(\'--pretrained_model\', type=str, default=\'\', help=\'Load a pretrained model before training starts.\')\n    parser.add_argument(\'--optimizer\', type=str, choices=[\'ADAGRAD\', \'ADADELTA\', \'ADAM\', \'RMSPROP\', \'MOM\'],\n                        help=\'The optimization algorithm to use\', default=\'ADAM\')\n    parser.add_argument(\'--log_device_mapping\', default=False, help=\'show device placement log\')\n    parser.add_argument(\'--moving_average_decay\', type=float,\n                        help=\'Exponential decay for tracking of training parameters.\', default=0.999)\n    parser.add_argument(\'--log_histograms\',\n                        help=\'Enables logging of weight/bias histograms in tensorboard.\', action=\'store_true\')\n    parser.add_argument(\'--prelogits_norm_loss_factor\', type=float,\n                        help=\'Loss based on the norm of the activations in the prelogits layer.\', default=2e-5)\n    parser.add_argument(\'--prelogits_norm_p\', type=float,\n                        help=\'Norm to use for prelogits norm loss.\', default=1.0)\n    parser.add_argument(\'--loss_type\', default=\'insightface\', help=\'loss type, choice type are insightface/cosine/combine\')\n    parser.add_argument(\'--margin_s\', type=float,\n                        help=\'insightface_loss/cosineface_losses/combine_loss loss scale.\', default=64.)\n    parser.add_argument(\'--margin_m\', type=float,\n                        help=\'insightface_loss/cosineface_losses/combine_loss loss margin.\', default=0.5)\n    parser.add_argument(\'--margin_a\', type=float,\n                        help=\'combine_loss loss margin a.\', default=1.0)\n    parser.add_argument(\'--margin_b\', type=float,\n                        help=\'combine_loss loss margin b.\', default=0.2)\n\n    args = parser.parse_args()\n    return args\n\nif __name__ == \'__main__\':\n    with tf.Graph().as_default():\n        os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n        args = get_parser()\n\n        # create log dir\n        subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n        log_dir = os.path.join(os.path.expanduser(args.log_file_path), subdir)\n        if not os.path.isdir(log_dir):  # Create the log directory if it doesn\'t exist\n            os.makedirs(log_dir)\n\n        # define global parameters\n        global_step = tf.Variable(name=\'global_step\', initial_value=0, trainable=False)\n        epoch = tf.Variable(name=\'epoch\', initial_value=-1, trainable=False)\n        # define placeholder\n        inputs = tf.placeholder(name=\'img_inputs\', shape=[None, *args.image_size, 3], dtype=tf.float32)\n        labels = tf.placeholder(name=\'img_labels\', shape=[None, ], dtype=tf.int64)\n        phase_train_placeholder = tf.placeholder_with_default(tf.constant(False, dtype=tf.bool), shape=None, name=\'phase_train\')\n\n        # prepare train dataset\n        # the image is substracted 127.5 and multiplied 1/128.\n        # random flip left right\n        tfrecords_f = os.path.join(args.tfrecords_file_path, \'tran.tfrecords\')\n        dataset = tf.data.TFRecordDataset(tfrecords_f)\n        dataset = dataset.map(parse_function)\n        #dataset = dataset.shuffle(buffer_size=args.buffer_size)\n        dataset = dataset.batch(args.train_batch_size)\n        iterator = dataset.make_initializable_iterator()\n        next_element = iterator.get_next()\n\n        # prepare validate datasets\n        ver_list = []\n        ver_name_list = []\n        for db in args.eval_datasets:\n            print(\'begin db %s convert.\' % db)\n            data_set = load_data(db, args.image_size, args)\n            ver_list.append(data_set)\n            ver_name_list.append(db)\n\n        # pretrained model path\n        pretrained_model = None\n        if args.pretrained_model:\n            pretrained_model = os.path.expanduser(args.pretrained_model)\n            print(\'Pre-trained model: %s\' % pretrained_model)\n\n        # identity the input, for inference\n        inputs = tf.identity(inputs, \'input\')\n\n        prelogits, net_points = inference(inputs, bottleneck_layer_size=args.embedding_size, phase_train=phase_train_placeholder, weight_decay=args.weight_decay)\n\n        # record the network architecture\n        hd = open(""./arch/txt/MobileFaceNet_Arch.txt"", \'w\')\n        for key in net_points.keys():\n            info = \'{}:{}\\n\'.format(key, net_points[key].get_shape().as_list())\n            hd.write(info)\n        hd.close()\n\n        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\'embeddings\')\n\n        # Norm for the prelogits\n        eps = 1e-5\n        prelogits_norm = tf.reduce_mean(tf.norm(tf.abs(prelogits) + eps, ord=args.prelogits_norm_p, axis=1))\n        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_norm * args.prelogits_norm_loss_factor)\n\n        # inference_loss, logit = cos_loss(prelogits, labels, args.class_number)\n        w_init_method = slim.initializers.xavier_initializer()\n        if args.loss_type == \'insightface\':\n            inference_loss, logit = insightface_loss(embeddings, labels, args.class_number, w_init_method)\n        elif args.loss_type == \'cosine\':\n            inference_loss, logit = cosineface_loss(embeddings, labels, args.class_number, w_init_method)\n        elif args.loss_type == \'combine\':\n            inference_loss, logit = combine_loss(embeddings, labels, args.train_batch_size, args.class_number, w_init_method)\n        else:\n            assert 0, \'loss type error, choice item just one of [insightface, cosine, combine], please check!\'\n        tf.add_to_collection(\'losses\', inference_loss)\n\n        # total losses\n        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n        total_loss = tf.add_n([inference_loss] + regularization_losses, name=\'total_loss\')\n\n        # define the learning rate schedule\n        learning_rate = tf.train.piecewise_constant(epoch, boundaries=args.lr_schedule, values=[0.1, 0.01, 0.001, 0.0001, 0.00001],\n                                         name=\'lr_schedule\')\n        \n        # define sess\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n        config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=args.log_device_mapping, gpu_options=gpu_options)\n        config.gpu_options.allow_growth = True\n        sess = tf.Session(config=config)\n\n        # calculate accuracy\n        pred = tf.nn.softmax(logit)\n        correct_prediction = tf.cast(tf.equal(tf.argmax(pred, 1), tf.cast(labels, tf.int64)), tf.float32)\n        Accuracy_Op = tf.reduce_mean(correct_prediction)\n\n        # summary writer\n        summary = tf.summary.FileWriter(args.summary_path, sess.graph)\n        summaries = []\n        # add train info to tensorboard summary\n        summaries.append(tf.summary.scalar(\'inference_loss\', inference_loss))\n        summaries.append(tf.summary.scalar(\'total_loss\', total_loss))\n        summaries.append(tf.summary.scalar(\'leraning_rate\', learning_rate))\n        summary_op = tf.summary.merge(summaries)\n\n        # train op\n        train_op = train(total_loss, global_step, args.optimizer, learning_rate, args.moving_average_decay,\n                         tf.global_variables(), summaries, args.log_histograms)\n        inc_global_step_op = tf.assign_add(global_step, 1, name=\'increment_global_step\')\n        inc_epoch_op = tf.assign_add(epoch, 1, name=\'increment_epoch\')\n\n        # record trainable variable\n        hd = open(""./arch/txt/trainable_var.txt"", ""w"")\n        for var in tf.trainable_variables():\n            hd.write(str(var))\n            hd.write(\'\\n\')\n        hd.close()\n\n        # saver to load pretrained model or save model\n        # MobileFaceNet_vars = [v for v in tf.trainable_variables() if v.name.startswith(\'MobileFaceNet\')]\n        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=args.saver_maxkeep)\n\n        # init all variables\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n\n        # load pretrained model\n        if pretrained_model:\n            print(\'Restoring pretrained model: %s\' % pretrained_model)\n            ckpt = tf.train.get_checkpoint_state(pretrained_model)\n            print(ckpt)\n            saver.restore(sess, ckpt.model_checkpoint_path)\n\n        # output file path\n        if not os.path.exists(args.log_file_path):\n            os.makedirs(args.log_file_path)\n        if not os.path.exists(args.ckpt_best_path):\n            os.makedirs(args.ckpt_best_path)\n\n        count = 0\n        total_accuracy = {}\n        for i in range(args.max_epoch):\n            sess.run(iterator.initializer)\n            _ = sess.run(inc_epoch_op)\n            while True:\n                try:\n                    images_train, labels_train = sess.run(next_element)\n\n                    feed_dict = {inputs: images_train, labels: labels_train, phase_train_placeholder: True}\n                    start = time.time()\n                    _, total_loss_val, inference_loss_val, reg_loss_val, _, acc_val = \\\n                    sess.run([train_op, total_loss, inference_loss, regularization_losses, inc_global_step_op, Accuracy_Op],\n                             feed_dict=feed_dict)\n                    end = time.time()\n                    pre_sec = args.train_batch_size/(end - start)\n\n                    count += 1\n                    # print training information\n                    if count > 0 and count % args.show_info_interval == 0:\n                        print(\'epoch %d, total_step %d, total loss is %.2f , inference loss is %.2f, reg_loss is %.2f, training accuracy is %.6f, time %.3f samples/sec\' %\n                              (i, count, total_loss_val, inference_loss_val, np.sum(reg_loss_val), acc_val, pre_sec))\n\n                    # save summary\n                    if count > 0 and count % args.summary_interval == 0:\n                        feed_dict = {inputs: images_train, labels: labels_train, phase_train_placeholder: True}\n                        summary_op_val = sess.run(summary_op, feed_dict=feed_dict)\n                        summary.add_summary(summary_op_val, count)\n\n                    # save ckpt files\n                    if count > 0 and count % args.ckpt_interval == 0:\n                        filename = \'MobileFaceNet_iter_{:d}\'.format(count) + \'.ckpt\'\n                        filename = os.path.join(args.ckpt_path, filename)\n                        saver.save(sess, filename)\n\n                    # validate\n                    if count > 0 and count % args.validate_interval == 0:\n                        print(\'\\nIteration\', count, \'testing...\')\n                        for db_index in range(len(ver_list)):\n                            start_time = time.time()\n                            data_sets, issame_list = ver_list[db_index]\n                            emb_array = np.zeros((data_sets.shape[0], args.embedding_size))\n                            nrof_batches = data_sets.shape[0] // args.test_batch_size\n                            for index in range(nrof_batches): # actual is same multiply 2, test data total\n                                start_index = index * args.test_batch_size\n                                end_index = min((index + 1) * args.test_batch_size, data_sets.shape[0])\n\n                                feed_dict = {inputs: data_sets[start_index:end_index, ...], phase_train_placeholder: False}\n                                emb_array[start_index:end_index, :] = sess.run(embeddings, feed_dict=feed_dict)\n\n                            tpr, fpr, accuracy, val, val_std, far = evaluate(emb_array, issame_list, nrof_folds=args.eval_nrof_folds)\n                            duration = time.time() - start_time\n\n                            print(""total time %.3fs to evaluate %d images of %s"" % (duration, data_sets.shape[0], ver_name_list[db_index]))\n                            print(\'Accuracy: %1.3f+-%1.3f\' % (np.mean(accuracy), np.std(accuracy)))\n                            print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n                            print(\'fpr and tpr: %1.3f %1.3f\' % (np.mean(fpr, 0), np.mean(tpr, 0)))\n\n                            auc = metrics.auc(fpr, tpr)\n                            print(\'Area Under Curve (AUC): %1.3f\' % auc)\n                            eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n                            print(\'Equal Error Rate (EER): %1.3f\\n\' % eer)\n\n                            with open(os.path.join(log_dir, \'{}_result.txt\'.format(ver_name_list[db_index])), \'at\') as f:\n                                f.write(\'%d\\t%.5f\\t%.5f\\n\' % (count, np.mean(accuracy), val))\n\n                            if ver_name_list == \'lfw\' and np.mean(accuracy) > 0.992:\n                                print(\'best accuracy is %.5f\' % np.mean(accuracy))\n                                filename = \'MobileFaceNet_iter_best_{:d}\'.format(count) + \'.ckpt\'\n                                filename = os.path.join(args.ckpt_best_path, filename)\n                                saver.save(sess, filename)\n\n                except tf.errors.OutOfRangeError:\n                    print(""End of epoch %d"" % i)\n                    break\n'"
verification.py,0,"b'""""""Helper for evaluation on the Labeled Faces in the Wild dataset\n""""""\n\n# MIT License\n#\n# Copyright (c) 2016 David Sandberg\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\nimport sklearn\nfrom scipy import interpolate\nimport datetime\n\nmax_threshold = 0\nmin_threshold = 4\n\ndef calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, pca=0):\n    assert (embeddings1.shape[0] == embeddings2.shape[0])\n    assert (embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)\n    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n\n    tprs = np.zeros((nrof_folds, nrof_thresholds))\n    fprs = np.zeros((nrof_folds, nrof_thresholds))\n    accuracy = np.zeros((nrof_folds))\n    indices = np.arange(nrof_pairs)\n    # print(\'pca\', pca)\n\n    if pca == 0:\n        diff = np.subtract(embeddings1, embeddings2)\n        dist = np.sum(np.square(diff), 1)\n\n    global max_threshold\n    global min_threshold\n\n    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n        # print(\'train_set\', train_set)\n        # print(\'test_set\', test_set)\n        if pca > 0:\n            print(\'doing pca on\', fold_idx)\n            embed1_train = embeddings1[train_set]\n            embed2_train = embeddings2[train_set]\n            _embed_train = np.concatenate((embed1_train, embed2_train), axis=0)\n            # print(_embed_train.shape)\n            pca_model = PCA(n_components=pca)\n            pca_model.fit(_embed_train)\n            embed1 = pca_model.transform(embeddings1)\n            embed2 = pca_model.transform(embeddings2)\n            embed1 = sklearn.preprocessing.normalize(embed1)\n            embed2 = sklearn.preprocessing.normalize(embed2)\n            # print(embed1.shape, embed2.shape)\n            diff = np.subtract(embed1, embed2)\n            dist = np.sum(np.square(diff), 1)\n\n        # Find the best threshold for the fold\n        acc_train = np.zeros((nrof_thresholds))\n        for threshold_idx, threshold in enumerate(thresholds):\n            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n        best_threshold_index = np.argmax(acc_train)\n        # print(\'best_threshold_index\', best_threshold_index, acc_train[best_threshold_index])\n        for threshold_idx, threshold in enumerate(thresholds):\n            tprs[fold_idx, threshold_idx], fprs[fold_idx, threshold_idx], _ = calculate_accuracy(threshold,\n                                                                                                 dist[test_set],\n                                                                                                 actual_issame[\n                                                                                                     test_set])\n        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set],\n                                                      actual_issame[test_set])\n\n        if max_threshold < thresholds[best_threshold_index]:\n            max_threshold = thresholds[best_threshold_index]\n        if min_threshold > thresholds[best_threshold_index]:\n            min_threshold = thresholds[best_threshold_index]\n    print(\'thresholds max: {} <=> min: {}\'.format(max_threshold, min_threshold))\n\n    tpr = np.mean(tprs, 0)\n    fpr = np.mean(fprs, 0)\n    return tpr, fpr, accuracy\n\n\ndef calculate_accuracy(threshold, dist, actual_issame):\n    predict_issame = np.less(dist, threshold)\n    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n\n    tpr = 0 if (tp + fn == 0) else float(tp) / float(tp + fn)\n    fpr = 0 if (fp + tn == 0) else float(fp) / float(fp + tn)\n    acc = float(tp + tn) / dist.size\n    return tpr, fpr, acc\n\n\ndef calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10):\n    \'\'\'\n    Copy from [insightface](https://github.com/deepinsight/insightface)\n    :param thresholds:\n    :param embeddings1:\n    :param embeddings2:\n    :param actual_issame:\n    :param far_target:\n    :param nrof_folds:\n    :return:\n    \'\'\'\n    assert (embeddings1.shape[0] == embeddings2.shape[0])\n    assert (embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)\n    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n\n    val = np.zeros(nrof_folds)\n    far = np.zeros(nrof_folds)\n\n    diff = np.subtract(embeddings1, embeddings2)\n    dist = np.sum(np.square(diff), 1)\n    indices = np.arange(nrof_pairs)\n\n    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n\n        # Find the threshold that gives FAR = far_target\n        far_train = np.zeros(nrof_thresholds)\n        for threshold_idx, threshold in enumerate(thresholds):\n            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n        if np.max(far_train) >= far_target:\n            f = interpolate.interp1d(far_train, thresholds, kind=\'slinear\')\n            threshold = f(far_target)\n        else:\n            threshold = 0.0\n\n        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n\n    val_mean = np.mean(val)\n    far_mean = np.mean(far)\n    val_std = np.std(val)\n\n    return val_mean, val_std, far_mean\n\n\ndef calculate_val_far(threshold, dist, actual_issame):\n    predict_issame = np.less(dist, threshold)\n    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    n_same = np.sum(actual_issame)\n    n_diff = np.sum(np.logical_not(actual_issame))\n    val = float(true_accept) / float(n_same)\n    far = float(false_accept) / float(n_diff)\n    return val, far\n\n\ndef evaluate(embeddings, actual_issame, nrof_folds=10, pca=0):\n    # Calculate evaluation metrics\n    thresholds = np.arange(0, 4, 0.01)\n    embeddings1 = embeddings[0::2]\n    embeddings2 = embeddings[1::2]\n    tpr, fpr, accuracy = calculate_roc(thresholds, embeddings1, embeddings2,\n                                       np.asarray(actual_issame), nrof_folds=nrof_folds, pca=pca)\n    thresholds = np.arange(0, 4, 0.001)\n    val, val_std, far = calculate_val(thresholds, embeddings1, embeddings2,\n                                      np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds)\n    return tpr, fpr, accuracy, val, val_std, far\n\n\ndef data_iter(datasets, batch_size):\n    data_num = datasets.shape[0]\n    for i in range(0, data_num, batch_size):\n        yield datasets[i:min(i+batch_size, data_num), ...]\n\n\ndef test(data_set, sess, embedding_tensor, batch_size, label_shape=None, feed_dict=None, input_placeholder=None):\n    \'\'\'\n    referenc official implementation [insightface](https://github.com/deepinsight/insightface)\n    :param data_set:\n    :param sess:\n    :param embedding_tensor:\n    :param batch_size:\n    :param label_shape:\n    :param feed_dict:\n    :param input_placeholder:\n    :return:\n    \'\'\'\n    print(\'testing verification..\')\n    data_list = data_set[0]\n    issame_list = data_set[1]\n    embeddings_list = []\n    time_consumed = 0.0\n    for i in range(len(data_list)):\n        datas = data_list[i]\n        embeddings = None\n        feed_dict.setdefault(input_placeholder, None)\n        for idx, data in enumerate(data_iter(datas, batch_size)):\n            data_tmp = data.copy()    # fix issues #4\n            data_tmp -= 127.5\n            data_tmp *= 0.0078125\n            feed_dict[input_placeholder] = data_tmp\n            time0 = datetime.datetime.now()\n            _embeddings = sess.run(embedding_tensor, feed_dict)\n            time_now = datetime.datetime.now()\n            diff = time_now - time0\n            time_consumed += diff.total_seconds()\n            if embeddings is None:\n                embeddings = np.zeros((datas.shape[0], _embeddings.shape[1]))\n            try:\n                embeddings[idx*batch_size:min((idx+1)*batch_size, datas.shape[0]), ...] = _embeddings\n            except ValueError:\n                print(\'idx*batch_size value is %d min((idx+1)*batch_size, datas.shape[0]) %d, batch_size %d, data.shape[0] %d\' %\n                      (idx*batch_size, min((idx+1)*batch_size, datas.shape[0]), batch_size, datas.shape[0]))\n                print(\'embedding shape is \', _embeddings.shape)\n        embeddings_list.append(embeddings)\n\n    _xnorm = 0.0\n    _xnorm_cnt = 0\n    for embed in embeddings_list:\n        for i in range(embed.shape[0]):\n            _em = embed[i]\n            _norm = np.linalg.norm(_em)\n            # print(_em.shape, _norm)\n            _xnorm += _norm\n            _xnorm_cnt += 1\n    _xnorm /= _xnorm_cnt\n\n    acc1 = 0.0\n    std1 = 0.0\n    embeddings = embeddings_list[0] + embeddings_list[1]\n    embeddings = sklearn.preprocessing.normalize(embeddings)\n    print(embeddings.shape)\n    print(\'infer time\', time_consumed)\n    _, _, accuracy, val, val_std, far = evaluate(embeddings, issame_list, nrof_folds=10)\n    acc2, std2 = np.mean(accuracy), np.std(accuracy)\n    return acc1, std1, acc2, std2, _xnorm, embeddings_list\n\n\ndef ver_test(ver_list, ver_name_list, nbatch, sess, embedding_tensor, batch_size, feed_dict, input_placeholder):\n    results = []\n    for i in range(len(ver_list)):\n        acc1, std1, acc2, std2, xnorm, embeddings_list = test(data_set=ver_list[i], sess=sess, embedding_tensor=embedding_tensor,\n                                                              batch_size=batch_size, feed_dict=feed_dict,\n                                                              input_placeholder=input_placeholder)\n        print(\'[%s][%d]XNorm: %f\' % (ver_name_list[i], nbatch, xnorm))\n        print(\'[%s][%d]Accuracy-Flip: %1.5f+-%1.5f\' % (ver_name_list[i], nbatch, acc2, std2))\n        results.append(acc2)\n    return results\n'"
losses/__init__.py,0,b''
losses/face_losses.py,70,"b'import tensorflow as tf\nimport math\n\n\ndef insightface_loss(embedding, labels, out_num, w_init=None, s=64., m=0.5):\n    \'\'\'\n    :param embedding: the input embedding vectors\n    :param labels:  the input labels, the shape should be eg: (batch_size, 1)\n    :param s: scalar value default is 64\n    :param out_num: output class num\n    :param m: the margin value, default is 0.5\n    :return: the final cacualted output, this output is send into the tf.nn.softmax directly\n    \'\'\'\n    cos_m = math.cos(m)\n    sin_m = math.sin(m)\n    mm = sin_m * m  # issue 1\n    threshold = math.cos(math.pi - m)\n    with tf.variable_scope(\'insightface_loss\'):\n        # inputs and weights norm\n        embedding_norm = tf.norm(embedding, axis=1, keepdims=True)\n        embedding = tf.div(embedding, embedding_norm, name=\'norm_embedding\')\n        weights = tf.get_variable(name=\'embedding_weights\', shape=(embedding.get_shape().as_list()[-1], out_num),\n                                  initializer=w_init, dtype=tf.float32)\n        weights_norm = tf.norm(weights, axis=0, keepdims=True)\n        weights = tf.div(weights, weights_norm, name=\'norm_weights\')\n        # cos(theta+m)\n        cos_t = tf.matmul(embedding, weights, name=\'cos_t\')\n        cos_t2 = tf.square(cos_t, name=\'cos_2\')\n        sin_t2 = tf.subtract(1., cos_t2, name=\'sin_2\')\n        sin_t = tf.sqrt(sin_t2, name=\'sin_t\')\n        cos_mt = s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name=\'cos_mt\')\n\n        # this condition controls the theta+m should in range [0, pi]\n        #      0<=theta+m<=pi\n        #     -m<=theta<=pi-m\n        cond_v = cos_t - threshold\n        cond = tf.cast(tf.nn.relu(cond_v, name=\'if_else\'), dtype=tf.bool)\n\n        keep_val = s*(cos_t - mm)\n        cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n\n        mask = tf.one_hot(labels, depth=out_num, name=\'one_hot_mask\')\n        # mask = tf.squeeze(mask, 1)\n        inv_mask = tf.subtract(1., mask, name=\'inverse_mask\')\n\n        s_cos_t = tf.multiply(s, cos_t, name=\'scalar_cos_t\')\n\n        logit = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name=\'arcface_loss_output\')\n        inference_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels))\n\n    return inference_loss, logit\n\n\ndef cosineface_loss(embedding, labels, out_num, w_init=None, s=30., m=0.4):\n    \'\'\'\n    :param embedding: the input embedding vectors\n    :param labels:  the input labels, the shape should be eg: (batch_size, 1)\n    :param s: scalar value, default is 30\n    :param out_num: output class num\n    :param m: the margin value, default is 0.4\n    :return: the final cacualted output, this output is send into the tf.nn.softmax directly\n    \'\'\'\n    with tf.variable_scope(\'cosineface_loss\'):\n        # inputs and weights norm\n        embedding_norm = tf.norm(embedding, axis=1, keep_dims=True)\n        embedding = tf.div(embedding, embedding_norm, name=\'norm_embedding\')\n        weights = tf.get_variable(name=\'embedding_weights\', shape=(embedding.get_shape().as_list()[-1], out_num),\n                                  initializer=w_init, dtype=tf.float32)\n        weights_norm = tf.norm(weights, axis=0, keep_dims=True)\n        weights = tf.div(weights, weights_norm, name=\'norm_weights\')\n        # cos_theta - m\n        cos_t = tf.matmul(embedding, weights, name=\'cos_t\')\n        cos_t_m = tf.subtract(cos_t, m, name=\'cos_t_m\')\n\n        mask = tf.one_hot(labels, depth=out_num, name=\'one_hot_mask\')\n        inv_mask = tf.subtract(1., mask, name=\'inverse_mask\')\n\n        logit = tf.add(s * tf.multiply(cos_t, inv_mask), s * tf.multiply(cos_t_m, mask), name=\'cosineface_loss_output\')\n        inference_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels))\n\n    return inference_loss, logit\n\n\ndef combine_loss(embedding, labels, batch_size, out_num, w_init, margin_a=1., margin_m=0.3, margin_b=0.2, s=64.):\n    \'\'\'\n    This code is contributed by RogerLo. Thanks for you contribution.\n\n    :param embedding: the input embedding vectors\n    :param labels:  the input labels, the shape should be eg: (batch_size, 1)\n    :param s: scalar value default is 64\n    :param batch_size: input batch size\n    :param out_num: output class num\n    :param m: the margin value, default is 0.5\n    :return: the final cacualted output, this output is send into the tf.nn.softmax directly\n    \'\'\'\n    with tf.variable_scope(\'combine_loss\'):\n        weights = tf.get_variable(name=\'embedding_weights\', shape=(embedding.get_shape().as_list()[-1], out_num),\n                                  initializer=w_init, dtype=tf.float32)\n        weights_unit = tf.nn.l2_normalize(weights, axis=0)\n        embedding_unit = tf.nn.l2_normalize(embedding, axis=1)\n        cos_t = tf.matmul(embedding_unit, weights_unit)\n        ordinal = tf.constant(list(range(0, batch_size)), tf.int64)\n        ordinal_y = tf.stack([ordinal, labels], axis=1)\n        zy = cos_t * s\n        sel_cos_t = tf.gather_nd(zy, ordinal_y)\n        if margin_a != 1.0 or margin_m != 0.0 or margin_b != 0.0:\n            if margin_a == 1.0 and margin_m == 0.0:\n                s_m = s * margin_b\n                new_zy = sel_cos_t - s_m\n            else:\n                cos_value = sel_cos_t / s\n                t = tf.acos(cos_value)\n                if margin_a != 1.0:\n                    t = t * margin_a\n                if margin_m > 0.0:\n                    t = t + margin_m\n                body = tf.cos(t)\n                if margin_b > 0.0:\n                    body = body - margin_b\n                new_zy = body * s\n        updated_logits = tf.add(zy, tf.scatter_nd(ordinal_y, tf.subtract(new_zy, sel_cos_t), (batch_size, out_num)))\n        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=updated_logits))\n        # predict_cls = tf.argmax(updated_logits, 1)\n        # accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(predict_cls, tf.int64), tf.cast(labels, tf.int64)), \'float\'))\n        # predict_cls_s = tf.argmax(zy, 1)\n        # accuracy_s = tf.reduce_mean(tf.cast(tf.equal(tf.cast(predict_cls_s, tf.int64), tf.cast(labels, tf.int64)), \'float\'))\n        # return zy, loss, accuracy, accuracy_s, predict_cls_s\n\n    return loss, updated_logits\n\ndef center_loss(features, label, alfa, nrof_classes):\n    """"""Center loss based on the paper ""A Discriminative Feature Learning Approach for Deep Face Recognition""\n       (http://ydwen.github.io/papers/WenECCV16.pdf)\n    """"""\n    nrof_features = features.get_shape()[1]\n    centers = tf.get_variable(\'centers\', [nrof_classes, nrof_features], dtype=tf.float32,\n        initializer=tf.constant_initializer(0), trainable=False)\n    label = tf.reshape(label, [-1])\n    centers_batch = tf.gather(centers, label)\n    diff = (1 - alfa) * (centers_batch - features)\n    centers = tf.scatter_sub(centers, label, diff)\n    with tf.control_dependencies([centers]):\n        loss = tf.reduce_mean(tf.square(features - centers_batch))\n    return loss, centers\n\n\ndef cos_loss(x, y, num_cls, reuse=False, alpha=0.35, scale=64, name=\'cos_loss\'):\n    \'\'\'\n    x: B x D - features\n    y: B x 1 - labels\n    num_cls: 1 - total class number\n    alpah: 1 - margin\n    scale: 1 - scaling paramter\n    \'\'\'\n    # define the classifier weights\n    xs = x.get_shape()\n    with tf.variable_scope(\'centers_var\', reuse=reuse) as center_scope:\n        w = tf.get_variable(""centers"", [xs[1], num_cls], dtype=tf.float32,\n                            initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n\n    # normalize the feature and weight\n    # (N,D)\n    x_feat_norm = tf.nn.l2_normalize(x, 1, 1e-10)\n    # (D,C)\n    w_feat_norm = tf.nn.l2_normalize(w, 0, 1e-10)\n\n    # get the scores after normalization\n    # (N,C)\n    xw_norm = tf.matmul(x_feat_norm, w_feat_norm)\n    # implemented by py_func\n    # value = tf.identity(xw)\n    # substract the marigin and scale it\n    # value = coco_func(xw_norm,y,alpha) * scale\n\n    # implemented by tf api\n    margin_xw_norm = xw_norm - alpha\n    label_onehot = tf.one_hot(y, num_cls)\n    value = scale * tf.where(tf.equal(label_onehot, 1), margin_xw_norm, xw_norm)\n\n    # compute the loss as softmax loss\n    cos_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=value))\n\n    return cos_loss, value\n'"
nets/L_Resnet_E_IR.py,32,"b'# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\'\'\'\nimplement ReNet50 for face recognition.\nOriginal author Andy.Wei\n\nImplemented the following paper:\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. ""Identity Mappings in Deep Residual Networks""\n\'\'\'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.contrib.model_pruning.python import pruning\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.training import moving_averages\nimport tensorflow as tf\nimport argparse\nimport sys\n\nvariable_scope = \'ResNet\'\n\ndef residual_unit_v3(data, out_filter, stride, dim_match, trainable, name):\n    """"""Return ResNet Unit symbol for building ResNet\n    Parameters\n    ----------\n    data : str\n        Input data\n    out_filter : int\n        Number of output channels\n    stride : tuple\n        Stride used in convolution\n    dim_match : Boolean\n        True means channel number between input and output is the same, otherwise means differ\n    trainable: Boolean\n        trainning or testing, True is trainning, otherwise is testing.\n    name : str\n        Base name of the operators\n    """"""\n    shape = [3, 3]\n    in_filter= data.get_shape().as_list()[-1]\n    shape.append(int(in_filter))\n    shape.append((out_filter))\n\n    # print(name)\n\n    bn1 = batch_normalization(data, variance_epsilon=2e-5, trainable=trainable, name=name + \'_bn1\')\n    bn1_pad = tf.pad(bn1, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n    conv1 = convolution(bn1_pad, group=1, shape=shape, strides=[1, 1], padding=\'VALID\', trainable=trainable, name=name + \'_conv1\')\n    bn2 = batch_normalization(conv1, variance_epsilon=2e-5, trainable=trainable, name=name + \'_bn2\')\n    relu1 = prelu(bn2, trainable=trainable, name=name + \'_relu1\')\n    relu1_pad = tf.pad(relu1, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n    shape[-2] = relu1_pad.get_shape().as_list()[-1]\n    conv2 = convolution(relu1_pad, group=1, shape=shape, strides=stride, padding=\'VALID\', trainable=trainable, name=name + \'_conv2\')\n    bn3 = batch_normalization(conv2, variance_epsilon=2e-5, trainable=trainable, name=name + \'_bn3\')\n\n    if dim_match:\n        shortcut = data\n    else:\n        shape = [1, 1]\n        in_filter = data.get_shape().as_list()[-1]\n        shape.append(int(in_filter))\n        shape.append((out_filter))\n\n        conv1sc = convolution(data, group=1, shape=shape, strides=stride, padding=\'VALID\', trainable=trainable, name=name + \'_conv1sc\')\n        shortcut = batch_normalization(conv1sc, variance_epsilon=2e-5, trainable=trainable, name=name + \'_sc\')\n\n    return bn3 + shortcut\n\n\ndef residual_unit(data, out_filter, stride, dim_match, trainable, name, **kwargs):\n    return residual_unit_v3(data, out_filter, stride, dim_match, trainable, name=name, **kwargs)\n\ndef prelu(input, trainable, name):\n    gamma = tf.get_variable(initializer=tf.constant(0.25,dtype=tf.float32,shape=[input.get_shape()[-1]]), trainable=trainable, name=name + ""_gamma"")\n    return tf.maximum(0.0, input) + gamma * tf.minimum(0.0, input)\n\nMOVING_AVERAGE_DECAY = 0.9997\nBN_DECAY = MOVING_AVERAGE_DECAY\nBN_EPSILON = 0.001\nCONV_WEIGHT_DECAY = 0.00004\nCONV_WEIGHT_STDDEV = 0.1\nFC_WEIGHT_DECAY = 0.00004\nFC_WEIGHT_STDDEV = 0.01\nRESNET_VARIABLES = \'resnet_variables\'\nUPDATE_OPS_COLLECTION = \'resnet_update_ops\'  # must be grouped with training op\n\ndef batch_normalization(input, trainable, name, **kwargs):\n    input_shape = input.get_shape()\n    shape = input_shape.as_list()[-1::]\n    axis = list(range(len(input_shape) - 1))\n    moving_mean = tf.get_variable(shape=shape, initializer=tf.zeros_initializer, trainable=trainable, name=name + ""_mean"")\n    moving_variance = tf.get_variable(shape=shape, initializer=tf.ones_initializer, trainable=trainable, name=name + ""_var"")\n    offset = tf.get_variable(shape=shape, initializer=tf.zeros_initializer, trainable=trainable, name=name + ""_bias"")\n    scale = tf.get_variable(shape=shape, initializer=tf.ones_initializer, trainable=trainable, name=name + ""_scale"") if name != \'fc1\' else None\n\n    mean, variance = tf.nn.moments(input, axis)\n    update_moving_mean = moving_averages.assign_moving_average(moving_mean, mean, BN_DECAY)\n    update_moving_variance = moving_averages.assign_moving_average(moving_variance, variance, BN_DECAY)\n    tf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_mean)\n    tf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_variance)\n    is_training = tf.convert_to_tensor(trainable, dtype=\'bool\', name=\'is_training\')\n    mean, variance = control_flow_ops.cond(is_training,\n        lambda: (mean, variance),\n        lambda: (moving_mean, moving_variance))\n\n    return tf.nn.batch_normalization(input, mean, variance, offset, scale, name=name, **kwargs)\n\ndef convolution(input, group, shape, trainable, name, **kwargs):\n    w = tf.get_variable(initializer=tf.truncated_normal(shape, stddev=0.1), trainable=trainable, name=name + ""_weight"")\n    if group == 1:\n        layer = tf.nn.convolution(input, pruning.apply_mask(w, name + ""_weight""), **kwargs)\n    else:\n        weight_groups = tf.split(w, num_or_size_splits=group, axis=-1)\n        xs = tf.split(input, num_or_size_splits=group, axis=-1)\n        convolved = [tf.nn.convolution(x, pruning.apply_mask(weight, name + ""_weight_groups""), **kwargs) for\n                     (x, weight) in zip(xs, weight_groups)]\n        layer = tf.concat(convolved, axis=-1)\n\n    if name.endswith(\'_sc\'):\n        b = tf.get_variable(initializer=tf.truncated_normal(input.get_shape().as_list()[-1::], stddev=0.1), trainable=trainable, name=name + ""_bias"")\n        layer = layer + b\n    return layer\n\ndef resnet(inputs, w_init, units, num_stages, filter_list, trainable, reuse=False):\n    """"""Return ResNet symbol of\n    Parameters\n    ----------\n    units : list\n        Number of units in each stage\n    num_stages : int\n        Number of stage\n    filter_list : list\n        Channel size of each stage\n    num_classes : int\n        Ouput size of symbol\n    dataset : str\n        Dataset type, only cifar10 and imagenet supports\n    """"""\n    #version_se = kwargs.get(\'version_se\', 1)\n    #version_input = kwargs.get(\'version_input\', 1)\n    #assert version_input >= 0\n    #version_output = kwargs.get(\'version_output\', \'E\')\n    #version_unit = kwargs.get(\'version_unit\', 3)\n    #print(version_se, version_input, version_output, version_unit)\n    num_unit = len(units)\n    assert (num_unit == num_stages)\n    inputs = inputs - 127.5\n    inputs = inputs * 0.0078125\n\n    with tf.variable_scope(variable_scope, reuse=reuse):\n        net = tf.pad(inputs, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = convolution(net, group=1, strides=[1, 1], shape=[3, 3, 3, 64], padding=\'VALID\', trainable=trainable, name=\'conv0\')\n        net = batch_normalization(net, variance_epsilon=2e-5, trainable=trainable, name=\'bn0\')\n        net = prelu(net, trainable=trainable, name=\'relu0\')\n\n        body = net\n        for i in range(num_stages):\n            body = residual_unit(body, filter_list[i + 1], (2, 2), False, trainable=trainable, name=\'stage%d_unit%d\' % (i + 1, 1))\n            for j in range(units[i] - 1):\n                body = residual_unit(body, filter_list[i + 1], (1, 1), True, trainable=trainable, name=\'stage%d_unit%d\' % (i + 1, j + 2))\n\n        bn1 = batch_normalization(body, variance_epsilon=2e-5, trainable=trainable, name=\'bn1\')\n        bn1_shape = bn1.get_shape().as_list()\n        bn1 = tf.reshape(bn1, shape=[-1, bn1_shape[1] * bn1_shape[2] * bn1_shape[3]], name=\'E_Reshapelayer\')\n        pre_fc1 = tf.layers.dense(bn1, units=512, kernel_initializer=w_init, use_bias=True)\n        fc1 = batch_normalization(pre_fc1, variance_epsilon=2e-5, trainable=trainable, name=\'fc1\')\n\n    return fc1, pre_fc1\n\n\ndef get_resnet(inputs, w_init, num_layers, trainable, reuse=False):\n    """"""\n    Adapted from https://github.com/tornadomeet/ResNet/blob/master/train_resnet.py\n    Original author Wei Wu\n    """"""\n    if num_layers >= 101:\n        filter_list = [64, 256, 512, 1024, 2048]\n        bottle_neck = True\n    else:\n        filter_list = [64, 64, 128, 256, 512]\n        bottle_neck = False\n    num_stages = 4\n    if num_layers == 18:\n        units = [2, 2, 2, 2]\n    elif num_layers == 34:\n        units = [3, 4, 6, 3]\n    elif num_layers == 49:\n        units = [3, 4, 14, 3]\n    elif num_layers == 50:\n        units = [3, 4, 14, 3]\n    elif num_layers == 74:\n        units = [3, 6, 24, 3]\n    elif num_layers == 90:\n        units = [3, 8, 30, 3]\n    elif num_layers == 100:\n        units = [3, 13, 30, 3]\n    elif num_layers == 101:\n        units = [3, 4, 23, 3]\n    elif num_layers == 152:\n        units = [3, 8, 36, 3]\n    elif num_layers == 200:\n        units = [3, 24, 36, 3]\n    elif num_layers == 269:\n        units = [3, 30, 48, 8]\n    else:\n        raise ValueError(""no experiments done on num_layers {}, you can do it yourself"".format(num_layers))\n\n    return resnet(inputs=inputs,\n                  w_init=w_init,\n                  units=units,\n                  num_stages=num_stages,\n                  filter_list=filter_list,\n                  trainable=trainable,\n                  reuse=reuse)\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--pretrained_model\', type=str, help=\'Load a pretrained model before training starts.\')\n    parser.add_argument(\'--ckpt_path\', type=str, help=\'the checkpoint path to save model.\')\n    args = parser.parse_args(sys.argv[1:])\n\n    with tf.Graph().as_default():\n        with tf.Session() as sess:\n            input = tf.placeholder(dtype=tf.float32, shape=[None, 112, 112, 3], name=\'input\')\n            trainable_placeholder = tf.placeholder_with_default(tf.constant(False, dtype=tf.bool), shape=None, name=\'trainable\')\n            w_init_method = tf.contrib.layers.xavier_initializer(uniform=False)\n            prelogits = get_resnet(input, w_init=w_init_method, num_layers=50, trainable=trainable_placeholder)\n\n            embeddings = tf.identity(prelogits, name=\'embeddings\')\n\n            saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n            print(args.pretrained_model)\n            ckpt = tf.train.get_checkpoint_state(args.pretrained_model)\n            print(ckpt)\n            saver.restore(sess, ckpt.model_checkpoint_path)\n            saver.save(sess, args.ckpt_path, global_step=0)\n\n    print(\'test finish!\')\n'"
nets/MobileFaceNet.py,15,"b'# Copyright 2018 The AI boy xsr-ai. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n""""""MobileFaceNets.\n\nMobileFaceNets, which use less than 1 million parameters and are specifically tailored for high-accuracy real-time\nface verification on mobile and embedded devices.\n\nhere is MobileFaceNets architecture, reference from MobileNet_V2 (https://github.com/xsr-ai/MobileNetv2_TF).\n\nAs described in https://arxiv.org/abs/1804.07573.\n\n  MobileFaceNets: Efficient CNNs for Accurate Real-time Face Verification on Mobile Devices\n\n  Sheng Chen, Yang Liu, Xiang Gao, Zhen Han\n\n""""""\n\n# Tensorflow mandates these.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import namedtuple\nimport functools\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n# Conv and InvResBlock namedtuple define layers of the MobileNet architecture\n# Conv defines 3x3 convolution layers\n# InvResBlock defines 3x3 depthwise convolution followed by 1x1 convolution.\n# stride is the stride of the convolution\n# depth is the number of channels or filters in a layer\nConv = namedtuple(\'Conv\', [\'kernel\', \'stride\', \'depth\', \'ratio\'])\nDepthwiseConv = namedtuple(\'DepthwiseConv\', [\'kernel\', \'stride\', \'depth\', \'ratio\'])\nInvResBlock = namedtuple(\'InvResBlock\', [\'kernel\', \'stride\', \'depth\', \'ratio\', \'repeate\'])\n\n# _CONV_DEFS specifies the MobileNet body\n_CONV_DEFS = [\n    Conv(kernel=[3, 3], stride=2, depth=64, ratio=1),\n    DepthwiseConv(kernel=[3, 3], stride=1, depth=64, ratio=1),\n\n    InvResBlock(kernel=[3, 3], stride=2, depth=64, ratio=2, repeate=5),\n    InvResBlock(kernel=[3, 3], stride=2, depth=128, ratio=4, repeate=1),\n    InvResBlock(kernel=[3, 3], stride=1, depth=128, ratio=2, repeate=6),\n    InvResBlock(kernel=[3, 3], stride=2, depth=128, ratio=4, repeate=1),\n    InvResBlock(kernel=[3, 3], stride=1, depth=128, ratio=2, repeate=2),\n\n    Conv(kernel=[1, 1], stride=1, depth=512, ratio=1),\n]\n\ndef inverted_block(net, input_filters, output_filters, expand_ratio, stride, scope=None):\n    \'\'\'fundamental network struture of inverted residual block\'\'\'\n    with tf.name_scope(scope):\n        res_block = slim.conv2d(inputs=net, num_outputs=input_filters * expand_ratio, kernel_size=[1, 1])\n        # depthwise conv2d\n        res_block = slim.separable_conv2d(inputs=res_block, num_outputs=None, kernel_size=[3, 3], stride=stride, depth_multiplier=1.0, normalizer_fn=slim.batch_norm)\n        res_block = slim.conv2d(inputs=res_block, num_outputs=output_filters, kernel_size=[1, 1], activation_fn=None)\n        # stride 2 blocks\n        if stride == 2:\n            return res_block\n        # stride 1 block\n        else:\n            if input_filters != output_filters:\n                net = slim.conv2d(inputs=net, num_outputs=output_filters, kernel_size=[1, 1], activation_fn=None)\n            return tf.add(res_block, net)\n\ndef mobilenet_v2_base(inputs,\n                      final_endpoint=\'Conv2d_7\',\n                      min_depth=8,\n                      conv_defs=None,\n                      scope=None):\n  """"""Mobilenet v2.\n\n  Constructs a Mobilenet v2 network from inputs to the given final endpoint.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_0\', \'Conv2d_1_InvResBlock\', \'Conv2d_2_InvResBlock\',\n      \'Conv2d_3_InvResBlock\', \'Conv2d_4_InvResBlock\', \'Conv2d_5_InvResBlock,\n      \'Conv2d_6_InvResBlock\', \'Conv2d_7_InvResBlock\', \'Conv2d_8\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced output depth to min_depth.\n    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values\n                is not allowed.\n  """"""\n  depth = lambda d: max(int(d), min_depth)\n  end_points = {}\n\n  if conv_defs is None:\n    conv_defs = _CONV_DEFS\n\n  with tf.variable_scope(scope, \'MobileFaceNet\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d], padding=\'SAME\'):\n\n      net = inputs\n      for i, conv_def in enumerate(conv_defs):\n        end_point_base = \'Conv2d_%d\' % i\n\n        if isinstance(conv_def, Conv):\n          end_point = end_point_base\n          net = slim.conv2d(net, depth(conv_def.depth), conv_def.kernel,\n                            stride=conv_def.stride,\n                            normalizer_fn=slim.batch_norm,\n                            scope=end_point)\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n\n        elif isinstance(conv_def, DepthwiseConv):\n            end_point = \'DepthwiseConv\'\n            # depthwise conv2d\n            net = slim.separable_conv2d(inputs=net, num_outputs=None, kernel_size=conv_def.kernel, stride=conv_def.stride,\n                                        depth_multiplier=1.0, normalizer_fn=slim.batch_norm)\n            net = slim.conv2d(inputs=net, num_outputs=conv_def.depth, kernel_size=[1, 1], activation_fn=None)\n            end_points[end_point] = net\n            if end_point == final_endpoint:\n                return net, end_points\n\n        elif isinstance(conv_def, InvResBlock):\n          end_point = end_point_base + \'_InvResBlock\'\n          # inverted bottleneck blocks\n          input_filters = net.shape[3].value\n          # first layer needs to consider stride\n          net = inverted_block(net, input_filters, depth(conv_def.depth), conv_def.ratio, conv_def.stride, end_point+\'_0\')\n          for index in range(1, conv_def.repeate):\n              suffix = \'_\' + str(index)\n              net = inverted_block(net, input_filters, depth(conv_def.depth), conv_def.ratio, 1, end_point+suffix)\n\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n\n        else:\n          raise ValueError(\'Unknown convolution type %s for layer %d\'\n                           % (conv_def.ltype, i))\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef mobilenet_v2(inputs,\n                 bottleneck_layer_size=128,\n                 is_training=False,\n                 min_depth=8,\n                 conv_defs=None,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'MobileFaceNet\',\n                 global_pool=False):\n  """"""Mobilenet v2 model for classification.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    bottleneck_layer_size: number of predicted classes. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    is_training: whether is training or not.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced output depth to min_depth..\n    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    global_pool: Optional boolean flag to control the avgpooling before the\n      logits layer. If false or unset, pooling is done with a fixed window\n      that reduces default-sized inputs to 1x1, while larger inputs lead to\n      larger outputs. If true, any input size is pooled down to 1x1.\n\n  Returns:\n    net: a 2D Tensor with the logits (pre-softmax activations) if bottleneck_layer_size\n      is a non-zero integer, or the non-dropped-out input to the logits layer\n      if bottleneck_layer_size is 0 or None.\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Invalid input tensor rank, expected 4, was: %d\' %\n                     len(input_shape))\n\n  with tf.variable_scope(scope, \'MobileFaceNet\', [inputs], reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):\n      net, end_points = mobilenet_v2_base(inputs, scope=scope, min_depth=min_depth, conv_defs=conv_defs)\n\n      with tf.variable_scope(\'Logits\'):\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name=\'global_pool\')\n          end_points[\'global_pool\'] = net\n        else:\n          # Pooling with a fixed kernel size.\n          kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n\n          # Global depthwise conv2d\n          net = slim.separable_conv2d(inputs=net, num_outputs=None, kernel_size=kernel_size, stride=1,\n                                      depth_multiplier=1.0, activation_fn=None, padding=\'VALID\')\n          net = slim.conv2d(inputs=net, num_outputs=512, kernel_size=[1, 1], stride=1, activation_fn=None, padding=\'VALID\')\n          end_points[\'GDConv\'] = net\n\n        if not bottleneck_layer_size:\n          return net, end_points\n        # 1 x 1 x 1024\n        # net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, bottleneck_layer_size, kernel_size=[1, 1], stride=1, activation_fn=None, scope=\'LinearConv1x1\')\n\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n\n  return logits, end_points\n\nmobilenet_v2.default_image_size = 112\n\n\ndef wrapped_partial(func, *args, **kwargs):\n  partial_func = functools.partial(func, *args, **kwargs)\n  functools.update_wrapper(partial_func, func)\n  return partial_func\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\ndef prelu(input, name=\'\'):\n    alphas = tf.get_variable(name=name + \'prelu_alphas\',initializer=tf.constant(0.25,dtype=tf.float32,shape=[input.get_shape()[-1]]))\n    pos = tf.nn.relu(input)\n    neg = alphas * (input - abs(input)) * 0.5\n    return pos + neg\n\ndef mobilenet_v2_arg_scope(is_training=True,\n                           weight_decay=0.00005,\n                           regularize_depthwise=False):\n  """"""Defines the default MobilenetV2 arg scope.\n\n  Args:\n    is_training: Whether or not we\'re training the model.\n    weight_decay: The weight decay to use for regularizing the model.\n    regularize_depthwise: Whether or not apply regularization on depthwise.\n\n  Returns:\n    An `arg_scope` to use for the mobilenet v2 model.\n  """"""\n  batch_norm_params = {\n      \'is_training\': is_training,\n      \'center\': True,\n      \'scale\': True,\n      \'fused\': True,\n      \'decay\': 0.995,\n      \'epsilon\': 2e-5,\n      # force in-place updates of mean and variance estimates\n      \'updates_collections\': None,\n      # Moving averages ends up in the trainable variables collection\n      \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n  }\n\n  # Set weight_decay for weights in Conv and InvResBlock layers.\n  #weights_init = tf.truncated_normal_initializer(stddev=stddev)\n  weights_init = tf.contrib.layers.xavier_initializer(uniform=False)\n  regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n  if regularize_depthwise:\n    depthwise_regularizer = regularizer\n  else:\n    depthwise_regularizer = None\n  with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                      weights_initializer=weights_init,\n                      activation_fn=prelu, normalizer_fn=slim.batch_norm): #tf.keras.layers.PReLU\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      with slim.arg_scope([slim.conv2d], weights_regularizer=regularizer):\n        with slim.arg_scope([slim.separable_conv2d],\n                            weights_regularizer=depthwise_regularizer) as sc:\n          return sc\n\ndef inference(images, bottleneck_layer_size=128, phase_train=False,\n              weight_decay=0.00005, reuse=False):\n    \'\'\'build a mobilenet_v2 graph to training or inference.\n\n    Args:\n        images: a tensor of shape [batch_size, height, width, channels].\n        bottleneck_layer_size: number of predicted classes. If 0 or None, the logits layer\n          is omitted and the input features to the logits layer (before dropout)\n          are returned instead.\n        phase_train: Whether or not we\'re training the model.\n        weight_decay: The weight decay to use for regularizing the model.\n        reuse: whether or not the network and its variables should be reused. To be\n          able to reuse \'scope\' must be given.\n\n    Returns:\n        net: a 2D Tensor with the logits (pre-softmax activations) if bottleneck_layer_size\n          is a non-zero integer, or the non-dropped-out input to the logits layer\n          if bottleneck_layer_size is 0 or None.\n        end_points: a dictionary from components of the network to the corresponding\n          activation.\n\n    Raises:\n        ValueError: Input rank is invalid.\n    \'\'\'\n    arg_scope = mobilenet_v2_arg_scope(is_training=phase_train, weight_decay=weight_decay)\n    with slim.arg_scope(arg_scope):\n        return mobilenet_v2(images, bottleneck_layer_size=bottleneck_layer_size, is_training=phase_train, reuse=reuse)'"
nets/MobileNetV3.py,35,"b'# -*- coding: utf-8 -*-\n#!/usr/bin/env python\n\n# Copyright 2019 aiboy.wei Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""\nImplementation of paper Searching for MobileNetV3, https://arxiv.org/abs/1905.02244\nauthor: aiboy.wei@outlook.com\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nMobileNetV3_Small_Spec = [\n    # Op            k    exp    out    SE     NL        s\n    [ ""ConvBnAct"",  3,   False, 16,    False, ""hswish"", 2 ],\n    [ ""bneck"",      3,   16,    16,    True,  ""relu"",   2 ],\n    [ ""bneck"",      3,   72,    24,    False, ""relu"",   2 ],\n    [ ""bneck"",      3,   88,    24,    False, ""relu"",   1 ],\n    [ ""bneck"",      5,   96,    40,    True,  ""hswish"", 2 ],\n    [ ""bneck"",      5,   240,   40,    True,  ""hswish"", 1 ],\n    [ ""bneck"",      5,   240,   40,    True,  ""hswish"", 1 ],\n    [ ""bneck"",      5,   120,   48,    True,  ""hswish"", 1 ],\n    [ ""bneck"",      5,   144,   48,    True,  ""hswish"", 1 ],\n    [ ""bneck"",      5,   288,   96,    True,  ""hswish"", 2 ],\n    [ ""bneck"",      5,   576,   96,    True,  ""hswish"", 1 ],\n    [ ""bneck"",      5,   576,   96,    True,  ""hswish"", 1 ],\n    [ ""ConvBnAct"",  1,   False, 576,   True,  ""hswish"", 1 ],\n    [ ""pool"",       7,   False, False, False, ""None"",   1 ],\n    [ ""ConvNBnAct"", 1,   False, 1280,  False, ""hswish"", 1 ],\n    [ ""ConvNBnAct"", 1,   False, 1000,  False, ""None"",   1 ],\n]\n\nMobileNetV3_Large_Spec = [\n    # Op            k    exp    out    SE     NL        s\n    [ ""ConvBnAct"",  3,   False, 16,    False, ""hswish"", 2 ],\n    [ ""bneck"",      3,   16,    16,    False, ""relu"",   1 ],\n    [ ""bneck"",      3,   64,    24,    False, ""relu"",   2 ],\n    [ ""bneck"",      3,   72,    24,    False, ""relu"",   1 ],\n    [ ""bneck"",      5,   72,    40,    True,  ""relu"",   2 ],\n    [ ""bneck"",      5,   120,   40,    True,  ""relu"",   1 ],\n    [ ""bneck"",      5,   120,   40,    True,  ""relu"",   1 ],\n    [ ""bneck"",      3,   240,   80,    False, ""hswish"", 2 ],\n    [ ""bneck"",      3,   200,   80,    False, ""hswish"", 1 ],\n    [ ""bneck"",      3,   184,   80,    False, ""hswish"", 1 ],\n    [ ""bneck"",      3,   184,   80,    False, ""hswish"", 1 ],\n    [ ""bneck"",      3,   480,   112,   True,  ""hswish"", 1 ],\n    [ ""bneck"",      3,   672,   112,   True,  ""hswish"", 1 ],\n    [ ""bneck"",      5,   672,   160,   True,  ""hswish"", 2 ],\n    [ ""bneck"",      5,   960,   160,   True,  ""hswish"", 1 ],\n    [ ""bneck"",      5,   960,   160,   True,  ""hswish"", 1 ],\n    [ ""ConvBnAct"",  1,   False, 960,   False, ""hswish"", 1 ],\n    [ ""pool"",       7,   False, False, False, ""None"",   1 ],\n    [ ""ConvNBnAct"", 1,   False, 1280,  False, ""hswish"", 1 ],\n    [ ""ConvNBnAct"", 1,   False, 1000,  False, ""None"",   1 ],\n]\n\ndef _make_divisible(v, divisor, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n\n    return new_v\n\nclass Linear(tf.keras.layers.Layer):\n    def __init__(self, name=""Linear""):\n        super(Linear, self).__init__(name=name)\n\n    def call(self, input):\n        return input\n\nclass HardSigmoid(tf.keras.layers.Layer):\n    def __init__(self, name=""HardSigmoid""):\n        super(HardSigmoid, self).__init__(name=name)\n        self.relu6 = tf.keras.layers.ReLU(max_value=6, name=""ReLU6"")\n\n    def call(self, input):\n        return self.relu6(input + 3.0) / 6.0\n\nclass HardSwish(tf.keras.layers.Layer):\n    def __init__(self, name=""HardSwish""):\n        super(HardSwish, self).__init__(name=name)\n        self.relu6 = tf.keras.layers.ReLU(max_value=6, name=""ReLU6"")\n\n    def call(self, input):\n        return input * self.relu6(input + 3.0) / 6.0\n\n_available_activation = {\n            ""relu"": tf.keras.layers.ReLU(name=""ReLU""),\n            ""relu6"": tf.keras.layers.ReLU(max_value=6, name=""ReLU6""),\n            ""hswish"": HardSwish(),\n            ""hsigmoid"": HardSigmoid(),\n            ""softmax"": tf.keras.layers.Softmax(name=""Softmax""),\n            ""None"": Linear(),\n        }\n\nclass SENet(tf.keras.layers.Layer):\n    def __init__(self, reduction=4, l2=2e-4, name=""SENet""):\n        super(SENet, self).__init__(name=name)\n        self.reduction = reduction\n        self.l2_reg = l2\n\n    def build(self, input_shape):\n        _, h, w, c = input_shape\n        self.gap = tf.keras.layers.GlobalAveragePooling2D(name=f\'AvgPool{h}x{w}\')\n        self.fc1 = tf.keras.layers.Dense(units=c//self.reduction, activation=""relu"", use_bias=False,\n                                         kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg), name=""Squeeze"")\n        self.fc2 = tf.keras.layers.Dense(units=c, activation=HardSigmoid(), use_bias=False,\n                                         kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg), name=""Excite"")\n        self.reshape = tf.keras.layers.Reshape((1, 1, c), name=f\'Reshape(-1, 1, 1, {c})\')\n\n        super().build(input_shape)\n\n    def call(self, input):\n        output = self.gap(input)\n        output = self.fc1(output)\n        output = self.fc2(output)\n        output = self.reshape(output)\n        return input * output\n\nclass ConvBnAct(tf.keras.layers.Layer):\n    def __init__(self, k, exp, out, SE, NL, s, l2, name=""ConvBnAct""):\n        super(ConvBnAct, self).__init__(name=name)\n        self.conv2d = tf.keras.layers.Conv2D(filters=out, kernel_size=k, strides=s, activation=None, padding=""same"",\n                                             kernel_regularizer=tf.keras.regularizers.l2(l2), name=""conv2d"")\n        self.bn = tf.keras.layers.BatchNormalization(momentum=0.99, name=""BatchNormalization"")\n        self.act = _available_activation[NL]\n\n    def call(self, input):\n        output = self.conv2d(input)\n        output = self.bn(output)\n        output = self.act(output)\n        return output\n\nclass ConvNBnAct(tf.keras.layers.Layer):\n    def __init__(self, k, exp, out, SE, NL, s, l2, name=""ConvNBnAct""):\n        super(ConvNBnAct, self).__init__(name=name)\n        self.act = _available_activation[NL]\n        self.fn = tf.keras.layers.Conv2D(filters=out, kernel_size=k, strides=s, activation=self.act, padding=""same"",\n                                         kernel_regularizer=tf.keras.regularizers.l2(l2),name=""conv2d"")\n\n    def call(self, input):\n        output = self.fn(input)\n        return output\n\nclass Pool(tf.keras.layers.Layer):\n    def __init__(self, k, exp, out, SE, NL, s, l2, name=""Pool""):\n        super(Pool, self).__init__(name=name)\n        self.gap = tf.keras.layers.AveragePooling2D(pool_size=(k, k), strides=1, name=f\'AvgPool{k}x{k}\')\n\n    def call(self, input):\n        output = self.gap(input)\n        return output\n\nclass BottleNeck(tf.keras.layers.Layer):\n    def __init__(self, k, exp, out, SE, NL, s, l2, name=""BottleNeck""):\n        super(BottleNeck, self).__init__(name=name)\n        self.use_se = SE\n        self.stride = s\n        self.exp_ch = exp\n        self.out_ch = out\n\n        self.expand = ConvBnAct(k=1, exp=exp, out=exp, SE=SE, NL=NL, s=1, l2=l2, name=""BottleNeckExpand"")\n        self.depthwise = tf.keras.layers.DepthwiseConv2D(\n            kernel_size=k,\n            strides=s,\n            padding=""same"",\n            use_bias=False,\n            depthwise_regularizer=tf.keras.regularizers.l2(l2),\n            name=f\'Depthwise{k}x{k}\',\n        )\n        self.pointwise = tf.keras.layers.Conv2D(\n            filters=out,\n            kernel_size=1,\n            strides=1,\n            use_bias=False,\n            kernel_regularizer=tf.keras.regularizers.l2(l2),\n            name=f\'Pointwise1x1\',\n        )\n        self.bn = tf.keras.layers.BatchNormalization(momentum=0.99, name=""BatchNormalization"")\n\n        if self.use_se:\n            self.se = SENet(name=""SEBottleneck"",)\n\n        self.act = _available_activation[NL]\n\n    def call(self, input):\n        output = self.expand(input)\n        output = self.depthwise(output)\n        output = self.bn(output)\n        if self.use_se:\n            output = self.se(output)\n        output = self.act(output)\n        output = self.pointwise(output)\n        output = self.bn(output)\n\n        if self.stride == 1 and self.exp_ch == self.out_ch:\n            return input + output\n        else:\n            return output\n\n_available_mobilenetv3_spec = {\n            ""small"": MobileNetV3_Small_Spec,\n            ""large"": MobileNetV3_Large_Spec,\n        }\n\n_available_operation = {\n            ""ConvBnAct"":  ConvBnAct,\n            ""bneck"":      BottleNeck,\n            ""pool"":       Pool,\n            ""ConvNBnAct"": ConvNBnAct,\n        }\n\nclass MobileNetV3(tf.keras.Model):\n    def __init__(self, type=""large"", classes_numbers=1000, width_multiplier=1.0, divisible_by=8,\n                 l2_reg=2e-5, dropout_rate=0.2, name=""MobileNetV3""):\n        super(MobileNetV3, self).__init__()\n        self.spec = _available_mobilenetv3_spec[type]\n        self.spec[-1][3] = classes_numbers # bottlenet layer size or class numbers\n        self._name = name+""_""+type\n        self.backbone = tf.keras.Sequential(name=""arch"")\n        for i, params in enumerate(self.spec):\n            Op, k, exp, out, SE, NL, s = params\n            inference_op = _available_operation[Op]\n\n            if isinstance(exp, int):\n                exp_ch = _make_divisible(exp * width_multiplier, divisible_by)\n            else:\n                exp_ch = None\n            if isinstance(out, int):\n                out_ch = _make_divisible(out * width_multiplier, divisible_by)\n            else:\n                out_ch = None\n\n            name = f\'{Op}_{i}\'\n            self.backbone.add(inference_op(k, exp_ch, out_ch, SE, NL, s, l2_reg, name))\n            if (type == ""small"" and i == 14) or (type == ""large"" and i == 18):\n                self.dropout = tf.keras.layers.Dropout(rate=dropout_rate, name=f\'{self._name}/Dropout\')\n                self.backbone.add(self.dropout)\n\n    def call(self, input):\n        output = self.backbone(input)\n        return output\n\n\nif __name__ == ""__main__"":\n    import numpy as np\n\n    for gpu in tf.config.experimental.list_physical_devices(\'GPU\'):\n        tf.compat.v2.config.experimental.set_memory_growth(gpu, True)\n    model = MobileNetV3(type=""small"")\n\n    data = np.zeros((10, 224, 224, 3), dtype=np.float32)\n    pre = model(data)\n    print(model.variables)\n    model.save_weights(""./mobilenetv3.h5"")\n    print(model.summary())'"
nets/__init__.py,0,b''
utils/__init__.py,0,b''
utils/common.py,18,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ndef _add_loss_summaries(total_loss, summaries):\n    """"""Add summaries for losses.\n\n    Generates moving average for all losses and associated summaries for\n    visualizing the performance of the network.\n\n    Args:\n      total_loss: Total loss from loss().\n    Returns:\n      loss_averages_op: op for generating moving averages of losses.\n    """"""\n    # Compute the moving average of all individual losses and the total loss.\n    loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n    losses = tf.get_collection(\'losses\')\n    loss_averages_op = loss_averages.apply(losses + [total_loss])\n\n    # Attach a scalar summmary to all individual losses and the total loss; do the\n    # same for the averaged version of the losses.\n    for l in losses + [total_loss]:\n        # Name each loss as \'(raw)\' and name the moving average version of the loss\n        # as the original loss name.\n        summaries.append(tf.summary.scalar(l.op.name + \' (raw)\', l))\n        summaries.append(tf.summary.scalar(l.op.name, loss_averages.average(l)))\n\n    return loss_averages_op\n\n\ndef train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, summaries,\n          log_histograms=True):\n    # Generate moving averages of all losses and associated summaries.\n    loss_averages_op = _add_loss_summaries(total_loss, summaries)\n\n    # Compute gradients.\n    with tf.control_dependencies([loss_averages_op]):\n        if optimizer == \'ADAGRAD\':\n            opt = tf.train.AdagradOptimizer(learning_rate)\n        elif optimizer == \'ADADELTA\':\n            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n        elif optimizer == \'ADAM\':\n            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n        elif optimizer == \'RMSPROP\':\n            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n        elif optimizer == \'MOM\':\n            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n        else:\n            raise ValueError(\'Invalid optimization algorithm\')\n\n        grads = opt.compute_gradients(total_loss, update_gradient_vars)\n\n    # Apply gradients.\n    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n    # Add histograms for trainable variables.\n    if log_histograms:\n        for var in tf.trainable_variables():\n            summaries.append(tf.summary.histogram(var.op.name, var))\n\n    # Add histograms for gradients.\n    if log_histograms:\n        for grad, var in grads:\n            if grad is not None:\n                summaries.append(tf.summary.histogram(var.op.name + \'/gradients\', grad))\n\n    # Track the moving averages of all trainable variables.\n    variable_averages = tf.train.ExponentialMovingAverage(\n        moving_average_decay, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n    with tf.control_dependencies([apply_gradient_op, variables_averages_op] + update_ops):\n        train_op = tf.no_op(name=\'train\')\n\n    return train_op'"
utils/data_process.py,19,"b'import tensorflow as tf\nfrom scipy import misc\nimport numpy as np\nimport mxnet as mx\nimport argparse\nimport random\nimport pickle\nimport cv2\nimport os\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description=\'data path information\'\n    )\n    parser.add_argument(\'--bin_path\', default=\'./datasets/faces_ms1m_112x112/train.rec\', type=str,\n                        help=\'path to the binary image file\')\n    parser.add_argument(\'--idx_path\', default=\'./datasets/faces_ms1m_112x112/train.idx\', type=str,\n                        help=\'path to the image index path\')\n    parser.add_argument(\'--tfrecords_file_path\', default=\'../datasets/tfrecords\', type=str,\n                        help=\'path to the output of tfrecords file path\')\n    args = parser.parse_args()\n    return args\n\ndef mx2tfrecords(imgidx, imgrec, args):\n    output_path = os.path.join(args.tfrecords_file_path, \'tran.tfrecords\')\n    if not os.path.exists(args.tfrecords_file_path):\n        os.makedirs(args.tfrecords_file_path)\n    writer = tf.python_io.TFRecordWriter(output_path)\n    random.shuffle(imgidx)\n    for i, index in enumerate(imgidx):\n        img_info = imgrec.read_idx(index)\n        header, img = mx.recordio.unpack(img_info)\n        label = int(header.label)\n        example = tf.train.Example(features=tf.train.Features(feature={\n            \'image_raw\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img])),\n            ""label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n        }))\n        writer.write(example.SerializeToString())  # Serialize To String\n        if i % 10000 == 0:\n            print(\'%d num image processed\' % i)\n    print(\'%d num image processed\' % i)\n    writer.close()\n\ndef random_rotate_image(image):\n    angle = np.random.uniform(low=-10.0, high=10.0)\n    return misc.imrotate(image, angle, \'bicubic\')\n\ndef parse_function(example_proto):\n    features = {\'image_raw\': tf.FixedLenFeature([], tf.string),\n                \'label\': tf.FixedLenFeature([], tf.int64)}\n    features = tf.parse_single_example(example_proto, features)\n    # You can do more image distortion here for training data\n    img = tf.image.decode_jpeg(features[\'image_raw\'])\n    img = tf.reshape(img, shape=(112, 112, 3))\n\n    #img = tf.py_func(random_rotate_image, [img], tf.uint8)\n    img = tf.cast(img, dtype=tf.float32)\n    img = tf.subtract(img, 127.5)\n    img = tf.multiply(img,  0.0078125)\n    img = tf.image.random_flip_left_right(img)\n    label = tf.cast(features[\'label\'], tf.int64)\n    return img, label\n\ndef create_tfrecords():\n    \'\'\'convert mxnet data to tfrecords.\'\'\'\n    id2range = {}\n    args = parse_args()\n\n    imgrec = mx.recordio.MXIndexedRecordIO(args.idx_path, args.bin_path, \'r\')\n    s = imgrec.read_idx(0)\n    header, _ = mx.recordio.unpack(s)\n    #print(header.label)\n    imgidx = list(range(1, int(header.label[0])))\n    seq_identity = range(int(header.label[0]), int(header.label[1]))\n    for identity in seq_identity:\n        s = imgrec.read_idx(identity)\n        header, _ = mx.recordio.unpack(s)\n        a, b = int(header.label[0]), int(header.label[1])\n        id2range[identity] = (a, b)\n    print(\'id2range\', len(id2range))\n    print(\'Number of examples in training set: {}\'.format(imgidx[-1]))\n\n    # generate tfrecords\n    mx2tfrecords(imgidx, imgrec, args)\n\ndef load_bin(db_name, image_size, args):\n    bins, issame_list = pickle.load(open(os.path.join(args.eval_db_path, db_name+\'.bin\'), \'rb\'), encoding=\'bytes\')\n    data_list = []\n    for _ in [0,1]:\n        data = np.empty((len(issame_list)*2, image_size[0], image_size[1], 3))\n        data_list.append(data)\n    for i in range(len(issame_list)*2):\n        _bin = bins[i]\n        img = mx.image.imdecode(_bin).asnumpy()\n        #img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        for flip in [0,1]:\n            if flip == 1:\n                img = np.fliplr(img)\n            data_list[flip][i, ...] = img\n        i += 1\n        if i % 1000 == 0:\n            print(\'loading bin\', i)\n    print(data_list[0].shape)\n\n    return data_list, issame_list\n\ndef load_data(db_name, image_size, args):\n    bins, issame_list = pickle.load(open(os.path.join(args.eval_db_path, db_name+\'.bin\'), \'rb\'), encoding=\'bytes\')\n    datasets = np.empty((len(issame_list)*2, image_size[0], image_size[1], 3))\n\n    for i in range(len(issame_list)*2):\n        _bin = bins[i]\n        img = mx.image.imdecode(_bin).asnumpy()\n        # img = cv2.imdecode(np.fromstring(_bin, np.uint8), -1)\n        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        #img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        img = img - 127.5\n        img = img * 0.0078125\n        datasets[i, ...] = img\n        i += 1\n        if i % 1000 == 0:\n            print(\'loading bin\', i)\n    print(datasets.shape)\n\n    return datasets, issame_list\n\ndef test_tfrecords():\n    args = parse_args()\n\n    config = tf.ConfigProto(allow_soft_placement=True)\n    sess = tf.Session(config=config)\n    # training datasets api config\n    tfrecords_f = os.path.join(args.tfrecords_file_path, \'tran.tfrecords\')\n    dataset = tf.data.TFRecordDataset(tfrecords_f)\n    dataset = dataset.map(parse_function)\n    dataset = dataset.shuffle(buffer_size=20000)\n    dataset = dataset.batch(32)\n    iterator = dataset.make_initializable_iterator()\n    next_element = iterator.get_next()\n    # begin iteration\n    for i in range(1000):\n        sess.run(iterator.initializer)\n        while True:\n            try:\n                images, labels = sess.run(next_element)\n                img = cv2.cvtColor(images[1, ...], cv2.COLOR_RGB2BGR)\n                cv2.imshow(\'test\', img)\n                cv2.waitKey(0)\n            except tf.errors.OutOfRangeError:\n                print(""End of dataset"")\n\nif __name__ == \'__main__\':\n    \'\'\'data process\'\'\'\n    create_tfrecords()\n\n\n\n\n'"
utils/freeze_graph.py,8,"b'""""""Imports a model metagraph and checkpoint file, converts the variables to constants\nand exports the model as a graphdef protobuf\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.framework import graph_util\nfrom six.moves import xrange\nimport tensorflow as tf\nimport argparse\nimport sys\nimport os\nimport re\n\ndef get_model_filenames(model_dir):\n    files = os.listdir(model_dir)\n    meta_files = [s for s in files if s.endswith(\'.meta\')]\n    if len(meta_files)==0:\n        raise ValueError(\'No meta file found in the model directory (%s)\' % model_dir)\n    elif len(meta_files)>1:\n        raise ValueError(\'There should not be more than one meta file in the model directory (%s)\' % model_dir)\n    meta_file = meta_files[0]\n    ckpt = tf.train.get_checkpoint_state(model_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n        ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n        return meta_file, ckpt_file\n\n    meta_files = [s for s in files if \'.ckpt\' in s]\n    max_step = -1\n    for f in files:\n        step_str = re.match(r\'(^model-[\\w\\- ]+.ckpt-(\\d+))\', f)\n        if step_str is not None and len(step_str.groups())>=2:\n            step = int(step_str.groups()[1])\n            if step > max_step:\n                max_step = step\n                ckpt_file = step_str.groups()[0]\n    return meta_file, ckpt_file\n\ndef main(args):\n    with tf.Graph().as_default():\n        with tf.Session() as sess:\n            # Load the model metagraph and checkpoint\n            print(\'Model directory: %s\' % args.model_dir)\n            meta_file, ckpt_file = get_model_filenames(os.path.expanduser(args.model_dir))\n            \n            print(\'Metagraph file: %s\' % meta_file)\n            print(\'Checkpoint file: %s\' % ckpt_file)\n\n            model_dir_exp = os.path.expanduser(args.model_dir)\n            saver = tf.train.import_meta_graph(os.path.join(model_dir_exp, meta_file), clear_devices=True)\n            tf.get_default_session().run(tf.global_variables_initializer())\n            tf.get_default_session().run(tf.local_variables_initializer())\n            saver.restore(tf.get_default_session(), os.path.join(model_dir_exp, ckpt_file))\n            \n            # Retrieve the protobuf graph definition and fix the batch norm nodes\n            input_graph_def = sess.graph.as_graph_def()\n            \n            # Freeze the graph def\n            output_graph_def = freeze_graph_def(sess, input_graph_def, \'embeddings\')\n\n        # Serialize and dump the output graph to the filesystem\n        with tf.gfile.GFile(args.output_file, \'wb\') as f:\n            f.write(output_graph_def.SerializeToString())\n        print(""%d ops in the final graph: %s"" % (len(output_graph_def.node), args.output_file))\n        \ndef freeze_graph_def(sess, input_graph_def, output_node_names):\n    for node in input_graph_def.node:\n        if node.op == \'RefSwitch\':\n            node.op = \'Switch\'\n            for index in xrange(len(node.input)):\n                if \'moving_\' in node.input[index]:\n                    node.input[index] = node.input[index] + \'/read\'\n        elif node.op == \'AssignSub\':\n            node.op = \'Sub\'\n            if \'use_locking\' in node.attr: del node.attr[\'use_locking\']\n        elif node.op == \'AssignAdd\':\n            node.op = \'Add\'\n            if \'use_locking\' in node.attr: del node.attr[\'use_locking\']\n    \n    # Get the list of important nodes\n    whitelist_names = []\n    for node in input_graph_def.node:\n        if (node.name.startswith(\'MobileFaceNet\') or node.name.startswith(\'embeddings\')):\n            whitelist_names.append(node.name)\n\n    # Replace all the variables in the graph with constants of the same values\n    output_graph_def = graph_util.convert_variables_to_constants(\n        sess, input_graph_def, output_node_names.split("",""),\n        variable_names_whitelist=whitelist_names)\n    return output_graph_def\n  \ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'model_dir\', type=str, \n        help=\'Directory containing the metagraph (.meta) file and the checkpoint (ckpt) file containing model parameters\')\n    parser.add_argument(\'output_file\', type=str, \n        help=\'Filename for the exported graphdef protobuf (.pb)\')\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
