file_path,api_count,code
srgan/model.py,44,"b'import numpy as np\nimport tensorflow as tf\n\nFLAGS = tf.app.flags.FLAGS\n\nclass Model:    \n    def __init__(self, name, features):\n        self.name = name\n        self.outputs = [features]\n    def _get_layer_str(self, layer=None):\n        if layer is None:\n            layer = self.get_num_layers()\n        return \'%s_L%03d\' % (self.name, layer+1)\n    def _get_num_inputs(self):\n        return int(self.get_output().get_shape()[-1])\n    def _glorot_initializer(self, prev_units, num_units, stddev_factor=1.0):\n        """"""Initialization in the style of Glorot 2010.\n        stddev_factor should be 1.0 for linear activations, and 2.0 for ReLUs""""""\n        stddev  = np.sqrt(stddev_factor / np.sqrt(prev_units*num_units))\n        return tf.truncated_normal([prev_units, num_units],\n                                    mean=0.0, stddev=stddev)\n    def _glorot_initializer_conv2d(self, prev_units, num_units, mapsize, stddev_factor=1.0):\n        """"""Initialization in the style of Glorot 2010.\n        stddev_factor should be 1.0 for linear activations, and 2.0 for ReLUs""""""\n        stddev  = np.sqrt(stddev_factor / (np.sqrt(prev_units*num_units)*mapsize*mapsize))\n        return tf.truncated_normal([mapsize, mapsize, prev_units, num_units],\n                                    mean=0.0, stddev=stddev)\n    def get_num_layers(self):\n        return len(self.outputs)\n    def add_batch_norm(self, scale=False):\n        """"""Adds a batch normalization layer to this model.\n        See ArXiv 1502.03167v3 for details.""""""\n        # TBD: This appears to be very flaky, often raising InvalidArgumentError internally\n        with tf.variable_scope(self._get_layer_str()):\n            out = tf.contrib.layers.batch_norm(self.get_output(), scale=scale)\n        self.outputs.append(out)\n        return self\n    def add_flatten(self):\n        """"""Transforms the output of this network to a 1D tensor""""""\n        with tf.variable_scope(self._get_layer_str()):\n            batch_size = int(self.get_output().get_shape()[0])\n            out = tf.reshape(self.get_output(), [batch_size, -1])\n        self.outputs.append(out)\n        return self\n    def add_dense(self, num_units, stddev_factor=1.0):\n        """"""Adds a dense linear layer to this model.\n        Uses Glorot 2010 initialization assuming linear activation.""""""\n        assert len(self.get_output().get_shape()) == 2, ""Previous layer must be 2-dimensional (batch, channels)""\n        with tf.variable_scope(self._get_layer_str()):\n            prev_units = self._get_num_inputs()\n            # Weight term\n            initw   = self._glorot_initializer(prev_units, num_units,\n                                               stddev_factor=stddev_factor)\n            weight  = tf.get_variable(\'weight\', initializer=initw)\n            # Bias term\n            initb   = tf.constant(0.0, shape=[num_units])\n            bias    = tf.get_variable(\'bias\', initializer=initb)\n            # Output of this layer\n            out     = tf.matmul(self.get_output(), weight) + bias\n        self.outputs.append(out)\n        return self\n    def add_sigmoid(self):\n        """"""Adds a sigmoid (0,1) activation function layer to this model.""""""\n        with tf.variable_scope(self._get_layer_str()):\n            prev_units = self._get_num_inputs()\n            out = tf.nn.sigmoid(self.get_output())\n        self.outputs.append(out)\n        return self\n    def add_softmax(self):\n        """"""Adds a softmax operation to this model""""""\n        with tf.variable_scope(self._get_layer_str()):\n            this_input = tf.square(self.get_output())\n            reduction_indices = list(range(1, len(this_input.get_shape())))\n            acc = tf.reduce_sum(this_input, reduction_indices=reduction_indices, keep_dims=True)\n            out = this_input / (acc+FLAGS.epsilon)\n            #out = tf.verify_tensor_all_finite(out, ""add_softmax failed; is sum equal to zero?"")\n        self.outputs.append(out)\n        return self\n    def add_relu(self):\n        """"""Adds a ReLU activation function to this model""""""\n        with tf.variable_scope(self._get_layer_str()):\n            out = tf.nn.relu(self.get_output())\n        self.outputs.append(out)\n        return self        \n    def add_elu(self):\n        """"""Adds a ELU activation function to this model""""""\n        with tf.variable_scope(self._get_layer_str()):\n            out = tf.nn.elu(self.get_output())\n        self.outputs.append(out)\n        return self\n    def add_lrelu(self, leak=.2):\n        """"""Adds a leaky ReLU (LReLU) activation function to this model""""""\n        with tf.variable_scope(self._get_layer_str()):\n            t1  = .5 * (1 + leak)\n            t2  = .5 * (1 - leak)\n            out = t1 * self.get_output() + \\\n                  t2 * tf.abs(self.get_output())\n        self.outputs.append(out)\n        return self\n    def add_conv2d(self, num_units, mapsize=1, stride=1, stddev_factor=1.0):\n        """"""Adds a 2D convolutional layer.""""""\n        assert len(self.get_output().get_shape()) == 4 and\n            ""Previous layer must be 4-dimensional (batch, width, height, channels)""\n        with tf.variable_scope(self._get_layer_str()):\n            prev_units = self._get_num_inputs()\n            # Weight term and convolution\n            initw  = self._glorot_initializer_conv2d(prev_units, num_units,\n                                                     mapsize,\n                                                     stddev_factor=stddev_factor)\n            weight = tf.get_variable(\'weight\', initializer=initw)\n            out    = tf.nn.conv2d(self.get_output(), weight,\n                                  strides=[1, stride, stride, 1],\n                                  padding=\'SAME\')\n            # Bias term\n            initb  = tf.constant(0.0, shape=[num_units])\n            bias   = tf.get_variable(\'bias\', initializer=initb)\n            out    = tf.nn.bias_add(out, bias)\n        self.outputs.append(out)\n        return self\n    def add_conv2d_transpose(self, num_units, mapsize=1, stride=1, stddev_factor=1.0):\n        """"""Adds a transposed 2D convolutional layer""""""\n        assert len(self.get_output().get_shape()) == 4 and \n            ""Previous layer must be 4-dimensional (batch, width, height, channels)""\n        with tf.variable_scope(self._get_layer_str()):\n            prev_units = self._get_num_inputs()\n            # Weight term and convolution\n            initw  = self._glorot_initializer_conv2d(prev_units, num_units,\n                                                     mapsize,\n                                                     stddev_factor=stddev_factor)\n            weight = tf.get_variable(\'weight\', initializer=initw)\n            weight = tf.transpose(weight, perm=[0, 1, 3, 2])\n            prev_output = self.get_output()\n            output_shape = [FLAGS.batch_size,\n                            int(prev_output.get_shape()[1]) * stride,\n                            int(prev_output.get_shape()[2]) * stride,\n                            num_units]\n            out    = tf.nn.conv2d_transpose(self.get_output(), weight,\n                                            output_shape=output_shape,\n                                            strides=[1, stride, stride, 1],\n                                            padding=\'SAME\')\n            # Bias term\n            initb  = tf.constant(0.0, shape=[num_units])\n            bias   = tf.get_variable(\'bias\', initializer=initb)\n            out    = tf.nn.bias_add(out, bias)\n        self.outputs.append(out)\n        return self\n    def add_residual_block(self, num_units, mapsize=3, num_layers=2, stddev_factor=1e-3):\n        """"""Adds a residual block as per Arxiv 1512.03385, Figure 3""""""\n        assert len(self.get_output().get_shape()) == 4 and \n            ""Previous layer must be 4-dimensional (batch, width, height, channels)""\n        # Add projection in series if needed prior to shortcut\n        if num_units != int(self.get_output().get_shape()[3]):\n            self.add_conv2d(num_units, mapsize=1, stride=1, stddev_factor=1.)\n        bypass = self.get_output()\n        # Residual block\n        for _ in range(num_layers):\n            self.add_batch_norm()\n            self.add_relu()\n            self.add_conv2d(num_units, mapsize=mapsize, stride=1, stddev_factor=stddev_factor)\n        self.add_sum(bypass)\n        return self\n    def add_bottleneck_residual_block(self, num_units, mapsize=3, stride=1, transpose=False):\n        """"""Adds a bottleneck residual block as per Arxiv 1512.03385, Figure 3""""""\n        assert len(self.get_output().get_shape()) == 4 and \n            ""Previous layer must be 4-dimensional (batch, width, height, channels)""\n        # Add projection in series if needed prior to shortcut\n        if num_units != int(self.get_output().get_shape()[3]) or stride != 1:\n            ms = 1 if stride == 1 else mapsize\n            #bypass.add_batch_norm() # TBD: Needed?\n            if transpose:\n                self.add_conv2d_transpose(num_units, mapsize=ms, stride=stride, stddev_factor=1.)\n            else:\n                self.add_conv2d(num_units, mapsize=ms, stride=stride, stddev_factor=1.)\n        bypass = self.get_output()\n        # Bottleneck residual block\n        self.add_batch_norm()\n        self.add_relu()\n        self.add_conv2d(num_units//4, mapsize=1,       stride=1,      stddev_factor=2.)\n        self.add_batch_norm()\n        self.add_relu()\n        if transpose:\n            self.add_conv2d_transpose(num_units//4,\n                                      mapsize=mapsize,\n                                      stride=1,\n                                      stddev_factor=2.)\n        else:\n            self.add_conv2d(num_units//4,\n                            mapsize=mapsize,\n                            stride=1,\n                            stddev_factor=2.)\n        self.add_batch_norm()\n        self.add_relu()\n        self.add_conv2d(num_units,    mapsize=1,       stride=1,      stddev_factor=2.)\n        self.add_sum(bypass)\n        return self\n    def add_sum(self, term):\n        """"""Adds a layer that sums the top layer with the given term""""""\n        with tf.variable_scope(self._get_layer_str()):\n            prev_shape = self.get_output().get_shape()\n            term_shape = term.get_shape()\n            #print(""%s %s"" % (prev_shape, term_shape))\n            assert prev_shape == term_shape and ""Can\'t sum terms with a different size""\n            out = tf.add(self.get_output(), term)\n        self.outputs.append(out)\n        return self\n    def add_mean(self):\n        """"""Adds a layer that averages the inputs from the previous layer""""""\n        with tf.variable_scope(self._get_layer_str()):\n            prev_shape = self.get_output().get_shape()\n            reduction_indices = list(range(len(prev_shape)))\n            assert len(reduction_indices) > 2 and ""Can\'t average a (batch, activation) tensor""\n            reduction_indices = reduction_indices[1:-1]\n            out = tf.reduce_mean(self.get_output(), reduction_indices=reduction_indices)\n        self.outputs.append(out)\n        return self\n    def add_upscale(self):\n        """"""Adds a layer that upscales the output by 2x through nearest neighbor interpolation""""""\n        prev_shape = self.get_output().get_shape()\n        size = [2 * int(s) for s in prev_shape[1:3]]\n        out  = tf.image.resize_nearest_neighbor(self.get_output(), size)\n        self.outputs.append(out)\n        return self        \n    def get_output(self):\n        """"""Returns the output from the topmost layer of the network""""""\n        return self.outputs[-1]\n    def get_variable(self, layer, name):\n        """"""Returns a variable given its layer and name.\n        The variable must already exist.""""""\n        scope      = self._get_layer_str(layer)\n        collection = tf.get_collection(tf.GraphKeys.VARIABLES, scope=scope)\n        # TBD: Ugly!\n        for var in collection:\n            if var.name[:-2] == scope+\'/\'+name:\n                return var\n        return None\n    def get_all_layer_variables(self, layer):\n        """"""Returns all variables in the given layer""""""\n        scope = self._get_layer_str(layer)\n        return tf.get_collection(tf.GraphKeys.VARIABLES, scope=scope)'"
rl/env/blackjack.py,0,"b'import gym\nfrom gym import spaces\nfrom gym.utils import seeding\n\ndef cmp(a, b):\n    return int((a > b)) - int((a < b))\n\n# 1 = Ace, 2-10 = Number cards, Jack/Queen/King = 10\ndeck = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10]\n\n\ndef draw_card(np_random):\n    return np_random.choice(deck)\n\n\ndef draw_hand(np_random):\n    return [draw_card(np_random), draw_card(np_random)]\n\n\ndef usable_ace(hand):  # Does this hand have a usable ace?\n    return 1 in hand and sum(hand) + 10 <= 21\n\n\ndef sum_hand(hand):  # Return current hand total\n    if usable_ace(hand):\n            return sum(hand) + 10\n    return sum(hand)\n\n\ndef is_bust(hand):  # Is this hand a bust?\n    return sum_hand(hand) > 21\n\n\ndef score(hand):  # What is the score of this hand (0 if bust)\n    return 0 if is_bust(hand) else sum_hand(hand)\n\n\ndef is_natural(hand):  # Is this hand a natural blackjack?\n    return sorted(hand) == [1, 10]\n\n\nclass BlackjackEnv(gym.Env):\n    """"""Simple blackjack environment\n    Blackjack is a card game where the goal is to obtain cards that sum to as\n    near as possible to 21 without going over.  They\'re playing against a fixed\n    dealer.\n    Face cards (Jack, Queen, King) have point value 10.\n    Aces can either count as 11 or 1, and it\'s called \'usable\' at 11.\n    This game is placed with an infinite deck (or with replacement).\n    The game starts with each (player and dealer) having one face up and one\n    face down card.\n    The player can request additional cards (hit=1) until they decide to stop\n    (stick=0) or exceed 21 (bust).\n    After the player sticks, the dealer reveals their facedown card, and draws\n    until their sum is 17 or greater.  If the dealer goes bust the player wins.\n    If neither player nor dealer busts, the outcome (win, lose, draw) is\n    decided by whose sum is closer to 21.  The reward for winning is +1,\n    drawing is 0, and losing is -1.\n    The observation of a 3-tuple of: the players current sum,\n    the dealer\'s one showing card (1-10 where 1 is ace),\n    and whether or not the player holds a usable ace (0 or 1).\n    This environment corresponds to the version of the blackjack problem\n    described in Example 5.1 in Reinforcement Learning: An Introduction\n    by Sutton and Barto (1998).\n    https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html\n    """"""\n    def __init__(self, natural=False):\n        self.action_space = spaces.Discrete(2)\n        self.observation_space = spaces.Tuple((\n            spaces.Discrete(32),\n            spaces.Discrete(11),\n            spaces.Discrete(2)))\n        self._seed()\n\n        # Flag to payout 1.5 on a ""natural"" blackjack win, like casino rules\n        # Ref: http://www.bicyclecards.com/how-to-play/blackjack/\n        self.natural = natural\n        # Start the first game\n        self._reset()        # Number of \n        self.nA = 2\n\n    def _seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def _step(self, action):\n        assert self.action_space.contains(action)\n        if action:  # hit: add a card to players hand and return\n            self.player.append(draw_card(self.np_random))\n            if is_bust(self.player):\n                done = True\n                reward = -1\n            else:\n                done = False\n                reward = 0\n        else:  # stick: play out the dealers hand, and score\n            done = True\n            while sum_hand(self.dealer) < 17:\n                self.dealer.append(draw_card(self.np_random))\n            reward = cmp(score(self.player), score(self.dealer))\n            if self.natural and is_natural(self.player) and reward == 1:\n                reward = 1.5\n        return self._get_obs(), reward, done, {}\n\n    def _get_obs(self):\n        return (sum_hand(self.player), self.dealer[0], usable_ace(self.player))\n\n    def _reset(self):\n        self.dealer = draw_hand(self.np_random)\n        self.player = draw_hand(self.np_random)\n\n        # Auto-draw another card if the score is less than 12\n        while sum_hand(self.player) < 12:\n            self.player.append(draw_card(self.np_random))\n\n        return self._get_obs()'"
rl/env/gridworld.py,0,"b'import numpy as np\nimport sys\nfrom gym.envs.toy_text import discrete\n\nUP     = 0\nRIGHT  = 1\nDOWN   = 2\nLEFT   = 3\n\nclass GridworldEnv(discrete.DiscreteEnv):\n    """"""\n    Grid World environment from Sutton\'s Reinforcement Learning book chapter 4.\n    You are an agent on an MxN grid and your goal is to reach the terminal\n    state at the top left or the bottom right corner.\n\n    For example, a 4x4 grid looks as follows:\n\n    T  o  o  o\n    o  x  o  o\n    o  o  o  o\n    o  o  o  T\n\n    x is your position and T are the two terminal states.\n\n    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n    Actions going off the edge leave you in your current state.\n    You receive a reward of -1 at each step until you reach a terminal state.\n    """"""\n\n    metadata = {\'render.modes\': [\'human\', \'ansi\']}\n\n    def __init__(self, shape=[4,4]):\n        if not isinstance(shape, (list, tuple)) or not len(shape) == 2:\n            raise ValueError(\'shape argument must be a list/tuple of length 2\')\n\n        self.shape = shape\n\n        nS = np.prod(shape)\n        nA = 4\n\n        MAX_Y = shape[0]\n        MAX_X = shape[1]\n\n        P = {}\n        grid = np.arange(nS).reshape(shape)\n        it = np.nditer(grid, flags=[\'multi_index\'])\n\n        while not it.finished:\n            s = it.iterindex\n            y, x = it.multi_index\n\n            P[s] = {a : [] for a in range(nA)}\n\n            is_done = lambda s: s == 0 or s == (nS - 1)\n            reward = 0.0 if is_done(s) else -1.0\n\n            # We\'re stuck in a terminal state\n            if is_done(s):\n                P[s][UP] = [(1.0, s, reward, True)]\n                P[s][RIGHT] = [(1.0, s, reward, True)]\n                P[s][DOWN] = [(1.0, s, reward, True)]\n                P[s][LEFT] = [(1.0, s, reward, True)]\n            # Not a terminal state\n            else:\n                ns_up = s if y == 0 else s - MAX_X\n                ns_right = s if x == (MAX_X - 1) else s + 1\n                ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n                ns_left = s if x == 0 else s - 1\n                P[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n                P[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n                P[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n                P[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n\n            it.iternext()\n\n        # Initial state distribution is uniform\n        isd = np.ones(nS) / nS\n\n        # We expose the model of the environment for educational purposes\n        # This should not be used in any model-free learning algorithm\n        self.P = P\n\n        super(GridworldEnv, self).__init__(nS, nA, P, isd)\n\n    def _render(self, mode=\'human\', close=False):\n        if close:\n            return\n\n        outfile = StringIO() if mode == \'ansi\' else sys.stdout\n\n        grid = np.arange(self.nS).reshape(self.shape)\n        it = np.nditer(grid, flags=[\'multi_index\'])\n        while not it.finished:\n            s = it.iterindex\n            y, x = it.multi_index\n\n            if self.s == s:\n                output = "" x ""\n            elif s == 0 or s == self.nS - 1:\n                output = "" T ""\n            else:\n                output = "" o ""\n\n            if x == 0:\n                output = output.lstrip() \n            if x == self.shape[1] - 1:\n                output = output.rstrip()\n\n            outfile.write(output)\n\n            if x == self.shape[1] - 1:\n                outfile.write(""\\n"")\n\n            it.iternext()'"
