file_path,api_count,code
cnn.py,95,"b""import os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom data_frame import DataFrame\nfrom tf_base_model import TFBaseModel\nfrom tf_utils import (\n    time_distributed_dense_layer, temporal_convolution_layer,\n    sequence_mean, sequence_smape, shape\n)\n\n\nclass DataReader(object):\n\n    def __init__(self, data_dir):\n        data_cols = [\n            'data',\n            'is_nan',\n            'page_id',\n            'project',\n            'access',\n            'agent',\n            'test_data',\n            'test_is_nan'\n        ]\n        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i))) for i in data_cols]\n\n        self.test_df = DataFrame(columns=data_cols, data=data)\n        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.95)\n\n        print 'train size', len(self.train_df)\n        print 'val size', len(self.val_df)\n        print 'test size', len(self.test_df)\n\n    def train_batch_generator(self, batch_size):\n        return self.batch_generator(\n            batch_size=batch_size,\n            df=self.train_df,\n            shuffle=True,\n            num_epochs=10000,\n            is_test=False\n        )\n\n    def val_batch_generator(self, batch_size):\n        return self.batch_generator(\n            batch_size=batch_size,\n            df=self.val_df,\n            shuffle=True,\n            num_epochs=10000,\n            is_test=False\n        )\n\n    def test_batch_generator(self, batch_size):\n        return self.batch_generator(\n            batch_size=batch_size,\n            df=self.test_df,\n            shuffle=True,\n            num_epochs=1,\n            is_test=True\n        )\n\n    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n        batch_gen = df.batch_generator(\n            batch_size=batch_size,\n            shuffle=shuffle,\n            num_epochs=num_epochs,\n            allow_smaller_final_batch=is_test\n        )\n        data_col = 'test_data' if is_test else 'data'\n        is_nan_col = 'test_is_nan' if is_test else 'is_nan'\n        for batch in batch_gen:\n            num_decode_steps = 64\n            full_seq_len = batch[data_col].shape[1]\n            max_encode_length = full_seq_len - num_decode_steps if not is_test else full_seq_len\n\n            x_encode = np.zeros([len(batch), max_encode_length])\n            y_decode = np.zeros([len(batch), num_decode_steps])\n            is_nan_encode = np.zeros([len(batch), max_encode_length])\n            is_nan_decode = np.zeros([len(batch), num_decode_steps])\n            encode_len = np.zeros([len(batch)])\n            decode_len = np.zeros([len(batch)])\n\n            for i, (seq, nan_seq) in enumerate(zip(batch[data_col], batch[is_nan_col])):\n                rand_len = np.random.randint(max_encode_length - 365 + 1, max_encode_length + 1)\n                x_encode_len = max_encode_length if is_test else rand_len\n                x_encode[i, :x_encode_len] = seq[:x_encode_len]\n                is_nan_encode[i, :x_encode_len] = nan_seq[:x_encode_len]\n                encode_len[i] = x_encode_len\n                decode_len[i] = num_decode_steps\n                if not is_test:\n                    y_decode[i, :] = seq[x_encode_len: x_encode_len + num_decode_steps]\n                    is_nan_decode[i, :] = nan_seq[x_encode_len: x_encode_len + num_decode_steps]\n\n            batch['x_encode'] = x_encode\n            batch['encode_len'] = encode_len\n            batch['y_decode'] = y_decode\n            batch['decode_len'] = decode_len\n            batch['is_nan_encode'] = is_nan_encode\n            batch['is_nan_decode'] = is_nan_decode\n\n            yield batch\n\n\nclass cnn(TFBaseModel):\n\n    def __init__(\n        self,\n        residual_channels=32,\n        skip_channels=32,\n        dilations=[2**i for i in range(8)]*3,\n        filter_widths=[2 for i in range(8)]*3,\n        num_decode_steps=64,\n        **kwargs\n    ):\n        self.residual_channels = residual_channels\n        self.skip_channels = skip_channels\n        self.dilations = dilations\n        self.filter_widths = filter_widths\n        self.num_decode_steps = num_decode_steps\n        super(cnn, self).__init__(**kwargs)\n\n    def transform(self, x):\n        return tf.log(x + 1) - tf.expand_dims(self.log_x_encode_mean, 1)\n\n    def inverse_transform(self, x):\n        return tf.exp(x + tf.expand_dims(self.log_x_encode_mean, 1)) - 1\n\n    def get_input_sequences(self):\n        self.x_encode = tf.placeholder(tf.float32, [None, None])\n        self.encode_len = tf.placeholder(tf.int32, [None])\n        self.y_decode = tf.placeholder(tf.float32, [None, self.num_decode_steps])\n        self.decode_len = tf.placeholder(tf.int32, [None])\n        self.is_nan_encode = tf.placeholder(tf.float32, [None, None])\n        self.is_nan_decode = tf.placeholder(tf.float32, [None, self.num_decode_steps])\n\n        self.page_id = tf.placeholder(tf.int32, [None])\n        self.project = tf.placeholder(tf.int32, [None])\n        self.access = tf.placeholder(tf.int32, [None])\n        self.agent = tf.placeholder(tf.int32, [None])\n\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.is_training = tf.placeholder(tf.bool)\n\n        self.log_x_encode_mean = sequence_mean(tf.log(self.x_encode + 1), self.encode_len)\n        self.log_x_encode = self.transform(self.x_encode)\n        self.x = tf.expand_dims(self.log_x_encode, 2)\n\n        self.encode_features = tf.concat([\n            tf.expand_dims(self.is_nan_encode, 2),\n            tf.expand_dims(tf.cast(tf.equal(self.x_encode, 0.0), tf.float32), 2),\n            tf.tile(tf.reshape(self.log_x_encode_mean, (-1, 1, 1)), (1, tf.shape(self.x_encode)[1], 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.project, 9), 1), (1, tf.shape(self.x_encode)[1], 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.access, 3), 1), (1, tf.shape(self.x_encode)[1], 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.agent, 2), 1), (1, tf.shape(self.x_encode)[1], 1)),\n        ], axis=2)\n\n        decode_idx = tf.tile(tf.expand_dims(tf.range(self.num_decode_steps), 0), (tf.shape(self.y_decode)[0], 1))\n        self.decode_features = tf.concat([\n            tf.one_hot(decode_idx, self.num_decode_steps),\n            tf.tile(tf.reshape(self.log_x_encode_mean, (-1, 1, 1)), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.project, 9), 1), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.access, 3), 1), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.agent, 2), 1), (1, self.num_decode_steps, 1)),\n        ], axis=2)\n\n        return self.x\n\n    def encode(self, x, features):\n        x = tf.concat([x, features], axis=2)\n\n        inputs = time_distributed_dense_layer(\n            inputs=x,\n            output_units=self.residual_channels,\n            activation=tf.nn.tanh,\n            scope='x-proj-encode'\n        )\n\n        skip_outputs = []\n        conv_inputs = [inputs]\n        for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)):\n            dilated_conv = temporal_convolution_layer(\n                inputs=inputs,\n                output_units=2*self.residual_channels,\n                convolution_width=filter_width,\n                causal=True,\n                dilation_rate=[dilation],\n                scope='dilated-conv-encode-{}'.format(i)\n            )\n            conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n            dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n\n            outputs = time_distributed_dense_layer(\n                inputs=dilated_conv,\n                output_units=self.skip_channels + self.residual_channels,\n                scope='dilated-conv-proj-encode-{}'.format(i)\n            )\n            skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n\n            inputs += residuals\n            conv_inputs.append(inputs)\n            skip_outputs.append(skips)\n\n        skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n        h = time_distributed_dense_layer(skip_outputs, 128, scope='dense-encode-1', activation=tf.nn.relu)\n        y_hat = time_distributed_dense_layer(h, 1, scope='dense-encode-2')\n\n        return y_hat, conv_inputs[:-1]\n\n    def initialize_decode_params(self, x, features):\n        x = tf.concat([x, features], axis=2)\n\n        inputs = time_distributed_dense_layer(\n            inputs=x,\n            output_units=self.residual_channels,\n            activation=tf.nn.tanh,\n            scope='x-proj-decode'\n        )\n\n        skip_outputs = []\n        conv_inputs = [inputs]\n        for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)):\n            dilated_conv = temporal_convolution_layer(\n                inputs=inputs,\n                output_units=2*self.residual_channels,\n                convolution_width=filter_width,\n                causal=True,\n                dilation_rate=[dilation],\n                scope='dilated-conv-decode-{}'.format(i)\n            )\n            conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n            dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n\n            outputs = time_distributed_dense_layer(\n                inputs=dilated_conv,\n                output_units=self.skip_channels + self.residual_channels,\n                scope='dilated-conv-proj-decode-{}'.format(i)\n            )\n            skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n\n            inputs += residuals\n            conv_inputs.append(inputs)\n            skip_outputs.append(skips)\n\n        skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n        h = time_distributed_dense_layer(skip_outputs, 128, scope='dense-decode-1', activation=tf.nn.relu)\n        y_hat = time_distributed_dense_layer(h, 1, scope='dense-decode-2')\n        return y_hat\n\n    def decode(self, x, conv_inputs, features):\n        batch_size = tf.shape(x)[0]\n\n        # initialize state tensor arrays\n        state_queues = []\n        for i, (conv_input, dilation) in enumerate(zip(conv_inputs, self.dilations)):\n            batch_idx = tf.range(batch_size)\n            batch_idx = tf.tile(tf.expand_dims(batch_idx, 1), (1, dilation))\n            batch_idx = tf.reshape(batch_idx, [-1])\n\n            queue_begin_time = self.encode_len - dilation - 1\n            temporal_idx = tf.expand_dims(queue_begin_time, 1) + tf.expand_dims(tf.range(dilation), 0)\n            temporal_idx = tf.reshape(temporal_idx, [-1])\n\n            idx = tf.stack([batch_idx, temporal_idx], axis=1)\n            slices = tf.reshape(tf.gather_nd(conv_input, idx), (batch_size, dilation, shape(conv_input, 2)))\n\n            layer_ta = tf.TensorArray(dtype=tf.float32, size=dilation + self.num_decode_steps)\n            layer_ta = layer_ta.unstack(tf.transpose(slices, (1, 0, 2)))\n            state_queues.append(layer_ta)\n\n        # initialize feature tensor array\n        features_ta = tf.TensorArray(dtype=tf.float32, size=self.num_decode_steps)\n        features_ta = features_ta.unstack(tf.transpose(features, (1, 0, 2)))\n\n        # initialize output tensor array\n        emit_ta = tf.TensorArray(size=self.num_decode_steps, dtype=tf.float32)\n\n        # initialize other loop vars\n        elements_finished = 0 >= self.decode_len\n        time = tf.constant(0, dtype=tf.int32)\n\n        # get initial x input\n        current_idx = tf.stack([tf.range(tf.shape(self.encode_len)[0]), self.encode_len - 1], axis=1)\n        initial_input = tf.gather_nd(x, current_idx)\n\n        def loop_fn(time, current_input, queues):\n            current_features = features_ta.read(time)\n            current_input = tf.concat([current_input, current_features], axis=1)\n\n            with tf.variable_scope('x-proj-decode', reuse=True):\n                w_x_proj = tf.get_variable('weights')\n                b_x_proj = tf.get_variable('biases')\n                x_proj = tf.nn.tanh(tf.matmul(current_input, w_x_proj) + b_x_proj)\n\n            skip_outputs, updated_queues = [], []\n            for i, (conv_input, queue, dilation) in enumerate(zip(conv_inputs, queues, self.dilations)):\n\n                state = queue.read(time)\n                with tf.variable_scope('dilated-conv-decode-{}'.format(i), reuse=True):\n                    w_conv = tf.get_variable('weights'.format(i))\n                    b_conv = tf.get_variable('biases'.format(i))\n                    dilated_conv = tf.matmul(state, w_conv[0, :, :]) + tf.matmul(x_proj, w_conv[1, :, :]) + b_conv\n                conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=1)\n                dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n\n                with tf.variable_scope('dilated-conv-proj-decode-{}'.format(i), reuse=True):\n                    w_proj = tf.get_variable('weights'.format(i))\n                    b_proj = tf.get_variable('biases'.format(i))\n                    concat_outputs = tf.matmul(dilated_conv, w_proj) + b_proj\n                skips, residuals = tf.split(concat_outputs, [self.skip_channels, self.residual_channels], axis=1)\n\n                x_proj += residuals\n                skip_outputs.append(skips)\n                updated_queues.append(queue.write(time + dilation, x_proj))\n\n            skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=1))\n            with tf.variable_scope('dense-decode-1', reuse=True):\n                w_h = tf.get_variable('weights')\n                b_h = tf.get_variable('biases')\n                h = tf.nn.relu(tf.matmul(skip_outputs, w_h) + b_h)\n\n            with tf.variable_scope('dense-decode-2', reuse=True):\n                w_y = tf.get_variable('weights')\n                b_y = tf.get_variable('biases')\n                y_hat = tf.matmul(h, w_y) + b_y\n\n            elements_finished = (time >= self.decode_len)\n            finished = tf.reduce_all(elements_finished)\n\n            next_input = tf.cond(\n                finished,\n                lambda: tf.zeros([batch_size, 1], dtype=tf.float32),\n                lambda: y_hat\n            )\n            next_elements_finished = (time >= self.decode_len - 1)\n\n            return (next_elements_finished, next_input, updated_queues)\n\n        def condition(unused_time, elements_finished, *_):\n            return tf.logical_not(tf.reduce_all(elements_finished))\n\n        def body(time, elements_finished, emit_ta, *state_queues):\n            (next_finished, emit_output, state_queues) = loop_fn(time, initial_input, state_queues)\n\n            emit = tf.where(elements_finished, tf.zeros_like(emit_output), emit_output)\n            emit_ta = emit_ta.write(time, emit)\n\n            elements_finished = tf.logical_or(elements_finished, next_finished)\n            return [time + 1, elements_finished, emit_ta] + list(state_queues)\n\n        returned = tf.while_loop(\n            cond=condition,\n            body=body,\n            loop_vars=[time, elements_finished, emit_ta] + state_queues\n        )\n\n        outputs_ta = returned[2]\n        y_hat = tf.transpose(outputs_ta.stack(), (1, 0, 2))\n        return y_hat\n\n    def calculate_loss(self):\n        x = self.get_input_sequences()\n\n        y_hat_encode, conv_inputs = self.encode(x, features=self.encode_features)\n        self.initialize_decode_params(x, features=self.decode_features)\n        y_hat_decode = self.decode(y_hat_encode, conv_inputs, features=self.decode_features)\n        y_hat_decode = self.inverse_transform(tf.squeeze(y_hat_decode, 2))\n        y_hat_decode = tf.nn.relu(y_hat_decode)\n\n        self.labels = self.y_decode\n        self.preds = y_hat_decode\n        self.loss = sequence_smape(self.labels, self.preds, self.decode_len, self.is_nan_decode)\n\n        self.prediction_tensors = {\n            'priors': self.x_encode,\n            'labels': self.labels,\n            'preds': self.preds,\n            'page_id': self.page_id,\n        }\n\n        return self.loss\n\n\nif __name__ == '__main__':\n    base_dir = './'\n\n    dr = DataReader(data_dir=os.path.join(base_dir, 'data/processed/'))\n\n    nn = cnn(\n        reader=dr,\n        log_dir=os.path.join(base_dir, 'logs'),\n        checkpoint_dir=os.path.join(base_dir, 'checkpoints'),\n        prediction_dir=os.path.join(base_dir, 'predictions'),\n        optimizer='adam',\n        learning_rate=.001,\n        batch_size=128,\n        num_training_steps=200000,\n        early_stopping_steps=5000,\n        warm_start_init_step=0,\n        regularization_constant=0.0,\n        keep_prob=1.0,\n        enable_parameter_averaging=False,\n        num_restarts=2,\n        min_steps_to_checkpoint=500,\n        log_interval=10,\n        num_validation_batches=1,\n        grad_clip=20,\n        residual_channels=32,\n        skip_channels=32,\n        dilations=[2**i for i in range(8)]*3,\n        filter_widths=[2 for i in range(8)]*3,\n        num_decode_steps=64,\n    )\n    nn.fit()\n    nn.restore()\n    nn.predict()\n"""
data_frame.py,0,"b'import copy\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\nclass DataFrame(object):\n\n    """"""Minimal pd.DataFrame analog for handling n-dimensional numpy matrices with additional\n    support for shuffling, batching, and train/test splitting.\n\n    Args:\n        columns: List of names corresponding to the matrices in data.\n        data: List of n-dimensional data matrices ordered in correspondence with columns.\n            All matrices must have the same leading dimension.  Data can also be fed a list of\n            instances of np.memmap, in which case RAM usage can be limited to the size of a\n            single batch.\n    """"""\n\n    def __init__(self, columns, data):\n        assert len(columns) == len(data), \'columns length does not match data length\'\n\n        lengths = [mat.shape[0] for mat in data]\n        assert len(set(lengths)) == 1, \'all matrices in data must have same first dimension\'\n\n        self.length = lengths[0]\n        self.columns = columns\n        self.data = data\n        self.dict = dict(zip(self.columns, self.data))\n        self.idx = np.arange(self.length)\n\n    def shapes(self):\n        return pd.Series(dict(zip(self.columns, [mat.shape for mat in self.data])))\n\n    def dtypes(self):\n        return pd.Series(dict(zip(self.columns, [mat.dtype for mat in self.data])))\n\n    def shuffle(self):\n        np.random.shuffle(self.idx)\n\n    def train_test_split(self, train_size, random_state=np.random.randint(10000)):\n        train_idx, test_idx = train_test_split(self.idx, train_size=train_size, random_state=random_state)\n        train_df = DataFrame(copy.copy(self.columns), [mat[train_idx] for mat in self.data])\n        test_df = DataFrame(copy.copy(self.columns), [mat[test_idx] for mat in self.data])\n        return train_df, test_df\n\n    def batch_generator(self, batch_size, shuffle=True, num_epochs=10000, allow_smaller_final_batch=False):\n        epoch_num = 0\n        while epoch_num < num_epochs:\n            if shuffle:\n                self.shuffle()\n\n            for i in range(0, self.length + 1, batch_size):\n                batch_idx = self.idx[i: i + batch_size]\n                if not allow_smaller_final_batch and len(batch_idx) != batch_size:\n                    break\n                yield DataFrame(columns=copy.copy(self.columns), data=[mat[batch_idx].copy() for mat in self.data])\n\n            epoch_num += 1\n\n    def iterrows(self):\n        for i in self.idx:\n            yield self[i]\n\n    def mask(self, mask):\n        return DataFrame(copy.copy(self.columns), [mat[mask] for mat in self.data])\n\n    def __iter__(self):\n        return self.dict.items().__iter__()\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, key):\n        if isinstance(key, str):\n            return self.dict[key]\n\n        elif isinstance(key, int):\n            return pd.Series(dict(zip(self.columns, [mat[self.idx[key]] for mat in self.data])))\n\n    def __setitem__(self, key, value):\n        assert value.shape[0] == len(self), \'matrix first dimension does not match\'\n        if key not in self.columns:\n            self.columns.append(key)\n            self.data.append(value)\n        self.dict[key] = value\n'"
prepare_data.py,0,"b""from __future__ import unicode_literals\n\nimport os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n\ndef parse_page(x):\n    x = x.split('_')\n    return ' '.join(x[:-3]), x[-3], x[-2], x[-1]\n\n\ndef nan_fill_forward(x):\n    for i in range(x.shape[0]):\n        fill_val = None\n        for j in range(x.shape[1] - 3, x.shape[1]):\n            if np.isnan(x[i, j]) and fill_val is not None:\n                x[i, j] = fill_val\n            else:\n                fill_val = x[i, j]\n    return x\n\n\ndf = pd.read_csv('data/raw/train_final.csv', encoding='utf-8')\ndate_cols = [i for i in df.columns if i != 'Page']\n\ndf['name'], df['project'], df['access'], df['agent'] = zip(*df['Page'].apply(parse_page))\n\nle = LabelEncoder()\ndf['project'] = le.fit_transform(df['project'])\ndf['access'] = le.fit_transform(df['access'])\ndf['agent'] = le.fit_transform(df['agent'])\ndf['page_id'] = le.fit_transform(df['Page'])\n\nif not os.path.isdir('data/processed'):\n    os.makedirs('data/processed')\n\ndf[['page_id', 'Page']].to_csv('data/processed/page_ids.csv', encoding='utf-8', index=False)\n\ndata = df[date_cols].values\nnp.save('data/processed/data.npy', np.nan_to_num(data))\nnp.save('data/processed/is_nan.npy', np.isnan(data).astype(int))\nnp.save('data/processed/project.npy', df['project'].values)\nnp.save('data/processed/access.npy', df['access'].values)\nnp.save('data/processed/agent.npy', df['agent'].values)\nnp.save('data/processed/page_id.npy', df['page_id'].values)\n\ntest_data = nan_fill_forward(df[date_cols].values)\nnp.save('data/processed/test_data.npy', np.nan_to_num(test_data))\nnp.save('data/processed/test_is_nan.npy', np.isnan(test_data).astype(int))\n"""
submit.py,0,"b'import os\n\nimport numpy as np\nimport pandas as pd\n\n\npreds_mat = np.load(os.path.join(\'predictions\', \'preds.npy\'))\nlabels_mat = np.load(os.path.join(\'predictions\', \'labels.npy\'))\npriors_mat = np.load(os.path.join(\'predictions\', \'priors.npy\'))\npages_mat = np.load(os.path.join(\'predictions\', \'page_id.npy\'))\n\ndf = pd.read_csv(\'data/raw/train_final.csv\', encoding=\'utf-8\')\ndate_cols = [i for i in df.columns if i != \'Page\']\ndatetimes = pd.to_datetime(date_cols, format=""%Y/%m/%d"")\nnext_date_cols = pd.date_range(start=datetimes[-1], periods=64, closed=\'right\')\npred_df = pd.DataFrame(preds_mat, columns=next_date_cols)\npred_df[\'page_id\'] = pages_mat\n\npage_id_df = pd.read_csv(\'data/processed/page_ids.csv\', encoding=\'utf-8\')\npred_df = pred_df.merge(page_id_df, how=\'left\', on=\'page_id\')\n\nsubmit_cols = list(next_date_cols[2:]) + [\'Page\']\npred_df = pred_df[submit_cols]\npred_df = pd.melt(pred_df, id_vars=\'Page\', var_name=\'date\', value_name=\'Visits\')\n\nkeys = pd.read_csv(\'data/raw/key_2.csv\', encoding=\'utf-8\')\nkeys[\'date\'] = keys.Page.apply(lambda a: a[-10:])\nkeys[\'Page\'] = keys.Page.apply(lambda a: a[:-11])\nkeys[\'date\'] = keys[\'date\'].astype(\'datetime64[ns]\')\n\npred_df = pred_df.merge(keys, how=\'left\', on=[\'Page\', \'date\'])\npred_df[\'Visits\'] = pred_df[\'Visits\'].map(np.round).astype(int)\npred_df = pred_df[[\'Id\', \'Visits\']].sort_values(by=\'Id\')\npred_df.to_csv(\'sub.csv\', encoding=\'utf-8\', index=False)\n'"
tf_base_model.py,21,"b'from collections import deque\nfrom datetime import datetime\nimport logging\nimport os\nimport pprint as pp\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_utils import shape\n\n\nclass TFBaseModel(object):\n\n    """"""Interface containing some boilerplate code for training tensorflow models.\n\n    Subclassing models must implement self.calculate_loss(), which returns a tensor for the batch loss.\n    Code for the training loop, parameter updates, checkpointing, and inference are implemented here and\n    subclasses are mainly responsible for building the computational graph beginning with the placeholders\n    and ending with the loss tensor.\n\n    Args:\n        reader: Class with attributes train_batch_generator, val_batch_generator, and test_batch_generator\n            that yield dictionaries mapping tf.placeholder names (as strings) to batch data (numpy arrays).\n        batch_size: Minibatch size.\n        learning_rate: Learning rate.\n        optimizer: \'rms\' for RMSProp, \'adam\' for Adam, \'sgd\' for SGD\n        grad_clip: Clip gradients elementwise to have norm at most equal to grad_clip.\n        regularization_constant:  Regularization constant applied to all trainable parameters.\n        keep_prob: 1 - p, where p is the dropout probability\n        early_stopping_steps:  Number of steps to continue training after validation loss has\n            stopped decreasing.\n        warm_start_init_step:  If nonzero, model will resume training a restored model beginning\n            at warm_start_init_step.\n        num_restarts:  After validation loss plateaus, the best checkpoint will be restored and the\n            learning rate will be halved.  This process will repeat num_restarts times.\n        enable_parameter_averaging:  If true, model saves exponential weighted averages of parameters\n            to separate checkpoint file.\n        min_steps_to_checkpoint:  Model only saves after min_steps_to_checkpoint training steps\n            have passed.\n        log_interval:  Train and validation accuracies are logged every log_interval training steps.\n        loss_averaging_window:  Train/validation losses are averaged over the last loss_averaging_window\n            training steps.\n        num_validation_batches:  Number of batches to be used in validation evaluation at each step.\n        log_dir: Directory where logs are written.\n        checkpoint_dir: Directory where checkpoints are saved.\n        prediction_dir: Directory where predictions/outputs are saved.\n    """"""\n\n    def __init__(\n        self,\n        reader,\n        batch_size=128,\n        num_training_steps=20000,\n        learning_rate=.01,\n        optimizer=\'adam\',\n        grad_clip=5,\n        regularization_constant=0.0,\n        keep_prob=1.0,\n        early_stopping_steps=3000,\n        warm_start_init_step=0,\n        num_restarts=None,\n        enable_parameter_averaging=False,\n        min_steps_to_checkpoint=100,\n        log_interval=20,\n        loss_averaging_window=100,\n        num_validation_batches=1,\n        log_dir=\'logs\',\n        checkpoint_dir=\'checkpoints\',\n        prediction_dir=\'predictions\'\n    ):\n\n        self.reader = reader\n        self.batch_size = batch_size\n        self.num_training_steps = num_training_steps\n        self.learning_rate = learning_rate\n        self.optimizer = optimizer\n        self.grad_clip = grad_clip\n        self.regularization_constant = regularization_constant\n        self.warm_start_init_step = warm_start_init_step\n        self.early_stopping_steps = early_stopping_steps\n        self.keep_prob_scalar = keep_prob\n        self.enable_parameter_averaging = enable_parameter_averaging\n        self.num_restarts = num_restarts\n        self.min_steps_to_checkpoint = min_steps_to_checkpoint\n        self.log_interval = log_interval\n        self.num_validation_batches = num_validation_batches\n        self.loss_averaging_window = loss_averaging_window\n\n        self.log_dir = log_dir\n        self.prediction_dir = prediction_dir\n        self.checkpoint_dir = checkpoint_dir\n        if self.enable_parameter_averaging:\n            self.checkpoint_dir_averaged = checkpoint_dir + \'_avg\'\n\n        self.init_logging(self.log_dir)\n        logging.info(\'\\nnew run with parameters:\\n{}\'.format(pp.pformat(self.__dict__)))\n\n        self.graph = self.build_graph()\n        self.session = tf.Session(graph=self.graph)\n        print \'built graph\'\n\n    def calculate_loss(self):\n        raise NotImplementedError(\'subclass must implement this\')\n\n    def fit(self):\n        with self.session.as_default():\n\n            if self.warm_start_init_step:\n                self.restore(self.warm_start_init_step)\n                step = self.warm_start_init_step\n            else:\n                self.session.run(self.init)\n                step = 0\n\n            train_generator = self.reader.train_batch_generator(self.batch_size)\n            val_generator = self.reader.val_batch_generator(self.num_validation_batches*self.batch_size)\n\n            train_loss_history = deque(maxlen=self.loss_averaging_window)\n            val_loss_history = deque(maxlen=self.loss_averaging_window)\n\n            best_validation_loss, best_validation_tstep = float(\'inf\'), 0\n            restarts = 0\n\n            while step < self.num_training_steps:\n\n                # validation evaluation\n                val_batch_df = val_generator.next()\n                val_feed_dict = {\n                    getattr(self, placeholder_name, None): data\n                    for placeholder_name, data in val_batch_df if hasattr(self, placeholder_name)\n                }\n\n                val_feed_dict.update({self.learning_rate_var: self.learning_rate})\n                if hasattr(self, \'keep_prob\'):\n                    val_feed_dict.update({self.keep_prob: 1.0})\n                if hasattr(self, \'is_training\'):\n                    val_feed_dict.update({self.is_training: False})\n\n                [val_loss] = self.session.run(\n                    fetches=[self.loss],\n                    feed_dict=val_feed_dict\n                )\n                val_loss_history.append(val_loss)\n\n                if hasattr(self, \'monitor_tensors\'):\n                    for name, tensor in self.monitor_tensors.items():\n                        [np_val] = self.session.run([tensor], feed_dict=val_feed_dict)\n                        print name\n                        print \'min\', np_val.min()\n                        print \'max\', np_val.max()\n                        print \'mean\', np_val.mean()\n                        print \'std\', np_val.std()\n                        print \'nans\', np.isnan(np_val).sum()\n                        print\n\n                # train step\n                train_batch_df = train_generator.next()\n                train_feed_dict = {\n                    getattr(self, placeholder_name, None): data\n                    for placeholder_name, data in train_batch_df if hasattr(self, placeholder_name)\n                }\n\n                train_feed_dict.update({self.learning_rate_var: self.learning_rate})\n                if hasattr(self, \'keep_prob\'):\n                    train_feed_dict.update({self.keep_prob: self.keep_prob_scalar})\n                if hasattr(self, \'is_training\'):\n                    train_feed_dict.update({self.is_training: True})\n\n                train_loss, _ = self.session.run(\n                    fetches=[self.loss, self.step],\n                    feed_dict=train_feed_dict\n                )\n                train_loss_history.append(train_loss)\n\n                if step % self.log_interval == 0:\n                    avg_train_loss = sum(train_loss_history) / len(train_loss_history)\n                    avg_val_loss = sum(val_loss_history) / len(val_loss_history)\n                    metric_log = (\n                        ""[[step {:>8}]]     ""\n                        ""[[train]]     loss: {:<12}     ""\n                        ""[[val]]     loss: {:<12}     ""\n                    ).format(step, round(avg_train_loss, 8), round(avg_val_loss, 8))\n                    logging.info(metric_log)\n\n                    if avg_val_loss < best_validation_loss:\n                        best_validation_loss = avg_val_loss\n                        best_validation_tstep = step\n                        if step > self.min_steps_to_checkpoint:\n                            self.save(step)\n                            if self.enable_parameter_averaging:\n                                self.save(step, averaged=True)\n\n                    if step - best_validation_tstep > self.early_stopping_steps:\n\n                        if self.num_restarts is None or restarts >= self.num_restarts:\n                            logging.info(\'best validation loss of {} at training step {}\'.format(\n                                best_validation_loss, best_validation_tstep))\n                            logging.info(\'early stopping - ending training.\')\n                            return\n\n                        if restarts < self.num_restarts:\n                            self.restore(best_validation_tstep)\n                            logging.info(\'halving learning rate\')\n                            self.learning_rate /= 2.0\n                            step = best_validation_tstep\n                            restarts += 1\n\n                step += 1\n\n            if step <= self.min_steps_to_checkpoint:\n                best_validation_tstep = step\n                self.save(step)\n                if self.enable_parameter_averaging:\n                    self.save(step, averaged=True)\n\n            logging.info(\'num_training_steps reached - ending training\')\n\n    def predict(self, chunk_size=512):\n        if not os.path.isdir(self.prediction_dir):\n            os.makedirs(self.prediction_dir)\n\n        if hasattr(self, \'prediction_tensors\'):\n            prediction_dict = {tensor_name: [] for tensor_name in self.prediction_tensors}\n\n            test_generator = self.reader.test_batch_generator(chunk_size)\n            for i, test_batch_df in enumerate(test_generator):\n                test_feed_dict = {\n                    getattr(self, placeholder_name, None): data\n                    for placeholder_name, data in test_batch_df if hasattr(self, placeholder_name)\n                }\n                if hasattr(self, \'keep_prob\'):\n                    test_feed_dict.update({self.keep_prob: 1.0})\n                if hasattr(self, \'is_training\'):\n                    test_feed_dict.update({self.is_training: False})\n\n                tensor_names, tf_tensors = zip(*self.prediction_tensors.items())\n                np_tensors = self.session.run(\n                    fetches=tf_tensors,\n                    feed_dict=test_feed_dict\n                )\n                for tensor_name, tensor in zip(tensor_names, np_tensors):\n                    prediction_dict[tensor_name].append(tensor)\n\n            for tensor_name, tensor in prediction_dict.items():\n                np_tensor = np.concatenate(tensor, 0)\n                save_file = os.path.join(self.prediction_dir, \'{}.npy\'.format(tensor_name))\n                logging.info(\'saving {} with shape {} to {}\'.format(tensor_name, np_tensor.shape, save_file))\n                np.save(save_file, np_tensor)\n\n        if hasattr(self, \'parameter_tensors\'):\n            for tensor_name, tensor in self.parameter_tensors.items():\n                np_tensor = tensor.eval(self.session)\n\n                save_file = os.path.join(self.prediction_dir, \'{}.npy\'.format(tensor_name))\n                logging.info(\'saving {} with shape {} to {}\'.format(tensor_name, np_tensor.shape, save_file))\n                np.save(save_file, np_tensor)\n\n    def save(self, step, averaged=False):\n        saver = self.saver_averaged if averaged else self.saver\n        checkpoint_dir = self.checkpoint_dir_averaged if averaged else self.checkpoint_dir\n        if not os.path.isdir(checkpoint_dir):\n            logging.info(\'creating checkpoint directory {}\'.format(checkpoint_dir))\n            os.mkdir(checkpoint_dir)\n\n        model_path = os.path.join(checkpoint_dir, \'model\')\n        logging.info(\'saving model to {}\'.format(model_path))\n        saver.save(self.session, model_path, global_step=step)\n\n    def restore(self, step=None, averaged=False):\n        saver = self.saver_averaged if averaged else self.saver\n        checkpoint_dir = self.checkpoint_dir_averaged if averaged else self.checkpoint_dir\n        if not step:\n            model_path = tf.train.latest_checkpoint(checkpoint_dir)\n            logging.info(\'restoring model parameters from {}\'.format(model_path))\n            saver.restore(self.session, model_path)\n        else:\n            model_path = os.path.join(\n                checkpoint_dir, \'model{}-{}\'.format(\'_avg\' if averaged else \'\', step)\n            )\n            logging.info(\'restoring model from {}\'.format(model_path))\n            saver.restore(self.session, model_path)\n\n    def init_logging(self, log_dir):\n        if not os.path.isdir(log_dir):\n            os.makedirs(log_dir)\n\n        date_str = datetime.now().strftime(\'%Y-%m-%d_%H-%M\')\n        log_file = \'log_{}.txt\'.format(date_str)\n\n        reload(logging)  # bad\n        logging.basicConfig(\n            filename=os.path.join(log_dir, log_file),\n            level=logging.INFO,\n            format=\'[[%(asctime)s]] %(message)s\',\n            datefmt=\'%m/%d/%Y %I:%M:%S %p\'\n        )\n        logging.getLogger().addHandler(logging.StreamHandler())\n\n    def update_parameters(self, loss):\n\n        if self.regularization_constant != 0:\n            l2_norm = tf.reduce_sum([tf.sqrt(tf.reduce_sum(tf.square(param))) for param in tf.trainable_variables()])\n            loss = loss + self.regularization_constant*l2_norm\n\n        optimizer = self.get_optimizer(self.learning_rate_var)\n        grads = optimizer.compute_gradients(loss)\n        clipped = [(tf.clip_by_value(g, -self.grad_clip, self.grad_clip), v_) for g, v_ in grads]\n\n        step = optimizer.apply_gradients(clipped, global_step=self.global_step)\n\n        if self.enable_parameter_averaging:\n            maintain_averages_op = self.ema.apply(tf.trainable_variables())\n            with tf.control_dependencies([step]):\n                self.step = tf.group(maintain_averages_op)\n        else:\n            self.step = step\n\n        logging.info(\'all parameters:\')\n        logging.info(pp.pformat([(var.name, shape(var)) for var in tf.global_variables()]))\n\n        logging.info(\'trainable parameters:\')\n        logging.info(pp.pformat([(var.name, shape(var)) for var in tf.trainable_variables()]))\n\n        logging.info(\'trainable parameter count:\')\n        logging.info(str(np.sum(np.prod(shape(var)) for var in tf.trainable_variables())))\n\n    def get_optimizer(self, learning_rate):\n        if self.optimizer == \'adam\':\n            return tf.train.AdamOptimizer(learning_rate)\n        elif self.optimizer == \'gd\':\n            return tf.train.GradientDescentOptimizer(learning_rate)\n        elif self.optimizer == \'rms\':\n            return tf.train.RMSPropOptimizer(learning_rate, decay=0.95, momentum=0.9)\n        else:\n            assert False, \'optimizer must be adam, gd, or rms\'\n\n    def build_graph(self):\n        with tf.Graph().as_default() as graph:\n            self.ema = tf.train.ExponentialMovingAverage(decay=0.995)\n            self.global_step = tf.Variable(0, trainable=False)\n            self.learning_rate_var = tf.Variable(0.0, trainable=False)\n\n            self.loss = self.calculate_loss()\n            self.update_parameters(self.loss)\n\n            self.saver = tf.train.Saver(max_to_keep=1)\n            if self.enable_parameter_averaging:\n                self.saver_averaged = tf.train.Saver(self.ema.variables_to_restore(), max_to_keep=1)\n\n            self.init = tf.global_variables_initializer()\n\n            return graph\n'"
tf_utils.py,26,"b'import tensorflow as tf\n\n\ndef temporal_convolution_layer(inputs, output_units, convolution_width, causal=False, dilation_rate=[1], bias=True,\n                               activation=None, dropout=None, scope=\'temporal-convolution-layer\', reuse=False):\n    """"""\n    Convolution over the temporal axis of sequence data.\n\n    Args:\n        inputs: Tensor of shape [batch size, max sequence length, input_units].\n        output_units: Output channels for convolution.\n        convolution_width: Number of timesteps to use in convolution.\n        causal: Output at timestep t is a function of inputs at or before timestep t.\n        dilation_rate:  Dilation rate along temporal axis.\n\n    Returns:\n        Tensor of shape [batch size, max sequence length, output_units].\n\n    """"""\n    with tf.variable_scope(scope, reuse=reuse):\n        if causal:\n            shift = (convolution_width / 2) + (int(dilation_rate[0] - 1) / 2)\n            pad = tf.zeros([tf.shape(inputs)[0], shift, inputs.shape.as_list()[2]])\n            inputs = tf.concat([pad, inputs], axis=1)\n\n        W = tf.get_variable(\n            name=\'weights\',\n            initializer=tf.random_normal_initializer(\n                mean=0,\n                stddev=1.0 / tf.sqrt(float(convolution_width)*float(shape(inputs, 2)))\n            ),\n            shape=[convolution_width, shape(inputs, 2), output_units]\n        )\n\n        z = tf.nn.convolution(inputs, W, padding=\'SAME\', dilation_rate=dilation_rate)\n        if bias:\n            b = tf.get_variable(\n                name=\'biases\',\n                initializer=tf.constant_initializer(),\n                shape=[output_units]\n            )\n            z = z + b\n        z = activation(z) if activation else z\n        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n        z = z[:, :-shift, :] if causal else z\n        return z\n\n\ndef time_distributed_dense_layer(inputs, output_units, bias=True, activation=None, batch_norm=None,\n                                 dropout=None, scope=\'time-distributed-dense-layer\', reuse=False):\n    """"""\n    Applies a shared dense layer to each timestep of a tensor of shape [batch_size, max_seq_len, input_units]\n    to produce a tensor of shape [batch_size, max_seq_len, output_units].\n\n    Args:\n        inputs: Tensor of shape [batch size, max sequence length, ...].\n        output_units: Number of output units.\n        activation: activation function.\n        dropout: dropout keep prob.\n\n    Returns:\n        Tensor of shape [batch size, max sequence length, output_units].\n\n    """"""\n    with tf.variable_scope(scope, reuse=reuse):\n        W = tf.get_variable(\n            name=\'weights\',\n            initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0 / float(shape(inputs, -1))),\n            shape=[shape(inputs, -1), output_units]\n        )\n        z = tf.einsum(\'ijk,kl->ijl\', inputs, W)\n        if bias:\n            b = tf.get_variable(\n                name=\'biases\',\n                initializer=tf.constant_initializer(),\n                shape=[output_units]\n            )\n            z = z + b\n\n        if batch_norm is not None:\n            z = tf.layers.batch_normalization(z, training=batch_norm, reuse=reuse)\n\n        z = activation(z) if activation else z\n        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n        return z\n\n\ndef shape(tensor, dim=None):\n    """"""Get tensor shape/dimension as list/int""""""\n    if dim is None:\n        return tensor.shape.as_list()\n    else:\n        return tensor.shape.as_list()[dim]\n\n\ndef sequence_smape(y, y_hat, sequence_lengths, is_nan):\n    max_sequence_length = tf.shape(y)[1]\n    y = tf.cast(y, tf.float32)\n    smape = 2*(tf.abs(y_hat - y) / (tf.abs(y) + tf.abs(y_hat)))\n\n    # ignore discontinuity\n    zero_loss = 2.0*tf.ones_like(smape)\n    nonzero_loss = smape\n    smape = tf.where(tf.logical_or(tf.equal(y, 0.0), tf.equal(y_hat, 0.0)), zero_loss, nonzero_loss)\n\n    sequence_mask = tf.cast(tf.sequence_mask(sequence_lengths, maxlen=max_sequence_length), tf.float32)\n    sequence_mask = sequence_mask*(1 - is_nan)\n    avg_smape = tf.reduce_sum(smape*sequence_mask) / tf.reduce_sum(sequence_mask)\n    return avg_smape\n\n\ndef sequence_mean(x, lengths):\n    return tf.reduce_sum(x, axis=1) / tf.cast(lengths, tf.float32)\n'"
cf/cnn.py,148,"b""import os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom data_frame import DataFrame\nfrom tf_base_model import TFBaseModel\nfrom tf_utils import (\n    time_distributed_dense_layer, temporal_convolution_layer,\n    sequence_mean, sequence_rmse, shape, bidirectional_lstm_layer\n)\n\n\nclass DataReader(object):\n\n    def __init__(self, data_dir):\n        data_cols = [\n            'x_raw',\n            'onpromotion',\n            'id',\n            'x',\n            'store_nbr',\n            'item_nbr',\n            'city',\n            'state',\n            'type',\n            'cluster',\n            'family',\n            'class',\n            'perishable',\n            'is_discrete',\n            'start_date',\n            'x_lags',\n            'xy_lags',\n            'ts',\n        ]\n        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n\n        self.test_df = DataFrame(columns=data_cols, data=data)\n        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.95)\n\n        self.num_city = self.test_df['city'].max() + 1\n        self.num_state = self.test_df['state'].max() + 1\n        self.num_type = self.test_df['type'].max() + 1\n        self.num_cluster = self.test_df['cluster'].max() + 1\n        self.num_family = self.test_df['family'].max() + 1\n        self.num_item_class = self.test_df['class'].max() + 1\n        self.num_perishable = self.test_df['perishable'].max() + 1\n        self.num_store_nbr = self.test_df['store_nbr'].max() + 1\n        self.num_item_nbr = self.test_df['item_nbr'].max() + 1\n\n        print 'train size', len(self.train_df)\n        print 'val size', len(self.val_df)\n        print 'test size', len(self.test_df)\n\n    def train_batch_generator(self, batch_size):\n        return self.batch_generator(\n            batch_size=batch_size,\n            df=self.train_df,\n            shuffle=True,\n            num_epochs=10000,\n            mode='train'\n        )\n\n    def val_batch_generator(self, batch_size):\n        return self.batch_generator(\n            batch_size=batch_size,\n            df=self.val_df,\n            shuffle=True,\n            num_epochs=10000,\n            mode='val'\n        )\n\n    def test_batch_generator(self, batch_size):\n        return self.batch_generator(\n            batch_size=batch_size,\n            df=self.test_df,\n            shuffle=True,\n            num_epochs=1,\n            mode='test'\n        )\n\n    def batch_generator(self, batch_size, df, mode, shuffle=True, num_epochs=10000):\n        batch_gen = df.batch_generator(\n            batch_size=batch_size,\n            shuffle=shuffle,\n            num_epochs=num_epochs,\n            allow_smaller_final_batch=(mode == 'test')\n        )\n        for batch in batch_gen:\n            num_decode_steps = 16\n            full_seq_len = batch['x'].shape[1] - num_decode_steps\n            max_encode_length = full_seq_len\n\n            x = np.zeros([len(batch), max_encode_length])\n            y = np.zeros([len(batch), num_decode_steps])\n            x_raw = np.zeros([len(batch), max_encode_length])\n            x_lags = np.zeros([len(batch), max_encode_length, batch['x_lags'].shape[2] + batch['xy_lags'].shape[2]])\n            y_lags = np.zeros([len(batch), num_decode_steps, batch['xy_lags'].shape[2]])\n            x_op = np.zeros([len(batch), max_encode_length])\n            y_op = np.zeros([len(batch), num_decode_steps])\n            x_len = np.zeros([len(batch)])\n            y_len = np.zeros([len(batch)])\n            x_idx = np.zeros([len(batch), max_encode_length])\n            y_idx = np.zeros([len(batch), num_decode_steps])\n            y_id = np.zeros([len(batch), num_decode_steps])\n            x_ts = np.zeros([len(batch), max_encode_length, batch['ts'].shape[2]])\n            weights = np.zeros([len(batch)])\n            weights[batch['perishable'] == 1] = 1.25\n            weights[batch['perishable'] == 0] = 1.0\n\n            for i, (data, data_raw, start_idx, x_lag, xy_lag, op, uid, ts) in enumerate(zip(\n                    batch['x'], batch['x_raw'], batch['start_date'], batch['x_lags'],\n                    batch['xy_lags'], batch['onpromotion'], batch['id'], batch['ts']\n                )\n            ):\n                seq_len = full_seq_len - start_idx\n                val_window = 365\n                train_window = 365\n\n                if mode == 'train':\n                    if seq_len == 0:\n                        rand_encode_len = 0\n                        weights[i] = 0\n                    elif seq_len <= train_window:\n                        rand_encode_len = np.random.randint(0, seq_len)\n                    else:\n                        rand_encode_len = np.random.randint(seq_len - train_window, seq_len)\n                    rand_decode_len = min(seq_len - rand_encode_len, num_decode_steps)\n\n                elif mode == 'val':\n                    if seq_len <= num_decode_steps:\n                        rand_encode_len = 0\n                        weights[i] = 0\n                    elif seq_len <= val_window + num_decode_steps:\n                        rand_encode_len = np.random.randint(0, seq_len - num_decode_steps + 1)\n                    else:\n                        rand_encode_len = np.random.randint(\n                            seq_len - (val_window + num_decode_steps), seq_len - num_decode_steps + 1)\n                    rand_decode_len = min(seq_len - rand_encode_len, num_decode_steps)\n\n                elif mode == 'test':\n                    rand_encode_len = seq_len\n                    rand_decode_len = num_decode_steps\n\n                end_idx = start_idx + rand_encode_len\n\n                x[i, :rand_encode_len] = data[start_idx: end_idx]\n                y[i, :rand_decode_len] = data[end_idx: end_idx + rand_decode_len]\n                x_raw[i, :rand_encode_len] = data_raw[start_idx: end_idx]\n\n                x_lags[i, :rand_encode_len, :x_lag.shape[1]] = x_lag[start_idx: end_idx, :]\n                x_lags[i, :rand_encode_len, x_lag.shape[1]:] = xy_lag[start_idx: end_idx, :]\n                y_lags[i, :rand_decode_len, :] = xy_lag[end_idx: end_idx + rand_decode_len, :]\n\n                x_op[i, :rand_encode_len] = op[start_idx: end_idx]\n                y_op[i, :rand_decode_len] = op[end_idx: end_idx + rand_decode_len]\n                x_ts[i, :rand_encode_len, :] = ts[start_idx: end_idx, :]\n                x_idx[i, :rand_encode_len] = np.floor(np.log(np.arange(rand_encode_len) + 1))\n                y_idx[i, :rand_decode_len] = np.floor(\n                    np.log(np.arange(rand_encode_len, rand_encode_len + rand_decode_len) + 1))\n                y_id[i, :rand_decode_len] = uid[end_idx: end_idx + rand_decode_len]\n                x_len[i] = end_idx - start_idx\n                y_len[i] = rand_decode_len\n\n            batch['x_'] = batch['x']\n            batch['x'] = x\n            batch['y'] = y\n            batch['x_raw'] = x_raw\n            batch['x_lags'] = x_lags\n            batch['y_lags'] = y_lags\n            batch['x_op'] = x_op\n            batch['y_op'] = y_op\n            batch['x_ts'] = x_ts\n            batch['x_idx'] = x_idx\n            batch['y_idx'] = y_idx\n            batch['y_id'] = y_id\n            batch['x_len'] = x_len\n            batch['y_len'] = y_len\n            batch['item_class'] = batch['class']\n            batch['weights'] = weights\n\n            yield batch\n\n\nclass cnn(TFBaseModel):\n\n    def __init__(\n        self,\n        residual_channels=32,\n        skip_channels=32,\n        dilations=[2**i for i in range(8)]*3,\n        filter_widths=[2 for i in range(8)]*3,\n        num_decode_steps=16,\n        **kwargs\n    ):\n        self.residual_channels = residual_channels\n        self.skip_channels = skip_channels\n        self.dilations = dilations\n        self.filter_widths = filter_widths\n        self.num_decode_steps = num_decode_steps\n        super(cnn, self).__init__(**kwargs)\n\n    def get_input_sequences(self):\n        self.x = tf.placeholder(tf.float32, [None, None])\n        self.y = tf.placeholder(tf.float32, [None, self.num_decode_steps])\n        self.x_raw = tf.placeholder(tf.float32, [None, None])\n        self.x_len = tf.placeholder(tf.int32, [None])\n        self.y_len = tf.placeholder(tf.int32, [None])\n        self.x_op = tf.placeholder(tf.int32, [None, None])\n        self.y_op = tf.placeholder(tf.int32, [None, self.num_decode_steps])\n        self.x_id = tf.placeholder(tf.int32, [None, None])\n        self.y_id = tf.placeholder(tf.int32, [None, self.num_decode_steps])\n        self.x_idx = tf.placeholder(tf.int32, [None, None])\n        self.y_idx = tf.placeholder(tf.int32, [None, self.num_decode_steps])\n\n        self.x_dow = tf.placeholder(tf.int32, [None, None])\n        self.y_dow = tf.placeholder(tf.int32, [None, self.num_decode_steps])\n        self.x_month = tf.placeholder(tf.int32, [None, None])\n        self.y_month = tf.placeholder(tf.int32, [None, self.num_decode_steps])\n        self.x_week = tf.placeholder(tf.int32, [None, None])\n        self.y_week = tf.placeholder(tf.int32, [None, self.num_decode_steps])\n\n        self.x_ts = tf.placeholder(tf.float32, [None, None, 16])\n        self.x_lags = tf.placeholder(tf.float32, [None, None, 12])\n        self.y_lags = tf.placeholder(tf.float32, [None, self.num_decode_steps, 9])\n\n        self.city = tf.placeholder(tf.int32, [None])\n        self.state = tf.placeholder(tf.int32, [None])\n        self.type = tf.placeholder(tf.int32, [None])\n        self.cluster = tf.placeholder(tf.int32, [None])\n        self.family = tf.placeholder(tf.int32, [None])\n        self.item_class = tf.placeholder(tf.int32, [None])\n        self.perishable = tf.placeholder(tf.int32, [None])\n        self.is_discrete = tf.placeholder(tf.float32, [None])\n        self.store_nbr = tf.placeholder(tf.int32, [None])\n        self.item_nbr = tf.placeholder(tf.int32, [None])\n\n        self.weights = tf.placeholder(tf.float32, [None])\n\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.is_training = tf.placeholder(tf.bool)\n\n        item_class_embeddings = tf.get_variable(\n            name='item_class_embeddings',\n            shape=[self.reader.num_item_class, 20],\n            dtype=tf.float32\n        )\n        item_class = tf.nn.embedding_lookup(item_class_embeddings, self.item_class)\n\n        item_nbr_embeddings = tf.get_variable(\n            name='item_nbr_embeddings',\n            shape=[self.reader.num_item_nbr, 50],\n            dtype=tf.float32\n        )\n        item_nbr = tf.nn.embedding_lookup(item_nbr_embeddings, self.item_nbr)\n\n        self.x_mean = tf.expand_dims(sequence_mean(self.x, self.x_len), 1)\n        self.x_centered = self.x - self.x_mean\n        self.y_centered = self.y - self.x_mean\n        self.x_ts_centered = self.x_ts - tf.expand_dims(self.x_mean, 2)\n        self.x_lags_centered = self.x_lags - tf.expand_dims(self.x_mean, 2)\n        self.y_lags_centered = self.y_lags - tf.expand_dims(self.x_mean, 2)\n        self.x_is_zero = tf.cast(tf.equal(self.x_raw, tf.zeros_like(self.x_raw)), tf.float32)\n        self.x_is_negative = tf.cast(tf.less(self.x_raw, tf.zeros_like(self.x_raw)), tf.float32)\n\n        self.encode_features = tf.concat([\n            self.x_ts_centered,\n            self.x_lags_centered,\n            tf.one_hot(self.x_op, 3),\n            tf.one_hot(self.x_idx, 9),\n            tf.expand_dims(self.x_is_zero, 2),\n            tf.expand_dims(self.x_is_negative, 2),\n            tf.tile(tf.expand_dims(self.x_mean, 2), (1, tf.shape(self.x)[1], 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.city, self.reader.num_city), 1), (1, tf.shape(self.x)[1], 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.state, self.reader.num_state), 1), (1, tf.shape(self.x)[1], 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.type, self.reader.num_type), 1), (1, tf.shape(self.x)[1], 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.cluster, self.reader.num_cluster), 1), (1, tf.shape(self.x)[1], 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.family, self.reader.num_family), 1), (1, tf.shape(self.x)[1], 1)),\n            tf.tile(tf.expand_dims(item_class, 1), (1, tf.shape(self.x)[1], 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.perishable, self.reader.num_perishable), 1), (1, tf.shape(self.x)[1], 1)),\n            tf.tile(tf.expand_dims(tf.expand_dims(self.is_discrete, 1), 2), (1, tf.shape(self.x)[1], 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.store_nbr, self.reader.num_store_nbr), 1), (1, tf.shape(self.x)[1], 1)),\n            tf.tile(tf.expand_dims(item_nbr, 1), (1, tf.shape(self.x)[1], 1)),\n        ], axis=2)\n\n        decode_idx = tf.tile(tf.expand_dims(tf.range(self.num_decode_steps), 0), (tf.shape(self.y)[0], 1))\n        decode_features = tf.concat([\n            self.y_lags_centered,\n            tf.one_hot(decode_idx, self.num_decode_steps),\n            tf.one_hot(self.y_op, 3),\n            tf.one_hot(self.y_idx, 9),\n            tf.tile(tf.expand_dims(self.x_mean, 2), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.city, self.reader.num_city), 1), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.state, self.reader.num_state), 1), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.type, self.reader.num_type), 1), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.cluster, self.reader.num_cluster), 1), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.family, self.reader.num_family), 1), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(item_class, 1), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.perishable, self.reader.num_perishable), 1), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(tf.expand_dims(self.is_discrete, 1), 2), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(tf.one_hot(self.store_nbr, self.reader.num_store_nbr), 1), (1, self.num_decode_steps, 1)),\n            tf.tile(tf.expand_dims(item_nbr, 1), (1, self.num_decode_steps, 1)),\n        ], axis=2)\n\n        lstm_decode_features = bidirectional_lstm_layer(decode_features, self.y_len, 100)\n        self.decode_features = tf.concat([decode_features, lstm_decode_features], axis=2)\n\n        return tf.expand_dims(self.x_centered, 2)\n\n    def encode(self, x, features):\n        x = tf.concat([x, features], axis=2)\n\n        h = time_distributed_dense_layer(\n            inputs=x,\n            output_units=self.residual_channels,\n            activation=tf.nn.tanh,\n            scope='x-init',\n        )\n        c = time_distributed_dense_layer(\n            inputs=x,\n            output_units=self.residual_channels,\n            activation=tf.nn.tanh,\n            scope='c-init',\n        )\n\n        conv_inputs = [h]\n        for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)[:-1]):\n            dilated_conv = temporal_convolution_layer(\n                inputs=h,\n                output_units=4*self.residual_channels,\n                convolution_width=filter_width,\n                causal=True,\n                dilation_rate=[dilation],\n                scope='dilated-conv-encode-{}'.format(i),\n            )\n            input_gate, conv_filter, conv_gate, emit_gate = tf.split(dilated_conv, 4, axis=2)\n\n            c = tf.nn.sigmoid(input_gate)*c + tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n            h = tf.nn.sigmoid(emit_gate)*tf.nn.tanh(c)\n            conv_inputs.append(h)\n\n        return conv_inputs\n\n    def initialize_decode_params(self, x, features):\n        x = tf.concat([x, features], axis=2)\n\n        h = time_distributed_dense_layer(\n            inputs=x,\n            output_units=self.residual_channels,\n            activation=tf.nn.tanh,\n            scope='h-init-decode',\n        )\n        c = time_distributed_dense_layer(\n            inputs=x,\n            output_units=self.residual_channels,\n            activation=tf.nn.tanh,\n            scope='c-init-decode',\n        )\n\n        skip_outputs = []\n        conv_inputs = [h]\n        for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)):\n            dilated_conv = temporal_convolution_layer(\n                inputs=h,\n                output_units=4*self.residual_channels,\n                convolution_width=filter_width,\n                causal=True,\n                dilation_rate=[dilation],\n                scope='dilated-conv-decode-{}'.format(i),\n            )\n            input_gate, conv_filter, conv_gate, emit_gate = tf.split(dilated_conv, 4, axis=2)\n\n            c = tf.nn.sigmoid(input_gate)*c + tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n            h = tf.nn.sigmoid(emit_gate)*tf.nn.tanh(c)\n\n            skip_outputs.append(h)\n            conv_inputs.append(h)\n\n        skip_outputs = tf.concat(skip_outputs, axis=2)\n        h = time_distributed_dense_layer(skip_outputs, 128, scope='dense-decode-1', activation=tf.nn.relu)\n        y_hat = time_distributed_dense_layer(h, 2, scope='dense-decode-2')\n        return y_hat\n\n    def decode(self, x, conv_inputs, features):\n        batch_size = tf.shape(x)[0]\n\n        # initialize state tensor arrays\n        state_queues = []\n        for i, (conv_input, dilation) in enumerate(zip(conv_inputs, self.dilations)):\n            batch_idx = tf.range(batch_size)\n            batch_idx = tf.tile(tf.expand_dims(batch_idx, 1), (1, dilation))\n            batch_idx = tf.reshape(batch_idx, [-1])\n\n            temporal_idx = tf.expand_dims(self.x_len, 1) + tf.expand_dims(tf.range(dilation), 0)\n            temporal_idx = tf.reshape(temporal_idx, [-1])\n\n            idx = tf.stack([batch_idx, temporal_idx], axis=1)\n            padding = tf.zeros([batch_size, dilation + 1, shape(conv_input, 2)])\n            conv_input = tf.concat([padding, conv_input], axis=1)\n            slices = tf.reshape(tf.gather_nd(conv_input, idx), (batch_size, dilation, shape(conv_input, 2)))\n\n            layer_ta = tf.TensorArray(dtype=tf.float32, size=dilation + self.num_decode_steps)\n            layer_ta = layer_ta.unstack(tf.transpose(slices, (1, 0, 2)))\n            state_queues.append(layer_ta)\n\n        # initialize feature tensor array\n        features_ta = tf.TensorArray(dtype=tf.float32, size=self.num_decode_steps)\n        features_ta = features_ta.unstack(tf.transpose(features, (1, 0, 2)))\n\n        # initialize output tensor array\n        emit_ta = tf.TensorArray(size=self.num_decode_steps, dtype=tf.float32)\n\n        # initialize other loop vars\n        elements_finished = 0 >= self.y_len\n        time = tf.constant(0, dtype=tf.int32)\n\n        # get initial x input\n        current_idx = tf.stack([tf.range(tf.shape(self.x_len)[0]), self.x_len - 1], axis=1)\n        initial_input = tf.gather_nd(x, current_idx)\n\n        def loop_fn(time, current_input, queues):\n            current_features = features_ta.read(time)\n            current_input = tf.concat([current_input, current_features], axis=1)\n\n            with tf.variable_scope('h-init-decode', reuse=True):\n                w_x_proj = tf.get_variable('weights')\n                b_x_proj = tf.get_variable('biases')\n                h = tf.nn.tanh(tf.matmul(current_input, w_x_proj) + b_x_proj)\n\n            with tf.variable_scope('c-init-decode', reuse=True):\n                w_x_proj = tf.get_variable('weights')\n                b_x_proj = tf.get_variable('biases')\n                c = tf.nn.tanh(tf.matmul(current_input, w_x_proj) + b_x_proj)\n\n            skip_outputs, updated_queues = [], []\n            for i, (queue, dilation) in enumerate(zip(queues, self.dilations)):\n\n                state = queue.read(time)\n                with tf.variable_scope('dilated-conv-decode-{}'.format(i), reuse=True):\n                    w_conv = tf.get_variable('weights')\n                    b_conv = tf.get_variable('biases')\n                    dilated_conv = tf.matmul(state, w_conv[0, :, :]) + tf.matmul(h, w_conv[1, :, :]) + b_conv\n\n                input_gate, conv_filter, conv_gate, emit_gate = tf.split(dilated_conv, 4, axis=1)\n\n                c = tf.nn.sigmoid(input_gate)*c + tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n                h = tf.nn.sigmoid(emit_gate)*tf.nn.tanh(c)\n\n                skip_outputs.append(h)\n                updated_queues.append(queue.write(time + dilation, h))\n\n            skip_outputs = tf.concat(skip_outputs, axis=1)\n            with tf.variable_scope('dense-decode-1', reuse=True):\n                w_h = tf.get_variable('weights')\n                b_h = tf.get_variable('biases')\n                h = tf.nn.relu(tf.matmul(skip_outputs, w_h) + b_h)\n\n            with tf.variable_scope('dense-decode-2', reuse=True):\n                w_y = tf.get_variable('weights')\n                b_y = tf.get_variable('biases')\n                y_hat = tf.matmul(h, w_y) + b_y\n\n            elements_finished = (time >= self.y_len)\n            finished = tf.reduce_all(elements_finished)\n\n            next_input = tf.cond(\n                finished,\n                lambda: tf.zeros([batch_size, 2], dtype=tf.float32),\n                lambda: y_hat\n            )\n            next_elements_finished = (time >= self.num_decode_steps - 1)\n\n            return (next_elements_finished, next_input, updated_queues)\n\n        def condition(unused_time, elements_finished, *_):\n            return tf.logical_not(tf.reduce_all(elements_finished))\n\n        def body(time, elements_finished, emit_ta, *state_queues):\n            (next_finished, emit_output, state_queues) = loop_fn(time, initial_input, state_queues)\n\n            emit = tf.where(elements_finished, tf.zeros_like(emit_output), emit_output)\n            emit_ta = emit_ta.write(time, emit)\n\n            elements_finished = tf.logical_or(elements_finished, next_finished)\n            return [time + 1, elements_finished, emit_ta] + list(state_queues)\n\n        returned = tf.while_loop(\n            cond=condition,\n            body=body,\n            loop_vars=[time, elements_finished, emit_ta] + state_queues\n        )\n\n        outputs_ta = returned[2]\n        y_hat = tf.transpose(outputs_ta.stack(), (1, 0, 2))\n        return y_hat\n\n    def calculate_loss(self):\n        x = self.get_input_sequences()\n\n        conv_inputs = self.encode(x, features=self.encode_features)\n        decode_x = tf.concat([x, 1.0 - tf.expand_dims(self.x_is_zero, 2)], axis=2)\n        self.initialize_decode_params(decode_x, features=self.decode_features)\n\n        y_hat = self.decode(decode_x, conv_inputs, features=self.decode_features)\n        y_hat, p = tf.unstack(y_hat, axis=2, num=2)\n        y_hat = tf.nn.sigmoid(p)*(y_hat + self.x_mean)\n        self.loss = sequence_rmse(self.y, y_hat, self.y_len, weights=self.weights)\n\n        self.prediction_tensors = {\n            'preds': tf.nn.relu(y_hat),\n            'lengths': self.x_len,\n            'ids': self.y_id,\n        }\n\n        return self.loss\n\n\nif __name__ == '__main__':\n    base_dir = './'\n\n    dr = DataReader(data_dir=os.path.join(base_dir, 'data/processed/'))\n\n    nn = cnn(\n        reader=dr,\n        log_dir=os.path.join(base_dir, 'logs'),\n        checkpoint_dir=os.path.join(base_dir, 'checkpoints'),\n        prediction_dir=os.path.join(base_dir, 'predictions'),\n        optimizer='adam',\n        learning_rates=[.001, .0005, .00025],\n        beta1_decays=[.9, .9, .9],\n        batch_sizes=[64, 128, 256],\n        num_training_steps=200000,\n        patiences=[5000, 5000, 5000],\n        warm_start_init_step=0,\n        regularization_constant=0.0,\n        keep_prob=1.0,\n        enable_parameter_averaging=True,\n        min_steps_to_checkpoint=500,\n        log_interval=50,\n        validation_batch_size=4*64,\n        grad_clip=20,\n        residual_channels=32,\n        skip_channels=32,\n        dilations=[2**i for i in range(9)]*3,\n        filter_widths=[2 for i in range(9)]*3,\n        num_decode_steps=16,\n        loss_averaging_window=200\n    )\n    nn.fit()\n    nn.restore()\n    nn.predict()\n"""
cf/data_frame.py,0,"b'import copy\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\nclass DataFrame(object):\n\n    """"""Minimal pd.DataFrame analog for handling n-dimensional numpy matrices with additional\n    support for shuffling, batching, and train/test splitting.\n\n    Args:\n        columns: List of names corresponding to the matrices in data.\n        data: List of n-dimensional data matrices ordered in correspondence with columns.\n            All matrices must have the same leading dimension.  Data can also be fed a list of\n            instances of np.memmap, in which case RAM usage can be limited to the size of a\n            single batch.\n    """"""\n\n    def __init__(self, columns, data, idx=None):\n        assert len(columns) == len(data), \'columns length does not match data length\'\n\n        lengths = [mat.shape[0] for mat in data]\n        assert len(set(lengths)) == 1, \'all matrices in data must have same first dimension\'\n\n        self.length = lengths[0]\n        self.columns = columns\n        self.data = data\n        self.dict = dict(zip(self.columns, self.data))\n        self.idx = idx if idx is not None else np.arange(self.length)\n\n    def shapes(self):\n        return pd.Series(dict(zip(self.columns, [mat.shape for mat in self.data])))\n\n    def dtypes(self):\n        return pd.Series(dict(zip(self.columns, [mat.dtype for mat in self.data])))\n\n    def shuffle(self):\n        np.random.shuffle(self.idx)\n\n    def train_test_split(self, train_size, random_state=np.random.randint(10000)):\n        train_idx, test_idx = train_test_split(self.idx, train_size=train_size, random_state=random_state)\n        train_df = DataFrame(copy.copy(self.columns), self.data, idx=train_idx)\n        test_df = DataFrame(copy.copy(self.columns), self.data, idx=test_idx)\n        return train_df, test_df\n\n    def batch_generator(self, batch_size, shuffle=True, num_epochs=10000, allow_smaller_final_batch=False):\n        epoch_num = 0\n        while epoch_num < num_epochs:\n            if shuffle:\n                self.shuffle()\n\n            for i in range(0, self.length + 1, batch_size):\n                batch_idx = self.idx[i: i + batch_size]\n                if not allow_smaller_final_batch and len(batch_idx) != batch_size:\n                    break\n                yield DataFrame(columns=copy.copy(self.columns), data=[mat[batch_idx].copy() for mat in self.data])\n\n            epoch_num += 1\n\n    def concat(self, other_df):\n        mats = []\n        for column in self.columns:\n            mats.append(np.concatenate(self[column], other_df[column]), axis=0)\n        return DataFrame(copy.copy(self.columns), mats)\n\n    def iterrows(self):\n        for i in self.idx:\n            yield self[i]\n\n    def mask(self, mask):\n        return DataFrame(copy.copy(self.columns), [mat[mask] for mat in self.data])\n\n    def items(self):\n        return self.dict.items()\n\n    def __iter__(self):\n        return self.dict.items().__iter__()\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, key):\n        if isinstance(key, str):\n            return self.dict[key]\n\n        elif isinstance(key, int):\n            return pd.Series(dict(zip(self.columns, [mat[self.idx[key]] for mat in self.data])))\n\n    def __setitem__(self, key, value):\n        assert value.shape[0] == len(self), \'matrix first dimension does not match\'\n        if key not in self.columns:\n            self.columns.append(key)\n            self.data.append(value)\n        self.dict[key] = value\n'"
cf/prepare_data.py,0,"b""import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndtypes = {\n    'id': np.int32,\n    'store_nbr': np.int8,\n    'item_nbr': np.int32,\n    'unit_sales': np.float16,\n}\n\ntrain = pd.read_csv('data/raw/train.csv', dtype=dtypes, parse_dates=[1])\ntest = pd.read_csv('data/raw/test.csv', dtype=dtypes, parse_dates=[1])\n\nis_test = test.groupby(['store_nbr', 'item_nbr']).apply(lambda x: pd.Series({'is_test': 1})).reset_index()\nis_discrete = train.groupby('item_nbr')['unit_sales'].apply(lambda x: np.all((x.values).astype(int) == x.values))\nis_discrete = is_discrete.reset_index().rename(columns={'unit_sales': 'is_discrete'})\nstart_date = train.groupby(['store_nbr', 'item_nbr'])['date'].min()\ntest_start = test['date'].min()\ntest['unit_sales'] = -1\nprint 'loaded data'\n\ndf = pd.concat([train, test], axis=0)\ndel train, test\ndf['onpromotion'] = df['onpromotion'].map(lambda x: int(x) if not np.isnan(x) else 2).astype(np.int8)\nprint 'concatenated'\n\ndf = df.merge(is_test, how='left', on=['store_nbr', 'item_nbr'])\ndf = df[df['is_test'] == 1].drop('is_test', axis=1)\nprint 'filtered test'\n\n# dates\ndate_range = pd.date_range(df['date'].min(), df['date'].max(), freq='D')\ndate_idx = range(len(date_range))\ndt_to_idx = dict(map(reversed, enumerate(date_range)))\ntest_start_idx = dt_to_idx[test_start]\ndf['date'] = df['date'].map(dt_to_idx.get)\nmissing_dates = list(set(date_idx) - set(df['date']))\n\n# pivot and reindex\ndf = df.pivot_table(index=['store_nbr', 'item_nbr'], columns='date')\nfill = np.zeros([df.shape[0], len(missing_dates)])\nfill[:] = np.nan\nmissing_df = pd.DataFrame(columns=missing_dates, data=fill)\n\nop = pd.concat([df['onpromotion'].reset_index(), missing_df], axis=1).fillna(2)\nop = op[['store_nbr', 'item_nbr'] + date_idx]\nop = op[date_idx].values.astype(np.int8)\n\nfor i in range(op.shape[1]):\n    nan_mask = op[:, i] == 2\n    p = .2*op[~nan_mask, i].mean()\n    if np.isnan(p):\n        p = 0\n    fill = np.random.binomial(n=1, p=p, size=nan_mask.sum())\n    op[nan_mask, i] = fill\nprint 'nan mean'\n\nuid = pd.concat([df['id'].reset_index(), missing_df], axis=1).fillna(0)\nuid = uid[['store_nbr', 'item_nbr'] + date_idx]\n\ndf = pd.concat([df['unit_sales'].reset_index(), missing_df], axis=1).fillna(0)\ndf = df[['store_nbr', 'item_nbr'] + date_idx]\n\nif not os.path.isdir('data/processed'):\n    os.makedirs('data/processed')\n\nnp.save('data/processed/x_raw.npy', df[date_idx].values.astype(np.float16))\nnp.save('data/processed/onpromotion.npy', op.astype(np.int8))\nnp.save('data/processed/id.npy', uid[date_idx].values.astype(np.int32))\nprint 'pivoted'\ndel op, uid\n\ndf[date_idx] = np.log(np.maximum(df[date_idx].values, 0) + 1)\ndf[date_idx] = df[date_idx].astype(np.float16)\nnp.save('data/processed/x.npy', df[date_idx].values)\n\n# non-temporal features\nstart_date = start_date.reset_index().rename(columns={'date': 'start_date'})\nstart_date['start_date'] = start_date['start_date']\ndf = df.merge(start_date, how='left', on=['store_nbr', 'item_nbr'])\ndf['start_date'] = df['start_date'].map(lambda x: dt_to_idx.get(x, test_start_idx))\ndel start_date\n\ndf = df.merge(is_discrete, how='left', on='item_nbr')\ndf['is_discrete'] = df['is_discrete'].fillna(0).astype(int)\ndel is_discrete\n\nstores = pd.read_csv('data/raw/stores.csv')\nencode_cols = [i for i in stores.columns if i != 'store_nbr']\nstores[encode_cols] = stores[encode_cols].apply(lambda x: LabelEncoder().fit_transform(x))\ndf = df.merge(stores, how='left', on='store_nbr')\ndel stores\n\nitems = pd.read_csv('data/raw/items.csv')\nencode_cols = ['family', 'class']\nitems[encode_cols] = items[encode_cols].apply(lambda x: LabelEncoder().fit_transform(x))\ndf = df.merge(items, how='left', on='item_nbr')\ndf['item_nbr'] = LabelEncoder().fit_transform(df['item_nbr'])\ndel items\n\nfeatures = [\n    ('store_nbr', np.int8),\n    ('item_nbr', np.int32),\n    ('city', np.int8),\n    ('state', np.int8),\n    ('type', np.int8),\n    ('cluster', np.int8),\n    ('family', np.int8),\n    ('class', np.int16),\n    ('perishable', np.int8),\n    ('is_discrete', np.int8),\n    ('start_date', np.int16)\n]\n\nfor feature, dtype in features:\n    vals = df[feature].values.astype(dtype)\n    np.save('data/processed/{}.npy'.format(feature), vals)\nprint 'finished non-temporal features'\n\n\n# lags\nx = df[date_idx].values\n\nx_lags = [1, 7, 14]\nlag_data = np.zeros([x.shape[0], x.shape[1], len(x_lags)], dtype=np.float16)\n\nfor i, lag in enumerate(x_lags):\n    lag_data[:, lag:, i] = x[:, :-lag]\n\nnp.save('data/processed/x_lags.npy', lag_data)\ndel lag_data\n\nxy_lags = [16, 21, 28, 35, 365/4, 365/2, 365, 365*2, 365*3]\nlag_data = np.zeros([x.shape[0], x.shape[1], len(xy_lags)], dtype=np.float16)\n\nfor i, lag in enumerate(xy_lags):\n    lag_data[:, lag:, i] = x[:, :-lag]\n\nnp.save('data/processed/xy_lags.npy', lag_data)\ndel lag_data\n\n# aggregate time series\ngroups = [\n    ['store_nbr'],\n    ['item_nbr'],\n    ['family'],\n    ['class'],\n    ['city'],\n    ['state'],\n    ['type'],\n    ['cluster'],\n    ['item_nbr', 'city'],\n    ['item_nbr', 'type'],\n    ['item_nbr', 'cluster'],\n    ['family', 'city'],\n    ['family', 'type'],\n    ['family', 'cluster'],\n    ['store_nbr', 'family'],\n    ['store_nbr', 'class']\n]\n\ndf_idx = df[['store_nbr', 'item_nbr', 'family', 'class', 'city', 'state', 'type', 'cluster']]\n\naux_ts = np.zeros([df.shape[0], len(date_idx), len(groups)], dtype=np.float16)\nfor i, group in enumerate(groups):\n    print i\n    ts = df.groupby(group)[date_idx].mean().reset_index()\n    ts = df_idx.merge(ts, how='left', on=group)\n    aux_ts[:, :, i] = ts[date_idx].fillna(0).values\nnp.save('data/processed/ts.npy', aux_ts)\n"""
cf/submit.py,0,"b""import pandas as pd\nimport numpy as np\nimport os\n\nlengths_mat = np.load(os.path.join('predictions', 'lengths.npy'))\npreds_mat = np.load(os.path.join('predictions', 'preds.npy'))\nids_mat = np.load(os.path.join('predictions', 'ids.npy'))\npreds_mat[lengths_mat == 0] = 0\n\ndf = pd.DataFrame({'id': ids_mat.flatten(), 'unit_sales': preds_mat.flatten()})\ndf['unit_sales'] = df['unit_sales'].map(np.expm1)\n\ndf[['id', 'unit_sales']].to_csv('sub.csv', index=False)\n"""
cf/tf_base_model.py,24,"b'from collections import deque\nfrom datetime import datetime\nimport logging\nimport os\nimport pprint as pp\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_utils import shape\n\n\nclass TFBaseModel(object):\n\n    """"""Interface containing some boilerplate code for training tensorflow models.\n\n    Subclassing models must implement self.calculate_loss(), which returns a tensor for the batch loss.\n    Code for the training loop, parameter updates, checkpointing, and inference are implemented here and\n    subclasses are mainly responsible for building the computational graph beginning with the placeholders\n    and ending with the loss tensor.\n\n    Args:\n        reader: Class with attributes train_batch_generator, val_batch_generator, and test_batch_generator\n            that yield dictionaries mapping tf.placeholder names (as strings) to batch data (numpy arrays).\n        batch_size: Minibatch size.\n        learning_rate: Learning rate.\n        optimizer: \'rms\' for RMSProp, \'adam\' for Adam, \'sgd\' for SGD\n        grad_clip: Clip gradients elementwise to have norm at most equal to grad_clip.\n        regularization_constant:  Regularization constant applied to all trainable parameters.\n        keep_prob: 1 - p, where p is the dropout probability\n        early_stopping_steps:  Number of steps to continue training after validation loss has\n            stopped decreasing.\n        warm_start_init_step:  If nonzero, model will resume training a restored model beginning\n            at warm_start_init_step.\n        num_restarts:  After validation loss plateaus, the best checkpoint will be restored and the\n            learning rate will be halved.  This process will repeat num_restarts times.\n        enable_parameter_averaging:  If true, model saves exponential weighted averages of parameters\n            to separate checkpoint file.\n        min_steps_to_checkpoint:  Model only saves after min_steps_to_checkpoint training steps\n            have passed.\n        log_interval:  Train and validation accuracies are logged every log_interval training steps.\n        loss_averaging_window:  Train/validation losses are averaged over the last loss_averaging_window\n            training steps.\n        num_validation_batches:  Number of batches to be used in validation evaluation at each step.\n        log_dir: Directory where logs are written.\n        checkpoint_dir: Directory where checkpoints are saved.\n        prediction_dir: Directory where predictions/outputs are saved.\n    """"""\n\n    def __init__(\n        self,\n        reader,\n        batch_sizes=[128],\n        num_training_steps=20000,\n        learning_rates=[.01],\n        beta1_decays=[.99],\n        optimizer=\'adam\',\n        grad_clip=5,\n        regularization_constant=0.0,\n        keep_prob=1.0,\n        patiences=[3000],\n        warm_start_init_step=0,\n        enable_parameter_averaging=False,\n        min_steps_to_checkpoint=100,\n        log_interval=20,\n        loss_averaging_window=100,\n        validation_batch_size=64,\n        log_dir=\'logs\',\n        checkpoint_dir=\'checkpoints\',\n        prediction_dir=\'predictions\'\n    ):\n\n        assert len(batch_sizes) == len(learning_rates) == len(patiences)\n        self.batch_sizes = batch_sizes\n        self.learning_rates = learning_rates\n        self.beta1_decays = beta1_decays\n        self.patiences = patiences\n        self.num_restarts = len(batch_sizes) - 1\n        self.restart_idx = 0\n        self.update_train_params()\n\n        self.reader = reader\n        self.num_training_steps = num_training_steps\n        self.optimizer = optimizer\n        self.grad_clip = grad_clip\n        self.regularization_constant = regularization_constant\n        self.warm_start_init_step = warm_start_init_step\n        self.keep_prob_scalar = keep_prob\n        self.enable_parameter_averaging = enable_parameter_averaging\n        self.min_steps_to_checkpoint = min_steps_to_checkpoint\n        self.log_interval = log_interval\n        self.loss_averaging_window = loss_averaging_window\n        self.validation_batch_size = validation_batch_size\n\n        self.log_dir = log_dir\n        self.prediction_dir = prediction_dir\n        self.checkpoint_dir = checkpoint_dir\n        if self.enable_parameter_averaging:\n            self.checkpoint_dir_averaged = checkpoint_dir + \'_avg\'\n\n        self.init_logging(self.log_dir)\n        logging.info(\'\\nnew run with parameters:\\n{}\'.format(pp.pformat(self.__dict__)))\n\n        self.graph = self.build_graph()\n        self.session = tf.Session(graph=self.graph)\n        print \'built graph\'\n\n    def update_train_params(self):\n        self.batch_size = self.batch_sizes[self.restart_idx]\n        self.learning_rate = self.learning_rates[self.restart_idx]\n        self.beta1_decay = self.beta1_decays[self.restart_idx]\n        self.early_stopping_steps = self.patiences[self.restart_idx]\n\n    def calculate_loss(self):\n        raise NotImplementedError(\'subclass must implement this\')\n\n    def fit(self):\n        with self.session.as_default():\n\n            if self.warm_start_init_step:\n                self.restore(self.warm_start_init_step)\n                step = self.warm_start_init_step\n            else:\n                self.session.run(self.init)\n                step = 0\n\n            train_generator = self.reader.train_batch_generator(self.batch_size)\n            val_generator = self.reader.val_batch_generator(self.validation_batch_size)\n\n            train_loss_history = deque(maxlen=self.loss_averaging_window)\n            val_loss_history = deque(maxlen=self.loss_averaging_window)\n            train_time_history = deque(maxlen=self.loss_averaging_window)\n            val_time_history = deque(maxlen=self.loss_averaging_window)\n\n            best_validation_loss, best_validation_tstep = float(\'inf\'), 0\n\n            while step < self.num_training_steps:\n\n                # validation evaluation\n                val_start = time.time()\n                val_batch_df = val_generator.next()\n                val_feed_dict = {\n                    getattr(self, placeholder_name, None): data\n                    for placeholder_name, data in val_batch_df.items() if hasattr(self, placeholder_name)\n                }\n\n                val_feed_dict.update({self.learning_rate_var: self.learning_rate, self.beta1_decay_var: self.beta1_decay})\n                if hasattr(self, \'keep_prob\'):\n                    val_feed_dict.update({self.keep_prob: 1.0})\n                if hasattr(self, \'is_training\'):\n                    val_feed_dict.update({self.is_training: False})\n\n                [val_loss] = self.session.run(\n                    fetches=[self.loss],\n                    feed_dict=val_feed_dict\n                )\n                val_loss_history.append(val_loss)\n                val_time_history.append(time.time() - val_start)\n\n                if hasattr(self, \'monitor_tensors\'):\n                    for name, tensor in self.monitor_tensors.items():\n                        [np_val] = self.session.run([tensor], feed_dict=val_feed_dict)\n                        print name\n                        print \'min\', np_val.min()\n                        print \'max\', np_val.max()\n                        print \'mean\', np_val.mean()\n                        print \'std\', np_val.std()\n                        print \'nans\', np.isnan(np_val).sum()\n                        print\n\n                # train step\n                train_start = time.time()\n                train_batch_df = train_generator.next()\n                train_feed_dict = {\n                    getattr(self, placeholder_name, None): data\n                    for placeholder_name, data in train_batch_df.items() if hasattr(self, placeholder_name)\n                }\n\n                train_feed_dict.update({self.learning_rate_var: self.learning_rate, self.beta1_decay_var: self.beta1_decay})\n                if hasattr(self, \'keep_prob\'):\n                    train_feed_dict.update({self.keep_prob: self.keep_prob_scalar})\n                if hasattr(self, \'is_training\'):\n                    train_feed_dict.update({self.is_training: True})\n\n                train_loss, _ = self.session.run(\n                    fetches=[self.loss, self.step],\n                    feed_dict=train_feed_dict\n                )\n                train_loss_history.append(train_loss)\n                train_time_history.append(time.time() - train_start)\n\n                if step % self.log_interval == 0:\n                    avg_train_loss = sum(train_loss_history) / len(train_loss_history)\n                    avg_val_loss = sum(val_loss_history) / len(val_loss_history)\n                    avg_train_time = sum(train_time_history) / len(train_time_history)\n                    avg_val_time = sum(val_time_history) / len(val_time_history)\n                    metric_log = (\n                        ""[[step {:>8}]]     ""\n                        ""[[train {:>4}s]]     loss: {:<12}     ""\n                        ""[[val {:>4}s]]     loss: {:<12}     ""\n                    ).format(\n                        step,\n                        round(avg_train_time, 4),\n                        round(avg_train_loss, 8),\n                        round(avg_val_time, 4),\n                        round(avg_val_loss, 8),\n                    )\n                    logging.info(metric_log)\n\n                    if avg_val_loss < best_validation_loss:\n                        best_validation_loss = avg_val_loss\n                        best_validation_tstep = step\n                        if step > self.min_steps_to_checkpoint:\n                            self.save(step)\n                            if self.enable_parameter_averaging:\n                                self.save(step, averaged=True)\n\n                    if step - best_validation_tstep > self.early_stopping_steps:\n\n                        if self.num_restarts is None or self.restart_idx >= self.num_restarts:\n                            logging.info(\'best validation loss of {} at training step {}\'.format(\n                                best_validation_loss, best_validation_tstep))\n                            logging.info(\'early stopping - ending training.\')\n                            return\n\n                        if self.restart_idx < self.num_restarts:\n                            self.restore(best_validation_tstep)\n                            step = best_validation_tstep\n                            self.restart_idx += 1\n                            self.update_train_params()\n                            train_generator = self.reader.train_batch_generator(self.batch_size)\n\n                step += 1\n\n            if step <= self.min_steps_to_checkpoint:\n                best_validation_tstep = step\n                self.save(step)\n                if self.enable_parameter_averaging:\n                    self.save(step, averaged=True)\n\n            logging.info(\'num_training_steps reached - ending training\')\n\n    def predict(self, chunk_size=256):\n        if not os.path.isdir(self.prediction_dir):\n            os.makedirs(self.prediction_dir)\n\n        if hasattr(self, \'prediction_tensors\'):\n            prediction_dict = {tensor_name: [] for tensor_name in self.prediction_tensors}\n\n            test_generator = self.reader.test_batch_generator(chunk_size)\n            for i, test_batch_df in enumerate(test_generator):\n                if i % 10 == 0:\n                    print i*len(test_batch_df)\n\n                test_feed_dict = {\n                    getattr(self, placeholder_name, None): data\n                    for placeholder_name, data in test_batch_df.items() if hasattr(self, placeholder_name)\n                }\n                if hasattr(self, \'keep_prob\'):\n                    test_feed_dict.update({self.keep_prob: 1.0})\n                if hasattr(self, \'is_training\'):\n                    test_feed_dict.update({self.is_training: False})\n\n                tensor_names, tf_tensors = zip(*self.prediction_tensors.items())\n                np_tensors = self.session.run(\n                    fetches=tf_tensors,\n                    feed_dict=test_feed_dict\n                )\n                for tensor_name, tensor in zip(tensor_names, np_tensors):\n                    prediction_dict[tensor_name].append(tensor)\n\n            for tensor_name, tensor in prediction_dict.items():\n                np_tensor = np.concatenate(tensor, 0)\n                save_file = os.path.join(self.prediction_dir, \'{}.npy\'.format(tensor_name))\n                logging.info(\'saving {} with shape {} to {}\'.format(tensor_name, np_tensor.shape, save_file))\n                np.save(save_file, np_tensor)\n\n        if hasattr(self, \'parameter_tensors\'):\n            for tensor_name, tensor in self.parameter_tensors.items():\n                np_tensor = tensor.eval(self.session)\n\n                save_file = os.path.join(self.prediction_dir, \'{}.npy\'.format(tensor_name))\n                logging.info(\'saving {} with shape {} to {}\'.format(tensor_name, np_tensor.shape, save_file))\n                np.save(save_file, np_tensor)\n\n    def save(self, step, averaged=False):\n        saver = self.saver_averaged if averaged else self.saver\n        checkpoint_dir = self.checkpoint_dir_averaged if averaged else self.checkpoint_dir\n        if not os.path.isdir(checkpoint_dir):\n            logging.info(\'creating checkpoint directory {}\'.format(checkpoint_dir))\n            os.mkdir(checkpoint_dir)\n\n        model_path = os.path.join(checkpoint_dir, \'model\')\n        logging.info(\'saving model to {}\'.format(model_path))\n        saver.save(self.session, model_path, global_step=step)\n\n    def restore(self, step=None, averaged=False):\n        saver = self.saver_averaged if averaged else self.saver\n        checkpoint_dir = self.checkpoint_dir_averaged if averaged else self.checkpoint_dir\n        if not step:\n            model_path = tf.train.latest_checkpoint(checkpoint_dir)\n            logging.info(\'restoring model parameters from {}\'.format(model_path))\n            saver.restore(self.session, model_path)\n        else:\n            model_path = os.path.join(\n                checkpoint_dir, \'model{}-{}\'.format(\'_avg\' if averaged else \'\', step)\n            )\n            logging.info(\'restoring model from {}\'.format(model_path))\n            saver.restore(self.session, model_path)\n\n    def init_logging(self, log_dir):\n        if not os.path.isdir(log_dir):\n            os.makedirs(log_dir)\n\n        date_str = datetime.now().strftime(\'%Y-%m-%d_%H-%M\')\n        log_file = \'log_{}.txt\'.format(date_str)\n\n        reload(logging)  # bad\n        logging.basicConfig(\n            filename=os.path.join(log_dir, log_file),\n            level=logging.INFO,\n            format=\'[[%(asctime)s]] %(message)s\',\n            datefmt=\'%m/%d/%Y %I:%M:%S %p\'\n        )\n        logging.getLogger().addHandler(logging.StreamHandler())\n\n    def update_parameters(self, loss):\n\n        if self.regularization_constant != 0:\n            l2_norm = tf.reduce_sum([tf.sqrt(tf.reduce_sum(tf.square(param))) for param in tf.trainable_variables()])\n            loss = loss + self.regularization_constant*l2_norm\n\n        optimizer = self.get_optimizer(self.learning_rate_var, self.beta1_decay_var)\n        grads = optimizer.compute_gradients(loss)\n        clipped = [(tf.clip_by_value(g, -self.grad_clip, self.grad_clip), v_) for g, v_ in grads]\n\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            step = optimizer.apply_gradients(clipped, global_step=self.global_step)\n\n        if self.enable_parameter_averaging:\n            maintain_averages_op = self.ema.apply(tf.trainable_variables())\n            with tf.control_dependencies([step]):\n                self.step = tf.group(maintain_averages_op)\n        else:\n            self.step = step\n\n        logging.info(\'all parameters:\')\n        logging.info(pp.pformat([(var.name, shape(var)) for var in tf.global_variables()]))\n\n        logging.info(\'trainable parameters:\')\n        logging.info(pp.pformat([(var.name, shape(var)) for var in tf.trainable_variables()]))\n\n        logging.info(\'trainable parameter count:\')\n        logging.info(str(np.sum(np.prod(shape(var)) for var in tf.trainable_variables())))\n\n    def get_optimizer(self, learning_rate, beta1_decay):\n        if self.optimizer == \'adam\':\n            return tf.train.AdamOptimizer(learning_rate, beta1=beta1_decay)\n        elif self.optimizer == \'gd\':\n            return tf.train.GradientDescentOptimizer(learning_rate)\n        elif self.optimizer == \'rms\':\n            return tf.train.RMSPropOptimizer(learning_rate, decay=beta1_decay, momentum=0.9)\n        else:\n            assert False, \'optimizer must be adam, gd, or rms\'\n\n    def build_graph(self):\n        with tf.Graph().as_default() as graph:\n            self.ema = tf.train.ExponentialMovingAverage(decay=0.99)\n            self.global_step = tf.Variable(0, trainable=False)\n            self.learning_rate_var = tf.Variable(0.0, trainable=False)\n            self.beta1_decay_var = tf.Variable(0.0, trainable=False)\n\n            self.loss = self.calculate_loss()\n            self.update_parameters(self.loss)\n\n            self.saver = tf.train.Saver(max_to_keep=1)\n            if self.enable_parameter_averaging:\n                self.saver_averaged = tf.train.Saver(self.ema.variables_to_restore(), max_to_keep=1)\n\n            self.init = tf.global_variables_initializer()\n\n            return graph\n'"
cf/tf_utils.py,33,"b'import tensorflow as tf\n\n\ndef temporal_convolution_layer(inputs, output_units, convolution_width, causal=False, dilation_rate=[1], bias=True,\n                               activation=None, dropout=None, scope=\'temporal-convolution-layer\', reuse=False,\n                               batch_norm=None):\n    """"""\n    Convolution over the temporal axis of sequence data.\n\n    Args:\n        inputs: Tensor of shape [batch size, max sequence length, input_units].\n        output_units: Output channels for convolution.\n        convolution_width: Number of timesteps to use in convolution.\n        causal: Output at timestep t is a function of inputs at or before timestep t.\n        dilation_rate:  Dilation rate along temporal axis.\n\n    Returns:\n        Tensor of shape [batch size, max sequence length, output_units].\n    """"""\n    with tf.variable_scope(scope, reuse=reuse):\n        if causal:\n            shift = (convolution_width / 2) + (int(dilation_rate[0] - 1) / 2)\n            pad = tf.zeros([tf.shape(inputs)[0], shift, inputs.shape.as_list()[2]])\n            inputs = tf.concat([pad, inputs], axis=1)\n\n        W = tf.get_variable(\n            name=\'weights\',\n            initializer=tf.random_normal_initializer(\n                mean=0,\n                stddev=1.0 / tf.sqrt(float(convolution_width)*float(shape(inputs, 2)))\n            ),\n            shape=[convolution_width, shape(inputs, 2), output_units]\n        )\n\n        z = tf.nn.convolution(inputs, W, padding=\'SAME\', dilation_rate=dilation_rate)\n        if bias:\n            b = tf.get_variable(\n                name=\'biases\',\n                initializer=tf.constant_initializer(),\n                shape=[output_units]\n            )\n            z = z + b\n\n        z = tf.layers.batch_normalization(z, training=batch_norm) if batch_norm is not None else z\n        z = activation(z) if activation else z\n        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n        z = z[:, :-shift, :] if causal else z\n        return z\n\n\ndef bidirectional_lstm_layer(inputs, lengths, state_size, scope=\'bi-lstm-layer\', reuse=False):\n    """"""\n    Bidirectional LSTM layer.\n    Args:\n        inputs: Tensor of shape [batch size, max sequence length, ...].\n        lengths: Tensor of shape [batch size].\n        state_size: LSTM state size.\n        keep_prob: 1 - p, where p is the dropout probability.\n    Returns:\n        Tensor of shape [batch size, max sequence length, 2*state_size] containing the concatenated\n        forward and backward lstm outputs at each timestep.\n    """"""\n    with tf.variable_scope(scope, reuse=reuse):\n        cell_fw = tf.contrib.rnn.LSTMCell(\n            state_size,\n            reuse=reuse\n        )\n        cell_bw = tf.contrib.rnn.LSTMCell(\n            state_size,\n            reuse=reuse\n        )\n        outputs, (output_fw, output_bw) = tf.nn.bidirectional_dynamic_rnn(\n            inputs=inputs,\n            cell_fw=cell_fw,\n            cell_bw=cell_bw,\n            sequence_length=lengths,\n            dtype=tf.float32\n        )\n        outputs = tf.concat(outputs, 2)\n        return outputs\n\n\ndef time_distributed_dense_layer(inputs, output_units, bias=True, activation=None, batch_norm=None,\n                                 dropout=None, scope=\'time-distributed-dense-layer\', reuse=False):\n    """"""\n    Applies a shared dense layer to each timestep of a tensor of shape [batch_size, max_seq_len, input_units]\n    to produce a tensor of shape [batch_size, max_seq_len, output_units].\n\n    Args:\n        inputs: Tensor of shape [batch size, max sequence length, ...].\n        output_units: Number of output units.\n        activation: activation function.\n        dropout: dropout keep prob.\n\n    Returns:\n        Tensor of shape [batch size, max sequence length, output_units].\n    """"""\n    with tf.variable_scope(scope, reuse=reuse):\n        W = tf.get_variable(\n            name=\'weights\',\n            initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0 / float(shape(inputs, -1))),\n            shape=[shape(inputs, -1), output_units]\n        )\n        z = tf.einsum(\'ijk,kl->ijl\', inputs, W)\n        if bias:\n            b = tf.get_variable(\n                name=\'biases\',\n                initializer=tf.constant_initializer(),\n                shape=[output_units]\n            )\n            z = z + b\n\n        z = tf.layers.batch_normalization(z, training=batch_norm) if batch_norm is not None else z\n        z = activation(z) if activation else z\n        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n        return z\n\n\ndef sequence_rmse(y, y_hat, sequence_lengths, weights):\n    """"""\n    Calculate weighted RMSE(y, y_hat).\n\n    Args:\n        y: Label tensor of shape [batch_size, timesteps]\n        y_hat: Prediction tensor of shape [batch_size, timesteps]\n        sequence_lengths: Length of sequences, tensor of shape [batch_size]\n        weights: Weights for each sequence, tensor of shape [batch_size]\n\n    Returns:\n        RMSE as a 0-dimensional tensor\n\n    """"""\n    square_error = tf.square(y_hat - y)\n    sequence_mask = tf.cast(tf.sequence_mask(sequence_lengths, maxlen=tf.shape(y)[1]), tf.float32)\n    weights = sequence_mask*tf.expand_dims(weights, 1)\n    avg_square_error = tf.reduce_sum(square_error*weights) / tf.reduce_sum(weights)\n    return tf.sqrt(avg_square_error)\n\n\ndef sequence_mean(x, lengths):\n    """"""\n    Compute mean across temporal axis of sequence.\n\n    Args:\n        x: Tensor of shape [batch_size, timesteps]\n        lengths: Lengths of sequences, tensor of shape [batch_size]\n\n    Returns:\n        Sequence means as tensor of shape [batch_size]\n    """"""\n    sequence_mask = tf.cast(tf.sequence_mask(lengths, maxlen=tf.shape(x)[1]), tf.float32)\n    mean = tf.reduce_sum(x*sequence_mask, axis=1) / (tf.cast(lengths, tf.float32))\n    return tf.where(tf.is_nan(mean), tf.zeros_like(mean), mean)\n\n\ndef shape(tensor, dim=None):\n    """"""Get tensor shape/dimension as list/int""""""\n    if dim is None:\n        return tensor.shape.as_list()\n    else:\n        return tensor.shape.as_list()[dim]\n'"
