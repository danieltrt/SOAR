file_path,api_count,code
hello_tensorflow.py,2,"b""from __future__ import print_function\n\n\nimport tensorflow as tf\n\nhello = tf.constant('Hello, TensorFlow!')\n\n# Start tf session\nsess = tf.Session()\n\nprint(sess.run(hello))\n"""
basic_networks/basic_tf_network.py,10,"b""# Basics NeuralNetwork of Tensorflow\nimport tensorflow as tf\nimport numpy as np\nx_data = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])\ny_data = tf.constant([2.0, 4.0, 6.0, 8.0, 10.0])\n\nx = tf.placeholder(tf.float32)\ny = tf.placeholder(tf.float32)\n\nweight = tf.Variable([1.0])\nbias = tf.Variable([0.0])\n\ninit = tf.initialize_all_variables()\npredict = (x * weight) \nloss = tf.reduce_mean(tf.square(predict - y) ) \noptimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss/2)\nsteps = 300\nwith tf.Session() as sess:\n    sess.run(init)\n    X = sess.run(x_data)\n    Y = sess.run(y_data)\n    for i in range(steps):\n    \tprint 'Step: ', i, ':'\n    \tprint 'Loss: ', sess.run(loss, {x: X, y: Y})\n    \tsess.run(optimizer, {x: X, y: Y})\n    print 'THE FINAL WEIGHT IS', sess.run(weight)[0]\n\n"""
basic_networks/basic_tf_with_hidden_network.py,13,"b""# Tensorflow Basics on NeuralNetwork with hidden layer\n\nimport tensorflow as tf\nimport numpy as np\n\nINPUT_NEURONS = 2\nHIDDEN_LAYER_1_NEURONS = 4\nOUTPUT_NEURONS = 1\nx_data = np.array([[0, 0], [0, 1], [1, 1]])\n\ny_data = np.array([0, 1, 1])\n\nx = tf.placeholder(tf.float32)\ny = tf.placeholder(tf.float32)\n\nh1_layer = {'weight': tf.Variable(np.random.rand(INPUT_NEURONS, HIDDEN_LAYER_1_NEURONS).astype(np.float32)),\n            'bias': tf.constant(0.0)}\noutput_layer = {'weight': tf.Variable(np.random.rand(HIDDEN_LAYER_1_NEURONS, OUTPUT_NEURONS).astype(np.float32)),\n                'bias': tf.constant(0.0)}\n\n\nh1 = tf.matmul(x, h1_layer['weight']) + h1_layer['bias']\nh1 = tf.sigmoid(h1)\n\npredict = tf.matmul(h1, output_layer['weight']) + output_layer['bias']\n\nloss = tf.reduce_mean(tf.square(predict - y))\noptimizer = tf.train.GradientDescentOptimizer(0.00001).minimize(loss)\n\nsess = tf.Session()\ninit = tf.initialize_all_variables()\nsess.run(init)\n\nfor i in range(50000):\n\t\tif i % 5000 == 0:\n\t\t\tprint 'Loss: ', sess.run(loss, {x: x_data, y: y_data})\n\t\tsess.run(optimizer, {x: x_data, y: y_data})\n\nprint sess.run(predict, {x: [[1, 1]] })\n"""
basics/basic_operations.py,3,b'from __future__ import print_function\n\nimport tensorflow as tf\n\n# Start tf session\nsess = tf.Session()\n\na = tf.constant(2)\nb = tf.constant(3)\n\nc = a+b\n\n# Print out operation everything is operation\nprint(a)\nprint(b)\nprint(c)\n\nprint(a+b)\n\n\n# Print out the result of operation\nprint(sess.run(a))\nprint(sess.run(b))\nprint(sess.run(c))\nprint(sess.run(a+b))\n'
basics/gradient_loss_and_weight.py,0,"b""# Basics of Neural Networking\nimport numpy as np\n\nno_inputs = 5\nx_data = np.array([1, 2, 3, 4, 5])\ny_data = np.array([2, 4, 6, 8, 10])\nweight = 1\nbias = 0.0\nlearning_rate = 0.01\n\n# The basic formula is Y = ( w * X ) + B\n# Y is the outputs\n# w is the weight\n# X is the inputs\n# B is the bias\n\nfor i in range(10):\n    print 'Step', i, ':'\n    output = x_data * weight + bias\n    # [ 0.3  0.6  0.9  1.2  1.5]\n\n    difference = y_data - output\n\n    gradient = 0\n    for i in range(no_inputs):\n        gradient += difference[i] * x_data[i]\n\n    gradient /= no_inputs\n\n    loss = (difference * difference) / no_inputs\n    print 'Loss:', np.sum(loss)\n\n    weight = weight + (learning_rate * gradient)\n\n    print 'Weight: ', weight, '\\n'\n"""
basics/using_placeholders.py,5,"b""from __future__ import print_function\n\n\nimport tensorflow as tf\n\na = tf.placeholder(tf.int16)\nb = tf.placeholder(tf.int16)\n\nadd = tf.add(a, b)\nmul = tf.multiply(a, b)\n\n# Same op?\nprint(add)\nprint(a + b)\nprint(mul)\nprint(a * b)\n\n# Launch the default graph\nwith tf.Session() as sess:\n    print(sess.run(add, feed_dict={a: 2, b: 3}))\n\n    # it's work!\n    feed = {a: 3, b: 5}\n    print(sess.run(mul, feed_dict=feed))\n"""
classification/softmax_classification.py,8,"b'from __future__ import print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfilepath = os.path.abspath(__file__)\nPWDPATH = os.path.dirname(filepath)\nDATAPATH = PWDPATH + \'/softmax_train.txt\'\n\nxy = np.loadtxt(DATAPATH, unpack=True, dtype=\'float32\')\nx_data = np.transpose(xy[0:3])\ny_data = np.transpose(xy[3:])\n\n\nX = tf.placeholder(""float"", [None, 3])\nY = tf.placeholder(""float"", [None, 3])\n\nW = tf.Variable(tf.zeros([3, 3]))\n\n# matrix shape X=[8, 3], W=[3, 3]\nhypothesis = tf.nn.softmax(tf.matmul(X, W))\n\nlearning_rate = 0.001\n\ncost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n\n    for step in range(2001):\n        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n        if step % 200 == 0:\n            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n'"
costs_and_gradients/hand_made_descent.py,7,"b'from __future__ import print_function\n\n\nimport tensorflow as tf\n\nx_data = [1, 2, 3]\ny_data = [2, 4, 6]\n\nW = tf.Variable(tf.random_uniform([1], -10.0, 10.0))\n\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nhypothesis = W * X\n\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\nlr = 0.1\ndescent = W - tf.multiply(lr, tf.reduce_mean(tf.multiply((tf.multiply(W, X) - Y), X)))\ntrain = W.assign(descent)\n\ninit = tf.global_variables_initializer()\n\nsess = tf.Session()\nsess.run(init)\n\nfor step in range(2001):\n    sess.run(train, feed_dict={X: x_data, Y: y_data})\n    if step % 20 == 0:\n        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n\nprint(sess.run(hypothesis, feed_dict={X: 5}))\nprint(sess.run(hypothesis, feed_dict={X: 2.5}))\n'"
costs_and_gradients/plotting_cost.py,5,"b""from __future__ import print_function\n\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\n\n# Graph Input\nX = [1., 2., 3.]\nY = [2., 4., 6.]\nn_samples = len(X)\n\n# model weight\nW = tf.placeholder(tf.float32)\n\n# Construct a linear model\nhypothesis = tf.multiply(X, W)\n\n# Cost function\n# for each y,\n# c = sum(y' - y)^2\n# take mean value\n# c / n_samples\ncost = tf.reduce_sum(tf.pow(hypothesis - Y, 2)) / n_samples\n\ninit = tf.global_variables_initializer()\n\n# for graphs\nW_val = []\ncost_val = []\n\n# Launch the graphs\nsess = tf.Session()\nsess.run(init)\n\nfor i in range(-30, 50):\n    print(i * -0.1, sess.run(cost, feed_dict={W: i * 0.1}))\n    W_val.append(i * 0.1)\n    cost_val.append(sess.run(cost, feed_dict={W: i * 0.1}))\n\nplt.plot(W_val, cost_val, 'ro')\nplt.ylabel('cost')\nplt.xlabel('W')\nplt.show()\n"""
regression/linear_regression.py,9,"b'from __future__ import print_function\n\nimport tensorflow as tf\n\nx_data = [1, 2, 3]\ny_data = [2, 4, 6]\n\nW = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\nb = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nhypothesis = W * X + b\n\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\na = tf.Variable(0.1)\noptimizer = tf.train.GradientDescentOptimizer(a)\ntrain = optimizer.minimize(cost)\n\ninit = tf.global_variables_initializer()\n\nsess = tf.Session()\nsess.run(init)\n\nfor step in range(2001):\n    sess.run(train, feed_dict={X: x_data, Y: y_data})\n    if step % 20 == 0:\n        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W), sess.run(b))\n\nprint(sess.run(hypothesis, feed_dict={X: 5}))\nprint(sess.run(hypothesis, feed_dict={X: 2.5}))\n'"
regression/logistic_regression.py,9,"b""from __future__ import print_function\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nfrom matplotlib import pyplot as plt\n\nfilepath = os.path.abspath(__file__)\nPWDPATH = os.path.dirname(filepath)\nDATAPATH = PWDPATH + '/logistic_train.txt'\n\nxy = np.loadtxt(DATAPATH, unpack=True, dtype='float32')\nx_data = xy[0:-1]\ny_data = xy[-1]\n\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))\n\nh = tf.matmul(W, X)\nhypothesis = tf.div(1., 1. + tf.exp(-h))\n\ncost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n\nlearning_rate = 0.1\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\ntrain = optimizer.minimize(cost)\n\ninit = tf.global_variables_initializer()\n\nW_val = []\ncost_val = []\n\nsess = tf.Session()\nsess.run(init)\n\nfor step in range(2001):\n    sess.run(train, feed_dict={X: x_data, Y: y_data})\n\n    W_val.append(sess.run(W)[0])\n    cost_val.append(sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n\n    if step % 20 == 0:\n        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W)[0])\n\nfig, (ax) = plt.subplots(1, 1)\nax.set_ylim(0, 10)\nax.plot(W_val, cost_val)\nplt.ylabel(sess.run(W))\nplt.xlabel('W')\nplt.show()\n"""
convolution_networks/BasicCnnModel/__init__.py,19,"b'import tensorflow as tf\nimport numpy as np\nimport os\n\nfrom scipy.ndimage import *\nfrom config import *\n\n\ndef load_images():\n  data = {""images"":[], ""labels"":[]}\n  horses = [file for file in os.listdir(RESIZED_IMAGE_FOLDER) if file.startswith(""horse"") ]\n  dogs = [file for file in os.listdir(RESIZED_IMAGE_FOLDER) if file.startswith(""dog"") ]\n  for horse in horses:\n    image = imread(os.path.join(RESIZED_IMAGE_FOLDER, horse), mode=\'L\')\n    data[\'images\'].append(image)\n    data[\'labels\'].append([1,0])\n\n  for dog in dogs:\n    image = imread(os.path.join(RESIZED_IMAGE_FOLDER, dog), mode=\'L\')\n    data[\'images\'].append(image)\n    data[\'labels\'].append([0,1])\n  return data\n\ndef new_weights(shape):\n  return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n \ndef new_biases(length):\n  return tf.Variable(tf.constant(0.05, shape=[length]))\n \ndef convolution_layer(input, num_of_input_channels, filter_size, num_of_filters, pooling=True):\n  shape = [filter_size, filter_size,  num_of_input_channels, num_of_filters]\n  weights = new_weights(shape=shape)\n  biases = new_biases(length=num_of_filters)\n  layer = tf.nn.conv2d(input=input, filter=weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n  layer += biases\n\n  if pooling:\n      layer = max_pooling_layer(layer)\n  layer = tf.nn.relu(layer)\n  return layer, weights\n\ndef max_pooling_layer(layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\'):\n  return tf.nn.max_pool(layer, ksize, strides, padding)\n\n\ndef flatten_layer(layer):\n  layer_shape = layer.get_shape()\n  num_features = layer_shape[1:4].num_elements()\n\n  layer_flat = tf.reshape(layer, [-1, num_features])\n  return layer_flat, num_features\n \ndef fully_connected_layer(input, num_inputs, num_outputs, relu=True): \n    weights = new_weights(shape=[num_inputs, num_outputs])\n    biases = new_biases(length=num_outputs)\n \n    layer = tf.matmul(input, weights) + biases\n    if relu:\n        layer = tf.nn.relu(layer)\n \n    return layer\n\n\n\ndata = load_images()\nimages, labels = data[\'images\'], data[\'labels\']\nclasses = [\'horse\', \'dog\']\nnum_classes = 2\nimg_size = 128\nnum_channels = 1\nfilter_size = 3\nnum_filters = 1\nepochs = 100000\n\nimg_size_flat= img_size * img_size * num_channels\nx = tf.placeholder(tf.float32, shape=[None, img_size_flat], name=\'x\')\nx_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\ny_true = tf.placeholder(tf.float32, shape=[None, num_classes], name=\'y_true\')\ny_true_cls = tf.argmax(y_true, dimension=1)\n\n\n"""""" \nThe Model\n""""""\nlayer_conv, weights_conv = convolution_layer(x_image, num_channels, filter_size, num_filters, pooling=True)\nlayer_flat, num_features = flatten_layer(layer_conv)\nlayer_fc = fully_connected_layer(layer_flat, num_features, num_classes, relu=True)\n\ny_pred_cls = tf.argmax(layer_fc, 1)\ncorrect_prediction = tf.equal(y_pred_cls, y_true_cls)\n\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc, labels=y_true)\ncost = tf.reduce_sum(cross_entropy)\noptimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n\n"""""" The Process """"""\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  for i in range(epochs):\n    if(i % 10000) == 0:\n      print \'\\nEpoch {0}\'.format(i)\n      sess.run(optimizer, {x: np.array(images).reshape(-1, 128*128),  y_true:np.array(labels)})\n      print ""Loss is {0}"".format(sess.run(cost, {x: np.array(images).reshape(-1, 128*128),  y_true:np.array(labels)} ))\n\n      result = sess.run(correct_prediction, {x: np.array(images).reshape(-1, 128*128),  y_true:np.array(labels)} )\n      print ""The number of right outputs: {0} out of {1}"".format(sum(result), len(result))'"
convolution_networks/BasicCnnModel/config.py,0,"b""import os\n\nCURR_FOLDER = os.path.dirname(os.path.realpath(__file__))\nIMAGE_FOLDER = os.path.join(CURR_FOLDER, 'images')\nRESIZED_IMAGE_FOLDER = os.path.join(CURR_FOLDER, 'resized')"""
convolution_networks/BasicCnnModel/image_resizer.py,0,"b'from resizeimage import resizeimage\nimport Image\nimport os\nfrom config import *\n\nprint IMAGE_FOLDER\n\nfiles = [file for file in os.listdir(IMAGE_FOLDER) if file.endswith("".jpg"") ]\n\nfor file in files:\n  with Image.open(os.path.join(IMAGE_FOLDER, file)).convert(\'LA\') as image:\n    resizeimage.resize_contain(image, [128,128]).save(os.path.join(RESIZED_IMAGE_FOLDER, file))\n'"
convolution_networks/Cifar10_img_recognition/__main__.py,22,"b'import tensorflow as tf\nimport numpy as np\nfrom conf import *\nfrom cifar10 import *\n\n# Working of our algorithm is as follows:\n# Conv1_layer -> Conv2_layer -> Flatten_layer -> FullyConnected_layer -> FullyConnected_layer(With 10 Classes)\n\n# Reading the data\nimage_data, image_cls, img_one_hot_cls = (load_training_data())\n\nimage_data_flat = image_data.reshape([-1,3072])\n\n# Function for defining weights\ndef new_weights(shape):\n\treturn tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n\n# Function for defining biases\ndef new_bias(length):\n\treturn tf.Variable(tf.constant(0.5, shape=[length]))\n\n# Function to create the convolution layer with/without max-pooling\ndef new_conv_layer(input, num_input_channels, filter_size, num_filters, use_pooling=True):\n\tshape = [filter_size, filter_size, num_input_channels, num_filters]\n\tweights = new_weights(shape = shape)\n\tbiases = new_bias(length = num_filters)\n\n\t# tf.nn.conv2d needs a 4D input\n\tlayer = tf.nn.conv2d(input = input, filter= weights, strides=[1,1,1,1], padding=\'SAME\')\n\tlayer += biases\n\tif use_pooling:\n\t\tlayer = tf.nn.max_pool(value = layer, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\'SAME\')\n\t# relu activation function converts all negatives to zero\n\tlayer = tf.nn.relu(layer)\n\treturn layer, weights\n\n# After all convolutions, we need to flatten the layer\ndef flatten_layer(layer):\n\tlayer_shape = layer.get_shape()\n\tnum_features = layer_shape[1:4].num_elements()\n\tlayer_flat = tf.reshape(layer, [-1, num_features])\n\treturn layer_flat, num_features\n\n# Fully connected layer\ndef new_fc_layer(input, num_inputs, num_outputs, use_relu=True):\n\tweights = new_weights(shape=[num_inputs, num_outputs])\n\tbiases = new_bias(length= num_outputs)\n\tlayer = tf.matmul(input, weights) + biases\n\tif use_relu:\n\t\tlayer = tf.nn.relu(layer)\n\treturn layer\n\n\n# The placeholder to hold the X and Y values while training\nx = tf.placeholder(tf.float32, shape=[None, img_size_flat], name=\'x\')\ny_true = tf.placeholder(tf.float32, shape=[None, 10], name=\'y_true\')\n\nx_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\ny_true_cls = tf.argmax(y_true, dimension=1)\n\n# The beginning of the process\nlayer_conv1, weights_conv1 = new_conv_layer(input = x_image, num_input_channels= num_channels, filter_size = filter1_size, num_filters = number_of_filter1, use_pooling=True)\nlayer_conv2, weights_conv2 = new_conv_layer(input = layer_conv1, num_input_channels= number_of_filter1, filter_size = filter2_size, num_filters = number_of_filter2, use_pooling=True)\nlayer_flat, num_features = flatten_layer(layer_conv2)\nlayer_fc1 = new_fc_layer(layer_flat, num_features, fc_size, True)\nlayer_fc2 = new_fc_layer(layer_fc1, fc_size, num_classes, False)\n\n# Finally Softmax function\ny_pred = tf.nn.softmax(layer_fc2)\ny_pred_cls = tf.argmax(y_pred, dimension=1)\n\n# Cost function calculation and optimization function\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,labels=y_true)\ncost = tf.reduce_mean(cross_entropy)\noptimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n\n# Checking for the right predictions\ncorrect_prediction = tf.equal(y_pred_cls, y_true_cls)\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n# TF Session initiation\nsession = tf.Session()\nsession.run(tf.global_variables_initializer())\n\n# The trainer function to iterate the training process to learn further\ndef optimize(num_iterations):\n\tx_batch, y_true_batch = image_data_flat, img_one_hot_cls\n\tfeed_dict_train = {x: x_batch[:500], y_true: y_true_batch[:500]}\n\tfeed_dict_test = {x: x_batch[500:1000], y_true: y_true_batch[500:1000]}\n\tfor i in range(num_iterations):\n\t\tsession.run(optimizer, feed_dict=feed_dict_train)\n\t\t# Print status every 10 iterations.\n\t\tif i % 10 == 0:\n\t\t# Calculate the accuracy on the training-set.\n\t\t\tacc = session.run(accuracy, feed_dict=feed_dict_test)\n\n\t\t# Message for printing.\n\t\t\tprint ""Step "",i+1,\': \', acc*100\n\noptimize(STEPS)\n\n# test_data = image_data[1115].reshape([1,3072])\n# print image_cls[1115]\n# print session.run(y_pred_cls, {x: test_data})\n'"
convolution_networks/Cifar10_img_recognition/cifar10.py,0,"b'########################################################################\n#\n# Functions for downloading the CIFAR-10 data-set from the internet\n# and loading it into memory.\n#\n# Implemented in Python 3.5\n#\n# Usage:\n# 1) Set the variable data_path with the desired storage path.\n# 2) Call maybe_download_and_extract() to download the data-set\n#    if it is not already located in the given data_path.\n# 3) Call load_class_names() to get an array of the class-names.\n# 4) Call load_training_data() and load_test_data() to get\n#    the images, class-numbers and one-hot encoded class-labels\n#    for the training-set and test-set.\n# 5) Use the returned data in your own program.\n#\n# Format:\n# The images for the training- and test-sets are returned as 4-dim numpy\n# arrays each with the shape: [image_number, height, width, channel]\n# where the individual pixels are floats between 0.0 and 1.0.\n#\n########################################################################\n#\n# This file is part of the TensorFlow Tutorials available at:\n#\n# https://github.com/Hvass-Labs/TensorFlow-Tutorials\n#\n# Published under the MIT License. See the file LICENSE for details.\n#\n# Copyright 2016 by Magnus Erik Hvass Pedersen\n#\n########################################################################\n\nimport numpy as np\nimport pickle\nimport os\nimport download\n\n########################################################################\n\n# Directory where you want to download and save the data-set.\n# Set this before you start calling any of the functions below.\ndata_path = ""data/cifar10/""\n\n# URL for the data-set on the internet.\ndata_url = ""https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz""\n\n########################################################################\n# Various constants for the size of the images.\n# Use these constants in your own program.\n\n# Width and height of each image.\nimg_size = 32\n\n# Number of channels in each image, 3 channels: Red, Green, Blue.\nnum_channels = 3\n\n# Length of an image when flattened to a 1-dim array.\nimg_size_flat = img_size * img_size * num_channels\n\n# Number of classes.\nnum_classes = 10\n\n########################################################################\n# Various constants used to allocate arrays of the correct size.\n\n# Number of files for the training-set.\n_num_files_train = 1\n\n# Number of images for each batch-file in the training-set.\n_images_per_file = 10000\n\n# Total number of images in the training-set.\n# This is used to pre-allocate arrays for efficiency.\n_num_images_train = _num_files_train * _images_per_file\n\n########################################################################\n# Private functions for downloading, unpacking and loading data-files.\n\n\ndef _get_file_path(filename=""""):\n    """"""\n    Return the full path of a data-file for the data-set.\n\n    If filename=="""" then return the directory of the files.\n    """"""\n\n    return os.path.join(data_path, filename)\n\n\ndef one_hot_encoded(class_numbers, num_classes=None):\n    """"""\n    Generate the One-Hot encoded class-labels from an array of integers.\n\n    For example, if class_number=2 and num_classes=4 then\n    the one-hot encoded label is the float array: [0. 0. 1. 0.]\n\n    :param class_numbers:\n        Array of integers with class-numbers.\n        Assume the integers are from zero to num_classes-1 inclusive.\n\n    :param num_classes:\n        Number of classes. If None then use max(cls)-1.\n\n    :return:\n        2-dim array of shape: [len(cls), num_classes]\n    """"""\n\n    # Find the number of classes if None is provided.\n    if num_classes is None:\n        num_classes = np.max(class_numbers) - 1\n\n    return np.eye(num_classes, dtype=float)[class_numbers]\n\n\ndef _unpickle(filename):\n    """"""\n    Unpickle the given file and return the data.\n\n    Note that the appropriate dir-name is prepended the filename.\n    """"""\n\n    # Create full path for the file.\n    file_path = _get_file_path(filename)\n\n    print(""Loading data: "" + file_path)\n\n    with open(file_path, mode=\'rb\') as file:\n        # In Python 3.X it is important to set the encoding,\n        # otherwise an exception is raised here.\n        data = pickle.load(file)\n\n    return data\n\n\ndef _convert_images(raw):\n    """"""\n    Convert images from the CIFAR-10 format and\n    return a 4-dim array with shape: [image_number, height, width, channel]\n    where the pixels are floats between 0.0 and 1.0.\n    """"""\n\n    # Convert the raw images from the data-files to floating-points.\n    raw_float = np.array(raw, dtype=float) / 255.0\n\n    # Reshape the array to 4-dimensions.\n    images = raw_float.reshape([-1, num_channels, img_size, img_size])\n\n    # Reorder the indices of the array.\n    images = images.transpose([0, 2, 3, 1])\n\n    return images\n\n\ndef _load_data(filename):\n    """"""\n    Load a pickled data-file from the CIFAR-10 data-set\n    and return the converted images (see above) and the class-number\n    for each image.\n    """"""\n\n    # Load the pickled data-file.\n    data = _unpickle(filename)\n\n    # Get the raw images.\n    raw_images = data[b\'data\']\n\n    # Get the class-numbers for each image. Convert to numpy-array.\n    cls = np.array(data[b\'labels\'])\n\n    # Convert the images.\n    images = _convert_images(raw_images)\n\n    return images, cls\n\n\n########################################################################\n# Public functions that you may call to download the data-set from\n# the internet and load the data into memory.\n\n\ndef maybe_download_and_extract():\n    """"""\n    Download and extract the CIFAR-10 data-set if it doesn\'t already exist\n    in data_path (set this variable first to the desired path).\n    """"""\n\n    download.maybe_download_and_extract(url=data_url, download_dir=data_path)\n\n\ndef load_class_names():\n    """"""\n    Load the names for the classes in the CIFAR-10 data-set.\n\n    Returns a list with the names. Example: names[3] is the name\n    associated with class-number 3.\n    """"""\n\n    # Load the class-names from the pickled file.\n    raw = _unpickle(filename=""batches.meta"")[b\'label_names\']\n\n    # Convert from binary strings.\n    names = [x.decode(\'utf-8\') for x in raw]\n\n    return names\n\n\ndef load_training_data():\n    """"""\n    Load all the training-data for the CIFAR-10 data-set.\n\n    The data-set is split into 5 data-files which are merged here.\n\n    Returns the images, class-numbers and one-hot encoded class-labels.\n    """"""\n\n    # Pre-allocate the arrays for the images and class-numbers for efficiency.\n    images = np.zeros(shape=[_num_images_train, img_size, img_size, num_channels], dtype=float)\n    cls = np.zeros(shape=[_num_images_train], dtype=int)\n\n    # Begin-index for the current batch.\n    begin = 0\n\n    # For each data-file.\n    for i in range(_num_files_train):\n        # Load the images and class-numbers from the data-file.\n        images_batch, cls_batch = _load_data(filename=""data_batch_"" + str(i + 1))\n\n        # Number of images in this batch.\n        num_images = len(images_batch)\n\n        # End-index for the current batch.\n        end = begin + num_images\n\n        # Store the images into the array.\n        images[begin:end, :] = images_batch\n\n        # Store the class-numbers into the array.\n        cls[begin:end] = cls_batch\n\n        # The begin-index for the next batch is the current end-index.\n        begin = end\n\n    return images, cls, one_hot_encoded(class_numbers=cls, num_classes=num_classes)\n\n\ndef load_test_data():\n    """"""\n    Load all the test-data for the CIFAR-10 data-set.\n\n    Returns the images, class-numbers and one-hot encoded class-labels.\n    """"""\n\n    images, cls = _load_data(filename=""test_batch"")\n\n    return images, cls, one_hot_encoded(class_numbers=cls, num_classes=num_classes)\n\n########################################################################\n'"
convolution_networks/Cifar10_img_recognition/conf.py,0,b'# Convolutional Layer 1\nfilter1_size = 5\nnumber_of_filter1 = 16\n\n# Convolutional Layer 2\nfilter2_size = 5\nnumber_of_filter2 = 36\n\n# Fully Connected Layer \nfc_size = 128\n\n# No of training steps\nSTEPS = 100'
convolution_networks/MNIST_img_recognition/__main__.py,22,"b'import tensorflow as tf\nimport numpy as np\nfrom conf import *\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n\n# Working of our algorithm is as follows:\n# Conv1_layer -> Conv2_layer -> Flatten_layer -> FullyConnected_layer -> FullyConnected_layer (With 10 Classes)\n\n# Reading handwritten digits from MNIST dataset\ndata = input_data.read_data_sets(\'data/MNIST/\', one_hot = True)\n# The informations about image dataset\nimg_size = 28\nimg_size_flat = 28 * 28\nimg_shape = (img_size, img_size)\nnum_channels = 1\nclasses = 10\ntrain_batch_size = 64\n\n# Function for defining weights\ndef new_weights(shape):\n\treturn tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n\n# Function for defining biases\ndef new_bias(length):\n\treturn tf.Variable(tf.constant(0.5, shape=[length]))\n\n# The filter is applied to image patches of the same size as the filter and\n# >strided< according to the strides argument.\n# strides = [1, 1, 1, 1] applies the filter to a patch at every offset,\n# strides = [1, 2, 2, 1] applies the filter to every other image patch in each dimension\n\n# Max Pooling Layer\n# Takes a pool of data (small rectangular boxes) form the convolutional layer\n# And then subsamples them to produce a single output block\n# Basically used to chuck out values that are insignificant to output, and\n# retains only the ones that are necessary\n# K is 2 so that a nice square is strided over\n\n# 1 1 2 4               {max(1,1,5,6) => 6}\n# 5 6 7 8 ====> 6 8     {max(5,6,7,8) => 8}\n# 3 2 1 0 ====> 3 4     {max(3,2,1,0) => 3}\n# 1 2 3 4               {max(1,2,3,4) => 4}\n\n# Typical values are 2x2 or no max-pooling. Very large input images may warrant\n# 4x4 pooling in the lower-layers. Keep in mind however, that this will reduce\n# the dimension of the signal by a factor of 16, and may result in throwing away\n# too much information.\n\n# Function to create the convolution layer with/without max-pooling\ndef new_conv_layer(input, num_input_channels, filter_size, num_filters, use_pooling=True):\n\tshape = [filter_size, filter_size, num_input_channels, num_filters]\n\tweights = new_weights(shape = shape)\n\tbiases = new_bias(length = num_filters)\n\n\t# tf.nn.conv2d needs a 4D input\n\tlayer = tf.nn.conv2d(input = input, filter= weights, strides=[1,1,1,1], padding=\'SAME\')\n\tlayer += biases\n\tif use_pooling:\n\t\tlayer = tf.nn.max_pool(value = layer, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\'SAME\')\n\t# relu activation function converts all negatives to zero\n\tlayer = tf.nn.relu(layer)\n\treturn layer, weights\n\n# After all convolutions, we need to flatten the layer\ndef flatten_layer(layer):\n\tlayer_shape = layer.get_shape()\n\tnum_features = layer_shape[1:4].num_elements()\n\tlayer_flat = tf.reshape(layer, [-1, num_features])\n\treturn layer_flat, num_features\n\n# Fully connected layer\ndef new_fc_layer(input, num_inputs, num_outputs, use_relu=True):\n\tweights = new_weights(shape=[num_inputs, num_outputs])\n\tbiases = new_bias(length= num_outputs)\n\tlayer = tf.matmul(input, weights) + biases\n\tif use_relu:\n\t\tlayer = tf.nn.relu(layer)\n\treturn layer\n\n\n# The placeholder to hold the X and Y values while training\nx = tf.placeholder(tf.float32, shape=[None, img_size_flat], name=\'x\')\ny_true = tf.placeholder(tf.float32, shape=[None, 10], name=\'y_true\')\n\nx_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\ny_true_cls = tf.argmax(y_true, dimension=1)\n\n# The beginning of the process\nlayer_conv1, weights_conv1 = new_conv_layer(input = x_image, num_input_channels= num_channels, filter_size = filter1_size, num_filters = number_of_filter1, use_pooling=True)\nlayer_conv2, weights_conv2 = new_conv_layer(input = layer_conv1, num_input_channels= number_of_filter1, filter_size = filter2_size, num_filters = number_of_filter2, use_pooling=True)\nlayer_flat, num_features = flatten_layer(layer_conv2)\nlayer_fc1 = new_fc_layer(layer_flat, num_features, fc_size, True)\nlayer_fc2 = new_fc_layer(layer_fc1, fc_size, classes, False)\n\n# Finally Softmax function\ny_pred = tf.nn.softmax(layer_fc2)\ny_pred_cls = tf.argmax(y_pred, dimension=1)\n\n# Cost function calculation and optimization function\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,labels=y_true)\ncost = tf.reduce_mean(cross_entropy)\noptimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n\n# Checking for the right predictions\ncorrect_prediction = tf.equal(y_pred_cls, y_true_cls)\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n# TF Session initiation\nsession = tf.Session()\nsession.run(tf.global_variables_initializer())\n\n# Counter for total number of iterations performed so far.\ntotal_iterations = 0\n\n# The trainer function to iterate the training process to learn further\ndef optimize(num_iterations):\n    for i in range(num_iterations):\n        x_batch, y_true_batch = data.train.next_batch(train_batch_size)\n        feed_dict_train = {x: x_batch, y_true: y_true_batch}\n        session.run(optimizer, feed_dict=feed_dict_train)\n\n        # Print status every 100 iterations.\n        if i % 100 == 0:\n            # Calculate the accuracy on the training-set.\n            acc = session.run(accuracy, feed_dict=feed_dict_train)\n\n            # Message for printing.\n            print ""Step "",i+1,\': \', acc*100\n\n\noptimize(STEPS)\n\n\n# test_number = data.train._images[2]\n# test_number = test_number.reshape([1,784])\n# print np.argmax(data.train._labels[2])\n# print session.run(y_pred_cls, {x: test_number})\n'"
convolution_networks/MNIST_img_recognition/conf.py,0,b'# Convolutional Layer 1\nfilter1_size = 5\nnumber_of_filter1 = 16\n\n# Convolutional Layer 2\nfilter2_size = 5\nnumber_of_filter2 = 36\n\n# Fully Connected Layer \nfc_size = 128\n\n# No of training steps\nSTEPS = 101'
projects/predicting_house_prices/__main__.py,25,"b'import tensorflow as tf \nimport pickle as pk\nimport numpy as np \nimport pandas as pd\nfrom conf import *\n\nclass NeuralNet:\n\tdef __init__(self):\n\t\tself.declare_all_small_things()\n\t\tself.initialize_variables()\n\n\tdef declare_all_small_things(self):\n\t\t# Reading and storing using Pandas\n\t\tx = pd.read_csv(FEATURES_CSV, header=None)\n\t\tx_data = x.values.astype(np.float32)\n\t\ty = pd.read_csv(OUTCOMES_CSV, header=None)\n\t\ty_data = y.values.astype(np.float32)\n\n\t\t# Splitting the train & test data\n\t\tself.x_train_data, self.x_test_data = x_data[:SPLIT], x_data[SPLIT:]\n\t\tself.y_train_data, self.y_test_data = y_data[:SPLIT], y_data[SPLIT:]\n\n\t\t# Some Constants Needed for the NN\n\t\tinput_size = 13\n\t\th1_size = 8\n\t\th2_size = 8\n\t\toutput_size = 1\n\t\tself.epochs = EPOCHS\n\t\t# End of the Constants\n\n\t\t# Our Network layers\n\t\tself.hidden_layer_1 = { \'weight\': tf.Variable(tf.random_normal([input_size,h1_size])),\n\t\t\t\t\t\t\t\t\t\t\t \'bias\': tf.Variable(tf.random_normal([1,h1_size])) }\n\t\tself.hidden_layer_2 = { \'weight\': tf.Variable(tf.random_normal([h1_size,h2_size])), \n\t\t\t\t\t\t\t\t\t\t\t \'bias\': tf.Variable(tf.random_normal([1,h2_size])) }\n\t\tself.output_layer = { \'weight\': tf.Variable(tf.random_normal([h2_size,output_size])), \n\t\t\t\t\t\t\t\t\t\t \'bias\': tf.Variable(tf.random_normal([1,output_size])) }\n\n\tdef neural_networking(self,input):\n\t\t# The process\n\t\th1 = tf.matmul(input, self.hidden_layer_1[\'weight\']) + self.hidden_layer_1[\'bias\']\n\t\th1 = tf.sigmoid(h1)\n\t\t# h1 = tf.nn.relu(h1)\n\t\th2 = tf.matmul(h1, self.hidden_layer_2[\'weight\']) + self.hidden_layer_2[\'bias\']\n\t\t# h2 = tf.sigmoid(h2)\n\t\th2 = tf.nn.relu(h2)\n\t\toutput = tf.matmul(h2, self.output_layer[\'weight\']) + self.output_layer[\'bias\']\n\t\toutput = tf.nn.dropout(output, 1)\n\t\treturn output\n\n\tdef initialize_variables(self):\n\t\t# TensorFlow placeholder and variables \n\t\tself.xs = tf.placeholder(tf.float32)\n\t\tself.ys = tf.placeholder(tf.float32)\n\n\t\t# Prediction \n\t\tself.predict = self.neural_networking(self.xs)\n\t\tdelta = tf.square(self.predict - self.ys)\n\t\tself.cost = tf.reduce_sum(delta)\n\t\toptimizer = tf.train.AdamOptimizer(0.0001)\n\t\tself.prediction = optimizer.minimize(self.cost)\n\n\t\t#Creating Session\n\t\tself.sess = tf.Session()\n\t\t\n\n\tdef train(self):\n\t\tself.sess.run(tf.initialize_all_variables())\n\t\tfor i in range(self.epochs):\n\t\t\tif i%100 == 0:\n\t\t\t\tprint ""Step: "", i\n\t\t\t\tprint \'LOSS: \', self.sess.run([self.cost], {self.xs:self.x_train_data, self.ys:self.y_train_data})\n\t\t\tself.sess.run(self.prediction, {self.xs:self.x_train_data, self.ys:self.y_train_data})\n\n\tdef accuracy(self):\n\t\t#Accuracy Calculation\n\t\top =  self.sess.run(self.predict, {self.xs: self.x_test_data})\n\t\tcounter = 0\n\t\tnum = LEN_DATA - SPLIT\n\t\tfor i in range(num):\n\t\t\tif(abs(self.y_test_data[i] -  op[i]) < 0.5):\n\t\t\t\tcounter +=  1\n\t\tprint \'Accuracy is :\', (counter * 100) / (num)\n\t\t\n\tdef save_model(self):\n\t\tsaver = tf.train.Saver()\n\t\tsave_path = saver.save(self.sess, MODEL_FILE)\n\t\tprint save_path\n\n\tdef load_model(self):\n\t\tsaver = tf.train.Saver()\n\t\tself.sess = tf.Session()\n\t\tself.sess.run(tf.initialize_all_variables())\n\t\tsaver.restore(self.sess, MODEL_FILE)\n\n\tdef normalize_user_input(self, user_inputs):\n\t\twith open(PICKLE_FILE) as file:\n\t\t\tmax_values = pk.load(file)\n\t\t\tmin_values = pk.load(file)\n\n\t\tfor i in range(len(user_inputs)):\n\t\t\t# Manual min-max scalar normalization\n\t\t\tuser_inputs[i] = ( user_inputs[i] - min_values[i] ) / (max_values[i] - min_values[i])\n\n\t\treturn user_inputs\n\n\n\n# You can run like this if needed\nx = NeuralNet()\nx.load_model()\nx.accuracy()\nx.train()\n# x.save_model()\nx.accuracy()\n\n\n\n'"
projects/predicting_house_prices/conf.py,0,"b""import os \nfilepath = os.path.abspath(__file__)\nPWDPATH = os.path.dirname(filepath)\n\nDATAPATH = os.path.join(PWDPATH , 'train.txt')\nMODEL_FILE = 'trained_model/model.ckpt'\nDATA_DIR = 'data'\nHOUSE_DATASET = os.path.join(PWDPATH , DATA_DIR , 'dataset.data')\nFEATURES_CSV = os.path.join(PWDPATH , DATA_DIR , 'features_normalized.csv')\nOUTCOMES_CSV  = os.path.join(PWDPATH , DATA_DIR , 'outcome.csv')\nPICKLE_FILE = os.path.join(PWDPATH, DATA_DIR, 'min_max.pkl')\nSPLIT = 250\nEPOCHS = 50000\nLEN_DATA = 506"""
projects/predicting_house_prices/normalize.py,0,"b""# Normalize our particular dataset for easy computation and faster loss minimization.\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom conf import *\nimport pickle\n\n# Reading the dataset\ndf = pd.read_csv(HOUSE_DATASET, sep=' ', header=None)\ndf.drop(df.columns[0],1, inplace=True)\n\n# Saving the features and outcomes as float32 type\nfeatures = df.drop(df.columns[13], 1).values.astype(np.float32)\n\noutcomes = df[13].values.astype(np.float32)\n# Normalizing the features and writing them to csv file\nfeatures_normalized = MinMaxScaler().fit_transform(features)\nfeatures_normalized = pd.DataFrame(features_normalized, index=None)\nfeatures_normalized.to_csv(FEATURES_CSV, header=False, index=False)\n\n# Writing them to csv file\noutcomes = pd.DataFrame(outcomes, index=None)\noutcomes.to_csv(OUTCOMES_CSV, header=False, index=False)\n\n# Saving the max and min value to use while prediction\nfeatures_max_data = features.max(0)\nfeatures_min_data = features.min(0)\n\n# Using pickle to store the max and min values\nwith open('data/min_max.pkl', 'w') as file:\n\tpickle.dump(features_max_data, file)\n\tpickle.dump(features_min_data, file)\n\n\n\n\n\n"""
projects/predicting_onset_of_diabetes/__main__.py,25,"b'import tensorflow as tf \nimport pickle as pk\nimport numpy as np \nimport pandas as pd\nfrom conf import *\n\nclass NeuralNet:\n\tdef __init__(self):\n\t\tself.declare_all_small_things()\n\t\tself.initialize_variables()\n\n\tdef declare_all_small_things(self):\n\t\t# Reading and storing using Pandas\n\t\tx = pd.read_csv(FEATURES_CSV, header=None)\n\t\tx_data = x.values.astype(np.float32)\n\t\ty = pd.read_csv(OUTCOMES_CSV, header=None)\n\t\ty_data = y.values.astype(np.float32)\n\n\t\t# Splitting the train & test data\n\t\tself.x_train_data, self.x_test_data = x_data[:SPLIT], x_data[SPLIT:]\n\t\tself.y_train_data, self.y_test_data = y_data[:SPLIT], y_data[SPLIT:]\n\n\t\t# Some Constants Needed for the NN\n\t\tinput_size = 8\n\t\th1_size = 8\n\t\th2_size = 8\n\t\toutput_size = 2\n\t\tself.epochs = EPOCHS\n\t\t# End of the Constants\n\n\t\t# Our Network layers\n\t\tself.hidden_layer_1 = { \'weight\': tf.Variable(tf.random_normal([input_size,h1_size])),\n\t\t\t\t\t\t\t\t\t\t\t \'bias\': tf.Variable(tf.random_normal([1,h1_size])) }\n\t\tself.hidden_layer_2 = { \'weight\': tf.Variable(tf.random_normal([h1_size,h2_size])), \n\t\t\t\t\t\t\t\t\t\t\t \'bias\': tf.Variable(tf.random_normal([1,h2_size])) }\n\t\tself.output_layer = { \'weight\': tf.Variable(tf.random_normal([h2_size,output_size])), \n\t\t\t\t\t\t\t\t\t\t \'bias\': tf.Variable(tf.random_normal([1,output_size])) }\n\n\tdef neural_networking(self,input):\n\t\t# The process\n\t\th1 = tf.matmul(input, self.hidden_layer_1[\'weight\']) + self.hidden_layer_1[\'bias\']\n\t\th1 = tf.sigmoid(h1)\n\t\t# h1 = tf.nn.relu(h1)\n\t\th2 = tf.matmul(h1, self.hidden_layer_2[\'weight\']) + self.hidden_layer_2[\'bias\']\n\t\t# h2 = tf.sigmoid(h2)\n\t\th2 = tf.nn.relu(h2)\n\t\toutput = tf.matmul(h2, self.output_layer[\'weight\']) + self.output_layer[\'bias\']\n\t\toutput = tf.nn.dropout(output, 1)\n\t\treturn output\n\n\tdef initialize_variables(self):\n\t\t# TensorFlow placeholder and variables \n\t\tself.xs = tf.placeholder(tf.float32)\n\t\tself.ys = tf.placeholder(tf.float32)\n\n\t\t# Prediction \n\t\tself.predict = self.neural_networking(self.xs)\n\t\tdelta = tf.square(self.predict - self.ys)\n\t\tself.cost = tf.reduce_sum(delta)\n\t\toptimizer = tf.train.AdamOptimizer(0.01)\n\t\tself.prediction = optimizer.minimize(self.cost)\n\n\t\t#Creating Session\n\t\tself.sess = tf.Session()\n\t\t\n\n\tdef train(self):\n\t\tself.sess.run(tf.initialize_all_variables())\n\t\tfor i in range(self.epochs):\n\t\t\tif i%100 == 0:\n\t\t\t\tprint ""Step: "", i\n\t\t\t\tprint \'LOSS: \', self.sess.run([self.cost], {self.xs:self.x_train_data, self.ys:self.y_train_data})\n\t\t\tself.sess.run(self.prediction, {self.xs:self.x_train_data, self.ys:self.y_train_data})\n\n\tdef accuracy(self):\n\t\t#Accuracy Calculation\n\t\top =  self.sess.run(self.predict, {self.xs: self.x_test_data})\n\t\tcounter = 0\n\t\tnum = 766 - SPLIT\n\t\tfor i in range(num):\n\t\t\tif(np.argmax(self.y_test_data[i]) == np.argmax(op[i])):\n\t\t\t\tcounter +=  1\n\t\tprint \'Accuracy is :\', (counter * 100) / (num)\n\t\t\n\tdef save_model(self):\n\t\tsaver = tf.train.Saver()\n\t\tsave_path = saver.save(self.sess, MODEL_FILE)\n\t\tprint save_path\n\n\tdef load_model(self):\n\t\tsaver = tf.train.Saver()\n\t\tself.sess = tf.Session()\n\t\tself.sess.run(tf.initialize_all_variables())\n\t\tsaver.restore(self.sess, MODEL_FILE)\n\n\tdef normalize_user_input(self, user_inputs):\n\t\twith open(PICKLE_FILE) as file:\n\t\t\tmax_values = pk.load(file)\n\t\t\tmin_values = pk.load(file)\n\n\t\tfor i in range(len(user_inputs)):\n\t\t\t# Manual min-max scalar normalization\n\t\t\tuser_inputs[i] = ( user_inputs[i] - min_values[i] ) / (max_values[i] - min_values[i])\n\n\t\treturn user_inputs\n\n\n\n# You can run like this if needed\n# x = NeuralNet()\n# x.load_model()\n# x.accuracy()\n# x.train()\n# x.save_model()\n# x.accuracy()\n\n\n\n'"
projects/predicting_onset_of_diabetes/conf.py,0,"b""import os \nfilepath = os.path.abspath(__file__)\nPWDPATH = os.path.dirname(filepath)\n\nDATAPATH = os.path.join(PWDPATH , 'train.txt')\nMODEL_FILE = 'trained_model/model.ckpt'\nDATA_DIR = 'data'\nDIABETES_DATASET = os.path.join(PWDPATH , DATA_DIR , 'dataset.data')\nFEATURES_CSV = os.path.join(PWDPATH , DATA_DIR , 'features_normalized.csv')\nOUTCOMES_CSV  = os.path.join(PWDPATH , DATA_DIR , 'outcome.csv')\nPICKLE_FILE = os.path.join(PWDPATH, DATA_DIR, 'min_max.pkl')\nSPLIT = 600\nEPOCHS = 10"""
projects/predicting_onset_of_diabetes/normalize.py,0,"b""# Normalize our particular diabetes dataset for easy computation and faster loss minimization.\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom conf import *\nimport pickle\n\n# Reading the diabetes dataset\ndf = pd.read_csv(DIABETES_DATASET)\n\n# Saving the features and outcomes as float32 type\nfeatures = df.drop('Outcome', 1).values.astype(np.float32)\noutcomes = df['Outcome'].values.astype(np.float32)\n\n# Normalizing the features and writing them to csv file\nfeatures_normalized = MinMaxScaler().fit_transform(features)\nfeatures_normalized = pd.DataFrame(features_normalized, index=None)\nfeatures_normalized.to_csv(FEATURES_CSV, header=False, index=False)\n\n# Saving the max and min value to use while prediction\nfeatures_max_data = features.max(0)\nfeatures_min_data = features.min(0)\n\n# One hot encoding the outcome dataset\n# Example:\n# if our outcome is 0, one_hot(0) -> [1, 0]\n# Reversal is done by using, np.argmax([1, 0]) -> 0\noutcomes_one_hot_encoded =  pd.get_dummies(outcomes)\noutcomes_one_hot_encoded.to_csv(OUTCOMES_CSV, header=False, index=False)\n\n# Using pickle to store the max and min values\nwith open('data/min_max.pkl', 'w') as file:\n\tpickle.dump(features_max_data, file)\n\tpickle.dump(features_min_data, file)\n\n\n\n\n\n"""
