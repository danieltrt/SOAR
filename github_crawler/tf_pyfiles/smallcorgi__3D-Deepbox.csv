file_path,api_count,code
data_processing.py,0,"b""import tensorflow as tf\nimport cv2, os\nimport numpy as np\nfrom random import shuffle\nimport copy\n\n#####\n#Training setting\nBIN, OVERLAP = 2, 0.1\nNORM_H, NORM_W = 224, 224\nVEHICLES = ['Car', 'Truck', 'Van', 'Tram','Pedestrian','Cyclist']\n\n\ndef compute_anchors(angle):\n    anchors = []\n    \n    wedge = 2.*np.pi/BIN\n    l_index = int(angle/wedge)\n    r_index = l_index + 1\n    \n    if (angle - l_index*wedge) < wedge/2 * (1+OVERLAP/2):\n        anchors.append([l_index, angle - l_index*wedge])\n        \n    if (r_index*wedge - angle) < wedge/2 * (1+OVERLAP/2):\n        anchors.append([r_index%BIN, angle - r_index*wedge])\n        \n    return anchors\n\ndef parse_annotation(label_dir, image_dir):\n    all_objs = []\n    dims_avg = {key:np.array([0, 0, 0]) for key in VEHICLES}\n    dims_cnt = {key:0 for key in VEHICLES}\n        \n    for label_file in sorted(os.listdir(label_dir)):\n        image_file = label_file.replace('txt', 'png')\n\n        for line in open(label_dir + label_file).readlines():\n            line = line.strip().split(' ')\n            truncated = np.abs(float(line[1]))\n            occluded  = np.abs(float(line[2]))\n\n            if line[0] in VEHICLES and truncated < 0.1 and occluded < 0.1:\n                new_alpha = float(line[3]) + np.pi/2.\n                if new_alpha < 0:\n                    new_alpha = new_alpha + 2.*np.pi\n                new_alpha = new_alpha - int(new_alpha/(2.*np.pi))*(2.*np.pi)\n\n                obj = {'name':line[0],\n                       'image':image_file,\n                       'xmin':int(float(line[4])),\n                       'ymin':int(float(line[5])),\n                       'xmax':int(float(line[6])),\n                       'ymax':int(float(line[7])),\n                       'dims':np.array([float(number) for number in line[8:11]]),\n                       'new_alpha': new_alpha\n                      }\n                \n                dims_avg[obj['name']]  = dims_cnt[obj['name']]*dims_avg[obj['name']] + obj['dims']\n                dims_cnt[obj['name']] += 1\n                dims_avg[obj['name']] /= dims_cnt[obj['name']]\n\n                all_objs.append(obj)\n    ###### flip data\n    for obj in all_objs:\n        # Fix dimensions\n        obj['dims'] = obj['dims'] - dims_avg[obj['name']]\n\n        # Fix orientation and confidence for no flip\n        orientation = np.zeros((BIN,2))\n        confidence = np.zeros(BIN)\n\n        anchors = compute_anchors(obj['new_alpha'])\n\n        for anchor in anchors:\n            orientation[anchor[0]] = np.array([np.cos(anchor[1]), np.sin(anchor[1])])\n            confidence[anchor[0]] = 1.\n\n        confidence = confidence / np.sum(confidence)\n\n        obj['orient'] = orientation\n        obj['conf'] = confidence\n\n        # Fix orientation and confidence for flip\n        orientation = np.zeros((BIN,2))\n        confidence = np.zeros(BIN)\n\n        anchors = compute_anchors(2.*np.pi - obj['new_alpha'])\n        for anchor in anchors:\n            orientation[anchor[0]] = np.array([np.cos(anchor[1]), np.sin(anchor[1])])\n            confidence[anchor[0]] = 1\n            \n        confidence = confidence / np.sum(confidence)\n\n        obj['orient_flipped'] = orientation\n        obj['conf_flipped'] = confidence\n            \n    return all_objs\n\n\ndef prepare_input_and_output(image_dir, train_inst):\n    ### Prepare image patch\n    xmin = train_inst['xmin'] #+ np.random.randint(-MAX_JIT, MAX_JIT+1)\n    ymin = train_inst['ymin'] #+ np.random.randint(-MAX_JIT, MAX_JIT+1)\n    xmax = train_inst['xmax'] #+ np.random.randint(-MAX_JIT, MAX_JIT+1)\n    ymax = train_inst['ymax'] #+ np.random.randint(-MAX_JIT, MAX_JIT+1)\n    img = cv2.imread(image_dir + train_inst['image'])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = copy.deepcopy(img[ymin:ymax+1,xmin:xmax+1]).astype(np.float32)\n    \n    # re-color the image\n    #img += np.random.randint(-2, 3, img.shape).astype('float32')\n    #t  = [np.random.uniform()]\n    #t += [np.random.uniform()]\n    #t += [np.random.uniform()]\n    #t = np.array(t)\n\n    #img = img * (1 + t)\n    #img = img / (255. * 2.)\n\n    # flip the image\n    flip = np.random.binomial(1, .5)\n    if flip > 0.5: img = cv2.flip(img, 1)\n        \n    # resize the image to standard size\n    img = cv2.resize(img, (NORM_H, NORM_W))\n    img = img - np.array([[[103.939, 116.779, 123.68]]])\n    #img = img[:,:,::-1]\n    \n    ### Fix orientation and confidence\n    if flip > 0.5:\n        return img, train_inst['dims'], train_inst['orient_flipped'], train_inst['conf_flipped']\n    else:\n        return img, train_inst['dims'], train_inst['orient'], train_inst['conf']\n\ndef data_gen(image_dir, all_objs, batch_size):\n    num_obj = len(all_objs)\n    \n    keys = range(num_obj)\n    np.random.shuffle(keys)\n    \n    l_bound = 0\n    r_bound = batch_size if batch_size < num_obj else num_obj\n    \n    while True:\n        if l_bound == r_bound:\n            l_bound  = 0\n            r_bound = batch_size if batch_size < num_obj else num_obj\n            np.random.shuffle(keys)\n        \n        currt_inst = 0\n        x_batch = np.zeros((r_bound - l_bound, 224, 224, 3))\n        d_batch = np.zeros((r_bound - l_bound, 3))\n        o_batch = np.zeros((r_bound - l_bound, BIN, 2))\n        c_batch = np.zeros((r_bound - l_bound, BIN))\n        \n        for key in keys[l_bound:r_bound]:\n            # augment input image and fix object's orientation and confidence\n            image, dimension, orientation, confidence = prepare_input_and_output(image_dir, all_objs[key])\n            \n            #plt.figure(figsize=(5,5))\n            #plt.imshow(image/255./2.); plt.show()\n            #print dimension\n            #print orientation\n            #print confidence\n            \n            x_batch[currt_inst, :] = image\n            d_batch[currt_inst, :] = dimension\n            o_batch[currt_inst, :] = orientation\n            c_batch[currt_inst, :] = confidence\n            \n            currt_inst += 1\n                \n        yield x_batch, [d_batch, o_batch, c_batch]\n        \n        l_bound  = r_bound\n        r_bound = r_bound + batch_size\n        if r_bound > num_obj: r_bound = num_obj\n\n"""
main.py,33,"b'import tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport cv2, os\nimport numpy as np\nimport time\nfrom random import shuffle\nfrom data_processing import *\nimport sys\nimport argparse\nfrom tqdm import tqdm\n\n#####\n#Training setting\n\nBIN, OVERLAP = 2, 0.1\nW = 1.\nALPHA = 1.\nMAX_JIT = 3\nNORM_H, NORM_W = 224, 224\nVEHICLES = [\'Car\', \'Truck\', \'Van\', \'Tram\',\'Pedestrian\',\'Cyclist\']\nBATCH_SIZE = 8\nlearning_rate = 0.0001\nepochs = 50\nsave_path = \'./model/\'\n\ndims_avg = {\'Cyclist\': np.array([ 1.73532436,  0.58028152,  1.77413709]), \'Van\': np.array([ 2.18928571,  1.90979592,  5.07087755]), \'Tram\': np.array([  3.56092896,   2.39601093,  18.34125683]), \'Car\': np.array([ 1.52159147,  1.64443089,  3.85813679]), \'Pedestrian\': np.array([ 1.75554637,  0.66860882,  0.87623049]), \'Truck\': np.array([  3.07392252,   2.63079903,  11.2190799 ])}\n\n\n#### Placeholder\ninputs = tf.placeholder(tf.float32, shape = [None, 224, 224, 3])\nd_label = tf.placeholder(tf.float32, shape = [None, 3])\no_label = tf.placeholder(tf.float32, shape = [None, BIN, 2])\nc_label = tf.placeholder(tf.float32, shape = [None, BIN])\n\n\ndef parse_args():\n    """"""Parse input arguments.""""""\n    parser = argparse.ArgumentParser(description=\'3D bounding box\')\n    parser.add_argument(\'--mode\',dest = \'mode\',help=\'train or test\',default = \'test\')\n    parser.add_argument(\'--image\',dest = \'image\',help=\'Image path\')\n    parser.add_argument(\'--label\',dest = \'label\',help=\'Label path\')\n    parser.add_argument(\'--box2d\',dest = \'box2d\',help=\'2D detection path\')\n    parser.add_argument(\'--output\',dest = \'output\',help=\'Output path\', default = \'./validation/result_2/\')\n    parser.add_argument(\'--model\',dest = \'model\')\n    parser.add_argument(\'--gpu\',dest = \'gpu\',default= \'0\')\n    args = parser.parse_args()\n\n    return args\n\n\ndef build_model():\n\n  #### build some layer \n  def LeakyReLU(x, alpha):\n      return tf.nn.relu(x) - alpha * tf.nn.relu(-x)\n\n  def orientation_loss(y_true, y_pred):\n      # Find number of anchors\n      anchors = tf.reduce_sum(tf.square(y_true), axis=2)\n      anchors = tf.greater(anchors, tf.constant(0.5))\n      anchors = tf.reduce_sum(tf.cast(anchors, tf.float32), 1)\n\n      # Define the loss\n      loss = (y_true[:,:,0]*y_pred[:,:,0] + y_true[:,:,1]*y_pred[:,:,1])\n      loss = tf.reduce_sum((2 - 2 * tf.reduce_mean(loss,axis=0))) / anchors\n\n      return tf.reduce_mean(loss)\n\n  #####\n  #Build Graph\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n                      weights_regularizer=slim.l2_regularizer(0.0005)):\n    net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n    net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n    net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n    net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n    net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n    net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n    net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n    net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n    net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n    net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n    conv5 = tf.contrib.layers.flatten(net)\n\n    #dimension = slim.fully_connected(conv5, 512, scope=\'fc7_d\')\n    dimension = slim.fully_connected(conv5, 512, activation_fn=None, scope=\'fc7_d\')\n    dimension = LeakyReLU(dimension, 0.1)\n    dimension = slim.dropout(dimension, 0.5, scope=\'dropout7_d\')\n    #dimension = slim.fully_connected(dimension, 3, scope=\'fc8_d\')\n    dimension = slim.fully_connected(dimension, 3, activation_fn=None, scope=\'fc8_d\')\n    #dimension = LeakyReLU(dimension, 0.1)\n\n    #loss_d = tf.reduce_mean(tf.square(d_label - dimension))\n    loss_d = tf.losses.mean_squared_error(d_label, dimension)\n\n    #orientation = slim.fully_connected(conv5, 256, scope=\'fc7_o\')\n    orientation = slim.fully_connected(conv5, 256, activation_fn=None, scope=\'fc7_o\')\n    orientation = LeakyReLU(orientation, 0.1)\n    orientation = slim.dropout(orientation, 0.5, scope=\'dropout7_o\')\n    #orientation = slim.fully_connected(orientation, BIN*2, scope=\'fc8_o\')\n    orientation = slim.fully_connected(orientation, BIN*2, activation_fn=None, scope=\'fc8_o\')\n    #orientation = LeakyReLU(orientation, 0.1)\n    orientation = tf.reshape(orientation, [-1, BIN, 2])\n    orientation = tf.nn.l2_normalize(orientation, dim=2)\n    loss_o = orientation_loss(o_label, orientation)\n\n    #confidence = slim.fully_connected(conv5, 256, scope=\'fc7_c\')\n    confidence = slim.fully_connected(conv5, 256, activation_fn=None, scope=\'fc7_c\')\n    confidence = LeakyReLU(confidence, 0.1)\n    confidence = slim.dropout(confidence, 0.5, scope=\'dropout7_c\')\n    confidence = slim.fully_connected(confidence, BIN, activation_fn=None, scope=\'fc8_c\')\n    loss_c = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=c_label, logits= confidence))\n   \n    confidence = tf.nn.softmax(confidence)\n    #loss_c = tf.reduce_mean(tf.square(c_label - confidence))\n    #loss_c = tf.losses.mean_squared_error(c_label, confidence)\n    \n    total_loss = 4. * loss_d + 8. * loss_o + loss_c\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n    \n    return dimension, orientation, confidence, total_loss, optimizer, loss_d, loss_o, loss_c\n\n\ndef train(image_dir, box2d_loc, label_dir):\n\n    # load data & gen data\n    all_objs = parse_annotation(label_dir, image_dir)\n    all_exams  = len(all_objs)\n    np.random.shuffle(all_objs)\n    train_gen = data_gen(image_dir, all_objs, BATCH_SIZE)\n    train_num = int(np.ceil(all_exams/BATCH_SIZE))\n    \n    ### buile graph\n    dimension, orientation, confidence, loss, optimizer, loss_d, loss_o, loss_c = build_model()\n\n    ### GPU config\n    tfconfig = tf.ConfigProto(allow_soft_placement=True)\n    tfconfig.gpu_options.allow_growth = True\n    sess = tf.Session(config=tfconfig)\n\n    # create a folder for saving model\n    if os.path.isdir(save_path) == False:\n        os.mkdir(save_path)\n    variables_to_restore = slim.get_variables()[:26] ## vgg16-conv5\n\n    saver = tf.train.Saver(max_to_keep=100)\n\n    #Load pretrain VGG model\n    ckpt_list = tf.contrib.framework.list_variables(\'./vgg_16.ckpt\')[1:-7]\n    new_ckpt_list = []\n    for name in range(1,len(ckpt_list),2):\n        tf.contrib.framework.init_from_checkpoint(\'./vgg_16.ckpt\', {ckpt_list[name-1][0]: variables_to_restore[name]})\n        tf.contrib.framework.init_from_checkpoint(\'./vgg_16.ckpt\', {ckpt_list[name][0]: variables_to_restore[name-1]})\n\n    # Initializing the variables\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n\n    # Start to train model\n    for epoch in range(epochs):\n        epoch_loss = np.zeros((train_num,1),dtype = float)\n        tStart_epoch = time.time()\n        batch_loss = 0.0\n        for num_iters in tqdm(range(train_num),ascii=True,desc=\'Epoch \'+str(epoch+1)+\' : Loss:\'+str(batch_loss)):\n            train_img, train_label = train_gen.next()\n            _,batch_loss = sess.run([optimizer,loss], feed_dict={inputs: train_img, d_label: train_label[0], o_label: train_label[1], c_label: train_label[2]})\n\n            epoch_loss[num_iters] = batch_loss \n\n        # save model\n        if (epoch+1) % 5 == 0:\n            saver.save(sess,save_path+""model"", global_step = epoch+1)\n\n        # Print some information\n        print ""Epoch:"", epoch+1, "" done. Loss:"", np.mean(epoch_loss)\n        tStop_epoch = time.time()\n        print ""Epoch Time Cost:"", round(tStop_epoch - tStart_epoch,2), ""s""\n        sys.stdout.flush()\n\ndef test(model, image_dir, box2d_loc, box3d_loc):\n\n    ### buile graph\n    dimension, orientation, confidence, loss, optimizer, loss_d, loss_o, loss_c = build_model()\n\n    ### GPU config \n    tfconfig = tf.ConfigProto(allow_soft_placement=True)\n    tfconfig.gpu_options.allow_growth = True\n    sess = tf.Session(config=tfconfig)\n\n    # Initializing the variables\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    # Restore model\n    saver = tf.train.Saver()\n    saver.restore(sess, model)\n\n    # create a folder for saving result\n    if os.path.isdir(box3d_loc) == False:\n        os.mkdir(box3d_loc)\n\n    # Load image & run testing\n    all_image = sorted(os.listdir(image_dir))\n\n    for f in all_image:\n        image_file = image_dir + f\n        box2d_file = box2d_loc + f.replace(\'png\', \'txt\')\n        box3d_file = box3d_loc + f.replace(\'png\', \'txt\')\n        print image_file\n        with open(box3d_file, \'w\') as box3d:\n            img = cv2.imread(image_file)\n            img = img.astype(np.float32, copy=False)\n\n            for line in open(box2d_file):\n                line = line.strip().split(\' \')\n                truncated = np.abs(float(line[1]))\n                occluded  = np.abs(float(line[2]))\n\n                obj = {\'xmin\':int(float(line[4])),\n                       \'ymin\':int(float(line[5])),\n                       \'xmax\':int(float(line[6])),\n                       \'ymax\':int(float(line[7])),\n                       }\n\n                patch = img[obj[\'ymin\']:obj[\'ymax\'],obj[\'xmin\']:obj[\'xmax\']]\n                patch = cv2.resize(patch, (NORM_H, NORM_W))\n                patch = patch - np.array([[[103.939, 116.779, 123.68]]])\n                patch = np.expand_dims(patch, 0)\n                prediction = sess.run([dimension, orientation, confidence], feed_dict={inputs: patch})\n                # Transform regressed angle\n                max_anc = np.argmax(prediction[2][0])\n                anchors = prediction[1][0][max_anc]\n\n                if anchors[1] > 0:\n                    angle_offset = np.arccos(anchors[0])\n                else:\n                    angle_offset = -np.arccos(anchors[0])\n\n                wedge = 2.*np.pi/BIN\n                angle_offset = angle_offset + max_anc*wedge\n                angle_offset = angle_offset % (2.*np.pi)\n\n                angle_offset = angle_offset - np.pi/2\n                if angle_offset > np.pi:\n                    angle_offset = angle_offset - (2.*np.pi)\n\n                line[3] = str(angle_offset)\n                 \n                line[-1] = angle_offset +np.arctan(float(line[11]) / float(line[13]))\n                \n                # Transform regressed dimension\n                if line[0] in VEHICLES:\n                    dims = dims_avg[line[0]] + prediction[0][0]\n                else:\n                    dims = dims_avg[\'Car\'] + prediction[0][0]\n\n                line = line[:8] + list(dims) + line[11:]\n                \n                # Write regressed 3D dim and oritent to file\n                line = \' \'.join([str(item) for item in line]) +\' \'+ str(np.max(prediction[2][0]))+ \'\\n\'\n                box3d.write(line)\n\n \n\nif __name__ == ""__main__"":\n    args = parse_args()\n\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    if args.image is None:\n        raise IOError((\'Image not found.\'.format(args.image)))\n    if args.box2d is None :\n        raise IOError((\'2D bounding box not found.\'.format(args.box2d)))\n\n    if args.mode == \'train\':\n        if args.label is None:\n            raise IOError((\'Label not found.\'.format(args.label)))\n\n        train(args.image, args.box2d, args.label)\n    else:\n        if args.model is None:\n            raise IOError((\'Model not found.\'.format(args.model)))\n\n        test(args.model, args.image, args.box2d, args.output)\n\n'"
