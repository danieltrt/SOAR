file_path,api_count,code
data/__init__.py,0,b''
lib/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    #adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n          return None;\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.iteritems():\n        if not os.path.exists(v):\n            return None;\n\n    return cudaconfig\n\nCUDA = locate_cuda()\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        print extra_postargs\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\next_modules = [\n    Extension(\n        ""utils.cython_bbox"",\n        [""utils/bbox.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n\tinclude_dirs = [numpy_include]\n\t),\n    Extension(\n        ""utils.cython_nms"",\n        [""utils/nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\n        ""nms.cpu_nms"",\n        [""nms/cpu_nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    )\n]\n\nif CUDA:\n    ext_modules.append(\n        Extension(\'nms.gpu_nms\',\n            [\'nms/nms_kernel.cu\', \'nms/gpu_nms.pyx\'],\n            library_dirs=[CUDA[\'lib64\']],\n            libraries=[\'cudart\'],\n            language=\'c++\',\n            runtime_library_dirs=[CUDA[\'lib64\']],\n            # this syntax is specific to this build system\n            # we\'re only going to use certain compiler args with nvcc and not with gcc\n            # the implementation of this trick is in customize_compiler() below\n            extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                                \'nvcc\': [\'-arch=sm_35\',\n                                         \'--ptxas-options=-v\',\n                                         \'-c\',\n                                         \'--compiler-options\',\n                                         ""\'-fPIC\'""]},\n            include_dirs = [numpy_include, CUDA[\'include\']]\n        )\n    )\n\nsetup(\n    name=\'fast_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
tools/_init_paths.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Set up paths for Fast R-CNN.""""""\n\nimport os.path as osp\nimport sys\n\ndef add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\nthis_dir = osp.dirname(__file__)\n\n# Add caffe to PYTHONPATH\ncaffe_path = osp.join(this_dir, \'..\', \'caffe-fast-rcnn\', \'python\')\nadd_path(caffe_path)\n\n# Add lib to PYTHONPATH\nlib_path = osp.join(this_dir, \'..\', \'lib\')\nadd_path(lib_path)\n\n\nlib_path = osp.join(this_dir,\'mftracker\')\nadd_path(lib_path)\n'"
tools/demo.py,3,"b'import _init_paths\nimport tensorflow as tf\nfrom fast_rcnn.config import cfg\nfrom fast_rcnn.test import im_detect\nfrom fast_rcnn.nms_wrapper import nms\nfrom utils.timer import Timer\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os, sys, cv2\nimport argparse\nfrom networks.factory import get_network\n\n\nCLASSES = (\'__background__\',\n           \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n           \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n           \'cow\', \'diningtable\', \'dog\', \'horse\',\n           \'motorbike\', \'person\', \'pottedplant\',\n           \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n\n\n#CLASSES = (\'__background__\',\'person\',\'bike\',\'motorbike\',\'car\',\'bus\')\n\ndef vis_detections(im, class_name, dets,ax, thresh=0.5):\n    """"""Draw detected bounding boxes.""""""\n    inds = np.where(dets[:, -1] >= thresh)[0]\n    if len(inds) == 0:\n        return\n\n    for i in inds:\n        bbox = dets[i, :4]\n        score = dets[i, -1]\n\n        ax.add_patch(\n            plt.Rectangle((bbox[0], bbox[1]),\n                          bbox[2] - bbox[0],\n                          bbox[3] - bbox[1], fill=False,\n                          edgecolor=\'red\', linewidth=3.5)\n            )\n        ax.text(bbox[0], bbox[1] - 2,\n                \'{:s} {:.3f}\'.format(class_name, score),\n                bbox=dict(facecolor=\'blue\', alpha=0.5),\n                fontsize=14, color=\'white\')\n\n    ax.set_title((\'{} detections with \'\n                  \'p({} | box) >= {:.1f}\').format(class_name, class_name,\n                                                  thresh),\n                  fontsize=14)\n    plt.axis(\'off\')\n    plt.tight_layout()\n    plt.draw()\n\n\ndef demo(sess, net, image_name):\n    """"""Detect object classes in an image using pre-computed object proposals.""""""\n\n    # Load the demo image\n    im_file = os.path.join(cfg.DATA_DIR, \'demo\', image_name)\n    #im_file = os.path.join(\'/home/corgi/Lab/label/pos_frame/ACCV/training/000001/\',image_name)\n    im = cv2.imread(im_file)\n\n    # Detect all object classes and regress object bounds\n    timer = Timer()\n    timer.tic()\n    scores, boxes = im_detect(sess, net, im)\n    timer.toc()\n    print (\'Detection took {:.3f}s for \'\n           \'{:d} object proposals\').format(timer.total_time, boxes.shape[0])\n\n    # Visualize detections for each class\n    im = im[:, :, (2, 1, 0)]\n    fig, ax = plt.subplots(figsize=(12, 12))\n    ax.imshow(im, aspect=\'equal\')\n\n    CONF_THRESH = 0.8\n    NMS_THRESH = 0.3\n    for cls_ind, cls in enumerate(CLASSES[1:]):\n        cls_ind += 1 # because we skipped background\n        cls_boxes = boxes[:, 4*cls_ind:4*(cls_ind + 1)]\n        cls_scores = scores[:, cls_ind]\n        dets = np.hstack((cls_boxes,\n                          cls_scores[:, np.newaxis])).astype(np.float32)\n        keep = nms(dets, NMS_THRESH)\n        dets = dets[keep, :]\n        vis_detections(im, cls, dets, ax, thresh=CONF_THRESH)\n\ndef parse_args():\n    """"""Parse input arguments.""""""\n    parser = argparse.ArgumentParser(description=\'Faster R-CNN demo\')\n    parser.add_argument(\'--gpu\', dest=\'gpu_id\', help=\'GPU device id to use [0]\',\n                        default=0, type=int)\n    parser.add_argument(\'--cpu\', dest=\'cpu_mode\',\n                        help=\'Use CPU mode (overrides --gpu)\',\n                        action=\'store_true\')\n    parser.add_argument(\'--net\', dest=\'demo_net\', help=\'Network to use [vgg16]\',\n                        default=\'VGGnet_test\')\n    parser.add_argument(\'--model\', dest=\'model\', help=\'Model path\',\n                        default=\' \')\n\n    args = parser.parse_args()\n\n    return args\nif __name__ == \'__main__\':\n    cfg.TEST.HAS_RPN = True  # Use RPN for proposals\n\n    args = parse_args()\n\n    if args.model == \' \':\n        raise IOError((\'Error: Model not found.\\n\'))\n        \n    # init session\n    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n    # load network\n    net = get_network(args.demo_net)\n    # load model\n    saver = tf.train.Saver(write_version=tf.train.SaverDef.V1)\n    saver.restore(sess, args.model)\n   \n    #sess.run(tf.initialize_all_variables())\n\n    print \'\\n\\nLoaded network {:s}\'.format(args.model)\n\n    # Warmup on a dummy image\n    im = 128 * np.ones((300, 300, 3), dtype=np.uint8)\n    for i in xrange(2):\n        _, _= im_detect(sess, net, im)\n\n    im_names = [\'000456.jpg\', \'000542.jpg\', \'001150.jpg\',\n                \'001763.jpg\', \'004545.jpg\']\n\n\n    for im_name in im_names:\n        print \'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\'\n        print \'Demo for data/demo/{}\'.format(im_name)\n        demo(sess, net, im_name)\n\n    plt.show()\n\n'"
tools/test_net.py,2,"b'#!/usr/bin/env python\n\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Test a Fast R-CNN network on an image database.""""""\n\nimport _init_paths\nfrom fast_rcnn.test import test_net\nfrom fast_rcnn.config import cfg, cfg_from_file\nfrom datasets.factory import get_imdb\nfrom networks.factory import get_network\nimport argparse\nimport pprint\nimport time, os, sys\nimport tensorflow as tf\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Test a Fast R-CNN network\')\n    parser.add_argument(\'--device\', dest=\'device\', help=\'device to use\',\n                        default=\'cpu\', type=str)\n    parser.add_argument(\'--device_id\', dest=\'device_id\', help=\'device id to use\',\n                        default=0, type=int)\n    parser.add_argument(\'--def\', dest=\'prototxt\',\n                        help=\'prototxt file defining the network\',\n                        default=None, type=str)\n    parser.add_argument(\'--weights\', dest=\'model\',\n                        help=\'model to test\',\n                        default=None, type=str)\n    parser.add_argument(\'--cfg\', dest=\'cfg_file\',\n                        help=\'optional config file\', default=None, type=str)\n    parser.add_argument(\'--wait\', dest=\'wait\',\n                        help=\'wait until net file exists\',\n                        default=True, type=bool)\n    parser.add_argument(\'--imdb\', dest=\'imdb_name\',\n                        help=\'dataset to test\',\n                        default=\'voc_2007_test\', type=str)\n    parser.add_argument(\'--comp\', dest=\'comp_mode\', help=\'competition mode\',\n                        action=\'store_true\')\n    parser.add_argument(\'--network\', dest=\'network_name\',\n                        help=\'name of the network\',\n                        default=None, type=str)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    print(\'Called with args:\')\n    print(args)\n\n    if args.cfg_file is not None:\n        cfg_from_file(args.cfg_file)\n\n    print(\'Using config:\')\n    pprint.pprint(cfg)\n\n    while not os.path.exists(args.model) and args.wait:\n        print(\'Waiting for {} to exist...\'.format(args.model))\n        time.sleep(10)\n\n    weights_filename = os.path.splitext(os.path.basename(args.model))[0]\n\n    imdb = get_imdb(args.imdb_name)\n    imdb.competition_mode(args.comp_mode)\n\n    device_name = \'/{}:{:d}\'.format(args.device,args.device_id)\n    print device_name\n\n    network = get_network(args.network_name)\n    print \'Use network `{:s}` in training\'.format(args.network_name)\n\n    if args.device == \'gpu\':\n        cfg.USE_GPU_NMS = True\n        cfg.GPU_ID = args.device_id\n    else:\n        cfg.USE_GPU_NMS = False\n\n    # start a session\n    saver = tf.train.Saver()\n    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n    saver.restore(sess, args.model)\n    print (\'Loading model weights from {:s}\').format(args.model)\n\n    test_net(sess, network, imdb, weights_filename)\n'"
tools/train_net.py,0,"b'#!/usr/bin/env python\n\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Train a Fast R-CNN network on a region of interest database.""""""\n\nimport _init_paths\nfrom fast_rcnn.train import get_training_roidb, train_net\nfrom fast_rcnn.config import cfg,cfg_from_file, cfg_from_list, get_output_dir\nfrom datasets.factory import get_imdb\nfrom networks.factory import get_network\nimport argparse\nimport pprint\nimport numpy as np\nimport sys\nimport pdb\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Train a Fast R-CNN network\')\n    parser.add_argument(\'--device\', dest=\'device\', help=\'device to use\',\n                        default=\'cpu\', type=str)\n    parser.add_argument(\'--device_id\', dest=\'device_id\', help=\'device id to use\',\n                        default=0, type=int)\n    parser.add_argument(\'--solver\', dest=\'solver\',\n                        help=\'solver prototxt\',\n                        default=None, type=str)\n    parser.add_argument(\'--iters\', dest=\'max_iters\',\n                        help=\'number of iterations to train\',\n                        default=70000, type=int)\n    parser.add_argument(\'--weights\', dest=\'pretrained_model\',\n                        help=\'initialize with pretrained model weights\',\n                        default=None, type=str)\n    parser.add_argument(\'--cfg\', dest=\'cfg_file\',\n                        help=\'optional config file\',\n                        default=None, type=str)\n    parser.add_argument(\'--imdb\', dest=\'imdb_name\',\n                        help=\'dataset to train on\',\n                        default=\'kitti_train\', type=str)\n    parser.add_argument(\'--rand\', dest=\'randomize\',\n                        help=\'randomize (do not use a fixed seed)\',\n                        action=\'store_true\')\n    parser.add_argument(\'--network\', dest=\'network_name\',\n                        help=\'name of the network\',\n                        default=None, type=str)\n    parser.add_argument(\'--set\', dest=\'set_cfgs\',\n                        help=\'set config keys\', default=None,\n                        nargs=argparse.REMAINDER)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    print(\'Called with args:\')\n    print(args)\n\n    if args.cfg_file is not None:\n        cfg_from_file(args.cfg_file)\n    if args.set_cfgs is not None:\n        cfg_from_list(args.set_cfgs)\n\n    print(\'Using config:\')\n    pprint.pprint(cfg)\n\n    if not args.randomize:\n        # fix the random seeds (numpy and caffe) for reproducibility\n        np.random.seed(cfg.RNG_SEED)\n    imdb = get_imdb(args.imdb_name)\n    print \'Loaded dataset `{:s}` for training\'.format(imdb.name)\n    roidb = get_training_roidb(imdb)\n\n    output_dir = get_output_dir(imdb, None)\n    print \'Output will be saved to `{:s}`\'.format(output_dir)\n\n    device_name = \'/{}:{:d}\'.format(args.device,args.device_id)\n    print device_name\n\n    network = get_network(args.network_name)\n    print \'Use network `{:s}` in training\'.format(args.network_name)\n\n    train_net(network, imdb, roidb, output_dir,\n              pretrained_model=args.pretrained_model,\n              max_iters=args.max_iters)\n'"
lib/datasets/__init__.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom .imdb import imdb\nfrom .pascal_voc import pascal_voc\nfrom .pascal3d import pascal3d\nfrom .imagenet3d import imagenet3d\nfrom .kitti import kitti\nfrom .kitti_tracking import kitti_tracking\nfrom .nissan import nissan\nfrom .nthu import nthu\nfrom . import factory\n\nimport os.path as osp\nROOT_DIR = osp.join(osp.dirname(__file__), \'..\', \'..\')\n\n# We assume your matlab binary is in your path and called `matlab\'.\n# If either is not true, just add it to your path and alias it as matlab, or\n# you could change this file.\nMATLAB = \'matlab_r2013b\'\n\n# http://stackoverflow.com/questions/377017/test-if-executable-exists-in-python\ndef _which(program):\n    import os\n    def is_exe(fpath):\n        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)\n\n    fpath, fname = os.path.split(program)\n    if fpath:\n        if is_exe(program):\n            return program\n    else:\n        for path in os.environ[""PATH""].split(os.pathsep):\n            path = path.strip(\'""\')\n            exe_file = os.path.join(path, program)\n            if is_exe(exe_file):\n                return exe_file\n\n    return None\n""""""\nif _which(MATLAB) is None:\n    msg = (""MATLAB command \'{}\' not found. ""\n           ""Please add \'{}\' to your PATH."").format(MATLAB, MATLAB)\n    raise EnvironmentError(msg)\n""""""\n'"
lib/datasets/coco.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom datasets.imdb import imdb\nimport datasets.ds_utils as ds_utils\nfrom fast_rcnn.config import cfg\nimport os.path as osp\nimport sys\nimport os\nimport numpy as np\nimport scipy.sparse\nimport scipy.io as sio\nimport cPickle\nimport json\nimport uuid\n# COCO API\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom pycocotools import mask as COCOmask\n\ndef _filter_crowd_proposals(roidb, crowd_thresh):\n    """"""\n    Finds proposals that are inside crowd regions and marks them with\n    overlap = -1 (for all gt rois), which means they will be excluded from\n    training.\n    """"""\n    for ix, entry in enumerate(roidb):\n        overlaps = entry[\'gt_overlaps\'].toarray()\n        crowd_inds = np.where(overlaps.max(axis=1) == -1)[0]\n        non_gt_inds = np.where(entry[\'gt_classes\'] == 0)[0]\n        if len(crowd_inds) == 0 or len(non_gt_inds) == 0:\n            continue\n        iscrowd = [int(True) for _ in xrange(len(crowd_inds))]\n        crowd_boxes = ds_utils.xyxy_to_xywh(entry[\'boxes\'][crowd_inds, :])\n        non_gt_boxes = ds_utils.xyxy_to_xywh(entry[\'boxes\'][non_gt_inds, :])\n        ious = COCOmask.iou(non_gt_boxes, crowd_boxes, iscrowd)\n        bad_inds = np.where(ious.max(axis=1) > crowd_thresh)[0]\n        overlaps[non_gt_inds[bad_inds], :] = -1\n        roidb[ix][\'gt_overlaps\'] = scipy.sparse.csr_matrix(overlaps)\n    return roidb\n\nclass coco(imdb):\n    def __init__(self, image_set, year):\n        imdb.__init__(self, \'coco_\' + year + \'_\' + image_set)\n        # COCO specific config options\n        self.config = {\'top_k\' : 2000,\n                       \'use_salt\' : True,\n                       \'cleanup\' : True,\n                       \'crowd_thresh\' : 0.7,\n                       \'min_size\' : 2}\n        # name, paths\n        self._year = year\n        self._image_set = image_set\n        self._data_path = osp.join(cfg.DATA_DIR, \'coco\')\n        # load COCO API, classes, class <-> id mappings\n        self._COCO = COCO(self._get_ann_file())\n        cats = self._COCO.loadCats(self._COCO.getCatIds())\n        self._classes = tuple([\'__background__\'] + [c[\'name\'] for c in cats])\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._class_to_coco_cat_id = dict(zip([c[\'name\'] for c in cats],\n                                              self._COCO.getCatIds()))\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        self.set_proposal_method(\'selective_search\')\n        self.competition_mode(False)\n\n        # Some image sets are ""views"" (i.e. subsets) into others.\n        # For example, minival2014 is a random 5000 image subset of val2014.\n        # This mapping tells us where the view\'s images and proposals come from.\n        self._view_map = {\n            \'minival2014\' : \'val2014\',          # 5k val2014 subset\n            \'valminusminival2014\' : \'val2014\',  # val2014 \\setminus minival2014\n        }\n        coco_name = image_set + year  # e.g., ""val2014""\n        self._data_name = (self._view_map[coco_name]\n                           if self._view_map.has_key(coco_name)\n                           else coco_name)\n        # Dataset splits that have ground-truth annotations (test splits\n        # do not have gt annotations)\n        self._gt_splits = (\'train\', \'val\', \'minival\')\n\n    def _get_ann_file(self):\n        prefix = \'instances\' if self._image_set.find(\'test\') == -1 \\\n                             else \'image_info\'\n        return osp.join(self._data_path, \'annotations\',\n                        prefix + \'_\' + self._image_set + self._year + \'.json\')\n\n    def _load_image_set_index(self):\n        """"""\n        Load image ids.\n        """"""\n        image_ids = self._COCO.getImgIds()\n        return image_ids\n\n    def _get_widths(self):\n        anns = self._COCO.loadImgs(self._image_index)\n        widths = [ann[\'width\'] for ann in anns]\n        return widths\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # Example image path for index=119993:\n        #   images/train2014/COCO_train2014_000000119993.jpg\n        file_name = (\'COCO_\' + self._data_name + \'_\' +\n                     str(index).zfill(12) + \'.jpg\')\n        image_path = osp.join(self._data_path, \'images\',\n                              self._data_name, file_name)\n        assert osp.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def selective_search_roidb(self):\n        return self._roidb_from_proposals(\'selective_search\')\n\n    def edge_boxes_roidb(self):\n        return self._roidb_from_proposals(\'edge_boxes_AR\')\n\n    def mcg_roidb(self):\n        return self._roidb_from_proposals(\'MCG\')\n\n    def _roidb_from_proposals(self, method):\n        """"""\n        Creates a roidb from pre-computed proposals of a particular methods.\n        """"""\n        top_k = self.config[\'top_k\']\n        cache_file = osp.join(self.cache_path, self.name +\n                              \'_{:s}_top{:d}\'.format(method, top_k) +\n                              \'_roidb.pkl\')\n\n        if osp.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{:s} {:s} roidb loaded from {:s}\'.format(self.name, method,\n                                                            cache_file)\n            return roidb\n\n        if self._image_set in self._gt_splits:\n            gt_roidb = self.gt_roidb()\n            method_roidb = self._load_proposals(method, gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, method_roidb)\n            # Make sure we don\'t use proposals that are contained in crowds\n            roidb = _filter_crowd_proposals(roidb, self.config[\'crowd_thresh\'])\n        else:\n            roidb = self._load_proposals(method, None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote {:s} roidb to {:s}\'.format(method, cache_file)\n        return roidb\n\n    def _load_proposals(self, method, gt_roidb):\n        """"""\n        Load pre-computed proposals in the format provided by Jan Hosang:\n        http://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-\n          computing/research/object-recognition-and-scene-understanding/how-\n          good-are-detection-proposals-really/\n        For MCG, use boxes from http://www.eecs.berkeley.edu/Research/Projects/\n          CS/vision/grouping/mcg/ and convert the file layout using\n        lib/datasets/tools/mcg_munge.py.\n        """"""\n        box_list = []\n        top_k = self.config[\'top_k\']\n        valid_methods = [\n            \'MCG\',\n            \'selective_search\',\n            \'edge_boxes_AR\',\n            \'edge_boxes_70\']\n        assert method in valid_methods\n\n        print \'Loading {} boxes\'.format(method)\n        for i, index in enumerate(self._image_index):\n            if i % 1000 == 0:\n                print \'{:d} / {:d}\'.format(i + 1, len(self._image_index))\n\n            box_file = osp.join(\n                cfg.DATA_DIR, \'coco_proposals\', method, \'mat\',\n                self._get_box_file(index))\n\n            raw_data = sio.loadmat(box_file)[\'boxes\']\n            boxes = np.maximum(raw_data - 1, 0).astype(np.uint16)\n            if method == \'MCG\':\n                # Boxes from the MCG website are in (y1, x1, y2, x2) order\n                boxes = boxes[:, (1, 0, 3, 2)]\n            # Remove duplicate boxes and very small boxes and then take top k\n            keep = ds_utils.unique_boxes(boxes)\n            boxes = boxes[keep, :]\n            keep = ds_utils.filter_small_boxes(boxes, self.config[\'min_size\'])\n            boxes = boxes[keep, :]\n            boxes = boxes[:top_k, :]\n            box_list.append(boxes)\n            # Sanity check\n            im_ann = self._COCO.loadImgs(index)[0]\n            width = im_ann[\'width\']\n            height = im_ann[\'height\']\n            ds_utils.validate_boxes(boxes, width=width, height=height)\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = osp.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if osp.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_coco_annotation(index)\n                    for index in self._image_index]\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n        return gt_roidb\n\n    def _load_coco_annotation(self, index):\n        """"""\n        Loads COCO bounding-box instance annotations. Crowd instances are\n        handled by marking their overlaps (with all categories) to -1. This\n        overlap value means that crowd ""instances"" are excluded from training.\n        """"""\n        im_ann = self._COCO.loadImgs(index)[0]\n        width = im_ann[\'width\']\n        height = im_ann[\'height\']\n\n        annIds = self._COCO.getAnnIds(imgIds=index, iscrowd=None)\n        objs = self._COCO.loadAnns(annIds)\n        # Sanitize bboxes -- some are invalid\n        valid_objs = []\n        for obj in objs:\n            x1 = np.max((0, obj[\'bbox\'][0]))\n            y1 = np.max((0, obj[\'bbox\'][1]))\n            x2 = np.min((width - 1, x1 + np.max((0, obj[\'bbox\'][2] - 1))))\n            y2 = np.min((height - 1, y1 + np.max((0, obj[\'bbox\'][3] - 1))))\n            if obj[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\n                obj[\'clean_bbox\'] = [x1, y1, x2, y2]\n                valid_objs.append(obj)\n        objs = valid_objs\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n        # Lookup table to map from COCO category ids to our internal class\n        # indices\n        coco_cat_id_to_class_ind = dict([(self._class_to_coco_cat_id[cls],\n                                          self._class_to_ind[cls])\n                                         for cls in self._classes[1:]])\n\n        for ix, obj in enumerate(objs):\n            cls = coco_cat_id_to_class_ind[obj[\'category_id\']]\n            boxes[ix, :] = obj[\'clean_bbox\']\n            gt_classes[ix] = cls\n            seg_areas[ix] = obj[\'area\']\n            if obj[\'iscrowd\']:\n                # Set overlap to -1 for all classes for crowd objects\n                # so they will be excluded during training\n                overlaps[ix, :] = -1.0\n            else:\n                overlaps[ix, cls] = 1.0\n\n        ds_utils.validate_boxes(boxes, width=width, height=height)\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_overlaps\' : overlaps,\n                \'flipped\' : False,\n                \'seg_areas\' : seg_areas}\n\n    def _get_box_file(self, index):\n        # first 14 chars / first 22 chars / all chars + .mat\n        # COCO_val2014_0/COCO_val2014_000000447/COCO_val2014_000000447991.mat\n        file_name = (\'COCO_\' + self._data_name +\n                     \'_\' + str(index).zfill(12) + \'.mat\')\n        return osp.join(file_name[:14], file_name[:22], file_name)\n\n    def _print_detection_eval_metrics(self, coco_eval):\n        IoU_lo_thresh = 0.5\n        IoU_hi_thresh = 0.95\n        def _get_thr_ind(coco_eval, thr):\n            ind = np.where((coco_eval.params.iouThrs > thr - 1e-5) &\n                           (coco_eval.params.iouThrs < thr + 1e-5))[0][0]\n            iou_thr = coco_eval.params.iouThrs[ind]\n            assert np.isclose(iou_thr, thr)\n            return ind\n\n        ind_lo = _get_thr_ind(coco_eval, IoU_lo_thresh)\n        ind_hi = _get_thr_ind(coco_eval, IoU_hi_thresh)\n        # precision has dims (iou, recall, cls, area range, max dets)\n        # area range index 0: all area ranges\n        # max dets index 2: 100 per image\n        precision = \\\n            coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, :, 0, 2]\n        ap_default = np.mean(precision[precision > -1])\n        print (\'~~~~ Mean and per-category AP @ IoU=[{:.2f},{:.2f}] \'\n               \'~~~~\').format(IoU_lo_thresh, IoU_hi_thresh)\n        print \'{:.1f}\'.format(100 * ap_default)\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            # minus 1 because of __background__\n            precision = coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, cls_ind - 1, 0, 2]\n            ap = np.mean(precision[precision > -1])\n            print \'{:.1f}\'.format(100 * ap)\n\n        print \'~~~~ Summary metrics ~~~~\'\n        coco_eval.summarize()\n\n    def _do_detection_eval(self, res_file, output_dir):\n        ann_type = \'bbox\'\n        coco_dt = self._COCO.loadRes(res_file)\n        coco_eval = COCOeval(self._COCO, coco_dt)\n        coco_eval.params.useSegm = (ann_type == \'segm\')\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        self._print_detection_eval_metrics(coco_eval)\n        eval_file = osp.join(output_dir, \'detection_results.pkl\')\n        with open(eval_file, \'wb\') as fid:\n            cPickle.dump(coco_eval, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'Wrote COCO eval results to: {}\'.format(eval_file)\n\n    def _coco_results_one_category(self, boxes, cat_id):\n        results = []\n        for im_ind, index in enumerate(self.image_index):\n            dets = boxes[im_ind].astype(np.float)\n            if dets == []:\n                continue\n            scores = dets[:, -1]\n            xs = dets[:, 0]\n            ys = dets[:, 1]\n            ws = dets[:, 2] - xs + 1\n            hs = dets[:, 3] - ys + 1\n            results.extend(\n              [{\'image_id\' : index,\n                \'category_id\' : cat_id,\n                \'bbox\' : [xs[k], ys[k], ws[k], hs[k]],\n                \'score\' : scores[k]} for k in xrange(dets.shape[0])])\n        return results\n\n    def _write_coco_results_file(self, all_boxes, res_file):\n        # [{""image_id"": 42,\n        #   ""category_id"": 18,\n        #   ""bbox"": [258.15,41.29,348.26,243.78],\n        #   ""score"": 0.236}, ...]\n        results = []\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Collecting {} results ({:d}/{:d})\'.format(cls, cls_ind,\n                                                          self.num_classes - 1)\n            coco_cat_id = self._class_to_coco_cat_id[cls]\n            results.extend(self._coco_results_one_category(all_boxes[cls_ind],\n                                                           coco_cat_id))\n        print \'Writing results json to {}\'.format(res_file)\n        with open(res_file, \'w\') as fid:\n            json.dump(results, fid)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        res_file = osp.join(output_dir, (\'detections_\' +\n                                         self._image_set +\n                                         self._year +\n                                         \'_results\'))\n        if self.config[\'use_salt\']:\n            res_file += \'_{}\'.format(str(uuid.uuid4()))\n        res_file += \'.json\'\n        self._write_coco_results_file(all_boxes, res_file)\n        # Only do evaluation on non-test sets\n        if self._image_set.find(\'test\') == -1:\n            self._do_detection_eval(res_file, output_dir)\n        # Optionally cleanup results json file\n        if self.config[\'cleanup\']:\n            os.remove(res_file)\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n'"
lib/datasets/ds_utils.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef unique_boxes(boxes, scale=1.0):\n    """"""Return indices of unique boxes.""""""\n    v = np.array([1, 1e3, 1e6, 1e9])\n    hashes = np.round(boxes * scale).dot(v)\n    _, index = np.unique(hashes, return_index=True)\n    return np.sort(index)\n\ndef xywh_to_xyxy(boxes):\n    """"""Convert [x y w h] box format to [x1 y1 x2 y2] format.""""""\n    return np.hstack((boxes[:, 0:2], boxes[:, 0:2] + boxes[:, 2:4] - 1))\n\ndef xyxy_to_xywh(boxes):\n    """"""Convert [x1 y1 x2 y2] box format to [x y w h] format.""""""\n    return np.hstack((boxes[:, 0:2], boxes[:, 2:4] - boxes[:, 0:2] + 1))\n\ndef validate_boxes(boxes, width=0, height=0):\n    """"""Check that a set of boxes are valid.""""""\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    assert (x1 >= 0).all()\n    assert (y1 >= 0).all()\n    assert (x2 >= x1).all()\n    assert (y2 >= y1).all()\n    assert (x2 < width).all()\n    assert (y2 < height).all()\n\ndef filter_small_boxes(boxes, min_size):\n    w = boxes[:, 2] - boxes[:, 0]\n    h = boxes[:, 3] - boxes[:, 1]\n    keep = np.where((w >= min_size) & (h > min_size))[0]\n    return keep\n'"
lib/datasets/factory.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Factory method for easily getting imdbs by name.""""""\n\n__sets = {}\n\nimport datasets.pascal_voc\nimport datasets.imagenet3d\nimport datasets.kitti\nimport datasets.kitti_tracking\nimport numpy as np\n\ndef _selective_search_IJCV_top_k(split, year, top_k):\n    """"""Return an imdb that uses the top k proposals from the selective search\n    IJCV code.\n    """"""\n    imdb = datasets.pascal_voc(split, year)\n    imdb.roidb_handler = imdb.selective_search_IJCV_roidb\n    imdb.config[\'top_k\'] = top_k\n    return imdb\n\n# Set up voc_<year>_<split> using selective search ""fast"" mode\nfor year in [\'2007\', \'2012\']:\n    for split in [\'train\', \'val\', \'trainval\', \'test\']:\n        name = \'voc_{}_{}\'.format(year, split)\n        __sets[name] = (lambda split=split, year=year:\n                datasets.pascal_voc(split, year))\n""""""\n# Set up voc_<year>_<split>_top_<k> using selective search ""quality"" mode\n# but only returning the first k boxes\nfor top_k in np.arange(1000, 11000, 1000):\n    for year in [\'2007\', \'2012\']:\n        for split in [\'train\', \'val\', \'trainval\', \'test\']:\n            name = \'voc_{}_{}_top_{:d}\'.format(year, split, top_k)\n            __sets[name] = (lambda split=split, year=year, top_k=top_k:\n                    _selective_search_IJCV_top_k(split, year, top_k))\n""""""\n\n# Set up voc_<year>_<split> using selective search ""fast"" mode\nfor year in [\'2007\']:\n    for split in [\'train\', \'val\', \'trainval\', \'test\']:\n        name = \'voc_{}_{}\'.format(year, split)\n        print name\n        __sets[name] = (lambda split=split, year=year:\n                datasets.pascal_voc(split, year))\n\n# KITTI dataset\nfor split in [\'train\', \'val\', \'trainval\', \'test\']:\n    name = \'kitti_{}\'.format(split)\n    print name\n    __sets[name] = (lambda split=split:\n            datasets.kitti(split))\n\n# Set up coco_2014_<split>\nfor year in [\'2014\']:\n    for split in [\'train\', \'val\', \'minival\', \'valminusminival\']:\n        name = \'coco_{}_{}\'.format(year, split)\n        __sets[name] = (lambda split=split, year=year: coco(split, year))\n\n# Set up coco_2015_<split>\nfor year in [\'2015\']:\n    for split in [\'test\', \'test-dev\']:\n        name = \'coco_{}_{}\'.format(year, split)\n        __sets[name] = (lambda split=split, year=year: coco(split, year))\n\n# NTHU dataset\nfor split in [\'71\', \'370\']:\n    name = \'nthu_{}\'.format(split)\n    print name\n    __sets[name] = (lambda split=split:\n            datasets.nthu(split))\n\n\ndef get_imdb(name):\n    """"""Get an imdb (image database) by name.""""""\n    if not __sets.has_key(name):\n        raise KeyError(\'Unknown dataset: {}\'.format(name))\n    return __sets[name]()\n\ndef list_imdbs():\n    """"""List all registered imdbs.""""""\n    return __sets.keys()\n'"
lib/datasets/imagenet3d.py,0,"b'__author__ = \'yuxiang\' # derived from honda.py by fyang\n\nimport datasets\nimport datasets.imagenet3d\nimport os\nimport PIL\nimport datasets.imdb\nimport numpy as np\nimport scipy.sparse\nfrom utils.cython_bbox import bbox_overlaps\nfrom utils.boxes_grid import get_boxes_grid\nimport subprocess\nimport cPickle\nfrom fast_rcnn.config import cfg\nimport math\nfrom rpn_msr.generate_anchors import generate_anchors\nimport sys\n\nclass imagenet3d(datasets.imdb):\n    def __init__(self, image_set, imagenet3d_path=None):\n        datasets.imdb.__init__(self, \'imagenet3d_\' + image_set)\n        self._image_set = image_set\n        self._imagenet3d_path = self._get_default_path() if imagenet3d_path is None \\\n                            else imagenet3d_path\n        self._data_path = os.path.join(self._imagenet3d_path, \'Images\')\n        self._classes = (\'__background__\', \'aeroplane\', \'ashtray\', \'backpack\', \'basket\', \\\n             \'bed\', \'bench\', \'bicycle\', \'blackboard\', \'boat\', \'bookshelf\', \'bottle\', \'bucket\', \\\n             \'bus\', \'cabinet\', \'calculator\', \'camera\', \'can\', \'cap\', \'car\', \'cellphone\', \'chair\', \\\n             \'clock\', \'coffee_maker\', \'comb\', \'computer\', \'cup\', \'desk_lamp\', \'diningtable\', \\\n             \'dishwasher\', \'door\', \'eraser\', \'eyeglasses\', \'fan\', \'faucet\', \'filing_cabinet\', \\\n             \'fire_extinguisher\', \'fish_tank\', \'flashlight\', \'fork\', \'guitar\', \'hair_dryer\', \\\n             \'hammer\', \'headphone\', \'helmet\', \'iron\', \'jar\', \'kettle\', \'key\', \'keyboard\', \'knife\', \\\n             \'laptop\', \'lighter\', \'mailbox\', \'microphone\', \'microwave\', \'motorbike\', \'mouse\', \\\n             \'paintbrush\', \'pan\', \'pen\', \'pencil\', \'piano\', \'pillow\', \'plate\', \'pot\', \'printer\', \\\n             \'racket\', \'refrigerator\', \'remote_control\', \'rifle\', \'road_pole\', \'satellite_dish\', \\\n             \'scissors\', \'screwdriver\', \'shoe\', \'shovel\', \'sign\', \'skate\', \'skateboard\', \'slipper\', \\\n             \'sofa\', \'speaker\', \'spoon\', \'stapler\', \'stove\', \'suitcase\', \'teapot\', \'telephone\', \\\n             \'toaster\', \'toilet\', \'toothbrush\', \'train\', \'trash_bin\', \'trophy\', \'tub\', \'tvmonitor\', \\\n             \'vending_machine\', \'washing_machine\', \'watch\', \'wheelchair\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.JPEG\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        self.config = {\'top_k\': 100000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._imagenet3d_path), \\\n                \'imagenet3d path does not exist: {}\'.format(self._imagenet3d_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self.image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n\n        image_path = os.path.join(self._data_path, index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        image_set_file = os.path.join(self._imagenet3d_path, \'Image_sets\', self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n\n        with open(image_set_file) as f:\n            image_index = [x.rstrip(\'\\n\') for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where imagenet3d is expected to be installed.\n        """"""\n        return os.path.join(datasets.ROOT_DIR, \'data\', \'ImageNet3D\')\n\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n\n        cache_file = os.path.join(self.cache_path, self.name + \'_\' + cfg.SUBCLS_NAME + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_imagenet3d_annotation(index)\n                    for index in self.image_index]\n\n        if cfg.IS_RPN:\n            # print out recall\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n\n    def _load_imagenet3d_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the imagenet3d format.\n        """"""\n\n        if self._image_set == \'test\' or self._image_set == \'test_1\' or self._image_set == \'test_2\':\n            lines = []\n        else:\n            filename = os.path.join(self._imagenet3d_path, \'Labels\', index + \'.txt\')\n            lines = []\n            with open(filename) as f:\n                for line in f:\n                    lines.append(line)\n\n        num_objs = len(lines)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        viewpoints = np.zeros((num_objs, 3), dtype=np.float32)          # azimuth, elevation, in-plane rotation\n        viewpoints_flipped = np.zeros((num_objs, 3), dtype=np.float32)  # azimuth, elevation, in-plane rotation\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            assert len(words) == 5 or len(words) == 8, \'Wrong label format: {}\'.format(index)\n            cls = self._class_to_ind[words[0]]\n            boxes[ix, :] = [float(n) for n in words[1:5]]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n            if len(words) == 8:\n                viewpoints[ix, :] = [float(n) for n in words[5:8]]\n                # flip the viewpoint\n                viewpoints_flipped[ix, 0] = -viewpoints[ix, 0]  # azimuth\n                viewpoints_flipped[ix, 1] = viewpoints[ix, 1]   # elevation\n                viewpoints_flipped[ix, 2] = -viewpoints[ix, 2]  # in-plane rotation\n            else:\n                viewpoints[ix, :] = np.inf\n                viewpoints_flipped[ix, :] = np.inf\n\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        viewindexes_azimuth = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        viewindexes_azimuth_flipped = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        viewindexes_elevation = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        viewindexes_elevation_flipped = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        viewindexes_rotation = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        viewindexes_rotation_flipped = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n        viewindexes_azimuth = scipy.sparse.csr_matrix(viewindexes_azimuth)\n        viewindexes_azimuth_flipped = scipy.sparse.csr_matrix(viewindexes_azimuth_flipped)\n        viewindexes_elevation = scipy.sparse.csr_matrix(viewindexes_elevation)\n        viewindexes_elevation_flipped = scipy.sparse.csr_matrix(viewindexes_elevation_flipped)\n        viewindexes_rotation = scipy.sparse.csr_matrix(viewindexes_rotation)\n        viewindexes_rotation_flipped = scipy.sparse.csr_matrix(viewindexes_rotation_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                base_size = 16\n                ratios = cfg.TRAIN.RPN_ASPECTS\n                scales = cfg.TRAIN.RPN_SCALES\n                anchors = generate_anchors(base_size, ratios, scales)\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_viewpoints\': viewpoints,\n                \'gt_viewpoints_flipped\': viewpoints_flipped,\n                \'gt_viewindexes_azimuth\': viewindexes_azimuth,\n                \'gt_viewindexes_azimuth_flipped\': viewindexes_azimuth_flipped,\n                \'gt_viewindexes_elevation\': viewindexes_elevation,\n                \'gt_viewindexes_elevation_flipped\': viewindexes_elevation_flipped,\n                \'gt_viewindexes_rotation\': viewindexes_rotation,\n                \'gt_viewindexes_rotation_flipped\': viewindexes_rotation_flipped,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\' : overlaps,\n                \'gt_subindexes\': subindexes,\n                \'gt_subindexes_flipped\': subindexes_flipped,\n                \'flipped\' : False}\n\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            rpn_roidb = self._load_rpn_roidb(gt_roidb, model)\n            print \'Region proposal network boxes loaded\'\n            roidb = datasets.imdb.merge_roidbs(rpn_roidb, gt_roidb)\n        else:\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            roidb = self._load_rpn_roidb(None, model)\n            print \'Region proposal network boxes loaded\'\n\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        # print out recall\n        if self._image_set != \'test\':\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                if self._num_boxes_all[i] > 0:\n                    print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n\n        box_list = []\n        for ix, index in enumerate(self.image_index):\n            filename = os.path.join(self._imagenet3d_path, \'region_proposals\', model, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'{} data not found at: {}\'.format(model, filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            if model == \'selective_search\' or model == \'mcg\':\n                x1 = raw_data[:, 1].copy()\n                y1 = raw_data[:, 0].copy()\n                x2 = raw_data[:, 3].copy()\n                y2 = raw_data[:, 2].copy()\n            elif model == \'edge_boxes\':\n                x1 = raw_data[:, 0].copy()\n                y1 = raw_data[:, 1].copy()\n                x2 = raw_data[:, 2].copy() + raw_data[:, 0].copy()\n                y2 = raw_data[:, 3].copy() + raw_data[:, 1].copy()\n            elif model == \'rpn_caffenet\' or model == \'rpn_vgg16\':\n                x1 = raw_data[:, 0].copy()\n                y1 = raw_data[:, 1].copy()\n                x2 = raw_data[:, 2].copy()\n                y2 = raw_data[:, 3].copy()\n            else:\n                assert 1, \'region proposal not supported: {}\'.format(model)\n\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data[:, 0] = x1\n            raw_data[:, 1] = y1\n            raw_data[:, 2] = x2\n            raw_data[:, 3] = y2\n            raw_data = raw_data[inds,:4]\n\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n            print \'load {}: {}\'.format(model, index)\n\n            if gt_roidb is not None:\n                # compute overlaps between region proposals and gt boxes\n                boxes = gt_roidb[ix][\'boxes\'].copy()\n                gt_classes = gt_roidb[ix][\'gt_classes\'].copy()\n                # compute overlap\n                overlaps = bbox_overlaps(raw_data.astype(np.float), boxes.astype(np.float))\n                # check how many gt boxes are covered by anchors\n                if raw_data.shape[0] != 0:\n                    max_overlaps = overlaps.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n\n    def evaluate_detections(self, all_boxes, output_dir):\n\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing imagenet3d results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # detection and viewpoint\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:f} {:f} {:f} {:f} {:.32f} {:f} {:f} {:f}\\n\'.format(\\\n                                 cls, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4], dets[k, 6], dets[k, 7], dets[k, 8]))\n\n    # write detection results into one file\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n\n        # for each class\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            # open results file\n            filename = os.path.join(output_dir, \'detections_{}.txt\'.format(cls))\n            print \'Writing imagenet3d results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each image\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # detection and viewpoint\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:f} {:f} {:f} {:f} {:.32f} {:f} {:f} {:f}\\n\'.format(\\\n                                 index, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4], dets[k, 6], dets[k, 7], dets[k, 8]))\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing imagenet3d results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing imagenet3d results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\nif __name__ == \'__main__\':\n    d = datasets.imagenet3d(\'trainval\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/imdb.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nimport os.path as osp\nimport PIL\nfrom utils.cython_bbox import bbox_overlaps\nimport numpy as np\nimport scipy.sparse\nfrom fast_rcnn.config import cfg\n\n\nclass imdb(object):\n    """"""Image database.""""""\n\n    def __init__(self, name):\n        self._name = name\n        self._num_classes = 0\n        self._classes = []\n        self._image_index = []\n        self._obj_proposer = \'selective_search\'\n        self._roidb = None\n        print self.default_roidb\n        self._roidb_handler = self.default_roidb\n        # Use this dict for storing dataset specific config options\n        self.config = {}\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def num_classes(self):\n        return len(self._classes)\n\n    @property\n    def classes(self):\n        return self._classes\n\n    @property\n    def image_index(self):\n        return self._image_index\n\n    @property\n    def roidb_handler(self):\n        return self._roidb_handler\n\n    @roidb_handler.setter\n    def roidb_handler(self, val):\n        self._roidb_handler = val\n\n    def set_proposal_method(self, method):\n        method = eval(\'self.\' + method + \'_roidb\')\n        self.roidb_handler = method\n\n    @property\n    def roidb(self):\n        # A roidb is a list of dictionaries, each with the following keys:\n        #   boxes\n        #   gt_overlaps\n        #   gt_classes\n        #   flipped\n        if self._roidb is not None:\n            return self._roidb\n        self._roidb = self.roidb_handler()\n        return self._roidb\n\n    @property\n    def cache_path(self):\n        cache_path = osp.abspath(osp.join(cfg.DATA_DIR, \'cache\'))\n        if not os.path.exists(cache_path):\n            os.makedirs(cache_path)\n        return cache_path\n\n    @property\n    def num_images(self):\n      return len(self.image_index)\n\n    def image_path_at(self, i):\n        raise NotImplementedError\n\n    def default_roidb(self):\n        raise NotImplementedError\n\n    def evaluate_detections(self, all_boxes, output_dir=None):\n        """"""\n        all_boxes is a list of length number-of-classes.\n        Each list element is a list of length number-of-images.\n        Each of those list elements is either an empty list []\n        or a numpy array of detection.\n\n        all_boxes[class][image] = [] or np.array of shape #dets x 5\n        """"""\n        raise NotImplementedError\n\n    def _get_widths(self):\n      return [PIL.Image.open(self.image_path_at(i)).size[0]\n              for i in xrange(self.num_images)]\n\n    def append_flipped_images(self):\n        num_images = self.num_images\n        widths = self._get_widths()\n        for i in xrange(num_images):\n            boxes = self.roidb[i][\'boxes\'].copy()\n            oldx1 = boxes[:, 0].copy()\n            oldx2 = boxes[:, 2].copy()\n            boxes[:, 0] = widths[i] - oldx2 - 1\n            boxes[:, 2] = widths[i] - oldx1 - 1\n            assert (boxes[:, 2] >= boxes[:, 0]).all()\n            entry = {\'boxes\' : boxes,\n                     \'gt_overlaps\' : self.roidb[i][\'gt_overlaps\'],\n                     \'gt_classes\' : self.roidb[i][\'gt_classes\'],\n                     \'flipped\' : True}\n            self.roidb.append(entry)\n        self._image_index = self._image_index * 2\n\n    def evaluate_recall(self, candidate_boxes=None, thresholds=None,\n                        area=\'all\', limit=None):\n        """"""Evaluate detection proposal recall metrics.\n\n        Returns:\n            results: dictionary of results with keys\n                \'ar\': average recall\n                \'recalls\': vector recalls at each IoU overlap threshold\n                \'thresholds\': vector of IoU overlap thresholds\n                \'gt_overlaps\': vector of all ground-truth overlaps\n        """"""\n        # Record max overlap value for each gt box\n        # Return vector of overlap values\n        areas = { \'all\': 0, \'small\': 1, \'medium\': 2, \'large\': 3,\n                  \'96-128\': 4, \'128-256\': 5, \'256-512\': 6, \'512-inf\': 7}\n        area_ranges = [ [0**2, 1e5**2],    # all\n                        [0**2, 32**2],     # small\n                        [32**2, 96**2],    # medium\n                        [96**2, 1e5**2],   # large\n                        [96**2, 128**2],   # 96-128\n                        [128**2, 256**2],  # 128-256\n                        [256**2, 512**2],  # 256-512\n                        [512**2, 1e5**2],  # 512-inf\n                      ]\n        assert areas.has_key(area), \'unknown area range: {}\'.format(area)\n        area_range = area_ranges[areas[area]]\n        gt_overlaps = np.zeros(0)\n        num_pos = 0\n        for i in xrange(self.num_images):\n            # Checking for max_overlaps == 1 avoids including crowd annotations\n            # (...pretty hacking :/)\n            max_gt_overlaps = self.roidb[i][\'gt_overlaps\'].toarray().max(axis=1)\n            gt_inds = np.where((self.roidb[i][\'gt_classes\'] > 0) &\n                               (max_gt_overlaps == 1))[0]\n            gt_boxes = self.roidb[i][\'boxes\'][gt_inds, :]\n            gt_areas = self.roidb[i][\'seg_areas\'][gt_inds]\n            valid_gt_inds = np.where((gt_areas >= area_range[0]) &\n                                     (gt_areas <= area_range[1]))[0]\n            gt_boxes = gt_boxes[valid_gt_inds, :]\n            num_pos += len(valid_gt_inds)\n\n            if candidate_boxes is None:\n                # If candidate_boxes is not supplied, the default is to use the\n                # non-ground-truth boxes from this roidb\n                non_gt_inds = np.where(self.roidb[i][\'gt_classes\'] == 0)[0]\n                boxes = self.roidb[i][\'boxes\'][non_gt_inds, :]\n            else:\n                boxes = candidate_boxes[i]\n            if boxes.shape[0] == 0:\n                continue\n            if limit is not None and boxes.shape[0] > limit:\n                boxes = boxes[:limit, :]\n\n            overlaps = bbox_overlaps(boxes.astype(np.float),\n                                     gt_boxes.astype(np.float))\n\n            _gt_overlaps = np.zeros((gt_boxes.shape[0]))\n            for j in xrange(gt_boxes.shape[0]):\n                # find which proposal box maximally covers each gt box\n                argmax_overlaps = overlaps.argmax(axis=0)\n                # and get the iou amount of coverage for each gt box\n                max_overlaps = overlaps.max(axis=0)\n                # find which gt box is \'best\' covered (i.e. \'best\' = most iou)\n                gt_ind = max_overlaps.argmax()\n                gt_ovr = max_overlaps.max()\n                assert(gt_ovr >= 0)\n                # find the proposal box that covers the best covered gt box\n                box_ind = argmax_overlaps[gt_ind]\n                # record the iou coverage of this gt box\n                _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n                assert(_gt_overlaps[j] == gt_ovr)\n                # mark the proposal box and the gt box as used\n                overlaps[box_ind, :] = -1\n                overlaps[:, gt_ind] = -1\n            # append recorded iou coverage level\n            gt_overlaps = np.hstack((gt_overlaps, _gt_overlaps))\n\n        gt_overlaps = np.sort(gt_overlaps)\n        if thresholds is None:\n            step = 0.05\n            thresholds = np.arange(0.5, 0.95 + 1e-5, step)\n        recalls = np.zeros_like(thresholds)\n        # compute recall for each iou threshold\n        for i, t in enumerate(thresholds):\n            recalls[i] = (gt_overlaps >= t).sum() / float(num_pos)\n        # ar = 2 * np.trapz(recalls, thresholds)\n        ar = recalls.mean()\n        return {\'ar\': ar, \'recalls\': recalls, \'thresholds\': thresholds,\n                \'gt_overlaps\': gt_overlaps}\n\n    def create_roidb_from_box_list(self, box_list, gt_roidb):\n        assert len(box_list) == self.num_images, \\\n                \'Number of boxes must match number of ground-truth images\'\n        roidb = []\n        for i in xrange(self.num_images):\n            boxes = box_list[i]\n            num_boxes = boxes.shape[0]\n            overlaps = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n\n            if gt_roidb is not None and gt_roidb[i][\'boxes\'].size > 0:\n                gt_boxes = gt_roidb[i][\'boxes\']\n                gt_classes = gt_roidb[i][\'gt_classes\']\n                gt_overlaps = bbox_overlaps(boxes.astype(np.float),\n                                            gt_boxes.astype(np.float))\n                argmaxes = gt_overlaps.argmax(axis=1)\n                maxes = gt_overlaps.max(axis=1)\n                I = np.where(maxes > 0)[0]\n                overlaps[I, gt_classes[argmaxes[I]]] = maxes[I]\n\n            overlaps = scipy.sparse.csr_matrix(overlaps)\n            roidb.append({\n                \'boxes\' : boxes,\n                \'gt_classes\' : np.zeros((num_boxes,), dtype=np.int32),\n                \'gt_overlaps\' : overlaps,\n                \'flipped\' : False,\n                \'seg_areas\' : np.zeros((num_boxes,), dtype=np.float32),\n            })\n        return roidb\n\n    @staticmethod\n    def merge_roidbs(a, b):\n        assert len(a) == len(b)\n        for i in xrange(len(a)):\n            a[i][\'boxes\'] = np.vstack((a[i][\'boxes\'], b[i][\'boxes\']))\n            a[i][\'gt_classes\'] = np.hstack((a[i][\'gt_classes\'],\n                                            b[i][\'gt_classes\']))\n            a[i][\'gt_overlaps\'] = scipy.sparse.vstack([a[i][\'gt_overlaps\'],\n                                                       b[i][\'gt_overlaps\']])\n            a[i][\'seg_areas\'] = np.hstack((a[i][\'seg_areas\'],\n                                           b[i][\'seg_areas\']))\n        return a\n\n    def competition_mode(self, on):\n        """"""Turn competition mode on or off.""""""\n        pass\n'"
lib/datasets/imdb2.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nimport os.path as osp\nimport PIL\nfrom utils.cython_bbox import bbox_overlaps\nimport numpy as np\nimport scipy.sparse\nimport datasets\nfrom fast_rcnn.config import cfg\n\nclass imdb(object):\n    """"""Image database.""""""\n\n    def __init__(self, name):\n        self._name = name\n        self._num_classes = 0\n        self._num_subclasses = 0\n        self._subclass_mapping = []\n        self._classes = []\n        self._image_index = []\n        self._obj_proposer = \'selective_search\'\n        self._roidb = None\n        self._roidb_handler = self.default_roidb\n        # Use this dict for storing dataset specific config options\n        self.config = {}\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def num_classes(self):\n        return len(self._classes)\n\n    @property\n    def num_subclasses(self):\n        return self._num_subclasses\n\n    @property\n    def subclass_mapping(self):\n        return self._subclass_mapping\n\n    @property\n    def classes(self):\n        return self._classes\n\n    @property\n    def image_index(self):\n        return self._image_index\n\n    @property\n    def roidb_handler(self):\n        return self._roidb_handler\n\n    @roidb_handler.setter\n    def roidb_handler(self, val):\n        self._roidb_handler = val\n\n    @property\n    def roidb(self):\n        # A roidb is a list of dictionaries, each with the following keys:\n        #   boxes\n        #   gt_overlaps\n        #   gt_classes\n        #   flipped\n        if self._roidb is not None:\n            return self._roidb\n        self._roidb = self.roidb_handler()\n        return self._roidb\n\n    @property\n    def cache_path(self):\n        cache_path = osp.abspath(osp.join(datasets.ROOT_DIR, \'data\', \'cache\'))\n        if not os.path.exists(cache_path):\n            os.makedirs(cache_path)\n        return cache_path\n\n    @property\n    def num_images(self):\n      return len(self.image_index)\n\n    def image_path_at(self, i):\n        raise NotImplementedError\n\n    def default_roidb(self):\n        raise NotImplementedError\n\n    def evaluate_detections(self, all_boxes, output_dir=None):\n        """"""\n        all_boxes is a list of length number-of-classes.\n        Each list element is a list of length number-of-images.\n        Each of those list elements is either an empty list []\n        or a numpy array of detection.\n\n        all_boxes[class][image] = [] or np.array of shape #dets x 5\n        """"""\n        raise NotImplementedError\n\n    def evaluate_proposals(self, all_boxes, output_dir=None):\n        """"""\n        all_boxes is a list of length number-of-classes.\n        Each list element is a list of length number-of-images.\n        Each of those list elements is either an empty list []\n        or a numpy array of detection.\n\n        all_boxes[class][image] = [] or np.array of shape #dets x 5\n        """"""\n        raise NotImplementedError\n\n    def append_flipped_images(self):\n        num_images = self.num_images\n        widths = [PIL.Image.open(self.image_path_at(i)).size[0]\n                  for i in xrange(num_images)]\n        for i in xrange(num_images):\n            boxes = self.roidb[i][\'boxes\'].copy()\n            oldx1 = boxes[:, 0].copy()\n            oldx2 = boxes[:, 2].copy()\n            boxes[:, 0] = widths[i] - oldx2 - 1\n            boxes[:, 2] = widths[i] - oldx1 - 1\n            assert (boxes[:, 2] >= boxes[:, 0]).all()\n            assert (self.roidb[i][\'gt_subindexes\'].shape == self.roidb[i][\'gt_subindexes_flipped\'].shape), \\\n                \'gt_subindexes {}, gt_subindexes_flip {}\'.format(self.roidb[i][\'gt_subindexes\'].shape, \n                                                                 self.roidb[i][\'gt_subindexes_flipped\'].shape)\n\n            if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                entry = {\'boxes\' : boxes,\n                         \'gt_overlaps\' : self.roidb[i][\'gt_overlaps\'],\n                         \'gt_classes\' : self.roidb[i][\'gt_classes\'],\n                         \'gt_viewpoints\' : self.roidb[i][\'gt_viewpoints_flipped\'],\n                         \'gt_viewpoints_flipped\' : self.roidb[i][\'gt_viewpoints\'],\n                         \'gt_viewindexes_azimuth\' : self.roidb[i][\'gt_viewindexes_azimuth_flipped\'],\n                         \'gt_viewindexes_azimuth_flipped\' : self.roidb[i][\'gt_viewindexes_azimuth\'],\n                         \'gt_viewindexes_elevation\' : self.roidb[i][\'gt_viewindexes_elevation_flipped\'],\n                         \'gt_viewindexes_elevation_flipped\' : self.roidb[i][\'gt_viewindexes_elevation\'],\n                         \'gt_viewindexes_rotation\' : self.roidb[i][\'gt_viewindexes_rotation_flipped\'],\n                         \'gt_viewindexes_rotation_flipped\' : self.roidb[i][\'gt_viewindexes_rotation\'],\n                         \'gt_subclasses\' : self.roidb[i][\'gt_subclasses_flipped\'],\n                         \'gt_subclasses_flipped\' : self.roidb[i][\'gt_subclasses\'],\n                         \'gt_subindexes\' : self.roidb[i][\'gt_subindexes_flipped\'],\n                         \'gt_subindexes_flipped\' : self.roidb[i][\'gt_subindexes\'],\n                         \'flipped\' : True}\n            else:\n                entry = {\'boxes\' : boxes,\n                         \'gt_overlaps\' : self.roidb[i][\'gt_overlaps\'],\n                         \'gt_classes\' : self.roidb[i][\'gt_classes\'],\n                         \'gt_subclasses\' : self.roidb[i][\'gt_subclasses_flipped\'],\n                         \'gt_subclasses_flipped\' : self.roidb[i][\'gt_subclasses\'],\n                         \'gt_subindexes\' : self.roidb[i][\'gt_subindexes_flipped\'],\n                         \'gt_subindexes_flipped\' : self.roidb[i][\'gt_subindexes\'],\n                         \'flipped\' : True}\n            self.roidb.append(entry)\n        self._image_index = self._image_index * 2\n        print \'finish appending flipped images\'\n\n    def evaluate_recall(self, candidate_boxes, ar_thresh=0.5):\n        # Record max overlap value for each gt box\n        # Return vector of overlap values\n        gt_overlaps = np.zeros(0)\n        for i in xrange(self.num_images):\n            gt_inds = np.where(self.roidb[i][\'gt_classes\'] > 0)[0]\n            gt_boxes = self.roidb[i][\'boxes\'][gt_inds, :]\n\n            boxes = candidate_boxes[i]\n            if boxes.shape[0] == 0:\n                continue\n            overlaps = bbox_overlaps(boxes.astype(np.float),\n                                     gt_boxes.astype(np.float))\n\n            # gt_overlaps = np.hstack((gt_overlaps, overlaps.max(axis=0)))\n            _gt_overlaps = np.zeros((gt_boxes.shape[0]))\n            for j in xrange(gt_boxes.shape[0]):\n                argmax_overlaps = overlaps.argmax(axis=0)\n                max_overlaps = overlaps.max(axis=0)\n                gt_ind = max_overlaps.argmax()\n                gt_ovr = max_overlaps.max()\n                assert(gt_ovr >= 0)\n                box_ind = argmax_overlaps[gt_ind]\n                _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n                assert(_gt_overlaps[j] == gt_ovr)\n                overlaps[box_ind, :] = -1\n                overlaps[:, gt_ind] = -1\n\n            gt_overlaps = np.hstack((gt_overlaps, _gt_overlaps))\n\n        num_pos = gt_overlaps.size\n        gt_overlaps = np.sort(gt_overlaps)\n        step = 0.001\n        thresholds = np.minimum(np.arange(0.5, 1.0 + step, step), 1.0)\n        recalls = np.zeros_like(thresholds)\n        for i, t in enumerate(thresholds):\n            recalls[i] = (gt_overlaps >= t).sum() / float(num_pos)\n        ar = 2 * np.trapz(recalls, thresholds)\n\n        return ar, gt_overlaps, recalls, thresholds\n\n    def create_roidb_from_box_list(self, box_list, gt_roidb):\n        assert len(box_list) == self.num_images, \\\n                \'Number of boxes must match number of ground-truth images\'\n        roidb = []\n        for i in xrange(self.num_images):\n            boxes = box_list[i]\n            num_boxes = boxes.shape[0]\n            overlaps = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n            subindexes = np.zeros((num_boxes, self.num_classes), dtype=np.int32)\n            subindexes_flipped = np.zeros((num_boxes, self.num_classes), dtype=np.int32)\n            if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                viewindexes_azimuth = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n                viewindexes_azimuth_flipped = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n                viewindexes_elevation = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n                viewindexes_elevation_flipped = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n                viewindexes_rotation = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n                viewindexes_rotation_flipped = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n\n            if gt_roidb is not None:\n                gt_boxes = gt_roidb[i][\'boxes\']\n                if gt_boxes.shape[0] != 0 and num_boxes != 0:\n                    gt_classes = gt_roidb[i][\'gt_classes\']\n                    gt_subclasses = gt_roidb[i][\'gt_subclasses\']\n                    gt_subclasses_flipped = gt_roidb[i][\'gt_subclasses_flipped\']\n                    if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                        gt_viewpoints = gt_roidb[i][\'gt_viewpoints\']\n                        gt_viewpoints_flipped = gt_roidb[i][\'gt_viewpoints_flipped\']\n                    gt_overlaps = bbox_overlaps(boxes.astype(np.float),\n                                            gt_boxes.astype(np.float))\n                    argmaxes = gt_overlaps.argmax(axis=1)\n                    maxes = gt_overlaps.max(axis=1)\n                    I = np.where(maxes > 0)[0]\n                    overlaps[I, gt_classes[argmaxes[I]]] = maxes[I]\n                    subindexes[I, gt_classes[argmaxes[I]]] = gt_subclasses[argmaxes[I]]\n                    subindexes_flipped[I, gt_classes[argmaxes[I]]] = gt_subclasses_flipped[argmaxes[I]]\n                    if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                        viewindexes_azimuth[I, gt_classes[argmaxes[I]]] = gt_viewpoints[argmaxes[I], 0]\n                        viewindexes_azimuth_flipped[I, gt_classes[argmaxes[I]]] = gt_viewpoints_flipped[argmaxes[I], 0]\n                        viewindexes_elevation[I, gt_classes[argmaxes[I]]] = gt_viewpoints[argmaxes[I], 1]\n                        viewindexes_elevation_flipped[I, gt_classes[argmaxes[I]]] = gt_viewpoints_flipped[argmaxes[I], 1]\n                        viewindexes_rotation[I, gt_classes[argmaxes[I]]] = gt_viewpoints[argmaxes[I], 2]\n                        viewindexes_rotation_flipped[I, gt_classes[argmaxes[I]]] = gt_viewpoints_flipped[argmaxes[I], 2]\n\n            overlaps = scipy.sparse.csr_matrix(overlaps)\n            subindexes = scipy.sparse.csr_matrix(subindexes)\n            subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n            if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                viewindexes_azimuth = scipy.sparse.csr_matrix(viewindexes_azimuth)\n                viewindexes_azimuth_flipped = scipy.sparse.csr_matrix(viewindexes_azimuth_flipped)\n                viewindexes_elevation = scipy.sparse.csr_matrix(viewindexes_elevation)\n                viewindexes_elevation_flipped = scipy.sparse.csr_matrix(viewindexes_elevation_flipped)\n                viewindexes_rotation = scipy.sparse.csr_matrix(viewindexes_rotation)\n                viewindexes_rotation_flipped = scipy.sparse.csr_matrix(viewindexes_rotation_flipped)\n                roidb.append({\'boxes\' : boxes,\n                              \'gt_classes\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_viewpoints\': np.zeros((num_boxes, 3), dtype=np.float32),\n                              \'gt_viewpoints_flipped\': np.zeros((num_boxes, 3), dtype=np.float32),\n                              \'gt_viewindexes_azimuth\': viewindexes_azimuth,\n                              \'gt_viewindexes_azimuth_flipped\': viewindexes_azimuth_flipped,\n                              \'gt_viewindexes_elevation\': viewindexes_elevation,\n                              \'gt_viewindexes_elevation_flipped\': viewindexes_elevation_flipped,\n                              \'gt_viewindexes_rotation\': viewindexes_rotation,\n                              \'gt_viewindexes_rotation_flipped\': viewindexes_rotation_flipped,\n                              \'gt_subclasses\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_subclasses_flipped\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_overlaps\' : overlaps,\n                              \'gt_subindexes\': subindexes,\n                              \'gt_subindexes_flipped\': subindexes_flipped,\n                              \'flipped\' : False})\n            else:\n                roidb.append({\'boxes\' : boxes,\n                              \'gt_classes\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_subclasses\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_subclasses_flipped\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_overlaps\' : overlaps,\n                              \'gt_subindexes\': subindexes,\n                              \'gt_subindexes_flipped\': subindexes_flipped,\n                              \'flipped\' : False})\n        return roidb\n\n    @staticmethod\n    def merge_roidbs(a, b):\n        assert len(a) == len(b)\n        for i in xrange(len(a)):\n            a[i][\'boxes\'] = np.vstack((a[i][\'boxes\'], b[i][\'boxes\']))\n            a[i][\'gt_classes\'] = np.hstack((a[i][\'gt_classes\'],\n                                            b[i][\'gt_classes\']))\n            if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                a[i][\'gt_viewpoints\'] = np.vstack((a[i][\'gt_viewpoints\'],\n                                                b[i][\'gt_viewpoints\']))\n                a[i][\'gt_viewpoints_flipped\'] = np.vstack((a[i][\'gt_viewpoints_flipped\'],\n                                                b[i][\'gt_viewpoints_flipped\']))\n                a[i][\'gt_viewindexes_azimuth\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_azimuth\'],\n                                                b[i][\'gt_viewindexes_azimuth\']))\n                a[i][\'gt_viewindexes_azimuth_flipped\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_azimuth_flipped\'],\n                                                b[i][\'gt_viewindexes_azimuth_flipped\']))\n                a[i][\'gt_viewindexes_elevation\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_elevation\'],\n                                                b[i][\'gt_viewindexes_elevation\']))\n                a[i][\'gt_viewindexes_elevation_flipped\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_elevation_flipped\'],\n                                                b[i][\'gt_viewindexes_elevation_flipped\']))\n                a[i][\'gt_viewindexes_rotation\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_rotation\'],\n                                                b[i][\'gt_viewindexes_rotation\']))\n                a[i][\'gt_viewindexes_rotation_flipped\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_rotation_flipped\'],\n                                                b[i][\'gt_viewindexes_rotation_flipped\']))\n\n            a[i][\'gt_subclasses\'] = np.hstack((a[i][\'gt_subclasses\'],\n                                            b[i][\'gt_subclasses\']))\n            a[i][\'gt_subclasses_flipped\'] = np.hstack((a[i][\'gt_subclasses_flipped\'],\n                                            b[i][\'gt_subclasses_flipped\']))\n            a[i][\'gt_overlaps\'] = scipy.sparse.vstack([a[i][\'gt_overlaps\'],\n                                                       b[i][\'gt_overlaps\']])\n            a[i][\'gt_subindexes\'] = scipy.sparse.vstack((a[i][\'gt_subindexes\'],\n                                            b[i][\'gt_subindexes\']))\n            a[i][\'gt_subindexes_flipped\'] = scipy.sparse.vstack((a[i][\'gt_subindexes_flipped\'],\n                                            b[i][\'gt_subindexes_flipped\']))\n        return a\n\n    def competition_mode(self, on):\n        """"""Turn competition mode on or off.""""""\n        pass\n'"
lib/datasets/kitti.py,0,"b'__author__ = \'yuxiang\' # derived from honda.py by fyang\n\nimport datasets\nimport datasets.kitti\nimport os\nimport PIL\nimport datasets.imdb\nimport numpy as np\nimport scipy.sparse\nfrom utils.cython_bbox import bbox_overlaps\nfrom utils.boxes_grid import get_boxes_grid\nimport subprocess\nimport cPickle\nfrom fast_rcnn.config import cfg\nimport math\nfrom rpn_msr.generate_anchors import generate_anchors\n\nclass kitti(datasets.imdb):\n    def __init__(self, image_set, kitti_path=None):\n        datasets.imdb.__init__(self, \'kitti_\' + image_set)\n        self._image_set = image_set\n        self._kitti_path = self._get_default_path() if kitti_path is None \\\n                            else kitti_path\n        self._data_path = os.path.join(self._kitti_path, \'data_object_image_2\')\n        self._classes = (\'__background__\', \'Car\', \'Pedestrian\', \'Cyclist\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.png\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        if image_set == \'train\' or image_set == \'val\':\n            self._num_subclasses = 125 + 24 + 24 + 1\n            prefix = \'validation\'\n        else:\n            self._num_subclasses = 227 + 36 + 36 + 1\n            prefix = \'test\'\n\n        # load the mapping for subcalss to class\n        filename = os.path.join(self._kitti_path, cfg.SUBCLS_NAME, prefix, \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n        \n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        self.config = {\'top_k\': 100000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._kitti_path), \\\n                \'KITTI path does not exist: {}\'.format(self._kitti_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self.image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # set the prefix\n        if self._image_set == \'test\':\n            prefix = \'testing/image_2\'\n        else:\n            prefix = \'training/image_2\'\n\n        image_path = os.path.join(self._data_path, prefix, index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        image_set_file = os.path.join(self._kitti_path, self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n\n        with open(image_set_file) as f:\n            image_index = [x.rstrip(\'\\n\') for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where KITTI is expected to be installed.\n        """"""\n        return os.path.join(datasets.ROOT_DIR, \'data\', \'KITTI\')\n\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n\n        cache_file = os.path.join(self.cache_path, self.name + \'_\' + cfg.SUBCLS_NAME + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_kitti_voxel_exemplar_annotation(index)\n                    for index in self.image_index]\n\n        if cfg.IS_RPN:\n            # print out recall\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n\n    def _load_kitti_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the KITTI format.\n        """"""\n\n        if self._image_set == \'test\':\n            lines = []\n        else:\n            filename = os.path.join(self._data_path, \'training\', \'label_2\', index + \'.txt\')\n            lines = []\n            with open(filename) as f:\n                for line in f:\n                    line = line.replace(\'Van\', \'Car\')\n                    words = line.split()\n                    cls = words[0]\n                    truncation = float(words[1])\n                    occlusion = int(words[2])\n                    height = float(words[7]) - float(words[5])\n                    if cls in self._class_to_ind and truncation < 0.5 and occlusion < 3 and height > 25:\n                        lines.append(line)\n\n        num_objs = len(lines)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            cls = self._class_to_ind[words[0]]\n            boxes[ix, :] = [float(n) for n in words[4:8]]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                anchors = generate_anchors()\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\' : overlaps,\n                \'gt_subindexes\': subindexes,\n                \'gt_subindexes_flipped\': subindexes_flipped,\n                \'flipped\' : False}\n\n    def _load_kitti_voxel_exemplar_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the KITTI voxel exemplar format.\n        """"""\n        if self._image_set == \'train\':\n            prefix = \'validation\'\n        elif self._image_set == \'trainval\':\n            prefix = \'test\'\n        else:\n            return self._load_kitti_annotation(index)\n\n        filename = os.path.join(self._kitti_path, cfg.SUBCLS_NAME, prefix, index + \'.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        # the annotation file contains flipped objects    \n        lines = []\n        lines_flipped = []\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[1])\n                is_flip = int(words[2])\n                if subcls != -1:\n                    if is_flip == 0:\n                        lines.append(line)\n                    else:\n                        lines_flipped.append(line)\n        \n        num_objs = len(lines)\n\n        # store information of flipped objects\n        assert (num_objs == len(lines_flipped)), \'The number of flipped objects is not the same!\'\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        \n        for ix, line in enumerate(lines_flipped):\n            words = line.split()\n            subcls = int(words[1])\n            gt_subclasses_flipped[ix] = subcls\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            cls = self._class_to_ind[words[0]]\n            subcls = int(words[1])\n            boxes[ix, :] = [float(n) for n in words[3:7]]\n            gt_classes[ix] = cls\n            gt_subclasses[ix] = subcls\n            overlaps[ix, cls] = 1.0\n            subindexes[ix, cls] = subcls\n            subindexes_flipped[ix, cls] = gt_subclasses_flipped[ix]\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                base_size = 16\n                ratios = [3.0, 2.0, 1.5, 1.0, 0.75, 0.5, 0.25]\n                scales = 2**np.arange(1, 6, 0.5)\n                anchors = generate_anchors(base_size, ratios, scales)\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\': overlaps,\n                \'gt_subindexes\': subindexes, \n                \'gt_subindexes_flipped\': subindexes_flipped, \n                \'flipped\' : False}\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.SUBCLS_NAME + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n\n            print \'Loading region proposal network boxes...\'\n            if self._image_set == \'trainval\':\n                model = cfg.REGION_PROPOSAL + \'_227/\'\n            else:\n                model = cfg.REGION_PROPOSAL + \'_125/\'\n            rpn_roidb = self._load_rpn_roidb(gt_roidb, model)\n            print \'Region proposal network boxes loaded\'\n            roidb = datasets.imdb.merge_roidbs(rpn_roidb, gt_roidb)\n\n            # print \'Loading voxel pattern boxes...\'\n            # if self._image_set == \'trainval\':\n            #    model = \'3DVP_227\'\n            # else:\n            #    model = \'3DVP_125/\'\n            # vp_roidb = self._load_voxel_pattern_roidb(gt_roidb, model)\n            # print \'Voxel pattern boxes loaded\'\n            # roidb = datasets.imdb.merge_roidbs(vp_roidb, gt_roidb)\n\n            # print \'Loading selective search boxes...\'\n            # ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            # print \'Selective search boxes loaded\'\n\n            # print \'Loading ACF boxes...\'\n            # acf_roidb = self._load_acf_roidb(gt_roidb)\n            # print \'ACF boxes loaded\'\n\n            # roidb = datasets.imdb.merge_roidbs(ss_roidb, gt_roidb)\n            # roidb = datasets.imdb.merge_roidbs(roidb, acf_roidb)\n        else:\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL + \'_227/\'\n            roidb = self._load_rpn_roidb(None, model)\n            print \'Region proposal network boxes loaded\'\n\n            # print \'Loading voxel pattern boxes...\'\n            # model = \'3DVP_227/\'\n            # roidb = self._load_voxel_pattern_roidb(None, model)\n            # print \'Voxel pattern boxes loaded\'\n\n            # print \'Loading selective search boxes...\'\n            # roidb = self._load_selective_search_roidb(None)\n            # print \'Selective search boxes loaded\'\n\n            # print \'Loading ACF boxes...\'\n            # acf_roidb = self._load_acf_roidb(None)\n            # print \'ACF boxes loaded\'\n\n            # roidb = datasets.imdb.merge_roidbs(roidb, acf_roidb)\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        if self._image_set == \'test\':\n            prefix = model + \'testing\'\n        else:\n            prefix = model + \'training\'\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._kitti_path, \'region_proposals\',  prefix, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n            print \'load {}: {}\'.format(model, index)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_voxel_pattern_roidb(self, gt_roidb, model):\n        # set the prefix\n        if self._image_set == \'test\':\n            prefix = model + \'testing\'\n        else:\n            prefix = model + \'training\'\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._kitti_path, \'region_proposals\',  prefix, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'Voxel pattern data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 4))\n                else:\n                    raw_data = raw_data.reshape((1, 4))\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_box_list.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                box_list = cPickle.load(fid)\n            print \'{} boxes loaded from {}\'.format(self.name, cache_file)\n        else:\n            # set the prefix\n            model = \'selective_search/\'\n            if self._image_set == \'test\':\n                prefix = model + \'testing\'\n            else:\n                prefix = model + \'training\'\n\n            box_list = []\n            for index in self.image_index:\n                filename = os.path.join(self._kitti_path, \'region_proposals\', prefix, index + \'.txt\')\n                assert os.path.exists(filename), \\\n                    \'Selective search data not found at: {}\'.format(filename)\n                raw_data = np.loadtxt(filename, dtype=float)\n                box_list.append(raw_data[:min(self.config[\'top_k\'], raw_data.shape[0]), 1:])\n\n            with open(cache_file, \'wb\') as fid:\n                cPickle.dump(box_list, fid, cPickle.HIGHEST_PROTOCOL)\n            print \'wrote selective search boxes to {}\'.format(cache_file)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_acf_roidb(self, gt_roidb):\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_acf_box_list.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                box_list = cPickle.load(fid)\n            print \'{} boxes loaded from {}\'.format(self.name, cache_file)\n        else:\n            # set the prefix\n            model = \'ACF/\'\n            if self._image_set == \'test\':\n                prefix = model + \'testing\'\n            else:\n                prefix = model + \'training\'\n\n            box_list = []\n            for index in self.image_index:\n                filename = os.path.join(self._kitti_path, \'region_proposals\', prefix, index + \'.txt\')\n                assert os.path.exists(filename), \\\n                    \'ACF data not found at: {}\'.format(filename)\n                raw_data = np.loadtxt(filename, usecols=(2,3,4,5), dtype=float)\n                box_list.append(raw_data[:min(self.config[\'top_k\'], raw_data.shape[0]), :])\n\n            with open(cache_file, \'wb\') as fid:\n                cPickle.dump(box_list, fid, cPickle.HIGHEST_PROTOCOL)\n            print \'wrote ACF boxes to {}\'.format(cache_file)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        # load the mapping for subcalss the alpha (viewpoint)\n        if self._image_set == \'val\':\n            prefix = \'validation\'\n        elif self._image_set == \'test\':\n            prefix = \'test\'\n        else:\n            prefix = \'\'\n\n        filename = os.path.join(self._kitti_path, cfg.SUBCLS_NAME, prefix, \'mapping.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[3])\n\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing KITTI results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        if cfg.TEST.SUBCLS:\n                            subcls = int(dets[k, 5])\n                            cls_name = self.classes[self.subclass_mapping[subcls]]\n                            assert (cls_name == cls), \'subclass not in class\'\n                            alpha = mapping[subcls]\n                        else:\n                            alpha = -10\n                        f.write(\'{:s} -1 -1 {:f} {:f} {:f} {:f} {:f} -1 -1 -1 -1 -1 -1 -1 {:.32f}\\n\'.format(\\\n                                 cls, alpha, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    # write detection results into one file\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n        # open results file\n        filename = os.path.join(output_dir, \'detections.txt\')\n        print \'Writing all KITTI results to file \' + filename\n        with open(filename, \'wt\') as f:\n            # for each image\n            for im_ind, index in enumerate(self.image_index):\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        if cfg.TEST.SUBCLS:\n                            subcls = int(dets[k, 5])\n                            cls_name = self.classes[self.subclass_mapping[subcls]]\n                            assert (cls_name == cls), \'subclass not in class\'\n                        else:\n                            subcls = -1\n                        f.write(\'{:s} {:s} {:f} {:f} {:f} {:f} {:d} {:f}\\n\'.format(\\\n                                 index, cls, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], subcls, dets[k, 4]))\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing KITTI results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing KITTI results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\nif __name__ == \'__main__\':\n    d = datasets.kitti(\'train\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/kitti_tracking.py,0,"b'__author__ = \'yuxiang\'\n\nimport datasets\nimport datasets.kitti_tracking\nimport os\nimport PIL\nimport datasets.imdb\nimport numpy as np\nimport scipy.sparse\nfrom utils.cython_bbox import bbox_overlaps\nfrom utils.boxes_grid import get_boxes_grid\nimport subprocess\nimport cPickle\nfrom fast_rcnn.config import cfg\nimport math\nfrom rpn_msr.generate_anchors import generate_anchors\n\nclass kitti_tracking(datasets.imdb):\n    def __init__(self, image_set, seq_name, kitti_tracking_path=None):\n        datasets.imdb.__init__(self, \'kitti_tracking_\' + image_set + \'_\' + seq_name)\n        self._image_set = image_set\n        self._seq_name = seq_name\n        self._kitti_tracking_path = self._get_default_path() if kitti_tracking_path is None \\\n                            else kitti_tracking_path\n        self._data_path = os.path.join(self._kitti_tracking_path, image_set, \'image_02\')\n        self._classes = (\'__background__\', \'Car\', \'Pedestrian\', \'Cyclist\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.png\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        if image_set == \'training\' and seq_name != \'trainval\':\n            self._num_subclasses = 220 + 1\n        else:\n            self._num_subclasses = 472 + 1\n\n        # load the mapping for subcalss to class\n        if image_set == \'training\' and seq_name != \'trainval\':\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'train\', \'mapping.txt\')\n        else:\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'trainval\', \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n        \n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        self.config = {\'top_k\': 100000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._kitti_tracking_path), \\\n                \'kitti_tracking path does not exist: {}\'.format(self._kitti_tracking_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self.image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n\n        image_path = os.path.join(self._data_path, index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n\n        kitti_train_nums = [154, 447, 233, 144, 314, 297, 270, 800, 390, 803, 294, \\\n                            373, 78, 340, 106, 376, 209, 145, 339, 1059, 837]\n\n        kitti_test_nums = [465, 147, 243, 257, 421, 809, 114, 215, 165, 349, 1176, \\\n                           774, 694, 152, 850, 701, 510, 305, 180, 404, 173, 203, \\\n                           436, 430, 316, 176, 170, 85, 175]\n\n        if self._seq_name == \'train\' or self._seq_name == \'trainval\':\n\n            assert self._image_set == \'training\', \'Use train set or trainval set in testing\'\n\n            if self._seq_name == \'train\':\n                seq_index = [0, 1, 2, 3, 4, 5, 12, 13, 14, 15, 16]\n            else:\n                seq_index = range(0, 21)\n\n            # for each sequence\n            image_index = []\n            for i in xrange(len(seq_index)):\n                seq_idx = seq_index[i]\n                num = kitti_train_nums[seq_idx]\n                for j in xrange(num):\n                    image_index.append(\'{:04d}/{:06d}\'.format(seq_idx, j))\n        else:\n            # a single sequence\n            seq_num = int(self._seq_name)\n            if self._image_set == \'training\':\n                num = kitti_train_nums[seq_num]\n            else:\n                num = kitti_test_nums[seq_num]\n            image_index = []\n            for i in xrange(num):\n                image_index.append(\'{:04d}/{:06d}\'.format(seq_num, i))\n\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where kitti_tracking is expected to be installed.\n        """"""\n        return os.path.join(datasets.ROOT_DIR, \'data\', \'KITTI_Tracking\')\n\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n        """"""\n\n        cache_file = os.path.join(self.cache_path, self.name + \'_\' + cfg.SUBCLS_NAME + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_kitti_voxel_exemplar_annotation(index)\n                    for index in self.image_index]\n\n        if cfg.IS_RPN:\n            # print out recall\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n\n    def _load_kitti_voxel_exemplar_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the KITTI voxel exemplar format.\n        """"""\n        if self._image_set == \'training\' and self._seq_name != \'trainval\':\n            prefix = \'train\'\n        elif self._image_set == \'training\':\n            prefix = \'trainval\'\n        else:\n            prefix = \'\'\n\n        if prefix == \'\':\n            lines = []\n            lines_flipped = []\n        else:\n            filename = os.path.join(self._kitti_tracking_path, cfg.SUBCLS_NAME, prefix, index + \'.txt\')\n            if os.path.exists(filename):\n                print filename\n\n                # the annotation file contains flipped objects    \n                lines = []\n                lines_flipped = []\n                with open(filename) as f:\n                    for line in f:\n                        words = line.split()\n                        subcls = int(words[1])\n                        is_flip = int(words[2])\n                        if subcls != -1:\n                            if is_flip == 0:\n                                lines.append(line)\n                            else:\n                                lines_flipped.append(line)\n            else:\n                lines = []\n                lines_flipped = []\n        \n        num_objs = len(lines)\n\n        # store information of flipped objects\n        assert (num_objs == len(lines_flipped)), \'The number of flipped objects is not the same!\'\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        \n        for ix, line in enumerate(lines_flipped):\n            words = line.split()\n            subcls = int(words[1])\n            gt_subclasses_flipped[ix] = subcls\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            cls = self._class_to_ind[words[0]]\n            subcls = int(words[1])\n            boxes[ix, :] = [float(n) for n in words[3:7]]\n            gt_classes[ix] = cls\n            gt_subclasses[ix] = subcls\n            overlaps[ix, cls] = 1.0\n            subindexes[ix, cls] = subcls\n            subindexes_flipped[ix, cls] = gt_subclasses_flipped[ix]\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                base_size = 16\n                ratios = [3.0, 2.0, 1.5, 1.0, 0.75, 0.5, 0.25]\n                scales = 2**np.arange(1, 6, 0.5)\n                anchors = generate_anchors(base_size, ratios, scales)\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\': overlaps,\n                \'gt_subindexes\': subindexes, \n                \'gt_subindexes_flipped\': subindexes_flipped, \n                \'flipped\' : False}\n\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.SUBCLS_NAME + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'testing\':\n            gt_roidb = self.gt_roidb()\n\n            print \'Loading region proposal network boxes...\'\n            if self._image_set == \'trainval\':\n                model = cfg.REGION_PROPOSAL + \'_trainval/\'\n            else:\n                model = cfg.REGION_PROPOSAL + \'_train/\'\n            rpn_roidb = self._load_rpn_roidb(gt_roidb, model)\n            print \'Region proposal network boxes loaded\'\n            roidb = datasets.imdb.merge_roidbs(rpn_roidb, gt_roidb)\n        else:\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL + \'_trainval/\'\n            roidb = self._load_rpn_roidb(None, model)\n            print \'Region proposal network boxes loaded\'\n\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        prefix = model\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._kitti_tracking_path, \'region_proposals\',  prefix, self._image_set, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            print filename\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        # load the mapping for subcalss the alpha (viewpoint)\n        if self._image_set == \'training\' and self._seq_name != \'trainval\':\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'train\', \'mapping.txt\')\n        else:\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'trainval\', \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[3])\n\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index[5:] + \'.txt\')\n            print \'Writing kitti_tracking results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        alpha = mapping[subcls]\n                        f.write(\'{:s} -1 -1 {:f} {:f} {:f} {:f} {:f} -1 -1 -1 -1 -1 -1 -1 {:.32f}\\n\'.format(\\\n                                 cls, alpha, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    # write detection results into one file\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n        # load the mapping for subcalss the alpha (viewpoint)\n        if self._image_set == \'training\' and self._seq_name != \'trainval\':\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'train\', \'mapping.txt\')\n        else:\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'trainval\', \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[3])\n\n        # open results file\n        filename = os.path.join(output_dir, self._seq_name+\'.txt\')\n        print \'Writing all kitti_tracking results to file \' + filename\n        with open(filename, \'wt\') as f:\n            # for each image\n            for im_ind, index in enumerate(self.image_index):\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        alpha = mapping[subcls]\n                        f.write(\'{:d} -1 {:s} -1 -1 {:f} {:f} {:f} {:f} {:f} -1 -1 -1 -1000 -1000 -1000 -10 {:f}\\n\'.format(\\\n                                 im_ind, cls, alpha, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index[5:] + \'.txt\')\n            print \'Writing kitti_tracking results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing kitti_tracking results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\nif __name__ == \'__main__\':\n    d = datasets.kitti_tracking(\'training\', \'0000\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/nissan.py,0,"b'__author__ = \'yuxiang\'\n\nimport datasets\nimport datasets.nissan\nimport os\nimport PIL\nimport datasets.imdb\nimport numpy as np\nimport scipy.sparse\nfrom utils.cython_bbox import bbox_overlaps\nfrom utils.boxes_grid import get_boxes_grid\nimport subprocess\nimport cPickle\nfrom fast_rcnn.config import cfg\nimport math\nfrom rpn_msr.generate_anchors import generate_anchors\n\nclass nissan(datasets.imdb):\n    def __init__(self, image_set, nissan_path=None):\n        datasets.imdb.__init__(self, \'nissan_\' + image_set)\n        self._image_set = image_set\n        self._nissan_path = self._get_default_path() if nissan_path is None \\\n                            else nissan_path\n        self._data_path = os.path.join(self._nissan_path, \'Images\')\n        self._classes = (\'__background__\', \'Car\', \'Pedestrian\', \'Cyclist\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.png\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        self._num_subclasses = 227 + 36 + 36 + 1\n\n        # load the mapping for subcalss to class\n        filename = os.path.join(self._nissan_path, \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n        \n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        self.config = {\'top_k\': 100000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._nissan_path), \\\n                \'Nissan path does not exist: {}\'.format(self._nissan_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self.image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # set the prefix\n        prefix = self._image_set\n\n        image_path = os.path.join(self._data_path, prefix, index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        image_set_file = os.path.join(self._data_path, self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n\n        with open(image_set_file) as f:\n            image_index = [x.rstrip(\'\\n\') for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where NISSAN is expected to be installed.\n        """"""\n        return os.path.join(datasets.ROOT_DIR, \'data\', \'NISSAN\')\n\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n        No implementation.\n        """"""\n\n        gt_roidb = []\n        return gt_roidb\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        print \'Loading region proposal network boxes...\'\n        model = cfg.REGION_PROPOSAL\n        roidb = self._load_rpn_roidb(None, model)\n        print \'Region proposal network boxes loaded\'\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        prefix = model\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._nissan_path, \'region_proposals\',  prefix, self._image_set, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        # load the mapping for subcalss the alpha (viewpoint)\n        filename = os.path.join(self._nissan_path, \'mapping.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[3])\n\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing NISSAN results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        alpha = mapping[subcls]\n                        f.write(\'{:s} -1 -1 {:f} {:f} {:f} {:f} {:f} -1 -1 -1 -1 -1 -1 -1 {:.32f}\\n\'.format(\\\n                                 cls, alpha, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    # write detection results into one file\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n        # open results file\n        filename = os.path.join(output_dir, \'detections.txt\')\n        print \'Writing all NISSAN results to file \' + filename\n        with open(filename, \'wt\') as f:\n            # for each image\n            for im_ind, index in enumerate(self.image_index):\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        f.write(\'{:s} {:s} {:f} {:f} {:f} {:f} {:d} {:f}\\n\'.format(\\\n                                 index, cls, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], subcls, dets[k, 4]))\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing NISSAN results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing NISSAN results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\nif __name__ == \'__main__\':\n    d = datasets.nissan(\'2015-10-21-16-25-12\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/nthu.py,0,"b'__author__ = \'yuxiang\'\n\nimport datasets\nimport datasets.nthu\nimport os\nimport PIL\nimport datasets.imdb\nimport numpy as np\nimport scipy.sparse\nfrom utils.cython_bbox import bbox_overlaps\nfrom utils.boxes_grid import get_boxes_grid\nimport subprocess\nimport cPickle\nfrom fast_rcnn.config import cfg\nimport math\nfrom rpn_msr.generate_anchors import generate_anchors\n\nclass nthu(datasets.imdb):\n    def __init__(self, image_set, nthu_path=None):\n        datasets.imdb.__init__(self, \'nthu_\' + image_set)\n        self._image_set = image_set\n        self._nthu_path = self._get_default_path() if nthu_path is None \\\n                            else nthu_path\n        self._data_path = os.path.join(self._nthu_path, \'data\')\n        self._classes = (\'__background__\', \'Car\', \'Pedestrian\', \'Cyclist\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        self._num_subclasses = 227 + 36 + 36 + 1\n\n        # load the mapping for subcalss to class\n        filename = os.path.join(self._nthu_path, \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n        \n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        self.config = {\'top_k\': 100000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._nthu_path), \\\n                \'NTHU path does not exist: {}\'.format(self._nthu_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self.image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # set the prefix\n        prefix = self._image_set\n\n        image_path = os.path.join(self._data_path, prefix, index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        image_set_file = os.path.join(self._data_path, self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n\n        with open(image_set_file) as f:\n            image_index = [x.rstrip(\'\\n\') for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where nthu is expected to be installed.\n        """"""\n        return os.path.join(datasets.ROOT_DIR, \'data\', \'NTHU\')\n\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n        No implementation.\n        """"""\n\n        gt_roidb = []\n        return gt_roidb\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        print \'Loading region proposal network boxes...\'\n        model = cfg.REGION_PROPOSAL\n        roidb = self._load_rpn_roidb(None, model)\n        print \'Region proposal network boxes loaded\'\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        prefix = model\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._nthu_path, \'region_proposals\',  prefix, self._image_set, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        # load the mapping for subcalss the alpha (viewpoint)\n        filename = os.path.join(self._nthu_path, \'mapping.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[3])\n\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing nthu results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        alpha = mapping[subcls]\n                        f.write(\'{:s} -1 -1 {:f} {:f} {:f} {:f} {:f} -1 -1 -1 -1 -1 -1 -1 {:.32f}\\n\'.format(\\\n                                 cls, alpha, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    # write detection results into one file\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n        # open results file\n        filename = os.path.join(output_dir, \'detections.txt\')\n        print \'Writing all nthu results to file \' + filename\n        with open(filename, \'wt\') as f:\n            # for each image\n            for im_ind, index in enumerate(self.image_index):\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        f.write(\'{:s} {:s} {:f} {:f} {:f} {:f} {:d} {:f}\\n\'.format(\\\n                                 index, cls, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], subcls, dets[k, 4]))\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing nthu results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing nthu results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\nif __name__ == \'__main__\':\n    d = datasets.nthu(\'71\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/pascal3d.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport datasets\nimport datasets.pascal3d\nimport os\nimport PIL\nimport datasets.imdb\nimport xml.dom.minidom as minidom\nimport numpy as np\nimport scipy.sparse\nimport scipy.io as sio\nimport utils.cython_bbox\nimport cPickle\nimport subprocess\nfrom utils.cython_bbox import bbox_overlaps\nfrom utils.boxes_grid import get_boxes_grid\nfrom fast_rcnn.config import cfg\nimport math\nfrom rpn_msr.generate_anchors import generate_anchors\nimport sys\n\nclass pascal3d(datasets.imdb):\n    def __init__(self, image_set, pascal3d_path = None):\n        datasets.imdb.__init__(self, \'pascal3d_\' + image_set)\n        self._year = \'2012\'\n        self._image_set = image_set\n        self._pascal3d_path = self._get_default_path() if pascal3d_path is None \\\n                            else pascal3d_path\n        self._data_path = os.path.join(self._pascal3d_path, \'VOCdevkit\' + self._year, \'VOC\' + self._year)\n        self._classes = (\'__background__\', # always index 0\n                         \'aeroplane\', \'bicycle\', \'boat\',\n                         \'bottle\', \'bus\', \'car\', \'chair\',\n                         \'diningtable\', \'motorbike\',\n                         \'sofa\', \'train\', \'tvmonitor\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        if cfg.SUBCLS_NAME == \'voxel_exemplars\':\n            self._num_subclasses = 337 + 1\n        elif cfg.SUBCLS_NAME == \'pose_exemplars\':\n            self._num_subclasses = 260 + 1\n        else:\n            assert (1), \'cfg.SUBCLS_NAME not supported!\'\n\n        # load the mapping for subcalss to class\n        filename = os.path.join(self._pascal3d_path, cfg.SUBCLS_NAME, \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n        \n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        # PASCAL specific config options\n        self.config = {\'cleanup\'  : True,\n                       \'use_salt\' : True,\n                       \'top_k\'    : 2000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._pascal3d_path), \\\n                \'PASCAL3D path does not exist: {}\'.format(self._pascal3d_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        image_path = os.path.join(self._data_path, \'JPEGImages\',\n                                  index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        # Example path to image set file:\n        # self._pascal3d_path + /VOCdevkit2012/VOC2012/ImageSets/Main/val.txt\n        image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                      self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where PASCAL3D is expected to be installed.\n        """"""\n        return os.path.join(datasets.ROOT_DIR, \'data\', \'PASCAL3D\')\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_\' + cfg.SUBCLS_NAME + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_pascal3d_voxel_exemplar_annotation(index)\n                    for index in self.image_index]\n\n        if cfg.IS_RPN:\n            # print out recall\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n    def _load_pascal_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        """"""\n        filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n        # print \'Loading: {}\'.format(filename)\n        def get_data_from_tag(node, tag):\n            return node.getElementsByTagName(tag)[0].childNodes[0].data\n\n        with open(filename) as f:\n            data = minidom.parseString(f.read())\n\n        objs = data.getElementsByTagName(\'object\')\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        # Load object bounding boxes into a data frame.\n        for ix, obj in enumerate(objs):\n            # Make pixel indexes 0-based\n            x1 = float(get_data_from_tag(obj, \'xmin\')) - 1\n            y1 = float(get_data_from_tag(obj, \'ymin\')) - 1\n            x2 = float(get_data_from_tag(obj, \'xmax\')) - 1\n            y2 = float(get_data_from_tag(obj, \'ymax\')) - 1\n            name =  str(get_data_from_tag(obj, ""name"")).lower().strip()\n            if name in self._classes:\n                cls = self._class_to_ind[name]\n            else:\n                cls = 0\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                anchors = generate_anchors()\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\' : overlaps,\n                \'gt_subindexes\': subindexes,\n                \'gt_subindexes_flipped\': subindexes_flipped,\n                \'flipped\' : False}\n\n\n    def _load_pascal3d_voxel_exemplar_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the pascal subcategory exemplar format.\n        """"""\n\n        if self._image_set == \'val\':\n            return self._load_pascal_annotation(index)\n\n        filename = os.path.join(self._pascal3d_path, cfg.SUBCLS_NAME, index + \'.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        # the annotation file contains flipped objects    \n        lines = []\n        lines_flipped = []\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[1])\n                is_flip = int(words[2])\n                if subcls != -1:\n                    if is_flip == 0:\n                        lines.append(line)\n                    else:\n                        lines_flipped.append(line)\n        \n        num_objs = len(lines)\n\n        # store information of flipped objects\n        assert (num_objs == len(lines_flipped)), \'The number of flipped objects is not the same!\'\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        \n        for ix, line in enumerate(lines_flipped):\n            words = line.split()\n            subcls = int(words[1])\n            gt_subclasses_flipped[ix] = subcls\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            cls = self._class_to_ind[words[0]]\n            subcls = int(words[1])\n            # Make pixel indexes 0-based\n            boxes[ix, :] = [float(n)-1 for n in words[3:7]]\n            gt_classes[ix] = cls\n            gt_subclasses[ix] = subcls\n            overlaps[ix, cls] = 1.0\n            subindexes[ix, cls] = subcls\n            subindexes_flipped[ix, cls] = gt_subclasses_flipped[ix]\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                base_size = 16\n                ratios = [3.0, 2.0, 1.5, 1.0, 0.75, 0.5, 0.25]\n                scales = 2**np.arange(1, 6, 0.5)\n                anchors = generate_anchors(base_size, ratios, scales)\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\': overlaps,\n                \'gt_subindexes\': subindexes, \n                \'gt_subindexes_flipped\': subindexes_flipped, \n                \'flipped\' : False}\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.SUBCLS_NAME + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            rpn_roidb = self._load_rpn_roidb(gt_roidb, model)\n            print \'Region proposal network boxes loaded\'\n            roidb = datasets.imdb.merge_roidbs(rpn_roidb, gt_roidb)\n        else:\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            roidb = self._load_rpn_roidb(None, model)\n            print \'Region proposal network boxes loaded\'\n\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        if self._image_set == \'val\':\n            prefix = model + \'/validation\'\n        elif self._image_set == \'train\':\n            prefix = model + \'/training\'\n        else:\n            predix = \'\'\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._pascal3d_path, \'region_proposals\',  prefix, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n\n    def selective_search_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            roidb = datasets.imdb.merge_roidbs(gt_roidb, ss_roidb)\n        else:\n            roidb = self._load_selective_search_roidb(None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        filename = os.path.abspath(os.path.join(self.cache_path, \'..\',\n                                                \'selective_search_data\',\n                                                self.name + \'.mat\'))\n        assert os.path.exists(filename), \\\n               \'Selective search data not found at: {}\'.format(filename)\n        raw_data = sio.loadmat(filename)[\'boxes\'].ravel()\n\n        box_list = []\n        for i in xrange(raw_data.shape[0]):\n            box_list.append(raw_data[i][:, (1, 0, 3, 2)] - 1)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def selective_search_IJCV_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                \'{:s}_selective_search_IJCV_top_{:d}_roidb.pkl\'.\n                format(self.name, self.config[\'top_k\']))\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = self.gt_roidb()\n        ss_roidb = self._load_selective_search_IJCV_roidb(gt_roidb)\n        roidb = datasets.imdb.merge_roidbs(gt_roidb, ss_roidb)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_selective_search_IJCV_roidb(self, gt_roidb):\n        IJCV_path = os.path.abspath(os.path.join(self.cache_path, \'..\',\n                                                 \'selective_search_IJCV_data\',\n                                                 \'voc_\' + self._year))\n        assert os.path.exists(IJCV_path), \\\n               \'Selective search IJCV data not found at: {}\'.format(IJCV_path)\n\n        top_k = self.config[\'top_k\']\n        box_list = []\n        for i in xrange(self.num_images):\n            filename = os.path.join(IJCV_path, self.image_index[i] + \'.mat\')\n            raw_data = sio.loadmat(filename)\n            box_list.append((raw_data[\'boxes\'][:top_k, :]-1).astype(np.uint16))\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    # evaluate detection results\n    def evaluate_detections(self, all_boxes, output_dir):\n        # load the mapping for subcalss the azimuth (viewpoint)\n        filename = os.path.join(self._pascal3d_path, cfg.SUBCLS_NAME, \'mapping.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[2])\n\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Writing {} VOC results file\'.format(cls)\n            filename = os.path.join(output_dir, \'det_\' + self._image_set + \'_\' + cls + \'.txt\')\n            print filename\n\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        azimuth = mapping[subcls]\n                        f.write(\'{:s} {:.3f} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(index, dets[k, 4], azimuth,\n                                       dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1))\n\n    # evaluate detection results\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n        # open results file\n        filename = os.path.join(output_dir, \'detections.txt\')\n        print \'Writing all PASCAL3D results to file \' + filename\n        with open(filename, \'wt\') as f:\n            for im_ind, index in enumerate(self.image_index):\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        f.write(\'{:s} {:s} {:f} {:f} {:f} {:f} {:d} {:f}\\n\'.\n                                format(index, cls, dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1, subcls, dets[k, 4]))\n\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing PASCAL results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing PASCAL results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n\nif __name__ == \'__main__\':\n    d = datasets.pascal3d(\'train\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/pascal_voc.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom datasets.imdb import imdb\nimport datasets.ds_utils as ds_utils\nimport xml.etree.ElementTree as ET\nimport numpy as np\nimport scipy.sparse\nimport scipy.io as sio\nimport utils.cython_bbox\nimport cPickle\nimport subprocess\nimport uuid\nfrom voc_eval import voc_eval\nfrom fast_rcnn.config import cfg\nimport pdb\n\n\nclass pascal_voc(imdb):\n    def __init__(self, image_set, year, devkit_path=None):\n        imdb.__init__(self, \'voc_\' + year + \'_\' + image_set)\n        self._year = year\n        self._image_set = image_set\n        self._devkit_path = self._get_default_path() if devkit_path is None \\\n                            else devkit_path\n        self._data_path = os.path.join(self._devkit_path, \'VOC\' + self._year)\n        self._classes = (\'__background__\', # always index 0\n                         \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                         \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                         \'cow\', \'diningtable\', \'dog\', \'horse\',\n                         \'motorbike\', \'person\', \'pottedplant\',\n                         \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        #self._roidb_handler = self.selective_search_roidb\n        self._roidb_handler = self.gt_roidb\n        self._salt = str(uuid.uuid4())\n        self._comp_id = \'comp4\'\n\n        # PASCAL specific config options\n        self.config = {\'cleanup\'     : True,\n                       \'use_salt\'    : True,\n                       \'use_diff\'    : False,\n                       \'matlab_eval\' : False,\n                       \'rpn_file\'    : None,\n                       \'min_size\'    : 2}\n\n        assert os.path.exists(self._devkit_path), \\\n                \'VOCdevkit path does not exist: {}\'.format(self._devkit_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        image_path = os.path.join(self._data_path, \'JPEGImages\',\n                                  index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        # Example path to image set file:\n        # self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt\n        image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                      self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where PASCAL VOC is expected to be installed.\n        """"""\n        return os.path.join(cfg.DATA_DIR, \'VOCdevkit\' + self._year)\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_pascal_annotation(index)\n                    for index in self.image_index]\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n    def selective_search_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, ss_roidb)\n        else:\n            roidb = self._load_selective_search_roidb(None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def rpn_roidb(self):\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            rpn_roidb = self._load_rpn_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, rpn_roidb)\n        else:\n            roidb = self._load_rpn_roidb(None)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb):\n        filename = self.config[\'rpn_file\']\n        print \'loading {}\'.format(filename)\n        assert os.path.exists(filename), \\\n               \'rpn data not found at: {}\'.format(filename)\n        with open(filename, \'rb\') as f:\n            box_list = cPickle.load(f)\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        filename = os.path.abspath(os.path.join(cfg.DATA_DIR,\n                                                \'selective_search_data\',\n                                                self.name + \'.mat\'))\n        assert os.path.exists(filename), \\\n               \'Selective search data not found at: {}\'.format(filename)\n        raw_data = sio.loadmat(filename)[\'boxes\'].ravel()\n\n        box_list = []\n        for i in xrange(raw_data.shape[0]):\n            boxes = raw_data[i][:, (1, 0, 3, 2)] - 1\n            keep = ds_utils.unique_boxes(boxes)\n            boxes = boxes[keep, :]\n            keep = ds_utils.filter_small_boxes(boxes, self.config[\'min_size\'])\n            boxes = boxes[keep, :]\n            box_list.append(boxes)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_pascal_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        """"""\n        filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n        tree = ET.parse(filename)\n        objs = tree.findall(\'object\')\n        if not self.config[\'use_diff\']:\n            # Exclude the samples labeled as difficult\n            non_diff_objs = [\n                obj for obj in objs if int(obj.find(\'difficult\').text) == 0]\n            # if len(non_diff_objs) != len(objs):\n            #     print \'Removed {} difficult objects\'.format(\n            #         len(objs) - len(non_diff_objs))\n            objs = non_diff_objs\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        # ""Seg"" area for pascal is just the box area\n        seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n        # Load object bounding boxes into a data frame.\n        for ix, obj in enumerate(objs):\n            bbox = obj.find(\'bndbox\')\n            # Make pixel indexes 0-based\n            x1 = float(bbox.find(\'xmin\').text) - 1\n            y1 = float(bbox.find(\'ymin\').text) - 1\n            x2 = float(bbox.find(\'xmax\').text) - 1\n            y2 = float(bbox.find(\'ymax\').text) - 1\n            cls = self._class_to_ind[obj.find(\'name\').text.lower().strip()]\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n            seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_overlaps\' : overlaps,\n                \'flipped\' : False,\n                \'seg_areas\' : seg_areas}\n\n    def _get_comp_id(self):\n        comp_id = (self._comp_id + \'_\' + self._salt if self.config[\'use_salt\']\n            else self._comp_id)\n        return comp_id\n\n    def _get_voc_results_file_template(self):\n        # VOCdevkit/results/VOC2007/Main/<comp_id>_det_test_aeroplane.txt\n        filename = self._get_comp_id() + \'_det_\' + self._image_set + \'_{:s}.txt\'\n        path = os.path.join(\n            self._devkit_path,\n            \'results\',\n            \'VOC\' + self._year,\n            \'Main\',\n            filename)\n        return path\n\n    def _write_voc_results_file(self, all_boxes):\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Writing {} VOC results file\'.format(cls)\n            filename = self._get_voc_results_file_template().format(cls)\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(index, dets[k, -1],\n                                       dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1))\n\n    def _do_python_eval(self, output_dir = \'output\'):\n        annopath = os.path.join(\n            self._devkit_path,\n            \'VOC\' + self._year,\n            \'Annotations\',\n            \'{:s}.xml\')\n        imagesetfile = os.path.join(\n            self._devkit_path,\n            \'VOC\' + self._year,\n            \'ImageSets\',\n            \'Main\',\n            self._image_set + \'.txt\')\n        cachedir = os.path.join(self._devkit_path, \'annotations_cache\')\n        aps = []\n        # The PASCAL VOC metric changed in 2010\n        use_07_metric = True if int(self._year) < 2010 else False\n        print \'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\')\n        if not os.path.isdir(output_dir):\n            os.mkdir(output_dir)\n        for i, cls in enumerate(self._classes):\n            if cls == \'__background__\':\n                continue\n            filename = self._get_voc_results_file_template().format(cls)\n            rec, prec, ap = voc_eval(\n                filename, annopath, imagesetfile, cls, cachedir, ovthresh=0.5,\n                use_07_metric=use_07_metric)\n            aps += [ap]\n            print(\'AP for {} = {:.4f}\'.format(cls, ap))\n            with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'w\') as f:\n                cPickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n        print(\'Mean AP = {:.4f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'Results:\')\n        for ap in aps:\n            print(\'{:.3f}\'.format(ap))\n        print(\'{:.3f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'\')\n        print(\'--------------------------------------------------------------\')\n        print(\'Results computed with the **unofficial** Python eval code.\')\n        print(\'Results should be very close to the official MATLAB eval code.\')\n        print(\'Recompute with `./tools/reval.py --matlab ...` for your paper.\')\n        print(\'-- Thanks, The Management\')\n        print(\'--------------------------------------------------------------\')\n\n    def _do_matlab_eval(self, output_dir=\'output\'):\n        print \'-----------------------------------------------------\'\n        print \'Computing results with the official MATLAB eval code.\'\n        print \'-----------------------------------------------------\'\n        path = os.path.join(cfg.ROOT_DIR, \'lib\', \'datasets\',\n                            \'VOCdevkit-matlab-wrapper\')\n        cmd = \'cd {} && \'.format(path)\n        cmd += \'{:s} -nodisplay -nodesktop \'.format(cfg.MATLAB)\n        cmd += \'-r ""dbstop if error; \'\n        cmd += \'voc_eval(\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\'); quit;""\' \\\n               .format(self._devkit_path, self._get_comp_id(),\n                       self._image_set, output_dir)\n        print(\'Running:\\n{}\'.format(cmd))\n        status = subprocess.call(cmd, shell=True)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        self._write_voc_results_file(all_boxes)\n        self._do_python_eval(output_dir)\n        if self.config[\'matlab_eval\']:\n            self._do_matlab_eval(output_dir)\n        if self.config[\'cleanup\']:\n            for cls in self._classes:\n                if cls == \'__background__\':\n                    continue\n                filename = self._get_voc_results_file_template().format(cls)\n                os.remove(filename)\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n\nif __name__ == \'__main__\':\n    from datasets.pascal_voc import pascal_voc\n    d = pascal_voc(\'trainval\', \'2007\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/pascal_voc2.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport datasets\nimport datasets.pascal_voc\nimport os\nimport PIL\nimport datasets.imdb\nimport xml.dom.minidom as minidom\nimport numpy as np\nimport scipy.sparse\nimport scipy.io as sio\nimport utils.cython_bbox\nimport cPickle\nimport subprocess\nfrom utils.cython_bbox import bbox_overlaps\nfrom utils.boxes_grid import get_boxes_grid\nfrom fast_rcnn.config import cfg\nimport math\nfrom rpn_msr.generate_anchors import generate_anchors\nimport sys\n\nclass pascal_voc(datasets.imdb):\n    def __init__(self, image_set, year, pascal_path=None):\n        datasets.imdb.__init__(self, \'voc_\' + year + \'_\' + image_set)\n        self._year = year\n        self._image_set = image_set\n        self._pascal_path = self._get_default_path() if pascal_path is None \\\n                            else pascal_path\n        self._data_path = os.path.join(self._pascal_path, \'VOCdevkit\' + self._year, \'VOC\' + self._year)\n        self._classes = (\'__background__\', # always index 0\n                         \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                         \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                         \'cow\', \'diningtable\', \'dog\', \'horse\',\n                         \'motorbike\', \'person\', \'pottedplant\',\n                         \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        self._num_subclasses = 240 + 1\n\n        # load the mapping for subcalss to class\n        filename = os.path.join(self._pascal_path, \'subcategory_exemplars\', \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n        \n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        # PASCAL specific config options\n        self.config = {\'cleanup\'  : True,\n                       \'use_salt\' : True,\n                       \'top_k\'    : 2000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._pascal_path), \\\n                \'PASCAL path does not exist: {}\'.format(self._pascal_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        image_path = os.path.join(self._data_path, \'JPEGImages\',\n                                  index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        # Example path to image set file:\n        # self._pascal_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt\n        image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                      self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where PASCAL VOC is expected to be installed.\n        """"""\n        return os.path.join(datasets.ROOT_DIR, \'data\', \'PASCAL\')\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_pascal_subcategory_exemplar_annotation(index)\n                    for index in self.image_index]\n\n        if cfg.IS_RPN:\n            # print out recall\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n\n    def _load_pascal_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        """"""\n        filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n        # print \'Loading: {}\'.format(filename)\n        def get_data_from_tag(node, tag):\n            return node.getElementsByTagName(tag)[0].childNodes[0].data\n\n        with open(filename) as f:\n            data = minidom.parseString(f.read())\n\n        objs = data.getElementsByTagName(\'object\')\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        # Load object bounding boxes into a data frame.\n        for ix, obj in enumerate(objs):\n            # Make pixel indexes 0-based\n            x1 = float(get_data_from_tag(obj, \'xmin\')) - 1\n            y1 = float(get_data_from_tag(obj, \'ymin\')) - 1\n            x2 = float(get_data_from_tag(obj, \'xmax\')) - 1\n            y2 = float(get_data_from_tag(obj, \'ymax\')) - 1\n            cls = self._class_to_ind[\n                    str(get_data_from_tag(obj, ""name"")).lower().strip()]\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                anchors = generate_anchors()\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\' : overlaps,\n                \'gt_subindexes\': subindexes,\n                \'gt_subindexes_flipped\': subindexes_flipped,\n                \'flipped\' : False}\n\n\n    def _load_pascal_subcategory_exemplar_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the pascal subcategory exemplar format.\n        """"""\n        if self._image_set == \'test\':\n            return self._load_pascal_annotation(index)\n\n        filename = os.path.join(self._pascal_path, \'subcategory_exemplars\', index + \'.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        # the annotation file contains flipped objects    \n        lines = []\n        lines_flipped = []\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[1])\n                is_flip = int(words[2])\n                if subcls != -1:\n                    if is_flip == 0:\n                        lines.append(line)\n                    else:\n                        lines_flipped.append(line)\n        \n        num_objs = len(lines)\n\n        # store information of flipped objects\n        assert (num_objs == len(lines_flipped)), \'The number of flipped objects is not the same!\'\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        \n        for ix, line in enumerate(lines_flipped):\n            words = line.split()\n            subcls = int(words[1])\n            gt_subclasses_flipped[ix] = subcls\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            cls = self._class_to_ind[words[0]]\n            subcls = int(words[1])\n            # Make pixel indexes 0-based\n            boxes[ix, :] = [float(n)-1 for n in words[3:7]]\n            gt_classes[ix] = cls\n            gt_subclasses[ix] = subcls\n            overlaps[ix, cls] = 1.0\n            subindexes[ix, cls] = subcls\n            subindexes_flipped[ix, cls] = gt_subclasses_flipped[ix]\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                base_size = 16\n                ratios = [3.0, 2.0, 1.5, 1.0, 0.75, 0.5, 0.25]\n                scales = 2**np.arange(1, 6, 0.5)\n                anchors = generate_anchors(base_size, ratios, scales)\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\': overlaps,\n                \'gt_subindexes\': subindexes, \n                \'gt_subindexes_flipped\': subindexes_flipped, \n                \'flipped\' : False}\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            rpn_roidb = self._load_rpn_roidb(gt_roidb, model)\n            print \'Region proposal network boxes loaded\'\n            roidb = datasets.imdb.merge_roidbs(rpn_roidb, gt_roidb)\n        else:\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            roidb = self._load_rpn_roidb(None, model)\n            print \'Region proposal network boxes loaded\'\n\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        if self._image_set == \'test\':\n            prefix = model + \'/testing\'\n        else:\n            prefix = model + \'/training\'\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._pascal_path, \'region_proposals\',  prefix, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n\n    def selective_search_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            roidb = datasets.imdb.merge_roidbs(gt_roidb, ss_roidb)\n        else:\n            roidb = self._load_selective_search_roidb(None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        filename = os.path.abspath(os.path.join(self.cache_path, \'..\',\n                                                \'selective_search_data\',\n                                                self.name + \'.mat\'))\n        assert os.path.exists(filename), \\\n               \'Selective search data not found at: {}\'.format(filename)\n        raw_data = sio.loadmat(filename)[\'boxes\'].ravel()\n\n        box_list = []\n        for i in xrange(raw_data.shape[0]):\n            box_list.append(raw_data[i][:, (1, 0, 3, 2)] - 1)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def selective_search_IJCV_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                \'{:s}_selective_search_IJCV_top_{:d}_roidb.pkl\'.\n                format(self.name, self.config[\'top_k\']))\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = self.gt_roidb()\n        ss_roidb = self._load_selective_search_IJCV_roidb(gt_roidb)\n        roidb = datasets.imdb.merge_roidbs(gt_roidb, ss_roidb)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_selective_search_IJCV_roidb(self, gt_roidb):\n        IJCV_path = os.path.abspath(os.path.join(self.cache_path, \'..\',\n                                                 \'selective_search_IJCV_data\',\n                                                 \'voc_\' + self._year))\n        assert os.path.exists(IJCV_path), \\\n               \'Selective search IJCV data not found at: {}\'.format(IJCV_path)\n\n        top_k = self.config[\'top_k\']\n        box_list = []\n        for i in xrange(self.num_images):\n            filename = os.path.join(IJCV_path, self.image_index[i] + \'.mat\')\n            raw_data = sio.loadmat(filename)\n            box_list.append((raw_data[\'boxes\'][:top_k, :]-1).astype(np.uint16))\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n\n    def _write_voc_results_file(self, all_boxes):\n        use_salt = self.config[\'use_salt\']\n        comp_id = \'comp4\'\n        if use_salt:\n            comp_id += \'-{}\'.format(os.getpid())\n\n        # VOCdevkit/results/VOC2007/Main/comp4-44503_det_test_aeroplane.txt\n        path = os.path.join(self._pascal_path, \'VOCdevkit\' + self._year, \'results\', \'VOC\' + self._year,\n                            \'Main\', comp_id + \'_\')\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Writing {} VOC results file\'.format(cls)\n            filename = path + \'det_\' + self._image_set + \'_\' + cls + \'.txt\'\n            print filename\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(index, dets[k, 4],\n                                       dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1))\n        return comp_id\n\n    def _do_matlab_eval(self, comp_id, output_dir=\'output\'):\n        rm_results = self.config[\'cleanup\']\n\n        path = os.path.join(os.path.dirname(__file__),\n                            \'VOCdevkit-matlab-wrapper\')\n        cmd = \'cd {} && \'.format(path)\n        cmd += \'{:s} -nodisplay -nodesktop \'.format(datasets.MATLAB)\n        cmd += \'-r ""dbstop if error; \'\n        cmd += \'voc_eval(\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',{:d}); quit;""\' \\\n               .format(self._pascal_path + \'/VOCdevkit\' + self._year, comp_id,\n                       self._image_set, output_dir, int(rm_results))\n        print(\'Running:\\n{}\'.format(cmd))\n        status = subprocess.call(cmd, shell=True)\n\n    # evaluate detection results\n    def evaluate_detections(self, all_boxes, output_dir):\n        comp_id = self._write_voc_results_file(all_boxes)\n        self._do_matlab_eval(comp_id, output_dir)\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing PASCAL results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing PASCAL results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n\nif __name__ == \'__main__\':\n    d = datasets.pascal_voc(\'trainval\', \'2007\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/voc_eval.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\n\nimport xml.etree.ElementTree as ET\nimport os\nimport cPickle\nimport numpy as np\nimport pdb\ndef parse_rec(filename):\n    """""" Parse a PASCAL VOC xml file """"""\n    tree = ET.parse(filename)\n    objects = []\n    for obj in tree.findall(\'object\'):\n        obj_struct = {}\n        obj_struct[\'name\'] = obj.find(\'name\').text\n        obj_struct[\'pose\'] = obj.find(\'pose\').text\n        obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n        obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n        bbox = obj.find(\'bndbox\')\n        obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                              int(bbox.find(\'ymin\').text),\n                              int(bbox.find(\'xmax\').text),\n                              int(bbox.find(\'ymax\').text)]\n        objects.append(obj_struct)\n\n    return objects\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\ndef voc_eval(detpath,\n             annopath,\n             imagesetfile,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=False):\n    """"""rec, prec, ap = voc_eval(detpath,\n                                annopath,\n                                imagesetfile,\n                                classname,\n                                [ovthresh],\n                                [use_07_metric])\n\n    Top level function that does the PASCAL VOC evaluation.\n\n    detpath: Path to detections\n        detpath.format(classname) should produce the detection results file.\n    annopath: Path to annotations\n        annopath.format(imagename) should be the xml annotations file.\n    imagesetfile: Text file containing the list of images, one image per line.\n    classname: Category name (duh)\n    cachedir: Directory for caching the annotations\n    [ovthresh]: Overlap threshold (default = 0.5)\n    [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n        (default False)\n    """"""\n    # assumes detections are in detpath.format(classname)\n    # assumes annotations are in annopath.format(imagename)\n    # assumes imagesetfile is a text file with each line an image name\n    # cachedir caches the annotations in a pickle file\n\n    # first load gt\n    if not os.path.isdir(cachedir):\n        os.mkdir(cachedir)\n    cachefile = os.path.join(cachedir, \'annots.pkl\')\n    # read list of images\n    with open(imagesetfile, \'r\') as f:\n        lines = f.readlines()\n    imagenames = [x.strip() for x in lines]\n\n    if not os.path.isfile(cachefile):\n        # load annots\n        recs = {}\n        for i, imagename in enumerate(imagenames):\n            recs[imagename] = parse_rec(annopath.format(imagename))\n            if i % 100 == 0:\n                print \'Reading annotation for {:d}/{:d}\'.format(\n                    i + 1, len(imagenames))\n        # save\n        print \'Saving cached annotations to {:s}\'.format(cachefile)\n        with open(cachefile, \'w\') as f:\n            cPickle.dump(recs, f)\n    else:\n        # load\n        with open(cachefile, \'r\') as f:\n            recs = cPickle.load(f)\n\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n    for imagename in imagenames:\n        R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n        bbox = np.array([x[\'bbox\'] for x in R])\n        difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n        det = [False] * len(R)\n        npos = npos + sum(~difficult)\n        class_recs[imagename] = {\'bbox\': bbox,\n                                 \'difficult\': difficult,\n                                 \'det\': det}\n\n    # read dets\n    detfile = detpath.format(classname)\n    with open(detfile, \'r\') as f:\n        lines = f.readlines()\n    if any(lines) == 1:\n\n        splitlines = [x.strip().split(\' \') for x in lines]\n        image_ids = [x[0] for x in splitlines]\n        confidence = np.array([float(x[1]) for x in splitlines])\n        BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n        # sort by confidence\n        sorted_ind = np.argsort(-confidence)\n        sorted_scores = np.sort(-confidence)\n        BB = BB[sorted_ind, :]\n        image_ids = [image_ids[x] for x in sorted_ind]\n\n        # go down dets and mark TPs and FPs\n        nd = len(image_ids)\n        tp = np.zeros(nd)\n        fp = np.zeros(nd)\n        for d in range(nd):\n            R = class_recs[image_ids[d]]\n            bb = BB[d, :].astype(float)\n            ovmax = -np.inf\n            BBGT = R[\'bbox\'].astype(float)\n\n            if BBGT.size > 0:\n                # compute overlaps\n                # intersection\n                ixmin = np.maximum(BBGT[:, 0], bb[0])\n                iymin = np.maximum(BBGT[:, 1], bb[1])\n                ixmax = np.minimum(BBGT[:, 2], bb[2])\n                iymax = np.minimum(BBGT[:, 3], bb[3])\n                iw = np.maximum(ixmax - ixmin + 1., 0.)\n                ih = np.maximum(iymax - iymin + 1., 0.)\n                inters = iw * ih\n\n                # union\n                uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n                       (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n                       (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n                overlaps = inters / uni\n                ovmax = np.max(overlaps)\n                jmax = np.argmax(overlaps)\n\n            if ovmax > ovthresh:\n                if not R[\'difficult\'][jmax]:\n                    if not R[\'det\'][jmax]:\n                        tp[d] = 1.\n                        R[\'det\'][jmax] = 1\n                    else:\n                        fp[d] = 1.\n            else:\n                fp[d] = 1.\n\n        # compute precision recall\n        fp = np.cumsum(fp)\n        tp = np.cumsum(tp)\n        rec = tp / float(npos)\n        # avoid divide by zero in case the first detection matches a difficult\n        # ground truth\n        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        ap = voc_ap(rec, prec, use_07_metric)\n    else:\n         rec = -1\n         prec = -1\n         ap = -1\n\n    return rec, prec, ap\n'"
lib/fast_rcnn/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom . import config\nfrom . import train\nfrom . import test\n'
lib/fast_rcnn/bbox_transform.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef bbox_transform(ex_rois, gt_rois):\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = np.log(gt_widths / ex_widths)\n    targets_dh = np.log(gt_heights / ex_heights)\n\n    targets = np.vstack(\n        (targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n    return targets\n\ndef bbox_transform_inv(boxes, deltas):\n    if boxes.shape[0] == 0:\n        return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)\n\n    boxes = boxes.astype(deltas.dtype, copy=False)\n\n    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n    ctr_x = boxes[:, 0] + 0.5 * widths\n    ctr_y = boxes[:, 1] + 0.5 * heights\n\n    dx = deltas[:, 0::4]\n    dy = deltas[:, 1::4]\n    dw = deltas[:, 2::4]\n    dh = deltas[:, 3::4]\n\n    pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n    pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n    pred_w = np.exp(dw) * widths[:, np.newaxis]\n    pred_h = np.exp(dh) * heights[:, np.newaxis]\n\n    pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)\n    # x1\n    pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n    # y1\n    pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n    # x2\n    pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n    # y2\n    pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n    return pred_boxes\n\ndef clip_boxes(boxes, im_shape):\n    """"""\n    Clip boxes to image boundaries.\n    """"""\n\n    # x1 >= 0\n    boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)\n    # y1 >= 0\n    boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)\n    # x2 < im_shape[1]\n    boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)\n    # y2 < im_shape[0]\n    boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)\n    return boxes\n'"
lib/fast_rcnn/config.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Fast R-CNN config system.\n\nThis file specifies default config options for Fast R-CNN. You should not\nchange values in this file. Instead, you should write a config file (in yaml)\nand use cfg_from_file(yaml_file) to load it and override the default options.\n\nMost tools in $ROOT/tools take a --cfg option to specify an override file.\n    - See tools/{train,test}_net.py for example code that uses cfg_from_file()\n    - See experiments/cfgs/*.yml for example YAML config override files\n""""""\n\nimport os\nimport os.path as osp\nimport numpy as np\nfrom distutils import spawn\n# `pip install easydict` if you don\'t have it\nfrom easydict import EasyDict as edict\n\n__C = edict()\n# Consumers can get config by:\n#   from fast_rcnn_config import cfg\ncfg = __C\n\n#\n# Training options\n#\n\n__C.TRAIN = edict()\n#__C.NET_NAME = \'VGGnet\'\n# learning rate\n__C.TRAIN.LEARNING_RATE = 0.001\n__C.TRAIN.MOMENTUM = 0.9\n__C.TRAIN.GAMMA = 0.1\n__C.TRAIN.STEPSIZE = 50000\n__C.TRAIN.DISPLAY = 10\n__C.IS_MULTISCALE = False\n\n# Scales to compute real features\n#__C.TRAIN.SCALES_BASE = (0.25, 0.5, 1.0, 2.0, 3.0)\n#__C.TRAIN.SCALES_BASE = (1.0,)\n\n# parameters for ROI generating\n#__C.TRAIN.SPATIAL_SCALE = 0.0625\n#__C.TRAIN.KERNEL_SIZE = 5\n\n# Aspect ratio to use during training\n#__C.TRAIN.ASPECTS = (1, 0.75, 0.5, 0.25)\n#__C.TRAIN.ASPECTS= (1,)\n\n\n# Scales to use during training (can list multiple scales)\n# Each scale is the pixel size of an image\'s shortest side\n__C.TRAIN.SCALES = (600,)\n\n# Max pixel size of the longest side of a scaled input image\n__C.TRAIN.MAX_SIZE = 1000\n\n# Images to use per minibatch\n__C.TRAIN.IMS_PER_BATCH = 2\n\n# Minibatch size (number of regions of interest [ROIs])\n__C.TRAIN.BATCH_SIZE = 128\n\n# Fraction of minibatch that is labeled foreground (i.e. class > 0)\n__C.TRAIN.FG_FRACTION = 0.25\n\n# Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n__C.TRAIN.FG_THRESH = 0.5\n\n# Overlap threshold for a ROI to be considered background (class = 0 if\n# overlap in [LO, HI))\n__C.TRAIN.BG_THRESH_HI = 0.5\n__C.TRAIN.BG_THRESH_LO = 0.1\n\n# Use horizontally-flipped images during training?\n__C.TRAIN.USE_FLIPPED = True\n\n# Train bounding-box regressors\n__C.TRAIN.BBOX_REG = True\n\n# Overlap required between a ROI and ground-truth box in order for that ROI to\n# be used as a bounding-box regression training example\n__C.TRAIN.BBOX_THRESH = 0.5\n\n# Iterations between snapshots\n__C.TRAIN.SNAPSHOT_ITERS = 5000\n\n# solver.prototxt specifies the snapshot path prefix, this adds an optional\n# infix to yield the path: <prefix>[_<infix>]_iters_XYZ.caffemodel\n__C.TRAIN.SNAPSHOT_PREFIX = \'VGGnet_fast_rcnn\'\n__C.TRAIN.SNAPSHOT_INFIX = \'\'\n\n# Use a prefetch thread in roi_data_layer.layer\n# So far I haven\'t found this useful; likely more engineering work is required\n__C.TRAIN.USE_PREFETCH = False\n\n# Normalize the targets (subtract empirical mean, divide by empirical stddev)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS = True\n# Deprecated (inside weights)\n__C.TRAIN.BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n# Normalize the targets using ""precomputed"" (or made up) means and stdevs\n# (BBOX_NORMALIZE_TARGETS must also be True)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED = False\n__C.TRAIN.BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)\n__C.TRAIN.BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)\n\n# Train using these proposals\n__C.TRAIN.PROPOSAL_METHOD = \'selective_search\'\n\n# Make minibatches from images that have similar aspect ratios (i.e. both\n# tall and thin or both short and wide) in order to avoid wasting computation\n# on zero-padding.\n__C.TRAIN.ASPECT_GROUPING = True\n\n# Use RPN to detect objects\n__C.TRAIN.HAS_RPN = False\n# IOU >= thresh: positive example\n__C.TRAIN.RPN_POSITIVE_OVERLAP = 0.7\n# IOU < thresh: negative example\n__C.TRAIN.RPN_NEGATIVE_OVERLAP = 0.3\n# If an anchor statisfied by positive and negative conditions set to negative\n__C.TRAIN.RPN_CLOBBER_POSITIVES = False\n# Max number of foreground examples\n__C.TRAIN.RPN_FG_FRACTION = 0.5\n# Total number of examples\n__C.TRAIN.RPN_BATCHSIZE = 256\n# NMS threshold used on RPN proposals\n__C.TRAIN.RPN_NMS_THRESH = 0.7\n# Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TRAIN.RPN_PRE_NMS_TOP_N = 12000\n# Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TRAIN.RPN_POST_NMS_TOP_N = 2000\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n__C.TRAIN.RPN_MIN_SIZE = 16\n# Deprecated (outside weights)\n__C.TRAIN.RPN_BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n# Give the positive RPN examples weight of p * 1 / {num positives}\n# and give negatives a weight of (1 - p)\n# Set to -1.0 to use uniform example weighting\n__C.TRAIN.RPN_POSITIVE_WEIGHT = -1.0\n\n# Enable timeline generation\n__C.TRAIN.DEBUG_TIMELINE = False\n\n#\n# Testing options\n#\n\n__C.TEST = edict()\n\n# Scales to use during testing (can list multiple scales)\n# Each scale is the pixel size of an image\'s shortest side\n__C.TEST.SCALES = (600,)\n\n# Max pixel size of the longest side of a scaled input image\n__C.TEST.MAX_SIZE = 1000\n\n# Overlap threshold used for non-maximum suppression (suppress boxes with\n# IoU >= this threshold)\n__C.TEST.NMS = 0.3\n\n# Experimental: treat the (K+1) units in the cls_score layer as linear\n# predictors (trained, eg, with one-vs-rest SVMs).\n__C.TEST.SVM = False\n\n# Test using bounding-box regressors\n__C.TEST.BBOX_REG = True\n\n# Propose boxes\n__C.TEST.HAS_RPN = True\n\n# Test using these proposals\n__C.TEST.PROPOSAL_METHOD = \'selective_search\'\n\n## NMS threshold used on RPN proposals\n__C.TEST.RPN_NMS_THRESH = 0.7\n## Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TEST.RPN_PRE_NMS_TOP_N = 6000\n#__C.TEST.RPN_PRE_NMS_TOP_N = 12000\n## Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TEST.RPN_POST_NMS_TOP_N = 300\n#__C.TEST.RPN_POST_NMS_TOP_N = 2000\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n__C.TEST.RPN_MIN_SIZE = 16\n\n# Enable timeline generation\n__C.TEST.DEBUG_TIMELINE = False\n\n#\n# MISC\n#\n\n# The mapping from image coordinates to feature map coordinates might cause\n# some boxes that are distinct in image space to become identical in feature\n# coordinates. If DEDUP_BOXES > 0, then DEDUP_BOXES is used as the scale factor\n# for identifying duplicate boxes.\n# 1/16 is correct for {Alex,Caffe}Net, VGG_CNN_M_1024, and VGG16\n__C.DEDUP_BOXES = 1./16.\n\n# Pixel mean values (BGR order) as a (1, 1, 3) array\n# We use the same pixel mean for all networks even though it\'s not exactly what\n# they were trained with\n__C.PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n\n# For reproducibility\n__C.RNG_SEED = 3\n\n# A small number that\'s used many times\n__C.EPS = 1e-14\n\n# Root directory of project\n__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__), \'..\', \'..\'))\n\n# Data directory\n__C.DATA_DIR = osp.abspath(osp.join(__C.ROOT_DIR, \'data\'))\n\n# Model directory\n__C.MODELS_DIR = osp.abspath(osp.join(__C.ROOT_DIR, \'models\', \'pascal_voc\'))\n\n# Name (or path to) the matlab executable\n__C.MATLAB = \'matlab\'\n\n# Place outputs under an experiments directory\n__C.EXP_DIR = \'default\'\n\n\nif spawn.find_executable(""nvcc""):\n    # Use GPU implementation of non-maximum suppression\n    __C.USE_GPU_NMS = True\n\n    # Default GPU device id\n    __C.GPU_ID = 0\nelse:\n    __C.USE_GPU_NMS = False\n\n\ndef get_output_dir(imdb, weights_filename):\n    """"""Return the directory where experimental artifacts are placed.\n    If the directory does not exist, it is created.\n\n    A canonical path is built using the name from an imdb and a network\n    (if not None).\n    """"""\n    outdir = osp.abspath(osp.join(__C.ROOT_DIR, \'output\', __C.EXP_DIR, imdb.name))\n    if weights_filename is not None:\n        outdir = osp.join(outdir, weights_filename)\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n    return outdir\n\ndef _merge_a_into_b(a, b):\n    """"""Merge config dictionary a into config dictionary b, clobbering the\n    options in b whenever they are also specified in a.\n    """"""\n    if type(a) is not edict:\n        return\n\n    for k, v in a.iteritems():\n        # a must specify keys that are in b\n        if not b.has_key(k):\n            raise KeyError(\'{} is not a valid config key\'.format(k))\n\n        # the types must match, too\n        old_type = type(b[k])\n        if old_type is not type(v):\n            if isinstance(b[k], np.ndarray):\n                v = np.array(v, dtype=b[k].dtype)\n            else:\n                raise ValueError((\'Type mismatch ({} vs. {}) \'\n                                \'for config key: {}\').format(type(b[k]),\n                                                            type(v), k))\n\n        # recursively merge dicts\n        if type(v) is edict:\n            try:\n                _merge_a_into_b(a[k], b[k])\n            except:\n                print(\'Error under config key: {}\'.format(k))\n                raise\n        else:\n            b[k] = v\n\ndef cfg_from_file(filename):\n    """"""Load a config file and merge it into the default options.""""""\n    import yaml\n    with open(filename, \'r\') as f:\n        yaml_cfg = edict(yaml.load(f))\n\n    _merge_a_into_b(yaml_cfg, __C)\n\ndef cfg_from_list(cfg_list):\n    """"""Set config keys via list (e.g., from command line).""""""\n    from ast import literal_eval\n    assert len(cfg_list) % 2 == 0\n    for k, v in zip(cfg_list[0::2], cfg_list[1::2]):\n        key_list = k.split(\'.\')\n        d = __C\n        for subkey in key_list[:-1]:\n            assert d.has_key(subkey)\n            d = d[subkey]\n        subkey = key_list[-1]\n        assert d.has_key(subkey)\n        try:\n            value = literal_eval(v)\n        except:\n            # handle the case when v is a string literal\n            value = v\n        assert type(value) == type(d[subkey]), \\\n            \'type {} does not match original type {}\'.format(\n            type(value), type(d[subkey]))\n        d[subkey] = value\n'"
lib/fast_rcnn/nms_wrapper.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom fast_rcnn.config import cfg\nif cfg.USE_GPU_NMS:\n    from nms.gpu_nms import gpu_nms\nfrom nms.cpu_nms import cpu_nms\n\ndef nms(dets, thresh, force_cpu=False):\n    """"""Dispatch to either CPU or GPU NMS implementations.""""""\n\n    if dets.shape[0] == 0:\n        return []\n    if cfg.USE_GPU_NMS and not force_cpu:\n        return gpu_nms(dets, thresh, device_id=cfg.GPU_ID)\n    else:\n        return cpu_nms(dets, thresh)\n'"
lib/fast_rcnn/test.py,3,"b'from fast_rcnn.config import cfg, get_output_dir\nimport argparse\nfrom utils.timer import Timer\nimport numpy as np\nimport cv2\nfrom utils.cython_nms import nms, nms_new\nfrom utils.boxes_grid import get_boxes_grid\nimport cPickle\nimport heapq\nfrom utils.blob import im_list_to_blob\nimport os\nimport math\nfrom rpn_msr.generate import imdb_proposals_det\nimport tensorflow as tf\nfrom fast_rcnn.bbox_transform import clip_boxes, bbox_transform_inv\nimport matplotlib.pyplot as plt\nfrom tensorflow.python.client import timeline\nimport time\n\ndef _get_image_blob(im):\n    """"""Converts an image into a network input.\n    Arguments:\n        im (ndarray): a color image in BGR order\n    Returns:\n        blob (ndarray): a data blob holding an image pyramid\n        im_scale_factors (list): list of image scales (relative to im) used\n            in the image pyramid\n    """"""\n    im_orig = im.astype(np.float32, copy=True)\n    im_orig -= cfg.PIXEL_MEANS\n\n    im_shape = im_orig.shape\n    im_size_min = np.min(im_shape[0:2])\n    im_size_max = np.max(im_shape[0:2])\n\n    processed_ims = []\n    im_scale_factors = []\n\n    for target_size in cfg.TEST.SCALES:\n        im_scale = float(target_size) / float(im_size_min)\n        # Prevent the biggest axis from being more than MAX_SIZE\n        if np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:\n            im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)\n        im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,\n                        interpolation=cv2.INTER_LINEAR)\n        im_scale_factors.append(im_scale)\n        processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, np.array(im_scale_factors)\n\ndef _get_rois_blob(im_rois, im_scale_factors):\n    """"""Converts RoIs into network inputs.\n    Arguments:\n        im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n        im_scale_factors (list): scale factors as returned by _get_image_blob\n    Returns:\n        blob (ndarray): R x 5 matrix of RoIs in the image pyramid\n    """"""\n    rois, levels = _project_im_rois(im_rois, im_scale_factors)\n    rois_blob = np.hstack((levels, rois))\n    return rois_blob.astype(np.float32, copy=False)\n\ndef _project_im_rois(im_rois, scales):\n    """"""Project image RoIs into the image pyramid built by _get_image_blob.\n    Arguments:\n        im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n        scales (list): scale factors as returned by _get_image_blob\n    Returns:\n        rois (ndarray): R x 4 matrix of projected RoI coordinates\n        levels (list): image pyramid levels used by each projected RoI\n    """"""\n    im_rois = im_rois.astype(np.float, copy=False)\n    scales = np.array(scales)\n\n    if len(scales) > 1:\n        widths = im_rois[:, 2] - im_rois[:, 0] + 1\n        heights = im_rois[:, 3] - im_rois[:, 1] + 1\n\n        areas = widths * heights\n        scaled_areas = areas[:, np.newaxis] * (scales[np.newaxis, :] ** 2)\n        diff_areas = np.abs(scaled_areas - 224 * 224)\n        levels = diff_areas.argmin(axis=1)[:, np.newaxis]\n    else:\n        levels = np.zeros((im_rois.shape[0], 1), dtype=np.int)\n\n    rois = im_rois * scales[levels]\n\n    return rois, levels\n\ndef _get_blobs(im, rois):\n    """"""Convert an image and RoIs within that image into network inputs.""""""\n    if cfg.TEST.HAS_RPN:\n        blobs = {\'data\' : None, \'rois\' : None}\n        blobs[\'data\'], im_scale_factors = _get_image_blob(im)\n    else:\n        blobs = {\'data\' : None, \'rois\' : None}\n        blobs[\'data\'], im_scale_factors = _get_image_blob(im)\n        if cfg.IS_MULTISCALE:\n            if cfg.IS_EXTRAPOLATING:\n                blobs[\'rois\'] = _get_rois_blob(rois, cfg.TEST.SCALES)\n            else:\n                blobs[\'rois\'] = _get_rois_blob(rois, cfg.TEST.SCALES_BASE)\n        else:\n            blobs[\'rois\'] = _get_rois_blob(rois, cfg.TEST.SCALES_BASE)\n\n    return blobs, im_scale_factors\n\ndef _clip_boxes(boxes, im_shape):\n    """"""Clip boxes to image boundaries.""""""\n    # x1 >= 0\n    boxes[:, 0::4] = np.maximum(boxes[:, 0::4], 0)\n    # y1 >= 0\n    boxes[:, 1::4] = np.maximum(boxes[:, 1::4], 0)\n    # x2 < im_shape[1]\n    boxes[:, 2::4] = np.minimum(boxes[:, 2::4], im_shape[1] - 1)\n    # y2 < im_shape[0]\n    boxes[:, 3::4] = np.minimum(boxes[:, 3::4], im_shape[0] - 1)\n    return boxes\n\n\ndef _rescale_boxes(boxes, inds, scales):\n    """"""Rescale boxes according to image rescaling.""""""\n\n    for i in xrange(boxes.shape[0]):\n        boxes[i,:] = boxes[i,:] / scales[int(inds[i])]\n\n    return boxes\n\n\ndef im_detect(sess, net, im, boxes=None):\n    """"""Detect object classes in an image given object proposals.\n    Arguments:\n        net (caffe.Net): Fast R-CNN network to use\n        im (ndarray): color image to test (in BGR order)\n        boxes (ndarray): R x 4 array of object proposals\n    Returns:\n        scores (ndarray): R x K array of object class scores (K includes\n            background as object category 0)\n        boxes (ndarray): R x (4*K) array of predicted bounding boxes\n    """"""\n\n    blobs, im_scales = _get_blobs(im, boxes)\n\n    # When mapping from image ROIs to feature map ROIs, there\'s some aliasing\n    # (some distinct image ROIs get mapped to the same feature ROI).\n    # Here, we identify duplicate feature ROIs, so we only compute features\n    # on the unique subset.\n    if cfg.DEDUP_BOXES > 0 and not cfg.TEST.HAS_RPN:\n        v = np.array([1, 1e3, 1e6, 1e9, 1e12])\n        hashes = np.round(blobs[\'rois\'] * cfg.DEDUP_BOXES).dot(v)\n        _, index, inv_index = np.unique(hashes, return_index=True,\n                                        return_inverse=True)\n        blobs[\'rois\'] = blobs[\'rois\'][index, :]\n        boxes = boxes[index, :]\n\n    if cfg.TEST.HAS_RPN:\n        im_blob = blobs[\'data\']\n        blobs[\'im_info\'] = np.array(\n            [[im_blob.shape[1], im_blob.shape[2], im_scales[0]]],\n            dtype=np.float32)\n    # forward pass\n    if cfg.TEST.HAS_RPN:\n        feed_dict={net.data: blobs[\'data\'], net.im_info: blobs[\'im_info\'], net.keep_prob: 1.0}\n    else:\n        feed_dict={net.data: blobs[\'data\'], net.rois: blobs[\'rois\'], net.keep_prob: 1.0}\n\n    run_options = None\n    run_metadata = None\n    if cfg.TEST.DEBUG_TIMELINE:\n        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n        run_metadata = tf.RunMetadata()\n\n    cls_score, cls_prob, bbox_pred, rois = sess.run([net.get_output(\'cls_score\'), net.get_output(\'cls_prob\'), net.get_output(\'bbox_pred\'),net.get_output(\'rois\')],\n                                                    feed_dict=feed_dict,\n                                                    options=run_options,\n                                                    run_metadata=run_metadata)\n\n    if cfg.TEST.HAS_RPN:\n        assert len(im_scales) == 1, ""Only single-image batch implemented""\n        boxes = rois[:, 1:5] / im_scales[0]\n\n\n    if cfg.TEST.SVM:\n        # use the raw scores before softmax under the assumption they\n        # were trained as linear SVMs\n        scores = cls_score\n    else:\n        # use softmax estimated probabilities\n        scores = cls_prob\n\n    if cfg.TEST.BBOX_REG:\n        # Apply bounding-box regression deltas\n        box_deltas = bbox_pred\n        pred_boxes = bbox_transform_inv(boxes, box_deltas)\n        pred_boxes = _clip_boxes(pred_boxes, im.shape)\n    else:\n        # Simply repeat the boxes, once for each class\n        pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n\n    if cfg.DEDUP_BOXES > 0 and not cfg.TEST.HAS_RPN:\n        # Map scores and predictions back to the original set of boxes\n        scores = scores[inv_index, :]\n        pred_boxes = pred_boxes[inv_index, :]\n\n    if cfg.TEST.DEBUG_TIMELINE:\n        trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n        trace_file = open(str(long(time.time() * 1000)) + \'-test-timeline.ctf.json\', \'w\')\n        trace_file.write(trace.generate_chrome_trace_format(show_memory=False))\n        trace_file.close()\n\n    return scores, pred_boxes\n\n\ndef vis_detections(im, class_name, dets, thresh=0.8):\n    """"""Visual debugging of detections.""""""\n    import matplotlib.pyplot as plt\n    #im = im[:, :, (2, 1, 0)]\n    for i in xrange(np.minimum(10, dets.shape[0])):\n        bbox = dets[i, :4]\n        score = dets[i, -1]\n        if score > thresh:\n            #plt.cla()\n            #plt.imshow(im)\n            plt.gca().add_patch(\n                plt.Rectangle((bbox[0], bbox[1]),\n                              bbox[2] - bbox[0],\n                              bbox[3] - bbox[1], fill=False,\n                              edgecolor=\'g\', linewidth=3)\n                )\n            plt.gca().text(bbox[0], bbox[1] - 2,\n                 \'{:s} {:.3f}\'.format(class_name, score),\n                 bbox=dict(facecolor=\'blue\', alpha=0.5),\n                 fontsize=14, color=\'white\')\n\n            plt.title(\'{}  {:.3f}\'.format(class_name, score))\n    #plt.show()\n\ndef apply_nms(all_boxes, thresh):\n    """"""Apply non-maximum suppression to all predicted boxes output by the\n    test_net method.\n    """"""\n    num_classes = len(all_boxes)\n    num_images = len(all_boxes[0])\n    nms_boxes = [[[] for _ in xrange(num_images)]\n                 for _ in xrange(num_classes)]\n    for cls_ind in xrange(num_classes):\n        for im_ind in xrange(num_images):\n            dets = all_boxes[cls_ind][im_ind]\n            if dets == []:\n                continue\n\n            x1 = dets[:, 0]\n            y1 = dets[:, 1]\n            x2 = dets[:, 2]\n            y2 = dets[:, 3]\n            scores = dets[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1) & (scores > cfg.TEST.DET_THRESHOLD))[0]\n            dets = dets[inds,:]\n            if dets == []:\n                continue\n\n            keep = nms(dets, thresh)\n            if len(keep) == 0:\n                continue\n            nms_boxes[cls_ind][im_ind] = dets[keep, :].copy()\n    return nms_boxes\n\n\ndef test_net(sess, net, imdb, weights_filename , max_per_image=300, thresh=0.05, vis=False):\n    """"""Test a Fast R-CNN network on an image database.""""""\n    num_images = len(imdb.image_index)\n    # all detections are collected into:\n    #    all_boxes[cls][image] = N x 5 array of detections in\n    #    (x1, y1, x2, y2, score)\n    all_boxes = [[[] for _ in xrange(num_images)]\n                 for _ in xrange(imdb.num_classes)]\n\n    output_dir = get_output_dir(imdb, weights_filename)\n    # timers\n    _t = {\'im_detect\' : Timer(), \'misc\' : Timer()}\n\n    if not cfg.TEST.HAS_RPN:\n        roidb = imdb.roidb\n\n    for i in xrange(num_images):\n        # filter out any ground truth boxes\n        if cfg.TEST.HAS_RPN:\n            box_proposals = None\n        else:\n            # The roidb may contain ground-truth rois (for example, if the roidb\n            # comes from the training or val split). We only want to evaluate\n            # detection on the *non*-ground-truth rois. We select those the rois\n            # that have the gt_classes field set to 0, which means there\'s no\n            # ground truth.\n            box_proposals = roidb[i][\'boxes\'][roidb[i][\'gt_classes\'] == 0]\n\n        im = cv2.imread(imdb.image_path_at(i))\n        _t[\'im_detect\'].tic()\n        scores, boxes = im_detect(sess, net, im, box_proposals)\n        _t[\'im_detect\'].toc()\n\n        _t[\'misc\'].tic()\n        if vis:\n            image = im[:, :, (2, 1, 0)]\n            plt.cla()\n            plt.imshow(image)\n\n        # skip j = 0, because it\'s the background class\n        for j in xrange(1, imdb.num_classes):\n            inds = np.where(scores[:, j] > thresh)[0]\n            cls_scores = scores[inds, j]\n            cls_boxes = boxes[inds, j*4:(j+1)*4]\n            cls_dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])) \\\n                .astype(np.float32, copy=False)\n            keep = nms(cls_dets, cfg.TEST.NMS)\n            cls_dets = cls_dets[keep, :]\n            if vis:\n                vis_detections(image, imdb.classes[j], cls_dets)\n            all_boxes[j][i] = cls_dets\n        if vis:\n           plt.show()\n        # Limit to max_per_image detections *over all classes*\n        if max_per_image > 0:\n            image_scores = np.hstack([all_boxes[j][i][:, -1]\n                                      for j in xrange(1, imdb.num_classes)])\n            if len(image_scores) > max_per_image:\n                image_thresh = np.sort(image_scores)[-max_per_image]\n                for j in xrange(1, imdb.num_classes):\n                    keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n                    all_boxes[j][i] = all_boxes[j][i][keep, :]\n        _t[\'misc\'].toc()\n\n        print \'im_detect: {:d}/{:d} {:.3f}s {:.3f}s\' \\\n              .format(i + 1, num_images, _t[\'im_detect\'].average_time,\n                      _t[\'misc\'].average_time)\n\n    det_file = os.path.join(output_dir, \'detections.pkl\')\n    with open(det_file, \'wb\') as f:\n        cPickle.dump(all_boxes, f, cPickle.HIGHEST_PROTOCOL)\n\n    print \'Evaluating detections\'\n    imdb.evaluate_detections(all_boxes, output_dir)\n\n'"
lib/fast_rcnn/train.py,32,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Train a Fast R-CNN network.""""""\n\nfrom fast_rcnn.config import cfg\nimport gt_data_layer.roidb as gdl_roidb\nimport roi_data_layer.roidb as rdl_roidb\nfrom roi_data_layer.layer import RoIDataLayer\nfrom utils.timer import Timer\nimport numpy as np\nimport os\nimport tensorflow as tf\nimport sys\nfrom tensorflow.python.client import timeline\nimport time\n\nclass SolverWrapper(object):\n    """"""A simple wrapper around Caffe\'s solver.\n    This wrapper gives us control over he snapshotting process, which we\n    use to unnormalize the learned bounding-box regression weights.\n    """"""\n\n    def __init__(self, sess, saver, network, imdb, roidb, output_dir, pretrained_model=None):\n        """"""Initialize the SolverWrapper.""""""\n        self.net = network\n        self.imdb = imdb\n        self.roidb = roidb\n        self.output_dir = output_dir\n        self.pretrained_model = pretrained_model\n\n        print \'Computing bounding-box regression targets...\'\n        if cfg.TRAIN.BBOX_REG:\n            self.bbox_means, self.bbox_stds = rdl_roidb.add_bbox_regression_targets(roidb)\n        print \'done\'\n\n        # For checkpoint\n        self.saver = saver\n\n    def snapshot(self, sess, iter):\n        """"""Take a snapshot of the network after unnormalizing the learned\n        bounding-box regression weights. This enables easy use at test-time.\n        """"""\n        net = self.net\n\n        if cfg.TRAIN.BBOX_REG and net.layers.has_key(\'bbox_pred\'):\n            # save original values\n            with tf.variable_scope(\'bbox_pred\', reuse=True):\n                weights = tf.get_variable(""weights"")\n                biases = tf.get_variable(""biases"")\n\n            orig_0 = weights.eval()\n            orig_1 = biases.eval()\n\n            # scale and shift with bbox reg unnormalization; then save snapshot\n            weights_shape = weights.get_shape().as_list()\n            sess.run(net.bbox_weights_assign, feed_dict={net.bbox_weights: orig_0 * np.tile(self.bbox_stds, (weights_shape[0], 1))})\n            sess.run(net.bbox_bias_assign, feed_dict={net.bbox_biases: orig_1 * self.bbox_stds + self.bbox_means})\n\n        if not os.path.exists(self.output_dir):\n            os.makedirs(self.output_dir)\n\n        infix = (\'_\' + cfg.TRAIN.SNAPSHOT_INFIX\n                 if cfg.TRAIN.SNAPSHOT_INFIX != \'\' else \'\')\n        filename = (cfg.TRAIN.SNAPSHOT_PREFIX + infix +\n                    \'_iter_{:d}\'.format(iter+1) + \'.ckpt\')\n        filename = os.path.join(self.output_dir, filename)\n\n        self.saver.save(sess, filename)\n        print \'Wrote snapshot to: {:s}\'.format(filename)\n\n        if cfg.TRAIN.BBOX_REG and net.layers.has_key(\'bbox_pred\'):\n            with tf.variable_scope(\'bbox_pred\', reuse=True):\n                # restore net to original state\n                sess.run(net.bbox_weights_assign, feed_dict={net.bbox_weights: orig_0})\n                sess.run(net.bbox_bias_assign, feed_dict={net.bbox_biases: orig_1})\n\n    def _modified_smooth_l1(self, sigma, bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights):\n        """"""\n            ResultLoss = outside_weights * SmoothL1(inside_weights * (bbox_pred - bbox_targets))\n            SmoothL1(x) = 0.5 * (sigma * x)^2,    if |x| < 1 / sigma^2\n                          |x| - 0.5 / sigma^2,    otherwise\n        """"""\n        sigma2 = sigma * sigma\n\n        inside_mul = tf.multiply(bbox_inside_weights, tf.subtract(bbox_pred, bbox_targets))\n\n        smooth_l1_sign = tf.cast(tf.less(tf.abs(inside_mul), 1.0 / sigma2), tf.float32)\n        smooth_l1_option1 = tf.multiply(tf.multiply(inside_mul, inside_mul), 0.5 * sigma2)\n        smooth_l1_option2 = tf.subtract(tf.abs(inside_mul), 0.5 / sigma2)\n        smooth_l1_result = tf.add(tf.multiply(smooth_l1_option1, smooth_l1_sign),\n                                  tf.multiply(smooth_l1_option2, tf.abs(tf.subtract(smooth_l1_sign, 1.0))))\n\n        outside_mul = tf.multiply(bbox_outside_weights, smooth_l1_result)\n\n        return outside_mul\n\n\n    def train_model(self, sess, max_iters):\n        """"""Network training loop.""""""\n\n        data_layer = get_data_layer(self.roidb, self.imdb.num_classes)\n\n        # RPN\n        # classification loss\n        rpn_cls_score = tf.reshape(self.net.get_output(\'rpn_cls_score_reshape\'),[-1,2])\n        rpn_label = tf.reshape(self.net.get_output(\'rpn-data\')[0],[-1])\n        rpn_cls_score = tf.reshape(tf.gather(rpn_cls_score,tf.where(tf.not_equal(rpn_label,-1))),[-1,2])\n        rpn_label = tf.reshape(tf.gather(rpn_label,tf.where(tf.not_equal(rpn_label,-1))),[-1])\n        rpn_cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=rpn_cls_score, labels=rpn_label))\n\n        # bounding box regression L1 loss\n        rpn_bbox_pred = self.net.get_output(\'rpn_bbox_pred\')\n        rpn_bbox_targets = tf.transpose(self.net.get_output(\'rpn-data\')[1],[0,2,3,1])\n        rpn_bbox_inside_weights = tf.transpose(self.net.get_output(\'rpn-data\')[2],[0,2,3,1])\n        rpn_bbox_outside_weights = tf.transpose(self.net.get_output(\'rpn-data\')[3],[0,2,3,1])\n\n        rpn_smooth_l1 = self._modified_smooth_l1(3.0, rpn_bbox_pred, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights)\n        rpn_loss_box = tf.reduce_mean(tf.reduce_sum(rpn_smooth_l1, reduction_indices=[1, 2, 3]))\n \n        # R-CNN\n        # classification loss\n        cls_score = self.net.get_output(\'cls_score\')\n        label = tf.reshape(self.net.get_output(\'roi-data\')[1],[-1])\n        cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=cls_score, labels=label))\n\n        # bounding box regression L1 loss\n        bbox_pred = self.net.get_output(\'bbox_pred\')\n        bbox_targets = self.net.get_output(\'roi-data\')[2]\n        bbox_inside_weights = self.net.get_output(\'roi-data\')[3]\n        bbox_outside_weights = self.net.get_output(\'roi-data\')[4]\n\n        smooth_l1 = self._modified_smooth_l1(1.0, bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights)\n        loss_box = tf.reduce_mean(tf.reduce_sum(smooth_l1, reduction_indices=[1]))\n\n        # final loss\n        loss = cross_entropy + loss_box + rpn_cross_entropy + rpn_loss_box\n\n        # optimizer and learning rate\n        global_step = tf.Variable(0, trainable=False)\n        lr = tf.train.exponential_decay(cfg.TRAIN.LEARNING_RATE, global_step,\n                                        cfg.TRAIN.STEPSIZE, 0.1, staircase=True)\n        momentum = cfg.TRAIN.MOMENTUM\n        train_op = tf.train.MomentumOptimizer(lr, momentum).minimize(loss, global_step=global_step)\n\n        # iintialize variables\n        sess.run(tf.global_variables_initializer())\n        if self.pretrained_model is not None:\n            print (\'Loading pretrained model \'\n                   \'weights from {:s}\').format(self.pretrained_model)\n            self.net.load(self.pretrained_model, sess, self.saver, True)\n\n        last_snapshot_iter = -1\n        timer = Timer()\n        for iter in range(max_iters):\n            # get one batch\n            blobs = data_layer.forward()\n\n            # Make one SGD update\n            feed_dict={self.net.data: blobs[\'data\'], self.net.im_info: blobs[\'im_info\'], self.net.keep_prob: 0.5, \\\n                           self.net.gt_boxes: blobs[\'gt_boxes\']}\n\n            run_options = None\n            run_metadata = None\n            if cfg.TRAIN.DEBUG_TIMELINE:\n                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n                run_metadata = tf.RunMetadata()\n\n            timer.tic()\n\n            rpn_loss_cls_value, rpn_loss_box_value,loss_cls_value, loss_box_value, _ = sess.run([rpn_cross_entropy, rpn_loss_box, cross_entropy, loss_box, train_op],\n                                                                                                feed_dict=feed_dict,\n                                                                                                options=run_options,\n                                                                                                run_metadata=run_metadata)\n\n            timer.toc()\n\n            if cfg.TRAIN.DEBUG_TIMELINE:\n                trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n                trace_file = open(str(long(time.time() * 1000)) + \'-train-timeline.ctf.json\', \'w\')\n                trace_file.write(trace.generate_chrome_trace_format(show_memory=False))\n                trace_file.close()\n\n            if (iter+1) % (cfg.TRAIN.DISPLAY) == 0:\n                print \'iter: %d / %d, total loss: %.4f, rpn_loss_cls: %.4f, rpn_loss_box: %.4f, loss_cls: %.4f, loss_box: %.4f, lr: %f\'%\\\n                        (iter+1, max_iters, rpn_loss_cls_value + rpn_loss_box_value + loss_cls_value + loss_box_value ,rpn_loss_cls_value, rpn_loss_box_value,loss_cls_value, loss_box_value, lr.eval())\n                print \'speed: {:.3f}s / iter\'.format(timer.average_time)\n\n            if (iter+1) % cfg.TRAIN.SNAPSHOT_ITERS == 0:\n                last_snapshot_iter = iter\n                self.snapshot(sess, iter)\n\n        if last_snapshot_iter != iter:\n            self.snapshot(sess, iter)\n\ndef get_training_roidb(imdb):\n    """"""Returns a roidb (Region of Interest database) for use in training.""""""\n    if cfg.TRAIN.USE_FLIPPED:\n        print \'Appending horizontally-flipped training examples...\'\n        imdb.append_flipped_images()\n        print \'done\'\n\n    print \'Preparing training data...\'\n    if cfg.TRAIN.HAS_RPN:\n        if cfg.IS_MULTISCALE:\n            gdl_roidb.prepare_roidb(imdb)\n        else:\n            rdl_roidb.prepare_roidb(imdb)\n    else:\n        rdl_roidb.prepare_roidb(imdb)\n    print \'done\'\n\n    return imdb.roidb\n\n\ndef get_data_layer(roidb, num_classes):\n    """"""return a data layer.""""""\n    if cfg.TRAIN.HAS_RPN:\n        if cfg.IS_MULTISCALE:\n            layer = GtDataLayer(roidb)\n        else:\n            layer = RoIDataLayer(roidb, num_classes)\n    else:\n        layer = RoIDataLayer(roidb, num_classes)\n\n    return layer\n\ndef filter_roidb(roidb):\n    """"""Remove roidb entries that have no usable RoIs.""""""\n\n    def is_valid(entry):\n        # Valid images have:\n        #   (1) At least one foreground RoI OR\n        #   (2) At least one background RoI\n        overlaps = entry[\'max_overlaps\']\n        # find boxes with sufficient overlap\n        fg_inds = np.where(overlaps >= cfg.TRAIN.FG_THRESH)[0]\n        # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n        bg_inds = np.where((overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                           (overlaps >= cfg.TRAIN.BG_THRESH_LO))[0]\n        # image is only valid if such boxes exist\n        valid = len(fg_inds) > 0 or len(bg_inds) > 0\n        return valid\n\n    num = len(roidb)\n    filtered_roidb = [entry for entry in roidb if is_valid(entry)]\n    num_after = len(filtered_roidb)\n    print \'Filtered {} roidb entries: {} -> {}\'.format(num - num_after,\n                                                       num, num_after)\n    return filtered_roidb\n\n\ndef train_net(network, imdb, roidb, output_dir, pretrained_model=None, max_iters=40000):\n    """"""Train a Fast R-CNN network.""""""\n    roidb = filter_roidb(roidb)\n    saver = tf.train.Saver(max_to_keep=100)\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n        sw = SolverWrapper(sess, saver, network, imdb, roidb, output_dir, pretrained_model=pretrained_model)\n        print \'Solving...\'\n        sw.train_model(sess, max_iters)\n        print \'done solving\'\n'"
lib/gt_data_layer/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n'
lib/gt_data_layer/layer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""The data layer used during training to train a Fast R-CNN network.\n\nGtDataLayer implements a Caffe Python layer.\n""""""\n\nimport caffe\nfrom fast_rcnn.config import cfg\nfrom gt_data_layer.minibatch import get_minibatch\nimport numpy as np\nimport yaml\nfrom multiprocessing import Process, Queue\n\nclass GtDataLayer(caffe.Layer):\n    """"""Fast R-CNN data layer used for training.""""""\n\n    def _shuffle_roidb_inds(self):\n        """"""Randomly permute the training roidb.""""""\n        self._perm = np.random.permutation(np.arange(len(self._roidb)))\n        self._cur = 0\n\n    def _get_next_minibatch_inds(self):\n        """"""Return the roidb indices for the next minibatch.""""""\n        if self._cur + cfg.TRAIN.IMS_PER_BATCH >= len(self._roidb):\n            self._shuffle_roidb_inds()\n\n        db_inds = self._perm[self._cur:self._cur + cfg.TRAIN.IMS_PER_BATCH]\n        self._cur += cfg.TRAIN.IMS_PER_BATCH\n\n        """"""\n        # sample images with gt objects\n        db_inds = np.zeros((cfg.TRAIN.IMS_PER_BATCH), dtype=np.int32)\n        i = 0\n        while (i < cfg.TRAIN.IMS_PER_BATCH):\n            ind = self._perm[self._cur]\n            num_objs = self._roidb[ind][\'boxes\'].shape[0]\n            if num_objs != 0:\n                db_inds[i] = ind\n                i += 1\n\n            self._cur += 1\n            if self._cur >= len(self._roidb):\n                self._shuffle_roidb_inds()\n        """"""\n\n        return db_inds\n\n    def _get_next_minibatch(self):\n        """"""Return the blobs to be used for the next minibatch.""""""\n        db_inds = self._get_next_minibatch_inds()\n        minibatch_db = [self._roidb[i] for i in db_inds]\n        return get_minibatch(minibatch_db, self._num_classes)\n\n    # this function is called in training the net\n    def set_roidb(self, roidb):\n        """"""Set the roidb to be used by this layer during training.""""""\n        self._roidb = roidb\n        self._shuffle_roidb_inds()\n\n    def setup(self, bottom, top):\n        """"""Setup the GtDataLayer.""""""\n\n        # parse the layer parameter string, which must be valid YAML\n        layer_params = yaml.load(self.param_str_)\n\n        self._num_classes = layer_params[\'num_classes\']\n\n        self._name_to_top_map = {\n            \'data\': 0,\n            \'info_boxes\': 1,\n            \'parameters\': 2}\n\n        # data blob: holds a batch of N images, each with 3 channels\n        # The height and width (100 x 100) are dummy values\n        num_scale_base = len(cfg.TRAIN.SCALES_BASE)\n        top[0].reshape(num_scale_base, 3, 100, 100)\n\n        # info boxes blob\n        top[1].reshape(1, 18)\n\n        # parameters blob\n        num_scale = len(cfg.TRAIN.SCALES)\n        num_aspect = len(cfg.TRAIN.ASPECTS)\n        top[2].reshape(2 + 2*num_scale + 2*num_aspect)\n            \n    def forward(self, bottom, top):\n        """"""Get blobs and copy them into this layer\'s top blob vector.""""""\n        blobs = self._get_next_minibatch()\n\n        for blob_name, blob in blobs.iteritems():\n            top_ind = self._name_to_top_map[blob_name]\n            # Reshape net\'s input blobs\n            top[top_ind].reshape(*(blob.shape))\n            # Copy data into net\'s input blobs\n            top[top_ind].data[...] = blob.astype(np.float32, copy=False)\n\n    def backward(self, top, propagate_down, bottom):\n        """"""This layer does not propagate gradients.""""""\n        pass\n\n    def reshape(self, bottom, top):\n        """"""Reshaping happens during the call to forward.""""""\n        pass\n'"
lib/gt_data_layer/minibatch.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Compute minibatch blobs for training a Fast R-CNN network.""""""\n\nimport numpy as np\nimport numpy.random as npr\nimport cv2\nfrom fast_rcnn.config import cfg\nfrom utils.blob import prep_im_for_blob, im_list_to_blob\n\ndef get_minibatch(roidb, num_classes):\n    """"""Given a roidb, construct a minibatch sampled from it.""""""\n    num_images = len(roidb)\n    assert(cfg.TRAIN.BATCH_SIZE % num_images == 0), \\\n        \'num_images ({}) must divide BATCH_SIZE ({})\'. \\\n        format(num_images, cfg.TRAIN.BATCH_SIZE)\n\n    # Get the input image blob, formatted for caffe\n    im_blob = _get_image_blob(roidb)\n\n    # build the box information blob\n    info_boxes_blob = np.zeros((0, 18), dtype=np.float32)\n    num_scale = len(cfg.TRAIN.SCALES)\n    for i in xrange(num_images):\n        info_boxes = roidb[i][\'info_boxes\']\n\n        # change the batch index\n        info_boxes[:,2] += i * num_scale\n        info_boxes[:,7] += i * num_scale\n\n        info_boxes_blob = np.vstack((info_boxes_blob, info_boxes))\n\n    # build the parameter blob\n    num_aspect = len(cfg.TRAIN.ASPECTS)\n    num = 2 + 2 * num_scale + 2 * num_aspect\n    parameters_blob = np.zeros((num), dtype=np.float32)\n    parameters_blob[0] = num_scale\n    parameters_blob[1] = num_aspect\n    parameters_blob[2:2+num_scale] = cfg.TRAIN.SCALES\n    parameters_blob[2+num_scale:2+2*num_scale] = cfg.TRAIN.SCALE_MAPPING\n    parameters_blob[2+2*num_scale:2+2*num_scale+num_aspect] = cfg.TRAIN.ASPECT_HEIGHTS\n    parameters_blob[2+2*num_scale+num_aspect:2+2*num_scale+2*num_aspect] = cfg.TRAIN.ASPECT_WIDTHS\n\n    # For debug visualizations\n    # _vis_minibatch(im_blob, rois_blob, labels_blob, sublabels_blob)\n\n    blobs = {\'data\': im_blob,\n             \'info_boxes\': info_boxes_blob,\n             \'parameters\': parameters_blob}\n\n    return blobs\n\ndef _get_image_blob(roidb):\n    """"""Builds an input blob from the images in the roidb at the different scales.\n    """"""\n    num_images = len(roidb)\n    processed_ims = []\n\n    for i in xrange(num_images):\n        # read image\n        im = cv2.imread(roidb[i][\'image\'])\n        if roidb[i][\'flipped\']:\n            im = im[:, ::-1, :]\n\n        im_orig = im.astype(np.float32, copy=True)\n        im_orig -= cfg.PIXEL_MEANS\n\n        # build image pyramid\n        for im_scale in cfg.TRAIN.SCALES_BASE:\n            im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,\n                        interpolation=cv2.INTER_LINEAR)\n\n            processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob\n\ndef _project_im_rois(im_rois, im_scale_factor):\n    """"""Project image RoIs into the rescaled training image.""""""\n    rois = im_rois * im_scale_factor\n    return rois\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    """"""Bounding-box regression targets are stored in a compact form in the\n    roidb.\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets). The loss weights\n    are similarly expanded.\n\n    Returns:\n        bbox_target_data (ndarray): N x 4K blob of regression targets\n        bbox_loss_weights (ndarray): N x 4K blob of loss weights\n    """"""\n    clss = bbox_target_data[:, 0]\n    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n    bbox_loss_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = 4 * cls\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n        bbox_loss_weights[ind, start:end] = [1., 1., 1., 1.]\n    return bbox_targets, bbox_loss_weights\n\n\ndef _vis_minibatch(im_blob, rois_blob, labels_blob, sublabels_blob):\n    """"""Visualize a mini-batch for debugging.""""""\n    import matplotlib.pyplot as plt\n    for i in xrange(rois_blob.shape[0]):\n        rois = rois_blob[i, :]\n        im_ind = rois[0]\n        roi = rois[2:]\n        im = im_blob[im_ind, :, :, :].transpose((1, 2, 0)).copy()\n        im += cfg.PIXEL_MEANS\n        im = im[:, :, (2, 1, 0)]\n        im = im.astype(np.uint8)\n        cls = labels_blob[i]\n        subcls = sublabels_blob[i]\n        plt.imshow(im)\n        print \'class: \', cls, \' subclass: \', subcls\n        plt.gca().add_patch(\n            plt.Rectangle((roi[0], roi[1]), roi[2] - roi[0],\n                          roi[3] - roi[1], fill=False,\n                          edgecolor=\'r\', linewidth=3)\n            )\n        plt.show()\n'"
lib/gt_data_layer/roidb.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Transform a roidb into a trainable roidb by adding a bunch of metadata.""""""\n\nimport numpy as np\nfrom fast_rcnn.config import cfg\nfrom utils.cython_bbox import bbox_overlaps\nfrom utils.boxes_grid import get_boxes_grid\nimport scipy.sparse\nimport PIL\nimport math\nimport os\nimport cPickle\nimport pdb\n\n\ndef prepare_roidb(imdb):\n    """"""Enrich the imdb\'s roidb by adding some derived quantities that\n    are useful for training. This function precomputes the maximum\n    overlap, taken over ground-truth boxes, between each ROI and\n    each ground-truth box. The class with maximum overlap is also\n    recorded.\n    """"""\n    cache_file = os.path.join(imdb.cache_path, imdb.name + \'_gt_roidb_prepared.pkl\')\n    if os.path.exists(cache_file):\n        with open(cache_file, \'rb\') as fid:\n            imdb._roidb = cPickle.load(fid)\n        print \'{} gt roidb prepared loaded from {}\'.format(imdb.name, cache_file)\n        return\n\n    roidb = imdb.roidb\n    for i in xrange(len(imdb.image_index)):\n        roidb[i][\'image\'] = imdb.image_path_at(i)\n        boxes = roidb[i][\'boxes\']\n        labels = roidb[i][\'gt_classes\']\n        info_boxes = np.zeros((0, 18), dtype=np.float32)\n\n        if boxes.shape[0] == 0:\n            roidb[i][\'info_boxes\'] = info_boxes\n            continue\n\n        # compute grid boxes\n        s = PIL.Image.open(imdb.image_path_at(i)).size\n        image_height = s[1]\n        image_width = s[0]\n        boxes_grid, cx, cy = get_boxes_grid(image_height, image_width)\n        \n        # for each scale\n        for scale_ind, scale in enumerate(cfg.TRAIN.SCALES):\n            boxes_rescaled = boxes * scale\n\n            # compute overlap\n            overlaps = bbox_overlaps(boxes_grid.astype(np.float), boxes_rescaled.astype(np.float))\n            max_overlaps = overlaps.max(axis = 1)\n            argmax_overlaps = overlaps.argmax(axis = 1)\n            max_classes = labels[argmax_overlaps]\n\n            # select positive boxes\n            fg_inds = []\n            for k in xrange(1, imdb.num_classes):\n                fg_inds.extend(np.where((max_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH))[0])\n\n            if len(fg_inds) > 0:\n                gt_inds = argmax_overlaps[fg_inds]\n                # bounding box regression targets\n                gt_targets = _compute_targets(boxes_grid[fg_inds,:], boxes_rescaled[gt_inds,:])\n                # scale mapping for RoI pooling\n                scale_ind_map = cfg.TRAIN.SCALE_MAPPING[scale_ind]\n                scale_map = cfg.TRAIN.SCALES[scale_ind_map]\n                # contruct the list of positive boxes\n                # (cx, cy, scale_ind, box, scale_ind_map, box_map, gt_label, gt_sublabel, target)\n                info_box = np.zeros((len(fg_inds), 18), dtype=np.float32)\n                info_box[:, 0] = cx[fg_inds]\n                info_box[:, 1] = cy[fg_inds]\n                info_box[:, 2] = scale_ind\n                info_box[:, 3:7] = boxes_grid[fg_inds,:]\n                info_box[:, 7] = scale_ind_map\n                info_box[:, 8:12] = boxes_grid[fg_inds,:] * scale_map / scale\n                info_box[:, 12] = labels[gt_inds]\n                info_box[:, 14:] = gt_targets\n                info_boxes = np.vstack((info_boxes, info_box))\n\n        roidb[i][\'info_boxes\'] = info_boxes\n\n    with open(cache_file, \'wb\') as fid:\n        cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n    print \'wrote gt roidb prepared to {}\'.format(cache_file)\n\ndef add_bbox_regression_targets(roidb):\n    """"""Add information needed to train bounding-box regressors.""""""\n    assert len(roidb) > 0\n    assert \'info_boxes\' in roidb[0], \'Did you call prepare_roidb first?\'\n\n    num_images = len(roidb)\n    # Infer number of classes from the number of columns in gt_overlaps\n    num_classes = roidb[0][\'gt_overlaps\'].shape[1]\n\n    # Compute values needed for means and stds\n    # var(x) = E(x^2) - E(x)^2\n    class_counts = np.zeros((num_classes, 1)) + cfg.EPS\n    sums = np.zeros((num_classes, 4))\n    squared_sums = np.zeros((num_classes, 4))\n    for im_i in xrange(num_images):\n        targets = roidb[im_i][\'info_boxes\']\n        for cls in xrange(1, num_classes):\n            cls_inds = np.where(targets[:, 12] == cls)[0]\n            if cls_inds.size > 0:\n                class_counts[cls] += cls_inds.size\n                sums[cls, :] += targets[cls_inds, 14:].sum(axis=0)\n                squared_sums[cls, :] += (targets[cls_inds, 14:] ** 2).sum(axis=0)\n\n    means = sums / class_counts\n    stds = np.sqrt(squared_sums / class_counts - means ** 2)\n\n    # Normalize targets\n    for im_i in xrange(num_images):\n        targets = roidb[im_i][\'info_boxes\']\n        for cls in xrange(1, num_classes):\n            cls_inds = np.where(targets[:, 12] == cls)[0]\n            roidb[im_i][\'info_boxes\'][cls_inds, 14:] -= means[cls, :]\n            if stds[cls, 0] != 0:\n                roidb[im_i][\'info_boxes\'][cls_inds, 14:] /= stds[cls, :]\n\n    # These values will be needed for making predictions\n    # (the predicts will need to be unnormalized and uncentered)\n    return means.ravel(), stds.ravel()\n\ndef _compute_targets(ex_rois, gt_rois):\n    """"""Compute bounding-box regression targets for an image. The targets are scale invariance""""""\n\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + cfg.EPS\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + cfg.EPS\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + cfg.EPS\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + cfg.EPS\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = np.log(gt_widths / ex_widths)\n    targets_dh = np.log(gt_heights / ex_heights)\n\n    targets = np.zeros((ex_rois.shape[0], 4), dtype=np.float32)\n    targets[:, 0] = targets_dx\n    targets[:, 1] = targets_dy\n    targets[:, 2] = targets_dw\n    targets[:, 3] = targets_dh\n    return targets\n'"
lib/networks/VGGnet_test.py,3,"b""import tensorflow as tf\nfrom networks.network import Network\n\nn_classes = 21\n_feat_stride = [16,]\nanchor_scales = [8, 16, 32] \n\nclass VGGnet_test(Network):\n    def __init__(self, trainable=True):\n        self.inputs = []\n        self.data = tf.placeholder(tf.float32, shape=[None, None, None, 3])\n        self.im_info = tf.placeholder(tf.float32, shape=[None, 3])\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.layers = dict({'data':self.data, 'im_info':self.im_info})\n        self.trainable = trainable\n        self.setup()\n\n    def setup(self):\n        (self.feed('data')\n             .conv(3, 3, 64, 1, 1, name='conv1_1', trainable=False)\n             .conv(3, 3, 64, 1, 1, name='conv1_2', trainable=False)\n             .max_pool(2, 2, 2, 2, padding='VALID', name='pool1')\n             .conv(3, 3, 128, 1, 1, name='conv2_1', trainable=False)\n             .conv(3, 3, 128, 1, 1, name='conv2_2', trainable=False)\n             .max_pool(2, 2, 2, 2, padding='VALID', name='pool2')\n             .conv(3, 3, 256, 1, 1, name='conv3_1')\n             .conv(3, 3, 256, 1, 1, name='conv3_2')\n             .conv(3, 3, 256, 1, 1, name='conv3_3')\n             .max_pool(2, 2, 2, 2, padding='VALID', name='pool3')\n             .conv(3, 3, 512, 1, 1, name='conv4_1')\n             .conv(3, 3, 512, 1, 1, name='conv4_2')\n             .conv(3, 3, 512, 1, 1, name='conv4_3')\n             .max_pool(2, 2, 2, 2, padding='VALID', name='pool4')\n             .conv(3, 3, 512, 1, 1, name='conv5_1')\n             .conv(3, 3, 512, 1, 1, name='conv5_2')\n             .conv(3, 3, 512, 1, 1, name='conv5_3'))\n\n        (self.feed('conv5_3')\n             .conv(3,3,512,1,1,name='rpn_conv/3x3')\n             .conv(1,1,len(anchor_scales)*3*2,1,1,padding='VALID',relu = False,name='rpn_cls_score'))\n\n        (self.feed('rpn_conv/3x3')\n             .conv(1,1,len(anchor_scales)*3*4,1,1,padding='VALID',relu = False,name='rpn_bbox_pred'))\n\n        (self.feed('rpn_cls_score')\n             .reshape_layer(2,name = 'rpn_cls_score_reshape')\n             .softmax(name='rpn_cls_prob'))\n\n        (self.feed('rpn_cls_prob')\n             .reshape_layer(len(anchor_scales)*3*2,name = 'rpn_cls_prob_reshape'))\n\n        (self.feed('rpn_cls_prob_reshape','rpn_bbox_pred','im_info')\n             .proposal_layer(_feat_stride, anchor_scales, 'TEST', name = 'rois'))\n        \n        (self.feed('conv5_3', 'rois')\n             .roi_pool(7, 7, 1.0/16, name='pool_5')\n             .fc(4096, name='fc6')\n             .fc(4096, name='fc7')\n             .fc(n_classes, relu=False, name='cls_score')\n             .softmax(name='cls_prob'))\n\n        (self.feed('fc7')\n             .fc(n_classes*4, relu=False, name='bbox_pred'))\n\n"""
lib/networks/VGGnet_train.py,9,"b'import tensorflow as tf\nfrom networks.network import Network\n\n\n#define\n\nn_classes = 21\n_feat_stride = [16,]\nanchor_scales = [8, 16, 32]\n\nclass VGGnet_train(Network):\n    def __init__(self, trainable=True):\n        self.inputs = []\n        self.data = tf.placeholder(tf.float32, shape=[None, None, None, 3])\n        self.im_info = tf.placeholder(tf.float32, shape=[None, 3])\n        self.gt_boxes = tf.placeholder(tf.float32, shape=[None, 5])\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.layers = dict({\'data\':self.data, \'im_info\':self.im_info, \'gt_boxes\':self.gt_boxes})\n        self.trainable = trainable\n        self.setup()\n\n        # create ops and placeholders for bbox normalization process\n        with tf.variable_scope(\'bbox_pred\', reuse=True):\n            weights = tf.get_variable(""weights"")\n            biases = tf.get_variable(""biases"")\n\n            self.bbox_weights = tf.placeholder(weights.dtype, shape=weights.get_shape())\n            self.bbox_biases = tf.placeholder(biases.dtype, shape=biases.get_shape())\n\n            self.bbox_weights_assign = weights.assign(self.bbox_weights)\n            self.bbox_bias_assign = biases.assign(self.bbox_biases)\n\n    def setup(self):\n        (self.feed(\'data\')\n             .conv(3, 3, 64, 1, 1, name=\'conv1_1\', trainable=False)\n             .conv(3, 3, 64, 1, 1, name=\'conv1_2\', trainable=False)\n             .max_pool(2, 2, 2, 2, padding=\'VALID\', name=\'pool1\')\n             .conv(3, 3, 128, 1, 1, name=\'conv2_1\', trainable=False)\n             .conv(3, 3, 128, 1, 1, name=\'conv2_2\', trainable=False)\n             .max_pool(2, 2, 2, 2, padding=\'VALID\', name=\'pool2\')\n             .conv(3, 3, 256, 1, 1, name=\'conv3_1\')\n             .conv(3, 3, 256, 1, 1, name=\'conv3_2\')\n             .conv(3, 3, 256, 1, 1, name=\'conv3_3\')\n             .max_pool(2, 2, 2, 2, padding=\'VALID\', name=\'pool3\')\n             .conv(3, 3, 512, 1, 1, name=\'conv4_1\')\n             .conv(3, 3, 512, 1, 1, name=\'conv4_2\')\n             .conv(3, 3, 512, 1, 1, name=\'conv4_3\')\n             .max_pool(2, 2, 2, 2, padding=\'VALID\', name=\'pool4\')\n             .conv(3, 3, 512, 1, 1, name=\'conv5_1\')\n             .conv(3, 3, 512, 1, 1, name=\'conv5_2\')\n             .conv(3, 3, 512, 1, 1, name=\'conv5_3\'))\n        #========= RPN ============\n        (self.feed(\'conv5_3\')\n             .conv(3,3,512,1,1,name=\'rpn_conv/3x3\')\n             .conv(1,1,len(anchor_scales)*3*2 ,1 , 1, padding=\'VALID\', relu = False, name=\'rpn_cls_score\'))\n\n        (self.feed(\'rpn_cls_score\',\'gt_boxes\',\'im_info\',\'data\')\n             .anchor_target_layer(_feat_stride, anchor_scales, name = \'rpn-data\' ))\n\n        # Loss of rpn_cls & rpn_boxes\n\n        (self.feed(\'rpn_conv/3x3\')\n             .conv(1,1,len(anchor_scales)*3*4, 1, 1, padding=\'VALID\', relu = False, name=\'rpn_bbox_pred\'))\n\n        #========= RoI Proposal ============\n        (self.feed(\'rpn_cls_score\')\n             .reshape_layer(2,name = \'rpn_cls_score_reshape\')\n             .softmax(name=\'rpn_cls_prob\'))\n\n        (self.feed(\'rpn_cls_prob\')\n             .reshape_layer(len(anchor_scales)*3*2,name = \'rpn_cls_prob_reshape\'))\n\n        (self.feed(\'rpn_cls_prob_reshape\',\'rpn_bbox_pred\',\'im_info\')\n             .proposal_layer(_feat_stride, anchor_scales, \'TRAIN\',name = \'rpn_rois\'))\n\n        (self.feed(\'rpn_rois\',\'gt_boxes\')\n             .proposal_target_layer(n_classes,name = \'roi-data\'))\n\n\n        #========= RCNN ============\n        (self.feed(\'conv5_3\', \'roi-data\')\n             .roi_pool(7, 7, 1.0/16, name=\'pool_5\')\n             .fc(4096, name=\'fc6\')\n             .dropout(0.5, name=\'drop6\')\n             .fc(4096, name=\'fc7\')\n             .dropout(0.5, name=\'drop7\')\n             .fc(n_classes, relu=False, name=\'cls_score\')\n             .softmax(name=\'cls_prob\'))\n\n        (self.feed(\'drop7\')\n             .fc(n_classes*4, relu=False, name=\'bbox_pred\'))\n\n'"
lib/networks/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom .VGGnet_train import VGGnet_train\nfrom .VGGnet_test import VGGnet_test\nfrom . import factory\n'
lib/networks/factory.py,0,"b'# --------------------------------------------------------\n# SubCNN_TF\n# Copyright (c) 2016 CVGL Stanford\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Yu Xiang\n# --------------------------------------------------------\n\n""""""Factory method for easily getting imdbs by name.""""""\n\n__sets = {}\n\nimport networks.VGGnet_train\nimport networks.VGGnet_test\nimport pdb\nimport tensorflow as tf\n\n#__sets[\'VGGnet_train\'] = networks.VGGnet_train()\n\n#__sets[\'VGGnet_test\'] = networks.VGGnet_test()\n\n\ndef get_network(name):\n    """"""Get a network by name.""""""\n    #if not __sets.has_key(name):\n    #    raise KeyError(\'Unknown dataset: {}\'.format(name))\n    #return __sets[name]\n    if name.split(\'_\')[1] == \'test\':\n       return networks.VGGnet_test()\n    elif name.split(\'_\')[1] == \'train\':\n       return networks.VGGnet_train()\n    else:\n       raise KeyError(\'Unknown dataset: {}\'.format(name))\n    \n\ndef list_networks():\n    """"""List all registered imdbs.""""""\n    return __sets.keys()\n'"
lib/networks/network.py,48,"b'import numpy as np\nimport tensorflow as tf\nimport roi_pooling_layer.roi_pooling_op as roi_pool_op\nimport roi_pooling_layer.roi_pooling_op_grad\nfrom rpn_msr.proposal_layer_tf import proposal_layer as proposal_layer_py\nfrom rpn_msr.anchor_target_layer_tf import anchor_target_layer as anchor_target_layer_py\nfrom rpn_msr.proposal_target_layer_tf import proposal_target_layer as proposal_target_layer_py\n\n\n\nDEFAULT_PADDING = \'SAME\'\n\ndef layer(op):\n    def layer_decorated(self, *args, **kwargs):\n        # Automatically set a name if not provided.\n        name = kwargs.setdefault(\'name\', self.get_unique_name(op.__name__))\n        # Figure out the layer inputs.\n        if len(self.inputs)==0:\n            raise RuntimeError(\'No input variables found for layer %s.\'%name)\n        elif len(self.inputs)==1:\n            layer_input = self.inputs[0]\n        else:\n            layer_input = list(self.inputs)\n        # Perform the operation and get the output.\n        layer_output = op(self, layer_input, *args, **kwargs)\n        # Add to layer LUT.\n        self.layers[name] = layer_output\n        # This output is now the input for the next layer.\n        self.feed(layer_output)\n        # Return self for chained calls.\n        return self\n    return layer_decorated\n\nclass Network(object):\n    def __init__(self, inputs, trainable=True):\n        self.inputs = []\n        self.layers = dict(inputs)\n        self.trainable = trainable\n        self.setup()\n\n    def setup(self):\n        raise NotImplementedError(\'Must be subclassed.\')\n\n    def load(self, data_path, session, saver, ignore_missing=False):\n        if data_path.endswith(\'.ckpt\'):\n            saver.restore(session, data_path)\n        else:\n            data_dict = np.load(data_path).item()\n            for key in data_dict:\n                with tf.variable_scope(key, reuse=True):\n                    for subkey in data_dict[key]:\n                        try:\n                            var = tf.get_variable(subkey)\n                            session.run(var.assign(data_dict[key][subkey]))\n                            print ""assign pretrain model ""+subkey+ "" to ""+key\n                        except ValueError:\n                            print ""ignore ""+key\n                            if not ignore_missing:\n\n                                raise\n\n    def feed(self, *args):\n        assert len(args)!=0\n        self.inputs = []\n        for layer in args:\n            if isinstance(layer, basestring):\n                try:\n                    layer = self.layers[layer]\n                    print layer\n                except KeyError:\n                    print self.layers.keys()\n                    raise KeyError(\'Unknown layer name fed: %s\'%layer)\n            self.inputs.append(layer)\n        return self\n\n    def get_output(self, layer):\n        try:\n            layer = self.layers[layer]\n        except KeyError:\n            print self.layers.keys()\n            raise KeyError(\'Unknown layer name fed: %s\'%layer)\n        return layer\n\n    def get_unique_name(self, prefix):\n        id = sum(t.startswith(prefix) for t,_ in self.layers.items())+1\n        return \'%s_%d\'%(prefix, id)\n\n    def make_var(self, name, shape, initializer=None, trainable=True):\n        return tf.get_variable(name, shape, initializer=initializer, trainable=trainable)\n\n    def validate_padding(self, padding):\n        assert padding in (\'SAME\', \'VALID\')\n\n    @layer\n    def conv(self, input, k_h, k_w, c_o, s_h, s_w, name, relu=True, padding=DEFAULT_PADDING, group=1, trainable=True):\n        self.validate_padding(padding)\n        c_i = input.get_shape()[-1]\n        assert c_i%group==0\n        assert c_o%group==0\n        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n        with tf.variable_scope(name) as scope:\n\n            init_weights = tf.truncated_normal_initializer(0.0, stddev=0.01)\n            init_biases = tf.constant_initializer(0.0)\n            kernel = self.make_var(\'weights\', [k_h, k_w, c_i/group, c_o], init_weights, trainable)\n            biases = self.make_var(\'biases\', [c_o], init_biases, trainable)\n\n            if group==1:\n                conv = convolve(input, kernel)\n            else:\n                input_groups = tf.split(3, group, input)\n                kernel_groups = tf.split(3, group, kernel)\n                output_groups = [convolve(i, k) for i,k in zip(input_groups, kernel_groups)]\n                conv = tf.concat(3, output_groups)\n            if relu:\n                bias = tf.nn.bias_add(conv, biases)\n                return tf.nn.relu(bias, name=scope.name)\n            return tf.nn.bias_add(conv, biases, name=scope.name)\n\n    @layer\n    def relu(self, input, name):\n        return tf.nn.relu(input, name=name)\n\n    @layer\n    def max_pool(self, input, k_h, k_w, s_h, s_w, name, padding=DEFAULT_PADDING):\n        self.validate_padding(padding)\n        return tf.nn.max_pool(input,\n                              ksize=[1, k_h, k_w, 1],\n                              strides=[1, s_h, s_w, 1],\n                              padding=padding,\n                              name=name)\n\n    @layer\n    def avg_pool(self, input, k_h, k_w, s_h, s_w, name, padding=DEFAULT_PADDING):\n        self.validate_padding(padding)\n        return tf.nn.avg_pool(input,\n                              ksize=[1, k_h, k_w, 1],\n                              strides=[1, s_h, s_w, 1],\n                              padding=padding,\n                              name=name)\n\n    @layer\n    def roi_pool(self, input, pooled_height, pooled_width, spatial_scale, name):\n        # only use the first input\n        if isinstance(input[0], tuple):\n            input[0] = input[0][0]\n\n        if isinstance(input[1], tuple):\n            input[1] = input[1][0]\n\n        print input\n        return roi_pool_op.roi_pool(input[0], input[1],\n                                    pooled_height,\n                                    pooled_width,\n                                    spatial_scale,\n                                    name=name)[0]\n\n    @layer\n    def proposal_layer(self, input, _feat_stride, anchor_scales, cfg_key, name):\n        if isinstance(input[0], tuple):\n            input[0] = input[0][0]\n        return tf.reshape(tf.py_func(proposal_layer_py,[input[0],input[1],input[2], cfg_key, _feat_stride, anchor_scales], [tf.float32]),[-1,5],name =name)\n\n\n    @layer\n    def anchor_target_layer(self, input, _feat_stride, anchor_scales, name):\n        if isinstance(input[0], tuple):\n            input[0] = input[0][0]\n\n        with tf.variable_scope(name) as scope:\n\n            rpn_labels,rpn_bbox_targets,rpn_bbox_inside_weights,rpn_bbox_outside_weights = tf.py_func(anchor_target_layer_py,[input[0],input[1],input[2],input[3], _feat_stride, anchor_scales],[tf.float32,tf.float32,tf.float32,tf.float32])\n\n            rpn_labels = tf.convert_to_tensor(tf.cast(rpn_labels,tf.int32), name = \'rpn_labels\')\n            rpn_bbox_targets = tf.convert_to_tensor(rpn_bbox_targets, name = \'rpn_bbox_targets\')\n            rpn_bbox_inside_weights = tf.convert_to_tensor(rpn_bbox_inside_weights , name = \'rpn_bbox_inside_weights\')\n            rpn_bbox_outside_weights = tf.convert_to_tensor(rpn_bbox_outside_weights , name = \'rpn_bbox_outside_weights\')\n\n\n            return rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights\n\n\n    @layer\n    def proposal_target_layer(self, input, classes, name):\n        if isinstance(input[0], tuple):\n            input[0] = input[0][0]\n        with tf.variable_scope(name) as scope:\n\n            rois,labels,bbox_targets,bbox_inside_weights,bbox_outside_weights = tf.py_func(proposal_target_layer_py,[input[0],input[1],classes],[tf.float32,tf.float32,tf.float32,tf.float32,tf.float32])\n\n            rois = tf.reshape(rois,[-1,5] , name = \'rois\') \n            labels = tf.convert_to_tensor(tf.cast(labels,tf.int32), name = \'labels\')\n            bbox_targets = tf.convert_to_tensor(bbox_targets, name = \'bbox_targets\')\n            bbox_inside_weights = tf.convert_to_tensor(bbox_inside_weights, name = \'bbox_inside_weights\')\n            bbox_outside_weights = tf.convert_to_tensor(bbox_outside_weights, name = \'bbox_outside_weights\')\n\n           \n            return rois, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights\n\n\n    @layer\n    def reshape_layer(self, input, d,name):\n        input_shape = tf.shape(input)\n        if name == \'rpn_cls_prob_reshape\':\n             return tf.transpose(tf.reshape(tf.transpose(input,[0,3,1,2]),[input_shape[0],\n                    int(d),tf.cast(tf.cast(input_shape[1],tf.float32)/tf.cast(d,tf.float32)*tf.cast(input_shape[3],tf.float32),tf.int32),input_shape[2]]),[0,2,3,1],name=name)\n        else:\n             return tf.transpose(tf.reshape(tf.transpose(input,[0,3,1,2]),[input_shape[0],\n                    int(d),tf.cast(tf.cast(input_shape[1],tf.float32)*(tf.cast(input_shape[3],tf.float32)/tf.cast(d,tf.float32)),tf.int32),input_shape[2]]),[0,2,3,1],name=name)\n\n    @layer\n    def feature_extrapolating(self, input, scales_base, num_scale_base, num_per_octave, name):\n        return feature_extrapolating_op.feature_extrapolating(input,\n                              scales_base,\n                              num_scale_base,\n                              num_per_octave,\n                              name=name)\n\n    @layer\n    def lrn(self, input, radius, alpha, beta, name, bias=1.0):\n        return tf.nn.local_response_normalization(input,\n                                                  depth_radius=radius,\n                                                  alpha=alpha,\n                                                  beta=beta,\n                                                  bias=bias,\n                                                  name=name)\n\n    @layer\n    def concat(self, inputs, axis, name):\n        return tf.concat(concat_dim=axis, values=inputs, name=name)\n\n    @layer\n    def fc(self, input, num_out, name, relu=True, trainable=True):\n        with tf.variable_scope(name) as scope:\n            # only use the first input\n            if isinstance(input, tuple):\n                input = input[0]\n\n            input_shape = input.get_shape()\n            if input_shape.ndims == 4:\n                dim = 1\n                for d in input_shape[1:].as_list():\n                    dim *= d\n                feed_in = tf.reshape(tf.transpose(input,[0,3,1,2]), [-1, dim])\n            else:\n                feed_in, dim = (input, int(input_shape[-1]))\n\n            if name == \'bbox_pred\':\n                init_weights = tf.truncated_normal_initializer(0.0, stddev=0.001)\n                init_biases = tf.constant_initializer(0.0)\n            else:\n                init_weights = tf.truncated_normal_initializer(0.0, stddev=0.01)\n                init_biases = tf.constant_initializer(0.0)\n\n            weights = self.make_var(\'weights\', [dim, num_out], init_weights, trainable)\n            biases = self.make_var(\'biases\', [num_out], init_biases, trainable)\n\n            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n            fc = op(feed_in, weights, biases, name=scope.name)\n            return fc\n\n    @layer\n    def softmax(self, input, name):\n        input_shape = tf.shape(input)\n        if name == \'rpn_cls_prob\':\n            return tf.reshape(tf.nn.softmax(tf.reshape(input,[-1,input_shape[3]])),[-1,input_shape[1],input_shape[2],input_shape[3]],name=name)\n        else:\n            return tf.nn.softmax(input,name=name)\n\n    @layer\n    def dropout(self, input, keep_prob, name):\n        return tf.nn.dropout(input, keep_prob, name=name)\n'"
lib/nms/__init__.py,0,b''
lib/nms/py_cpu_nms.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef py_cpu_nms(dets, thresh):\n    """"""Pure Python NMS baseline.""""""\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
lib/roi_data_layer/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n'
lib/roi_data_layer/layer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""The data layer used during training to train a Fast R-CNN network.\n\nRoIDataLayer implements a Caffe Python layer.\n""""""\n\nfrom fast_rcnn.config import cfg\nfrom roi_data_layer.minibatch import get_minibatch\nimport numpy as np\n\nclass RoIDataLayer(object):\n    """"""Fast R-CNN data layer used for training.""""""\n\n    def __init__(self, roidb, num_classes):\n        """"""Set the roidb to be used by this layer during training.""""""\n        self._roidb = roidb\n        self._num_classes = num_classes\n        self._shuffle_roidb_inds()\n\n    def _shuffle_roidb_inds(self):\n        """"""Randomly permute the training roidb.""""""\n        self._perm = np.random.permutation(np.arange(len(self._roidb)))\n        self._cur = 0\n\n    def _get_next_minibatch_inds(self):\n        """"""Return the roidb indices for the next minibatch.""""""\n        \n        if cfg.TRAIN.HAS_RPN:\n            if self._cur + cfg.TRAIN.IMS_PER_BATCH >= len(self._roidb):\n                self._shuffle_roidb_inds()\n\n            db_inds = self._perm[self._cur:self._cur + cfg.TRAIN.IMS_PER_BATCH]\n            self._cur += cfg.TRAIN.IMS_PER_BATCH\n        else:\n            # sample images\n            db_inds = np.zeros((cfg.TRAIN.IMS_PER_BATCH), dtype=np.int32)\n            i = 0\n            while (i < cfg.TRAIN.IMS_PER_BATCH):\n                ind = self._perm[self._cur]\n                num_objs = self._roidb[ind][\'boxes\'].shape[0]\n                if num_objs != 0:\n                    db_inds[i] = ind\n                    i += 1\n\n                self._cur += 1\n                if self._cur >= len(self._roidb):\n                    self._shuffle_roidb_inds()\n\n        return db_inds\n\n    def _get_next_minibatch(self):\n        """"""Return the blobs to be used for the next minibatch.\n\n        If cfg.TRAIN.USE_PREFETCH is True, then blobs will be computed in a\n        separate process and made available through self._blob_queue.\n        """"""\n        db_inds = self._get_next_minibatch_inds()\n        minibatch_db = [self._roidb[i] for i in db_inds]\n        return get_minibatch(minibatch_db, self._num_classes)\n            \n    def forward(self):\n        """"""Get blobs and copy them into this layer\'s top blob vector.""""""\n        blobs = self._get_next_minibatch()\n        return blobs\n'"
lib/roi_data_layer/minibatch.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Compute minibatch blobs for training a Fast R-CNN network.""""""\n\nimport numpy as np\nimport numpy.random as npr\nimport cv2\nfrom fast_rcnn.config import cfg\nfrom utils.blob import prep_im_for_blob, im_list_to_blob\n\ndef get_minibatch(roidb, num_classes):\n    """"""Given a roidb, construct a minibatch sampled from it.""""""\n    num_images = len(roidb)\n    # Sample random scales to use for each image in this batch\n    random_scale_inds = npr.randint(0, high=len(cfg.TRAIN.SCALES),\n                                    size=num_images)\n    assert(cfg.TRAIN.BATCH_SIZE % num_images == 0), \\\n        \'num_images ({}) must divide BATCH_SIZE ({})\'. \\\n        format(num_images, cfg.TRAIN.BATCH_SIZE)\n    rois_per_image = cfg.TRAIN.BATCH_SIZE / num_images\n    fg_rois_per_image = np.round(cfg.TRAIN.FG_FRACTION * rois_per_image)\n\n    # Get the input image blob, formatted for caffe\n    im_blob, im_scales = _get_image_blob(roidb, random_scale_inds)\n\n    blobs = {\'data\': im_blob}\n\n    if cfg.TRAIN.HAS_RPN:\n        assert len(im_scales) == 1, ""Single batch only""\n        assert len(roidb) == 1, ""Single batch only""\n        # gt boxes: (x1, y1, x2, y2, cls)\n        gt_inds = np.where(roidb[0][\'gt_classes\'] != 0)[0]\n        gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)\n        gt_boxes[:, 0:4] = roidb[0][\'boxes\'][gt_inds, :] * im_scales[0]\n        gt_boxes[:, 4] = roidb[0][\'gt_classes\'][gt_inds]\n        blobs[\'gt_boxes\'] = gt_boxes\n        blobs[\'im_info\'] = np.array(\n            [[im_blob.shape[1], im_blob.shape[2], im_scales[0]]],\n            dtype=np.float32)\n    else: # not using RPN\n        # Now, build the region of interest and label blobs\n        rois_blob = np.zeros((0, 5), dtype=np.float32)\n        labels_blob = np.zeros((0), dtype=np.float32)\n        bbox_targets_blob = np.zeros((0, 4 * num_classes), dtype=np.float32)\n        bbox_inside_blob = np.zeros(bbox_targets_blob.shape, dtype=np.float32)\n        # all_overlaps = []\n        for im_i in xrange(num_images):\n            labels, overlaps, im_rois, bbox_targets, bbox_inside_weights \\\n                = _sample_rois(roidb[im_i], fg_rois_per_image, rois_per_image,\n                               num_classes)\n\n            # Add to RoIs blob\n            rois = _project_im_rois(im_rois, im_scales[im_i])\n            batch_ind = im_i * np.ones((rois.shape[0], 1))\n            rois_blob_this_image = np.hstack((batch_ind, rois))\n            rois_blob = np.vstack((rois_blob, rois_blob_this_image))\n\n            # Add to labels, bbox targets, and bbox loss blobs\n            labels_blob = np.hstack((labels_blob, labels))\n            bbox_targets_blob = np.vstack((bbox_targets_blob, bbox_targets))\n            bbox_inside_blob = np.vstack((bbox_inside_blob, bbox_inside_weights))\n            # all_overlaps = np.hstack((all_overlaps, overlaps))\n\n        # For debug visualizations\n        # _vis_minibatch(im_blob, rois_blob, labels_blob, all_overlaps)\n\n        blobs[\'rois\'] = rois_blob\n        blobs[\'labels\'] = labels_blob\n\n        if cfg.TRAIN.BBOX_REG:\n            blobs[\'bbox_targets\'] = bbox_targets_blob\n            blobs[\'bbox_inside_weights\'] = bbox_inside_blob\n            blobs[\'bbox_outside_weights\'] = \\\n                np.array(bbox_inside_blob > 0).astype(np.float32)\n\n    return blobs\n\ndef _sample_rois(roidb, fg_rois_per_image, rois_per_image, num_classes):\n    """"""Generate a random sample of RoIs comprising foreground and background\n    examples.\n    """"""\n    # label = class RoI has max overlap with\n    labels = roidb[\'max_classes\']\n    overlaps = roidb[\'max_overlaps\']\n    rois = roidb[\'boxes\']\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = np.where(overlaps >= cfg.TRAIN.FG_THRESH)[0]\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = int(np.minimum(fg_rois_per_image, fg_inds.size))\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(\n                fg_inds, size=fg_rois_per_this_image, replace=False)\n\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where((overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                       (overlaps >= cfg.TRAIN.BG_THRESH_LO))[0]\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    bg_rois_per_this_image = np.minimum(bg_rois_per_this_image,\n                                        bg_inds.size)\n    # Sample foreground regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(\n                bg_inds, size=bg_rois_per_this_image, replace=False)\n\n    # The indices that we\'re selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds)\n    # Select sampled values from various arrays:\n    labels = labels[keep_inds]\n    # Clamp labels for the background RoIs to 0\n    labels[fg_rois_per_this_image:] = 0\n    overlaps = overlaps[keep_inds]\n    rois = rois[keep_inds]\n\n    bbox_targets, bbox_inside_weights = _get_bbox_regression_labels(\n            roidb[\'bbox_targets\'][keep_inds, :], num_classes)\n\n    return labels, overlaps, rois, bbox_targets, bbox_inside_weights\n\ndef _get_image_blob(roidb, scale_inds):\n    """"""Builds an input blob from the images in the roidb at the specified\n    scales.\n    """"""\n    num_images = len(roidb)\n    processed_ims = []\n    im_scales = []\n    for i in xrange(num_images):\n        im = cv2.imread(roidb[i][\'image\'])\n        if roidb[i][\'flipped\']:\n            im = im[:, ::-1, :]\n        target_size = cfg.TRAIN.SCALES[scale_inds[i]]\n        im, im_scale = prep_im_for_blob(im, cfg.PIXEL_MEANS, target_size,\n                                        cfg.TRAIN.MAX_SIZE)\n        im_scales.append(im_scale)\n        processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, im_scales\n\ndef _project_im_rois(im_rois, im_scale_factor):\n    """"""Project image RoIs into the rescaled training image.""""""\n    rois = im_rois * im_scale_factor\n    return rois\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    """"""Bounding-box regression targets are stored in a compact form in the\n    roidb.\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets). The loss weights\n    are similarly expanded.\n\n    Returns:\n        bbox_target_data (ndarray): N x 4K blob of regression targets\n        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n    """"""\n    clss = np.array(bbox_target_data[:, 0], dtype=np.uint16, copy=True)\n    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n    bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = 4 * cls\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n        bbox_inside_weights[ind, start:end] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS\n    return bbox_targets, bbox_inside_weights\n\ndef _vis_minibatch(im_blob, rois_blob, labels_blob, overlaps):\n    """"""Visualize a mini-batch for debugging.""""""\n    import matplotlib.pyplot as plt\n    for i in xrange(rois_blob.shape[0]):\n        rois = rois_blob[i, :]\n        im_ind = rois[0]\n        roi = rois[1:]\n        im = im_blob[im_ind, :, :, :].transpose((1, 2, 0)).copy()\n        im += cfg.PIXEL_MEANS\n        im = im[:, :, (2, 1, 0)]\n        im = im.astype(np.uint8)\n        cls = labels_blob[i]\n        plt.imshow(im)\n        print \'class: \', cls, \' overlap: \', overlaps[i]\n        plt.gca().add_patch(\n            plt.Rectangle((roi[0], roi[1]), roi[2] - roi[0],\n                          roi[3] - roi[1], fill=False,\n                          edgecolor=\'r\', linewidth=3)\n            )\n        plt.show()\n'"
lib/roi_data_layer/minibatch2.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Compute minibatch blobs for training a Fast R-CNN network.""""""\n\nimport numpy as np\nimport numpy.random as npr\nimport cv2\nfrom fast_rcnn.config import cfg\nfrom utils.blob import prep_im_for_blob, im_list_to_blob\n\ndef get_minibatch(roidb, num_classes):\n    """"""Given a roidb, construct a minibatch sampled from it.""""""\n    num_images = len(roidb)\n\n    assert(cfg.TRAIN.BATCH_SIZE % num_images == 0), \\\n        \'num_images ({}) must divide BATCH_SIZE ({})\'. \\\n        format(num_images, cfg.TRAIN.BATCH_SIZE)\n    rois_per_image = cfg.TRAIN.BATCH_SIZE / num_images\n    fg_rois_per_image = np.round(cfg.TRAIN.FG_FRACTION * rois_per_image)\n\n    if cfg.IS_MULTISCALE:\n        im_blob, im_scales = _get_image_blob_multiscale(roidb)\n    else:\n        # Get the input image blob, formatted for caffe\n        # Sample random scales to use for each image in this batch\n        random_scale_inds = npr.randint(0, high=len(cfg.TRAIN.SCALES_BASE), size=num_images)\n        im_blob, im_scales = _get_image_blob(roidb, random_scale_inds)\n\n    blobs = {\'data\': im_blob}\n\n    if cfg.TRAIN.HAS_RPN:\n        assert len(im_scales) == 1, ""Single batch only""\n        assert len(roidb) == 1, ""Single batch only""\n        # gt boxes: (x1, y1, x2, y2, cls)\n        gt_inds = np.where(roidb[0][\'gt_classes\'] != 0)[0]\n        gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)\n        gt_boxes[:, 0:4] = roidb[0][\'boxes\'][gt_inds, :] * im_scales[0]\n        gt_boxes[:, 4] = roidb[0][\'gt_classes\'][gt_inds]\n        blobs[\'gt_boxes\'] = gt_boxes\n        blobs[\'im_info\'] = np.array(\n            [[im_blob.shape[1], im_blob.shape[2], im_scales[0]]],\n            dtype=np.float32)\n\n\n    else:\n        # Now, build the region of interest and label blobs\n        rois_blob = np.zeros((0, 5), dtype=np.float32)\n        labels_blob = np.zeros((0), dtype=np.float32)\n        bbox_targets_blob = np.zeros((0, 4 * num_classes), dtype=np.float32)\n        bbox_inside_blob = np.zeros(bbox_targets_blob.shape, dtype=np.float32)\n\n        # all_overlaps = []\n        for im_i in xrange(num_images):\n            labels, overlaps, im_rois, bbox_targets, bbox_inside_weights, sublabels \\\n                    = _sample_rois(roidb[im_i], fg_rois_per_image, rois_per_image, num_classes)\n\n            # Add to RoIs blob\n            if cfg.IS_MULTISCALE:\n                if cfg.IS_EXTRAPOLATING:\n                    rois, levels = _project_im_rois_multiscale(im_rois, cfg.TRAIN.SCALES)\n                    batch_ind = im_i * len(cfg.TRAIN.SCALES) + levels\n                else:\n                    rois, levels = _project_im_rois_multiscale(im_rois, cfg.TRAIN.SCALES_BASE)\n                    batch_ind = im_i * len(cfg.TRAIN.SCALES_BASE) + levels\n            else:\n                rois = _project_im_rois(im_rois, im_scales[im_i])\n                batch_ind = im_i * np.ones((rois.shape[0], 1))\n\n            rois_blob_this_image = np.hstack((batch_ind, rois))\n            rois_blob = np.vstack((rois_blob, rois_blob_this_image))\n\n            # Add to labels, bbox targets, and bbox loss blobs\n            labels_blob = np.hstack((labels_blob, labels))\n            bbox_targets_blob = np.vstack((bbox_targets_blob, bbox_targets))\n            bbox_inside_blob = np.vstack((bbox_inside_blob, bbox_inside_weights))\n\n            # all_overlaps = np.hstack((all_overlaps, overlaps))\n\n        # For debug visualizations\n        # _vis_minibatch(im_blob, rois_blob, labels_blob, all_overlaps, sublabels_blob, view_targets_blob, view_inside_blob)\n        # _vis_minibatch(im_blob, rois_blob, labels_blob, all_overlaps, sublabels_blob)\n\n        blobs[\'rois\'] = rois_blob\n        blobs[\'labels\'] = labels_blob\n\n        if cfg.TRAIN.BBOX_REG:\n            blobs[\'bbox_targets\'] = bbox_targets_blob\n            blobs[\'bbox_inside_weights\'] = bbox_inside_blob\n            blobs[\'bbox_outside_weights\'] = np.array(bbox_inside_blob > 0).astype(np.float32)\n\n    return blobs\n\ndef _sample_rois(roidb, fg_rois_per_image, rois_per_image, num_classes):\n    """"""Generate a random sample of RoIs comprising foreground and background\n    examples.\n    """"""\n    # label = class RoI has max overlap with\n    labels = roidb[\'max_classes\']\n    overlaps = roidb[\'max_overlaps\']\n    rois = roidb[\'boxes\']\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = []\n    for i in xrange(1, num_classes):\n        fg_inds.extend(np.where((labels == i) & (overlaps >= cfg.TRAIN.FG_THRESH))[0])\n    fg_inds = np.array(fg_inds)\n\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = np.minimum(fg_rois_per_image, fg_inds.size)\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(fg_inds, size=fg_rois_per_this_image,\n                             replace=False)\n\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = []\n    for i in xrange(1, num_classes):\n        bg_inds.extend( np.where((labels == i) & (overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                        (overlaps >= cfg.TRAIN.BG_THRESH_LO))[0] )\n\n    if len(bg_inds) < bg_rois_per_this_image:\n        for i in xrange(1, num_classes):\n            bg_inds.extend( np.where((labels == i) & (overlaps < cfg.TRAIN.BG_THRESH_HI))[0] )\n\n    if len(bg_inds) < bg_rois_per_this_image:\n        bg_inds.extend( np.where(overlaps < cfg.TRAIN.BG_THRESH_HI)[0] )\n    bg_inds = np.array(bg_inds, dtype=np.int32)\n\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = np.minimum(bg_rois_per_this_image,\n                                        bg_inds.size)\n    # Sample foreground regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(bg_inds, size=bg_rois_per_this_image,\n                             replace=False)\n\n    # The indices that we\'re selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds).astype(int)\n    # print \'{} foregrounds and {} backgrounds\'.format(fg_inds.size, bg_inds.size)\n    # Select sampled values from various arrays:\n    labels = labels[keep_inds]\n    # Clamp labels for the background RoIs to 0\n    labels[fg_rois_per_this_image:] = 0\n    overlaps = overlaps[keep_inds]\n    rois = rois[keep_inds]\n    sublabels = sublabels[keep_inds]\n    sublabels[fg_rois_per_this_image:] = 0\n\n    bbox_targets, bbox_loss_weights = \\\n            _get_bbox_regression_labels(roidb[\'bbox_targets\'][keep_inds, :],\n                                        num_classes)\n\n    if cfg.TRAIN.VIEWPOINT or cfg.TEST.VIEWPOINT:\n        viewpoints = viewpoints[keep_inds]\n        view_targets, view_loss_weights = \\\n                _get_viewpoint_estimation_labels(viewpoints, labels, num_classes)\n        return labels, overlaps, rois, bbox_targets, bbox_loss_weights, sublabels, view_targets, view_loss_weights\n\n    return labels, overlaps, rois, bbox_targets, bbox_loss_weights, sublabels\n\ndef _get_image_blob(roidb, scale_inds):\n    """"""Builds an input blob from the images in the roidb at the specified\n    scales.\n    """"""\n    num_images = len(roidb)\n    processed_ims = []\n    im_scales = []\n    for i in xrange(num_images):\n        im = cv2.imread(roidb[i][\'image\'])\n        if roidb[i][\'flipped\']:\n            im = im[:, ::-1, :]\n\n        im_orig = im.astype(np.float32, copy=True)\n        im_orig -= cfg.PIXEL_MEANS\n\n        im_scale = cfg.TRAIN.SCALES_BASE[scale_inds[i]]\n        im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)\n\n        im_scales.append(im_scale)\n        processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, im_scales\n\n\ndef _get_image_blob_multiscale(roidb):\n    """"""Builds an input blob from the images in the roidb at multiscales.\n    """"""\n    num_images = len(roidb)\n    processed_ims = []\n    im_scales = []\n    scales = cfg.TRAIN.SCALES_BASE\n    for i in xrange(num_images):\n        im = cv2.imread(roidb[i][\'image\'])\n        if roidb[i][\'flipped\']:\n            im = im[:, ::-1, :]\n\n        im_orig = im.astype(np.float32, copy=True)\n        im_orig -= cfg.PIXEL_MEANS\n\n        for im_scale in scales:\n            im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)\n            im_scales.append(im_scale)\n            processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, im_scales\n\n\ndef _project_im_rois(im_rois, im_scale_factor):\n    """"""Project image RoIs into the rescaled training image.""""""\n    rois = im_rois * im_scale_factor\n    return rois\n\n\ndef _project_im_rois_multiscale(im_rois, scales):\n    """"""Project image RoIs into the image pyramid built by _get_image_blob.\n\n    Arguments:\n        im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n        scales (list): scale factors as returned by _get_image_blob\n\n    Returns:\n        rois (ndarray): R x 4 matrix of projected RoI coordinates\n        levels (list): image pyramid levels used by each projected RoI\n    """"""\n    im_rois = im_rois.astype(np.float, copy=False)\n    scales = np.array(scales)\n\n    if len(scales) > 1:\n        widths = im_rois[:, 2] - im_rois[:, 0] + 1\n        heights = im_rois[:, 3] - im_rois[:, 1] + 1\n\n        areas = widths * heights\n        scaled_areas = areas[:, np.newaxis] * (scales[np.newaxis, :] ** 2)\n        diff_areas = np.abs(scaled_areas - 224 * 224)\n        levels = diff_areas.argmin(axis=1)[:, np.newaxis]\n    else:\n        levels = np.zeros((im_rois.shape[0], 1), dtype=np.int)\n\n    rois = im_rois * scales[levels]\n\n    return rois, levels\n\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    """"""Bounding-box regression targets are stored in a compact form in the\n    roidb.\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets). The loss weights\n    are similarly expanded.\n\n    Returns:\n        bbox_target_data (ndarray): N x 4K blob of regression targets\n        bbox_loss_weights (ndarray): N x 4K blob of loss weights\n    """"""\n    clss = bbox_target_data[:, 0]\n    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n    bbox_loss_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = 4 * cls\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n        bbox_loss_weights[ind, start:end] = [1., 1., 1., 1.]\n    return bbox_targets, bbox_loss_weights\n\n\ndef _get_viewpoint_estimation_labels(viewpoint_data, clss, num_classes):\n    """"""Bounding-box regression targets are stored in a compact form in the\n    roidb.\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets). The loss weights\n    are similarly expanded.\n\n    Returns:\n        view_target_data (ndarray): N x 3K blob of regression targets\n        view_loss_weights (ndarray): N x 3K blob of loss weights\n    """"""\n    view_targets = np.zeros((clss.size, 3 * num_classes), dtype=np.float32)\n    view_loss_weights = np.zeros(view_targets.shape, dtype=np.float32)\n    inds = np.where( (clss > 0) & np.isfinite(viewpoint_data[:,0]) & np.isfinite(viewpoint_data[:,1]) & np.isfinite(viewpoint_data[:,2]) )[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = 3 * cls\n        end = start + 3\n        view_targets[ind, start:end] = viewpoint_data[ind, :]\n        view_loss_weights[ind, start:end] = [1., 1., 1.]\n\n    assert not np.isinf(view_targets).any(), \'viewpoint undefined\'\n    return view_targets, view_loss_weights\n\n\ndef _vis_minibatch(im_blob, rois_blob, labels_blob, overlaps, sublabels_blob, view_targets_blob=None, view_inside_blob=None):\n    """"""Visualize a mini-batch for debugging.""""""\n    import matplotlib.pyplot as plt\n    import math\n    for i in xrange(min(rois_blob.shape[0], 10)):\n        rois = rois_blob[i, :]\n        im_ind = rois[0]\n        roi = rois[1:]\n        im = im_blob[im_ind, :, :, :].transpose((1, 2, 0)).copy()\n        im += cfg.PIXEL_MEANS\n        im = im[:, :, (2, 1, 0)]\n        im = im.astype(np.uint8)\n        cls = labels_blob[i]\n        subcls = sublabels_blob[i]\n        plt.imshow(im)\n        print \'class: \', cls, \' subclass: \', subcls, \' overlap: \', overlaps[i]\n\n        start = 3 * cls\n        end = start + 3\n        # print \'view: \', view_targets_blob[i, start:end] * 180 / math.pi\n        # print \'view weights: \', view_inside_blob[i, start:end]\n\n        plt.gca().add_patch(\n            plt.Rectangle((roi[0], roi[1]), roi[2] - roi[0],\n                          roi[3] - roi[1], fill=False,\n                          edgecolor=\'r\', linewidth=3)\n            )\n        plt.show()\n'"
lib/roi_data_layer/roidb.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Transform a roidb into a trainable roidb by adding a bunch of metadata.""""""\n\nimport numpy as np\nfrom fast_rcnn.config import cfg\nfrom fast_rcnn.bbox_transform import bbox_transform\nfrom utils.cython_bbox import bbox_overlaps\nimport PIL\n\ndef prepare_roidb(imdb):\n    """"""Enrich the imdb\'s roidb by adding some derived quantities that\n    are useful for training. This function precomputes the maximum\n    overlap, taken over ground-truth boxes, between each ROI and\n    each ground-truth box. The class with maximum overlap is also\n    recorded.\n    """"""\n    sizes = [PIL.Image.open(imdb.image_path_at(i)).size\n             for i in xrange(imdb.num_images)]\n    roidb = imdb.roidb\n    for i in xrange(len(imdb.image_index)):\n        roidb[i][\'image\'] = imdb.image_path_at(i)\n        roidb[i][\'width\'] = sizes[i][0]\n        roidb[i][\'height\'] = sizes[i][1]\n        # need gt_overlaps as a dense array for argmax\n        gt_overlaps = roidb[i][\'gt_overlaps\'].toarray()\n        # max overlap with gt over classes (columns)\n        max_overlaps = gt_overlaps.max(axis=1)\n        # gt class that had the max overlap\n        max_classes = gt_overlaps.argmax(axis=1)\n        roidb[i][\'max_classes\'] = max_classes\n        roidb[i][\'max_overlaps\'] = max_overlaps\n        # sanity checks\n        # max overlap of 0 => class should be zero (background)\n        zero_inds = np.where(max_overlaps == 0)[0]\n        assert all(max_classes[zero_inds] == 0)\n        # max overlap > 0 => class should not be zero (must be a fg class)\n        nonzero_inds = np.where(max_overlaps > 0)[0]\n        assert all(max_classes[nonzero_inds] != 0)\n\ndef add_bbox_regression_targets(roidb):\n    """"""Add information needed to train bounding-box regressors.""""""\n    assert len(roidb) > 0\n    assert \'max_classes\' in roidb[0], \'Did you call prepare_roidb first?\'\n\n    num_images = len(roidb)\n    # Infer number of classes from the number of columns in gt_overlaps\n    num_classes = roidb[0][\'gt_overlaps\'].shape[1]\n    for im_i in xrange(num_images):\n        rois = roidb[im_i][\'boxes\']\n        max_overlaps = roidb[im_i][\'max_overlaps\']\n        max_classes = roidb[im_i][\'max_classes\']\n        roidb[im_i][\'bbox_targets\'] = \\\n                _compute_targets(rois, max_overlaps, max_classes)\n\n    if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n        # Use fixed / precomputed ""means"" and ""stds"" instead of empirical values\n        means = np.tile(\n                np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS), (num_classes, 1))\n        stds = np.tile(\n                np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS), (num_classes, 1))\n    else:\n        # Compute values needed for means and stds\n        # var(x) = E(x^2) - E(x)^2\n        class_counts = np.zeros((num_classes, 1)) + cfg.EPS\n        sums = np.zeros((num_classes, 4))\n        squared_sums = np.zeros((num_classes, 4))\n        for im_i in xrange(num_images):\n            targets = roidb[im_i][\'bbox_targets\']\n            for cls in xrange(1, num_classes):\n                cls_inds = np.where(targets[:, 0] == cls)[0]\n                if cls_inds.size > 0:\n                    class_counts[cls] += cls_inds.size\n                    sums[cls, :] += targets[cls_inds, 1:].sum(axis=0)\n                    squared_sums[cls, :] += \\\n                            (targets[cls_inds, 1:] ** 2).sum(axis=0)\n\n        means = sums / class_counts\n        stds = np.sqrt(squared_sums / class_counts - means ** 2)\n\n    print \'bbox target means:\'\n    print means\n    print means[1:, :].mean(axis=0) # ignore bg class\n    print \'bbox target stdevs:\'\n    print stds\n    print stds[1:, :].mean(axis=0) # ignore bg class\n\n    # Normalize targets\n    if cfg.TRAIN.BBOX_NORMALIZE_TARGETS:\n        print ""Normalizing targets""\n        for im_i in xrange(num_images):\n            targets = roidb[im_i][\'bbox_targets\']\n            for cls in xrange(1, num_classes):\n                cls_inds = np.where(targets[:, 0] == cls)[0]\n                roidb[im_i][\'bbox_targets\'][cls_inds, 1:] -= means[cls, :]\n                roidb[im_i][\'bbox_targets\'][cls_inds, 1:] /= stds[cls, :]\n    else:\n        print ""NOT normalizing targets""\n\n    # These values will be needed for making predictions\n    # (the predicts will need to be unnormalized and uncentered)\n    return means.ravel(), stds.ravel()\n\ndef _compute_targets(rois, overlaps, labels):\n    """"""Compute bounding-box regression targets for an image.""""""\n    # Indices of ground-truth ROIs\n    gt_inds = np.where(overlaps == 1)[0]\n    if len(gt_inds) == 0:\n        # Bail if the image has no ground-truth ROIs\n        return np.zeros((rois.shape[0], 5), dtype=np.float32)\n    # Indices of examples for which we try to make predictions\n    ex_inds = np.where(overlaps >= cfg.TRAIN.BBOX_THRESH)[0]\n\n    # Get IoU overlap between each ex ROI and gt ROI\n    ex_gt_overlaps = bbox_overlaps(\n        np.ascontiguousarray(rois[ex_inds, :], dtype=np.float),\n        np.ascontiguousarray(rois[gt_inds, :], dtype=np.float))\n\n    # Find which gt ROI each ex ROI has max overlap with:\n    # this will be the ex ROI\'s gt target\n    gt_assignment = ex_gt_overlaps.argmax(axis=1)\n    gt_rois = rois[gt_inds[gt_assignment], :]\n    ex_rois = rois[ex_inds, :]\n\n    targets = np.zeros((rois.shape[0], 5), dtype=np.float32)\n    targets[ex_inds, 0] = labels[ex_inds]\n    targets[ex_inds, 1:] = bbox_transform(ex_rois, gt_rois)\n    return targets\n'"
lib/roi_data_layer/roidb2.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Transform a roidb into a trainable roidb by adding a bunch of metadata.""""""\n\nimport numpy as np\nfrom fast_rcnn.config import cfg\nimport utils.cython_bbox\n\ndef prepare_roidb(imdb):\n    """"""Enrich the imdb\'s roidb by adding some derived quantities that\n    are useful for training. This function precomputes the maximum\n    overlap, taken over ground-truth boxes, between each ROI and\n    each ground-truth box. The class with maximum overlap is also\n    recorded.\n    """"""\n    roidb = imdb.roidb\n    for i in xrange(len(imdb.image_index)):\n        roidb[i][\'image\'] = imdb.image_path_at(i)\n        # need gt_overlaps as a dense array for argmax\n        gt_overlaps = roidb[i][\'gt_overlaps\'].toarray()\n        # max overlap with gt over classes (columns)\n        max_overlaps = gt_overlaps.max(axis=1)\n        # gt class that had the max overlap\n        max_classes = gt_overlaps.argmax(axis=1)\n\n        roidb[i][\'max_classes\'] = max_classes\n        roidb[i][\'max_overlaps\'] = max_overlaps\n\n        # sanity checks\n        # max overlap of 0 => class should be zero (background)\n        zero_inds = np.where(max_overlaps == 0)[0]\n        assert all(max_classes[zero_inds] == 0)\n        # max overlap > 0 => class should not be zero (must be a fg class)\n        nonzero_inds = np.where(max_overlaps > 0)[0]\n        assert all(max_classes[nonzero_inds] != 0)\n\ndef add_bbox_regression_targets(roidb):\n    """"""Add information needed to train bounding-box regressors.""""""\n    assert len(roidb) > 0\n    assert \'max_classes\' in roidb[0], \'Did you call prepare_roidb first?\'\n\n    num_images = len(roidb)\n    # Infer number of classes from the number of columns in gt_overlaps\n    num_classes = roidb[0][\'gt_overlaps\'].shape[1]\n    for im_i in xrange(num_images):\n        rois = roidb[im_i][\'boxes\']\n        max_overlaps = roidb[im_i][\'max_overlaps\']\n        max_classes = roidb[im_i][\'max_classes\']\n        roidb[im_i][\'bbox_targets\'] = \\\n                _compute_targets(rois, max_overlaps, max_classes, num_classes)\n\n    # Compute values needed for means and stds\n    # var(x) = E(x^2) - E(x)^2\n    class_counts = np.zeros((num_classes, 1)) + cfg.EPS\n    sums = np.zeros((num_classes, 4))\n    squared_sums = np.zeros((num_classes, 4))\n    for im_i in xrange(num_images):\n        targets = roidb[im_i][\'bbox_targets\']\n        for cls in xrange(1, num_classes):\n            cls_inds = np.where(targets[:, 0] == cls)[0]\n            if cls_inds.size > 0:\n                class_counts[cls] += cls_inds.size\n                sums[cls, :] += targets[cls_inds, 1:].sum(axis=0)\n                squared_sums[cls, :] += (targets[cls_inds, 1:] ** 2).sum(axis=0)\n\n    means = sums / class_counts\n    stds = np.sqrt(squared_sums / class_counts - means ** 2)\n\n    # Normalize targets\n    for im_i in xrange(num_images):\n        targets = roidb[im_i][\'bbox_targets\']\n        for cls in xrange(1, num_classes):\n            cls_inds = np.where(targets[:, 0] == cls)[0]\n            roidb[im_i][\'bbox_targets\'][cls_inds, 1:] -= means[cls, :]\n            if stds[cls, 0] != 0:\n                roidb[im_i][\'bbox_targets\'][cls_inds, 1:] /= stds[cls, :]\n\n    # These values will be needed for making predictions\n    # (the predicts will need to be unnormalized and uncentered)\n    return means.ravel(), stds.ravel()\n\ndef _compute_targets(rois, overlaps, labels, num_classes):\n    """"""Compute bounding-box regression targets for an image.""""""\n    # Ensure ROIs are floats\n    rois = rois.astype(np.float, copy=False)\n\n    # Indices of ground-truth ROIs\n    gt_inds = np.where(overlaps == 1)[0]\n    # Indices of examples for which we try to make predictions\n    ex_inds = []\n    for i in xrange(1, num_classes):\n        ex_inds.extend( np.where((labels == i) & (overlaps >= cfg.TRAIN.BBOX_THRESH))[0] )\n\n    # Get IoU overlap between each ex ROI and gt ROI\n    ex_gt_overlaps = utils.cython_bbox.bbox_overlaps(rois[ex_inds, :],\n                                                     rois[gt_inds, :])\n\n    # Find which gt ROI each ex ROI has max overlap with:\n    # this will be the ex ROI\'s gt target\n    if ex_gt_overlaps.shape[0] != 0:\n        gt_assignment = ex_gt_overlaps.argmax(axis=1)\n    else:\n        gt_assignment = []\n    gt_rois = rois[gt_inds[gt_assignment], :]\n    ex_rois = rois[ex_inds, :]\n\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + cfg.EPS\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + cfg.EPS\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + cfg.EPS\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + cfg.EPS\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = np.log(gt_widths / ex_widths)\n    targets_dh = np.log(gt_heights / ex_heights)\n\n    targets = np.zeros((rois.shape[0], 5), dtype=np.float32)\n    targets[ex_inds, 0] = labels[ex_inds]\n    targets[ex_inds, 1] = targets_dx\n    targets[ex_inds, 2] = targets_dy\n    targets[ex_inds, 3] = targets_dw\n    targets[ex_inds, 4] = targets_dh\n    return targets\n'"
lib/roi_pooling_layer/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n'
lib/roi_pooling_layer/roi_pooling_op.py,1,"b""import tensorflow as tf\nimport os.path as osp\n\nfilename = osp.join(osp.dirname(__file__), 'roi_pooling.so')\n_roi_pooling_module = tf.load_op_library(filename)\nroi_pool = _roi_pooling_module.roi_pool\nroi_pool_grad = _roi_pooling_module.roi_pool_grad\n"""
lib/roi_pooling_layer/roi_pooling_op_grad.py,1,"b'import tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport roi_pooling_op\nimport pdb\n\n\n@ops.RegisterShape(""RoiPool"")\ndef _roi_pool_shape(op):\n  """"""Shape function for the RoiPool op.\n\n  """"""\n  dims_data = op.inputs[0].get_shape().as_list()\n  channels = dims_data[3]\n  dims_rois = op.inputs[1].get_shape().as_list()\n  num_rois = dims_rois[0]\n\n  pooled_height = op.get_attr(\'pooled_height\')\n  pooled_width = op.get_attr(\'pooled_width\')\n\n  output_shape = tf.TensorShape([num_rois, pooled_height, pooled_width, channels])\n  return [output_shape, output_shape]\n\n@ops.RegisterGradient(""RoiPool"")\ndef _roi_pool_grad(op, grad, _):\n  """"""The gradients for `roi_pool`.\n  Args:\n    op: The `roi_pool` `Operation` that we are differentiating, which we can use\n      to find the inputs and outputs of the original op.\n    grad: Gradient with respect to the output of the `roi_pool` op.\n  Returns:\n    Gradients with respect to the input of `zero_out`.\n  """"""\n  data = op.inputs[0]\n  rois = op.inputs[1]\n  argmax = op.outputs[1]\n  pooled_height = op.get_attr(\'pooled_height\')\n  pooled_width = op.get_attr(\'pooled_width\')\n  spatial_scale = op.get_attr(\'spatial_scale\')\n\n  # compute gradient\n  data_grad = roi_pooling_op.roi_pool_grad(data, rois, argmax, grad, pooled_height, pooled_width, spatial_scale)\n\n  return [data_grad, None]  # List of one Tensor, since we have one input\n'"
lib/roi_pooling_layer/roi_pooling_op_test.py,12,"b""import tensorflow as tf\nimport numpy as np\nimport roi_pooling_op\nimport roi_pooling_op_grad\nimport tensorflow as tf\nimport pdb\n\n\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\narray = np.random.rand(32, 100, 100, 3)\ndata = tf.convert_to_tensor(array, dtype=tf.float32)\nrois = tf.convert_to_tensor([[0, 10, 10, 20, 20], [31, 30, 30, 40, 40]], dtype=tf.float32)\n\nW = weight_variable([3, 3, 3, 1])\nh = conv2d(data, W)\n\n[y, argmax] = roi_pooling_op.roi_pool(h, rois, 6, 6, 1.0/3)\npdb.set_trace()\ny_data = tf.convert_to_tensor(np.ones((2, 6, 6, 1)), dtype=tf.float32)\nprint y_data, y, argmax\n\n# Minimize the mean squared errors.\nloss = tf.reduce_mean(tf.square(y - y_data))\noptimizer = tf.train.GradientDescentOptimizer(0.5)\ntrain = optimizer.minimize(loss)\n\ninit = tf.initialize_all_variables()\n\n# Launch the graph.\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\nsess.run(init)\npdb.set_trace()\nfor step in xrange(10):\n    sess.run(train)\n    print(step, sess.run(W))\n    print(sess.run(y))\n\n#with tf.device('/gpu:0'):\n#  result = module.roi_pool(data, rois, 1, 1, 1.0/1)\n#  print result.eval()\n#with tf.device('/cpu:0'):\n#  run(init)\n"""
lib/rpn_msr/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n'
lib/rpn_msr/anchor_target_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport os\nimport caffe\nimport yaml\nfrom fast_rcnn.config import cfg\nimport numpy as np\nimport numpy.random as npr\nfrom generate_anchors import generate_anchors\nfrom utils.cython_bbox import bbox_overlaps\nfrom fast_rcnn.bbox_transform import bbox_transform\n\nDEBUG = False \n\nclass AnchorTargetLayer(caffe.Layer):\n    """"""\n    Assign anchors to ground-truth targets. Produces anchor classification\n    labels and bounding-box regression targets.\n    """"""\n\n    def setup(self, bottom, top):\n        self._anchors = generate_anchors(cfg.TRAIN.RPN_BASE_SIZE, cfg.TRAIN.RPN_ASPECTS, cfg.TRAIN.RPN_SCALES)\n        self._num_anchors = self._anchors.shape[0]\n\n        if DEBUG:\n            print \'anchors:\'\n            print self._anchors\n            print \'anchor shapes:\'\n            print np.hstack((\n                self._anchors[:, 2::4] - self._anchors[:, 0::4],\n                self._anchors[:, 3::4] - self._anchors[:, 1::4],\n            ))\n            self._counts = cfg.EPS\n            self._sums = np.zeros((1, 4))\n            self._squared_sums = np.zeros((1, 4))\n            self._fg_sum = 0\n            self._bg_sum = 0\n            self._count = 0\n\n        layer_params = yaml.load(self.param_str_)\n        self._feat_stride = layer_params[\'feat_stride\']\n\n        # allow boxes to sit over the edge by a small amount\n        self._allowed_border = layer_params.get(\'allowed_border\', 0)\n\n        height, width = bottom[0].data.shape[-2:]\n        if DEBUG:\n            print \'AnchorTargetLayer: height\', height, \'width\', width\n\n        A = self._num_anchors\n        # labels\n        top[0].reshape(1, 1, A * height, width)\n        # bbox_targets\n        top[1].reshape(1, A * 4, height, width)\n        # bbox_inside_weights\n        top[2].reshape(1, A * 4, height, width)\n        # bbox_outside_weights\n        top[3].reshape(1, A * 4, height, width)\n\n    def forward(self, bottom, top):\n        # Algorithm:\n        #\n        # for each (H, W) location i\n        #   generate 9 anchor boxes centered on cell i\n        #   apply predicted bbox deltas at cell i to each of the 9 anchors\n        # filter out-of-image anchors\n        # measure GT overlap\n\n        assert bottom[0].data.shape[0] == 1, \\\n            \'Only single item batches are supported\'\n\n        # map of shape (..., H, W)\n        height, width = bottom[0].data.shape[-2:]\n        # GT boxes (x1, y1, x2, y2, label)\n        gt_boxes = bottom[1].data\n        # im_info\n        im_info = bottom[2].data[0, :]\n\n        if DEBUG:\n            print \'\'\n            print \'im_size: ({}, {})\'.format(im_info[0], im_info[1])\n            print \'scale: {}\'.format(im_info[2])\n            print \'height, width: ({}, {})\'.format(height, width)\n            print \'rpn: gt_boxes.shape\', gt_boxes.shape\n            print \'rpn: gt_boxes\', gt_boxes\n\n        # 1. Generate proposals from bbox deltas and shifted anchors\n        shift_x = np.arange(0, width) * self._feat_stride\n        shift_y = np.arange(0, height) * self._feat_stride\n        shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n        shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n        # add A anchors (1, A, 4) to\n        # cell K shifts (K, 1, 4) to get\n        # shift anchors (K, A, 4)\n        # reshape to (K*A, 4) shifted anchors\n        A = self._num_anchors\n        K = shifts.shape[0]\n        all_anchors = (self._anchors.reshape((1, A, 4)) +\n                       shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n        all_anchors = all_anchors.reshape((K * A, 4))\n        total_anchors = int(K * A)\n\n        # only keep anchors inside the image\n        inds_inside = np.where(\n            (all_anchors[:, 0] >= -self._allowed_border) &\n            (all_anchors[:, 1] >= -self._allowed_border) &\n            (all_anchors[:, 2] < im_info[1] + self._allowed_border) &  # width\n            (all_anchors[:, 3] < im_info[0] + self._allowed_border)    # height\n        )[0]\n\n        if DEBUG:\n            print \'total_anchors\', total_anchors\n            print \'inds_inside\', len(inds_inside)\n\n        # keep only inside anchors\n        anchors = all_anchors[inds_inside, :]\n        if DEBUG:\n            print \'anchors.shape\', anchors.shape\n\n        # label: 1 is positive, 0 is negative, -1 is dont care\n        labels = np.empty((len(inds_inside), ), dtype=np.float32)\n        labels.fill(-1)\n\n        # overlaps between the anchors and the gt boxes\n        # overlaps (ex, gt)\n        if gt_boxes.shape[0] != 0:\n            overlaps = bbox_overlaps(\n                np.ascontiguousarray(anchors, dtype=np.float),\n                np.ascontiguousarray(gt_boxes, dtype=np.float))\n            argmax_overlaps = overlaps.argmax(axis=1)\n            max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n            gt_argmax_overlaps = overlaps.argmax(axis=0)\n            gt_max_overlaps = overlaps[gt_argmax_overlaps, np.arange(overlaps.shape[1])]\n            gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n\n            if not cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n                # assign bg labels first so that positive labels can clobber them\n                labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n\n            # fg label: for each gt, anchor with highest overlap\n            labels[gt_argmax_overlaps] = 1\n\n            # fg label: above threshold IOU\n            labels[max_overlaps >= cfg.TRAIN.RPN_POSITIVE_OVERLAP] = 1\n\n            if cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n                # assign bg labels last so that negative labels can clobber positives\n                labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n        else:\n            labels.fill(0)\n\n        # subsample positive labels if we have too many\n        num_fg = int(cfg.TRAIN.RPN_FG_FRACTION * cfg.TRAIN.RPN_BATCHSIZE)\n        fg_inds = np.where(labels == 1)[0]\n        if len(fg_inds) > num_fg:\n            disable_inds = npr.choice(\n                fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n            labels[disable_inds] = -1\n\n        # subsample negative labels if we have too many\n        num_bg = cfg.TRAIN.RPN_BATCHSIZE - np.sum(labels == 1)\n        bg_inds = np.where(labels == 0)[0]\n        if len(bg_inds) > num_bg:\n            disable_inds = npr.choice(\n                bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n            labels[disable_inds] = -1\n            #print ""was %s inds, disabling %s, now %s inds"" % (\n                #len(bg_inds), len(disable_inds), np.sum(labels == 0))\n\n        bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n        if gt_boxes.shape[0] != 0:\n            bbox_targets = _compute_targets(anchors, gt_boxes[argmax_overlaps, :])\n\n        bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n        bbox_inside_weights[labels == 1, :] = np.array(cfg.TRAIN.RPN_BBOX_INSIDE_WEIGHTS)\n\n        bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n        if cfg.TRAIN.RPN_POSITIVE_WEIGHT < 0:\n            # uniform weighting of examples (given non-uniform sampling)\n            num_examples = np.sum(labels >= 0)\n            positive_weights = np.ones((1, 4)) * 1.0 / num_examples\n            negative_weights = np.ones((1, 4)) * 1.0 / num_examples\n        else:\n            assert ((cfg.TRAIN.RPN_POSITIVE_WEIGHT > 0) &\n                    (cfg.TRAIN.RPN_POSITIVE_WEIGHT < 1))\n            positive_weights = (cfg.TRAIN.RPN_POSITIVE_WEIGHT /\n                                np.sum(labels == 1))\n            negative_weights = ((1.0 - cfg.TRAIN.RPN_POSITIVE_WEIGHT) /\n                                np.sum(labels == 0))\n        bbox_outside_weights[labels == 1, :] = positive_weights\n        bbox_outside_weights[labels == 0, :] = negative_weights\n\n        if DEBUG:\n            self._sums += bbox_targets[labels == 1, :].sum(axis=0)\n            self._squared_sums += (bbox_targets[labels == 1, :] ** 2).sum(axis=0)\n            self._counts += np.sum(labels == 1)\n            means = self._sums / self._counts\n            stds = np.sqrt(self._squared_sums / self._counts - means ** 2)\n            print \'means:\'\n            print means\n            print \'stdevs:\'\n            print stds\n\n        # map up to original set of anchors\n        labels = _unmap(labels, total_anchors, inds_inside, fill=-1)\n        bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n        bbox_inside_weights = _unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)\n        bbox_outside_weights = _unmap(bbox_outside_weights, total_anchors, inds_inside, fill=0)\n\n        if DEBUG:\n            if gt_boxes.shape[0] != 0:\n                print \'rpn: max max_overlap\', np.max(max_overlaps)\n            else:\n                print \'rpn: max max_overlap\', 0\n            print \'rpn: num_positive\', np.sum(labels == 1)\n            print \'rpn: num_negative\', np.sum(labels == 0)\n            self._fg_sum += np.sum(labels == 1)\n            self._bg_sum += np.sum(labels == 0)\n            self._count += 1\n            print \'rpn: num_positive avg\', self._fg_sum / self._count\n            print \'rpn: num_negative avg\', self._bg_sum / self._count\n\n        # labels\n        labels = labels.reshape((1, height, width, A)).transpose(0, 3, 1, 2)\n        labels = labels.reshape((1, 1, A * height, width))\n        top[0].reshape(*labels.shape)\n        top[0].data[...] = labels\n\n        # bbox_targets\n        bbox_targets = bbox_targets \\\n            .reshape((1, height, width, A * 4)).transpose(0, 3, 1, 2)\n        top[1].reshape(*bbox_targets.shape)\n        top[1].data[...] = bbox_targets\n\n        # bbox_inside_weights\n        bbox_inside_weights = bbox_inside_weights \\\n            .reshape((1, height, width, A * 4)).transpose(0, 3, 1, 2)\n        assert bbox_inside_weights.shape[2] == height\n        assert bbox_inside_weights.shape[3] == width\n        top[2].reshape(*bbox_inside_weights.shape)\n        top[2].data[...] = bbox_inside_weights\n\n        # bbox_outside_weights\n        bbox_outside_weights = bbox_outside_weights \\\n            .reshape((1, height, width, A * 4)).transpose(0, 3, 1, 2)\n        assert bbox_outside_weights.shape[2] == height\n        assert bbox_outside_weights.shape[3] == width\n        top[3].reshape(*bbox_outside_weights.shape)\n        top[3].data[...] = bbox_outside_weights\n\n    def backward(self, top, propagate_down, bottom):\n        """"""This layer does not propagate gradients.""""""\n        pass\n\n    def reshape(self, bottom, top):\n        """"""Reshaping happens during the call to forward.""""""\n        pass\n\n\ndef _unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if len(data.shape) == 1:\n        ret = np.empty((count, ), dtype=np.float32)\n        ret.fill(fill)\n        ret[inds] = data\n    else:\n        ret = np.empty((count, ) + data.shape[1:], dtype=np.float32)\n        ret.fill(fill)\n        ret[inds, :] = data\n    return ret\n\n\ndef _compute_targets(ex_rois, gt_rois):\n    """"""Compute bounding-box regression targets for an image.""""""\n\n    assert ex_rois.shape[0] == gt_rois.shape[0]\n    assert ex_rois.shape[1] == 4\n    assert gt_rois.shape[1] == 5\n\n    return bbox_transform(ex_rois, gt_rois[:, :4]).astype(np.float32, copy=False)\n'"
lib/rpn_msr/anchor_target_layer_tf.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport os\nimport yaml\nfrom fast_rcnn.config import cfg\nimport numpy as np\nimport numpy.random as npr\nfrom generate_anchors import generate_anchors\nfrom utils.cython_bbox import bbox_overlaps\nfrom fast_rcnn.bbox_transform import bbox_transform\nimport pdb\n\nDEBUG = False\n\ndef anchor_target_layer(rpn_cls_score, gt_boxes, im_info, data, _feat_stride = [16,], anchor_scales = [4 ,8, 16, 32]):\n    """"""\n    Assign anchors to ground-truth targets. Produces anchor classification\n    labels and bounding-box regression targets.\n    """"""\n    _anchors = generate_anchors(scales=np.array(anchor_scales))\n    _num_anchors = _anchors.shape[0]\n\n    if DEBUG:\n        print \'anchors:\'\n        print _anchors\n        print \'anchor shapes:\'\n        print np.hstack((\n            _anchors[:, 2::4] - _anchors[:, 0::4],\n            _anchors[:, 3::4] - _anchors[:, 1::4],\n        ))\n        _counts = cfg.EPS\n        _sums = np.zeros((1, 4))\n        _squared_sums = np.zeros((1, 4))\n        _fg_sum = 0\n        _bg_sum = 0\n        _count = 0\n\n    # allow boxes to sit over the edge by a small amount\n    _allowed_border =  0\n    # map of shape (..., H, W)\n    #height, width = rpn_cls_score.shape[1:3]\n\n    im_info = im_info[0]\n\n    # Algorithm:\n    #\n    # for each (H, W) location i\n    #   generate 9 anchor boxes centered on cell i\n    #   apply predicted bbox deltas at cell i to each of the 9 anchors\n    # filter out-of-image anchors\n    # measure GT overlap\n\n    assert rpn_cls_score.shape[0] == 1, \\\n        \'Only single item batches are supported\'\n\n    # map of shape (..., H, W)\n    height, width = rpn_cls_score.shape[1:3]\n\n    if DEBUG:\n        print \'AnchorTargetLayer: height\', height, \'width\', width\n        print \'\'\n        print \'im_size: ({}, {})\'.format(im_info[0], im_info[1])\n        print \'scale: {}\'.format(im_info[2])\n        print \'height, width: ({}, {})\'.format(height, width)\n        print \'rpn: gt_boxes.shape\', gt_boxes.shape\n        print \'rpn: gt_boxes\', gt_boxes\n\n    # 1. Generate proposals from bbox deltas and shifted anchors\n    shift_x = np.arange(0, width) * _feat_stride\n    shift_y = np.arange(0, height) * _feat_stride\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                        shift_x.ravel(), shift_y.ravel())).transpose()\n    # add A anchors (1, A, 4) to\n    # cell K shifts (K, 1, 4) to get\n    # shift anchors (K, A, 4)\n    # reshape to (K*A, 4) shifted anchors\n    A = _num_anchors\n    K = shifts.shape[0]\n    all_anchors = (_anchors.reshape((1, A, 4)) +\n                   shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n    all_anchors = all_anchors.reshape((K * A, 4))\n    total_anchors = int(K * A)\n\n    # only keep anchors inside the image\n    inds_inside = np.where(\n        (all_anchors[:, 0] >= -_allowed_border) &\n        (all_anchors[:, 1] >= -_allowed_border) &\n        (all_anchors[:, 2] < im_info[1] + _allowed_border) &  # width\n        (all_anchors[:, 3] < im_info[0] + _allowed_border)    # height\n    )[0]\n\n    if DEBUG:\n        print \'total_anchors\', total_anchors\n        print \'inds_inside\', len(inds_inside)\n\n    # keep only inside anchors\n    anchors = all_anchors[inds_inside, :]\n    if DEBUG:\n        print \'anchors.shape\', anchors.shape\n\n    # label: 1 is positive, 0 is negative, -1 is dont care\n    labels = np.empty((len(inds_inside), ), dtype=np.float32)\n    labels.fill(-1)\n\n    # overlaps between the anchors and the gt boxes\n    # overlaps (ex, gt)\n    overlaps = bbox_overlaps(\n        np.ascontiguousarray(anchors, dtype=np.float),\n        np.ascontiguousarray(gt_boxes, dtype=np.float))\n    argmax_overlaps = overlaps.argmax(axis=1)\n    max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n    gt_argmax_overlaps = overlaps.argmax(axis=0)\n    gt_max_overlaps = overlaps[gt_argmax_overlaps,\n                               np.arange(overlaps.shape[1])]\n    gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n\n    if not cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n        # assign bg labels first so that positive labels can clobber them\n        labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n\n    # fg label: for each gt, anchor with highest overlap\n    labels[gt_argmax_overlaps] = 1\n\n    # fg label: above threshold IOU\n    labels[max_overlaps >= cfg.TRAIN.RPN_POSITIVE_OVERLAP] = 1\n\n    if cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n        # assign bg labels last so that negative labels can clobber positives\n        labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n\n    # subsample positive labels if we have too many\n    num_fg = int(cfg.TRAIN.RPN_FG_FRACTION * cfg.TRAIN.RPN_BATCHSIZE)\n    fg_inds = np.where(labels == 1)[0]\n    if len(fg_inds) > num_fg:\n        disable_inds = npr.choice(\n            fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n        labels[disable_inds] = -1\n\n    # subsample negative labels if we have too many\n    num_bg = cfg.TRAIN.RPN_BATCHSIZE - np.sum(labels == 1)\n    bg_inds = np.where(labels == 0)[0]\n    if len(bg_inds) > num_bg:\n        disable_inds = npr.choice(\n            bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n        labels[disable_inds] = -1\n        #print ""was %s inds, disabling %s, now %s inds"" % (\n            #len(bg_inds), len(disable_inds), np.sum(labels == 0))\n\n    bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n    bbox_targets = _compute_targets(anchors, gt_boxes[argmax_overlaps, :])\n\n    bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n    bbox_inside_weights[labels == 1, :] = np.array(cfg.TRAIN.RPN_BBOX_INSIDE_WEIGHTS)\n\n    bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n    if cfg.TRAIN.RPN_POSITIVE_WEIGHT < 0:\n        # uniform weighting of examples (given non-uniform sampling)\n        num_examples = np.sum(labels >= 0)\n        positive_weights = np.ones((1, 4)) * 1.0 / num_examples\n        negative_weights = np.ones((1, 4)) * 1.0 / num_examples\n    else:\n        assert ((cfg.TRAIN.RPN_POSITIVE_WEIGHT > 0) &\n                (cfg.TRAIN.RPN_POSITIVE_WEIGHT < 1))\n        positive_weights = (cfg.TRAIN.RPN_POSITIVE_WEIGHT /\n                            np.sum(labels == 1))\n        negative_weights = ((1.0 - cfg.TRAIN.RPN_POSITIVE_WEIGHT) /\n                            np.sum(labels == 0))\n    bbox_outside_weights[labels == 1, :] = positive_weights\n    bbox_outside_weights[labels == 0, :] = negative_weights\n\n    if DEBUG:\n        _sums += bbox_targets[labels == 1, :].sum(axis=0)\n        _squared_sums += (bbox_targets[labels == 1, :] ** 2).sum(axis=0)\n        _counts += np.sum(labels == 1)\n        means = _sums / _counts\n        stds = np.sqrt(_squared_sums / _counts - means ** 2)\n        print \'means:\'\n        print means\n        print \'stdevs:\'\n        print stds\n\n    # map up to original set of anchors\n    labels = _unmap(labels, total_anchors, inds_inside, fill=-1)\n    bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n    bbox_inside_weights = _unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)\n    bbox_outside_weights = _unmap(bbox_outside_weights, total_anchors, inds_inside, fill=0)\n\n    if DEBUG:\n        print \'rpn: max max_overlap\', np.max(max_overlaps)\n        print \'rpn: num_positive\', np.sum(labels == 1)\n        print \'rpn: num_negative\', np.sum(labels == 0)\n        _fg_sum += np.sum(labels == 1)\n        _bg_sum += np.sum(labels == 0)\n        _count += 1\n        print \'rpn: num_positive avg\', _fg_sum / _count\n        print \'rpn: num_negative avg\', _bg_sum / _count\n\n    # labels\n    #pdb.set_trace()\n    labels = labels.reshape((1, height, width, A)).transpose(0, 3, 1, 2)\n    labels = labels.reshape((1, 1, A * height, width))\n    rpn_labels = labels\n\n    # bbox_targets\n    bbox_targets = bbox_targets \\\n        .reshape((1, height, width, A * 4)).transpose(0, 3, 1, 2)\n\n    rpn_bbox_targets = bbox_targets\n    # bbox_inside_weights\n    bbox_inside_weights = bbox_inside_weights \\\n        .reshape((1, height, width, A * 4)).transpose(0, 3, 1, 2)\n    #assert bbox_inside_weights.shape[2] == height\n    #assert bbox_inside_weights.shape[3] == width\n\n    rpn_bbox_inside_weights = bbox_inside_weights\n\n    # bbox_outside_weights\n    bbox_outside_weights = bbox_outside_weights \\\n        .reshape((1, height, width, A * 4)).transpose(0, 3, 1, 2)\n    #assert bbox_outside_weights.shape[2] == height\n    #assert bbox_outside_weights.shape[3] == width\n\n    rpn_bbox_outside_weights = bbox_outside_weights\n\n    return rpn_labels,rpn_bbox_targets,rpn_bbox_inside_weights,rpn_bbox_outside_weights\n\n\n\ndef _unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if len(data.shape) == 1:\n        ret = np.empty((count, ), dtype=np.float32)\n        ret.fill(fill)\n        ret[inds] = data\n    else:\n        ret = np.empty((count, ) + data.shape[1:], dtype=np.float32)\n        ret.fill(fill)\n        ret[inds, :] = data\n    return ret\n\n\ndef _compute_targets(ex_rois, gt_rois):\n    """"""Compute bounding-box regression targets for an image.""""""\n\n    assert ex_rois.shape[0] == gt_rois.shape[0]\n    assert ex_rois.shape[1] == 4\n    assert gt_rois.shape[1] == 5\n\n    return bbox_transform(ex_rois, gt_rois[:, :4]).astype(np.float32, copy=False)\n'"
lib/rpn_msr/generate.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom fast_rcnn.config import cfg\nfrom utils.blob import im_list_to_blob\nfrom utils.timer import Timer\nimport numpy as np\nimport cv2\n\ndef _vis_proposals(im, dets, thresh=0.5):\n    """"""Draw detected bounding boxes.""""""\n    inds = np.where(dets[:, -1] >= thresh)[0]\n    if len(inds) == 0:\n        return\n\n    class_name = \'obj\'\n    im = im[:, :, (2, 1, 0)]\n    fig, ax = plt.subplots(figsize=(12, 12))\n    ax.imshow(im, aspect=\'equal\')\n    for i in inds:\n        bbox = dets[i, :4]\n        score = dets[i, -1]\n\n        ax.add_patch(\n            plt.Rectangle((bbox[0], bbox[1]),\n                          bbox[2] - bbox[0],\n                          bbox[3] - bbox[1], fill=False,\n                          edgecolor=\'red\', linewidth=3.5)\n            )\n        ax.text(bbox[0], bbox[1] - 2,\n                \'{:s} {:.3f}\'.format(class_name, score),\n                bbox=dict(facecolor=\'blue\', alpha=0.5),\n                fontsize=14, color=\'white\')\n\n    ax.set_title((\'{} detections with \'\n                  \'p({} | box) >= {:.1f}\').format(class_name, class_name,\n                                                  thresh),\n                  fontsize=14)\n    plt.axis(\'off\')\n    plt.tight_layout()\n    plt.draw()\n\ndef _get_image_blob(im):\n    """"""Converts an image into a network input.\n\n    Arguments:\n        im (ndarray): a color image in BGR order\n\n    Returns:\n        blob (ndarray): a data blob holding an image pyramid\n        im_scale_factors (list): list of image scales (relative to im) used\n            in the image pyramid\n    """"""\n    im_orig = im.astype(np.float32, copy=True)\n    im_orig -= cfg.PIXEL_MEANS\n\n    processed_ims = []\n\n    assert len(cfg.TEST.SCALES_BASE) == 1\n    im_scale = cfg.TRAIN.SCALES_BASE[0]\n\n    im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,\n                    interpolation=cv2.INTER_LINEAR)\n    im_info = np.hstack((im.shape[:2], im_scale))[np.newaxis, :]\n    processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, im_info\n\ndef im_proposals(net, im):\n    """"""Generate RPN proposals on a single image.""""""\n    blobs = {}\n    blobs[\'data\'], blobs[\'im_info\'] = _get_image_blob(im)\n    net.blobs[\'data\'].reshape(*(blobs[\'data\'].shape))\n    net.blobs[\'im_info\'].reshape(*(blobs[\'im_info\'].shape))\n    blobs_out = net.forward(\n            data=blobs[\'data\'].astype(np.float32, copy=False),\n            im_info=blobs[\'im_info\'].astype(np.float32, copy=False))\n\n    scale = blobs[\'im_info\'][0, 2]\n    boxes = blobs_out[\'rois\'][:, 1:].copy() / scale\n    scores = blobs_out[\'scores\'].copy()\n    return boxes, scores\n\ndef imdb_proposals(net, imdb):\n    """"""Generate RPN proposals on all images in an imdb.""""""\n\n    _t = Timer()\n    imdb_boxes = [[] for _ in xrange(imdb.num_images)]\n    for i in xrange(imdb.num_images):\n        im = cv2.imread(imdb.image_path_at(i))\n        _t.tic()\n        imdb_boxes[i], scores = im_proposals(net, im)\n        _t.toc()\n        print \'im_proposals: {:d}/{:d} {:.3f}s\' \\\n              .format(i + 1, imdb.num_images, _t.average_time)\n        if 0:\n            dets = np.hstack((imdb_boxes[i], scores))\n            # from IPython import embed; embed()\n            _vis_proposals(im, dets[:3, :], thresh=0.9)\n            plt.show()\n\n    return imdb_boxes\n\ndef imdb_proposals_det(net, imdb):\n    """"""Generate RPN proposals on all images in an imdb.""""""\n\n    _t = Timer()\n    imdb_boxes = [[] for _ in xrange(imdb.num_images)]\n    for i in xrange(imdb.num_images):\n        im = cv2.imread(imdb.image_path_at(i))\n        _t.tic()\n        boxes, scores = im_proposals(net, im)\n        _t.toc()\n        print \'im_proposals: {:d}/{:d} {:.3f}s\' \\\n              .format(i + 1, imdb.num_images, _t.average_time)\n        dets = np.hstack((boxes, scores))\n        imdb_boxes[i] = dets\n\n        if 0:            \n            # from IPython import embed; embed()\n            _vis_proposals(im, dets[:3, :], thresh=0.9)\n            plt.show()\n\n    return imdb_boxes\n'"
lib/rpn_msr/generate_anchors.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport numpy as np\n\n# Verify that we compute the same anchors as Shaoqing\'s matlab implementation:\n#\n#    >> load output/rpn_cachedir/faster_rcnn_VOC2007_ZF_stage1_rpn/anchors.mat\n#    >> anchors\n#\n#    anchors =\n#\n#       -83   -39   100    56\n#      -175   -87   192   104\n#      -359  -183   376   200\n#       -55   -55    72    72\n#      -119  -119   136   136\n#      -247  -247   264   264\n#       -35   -79    52    96\n#       -79  -167    96   184\n#      -167  -343   184   360\n\n#array([[ -83.,  -39.,  100.,   56.],\n#       [-175.,  -87.,  192.,  104.],\n#       [-359., -183.,  376.,  200.],\n#       [ -55.,  -55.,   72.,   72.],\n#       [-119., -119.,  136.,  136.],\n#       [-247., -247.,  264.,  264.],\n#       [ -35.,  -79.,   52.,   96.],\n#       [ -79., -167.,   96.,  184.],\n#       [-167., -343.,  184.,  360.]])\n\ndef generate_anchors(base_size=16, ratios=[0.5, 1, 2],\n                     scales=2**np.arange(3, 6)):\n    """"""\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales wrt a reference (0, 0, 15, 15) window.\n    """"""\n\n    base_anchor = np.array([1, 1, base_size, base_size]) - 1\n    ratio_anchors = _ratio_enum(base_anchor, ratios)\n    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)\n                         for i in xrange(ratio_anchors.shape[0])])\n    return anchors\n\ndef _whctrs(anchor):\n    """"""\n    Return width, height, x center, and y center for an anchor (window).\n    """"""\n\n    w = anchor[2] - anchor[0] + 1\n    h = anchor[3] - anchor[1] + 1\n    x_ctr = anchor[0] + 0.5 * (w - 1)\n    y_ctr = anchor[1] + 0.5 * (h - 1)\n    return w, h, x_ctr, y_ctr\n\ndef _mkanchors(ws, hs, x_ctr, y_ctr):\n    """"""\n    Given a vector of widths (ws) and heights (hs) around a center\n    (x_ctr, y_ctr), output a set of anchors (windows).\n    """"""\n\n    ws = ws[:, np.newaxis]\n    hs = hs[:, np.newaxis]\n    anchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n                         y_ctr - 0.5 * (hs - 1),\n                         x_ctr + 0.5 * (ws - 1),\n                         y_ctr + 0.5 * (hs - 1)))\n    return anchors\n\ndef _ratio_enum(anchor, ratios):\n    """"""\n    Enumerate a set of anchors for each aspect ratio wrt an anchor.\n    """"""\n\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    size = w * h\n    size_ratios = size / ratios\n    ws = np.round(np.sqrt(size_ratios))\n    hs = np.round(ws * ratios)\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\ndef _scale_enum(anchor, scales):\n    """"""\n    Enumerate a set of anchors for each scale wrt an anchor.\n    """"""\n\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    ws = w * scales\n    hs = h * scales\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\nif __name__ == \'__main__\':\n    import time\n    t = time.time()\n    a = generate_anchors()\n    print time.time() - t\n    print a\n    from IPython import embed; embed()\n'"
lib/rpn_msr/proposal_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport caffe\nimport numpy as np\nimport yaml\nfrom fast_rcnn.config import cfg\nfrom generate_anchors import generate_anchors\nfrom fast_rcnn.bbox_transform import bbox_transform_inv, clip_boxes\nfrom fast_rcnn.nms_wrapper import nms\n\nDEBUG = False\n\nclass ProposalLayer(caffe.Layer):\n    """"""\n    Outputs object detection proposals by applying estimated bounding-box\n    transformations to a set of regular boxes (called ""anchors"").\n    """"""\n\n    def setup(self, bottom, top):\n        # parse the layer parameter string, which must be valid YAML\n        layer_params = yaml.load(self.param_str_)\n\n        self._feat_stride = layer_params[\'feat_stride\']\n        self._anchors     = generate_anchors(cfg.TRAIN.RPN_BASE_SIZE, cfg.TRAIN.RPN_ASPECTS, cfg.TRAIN.RPN_SCALES)\n        self._num_anchors = self._anchors.shape[0]\n\n        if DEBUG:\n            print \'feat_stride: {}\'.format(self._feat_stride)\n            print \'anchors:\'\n            print self._anchors\n\n        # rois blob: holds R regions of interest, each is a 5-tuple\n        # (n, x1, y1, x2, y2) specifying an image batch index n and a\n        # rectangle (x1, y1, x2, y2)\n        top[0].reshape(1, 5)\n\n        # scores blob: holds scores for R regions of interest\n        if len(top) > 1:\n            top[1].reshape(1, 1, 1, 1)\n\n    def forward(self, bottom, top):\n        # Algorithm:\n        #\n        # for each (H, W) location i\n        #   generate A anchor boxes centered on cell i\n        #   apply predicted bbox deltas at cell i to each of the A anchors\n        # clip predicted boxes to image\n        # remove predicted boxes with either height or width < threshold\n        # sort all (proposal, score) pairs by score from highest to lowest\n        # take top pre_nms_topN proposals before NMS\n        # apply NMS with threshold 0.7 to remaining proposals\n        # take after_nms_topN proposals after NMS\n        # return the top proposals (-> RoIs top, scores top)\n\n        assert bottom[0].data.shape[0] == 1, \\\n            \'Only single item batches are supported\'\n        # cfg_key = str(self.phase) # either \'TRAIN\' or \'TEST\'\n        cfg_key = \'TEST\'\n        pre_nms_topN  = cfg[cfg_key].RPN_PRE_NMS_TOP_N\n        post_nms_topN = cfg[cfg_key].RPN_POST_NMS_TOP_N\n        nms_thresh    = cfg[cfg_key].RPN_NMS_THRESH\n        min_size      = cfg[cfg_key].RPN_MIN_SIZE\n\n        # the first set of _num_anchors channels are bg probs\n        # the second set are the fg probs, which we want\n        scores = bottom[0].data[:, self._num_anchors:, :, :]\n        bbox_deltas = bottom[1].data\n        im_info = bottom[2].data[0, :]\n\n        if DEBUG:\n            print \'im_size: ({}, {})\'.format(im_info[0], im_info[1])\n            print \'scale: {}\'.format(im_info[2])\n\n        # 1. Generate proposals from bbox deltas and shifted anchors\n        height, width = scores.shape[-2:]\n\n        if DEBUG:\n            print \'score map size: {}\'.format(scores.shape)\n\n        # Enumerate all shifts\n        shift_x = np.arange(0, width) * self._feat_stride\n        shift_y = np.arange(0, height) * self._feat_stride\n        shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n        shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n\n        # Enumerate all shifted anchors:\n        #\n        # add A anchors (1, A, 4) to\n        # cell K shifts (K, 1, 4) to get\n        # shift anchors (K, A, 4)\n        # reshape to (K*A, 4) shifted anchors\n        A = self._num_anchors\n        K = shifts.shape[0]\n        anchors = self._anchors.reshape((1, A, 4)) + \\\n                  shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n        anchors = anchors.reshape((K * A, 4))\n\n        # Transpose and reshape predicted bbox transformations to get them\n        # into the same order as the anchors:\n        #\n        # bbox deltas will be (1, 4 * A, H, W) format\n        # transpose to (1, H, W, 4 * A)\n        # reshape to (1 * H * W * A, 4) where rows are ordered by (h, w, a)\n        # in slowest to fastest order\n        bbox_deltas = bbox_deltas.transpose((0, 2, 3, 1)).reshape((-1, 4))\n\n        # Same story for the scores:\n        #\n        # scores are (1, A, H, W) format\n        # transpose to (1, H, W, A)\n        # reshape to (1 * H * W * A, 1) where rows are ordered by (h, w, a)\n        scores = scores.transpose((0, 2, 3, 1)).reshape((-1, 1))\n\n        # Convert anchors into proposals via bbox transformations\n        proposals = bbox_transform_inv(anchors, bbox_deltas)\n\n        # 2. clip predicted boxes to image\n        proposals = clip_boxes(proposals, im_info[:2])\n\n        # 3. remove predicted boxes with either height or width < threshold\n        # (NOTE: convert min_size to input image scale stored in im_info[2])\n        keep = _filter_boxes(proposals, min_size * im_info[2])\n        proposals = proposals[keep, :]\n        scores = scores[keep]\n\n        # 4. sort all (proposal, score) pairs by score from highest to lowest\n        # 5. take top pre_nms_topN (e.g. 6000)\n        order = scores.ravel().argsort()[::-1]\n        if pre_nms_topN > 0:\n            order = order[:pre_nms_topN]\n        proposals = proposals[order, :]\n        scores = scores[order]\n\n        # 6. apply nms (e.g. threshold = 0.7)\n        # 7. take after_nms_topN (e.g. 300)\n        # 8. return the top proposals (-> RoIs top)\n        keep = nms(np.hstack((proposals, scores)), nms_thresh)\n        if post_nms_topN > 0:\n            keep = keep[:post_nms_topN]\n        proposals = proposals[keep, :]\n        scores = scores[keep]\n        print scores.shape\n\n        # Output rois blob\n        # Our RPN implementation only supports a single input image, so all\n        # batch inds are 0\n        batch_inds = np.zeros((proposals.shape[0], 1), dtype=np.float32)\n        blob = np.hstack((batch_inds, proposals.astype(np.float32, copy=False)))\n        top[0].reshape(*(blob.shape))\n        top[0].data[...] = blob\n\n        # [Optional] output scores blob\n        if len(top) > 1:\n            top[1].reshape(*(scores.shape))\n            top[1].data[...] = scores\n\n    def backward(self, top, propagate_down, bottom):\n        """"""This layer does not propagate gradients.""""""\n        pass\n\n    def reshape(self, bottom, top):\n        """"""Reshaping happens during the call to forward.""""""\n        pass\n\ndef _filter_boxes(boxes, min_size):\n    """"""Remove all boxes with any side smaller than min_size.""""""\n    ws = boxes[:, 2] - boxes[:, 0] + 1\n    hs = boxes[:, 3] - boxes[:, 1] + 1\n    keep = np.where((ws >= min_size) & (hs >= min_size))[0]\n    return keep\n'"
lib/rpn_msr/proposal_layer_tf.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport numpy as np\nimport yaml\nfrom fast_rcnn.config import cfg\nfrom generate_anchors import generate_anchors\nfrom fast_rcnn.bbox_transform import bbox_transform_inv, clip_boxes\nfrom fast_rcnn.nms_wrapper import nms\nimport pdb\n\n\nDEBUG = False\n""""""\nOutputs object detection proposals by applying estimated bounding-box\ntransformations to a set of regular boxes (called ""anchors"").\n""""""\ndef proposal_layer(rpn_cls_prob_reshape,rpn_bbox_pred,im_info,cfg_key,_feat_stride = [16,],anchor_scales = [8, 16, 32]):\n    # Algorithm:\n    #\n    # for each (H, W) location i\n    #   generate A anchor boxes centered on cell i\n    #   apply predicted bbox deltas at cell i to each of the A anchors\n    # clip predicted boxes to image\n    # remove predicted boxes with either height or width < threshold\n    # sort all (proposal, score) pairs by score from highest to lowest\n    # take top pre_nms_topN proposals before NMS\n    # apply NMS with threshold 0.7 to remaining proposals\n    # take after_nms_topN proposals after NMS\n    # return the top proposals (-> RoIs top, scores top)\n    #layer_params = yaml.load(self.param_str_)\n    _anchors = generate_anchors(scales=np.array(anchor_scales))\n    _num_anchors = _anchors.shape[0]\n    rpn_cls_prob_reshape = np.transpose(rpn_cls_prob_reshape,[0,3,1,2])\n    rpn_bbox_pred = np.transpose(rpn_bbox_pred,[0,3,1,2])\n    #rpn_cls_prob_reshape = np.transpose(np.reshape(rpn_cls_prob_reshape,[1,rpn_cls_prob_reshape.shape[0],rpn_cls_prob_reshape.shape[1],rpn_cls_prob_reshape.shape[2]]),[0,3,2,1])\n    #rpn_bbox_pred = np.transpose(rpn_bbox_pred,[0,3,2,1])\n    im_info = im_info[0]\n\n    assert rpn_cls_prob_reshape.shape[0] == 1, \\\n        \'Only single item batches are supported\'\n    # cfg_key = str(self.phase) # either \'TRAIN\' or \'TEST\'\n    #cfg_key = \'TEST\'\n    pre_nms_topN  = cfg[cfg_key].RPN_PRE_NMS_TOP_N\n    post_nms_topN = cfg[cfg_key].RPN_POST_NMS_TOP_N\n    nms_thresh    = cfg[cfg_key].RPN_NMS_THRESH\n    min_size      = cfg[cfg_key].RPN_MIN_SIZE\n\n    # the first set of _num_anchors channels are bg probs\n    # the second set are the fg probs, which we want\n    scores = rpn_cls_prob_reshape[:, _num_anchors:, :, :]\n    bbox_deltas = rpn_bbox_pred\n    #im_info = bottom[2].data[0, :]\n\n    if DEBUG:\n        print \'im_size: ({}, {})\'.format(im_info[0], im_info[1])\n        print \'scale: {}\'.format(im_info[2])\n\n    # 1. Generate proposals from bbox deltas and shifted anchors\n    height, width = scores.shape[-2:]\n\n    if DEBUG:\n        print \'score map size: {}\'.format(scores.shape)\n\n    # Enumerate all shifts\n    shift_x = np.arange(0, width) * _feat_stride\n    shift_y = np.arange(0, height) * _feat_stride\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                        shift_x.ravel(), shift_y.ravel())).transpose()\n\n    # Enumerate all shifted anchors:\n    #\n    # add A anchors (1, A, 4) to\n    # cell K shifts (K, 1, 4) to get\n    # shift anchors (K, A, 4)\n    # reshape to (K*A, 4) shifted anchors\n    A = _num_anchors\n    K = shifts.shape[0]\n    anchors = _anchors.reshape((1, A, 4)) + \\\n              shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n    anchors = anchors.reshape((K * A, 4))\n\n    # Transpose and reshape predicted bbox transformations to get them\n    # into the same order as the anchors:\n    #\n    # bbox deltas will be (1, 4 * A, H, W) format\n    # transpose to (1, H, W, 4 * A)\n    # reshape to (1 * H * W * A, 4) where rows are ordered by (h, w, a)\n    # in slowest to fastest order\n    bbox_deltas = bbox_deltas.transpose((0, 2, 3, 1)).reshape((-1, 4))\n\n    # Same story for the scores:\n    #\n    # scores are (1, A, H, W) format\n    # transpose to (1, H, W, A)\n    # reshape to (1 * H * W * A, 1) where rows are ordered by (h, w, a)\n    scores = scores.transpose((0, 2, 3, 1)).reshape((-1, 1))\n\n    # Convert anchors into proposals via bbox transformations\n    proposals = bbox_transform_inv(anchors, bbox_deltas)\n\n    # 2. clip predicted boxes to image\n    proposals = clip_boxes(proposals, im_info[:2])\n\n    # 3. remove predicted boxes with either height or width < threshold\n    # (NOTE: convert min_size to input image scale stored in im_info[2])\n    keep = _filter_boxes(proposals, min_size * im_info[2])\n    proposals = proposals[keep, :]\n    scores = scores[keep]\n\n    # 4. sort all (proposal, score) pairs by score from highest to lowest\n    # 5. take top pre_nms_topN (e.g. 6000)\n    order = scores.ravel().argsort()[::-1]\n    if pre_nms_topN > 0:\n        order = order[:pre_nms_topN]\n    proposals = proposals[order, :]\n    scores = scores[order]\n\n    # 6. apply nms (e.g. threshold = 0.7)\n    # 7. take after_nms_topN (e.g. 300)\n    # 8. return the top proposals (-> RoIs top)\n    keep = nms(np.hstack((proposals, scores)), nms_thresh)\n    if post_nms_topN > 0:\n        keep = keep[:post_nms_topN]\n    proposals = proposals[keep, :]\n    scores = scores[keep]\n    # Output rois blob\n    # Our RPN implementation only supports a single input image, so all\n    # batch inds are 0\n    batch_inds = np.zeros((proposals.shape[0], 1), dtype=np.float32)\n    blob = np.hstack((batch_inds, proposals.astype(np.float32, copy=False)))\n    return blob\n    #top[0].reshape(*(blob.shape))\n    #top[0].data[...] = blob\n\n    # [Optional] output scores blob\n    #if len(top) > 1:\n    #    top[1].reshape(*(scores.shape))\n    #    top[1].data[...] = scores\n\ndef _filter_boxes(boxes, min_size):\n    """"""Remove all boxes with any side smaller than min_size.""""""\n    ws = boxes[:, 2] - boxes[:, 0] + 1\n    hs = boxes[:, 3] - boxes[:, 1] + 1\n    keep = np.where((ws >= min_size) & (hs >= min_size))[0]\n    return keep\n'"
lib/rpn_msr/proposal_target_layer_tf.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport yaml\nimport numpy as np\nimport numpy.random as npr\nfrom fast_rcnn.config import cfg\nfrom fast_rcnn.bbox_transform import bbox_transform\nfrom utils.cython_bbox import bbox_overlaps\nimport pdb\n\nDEBUG = False\n\ndef proposal_target_layer(rpn_rois, gt_boxes,_num_classes):\n    """"""\n    Assign object detection proposals to ground-truth targets. Produces proposal\n    classification labels and bounding-box regression targets.\n    """"""\n\n    # Proposal ROIs (0, x1, y1, x2, y2) coming from RPN\n    # (i.e., rpn.proposal_layer.ProposalLayer), or any other source\n    all_rois = rpn_rois\n    # TODO(rbg): it\'s annoying that sometimes I have extra info before\n    # and other times after box coordinates -- normalize to one format\n\n    # Include ground-truth boxes in the set of candidate rois\n    zeros = np.zeros((gt_boxes.shape[0], 1), dtype=gt_boxes.dtype)\n    all_rois = np.vstack(\n        (all_rois, np.hstack((zeros, gt_boxes[:, :-1])))\n    )\n\n    # Sanity check: single batch only\n    assert np.all(all_rois[:, 0] == 0), \\\n            \'Only single item batches are supported\'\n\n    num_images = 1\n    rois_per_image = cfg.TRAIN.BATCH_SIZE / num_images\n    fg_rois_per_image = np.round(cfg.TRAIN.FG_FRACTION * rois_per_image)\n\n    # Sample rois with classification labels and bounding box regression\n    # targets\n    labels, rois, bbox_targets, bbox_inside_weights = _sample_rois(\n        all_rois, gt_boxes, fg_rois_per_image,\n        rois_per_image, _num_classes)\n\n    if DEBUG:\n        print \'num fg: {}\'.format((labels > 0).sum())\n        print \'num bg: {}\'.format((labels == 0).sum())\n        _count += 1\n        _fg_num += (labels > 0).sum()\n        _bg_num += (labels == 0).sum()\n        print \'num fg avg: {}\'.format(_fg_num / _count)\n        print \'num bg avg: {}\'.format(_bg_num / _count)\n        print \'ratio: {:.3f}\'.format(float(_fg_num) / float(_bg_num))\n\n    rois = rois.reshape(-1,5)\n    labels = labels.reshape(-1,1)\n    bbox_targets = bbox_targets.reshape(-1,_num_classes*4)\n    bbox_inside_weights = bbox_inside_weights.reshape(-1,_num_classes*4)\n\n    bbox_outside_weights = np.array(bbox_inside_weights > 0).astype(np.float32)\n\n    return rois,labels,bbox_targets,bbox_inside_weights,bbox_outside_weights\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    """"""Bounding-box regression targets (bbox_target_data) are stored in a\n    compact form N x (class, tx, ty, tw, th)\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets).\n\n    Returns:\n        bbox_target (ndarray): N x 4K blob of regression targets\n        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n    """"""\n\n    clss = np.array(bbox_target_data[:, 0], dtype=np.uint16, copy=True)\n    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n    bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = 4 * cls\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n        bbox_inside_weights[ind, start:end] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS\n    return bbox_targets, bbox_inside_weights\n\n\ndef _compute_targets(ex_rois, gt_rois, labels):\n    """"""Compute bounding-box regression targets for an image.""""""\n\n    assert ex_rois.shape[0] == gt_rois.shape[0]\n    assert ex_rois.shape[1] == 4\n    assert gt_rois.shape[1] == 4\n\n    targets = bbox_transform(ex_rois, gt_rois)\n    if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n        # Optionally normalize targets by a precomputed mean and stdev\n        targets = ((targets - np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS))\n                / np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS))\n    return np.hstack(\n            (labels[:, np.newaxis], targets)).astype(np.float32, copy=False)\n\ndef _sample_rois(all_rois, gt_boxes, fg_rois_per_image, rois_per_image, num_classes):\n    """"""Generate a random sample of RoIs comprising foreground and background\n    examples.\n    """"""\n    # overlaps: (rois x gt_boxes)\n    overlaps = bbox_overlaps(\n        np.ascontiguousarray(all_rois[:, 1:5], dtype=np.float),\n        np.ascontiguousarray(gt_boxes[:, :4], dtype=np.float))\n    gt_assignment = overlaps.argmax(axis=1)\n    max_overlaps = overlaps.max(axis=1)\n    labels = gt_boxes[gt_assignment, 4]\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = np.where(max_overlaps >= cfg.TRAIN.FG_THRESH)[0]\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = int(min(fg_rois_per_image, fg_inds.size))\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(fg_inds, size=fg_rois_per_this_image, replace=False)\n\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where((max_overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                       (max_overlaps >= cfg.TRAIN.BG_THRESH_LO))[0]\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    bg_rois_per_this_image = min(bg_rois_per_this_image, bg_inds.size)\n    # Sample background regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(bg_inds, size=bg_rois_per_this_image, replace=False)\n\n    # The indices that we\'re selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds)\n    # Select sampled values from various arrays:\n    labels = labels[keep_inds]\n    # Clamp labels for the background RoIs to 0\n    labels[fg_rois_per_this_image:] = 0\n    rois = all_rois[keep_inds]\n\n    bbox_target_data = _compute_targets(\n        rois[:, 1:5], gt_boxes[gt_assignment[keep_inds], :4], labels)\n\n    bbox_targets, bbox_inside_weights = \\\n        _get_bbox_regression_labels(bbox_target_data, num_classes)\n\n    return labels, rois, bbox_targets, bbox_inside_weights\n'"
lib/utils/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n'
lib/utils/blob.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Blob helper functions.""""""\n\nimport numpy as np\nimport cv2\n\ndef im_list_to_blob(ims):\n    """"""Convert a list of images into a network input.\n\n    Assumes images are already prepared (means subtracted, BGR order, ...).\n    """"""\n    max_shape = np.array([im.shape for im in ims]).max(axis=0)\n    num_images = len(ims)\n    blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),\n                    dtype=np.float32)\n    for i in xrange(num_images):\n        im = ims[i]\n        blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n\n    return blob\n\ndef prep_im_for_blob(im, pixel_means, target_size, max_size):\n    """"""Mean subtract and scale an image for use in a blob.""""""\n    im = im.astype(np.float32, copy=False)\n    im -= pixel_means\n    im_shape = im.shape\n    im_size_min = np.min(im_shape[0:2])\n    im_size_max = np.max(im_shape[0:2])\n    im_scale = float(target_size) / float(im_size_min)\n    # Prevent the biggest axis from being more than MAX_SIZE\n    if np.round(im_scale * im_size_max) > max_size:\n        im_scale = float(max_size) / float(im_size_max)\n    im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,\n                    interpolation=cv2.INTER_LINEAR)\n\n    return im, im_scale\n'"
lib/utils/boxes_grid.py,0,"b'# --------------------------------------------------------\n# Subcategory CNN\n# Copyright (c) 2015 CVGL Stanford\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Yu Xiang\n# --------------------------------------------------------\n\nimport numpy as np\nimport math\nfrom fast_rcnn.config import cfg\n\ndef get_boxes_grid(image_height, image_width):\n    """"""\n    Return the boxes on image grid.\n    """"""\n\n    # height and width of the heatmap\n    if cfg.NET_NAME == \'CaffeNet\':\n        height = np.floor((image_height * max(cfg.TRAIN.SCALES) - 1) / 4.0 + 1)\n        height = np.floor((height - 1) / 2.0 + 1 + 0.5)\n        height = np.floor((height - 1) / 2.0 + 1 + 0.5)\n\n        width = np.floor((image_width * max(cfg.TRAIN.SCALES) - 1) / 4.0 + 1)\n        width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n        width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n    elif cfg.NET_NAME == \'VGGnet\':\n        height = np.floor(image_height * max(cfg.TRAIN.SCALES) / 2.0 + 0.5)\n        height = np.floor(height / 2.0 + 0.5)\n        height = np.floor(height / 2.0 + 0.5)\n        height = np.floor(height / 2.0 + 0.5)\n\n        width = np.floor(image_width * max(cfg.TRAIN.SCALES) / 2.0 + 0.5)\n        width = np.floor(width / 2.0 + 0.5)\n        width = np.floor(width / 2.0 + 0.5)\n        width = np.floor(width / 2.0 + 0.5)\n    else:\n        assert (1), \'The network architecture is not supported in utils.get_boxes_grid!\'\n\n    # compute the grid box centers\n    h = np.arange(height)\n    w = np.arange(width)\n    y, x = np.meshgrid(h, w, indexing=\'ij\') \n    centers = np.dstack((x, y))\n    centers = np.reshape(centers, (-1, 2))\n    num = centers.shape[0]\n\n    # compute width and height of grid box\n    area = cfg.TRAIN.KERNEL_SIZE * cfg.TRAIN.KERNEL_SIZE\n    aspect = cfg.TRAIN.ASPECTS  # height / width\n    num_aspect = len(aspect)\n    widths = np.zeros((1, num_aspect), dtype=np.float32)\n    heights = np.zeros((1, num_aspect), dtype=np.float32)\n    for i in xrange(num_aspect):\n        widths[0,i] = math.sqrt(area / aspect[i])\n        heights[0,i] = widths[0,i] * aspect[i]\n\n    # construct grid boxes\n    centers = np.repeat(centers, num_aspect, axis=0)\n    widths = np.tile(widths, num).transpose()\n    heights = np.tile(heights, num).transpose()\n\n    x1 = np.reshape(centers[:,0], (-1, 1)) - widths * 0.5\n    x2 = np.reshape(centers[:,0], (-1, 1)) + widths * 0.5\n    y1 = np.reshape(centers[:,1], (-1, 1)) - heights * 0.5\n    y2 = np.reshape(centers[:,1], (-1, 1)) + heights * 0.5\n    \n    boxes_grid = np.hstack((x1, y1, x2, y2)) / cfg.TRAIN.SPATIAL_SCALE\n\n    return boxes_grid, centers[:,0], centers[:,1]\n'"
lib/utils/nms.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef nms(dets, thresh):\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
lib/utils/timer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport time\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n'"
