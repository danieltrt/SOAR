file_path,api_count,code
evaluation.py,0,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nimport os\nfrom glob import glob\n\nimport click\nfrom tf_crnn.callbacks import CustomLoaderCallback, FOLDER_SAVED_MODEL\nfrom tf_crnn.config import Params, CONST\nfrom tf_crnn.data_handler import dataset_generator\nfrom tf_crnn.preprocessing import preprocess_csv\nfrom tf_crnn.model import get_model_train\n\n\n@click.command()\n@click.option(\'--csv_filename\')\n@click.option(\'--model_dir\')\ndef evaluation(csv_filename: str,\n               model_dir: str):\n\n    config_filename = os.path.join(model_dir, \'config.json\')\n    parameters = Params.from_json_file(config_filename)\n\n    saving_dir = os.path.join(parameters.output_model_dir, FOLDER_SAVED_MODEL)\n\n    # Callback for model weights loading\n    last_time_stamp = max([int(p.split(os.path.sep)[-1].split(\'-\')[0])\n                           for p in glob(os.path.join(saving_dir, \'*\'))])\n    loading_dir = os.path.join(saving_dir, str(last_time_stamp))\n    ld_callback = CustomLoaderCallback(loading_dir)\n\n    # Preprocess csv data\n    csv_evaluation_file = os.path.join(parameters.output_model_dir, CONST.PREPROCESSING_FOLDER, \'evaluation_data.csv\')\n    n_samples = preprocess_csv(csv_filename,\n                               parameters,\n                               csv_evaluation_file)\n\n    dataset_evaluation = dataset_generator([csv_evaluation_file],\n                                           parameters,\n                                           batch_size=parameters.eval_batch_size,\n                                           num_epochs=1)\n\n    # get model and evaluation\n    model = get_model_train(parameters)\n    eval_output = model.evaluate(dataset_evaluation,\n                                 callbacks=[ld_callback])\n    print(\'-- Metrics: \', eval_output)\n\n\nif __name__ == \'__main__\':\n    evaluation()\n'"
prediction.py,0,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nimport os\nfrom glob import glob\n\nimport click\nfrom tf_crnn.callbacks import CustomPredictionSaverCallback, FOLDER_SAVED_MODEL\nfrom tf_crnn.config import Params\nfrom tf_crnn.data_handler import dataset_generator\nfrom tf_crnn.model import get_model_inference\n\n\n@click.command()\n@click.option(\'--csv_filename\', help=\'A csv file containing the path to the images to predict\')\n@click.option(\'--output_model_dir\', help=\'Directory where all the exported data related to an experiment has been saved\')\ndef prediction(csv_filename: str,\n               output_model_dir: str):\n    parameters = Params.from_json_file(os.path.join(output_model_dir, \'config.json\'))\n\n    saving_dir = os.path.join(output_model_dir, FOLDER_SAVED_MODEL)\n    last_time_stamp = str(max([int(p.split(os.path.sep)[-1].split(\'-\')[0])\n                           for p in glob(os.path.join(saving_dir, \'*\'))]))\n    model = get_model_inference(parameters, os.path.join(saving_dir, last_time_stamp, \'weights.h5\'))\n\n    dataset_test = dataset_generator([csv_filename],\n                                     parameters,\n                                     use_labels=False,\n                                     batch_size=parameters.eval_batch_size,\n                                     shuffle=False)\n\n    ps_callback = CustomPredictionSaverCallback(output_model_dir, parameters)\n\n    _, _, _ = model.predict(x=dataset_test, callbacks=[ps_callback])\n\n\nif __name__ == \'__main__\':\n    prediction()\n'"
setup.py,0,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nfrom setuptools import setup, find_packages\n\nsetup(name=\'tf_crnn\',\n      version=\'0.6.0\',\n      license=\'GPL\',\n      author=\'Sofia Ares Oliveira\',\n      url=\'https://github.com/solivr/tf-crnn\',\n      description=\'TensorFlow Convolutional Recurrent Neural Network (CRNN)\',\n      install_requires=[\n            \'imageio\',\n            \'numpy\',\n            \'tqdm\',\n            \'sacred\',\n            \'opencv-python\',\n            \'pandas\',\n            \'click\',\n            #\'tensorflow-addons\',\n            \'tensorflow-gpu\',\n            \'taputapu\'\n      ],\n      dependency_links=[\'https://github.com/solivr/taputapu/tarball/master#egg=taputapu-1.0\'],\n      extras_require={\n            \'doc\': [\n                  \'sphinx\',\n                  \'sphinx-autodoc-typehints\',\n                  \'sphinx-rtd-theme\',\n                  \'sphinxcontrib-bibtex\',\n                  \'sphinxcontrib-websupport\'\n            ],\n      },\n      packages=find_packages(where=\'.\'),\n      zip_safe=False)\n'"
training.py,3,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nimport logging\nlogging.getLogger(""tensorflow"").setLevel(logging.INFO)\n\nfrom tf_crnn.config import Params\nfrom tf_crnn.model import get_model_train\nfrom tf_crnn.preprocessing import data_preprocessing\nfrom tf_crnn.data_handler import dataset_generator\nfrom tf_crnn.callbacks import CustomLoaderCallback, CustomSavingCallback, LRTensorBoard, EPOCH_FILENAME, FOLDER_SAVED_MODEL\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport json\nimport pickle\nfrom glob import glob\nfrom sacred import Experiment, SETTINGS\n\nSETTINGS.CONFIG.READ_ONLY_CONFIG = False\n\nex = Experiment(\'crnn\')\n\nex.add_config(\'config.json\')\n\n@ex.automain\ndef training(_config: dict):\n    parameters = Params(**_config)\n\n    export_config_filename =  os.path.join(parameters.output_model_dir, \'config.json\')\n    saving_dir = os.path.join(parameters.output_model_dir, FOLDER_SAVED_MODEL)\n\n    if not parameters.restore_model:\n        # check if output folder already exists\n        assert not os.path.isdir(parameters.output_model_dir), \\\n            \'{} already exists, you cannot use it as output directory.\'.format(parameters.output_model_dir)\n            # \'Set ""restore_model=True"" to continue training, or delete dir ""rm -r {0}""\'.format(parameters.output_model_dir)\n        os.makedirs(parameters.output_model_dir)\n\n    # data and csv preprocessing\n    csv_train_file, csv_eval_file, \\\n    n_samples_train, n_samples_eval = data_preprocessing(parameters)\n\n    # export config file in model output dir\n    with open(export_config_filename, \'w\') as file:\n        json.dump(parameters.to_dict(), file)\n\n    # Create callbacks\n    logdir = os.path.join(parameters.output_model_dir, \'logs\')\n    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir,\n                                                 profile_batch=0)\n\n    lrtb_callback = LRTensorBoard(log_dir=logdir,\n                                  profile_batch=0)\n\n    lr_callback = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5,\n                                                       patience=10,\n                                                       cooldown=0,\n                                                       min_lr=1e-8,\n                                                       verbose=1)\n\n    es_callback = tf.keras.callbacks.EarlyStopping(min_delta=0.005,\n                                                   patience=20,\n                                                   verbose=1)\n\n    sv_callback = CustomSavingCallback(saving_dir,\n                                       saving_freq=parameters.save_interval,\n                                       save_best_only=True,\n                                       keep_max_models=3)\n\n    list_callbacks = [tb_callback, lrtb_callback, lr_callback, es_callback, sv_callback]\n\n    if parameters.restore_model:\n        last_time_stamp = max([int(p.split(os.path.sep)[-1].split(\'-\')[0])\n                               for p in glob(os.path.join(saving_dir, \'*\'))])\n\n        loading_dir = os.path.join(saving_dir, str(last_time_stamp))\n        ld_callback = CustomLoaderCallback(loading_dir)\n\n        list_callbacks.append(ld_callback)\n\n        with open(os.path.join(loading_dir, EPOCH_FILENAME), \'rb\') as f:\n            initial_epoch = pickle.load(f)\n\n        epochs = initial_epoch + parameters.n_epochs\n    else:\n        initial_epoch = 0\n        epochs = parameters.n_epochs\n\n    # Get model\n    model = get_model_train(parameters)\n\n    # Get datasets\n    dataset_train = dataset_generator([csv_train_file],\n                                      parameters,\n                                      batch_size=parameters.train_batch_size,\n                                      data_augmentation=parameters.data_augmentation,\n                                      num_epochs=parameters.n_epochs)\n\n    dataset_eval = dataset_generator([csv_eval_file],\n                                     parameters,\n                                     batch_size=parameters.eval_batch_size,\n                                     data_augmentation=False,\n                                     num_epochs=parameters.n_epochs)\n\n    # Train model\n    model.fit(dataset_train,\n              epochs=epochs,\n              initial_epoch=initial_epoch,\n              steps_per_epoch=np.floor(n_samples_train / parameters.train_batch_size),\n              validation_data=dataset_eval,\n              validation_steps=np.floor(n_samples_eval / parameters.eval_batch_size),\n              callbacks=list_callbacks)\n'"
doc/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'..\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'tf_crnn\'\ncopyright = \'2019, Digital Humanities Lab - EPFL\'\nauthor = \'Sofia ARES OLIVEIRA\'\n\n# The short X.Y version\nversion = \'\'\n# The full version, including alpha/beta/rc tags\nrelease = \'\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    \'sphinxcontrib.bibtex\',  # for bibtex\n    \'sphinx_autodoc_typehints\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'tf_crnndoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'tf_crnn.tex\', \'tf\\\\_crnn Documentation\',\n     author, \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'tf_crnn\', \'tf_crnn Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'tf_crnn\', \'tf_crnn Documentation\',\n     author, \'tf_crnn\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n\n# -- Extension configuration -------------------------------------------------\nautodoc_mock_imports = [\n    # \'numpy\',\n    \'tensorflow\',\n    \'tensorflow_addons\',\n    \'pandas\',\n    \'typing\',\n    \'cv2\'\n]'"
hlp/alphabet_helpers.py,0,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nfrom typing import List, Union\nimport csv\nimport json\nimport numpy as np\nimport pandas as pd\n\n\ndef get_alphabet_units_from_input_data(csv_filename: str,\n                                       split_char: str=\'|\'):\n    """"""\n    Get alphabet units from the input_data csv file (which contains in each row the tuple\n    (filename image segment, transcription formatted))\n\n    :param csv_filename: csv file containing the input data\n    :param split_char: splitting character in input_data separting the alphabet units\n    :return:\n    """"""\n    df = pd.read_csv(csv_filename, sep=\';\', header=None, names=[\'image\', \'labels\'],\n                     encoding=\'utf8\', escapechar=""\\\\"", quoting=3)\n    transcriptions = list(df.labels.apply(lambda x: x.split(split_char)))\n\n    unique_units = np.unique([chars for list_chars in transcriptions for chars in list_chars])\n\n    return unique_units\n\n\ndef generate_alphabet_file(csv_filenames: List[str],\n                           alphabet_filename: str):\n    """"""\n\n    :param csv_filenames:\n    :param alphabet_filename:\n    :return:\n    """"""\n    symbols = list()\n    for file in csv_filenames:\n        symbols.append(get_alphabet_units_from_input_data(file))\n\n    alphabet_units = np.unique(np.concatenate(symbols))\n\n    alphabet_lookup = dict([(au, i+1)for i, au in enumerate(alphabet_units)])\n\n    with open(alphabet_filename, \'w\') as f:\n        json.dump(alphabet_lookup, f)\n\n\ndef get_abbreviations_from_csv(csv_filename: str) -> List[str]:\n    with open(csv_filename, \'r\', encoding=\'utf8\') as f:\n        csvreader = csv.reader(f, delimiter=\'\\n\')\n        alphabet_units = [row[0] for row in csvreader]\n    return alphabet_units\n\n\n# def make_json_lookup_alphabet(string_chars: str=None) -> dict:\n#     """"""\n#\n#     :param string_chars: for example string.ascii_letters, string.digits\n#     :return:\n#     """"""\n#     lookup = dict()\n#     if string_chars:\n#         # Add characters to lookup table\n#         lookup.update({char: ord(char) for char in string_chars})\n#\n#     return map_lookup(lookup)\n\n\n# def load_lookup_from_json(json_filenames: Union[List[str], str])-> dict:\n#     """"""\n#     Load a lookup table from a json file to a dictionnary\n#     :param json_filenames: either a filename or a list of filenames\n#     :return:\n#     """"""\n#\n#     lookup = dict()\n#     if isinstance(json_filenames, list):\n#         for file in json_filenames:\n#             with open(file, \'r\', encoding=\'utf8\') as f:\n#                 data_dict = json.load(f)\n#             lookup.update(data_dict)\n#\n#     elif isinstance(json_filenames, str):\n#         with open(json_filenames, \'r\', encoding=\'utf8\') as f:\n#             lookup = json.load(f)\n#\n#     return map_lookup(lookup)\n\n\n# def map_lookup(lookup_table: dict, unique_entry: bool=True)-> dict:\n#     """"""\n#     Converts an existing lookup table with minimal range code ([1, len(lookup_table)-1])\n#     and avoids multiple instances of the same code label (bijectivity)\n#\n#     :param lookup_table: dictionary to be mapped {alphabet_unit : code label}\n#     :param unique_entry: If each alphabet unit has a unique code and each code a unique alphabet unique (\'bijective\'),\n#                         only True is implemented for now\n#     :return: a mapped dictionary\n#     """"""\n#\n#     # Create tuple (alphabet unit, code)\n#     tuple_char_code = list(zip(list(lookup_table.keys()), list(lookup_table.values())))\n#     # Sort by code\n#     tuple_char_code.sort(key=lambda x: x[1])\n#\n#     # If each alphabet unit has a unique code and each code a unique alphabet unique (\'bijective\')\n#     if unique_entry:\n#         mapped_lookup = [[tp[0], i + 1] for i, tp in enumerate(tuple_char_code)]\n#     else:\n#         raise NotImplementedError\n#         # Todo\n#\n#     return dict(mapped_lookup)\n'"
hlp/csv_helpers.py,0,"b'#!/usr/bin/env python\n__author__ = \'solivr\'\n__license__ = ""GPL""\n\nimport csv\nimport os\nimport argparse\nfrom tqdm import tqdm, trange\n\n\ndef csv_rel2abs_path_convertor(csv_filenames: str, delimiter: str=\' \', encoding=\'utf8\') -> None:\n    """"""\n    Convert relative paths into absolute paths\n\n    :param csv_filenames: filename of csv\n    :param delimiter: character to delimit felds in csv\n    :param encoding: encoding format of csv file\n    :return:\n    """"""\n\n    for filename in tqdm(csv_filenames):\n        absolute_path, basename = os.path.split(os.path.abspath(filename))\n        relative_paths = list()\n        labels = list()\n        # Reading CSV\n        with open(filename, \'r\', encoding=encoding) as f:\n            csvreader = csv.reader(f, delimiter=delimiter)\n            for row in csvreader:\n                relative_paths.append(row[0])\n                labels.append(row[1])\n\n        # Writing converted_paths CSV\n        export_filename = os.path.join(absolute_path, \'{}_abs{}\'.format(*os.path.splitext(basename)))\n        with open(export_filename, \'w\', encoding=encoding) as f:\n            csvwriter = csv.writer(f, delimiter=delimiter)\n            for i in trange(0, len(relative_paths)):\n                csvwriter.writerow([os.path.abspath(os.path.join(absolute_path, relative_paths[i])), labels[i]])\n\n\ndef csv_filtering_chars_from_labels(csv_filename: str, chars_to_remove: str,\n                                    delimiter: str=\' \', encoding=\'utf8\') -> int:\n    """"""\n    Remove labels containing chars_to_remove in csv_filename\n\n    :param chars_to_remove: string (or list) with the undesired characters\n    :param csv_filename: filenmae of csv\n    :param delimiter: delimiter character\n    :param encoding: encoding format of csv file\n    :return: number of deleted labels\n    """"""\n\n    if not isinstance(chars_to_remove, list):\n        chars_to_remove = list(chars_to_remove)\n\n    paths = list()\n    labels = list()\n    n_deleted = 0\n    with open(csv_filename, \'r\', encoding=encoding) as file:\n        csvreader = csv.reader(file, delimiter=delimiter)\n        for row in csvreader:\n            if not any((d in chars_to_remove) for d in row[1]):\n                paths.append(row[0])\n                labels.append(row[1])\n            else:\n                n_deleted += 1\n\n    with open(csv_filename, \'w\', encoding=encoding) as file:\n        csvwriter = csv.writer(file, delimiter=delimiter)\n        for i in tqdm(range(len(paths)), total=len(paths)):\n            csvwriter.writerow([paths[i], labels[i]])\n\n    return n_deleted\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-i\', \'--input_files\', type=str, required=True, help=\'CSV filename to convert\', nargs=\'*\')\n    parser.add_argument(\'-d\', \'--delimiter_char\', type=str, help=\'CSV delimiter character\', default=\' \')\n\n    args = vars(parser.parse_args())\n\n    csv_filenames = args.get(\'input_files\')\n\n    csv_rel2abs_path_convertor(csv_filenames, delimiter=args.get(\'delimiter_char\'))\n\n'"
hlp/numbers_mnist_generator.py,0,"b'#!/usr/bin/env python\n__author__ = \'solivr\'\n__license__ = ""GPL""\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport numpy as np\nimport os\nimport csv\nfrom imageio import imsave\nfrom tqdm import tqdm\nimport random\nimport argparse\n\n\ndef generate_random_image_numbers(mnist_dir, dataset, output_dir, csv_filename, n_numbers):\n\n    mnist = input_data.read_data_sets(mnist_dir, one_hot=False)\n\n    output_dir_img = os.path.join(output_dir, \'images\')\n    if not os.path.exists(output_dir):\n        os.mkdir(output_dir)\n    if not os.path.exists(output_dir_img):\n        os.mkdir(output_dir_img)\n\n    if dataset == \'train\':\n        dataset = mnist.train\n    elif dataset == \'validation\':\n        dataset = mnist.validation\n    elif dataset == \'test\':\n        dataset = mnist.test\n\n    list_paths = list()\n    list_labels = list()\n\n    for i in tqdm(range(n_numbers), total=n_numbers):\n        n_digits = random.randint(3, 8)\n        digits, labels = dataset.next_batch(n_digits)\n        # Reshape to have 28x28 image\n        square_digits = np.reshape(digits, [-1, 28, 28])\n        # White background\n        square_digits = -(square_digits - 1) * 255\n        stacked_number = np.hstack(square_digits[:, :, 4:-4])\n        stacked_label = \'\'.join(map(str, labels))\n        # chans3 = np.dstack([stacked_number]*3)\n\n        # Save image number\n        img_filename = \'{:09}_{}.jpg\'.format(i, stacked_label)\n        img_path = os.path.join(output_dir_img, img_filename)\n        imsave(img_path, stacked_number)\n\n        # Add to list of paths and list of labels\n        list_paths.append(img_filename)\n        list_labels.append(stacked_label)\n\n    root = \'./images\'\n    csv_path = os.path.join(output_dir, csv_filename)\n    with open(csv_path, \'w\') as csvfile:\n        for i in tqdm(range(len(list_paths)), total=len(list_paths)):\n            csvwriter = csv.writer(csvfile, delimiter=\' \')\n            csvwriter.writerow([os.path.join(root, list_paths[i]), list_labels[i]])\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-md\', \'--mnist_dir\', type=str, help=\'Directory for MNIST data\', default=\'./MNIST_data\')\n    parser.add_argument(\'-d\', \'--dataset\', type=str, help=\'Dataset wanted (train, test, validation)\', default=\'train\')\n    parser.add_argument(\'-csv\', \'--csv_filename\', type=str, help=\'CSV filename to output paths and labels\')\n    parser.add_argument(\'-od\', \'--output_dir\', type=str, help=\'Directory to output images and csv files\', default=\'./output_numbers\')\n    parser.add_argument(\'-n\', \'--n_samples\', type=int, help=\'Desired numbers of generated samples\', default=1000)\n\n    args = parser.parse_args()\n\n    generate_random_image_numbers(args.mnist_dir, args.dataset, args.output_dir, args.csv_filename, args.n_samples)\n\n\n'"
hlp/prepare_iam.py,0,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nfrom taputapu.databases import iam\nimport os\nfrom glob import glob\nfrom string_data_manager import tf_crnn_label_formatting\nfrom alphabet_helpers import generate_alphabet_file\nimport click\n\n\n@click.command()\n@click.option(\'--download_dir\')\n@click.option(\'--generated_data_dir\')\ndef prepare_iam_data(download_dir: str,\n                     generated_data_dir: str):\n\n    # Download data\n    print(\'Starting downloads...\')\n    iam.download(download_dir)\n\n    # Extract archives\n    print(\'Starting extractions...\')\n    iam.extract(download_dir)\n\n    print(\'Generating files for the experiment...\')\n    # Generate splits (same format as ascii files)\n    export_splits_dir = os.path.join(generated_data_dir, \'generated_splits\')\n    os.makedirs(export_splits_dir, exist_ok=True)\n\n    iam.generate_splits_txt(os.path.join(download_dir, \'ascii\', \'lines.txt\'),\n                            os.path.join(download_dir, \'largeWriterIndependentTextLineRecognitionTask\'),\n                            export_splits_dir)\n\n    # Generate csv from .txt splits files\n    export_csv_dir = os.path.join(generated_data_dir, \'generated_csv\')\n    os.makedirs(export_csv_dir, exist_ok=True)\n\n    for file in glob(os.path.join(export_splits_dir, \'*\')):\n        export_basename = os.path.basename(file).split(\'.\')[0]\n        iam.create_experiment_csv(file,\n                                  os.path.join(download_dir, \'lines\'),\n                                  os.path.join(export_csv_dir, export_basename + \'.csv\'),\n                                  False,\n                                  True)\n\n    # Format string label to tf_crnn formatting\n    for csv_filename in glob(os.path.join(export_csv_dir, \'*\')):\n        tf_crnn_label_formatting(csv_filename)\n\n    # Generate alphabet\n    alphabet_dir = os.path.join(generated_data_dir, \'generated_alphabet\')\n    os.makedirs(alphabet_dir, exist_ok=True)\n\n    generate_alphabet_file(glob(os.path.join(export_csv_dir, \'*\')),\n                           os.path.join(alphabet_dir, \'iam_alphabet_lookup.json\'))\n\n\nif __name__ == \'__main__\':\n    prepare_iam_data()\n'"
hlp/string_data_manager.py,1,"b'#!/usr/bin/env python\n__author__ = \'solivr\'\n__licence__ = \'GPL\'\n\nimport pandas as pd\n\n_accents_list = \'\xc3\xa0\xc3\xa9\xc3\xa8\xc3\xac\xc3\xae\xc3\xb3\xc3\xb2\xc3\xb9\'\n_accent_mapping = {\'\xc3\xa0\': \'a\',\n                   \'\xc3\xa9\': \'e\',\n                   \'\xc3\xa8\': \'e\',\n                   \'\xc3\xac\': \'i\',\n                   \'\xc3\xae\': \'i\',\n                   \'\xc3\xb3\': \'o\',\n                   \'\xc3\xb2\': \'o\',\n                   \'\xc3\xb9\': \'u\'}\n\n\ndef map_accentuated_characters_in_dataframe(dataframe_transcriptions: pd.DataFrame,\n                                            dict_mapping: dict=_accent_mapping) -> pd.DataFrame:\n    """"""\n\n    :param dataframe_transcriptions: must have a field \'transcription\'\n    :param dict_mapping\n    :return:\n    """"""\n    items = dataframe_transcriptions.transcription.iteritems()\n\n    for i in range(dataframe_transcriptions.transcription.count()):\n        df_id, transcription = next(items)\n        # https://stackoverflow.com/questions/30020184/how-to-find-the-first-index-of-any-of-a-set-of-characters-in-a-string\n        ch_index = next((i for i, ch in enumerate(transcription) if ch in _accents_list), None)\n        while ch_index is not None:\n            transcription = list(transcription)\n            ch = transcription[ch_index]\n            transcription[ch_index] = dict_mapping[ch]\n            transcription = \'\'.join(transcription)\n            dataframe_transcriptions.at[df_id, \'transcription\'] = transcription\n            ch_index = next((i for i, ch in enumerate(transcription) if ch in _accents_list), None)\n\n    return dataframe_transcriptions\n\n\ndef map_accentuated_characters_in_string(string_to_format: str, dict_mapping: dict=_accent_mapping) -> str:\n    """"""\n\n    :param string_to_format:\n    :param dict_mapping:\n    :return:\n    """"""\n    # https://stackoverflow.com/questions/30020184/how-to-find-the-first-index-of-any-of-a-set-of-characters-in-a-string\n    ch_index = next((i for i, ch in enumerate(string_to_format) if ch in _accents_list), None)\n    while ch_index is not None:\n        string_to_format = list(string_to_format)\n        ch = string_to_format[ch_index]\n        string_to_format[ch_index] = dict_mapping[ch]\n        string_to_format = \'\'.join(string_to_format)\n        ch_index = next((i for i, ch in enumerate(string_to_format) if ch in _accents_list), None)\n\n    return string_to_format\n\n\ndef format_string_for_tf_split(string_to_format: str,\n                               separator_character: str= \'|\',\n                               replace_brackets_abbreviations=True) -> str:\n    """"""\n    Formats transcriptions to be split by tf.string_split using character separator ""|""\n\n    :param string_to_format: string to format\n    :param separator_character: character that separates alphabet units\n    :param replace_brackets_abbreviations: if True will replace \'[\' and \']\' chars by separator character\n    :return:\n    """"""\n\n    if replace_brackets_abbreviations:\n        # Replace ""[]"" chars by ""|""\n        string_to_format = string_to_format.replace(""["", separator_character).replace(""]"", separator_character)\n\n    splits = string_to_format.split(separator_character)\n\n    final_string = separator_character\n    # Case where string starts with a separator_character\n    if splits[0] == \'\':\n        for i, sp in enumerate(splits):\n            if i % 2 > 0:  # uneven -> abbreviation\n                final_string += separator_character + sp + separator_character\n            else:  # even -> no abbreviation\n                final_string += sp.replace(\'\', separator_character)[1:-1]\n\n    else:\n        for i, sp in enumerate(splits):\n            if i % 2 > 0:  # uneven -> no abbreviation\n                final_string += separator_character + sp + separator_character\n            else:  # even -> abbreviation\n                final_string += sp.replace(\'\', separator_character)[1:-1]\n\n    # Add separator at beginning or end of string if it hasn\'t been added yet\n    if final_string[1] == separator_character:\n        final_string = final_string[1:]\n    if final_string[-1] != separator_character:\n        final_string += separator_character\n\n    return final_string\n\n\ndef tf_crnn_label_formatting(csv_filename: str):\n\n    def _string_formatting(string_to_format: str,\n                           separator_character: str = \'|\'):\n        chars = list(string_to_format)\n        formated_string = separator_character + \'{}\'.format(separator_character).join(chars) + separator_character\n        return formated_string\n\n    df = pd.read_csv(csv_filename, sep=\';\', header=None, names=[\'image\', \'labels\'], encoding=\'utf8\',\n                     escapechar=""\\\\"", quoting=3)\n\n    df.labels = df.labels.apply(lambda x: _string_formatting(x))\n\n    df.to_csv(csv_filename, sep=\';\', encoding=\'utf-8\', header=False, index=False, escapechar=""\\\\"", quoting=3)\n\n\ndef lower_abbreviation_in_string(string_to_format: str):\n    # Split with \'[\'\n    tokens_opening = string_to_format.split(\'[\')\n\n    valid_string = True\n    final_string = tokens_opening[0]\n    for tok in tokens_opening[1:]:\n        if len(tok) > 1:\n            token_closing = tok.split(\']\')\n            if len(token_closing) == 2:  # checks if abbreviation starts with [ and ends with ]\n                final_string += \'[\' + token_closing[0].lower() + \']\' + token_closing[1]\n            else:  # No closing \']\'\n                valid_string = False\n        else:\n            final_string += \']\'\n    if valid_string:\n        return final_string\n    else:\n        return \'\'\n\n\ndef add_abbreviation_brackets(label: str):\n    """"""\n    Adds brackets in formatted strings i.e label= \'|B|e|n|e|t|t|a| |M|a|z|z|o|l|e|n|i| |quondam| |A|n|z|o|l|o|\'\n    turns to \'|B|e|n|e|t|t|a| |M|a|z|z|o|l|e|n|i| |[quondam]| |A|n|z|o|l|o|\'\n    :param label:\n    :return:\n    """"""\n    splits = label.split(\'|\')\n\n    is_abbrev = [len(tok) > 1 for tok in splits]\n    bracketing = [\'[\' + tok + \']\' if abbrev else tok for (tok, abbrev) in zip(splits, is_abbrev)]\n\n    return \'|\'.join(bracketing)'"
tf_crnn/__init__.py,0,"b'r""""""\n\n\nData handling for input function\n--------------------------------\n.. currentmodule:: tf_crnn.data_handler\n\n.. autosummary::\n    dataset_generator\n    padding_inputs_width\n    augment_data\n    random_rotation\n\n\nModel definitions\n-----------------\n.. currentmodule:: tf_crnn.model\n\n.. autosummary::\n    ConvBlock\n    get_model_train\n    get_model_inference\n    get_crnn_output\n\n\nConfig for training\n-------------------\n.. currentmodule:: tf_crnn.config\n\n.. autosummary::\n    Alphabet\n    Params\n    import_params_from_json\n\n\nCustom Callbacks\n----------------\n.. currentmodule:: tf_crnn.callbacks\n\n.. autosummary::\n    CustomSavingCallback\n    LRTensorBoard\n    CustomLoaderCallback\n    CustomPredictionSaverCallback\n\n\nPreprocessing data\n------------------\n.. currentmodule:: tf_crnn.preprocessing\n\n.. autosummary::\n    data_preprocessing\n    preprocess_csv\n\n\n----\n\n""""""\n\n_DATA_HANDLING = [\n    \'dataset_generator\',\n    \'padding_inputs_width\',\n    \'augment_data\',\n    \'random_rotation\'\n]\n\n_CONFIG = [\n    \'Alphabet\',\n    \'Params\',\n    \'import_params_from_json\'\n\n]\n\n_MODEL = [\n    \'ConvBlock\',\n    \'get_model_train\',\n    \'get_model_inference\'\n    \'get_crnn_output\'\n]\n\n_CALLBACKS = [\n    \'CustomSavingCallback\',\n    \'CustomLoaderCallback\',\n    \'CustomPredictionSaverCallback\',\n    \'LRTensorBoard\'\n]\n\n_PREPROCESSING = [\n    \'data_preprocessing\',\n    \'preprocess_csv\'\n]\n\n__all__ = _DATA_HANDLING + _CONFIG + _MODEL + _CALLBACKS + _PREPROCESSING\n\nfrom tf_crnn.config import *\nfrom tf_crnn.model import *\nfrom tf_crnn.callbacks import *\nfrom tf_crnn.preprocessing import *\nfrom tf_crnn.data_handler import *'"
tf_crnn/callbacks.py,3,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import Callback, TensorBoard\nimport os\nimport shutil\nimport pickle\nimport json\nimport time\nimport numpy as np\nfrom .config import Params\n\n\nMODEL_WEIGHTS_FILENAME = \'weights.h5\'\nOPTIMIZER_WEIGHTS_FILENAME = \'optimizer_weights.pkl\'\nLEARNING_RATE_FILENAME = \'learning_rate.pkl\'\nLAYERS_FILENAME = \'architecture.json\'\nEPOCH_FILENAME = \'epoch.pkl\'\nFOLDER_SAVED_MODEL = \'saving\'\n\n\nclass CustomSavingCallback(Callback):\n    """"""\n    Callback to save weights, architecture, and optimizer at the end of training.\n    Inspired by `ModelCheckpoint`.\n\n    :ivar output_dir: path to the folder where files will be saved\n    :vartype output_dir: str\n    :ivar saving_freq: save every `n` epochs\n    :vartype saving_freq: int\n    :ivar save_best_only: wether to save a model if it is best thant the last saving\n    :vartype save_best_only: bool\n    :ivar keep_max_models: number of models to keep, the older ones will be deleted\n    :vartype keep_max_models: int\n    """"""\n    def __init__(self,\n                 output_dir: str,\n                 saving_freq:int,\n                 save_best_only: bool=False,\n                 keep_max_models:int=5):\n        super(CustomSavingCallback, self).__init__()\n\n        self.saving_dir = output_dir\n        self.saving_freq = saving_freq\n        self.save_best_only = save_best_only\n        self.keep_max_models = keep_max_models\n\n        self.epochs_since_last_save = 0\n\n        self.monitor = \'val_loss\'\n        self.monitor_op = np.less\n        self.best_value = np.Inf # todo: when restoring model we could also restore val_loss and metric\n\n    def on_epoch_begin(self,\n                       epoch,\n                       logs=None):\n        self._current_epoch = epoch\n\n    def on_epoch_end(self,\n                     epoch,\n                     logs=None):\n\n        self.logs = logs\n        self.epochs_since_last_save += 1\n\n        if self.epochs_since_last_save == self.saving_freq:\n            self._export_model(logs)\n            self.epochs_since_last_save = 0\n\n    def on_train_end(self,\n                     logs=None):\n        self._export_model(self.logs)\n        self.epochs_since_last_save = 0\n\n\n    def _export_model(self, logs):\n        timestamp = str(int(time.time()))\n        folder = os.path.join(self.saving_dir, timestamp)\n\n        if self.save_best_only:\n            current_value = logs.get(self.monitor)\n\n            if self.monitor_op(current_value, self.best_value):\n                print(\'\\n{} improved from {:0.5f} to {:0.5f},\'\n                      \' saving model to {}\'.format(self.monitor, self.best_value,\n                                                   current_value, folder))\n                self.best_value = current_value\n\n            else:\n                print(\'\\n{} did not improve from {:0.5f}\'.format(self.monitor, self.best_value))\n                return\n\n        os.makedirs(folder)\n\n        # save architecture\n        model_json = self.model.to_json()\n        with open(os.path.join(folder, LAYERS_FILENAME), \'w\') as f:\n            json.dump(model_json, f)\n\n        # model weights\n        self.model.save_weights(os.path.join(folder, MODEL_WEIGHTS_FILENAME))\n\n        # optimizer weights\n        optimizer_weights = tf.keras.backend.batch_get_value(self.model.optimizer.weights)\n        with open(os.path.join(folder, OPTIMIZER_WEIGHTS_FILENAME), \'wb\') as f:\n            pickle.dump(optimizer_weights, f)\n\n        # learning rate\n        learning_rate = self.model.optimizer.learning_rate\n        with open(os.path.join(folder, LEARNING_RATE_FILENAME), \'wb\') as f:\n            pickle.dump(learning_rate, f)\n\n        # n epochs\n        epoch = self._current_epoch + 1\n        with open(os.path.join(folder, EPOCH_FILENAME), \'wb\') as f:\n            pickle.dump(epoch, f)\n\n        self._clean_exports()\n\n    def _clean_exports(self):\n        timestamp_folders = [int(f) for f in os.listdir(self.saving_dir)]\n        timestamp_folders.sort(reverse=True)\n\n        if len(timestamp_folders) > self.keep_max_models:\n            folders_to_remove = timestamp_folders[self.keep_max_models:]\n            for f in folders_to_remove:\n                shutil.rmtree(os.path.join(self.saving_dir, str(f)))\n\n\n\nclass CustomLoaderCallback(Callback):\n    """"""\n    Callback to load necessary weight and parameters for training, evaluation and prediction.\n\n    :ivar loading_dir: path to directory to save logs\n    :vartype loading_dir: str\n    """"""\n    def __init__(self,\n                 loading_dir: str):\n        super(CustomLoaderCallback, self).__init__()\n\n        self.loading_dir = loading_dir\n\n    def set_model(self, model):\n        self.model = model\n\n        print(\'-- Loading \', self.loading_dir)\n        # Load model weights\n        self.model.load_weights(os.path.join(self.loading_dir, MODEL_WEIGHTS_FILENAME))\n\n        # Load optimizer params\n        with open(os.path.join(self.loading_dir, OPTIMIZER_WEIGHTS_FILENAME), \'rb\') as f:\n            optimizer_weights = pickle.load(f)\n        with open(os.path.join(self.loading_dir, LEARNING_RATE_FILENAME), \'rb\') as f:\n            learning_rate = pickle.load(f)\n\n        # Set optimizer params\n        self.model.optimizer.learning_rate.assign(learning_rate)\n        self.model._make_train_function()\n        self.model.optimizer.set_weights(optimizer_weights)\n\n\nclass CustomPredictionSaverCallback(Callback):\n    """"""\n    Callback to save prediction decoded outputs.\n    This will save the decoded outputs into a file.\n\n    :ivar output_dir: path to directory to save logs\n    :vartype output_dir: str\n    :ivar parameters: parameters of the experiment (``Params``)\n    :vartype parameters: Params\n    """"""\n    def __init__(self,\n                 output_dir: str,\n                 parameters: Params):\n        super(CustomPredictionSaverCallback, self).__init__()\n\n        self.saving_dir = output_dir\n        self.parameters = parameters\n\n    # Inference\n    def on_predict_begin(self,\n                         logs=None):\n        # Create file to add predictions\n        timestamp = str(int(time.time()))\n        self._prediction_filename = os.path.join(self.saving_dir, \'predictions-{}.txt\'.format(timestamp))\n\n    def on_predict_batch_end(self,\n                             batch,\n                             logs):\n        logits, seq_len, filenames = logs[\'outputs\']\n\n        codes = tf.keras.backend.ctc_decode(logits, tf.squeeze(seq_len), greedy=True)[0][0].numpy()\n        strings = [\'\'.join([self.parameters.alphabet.lookup_int2str[c] for c in lc if c != -1]) for lc in codes]\n\n        with open(self._prediction_filename, \'ab\') as f:\n            for n, s in zip(filenames, strings):\n                n = n[0] # n is a list of one element\n                f.write((n.decode() + \';\' + s + \'\\n\').encode(\'utf8\'))\n\n\nclass LRTensorBoard(TensorBoard):\n    """"""\n    Adds learning rate to TensorBoard scalars.\n\n    :ivar logdir: path to directory to save logs\n    :vartype logdir: str\n    """"""\n    # From https://github.com/keras-team/keras/pull/9168#issuecomment-359901128\n    def __init__(self,\n                 log_dir: str,\n                 **kwargs):  # add other arguments to __init__ if you need\n        super(LRTensorBoard, self).__init__(log_dir=log_dir, **kwargs)\n\n    def on_epoch_end(self,\n                     epoch,\n                     logs=None):\n        logs.update({\'lr\': tf.keras.backend.eval(self.model.optimizer.lr)})\n        super(LRTensorBoard, self).on_epoch_end(epoch, logs)'"
tf_crnn/config.py,0,"b'#!/usr/bin/env python\n__author__ = \'solivr\'\n__license__ = ""GPL""\n\nimport csv\nimport json\nimport os\nimport string\nfrom functools import reduce\nfrom glob import glob\nfrom typing import List, Union\nimport pandas as pd\n\n\nclass CONST:\n    DIMENSION_REDUCTION_W_POOLING = 2*2  # 2x2 pooling in dimension W on layer 1 and 2\n    PREPROCESSING_FOLDER = \'preprocessed\'\n\n\nclass Alphabet:\n    """"""\n    Class for alphabet / symbols units.\n\n    :ivar _blank_symbol: Blank symbol used for CTC\n    :vartype _blank_symbol: str\n    :ivar _alphabet_units: list of elements composing the alphabet. The units may be a single character or multiple characters.\n    :vartype _alphabet_units: List[str]\n    :ivar _codes: Each alphabet unit has a unique corresponding code.\n    :vartype _codes: List[int]\n    :ivar _nclasses: number of alphabet units.\n    :vartype _nclasses: int\n    """"""\n    def __init__(self, lookup_alphabet_file: str=None, blank_symbol: str=\'$\'):\n\n        self._blank_symbol = blank_symbol\n\n        if lookup_alphabet_file:\n            lookup_alphabet = self.load_lookup_from_json(lookup_alphabet_file)\n            # Blank symbol must have the largest value\n            if self._blank_symbol in lookup_alphabet.keys():\n\n                # TODO : check if blank symbol is the last one\n                assert lookup_alphabet[self._blank_symbol] == max(lookup_alphabet.values()), \\\n                    ""Blank symbol should have the largest code integer""\n                lookup_alphabet[self._blank_symbol] = max(lookup_alphabet.values()) + 1\n            else:\n                lookup_alphabet.update({self._blank_symbol: max(lookup_alphabet.values()) + 1})\n\n            self._alphabet_units = list(lookup_alphabet.keys())\n            self._codes = list(lookup_alphabet.values())\n            self._nclasses = len(self.codes) + 1  # n_classes should be + 1 of labels codes\n\n            if 0 in self._codes:\n                raise ValueError(\'0 code is in the lookup table, you should\'\'nt use it.\')\n\n            self.lookup_int2str = dict(zip(self.codes, self.alphabet_units))\n\n    def check_input_file_alphabet(self, csv_filenames: List[str],\n                                  discarded_chars: str=\';|{}\'.format(string.whitespace[1:]),\n                                  csv_delimiter: str="";"") -> None:\n        """"""\n        Checks if labels of input files contains only characters that are in the Alphabet.\n\n        :param csv_filenames: list of the csv filename\n        :param discarded_chars: discarded characters\n        :param csv_delimiter: character delimiting field in the csv file\n        :return:\n        """"""\n        assert isinstance(csv_filenames, list), \'csv_filenames argument is not a list\'\n\n        alphabet_set = set(self.alphabet_units)\n\n        for filename in csv_filenames:\n            input_chars_set = set()\n\n            with open(filename, \'r\', encoding=\'utf8\') as f:\n                csvreader = csv.reader(f, delimiter=csv_delimiter, escapechar=\'\\\\\', quoting=0)\n                for line in csvreader:\n                    input_chars_set.update(line[1])\n\n            # Discard all whitespaces except space \' \'\n            for whitespace in discarded_chars:\n                input_chars_set.discard(whitespace)\n\n            extra_chars = input_chars_set - alphabet_set\n            assert len(extra_chars) == 0, \'There are {} unknown chars in {} : {}\'.format(len(extra_chars),\n                                                                                         filename, extra_chars)\n\n    @classmethod\n    def map_lookup(cls, lookup_table: dict, unique_entry: bool = True) -> dict:\n        """"""\n        Converts an existing lookup table with minimal range code ([1, len(lookup_table)-1])\n        and avoids multiple instances of the same code label (bijectivity)\n\n        :param lookup_table: dictionary to be mapped {alphabet_unit : code label}\n        :param unique_entry: If each alphabet unit has a unique code and each code a unique alphabet unique (\'bijective\'),\n                            only True is implemented for now\n        :return: a mapped dictionary\n        """"""\n\n        # Create tuple (alphabet unit, code)\n        tuple_char_code = list(zip(list(lookup_table.keys()), list(lookup_table.values())))\n        # Sort by code\n        tuple_char_code.sort(key=lambda x: x[1])\n\n        # If each alphabet unit has a unique code and each code a unique alphabet unique (\'bijective\')\n        if unique_entry:\n            mapped_lookup = [[tp[0], i + 1] for i, tp in enumerate(tuple_char_code)]\n        else:\n            raise NotImplementedError\n            # Todo\n\n        return dict(mapped_lookup)\n\n    @classmethod\n    def create_lookup_from_labels(cls, csv_files: List[str], export_lookup_filename: str,\n                                  original_lookup_filename: str=None):\n        """"""\n        Create a lookup dictionary for csv files containing labels. Exports a json file with the Alphabet.\n\n        :param csv_files: list of files to get the labels from (should be of format path;label)\n        :param export_lookup_filename: filename to export alphabet lookup dictionary\n        :param original_lookup_filename: original lookup filename to update (optional)\n        :return:\n        """"""\n        if original_lookup_filename:\n            with open(original_lookup_filename, \'r\') as f:\n                lookup = json.load(f)\n            set_chars = set(list(lookup.keys()))\n        else:\n            set_chars = set(list(string.ascii_letters) + list(string.digits))\n            lookup = dict()\n\n        for filename in csv_files:\n            data = pd.read_csv(filename, sep=\';\', encoding=\'utf8\', error_bad_lines=False, header=None,\n                               names=[\'path\', \'transcription\'], escapechar=\'\\\\\')\n            for index, row in data.iterrows():\n                set_chars.update(row.transcription.split(\'|\'))\n\n        # Update (key, values) of lookup table\n        for el in set_chars:\n            if el not in lookup.keys():\n                lookup[el] = max(lookup.values()) + 1 if lookup.values() else 0\n\n        lookup = cls.map_lookup(lookup)\n\n        # Save new lookup\n        with open(export_lookup_filename, \'w\', encoding=\'utf8\') as f:\n            json.dump(lookup, f)\n\n    @classmethod\n    def load_lookup_from_json(cls, json_filenames: Union[List[str], str]) -> dict:\n        """"""\n        Load a lookup table from a json file to a dictionnary\n        :param json_filenames: either a filename or a list of filenames\n        :return:\n        """"""\n\n        lookup = dict()\n        if isinstance(json_filenames, list):\n            for file in json_filenames:\n                with open(file, \'r\', encoding=\'utf8\') as f:\n                    data_dict = json.load(f)\n                lookup.update(data_dict)\n\n        elif isinstance(json_filenames, str):\n            with open(json_filenames, \'r\', encoding=\'utf8\') as f:\n                lookup = json.load(f)\n\n        return cls.map_lookup(lookup)\n\n    @classmethod\n    def make_json_lookup_alphabet(cls, string_chars: str = None) -> dict:\n        """"""\n\n        :param string_chars: for example string.ascii_letters, string.digits\n        :return:\n        """"""\n        lookup = dict()\n        if string_chars:\n            # Add characters to lookup table\n            lookup.update({char: ord(char) for char in string_chars})\n\n        return cls.map_lookup(lookup)\n\n    @property\n    def n_classes(self):\n        return self._nclasses\n\n    @property\n    def blank_symbol(self):\n        return self._blank_symbol\n\n    @property\n    def codes(self):\n        return self._codes\n\n    @property\n    def alphabet_units(self):\n        return self._alphabet_units\n\n\nclass Params:\n    """"""\n    Class for parameters of the model and the experiment\n\n    :ivar input_shape: input shape of the image to batch (this is the shape after data augmentation).\n        The original will either be resized or pad depending on its original size\n    :vartype input_shape: Tuple[int, int]\n    :ivar input_channels: number of color channels for input image (default: 1)\n    :vartype input_channels: int\n    :ivar cnn_features_list: a list of length `n_layers` containing the number of features for each convolutionl layer\n        (default: [16, 32, 64, 96, 128])\n    :vartype cnn_features_list: List(int)\n    :ivar cnn_kernel_size: a list of length `n_layers` containing the size of the kernel for each convolutionl layer\n        (default: [3, 3, 3, 3, 3])\n    :vartype cnn_kernel_size: List(int)\n    :ivar cnn_stride_size: a list of length `n_layers` containing the stride size each convolutionl layer\n        (default: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1)])\n    :vartype cnn_stride_size: List((int, int))\n    :ivar cnn_pool_size: a list of length `n_layers` containing the pool size each MaxPool layer\n        default: ([(2, 2), (2, 2), (2, 2), (2, 2), (1, 1)])\n    :vartype cnn_pool_size: List((int, int))\n    :ivar cnn_batch_norm: a list of length `n_layers` containing a bool that indicated wether or not to use batch normalization\n        (default: [False, False, False, False, False])\n    :vartype cnn_batch_norm: List(bool)\n    :ivar rnn_units: a list containing the number of units per rnn layer (default: 256)\n    :vartype rnn_units: List(int)\n    :ivar num_beam_paths: number of paths (transcriptions) to return for ctc beam search (only used when predicting)\n    :vartype num_beam_paths: int\n    :ivar csv_delimiter: character to delimit csv input files (default: \';\')\n    :vartype csv_delimiter: str\n    :ivar string_split_delimiter: character that delimits each alphabet unit in the labels (default: \'|\')\n    :vartype string_split_delimiter: str\n    :ivar csv_files_train: csv filename which contains the (path;label) of each training sample\n    :vartype csv_files_train: str\n    :ivar csv_files_eval: csv filename which contains the (path;label) of each eval sample\n    :vartype csv_files_eval: str\n    :ivar lookup_alphabet_file: json file that contains the mapping alphabet units <-> codes\n    :vartype lookup_alphabet_file: str\n    :ivar blank_symbol: symbol for to be considered as blank by the CTC decoder (default: \'$\')\n    :vartype blank_symbol: str\n    :ivar max_chars_per_string: maximum characters per sample (to avoid CTC decoder errors) (default: 75)\n    :vartype max_chars_per_string: int\n    :ivar data_augmentation: if True augments data on the fly (default: true)\n    :vartype data_augmentation: bool\n    :ivar data_augmentation_max_rotation: max permitted roation to apply to image during training in radians (default: 0.005)\n    :vartype data_augmentation_max_rotation: float\n    :ivar data_augmentation_max_slant: maximum angle for slant augmentation (default: 0.7)\n    :vartype data_augmentation_max_slant: float\n    :ivar n_epochs: numbers of epochs to run the training (default: 50)\n    :vartype n_epochs: int\n    :ivar train_batch_size: batch size during training (default: 64)\n    :vartype train_batch_size: int\n    :ivar eval_batch_size: batch size during evaluation (default: 128)\n    :vartype eval_batch_size: int\n    :ivar learning_rate: initial learning rate (default: 1e-4)\n    :vartype learning_rate: float\n    :ivar evaluate_every_epoch: evaluate every \'evaluate_every_epoch\' epoch (default: 5)\n    :vartype evaluate_every_epoch: int\n    :ivar save_interval: save the model every \'save_interval\' epoch (default: 20)\n    :vartype save_interval: int\n    :ivar optimizer: which optimizer to use (\'adam\', \'rms\', \'ada\') (default: \'adam\')\n    :vartype optimizer: str\n    :ivar output_model_dir: output directory where the model will be saved and exported\n    :vartype output_model_dir: str\n    :ivar restore_model: boolean to continue training with saved weights (default: False)\n    :vartype restore_model: bool\n    """"""\n    def __init__(self, **kwargs):\n        # model params\n        self.input_shape = kwargs.get(\'input_shape\', (96, 1400))\n        self.input_channels = kwargs.get(\'input_channels\', 1)\n        self.cnn_features_list = kwargs.get(\'cnn_features_list\', [16, 32, 64, 96, 128])\n        self.cnn_kernel_size = kwargs.get(\'cnn_kernel_size\', [3, 3, 3, 3, 3])\n        self.cnn_stride_size = kwargs.get(\'cnn_stride_size\', [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1)])\n        self.cnn_pool_size = kwargs.get(\'cnn_pool_size\', [(2, 2), (2, 2), (2, 2), (2, 2), (1, 1)])\n        self.cnn_batch_norm = kwargs.get(\'cnn_batch_norm\', [False, False, False, False, False])\n        self.rnn_units = kwargs.get(\'rnn_units\', [256, 256])\n        # self._keep_prob_dropout = kwargs.get(\'keep_prob_dropout\', 0.5)\n        self.num_beam_paths = kwargs.get(\'num_beam_paths\', 1)\n        # csv params\n        self.csv_delimiter = kwargs.get(\'csv_delimiter\', \';\')\n        self.string_split_delimiter = kwargs.get(\'string_split_delimiter\', \'|\')\n        self.csv_files_train = kwargs.get(\'csv_files_train\')\n        self.csv_files_eval = kwargs.get(\'csv_files_eval\')\n        # alphabet params\n        self.blank_symbol = kwargs.get(\'blank_symbol\', \'$\')\n        self.max_chars_per_string = kwargs.get(\'max_chars_per_string\', 75)\n        self.lookup_alphabet_file = kwargs.get(\'lookup_alphabet_file\')\n        # data augmentation params\n        self.data_augmentation = kwargs.get(\'data_augmentation\', True),\n        self.data_augmentation_max_rotation = kwargs.get(\'data_augmentation_max_rotation\', 0.005)\n        self.data_augmentation_max_slant = kwargs.get(\'data_augmentation_max_slant\', 0.7)\n        # training params\n        self.n_epochs = kwargs.get(\'n_epochs\', 50)\n        self.train_batch_size = kwargs.get(\'train_batch_size\', 64)\n        self.eval_batch_size = kwargs.get(\'eval_batch_size\', 128)\n        self.learning_rate = kwargs.get(\'learning_rate\', 1e-4)\n        self.optimizer = kwargs.get(\'optimizer\', \'adam\')\n        self.output_model_dir = kwargs.get(\'output_model_dir\', \'\')\n        self.evaluate_every_epoch = kwargs.get(\'evaluate_every_epoch\', 5)\n        self.save_interval = kwargs.get(\'save_interval\', 20)\n        self.restore_model = kwargs.get(\'restore_model\', False)\n\n        self._assign_alphabet()\n\n        cnn_params = zip(self.cnn_pool_size, self.cnn_stride_size)\n        self.downscale_factor = reduce(lambda i, j: i * j, map(lambda k: k[0][1] * k[1][1], cnn_params))\n\n        # TODO add additional checks for the architecture\n        assert len(self.cnn_features_list) == len(self.cnn_kernel_size) == len(self.cnn_stride_size) \\\n               == len(self.cnn_pool_size) == len(self.cnn_batch_norm), \\\n            ""Length of parameters of model are not the same, check that all the layers parameters have the same length.""\n\n        max_input_width = (self.max_chars_per_string + 1) * self.downscale_factor\n        assert max_input_width <= self.input_shape[1], ""Maximum length of labels is {}, input width should be greater or "" \\\n                                                       ""equal to {} but is {}"".format(self.max_chars_per_string,\n                                                                                      max_input_width,\n                                                                                      self.input_shape[1])\n\n        assert self.optimizer in [\'adam\', \'rms\', \'ada\'], \'Unknown optimizer {}\'.format(self.optimizer)\n\n        if os.path.isdir(self.output_model_dir):\n            print(\'WARNING : The output directory {} already exists.\'.format(self.output_model_dir))\n\n    def show_experiment_params(self) -> dict:\n        """"""\n        Returns a dictionary with the variables of the class.\n\n        :return:\n        """"""\n        return vars(self)\n\n    def _assign_alphabet(self):\n        self.alphabet = Alphabet(lookup_alphabet_file=self.lookup_alphabet_file, blank_symbol=self.blank_symbol)\n\n    # @property\n    # def keep_prob_dropout(self):\n    #     return self._keep_prob_dropout\n    #\n    # @keep_prob_dropout.setter\n    # def keep_prob_dropout(self, value):\n    #     assert (0.0 < value <= 1.0), \'Must be 0.0 < value <= 1.0\'\n    #     self._keep_prob_dropout = value\n\n    def to_dict(self) -> dict:\n        """"""\n        Returns the parameters as a dictionary\n\n        :return:\n        """"""\n        new_dict = self.__dict__.copy()\n        del new_dict[\'alphabet\']\n        del new_dict[\'downscale_factor\']\n        return new_dict\n\n    @classmethod\n    def from_json_file(cls, json_file: str):\n        """"""\n        Given a json file, creates a ``Params`` object.\n\n        :param json_file: path to the json file\n        :return: ``Params`` object\n        """"""\n        with open(json_file, \'r\') as file:\n            config = json.load(file)\n\n        return cls(**config)\n\n\ndef import_params_from_json(model_directory: str=None, json_filename: str=None) -> dict:\n    """"""\n    Read the exported json file with parameters of the experiment.\n\n    :param model_directory: Direcoty where the odel was exported\n    :param json_filename: filename of the file\n    :return: a dictionary containing the parameters of the experiment\n    """"""\n\n    assert not all(p is None for p in [model_directory, json_filename]), \'One argument at least should not be None\'\n\n    if model_directory:\n        # Import parameters from the json file\n        try:\n            json_filename = glob(os.path.join(model_directory, \'model_params*.json\'))[-1]\n        except IndexError:\n            print(\'No json found in dir {}\'.format(model_directory))\n            raise FileNotFoundError\n    else:\n        if not os.path.isfile(json_filename):\n            print(\'No json found with filename {}\'.format(json_filename))\n            raise FileNotFoundError\n\n    with open(json_filename, \'r\') as data_json:\n        params_json = json.load(data_json)\n\n    # Remove \'private\' keys\n    keys = list(params_json.keys())\n    for key in keys:\n        if key[0] == \'_\':\n            params_json.pop(key)\n\n    return params_json\n'"
tf_crnn/data_handler.py,107,"b'#!/usr/bin/env python\n__author__ = \'solivr\'\n__license__ = ""GPL""\n\nimport tensorflow as tf\nfrom tensorflow_addons.image.transform_ops import rotate, transform\nfrom .config import Params, CONST\nfrom typing import Tuple, Union, List\nimport collections\n\n\n@tf.function\ndef random_rotation(img: tf.Tensor,\n                    max_rotation: float=0.1,\n                    crop: bool=True,\n                    minimum_width: int=0) -> tf.Tensor:  # adapted from SeguinBe\n    """"""\n    Rotates an image with a random angle.\n    See https://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders for formulae\n\n    :param img: Tensor\n    :param max_rotation: maximum angle to rotate (radians)\n    :param crop: boolean to crop or not the image after rotation\n    :param minimum_width: minimum width of image after data augmentation\n    :return:\n    """"""\n    with tf.name_scope(\'RandomRotation\'):\n        rotation = tf.random.uniform([], -max_rotation, max_rotation, name=\'pick_random_angle\')\n        # rotated_image = tf.contrib.image.rotate(img, rotation, interpolation=\'BILINEAR\')\n        rotated_image = rotate(tf.expand_dims(img, axis=0), rotation, interpolation=\'BILINEAR\')\n        rotated_image = tf.squeeze(rotated_image, axis=0)\n        if crop:\n            rotation = tf.abs(rotation)\n            original_shape = tf.shape(rotated_image)[:2]\n            h, w = original_shape[0], original_shape[1]\n            old_l, old_s = tf.cond(h > w, lambda: [h, w], lambda: [w, h])\n            old_l, old_s = tf.cast(old_l, tf.float32), tf.cast(old_s, tf.float32)\n            new_l = (old_l * tf.cos(rotation) - old_s * tf.sin(rotation)) / tf.cos(2*rotation)\n            new_s = (old_s - tf.sin(rotation) * new_l) / tf.cos(rotation)\n            new_h, new_w = tf.cond(h > w, lambda: [new_l, new_s], lambda: [new_s, new_l])\n            new_h, new_w = tf.cast(new_h, tf.int32), tf.cast(new_w, tf.int32)\n            bb_begin = tf.cast(tf.math.ceil((h-new_h)/2), tf.int32), tf.cast(tf.math.ceil((w-new_w)/2), tf.int32)\n            # Test sliced\n            rotated_image_crop = tf.cond(\n                tf.logical_and(bb_begin[0] < h - bb_begin[0], bb_begin[1] < w - bb_begin[1]),\n                true_fn=lambda: rotated_image[bb_begin[0]:h - bb_begin[0], bb_begin[1]:w - bb_begin[1], :],\n                false_fn=lambda: img,\n                name=\'check_slices_indices\'\n            )\n            # rotated_image_crop = rotated_image[bb_begin[0]:h - bb_begin[0], bb_begin[1]:w - bb_begin[1], :]\n\n            # If crop removes the entire image, keep the original image\n            rotated_image = tf.cond(tf.less_equal(tf.shape(rotated_image_crop)[1], minimum_width),\n                                    true_fn=lambda: img,\n                                    false_fn=lambda: rotated_image_crop,\n                                    name=\'check_size_crop\')\n\n        return rotated_image\n\n\n# def random_padding(image: tf.Tensor, max_pad_w: int=5, max_pad_h: int=10) -> tf.Tensor:\n#     """"""\n#     Given an image will pad its border adding a random number of rows and columns\n#\n#     :param image: image to pad\n#     :param max_pad_w: maximum padding in width\n#     :param max_pad_h: maximum padding in height\n#     :return: a padded image\n#     """"""\n#     # TODO specify image shape in doc\n#\n#     w_pad = list(np.random.randint(0, max_pad_w, size=[2]))\n#     h_pad = list(np.random.randint(0, max_pad_h, size=[2]))\n#     paddings = [h_pad, w_pad, [0, 0]]\n#\n#     return tf.pad(image, paddings, mode=\'REFLECT\', name=\'random_padding\')\n\n@tf.function\ndef augment_data(image: tf.Tensor,\n                 max_rotation: float=0.1,\n                 minimum_width: int=0) -> tf.Tensor:\n    """"""\n    Data augmentation on an image (padding, brightness, contrast, rotation)\n\n    :param image: Tensor\n    :param max_rotation: float, maximum permitted rotation (in radians)\n    :param minimum_width: minimum width of image after data augmentation\n    :return: Tensor\n    """"""\n    with tf.name_scope(\'DataAugmentation\'):\n\n        # Random padding\n        # image = random_padding(image)\n\n        # TODO : add random scaling\n        image = tf.image.random_brightness(image, max_delta=0.1)\n        image = tf.image.random_contrast(image, 0.5, 1.5)\n        image = random_rotation(image, max_rotation, crop=True, minimum_width=minimum_width)\n\n        if image.shape[-1] >= 3:\n            image = tf.image.random_hue(image, 0.2)\n            image = tf.image.random_saturation(image, 0.5, 1.5)\n\n        return image\n\n@tf.function\ndef get_resized_width(image: tf.Tensor,\n                      target_height: int,\n                      increment: int):\n    """"""\n    Resizes the image according to `target_height`.\n\n    :param image: image to resize\n    :param target_height: height of the resized image\n    :param increment: reduction factor due to pooling between input width and output width,\n                        this makes sure that the final width will be a multiple of increment\n    :return: resized image\n    """"""\n\n    image_shape = tf.shape(image)\n    image_ratio = tf.divide(image_shape[1], image_shape[0], name=\'ratio\')\n\n    new_width = tf.cast(tf.round((image_ratio * target_height) / increment) * increment, tf.int32)\n    f1 = lambda: (new_width, image_ratio)\n    f2 = lambda: (target_height, tf.constant(1.0, dtype=tf.float64))\n    if tf.math.less_equal(new_width, 0):\n        return f2()\n    else:\n        return f1()\n\n\n@tf.function\ndef padding_inputs_width(image: tf.Tensor,\n                         target_shape: Tuple[int, int],\n                         increment: int) -> Tuple[tf.Tensor, tf.Tensor]:\n    """"""\n    Given an input image, will pad it to return a target_shape size padded image.\n    There are 3 cases:\n         - image width > target width : simple resizing to shrink the image\n         - image width >= 0.5*target width : pad the image\n         - image width < 0.5*target width : replicates the image segment and appends it\n\n    :param image: Tensor of shape [H,W,C]\n    :param target_shape: final shape after padding [H, W]\n    :param increment: reduction factor due to pooling between input width and output width,\n                        this makes sure that the final width will be a multiple of increment\n    :return: (image padded, output width)\n    """"""\n\n    target_ratio = target_shape[1]/target_shape[0]\n    target_w = target_shape[1]\n    # Compute ratio to keep the same ratio in new image and get the size of padding\n    # necessary to have the final desired shape\n    new_h = target_shape[0]\n    new_w, ratio = get_resized_width(image, new_h, increment)\n\n    # Definitions for cases\n    def pad_fn():\n        with tf.name_scope(\'mirror_padding\'):\n            pad = tf.subtract(target_w, new_w)\n\n            img_resized = tf.image.resize(image, [new_h, new_w])\n\n            # Padding to have the desired width\n            paddings = [[0, 0], [0, pad], [0, 0]]\n            pad_image = tf.pad(img_resized, paddings, mode=\'SYMMETRIC\', name=None)\n\n            # Set manually the shape\n            pad_image.set_shape([target_shape[0], target_shape[1], img_resized.get_shape()[2]])\n\n            return pad_image, (new_h, new_w)\n\n    def replicate_fn():\n        with tf.name_scope(\'replication_padding\'):\n            img_resized = tf.image.resize(image, [new_h, new_w])\n\n            # If one symmetry is not enough to have a full width\n            # Count number of replications needed\n            n_replication = tf.cast(tf.math.ceil(target_shape[1]/new_w), tf.int32)\n            img_replicated = tf.tile(img_resized, tf.stack([1, n_replication, 1]))\n            pad_image = tf.image.crop_to_bounding_box(image=img_replicated, offset_height=0, offset_width=0,\n                                                      target_height=target_shape[0], target_width=target_shape[1])\n\n            # Set manually the shape\n            pad_image.set_shape([target_shape[0], target_shape[1], img_resized.get_shape()[2]])\n\n            return pad_image, (new_h, new_w)\n\n    def simple_resize():\n        with tf.name_scope(\'simple_resize\'):\n            img_resized = tf.image.resize(image, target_shape)\n\n            img_resized.set_shape([target_shape[0], target_shape[1], img_resized.get_shape()[2]])\n\n            return img_resized, tuple(target_shape)\n\n    # case 1 : new_w >= target_w\n    if tf.logical_and(tf.greater_equal(ratio, target_ratio), tf.greater_equal(new_w, target_w)):\n            pad_image, (new_h, new_w) =  simple_resize()\n    # case 2 : new_w >= target_w/2 & new_w < target_w & ratio < target_ratio\n    elif tf.logical_and(tf.less(ratio, target_ratio),\n                        tf.logical_and(tf.greater_equal(new_w, tf.cast(tf.divide(target_w, 2), tf.int32)),\n                                       tf.less(new_w, target_w))):\n        pad_image, (new_h, new_w) = pad_fn()\n    # case 3 : new_w < target_w/2 & new_w < target_w & ratio < target_ratio\n    elif tf.logical_and(tf.less(ratio, target_ratio),\n                        tf.logical_and(tf.less(new_w, target_w),\n                                       tf.less(new_w, tf.cast(tf.divide(target_w, 2), tf.int32)))):\n        pad_image, (new_h, new_w) = replicate_fn()\n    else:\n        pad_image, (new_h, new_w) = simple_resize()\n\n    return pad_image, new_w\n\n\n# def apply_slant(image: np.ndarray, alpha: np.ndarray) -> (np.ndarray, np.ndarray):\n#     alpha = alpha[0]\n#\n#     def _find_background_color(image: np.ndarray) -> int:\n#         """"""\n#         Given a grayscale image, finds the background color value\n#         :param image: grayscale image\n#         :return: background color value (int)\n#         """"""\n#         # Otsu\'s thresholding after Gaussian filtering\n#         blur = cv2.GaussianBlur(image[:, :, 0].astype(np.uint8), (5, 5), 0)\n#         thresh_value, thresholded_image = cv2.threshold(blur.astype(np.uint8), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n#\n#         # Find which is the background (0 or 255). Supposing that the background color occurrence is higher\n#         # than the writing color\n#         counts, bin_edges = np.histogram(thresholded_image, bins=2)\n#         background_color = int(np.median(image[thresholded_image == 255 * np.argmax(counts)]))\n#\n#         return background_color\n#\n#     shape_image = image.shape\n#     shift = max(-alpha * shape_image[0], 0)\n#     output_size = (int(shape_image[1] + np.ceil(abs(alpha * shape_image[0]))), int(shape_image[0]))\n#\n#     warpM = np.array([[1, alpha, shift], [0, 1, 0]])\n#\n#     # Find color of background in order to replicate it in the borders\n#     border_value = _find_background_color(image)\n#\n#     image_warp = cv2.warpAffine(image, np.array(warpM), output_size, borderValue=border_value)\n#\n#     return image_warp, np.array(output_size)\n\n\ndef dataset_generator(csv_filename: Union[List[str], str],\n                      params: Params,\n                      use_labels: bool=True,\n                      batch_size: int=64,\n                      data_augmentation: bool=False,\n                      num_epochs: int=None,\n                      shuffle: bool=True):\n    """"""\n    Generates the dataset for the experiment.\n\n\n    :param csv_filename: Path to csv file containing the data\n    :param params: parameters df the experiment (``Params``)\n    :param use_labels: boolean to indicate dataset generation during training / evaluation (true) or prediction (false)\n    :param batch_size: size of the generated batches\n    :param data_augmentation: whether to use data augmentation strategies or not\n    :param num_epochs: number of epochs to repeat the dataset generation\n    :param shuffle: whether to suffle the data\n    :return: ``tf.data.Dataset``\n    """"""\n    do_padding = True\n\n    if use_labels:\n        column_defaults = [[\'None\'], [\'None\'], tf.int32]\n        column_names = [\'paths\', \'label_codes\', \'label_seq_length\']\n        label_name = \'label_codes\'\n    else:\n        column_defaults = [[\'None\']]\n        column_names = [\'paths\']\n        label_name = None\n\n    num_parallel_reads = 1\n\n    # ----- from data.experimental.make_csv_dataset\n    def filename_to_dataset(filename):\n        dataset = tf.data.experimental.CsvDataset(filename,\n                                                  record_defaults=column_defaults,\n                                                  field_delim=params.csv_delimiter,\n                                                  header=False)\n        return dataset\n\n    def map_fn(*columns):\n        """"""Organizes columns into a features dictionary.\n        Args:\n          *columns: list of `Tensor`s corresponding to one csv record.\n        Returns:\n          An OrderedDict of feature names to values for that particular record. If\n          label_name is provided, extracts the label feature to be returned as the\n          second element of the tuple.\n        """"""\n        features = collections.OrderedDict(zip(column_names, columns))\n        if label_name is not None:\n            label = features.pop(label_name)\n            return features, label\n\n        return features\n\n    dataset = tf.data.Dataset.from_tensor_slices(csv_filename)\n    # Read files sequentially (if num_parallel_reads=1) or in parallel\n    # dataset = dataset.apply(tf.data.experimental.parallel_interleave(filename_to_dataset,\n    #                                                                  cycle_length=num_parallel_reads))\n    dataset = dataset.interleave(filename_to_dataset, cycle_length=num_parallel_reads,\n                                 num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    dataset = dataset.map(map_fn)\n    # -----\n\n    def _load_image(features: dict, labels=None):\n        path = features[\'paths\']\n        image_content = tf.io.read_file(path)\n        image = tf.io.decode_jpeg(image_content, channels=params.input_channels,\n                                  try_recover_truncated=True, name=\'image_decoding_op\')\n\n        if use_labels:\n            return {\'input_images\': image,\n                    \'label_seq_length\': features[\'label_seq_length\']}, labels\n        else:\n            return {\'input_images\': image,\n                    \'filename_images\': path}\n\n    def _apply_slant(features: dict, labels=None):\n        image = features[\'input_images\']\n        height_image = tf.cast(tf.shape(image)[0], dtype=tf.float32)\n\n        with tf.name_scope(\'add_slant\'):\n            alpha = tf.random.uniform([],\n                                      -params.data_augmentation_max_slant,\n                                      params.data_augmentation_max_slant,\n                                      name=\'pick_random_slant_angle\')\n\n            shiftx = tf.math.maximum(tf.math.multiply(-alpha, height_image), 0)\n\n            # Pad in order not to loose image info when transformation is applied\n            x_pad = 0\n            y_pad = tf.math.round(tf.math.ceil(tf.math.abs(tf.math.multiply(alpha, height_image))))\n            y_pad = tf.cast(y_pad, dtype=tf.int32)\n            paddings = [[x_pad, x_pad], [y_pad, 0], [0, 0]]\n            transform_matrix = [1, alpha, shiftx, 0, 1, 0, 0, 0]\n\n            # Apply transformation to image\n            image_pad = tf.pad(image, paddings)\n            image_transformed = transform(image_pad, transform_matrix, interpolation=\'BILINEAR\')\n\n            # Apply transformation to mask. The mask will be used to retrieve the pixels that have been filled\n            # with zero during transformation and update their value with background value\n            # TODO : Would be better to have some kind of binarization (i.e Otsu) and get the mean background value\n            background_pixel_value = 255\n            empty = background_pixel_value * tf.ones(tf.shape(image))\n            empty_pad = tf.pad(empty, paddings)\n            empty_transformed = tf.subtract(\n                tf.cast(background_pixel_value, dtype=tf.int32),\n                tf.cast(transform(empty_pad, transform_matrix, interpolation=\'NEAREST\'), dtype=tf.int32)\n            )\n\n            # Update additional zeros values with background_pixel_value and cast result to uint8\n            image = tf.add(tf.cast(image_transformed, dtype=tf.int32), empty_transformed)\n            image = tf.cast(image, tf.uint8)\n\n        features[\'input_images\'] = image\n        return features, labels if use_labels else features\n\n    def _data_augment_fn(features: dict, labels=None) -> tf.data.Dataset:\n        image = features[\'input_images\']\n        image = augment_data(image, params.data_augmentation_max_rotation, minimum_width=params.max_chars_per_string)\n\n        features.update({\'input_images\': image})\n        return features, labels if use_labels else features\n\n    def _pad_image_or_resize(features: dict, labels=None):\n        image = features[\'input_images\']\n        if do_padding:\n            with tf.name_scope(\'padding\'):\n                image, img_width = padding_inputs_width(image, target_shape=params.input_shape,\n                                                        increment=params.downscale_factor) # todo this needs to be updated\n        # Resize\n        else:\n            image = tf.image.resize(image, size=params.input_shape)\n            img_width = tf.shape(image)[1]\n\n        input_seq_length = tf.cast(tf.floor(tf.divide(img_width, params.downscale_factor)), tf.int32)\n        if use_labels:\n            assert_op = tf.debugging.assert_greater_equal(input_seq_length,\n                                                          features[\'label_seq_length\'])\n            with tf.control_dependencies([assert_op]):\n                return {\'input_images\': image,\n                        \'label_seq_length\': features[\'label_seq_length\'],\n                        \'input_seq_length\': input_seq_length}, labels\n        else:\n            return {\'input_images\': image,\n                    \'input_seq_length\': input_seq_length,\n                    \'filename_images\': features[\'filename_images\']}\n\n    def _normalize_image(features: dict, labels=None):\n        image = tf.cast(features[\'input_images\'], tf.float32)\n        image = tf.image.per_image_standardization(image)\n\n        features[\'input_images\'] = image\n        return features, labels if use_labels else features\n\n    def _format_label_codes(features: dict, string_label_codes):\n        splits = tf.strings.split([string_label_codes], sep=\' \')\n        label_codes = tf.squeeze(tf.strings.to_number(splits, out_type=tf.int32), axis=0)\n\n        features.update({\'label_codes\': label_codes})\n        return features, [0]\n\n\n    num_parallel_calls = tf.data.experimental.AUTOTUNE\n    #  1. load image 2. data augmentation 3. padding\n    dataset = dataset.map(_load_image, num_parallel_calls=num_parallel_calls)\n    # this causes problems when using the same cache for training, validation and prediction data...\n    # dataset = dataset.cache(filename=os.path.join(params.output_model_dir, \'cache.tf-data\'))\n    if data_augmentation and params.data_augmentation_max_slant != 0:\n        dataset = dataset.map(_apply_slant, num_parallel_calls=num_parallel_calls)\n    if data_augmentation:\n        dataset = dataset.map(_data_augment_fn, num_parallel_calls=num_parallel_calls)\n    dataset = dataset.map(_normalize_image, num_parallel_calls=num_parallel_calls)\n    dataset = dataset.map(_pad_image_or_resize, num_parallel_calls=num_parallel_calls)\n    dataset = dataset.map(_format_label_codes, num_parallel_calls=num_parallel_calls) if use_labels else dataset\n    dataset = dataset.shuffle(10 * batch_size, reshuffle_each_iteration=False) if shuffle else dataset\n    dataset = dataset.repeat(num_epochs) if num_epochs is not None else dataset\n\n    return dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n\n\n# def dataset_prediction(image_filenames: Union[List[str], str]=None,\n#                        csv_filename: str=None,\n#                        params: Params=None,\n#                        batch_size: int=64):\n#\n#     assert params, \'params cannot be None\'\n#     assert image_filenames or csv_filename, \'You need to feed an input (image_filenames or csv_filename)\'\n#\n#     do_padding = True\n#\n#     def _load_image(path):\n#         image_content = tf.io.read_file(path)\n#         image = tf.io.decode_jpeg(image_content, channels=params.input_channels,\n#                                   try_recover_truncated=True, name=\'image_decoding_op\')\n#\n#         return {\'input_images\': image}\n#\n#     def _normalize_image(features: dict):\n#         image = tf.cast(features[\'input_images\'], tf.float32)\n#         image = tf.image.per_image_standardization(image)\n#\n#         features[\'input_images\'] = image\n#         return features\n#\n#     def _pad_image_or_resize(features: dict):\n#         image = features[\'input_images\']\n#         if do_padding:\n#             with tf.name_scope(\'padding\'):\n#                 image, img_width = padding_inputs_width(image, target_shape=params.input_shape,\n#                                                         increment=CONST.DIMENSION_REDUCTION_W_POOLING)\n#         # Resize\n#         else:\n#             image = tf.image.resize(image, size=params.input_shape)\n#             img_width = tf.shape(image)[1]\n#\n#         input_seq_length = tf.cast(tf.floor(tf.math.divide(img_width, params.n_pool)), tf.int32)\n#\n#         return {\'input_images\': image,\n#                 \'input_seq_length\': input_seq_length}\n#     if image_filenames is not None:\n#         dataset = tf.data.Dataset.from_tensor_slices(image_filenames)\n#     elif csv_filename is not None:\n#         column_defaults = [[\'None\']]\n#         dataset = tf.data.experimental.CsvDataset(csv_filename,\n#                                                   record_defaults=column_defaults,\n#                                                   field_delim=params.csv_delimiter,\n#                                                   header=False)\n#         # dataset = dataset.map(map_fn)\n#     dataset = dataset.map(_load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n#     dataset = dataset.map(_normalize_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n#     dataset = dataset.map(_pad_image_or_resize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n#\n#     return dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n'"
tf_crnn/model.py,26,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.backend import ctc_batch_cost, ctc_decode\nfrom tensorflow.keras.layers import Layer, Conv2D, BatchNormalization, MaxPool2D, Input, Permute, \\\n    Reshape, Bidirectional, LSTM, Dense, Softmax, Lambda\nfrom typing import List, Tuple\nfrom .config import Params\n\n\nclass ConvBlock(Layer):\n    """"""\n    Convolutional block class.\n    It is composed of a `Conv2D` layer, a `BatchNormaization` layer (optional),\n    a `MaxPool2D` layer (optional) and a `ReLu` activation.\n\n    :ivar features: number of features of the convolutional layer\n    :vartype features: int\n    :ivar kernel_size: size of the convolutional kernel\n    :vartype kernel_size: int\n    :ivar stride: stride of the convolutional layer\n    :vartype stride: int, int\n    :ivar cnn_padding: padding of the convolution (\'same\' or \'valid\')\n    :vartype cnn_padding:\n    :ivar pool_size: size of the maxpooling\n    :vartype pool_size: int, int\n    :ivar batchnorm: use batch norm or not\n    :vartype batchnorm: bool\n    """"""\n    def __init__(self,\n                 features: int,\n                 kernel_size: int,\n                 stride: Tuple[int, int],\n                 cnn_padding: str,\n                 pool_size: Tuple[int, int],\n                 batchnorm: bool,\n                 **kwargs):\n        super(ConvBlock, self).__init__(**kwargs)\n        self.conv = Conv2D(features,\n                           kernel_size,\n                           strides=stride,\n                           padding=cnn_padding)\n        self.bn = BatchNormalization(renorm=True,\n                                     renorm_clipping={\'rmax\': 1e2, \'rmin\': 1e-1, \'dmax\': 1e1},\n                                     trainable=True) if batchnorm else None\n        self.pool = MaxPool2D(pool_size=pool_size,\n                              padding=\'same\') if list(pool_size) > [1, 1] else None\n\n        # for config purposes\n        self._features = features\n        self._kernel_size = kernel_size\n        self._stride = stride\n        self._cnn_padding = cnn_padding\n        self._pool_size = pool_size\n        self._batchnorm = batchnorm\n\n    def call(self, inputs, training=False):\n        x = self.conv(inputs)\n        if self.bn is not None:\n            x = self.bn(x, training=training)\n        if self.pool is not None:\n            x = self.pool(x)\n        x = tf.nn.relu(x)\n        return x\n\n    def get_config(self) -> dict:\n        """"""\n        Get a dictionary with all the necessary properties to recreate the same layer.\n\n        :return: dictionary containing the properties of the layer\n        """"""\n        super_config = super(ConvBlock, self).get_config()\n        config = {\n            \'features\': self._features,\n            \'kernel_size\': self._kernel_size,\n            \'stride\': self._stride,\n            \'cnn_padding\': self._cnn_padding,\n            \'pool_size\': self._pool_size,\n            \'batchnorm\': self._batchnorm\n        }\n        return dict(list(super_config.items()) + list(config.items()))\n\n\ndef get_crnn_output(input_images,\n                    parameters: Params=None) -> tf.Tensor:\n    """"""\n    Creates the CRNN network and returns it\'s output.\n    Passes the `input_images` through the network and returns its output\n\n    :param input_images: images to process (B, H, W, C)\n    :param parameters: parameters of the model (``Params``)\n    :return: the output of the CRNN model\n    """"""\n\n    # params of the architecture\n    cnn_features_list = parameters.cnn_features_list\n    cnn_kernel_size = parameters.cnn_kernel_size\n    cnn_pool_size = parameters.cnn_pool_size\n    cnn_stride_size = parameters.cnn_stride_size\n    cnn_batch_norm = parameters.cnn_batch_norm\n    rnn_units = parameters.rnn_units\n\n    # CNN layers\n    cnn_params = zip(cnn_features_list, cnn_kernel_size, cnn_stride_size, cnn_pool_size, cnn_batch_norm)\n    conv_layers = [ConvBlock(ft, ks, ss, \'same\', psz, bn) for ft, ks, ss, psz, bn in cnn_params]\n\n    x = conv_layers[0](input_images)\n    for conv in conv_layers[1:]:\n        x = conv(x)\n\n    # Permutation and reshape\n    x = Permute((2, 1, 3))(x)\n    shape = x.get_shape().as_list()\n    x = Reshape((shape[1], shape[2] * shape[3]))(x)  # [B, W, H*C]\n\n    # RNN layers\n    rnn_layers = [Bidirectional(LSTM(ru, dropout=0.5, return_sequences=True, time_major=False)) for ru in\n                  rnn_units]\n    for rnn in rnn_layers:\n        x = rnn(x)\n\n    # Dense and softmax\n    x = Dense(parameters.alphabet.n_classes)(x)\n    net_output = Softmax()(x)\n\n    return net_output\n\n\ndef get_model_train(parameters: Params):\n    """"""\n    Constructs the full model for training.\n    Defines inputs and outputs, loss function and metric (CER).\n\n    :param parameters: parameters of the model (``Params``)\n    :return: the model (``tf.Keras.Model``)\n    """"""\n\n    h, w = parameters.input_shape\n    c = parameters.input_channels\n\n    input_images = Input(shape=(h, w, c), name=\'input_images\')\n    input_seq_len = Input(shape=[1], dtype=tf.int32, name=\'input_seq_length\')\n\n    label_codes = Input(shape=(parameters.max_chars_per_string), dtype=tf.int32, name=\'label_codes\')\n    label_seq_length = Input(shape=[1], dtype=tf.int32, name=\'label_seq_length\')\n\n    net_output = get_crnn_output(input_images, parameters)\n\n    # Loss function\n    def warp_ctc_loss(y_true, y_pred):\n        return ctc_batch_cost(label_codes, y_pred, input_seq_len, label_seq_length)\n\n    # Metric function\n    def warp_cer_metric(y_true, y_pred):\n        pred_sequence_length, true_sequence_length = input_seq_len, label_seq_length\n\n        # y_pred needs to be decoded (its the logits)\n        pred_codes_dense = ctc_decode(y_pred, tf.squeeze(pred_sequence_length, axis=-1), greedy=True)\n        pred_codes_dense = tf.squeeze(tf.cast(pred_codes_dense[0], tf.int64), axis=0)  # only [0] if greedy=true\n\n        # create sparse tensor\n        idx = tf.where(tf.not_equal(pred_codes_dense, -1))\n        pred_codes_sparse = tf.SparseTensor(tf.cast(idx, tf.int64),\n                                            tf.gather_nd(pred_codes_dense, idx),\n                                            tf.cast(tf.shape(pred_codes_dense), tf.int64))\n\n        idx = tf.where(tf.not_equal(label_codes, 0))\n        label_sparse = tf.SparseTensor(tf.cast(idx, tf.int64),\n                                       tf.gather_nd(label_codes, idx),\n                                       tf.cast(tf.shape(label_codes), tf.int64))\n        label_sparse = tf.cast(label_sparse, tf.int64)\n\n        # Compute edit distance and total chars count\n        distance = tf.reduce_sum(tf.edit_distance(pred_codes_sparse, label_sparse, normalize=False))\n        count_chars = tf.reduce_sum(true_sequence_length)\n\n        return tf.divide(distance, tf.cast(count_chars, tf.float32), name=\'CER\')\n\n    # Define model and compile it\n    model = Model(inputs=[input_images, label_codes, input_seq_len, label_seq_length], outputs=net_output, name=\'CRNN\')\n    optimizer = tf.keras.optimizers.Adam(learning_rate=parameters.learning_rate)\n    model.compile(loss=[warp_ctc_loss],\n                  optimizer=optimizer,\n                  metrics=[warp_cer_metric],\n                  experimental_run_tf_function=False) # TODO this is set to true by default but does not seem to work...\n\n    return model\n\n\ndef get_model_inference(parameters: Params,\n                        weights_path: str=None):\n    """"""\n    Constructs the full model for prediction.\n    Defines inputs and outputs, and loads the weights.\n\n\n    :param parameters: parameters of the model (``Params``)\n    :param weights_path: path to the weights (.h5 file)\n    :return: the model (``tf.Keras.Model``)\n    """"""\n    h, w = parameters.input_shape\n    c = parameters.input_channels\n\n    input_images = Input(shape=(h, w, c), name=\'input_images\')\n    input_seq_len = Input(shape=[1], dtype=tf.int32, name=\'input_seq_length\')\n    filename_images = Input(shape=[1], dtype=tf.string, name=\'filename_images\')\n\n    net_output = get_crnn_output(input_images, parameters)\n    output_seq_len = tf.identity(input_seq_len)  # need this op to pass it to output\n    filenames = tf.identity(filename_images)\n\n    model = Model(inputs=[input_images, input_seq_len, filename_images], outputs=[net_output, output_seq_len, filenames])\n\n    if weights_path:\n        model.load_weights(weights_path)\n\n    return model\n'"
tf_crnn/preprocessing.py,1,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nimport re\nimport numpy as np\nimport os\nfrom .config import Params, CONST\nimport pandas as pd\nfrom typing import List, Tuple\nfrom taputapu.io.image import get_image_shape_without_loading\n\n\ndef _convert_label_to_dense_codes(labels: List[str],\n                                  split_char: str,\n                                  max_width: int,\n                                  table_str2int: dict):\n    """"""\n    Converts a list of formatted string to a dense matrix of codes\n\n    :param labels: list of strings containing formatted labels\n    :param split_char: character to split the formatted label\n    :param max_width: maximum length of string label (max_n_chars = max_width_dense_codes)\n    :param table_str2int: mapping table between alphabet units and alphabet codes\n    :return: dense matrix N x max_width, list of the lengths of each string (length N)\n    """"""\n    labels_chars = [[c for c in label.split(split_char) if c] for label in labels]\n    codes_list = [[table_str2int[c] for c in list_char] for list_char in labels_chars]\n\n    seq_lengths = [len(cl) for cl in codes_list]\n\n    dense_codes = list()\n    for ls in codes_list:\n        dense_codes.append(ls + np.maximum(0, (max_width - len(ls))) * [0])\n\n    return dense_codes, seq_lengths\n\n\ndef _compute_length_inputs(path: str,\n                           target_shape: Tuple[int, int]):\n\n    w, h = get_image_shape_without_loading(path)\n    ratio = w / h\n\n    new_h = target_shape[0]\n    new_w = np.minimum(new_h * ratio, target_shape[1])\n\n    return new_w\n\n\ndef preprocess_csv(csv_filename: str,\n                   parameters: Params,\n                   output_csv_filename: str) -> int:\n    """"""\n    Converts the original csv data to the format required by the experiment.\n    Removes the samples which labels have too many characters. Computes the widths of input images and removes the\n    samples which have more characters per label than image width. Converts the string labels to dense codes.\n    The output csv file contains the path to the image, the dense list of codes corresponding to the alphabets units\n    (which are padded with 0 if `len(label)` < `max_len`) and the length of the label sequence.\n\n    :param csv_filename: path to csv file\n    :param parameters: parameters of the experiment (``Params``)\n    :param output_csv_filename: path to the output csv file\n    :return: number of samples in the output csv file\n    """"""\n\n    # Conversion table\n    table_str2int = dict(zip(parameters.alphabet.alphabet_units, parameters.alphabet.codes))\n\n    # Read file\n    dataframe = pd.read_csv(csv_filename,\n                            sep=parameters.csv_delimiter,\n                            header=None,\n                            names=[\'paths\', \'labels\'],\n                            encoding=\'utf8\',\n                            escapechar=""\\\\"",\n                            quoting=0)\n\n    original_len = len(dataframe)\n\n    dataframe[\'label_string\'] = dataframe.labels.apply(lambda x: re.sub(re.escape(parameters.string_split_delimiter), \'\', x))\n    dataframe[\'label_len\'] = dataframe.label_string.apply(lambda x: len(x))\n\n    # remove long labels\n    dataframe = dataframe[dataframe.label_len <= parameters.max_chars_per_string]\n\n    # Compute width images (after resizing)\n    dataframe[\'input_length\'] = dataframe.paths.apply(lambda x: _compute_length_inputs(x, parameters.input_shape))\n    dataframe.input_length = dataframe.input_length.apply(lambda x: np.floor(x / parameters.downscale_factor))\n    # Remove items with longer label than input\n    dataframe = dataframe[dataframe.label_len < dataframe.input_length]\n\n    final_length = len(dataframe)\n\n    n_removed = original_len - final_length\n    if n_removed > 0:\n        print(\'-- Removed {} samples ({:.2f} %)\'.format(n_removed,\n                                                        100 * n_removed / original_len))\n\n    # Convert fields to list\n    paths = dataframe.paths.to_list()\n    labels = dataframe.labels.to_list()\n\n    # Convert string labels to dense codes\n    label_dense_codes, label_seq_length = _convert_label_to_dense_codes(labels,\n                                                                        parameters.string_split_delimiter,\n                                                                        parameters.max_chars_per_string,\n                                                                        table_str2int)\n    # format in string to be easily parsed by tf.data\n    string_label_codes = [[str(ldc) for ldc in list_ldc] for list_ldc in label_dense_codes]\n    string_label_codes = [\' \'.join(list_slc) for list_slc in string_label_codes]\n\n    data = {\'paths\': paths, \'label_codes\': string_label_codes, \'label_len\': label_seq_length}\n    new_dataframe = pd.DataFrame(data)\n\n    new_dataframe.to_csv(output_csv_filename,\n                         sep=parameters.csv_delimiter,\n                         header=False,\n                         encoding=\'utf8\',\n                         index=False,\n                         escapechar=""\\\\"",\n                         quoting=0)\n    return len(new_dataframe)\n\n\ndef data_preprocessing(params: Params) -> (str, str, int, int):\n    """"""\n    Preporcesses the data for the experiment (training and evaluation data).\n    Exports the updated csv files into `<output_model_dir>/preprocessed/updated_{eval,train}.csv`\n\n    :param params: parameters of the experiment (``Params``)\n    :return: output path files, number of samples (for train and evaluation data)\n    """"""\n    output_dir = os.path.join(params.output_model_dir, CONST.PREPROCESSING_FOLDER)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    else:\n        \'Output directory {} already exists\'.format(output_dir)\n\n    csv_train_output = os.path.join(output_dir, \'updated_train.csv\')\n    csv_eval_output = os.path.join(output_dir, \'updated_eval.csv\')\n\n    # Preprocess train csv\n    n_samples_train = preprocess_csv(params.csv_files_train, params, csv_train_output)\n\n    # Preprocess train csv\n    n_samples_eval = preprocess_csv(params.csv_files_eval, params, csv_eval_output)\n\n    return csv_train_output, csv_eval_output, n_samples_train, n_samples_eval\n\n'"
