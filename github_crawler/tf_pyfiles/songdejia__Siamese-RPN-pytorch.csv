file_path,api_count,code
code/__init__.py,0,b''
code/data_loader.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-11-09 17:22:06\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-11-21 16:58:12\nimport sys\nimport os\nimport os.path as osp\nimport time\nimport cv2\nimport torch\nimport random\nfrom PIL import Image, ImageOps, ImageStat, ImageDraw\nfrom torchvision import datasets, transforms, utils\nimport numpy as np\ndef get_transform_for_train():\n    transform_list = []\n\n    transform_list.append(transforms.ToTensor())\n    \n    transform_list.append(transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5)))\n    \n    return transforms.Compose(transform_list)\n\nclass Anchor_ms():\n    def __init__(self,feature_w,feature_h):\n        self.w = feature_w\n        self.h = feature_h\n        self.base   = 64 #target area in detection is about 3000 \n        self.stride = 15\n        self.scale  = [1/3, 1/2, 1, 2, 3]\n        self.width  = 256\n        self.height = 256\n        self.anchors=self.gen_anchors()#xywh\n\n    def gen_single_anchor(self):\n        # return corner(inside or outside)\n        scale=np.array(self.scale, dtype = np.float32)\n        s=self.base * self.base\n        w=np.sqrt(s/scale)\n        h=w*scale\n        #c_x=(self.base-1)//2\n        #c_y=(self.base-1)//2\n        c_x=(self.stride-1)//2\n        c_y=(self.stride-1)//2\n        anchor=np.vstack([c_x*np.ones_like(scale, dtype=np.float32),c_y*np.ones_like(scale, dtype=np.float32),w,h])\n        anchor=anchor.transpose()           #[x,y,w,h]\n        anchor=self.center_to_corner(anchor).astype(np.int32)#[x1,y1,x2,y2]\n        return anchor\n\n    def gen_anchors(self):\n        anchor=self.gen_single_anchor()\n        k=anchor.shape[0]\n        shift_x=[x*self.stride for x in range(self.w)]\n        shift_y=[y*self.stride for y in range(self.h)]\n        shift_x,shift_y=np.meshgrid(shift_x,shift_y) #(17, 17) (17, 17)\n        shifts=np.vstack([shift_x.ravel(),shift_y.ravel(),shift_x.ravel(),shift_y.ravel()]).transpose()#(289, 4)\n        a=shifts.shape[0]\n        anchors=anchor.reshape((1,k,4))+shifts.reshape((a,1,4)) # corner\n        anchors=anchors.reshape((a*k,4))#[x1,y1,x2,y2]\n        anchors=self.corner_to_center(anchors).astype(np.float32)#[x,y,w,h]\n        return anchors\n\n    def diff_anchor_gt(self, gt):\n        #gt [x,y,w,h]\n        #anchors=self.gen_anchors()#[x,y,w,h]\n        eps = 0.01\n        anchors = self.anchors.copy()\n        gt = gt.copy()\n        diff = np.zeros_like(anchors, dtype = np.float32)\n        diff[:,0] = (gt[0] - anchors[:,0])/(anchors[:,2] + eps)\n        diff[:,1] = (gt[1] - anchors[:,1])/(anchors[:,3] + eps)\n        diff[:,2] = np.log((gt[2] + eps)/(anchors[:,2] + eps))\n        diff[:,3] = np.log((gt[3] + eps)/(anchors[:,3] + eps))\n        return diff#[dx,dy,dw,dh]\n\n    def center_to_corner(self, box):\n        box_ = np.zeros_like(box, dtype = np.float32)\n        box_[:,0]=box[:,0]-(box[:,2]-1)/2\n        box_[:,1]=box[:,1]-(box[:,3]-1)/2\n        box_[:,2]=box[:,0]+(box[:,2]-1)/2\n        box_[:,3]=box[:,1]+(box[:,3]-1)/2\n        box_ = box_.astype(np.int32)\n        return box_\n\n    def corner_to_center(self, box):\n        box = box.copy()\n        box_ = np.zeros_like(box, dtype = np.float32)\n        box_[:,0]=box[:,0]+(box[:,2]-box[:,0])/2\n        box_[:,1]=box[:,1]+(box[:,3]-box[:,1])/2\n        box_[:,2]=(box[:,2]-box[:,0])\n        box_[:,3]=(box[:,3]-box[:,1])\n        box_ = box_.astype(np.int32)\n        return box_\n\n    def pos_neg_anchor(self, gt):\n        gt = gt.copy()\n        gt_corner = self.center_to_corner(np.array(gt, dtype = np.float32).reshape(1, 4))\n        an_corner = self.center_to_corner(np.array(self.anchors, dtype = np.float32))\n        iou_value = self.iou(an_corner, gt_corner).reshape(-1) #(1445)\n        max_iou = max(iou_value)\n        pos, neg = np.zeros_like(iou_value), np.zeros_like(iou_value)\n        pos_index = np.argsort(iou_value)[::-1][:16]\n\n        neg_cand = np.where(iou_value < 0.2)[0]\n        neg_ind = np.random.choice(neg_cand, 48, replace = False)\n        if max_iou > 0.3:\n            pos[pos_index] = 1\n\n        neg[neg_ind] = 1\n        return pos, neg        \n\n    def iou(self, box1, box2):\n        box1, box2 = box1.copy(), box2.copy()\n        N=box1.shape[0]\n        K=box2.shape[0]\n        box1=np.array(box1.reshape((N,1,4)))+np.zeros((1,K,4))#box1=[N,K,4]\n        box2=np.array(box2.reshape((1,K,4)))+np.zeros((N,1,4))#box1=[N,K,4]\n        x_max=np.max(np.stack((box1[:,:,0],box2[:,:,0]),axis=-1),axis=2)\n        x_min=np.min(np.stack((box1[:,:,2],box2[:,:,2]),axis=-1),axis=2)\n        y_max=np.max(np.stack((box1[:,:,1],box2[:,:,1]),axis=-1),axis=2)\n        y_min=np.min(np.stack((box1[:,:,3],box2[:,:,3]),axis=-1),axis=2)\n        tb=x_min-x_max\n        lr=y_min-y_max\n        tb[np.where(tb<0)]=0\n        lr[np.where(lr<0)]=0\n        over_square=tb*lr\n        all_square=(box1[:,:,2]-box1[:,:,0])*(box1[:,:,3]-box1[:,:,1])+(box2[:,:,2]-box2[:,:,0])*(box2[:,:,3]-box2[:,:,1])-over_square\n        return over_square/all_square\n\nclass TrainDataLoader(object):\n    def __init__(self, img_dir_path, out_feature = 17, max_inter = 5, check = False, tmp_dir = \'../tmp/visualization\'):\n        self.anchor_generator = Anchor_ms(out_feature, out_feature)\n        self.img_dir_path = img_dir_path # this is a root dir contain subclass\n        self.max_inter = max_inter\n        self.sub_class_dir = [sub_class_dir for sub_class_dir in os.listdir(img_dir_path) if os.path.isdir(os.path.join(img_dir_path, sub_class_dir))] \n        self.anchors = self.anchor_generator.gen_anchors() #centor\n        self.ret = {}\n        self.check = check\n        self.tmp_dir = self.init_dir(tmp_dir)\n        self.count = 0\n        self.ret[\'tmp_dir\'] = tmp_dir\n\n    def init_dir(self, tmp_dir):\n        if not osp.exists(tmp_dir):\n            os.makedirs(tmp_dir)\n        return tmp_dir\n\n    def _pick_img_pairs(self, index_of_subclass):\n        """"""\n        img_dir_path -> sub_class_dir_path -> template_img_path\n        """"""\n        assert index_of_subclass < len(self.sub_class_dir), \'index_of_subclass should less than total classes\'\n        sub_class_dir_basename = self.sub_class_dir[index_of_subclass]\n        sub_class_dir_path = os.path.join(self.img_dir_path, sub_class_dir_basename)\n        sub_class_img_name = [img_name for img_name in os.listdir(sub_class_dir_path) if not img_name.find(\'.jpg\') == -1]        \n        sub_class_img_name = sorted(sub_class_img_name)\n        sub_class_img_num = len(sub_class_img_name)\n        sub_class_gt_name  = \'groundtruth.txt\'\n\n        # select template, detection\n        # template_index = random.choice(range(0, sub_class_img_num - self.max_inter))\n        # detection_index= random.choice(range(self.max_inter)) + template_index\n        template_index  = 0\n        detection_index = template_index + 1\n\n        template_name, detection_name  = sub_class_img_name[template_index], sub_class_img_name[detection_index]\n        template_img_path, detection_img_path = osp.join(sub_class_dir_path, template_name), osp.join(sub_class_dir_path, detection_name)\n        gt_path = osp.join(sub_class_dir_path, sub_class_gt_name)\n        with open(gt_path, \'r\') as f:\n            lines = f.readlines()\n        \n        self.ret[\'template_img_path\']    = template_img_path\n        self.ret[\'detection_img_path\']   = detection_img_path\n        self.ret[\'template_target_x1y1wh\'] = [int(float(i)) for i in lines[template_index].strip(\'\\n\').split(\',\')]\n        self.ret[\'detection_target_x1y1wh\']= [int(float(i)) for i in lines[detection_index].strip(\'\\n\').split(\',\')]\n        t1, t2 = self.ret[\'template_target_x1y1wh\'].copy(), self.ret[\'detection_target_x1y1wh\'].copy()\n        self.ret[\'template_target_xywh\'] = [t1[0]+t1[2]//2, t1[1]+t1[3]//2, t1[2], t1[3]]\n        self.ret[\'detection_target_xywh\']= [t2[0]+t2[2]//2, t2[1]+t2[3]//2, t2[2], t2[3]]\n        self.ret[\'anchors\'] = self.anchors\n        self._average()\n\n        if self.check:\n            s = osp.join(self.tmp_dir, \'0_check_label\')\n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            template = Image.open(self.ret[\'template_img_path\'])\n            x, y, w, h = self.ret[\'template_target_xywh\'].copy()\n            x1, y1, x3, y3 = x-w//2, y-h//2, x+w//2, y+h//2 \n            draw = ImageDraw.Draw(template)\n            draw.line([(x1, y1), (x3, y1), (x3, y3), (x1, y3), (x1, y1)], width=1, fill=\'red\')\n            save_path = osp.join(s,\'idx_{:04d}_class_{}_template_idx_{}.jpg\'.format(self.count, sub_class_dir_basename, template_index))\n            template.save(save_path)\n\n            detection = Image.open(self.ret[\'detection_img_path\'])\n            x, y, w, h = self.ret[\'detection_target_xywh\'].copy()\n            x1, y1, x3, y3 = x-w//2, y-h//2, x+w//2, y+h//2 \n            draw = ImageDraw.Draw(detection)\n            draw.line([(x1, y1), (x3, y1), (x3, y3), (x1, y3), (x1, y1)], width=1, fill=\'red\')\n            save_path = osp.join(s,\'idx_{:04d}_class_{}_detection_idx_{}.jpg\'.format(self.count, sub_class_dir_basename, detection_index))\n            detection.save(save_path)\n\n        \n    def _average(self):\n        assert self.ret.__contains__(\'template_img_path\'), \'no template path\'\n        assert self.ret.__contains__(\'detection_img_path\'),\'no detection path\'\n        template = Image.open(self.ret[\'template_img_path\'])\n        detection= Image.open(self.ret[\'detection_img_path\'])\n        \n        mean_template = tuple(map(round, ImageStat.Stat(template).mean))\n        mean_detection= tuple(map(round, ImageStat.Stat(detection).mean))\n        self.ret[\'mean_template\'] = mean_template\n        self.ret[\'mean_detection\']= mean_detection\n\n    def _pad_crop_and_resize(self):\n        template_img_path = self.ret[\'template_img_path\']\n        template_img = Image.open(template_img_path)\n        detection_img_path= self.ret[\'detection_img_path\']\n        detection_img = Image.open(detection_img_path)\n\n        w, h = template_img.size\n        cx, cy, tw, th = self.ret[\'template_target_xywh\']\n        p = round((tw + th)/2, 2)\n        template_square_size  = int(np.sqrt((tw + p)*(th + p))) #a\n        detection_square_size = int(template_square_size * 2)   #A\n        \n        # pad\n        detection_lt_x, detection_lt_y = cx - detection_square_size//2, cy - detection_square_size//2\n        detection_rb_x, detection_rb_y = cx + detection_square_size//2, cy + detection_square_size//2\n        left   = -detection_lt_x if detection_lt_x < 0 else 0\n        top    = -detection_lt_y if detection_lt_y < 0 else 0\n        right  =  detection_rb_x - w if detection_rb_x > w else 0\n        bottom =  detection_rb_y - h if detection_rb_y > h else 0\n        padding = (int(left), int(top), int(right), int(bottom))\n\n        self.ret[\'new_template_img_padding\'] = ImageOps.expand(template_img,  border=padding, fill=self.ret[\'mean_template\'])\n        self.ret[\'new_detection_img_padding\']= ImageOps.expand(detection_img, border=padding, fill=self.ret[\'mean_detection\'])\n        new_w, new_h = left + right + w, top + bottom + h\n            \n        # crop part\n        ## template part\n        tl = cx + left - template_square_size//2\n        tt = cy + top  - template_square_size//2\n        tr = new_w - tl - template_square_size\n        tb = new_h - tt - template_square_size\n        self.ret[\'template_cropped\'] = ImageOps.crop(self.ret[\'new_template_img_padding\'], (tl, tt, tr, tb))\n        #self.ret[\'template_cropped\'].save(\'/home/songyu/djsong/srpn/srpn/tmp/visualization/tmp/{}_0_template_.jpg\'.format(self.count))\n\n        ## detection part\n        dl = np.clip(cx + left - detection_square_size//2, 0, new_w - detection_square_size)\n        dt = np.clip(cy + top  - detection_square_size//2, 0, new_h - detection_square_size)\n        dr = np.clip(new_w - dl - detection_square_size, 0, new_w - detection_square_size)\n        db = np.clip(new_h - dt - detection_square_size, 0, new_h - detection_square_size ) \n        self.ret[\'detection_cropped\']= ImageOps.crop(self.ret[\'new_detection_img_padding\'],(dl, dt, dr, db))  \n        #self.ret[\'detection_cropped\'].save(\'/home/songyu/djsong/srpn/srpn/tmp/visualization/tmp/{}_1_detection.jpg\'.format(self.count))\n\n        self.ret[\'detection_tlcords_of_original_image\'] = (cx - detection_square_size//2 , cy - detection_square_size//2)\n        self.ret[\'detection_tlcords_of_padding_image\']  = (cx - detection_square_size//2 + left, cy - detection_square_size//2 + top)\n        self.ret[\'detection_rbcords_of_padding_image\']  = (cx + detection_square_size//2 + left, cy + detection_square_size//2 + top)\n        \n        # resize\n        self.ret[\'template_cropped_resized\'] = self.ret[\'template_cropped\'].copy().resize((127, 127))\n        self.ret[\'detection_cropped_resized\']= self.ret[\'detection_cropped\'].copy().resize((256, 256))\n        self.ret[\'template_cropprd_resized_ratio\'] = round(127/template_square_size, 2)\n        self.ret[\'detection_cropped_resized_ratio\'] = round(256/detection_square_size, 2)\n        \n        # compute target in detection, and then we will compute IOU\n        # whether target in detection part\n        x, y, w, h = self.ret[\'detection_target_xywh\']\n        self.ret[\'target_tlcords_of_padding_image\'] = (x+left-w//2, y+top-h//2)\n        self.ret[\'target_rbcords_of_padding_image\'] = (x+left+w//2, y+top+h//2)\n        if self.check:\n            # \xe5\x9c\xa8 padding\xe5\x9b\xbe\xe4\xb8\x8a\xe4\xbd\x9c\xe5\x87\xba\xe5\x90\x84\xe9\x83\xa8\xe5\x88\x86\n            s = osp.join(self.tmp_dir, \'1_padding_img_with_detection_and_target\')\n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            im = self.ret[\'new_detection_img_padding\']\n            draw = ImageDraw.Draw(im)\n            x1, y1 = self.ret[\'target_tlcords_of_padding_image\']\n            x2, y2 = self.ret[\'target_rbcords_of_padding_image\']\n            draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\') # target in padding\n\n            x1, y1 = self.ret[\'detection_tlcords_of_padding_image\']\n            x2, y2 = self.ret[\'detection_rbcords_of_padding_image\']\n            draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'green\') # detection in padding\n\n            save_path = osp.join(s, \'{:04d}.jpg\'.format(self.count))\n            im.save(save_path) \n\n        ### use cords about padding to compute cords about detection \n        ### modify cords because not all the object in the detection\n        x11, y11 = self.ret[\'detection_tlcords_of_padding_image\']\n        x12, y12 = self.ret[\'detection_rbcords_of_padding_image\']\n        x21, y21 = self.ret[\'target_tlcords_of_padding_image\']\n        x22, y22 = self.ret[\'target_rbcords_of_padding_image\']\n        x1_of_d = x21 - x11\n        y1_of_d = y21 - y11\n        x3_of_d = x22 - x11\n        y3_of_d = y22 - y11\n        x1 = np.clip(x1_of_d, 0, x12-x11).astype(np.int32)\n        y1 = np.clip(y1_of_d, 0, y12-y11).astype(np.int32)\n        x2 = np.clip(x3_of_d, 0, x12-x11).astype(np.int32)\n        y2 = np.clip(y3_of_d, 0, y12-y11).astype(np.int32)\n        self.ret[\'target_in_detection_x1y1x2y2\']=[x1, y1, x2, y2]\n        if self.check:\n            #\xe7\x94\xbb\xe5\x87\xbadetection\xe5\x9b\xbe\n            s = osp.join(self.tmp_dir, \'2_cropped_detection\')\n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            im = self.ret[\'detection_cropped\'].copy()\n            draw = ImageDraw.Draw(im)\n            draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')\n            save_path = osp.join(s, \'{:04d}.jpg\'.format(self.count))\n            im.save(save_path)\n\n        cords_in_cropped_detection = np.array((x1, y1, x2, y2))\n        cords_in_cropped_resized_detection = (cords_in_cropped_detection * self.ret[\'detection_cropped_resized_ratio\']).astype(np.int32)\n        x1, y1, x2, y2 = cords_in_cropped_resized_detection\n        cx, cy, w, h = (x1+x2)//2, (y1+y2)//2, x2-x1, y2-y1\n        self.ret[\'target_in_resized_detection_x1y1x2y2\'] = np.array((x1, y1, x2, y2)).astype(np.int32)\n        self.ret[\'target_in_resized_detection_xywh\'] = np.array((cx, cy, w, h)).astype(np.int32)\n        self.ret[\'area_target_in_resized_detection\'] = w * h\n\n        if self.check:\n            #\xe7\x94\xbb\xe5\x87\xbaresized detection\xe5\x9b\xbe\n            s = osp.join(self.tmp_dir, \'3_resized_detection\')\n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            im = self.ret[\'detection_cropped_resized\'].copy()\n            draw = ImageDraw.Draw(im)\n            draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')\n            save_path = osp.join(s, \'{:04d}.jpg\'.format(self.count))\n            im.save(save_path)\n\n    def _generate_pos_neg_diff(self):\n        gt_box_in_detection = self.ret[\'target_in_resized_detection_xywh\'].copy()\n        pos, neg = self.anchor_generator.pos_neg_anchor(gt_box_in_detection) #mask\n        diff     = self.anchor_generator.diff_anchor_gt(gt_box_in_detection)\n       \n        pos, neg, diff = pos.reshape((-1, 1)), neg.reshape((-1,1)), diff.reshape((-1, 4))\n        class_target = np.array([-100.] * self.anchors.shape[0]) \n        \n        pos_index = np.where(pos == 1)[0]\n\n        self.ret[\'pos_anchors\'] = np.array(self.ret[\'anchors\'][pos_index, :], dtype=np.int32)\n        pos_index = np.where(pos == 1)[0]\n        pos_num = len(pos_index)\n        if pos_num == 16:\n            class_target[pos_index] = 1\n        class_target[np.where(neg == 1)[0]] = 0 #pos 1 neg 0 ignore -100\n\n        # draw pos and neg anchor box\n        if self.check:\n            s = osp.join(self.tmp_dir, \'4_pos_neg_anchors\')\n            if not os.path.exists(s):\n                os.makedirs(s)\n            \n            pos = pos.squeeze()\n            neg = neg.squeeze()\n            pos_index = np.array(np.where(pos == 1)).reshape(-1)\n            neg_index = np.array(np.where(neg == 1)).reshape(-1)\n            if len(pos_index) != 16 and len(pos_index) != 0:\n                sys.exit(0)\n            im = self.ret[\'detection_cropped_resized\'].copy()\n            draw = ImageDraw.Draw(im)\n            """"""\n            if len(pos_index) == 16:\n                for i in range(16):\n                    index = pos_index[i]\n                    cx ,cy, w, h = self.anchors[index]\n                    if w == 0 or h == 0:\n                        print(\'w h 0\')\n                        sys.exit(0) \n                    x1, y1, x2, y2 = int(cx-w/2), int(cy-h/2), int(cx+w/2), int(cy+h/2)\n                    draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')\n            for i in range(48):\n                index = neg_index[i]\n                cx ,cy, w, h = self.anchors[index]\n                x1, y1, x2, y2 = int(cx-w/2), int(cy-h/2), int(cx+w/2), int(cy+h/2)\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'green\')  \n                #print(\'{:02d} neg {:02d} cords {} {} {} {}\'.format(count, i, cx ,cy, w, h))\n            """"""\n            save_path = osp.join(s, \'{:04d}.jpg\'.format(self.count))\n            im.save(save_path)\n        \n        \n        if self.check:\n            s = osp.join(self.tmp_dir, \'5_all_anchors\') \n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            for i in range(self.anchors.shape[0]):\n                x1, y1, x2, y2 = self.ret[\'target_in_resized_detection_x1y1x2y2\']\n                im = self.ret[\'detection_cropped_resized\']\n                draw = ImageDraw.Draw(im)\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')\n\n                cx, cy, w, h = self.anchors[i]\n                x1, y1, x2, y2 = cx-w//2,cy-h//2,cx+w//2,cy+h//2\n                draw = ImageDraw.Draw(im)\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'green\')\n                save_path = osp.join(s, \'img_{:04d}_anchor_{:05d}.jpg\'.format(self.count, i))\n                im.save(save_path)\n         \n\n        """""" \n            pos = pos.squeeze()\n            neg = neg.squeeze()\n            print(pos.shape)\n            pos_index = np.where(pos != 0)\n            print(pos_index)\n            #sys.exit(0)\n            pos_anchors = self.anchors[pos_index]\n            neg_anchors = self.anchors[neg_index]\n            for i in range(pos_anchors.shape[0]):\n                cx ,cy, w, h = pos_anchors[i]\n                x1, y1, x2, y2 = cx-w//2,cy-h//2,cx+w//2,cy+h//2\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')\n\n            for i in range(neg_anchors.shape[0]):\n                cx ,cy, w, h = pos_anchors[i]\n                x1, y1, x2, y2 = cx-w//2,cy-h//2,cx+w//2,cy+h//2\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'green\')  \n                \n            save_path = osp.join(s, \'{:04d}.jpg\'.format(self.count))\n            im.save(save_path)\n        """"""\n            \n\n        class_logits = class_target.reshape(-1, 1)\n        pos_neg_diff = np.hstack((class_logits, diff))\n        #print(pos_neg_diff[pos_index])\n        #print(pos_neg_diff[neg_index])\n        return pos_neg_diff\n\n    def _tranform(self):\n        """"""PIL to Tensor""""""\n        template_pil = self.ret[\'template_cropped_resized\'].copy()\n        detection_pil= self.ret[\'detection_cropped_resized\'].copy()\n        pos_neg_diff = self.ret[\'pos_neg_diff\'].copy()\n\n        transform = get_transform_for_train()\n        template_tensor = transform(template_pil)\n        detection_tensor= transform(detection_pil)\n        self.ret[\'template_tensor\'] = template_tensor.unsqueeze(0)\n        self.ret[\'detection_tensor\']= detection_tensor.unsqueeze(0)\n        self.ret[\'pos_neg_diff_tensor\'] = torch.Tensor(pos_neg_diff)\n\n\n    def __get__(self, index):\n        self._pick_img_pairs(index) #ok\n        self._pad_crop_and_resize()\n        self.ret[\'pos_neg_diff\'] = self._generate_pos_neg_diff()\n        self._tranform()\n        self.count += 1\n        return self.ret\n    \n\n\n    def __len__(self):\n        return len(self.sub_class_dir)\n\nclass TestDataLoader(object):\n    def __init__(self, img_dir_path, out_feature = 17, max_inter = 100, check = False, tmp_dir = \'../tmp/visualization\'):\n        self.anchor_generator = Anchor_ms(out_feature, out_feature)\n        self.img_dir_path = img_dir_path\n        self.max_inter = max_inter\n        self.sub_class_dir = [sub_class_dir for sub_class_dir in os.listdir(img_dir_path) if os.path.isdir(os.path.join(img_dir_path, sub_class_dir))] \n        self.anchors = self.anchor_generator.gen_anchors() #ok\n        self.ret = {}\n        self.check = check\n        self.tmp_dir = tmp_dir \n        self.count = 0\n        if not osp.exists(self.tmp_dir):\n            os.makedirs(self.tmp_dir)\n\n\n\n    def _pick_img_pairs(self, index_of_subclass):\n\n        assert index_of_subclass < len(self.sub_class_dir), \'index_of_subclass should less than total classes\'\n        sub_class_dir_basename = self.sub_class_dir[index_of_subclass]\n        sub_class_dir_path = os.path.join(self.img_dir_path, sub_class_dir_basename)\n        sub_class_img_name = [img_name for img_name in os.listdir(sub_class_dir_path) if not img_name.find(\'.jpg\') == -1]        \n        sub_class_img_name = sorted(sub_class_img_name)\n        sub_class_img_num = len(sub_class_img_name)\n        sub_class_gt_name  = \'groundtruth.txt\'\n\n        # select template, detection\n        template_index = random.choice(range(0, sub_class_img_num - self.max_inter))\n        detection_index= random.choice(range(self.max_inter)) + template_index\n        template_name  = sub_class_img_name[template_index]\n        detection_name = sub_class_img_name[detection_index]\n        template_img_path  = os.path.join(sub_class_dir_path, template_name)\n        detection_img_path = os.path.join(sub_class_dir_path, detection_name)\n        gt_path = osp.join(sub_class_dir_path, sub_class_gt_name)\n        with open(gt_path, \'r\') as f:\n            lines = f.readlines()\n        self.ret[\'template_img_path\']    = template_img_path\n        self.ret[\'detection_img_path\']   = detection_img_path\n        self.ret[\'template_target_x1y1wh\'] = [float(i) for i in lines[template_index].strip(\'\\n\').split(\',\')]\n        self.ret[\'detection_target_x1y1wh\']= [float(i) for i in lines[detection_index].strip(\'\\n\').split(\',\')]\n        t1, t2 = self.ret[\'template_target_x1y1wh\'], self.ret[\'detection_target_x1y1wh\']\n        self.ret[\'template_target_xywh\'] = t1[0]+t1[2]//2, t1[1]+t1[3]//2, t1[2], t1[3]\n        self.ret[\'detection_target_xywh\']= t2[0]+t2[2]//2, t2[1]+t2[3]//2, t2[2], t2[3]\n        self.ret[\'anchors\'] = self.anchors\n        if self.check:\n            s = osp.join(self.tmp_dir, \'0_check_label\')\n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            template = Image.open(self.ret[\'template_img_path\'])\n            x, y, w, h = self.ret[\'template_target_xywh\']\n            x1, y1, x3, y3 = x-w//2, y-h//2, x+w//2, y+h//2 \n            draw = ImageDraw.Draw(template)\n            draw.line([(x1, y1), (x3, y1), (x3, y3), (x1, y3), (x1, y1)], width=1, fill=\'red\')\n            save_path = osp.join(s,\'idx_{:04d}_class_{}_template_idx_{}.jpg\'.format(self.count, sub_class_dir_basename, template_index))\n            template.save(save_path)\n\n            detection = Image.open(self.ret[\'detection_img_path\'])\n            x, y, w, h = self.ret[\'detection_target_xywh\']\n            x1, y1, x3, y3 = x-w//2, y-h//2, x+w//2, y+h//2 \n            draw = ImageDraw.Draw(detection)\n            draw.line([(x1, y1), (x3, y1), (x3, y3), (x1, y3), (x1, y1)], width=1, fill=\'red\')\n            save_path = osp.join(s,\'idx_{:04d}_class_{}_detection_idx_{}.jpg\'.format(self.count, sub_class_dir_basename, detection_index))\n            detection.save(save_path)\n\n        self._average()\n        \n    def _average(self):\n        assert self.ret.__contains__(\'template_img_path\'), \'no template path\'\n        assert self.ret.__contains__(\'detection_img_path\'),\'no detection path\'\n        template = Image.open(self.ret[\'template_img_path\'])\n        detection= Image.open(self.ret[\'detection_img_path\'])\n        \n        mean_template = tuple(map(round, ImageStat.Stat(template).mean))\n        mean_detection= tuple(map(round, ImageStat.Stat(detection).mean))\n\n        self.ret[\'mean_template\'] = (mean_template[0], mean_template[1], mean_template[2])\n        self.ret[\'mean_detection\']= (mean_detection[0],mean_detection[1],mean_detection[2])\n\n    def _pad_crop_and_resize(self):\n        template_img_path = self.ret[\'template_img_path\']\n        template_img = Image.open(template_img_path)\n        detection_img_path= self.ret[\'detection_img_path\']\n        detection_img = Image.open(detection_img_path)\n\n        w, h = template_img.size\n        cx, cy, tw, th = self.ret[\'template_target_xywh\']\n        p = round((tw + th)/2, 2)\n        template_square_size = np.sqrt((tw + p)*(th + p)) #a\n        detection_square_size = template_square_size * 2  #A\n        \n        # pad\n        detection_lt_x, detection_lt_y = cx - detection_square_size//2, cy - detection_square_size//2\n        detection_rb_x, detection_rb_y = cx + detection_square_size//2, cy + detection_square_size//2\n        left   = -detection_lt_x if detection_lt_x < 0 else 0\n        top    = -detection_lt_y if detection_lt_y < 0 else 0\n        right  =  detection_rb_x - w if detection_rb_x > w else 0\n        bottom =  detection_rb_y - h if detection_rb_y > h else 0\n        padding = (int(left), int(top), int(right), int(bottom))\n\n        self.ret[\'new_template_img_padding\'] = ImageOps.expand(template_img,  border=padding, fill=self.ret[\'mean_template\'])\n        self.ret[\'new_detection_img_padding\']= ImageOps.expand(detection_img, border=padding, fill=self.ret[\'mean_detection\'])\n        new_w, new_h = left + right + w, top + bottom + h\n            \n        # crop part\n        ## template part\n        tl = cx + left - template_square_size//2\n        tt = cy + top  - template_square_size//2\n        tr = new_w - tl - template_square_size\n        tb = new_h - tt - template_square_size\n        self.ret[\'template_cropped\'] = ImageOps.crop(self.ret[\'new_template_img_padding\'], (tl, tt, tr, tb))\n        #self.ret[\'template_cropped\'].save(\'/home/songyu/djsong/srpn/srpn/tmp/visualization/tmp/{}_0_template_.jpg\'.format(self.count))\n\n        ## detection part\n        dl = cx + left - detection_square_size//2\n        dt = cy + top  - detection_square_size//2\n        dr = new_w - dl - detection_square_size\n        db = new_h - dt - detection_square_size \n        self.ret[\'detection_cropped\']= ImageOps.crop(self.ret[\'new_detection_img_padding\'],(dl, dt, dr, db))  \n        #self.ret[\'detection_cropped\'].save(\'/home/songyu/djsong/srpn/srpn/tmp/visualization/tmp/{}_1_detection.jpg\'.format(self.count))\n\n        self.ret[\'detection_tlcords_of_original_image\'] = (cx - detection_square_size//2 , cy - detection_square_size//2)\n        self.ret[\'detection_tlcords_of_padding_image\']  = (cx - detection_square_size//2 + left, cy - detection_square_size//2 + top)\n        self.ret[\'detection_rbcords_of_padding_image\']  = (cx + detection_square_size//2 + left, cy + detection_square_size//2 + top)\n        self.ret[\'template_cropped_resized\'] = self.ret[\'template_cropped\'].resize((127, 127))\n        self.ret[\'detection_cropped_resized\']= self.ret[\'detection_cropped\'].resize((256, 256))\n        self.ret[\'template_cropprd_resized_ratio\'] = round(127/template_square_size, 2)\n        self.ret[\'detection_cropped_resized_ratio\'] = round(256/detection_square_size, 2)\n        \n        # compute target in detection, and then we will compute IOU\n        # whether target in detection part\n        x, y, w, h = self.ret[\'detection_target_xywh\']\n        self.ret[\'target_tlcords_of_padding_image\'] = (x+left-w//2, y+top-h//2)\n        self.ret[\'target_rbcords_of_padding_image\'] = (x+left+w//2, y+top+h//2)\n        if self.check:\n            # \xe5\x9c\xa8 padding\xe5\x9b\xbe\xe4\xb8\x8a\xe4\xbd\x9c\xe5\x87\xba\xe5\x90\x84\xe9\x83\xa8\xe5\x88\x86\n            s = osp.join(self.tmp_dir, \'1_padding_img_with_detection_and_target\')\n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            im = self.ret[\'new_detection_img_padding\']\n            draw = ImageDraw.Draw(im)\n            x1, y1 = self.ret[\'target_tlcords_of_padding_image\']\n            x2, y2 = self.ret[\'target_rbcords_of_padding_image\']\n            draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\') # target in padding\n\n            x1, y1 = self.ret[\'detection_tlcords_of_padding_image\']\n            x2, y2 = self.ret[\'detection_rbcords_of_padding_image\']\n            draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'green\') # detection in padding\n\n            save_path = osp.join(s, \'{:04d}.jpg\'.format(self.count))\n            im.save(save_path) \n\n        ### use cords about padding to compute cords about detection \n        x11, y11 = self.ret[\'detection_tlcords_of_padding_image\']\n        x12, y12 = self.ret[\'detection_rbcords_of_padding_image\']\n        x21, y21 = self.ret[\'target_tlcords_of_padding_image\']\n        x22, y22 = self.ret[\'target_rbcords_of_padding_image\']\n        x1_of_d = x21 - x11\n        y1_of_d = y21 - y11\n        x3_of_d = x22 - x11\n        y3_of_d = y22 - y11\n        x1 = np.clip(x1_of_d, 0, x12-x11)\n        y1 = np.clip(y1_of_d, 0, y12-y11)\n        x2 = np.clip(x3_of_d, 0, x12-x11)\n        y2 = np.clip(y3_of_d, 0, y12-y11)\n        if self.check:\n            #\xe7\x94\xbb\xe5\x87\xbadetection\xe5\x9b\xbe\n            s = osp.join(self.tmp_dir, \'2_cropped_detection\')\n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            im = self.ret[\'detection_cropped\']\n            draw = ImageDraw.Draw(im)\n            draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')\n            save_path = osp.join(s, \'{:04d}.jpg\'.format(self.count))\n            im.save(save_path)\n\n        cords_in_cropped_detection = np.array((x1, y1, x2, y2))\n        cords_in_cropped_resized_detection = (cords_in_cropped_detection * self.ret[\'detection_cropped_resized_ratio\']).astype(np.int32)\n        x1, y1, x2, y2 = cords_in_cropped_resized_detection\n        cx, cy, w, h = (x1+x2)//2, (y1+y2)//2, x2-x1, y2-y1\n        self.ret[\'target_in_resized_detection_x1y1x2y2\'] = np.array((x1, y1, x2, y2)).astype(np.int32)\n        self.ret[\'target_in_resized_detection_xywh\'] = np.array((cx, cy, w, h)).astype(np.int32)\n        self.ret[\'area_target_in_resized_detection\'] = w * h\n\n        if self.check:\n            #\xe7\x94\xbb\xe5\x87\xbaresized detection\xe5\x9b\xbe\n            s = osp.join(self.tmp_dir, \'3_resized_detection\')\n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            im = self.ret[\'detection_cropped_resized\']\n            draw = ImageDraw.Draw(im)\n            draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')\n            save_path = osp.join(s, \'{:04d}.jpg\'.format(self.count))\n            im.save(save_path)\n\n    def _generate_pos_neg_diff(self):\n        anchors = self.anchors\n        gt_box_in_detection = self.ret[\'target_in_resized_detection_xywh\']\n        pos, neg = self.anchor_generator.pos_neg_ahchor(gt_box_in_detection, anchors) #mask\n        diff = self.anchor_generator.diff_anchor_gt(gt_box_in_detection, anchors)\n       \n        pos, neg, diff = pos.reshape((-1, 1)), neg.reshape((-1,1)), diff.reshape((-1, 4)) # 5120\n        class_target = np.array([-100.] * self.anchors.shape[0]).reshape((-1,1)) #5120\n        class_target[np.where(pos == 1)] = 1\n        class_target[np.where(neg == 1)] = 0 #pos 1 neg 0 ignore -100\n        class_target = class_target.reshape(-1)\n\n        # draw pos and neg anchor box\n        if self.check:\n            s = osp.join(self.tmp_dir, \'4_pos_neg_anchors\')\n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            im = self.ret[\'detection_cropped_resized\']\n            draw = ImageDraw.Draw(im)\n            \n            pos = pos.squeeze()\n            neg = neg.squeeze()\n            pos_index = np.array(np.where(pos == 1)).reshape(-1)\n            neg_index = np.array(np.where(neg == 1)).reshape(-1)\n            count = 0\n            for i in range(16):\n                if pos_index.shape[0] == 0:\n                    break\n                index = pos_index[i]\n                cx ,cy, w, h = self.anchors[index]\n                x1, y1, x2, y2 = int(cx-w/2), int(cy-h/2), int(cx+w/2), int(cy+h/2)\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')\n                #print(\'{:02d} pos {:02d} cords {} {} {} {}\'.format(count, i, cx ,cy, w, h))\n\n            for i in range(48):\n                index = neg_index[i]\n                cx ,cy, w, h = self.anchors[index]\n                x1, y1, x2, y2 = int(cx-w/2), int(cy-h/2), int(cx+w/2), int(cy+h/2)\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'green\')  \n                #print(\'{:02d} neg {:02d} cords {} {} {} {}\'.format(count, i, cx ,cy, w, h))\n\n            save_path = osp.join(s, \'{:04d}.jpg\'.format(self.count))\n            im.save(save_path)\n        \n\n        if self.check:\n            s = osp.join(self.tmp_dir, \'5_all_anchors\') \n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            for i in range(self.anchors.shape[0]):\n                x1, y1, x2, y2 = self.ret[\'target_in_resized_detection_x1y1x2y2\']\n                im = self.ret[\'detection_cropped_resized\']\n                draw = ImageDraw.Draw(im)\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')\n\n                cx, cy, w, h = self.anchors[i]\n                x1, y1, x2, y2 = cx-w//2,cy-h//2,cx+w//2,cy+h//2\n                draw = ImageDraw.Draw(im)\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'green\')\n                save_path = osp.join(s, \'img_{:04d}_anchor_{:05d}.jpg\'.format(self.count, i))\n                im.save(save_path)\n            \n\n        """""" \n            pos = pos.squeeze()\n            neg = neg.squeeze()\n            print(pos.shape)\n            pos_index = np.where(pos != 0)\n            print(pos_index)\n            #sys.exit(0)\n            pos_anchors = self.anchors[pos_index]\n            neg_anchors = self.anchors[neg_index]\n            for i in range(pos_anchors.shape[0]):\n                cx ,cy, w, h = pos_anchors[i]\n                x1, y1, x2, y2 = cx-w//2,cy-h//2,cx+w//2,cy+h//2\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')\n\n            for i in range(neg_anchors.shape[0]):\n                cx ,cy, w, h = pos_anchors[i]\n                x1, y1, x2, y2 = cx-w//2,cy-h//2,cx+w//2,cy+h//2\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'green\')  \n                \n            save_path = osp.join(s, \'{:04d}.jpg\'.format(self.count))\n            im.save(save_path)\n        """"""\n            \n\n        class_logits = class_target.reshape(-1, 1)\n        pos_neg_diff = np.hstack((class_logits, diff))\n        return pos_neg_diff\n\n    def _tranform(self):\n        """"""PIL to Tensor""""""\n        template_pil = self.ret[\'template_cropped_resized\']\n        detection_pil= self.ret[\'detection_cropped_resized\']\n        pos_neg_diff = self.ret[\'pos_neg_diff\']\n\n        transform = get_transform_for_train()\n        template_tensor = transform(template_pil)\n        detection_tensor= transform(detection_pil)\n        self.ret[\'template_tensor\'] = template_tensor.unsqueeze(0)\n        self.ret[\'detection_tensor\']= detection_tensor.unsqueeze(0)\n        self.ret[\'pos_neg_diff_tensor\'] = torch.Tensor(pos_neg_diff)\n\n    def __get__(self, index):\n        self._pick_img_pairs(index) #ok\n        #self._pad_crop_and_resize()\n        #self.ret[\'pos_neg_diff\'] = self._generate_pos_neg_diff()\n        #self._tranform()\n        #self.count += 1\n        return self.ret\n\n    def __len__(self):\n        return len(self.sub_class_dir)\n\ndef compute_average_value(img_path):\n    """"""\n    compute average value of several channels\n    """"""\n    img = cv2.imread(img_path)\n    w, h, c = img.shape\n    num_pix = w * h\n    avg = [np.sum(img[:, :, i])/num_pix for i in range(c)]\n    return avg\n\nif __name__ == \'__main__\':\n    # we will do a test for dataloader\n    loader = TrainDataLoader(\'/home/song/srpn/dataset/simple_vot13\', check = True)\n    #print(loader.__len__())\n    index_list = range(loader.__len__())\n    for i in range(1000):\n        ret = loader.__get__(random.choice(index_list))\n        label = ret[\'pos_neg_diff\'][:, 0].reshape(-1)\n        pos_index = list(np.where(label == 1)[0])\n        pos_num = len(pos_index)\n        print(pos_index)\n        print(pos_num)\n        if pos_num != 0 and pos_num != 16:\n            print(pos_num)\n            sys.exit(0)\n        print(i)\n\n\n\n'"
code/net.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-11-05 11:16:24\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-11-19 22:11:49\nimport torch.utils.model_zoo as model_zoo\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport time\nmodel_urls = {\'alexnet\': \'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\'}\nclass SiameseRPN(nn.Module):\n    def __init__(self):\n        super(SiameseRPN, self).__init__()\n        self.features = nn.Sequential(                  #1, 3, 256, 256\n            nn.Conv2d(3, 64, kernel_size=11, stride=2), #1, 64,123, 123\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),      #1, 64, 60,  60\n            nn.Conv2d(64, 192, kernel_size=5),          #1,192, 56,  56\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),      #1,192, 27,  27\n            nn.Conv2d(192, 384, kernel_size=3),         #1,384, 25,  25 \n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3),         #1,256, 23,  23\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3),         #1,256, 21,  21\n        )\n        \n        self.k = 5\n        self.s = 4\n        self.conv1 = nn.Conv2d(256, 2*self.k*256, kernel_size=3)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(256, 4*self.k*256, kernel_size=3)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(256, 256, kernel_size=3)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.conv4 = nn.Conv2d(256, 256, kernel_size=3)\n        self.relu4 = nn.ReLU(inplace=True)\n\n        self.cconv = nn.Conv2d(256, 2* self.k, kernel_size = 4, bias = False)\n        self.rconv = nn.Conv2d(256, 4* self.k, kernel_size = 4, bias = False)\n        \n        #self.reset_params()\n        \n    def reset_params(self):\n        pretrained_dict = model_zoo.load_url(model_urls[\'alexnet\'])\n        model_dict = self.state_dict()\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        model_dict.update(pretrained_dict)\n        self.load_state_dict(model_dict)\n        print(\'Load Alexnet models Done\' )\n            \n    def forward(self, template, detection):\n        """"""\n        \xe6\x8a\x8atemplate\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab,\xe5\x9d\x90\xe6\xa0\x87\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe4\xbd\x9c\xe4\xb8\xba\xe6\xa3\x80\xe6\xb5\x8bcconv\xe5\x92\x8crconv\xe7\x9a\x84\xe6\xa3\x80\xe6\xb5\x8b\xe5\x99\xa8\n        \xe6\x8a\x8ackernel, rkernel\xe8\xbd\xac\xe6\x8d\xa2\xe5\x88\xb0cconv, rconv\n        """"""\n        template = self.features(template)            \n        detection = self.features(detection)         \n        \n        ckernal = self.conv1(template)\n        ckernal = ckernal.view(2* self.k, 256, 4, 4)\n        cinput  = self.conv3(detection)                \n\n\n        rkernal = self.conv2(template)\n        rkernal = rkernal.view(4* self.k, 256, 4, 4)\n        rinput  = self.conv4(detection)\n\n        coutput = F.conv2d(cinput, ckernal)\n        routput = F.conv2d(rinput, rkernal) \n        """"""\n        print(\'++++++++++++++++++++++++++++++++++++++++++++++++++\')\n        print(\'c branch conv1 template  weight\', self.conv1.weight[0,0,0,0])\n        print(\'c branch conv3 detection weight\', self.conv3.weight[0,0,0,0])\n        """"""\n        return coutput, routput\n\n    def resume(self, weight):\n        checkpoint = torch.load(weight)\n        self.load_state_dict(checkpoint)\n        print(\'Resume checkpoint\')\n\n\nclass SiameseRPN_bn(nn.Module):\n    def __init__(self):\n        super(SiameseRPN_bn, self).__init__()\n        self.features = nn.Sequential(                  #1, 3, 256, 256\n            nn.Conv2d(3, 64, kernel_size=11, stride=2), #1, 64,123, 123\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),      #1, 64, 60,  60\n            nn.Conv2d(64, 192, kernel_size=5),          #1,192, 56,  56\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),      #1,192, 27,  27\n            nn.Conv2d(192, 384, kernel_size=3),         #1,384, 25,  25 \n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3),         #1,256, 23,  23\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3),         #1,256, 21,  21\n            nn.BatchNorm2d(256),\n        )\n        \n        self.k = 5\n        self.s = 4\n        self.conv1 = nn.Conv2d(256, 2*self.k*256, kernel_size=3)\n        self.bn1   = nn.BatchNorm2d(2*self.k*256)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(256, 4*self.k*256, kernel_size=3)\n        self.bn2   = nn.BatchNorm2d(4*self.k*256)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(256, 256, kernel_size=3)\n        self.bn3   = nn.BatchNorm2d(256)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.conv4 = nn.Conv2d(256, 256, kernel_size=3)\n        self.bn4   = nn.BatchNorm2d(256)\n        self.relu4 = nn.ReLU(inplace=True)\n\n        self.cconv = nn.Conv2d(256, 2* self.k, kernel_size = 4, bias = False)\n        self.rconv = nn.Conv2d(256, 4* self.k, kernel_size = 4, bias = False)\n        \n        #self.reset_params()\n        \n    def reset_params(self):\n        pretrained_dict = model_zoo.load_url(model_urls[\'alexnet\'])\n        model_dict = self.state_dict()\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        model_dict.update(pretrained_dict)\n        self.load_state_dict(model_dict)\n        print(\'Load Alexnet models Done\' )\n            \n    def forward(self, template, detection):\n        """"""\n        \xe6\x8a\x8atemplate\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab,\xe5\x9d\x90\xe6\xa0\x87\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe4\xbd\x9c\xe4\xb8\xba\xe6\xa3\x80\xe6\xb5\x8bcconv\xe5\x92\x8crconv\xe7\x9a\x84\xe6\xa3\x80\xe6\xb5\x8b\xe5\x99\xa8\n        \xe6\x8a\x8ackernel, rkernel\xe8\xbd\xac\xe6\x8d\xa2\xe5\x88\xb0cconv, rconv\n        """"""\n        template = self.features(template)            #\n        detection = self.features(detection)          #21\n        \n        ckernal = self.bn1(self.conv1(template))\n        ckernal = ckernal.view(2* self.k, 256, 4, 4)\n        cinput  = self.bn3(self.conv3(detection))                #21 -> 19\n\n\n        rkernal = self.bn2(self.conv2(template))\n        rkernal = rkernal.view(4* self.k, 256, 4, 4)\n        rinput  = self.bn4(self.conv4(detection))\n\n        coutput = F.conv2d(cinput, ckernal)\n        routput = F.conv2d(rinput, rkernal) \n        """"""\n        print(\'++++++++++++++++++++++++++++++++++++++++++++++++++\')\n        print(\'c branch conv1 template  weight\', self.conv1.weight[0,0,0,0])\n        print(\'c branch conv3 detection weight\', self.conv3.weight[0,0,0,0])\n        """"""\n        return coutput, routput\n\n    def resume(self, weight):\n        checkpoint = torch.load(weight)\n        self.load_state_dict(checkpoint)\n        print(\'Resume checkpoint\')\n\nif __name__ == \'__main__\':\n    model = SiameseRPN()\n\n    template = torch.ones((1, 3, 127, 127))\n    detection= torch.ones((1, 3, 256, 256))\n\n    y1, y2 = model(template, detection)\n    print(y1.shape) #[1, 10, 17, 17]\n    print(y2.shape) #[1, 20, 17, 17]15\n'"
code/otb_SiamRPN.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-11-05 16:04:00\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-11-08 16:38:20\nimport sys\nimport cv2  # imread\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport scipy.io as scio\nimport os\nimport time\nfrom os.path import realpath, dirname, join\nfrom net import SiamRPNBIG\nfrom run_SiamRPN import SiamRPN_init, SiamRPN_track\nfrom utils import get_axis_aligned_bbox, cxy_wh_2_rect\n\n# load net\nnet_file = join(realpath(dirname(__file__)), \'SiamRPNBIG.model\')\nnet = SiamRPNBIG()\nnet.load_state_dict(torch.load(net_file))\nnet.eval().cuda()\n\nOTB100_path = \'/home/song/srpn/dataset/otb100\'\nresult_path = \'/home/song/srpn/result/\'\n\n\n# warm up\nfor i in range(10):\n\tnet.temple(torch.autograd.Variable(torch.FloatTensor(1, 3, 127, 127)).cuda())\n\tnet(torch.autograd.Variable(torch.FloatTensor(1, 3, 255, 255)).cuda())\nidx=0\n\nnames = [name for name in os.listdir(\'/home/song/srpn/dataset/otb100\')]\n# human4\n# skating2\n""""""\nfor i, name in enumerate(names):\n\tprint(\'idx == {:03d} name == {:10}\'.format(i, name))\n""""""\nfor ids, x in enumerate(os.walk(OTB100_path)):\n\tit1, it2, it3 = x #it1 **/img   it2 []  it3 [img1, img2, ...] \n\tif it1.rfind(\'img\')!=-1 and len(it3) > 50:#Python rfind() \xe8\xbf\x94\xe5\x9b\x9e\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe6\xac\xa1\xe5\x87\xba\xe7\x8e\xb0\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae(\xe4\xbb\x8e\xe5\x8f\xb3\xe5\x90\x91\xe5\xb7\xa6\xe6\x9f\xa5\xe8\xaf\xa2)\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\xb2\xa1\xe6\x9c\x89\xe5\x8c\xb9\xe9\x85\x8d\xe9\xa1\xb9\xe5\x88\x99\xe8\xbf\x94\xe5\x9b\x9e-1\xe3\x80\x82\n\t\tname = it1.split(\'/\')[-2]\n\t\timgpath=[]\n\t\tit3 = sorted(it3)\n\t\tfor inames in it3:\n\t\t\timgpath.append(os.path.join(it1, inames))\n\n\t\tgtpath=os.path.join(OTB100_path, name, \'groundtruth_rect.txt\')\n\t\tgt=(open(gtpath, \'r\')).readline()\n\t\tif gt.find(\',\')!=-1:\n\t\t\ttoks=map(float, gt.split(\',\'))\n\t\telse:\n\t\t\ttoks=map(float, gt.split(\'\t\'))\n\n\t\t# ground truth\xe6\x98\xaf\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe7\x82\xb9\xe5\x92\x8cw,h\n\t\t# target_pos \xe7\x9b\xae\xe6\xa0\x87\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\n\t\t# target_sz  w,h\n\t\tcx=toks[0]+toks[2]*0.5\n\t\tcy=toks[1]+toks[3]*0.5\n\t\tw=toks[2]\n\t\th=toks[3]\n\t\ttarget_pos, target_sz = np.array([cx, cy]), np.array([w, h])\n\t\t\n\t\t# \xe7\xac\xac\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe4\xbd\x9c\xe4\xb8\xba\xe6\xa8\xa1\xe7\x89\x88\n\t\tim = cv2.imread(imgpath[0])\n\t\tstate = SiamRPN_init(im, target_pos, target_sz, net)\n\t\tbbox=[];\n\t\ttotalframe=len(imgpath)\n\t\tttime=0\n\t\tsave_template_dir = os.path.join(\'/home/song/srpn/tmp\', name)\n\t\tif not os.path.exists(save_template_dir):\n\t\t\tos.makedirs(save_template_dir)\n\t\tsave_template_file = os.path.join(\'/home/song/srpn/tmp\', name, \'000_a_template.jpg\')\n\t\ttemplate = state[\'template\'].astype(np.int32)\n\t\tcv2.imwrite(save_template_file, template)\n\t\tprint(\'save template at {}\'.format(save_template_file))\n\n\t\t# \xe7\xac\xac\xe4\xba\x8c\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x9b\xb4\xe5\x88\xb0\xe7\xbb\x93\xe5\xb0\xbe\xe4\xbd\x9c\xe4\xb8\xba\xe6\x90\x9c\xe7\xb4\xa2\xe5\x9b\xbe\n\t\tif len(imgpath) == 1:\n\t\t\tprint(name + \'have only one img\')\n\t\t\tcontinue\n\n\t\tfor ids, imgs in enumerate(imgpath[1:]):\n\t\t\t""""""\n\t\t\tthe index img to detect\n\t\t\t""""""\n\t\t\tim = cv2.imread(imgs)\n\t\t\tttime1=time.time()\n\t\t\tstate = SiamRPN_track(state, im, ids, name)  # track, ids img for name class\n\t\t\tres = cxy_wh_2_rect(state[\'target_pos\'], state[\'target_sz\'])\n\t\t\tbbox.append(res)\n\t\t\tttime=ttime+time.time()-ttime1\n\n\t\tprint (\'Idx:%03d == total frame:%04d == speed:%03d fps\'%(idx+1, totalframe, (totalframe-1)/ttime))\n\t\tsaveroot=result_path+name+\'.mat\'\n\t\tscio.savemat(saveroot,{\'bbox\':bbox})\n\t\tidx=idx+1\n'"
code/run_SiamRPN.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-11-05 19:29:07\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-11-08 17:07:31\n# --------------------------------------------------------\n# DaSiamRPN\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport numpy as np\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport cv2\nimport os\nimport sys\n\n\nfrom utils import get_subwindow_tracking\n""""""\ntarget size 127\nstride 8\ndetection size 271\ntotal 127 + (19-1)*8 = 271\n""""""\n\n\ndef generate_anchor(total_stride, scales, ratios, score_size):\n    """"""\n    \xe7\x94\x9f\xe6\x88\x90anchor\n    total_stride 8\n    scales = [8, ]\n    ratios = [0.33, 0.5, 1, 2, 3]\n    score_size = 19\n\n    \xe4\xba\xa7\xe7\x94\x9ftop-left and w,h\xe7\x9a\x84\n    """"""\n    anchor_num = len(ratios) * len(scales)\n    anchor = np.zeros((anchor_num, 4),  dtype=np.float32)\n    size = total_stride * total_stride # 8 * 8\n    count = 0\n    #\xe8\xbf\x99\xe9\x87\x8c\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe6\x98\xaf\xe8\xae\xa1\xe7\xae\x97\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe7\x9a\x84anchor \n    #\xe8\xbf\x99\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe5\x90\x84\xe4\xb8\xaa\xe5\xb0\xba\xe5\xba\xa6\xe5\x90\x84\xe4\xb8\xaa\xe6\xaf\x94\xe4\xbe\x8b\xe9\x83\xbd\xe7\xae\x97\xe4\xba\x86wh \xef\xbc\x8cxy \xe5\xbe\x85\xe5\xae\x9a\n    #\xe6\xaf\x8f\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe9\x83\xbdwh\xe6\x98\xaf\xe7\xa1\xae\xe5\xae\x9a\xe7\x9a\x84\n    for ratio in ratios:\n        ws = int(np.sqrt(size / ratio)) # 8 / sqrt(ratio)\n        hs = int(ws * ratio)            # 8 * sqrt(ratio)\n        for scale in scales:\n            wws = ws * scale            # 64 / sqrt(ratio)\n            hhs = hs * scale            # 64 * sqrt(ratio)\n            anchor[count, 0] = 0\n            anchor[count, 1] = 0\n            anchor[count, 2] = wws\n            anchor[count, 3] = hhs\n            count += 1\n\n    #\xe9\x87\x8d\xe5\xa4\x8d\xe6\x8a\x8a\xe6\xaf\x8f\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe9\x83\xbdanchor\xe9\x83\xbd\xe5\xa0\x86\xe5\x8f\xa0\xe8\xb5\xb7\xe6\x9d\xa5\n    #\xe5\xb9\xb6\xe5\xa1\xab\xe5\x85\x85\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\n    anchor = np.tile(anchor, score_size * score_size).reshape((-1, 4))\n    ori = - (score_size / 2) * total_stride #\xe8\xbf\x99\xe9\x87\x8canchor\xe6\x98\xaf\xe4\xbb\xa5\xe4\xb8\xad\xe5\xbf\x83\xe4\xb8\xba\xe5\xaf\xb9\xe7\xa7\xb0\xe7\x82\xb9\xe7\x9a\x849\xef\xbc\x8c1\xef\xbc\x8c9\n    #print([ori + total_stride * dx for dx in range(score_size)])\n    #[-72, -64, -56, -48, -40, -32, -24, -16, -8, 0, 8, 16, 24, 32, 40, 48, 56, 64, 72]\n    #\n    xx, yy = np.meshgrid([ori + total_stride * dx for dx in range(score_size)],\n                         [ori + total_stride * dy for dy in range(score_size)])\n    xx, yy = np.tile(xx.flatten(), (anchor_num, 1)).flatten(), \\\n             np.tile(yy.flatten(), (anchor_num, 1)).flatten()\n    anchor[:, 0], anchor[:, 1] = xx.astype(np.float32), yy.astype(np.float32)\n\n    return anchor\n\n\nclass TrackerConfig(object):\n    # These are the default hyper-params for DaSiamRPN 0.3827\n    windowing = \'cosine\'  # to penalize large displacements [cosine/uniform]\n    # Params from the network architecture, have to be consistent with the training\n    exemplar_size = 127  # input z size\n    instance_size = 271  # input x size (search region)\n    total_stride  = 8\n    score_size = (instance_size-exemplar_size)/total_stride+1\n    context_amount = 0.5  # context amount for the exemplar\n    ratios = [0.33, 0.5, 1, 2, 3]\n    scales = [8, ]\n    anchor_num = len(ratios) * len(scales)\n    anchor = []\n    penalty_k = 0.055\n    window_influence = 0.42\n    lr = 0.295\n    template = None\n\n\ndef tracker_eval(net, x_crop, target_pos, target_sz, window, scale_z, p, ids, name, original_img, root_path = \'/home/song/srpn/tmp\'):\n    delta, score = net(x_crop) \n\n    delta = delta.permute(1, 2, 3, 0).contiguous().view(4, -1).data.cpu().numpy()\n    score = F.softmax(score.permute(1, 2, 3, 0).contiguous().view(2, -1), dim=0).data[1, :].cpu().numpy()\n    """"""\n    for i in range(p.anchor.shape[0]):\n        print(\'anchor  ====>  {}\'.format(p.anchor[i]))\n    """"""\n    delta[0, :] = delta[0, :] * p.anchor[:, 2] + p.anchor[:, 0]\n    delta[1, :] = delta[1, :] * p.anchor[:, 3] + p.anchor[:, 1]\n    delta[2, :] = np.exp(delta[2, :]) * p.anchor[:, 2]\n    delta[3, :] = np.exp(delta[3, :]) * p.anchor[:, 3]\n    # delta[0, :] is x = delta * w + x\n    # delta[1, :] is y = delta * y + y\n    # delta[2, :] is w = exp(w, delta)\n    # delta[3, :] is h = exp(h, delta)\n\n    # compute change ratio (r, 1/r)\n    def change(r):\n        return np.maximum(r, 1./r)\n\n    # compute size of larger area\n    def sz(w, h):\n        pad = (w + h) * 0.5\n        sz2 = (w + pad) * (h + pad)\n        return np.sqrt(sz2)\n    # compute size of larger area, input []\n    def sz_wh(wh):\n        pad = (wh[0] + wh[1]) * 0.5\n        sz2 = (wh[0] + pad) * (wh[1] + pad)\n        return np.sqrt(sz2)\n\n    ##############################################################\n    # score => pscore => pscore+window\n    # size penalty, delta is proposal\n    s_c = change(sz(delta[2, :], delta[3, :]) / (sz_wh(target_sz)))  # scale penalty, bbox scale ratio(area ratio)\n    r_c = change((target_sz[0] / target_sz[1]) / (delta[2, :] / delta[3, :]))  # ratio penalty\n\n    penalty = np.exp(-(r_c * s_c - 1.) * p.penalty_k)\n    pscore = penalty * score\n\n    # window float, \n    pscore = pscore * (1 - p.window_influence) + window * p.window_influence\n    best_pscore_id = np.argmax(pscore)\n\n    target = delta[:, best_pscore_id] / scale_z\n    target_sz = target_sz / scale_z\n    lr = penalty[best_pscore_id] * score[best_pscore_id] * p.lr # a kind of score\n\n    res_x = target[0] + target_pos[0]\n    res_y = target[1] + target_pos[1]\n\n    res_w = target_sz[0] * (1 - lr) + target[2] * lr\n    res_h = target_sz[1] * (1 - lr) + target[3] * lr\n\n    target_pos = np.array([res_x, res_y])\n    target_sz = np.array([res_w, res_h])\n\n    ##### vis#################################################\n    im_h, im_w, _ = original_img.shape\n    res_x = max(0, min(im_w, target_pos[0]))\n    res_y = max(0, min(im_h, target_pos[1]))\n    res_w = max(10, min(im_w, target_sz[0]))\n    res_h = max(10, min(im_h, target_sz[1]))\n\n    x1 = res_x - res_w/2\n    x2 = res_x + res_w/2\n    x3 = x2\n    x4 = x1\n    y1 = res_y - res_h/2\n    y2 = y1\n    y3 = res_y + res_h/2\n    y4 = y3\n    box = np.array([[x1, y1], [x2, y2], [x3, y3], [x4, y4]])\n    im = x_crop #(1L, 3L, 271L, 271L\n    im = im.squeeze(0).permute((1,2,0)).data.cpu().numpy()\n    cv2.polylines(original_img, [box.astype(np.int32).reshape((-1, 1, 2))], True, color=(255, 255, 0), thickness=1)\n    save_dir_path = os.path.join(root_path, name)\n    if not os.path.exists(save_dir_path):\n        os.makedirs(save_dir_path)\n    img_file = os.path.join(save_dir_path, \'{:03d}_detection_output.jpg\'.format(ids+1))\n    cv2.imwrite(img_file, original_img)\n    print(\'save at {}\'.format(img_file))\n    ##################################################################\n    return target_pos, target_sz, score[best_pscore_id]\n\n\ndef SiamRPN_init(im, target_pos, target_sz, net):\n    """"""\n    \xe8\xbe\x93\xe5\x85\xa5\xe7\xac\xac\xe4\xb8\x80\xe5\xb8\xa7\n    target_pos [center_x, center_y]\n    target_sz  [w, h]\n    net \n\n\n    return\n    state[\'im_h\']\n    state[\'im_w\']\n    state[\'p\']  config for tracker\n    state[\'net\']\n    state[\'avg_chan\'] \xe9\x80\x9a\xe9\x81\x93\xe5\x9d\x87\xe5\x80\xbc\n    """"""\n    state = dict()\n    p = TrackerConfig()\n    state[\'im_h\'] = im.shape[0]\n    state[\'im_w\'] = im.shape[1]\n\n    # Input size, if target is small, input should be large?\n    if ((target_sz[0] * target_sz[1]) / float(state[\'im_h\'] * state[\'im_w\'])) < 0.004:\n        p.instance_size = 287  # small object big search region\n    else:\n        p.instance_size = 271\n\n    # Input size - Template size\n    # \xe8\xae\xa1\xe7\xae\x97\xe6\xaf\x8f\xe8\xa1\x8c\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\x84\x9f\xe5\x8f\x97\xe9\x87\x8e\n    # \xe6\xaf\x8f\xe4\xb8\xaa\xe6\x84\x9f\xe5\x8f\x97\xe9\x87\x8esize instance_size\n    # \xe6\xaf\x8f\xe6\xac\xa1\xe7\xa7\xbb\xe5\x8a\xa8total_stride\n    p.score_size = (p.instance_size - p.exemplar_size) / p.total_stride + 1\n\n    p.anchor = generate_anchor(p.total_stride, p.scales, p.ratios, p.score_size)\n\n    # \xe6\xaf\x8f\xe5\x9c\xa8\xe4\xb8\x80\xe7\xbb\xb4\xe5\x81\x9a\xe5\xb9\xb3\xe5\x9d\x87\xe5\x88\x99\xe4\xb8\x8b\xe9\x99\x8d\xe4\xb8\x80\xe7\xbb\xb4\n    # 1024 * 1024 * 3 => [x1, x2, x3]\n    avg_chans = np.mean(im, axis=(0, 1))\n\n    # \xe6\x89\xa9\xe5\xa4\xa7template\xe8\x8c\x83\xe5\x9b\xb4\n    # \xe5\xb9\xb6\xe4\xb8\x94\xe9\x9c\x80\xe8\xa6\x81\xe5\xbd\x92\xe4\xb8\x80\xe6\x88\x90\xe6\xad\xa3\xe6\x96\xb9\xe5\xbd\xa2\n    # detection\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe5\xbd\x92\xe4\xb8\x80\n    # w_ -> w + (w+h)/2\n    # h_ -> h + (w+h)/2 \n    # s_ -> sqrt(w_ * h_)\n    # target\xe6\x98\xaf\xe5\xae\x9e\xe9\x99\x85bg \xe8\x80\x8cs_z\xe6\x98\xaf\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe6\x8a\x8abg\xe5\x8f\x98\xe6\x88\x90\xe4\xba\x86\xe6\xad\xa3\xe6\x96\xb9\xe5\xbd\xa2\n    wc_z = target_sz[0] + p.context_amount * sum(target_sz)\n    hc_z = target_sz[1] + p.context_amount * sum(target_sz)\n    s_z = round(np.sqrt(wc_z * hc_z))\n    \n    # initialize the exemplar\n    # \xe5\xb0\x86\xe6\xba\xa2\xe5\x87\xba\xe9\x83\xa8\xe5\x88\x86\xe7\x94\xa8avg\xe8\xa1\xa5\xe5\x85\x85\n    # target_pos\xe6\x98\xaf\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\n    # s_z\xe6\x98\xaf\xe5\xbd\x92\xe4\xb8\x80\xe5\x90\x8e\xe6\xad\xa3\xe6\x96\xb9\xe5\xbd\xa2\xe5\xa4\xa7\xe5\xb0\x8f\n    # exempler_size\xe6\x98\xaf\xe5\x90\x8e\xe9\x9d\xa2\xe9\x9c\x80\xe8\xa6\x81resize\xe7\x9a\x84127\n    z_crop = get_subwindow_tracking(im, target_pos, p.exemplar_size, s_z, avg_chans)\n    template = z_crop.numpy().transpose((1,2,0))\n    state[\'template\']=template\n\n    z = Variable(z_crop.unsqueeze(0))\n    net.temple(z.cuda())\n\n    if p.windowing == \'cosine\':\n        #outer (x1, x2)\n        #x1\xe4\xb8\xad\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x80\xbc\xe5\x8f\x98\xe4\xb8\xbax2\xe8\xa1\x8c\xe5\x90\x91\xe9\x87\x8f\xe7\x9a\x84\xe5\x80\x8d\xe6\x95\xb0\n        window = np.outer(np.hanning(p.score_size), np.hanning(p.score_size))\n    elif p.windowing == \'uniform\':\n        window = np.ones((p.score_size, p.score_size))\n    window = np.tile(window.flatten(), p.anchor_num)#np.tile\xe5\xa4\x8d\xe5\x88\xb6(row, col)\xe5\x80\x8d or directly copy x\n\n    state[\'p\'] = p\n    state[\'net\'] = net\n    state[\'avg_chans\'] = avg_chans\n    state[\'window\'] = window\n    state[\'target_pos\'] = target_pos\n    state[\'target_sz\'] = target_sz\n\n    return state\n\n\ndef SiamRPN_track(state, im, ids, name):\n    p = state[\'p\']\n    net = state[\'net\']\n    avg_chans = state[\'avg_chans\']\n    window = state[\'window\']\n    target_pos = state[\'target_pos\']\n    target_sz = state[\'target_sz\'] #background bbox\n\n    wc_z = target_sz[1] + p.context_amount * sum(target_sz)\n    hc_z = target_sz[0] + p.context_amount * sum(target_sz)\n    s_z = np.sqrt(wc_z * hc_z)\n    scale_z = p.exemplar_size / s_z #scale ratio of template\n    d_search = (p.instance_size - p.exemplar_size) / 2\n    pad = d_search / scale_z\n    s_x = s_z + 2 * pad\n\n    # extract scaled crops for search region x at previous target position\n    # \xe8\xbf\x99\xe9\x87\x8c\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe7\x9b\xae\xe6\xa0\x87\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe4\xbb\x8d\xe7\x84\xb6\xe5\x9c\xa8\xe5\x8e\x9f\xe4\xbd\x8d\n    # \xe7\x84\xb6\xe5\x90\x8e\xe4\xbb\xa5\xe6\xad\xa4\xe4\xb8\xad\xe5\xbf\x83\xe6\x88\xaa\xe5\x8f\x96 s_x \xe5\xb9\xb6\xe4\xb8\x94\xe5\x81\x9a\xe7\xbc\xa9\xe6\x94\xbe\n    # \xe8\xbf\x99\xe6\xa0\xb7\xe5\x81\x9a\xe7\x9a\x84\xe7\xbc\xba\xe7\x82\xb9\xe5\x9c\xa8\xe4\xba\x8e\xe5\xa6\x82\xe6\x9e\x9c\xe9\xab\x98\xe9\x80\x9f\xe7\xa7\xbb\xe5\x8a\xa8 \xe5\xb0\xb1\xe5\xae\xb9\xe6\x98\x93crop\xe4\xb8\x8d\xe5\x88\xb0\n    x_crop = Variable(get_subwindow_tracking(im, target_pos, p.instance_size, round(s_x), avg_chans).unsqueeze(0))\n    #print(x_crop.shape)#(1L, 3L, 271L, 271L)\n    save_img = x_crop.data.squeeze(0).numpy().transpose((1,2,0)).astype(np.int32)\n    save_path = os.path.join(\'/home/song/srpn/tmp\', name, \'{:03d}_detection_input.jpg\'.format(ids))\n    cv2.imwrite(save_path, save_img)\n    print(\'save detection input image @ {}\'.format(save_path))\n\n    target_pos, target_sz, score = tracker_eval(net, x_crop.cuda(), target_pos, target_sz * scale_z, window, scale_z, p, ids, name, im)\n    target_pos[0] = max(0, min(state[\'im_w\'], target_pos[0]))\n    target_pos[1] = max(0, min(state[\'im_h\'], target_pos[1]))\n    target_sz[0] = max(10, min(state[\'im_w\'], target_sz[0]))\n    target_sz[1] = max(10, min(state[\'im_h\'], target_sz[1]))\n    state[\'target_pos\'] = target_pos\n    state[\'target_sz\'] = target_sz\n    state[\'score\'] = score\n    return state\n'"
code/test_siamrpn.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-11-09 10:06:59\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-11-20 21:26:08\nimport os\nimport random\nimport sys; sys.path.append(\'../\')\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport argparse\nfrom data_loader import TrainDataLoader\nfrom PIL import Image, ImageOps, ImageStat, ImageDraw\nfrom net import SiameseRPN\nfrom torch.nn import init\n\nparser = argparse.ArgumentParser(description=\'PyTorch SiameseRPN Training\')\n\nparser.add_argument(\'--train_path\', default=\'/home/song/srpn/dataset/simple_vot13\', metavar=\'DIR\',help=\'path to dataset\')\n\nparser.add_argument(\'--weight_dir\', default=\'/home/song/srpn/weight\', metavar=\'DIR\',help=\'path to weight\')\n\nparser.add_argument(\'--checkpoint_path\', default=\'/home/song/srpn/weight/epoch_0060_weights.pth.tar\', help=\'resume\')\n\nparser.add_argument(\'--max_epoches\', default=100, type=int, metavar=\'N\', help=\'number of total epochs to run\')\n\nparser.add_argument(\'--max_batches\', default=10000, type=int, metavar=\'N\', help=\'number of batch in one epoch\')\n\nparser.add_argument(\'--init_type\',  default=\'xavier\', type=str, metavar=\'INIT\', help=\'init net\')\n\nparser.add_argument(\'--lr\', default=0.001, type=float, metavar=\'LR\', help=\'initial learning rate\')\n\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'momentum\', help=\'momentum\')\n\nparser.add_argument(\'--weight_decay\', \'--wd\', default=5e-5, type=float, metavar=\'W\', help=\'weight decay (default: 1e-4)\')\n\ndef main():\n    """""" dataloader """"""\n    args = parser.parse_args()\n    data_loader = TrainDataLoader(args.train_path, check = False)\n\n    """""" Model on gpu """"""\n    model = SiameseRPN()\n    model = model.cuda()\n    cudnn.benchmark = True\n\n    """""" loss and optimizer """"""\n    criterion = MultiBoxLoss()\n\n    """""" load weights """"""\n    init_weights(model)\n    if args.checkpoint_path == None:\n        sys.exit(\'please input trained model\')\n    else:\n        assert os.path.isfile(args.checkpoint_path), \'{} is not valid checkpoint_path\'.format(args.checkpoint_path)\n        checkpoint = torch.load(args.checkpoint_path)\n        start = checkpoint[\'epoch\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n\n    """""" test phase """"""        \n    index_list = range(data_loader.__len__()) \n    for example in range(args.max_batches):\n        ret = data_loader.__get__(random.choice(index_list)) \n        template = ret[\'template_tensor\'].cuda()\n        detection= ret[\'detection_tensor\'].cuda()\n        pos_neg_diff = ret[\'pos_neg_diff_tensor\'].cuda()\n        cout, rout = model(template, detection) #[1, 10, 17, 17], [1, 20, 17, 17]\n\n        cout = cout.reshape(-1, 2)\n        rout = rout.reshape(-1, 4)\n        cout = cout.cpu().detach().numpy()\n        score = 1/(1 + np.exp(cout[:,1]-cout[:,0]))\n        diff   = rout.cpu().detach().numpy() #1445\n        \n        num_proposals = 1\n        score_64_index = np.argsort(score)[::-1][:num_proposals]\n\n        score64 = score[score_64_index]\n        diffs64 = diff[score_64_index, :] \n        anchors64 = ret[\'anchors\'][score_64_index]\n        proposals_x = (anchors64[:, 0] + anchors64[:, 2] * diffs64[:, 0]).reshape(-1, 1)\n        proposals_y = (anchors64[:, 1] + anchors64[:, 3] * diffs64[:, 1]).reshape(-1, 1)\n        proposals_w = (anchors64[:, 2] * np.exp(diffs64[:, 2])).reshape(-1, 1)\n        proposals_h = (anchors64[:, 3] * np.exp(diffs64[:, 3])).reshape(-1, 1)\n        proposals = np.hstack((proposals_x, proposals_y, proposals_w, proposals_h))\n\n        d = os.path.join(ret[\'tmp_dir\'], \'6_pred_proposals\')\n        if not os.path.exists(d):\n            os.makedirs(d)\n\n        detection = ret[\'detection_cropped_resized\']\n        save_path = os.path.join(ret[\'tmp_dir\'], \'6_pred_proposals\', \'{:04d}_1_detection.jpg\'.format(example))\n        detection.save(save_path)\n\n        template = ret[\'template_cropped_resized\']\n        save_path = os.path.join(ret[\'tmp_dir\'], \'6_pred_proposals\', \'{:04d}_0_template.jpg\'.format(example))\n        template.save(save_path)\n\n        """""" \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96 """"""\n        draw = ImageDraw.Draw(detection)\n        for i in range(num_proposals):\n            x, y, w, h = proposals_x[i], proposals_y[i], proposals_w[i], proposals_h[i]\n            x1, y1, x2, y2 = x-w//2, y-h//2, x+w//2, y+h//2\n            draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')\n        """""" save detection template proposals""""""\n\n\n        save_path = os.path.join(ret[\'tmp_dir\'], \'6_pred_proposals\', \'{:04d}_2_proposals.jpg\'.format(example))\n        detection.save(save_path)\n\n        print(\'save at {}\'.format(save_path))\n            # restore\n\n\n\n\n        """"""\n\n\n            predictions = (cout, rout)\n            targets = pos_neg_diff\n\n            closs, rloss = criterion(predictions, targets)\n            loss = closs + rloss\n            closses.update(closs.cpu().item())\n            rlosses.update(rloss.cpu().item())\n\n\n\n            print(""epoch:{:04d} example:{:06d} lr:{:.2f} closs:{:.2f}\\trloss:{:.2f}"".format(epoch, example, cur_lr, closses.avg, rlosses.avg))\n    \n        if epoch % 5 == 0 :\n            file_path = os.path.join(args.weight_dir, \'epoch_{:04d}_weights.pth.tar\'.format(epoch))\n            state = {\n            \'epoch\' :epoch+1,\n            \'state_dict\' :model.state_dict(),\n            \'optimizer\' : optimizer.state_dict(),\n            }\n            torch.save(state, file_path)\n        """"""\n\n\ndef init_weights(net, init_type=\'normal\', gain=0.02):\n    def init_func(m):\n        # this will apply to each layer\n        classname = m.__class__.__name__\n        if hasattr(m, \'weight\') and (classname.find(\'conv\')!=-1 or classname.find(\'Linear\')!=-1):\n            if init_type==\'normal\':\n                init.normal_(m.weight.data, 0.0, gain)\n            elif init_type == \'xavier\':\n                init.xavier_normal_(m.weight.data, gain=gain)\n            elif init_type == \'kaiming\':\n                init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')#good for relu\n            elif init_type == \'orthogonal\':\n                init.orthogonal_(m.weight.data, gain=gain)\n            else:\n                raise NotImplementedError(\'initialization method [%s] is not implemented\' % init_type)\n            \n            if hasattr(m, \'bias\') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find(\'BatchNorm2d\') != -1:\n            init.normal_(m.weight.data, 1.0, gain)\n            init.constant_(m.bias.data, 0.0)\n    #print(\'initialize network with %s\' % init_type)\n    net.apply(init_func)\n\n\n\nclass MultiBoxLoss(nn.Module):\n    def __init__(self):\n        super(MultiBoxLoss, self).__init__()\n        self.closs = torch.nn.CrossEntropyLoss()\n        self.rloss = torch.nn.SmoothL1Loss()\n\n    def forward(self, predictions, targets):\n        cout, rout = predictions\n        cout = cout.reshape(1, 2, -1)\n        rout = rout.reshape(-1, 4)\n        class_gt, diff = targets[:,0].unsqueeze(0).long(), targets[:,1:]\n        closs = self.closs(cout, class_gt)#1,2,*  1,*\n\n        pos_index = np.where(class_gt == 1)[1]\n        if pos_index.shape[0] == 0:\n            rloss = torch.FloatTensor([0]).cuda()\n        else:\n            rout_pos = rout[pos_index]\n            diff_pos = diff[pos_index]\n            \n            #print(rout_pos)\n            #print(diff_pos)\n            rloss = self.rloss(rout_pos, diff_pos) #16\n        return closs/64, rloss/16 \n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef adjust_learning_rate(lr, optimizer, epoch, gamma=0.1):\n    """"""Sets the learning rate to the initial LR decayed 0.9 every 50 epochs""""""\n    lr = lr * (0.9 ** (epoch // 1))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return lr\n\nif __name__ == \'__main__\':\n    main()\n \n\n'"
code/train_siamrpn.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-11-09 10:06:59\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-11-22 15:24:22\nimport os\nimport os.path as osp\nimport random\nimport time\nimport sys; sys.path.append(\'../\')\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as F\nimport argparse\nfrom PIL import Image, ImageOps, ImageStat, ImageDraw\nfrom data_loader import TrainDataLoader\nfrom net import SiameseRPN, SiameseRPN_bn\nfrom torch.nn import init\n\nparser = argparse.ArgumentParser(description=\'PyTorch SiameseRPN Training\')\n\nparser.add_argument(\'--train_path\', default=\'/home/song/srpn/dataset/vot13\', metavar=\'DIR\',help=\'path to dataset\')\n\nparser.add_argument(\'--weight_dir\', default=\'/home/song/srpn/weight\', metavar=\'DIR\',help=\'path to weight\')\n\nparser.add_argument(\'--checkpoint_path\', default=None, help=\'resume\')\n\nparser.add_argument(\'--max_epoches\', default=10000, type=int, metavar=\'N\', help=\'number of total epochs to run\')\n\nparser.add_argument(\'--max_batches\', default=0, type=int, metavar=\'N\', help=\'number of batch in one epoch\')\n\nparser.add_argument(\'--init_type\',  default=\'xavier\', type=str, metavar=\'INIT\', help=\'init net\')\n\nparser.add_argument(\'--lr\', default=0.001, type=float, metavar=\'LR\', help=\'initial learning rate\')\n\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'momentum\', help=\'momentum\')\n\nparser.add_argument(\'--weight_decay\', \'--wd\', default=5e-5, type=float, metavar=\'W\', help=\'weight decay (default: 1e-4)\')\n\ndef main():\n    """""" train dataloader """"""\n    args = parser.parse_args()\n    data_loader = TrainDataLoader(args.train_path, check = True)\n    if not os.path.exists(args.weight_dir):\n        os.makedirs(args.weight_dir)\n\n    """""" compute max_batches """"""\n    for root, dirs, files in os.walk(args.train_path):\n        for dirname in dirs:\n            dir_path = os.path.join(root, dirname)\n            args.max_batches += len(os.listdir(dir_path))\n    inter = args.max_batches//10\n    print(\'Max batches:{} in one epoch \'.format(args.max_batches))\n\n    """""" Model on gpu """"""\n    model = SiameseRPN()\n    model = model.cuda()\n    cudnn.benchmark = True\n\n    """""" loss and optimizer """"""\n    criterion = MultiBoxLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay = args.weight_decay)\n\n    """""" load weights """"""\n    init_weights(model)\n    if not args.checkpoint_path == None:\n        assert os.path.isfile(args.checkpoint_path), \'{} is not valid checkpoint_path\'.format(args.checkpoint_path)\n        try:\n            checkpoint = torch.load(args.checkpoint_path)\n            start = checkpoint[\'epoch\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        except:\n            start = 0\n            init_weights(model)\n    else:\n        start = 0\n\n    """""" train phase """"""\n    closses, rlosses, tlosses = AverageMeter(), AverageMeter(), AverageMeter()\n    for epoch in range(start, args.max_epoches):\n        cur_lr = adjust_learning_rate(args.lr, optimizer, epoch, gamma=0.1)\n        index_list = range(data_loader.__len__()) \n        #for example in range(args.max_batches):\n        for example in range(900):\n            ret = data_loader.__get__(random.choice(index_list)) \n            template = ret[\'template_tensor\'].cuda()\n            detection= ret[\'detection_tensor\'].cuda()\n            pos_neg_diff = ret[\'pos_neg_diff_tensor\'].cuda() if ret[\'pos_neg_diff_tensor\'] is not None else None\n            \n            cout, rout = model(template, detection)\n            \n            predictions = (cout, rout)\n            targets = pos_neg_diff\n\n            area = ret[\'area_target_in_resized_detection\']\n            num_pos = len(np.where(pos_neg_diff == 1)[0])\n            if area == 0 or num_pos == 0 or pos_neg_diff is None:\n                continue\n\n\n\n            closs, rloss, loss, reg_pred, reg_target, pos_index, neg_index = criterion(predictions, targets)\n            \n            # debug for class\n            cout = cout.squeeze().permute(1, 2, 0).reshape(-1, 2)\n            cout = cout.cpu().detach().numpy()\n            print(cout.shape)\n            score = 1/(1 + np.exp(cout[:,0]-cout[:,1]))\n            print(score[pos_index])\n            print(score[neg_index])\n            #time.sleep(1)\n\n            # debug for reg\n            tmp_dir = \'/home/song/srpn/tmp/visualization/7_train_debug_pos_anchors\'\n            if not os.path.exists(tmp_dir):\n                os.makedirs(tmp_dir)\n            detection = ret[\'detection_cropped_resized\'].copy()\n            draw = ImageDraw.Draw(detection)\n            pos_anchors = ret[\'pos_anchors\'].copy()\n            \n            # pos anchor\xe7\x9a\x84\xe5\x9b\x9e\xe5\xbd\x92\xe6\x83\x85\xe5\x86\xb5\n            x = pos_anchors[:,0] + pos_anchors[:, 2] * reg_pred[pos_index, 0].cpu().detach().numpy()\n            y = pos_anchors[:,1] + pos_anchors[:, 3] * reg_pred[pos_index, 1].cpu().detach().numpy()\n            w = pos_anchors[:,2] * np.exp(reg_pred[pos_index, 2].cpu().detach().numpy())\n            h = pos_anchors[:,3] * np.exp(reg_pred[pos_index, 3].cpu().detach().numpy())\n            x1s, y1s, x2s, y2s = x - w//2, y - h//2, x + w//2, y + h//2\n            for i in range(2):\n                x1, y1, x2, y2 = x1s[i], y1s[i], x2s[i], y2s[i]\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\') #predict\n            \n            # \xe5\xba\x94\xe5\xbd\x93\xe7\x9a\x84gt\n            x = pos_anchors[:,0] + pos_anchors[:, 2] * reg_target[pos_index, 0].cpu().detach().numpy()\n            y = pos_anchors[:,1] + pos_anchors[:, 3] * reg_target[pos_index, 1].cpu().detach().numpy()\n            w = pos_anchors[:,2] * np.exp(reg_target[pos_index, 2].cpu().detach().numpy())\n            h = pos_anchors[:,3] * np.exp(reg_target[pos_index, 3].cpu().detach().numpy())\n            x1s, y1s, x2s, y2s = x - w//2, y-h//2, x + w//2, y + h//2\n            for i in range(2):\n                x1, y1, x2, y2 = x1s[i], y1s[i], x2s[i], y2s[i]\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'green\') #gt\n\n            # \xe6\x89\xbe\xe5\x88\x86\xe6\x95\xb0zui da de,\n            m_indexs = np.argsort(score)[::-1][:5]\n            for m_index in m_indexs:\n                diff = reg_pred[m_index].cpu().detach().numpy()\n                anc  = ret[\'anchors\'][m_index]\n                x = anc[0] + anc[0] * diff[0]\n                y = anc[1] + anc[1] * diff[1]\n                w = anc[2]*np.exp(diff[2])\n                h = anc[3]*np.exp(diff[3])\n                x1, y1, x2, y2 = x - w//2, y - h//2, x + w//2, y + h//2\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=2, fill=\'black\')\n\n\n            save_path = osp.join(tmp_dir, \'epoch_{:04d}_{:04d}_{:02d}.jpg\'.format(epoch, example, i))\n            detection.save(save_path)\n\n            closs_ = closs.cpu().item()\n            if np.isnan(closs_): \n               sys.exit(0)\n\n            #loss = closs + rloss\n            closses.update(closs.cpu().item())\n            rlosses.update(rloss.cpu().item())\n            tlosses.update(loss.cpu().item())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            #time.sleep(1)\n\n            print(""Epoch:{:04d}\\texample:{:08d}/{:08d}({:.2f})\\tlr:{:.7f}\\tcloss:{:.4f}\\trloss:{:.4f}\\ttloss:{:.4f}"".format(epoch, example+1, args.max_batches, 100*(example+1)/args.max_batches, cur_lr, closses.avg, rlosses.avg, tlosses.avg ))\n\n\n    \n        if epoch % 5 == 0 :\n            file_path = os.path.join(args.weight_dir, \'epoch_{:04d}_weights.pth.tar\'.format(epoch))\n            state = {\n            \'epoch\' :epoch+1,\n            \'state_dict\' :model.state_dict(),\n            \'optimizer\' : optimizer.state_dict(),\n            }\n            torch.save(state, file_path)\n\ndef init_weights(net, init_type=\'normal\', gain=0.02):\n    def init_func(m):\n        # this will apply to each layer\n        classname = m.__class__.__name__\n        if hasattr(m, \'weight\') and (classname.find(\'conv\')!=-1 or classname.find(\'Linear\')!=-1):\n            if init_type==\'normal\':\n                init.normal_(m.weight.data, 0.0, gain)\n            elif init_type == \'xavier\':\n                init.xavier_normal_(m.weight.data, gain=gain)\n            elif init_type == \'kaiming\':\n                init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')#good for relu\n            elif init_type == \'orthogonal\':\n                init.orthogonal_(m.weight.data, gain=gain)\n            else:\n                raise NotImplementedError(\'initialization method [%s] is not implemented\' % init_type)\n            \n            if hasattr(m, \'bias\') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find(\'BatchNorm2d\') != -1:\n            init.normal_(m.weight.data, 1.0, gain)\n            init.constant_(m.bias.data, 0.0)\n    #print(\'initialize network with %s\' % init_type)\n    net.apply(init_func)\n\nclass MultiBoxLoss(nn.Module):\n    def __init__(self):\n        super(MultiBoxLoss, self).__init__()\n\n    def forward(self, predictions, targets):\n        print(\'+++++++++++++++++++++++++++++++++++\')\n        cout, rout = predictions\n\n        """""" class """"""\n\n        class_pred   = cout.squeeze().permute(1,2,0).reshape(-1, 2)\n        class_target = targets[:, 0].long()\n        pos_index = list(np.where(class_target == 1)[0])\n        neg_index = list(np.where(class_target == 0)[0])\n        class_target = class_target[pos_index + neg_index]\n        class_pred   = class_pred[pos_index + neg_index]\n\n        closs = F.cross_entropy(class_pred, class_target, size_average=False, reduce=False)\n        closs = torch.div(torch.sum(closs[np.where(class_target != -100)]), 64)\n        \n        reg_pred = rout.view(-1, 4)\n        reg_target = targets[:, 1:] #[1445, 4]\n        rloss = F.smooth_l1_loss(reg_pred, reg_target, size_average=False, reduce=False)\n        rloss = torch.div(torch.sum(rloss[np.where(class_target == 1)]), 16)\n\n\n        #debug vis pos anchor\n        loss = closs + rloss\n        return closs, rloss, loss, reg_pred, reg_target, pos_index, neg_index\n\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef adjust_learning_rate(lr, optimizer, epoch, gamma=0.1):\n    """"""Sets the learning rate to the initial LR decayed 0.9 every 50 epochs""""""\n    lr = lr * (0.9 ** (epoch // 1))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return lr\n\nif __name__ == \'__main__\':\n    main()\n \n\n'"
code/utils.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-11-05 21:53:12\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-11-08 16:00:53\n# --------------------------------------------------------\n# DaSiamRPN\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport cv2\nimport torch\nimport numpy as np\n\n\ndef to_numpy(tensor):\n    if torch.is_tensor(tensor):\n        return tensor.cpu().numpy()\n    elif type(tensor).__module__ != \'numpy\':\n        raise ValueError(""Cannot convert {} to numpy array""\n                         .format(type(tensor)))\n    return tensor\n\n\ndef to_torch(ndarray):\n    if type(ndarray).__module__ == \'numpy\':\n        return torch.from_numpy(ndarray)\n    elif not torch.is_tensor(ndarray):\n        raise ValueError(""Cannot convert {} to torch tensor""\n                         .format(type(ndarray)))\n    return ndarray\n\n\ndef im_to_numpy(img):\n    img = to_numpy(img)\n    img = np.transpose(img, (1, 2, 0))  # H*W*C\n    return img\n\n\ndef im_to_torch(img):\n    """"""\n    cv img [h, w, c]\n    torch  [c, h, w]\n    """"""\n    img = np.transpose(img, (2, 0, 1))  # C*H*W\n    img = to_torch(img).float()\n    return img\n\n\ndef torch_to_img(img):\n    img = to_numpy(torch.squeeze(img, 0))\n    img = np.transpose(img, (1, 2, 0))  # H*W*C\n    return img\n\n\ndef get_subwindow_tracking(im, pos, model_sz, original_sz, avg_chans, out_mode=\'torch\', new=False):\n    """"""\n    img  -- original image\n    pos  -- [c_x, c_y]  \n    model_sz 127                   \xe6\x9c\x80\xe5\x90\x8e\xe9\x9c\x80\xe8\xa6\x81resize\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\n    original s_ -> sqrt(w_ * h_)   \xe5\xaf\xb9target\xe6\x93\x8d\xe4\xbd\x9c\xe5\x90\x8e\xe7\x9a\x84size\n    avg\n\n    template\xe6\x98\xaf\xe5\xbf\x85\xe9\xa1\xbb\xe6\xad\xa3\xe6\x96\xb9\xe5\xbd\xa2\n    detection\xe4\xb8\x8d\xe6\x98\xaf\xe6\xad\xa3\xe6\x96\xb9\xe5\xbd\xa2\n    """"""\n    #example \xe6\x94\xbe\xe5\xa4\xa7\xe5\x90\x8e\xe9\x98\xb2\xe6\xad\xa2\xe6\xba\xa2\xe5\x87\xba\n    if isinstance(pos, float):\n        pos = [pos, pos]\n    sz = original_sz\n    im_sz = im.shape\n    c = (original_sz+1) / 2\n\n    #context_xmin/xmax \xe4\xbb\xa3\xe8\xa1\xa8\xe5\x8f\x98\xe5\x8c\x96\xe5\x90\x8e\xe7\x9a\x84template\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x88\xe5\x8f\xaf\xe8\x83\xbd\xe4\xb8\xba\xe8\xb4\x9f\xef\xbc\x89\n    context_xmin = round(pos[0] - c)  # floor(pos(2) - sz(2) / 2);\n    context_xmax = context_xmin + sz - 1\n    context_ymin = round(pos[1] - c)  # floor(pos(1) - sz(1) / 2);\n    context_ymax = context_ymin + sz - 1\n\n    # \xe5\xa6\x82\xe6\x9e\x9c\xe5\xa4\xa7\xe4\xba\x8e0 \xe5\x88\x99\xe4\xb8\x8dpad\n    # \xe5\xa6\x82\xe6\x9e\x9c\xe6\xba\xa2\xe5\x87\xba\xe4\xba\x86 \xe5\x88\x99pad\xe4\xb8\x80\xe5\xae\x9a\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84\xe8\xb7\x9d\xe7\xa6\xbb\n    left_pad = int(max(0., -context_xmin))\n    top_pad = int(max(0., -context_ymin))\n    right_pad = int(max(0., context_xmax - im_sz[1] + 1))\n    bottom_pad = int(max(0., context_ymax - im_sz[0] + 1))\n\n    # \xe5\x9c\xa8pad\xe5\x90\x8e\xe7\x9a\x84\xe5\x8e\x9f\xe5\x9b\xbe\xe4\xb8\x8a\xe9\x87\x8d\xe6\x96\xb0\xe6\xa0\x87\xe6\xb3\xa8template\xe4\xbf\xa1\xe6\x81\xaf\n    context_xmin = context_xmin + left_pad\n    context_xmax = context_xmax + left_pad\n    context_ymin = context_ymin + top_pad\n    context_ymax = context_ymax + top_pad\n\n    # zzp: a more easy speed version\n    # \xe7\x94\xa8avg\xe6\x9d\xa5\xe5\xa1\xab\xe5\x85\x85\xe8\xbe\xb9\xe7\x95\x8c\xe9\x83\xa8\xe5\x88\x86\n    r, c, k = im.shape\n    if any([top_pad, bottom_pad, left_pad, right_pad]):\n        te_im = np.zeros((r + top_pad + bottom_pad, c + left_pad + right_pad, k), np.uint8)  # 0 is better than 1 initialization\n        te_im[top_pad:top_pad + r, left_pad:left_pad + c, :] = im\n        if top_pad:\n            te_im[0:top_pad, left_pad:left_pad + c, :] = avg_chans\n        if bottom_pad:\n            te_im[r + top_pad:, left_pad:left_pad + c, :] = avg_chans\n        if left_pad:\n            te_im[:, 0:left_pad, :] = avg_chans\n        if right_pad:\n            te_im[:, c + left_pad:, :] = avg_chans\n        im_patch_original = te_im[int(context_ymin):int(context_ymax + 1), int(context_xmin):int(context_xmax + 1), :]\n    else:\n        im_patch_original = im[int(context_ymin):int(context_ymax + 1), int(context_xmin):int(context_xmax + 1), :]\n\n    if not np.array_equal(model_sz, original_sz):\n        im_patch = cv2.resize(im_patch_original, (model_sz, model_sz))  # zzp: use cv to get a better speed\n    else:\n        im_patch = im_patch_original\n\n    return im_to_torch(im_patch) if out_mode in \'torch\' else im_patch\n\n\ndef cxy_wh_2_rect(pos, sz):\n    return np.array([pos[0]-sz[0]/2, pos[1]-sz[1]/2, sz[0], sz[1]])  # 0-index\n\n\ndef rect_2_cxy_wh(rect):\n    return np.array([rect[0]+rect[2]/2, rect[1]+rect[3]/2]), np.array([rect[2], rect[3]])  # 0-index\n\n\ndef get_axis_aligned_bbox(region):\n    region = np.array([region[0][0][0], region[0][0][1], region[0][1][0], region[0][1][1],\n                       region[0][2][0], region[0][2][1], region[0][3][0], region[0][3][1]])\n    cx = np.mean(region[0::2])\n    cy = np.mean(region[1::2])\n    x1 = min(region[0::2])\n    x2 = max(region[0::2])\n    y1 = min(region[1::2])\n    y2 = max(region[1::2])\n    A1 = np.linalg.norm(region[0:2] - region[2:4]) * np.linalg.norm(region[2:4] - region[4:6])\n    A2 = (x2 - x1) * (y2 - y1)\n    s = np.sqrt(A1 / A2)\n    w = s * (x2 - x1) + 1\n    h = s * (y2 - y1) + 1\n    return cx, cy, w, h'"
code/video2image.py,1,"b'# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-11-09 10:13:23\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-11-09 17:21:16\nimport pandas as pd\nimport cv2\nimport os\nimport sys\n\nworkspace = os.path.abspath(\'./\')\n#### annotation #######################################################################\nos.chdir(workspace)\nyt_bb_ann_path = os.path.abspath(\'../youtube_BB/annotation\')\nif not os.path.exists(yt_bb_ann_path):\n\tos.makedirs(yt_bb_ann_path)\n\tos.chdir(yt_bb_ann_path)\n\tos.system(\'wget https://research.google.com/youtube-bb/yt_bb_detection_train.csv.gz\') # here may need vpn in China\n\tos.system(\'wget https://research.google.com/youtube-bb/yt_bb_detection_validation.csv.gz\') # here may need vpn in China\n\n\n#### use script to download video #######################################################\nos.chdir(workspace)\nyt_bb_video_path = os.path.abspath(\'../youtube_BB/video\')\nyt_bb_script_path = os.path.abspath(\'../youtube_BB/youtube-bb-script/youtube-bb\')\nif not os.path.exists(yt_bb_video_path):\n\tos.makedirs(yt_bb_video_path)\n\tos.chdir(yt_bb_script_path)\n\tos.system(\'pip install -r requirements.txt\')\n\tos.system(\'python3 download.py {} 6\'.format(yt_bb_video_path))\n\n\n\n### trans video2pic ######################################################################\nf = pd.read_csv(os.path.join(yt_bb_ann_path, \'yt_bb_detection_train.csv\'), header=None)\nf.columns = [\'youtube_id\',\'timestamp_ms\',\'class_id\',\'class_name\',\'object_id\',\'object_presence\',\'xmin\',\'xmax\',\'ymin\',\'ymax\']\n#print(f[\'youtube_id\'])\n\nos.chdir(workspace)\nfor subdir_name in os.listdir(yt_bb_video_path):\n\tsubdir_path = os.path.join(yt_bb_video_path, subdir_name)\n\tfor mp4_name in os.listdir(subdir_path):\n\t\tmp4_path = os.path.join(subdir_path, mp4_name)\n\t\tvideo = cv2.VideoCapture(mp4_path) #VideoCapture()\xe4\xb8\xad\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf0\xef\xbc\x8c\xe8\xa1\xa8\xe7\xa4\xba\xe6\x89\x93\xe5\xbc\x80\xe7\xac\x94\xe8\xae\xb0\xe6\x9c\xac\xe7\x9a\x84\xe5\x86\x85\xe7\xbd\xae\xe6\x91\x84\xe5\x83\x8f\xe5\xa4\xb4\xef\xbc\x8c\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf\xe8\xa7\x86\xe9\xa2\x91\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\xe5\x88\x99\xe6\x89\x93\xe5\xbc\x80\xe8\xa7\x86\xe9\xa2\x91\n\t\t#ret, frame = video.read() # cap.read()\xe6\x8c\x89\xe5\xb8\xa7\xe8\xaf\xbb\xe5\x8f\x96\xe8\xa7\x86\xe9\xa2\x91\xef\xbc\x8cret,frame\xe6\x98\xaf\xe8\x8e\xb7cap.read()\xe6\x96\xb9\xe6\xb3\x95\xe7\x9a\x84\xe4\xb8\xa4\xe4\xb8\xaa\xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc\xe3\x80\x82\xe5\x85\xb6\xe4\xb8\xadret\xe6\x98\xaf\xe5\xb8\x83\xe5\xb0\x94\xe5\x80\xbc\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe8\xaf\xbb\xe5\x8f\x96\xe5\xb8\xa7\xe6\x98\xaf\xe6\xad\xa3\xe7\xa1\xae\xe7\x9a\x84\xe5\x88\x99\xe8\xbf\x94\xe5\x9b\x9eTrue\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\xe8\xaf\xbb\xe5\x8f\x96\xe5\x88\xb0\xe7\xbb\x93\xe5\xb0\xbe\xef\xbc\x8c\xe5\xae\x83\xe7\x9a\x84\xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc\xe5\xb0\xb1\xe4\xb8\xbaFalse\n\t\t\n\t\t# \xe9\x80\x9a\xe8\xbf\x87mp4_name\xe5\xaf\xbb\xe6\x89\xbe\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84bg info\n\t\tmp4_name = mp4_name.strip(\'.mp4\')\n\t\t# test\n\t\tmp4_name = \'ej4xM04ipxM\'\n\t\tprint(\'mp4 name == {}\'.format(mp4_name))\n\t\t\n\n\t\t############# \xe6\x8c\x89\xe5\xb8\xa7\xe8\xaf\xbb\xe5\x9b\xbe\xe7\x89\x87 ###############\n\t\tsave_img_dir = os.path.abspath(\'../youtube_BB/image\')\n\t\tsave_img_subdir = os.path.join(save_img_dir, mp4_name)\n\t\tif not os.path.exists(save_img_subdir):\n\t\t\tos.makedirs(save_img_subdir)\n\n\t\tret = True # by default the first frame is not the last one\n\t\tindex = 0\n\t\twhile ret:#\xe5\xbd\x93\xe5\x89\x8d\xe4\xb8\x80\xe5\xb8\xa7\xe4\xb8\x8d\xe6\x98\xaf\xe7\xbb\x93\xe5\xb0\xbe,\xe5\xb0\xb1\xe8\xaf\xbb\xe4\xb8\x8b\xe4\xb8\x80\xe5\xb8\xa7\n\t\t\tret, frame = video.read()\n\t\t\tsave_img_file = os.path.join(save_img_subdir, \'{:04d}.jpg\'.format(index))\n\t\t\tcv2.imwrite(save_img_file, frame)\n\t\t\tindex += 1\n\n\n\n\t\t############# \xe6\x8c\x89\xe5\xb8\xa7\xe8\xaf\xbbgt  ###############\n\t\tlines = []\t\t\n\t\tf_frames_of_mp4 = f.loc[f[\'youtube_id\'] == mp4_name]\n\t\tsave_gt_subdir = os.path.join(yt_bb_ann_path, mp4_name)\n\t\tif not os.path.exists(save_gt_subdir):\n\t\t\tos.makedirs(save_gt_subdir)\n\t\tsave_gt_file = os.path.join(save_gt_subdir, \'groundtruth_rect.txt\')\n\t\tfor frame_index, frame_id in enumerate(f_frames_of_mp4[\'youtube_id\']):\n\t\t\tinfo_of_this_frame = f_frames_of_mp4.iloc[frame_index]\n\n\t\t\tids             = info_of_this_frame[\'youtube_id\']\n\t\t\ttime            = info_of_this_frame[\'timestamp_ms\']\n\t\t\tclass_id        = info_of_this_frame[\'class_id\']\n\t\t\tclass_name      = info_of_this_frame[\'class_name\']\n\t\t\tobject_id       = info_of_this_frame[\'object_id\']\n\t\t\tobject_presence = info_of_this_frame[\'object_presence\']\n\t\t\txmin            = info_of_this_frame[\'xmin\']\n\t\t\txmax            = info_of_this_frame[\'xmax\']\n\t\t\tymin            = info_of_this_frame[\'ymin\']\n\t\t\tymax            = info_of_this_frame[\'ymax\']\n\n\t\t\tc_x             = (xmin + xmax)//2\n\t\t\tc_y             = (ymin + ymax)//2\n\t\t\tw               = xmax - xmin\n\t\t\th               = ymax - ymin\n\t\t\tnewline = ""{},{},{},{}\\n"".format(c_x, c_y, w, h)\n\t\t\tlines.append(newline)\n\n\t\t\tprint(\'Video {:12} == Frame index:{:03d} == time:{:06d} == class:{:8} == object_presence:{:8}\'.format(frame_id, frame_index, time, class_name, object_presence)) \t\t\n\t\twith open(save_gt_file, \'w\') as f:\n\t\t\tf.writelines(lines)\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
code/vot.py,0,"b'""""""\n\\file vot.py\n\n@brief Python utility functions for VOT integration\n\n@author Luka Cehovin, Alessio Dore\n\n@date 2016\n\n""""""\n\nimport sys\nimport copy\nimport collections\n\ntry:\n    import trax\n    import trax.server\n    TRAX = True\nexcept ImportError:\n    TRAX = False\n\n#https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001411031239400f7181f65f33a4623bc42276a605debf6000\n#cords class\nRectangle = collections.namedtuple(\'Rectangle\', [\'x\', \'y\', \'width\', \'height\'])\nPoint = collections.namedtuple(\'Point\', [\'x\', \'y\'])\nPolygon = collections.namedtuple(\'Polygon\', [\'points\'])\n\ndef parse_region(string):\n    tokens = map(float, string.split(\',\'))\n    if len(tokens) == 4:\n        return Rectangle(tokens[0], tokens[1], tokens[2], tokens[3])\n    elif len(tokens) % 2 == 0 and len(tokens) > 4:\n        return Polygon([Point(tokens[i],tokens[i+1]) for i in xrange(0,len(tokens),2)])\n    return None\n\ndef encode_region(region):\n    if isinstance(region, Polygon):\n        return \',\'.join([\'{},{}\'.format(p.x,p.y) for p in region.points])\n    elif isinstance(region, Rectangle):\n        return \'{},{},{},{}\'.format(region.x, region.y, region.width, region.height)\n    else:\n        return """"\n\ndef convert_region(region, to):\n\n    if to == \'rectangle\':\n\n        if isinstance(region, Rectangle):\n            return copy.copy(region)\n        elif isinstance(region, Polygon):\n            top = sys.float_info.max\n            bottom = sys.float_info.min\n            left = sys.float_info.max\n            right = sys.float_info.min\n\n            for point in region.points:\n                top = min(top, point.y)\n                bottom = max(bottom, point.y)\n                left = min(left, point.x)\n                right = max(right, point.x)\n\n            return Rectangle(left, top, right - left, bottom - top)\n\n        else:\n            return None\n    if to == \'polygon\':\n\n        if isinstance(region, Rectangle):\n            points = []\n            points.append((region.x, region.y))\n            points.append((region.x + region.width, region.y))\n            points.append((region.x + region.width, region.y + region.height))\n            points.append((region.x, region.y + region.height))\n            return Polygon(points)\n\n        elif isinstance(region, Polygon):\n            return copy.copy(region)\n        else:\n            return None\n\n    return None\n\nclass VOT(object):\n    """""" Base class for Python VOT integration """"""\n    def __init__(self, region_format):\n        """""" Constructor\n\n        Args:\n            region_format: Region format options\n        """"""\n        assert(region_format in [\'rectangle\', \'polygon\'])\n        if TRAX:\n            options = trax.server.ServerOptions(region_format, trax.image.PATH)\n            self._trax = trax.server.Server(options)\n\n            request = self._trax.wait()\n            assert(request.type == \'initialize\')\n            if request.region.type == \'polygon\':\n                self._region = Polygon([Point(x[0], x[1]) for x in request.region.points])\n            else:\n                self._region = Rectangle(request.region.x, request.region.y, request.region.width, request.region.height)\n            self._image = str(request.image)\n            self._trax.status(request.region)\n        else:\n            self._files = [x.strip(\'\\n\') for x in open(\'images.txt\', \'r\').readlines()]\n            self._frame = 0\n            self._region = convert_region(parse_region(open(\'region.txt\', \'r\').readline()), region_format)\n            self._result = []\n\n    def region(self):\n        """"""\n        Send configuration message to the client and receive the initialization\n        region and the path of the first image\n\n        Returns:\n            initialization region\n        """"""\n\n        return self._region\n\n    def report(self, region, confidence = 0):\n        """"""\n        Report the tracking results to the client\n\n        Arguments:\n            region: region for the frame\n        """"""\n        assert(isinstance(region, Rectangle) or isinstance(region, Polygon))\n        if TRAX:\n            if isinstance(region, Polygon):\n                tregion = trax.region.Polygon([(x.x, x.y) for x in region.points])\n            else:\n                tregion = trax.region.Rectangle(region.x, region.y, region.width, region.height)\n            self._trax.status(tregion, {""confidence"" : confidence})\n        else:\n            self._result.append(region)\n            self._frame += 1\n\n    def frame(self):\n        """"""\n        Get a frame (image path) from client\n\n        Returns:\n            absolute path of the image\n        """"""\n        if TRAX:\n            if hasattr(self, ""_image""):\n                image = str(self._image)\n                del self._image\n                return image\n\n            request = self._trax.wait()\n\n            if request.type == \'frame\':\n                return str(request.image)\n            else:\n                return None\n\n        else:\n            if self._frame >= len(self._files):\n                return None\n            return self._files[self._frame]\n\n    def quit(self):\n        if TRAX:\n            self._trax.quit()\n        elif hasattr(self, \'_result\'):\n            with open(\'output.txt\', \'w\') as f:\n                for r in self._result:\n                    f.write(encode_region(r))\n                    f.write(\'\\n\')\n\n    def __del__(self):\n        self.quit()\n\n'"
code/vot_SiamRPN.py,0,"b'# --------------------------------------------------------\n# DaSiamRPN\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\n#!/usr/bin/python\n\nimport vot\nfrom vot import Rectangle\nimport sys\nimport cv2  # imread\nimport torch\nimport numpy as np\nfrom os.path import realpath, dirname, join\n\nfrom net import SiamRPNBIG\nfrom run_SiamRPN import SiamRPN_init, SiamRPN_track\nfrom utils import get_axis_aligned_bbox, cxy_wh_2_rect\n\n# load net\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe7\xbd\x91\xe7\xbb\x9c\nnet_file = join(realpath(dirname(__file__)), \'SiamRPNBIG.model\')\nnet = SiamRPNBIG()\nnet.load_state_dict(torch.load(net_file))\nnet.eval().cuda()\n\n# warm up\nfor i in range(10):\n    net.temple(torch.autograd.Variable(torch.FloatTensor(1, 3, 127, 127)).cuda())\n    net(torch.autograd.Variable(torch.FloatTensor(1, 3, 255, 255)).cuda())\n\n# start to track\nhandle = vot.VOT(""polygon"")\nPolygon = handle.region()\ncx, cy, w, h = get_axis_aligned_bbox(Polygon)\n\nimage_file = handle.frame()\nif not image_file:\n    sys.exit(0)\n\ntarget_pos, target_sz = np.array([cx, cy]), np.array([w, h])\nim = cv2.imread(image_file)  # HxWxC\nstate = SiamRPN_init(im, target_pos, target_sz, net)  # init tracker\nwhile True:\n    image_file = handle.frame()\n    if not image_file:\n        break\n    im = cv2.imread(image_file)  # HxWxC\n    state = SiamRPN_track(state, im)  # track\n    res = cxy_wh_2_rect(state[\'target_pos\'], state[\'target_sz\'])\n\n    handle.report(Rectangle(res[0], res[1], res[2], res[3]))\n\n'"
code_v1.0/__init__.py,0,b''
code_v1.0/data_loader.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-11-09 17:22:06\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-11-23 17:01:54\nimport sys\nimport os\nimport os.path as osp\nimport time\nimport cv2\nimport torch\nimport random\nfrom PIL import Image, ImageOps, ImageStat, ImageDraw\nfrom torchvision import datasets, transforms, utils\nimport numpy as np\n\n\nclass Anchor_ms(object):\n    """"""\n    stable version for anchor generator\n    """"""\n    def __init__(self, feature_w, feature_h):\n        self.w      = feature_w\n        self.h      = feature_h\n        self.base   = 64                   # base size for anchor box\n        self.stride = 15                   # center point shift stride\n        self.scale  = [1/3, 1/2, 1, 2, 3]  # aspect ratio\n        self.anchors= self.gen_anchors()   # xywh\n        self.eps    = 0.01\n    \n    def gen_single_anchor(self):\n        scale = np.array(self.scale, dtype = np.float32)\n        s = self.base * self.base\n        w, h = np.sqrt(s/scale), np.sqrt(s*scale) \n        c_x, c_y = (self.stride-1)//2, (self.stride-1)//2\n        anchor = np.vstack([c_x*np.ones_like(scale, dtype=np.float32), c_y*np.ones_like(scale, dtype=np.float32), w, h]).transpose()\n        anchor = self.center_to_corner(anchor)\n        return anchor\n\n    def gen_anchors(self):\n        anchor=self.gen_single_anchor()\n        k = anchor.shape[0]\n        delta_x, delta_y = [x*self.stride for x in range(self.w)], [y*self.stride for y in range(self.h)]\n        shift_x, shift_y = np.meshgrid(delta_x, delta_y)\n        shifts = np.vstack([shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel()]).transpose()\n        a = shifts.shape[0]\n        anchors = (anchor.reshape((1,k,4))+shifts.reshape((a,1,4))).reshape((a*k, 4)) # corner format   \n        anchors = self.corner_to_center(anchors)\n        return anchors\n\n    # float\n    def diff_anchor_gt(self, gt):\n        eps = self.eps\n        anchors, gt = self.anchors.copy(), gt.copy()\n        diff = np.zeros_like(anchors, dtype = np.float32)\n        diff[:,0] = (gt[0] - anchors[:,0])/(anchors[:,2] + eps)\n        diff[:,1] = (gt[1] - anchors[:,1])/(anchors[:,3] + eps)\n        diff[:,2] = np.log((gt[2] + eps)/(anchors[:,2] + eps))\n        diff[:,3] = np.log((gt[3] + eps)/(anchors[:,3] + eps))\n        return diff\n\n    # float\n    def center_to_corner(self, box):\n        box = box.copy()\n        box_ = np.zeros_like(box, dtype = np.float32)\n        box_[:,0]=box[:,0]-(box[:,2]-1)/2\n        box_[:,1]=box[:,1]-(box[:,3]-1)/2\n        box_[:,2]=box[:,0]+(box[:,2]-1)/2\n        box_[:,3]=box[:,1]+(box[:,3]-1)/2\n        box_ = box_.astype(np.float32)\n        return box_\n\n    # float\n    def corner_to_center(self, box):\n        box = box.copy()\n        box_ = np.zeros_like(box, dtype = np.float32)\n        box_[:,0]=box[:,0]+(box[:,2]-box[:,0])/2\n        box_[:,1]=box[:,1]+(box[:,3]-box[:,1])/2\n        box_[:,2]=(box[:,2]-box[:,0])\n        box_[:,3]=(box[:,3]-box[:,1])\n        box_ = box_.astype(np.float32)\n        return box_\n\n    def pos_neg_anchor(self, gt, pos_num=16, neg_num=48, threshold_pos=0.5, threshold_neg=0.1):\n        gt = gt.copy()\n        gt_corner = self.center_to_corner(np.array(gt, dtype = np.float32).reshape(1, 4))\n        an_corner = self.center_to_corner(np.array(self.anchors, dtype = np.float32))\n        iou_value = self.iou(an_corner, gt_corner).reshape(-1) #(1445)\n        max_iou   = max(iou_value)\n        pos, neg  = np.zeros_like(iou_value, dtype=np.int32), np.zeros_like(iou_value, dtype=np.int32)\n        \n        # pos\n        pos_cand = np.argsort(iou_value)[::-1][:30]\n        pos_index = np.random.choice(pos_cand, pos_num, replace = False)\n        if max_iou > threshold_pos:\n            pos[pos_index] = 1\n\n        # neg\n        neg_cand = np.where(iou_value < threshold_neg)[0]\n        neg_ind = np.random.choice(neg_cand, neg_num, replace = False)\n        neg[neg_ind] = 1\n\n        return pos, neg        \n\n    def iou(self,box1,box2):\n        box1, box2 = box1.copy(), box2.copy()\n        N=box1.shape[0]\n        K=box2.shape[0]\n        box1=np.array(box1.reshape((N,1,4)))+np.zeros((1,K,4))#box1=[N,K,4]\n        box2=np.array(box2.reshape((1,K,4)))+np.zeros((N,1,4))#box1=[N,K,4]\n        x_max=np.max(np.stack((box1[:,:,0],box2[:,:,0]),axis=-1),axis=2)\n        x_min=np.min(np.stack((box1[:,:,2],box2[:,:,2]),axis=-1),axis=2)\n        y_max=np.max(np.stack((box1[:,:,1],box2[:,:,1]),axis=-1),axis=2)\n        y_min=np.min(np.stack((box1[:,:,3],box2[:,:,3]),axis=-1),axis=2)\n        tb=x_min-x_max\n        lr=y_min-y_max\n        tb[np.where(tb<0)]=0\n        lr[np.where(lr<0)]=0\n        over_square=tb*lr\n        all_square=(box1[:,:,2]-box1[:,:,0])*(box1[:,:,3]-box1[:,:,1])+(box2[:,:,2]-box2[:,:,0])*(box2[:,:,3]-box2[:,:,1])-over_square\n        return over_square/all_square\n\nclass TrainDataLoader(object):\n    def __init__(self, img_dir_path, out_feature = 17, max_inter = 80, check = False, tmp_dir = \'../tmp/visualization\'):\n        assert osp.isdir(img_dir_path), \'input img_dir_path error\'\n        self.anchor_generator = Anchor_ms(out_feature, out_feature)\n        self.img_dir_path = img_dir_path # this is a root dir contain subclass\n        self.max_inter = max_inter\n        self.sub_class_dir = [sub_class_dir for sub_class_dir in os.listdir(img_dir_path) if os.path.isdir(os.path.join(img_dir_path, sub_class_dir))] \n        self.anchors = self.anchor_generator.gen_anchors() #centor\n        self.ret = {}\n        self.check = check\n        self.tmp_dir = self.init_dir(tmp_dir)\n        self.ret[\'tmp_dir\'] = tmp_dir\n        self.ret[\'check\'] = check\n        self.count = 0\n\n    def init_dir(self, tmp_dir):\n        if not osp.exists(tmp_dir):\n            os.makedirs(tmp_dir)\n        return tmp_dir\n\n    def get_transform_for_train(self):\n        transform_list = []\n        transform_list.append(transforms.ToTensor())\n        transform_list.append(transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)))\n        return transforms.Compose(transform_list)\n\n    # tuple  \n    def _average(self):\n        assert self.ret.__contains__(\'template_img_path\'), \'no template path\'\n        assert self.ret.__contains__(\'detection_img_path\'),\'no detection path\'\n        template = Image.open(self.ret[\'template_img_path\'])\n        detection= Image.open(self.ret[\'detection_img_path\'])\n        \n        mean_template = tuple(map(round, ImageStat.Stat(template).mean))\n        mean_detection= tuple(map(round, ImageStat.Stat(detection).mean))\n        self.ret[\'mean_template\'] = mean_template\n        self.ret[\'mean_detection\']= mean_detection\n\n    def _pick_img_pairs(self, index_of_subclass):\n        # img_dir_path -> sub_class_dir_path -> template_img_path\n        # use index_of_subclass to select a sub directory\n        assert index_of_subclass < len(self.sub_class_dir), \'index_of_subclass should less than total classes\'\n        sub_class_dir_basename = self.sub_class_dir[index_of_subclass]\n        sub_class_dir_path = os.path.join(self.img_dir_path, sub_class_dir_basename)\n        sub_class_img_name = [img_name for img_name in os.listdir(sub_class_dir_path) if not img_name.find(\'.jpg\') == -1]        \n        sub_class_img_name = sorted(sub_class_img_name)\n        sub_class_img_num  = len(sub_class_img_name)\n        sub_class_gt_name  = \'groundtruth.txt\'\n\n        # select template, detection\n        # ++++++++++++++++++++++++++++ add break in sequeence [0,0,0,0] ++++++++++++++++++++++++++++++++++\n        status = True\n        while status:\n            if self.max_inter >= sub_class_img_num-1:\n                self.max_inter = sub_class_img_num//2\n\n            template_index = np.clip(random.choice(range(0, max(1, sub_class_img_num - self.max_inter))), 0, sub_class_img_num-1)\n            detection_index= np.clip(random.choice(range(1, max(2, self.max_inter))) + template_index, 0, sub_class_img_num-1)\n\n            template_name, detection_name  = sub_class_img_name[template_index], sub_class_img_name[detection_index]\n            template_img_path, detection_img_path = osp.join(sub_class_dir_path, template_name), osp.join(sub_class_dir_path, detection_name)\n            gt_path = osp.join(sub_class_dir_path, sub_class_gt_name)\n            with open(gt_path, \'r\') as f:\n                lines = f.readlines()\n            cords_of_template_abs  = [abs(int(float(i))) for i in lines[template_index].strip(\'\\n\').split(\',\')[:4]]\n            cords_of_detection_abs = [abs(int(float(i))) for i in lines[detection_index].strip(\'\\n\').split(\',\')[:4]]\n            \n            if cords_of_template_abs[2]*cords_of_template_abs[3]*cords_of_detection_abs[2]*cords_of_detection_abs[3] != 0: \n                status = False\n            else:\n                print(\'Warning : Encounter object missing, reinitializing ...\')\n\n        # load infomation of template and detection\n        self.ret[\'template_img_path\']      = template_img_path\n        self.ret[\'detection_img_path\']     = detection_img_path\n        self.ret[\'template_target_x1y1wh\'] = [int(float(i)) for i in lines[template_index].strip(\'\\n\').split(\',\')[:4]]\n        self.ret[\'detection_target_x1y1wh\']= [int(float(i)) for i in lines[detection_index].strip(\'\\n\').split(\',\')[:4]]\n        t1, t2 = self.ret[\'template_target_x1y1wh\'].copy(), self.ret[\'detection_target_x1y1wh\'].copy()\n        self.ret[\'template_target_xywh\'] = np.array([t1[0]+t1[2]//2, t1[1]+t1[3]//2, t1[2], t1[3]], np.float32)\n        self.ret[\'detection_target_xywh\']= np.array([t2[0]+t2[2]//2, t2[1]+t2[3]//2, t2[2], t2[3]], np.float32)\n        self.ret[\'anchors\'] = self.anchors\n        self._average()\n\n        if self.check:\n            s = osp.join(self.tmp_dir, \'0_check_bbox_groundtruth\')\n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            template = Image.open(self.ret[\'template_img_path\'])\n            x, y, w, h = self.ret[\'template_target_xywh\'].copy()\n            x1, y1, x3, y3 = int(x-w//2), int(y-h//2), int(x+w//2), int(y+h//2)\n            draw = ImageDraw.Draw(template)\n            draw.line([(x1, y1), (x3, y1), (x3, y3), (x1, y3), (x1, y1)], width=1, fill=\'red\')\n            save_path = osp.join(s,\'idx_{:04d}_class_{}_template_idx_{}.jpg\'.format(self.count, sub_class_dir_basename, template_index))\n            template.save(save_path)\n\n            detection = Image.open(self.ret[\'detection_img_path\'])\n            x, y, w, h = self.ret[\'detection_target_xywh\'].copy()\n            x1, y1, x3, y3 = int(x-w//2), int(y-h//2), int(x+w//2), int(y+h//2) \n            draw = ImageDraw.Draw(detection)\n            draw.line([(x1, y1), (x3, y1), (x3, y3), (x1, y3), (x1, y1)], width=1, fill=\'red\')\n            save_path = osp.join(s,\'idx_{:04d}_class_{}_detection_idx_{}.jpg\'.format(self.count, sub_class_dir_basename, detection_index))\n            detection.save(save_path)\n\n    def _pad_crop_resize(self):\n        template_img, detection_img = Image.open(self.ret[\'template_img_path\']), Image.open(self.ret[\'detection_img_path\'])\n\n        w, h = template_img.size\n        cx, cy, tw, th = self.ret[\'template_target_xywh\']\n        p = round((tw + th)/2, 2)\n        template_square_size  = int(np.sqrt((tw + p)*(th + p))) #a\n        detection_square_size = int(template_square_size * 2)   #A =2a\n        \n        # pad\n        detection_lt_x, detection_lt_y = cx - detection_square_size//2, cy - detection_square_size//2\n        detection_rb_x, detection_rb_y = cx + detection_square_size//2, cy + detection_square_size//2\n        left   = -detection_lt_x if detection_lt_x < 0 else 0\n        top    = -detection_lt_y if detection_lt_y < 0 else 0\n        right  =  detection_rb_x - w if detection_rb_x > w else 0\n        bottom =  detection_rb_y - h if detection_rb_y > h else 0\n        padding = tuple(map(int, [left, top, right, bottom]))\n        new_w, new_h = left + right + w, top + bottom + h\n\n        # pad load\n        self.ret[\'padding\'] = padding\n        self.ret[\'new_template_img_padding_size\'] = (new_w, new_h)\n        self.ret[\'new_template_img_padding\'] = ImageOps.expand(template_img,  border=padding, fill=self.ret[\'mean_template\'])\n        self.ret[\'new_detection_img_padding\']= ImageOps.expand(detection_img, border=padding, fill=self.ret[\'mean_detection\'])\n            \n        # crop\n        tl = cx + left - template_square_size//2\n        tt = cy + top  - template_square_size//2\n        tr = new_w - tl - template_square_size\n        tb = new_h - tt - template_square_size\n        self.ret[\'template_cropped\'] = ImageOps.crop(self.ret[\'new_template_img_padding\'].copy(), (tl, tt, tr, tb))\n\n        dl = np.clip(cx + left - detection_square_size//2, 0, new_w - detection_square_size)\n        dt = np.clip(cy + top  - detection_square_size//2, 0, new_h - detection_square_size)\n        dr = np.clip(new_w - dl - detection_square_size, 0, new_w - detection_square_size)\n        db = np.clip(new_h - dt - detection_square_size, 0, new_h - detection_square_size ) \n        self.ret[\'detection_cropped\']= ImageOps.crop(self.ret[\'new_detection_img_padding\'].copy(), (dl, dt, dr, db))  \n\n        self.ret[\'detection_tlcords_of_original_image\'] = (cx - detection_square_size//2 , cy - detection_square_size//2)\n        self.ret[\'detection_tlcords_of_padding_image\']  = (cx - detection_square_size//2 + left, cy - detection_square_size//2 + top)\n        self.ret[\'detection_rbcords_of_padding_image\']  = (cx + detection_square_size//2 + left, cy + detection_square_size//2 + top)\n        \n        # resize\n        self.ret[\'template_cropped_resized\'] = self.ret[\'template_cropped\'].copy().resize((127, 127))\n        self.ret[\'detection_cropped_resized\']= self.ret[\'detection_cropped\'].copy().resize((256, 256))\n        self.ret[\'template_cropprd_resized_ratio\'] = round(127/template_square_size, 2)\n        self.ret[\'detection_cropped_resized_ratio\'] = round(256/detection_square_size, 2)\n        \n        # compute target in detection, and then we will compute IOU\n        # whether target in detection part\n        x, y, w, h = self.ret[\'detection_target_xywh\']\n        self.ret[\'target_tlcords_of_padding_image\'] = np.array([int(x+left-w//2), int(y+top-h//2)], dtype = np.float32)\n        self.ret[\'target_rbcords_of_padding_image\'] = np.array([int(x+left+w//2), int(y+top+h//2)], dtype = np.float32)\n        if self.check:\n            s = osp.join(self.tmp_dir, \'1_check_detection_target_in_padding\')\n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            im = self.ret[\'new_detection_img_padding\']\n            draw = ImageDraw.Draw(im)\n            x1, y1 = self.ret[\'target_tlcords_of_padding_image\']\n            x2, y2 = self.ret[\'target_rbcords_of_padding_image\']\n            draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\') # target in padding\n\n            x1, y1 = self.ret[\'detection_tlcords_of_padding_image\']\n            x2, y2 = self.ret[\'detection_rbcords_of_padding_image\']\n            draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'green\') # detection in padding\n\n            save_path = osp.join(s, \'{:04d}.jpg\'.format(self.count))\n            im.save(save_path) \n\n        ### use cords of padding to compute cords about detection \n        ### modify cords because not all the object in the detection\n        x11, y11 = self.ret[\'detection_tlcords_of_padding_image\']\n        x12, y12 = self.ret[\'detection_rbcords_of_padding_image\']\n        x21, y21 = self.ret[\'target_tlcords_of_padding_image\']\n        x22, y22 = self.ret[\'target_rbcords_of_padding_image\']\n        x1_of_d, y1_of_d, x3_of_d, y3_of_d = int(x21-x11), int(y21-y11), int(x22-x11), int(y22-y11)\n        x1 = np.clip(x1_of_d, 0, x12-x11).astype(np.float32)\n        y1 = np.clip(y1_of_d, 0, y12-y11).astype(np.float32)\n        x2 = np.clip(x3_of_d, 0, x12-x11).astype(np.float32)\n        y2 = np.clip(y3_of_d, 0, y12-y11).astype(np.float32)\n        self.ret[\'target_in_detection_x1y1x2y2\']=np.array([x1, y1, x2, y2], dtype = np.float32)\n        if self.check:\n            s = osp.join(self.tmp_dir, \'2_check_target_in_cropped_detection\')\n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            im = self.ret[\'detection_cropped\'].copy()\n            draw = ImageDraw.Draw(im)\n            draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')\n            save_path = osp.join(s, \'{:04d}.jpg\'.format(self.count))\n            im.save(save_path)\n\n        cords_in_cropped_detection = np.array((x1, y1, x2, y2), dtype = np.float32)\n        cords_in_cropped_resized_detection = (cords_in_cropped_detection * self.ret[\'detection_cropped_resized_ratio\']).astype(np.int32)\n        x1, y1, x2, y2 = cords_in_cropped_resized_detection\n        cx, cy, w, h = (x1+x2)//2, (y1+y2)//2, x2-x1, y2-y1\n        self.ret[\'target_in_resized_detection_x1y1x2y2\'] = np.array((x1, y1, x2, y2), dtype = np.int32)\n        self.ret[\'target_in_resized_detection_xywh\']     = np.array((cx, cy, w,  h) , dtype = np.int32)\n        self.ret[\'area_target_in_resized_detection\'] = w * h\n\n        if self.check:\n            s = osp.join(self.tmp_dir, \'3_check_target_in_cropped_resized_detection\')\n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            im = self.ret[\'detection_cropped_resized\'].copy()\n            draw = ImageDraw.Draw(im)\n            draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')\n            save_path = osp.join(s, \'{:04d}.jpg\'.format(self.count))\n            im.save(save_path)\n\n    def _generate_pos_neg_diff(self):\n        gt_box_in_detection = self.ret[\'target_in_resized_detection_xywh\'].copy()\n        pos, neg = self.anchor_generator.pos_neg_anchor(gt_box_in_detection)\n        diff     = self.anchor_generator.diff_anchor_gt(gt_box_in_detection)\n        pos, neg, diff = pos.reshape((-1, 1)), neg.reshape((-1,1)), diff.reshape((-1, 4))\n        class_target = np.array([-100.] * self.anchors.shape[0], np.int32) \n        \n        # pos\n        pos_index = np.where(pos == 1)[0]\n        pos_num = len(pos_index)\n        self.ret[\'pos_anchors\'] = np.array(self.ret[\'anchors\'][pos_index, :], dtype=np.int32) if not pos_num == 0 else None\n        if pos_num > 0:\n            class_target[pos_index] = 1\n        \n        # neg \n        neg_index = np.where(neg == 1)[0]\n        neg_num = len(neg_index)\n        class_target[neg_index] = 0\n\n        # draw pos and neg anchor box\n        if self.check:\n            s = osp.join(self.tmp_dir, \'4_check_pos_neg_anchors\')\n            if not os.path.exists(s):\n                os.makedirs(s)\n            \n            im = self.ret[\'detection_cropped_resized\'].copy()\n            draw = ImageDraw.Draw(im)\n            if pos_num == 16:\n                for i in range(pos_num):\n                    index = pos_index[i]\n                    cx ,cy, w, h = self.anchors[index]\n                    if w * h == 0:\n                        print(\'anchor area error\')\n                        sys.exit(0) \n                    x1, y1, x2, y2 = int(cx-w/2), int(cy-h/2), int(cx+w/2), int(cy+h/2)\n                    draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')\n            \n            for i in range(neg_num):\n                index = neg_index[i]\n                cx ,cy, w, h = self.anchors[index]\n                x1, y1, x2, y2 = int(cx-w/2), int(cy-h/2), int(cx+w/2), int(cy+h/2)\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'green\')  \n            save_path = osp.join(s, \'{:04d}.jpg\'.format(self.count))\n            im.save(save_path)\n        \n        #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n        # when training, this part can be delete to speed up\n        """"""\n        if self.check:\n            s = osp.join(self.tmp_dir, \'5_check_all_anchors\') \n            if not os.path.exists(s):\n                os.makedirs(s)\n\n            for i in range(self.anchors.shape[0]):\n                x1, y1, x2, y2 = self.ret[\'target_in_resized_detection_x1y1x2y2\']\n                im = self.ret[\'detection_cropped_resized\'].copy()\n                draw = ImageDraw.Draw(im)\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')\n\n                cx, cy, w, h = self.anchors[i]\n                x1, y1, x2, y2 = cx-w//2,cy-h//2,cx+w//2,cy+h//2\n                draw = ImageDraw.Draw(im)\n                draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'green\')\n                save_path = osp.join(s, \'img_{:04d}_anchor_{:05d}.jpg\'.format(self.count, i))\n                im.save(save_path)\n        """"""    \n        #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n    \n        class_logits = class_target.reshape(-1, 1)\n        pos_neg_diff = np.hstack((class_logits, diff))\n        self.ret[\'pos_neg_diff\'] = pos_neg_diff\n        return pos_neg_diff\n\n    def _tranform(self):\n        """"""PIL to Tensor""""""\n        template_pil = self.ret[\'template_cropped_resized\'].copy()\n        detection_pil= self.ret[\'detection_cropped_resized\'].copy()\n        pos_neg_diff = self.ret[\'pos_neg_diff\'].copy()\n\n        transform = self.get_transform_for_train()\n        template_tensor = transform(template_pil)\n        detection_tensor= transform(detection_pil)\n        self.ret[\'template_tensor\'] = template_tensor.unsqueeze(0)\n        self.ret[\'detection_tensor\']= detection_tensor.unsqueeze(0)\n        self.ret[\'pos_neg_diff_tensor\'] = torch.Tensor(pos_neg_diff)\n\n\n    def __get__(self, index):\n        self._pick_img_pairs(index)\n        self._pad_crop_resize()\n        self._generate_pos_neg_diff()\n        self._tranform()\n        self.count += 1\n        return self.ret\n    \n    def __len__(self):\n        return len(self.sub_class_dir)\n\nif __name__ == \'__main__\':\n    # we will do a test for dataloader\n    loader = TrainDataLoader(\'/home/song/srpn/dataset/simple_vot13\', check = True)\n    #print(loader.__len__())\n    index_list = range(loader.__len__())\n    for i in range(1000):\n        ret = loader.__get__(random.choice(index_list))\n        label = ret[\'pos_neg_diff\'][:, 0].reshape(-1)\n        pos_index = list(np.where(label == 1)[0])\n        pos_num = len(pos_index)\n        print(pos_index)\n        print(pos_num)\n        if pos_num != 0 and pos_num != 16:\n            print(pos_num)\n            sys.exit(0)\n        print(i)\n\n\n\n'"
code_v1.0/net.py,0,"b""# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-11-05 11:16:24\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-11-23 15:44:42\nimport torch.utils.model_zoo as model_zoo\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport time\nmodel_urls = {'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth'}\nclass SiameseRPN(nn.Module):\n    def __init__(self, test_video=False):\n        super(SiameseRPN, self).__init__()\n        self.features = nn.Sequential(                  #1, 3, 256, 256\n            nn.Conv2d(3, 64, kernel_size=11, stride=2), #1, 64,123, 123\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),      #1, 64, 60,  60\n            nn.Conv2d(64, 192, kernel_size=5),          #1,192, 56,  56\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),      #1,192, 27,  27\n            nn.Conv2d(192, 384, kernel_size=3),         #1,384, 25,  25 \n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3),         #1,256, 23,  23\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3),         #1,256, 21,  21\n        )\n        \n        self.k = 5\n        self.s = 4\n        self.conv1 = nn.Conv2d(256, 2*self.k*256, kernel_size=3)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(256, 4*self.k*256, kernel_size=3)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(256, 256, kernel_size=3)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.conv4 = nn.Conv2d(256, 256, kernel_size=3)\n        self.relu4 = nn.ReLU(inplace=True)\n\n        self.cconv = nn.Conv2d(256, 2* self.k, kernel_size = 4, bias = False)\n        self.rconv = nn.Conv2d(256, 4* self.k, kernel_size = 4, bias = False)\n        \n        #self.reset_params() # we will not reset parameter \n        \n    def reset_params(self):\n        pretrained_dict = model_zoo.load_url(model_urls['alexnet'])\n        model_dict = self.state_dict()\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        model_dict.update(pretrained_dict)\n        self.load_state_dict(model_dict)\n        print('Load Alexnet models Done' )\n            \n    def forward(self, template, detection):\n        template = self.features(template)            \n        detection = self.features(detection)         \n        \n        ckernal = self.conv1(template)\n        ckernal = ckernal.view(2* self.k, 256, 4, 4)\n        cinput  = self.conv3(detection)                \n\n\n        rkernal = self.conv2(template)\n        rkernal = rkernal.view(4* self.k, 256, 4, 4)\n        rinput  = self.conv4(detection)\n\n        coutput = F.conv2d(cinput, ckernal)\n        routput = F.conv2d(rinput, rkernal) \n\n        coutput = coutput.squeeze().permute(1,2,0).reshape(-1, 2)\n        routput = routput.squeeze().permute(1,2,0).reshape(-1, 4)\n        return coutput, routput\n\n    def resume(self, weight):\n        checkpoint = torch.load(weight)\n        self.load_state_dict(checkpoint)\n        print('Resume checkpoint from {}'.format(weight))\n\n\nif __name__ == '__main__':\n    model = SiameseRPN()\n\n    template = torch.ones((1, 3, 127, 127))\n    detection= torch.ones((1, 3, 256, 256))\n\n    y1, y2 = model(template, detection)\n    print(y1.shape) #[1, 10, 17, 17]\n    print(y2.shape) #[1, 20, 17, 17]15\n"""
code_v1.0/train_siamrpn.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-11-09 10:06:59\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-11-23 17:26:44\nimport os\nimport os.path as osp\nimport random\nimport time\nimport sys; sys.path.append(\'../\')\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as F\nimport argparse\nfrom PIL import Image, ImageOps, ImageStat, ImageDraw\nfrom data_loader import TrainDataLoader\nfrom net import SiameseRPN\nfrom torch.nn import init\nfrom shapely.geometry import Polygon\n\nparser = argparse.ArgumentParser(description=\'PyTorch SiameseRPN Training\')\n\nparser.add_argument(\'--train_path\', default=\'/home/song/srpn/dataset/vot13\', metavar=\'DIR\',help=\'path to dataset\')\n\nparser.add_argument(\'--weight_dir\', default=\'/home/song/srpn/weight\', metavar=\'DIR\',help=\'path to weight\')\n\nparser.add_argument(\'--checkpoint_path\', default=None, help=\'resume\')\n\nparser.add_argument(\'--max_epoches\', default=10000, type=int, metavar=\'N\', help=\'number of total epochs to run\')\n\nparser.add_argument(\'--max_batches\', default=0, type=int, metavar=\'N\', help=\'number of batch in one epoch\')\n\nparser.add_argument(\'--init_type\',  default=\'xavier\', type=str, metavar=\'INIT\', help=\'init net\')\n\nparser.add_argument(\'--lr\', default=0.001, type=float, metavar=\'LR\', help=\'initial learning rate\')\n\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'momentum\', help=\'momentum\')\n\nparser.add_argument(\'--weight_decay\', \'--wd\', default=5e-5, type=float, metavar=\'W\', help=\'weight decay (default: 1e-4)\')\n\nparser.add_argument(\'--debug\', default=False, type=bool,  help=\'whether to debug\')\n\ndef main():\n    """""" train dataloader """"""\n    args = parser.parse_args()\n    data_loader = TrainDataLoader(args.train_path, check = args.debug)\n    if not os.path.exists(args.weight_dir):\n        os.makedirs(args.weight_dir)\n\n    """""" compute max_batches """"""\n    for root, dirs, files in os.walk(args.train_path):\n        for dirname in dirs:\n            dir_path = os.path.join(root, dirname)\n            args.max_batches += len(os.listdir(dir_path))\n\n    """""" Model on gpu """"""\n    model = SiameseRPN()\n    model = model.cuda()\n    cudnn.benchmark = True\n\n    """""" loss and optimizer """"""\n    criterion = MultiBoxLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay = args.weight_decay)\n\n    """""" load weights """"""\n    init_weights(model)\n    if not args.checkpoint_path == None:\n        assert os.path.isfile(args.checkpoint_path), \'{} is not valid checkpoint_path\'.format(args.checkpoint_path)\n        try:\n            checkpoint = torch.load(args.checkpoint_path)\n            start = checkpoint[\'epoch\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        except:\n            start = 0\n            init_weights(model)\n    else:\n        start = 0\n\n    """""" train phase """"""\n    closses, rlosses, tlosses = AverageMeter(), AverageMeter(), AverageMeter()\n    steps = 0\n    for epoch in range(start, args.max_epoches):\n        cur_lr = adjust_learning_rate(args.lr, optimizer, epoch, gamma=0.1)\n        index_list = range(data_loader.__len__()) \n        for example in range(args.max_batches):\n            ret = data_loader.__get__(random.choice(index_list)) \n            template = ret[\'template_tensor\'].cuda()\n            detection= ret[\'detection_tensor\'].cuda()\n            pos_neg_diff = ret[\'pos_neg_diff_tensor\'].cuda()\n            cout, rout = model(template, detection)\n            predictions, targets = (cout, rout), pos_neg_diff\n            closs, rloss, loss, reg_pred, reg_target, pos_index, neg_index = criterion(predictions, targets)\n            closs_ = closs.cpu().item()\n\n            if np.isnan(closs_): \n               sys.exit(0)\n\n            closses.update(closs.cpu().item())\n            rlosses.update(rloss.cpu().item())\n            tlosses.update(loss.cpu().item())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            steps += 1\n\n                        \n            cout = cout.cpu().detach().numpy()\n            score = 1/(1 + np.exp(cout[:,0]-cout[:,1]))\n\n            # ++++++++++++ post process below just for debug ++++++++++++++++++++++++\n            # ++++++++++++++++++++ v1.0 add penalty +++++++++++++++++++++++++++++++++\n            if ret[\'pos_anchors\'] is not None:\n                penalty_k = 0.055\n                tx, ty, tw, th = ret[\'template_target_xywh\'].copy()\n                tw *= ret[\'template_cropprd_resized_ratio\']\n                th *= ret[\'template_cropprd_resized_ratio\']\n\n                anchors = ret[\'anchors\'].copy()\n                w = anchors[:,2] * np.exp(reg_pred[:, 2].cpu().detach().numpy())\n                h = anchors[:,3] * np.exp(reg_pred[:, 3].cpu().detach().numpy())\n\n                eps = 1e-2\n                change_w = np.maximum(w/(tw+eps), tw/(w+eps))\n                change_h = np.maximum(h/(th+eps), th/(h+eps))\n                penalty = np.exp(-(change_w + change_h - 1) * penalty_k)\n                pscore = score * penalty\n            else:\n                pscore = score\n\n            # +++++++++++++++++++ v1.0 add window default cosine ++++++++++++++++++++++\n            score_size = 17\n            window_influence = 0.42\n            window = (np.outer(np.hanning(score_size), np.hanning(score_size)).reshape(17,17,1) + np.zeros((1, 1, 5))).reshape(-1)\n            pscore = pscore * (1 - window_influence) + window * window_influence\n            score_old = score\n            score = pscore #from 0.2 - 0.7\n\n            # +++++++++++++++++++ v1.0 add nms ++++++++++++++++++++++++++++++++++++++++++++\n            nms = False\n            nms_threshold = 0.6\n            start = time.time()\n            anchors = ret[\'anchors\'].copy()\n            x = anchors[:,0] + anchors[:, 2] * reg_pred[:, 0].cpu().detach().numpy()\n            y = anchors[:,1] + anchors[:, 3] * reg_pred[:, 1].cpu().detach().numpy()\n            w = anchors[:,2] * np.exp(reg_pred[:, 2].cpu().detach().numpy())\n            h = anchors[:,3] * np.exp(reg_pred[:, 3].cpu().detach().numpy())\n            x1 = np.clip(x-w//2, 0, 256)\n            x2 = np.clip(x+w//2, 0, 256)\n            x3 = np.clip(x+w//2, 0, 256)\n            x4 = np.clip(x-w//2, 0, 256)\n            y1 = np.clip(y-h//2, 0, 256)\n            y2 = np.clip(y-h//2, 0, 256)\n            y3 = np.clip(y+h//2, 0, 256)\n            y4 = np.clip(y+h//2, 0, 256)\n            slist = map(reshape, [x1, y1, x2, y2, x3, y3, x4, y4, score])\n            s = np.hstack(slist)\n            maxscore = max(s[:, 8])\n            if nms and maxscore > nms_threshold:\n                proposals = standard_nms(s, nms_threshold)\n                proposals = proposals if proposals.shape[0] != 0 else s\n                print(\'nms spend {:.2f}ms\'.format(1000*(time.time()-start)))\n            else:\n                proposals = s\n            # ++++++++++++++++++++ debug for class ++++++++++++++++++++++++++++++++++++\n            # print(score[pos_index])  # this should tend to be 1\n            # print(score[neg_index])  # this should tend to be 0\n\n\n            # ++++++++++++++++++++ debug for reg ++++++++++++++++++++++++++++++++++++++\n            tmp_dir = \'/home/song/srpn/tmp/visualization/7_check_train_phase_debug_pos_anchors\'\n            if not os.path.exists(tmp_dir):\n                os.makedirs(tmp_dir)\n            detection = ret[\'detection_cropped_resized\'].copy()\n            draw = ImageDraw.Draw(detection)\n            pos_anchors = ret[\'pos_anchors\'].copy() if ret[\'pos_anchors\'] is not None else None\n            \n            if pos_anchors is not None:\n                # draw pos anchors\n                x = pos_anchors[:, 0]\n                y = pos_anchors[:, 1]\n                w = pos_anchors[:, 2]\n                h = pos_anchors[:, 3]\n                x1s, y1s, x2s, y2s = x - w//2, y - h//2, x + w//2, y + h//2\n                for i in range(16):\n                    x1, y1, x2, y2 = x1s[i], y1s[i], x2s[i], y2s[i]\n                    draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'white\') # pos anchor\n\n                # pos anchor transform to red box after prediction\n                x = pos_anchors[:,0] + pos_anchors[:, 2] * reg_pred[pos_index, 0].cpu().detach().numpy()\n                y = pos_anchors[:,1] + pos_anchors[:, 3] * reg_pred[pos_index, 1].cpu().detach().numpy()\n                w = pos_anchors[:,2] * np.exp(reg_pred[pos_index, 2].cpu().detach().numpy())\n                h = pos_anchors[:,3] * np.exp(reg_pred[pos_index, 3].cpu().detach().numpy())\n                x1s, y1s, x2s, y2s = x - w//2, y - h//2, x + w//2, y + h//2\n                for i in range(16):\n                    x1, y1, x2, y2 = x1s[i], y1s[i], x2s[i], y2s[i]\n                    draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'red\')  # predict(white -> red)\n                \n                # pos anchor should be transformed to green gt box, if red and green is same, it is overfitting\n                x = pos_anchors[:,0] + pos_anchors[:, 2] * reg_target[pos_index, 0].cpu().detach().numpy()\n                y = pos_anchors[:,1] + pos_anchors[:, 3] * reg_target[pos_index, 1].cpu().detach().numpy()\n                w = pos_anchors[:,2] * np.exp(reg_target[pos_index, 2].cpu().detach().numpy())\n                h = pos_anchors[:,3] * np.exp(reg_target[pos_index, 3].cpu().detach().numpy())\n                x1s, y1s, x2s, y2s = x - w//2, y-h//2, x + w//2, y + h//2\n                for i in range(16):\n                    x1, y1, x2, y2 = x1s[i], y1s[i], x2s[i], y2s[i]\n                    draw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill=\'green\') # gt  (white -> green)\n                x1, y1, x3, y3 = x1s[0], y1s[0], x2s[0], y2s[0]\n            else:\n                x1, y1, x3, y3 = 0, 0, 0, 0\n            # top1 proposal after nms (white)\n            if nms:\n                index = np.argsort(proposals[:, 8])[::-1][0]\n                x1, y1, x2, y2, x3, y3, x4, y4, _ = proposals[index]\n                draw.line([(x1, y1), (x2, y2), (x3, y3), (x4, y4), (x1, y1)], width=3, fill=\'yellow\')\n            save_path = osp.join(tmp_dir, \'epoch_{:010d}_{:010d}_anchor_pred.jpg\'.format(epoch, example))\n            detection.save(save_path)\n\n            \n            # +++++++++++++++++++ v1.0 restore ++++++++++++++++++++++++++++++++++++++++\n            ratio = ret[\'detection_cropped_resized_ratio\']\n            detection_cropped = ret[\'detection_cropped\'].copy()\n            detection_cropped_resized = ret[\'detection_cropped_resized\'].copy()\n            original = Image.open(ret[\'detection_img_path\'])\n            x_, y_ = ret[\'detection_tlcords_of_original_image\']\n            draw = ImageDraw.Draw(original)\n            w, h = original.size\n            """""" un resized """"""\n            x1, y1, x3, y3 = x1/ratio, y1/ratio, y3/ratio, y3/ratio\n\n            """""" un cropped """"""\n            x1 = np.clip(x_ + x1, 0, w-1).astype(np.int32) # uncropped #target_of_original_img\n            y1 = np.clip(y_ + y1, 0, h-1).astype(np.int32)\n            x3 = np.clip(x_ + x3, 0, w-1).astype(np.int32)\n            y3 = np.clip(y_ + y3, 0, h-1).astype(np.int32)\n\n            draw.line([(x1, y1), (x3, y1), (x3, y3), (x1, y3), (x1, y1)], width=3, fill=\'yellow\')\n            save_path = osp.join(tmp_dir, \'epoch_{:010d}_{:010d}_restore.jpg\'.format(epoch, example))\n            original.save(save_path)\n            \n            print(""Epoch:{:04d}\\texample:{:06d}/{:06d}({:.2f})%\\tsteps:{:010d}\\tlr:{:.7f}\\tcloss:{:.4f}\\trloss:{:.4f}\\ttloss:{:.4f}"".format(epoch, example+1, args.max_batches, 100*(example+1)/args.max_batches, steps, cur_lr, closses.avg, rlosses.avg, tlosses.avg ))\n\n        if steps % 10000 == 0:\n            file_path = os.path.join(args.weight_dir, \'weights-{:07d}.pth.tar\'.format(steps))\n            state = {\n            \'epoch\' :epoch+1,\n            \'state_dict\' :model.state_dict(),\n            \'optimizer\' : optimizer.state_dict(),\n            }\n            torch.save(state, file_path)\n\ndef intersection(g, p):\n    g = Polygon(g[:8].reshape((4, 2)))\n    p = Polygon(p[:8].reshape((4, 2)))\n    if not g.is_valid or not p.is_valid:\n        return 0\n    inter = Polygon(g).intersection(Polygon(p)).area\n    union = g.area + p.area - inter\n    if union == 0:\n        return 0\n    else:\n        return inter/union\n\ndef standard_nms(S, thres):\n    """""" use pre_thres to filter """"""\n    index = np.where(S[:, 8] > thres)[0]\n    S = S[index] # ~ 100, 4\n\n    # Then use standard nms\n    order = np.argsort(S[:, 8])[::-1]\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        ovr = np.array([intersection(S[i], S[t]) for t in order[1:]])\n\n        inds = np.where(ovr <= thres)[0]\n        order = order[inds+1]\n    return S[keep]\n\ndef reshape(x):\n    t = np.array(x, dtype = np.float32)\n    return t.reshape(-1, 1)\n\ndef init_weights(net, init_type=\'normal\', gain=0.02):\n    def init_func(m):\n        # this will apply to each layer\n        classname = m.__class__.__name__\n        if hasattr(m, \'weight\') and (classname.find(\'conv\')!=-1 or classname.find(\'Linear\')!=-1):\n            if init_type==\'normal\':\n                init.normal_(m.weight.data, 0.0, gain)\n            elif init_type == \'xavier\':\n                init.xavier_normal_(m.weight.data, gain=gain)\n            elif init_type == \'kaiming\':\n                init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')#good for relu\n            elif init_type == \'orthogonal\':\n                init.orthogonal_(m.weight.data, gain=gain)\n            else:\n                raise NotImplementedError(\'initialization method [%s] is not implemented\' % init_type)\n            \n            if hasattr(m, \'bias\') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find(\'BatchNorm2d\') != -1:\n            init.normal_(m.weight.data, 1.0, gain)\n            init.constant_(m.bias.data, 0.0)\n    #print(\'initialize network with %s\' % init_type)\n    net.apply(init_func)\n\nclass MultiBoxLoss(nn.Module):\n    def __init__(self):\n        super(MultiBoxLoss, self).__init__()\n\n    def forward(self, predictions, targets):\n        print(\'+++++++++++++++++++++++++++++++++++++++++++++++++++\')\n        cout, rout = predictions\n        """""" class """"""\n        class_pred, class_target = cout, targets[:, 0].long()\n        pos_index , neg_index    = list(np.where(class_target == 1)[0]), list(np.where(class_target == 0)[0])\n        pos_num, neg_num         = len(pos_index), len(neg_index)\n        class_pred, class_target = class_pred[pos_index + neg_index], class_target[pos_index + neg_index]\n\n        closs = F.cross_entropy(class_pred, class_target, size_average=False, reduce=False)\n        closs = torch.div(torch.sum(closs), 64)\n\n        """""" regression """"""\n        reg_pred = rout\n        reg_target = targets[:, 1:]\n        rloss = F.smooth_l1_loss(reg_pred, reg_target, size_average=False, reduce=False) #1445, 4\n        rloss = torch.div(torch.sum(rloss, dim = 1), 4)\n        rloss = torch.div(torch.sum(rloss[pos_index]), 16)\n\n        loss = closs + rloss\n        return closs, rloss, loss, reg_pred, reg_target, pos_index, neg_index\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef adjust_learning_rate(lr, optimizer, epoch, gamma=0.1):\n    """"""Sets the learning rate to the initial LR decayed 0.9 every 50 epochs""""""\n    lr = lr * (0.9 ** (epoch // 1))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return lr\n\nif __name__ == \'__main__\':\n    main()\n \n\n'"
script/compute_max_sequence_length.py,0,"b""# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-11-22 15:35:00\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-11-22 15:38:54\nimport os\n\npath = '/home/song/srpn/dataset/vid/ILSVRC2015_VID_train_0000'\nmaxlen = 0\nminlen = 40\nfor root, dirs, files in os.walk(path):\n\tif len(files) == 0:\n\t\tcontinue\n\tmaxlen = max(len(files), maxlen)\n\tminlen = min(len(files), minlen)\n\n\tprint(maxlen)\n\tprint(minlen)"""
script/process_otb15_gt.py,1,"b""# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-10-30 16:56:21\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-10-30 16:57:54\nimport os\ndata_root = '/home/song/workspace/srpn/OTB2015/otb15'\n\nfor d in os.listdir(data_root):\n\tdir_path = os.path.join(data_root, d)\n\tos.chdir(dir_path)\n\tos.system('mkdir label')\n\tgt_dir_path = os.path.join(dir_path, 'label')\n\tif not os.path.exists(gt_dir_path):\n\t\tos.path.mkdir(gt_dir_path)\n\n\n\tgt_path = os.path.join(dir_path, 'groundtruth_rect.txt')\n\twith open(gt_path, 'r') as f:\n\t\tcontents = f.readlines()\n\n\tfor idx, line in enumerate(contents):\n\t\tprint(line.strip('\\n'))\n\t\tname = '{:04d}.txt'.format(idx+1)\n\t\tnew_gt_path = os.path.join(gt_dir_path, name)\n\t\twith open(new_gt_path, 'w') as f:\n\t\t\tf.write(line)"""
script/process_vid.py,1,"b""# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-11-22 10:48:51\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-11-22 15:06:36\nimport os\nimport sys\nvid_path = '/home/song/srpn/dataset/vid/ILSVRC2015_VID_train_0000'\n\ndef remove_space(l):\n\tns = []\n\tfor i in l:\n\t\tn = i.replace('\\t', '')\n\t\tns.append(n)\n\treturn ns\n\ndef trans(l):\n\tns = []\n\tfor i in l:\n\t\tx1, y2, w, h = map(float, i.split(',')[:4])\n\t\tx2, y1 = x1 + w, y2 - h\n\t\tline = '{},{},{},{}\\n'.format(x1, y1, w, h)\n\t\tns.append(line)\n\treturn ns\n\ncount = 0\nfor path, dirs, files in os.walk(vid_path):\n\tfor filename in files:\n\t\tif filename.find('JPEG') != -1:\n\t\t\tnewname = filename.split('.')[0]+'.jpg'\n\t\telif filename.find('txt') != -1:\n\t\t\tnewname = 'groundtruth.txt'\n\t\telse:\n\t\t\tnewname = filename\n\t\told = os.path.join(path, filename)\n\t\tnew = os.path.join(path, newname)\n\t\tos.rename(old, new)\n\n\t\tif filename.find('txt') != -1:\n\t\t\twith open(new, 'r') as f:\n\t\t\t\tlines = f.readlines()\n\t\t\t\tns = remove_space(lines)\n\t\t\t\tn  = trans(ns)\n\n\t\t\twith open(new, 'w') as f:\n\t\t\t\tf.writelines(n)\n\n\t\tcount += 1\n\t\tprint(count)"""
script/process_vot15_gt.py,1,"b""# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-10-30 14:37:35\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-10-30 14:47:14\nimport os\ndata_root = '/home/song/workspace/srpn/OTB2015/otb15'\n\nfor d in os.listdir(data_root):\n\tdir_path = os.path.join(data_root, d)\n\tos.chdir(dir_path)\n\tos.system('mkdir label')\n\tgt_dir_path = os.path.join(dir_path, 'label')\n\tif not os.path.exists(gt_dir_path):\n\t\tos.path.mkdir(gt_dir_path)\n\n\n\tgt_path = os.path.join(dir_path, 'groundtruth.txt')\n\twith open(gt_path, 'r') as f:\n\t\tcontents = f.readlines()\n\n\tfor idx, line in enumerate(contents):\n\t\tprint(line.strip('\\n'))\n\t\tname = '{:08d}.txt'.format(idx+1)\n\t\tnew_gt_path = os.path.join(gt_dir_path, name)\n\t\twith open(new_gt_path, 'w') as f:\n\t\t\tf.write(line)\n\t\t\n\n\n\n\n\n"""
script/process_vot15_img.py,0,"b""# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-10-30 14:36:57\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-10-30 14:37:54\nimport os\nimport os.path as osp\ndata_root = '/home/song/workspace/srpn/OTB2015/otb15'\n\nfor d in os.listdir(data_root):\n\tif os.path.isdir(osp.join(data_root, d)):\n\t\t\n\t\tnew_dir = osp.join(data_root, d)\n\t\tos.chdir(new_dir)\n\t\tif not os.path.exists(osp.join(data_root, 'img')):\n\t\t\tos.system('mkdir img && mv *.jpg ./img')\n\n"""
script/show_img.py,0,"b""# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-11-22 17:06:59\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-11-22 17:07:51\nfrom PIL import Image\n\npath = '/home/song/srpn/dataset/vid/ILSVRC2015_VID_train_0000/ILSVRC2015_train_00139005/000191.jpg'\n\nim = Image.open(path)\nim.show()"""
script/unzip_otb15.py,0,"b""# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-10-30 16:51:20\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-10-30 16:52:32\nimport os\nroot = '/home/song/workspace/srpn/OTB2015/otb15'\n\nos.chdir(root)\nfor file in os.listdir(root):\n\tos.system('unzip {}'.format(file))\n"""
script/vis_gt_box.py,0,"b""# -*- coding: utf-8 -*-\n# @Author: Song Dejia\n# @Date:   2018-10-30 19:14:37\n# @Last Modified by:   Song Dejia\n# @Last Modified time: 2018-11-22 15:12:36\nimport os\nimport cv2\nimport sys\nimport numpy as np\nfrom PIL import Image, ImageDraw\ndata_dir = '/home/song/srpn/dataset/vid/ILSVRC2015_VID_train_0000/ILSVRC2015_train_00010013' # contain img and groundtruth.txt\nsave_dir = '/home/song/srpn/tmp/tmp_script'\n\nimgnames = [name for name in os.listdir(data_dir) if name.find('.jpg') != -1]\nimgnames = sorted(imgnames)\n\ngt_path = os.path.join(data_dir, 'groundtruth.txt')\nwith open(gt_path, 'r') as f:\n\tlines = f.readlines()\n\nfor idx, i in enumerate(imgnames):\n\tprint(idx)\n\t# gt\n\tline = lines[idx]\n\tx1, y1, w, h = [int(float(i)) for i in line.split(',')[:4]]\n\tx2 = x1 + w\n\ty2 = y1 + h \n\t# img\n\timgpath = os.path.join(data_dir, i)\n\tim = Image.open(imgpath)\n\tdraw = ImageDraw.Draw(im)\n\tdraw.line([(x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)], width=1, fill='red')\n\n\tsave_path = os.path.join(save_dir, '{}.jpg'.format(idx))\n\tim.save(save_path)\n"""
